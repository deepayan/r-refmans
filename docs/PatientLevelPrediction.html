<!DOCTYPE html><html lang="en"><head><title>Help for package PatientLevelPrediction</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {PatientLevelPrediction}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#PatientLevelPrediction'><p>PatientLevelPrediction</p></a></li>
<li><a href='#averagePrecision'><p>Calculate the average precision</p></a></li>
<li><a href='#brierScore'><p>brierScore</p></a></li>
<li><a href='#calibrationInLarge'><p>Calculate the calibration in large</p></a></li>
<li><a href='#calibrationLine'><p>calibrationLine</p></a></li>
<li><a href='#computeAuc'><p>Compute the area under the ROC curve</p></a></li>
<li><a href='#computeGridPerformance'><p>Computes grid performance with a specified performance function</p></a></li>
<li><a href='#configurePython'><p>Sets up a python environment to use for PLP (can be conda or venv)</p></a></li>
<li><a href='#covariateSummary'><p>covariateSummary</p></a></li>
<li><a href='#createCohortCovariateSettings'><p>Extracts covariates based on cohorts</p></a></li>
<li><a href='#createDatabaseDetails'><p>Create a setting that holds the details about the cdmDatabase connection for data extraction</p></a></li>
<li><a href='#createDatabaseSchemaSettings'><p>Create the PatientLevelPrediction database result schema settings</p></a></li>
<li><a href='#createDefaultExecuteSettings'><p>Creates default list of settings specifying what parts of runPlp to execute</p></a></li>
<li><a href='#createDefaultSplitSetting'><p>Create the settings for defining how the plpData are split into</p>
test/validation/train sets using default splitting functions
(either random stratified by outcome, time or subject splitting)</a></li>
<li><a href='#createExecuteSettings'><p>Creates list of settings specifying what parts of runPlp to execute</p></a></li>
<li><a href='#createExistingSplitSettings'><p>Create the settings for defining how the plpData are split into</p>
test/validation/train sets using an existing split - good to use for
reproducing results from a different run</a></li>
<li><a href='#createFeatureEngineeringSettings'><p>Create the settings for defining any feature engineering that will be done</p></a></li>
<li><a href='#createGlmModel'><p>createGlmModel</p></a></li>
<li><a href='#createIterativeImputer'><p>Create Iterative Imputer settings</p></a></li>
<li><a href='#createLearningCurve'><p>createLearningCurve</p></a></li>
<li><a href='#createLogSettings'><p>Create the settings for logging the progression of the analysis</p></a></li>
<li><a href='#createModelDesign'><p>Specify settings for developing a single model</p></a></li>
<li><a href='#createNormalizer'><p>Create the settings for normalizing the data @param type The type of normalization to use, either &quot;minmax&quot; or &quot;robust&quot;</p></a></li>
<li><a href='#createPlpResultTables'><p>Create the results tables to store PatientLevelPrediction models and results into a database</p></a></li>
<li><a href='#createPreprocessSettings'><p>Create the settings for preprocessing the trainData.</p></a></li>
<li><a href='#createRandomForestFeatureSelection'><p>Create the settings for random foreat based feature selection</p></a></li>
<li><a href='#createRareFeatureRemover'><p>Create the settings for removing rare features</p></a></li>
<li><a href='#createRestrictPlpDataSettings'><p>createRestrictPlpDataSettings define extra restriction settings when calling getPlpData</p></a></li>
<li><a href='#createSampleSettings'><p>Create the settings for defining how the trainData from <code>splitData</code> are sampled using</p>
default sample functions.</a></li>
<li><a href='#createSimpleImputer'><p>Create Simple Imputer settings</p></a></li>
<li><a href='#createSklearnModel'><p>Plug an existing scikit learn python model into the</p>
PLP framework</a></li>
<li><a href='#createSplineSettings'><p>Create the settings for adding a spline for continuous variables</p></a></li>
<li><a href='#createStratifiedImputationSettings'><p>Create the settings for using stratified imputation.</p></a></li>
<li><a href='#createStudyPopulation'><p>Create a study population</p></a></li>
<li><a href='#createStudyPopulationSettings'><p>create the study population settings</p></a></li>
<li><a href='#createTempModelLoc'><p>Create a temporary model location</p></a></li>
<li><a href='#createUnivariateFeatureSelection'><p>Create the settings for defining any feature selection that will be done</p></a></li>
<li><a href='#createValidationDesign'><p>createValidationDesign - Define the validation design for external validation</p></a></li>
<li><a href='#createValidationSettings'><p>createValidationSettings define optional settings for performing external validation</p></a></li>
<li><a href='#diagnoseMultiplePlp'><p>Run a list of predictions diagnoses</p></a></li>
<li><a href='#diagnosePlp'><p>diagnostic - Investigates the prediction problem settings - use before training a model</p></a></li>
<li><a href='#evaluatePlp'><p>evaluatePlp</p></a></li>
<li><a href='#externalValidateDbPlp'><p>externalValidateDbPlp - Validate a model on new databases</p></a></li>
<li><a href='#extractDatabaseToCsv'><p>Exports all the results from a database into csv files</p></a></li>
<li><a href='#fitPlp'><p>fitPlp</p></a></li>
<li><a href='#getCalibrationSummary'><p>Get a sparse summary of the calibration</p></a></li>
<li><a href='#getCohortCovariateData'><p>Extracts covariates based on cohorts</p></a></li>
<li><a href='#getDemographicSummary'><p>Get a demographic summary</p></a></li>
<li><a href='#getEunomiaPlpData'><p>Create a plpData object from the Eunomia database'</p></a></li>
<li><a href='#getPlpData'><p>Extract the patient level prediction data from the server</p></a></li>
<li><a href='#getPredictionDistribution'><p>Calculates the prediction distribution</p></a></li>
<li><a href='#getPredictionDistribution_binary'><p>Calculates the prediction distribution</p></a></li>
<li><a href='#getThresholdSummary'><p>Calculate all measures for sparse ROC</p></a></li>
<li><a href='#ici'><p>Calculate the Integrated Calibration Index from Austin and Steyerberg</p>
https://onlinelibrary.wiley.com/doi/full/10.1002/sim.8281</a></li>
<li><a href='#insertCsvToDatabase'><p>Function to insert results into a database from csvs</p></a></li>
<li><a href='#insertResultsToSqlite'><p>Create sqlite database with the results</p></a></li>
<li><a href='#iterativeImpute'><p>Imputation</p></a></li>
<li><a href='#listAppend'><p>join two lists</p></a></li>
<li><a href='#listCartesian'><p>Cartesian product</p></a></li>
<li><a href='#loadPlpAnalysesJson'><p>Load the multiple prediction json settings from a file</p></a></li>
<li><a href='#loadPlpData'><p>Load the plpData from a folder</p></a></li>
<li><a href='#loadPlpModel'><p>loads the plp model</p></a></li>
<li><a href='#loadPlpResult'><p>Loads the evalaution dataframe</p></a></li>
<li><a href='#loadPlpShareable'><p>Loads the plp result saved as json/csv files for transparent sharing</p></a></li>
<li><a href='#loadPrediction'><p>Loads the prediction dataframe to json</p></a></li>
<li><a href='#MapIds'><p>Map covariate and row Ids so they start from 1</p></a></li>
<li><a href='#migrateDataModel'><p>Migrate Data model</p></a></li>
<li><a href='#minMaxNormalize'><p>A function that normalizes continous features to have values between 0 and 1</p></a></li>
<li><a href='#modelBasedConcordance'><p>Calculate the model-based concordance, which is a calculation of the expected</p>
discrimination performance of a model under the assumption the model predicts
the &quot;TRUE&quot; outcome as detailed in van Klaveren et al.
https://pubmed.ncbi.nlm.nih.gov/27251001/</a></li>
<li><a href='#outcomeSurvivalPlot'><p>Plot the outcome incidence over time</p></a></li>
<li><a href='#pfi'><p>Permutation Feature Importance</p></a></li>
<li><a href='#plotDemographicSummary'><p>Plot the Observed vs. expected incidence, by age and gender</p></a></li>
<li><a href='#plotF1Measure'><p>Plot the F1 measure efficiency frontier using the sparse thresholdSummary data frame</p></a></li>
<li><a href='#plotGeneralizability'><p>Plot the train/test generalizability diagnostic</p></a></li>
<li><a href='#plotLearningCurve'><p>plotLearningCurve</p></a></li>
<li><a href='#plotNetBenefit'><p>Plot the net benefit</p></a></li>
<li><a href='#plotPlp'><p>Plot all the PatientLevelPrediction plots</p></a></li>
<li><a href='#plotPrecisionRecall'><p>Plot the precision-recall curve using the sparse thresholdSummary data frame</p></a></li>
<li><a href='#plotPredictedPDF'><p>Plot the Predicted probability density function, showing prediction overlap between true and false cases</p></a></li>
<li><a href='#plotPredictionDistribution'><p>Plot the side-by-side boxplots of prediction distribution, by class</p></a></li>
<li><a href='#plotPreferencePDF'><p>Plot the preference score probability density function, showing prediction overlap between true and false cases</p>
#'</a></li>
<li><a href='#plotSmoothCalibration'><p>Plot the smooth calibration as detailed in Calster et al. &quot;A calibration heirarchy for risk models</p>
was defined: from utopia to empirical data&quot; (2016)</a></li>
<li><a href='#plotSparseCalibration'><p>Plot the calibration</p></a></li>
<li><a href='#plotSparseCalibration2'><p>Plot the conventional calibration</p></a></li>
<li><a href='#plotSparseRoc'><p>Plot the ROC curve using the sparse thresholdSummary data frame</p></a></li>
<li><a href='#plotVariableScatterplot'><p>Plot the variable importance scatterplot</p></a></li>
<li><a href='#pmmFit'><p>Predictive mean matching using lasso</p></a></li>
<li><a href='#predictCyclops'><p>Create predictive probabilities</p></a></li>
<li><a href='#predictGlm'><p>predict using a logistic regression model</p></a></li>
<li><a href='#predictPlp'><p>predictPlp</p></a></li>
<li><a href='#preprocessData'><p>A function that wraps around FeatureExtraction::tidyCovariateData to normalise</p>
the data and remove rare or redundant features</a></li>
<li><a href='#print.plpData'><p>Print a plpData object</p></a></li>
<li><a href='#print.summary.plpData'><p>Print a summary.plpData object</p></a></li>
<li><a href='#recalibratePlp'><p>recalibratePlp</p></a></li>
<li><a href='#recalibratePlpRefit'><p>recalibratePlpRefit</p></a></li>
<li><a href='#removeRareFeatures'><p>A function that removes rare features from the data</p></a></li>
<li><a href='#robustNormalize'><p>A function that normalizes continous by the interquartile range and</p>
optionally forces the resulting values to be between -3 and 3 with
f(x) = x / sqrt(1 + (x/3)^2)
'@details uses (value - median) / iqr to normalize the data and then can
applies the function f(x) = x / sqrt(1 + (x/3)^2) to the normalized values.
This forces the values to be between -3 and 3 while preserving the relative
ordering of the values.
based on https://arxiv.org/abs/2407.04491 for more details</a></li>
<li><a href='#runMultiplePlp'><p>Run a list of predictions analyses</p></a></li>
<li><a href='#runPlp'><p>runPlp - Develop and internally evaluate a model using specified settings</p></a></li>
<li><a href='#savePlpAnalysesJson'><p>Save the modelDesignList to a json file</p></a></li>
<li><a href='#savePlpData'><p>Save the plpData to folder</p></a></li>
<li><a href='#savePlpModel'><p>Saves the plp model</p></a></li>
<li><a href='#savePlpResult'><p>Saves the result from runPlp into the location directory</p></a></li>
<li><a href='#savePlpShareable'><p>Save the plp result as json files and csv files for transparent sharing</p></a></li>
<li><a href='#savePrediction'><p>Saves the prediction dataframe to a json file</p></a></li>
<li><a href='#setAdaBoost'><p>Create setting for AdaBoost with python DecisionTreeClassifier base estimator</p></a></li>
<li><a href='#setCoxModel'><p>Create setting for lasso Cox model</p></a></li>
<li><a href='#setDecisionTree'><p>Create setting for the scikit-learn DecisionTree with python</p></a></li>
<li><a href='#setGradientBoostingMachine'><p>Create setting for gradient boosting machine model using gbm_xgboost implementation</p></a></li>
<li><a href='#setIterativeHardThresholding'><p>Create setting for Iterative Hard Thresholding model</p></a></li>
<li><a href='#setLassoLogisticRegression'><p>Create modelSettings for lasso logistic regression</p></a></li>
<li><a href='#setLightGBM'><p>Create setting for gradient boosting machine model using lightGBM</p>
(https://github.com/microsoft/LightGBM/tree/master/R-package).</a></li>
<li><a href='#setMLP'><p>Create setting for neural network model with python's scikit-learn. For</p>
bigger models, consider using <code>DeepPatientLevelPrediction</code> package.</a></li>
<li><a href='#setNaiveBayes'><p>Create setting for naive bayes model with python</p></a></li>
<li><a href='#setPythonEnvironment'><p>Use the python environment created using configurePython()</p></a></li>
<li><a href='#setRandomForest'><p>Create setting for random forest model using sklearn</p></a></li>
<li><a href='#setSVM'><p>Create setting for the python sklearn SVM (SVC function)</p></a></li>
<li><a href='#simpleImpute'><p>Simple Imputation</p></a></li>
<li><a href='#simulatePlpData'><p>Generate simulated data</p></a></li>
<li><a href='#simulationProfile'><p>A simulation profile for generating synthetic patient level prediction data</p></a></li>
<li><a href='#sklearnFromJson'><p>Loads sklearn python model from json</p></a></li>
<li><a href='#sklearnToJson'><p>Saves sklearn python model object to json in path</p></a></li>
<li><a href='#splitData'><p>Split the plpData into test/train sets using a splitting settings of class</p>
<code>splitSettings</code></a></li>
<li><a href='#summary.plpData'><p>Summarize a plpData object</p></a></li>
<li><a href='#toSparseM'><p>Convert the plpData in COO format into a sparse R matrix</p></a></li>
<li><a href='#validateExternal'><p>validateExternal - Validate model performance on new data</p></a></li>
<li><a href='#validateMultiplePlp'><p>externally validate the multiple plp models across new datasets</p></a></li>
<li><a href='#viewDatabaseResultPlp'><p>open a local shiny app for viewing the result of a PLP analyses from a database</p></a></li>
<li><a href='#viewMultiplePlp'><p>open a local shiny app for viewing the result of a multiple PLP analyses</p></a></li>
<li><a href='#viewPlp'><p>viewPlp - Interactively view the performance and model settings</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Develop Clinical Prediction Models Using the Common Data Model</td>
</tr>
<tr>
<td>Version:</td>
<td>6.4.0</td>
</tr>
<tr>
<td>Date:</td>
<td>2025-02-05</td>
</tr>
<tr>
<td>Description:</td>
<td>A user friendly way to create patient level prediction models using
  the Observational Medical Outcomes Partnership Common Data Model. Given a cohort
  of interest and an outcome of interest, the package can use data in the Common
  Data Model to build a large set of features. These features can then be used to
  fit a predictive model with a number of machine learning algorithms. This is
  further described in Reps (2017) &lt;<a href="https://doi.org/10.1093%2Fjamia%2Focy032">doi:10.1093/jamia/ocy032</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.apache.org/licenses/LICENSE-2.0">Apache License 2.0</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://ohdsi.github.io/PatientLevelPrediction/">https://ohdsi.github.io/PatientLevelPrediction/</a>,
<a href="https://github.com/OHDSI/PatientLevelPrediction">https://github.com/OHDSI/PatientLevelPrediction</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/OHDSI/PatientLevelPrediction/issues">https://github.com/OHDSI/PatientLevelPrediction/issues</a></td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.0.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>Andromeda, Cyclops (&ge; 3.0.0), DatabaseConnector (&ge; 6.0.0),
digest, dplyr, FeatureExtraction (&ge; 3.0.0), Matrix, memuse,
ParallelLogger (&ge; 2.0.0), pROC, PRROC, rlang, SqlRender (&ge;
1.1.3), tidyr, utils</td>
</tr>
<tr>
<td>Suggests:</td>
<td>curl, Eunomia (&ge; 2.0.0), glmnet, ggplot2, gridExtra,
IterativeHardThresholding, knitr, lightgbm, Metrics, mgcv,
OhdsiShinyAppBuilder (&ge; 1.0.0), parallel, polspline, readr,
ResourceSelection, ResultModelManager (&ge; 0.2.0), reticulate
(&ge; 1.30), rmarkdown, RSQLite, scoring, survival, survminer,
testthat, withr, xgboost (&gt; 1.3.2.1)</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.2</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2025-02-10 11:35:44 UTC; egill</td>
</tr>
<tr>
<td>Author:</td>
<td>Egill Fridgeirsson [aut, cre],
  Jenna Reps [aut],
  Martijn Schuemie [aut],
  Marc Suchard [aut],
  Patrick Ryan [aut],
  Peter Rijnbeek [aut],
  Observational Health Data Science and Informatics [cph]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Egill Fridgeirsson &lt;e.fridgeirsson@erasmusmc.nl&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2025-02-11 09:30:14 UTC</td>
</tr>
</table>
<hr>
<h2 id='PatientLevelPrediction'>PatientLevelPrediction</h2><span id='topic+PatientLevelPrediction-package'></span><span id='topic+PatientLevelPrediction'></span>

<h3>Description</h3>

<p>A package for running predictions using data in the OMOP CDM
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Egill Fridgeirsson <a href="mailto:e.fridgeirsson@erasmusmc.nl">e.fridgeirsson@erasmusmc.nl</a>
</p>
<p>Authors:
</p>

<ul>
<li><p> Jenna Reps <a href="mailto:jreps@its.jnj.com">jreps@its.jnj.com</a>
</p>
</li>
<li><p> Martijn Schuemie
</p>
</li>
<li><p> Marc Suchard
</p>
</li>
<li><p> Patrick Ryan
</p>
</li>
<li><p> Peter Rijnbeek
</p>
</li></ul>

<p>Other contributors:
</p>

<ul>
<li><p> Observational Health Data Science and Informatics [copyright holder]
</p>
</li></ul>



<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://ohdsi.github.io/PatientLevelPrediction/">https://ohdsi.github.io/PatientLevelPrediction/</a>
</p>
</li>
<li> <p><a href="https://github.com/OHDSI/PatientLevelPrediction">https://github.com/OHDSI/PatientLevelPrediction</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/OHDSI/PatientLevelPrediction/issues">https://github.com/OHDSI/PatientLevelPrediction/issues</a>
</p>
</li></ul>


<hr>
<h2 id='averagePrecision'>Calculate the average precision</h2><span id='topic+averagePrecision'></span>

<h3>Description</h3>

<p>Calculate the average precision
</p>


<h3>Usage</h3>

<pre><code class='language-R'>averagePrecision(prediction)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="averagePrecision_+3A_prediction">prediction</code></td>
<td>
<p>A prediction object</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Calculates the average precision from a predition object
</p>


<h3>Value</h3>

<p>The average precision value
</p>


<h3>Examples</h3>

<pre><code class='language-R'>prediction &lt;- data.frame(
  value = c(0.1, 0.2, 0.3, 0.4, 0.5),
  outcomeCount = c(0, 1, 0, 1, 1)
)
averagePrecision(prediction)
</code></pre>

<hr>
<h2 id='brierScore'>brierScore</h2><span id='topic+brierScore'></span>

<h3>Description</h3>

<p>brierScore
</p>


<h3>Usage</h3>

<pre><code class='language-R'>brierScore(prediction)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="brierScore_+3A_prediction">prediction</code></td>
<td>
<p>A prediction dataframe</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Calculates the brierScore from prediction object
</p>


<h3>Value</h3>

<p>A list containing the brier score and the scaled brier score
</p>


<h3>Examples</h3>

<pre><code class='language-R'>prediction &lt;- data.frame(
  value = c(0.1, 0.2, 0.3, 0.4, 0.5),
  outcomeCount = c(0, 1, 0, 1, 1))
brierScore(prediction)
</code></pre>

<hr>
<h2 id='calibrationInLarge'>Calculate the calibration in large</h2><span id='topic+calibrationInLarge'></span>

<h3>Description</h3>

<p>Calculate the calibration in large
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calibrationInLarge(prediction)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="calibrationInLarge_+3A_prediction">prediction</code></td>
<td>
<p>A prediction dataframe</p>
</td></tr>
</table>


<h3>Value</h3>

<p>data.frame with meanPredictionRisk, observedRisk, and N
</p>

<hr>
<h2 id='calibrationLine'>calibrationLine</h2><span id='topic+calibrationLine'></span>

<h3>Description</h3>

<p>calibrationLine
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calibrationLine(prediction, numberOfStrata = 10)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="calibrationLine_+3A_prediction">prediction</code></td>
<td>
<p>A prediction object</p>
</td></tr>
<tr><td><code id="calibrationLine_+3A_numberofstrata">numberOfStrata</code></td>
<td>
<p>The number of groups to split the prediction into</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing the calibrationLine coefficients, the aggregate data used
to fit the line and the Hosmer-Lemeshow goodness of fit test
</p>


<h3>Examples</h3>

<pre><code class='language-R'>prediction &lt;- data.frame(
  value = c(0.1, 0.2, 0.3, 0.4, 0.5),
  outcomeCount = c(0, 1, 0, 1, 1))
calibrationLine(prediction, numberOfStrata = 1)
</code></pre>

<hr>
<h2 id='computeAuc'>Compute the area under the ROC curve</h2><span id='topic+computeAuc'></span>

<h3>Description</h3>

<p>Compute the area under the ROC curve
</p>


<h3>Usage</h3>

<pre><code class='language-R'>computeAuc(prediction, confidenceInterval = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="computeAuc_+3A_prediction">prediction</code></td>
<td>
<p>A prediction object as generated using the
<code><a href="stats.html#topic+predict">predict</a></code> functions.</p>
</td></tr>
<tr><td><code id="computeAuc_+3A_confidenceinterval">confidenceInterval</code></td>
<td>
<p>Should 95 percebt confidence intervals be computed?</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Computes the area under the ROC curve for the predicted probabilities, given the true observed
outcomes.
</p>


<h3>Value</h3>

<p>A data.frame containing the AUC and optionally the 95% confidence interval
</p>


<h3>Examples</h3>

<pre><code class='language-R'>prediction &lt;- data.frame(
  value = c(0.1, 0.2, 0.3, 0.4, 0.5),
  outcomeCount = c(0, 1, 0, 1, 1))
computeAuc(prediction)
</code></pre>

<hr>
<h2 id='computeGridPerformance'>Computes grid performance with a specified performance function</h2><span id='topic+computeGridPerformance'></span>

<h3>Description</h3>

<p>Computes grid performance with a specified performance function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>computeGridPerformance(prediction, param, performanceFunct = "computeAuc")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="computeGridPerformance_+3A_prediction">prediction</code></td>
<td>
<p>a dataframe with predictions and outcomeCount per rowId</p>
</td></tr>
<tr><td><code id="computeGridPerformance_+3A_param">param</code></td>
<td>
<p>a list of hyperparameters</p>
</td></tr>
<tr><td><code id="computeGridPerformance_+3A_performancefunct">performanceFunct</code></td>
<td>
<p>a string specifying which performance function to use
. Default <code>'compute_AUC'</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with overview of the performance
</p>


<h3>Examples</h3>

<pre><code class='language-R'>prediction &lt;- data.frame(rowId = c(1, 2, 3, 4, 5),
                         outcomeCount = c(0, 1, 0, 1, 0),
                         value = c(0.1, 0.9, 0.2, 0.8, 0.3),
                         index = c(1, 1, 1, 1, 1))
param &lt;- list(hyperParam1 = 5, hyperParam2 = 100)
computeGridPerformance(prediction, param, performanceFunct = "computeAuc")
</code></pre>

<hr>
<h2 id='configurePython'>Sets up a python environment to use for PLP (can be conda or venv)</h2><span id='topic+configurePython'></span>

<h3>Description</h3>

<p>Sets up a python environment to use for PLP (can be conda or venv)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>configurePython(envname = "PLP", envtype = NULL, condaPythonVersion = "3.11")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="configurePython_+3A_envname">envname</code></td>
<td>
<p>A string for the name of the virtual environment (default is 'PLP')</p>
</td></tr>
<tr><td><code id="configurePython_+3A_envtype">envtype</code></td>
<td>
<p>An option for specifying the environment as'conda' or 'python'.  If NULL then the default is 'conda' for windows users and 'python' for non-windows users</p>
</td></tr>
<tr><td><code id="configurePython_+3A_condapythonversion">condaPythonVersion</code></td>
<td>
<p>String, Python version to use when creating a conda environment</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function creates a python environment that can be used by PatientLevelPrediction
and installs all the required package dependancies.
</p>


<h3>Value</h3>

<p>location of the created conda or virtual python environment
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run:  
 configurePython(envname="PLP", envtype="conda")

## End(Not run)
</code></pre>

<hr>
<h2 id='covariateSummary'>covariateSummary</h2><span id='topic+covariateSummary'></span>

<h3>Description</h3>

<p>Summarises the covariateData to calculate the mean and standard deviation per covariate
if the labels are given it also stratifies this by class label and if the trainRowIds and testRowIds
specifying the patients in the train/test sets respectively are input, these values are also stratified
by train and test set
</p>


<h3>Usage</h3>

<pre><code class='language-R'>covariateSummary(
  covariateData,
  cohort,
  labels = NULL,
  strata = NULL,
  variableImportance = NULL,
  featureEngineering = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="covariateSummary_+3A_covariatedata">covariateData</code></td>
<td>
<p>The covariateData part of the plpData that is
extracted using <code>getPlpData</code></p>
</td></tr>
<tr><td><code id="covariateSummary_+3A_cohort">cohort</code></td>
<td>
<p>The patient cohort to calculate the summary</p>
</td></tr>
<tr><td><code id="covariateSummary_+3A_labels">labels</code></td>
<td>
<p>A data.frame with the columns rowId and outcomeCount</p>
</td></tr>
<tr><td><code id="covariateSummary_+3A_strata">strata</code></td>
<td>
<p>A data.frame containing the columns rowId, strataName</p>
</td></tr>
<tr><td><code id="covariateSummary_+3A_variableimportance">variableImportance</code></td>
<td>
<p>A data.frame with the columns covariateId and
value (the variable importance value)</p>
</td></tr>
<tr><td><code id="covariateSummary_+3A_featureengineering">featureEngineering</code></td>
<td>
<p>(currently not used )
A function or list of functions specifying any feature engineering
to create covariates before summarising</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function calculates various metrics to measure the performance of the model
</p>


<h3>Value</h3>

<p>A data.frame containing: CovariateCount, CovariateMean and CovariateStDev
for any specified stratification
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("simulationProfile")
plpData &lt;- simulatePlpData(simulationProfile, n=100)
covariateSummary &lt;- covariateSummary(plpData$covariateData, plpData$cohorts)
head(covariateSummary)
</code></pre>

<hr>
<h2 id='createCohortCovariateSettings'>Extracts covariates based on cohorts</h2><span id='topic+createCohortCovariateSettings'></span>

<h3>Description</h3>

<p>Extracts covariates based on cohorts
</p>


<h3>Usage</h3>

<pre><code class='language-R'>createCohortCovariateSettings(
  cohortName,
  settingId,
  cohortDatabaseSchema = NULL,
  cohortTable = NULL,
  cohortId,
  startDay = -30,
  endDay = 0,
  count = FALSE,
  ageInteraction = FALSE,
  lnAgeInteraction = FALSE,
  analysisId = 456
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="createCohortCovariateSettings_+3A_cohortname">cohortName</code></td>
<td>
<p>Name for the cohort</p>
</td></tr>
<tr><td><code id="createCohortCovariateSettings_+3A_settingid">settingId</code></td>
<td>
<p>A unique id for the covariate time and</p>
</td></tr>
<tr><td><code id="createCohortCovariateSettings_+3A_cohortdatabaseschema">cohortDatabaseSchema</code></td>
<td>
<p>The schema of the database with the cohort. If
nothing is specified then the cohortDatabaseSchema from databaseDetails at runtime is used.</p>
</td></tr>
<tr><td><code id="createCohortCovariateSettings_+3A_cohorttable">cohortTable</code></td>
<td>
<p>the table name that contains the covariate cohort. If
nothing is specified then the cohortTable from databaseDetails at runtime is used.</p>
</td></tr>
<tr><td><code id="createCohortCovariateSettings_+3A_cohortid">cohortId</code></td>
<td>
<p>cohort id for the covariate cohort</p>
</td></tr>
<tr><td><code id="createCohortCovariateSettings_+3A_startday">startDay</code></td>
<td>
<p>The number of days prior to index to start observing the cohort</p>
</td></tr>
<tr><td><code id="createCohortCovariateSettings_+3A_endday">endDay</code></td>
<td>
<p>The number of days prior to index to stop observing the cohort</p>
</td></tr>
<tr><td><code id="createCohortCovariateSettings_+3A_count">count</code></td>
<td>
<p>If FALSE the covariate value is binary (1 means cohort occurred between index+startDay and index+endDay, 0 means it did not)
If TRUE then the covariate value is the number of unique cohort_start_dates between index+startDay and index+endDay</p>
</td></tr>
<tr><td><code id="createCohortCovariateSettings_+3A_ageinteraction">ageInteraction</code></td>
<td>
<p>If TRUE multiple covariate value by the patient's age in years</p>
</td></tr>
<tr><td><code id="createCohortCovariateSettings_+3A_lnageinteraction">lnAgeInteraction</code></td>
<td>
<p>If TRUE multiple covariate value by the log of the patient's age in years</p>
</td></tr>
<tr><td><code id="createCohortCovariateSettings_+3A_analysisid">analysisId</code></td>
<td>
<p>The analysisId for the covariate</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The user specifies a cohort and time period and then a covariate is constructed whether they are in the
cohort during the time periods relative to target population cohort index
</p>


<h3>Value</h3>

<p>An object of class <code>covariateSettings</code> specifying how to create the cohort covariate with the covariateId
cohortId x 100000 + settingId x 1000 + analysisId
</p>


<h3>Examples</h3>

<pre><code class='language-R'>createCohortCovariateSettings(cohortName="testCohort",
                              settingId=1,
                              cohortId=1,
                              cohortDatabaseSchema="cohorts",
                              cohortTable="cohort_table")
</code></pre>

<hr>
<h2 id='createDatabaseDetails'>Create a setting that holds the details about the cdmDatabase connection for data extraction</h2><span id='topic+createDatabaseDetails'></span>

<h3>Description</h3>

<p>Create a setting that holds the details about the cdmDatabase connection for data extraction
</p>


<h3>Usage</h3>

<pre><code class='language-R'>createDatabaseDetails(
  connectionDetails,
  cdmDatabaseSchema,
  cdmDatabaseName,
  cdmDatabaseId,
  tempEmulationSchema = cdmDatabaseSchema,
  cohortDatabaseSchema = cdmDatabaseSchema,
  cohortTable = "cohort",
  outcomeDatabaseSchema = cdmDatabaseSchema,
  outcomeTable = cohortTable,
  targetId = NULL,
  outcomeIds = NULL,
  cdmVersion = 5,
  cohortId = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="createDatabaseDetails_+3A_connectiondetails">connectionDetails</code></td>
<td>
<p>An R object of type <code>connectionDetails</code> created using the
function <code>createConnectionDetails</code> in the
<code>DatabaseConnector</code> package.</p>
</td></tr>
<tr><td><code id="createDatabaseDetails_+3A_cdmdatabaseschema">cdmDatabaseSchema</code></td>
<td>
<p>The name of the database schema that contains the OMOP CDM
instance. Requires read permissions to this database. On SQL
Server, this should specifiy both the database and the schema,
so for example 'cdm_instance.dbo'.</p>
</td></tr>
<tr><td><code id="createDatabaseDetails_+3A_cdmdatabasename">cdmDatabaseName</code></td>
<td>
<p>A string with the name of the database - this is used in the shiny app and when externally validating models to name the result list and to specify the folder name when saving validation results (defaults to cdmDatabaseSchema if not specified)</p>
</td></tr>
<tr><td><code id="createDatabaseDetails_+3A_cdmdatabaseid">cdmDatabaseId</code></td>
<td>
<p>A string with a unique identifier for the database and version - this is stored in the plp object for future reference and used by the shiny app (defaults to cdmDatabaseSchema if not specified)</p>
</td></tr>
<tr><td><code id="createDatabaseDetails_+3A_tempemulationschema">tempEmulationSchema</code></td>
<td>
<p>For dmbs like Oracle only: the name of the database schema where you
want all temporary tables to be managed. Requires
create/insert permissions to this database.</p>
</td></tr>
<tr><td><code id="createDatabaseDetails_+3A_cohortdatabaseschema">cohortDatabaseSchema</code></td>
<td>
<p>The name of the database schema that is the location where the
target cohorts are available.  Requires read
permissions to this database.</p>
</td></tr>
<tr><td><code id="createDatabaseDetails_+3A_cohorttable">cohortTable</code></td>
<td>
<p>The tablename that contains the target cohorts.  Expectation is cohortTable
has format of COHORT table: COHORT_DEFINITION_ID, SUBJECT_ID,
COHORT_START_DATE, COHORT_END_DATE.</p>
</td></tr>
<tr><td><code id="createDatabaseDetails_+3A_outcomedatabaseschema">outcomeDatabaseSchema</code></td>
<td>
<p>The name of the database schema that is the location where the
data used to define the outcome cohorts is available. Requires read permissions to
this database.</p>
</td></tr>
<tr><td><code id="createDatabaseDetails_+3A_outcometable">outcomeTable</code></td>
<td>
<p>The tablename that contains the outcome cohorts.  Expectation is
outcomeTable has format of COHORT table: COHORT_DEFINITION_ID,
SUBJECT_ID, COHORT_START_DATE, COHORT_END_DATE.</p>
</td></tr>
<tr><td><code id="createDatabaseDetails_+3A_targetid">targetId</code></td>
<td>
<p>An integer specifying the cohort id for the target cohort</p>
</td></tr>
<tr><td><code id="createDatabaseDetails_+3A_outcomeids">outcomeIds</code></td>
<td>
<p>A single integer or vector of integers specifying the cohort ids for the outcome cohorts</p>
</td></tr>
<tr><td><code id="createDatabaseDetails_+3A_cdmversion">cdmVersion</code></td>
<td>
<p>Define the OMOP CDM version used: currently support &quot;4&quot; and &quot;5&quot;.</p>
</td></tr>
<tr><td><code id="createDatabaseDetails_+3A_cohortid">cohortId</code></td>
<td>
<p>(depreciated: use targetId) old input for the target cohort id</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function simply stores the settings for communicating with the cdmDatabase when extracting
the target cohort and outcomes
</p>


<h3>Value</h3>

<p>A list with the the database specific settings: </p>

<ul>
<li><p><code>connectionDetails</code>: An R object of type <code>connectionDetails</code> created using the function <code>createConnectionDetails</code> in the <code>DatabaseConnector</code> package.
</p>
</li>
<li><p><code>cdmDatabaseSchema</code>: The name of the database schema that contains the OMOP CDM instance.
</p>
</li>
<li><p><code>cdmDatabaseName</code>: A string with the name of the database - this is used in the shiny app and when externally validating models to name the result list and to specify the folder name when saving validation results (defaults to cdmDatabaseSchema if not specified).
</p>
</li>
<li><p><code>cdmDatabaseId</code>: A string with a unique identifier for the database and version - this is stored in the plp object for future reference and used by the shiny app (defaults to cdmDatabaseSchema if not specified).
</p>
</li>
<li><p><code>tempEmulationSchema</code>: The name of a databae schema where you want all temporary tables to be managed. Requires create/insert permissions to this database.
</p>
</li>
<li><p><code>cohortDatabaseSchema</code>: The name of the database schema that is the location where the target cohorts are available. Requires read permissions to this schema.
</p>
</li>
<li><p><code>cohortTable</code>: The tablename that contains the target cohorts. Expectation is cohortTable has format of COHORT table: COHORT_DEFINITION_ID, SUBJECT_ID, COHORT_START_DATE, COHORT_END_DATE.
</p>
</li>
<li><p><code>outcomeDatabaseSchema</code>: The name of the database schema that is the location where the data used to define the outcome cohorts is available. Requires read permissions to this database.
</p>
</li>
<li><p><code>outcomeTable</code>: The tablename that contains the outcome cohorts. Expectation is outcomeTable has format of COHORT table: COHORT_DEFINITION_ID, SUBJECT_ID, COHORT_START_DATE, COHORT_END_DATE.
</p>
</li>
<li><p><code>targetId</code>: An integer specifying the cohort id for the target cohort
</p>
</li>
<li><p><code>outcomeIds</code>: A single integer or vector of integers specifying the cohort ids for the outcome cohorts
</p>
</li>
<li><p><code>cdmVersion</code>: Define the OMOP CDM version used: currently support &quot;4&quot; and &quot;5&quot;.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>
 
connectionDetails &lt;- Eunomia::getEunomiaConnectionDetails()
# create the database details for Eunomia example database
createDatabaseDetails(
  connectionDetails = connectionDetails,
  cdmDatabaseSchema = "main",
  cdmDatabaseName = "main",
  cohortDatabaseSchema = "main",
  cohortTable = "cohort",
  outcomeDatabaseSchema = "main",
  outcomeTable = "cohort",
  targetId = 1, # users of celecoxib
  outcomeIds = 3, # GIbleed
  cdmVersion = 5)


</code></pre>

<hr>
<h2 id='createDatabaseSchemaSettings'>Create the PatientLevelPrediction database result schema settings</h2><span id='topic+createDatabaseSchemaSettings'></span>

<h3>Description</h3>

<p>This function specifies where the results schema is and lets you pick a different schema for the cohorts and databases
</p>


<h3>Usage</h3>

<pre><code class='language-R'>createDatabaseSchemaSettings(
  resultSchema = "main",
  tablePrefix = "",
  targetDialect = "sqlite",
  tempEmulationSchema = getOption("sqlRenderTempEmulationSchema"),
  cohortDefinitionSchema = resultSchema,
  tablePrefixCohortDefinitionTables = tablePrefix,
  databaseDefinitionSchema = resultSchema,
  tablePrefixDatabaseDefinitionTables = tablePrefix
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="createDatabaseSchemaSettings_+3A_resultschema">resultSchema</code></td>
<td>
<p>(string) The name of the database schema with the result tables.</p>
</td></tr>
<tr><td><code id="createDatabaseSchemaSettings_+3A_tableprefix">tablePrefix</code></td>
<td>
<p>(string) A string that appends to the PatientLevelPrediction result tables</p>
</td></tr>
<tr><td><code id="createDatabaseSchemaSettings_+3A_targetdialect">targetDialect</code></td>
<td>
<p>(string) The database management system being used</p>
</td></tr>
<tr><td><code id="createDatabaseSchemaSettings_+3A_tempemulationschema">tempEmulationSchema</code></td>
<td>
<p>(string) The temp schema used when the database management system is oracle</p>
</td></tr>
<tr><td><code id="createDatabaseSchemaSettings_+3A_cohortdefinitionschema">cohortDefinitionSchema</code></td>
<td>
<p>(string) The name of the database schema with the cohort definition tables (defaults to resultSchema).</p>
</td></tr>
<tr><td><code id="createDatabaseSchemaSettings_+3A_tableprefixcohortdefinitiontables">tablePrefixCohortDefinitionTables</code></td>
<td>
<p>(string) A string that appends to the cohort definition tables</p>
</td></tr>
<tr><td><code id="createDatabaseSchemaSettings_+3A_databasedefinitionschema">databaseDefinitionSchema</code></td>
<td>
<p>(string) The name of the database schema with the database definition tables (defaults to resultSchema).</p>
</td></tr>
<tr><td><code id="createDatabaseSchemaSettings_+3A_tableprefixdatabasedefinitiontables">tablePrefixDatabaseDefinitionTables</code></td>
<td>
<p>(string) A string that appends to the database definition tables</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function can be used to specify the database settings used to upload PatientLevelPrediction results into a database
</p>


<h3>Value</h3>

<p>Returns a list of class 'plpDatabaseResultSchema' with all the database settings
</p>


<h3>Examples</h3>

<pre><code class='language-R'>createDatabaseSchemaSettings(resultSchema = "cdm",
                             tablePrefix = "plp_")

</code></pre>

<hr>
<h2 id='createDefaultExecuteSettings'>Creates default list of settings specifying what parts of runPlp to execute</h2><span id='topic+createDefaultExecuteSettings'></span>

<h3>Description</h3>

<p>Creates default list of settings specifying what parts of runPlp to execute
</p>


<h3>Usage</h3>

<pre><code class='language-R'>createDefaultExecuteSettings()
</code></pre>


<h3>Details</h3>

<p>runs split, preprocess, model development and covariate summary
</p>


<h3>Value</h3>

<p>list with TRUE for split, preprocess, model development and covariate summary
</p>


<h3>Examples</h3>

<pre><code class='language-R'>createDefaultExecuteSettings()
</code></pre>

<hr>
<h2 id='createDefaultSplitSetting'>Create the settings for defining how the plpData are split into
test/validation/train sets using default splitting functions
(either random stratified by outcome, time or subject splitting)</h2><span id='topic+createDefaultSplitSetting'></span>

<h3>Description</h3>

<p>Create the settings for defining how the plpData are split into
test/validation/train sets using default splitting functions
(either random stratified by outcome, time or subject splitting)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>createDefaultSplitSetting(
  testFraction = 0.25,
  trainFraction = 0.75,
  splitSeed = sample(1e+05, 1),
  nfold = 3,
  type = "stratified"
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="createDefaultSplitSetting_+3A_testfraction">testFraction</code></td>
<td>
<p>(numeric) A real number between 0 and 1
indicating the test set fraction of the data</p>
</td></tr>
<tr><td><code id="createDefaultSplitSetting_+3A_trainfraction">trainFraction</code></td>
<td>
<p>(numeric) A real number between 0 and 1 indicating the
train set fraction of the data. If not set train is equal to 1 - test</p>
</td></tr>
<tr><td><code id="createDefaultSplitSetting_+3A_splitseed">splitSeed</code></td>
<td>
<p>(numeric) A seed to use when splitting the data for
reproducibility (if not set a random number will be generated)</p>
</td></tr>
<tr><td><code id="createDefaultSplitSetting_+3A_nfold">nfold</code></td>
<td>
<p>(numeric) An integer &gt; 1 specifying the number of
folds used in cross validation</p>
</td></tr>
<tr><td><code id="createDefaultSplitSetting_+3A_type">type</code></td>
<td>
<p>(character) Choice of: </p>

<ul>
<li><p>'stratified' Each data point is
randomly assigned into the test or a train fold set but this is done
stratified such that the outcome rate is consistent in each partition
</p>
</li>
<li><p>'time' Older data are assigned
into the training set and newer data are assigned into the test set
</p>
</li>
<li><p>'subject' Data are partitioned by
subject, if a subject is in the data more than once, all the data points for
the subject are assigned either into the test data or into the train data
(not both).
</p>
</li></ul>
</td></tr>
</table>


<h3>Details</h3>

<p>Returns an object of class <code>splitSettings</code> that specifies the
splitting function that will be called and the settings
</p>


<h3>Value</h3>

<p>An object of class <code>splitSettings</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>createDefaultSplitSetting(testFraction=0.25, trainFraction=0.75, nfold=3,
                          splitSeed=42)
</code></pre>

<hr>
<h2 id='createExecuteSettings'>Creates list of settings specifying what parts of runPlp to execute</h2><span id='topic+createExecuteSettings'></span>

<h3>Description</h3>

<p>Creates list of settings specifying what parts of runPlp to execute
</p>


<h3>Usage</h3>

<pre><code class='language-R'>createExecuteSettings(
  runSplitData = FALSE,
  runSampleData = FALSE,
  runFeatureEngineering = FALSE,
  runPreprocessData = FALSE,
  runModelDevelopment = FALSE,
  runCovariateSummary = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="createExecuteSettings_+3A_runsplitdata">runSplitData</code></td>
<td>
<p>TRUE or FALSE whether to split data into train/test</p>
</td></tr>
<tr><td><code id="createExecuteSettings_+3A_runsampledata">runSampleData</code></td>
<td>
<p>TRUE or FALSE whether to over or under sample</p>
</td></tr>
<tr><td><code id="createExecuteSettings_+3A_runfeatureengineering">runFeatureEngineering</code></td>
<td>
<p>TRUE or FALSE whether to do feature engineering</p>
</td></tr>
<tr><td><code id="createExecuteSettings_+3A_runpreprocessdata">runPreprocessData</code></td>
<td>
<p>TRUE or FALSE whether to do preprocessing</p>
</td></tr>
<tr><td><code id="createExecuteSettings_+3A_runmodeldevelopment">runModelDevelopment</code></td>
<td>
<p>TRUE or FALSE whether to develop the model</p>
</td></tr>
<tr><td><code id="createExecuteSettings_+3A_runcovariatesummary">runCovariateSummary</code></td>
<td>
<p>TRUE or FALSE whether to create covariate summary</p>
</td></tr>
</table>


<h3>Details</h3>

<p>define what parts of runPlp to execute
</p>


<h3>Value</h3>

<p>list with TRUE/FALSE for each part of runPlp
</p>


<h3>Examples</h3>

<pre><code class='language-R'># create settings with only split and model development
createExecuteSettings(runSplitData = TRUE, runModelDevelopment = TRUE) 
</code></pre>

<hr>
<h2 id='createExistingSplitSettings'>Create the settings for defining how the plpData are split into
test/validation/train sets using an existing split - good to use for
reproducing results from a different run</h2><span id='topic+createExistingSplitSettings'></span>

<h3>Description</h3>

<p>Create the settings for defining how the plpData are split into
test/validation/train sets using an existing split - good to use for
reproducing results from a different run
</p>


<h3>Usage</h3>

<pre><code class='language-R'>createExistingSplitSettings(splitIds)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="createExistingSplitSettings_+3A_splitids">splitIds</code></td>
<td>
<p>(data.frame) A data frame with rowId and index columns of
type integer/numeric. Index is -1 for test set, positive integer for train
set folds</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>splitSettings</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># rowId 1 is in fold 1, rowId 2 is in fold 2, rowId 3 is in the test set
# rowId 4 is in fold 1, rowId 5 is in fold 2
createExistingSplitSettings(splitIds = data.frame(rowId = c(1, 2, 3, 4, 5),
                                                  index = c(1, 2, -1, 1, 2)))
</code></pre>

<hr>
<h2 id='createFeatureEngineeringSettings'>Create the settings for defining any feature engineering that will be done</h2><span id='topic+createFeatureEngineeringSettings'></span>

<h3>Description</h3>

<p>Create the settings for defining any feature engineering that will be done
</p>


<h3>Usage</h3>

<pre><code class='language-R'>createFeatureEngineeringSettings(type = "none")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="createFeatureEngineeringSettings_+3A_type">type</code></td>
<td>
<p>(character) Choice of:  </p>

<ul>
<li><p>'none' No feature engineering - this is the default
</p>
</li></ul>
</td></tr>
</table>


<h3>Details</h3>

<p>Returns an object of class <code>featureEngineeringSettings</code> that specifies the sampling function that will be called and the settings
</p>


<h3>Value</h3>

<p>An object of class <code>featureEngineeringSettings</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>createFeatureEngineeringSettings(type = "none")
</code></pre>

<hr>
<h2 id='createGlmModel'>createGlmModel</h2><span id='topic+createGlmModel'></span>

<h3>Description</h3>

<p>Create a generalized linear model that can be used in the
PatientLevelPrediction package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>createGlmModel(coefficients, intercept = 0, mapping = "logistic")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="createGlmModel_+3A_coefficients">coefficients</code></td>
<td>
<p>A dataframe containing two columns, coefficients and
covariateId, both of type numeric. The covariateId column must contain
valid covariateIds that match those used in the <code>FeatureExtraction</code>
package.</p>
</td></tr>
<tr><td><code id="createGlmModel_+3A_intercept">intercept</code></td>
<td>
<p>A numeric value representing the intercept of the model.</p>
</td></tr>
<tr><td><code id="createGlmModel_+3A_mapping">mapping</code></td>
<td>
<p>A string representing the mapping from the
linear predictors to outcome probabilities. For generalized linear models
this is the inverse of the link function. Supported values is only
&quot;logistic&quot; for logistic regression model at the moment.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A model object containing the model (Coefficients and intercept)
and the prediction function.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>coefficients &lt;- data.frame(
  covariateId = c(1002),
  coefficient = c(0.05))
model &lt;- createGlmModel(coefficients, intercept = -2.5)
data("simulationProfile")
plpData &lt;- simulatePlpData(simulationProfile, n=50)
prediction &lt;- predictPlp(model, plpData, plpData$cohorts)
# see the predicted risk values
prediction$value
</code></pre>

<hr>
<h2 id='createIterativeImputer'>Create Iterative Imputer settings</h2><span id='topic+createIterativeImputer'></span>

<h3>Description</h3>

<p>This function creates the settings for an iterative imputer
which first removes features with more than <code>missingThreshold</code> missing values
and then imputes the missing values iteratively using chained equations
</p>


<h3>Usage</h3>

<pre><code class='language-R'>createIterativeImputer(
  missingThreshold = 0.3,
  method = "pmm",
  methodSettings = list(pmm = list(k = 5, iterations = 5))
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="createIterativeImputer_+3A_missingthreshold">missingThreshold</code></td>
<td>
<p>The threshold for missing values to remove a feature</p>
</td></tr>
<tr><td><code id="createIterativeImputer_+3A_method">method</code></td>
<td>
<p>The method to use for imputation, currently only &quot;pmm&quot; is supported</p>
</td></tr>
<tr><td><code id="createIterativeImputer_+3A_methodsettings">methodSettings</code></td>
<td>
<p>A list of settings for the imputation method to use.
Currently only &quot;pmm&quot; is supported with the following settings:
</p>

<ul>
<li><p> k: The number of donors to use for matching
</p>
</li>
<li><p> iterations: The number of iterations to use for imputation
</p>
</li></ul>
</td></tr>
</table>


<h3>Value</h3>

<p>The settings for the iterative imputer of class <code>featureEngineeringSettings</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># create imputer to impute values with missingness less than 30% using 
# predictive mean matching in 5 iterations with 5 donors
createIterativeImputer(missingThreshold = 0.3, method = "pmm",
                       methodSettings = list(pmm = list(k = 5, iterations = 5)))
</code></pre>

<hr>
<h2 id='createLearningCurve'>createLearningCurve</h2><span id='topic+createLearningCurve'></span>

<h3>Description</h3>

<p>Creates a learning curve object, which can be plotted using the
<code>plotLearningCurve()</code> function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>createLearningCurve(
  plpData,
  outcomeId,
  parallel = TRUE,
  cores = 4,
  modelSettings,
  saveDirectory = NULL,
  analysisId = "learningCurve",
  populationSettings = createStudyPopulationSettings(),
  splitSettings = createDefaultSplitSetting(),
  trainFractions = c(0.25, 0.5, 0.75),
  trainEvents = NULL,
  sampleSettings = createSampleSettings(),
  featureEngineeringSettings = createFeatureEngineeringSettings(),
  preprocessSettings = createPreprocessSettings(minFraction = 0.001, normalize = TRUE),
  logSettings = createLogSettings(),
  executeSettings = createExecuteSettings(runSplitData = TRUE, runSampleData = FALSE,
    runFeatureEngineering = FALSE, runPreprocessData = TRUE, runModelDevelopment = TRUE,
    runCovariateSummary = FALSE)
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="createLearningCurve_+3A_plpdata">plpData</code></td>
<td>
<p>An object of type <code>plpData</code> - the patient level prediction
data extracted from the CDM.</p>
</td></tr>
<tr><td><code id="createLearningCurve_+3A_outcomeid">outcomeId</code></td>
<td>
<p>(integer) The ID of the outcome.</p>
</td></tr>
<tr><td><code id="createLearningCurve_+3A_parallel">parallel</code></td>
<td>
<p>Whether to run the code in parallel</p>
</td></tr>
<tr><td><code id="createLearningCurve_+3A_cores">cores</code></td>
<td>
<p>The number of computer cores to use if running in parallel</p>
</td></tr>
<tr><td><code id="createLearningCurve_+3A_modelsettings">modelSettings</code></td>
<td>
<p>An object of class <code>modelSettings</code> created using one of the function:
</p>

<ul>
<li> <p><code>setLassoLogisticRegression()</code> A lasso logistic regression model
</p>
</li>
<li> <p><code>setGradientBoostingMachine()</code> A gradient boosting machine
</p>
</li>
<li> <p><code>setAdaBoost()</code> An ada boost model
</p>
</li>
<li> <p><code>setRandomForest()</code> A random forest model
</p>
</li>
<li> <p><code>setDecisionTree()</code> A decision tree model
</p>
</li>
<li> <p><code>setKNN()</code> A KNN model
</p>
</li></ul>
</td></tr>
<tr><td><code id="createLearningCurve_+3A_savedirectory">saveDirectory</code></td>
<td>
<p>The path to the directory where the results will be saved (if NULL uses working directory)</p>
</td></tr>
<tr><td><code id="createLearningCurve_+3A_analysisid">analysisId</code></td>
<td>
<p>(integer) Identifier for the analysis. It is used to create, e.g., the result folder. Default is a timestamp.</p>
</td></tr>
<tr><td><code id="createLearningCurve_+3A_populationsettings">populationSettings</code></td>
<td>
<p>An object of type <code>populationSettings</code> created using <code>createStudyPopulationSettings</code> that
specifies how the data class labels are defined and addition any exclusions to apply to the
plpData cohort</p>
</td></tr>
<tr><td><code id="createLearningCurve_+3A_splitsettings">splitSettings</code></td>
<td>
<p>An object of type <code>splitSettings</code> that specifies how to split the data into train/validation/test.
The default settings can be created using <code>createDefaultSplitSetting</code>.</p>
</td></tr>
<tr><td><code id="createLearningCurve_+3A_trainfractions">trainFractions</code></td>
<td>
<p>A list of training fractions to create models for.
Note, providing <code>trainEvents</code> will override your input to
<code>trainFractions</code>.</p>
</td></tr>
<tr><td><code id="createLearningCurve_+3A_trainevents">trainEvents</code></td>
<td>
<p>Events have shown to be determinant of model performance.
Therefore, it is recommended to provide <code>trainEvents</code> rather than
<code>trainFractions</code>. Note, providing <code>trainEvents</code> will override
your input to <code>trainFractions</code>. The format should be as follows:
</p>

<ul>
<li> <p><code>c(500, 1000, 1500) </code> - a list of training events
</p>
</li></ul>
</td></tr>
<tr><td><code id="createLearningCurve_+3A_samplesettings">sampleSettings</code></td>
<td>
<p>An object of type <code>sampleSettings</code> that specifies any under/over sampling to be done.
The default is none.</p>
</td></tr>
<tr><td><code id="createLearningCurve_+3A_featureengineeringsettings">featureEngineeringSettings</code></td>
<td>
<p>An object of <code>featureEngineeringSettings</code> specifying any feature engineering to be learned (using the train data)</p>
</td></tr>
<tr><td><code id="createLearningCurve_+3A_preprocesssettings">preprocessSettings</code></td>
<td>
<p>An object of <code>preprocessSettings</code>. This setting specifies the minimum fraction of
target population who must have a covariate for it to be included in the model training
and whether to normalise the covariates before training</p>
</td></tr>
<tr><td><code id="createLearningCurve_+3A_logsettings">logSettings</code></td>
<td>
<p>An object of <code>logSettings</code> created using <code>createLogSettings</code>
specifying how the logging is done</p>
</td></tr>
<tr><td><code id="createLearningCurve_+3A_executesettings">executeSettings</code></td>
<td>
<p>An object of <code>executeSettings</code> specifying which parts of the analysis to run</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A learning curve object containing the various performance measures
obtained by the model for each training set fraction. It can be plotted
using <code>plotLearningCurve</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
 
data("simulationProfile")
plpData &lt;- simulatePlpData(simulationProfile, n = 1000)
outcomeId &lt;- 3
modelSettings &lt;- setLassoLogisticRegression(seed=42)
learningCurve &lt;- createLearningCurve(plpData, outcomeId, modelSettings = modelSettings,
saveDirectory = file.path(tempdir(), "learningCurve"), cores = 2)
# clean up
unlink(file.path(tempdir(), "learningCurve"), recursive = TRUE)


</code></pre>

<hr>
<h2 id='createLogSettings'>Create the settings for logging the progression of the analysis</h2><span id='topic+createLogSettings'></span>

<h3>Description</h3>

<p>Create the settings for logging the progression of the analysis
</p>


<h3>Usage</h3>

<pre><code class='language-R'>createLogSettings(
  verbosity = "DEBUG",
  timeStamp = TRUE,
  logName = "runPlp Log"
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="createLogSettings_+3A_verbosity">verbosity</code></td>
<td>
<p>Sets the level of the verbosity. If the log level is at or higher in priority than the logger threshold, a message will print. The levels are:
</p>

<ul>
<li><p> DEBUG Highest verbosity showing all debug statements
</p>
</li>
<li><p> TRACE Showing information about start and end of steps
</p>
</li>
<li><p> INFO Show informative information (Default)
</p>
</li>
<li><p> WARN Show warning messages
</p>
</li>
<li><p> ERROR Show error messages
</p>
</li>
<li><p> FATAL Be silent except for fatal errors
</p>
</li></ul>
</td></tr>
<tr><td><code id="createLogSettings_+3A_timestamp">timeStamp</code></td>
<td>
<p>If TRUE a timestamp will be added to each logging statement. Automatically switched on for TRACE level.</p>
</td></tr>
<tr><td><code id="createLogSettings_+3A_logname">logName</code></td>
<td>
<p>A string reference for the logger</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Returns an object of class <code>logSettings</code> that specifies the logger settings
</p>


<h3>Value</h3>

<p>An object of class <code>logSettings</code> containing the settings for the logger
</p>


<h3>Examples</h3>

<pre><code class='language-R'># create a log settings object with DENUG verbosity, timestamp and log name 
# "runPlp Log". This needs to be passed to `runPlp`.
createLogSettings(verbosity = "DEBUG", timeStamp = TRUE, logName = "runPlp Log")
</code></pre>

<hr>
<h2 id='createModelDesign'>Specify settings for developing a single model</h2><span id='topic+createModelDesign'></span>

<h3>Description</h3>

<p>Specify settings for developing a single model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>createModelDesign(
  targetId = NULL,
  outcomeId = NULL,
  restrictPlpDataSettings = createRestrictPlpDataSettings(),
  populationSettings = createStudyPopulationSettings(),
  covariateSettings = FeatureExtraction::createDefaultCovariateSettings(),
  featureEngineeringSettings = NULL,
  sampleSettings = NULL,
  preprocessSettings = NULL,
  modelSettings = NULL,
  splitSettings = createDefaultSplitSetting(),
  runCovariateSummary = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="createModelDesign_+3A_targetid">targetId</code></td>
<td>
<p>The id of the target cohort that will be used for data extraction (e.g., the ATLAS id)</p>
</td></tr>
<tr><td><code id="createModelDesign_+3A_outcomeid">outcomeId</code></td>
<td>
<p>The id of the outcome that will be used for data extraction (e.g., the ATLAS id)</p>
</td></tr>
<tr><td><code id="createModelDesign_+3A_restrictplpdatasettings">restrictPlpDataSettings</code></td>
<td>
<p>The settings specifying the extra restriction settings when extracting the data created using <code>createRestrictPlpDataSettings()</code>.</p>
</td></tr>
<tr><td><code id="createModelDesign_+3A_populationsettings">populationSettings</code></td>
<td>
<p>The population settings specified by <code>createStudyPopulationSettings()</code></p>
</td></tr>
<tr><td><code id="createModelDesign_+3A_covariatesettings">covariateSettings</code></td>
<td>
<p>The covariate settings, this can be a list or a single <code>'covariateSetting'</code> object.</p>
</td></tr>
<tr><td><code id="createModelDesign_+3A_featureengineeringsettings">featureEngineeringSettings</code></td>
<td>
<p>Either NULL or an object of class <code>featureEngineeringSettings</code> specifying any feature engineering used during model development</p>
</td></tr>
<tr><td><code id="createModelDesign_+3A_samplesettings">sampleSettings</code></td>
<td>
<p>Either NULL or an object of class <code>sampleSettings</code> with the over/under sampling settings used for model development</p>
</td></tr>
<tr><td><code id="createModelDesign_+3A_preprocesssettings">preprocessSettings</code></td>
<td>
<p>Either NULL or an object of class <code>preprocessSettings</code> created using <code>createPreprocessingSettings()</code></p>
</td></tr>
<tr><td><code id="createModelDesign_+3A_modelsettings">modelSettings</code></td>
<td>
<p>The model settings such as <code>setLassoLogisticRegression()</code></p>
</td></tr>
<tr><td><code id="createModelDesign_+3A_splitsettings">splitSettings</code></td>
<td>
<p>The train/validation/test splitting used by all analyses created using <code>createDefaultSplitSetting()</code></p>
</td></tr>
<tr><td><code id="createModelDesign_+3A_runcovariatesummary">runCovariateSummary</code></td>
<td>
<p>Whether to run the covariateSummary</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This specifies a single analysis for developing as single model
</p>


<h3>Value</h3>

<p>A list with analysis settings used to develop a single prediction model
</p>


<h3>Examples</h3>

<pre><code class='language-R'># L1 logistic regression model to predict the outcomeId 2 using the targetId 2
# with with default population, restrictPlp, split, and covariate settings
createModelDesign(
  targetId = 1,
  outcomeId = 2,
  modelSettings = setLassoLogisticRegression(seed=42),
  populationSettings = createStudyPopulationSettings(),
  restrictPlpDataSettings = createRestrictPlpDataSettings(),
  covariateSettings = FeatureExtraction::createDefaultCovariateSettings(),
  splitSettings = createDefaultSplitSetting(splitSeed = 42),
  runCovariateSummary = TRUE
)
</code></pre>

<hr>
<h2 id='createNormalizer'>Create the settings for normalizing the data @param type The type of normalization to use, either &quot;minmax&quot; or &quot;robust&quot;</h2><span id='topic+createNormalizer'></span>

<h3>Description</h3>

<p>Create the settings for normalizing the data @param type The type of normalization to use, either &quot;minmax&quot; or &quot;robust&quot;
</p>


<h3>Usage</h3>

<pre><code class='language-R'>createNormalizer(type = "minmax", settings = list())
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="createNormalizer_+3A_type">type</code></td>
<td>
<p>The type of normalization to use, either &quot;minmax&quot; or &quot;robust&quot;</p>
</td></tr>
<tr><td><code id="createNormalizer_+3A_settings">settings</code></td>
<td>
<p>A list of settings for the normalization.
For robust normalization, the settings list can contain a boolean value for
clip, which clips the values to be between -3 and 3 after normalization. See
https://arxiv.org/abs/2407.04491</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>featureEngineeringSettings</code>
</p>
<p>An object of class <code>featureEngineeringSettings</code>'
</p>


<h3>Examples</h3>

<pre><code class='language-R'># create a minmax normalizer that normalizes the data between 0 and 1
normalizer &lt;- createNormalizer(type = "minmax")
# create a robust normalizer that normalizes the data by the interquartile range
# and squeezes the values to be between -3 and 3
normalizer &lt;- createNormalizer(type = "robust", settings = list(clip = TRUE))
</code></pre>

<hr>
<h2 id='createPlpResultTables'>Create the results tables to store PatientLevelPrediction models and results into a database</h2><span id='topic+createPlpResultTables'></span>

<h3>Description</h3>

<p>This function executes a large set of SQL statements to create tables that can store models and results
</p>


<h3>Usage</h3>

<pre><code class='language-R'>createPlpResultTables(
  connectionDetails,
  targetDialect = "postgresql",
  resultSchema,
  deleteTables = TRUE,
  createTables = TRUE,
  tablePrefix = "",
  tempEmulationSchema = getOption("sqlRenderTempEmulationSchema"),
  testFile = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="createPlpResultTables_+3A_connectiondetails">connectionDetails</code></td>
<td>
<p>The database connection details</p>
</td></tr>
<tr><td><code id="createPlpResultTables_+3A_targetdialect">targetDialect</code></td>
<td>
<p>The database management system being used</p>
</td></tr>
<tr><td><code id="createPlpResultTables_+3A_resultschema">resultSchema</code></td>
<td>
<p>The name of the database schema that the result tables will be created.</p>
</td></tr>
<tr><td><code id="createPlpResultTables_+3A_deletetables">deleteTables</code></td>
<td>
<p>If true any existing tables matching the PatientLevelPrediction result tables names will be deleted</p>
</td></tr>
<tr><td><code id="createPlpResultTables_+3A_createtables">createTables</code></td>
<td>
<p>If true the PatientLevelPrediction result tables will be created</p>
</td></tr>
<tr><td><code id="createPlpResultTables_+3A_tableprefix">tablePrefix</code></td>
<td>
<p>A string that appends to the PatientLevelPrediction result tables</p>
</td></tr>
<tr><td><code id="createPlpResultTables_+3A_tempemulationschema">tempEmulationSchema</code></td>
<td>
<p>The temp schema used when the database management system is oracle</p>
</td></tr>
<tr><td><code id="createPlpResultTables_+3A_testfile">testFile</code></td>
<td>
<p>(used for testing) The location of an sql file with the table creation code</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function can be used to create (or delete) PatientLevelPrediction result tables
</p>


<h3>Value</h3>

<p>Returns NULL but creates or deletes the required tables in the specified database schema(s).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# create a sqlite database with the PatientLevelPrediction result tables
connectionDetails &lt;- DatabaseConnector::createConnectionDetails(
  dbms = "sqlite",
  server = file.path(tempdir(), "test.sqlite"))
createPlpResultTables(connectionDetails = connectionDetails,
                      targetDialect = "sqlite",
                      resultSchema = "main",
                      tablePrefix = "plp_")
# delete the tables
createPlpResultTables(connectionDetails = connectionDetails,
                      targetDialect = "sqlite",
                      resultSchema = "main",
                      deleteTables = TRUE,
                      createTables = FALSE,
                      tablePrefix = "plp_")
# clean up the database file
unlink(file.path(tempdir(), "test.sqlite"))

</code></pre>

<hr>
<h2 id='createPreprocessSettings'>Create the settings for preprocessing the trainData.</h2><span id='topic+createPreprocessSettings'></span>

<h3>Description</h3>

<p>Create the settings for preprocessing the trainData.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>createPreprocessSettings(
  minFraction = 0.001,
  normalize = TRUE,
  removeRedundancy = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="createPreprocessSettings_+3A_minfraction">minFraction</code></td>
<td>
<p>The minimum fraction of target population who must have a
covariate for it to be included in the model training</p>
</td></tr>
<tr><td><code id="createPreprocessSettings_+3A_normalize">normalize</code></td>
<td>
<p>Whether to normalise the covariates before training
(Default: TRUE)</p>
</td></tr>
<tr><td><code id="createPreprocessSettings_+3A_removeredundancy">removeRedundancy</code></td>
<td>
<p>Whether to remove redundant features (Default: TRUE)
Redundant features are features that within an analysisId together cover all
observations. For example with ageGroups, if you have ageGroup 0-18 and 18-100
and all patients are in one of these groups, then one of these groups is redundant.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Returns an object of class <code>preprocessingSettings</code> that specifies how to
preprocess the training data
</p>


<h3>Value</h3>

<p>An object of class <code>preprocessingSettings</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Create the settings for preprocessing, remove no features, normalise the data
createPreprocessSettings(minFraction = 0.0, normalize = TRUE, removeRedundancy = FALSE)
</code></pre>

<hr>
<h2 id='createRandomForestFeatureSelection'>Create the settings for random foreat based feature selection</h2><span id='topic+createRandomForestFeatureSelection'></span>

<h3>Description</h3>

<p>Create the settings for random foreat based feature selection
</p>


<h3>Usage</h3>

<pre><code class='language-R'>createRandomForestFeatureSelection(ntrees = 2000, maxDepth = 17)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="createRandomForestFeatureSelection_+3A_ntrees">ntrees</code></td>
<td>
<p>number of tree in forest</p>
</td></tr>
<tr><td><code id="createRandomForestFeatureSelection_+3A_maxdepth">maxDepth</code></td>
<td>
<p>MAx depth of each tree</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Returns an object of class <code>featureEngineeringSettings</code> that specifies the sampling function that will be called and the settings
</p>


<h3>Value</h3>

<p>An object of class <code>featureEngineeringSettings</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run:  #' featureSelector &lt;- createRandomForestFeatureSelection(ntrees = 2000, maxDepth = 10)

</code></pre>

<hr>
<h2 id='createRareFeatureRemover'>Create the settings for removing rare features</h2><span id='topic+createRareFeatureRemover'></span>

<h3>Description</h3>

<p>Create the settings for removing rare features
</p>


<h3>Usage</h3>

<pre><code class='language-R'>createRareFeatureRemover(threshold = 0.001)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="createRareFeatureRemover_+3A_threshold">threshold</code></td>
<td>
<p>The minimum fraction of the training data that must have a
feature for it to be included</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>featureEngineeringSettings</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
 
# create a rare feature remover that removes features that are present in less
# than 1% of the population
rareFeatureRemover &lt;- createRareFeatureRemover(threshold = 0.01)
plpData &lt;- getEunomiaPlpData()
analysisId &lt;- "rareFeatureRemover"
saveLocation &lt;- file.path(tempdir(), analysisId)
results &lt;- runPlp(
  plpData = plpData,
  featureEngineeringSettings = rareFeatureRemover,
  outcomeId = 3,
 executeSettings = createExecuteSettings(
   runModelDevelopment = TRUE,
   runSplitData = TRUE,
   runFeatureEngineering = TRUE),
 saveDirectory = saveLocation,
 analysisId = analysisId)
# clean up 
unlink(saveLocation, recursive = TRUE)
 

</code></pre>

<hr>
<h2 id='createRestrictPlpDataSettings'>createRestrictPlpDataSettings define extra restriction settings when calling getPlpData</h2><span id='topic+createRestrictPlpDataSettings'></span>

<h3>Description</h3>

<p>This function creates the settings used to restrict the target cohort when calling getPlpData
</p>


<h3>Usage</h3>

<pre><code class='language-R'>createRestrictPlpDataSettings(
  studyStartDate = "",
  studyEndDate = "",
  firstExposureOnly = FALSE,
  washoutPeriod = 0,
  sampleSize = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="createRestrictPlpDataSettings_+3A_studystartdate">studyStartDate</code></td>
<td>
<p>A calendar date specifying the minimum date that a cohort index
date can appear. Date format is 'yyyymmdd'.</p>
</td></tr>
<tr><td><code id="createRestrictPlpDataSettings_+3A_studyenddate">studyEndDate</code></td>
<td>
<p>A calendar date specifying the maximum date that a cohort index
date can appear. Date format is 'yyyymmdd'. Important: the study
end data is also used to truncate risk windows, meaning no outcomes
beyond the study end date will be considered.</p>
</td></tr>
<tr><td><code id="createRestrictPlpDataSettings_+3A_firstexposureonly">firstExposureOnly</code></td>
<td>
<p>Should only the first exposure per subject be included? Note that
this is typically done in the <code>createStudyPopulation</code> function,
but can already be done here for efficiency reasons.</p>
</td></tr>
<tr><td><code id="createRestrictPlpDataSettings_+3A_washoutperiod">washoutPeriod</code></td>
<td>
<p>The mininum required continuous observation time prior to index
date for a person to be included in the at risk cohort. Note that
this is typically done in the <code>createStudyPopulation</code> function,
but can already be done here for efficiency reasons.</p>
</td></tr>
<tr><td><code id="createRestrictPlpDataSettings_+3A_samplesize">sampleSize</code></td>
<td>
<p>If not NULL, the number of people to sample from the target cohort</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Users need to specify the extra restrictions to apply when downloading the target cohort
</p>


<h3>Value</h3>

<p>A setting object of class <code>restrictPlpDataSettings</code> containing a list of
the settings: </p>

<ul>
<li><p><code>studyStartDate</code>: A calendar date specifying the minimum date that a cohort index date can appear
</p>
</li>
<li><p><code>studyEndDate</code>: A calendar date specifying the maximum date that a cohort index date can appear
</p>
</li>
<li><p><code>firstExposureOnly</code>: Should only the first exposure per subject be included
</p>
</li>
<li><p><code>washoutPeriod</code>: The mininum required continuous observation time prior to index date for a person to be included in the at risk cohort
</p>
</li>
<li><p><code>sampleSize</code>: If not NULL, the number of people to sample from the target cohort
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'># restrict to 2010, first exposure only, require washout period of 365 day
# and sample 1000 people
createRestrictPlpDataSettings(studyStartDate = "20100101", studyEndDate = "20101231", 
firstExposureOnly = TRUE, washoutPeriod = 365, sampleSize = 1000)
</code></pre>

<hr>
<h2 id='createSampleSettings'>Create the settings for defining how the trainData from <code>splitData</code> are sampled using
default sample functions.</h2><span id='topic+createSampleSettings'></span>

<h3>Description</h3>

<p>Create the settings for defining how the trainData from <code>splitData</code> are sampled using
default sample functions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>createSampleSettings(
  type = "none",
  numberOutcomestoNonOutcomes = 1,
  sampleSeed = sample(10000, 1)
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="createSampleSettings_+3A_type">type</code></td>
<td>
<p>(character) Choice of:  </p>

<ul>
<li><p> 'none' No sampling is applied - this is the default
</p>
</li>
<li><p> 'underSample' Undersample the non-outcome class to make the data more balanced
</p>
</li>
<li><p> 'overSample' Oversample the outcome class by adding in each outcome multiple times
</p>
</li></ul>
</td></tr>
<tr><td><code id="createSampleSettings_+3A_numberoutcomestononoutcomes">numberOutcomestoNonOutcomes</code></td>
<td>
<p>(numeric) A numeric specifying the required number of outcomes per non-outcomes</p>
</td></tr>
<tr><td><code id="createSampleSettings_+3A_sampleseed">sampleSeed</code></td>
<td>
<p>(numeric) A seed to use when splitting the data for reproducibility (if not set a random number will be generated)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Returns an object of class <code>sampleSettings</code> that specifies the sampling function that will be called and the settings
</p>


<h3>Value</h3>

<p>An object of class <code>sampleSettings</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

# sample even rate of outcomes to non-outcomes
sampleSetting &lt;- createSampleSettings(type = "underSample", 
                                      numberOutcomestoNonOutcomes = 1, 
                                      sampleSeed = 42)


</code></pre>

<hr>
<h2 id='createSimpleImputer'>Create Simple Imputer settings</h2><span id='topic+createSimpleImputer'></span>

<h3>Description</h3>

<p>This function creates the settings for a simple imputer
which imputes missing values with the mean or median
</p>


<h3>Usage</h3>

<pre><code class='language-R'>createSimpleImputer(method = "mean", missingThreshold = 0.3)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="createSimpleImputer_+3A_method">method</code></td>
<td>
<p>The method to use for imputation, either &quot;mean&quot; or &quot;median&quot;</p>
</td></tr>
<tr><td><code id="createSimpleImputer_+3A_missingthreshold">missingThreshold</code></td>
<td>
<p>The threshold for missing values to be imputed vs removed</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The settings for the single imputer of class <code>featureEngineeringSettings</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># create imputer to impute values with missingness less than 10% using the median
# of observed values
createSimpleImputer(method = "median", missingThreshold = 0.10)
</code></pre>

<hr>
<h2 id='createSklearnModel'>Plug an existing scikit learn python model into the
PLP framework</h2><span id='topic+createSklearnModel'></span>

<h3>Description</h3>

<p>Plug an existing scikit learn python model into the
PLP framework
</p>


<h3>Usage</h3>

<pre><code class='language-R'>createSklearnModel(
  modelLocation = "/model",
  covariateMap = data.frame(columnId = 1:2, covariateId = c(1, 2), ),
  covariateSettings,
  populationSettings,
  isPickle = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="createSklearnModel_+3A_modellocation">modelLocation</code></td>
<td>
<p>The location of the folder that contains the model as
model.pkl</p>
</td></tr>
<tr><td><code id="createSklearnModel_+3A_covariatemap">covariateMap</code></td>
<td>
<p>A data.frame with the columns: columnId and covariateId.
<code>covariateId</code> from FeatureExtraction is the standard OHDSI covariateId.
<code>columnId</code> is the column location the model expects that covariate to be in.
For example, if you had a column called 'age' in your model and this was the
3rd column when fitting the model, then the values for columnId would be 3,
covariateId would be 1002 (the covariateId for age in years) and</p>
</td></tr>
<tr><td><code id="createSklearnModel_+3A_covariatesettings">covariateSettings</code></td>
<td>
<p>The settings for the standardized covariates</p>
</td></tr>
<tr><td><code id="createSklearnModel_+3A_populationsettings">populationSettings</code></td>
<td>
<p>The settings for the population, this includes the
time-at-risk settings and inclusion criteria.</p>
</td></tr>
<tr><td><code id="createSklearnModel_+3A_ispickle">isPickle</code></td>
<td>
<p>If the model should be saved as a pickle set this to TRUE if
it should be saved as json set this to FALSE.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function lets users add an existing scikit learn model that is saved as
model.pkl into PLP format.  covariateMap is a mapping between standard
covariateIds and the model columns. The user also needs to specify the
covariate settings and population settings as these are used to determine
the standard PLP model design.
</p>


<h3>Value</h3>

<p>An object of class plpModel, this is a list that contains:
model (the location of the model.pkl),
preprocessing (settings for mapping the covariateIds to the model
column mames),
modelDesign (specification of the model design),
trainDetails (information about the model fitting) and
covariateImportance.
</p>
<p>You can use the output as an input in PatientLevelPrediction::predictPlp to
apply the model and calculate the risk for patients.
</p>

<hr>
<h2 id='createSplineSettings'>Create the settings for adding a spline for continuous variables</h2><span id='topic+createSplineSettings'></span>

<h3>Description</h3>

<p>Create the settings for adding a spline for continuous variables
</p>


<h3>Usage</h3>

<pre><code class='language-R'>createSplineSettings(continousCovariateId, knots, analysisId = 683)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="createSplineSettings_+3A_continouscovariateid">continousCovariateId</code></td>
<td>
<p>The covariateId to apply splines to</p>
</td></tr>
<tr><td><code id="createSplineSettings_+3A_knots">knots</code></td>
<td>
<p>Either number of knots of vector of split values</p>
</td></tr>
<tr><td><code id="createSplineSettings_+3A_analysisid">analysisId</code></td>
<td>
<p>The analysisId to use for the spline covariates</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Returns an object of class <code>featureEngineeringSettings</code> that specifies the sampling function that will be called and the settings
</p>


<h3>Value</h3>

<p>An object of class <code>featureEngineeringSettings</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># create splines for age (1002) with 5 knots
createSplineSettings(continousCovariateId = 1002, knots = 5, analysisId = 683)
</code></pre>

<hr>
<h2 id='createStratifiedImputationSettings'>Create the settings for using stratified imputation.</h2><span id='topic+createStratifiedImputationSettings'></span>

<h3>Description</h3>

<p>Create the settings for using stratified imputation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>createStratifiedImputationSettings(covariateId, ageSplits = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="createStratifiedImputationSettings_+3A_covariateid">covariateId</code></td>
<td>
<p>The covariateId that needs imputed values</p>
</td></tr>
<tr><td><code id="createStratifiedImputationSettings_+3A_agesplits">ageSplits</code></td>
<td>
<p>A vector of age splits in years to create age groups</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Returns an object of class <code>featureEngineeringSettings</code> that specifies
how to do stratified imputation. This function splits the covariate into
age groups and fits splines to the covariate within each age group. The spline
values are then used to impute missing values.
</p>


<h3>Value</h3>

<p>An object of class <code>featureEngineeringSettings</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># create a stratified imputation settings for covariate 1050 with age splits 
# at 50 and 70
stratifiedImputationSettings &lt;- 
  createStratifiedImputationSettings(covariateId = 1050, ageSplits = c(50, 70))
</code></pre>

<hr>
<h2 id='createStudyPopulation'>Create a study population</h2><span id='topic+createStudyPopulation'></span>

<h3>Description</h3>

<p>Create a study population
</p>


<h3>Usage</h3>

<pre><code class='language-R'>createStudyPopulation(
  plpData,
  outcomeId = plpData$metaData$databaseDetails$outcomeIds[1],
  populationSettings = createStudyPopulationSettings(),
  population = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="createStudyPopulation_+3A_plpdata">plpData</code></td>
<td>
<p>An object of type <code>plpData</code> as generated using
<code>getplpData</code>.</p>
</td></tr>
<tr><td><code id="createStudyPopulation_+3A_outcomeid">outcomeId</code></td>
<td>
<p>The  ID of the outcome.</p>
</td></tr>
<tr><td><code id="createStudyPopulation_+3A_populationsettings">populationSettings</code></td>
<td>
<p>An object of class populationSettings created using <code>createPopulationSettings</code></p>
</td></tr>
<tr><td><code id="createStudyPopulation_+3A_population">population</code></td>
<td>
<p>If specified, this population will be used as the starting point instead of the
cohorts in the <code>plpData</code> object.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Create a study population by enforcing certain inclusion and exclusion criteria, defining
a risk window, and determining which outcomes fall inside the risk window.
</p>


<h3>Value</h3>

<p>A data frame specifying the study population. This data frame will have the following columns:
</p>

<dl>
<dt>rowId</dt><dd><p>A unique identifier for an exposure</p>
</dd>
<dt>subjectId</dt><dd><p>The person ID of the subject</p>
</dd>
<dt>cohortStartdate</dt><dd><p>The index date</p>
</dd>
<dt>outcomeCount</dt><dd><p>The number of outcomes observed during the risk window</p>
</dd>
<dt>timeAtRisk</dt><dd><p>The number of days in the risk window</p>
</dd>
<dt>survivalTime</dt><dd><p>The number of days until either the outcome or the end of the risk window</p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'>  
data("simulationProfile")
plpData &lt;- simulatePlpData(simulationProfile, n = 100)
# Create study population, require time at risk of 30 days. The risk window is 1 to 90 days.
populationSettings &lt;- createStudyPopulationSettings(requireTimeAtRisk = TRUE,
                                                     minTimeAtRisk = 30,
                                                     riskWindowStart = 1,
                                                     riskWindowEnd = 90)
population &lt;- createStudyPopulation(plpData, outcomeId = 3, populationSettings)

</code></pre>

<hr>
<h2 id='createStudyPopulationSettings'>create the study population settings</h2><span id='topic+createStudyPopulationSettings'></span>

<h3>Description</h3>

<p>create the study population settings
</p>


<h3>Usage</h3>

<pre><code class='language-R'>createStudyPopulationSettings(
  binary = TRUE,
  includeAllOutcomes = TRUE,
  firstExposureOnly = FALSE,
  washoutPeriod = 0,
  removeSubjectsWithPriorOutcome = TRUE,
  priorOutcomeLookback = 99999,
  requireTimeAtRisk = TRUE,
  minTimeAtRisk = 364,
  riskWindowStart = 1,
  startAnchor = "cohort start",
  riskWindowEnd = 365,
  endAnchor = "cohort start",
  restrictTarToCohortEnd = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="createStudyPopulationSettings_+3A_binary">binary</code></td>
<td>
<p>Forces the outcomeCount to be 0 or 1 (use for binary prediction problems)</p>
</td></tr>
<tr><td><code id="createStudyPopulationSettings_+3A_includealloutcomes">includeAllOutcomes</code></td>
<td>
<p>(binary) indicating whether to include people with outcomes who are not observed for the whole at risk period</p>
</td></tr>
<tr><td><code id="createStudyPopulationSettings_+3A_firstexposureonly">firstExposureOnly</code></td>
<td>
<p>Should only the first exposure per subject be included? Note that
this is typically done in the <code>createStudyPopulation</code> function,</p>
</td></tr>
<tr><td><code id="createStudyPopulationSettings_+3A_washoutperiod">washoutPeriod</code></td>
<td>
<p>The mininum required continuous observation time prior to index
date for a person to be included in the cohort.</p>
</td></tr>
<tr><td><code id="createStudyPopulationSettings_+3A_removesubjectswithprioroutcome">removeSubjectsWithPriorOutcome</code></td>
<td>
<p>Remove subjects that have the outcome prior to the risk window start?</p>
</td></tr>
<tr><td><code id="createStudyPopulationSettings_+3A_prioroutcomelookback">priorOutcomeLookback</code></td>
<td>
<p>How many days should we look back when identifying prior outcomes?</p>
</td></tr>
<tr><td><code id="createStudyPopulationSettings_+3A_requiretimeatrisk">requireTimeAtRisk</code></td>
<td>
<p>Should subject without time at risk be removed?</p>
</td></tr>
<tr><td><code id="createStudyPopulationSettings_+3A_mintimeatrisk">minTimeAtRisk</code></td>
<td>
<p>The minimum number of days at risk required to be included</p>
</td></tr>
<tr><td><code id="createStudyPopulationSettings_+3A_riskwindowstart">riskWindowStart</code></td>
<td>
<p>The start of the risk window (in days) relative to the index date (+
days of exposure if the <code>addExposureDaysToStart</code> parameter is
specified).</p>
</td></tr>
<tr><td><code id="createStudyPopulationSettings_+3A_startanchor">startAnchor</code></td>
<td>
<p>The anchor point for the start of the risk window. Can be &quot;cohort start&quot; or &quot;cohort end&quot;.</p>
</td></tr>
<tr><td><code id="createStudyPopulationSettings_+3A_riskwindowend">riskWindowEnd</code></td>
<td>
<p>The end of the risk window (in days) relative to the index data (+
days of exposure if the <code>addExposureDaysToEnd</code> parameter is
specified).</p>
</td></tr>
<tr><td><code id="createStudyPopulationSettings_+3A_endanchor">endAnchor</code></td>
<td>
<p>The anchor point for the end of the risk window. Can be &quot;cohort start&quot; or &quot;cohort end&quot;.</p>
</td></tr>
<tr><td><code id="createStudyPopulationSettings_+3A_restricttartocohortend">restrictTarToCohortEnd</code></td>
<td>
<p>If using a survival model and you want the time-at-risk to end at the cohort end date set this to T</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of type populationSettings containing all the settings required
for creating the study population
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Create study population settings with a washout period of 30 days and a 
# risk window of 1 to 90 days
populationSettings &lt;- createStudyPopulationSettings(washoutPeriod = 30, 
                                                    riskWindowStart = 1,
                                                    riskWindowEnd = 90)
</code></pre>

<hr>
<h2 id='createTempModelLoc'>Create a temporary model location</h2><span id='topic+createTempModelLoc'></span>

<h3>Description</h3>

<p>Create a temporary model location
</p>


<h3>Usage</h3>

<pre><code class='language-R'>createTempModelLoc()
</code></pre>


<h3>Value</h3>

<p>A string for the location of the temporary model location
</p>


<h3>Examples</h3>

<pre><code class='language-R'>modelLoc &lt;- createTempModelLoc()
dir.exists(modelLoc)
# clean up
unlink(modelLoc, recursive = TRUE)
</code></pre>

<hr>
<h2 id='createUnivariateFeatureSelection'>Create the settings for defining any feature selection that will be done</h2><span id='topic+createUnivariateFeatureSelection'></span>

<h3>Description</h3>

<p>Create the settings for defining any feature selection that will be done
</p>


<h3>Usage</h3>

<pre><code class='language-R'>createUnivariateFeatureSelection(k = 100)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="createUnivariateFeatureSelection_+3A_k">k</code></td>
<td>
<p>This function returns the K features most associated
(univariately) to the outcome</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Returns an object of class <code>featureEngineeringSettings</code> that specifies
the function that will be called and the settings. Uses the scikit-learn
SelectKBest function with chi2 for univariate feature selection.
</p>


<h3>Value</h3>

<p>An object of class <code>featureEngineeringSettings</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run:  #' # create a feature selection that selects the 100 most associated features
featureSelector &lt;- createUnivariateFeatureSelection(k = 100) 

## End(Not run)
</code></pre>

<hr>
<h2 id='createValidationDesign'>createValidationDesign - Define the validation design for external validation</h2><span id='topic+createValidationDesign'></span>

<h3>Description</h3>

<p>createValidationDesign - Define the validation design for external validation
</p>


<h3>Usage</h3>

<pre><code class='language-R'>createValidationDesign(
  targetId,
  outcomeId,
  populationSettings = NULL,
  restrictPlpDataSettings = NULL,
  plpModelList,
  recalibrate = NULL,
  runCovariateSummary = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="createValidationDesign_+3A_targetid">targetId</code></td>
<td>
<p>The targetId of the target cohort to validate on</p>
</td></tr>
<tr><td><code id="createValidationDesign_+3A_outcomeid">outcomeId</code></td>
<td>
<p>The outcomeId of the outcome cohort to validate on</p>
</td></tr>
<tr><td><code id="createValidationDesign_+3A_populationsettings">populationSettings</code></td>
<td>
<p>A list of population restriction settings created
by <code>createPopulationSettings</code>. Default is NULL and then this is taken
from the model</p>
</td></tr>
<tr><td><code id="createValidationDesign_+3A_restrictplpdatasettings">restrictPlpDataSettings</code></td>
<td>
<p>A list of plpData restriction settings
created by <code>createRestrictPlpDataSettings</code>. Default is NULL and then
this is taken from the model.</p>
</td></tr>
<tr><td><code id="createValidationDesign_+3A_plpmodellist">plpModelList</code></td>
<td>
<p>A list of plpModels objects created by <code>runPlp</code> or a path to such objects</p>
</td></tr>
<tr><td><code id="createValidationDesign_+3A_recalibrate">recalibrate</code></td>
<td>
<p>A vector of characters specifying the recalibration method to apply,</p>
</td></tr>
<tr><td><code id="createValidationDesign_+3A_runcovariatesummary">runCovariateSummary</code></td>
<td>
<p>whether to run the covariate summary for the validation data</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A validation design object of class <code>validationDesign</code> or a list of such objects
</p>


<h3>Examples</h3>

<pre><code class='language-R'># create a validation design for targetId 1 and outcomeId 2 one l1 model and 
# one gradient boosting model
createValidationDesign(1, 2, plpModelList = list(
"pathToL1model", "PathToGBMModel"))
</code></pre>

<hr>
<h2 id='createValidationSettings'>createValidationSettings define optional settings for performing external validation</h2><span id='topic+createValidationSettings'></span>

<h3>Description</h3>

<p>This function creates the settings required by externalValidatePlp
</p>


<h3>Usage</h3>

<pre><code class='language-R'>createValidationSettings(recalibrate = NULL, runCovariateSummary = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="createValidationSettings_+3A_recalibrate">recalibrate</code></td>
<td>
<p>A vector of characters specifying the recalibration method to apply</p>
</td></tr>
<tr><td><code id="createValidationSettings_+3A_runcovariatesummary">runCovariateSummary</code></td>
<td>
<p>Whether to run the covariate summary for the validation data</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Users need to specify whether they want to sample or recalibate when performing external validation
</p>


<h3>Value</h3>

<p>A setting object of class <code>validationSettings</code> containing a list of settings for externalValidatePlp
</p>


<h3>Examples</h3>

<pre><code class='language-R'># do weak recalibration and don't run covariate summary
createValidationSettings(recalibrate = "weakRecalibration", 
                         runCovariateSummary = FALSE)

</code></pre>

<hr>
<h2 id='diagnoseMultiplePlp'>Run a list of predictions diagnoses</h2><span id='topic+diagnoseMultiplePlp'></span>

<h3>Description</h3>

<p>Run a list of predictions diagnoses
</p>


<h3>Usage</h3>

<pre><code class='language-R'>diagnoseMultiplePlp(
  databaseDetails = createDatabaseDetails(),
  modelDesignList = list(createModelDesign(targetId = 1, outcomeId = 2, modelSettings =
    setLassoLogisticRegression()), createModelDesign(targetId = 1, outcomeId = 3,
    modelSettings = setLassoLogisticRegression())),
  cohortDefinitions = NULL,
  logSettings = createLogSettings(verbosity = "DEBUG", timeStamp = TRUE, logName =
    "diagnosePlp Log"),
  saveDirectory = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="diagnoseMultiplePlp_+3A_databasedetails">databaseDetails</code></td>
<td>
<p>The database settings created using <code>createDatabaseDetails()</code></p>
</td></tr>
<tr><td><code id="diagnoseMultiplePlp_+3A_modeldesignlist">modelDesignList</code></td>
<td>
<p>A list of model designs created using <code>createModelDesign()</code></p>
</td></tr>
<tr><td><code id="diagnoseMultiplePlp_+3A_cohortdefinitions">cohortDefinitions</code></td>
<td>
<p>A list of cohort definitions for the target and outcome cohorts</p>
</td></tr>
<tr><td><code id="diagnoseMultiplePlp_+3A_logsettings">logSettings</code></td>
<td>
<p>The setting spexcifying the logging for the analyses created using <code>createLogSettings()</code></p>
</td></tr>
<tr><td><code id="diagnoseMultiplePlp_+3A_savedirectory">saveDirectory</code></td>
<td>
<p>Name of the folder where all the outputs will written to.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function will run all specified prediction design diagnoses.
</p>


<h3>Value</h3>

<p>A data frame with the following columns: </p>

<table>
<tr>
 <td style="text-align: left;"> <code style="white-space: pre;">&#8288;analysisId&#8288;</code> </td><td style="text-align: left;"> The unique identifier
for a set of analysis choices.</td>
</tr>
<tr>
 <td style="text-align: left;"> <code style="white-space: pre;">&#8288;targetId&#8288;</code> </td><td style="text-align: left;"> The ID of the target cohort populations.</td>
</tr>
<tr>
 <td style="text-align: left;">
<code style="white-space: pre;">&#8288;outcomeId&#8288;</code> </td><td style="text-align: left;"> The ID of the outcomeId.</td>
</tr>
<tr>
 <td style="text-align: left;"> <code style="white-space: pre;">&#8288;dataLocation&#8288;</code> </td><td style="text-align: left;"> The location where the plpData was saved
</td>
</tr>
<tr>
 <td style="text-align: left;"> <code style="white-space: pre;">&#8288;the settings ids&#8288;</code> </td><td style="text-align: left;"> The ids for all other settings used for model development.</td>
</tr>
<tr>
 <td style="text-align: left;"> </td>
</tr>

</table>


<hr>
<h2 id='diagnosePlp'>diagnostic - Investigates the prediction problem settings - use before training a model</h2><span id='topic+diagnosePlp'></span>

<h3>Description</h3>

<p>This function runs a set of prediction diagnoses to help pick a suitable T, O, TAR and determine
whether the prediction problem is worth executing.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>diagnosePlp(
  plpData = NULL,
  outcomeId,
  analysisId,
  populationSettings,
  splitSettings = createDefaultSplitSetting(),
  sampleSettings = createSampleSettings(),
  saveDirectory = NULL,
  featureEngineeringSettings = createFeatureEngineeringSettings(),
  modelSettings = setLassoLogisticRegression(),
  logSettings = createLogSettings(verbosity = "DEBUG", timeStamp = TRUE, logName =
    "diagnosePlp Log"),
  preprocessSettings = createPreprocessSettings()
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="diagnosePlp_+3A_plpdata">plpData</code></td>
<td>
<p>An object of type <code>plpData</code> - the patient level prediction
data extracted from the CDM.  Can also include an initial population as
plpData$popualtion.</p>
</td></tr>
<tr><td><code id="diagnosePlp_+3A_outcomeid">outcomeId</code></td>
<td>
<p>(integer) The ID of the outcome.</p>
</td></tr>
<tr><td><code id="diagnosePlp_+3A_analysisid">analysisId</code></td>
<td>
<p>(integer) Identifier for the analysis. It is used to create, e.g., the result folder. Default is a timestamp.</p>
</td></tr>
<tr><td><code id="diagnosePlp_+3A_populationsettings">populationSettings</code></td>
<td>
<p>An object of type <code>populationSettings</code> created using <code>createStudyPopulationSettings</code> that
specifies how the data class labels are defined and addition any exclusions to apply to the
plpData cohort</p>
</td></tr>
<tr><td><code id="diagnosePlp_+3A_splitsettings">splitSettings</code></td>
<td>
<p>An object of type <code>splitSettings</code> that specifies how to split the data into train/validation/test.
The default settings can be created using <code>createDefaultSplitSetting</code>.</p>
</td></tr>
<tr><td><code id="diagnosePlp_+3A_samplesettings">sampleSettings</code></td>
<td>
<p>An object of type <code>sampleSettings</code> that specifies any under/over sampling to be done.
The default is none.</p>
</td></tr>
<tr><td><code id="diagnosePlp_+3A_savedirectory">saveDirectory</code></td>
<td>
<p>The path to the directory where the results will be saved (if NULL uses working directory)</p>
</td></tr>
<tr><td><code id="diagnosePlp_+3A_featureengineeringsettings">featureEngineeringSettings</code></td>
<td>
<p>An object of <code>featureEngineeringSettings</code> specifying any feature engineering to be learned (using the train data)</p>
</td></tr>
<tr><td><code id="diagnosePlp_+3A_modelsettings">modelSettings</code></td>
<td>
<p>An object of class <code>modelSettings</code> created using one of the function:
</p>

<ul>
<li><p> setLassoLogisticRegression() A lasso logistic regression model
</p>
</li>
<li><p> setGradientBoostingMachine() A gradient boosting machine
</p>
</li>
<li><p> setAdaBoost() An ada boost model
</p>
</li>
<li><p> setRandomForest() A random forest model
</p>
</li>
<li><p> setDecisionTree() A decision tree model
</p>
</li></ul>
</td></tr>
<tr><td><code id="diagnosePlp_+3A_logsettings">logSettings</code></td>
<td>
<p>An object of <code>logSettings</code> created using <code>createLogSettings</code>
specifying how the logging is done</p>
</td></tr>
<tr><td><code id="diagnosePlp_+3A_preprocesssettings">preprocessSettings</code></td>
<td>
<p>An object of <code>preprocessSettings</code>. This setting specifies the minimum fraction of
target population who must have a covariate for it to be included in the model training
and whether to normalise the covariates before training</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Users can define set of Ts, Os, databases and population settings.  A list of data.frames containing details such as
follow-up time distribution, time-to-event information, characteriszation details, time from last prior event,
observation time distribution.
</p>


<h3>Value</h3>

<p>An object containing the model or location where the model is saved, the data selection settings, the preprocessing
and training settings as well as various performance measures obtained by the model.
</p>

<ul>
<li><p><code>distribution</code>: List for each O of a data.frame containing: i) Time to observation end distribution, ii) Time from observation start distribution, iii) Time to event distribution and iv) Time from last prior event to index distribution (only for patients in T who have O before index) 
</p>
</li>
<li><p><code>incident</code>: List for each O of incidence of O in T during TAR
</p>
</li>
<li><p><code>characterization</code>: List for each O of Characterization of T, TnO, Tn~O
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>
 

# load the data
plpData &lt;- getEunomiaPlpData()
populationSettings &lt;- createStudyPopulationSettings(minTimeAtRisk = 1)
saveDirectory &lt;- file.path(tempdir(), "diagnosePlp")
diagnosis &lt;- diagnosePlp(plpData = plpData, outcomeId = 3, analysisId = 1, 
     populationSettings = populationSettings, saveDirectory = saveDirectory)
# clean up
unlink(saveDirectory, recursive = TRUE)


</code></pre>

<hr>
<h2 id='evaluatePlp'>evaluatePlp</h2><span id='topic+evaluatePlp'></span>

<h3>Description</h3>

<p>Evaluates the performance of the patient level prediction model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>evaluatePlp(prediction, typeColumn = "evaluationType")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="evaluatePlp_+3A_prediction">prediction</code></td>
<td>
<p>The patient level prediction model's prediction</p>
</td></tr>
<tr><td><code id="evaluatePlp_+3A_typecolumn">typeColumn</code></td>
<td>
<p>The column name in the prediction object that is used to
stratify the evaluation</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function calculates various metrics to measure the performance of the model
</p>


<h3>Value</h3>

<p>An object of class plpEvaluation containing the following components
</p>

<ul>
<li><p> evaluationStatistics: A data frame containing the evaluation statistics'
</p>
</li>
<li><p> thresholdSummary: A data frame containing the threshold summary'
</p>
</li>
<li><p> demographicSummary: A data frame containing the demographic summary'
</p>
</li>
<li><p> calibrationSummary: A data frame containing the calibration summary'
</p>
</li>
<li><p> predictionDistribution: A data frame containing the prediction distribution'
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'> 
data("simulationProfile")
plpData &lt;- simulatePlpData(simulationProfile, n= 1500)
population &lt;- createStudyPopulation(plpData, outcomeId = 3, 
                                    populationSettings = createStudyPopulationSettings())
data &lt;- splitData(plpData, population, splitSettings=createDefaultSplitSetting(splitSeed=42))
data$Train$covariateData &lt;- preprocessData(data$Train$covariateData, 
                                           createPreprocessSettings())
path &lt;- file.path(tempdir(), "plp")
model &lt;- fitPlp(data$Train, modelSettings=setLassoLogisticRegression(seed=42),
                analysisId=1, analysisPath = path)
evaluatePlp(model$prediction) # Train and CV metrics

</code></pre>

<hr>
<h2 id='externalValidateDbPlp'>externalValidateDbPlp - Validate a model on new databases</h2><span id='topic+externalValidateDbPlp'></span>

<h3>Description</h3>

<p>This function extracts data using a user specified connection and cdm_schema, applied the model and then calcualtes the performance
</p>


<h3>Usage</h3>

<pre><code class='language-R'>externalValidateDbPlp(
  plpModel,
  validationDatabaseDetails = createDatabaseDetails(),
  validationRestrictPlpDataSettings = createRestrictPlpDataSettings(),
  settings = createValidationSettings(recalibrate = "weakRecalibration"),
  logSettings = createLogSettings(verbosity = "INFO", logName = "validatePLP"),
  outputFolder = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="externalValidateDbPlp_+3A_plpmodel">plpModel</code></td>
<td>
<p>The model object returned by runPlp() containing the trained model</p>
</td></tr>
<tr><td><code id="externalValidateDbPlp_+3A_validationdatabasedetails">validationDatabaseDetails</code></td>
<td>
<p>A list of objects of class <code>databaseDetails</code> created using <code>createDatabaseDetails</code></p>
</td></tr>
<tr><td><code id="externalValidateDbPlp_+3A_validationrestrictplpdatasettings">validationRestrictPlpDataSettings</code></td>
<td>
<p>A list of population restriction settings created by <code>createRestrictPlpDataSettings()</code></p>
</td></tr>
<tr><td><code id="externalValidateDbPlp_+3A_settings">settings</code></td>
<td>
<p>A settings object of class <code>validationSettings</code> created using <code>createValidationSettings</code></p>
</td></tr>
<tr><td><code id="externalValidateDbPlp_+3A_logsettings">logSettings</code></td>
<td>
<p>An object of <code>logSettings</code> created using <code>createLogSettings</code>
specifying how the logging is done</p>
</td></tr>
<tr><td><code id="externalValidateDbPlp_+3A_outputfolder">outputFolder</code></td>
<td>
<p>The directory to save the validation results to (subfolders are created per database in validationDatabaseDetails)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Users need to input a trained model (the output of runPlp()) and new database connections. The function will return a list of length equal to the
number of cdm_schemas input with the performance on the new data
</p>


<h3>Value</h3>

<p>An externalValidatePlp object containing the following components
</p>

<ul>
<li><p> model: The model object
</p>
</li>
<li><p> executionSummary: A list of execution details
</p>
</li>
<li><p> prediction: A dataframe containing the predictions
</p>
</li>
<li><p> performanceEvaluation: A dataframe containing the performance metrics
</p>
</li>
<li><p> covariateSummary: A dataframe containing the covariate summary
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>
 
data("simulationProfile")
plpData &lt;- simulatePlpData(simulationProfile, n=1000)
# first fit a model on some data, default is a L1 logistic regression
saveLoc &lt;- file.path(tempdir(), "development")
results &lt;- runPlp(plpData, 
                  outcomeId = 3,
                  saveDirectory = saveLoc,
                  populationSettings = 
                   createStudyPopulationSettings(requireTimeAtRisk=FALSE)
                  )
connectionDetails &lt;- Eunomia::getEunomiaConnectionDetails()
Eunomia::createCohorts(connectionDetails)
# now validate the model on Eunomia
validationDatabaseDetails &lt;- createDatabaseDetails(
  connectionDetails = connectionDetails,
  cdmDatabaseSchema = "main",
  cdmDatabaseName = "main",
  cohortDatabaseSchema = "main",
  cohortTable = "cohort",
  outcomeDatabaseSchema = "main",
  outcomeTable = "cohort",
  targetId = 1, # users of celecoxib
  outcomeIds = 3, # GIbleed
  cdmVersion = 5)
path &lt;- file.path(tempdir(), "validation")
externalValidateDbPlp(results$model, validationDatabaseDetails, outputFolder = path)
# clean up
unlink(saveLoc, recursive = TRUE)
unlink(path, recursive = TRUE)


</code></pre>

<hr>
<h2 id='extractDatabaseToCsv'>Exports all the results from a database into csv files</h2><span id='topic+extractDatabaseToCsv'></span>

<h3>Description</h3>

<p>Exports all the results from a database into csv files
</p>


<h3>Usage</h3>

<pre><code class='language-R'>extractDatabaseToCsv(
  conn = NULL,
  connectionDetails,
  databaseSchemaSettings = createDatabaseSchemaSettings(resultSchema = "main"),
  csvFolder,
  minCellCount = 5,
  sensitiveColumns = getPlpSensitiveColumns(),
  fileAppend = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="extractDatabaseToCsv_+3A_conn">conn</code></td>
<td>
<p>The connection to the database with the results</p>
</td></tr>
<tr><td><code id="extractDatabaseToCsv_+3A_connectiondetails">connectionDetails</code></td>
<td>
<p>The connectionDetails for the result database</p>
</td></tr>
<tr><td><code id="extractDatabaseToCsv_+3A_databaseschemasettings">databaseSchemaSettings</code></td>
<td>
<p>The result database schema settings</p>
</td></tr>
<tr><td><code id="extractDatabaseToCsv_+3A_csvfolder">csvFolder</code></td>
<td>
<p>Location to save the csv files</p>
</td></tr>
<tr><td><code id="extractDatabaseToCsv_+3A_mincellcount">minCellCount</code></td>
<td>
<p>The min value to show in cells that are sensitive (values less than this value will be replaced with -1)</p>
</td></tr>
<tr><td><code id="extractDatabaseToCsv_+3A_sensitivecolumns">sensitiveColumns</code></td>
<td>
<p>A named list (name of table columns belong to) with a list of columns to apply the minCellCount to.</p>
</td></tr>
<tr><td><code id="extractDatabaseToCsv_+3A_fileappend">fileAppend</code></td>
<td>
<p>If set to a string this will be appended to the start of the csv file names</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Extracts the results from a database into a set of csv files
</p>


<h3>Value</h3>

<p>The directory path where the results were saved
</p>


<h3>Examples</h3>

<pre><code class='language-R'> 
# develop a simple model on simulated data
data("simulationProfile")
plpData &lt;- simulatePlpData(simulationProfile, n = 500)
saveLoc &lt;- file.path(tempdir(), "extractDatabaseToCsv")
results &lt;- runPlp(plpData, outcomeId = 3, saveDirectory = saveLoc)
# now upload the results to a sqlite database
databasePath &lt;- insertResultsToSqlite(saveLoc)
# now extract the results to csv
connectionDetails &lt;- 
  DatabaseConnector::createConnectionDetails(dbms = "sqlite", 
                                             server = databasePath)
extractDatabaseToCsv(
  connectionDetails = connectionDetails,
  csvFolder = file.path(saveLoc, "csv")
)
# show csv file
list.files(file.path(saveLoc, "csv"))
# clean up
unlink(saveLoc, recursive = TRUE)

</code></pre>

<hr>
<h2 id='fitPlp'>fitPlp</h2><span id='topic+fitPlp'></span>

<h3>Description</h3>

<p>Train various models using a default parameter grid search or user specified
parameters
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fitPlp(trainData, modelSettings, search = "grid", analysisId, analysisPath)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="fitPlp_+3A_traindata">trainData</code></td>
<td>
<p>An object of type <code>trainData</code> created using <code>splitData</code>
data extracted from the CDM.</p>
</td></tr>
<tr><td><code id="fitPlp_+3A_modelsettings">modelSettings</code></td>
<td>
<p>An object of class <code>modelSettings</code> created using
one of the <code>createModelSettings</code> functions</p>
</td></tr>
<tr><td><code id="fitPlp_+3A_search">search</code></td>
<td>
<p>The search strategy for the hyper-parameter selection (currently not used)</p>
</td></tr>
<tr><td><code id="fitPlp_+3A_analysisid">analysisId</code></td>
<td>
<p>The id of the analysis</p>
</td></tr>
<tr><td><code id="fitPlp_+3A_analysispath">analysisPath</code></td>
<td>
<p>The path of the analysis</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The user can define the machine learning model to train
</p>


<h3>Value</h3>

<p>An object of class <code>plpModel</code> containing:
</p>
<table role = "presentation">
<tr><td><code>model</code></td>
<td>
<p>The trained prediction model</p>
</td></tr>
<tr><td><code>preprocessing</code></td>
<td>
<p>The preprocessing required when applying the model</p>
</td></tr>
<tr><td><code>prediction</code></td>
<td>
<p>The cohort data.frame with the predicted risk column added</p>
</td></tr>
<tr><td><code>modelDesign</code></td>
<td>
<p>A list specifiying the modelDesign settings used to fit the model</p>
</td></tr>
<tr><td><code>trainDetails</code></td>
<td>
<p>The model meta data</p>
</td></tr>
<tr><td><code>covariateImportance</code></td>
<td>
<p>The covariate importance for the model</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'> 
# simulate data
data("simulationProfile")
plpData &lt;- simulatePlpData(simulationProfile, n=1000)
# create study population, split into train/test and preprocess with default settings
population &lt;- createStudyPopulation(plpData, outcomeId = 3)
data &lt;- splitData(plpData, population, createDefaultSplitSetting())
data$Train$covariateData &lt;- preprocessData(data$Train$covariateData)
saveLoc &lt;- file.path(tempdir(), "fitPlp")
# fit a lasso logistic regression model using the training data
plpModel &lt;- fitPlp(data$Train, modelSettings=setLassoLogisticRegression(seed=42),
                   analysisId=1, analysisPath=saveLoc)
# show evaluationSummary for model
evaluatePlp(plpModel$prediction)$evaluationSummary
# clean up
unlink(saveLoc, recursive = TRUE)

</code></pre>

<hr>
<h2 id='getCalibrationSummary'>Get a sparse summary of the calibration</h2><span id='topic+getCalibrationSummary'></span>

<h3>Description</h3>

<p>Get a sparse summary of the calibration
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getCalibrationSummary(
  prediction,
  predictionType,
  typeColumn = "evaluation",
  numberOfStrata = 10,
  truncateFraction = 0.05
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="getCalibrationSummary_+3A_prediction">prediction</code></td>
<td>
<p>A prediction object as generated using the
<code><a href="stats.html#topic+predict">predict</a></code> functions.</p>
</td></tr>
<tr><td><code id="getCalibrationSummary_+3A_predictiontype">predictionType</code></td>
<td>
<p>The type of prediction (binary or survival)</p>
</td></tr>
<tr><td><code id="getCalibrationSummary_+3A_typecolumn">typeColumn</code></td>
<td>
<p>A column that is used to stratify the results</p>
</td></tr>
<tr><td><code id="getCalibrationSummary_+3A_numberofstrata">numberOfStrata</code></td>
<td>
<p>The number of strata in the plot.</p>
</td></tr>
<tr><td><code id="getCalibrationSummary_+3A_truncatefraction">truncateFraction</code></td>
<td>
<p>This fraction of probability values will be ignored when plotting, to
avoid the x-axis scale being dominated by a few outliers.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Generates a sparse summary showing the predicted probabilities and the observed fractions. Predictions are
stratified into equally sized bins of predicted probabilities.
</p>


<h3>Value</h3>

<p>A dataframe with the calibration summary
</p>


<h3>Examples</h3>

<pre><code class='language-R'># simulate data
data("simulationProfile")
plpData &lt;- simulatePlpData(simulationProfile, n=500)
# create study population, split into train/test and preprocess with default settings
population &lt;- createStudyPopulation(plpData, outcomeId = 3)
data &lt;- splitData(plpData, population, createDefaultSplitSetting())
data$Train$covariateData &lt;- preprocessData(data$Train$covariateData)
saveLoc &lt;- file.path(tempdir(), "calibrationSummary")
# fit a lasso logistic regression model using the training data
plpModel &lt;- fitPlp(data$Train, modelSettings=setLassoLogisticRegression(seed=42),
                   analysisId=1, analysisPath=saveLoc)
calibrationSummary &lt;- getCalibrationSummary(plpModel$prediction, 
                                            "binary", 
                                            numberOfStrata = 10,
                                            typeColumn = "evaluationType")
calibrationSummary
# clean up
unlink(saveLoc, recursive = TRUE)
</code></pre>

<hr>
<h2 id='getCohortCovariateData'>Extracts covariates based on cohorts</h2><span id='topic+getCohortCovariateData'></span>

<h3>Description</h3>

<p>Extracts covariates based on cohorts
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getCohortCovariateData(
  connection,
  tempEmulationSchema = NULL,
  oracleTempSchema = NULL,
  cdmDatabaseSchema,
  cdmVersion = "5",
  cohortTable = "#cohort_person",
  rowIdField = "row_id",
  aggregated,
  cohortIds,
  covariateSettings,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="getCohortCovariateData_+3A_connection">connection</code></td>
<td>
<p>The database connection</p>
</td></tr>
<tr><td><code id="getCohortCovariateData_+3A_tempemulationschema">tempEmulationSchema</code></td>
<td>
<p>The schema to use for temp tables</p>
</td></tr>
<tr><td><code id="getCohortCovariateData_+3A_oracletempschema">oracleTempSchema</code></td>
<td>
<p>DEPRECATED The temp schema if using oracle</p>
</td></tr>
<tr><td><code id="getCohortCovariateData_+3A_cdmdatabaseschema">cdmDatabaseSchema</code></td>
<td>
<p>The schema of the OMOP CDM data</p>
</td></tr>
<tr><td><code id="getCohortCovariateData_+3A_cdmversion">cdmVersion</code></td>
<td>
<p>version of the OMOP CDM data</p>
</td></tr>
<tr><td><code id="getCohortCovariateData_+3A_cohorttable">cohortTable</code></td>
<td>
<p>the table name that contains the target population cohort</p>
</td></tr>
<tr><td><code id="getCohortCovariateData_+3A_rowidfield">rowIdField</code></td>
<td>
<p>string representing the unique identifier in the target population cohort</p>
</td></tr>
<tr><td><code id="getCohortCovariateData_+3A_aggregated">aggregated</code></td>
<td>
<p>whether the covariate should be aggregated</p>
</td></tr>
<tr><td><code id="getCohortCovariateData_+3A_cohortids">cohortIds</code></td>
<td>
<p>cohort id for the target cohort</p>
</td></tr>
<tr><td><code id="getCohortCovariateData_+3A_covariatesettings">covariateSettings</code></td>
<td>
<p>settings for the covariate cohorts and time periods</p>
</td></tr>
<tr><td><code id="getCohortCovariateData_+3A_...">...</code></td>
<td>
<p>additional arguments from FeatureExtraction</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The user specifies a cohort and time period and then a covariate is constructed whether they are in the
cohort during the time periods relative to target population cohort index
</p>


<h3>Value</h3>

<p>CovariateData object with covariates, covariateRef, and analysisRef tables
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
 
library(DatabaseConnector)
connectionDetails &lt;- Eunomia::getEunomiaConnectionDetails()
# create some cohort of people born in 1969, index date is their date of birth
con &lt;- connect(connectionDetails)
executeSql(con, "INSERT INTO main.cohort 
                 SELECT 1969 as COHORT_DEFINITION_ID, PERSON_ID as SUBJECT_ID,
                 BIRTH_DATETIME as COHORT_START_DATE, BIRTH_DATETIME as COHORT_END_DATE
                 FROM main.person WHERE YEAR_OF_BIRTH = 1969")
covariateData &lt;- getCohortCovariateData(connection = con,
                                        cdmDatabaseSchema = "main",
                                        aggregated = FALSE,
                                        rowIdField = "SUBJECT_ID",
                                       cohortTable = "cohort",
                                        covariateSettings = createCohortCovariateSettings(
                                                               cohortName="summerOfLove",
                                                               cohortId=1969, 
                                                               settingId=1,
                                                               cohortDatabaseSchema="main",
                                                               cohortTable="cohort"))
covariateData$covariateRef
covariateData$covariates


</code></pre>

<hr>
<h2 id='getDemographicSummary'>Get a demographic summary</h2><span id='topic+getDemographicSummary'></span>

<h3>Description</h3>

<p>Get a demographic summary
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getDemographicSummary(prediction, predictionType, typeColumn = "evaluation")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="getDemographicSummary_+3A_prediction">prediction</code></td>
<td>
<p>A prediction object</p>
</td></tr>
<tr><td><code id="getDemographicSummary_+3A_predictiontype">predictionType</code></td>
<td>
<p>The type of prediction (binary or survival)</p>
</td></tr>
<tr><td><code id="getDemographicSummary_+3A_typecolumn">typeColumn</code></td>
<td>
<p>A column that is used to stratify the results</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Generates a data.frame with a prediction summary per each 5 year age group
and gender group
</p>


<h3>Value</h3>

<p>A dataframe with the demographic summary
</p>


<h3>Examples</h3>

<pre><code class='language-R'> 
# simulate data
data("simulationProfile")
plpData &lt;- simulatePlpData(simulationProfile, n=500)
# create study population, split into train/test and preprocess with default settings
population &lt;- createStudyPopulation(plpData, outcomeId = 3)
data &lt;- splitData(plpData, population, createDefaultSplitSetting())
data$Train$covariateData &lt;- preprocessData(data$Train$covariateData)
saveLoc &lt;- file.path(tempdir(), "demographicSummary")
# fit a lasso logistic regression model using the training data
plpModel &lt;- fitPlp(data$Train, modelSettings=setLassoLogisticRegression(seed=42),
                   analysisId=1, analysisPath=saveLoc)
demographicSummary &lt;- getDemographicSummary(plpModel$prediction, 
                                            "binary", 
                                            typeColumn = "evaluationType")
# show the demographic summary dataframe
str(demographicSummary)
# clean up
unlink(saveLoc, recursive = TRUE)

</code></pre>

<hr>
<h2 id='getEunomiaPlpData'>Create a plpData object from the Eunomia database'</h2><span id='topic+getEunomiaPlpData'></span>

<h3>Description</h3>

<p>This function creates a plpData object from the Eunomia database. It gets
the connection details, creates the cohorts, and extracts the data. The cohort
is predicting GIbleed in new users of celecoxib.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getEunomiaPlpData(covariateSettings = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="getEunomiaPlpData_+3A_covariatesettings">covariateSettings</code></td>
<td>
<p>A list of covariateSettings objects created using the
<code>createCovariateSettings</code> function in the <code>FeatureExtraction</code> package.
If nothing is specified covariates with age, gender, conditions and drug era are used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of type <code>plpData</code>, containing information on the cohorts, their
outcomes, and baseline covariates. Information about multiple outcomes can be
captured at once for efficiency reasons. This object is a list with the
following components: </p>
 <dl>
<dt>outcomes</dt><dd><p>A data frame listing the
outcomes per person, including the time to event, and the outcome id</p>
</dd>
<dt>cohorts</dt><dd><p>A data frame listing the persons in each cohort, listing their
exposure status as well as the time to the end of the observation period and
time to the end of the cohort</p>
</dd> <dt>covariateData</dt><dd><p>An Andromeda object created
with the <code>FeatureExtraction</code> package. This object contains the following items:
</p>
 <dl>
<dt>covariates</dt><dd><p>An Andromeda table listing the covariates per
person in the two cohorts. This is done using a sparse representation:
covariates with a value of 0 are omitted to save space. Usually has three
columns, rowId, covariateId and covariateValue'.</p>
</dd> <dt>covariateRef</dt><dd><p>An
Andromeda table describing the covariates that have been extracted.</p>
</dd>
<dt>AnalysisRef</dt><dd><p>An Andromeda table with information about which analysisIds
from 'FeatureExtraction' were used.</p>
</dd> </dl>
</dd></dl>



<h3>Examples</h3>

<pre><code class='language-R'>
 
covariateSettings &lt;- FeatureExtraction::createCovariateSettings(
  useDemographicsAge = TRUE,
  useDemographicsGender = TRUE,
  useConditionOccurrenceAnyTimePrior = TRUE
)
plpData &lt;- getEunomiaPlpData(covariateSettings = covariateSettings)


</code></pre>

<hr>
<h2 id='getPlpData'>Extract the patient level prediction data from the server</h2><span id='topic+getPlpData'></span>

<h3>Description</h3>

<p>This function executes a large set of SQL statements against the database in OMOP CDM format to
extract the data needed to perform the analysis.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getPlpData(databaseDetails, covariateSettings, restrictPlpDataSettings = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="getPlpData_+3A_databasedetails">databaseDetails</code></td>
<td>
<p>The cdm database details created using <code>createDatabaseDetails()</code></p>
</td></tr>
<tr><td><code id="getPlpData_+3A_covariatesettings">covariateSettings</code></td>
<td>
<p>An object of type <code>covariateSettings</code> or a list of such objects as created using the
<code>createCovariateSettings</code> function in the
<code>FeatureExtraction</code> package.</p>
</td></tr>
<tr><td><code id="getPlpData_+3A_restrictplpdatasettings">restrictPlpDataSettings</code></td>
<td>
<p>Extra settings to apply to the target population while extracting data.
Created using <code>createRestrictPlpDataSettings()</code>. This is optional.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Based on the arguments, the at risk cohort data is retrieved, as well as outcomes
occurring in these subjects. The at risk cohort is identified  through
user-defined cohorts in a cohort table either inside the CDM instance or in a separate schema.
Similarly, outcomes are identified
through user-defined cohorts in a cohort table either inside the CDM instance or in a separate
schema. Covariates are automatically extracted from the appropriate tables within the CDM.
If you wish to exclude concepts from covariates you will need to
manually add the concept_ids and descendants to the <code>excludedCovariateConceptIds</code> of the
<code>covariateSettings</code> argument.
</p>


<h3>Value</h3>

<p>'r plpDataObjectDoc()'
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
 
 # use Eunomia database
 connectionDetails &lt;- Eunomia::getEunomiaConnectionDetails()
 Eunomia::createCohorts(connectionDetails)
 outcomeId &lt;- 3 # GIbleed
 databaseDetails &lt;- createDatabaseDetails(
   connectionDetails = connectionDetails,
   cdmDatabaseSchema = "main",
   cdmDatabaseName = "main",
   cohortDatabaseSchema = "main",
   cohortTable = "cohort",
   outcomeDatabaseSchema = "main",
   outcomeTable = "cohort",
   targetId = 1,
   outcomeIds = outcomeId,
   cdmVersion = 5
 )

 covariateSettings &lt;- FeatureExtraction::createCovariateSettings(
   useDemographicsAge = TRUE,
   useDemographicsGender = TRUE,
   useConditionOccurrenceAnyTimePrior = TRUE
 )

 plpData &lt;- getPlpData(
   databaseDetails = databaseDetails,
   covariateSettings = covariateSettings,
   restrictPlpDataSettings = createRestrictPlpDataSettings()
 )


</code></pre>

<hr>
<h2 id='getPredictionDistribution'>Calculates the prediction distribution</h2><span id='topic+getPredictionDistribution'></span>

<h3>Description</h3>

<p>Calculates the prediction distribution
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getPredictionDistribution(
  prediction,
  predictionType = "binary",
  typeColumn = "evaluation"
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="getPredictionDistribution_+3A_prediction">prediction</code></td>
<td>
<p>A prediction object</p>
</td></tr>
<tr><td><code id="getPredictionDistribution_+3A_predictiontype">predictionType</code></td>
<td>
<p>The type of prediction (binary or survival)</p>
</td></tr>
<tr><td><code id="getPredictionDistribution_+3A_typecolumn">typeColumn</code></td>
<td>
<p>A column that is used to stratify the results</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Calculates the quantiles from a predition object
</p>


<h3>Value</h3>

<p>The 0.00, 0.1, 0.25, 0.5, 0.75, 0.9, 1.00 quantile pf the prediction,
the mean and standard deviation per class
</p>


<h3>Examples</h3>

<pre><code class='language-R'>prediction &lt;- data.frame(rowId = 1:100, 
                         outcomeCount = stats::rbinom(1:100, 1, prob=0.5), 
                         value = runif(100), 
                         evaluation = rep("Train", 100))
getPredictionDistribution(prediction)
</code></pre>

<hr>
<h2 id='getPredictionDistribution_binary'>Calculates the prediction distribution</h2><span id='topic+getPredictionDistribution_binary'></span>

<h3>Description</h3>

<p>Calculates the prediction distribution
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getPredictionDistribution_binary(prediction, evalColumn, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="getPredictionDistribution_binary_+3A_prediction">prediction</code></td>
<td>
<p>A prediction object</p>
</td></tr>
<tr><td><code id="getPredictionDistribution_binary_+3A_evalcolumn">evalColumn</code></td>
<td>
<p>A column that is used to stratify the results</p>
</td></tr>
<tr><td><code id="getPredictionDistribution_binary_+3A_...">...</code></td>
<td>
<p>Other inputs</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Calculates the quantiles from a predition object
</p>


<h3>Value</h3>

<p>The 0.00, 0.1, 0.25, 0.5, 0.75, 0.9, 1.00 quantile pf the prediction,
the mean and standard deviation per class
</p>

<hr>
<h2 id='getThresholdSummary'>Calculate all measures for sparse ROC</h2><span id='topic+getThresholdSummary'></span>

<h3>Description</h3>

<p>Calculate all measures for sparse ROC
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getThresholdSummary(
  prediction,
  predictionType = "binary",
  typeColumn = "evaluation"
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="getThresholdSummary_+3A_prediction">prediction</code></td>
<td>
<p>A prediction object</p>
</td></tr>
<tr><td><code id="getThresholdSummary_+3A_predictiontype">predictionType</code></td>
<td>
<p>The type of prediction (binary or survival)</p>
</td></tr>
<tr><td><code id="getThresholdSummary_+3A_typecolumn">typeColumn</code></td>
<td>
<p>A column that is used to stratify the results</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Calculates the TP, FP, TN, FN, TPR, FPR, accuracy, PPF, FOR and Fmeasure
from a prediction object
</p>


<h3>Value</h3>

<p>A data.frame with TP, FP, TN, FN, TPR, FPR, accuracy, PPF, FOR and Fmeasure
</p>


<h3>Examples</h3>

<pre><code class='language-R'>prediction &lt;- data.frame(rowId = 1:100, 
                         outcomeCount = stats::rbinom(1:100, 1, prob=0.5),
                         value = runif(100), 
                         evaluation = rep("Train", 100))
summary &lt;- getThresholdSummary(prediction)
str(summary)
</code></pre>

<hr>
<h2 id='ici'>Calculate the Integrated Calibration Index from Austin and Steyerberg
https://onlinelibrary.wiley.com/doi/full/10.1002/sim.8281</h2><span id='topic+ici'></span>

<h3>Description</h3>

<p>Calculate the Integrated Calibration Index from Austin and Steyerberg
https://onlinelibrary.wiley.com/doi/full/10.1002/sim.8281
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ici(prediction)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ici_+3A_prediction">prediction</code></td>
<td>
<p>the prediction object found in the plpResult object</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Calculate the Integrated Calibration Index
</p>


<h3>Value</h3>

<p>Integrated Calibration Index value or NULL if the calculation fails
</p>


<h3>Examples</h3>

<pre><code class='language-R'>prediction &lt;- data.frame(rowId = 1:100, 
                        outcomeCount = stats::rbinom(1:100, 1, prob=0.5),
                        value = runif(100), 
                        evaluation = rep("Train", 100))
ici(prediction)
</code></pre>

<hr>
<h2 id='insertCsvToDatabase'>Function to insert results into a database from csvs</h2><span id='topic+insertCsvToDatabase'></span>

<h3>Description</h3>

<p>This function converts a folder with csv results into plp objects and loads
them into a plp result database
</p>


<h3>Usage</h3>

<pre><code class='language-R'>insertCsvToDatabase(
  csvFolder,
  connectionDetails,
  databaseSchemaSettings,
  modelSaveLocation,
  csvTableAppend = ""
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="insertCsvToDatabase_+3A_csvfolder">csvFolder</code></td>
<td>
<p>The location to the csv folder with the plp results</p>
</td></tr>
<tr><td><code id="insertCsvToDatabase_+3A_connectiondetails">connectionDetails</code></td>
<td>
<p>A connection details for the plp results database that the csv results will be inserted into</p>
</td></tr>
<tr><td><code id="insertCsvToDatabase_+3A_databaseschemasettings">databaseSchemaSettings</code></td>
<td>
<p>A object created by <code>createDatabaseSchemaSettings</code> with all the settings specifying the result tables to insert the csv results into</p>
</td></tr>
<tr><td><code id="insertCsvToDatabase_+3A_modelsavelocation">modelSaveLocation</code></td>
<td>
<p>The location to save any models from the csv folder - this should be the same location you picked when inserting other models into the database</p>
</td></tr>
<tr><td><code id="insertCsvToDatabase_+3A_csvtableappend">csvTableAppend</code></td>
<td>
<p>A string that appends the csv file names</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The user needs to have plp csv results in a single folder and an existing plp
result database
</p>


<h3>Value</h3>

<p>Returns a data.frame indicating whether the results were inported into the database
</p>


<h3>Examples</h3>

<pre><code class='language-R'> 
# develop a simple model on simulated data
data("simulationProfile")
plpData &lt;- simulatePlpData(simulationProfile, n=1000)
saveLoc &lt;- file.path(tempdir(), "extractDatabaseToCsv")
results &lt;- runPlp(plpData, outcomeId=3, saveDirectory=saveLoc)
# now upload the results to a sqlite database
databasePath &lt;- insertResultsToSqlite(saveLoc)
# now extract the results to csv
connectionDetails &lt;- 
 DatabaseConnector::createConnectionDetails(dbms = "sqlite", 
                                            server = databasePath)
extractDatabaseToCsv(connectionDetails = connectionDetails,
                     csvFolder = file.path(saveLoc, "csv"))
# show csv file
list.files(file.path(saveLoc, "csv"))
# now insert the csv results into a database
newDatabasePath &lt;- file.path(tempdir(), "newDatabase.sqlite")
connectionDetails &lt;- 
 DatabaseConnector::createConnectionDetails(dbms = "sqlite", 
                                            server = newDatabasePath)
insertCsvToDatabase(csvFolder = file.path(saveLoc, "csv"),
                     connectionDetails = connectionDetails,
                     databaseSchemaSettings = createDatabaseSchemaSettings(),
                     modelSaveLocation = file.path(saveLoc, "models"))
# clean up
unlink(saveLoc, recursive = TRUE)

</code></pre>

<hr>
<h2 id='insertResultsToSqlite'>Create sqlite database with the results</h2><span id='topic+insertResultsToSqlite'></span>

<h3>Description</h3>

<p>This function create an sqlite database with the PLP result schema and inserts all results
</p>


<h3>Usage</h3>

<pre><code class='language-R'>insertResultsToSqlite(
  resultLocation,
  cohortDefinitions = NULL,
  databaseList = NULL,
  sqliteLocation = file.path(resultLocation, "sqlite")
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="insertResultsToSqlite_+3A_resultlocation">resultLocation</code></td>
<td>
<p>(string) location of directory where the main package results were saved</p>
</td></tr>
<tr><td><code id="insertResultsToSqlite_+3A_cohortdefinitions">cohortDefinitions</code></td>
<td>
<p>A set of one or more cohorts extracted using ROhdsiWebApi::exportCohortDefinitionSet()</p>
</td></tr>
<tr><td><code id="insertResultsToSqlite_+3A_databaselist">databaseList</code></td>
<td>
<p>A list created by <code>createDatabaseList</code> to specify the databases</p>
</td></tr>
<tr><td><code id="insertResultsToSqlite_+3A_sqlitelocation">sqliteLocation</code></td>
<td>
<p>(string) location of directory where the sqlite database will be saved</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function can be used upload PatientLevelPrediction results into an sqlite database
</p>


<h3>Value</h3>

<p>Returns the location of the sqlite database file
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
 
plpData &lt;- getEunomiaPlpData()
saveLoc &lt;- file.path(tempdir(), "insertResultsToSqlite")
results &lt;- runPlp(plpData, outcomeId = 3, analysisId = 1, saveDirectory = saveLoc)
databaseFile &lt;- insertResultsToSqlite(saveLoc, cohortDefinitions = NULL, 
                                      sqliteLocation = file.path(saveLoc, "sqlite"))
# check there is some data in the database
library(DatabaseConnector)
connectionDetails &lt;- createConnectionDetails(
  dbms = "sqlite",
  server = databaseFile)
conn &lt;- connect(connectionDetails)
# All tables should be created
getTableNames(conn, databaseSchema = "main")
# There is data in the tables
querySql(conn, "SELECT * FROM main.model_designs limit 10")
# clean up
unlink(saveLoc, recursive = TRUE)


</code></pre>

<hr>
<h2 id='iterativeImpute'>Imputation</h2><span id='topic+iterativeImpute'></span>

<h3>Description</h3>

<p>This function does single imputation with predictive mean matchin
</p>


<h3>Usage</h3>

<pre><code class='language-R'>iterativeImpute(trainData, featureEngineeringSettings, done = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="iterativeImpute_+3A_traindata">trainData</code></td>
<td>
<p>The data to be imputed</p>
</td></tr>
<tr><td><code id="iterativeImpute_+3A_featureengineeringsettings">featureEngineeringSettings</code></td>
<td>
<p>The settings for the imputation</p>
</td></tr>
<tr><td><code id="iterativeImpute_+3A_done">done</code></td>
<td>
<p>Whether the imputation has already been done (bool)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The imputed data
</p>

<hr>
<h2 id='listAppend'>join two lists</h2><span id='topic+listAppend'></span>

<h3>Description</h3>

<p>join two lists
</p>


<h3>Usage</h3>

<pre><code class='language-R'>listAppend(a, b)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="listAppend_+3A_a">a</code></td>
<td>
<p>A list</p>
</td></tr>
<tr><td><code id="listAppend_+3A_b">b</code></td>
<td>
<p>Another list</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function joins two lists
</p>


<h3>Value</h3>

<p>the joined list
</p>


<h3>Examples</h3>

<pre><code class='language-R'>a &lt;- list(a = 1, b = 2)
b &lt;- list(c = 3, d = 4)
listAppend(a, b)

</code></pre>

<hr>
<h2 id='listCartesian'>Cartesian product</h2><span id='topic+listCartesian'></span>

<h3>Description</h3>

<p>Computes the Cartesian product of all the combinations of elements in a list
</p>


<h3>Usage</h3>

<pre><code class='language-R'>listCartesian(allList)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="listCartesian_+3A_alllist">allList</code></td>
<td>
<p>a list of lists</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with all possible combinations from the input list of lists
</p>


<h3>Examples</h3>

<pre><code class='language-R'>listCartesian(list(list(1, 2), list(3, 4)))
</code></pre>

<hr>
<h2 id='loadPlpAnalysesJson'>Load the multiple prediction json settings from a file</h2><span id='topic+loadPlpAnalysesJson'></span>

<h3>Description</h3>

<p>Load the multiple prediction json settings from a file
</p>


<h3>Usage</h3>

<pre><code class='language-R'>loadPlpAnalysesJson(jsonFileLocation)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="loadPlpAnalysesJson_+3A_jsonfilelocation">jsonFileLocation</code></td>
<td>
<p>The location of the file 'predictionAnalysisList.json' with the modelDesignList</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function interprets a json with the multiple prediction settings and creates a list
that can be combined with connection settings to run a multiple prediction study
</p>


<h3>Value</h3>

<p>A list with the modelDesignList and cohortDefinitions
</p>


<h3>Examples</h3>

<pre><code class='language-R'>modelDesign &lt;- createModelDesign(targetId = 1, outcomeId = 2, 
                                 modelSettings = setLassoLogisticRegression())
saveLoc &lt;- file.path(tempdir(), "loadPlpAnalysesJson")
savePlpAnalysesJson(modelDesignList = modelDesign, saveDirectory = saveLoc)
loadPlpAnalysesJson(file.path(saveLoc, "predictionAnalysisList.json"))
# clean use
unlink(saveLoc, recursive = TRUE)
</code></pre>

<hr>
<h2 id='loadPlpData'>Load the plpData from a folder</h2><span id='topic+loadPlpData'></span>

<h3>Description</h3>

<p><code>loadPlpData</code> loads an object of type plpData from a folder in the file
system.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>loadPlpData(file, readOnly = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="loadPlpData_+3A_file">file</code></td>
<td>
<p>The name of the folder containing the data.</p>
</td></tr>
<tr><td><code id="loadPlpData_+3A_readonly">readOnly</code></td>
<td>
<p>If true, the data is opened read only.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The data will be written to a set of files in the folder specified by the user.
</p>


<h3>Value</h3>

<p>An object of class plpData.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("simulationProfile")
plpData &lt;- simulatePlpData(simulationProfile, n = 500)
saveLoc &lt;- file.path(tempdir(), "loadPlpData")
savePlpData(plpData, saveLoc)
dir(saveLoc)
# clean up
unlink(saveLoc, recursive = TRUE)
</code></pre>

<hr>
<h2 id='loadPlpModel'>loads the plp model</h2><span id='topic+loadPlpModel'></span>

<h3>Description</h3>

<p>loads the plp model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>loadPlpModel(dirPath)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="loadPlpModel_+3A_dirpath">dirPath</code></td>
<td>
<p>The location of the model</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Loads a plp model that was saved using <code>savePlpModel()</code>
</p>


<h3>Value</h3>

<div class="sourceCode"><pre>                    The plpModel object
</pre></div>


<h3>Examples</h3>

<pre><code class='language-R'> 
data("simulationProfile")
plpData &lt;- simulatePlpData(simulationProfile, n = 1000)
saveLoc &lt;- file.path(tempdir(), "loadPlpModel")
plpResult &lt;- runPlp(plpData, outcomeId = 3, saveDirectory = saveLoc)
savePlpModel(plpResult$model, file.path(saveLoc, "savedModel"))
loadedModel &lt;- loadPlpModel(file.path(saveLoc, "savedModel"))
# show design of loaded model
str(loadedModel$modelDesign)

# clean up
unlink(saveLoc, recursive = TRUE)

</code></pre>

<hr>
<h2 id='loadPlpResult'>Loads the evalaution dataframe</h2><span id='topic+loadPlpResult'></span>

<h3>Description</h3>

<p>Loads the evalaution dataframe
</p>


<h3>Usage</h3>

<pre><code class='language-R'>loadPlpResult(dirPath)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="loadPlpResult_+3A_dirpath">dirPath</code></td>
<td>
<p>The directory where the evaluation was saved</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Loads the evaluation
</p>


<h3>Value</h3>

<div class="sourceCode"><pre>                       The runPlp object
</pre></div>


<h3>Examples</h3>

<pre><code class='language-R'> 
data("simulationProfile")
plpData &lt;- simulatePlpData(simulationProfile, n = 1000)
saveLoc &lt;- file.path(tempdir(), "loadPlpResult")
results &lt;- runPlp(plpData, outcomeId = 3, saveDirectory = saveLoc)
savePlpResult(results, saveLoc)
loadedResults &lt;- loadPlpResult(saveLoc)
# clean up
unlink(saveLoc, recursive = TRUE)

</code></pre>

<hr>
<h2 id='loadPlpShareable'>Loads the plp result saved as json/csv files for transparent sharing</h2><span id='topic+loadPlpShareable'></span>

<h3>Description</h3>

<p>Loads the plp result saved as json/csv files for transparent sharing
</p>


<h3>Usage</h3>

<pre><code class='language-R'>loadPlpShareable(loadDirectory)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="loadPlpShareable_+3A_loaddirectory">loadDirectory</code></td>
<td>
<p>The directory with the results as json/csv files</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Load the main results from json/csv files into a runPlp object
</p>


<h3>Value</h3>

<div class="sourceCode"><pre>                             The runPlp object
</pre></div>


<h3>Examples</h3>

<pre><code class='language-R'> 
data("simulationProfile")
plpData &lt;- simulatePlpData(simulationProfile, n = 1000)
saveLoc &lt;- file.path(tempdir(), "loadPlpShareable")
results &lt;- runPlp(plpData, outcomeId = 3, saveDirectory = saveLoc)
savePlpShareable(results, saveLoc)
dir(saveLoc)
loadedResults &lt;- loadPlpShareable(saveLoc)
# clean up
unlink(saveLoc, recursive = TRUE)

</code></pre>

<hr>
<h2 id='loadPrediction'>Loads the prediction dataframe to json</h2><span id='topic+loadPrediction'></span>

<h3>Description</h3>

<p>Loads the prediction dataframe to json
</p>


<h3>Usage</h3>

<pre><code class='language-R'>loadPrediction(fileLocation)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="loadPrediction_+3A_filelocation">fileLocation</code></td>
<td>
<p>The location with the saved prediction</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Loads the prediciton json file
</p>


<h3>Value</h3>

<div class="sourceCode"><pre>                            The prediction data.frame
</pre></div>


<h3>Examples</h3>

<pre><code class='language-R'> 
data("simulationProfile")
plpData &lt;- simulatePlpData(simulationProfile, n = 1000)
saveLoc &lt;- file.path(tempdir(), "loadPrediction")
results &lt;- runPlp(plpData, outcomeId = 3, saveDirectory = saveLoc)
savePrediction(results$prediction, saveLoc)
dir(saveLoc)
loadedPrediction &lt;- loadPrediction(file.path(saveLoc, "prediction.json"))

</code></pre>

<hr>
<h2 id='MapIds'>Map covariate and row Ids so they start from 1</h2><span id='topic+MapIds'></span>

<h3>Description</h3>

<p>this functions takes covariate data and a cohort/population and remaps
the covariate and row ids, restricts to pop and saves/creates mapping
</p>


<h3>Usage</h3>

<pre><code class='language-R'>MapIds(covariateData, cohort = NULL, mapping = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="MapIds_+3A_covariatedata">covariateData</code></td>
<td>
<p>a covariateData object</p>
</td></tr>
<tr><td><code id="MapIds_+3A_cohort">cohort</code></td>
<td>
<p>if specified rowIds restricted to the ones in cohort</p>
</td></tr>
<tr><td><code id="MapIds_+3A_mapping">mapping</code></td>
<td>
<p>A pre defined mapping to use</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a new <code>covariateData</code> object with remapped covariate and row ids
</p>


<h3>Examples</h3>

<pre><code class='language-R'>covariateData &lt;- Andromeda::andromeda(
  covariates = data.frame(rowId = c(1, 3, 5, 7, 9), 
                          covariateId = c(10, 20, 10, 10, 20),
                          covariateValue = c(1, 1, 1, 1, 1)),
  covariateRef = data.frame(covariateId = c(10, 20), 
                              covariateNames = c("covariateA", 
                                                 "covariateB"),
                              analysisId = c(1, 1)))
mappedData &lt;- MapIds(covariateData)
# columnId and rowId are now starting from 1 and are consecutive
mappedData$covariates
</code></pre>

<hr>
<h2 id='migrateDataModel'>Migrate Data model</h2><span id='topic+migrateDataModel'></span>

<h3>Description</h3>

<p>Migrate data from current state to next state
</p>
<p>It is strongly advised that you have a backup of all data (either sqlite files, a backup database (in the case you
are using a postgres backend) or have kept the csv/zip files from your data generation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>migrateDataModel(connectionDetails, databaseSchema, tablePrefix = "")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="migrateDataModel_+3A_connectiondetails">connectionDetails</code></td>
<td>
<p>DatabaseConnector connection details object</p>
</td></tr>
<tr><td><code id="migrateDataModel_+3A_databaseschema">databaseSchema</code></td>
<td>
<p>String schema where database schema lives</p>
</td></tr>
<tr><td><code id="migrateDataModel_+3A_tableprefix">tablePrefix</code></td>
<td>
<p>(Optional) Use if a table prefix is used before table names (e.g. &quot;cd_&quot;)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Nothing. Is called for side effects of migrating data model in the
database
</p>

<hr>
<h2 id='minMaxNormalize'>A function that normalizes continous features to have values between 0 and 1</h2><span id='topic+minMaxNormalize'></span>

<h3>Description</h3>

<p>A function that normalizes continous features to have values between 0 and 1
</p>


<h3>Usage</h3>

<pre><code class='language-R'>minMaxNormalize(trainData, featureEngineeringSettings, done = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="minMaxNormalize_+3A_traindata">trainData</code></td>
<td>
<p>The training data to be normalized</p>
</td></tr>
<tr><td><code id="minMaxNormalize_+3A_featureengineeringsettings">featureEngineeringSettings</code></td>
<td>
<p>The settings for the normalization</p>
</td></tr>
<tr><td><code id="minMaxNormalize_+3A_done">done</code></td>
<td>
<p>Whether the data has already been normalized (bool)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>uses value - min / (max - min) to normalize the data
</p>


<h3>Value</h3>

<p>The normalized data
</p>

<hr>
<h2 id='modelBasedConcordance'>Calculate the model-based concordance, which is a calculation of the expected
discrimination performance of a model under the assumption the model predicts
the &quot;TRUE&quot; outcome as detailed in van Klaveren et al.
https://pubmed.ncbi.nlm.nih.gov/27251001/</h2><span id='topic+modelBasedConcordance'></span>

<h3>Description</h3>

<p>Calculate the model-based concordance, which is a calculation of the expected
discrimination performance of a model under the assumption the model predicts
the &quot;TRUE&quot; outcome as detailed in van Klaveren et al.
https://pubmed.ncbi.nlm.nih.gov/27251001/
</p>


<h3>Usage</h3>

<pre><code class='language-R'>modelBasedConcordance(prediction)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="modelBasedConcordance_+3A_prediction">prediction</code></td>
<td>
<p>the prediction object found in the plpResult object</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Calculate the model-based concordance
</p>


<h3>Value</h3>

<p>The model-based concordance value
</p>


<h3>Examples</h3>

<pre><code class='language-R'>prediction &lt;- data.frame(value = runif(100))
modelBasedConcordance(prediction)
</code></pre>

<hr>
<h2 id='outcomeSurvivalPlot'>Plot the outcome incidence over time</h2><span id='topic+outcomeSurvivalPlot'></span>

<h3>Description</h3>

<p>Plot the outcome incidence over time
</p>


<h3>Usage</h3>

<pre><code class='language-R'>outcomeSurvivalPlot(
  plpData,
  outcomeId,
  populationSettings = createStudyPopulationSettings(binary = TRUE, includeAllOutcomes =
    TRUE, firstExposureOnly = FALSE, washoutPeriod = 0, removeSubjectsWithPriorOutcome =
    TRUE, priorOutcomeLookback = 99999, requireTimeAtRisk = FALSE, riskWindowStart = 1,
    startAnchor = "cohort start", riskWindowEnd = 3650, endAnchor = "cohort start"),
  riskTable = TRUE,
  confInt = TRUE,
  yLabel = "Fraction of those who are outcome free in target population"
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="outcomeSurvivalPlot_+3A_plpdata">plpData</code></td>
<td>
<p>The plpData object returned by running getPlpData()</p>
</td></tr>
<tr><td><code id="outcomeSurvivalPlot_+3A_outcomeid">outcomeId</code></td>
<td>
<p>The cohort id corresponding to the outcome</p>
</td></tr>
<tr><td><code id="outcomeSurvivalPlot_+3A_populationsettings">populationSettings</code></td>
<td>
<p>The population settings created using <code>createStudyPopulationSettings</code></p>
</td></tr>
<tr><td><code id="outcomeSurvivalPlot_+3A_risktable">riskTable</code></td>
<td>
<p>(binary) Whether to include a table at the bottom  of the plot showing the number of people at risk over time</p>
</td></tr>
<tr><td><code id="outcomeSurvivalPlot_+3A_confint">confInt</code></td>
<td>
<p>(binary) Whether to include a confidence interval</p>
</td></tr>
<tr><td><code id="outcomeSurvivalPlot_+3A_ylabel">yLabel</code></td>
<td>
<p>(string) The label for the y-axis</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This creates a survival plot that can be used to pick a suitable time-at-risk period
</p>


<h3>Value</h3>

<p>A <code>ggsurvplot</code> object
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data("simulationProfile")
plpData &lt;- simulatePlpData(simulationProfile, n=1000)
plotObject &lt;- outcomeSurvivalPlot(plpData, outcomeId = 3)
print(plotObject)

</code></pre>

<hr>
<h2 id='pfi'>Permutation Feature Importance</h2><span id='topic+pfi'></span>

<h3>Description</h3>

<p>Calculate the permutation feature importance (pfi) for a PLP model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pfi(
  plpResult,
  population,
  plpData,
  repeats = 1,
  covariates = NULL,
  cores = NULL,
  log = NULL,
  logthreshold = "INFO"
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="pfi_+3A_plpresult">plpResult</code></td>
<td>
<p>An object of type <code>runPlp</code></p>
</td></tr>
<tr><td><code id="pfi_+3A_population">population</code></td>
<td>
<p>The population created using createStudyPopulation() who will have their risks predicted</p>
</td></tr>
<tr><td><code id="pfi_+3A_plpdata">plpData</code></td>
<td>
<p>An object of type <code>plpData</code> - the patient level prediction
data extracted from the CDM.</p>
</td></tr>
<tr><td><code id="pfi_+3A_repeats">repeats</code></td>
<td>
<p>The number of times to permute each covariate</p>
</td></tr>
<tr><td><code id="pfi_+3A_covariates">covariates</code></td>
<td>
<p>A vector of covariates to calculate the pfi for.  If NULL it uses all covariates included in the model.</p>
</td></tr>
<tr><td><code id="pfi_+3A_cores">cores</code></td>
<td>
<p>Number of cores to use when running this (it runs in parallel)</p>
</td></tr>
<tr><td><code id="pfi_+3A_log">log</code></td>
<td>
<p>A location to save the log for running pfi</p>
</td></tr>
<tr><td><code id="pfi_+3A_logthreshold">logthreshold</code></td>
<td>
<p>The log threshold (e.g., INFO, TRACE, ...)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function permutes the each covariate/features <code>repeats</code> times and
calculates the mean AUC change caused by the permutation.
</p>


<h3>Value</h3>

<p>A dataframe with the covariateIds and the pfi (change in AUC caused by permuting the covariate) value
</p>


<h3>Examples</h3>

<pre><code class='language-R'> 
library(dplyr)
# simulate some data
data("simulationProfile")
plpData &lt;- simulatePlpData(simulationProfile, n=1000)
# now fit a model
saveLoc &lt;- file.path(tempdir(), "pfi")
plpResult &lt;- runPlp(plpData, outcomeId = 3, saveDirectory = saveLoc)
population &lt;- createStudyPopulation(plpData, outcomeId = 3)
pfi(plpResult, population, plpData, repeats = 1, cores = 1)
# compare to model coefficients
plpResult$model$covariateImportance %&gt;% dplyr::filter(.data$covariateValue != 0)
# clean up
unlink(saveLoc, recursive = TRUE)

</code></pre>

<hr>
<h2 id='plotDemographicSummary'>Plot the Observed vs. expected incidence, by age and gender</h2><span id='topic+plotDemographicSummary'></span>

<h3>Description</h3>

<p>Plot the Observed vs. expected incidence, by age and gender
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotDemographicSummary(
  plpResult,
  typeColumn = "evaluation",
  saveLocation = NULL,
  fileName = "roc.png"
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plotDemographicSummary_+3A_plpresult">plpResult</code></td>
<td>
<p>A plp result object as generated using the <code><a href="#topic+runPlp">runPlp</a></code> function.</p>
</td></tr>
<tr><td><code id="plotDemographicSummary_+3A_typecolumn">typeColumn</code></td>
<td>
<p>The name of the column specifying the evaluation type</p>
</td></tr>
<tr><td><code id="plotDemographicSummary_+3A_savelocation">saveLocation</code></td>
<td>
<p>Directory to save plot (if NULL plot is not saved)</p>
</td></tr>
<tr><td><code id="plotDemographicSummary_+3A_filename">fileName</code></td>
<td>
<p>Name of the file to save to plot, for example
'plot.png'. See the function <code>ggsave</code> in the ggplot2 package for
supported file formats.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Create a plot showing the Observed vs. expected incidence, by age and gender
#'
</p>


<h3>Value</h3>

<p>A ggplot object. Use the <code><a href="ggplot2.html#topic+ggsave">ggsave</a></code> function to save to file in a different
format.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

data("simulationProfile")
plpData &lt;- simulatePlpData(simulationProfile, n=1000)
saveLoc &lt;- file.path(tempdir(), "plotDemographicSummary")
plpResult &lt;- runPlp(plpData, outcomeId = 3, saveDirectory = saveLoc)
plotDemographicSummary(plpResult)
# clean up 
unlink(saveLoc, recursive = TRUE)


</code></pre>

<hr>
<h2 id='plotF1Measure'>Plot the F1 measure efficiency frontier using the sparse thresholdSummary data frame</h2><span id='topic+plotF1Measure'></span>

<h3>Description</h3>

<p>Plot the F1 measure efficiency frontier using the sparse thresholdSummary data frame
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotF1Measure(
  plpResult,
  typeColumn = "evaluation",
  saveLocation = NULL,
  fileName = "roc.png"
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plotF1Measure_+3A_plpresult">plpResult</code></td>
<td>
<p>A plp result object as generated using the <code><a href="#topic+runPlp">runPlp</a></code> function.</p>
</td></tr>
<tr><td><code id="plotF1Measure_+3A_typecolumn">typeColumn</code></td>
<td>
<p>The name of the column specifying the evaluation type</p>
</td></tr>
<tr><td><code id="plotF1Measure_+3A_savelocation">saveLocation</code></td>
<td>
<p>Directory to save plot (if NULL plot is not saved)</p>
</td></tr>
<tr><td><code id="plotF1Measure_+3A_filename">fileName</code></td>
<td>
<p>Name of the file to save to plot, for example
'plot.png'. See the function <code>ggsave</code> in the ggplot2 package for
supported file formats.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Create a plot showing the F1 measure efficiency frontier using the sparse thresholdSummary data frame
</p>


<h3>Value</h3>

<p>A ggplot object. Use the <code><a href="ggplot2.html#topic+ggsave">ggsave</a></code> function to save to file in a different
format.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
 
data("simulationProfile")
plpData &lt;- simulatePlpData(simulationProfile, n=1000)
saveLoc &lt;- file.path(tempdir(), "plotF1Measure")
results &lt;- runPlp(plpData, outcomeId = 3, saveDirectory = saveLoc)
plotF1Measure(results)
# clean up
unlink(saveLoc, recursive = TRUE)


</code></pre>

<hr>
<h2 id='plotGeneralizability'>Plot the train/test generalizability diagnostic</h2><span id='topic+plotGeneralizability'></span>

<h3>Description</h3>

<p>Plot the train/test generalizability diagnostic
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotGeneralizability(
  covariateSummary,
  saveLocation = NULL,
  fileName = "Generalizability.png"
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plotGeneralizability_+3A_covariatesummary">covariateSummary</code></td>
<td>
<p>A prediction object as generated using the
<code><a href="#topic+runPlp">runPlp</a></code> function.</p>
</td></tr>
<tr><td><code id="plotGeneralizability_+3A_savelocation">saveLocation</code></td>
<td>
<p>Directory to save plot (if NULL plot is not saved)</p>
</td></tr>
<tr><td><code id="plotGeneralizability_+3A_filename">fileName</code></td>
<td>
<p>Name of the file to save to plot, for example
'plot.png'. See the function <code>ggsave</code> in the ggplot2 package for
supported file formats.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Create a plot showing the train/test generalizability diagnostic
#'
</p>


<h3>Value</h3>

<p>A ggplot object. Use the <code><a href="ggplot2.html#topic+ggsave">ggsave</a></code> function to save to file in a different
format.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
 
data("simulationProfile")
plpData &lt;- simulatePlpData(simulationProfile, n=1000)
population &lt;- createStudyPopulation(plpData, outcomeId = 3)
data &lt;- splitData(plpData, population = population)
strata &lt;- data.frame(
 rowId = c(data$Train$labels$rowId, data$Test$labels$rowId),
 strataName = c(rep("Train", nrow(data$Train$labels)),
                rep("Test", nrow(data$Test$labels))))
covariateSummary &lt;- covariateSummary(plpData$covariateData, 
                                     cohort = dplyr::select(population, "rowId"),
 strata = strata, labels = population)
plotGeneralizability(covariateSummary)


</code></pre>

<hr>
<h2 id='plotLearningCurve'>plotLearningCurve</h2><span id='topic+plotLearningCurve'></span>

<h3>Description</h3>

<p>Create a plot of the learning curve using the object returned
from <code>createLearningCurve</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotLearningCurve(
  learningCurve,
  metric = "AUROC",
  abscissa = "events",
  plotTitle = "Learning Curve",
  plotSubtitle = NULL,
  fileName = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plotLearningCurve_+3A_learningcurve">learningCurve</code></td>
<td>
<p>An object returned by <code><a href="#topic+createLearningCurve">createLearningCurve</a></code>
function.</p>
</td></tr>
<tr><td><code id="plotLearningCurve_+3A_metric">metric</code></td>
<td>
<p>Specifies the metric to be plotted:
</p>

<ul>
<li><p><code>'AUROC'</code> - use the area under the Receiver Operating
Characteristic curve
</p>
</li>
<li><p><code>'AUPRC'</code> - use the area under the Precision-Recall curve
</p>
</li>
<li><p><code>'sBrier'</code> - use the scaled Brier score
</p>
</li></ul>
</td></tr>
<tr><td><code id="plotLearningCurve_+3A_abscissa">abscissa</code></td>
<td>
<p>Specify the abscissa metric to be plotted:
</p>

<ul>
<li><p><code>'events'</code> - use number of events
</p>
</li>
<li><p><code>'observations'</code> - use number of observations
</p>
</li></ul>
</td></tr>
<tr><td><code id="plotLearningCurve_+3A_plottitle">plotTitle</code></td>
<td>
<p>Title of the learning curve plot.</p>
</td></tr>
<tr><td><code id="plotLearningCurve_+3A_plotsubtitle">plotSubtitle</code></td>
<td>
<p>Subtitle of the learning curve plot.</p>
</td></tr>
<tr><td><code id="plotLearningCurve_+3A_filename">fileName</code></td>
<td>
<p>Filename of plot to be saved, for example <code>'plot.png'</code>.
See the function <code>ggsave</code> in the ggplot2 package for supported file
formats.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A ggplot object. Use the <code><a href="ggplot2.html#topic+ggsave">ggsave</a></code> function to save to
file in a different format.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
 
data("simulationProfile")
plpData &lt;- simulatePlpData(simulationProfile, n = 1000)
outcomeId &lt;- 3
modelSettings &lt;- setLassoLogisticRegression(seed=42)
learningCurve &lt;- createLearningCurve(plpData, outcomeId, modelSettings = modelSettings,
saveDirectory = file.path(tempdir(), "learningCurve"), cores = 2)
plotLearningCurve(learningCurve)


</code></pre>

<hr>
<h2 id='plotNetBenefit'>Plot the net benefit</h2><span id='topic+plotNetBenefit'></span>

<h3>Description</h3>

<p>Plot the net benefit
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotNetBenefit(
  plpResult,
  typeColumn = "evaluation",
  saveLocation = NULL,
  fileName = "netBenefit.png",
  evalType = NULL,
  ylim = NULL,
  xlim = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plotNetBenefit_+3A_plpresult">plpResult</code></td>
<td>
<p>A plp result object as generated using the <code><a href="#topic+runPlp">runPlp</a></code> function.</p>
</td></tr>
<tr><td><code id="plotNetBenefit_+3A_typecolumn">typeColumn</code></td>
<td>
<p>The name of the column specifying the evaluation type</p>
</td></tr>
<tr><td><code id="plotNetBenefit_+3A_savelocation">saveLocation</code></td>
<td>
<p>Directory to save plot (if NULL plot is not saved)</p>
</td></tr>
<tr><td><code id="plotNetBenefit_+3A_filename">fileName</code></td>
<td>
<p>Name of the file to save to plot, for example 'plot.png'. See the function <code>ggsave</code> in the ggplot2 package for supported file formats.</p>
</td></tr>
<tr><td><code id="plotNetBenefit_+3A_evaltype">evalType</code></td>
<td>
<p>Which evaluation type to plot for. For example <code>Test</code>, <code>Train</code>. If NULL everything is plotted</p>
</td></tr>
<tr><td><code id="plotNetBenefit_+3A_ylim">ylim</code></td>
<td>
<p>The y limits for the plot, if NULL the limits are calculated from the data</p>
</td></tr>
<tr><td><code id="plotNetBenefit_+3A_xlim">xlim</code></td>
<td>
<p>The x limits for the plot, if NULL the limits are calculated from the data</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of ggplot objects or a single ggplot object if only one evaluation type is plotted
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
 
data("simulationProfile")
plpData &lt;- simulatePlpData(simulationProfile, n=1000)
saveLoc &lt;- file.path(tempdir(), "plotNetBenefit")
results &lt;- runPlp(plpData, outcomeId = 3, saveDirectory = saveLoc)
plotNetBenefit(results)
# clean up
unlink(saveLoc, recursive = TRUE)


</code></pre>

<hr>
<h2 id='plotPlp'>Plot all the PatientLevelPrediction plots</h2><span id='topic+plotPlp'></span>

<h3>Description</h3>

<p>Plot all the PatientLevelPrediction plots
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotPlp(plpResult, saveLocation = NULL, typeColumn = "evaluation")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plotPlp_+3A_plpresult">plpResult</code></td>
<td>
<p>Object returned by the runPlp() function</p>
</td></tr>
<tr><td><code id="plotPlp_+3A_savelocation">saveLocation</code></td>
<td>
<p>Name of the directory where the plots should be saved (NULL means no saving)</p>
</td></tr>
<tr><td><code id="plotPlp_+3A_typecolumn">typeColumn</code></td>
<td>
<p>The name of the column specifying the evaluation type
(to stratify the plots)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Create a directory with all the plots
</p>


<h3>Value</h3>

<p>TRUE if it ran, plots are saved in the specified directory
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
 
data("simulationProfile")
plpData &lt;- simulatePlpData(simulationProfile, n=1000)
saveLoc &lt;- file.path(tempdir(), "plotPlp")
results &lt;- runPlp(plpData, outcomeId = 3, saveDirectory = saveLoc)
plotPlp(results)
# clean up
unlink(saveLoc, recursive = TRUE)


</code></pre>

<hr>
<h2 id='plotPrecisionRecall'>Plot the precision-recall curve using the sparse thresholdSummary data frame</h2><span id='topic+plotPrecisionRecall'></span>

<h3>Description</h3>

<p>Plot the precision-recall curve using the sparse thresholdSummary data frame
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotPrecisionRecall(
  plpResult,
  typeColumn = "evaluation",
  saveLocation = NULL,
  fileName = "roc.png"
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plotPrecisionRecall_+3A_plpresult">plpResult</code></td>
<td>
<p>A plp result object as generated using the <code><a href="#topic+runPlp">runPlp</a></code> function.</p>
</td></tr>
<tr><td><code id="plotPrecisionRecall_+3A_typecolumn">typeColumn</code></td>
<td>
<p>The name of the column specifying the evaluation type</p>
</td></tr>
<tr><td><code id="plotPrecisionRecall_+3A_savelocation">saveLocation</code></td>
<td>
<p>Directory to save plot (if NULL plot is not saved)</p>
</td></tr>
<tr><td><code id="plotPrecisionRecall_+3A_filename">fileName</code></td>
<td>
<p>Name of the file to save to plot, for example
'plot.png'. See the function <code>ggsave</code> in the ggplot2 package for
supported file formats.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Create a plot showing the precision-recall curve using the sparse thresholdSummary data frame
</p>


<h3>Value</h3>

<p>A ggplot object. Use the <code><a href="ggplot2.html#topic+ggsave">ggsave</a></code> function to save to file in a different
format.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
 
data("simulationProfile")
plpData &lt;- simulatePlpData(simulationProfile, n=1000)
saveLoc &lt;- file.path(tempdir(), "plotPrecisionRecall")
results &lt;- runPlp(plpData, outcomeId = 3, saveDirectory = saveLoc)
plotPrecisionRecall(results)
# clean up
unlink(saveLoc, recursive = TRUE)


</code></pre>

<hr>
<h2 id='plotPredictedPDF'>Plot the Predicted probability density function, showing prediction overlap between true and false cases</h2><span id='topic+plotPredictedPDF'></span>

<h3>Description</h3>

<p>Plot the Predicted probability density function, showing prediction overlap between true and false cases
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotPredictedPDF(
  plpResult,
  typeColumn = "evaluation",
  saveLocation = NULL,
  fileName = "PredictedPDF.png"
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plotPredictedPDF_+3A_plpresult">plpResult</code></td>
<td>
<p>A plp result object as generated using the <code><a href="#topic+runPlp">runPlp</a></code> function.</p>
</td></tr>
<tr><td><code id="plotPredictedPDF_+3A_typecolumn">typeColumn</code></td>
<td>
<p>The name of the column specifying the evaluation type</p>
</td></tr>
<tr><td><code id="plotPredictedPDF_+3A_savelocation">saveLocation</code></td>
<td>
<p>Directory to save plot (if NULL plot is not saved)</p>
</td></tr>
<tr><td><code id="plotPredictedPDF_+3A_filename">fileName</code></td>
<td>
<p>Name of the file to save to plot, for example
'plot.png'. See the function <code>ggsave</code> in the ggplot2 package for
supported file formats.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Create a plot showing the predicted probability density function, showing prediction overlap between true and false cases
</p>


<h3>Value</h3>

<p>A ggplot object. Use the <code><a href="ggplot2.html#topic+ggsave">ggsave</a></code> function to save to file in a different
format.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
 
data("simulationProfile")
plpData &lt;- simulatePlpData(simulationProfile, n=1000)
saveLoc &lt;- file.path(tempdir(), "plotPredictedPDF")
results &lt;- runPlp(plpData, outcomeId = 3, saveDirectory = saveLoc)
plotPredictedPDF(results)
# clean up
unlink(saveLoc, recursive = TRUE)


</code></pre>

<hr>
<h2 id='plotPredictionDistribution'>Plot the side-by-side boxplots of prediction distribution, by class</h2><span id='topic+plotPredictionDistribution'></span>

<h3>Description</h3>

<p>Plot the side-by-side boxplots of prediction distribution, by class
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotPredictionDistribution(
  plpResult,
  typeColumn = "evaluation",
  saveLocation = NULL,
  fileName = "PredictionDistribution.png"
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plotPredictionDistribution_+3A_plpresult">plpResult</code></td>
<td>
<p>A plp result object as generated using the <code><a href="#topic+runPlp">runPlp</a></code> function.</p>
</td></tr>
<tr><td><code id="plotPredictionDistribution_+3A_typecolumn">typeColumn</code></td>
<td>
<p>The name of the column specifying the evaluation type</p>
</td></tr>
<tr><td><code id="plotPredictionDistribution_+3A_savelocation">saveLocation</code></td>
<td>
<p>Directory to save plot (if NULL plot is not saved)</p>
</td></tr>
<tr><td><code id="plotPredictionDistribution_+3A_filename">fileName</code></td>
<td>
<p>Name of the file to save to plot, for example
'plot.png'. See the function <code>ggsave</code> in the ggplot2 package for
supported file formats.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Create a plot showing the side-by-side boxplots of prediction distribution, by class
#'
</p>


<h3>Value</h3>

<p>A ggplot object. Use the <code><a href="ggplot2.html#topic+ggsave">ggsave</a></code> function to save to file in a different
format.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
 
data("simulationProfile")
plpData &lt;- simulatePlpData(simulationProfile, n=1000)
saveLoc &lt;- file.path(tempdir(), "plotPredictionDistribution")
results &lt;- runPlp(plpData, outcomeId = 3, saveDirectory = saveLoc)
plotPredictionDistribution(results)
# clean up
unlink(saveLoc, recursive = TRUE)


</code></pre>

<hr>
<h2 id='plotPreferencePDF'>Plot the preference score probability density function, showing prediction overlap between true and false cases
#'</h2><span id='topic+plotPreferencePDF'></span>

<h3>Description</h3>

<p>Plot the preference score probability density function, showing prediction overlap between true and false cases
#'
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotPreferencePDF(
  plpResult,
  typeColumn = "evaluation",
  saveLocation = NULL,
  fileName = "plotPreferencePDF.png"
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plotPreferencePDF_+3A_plpresult">plpResult</code></td>
<td>
<p>A plp result object as generated using the <code><a href="#topic+runPlp">runPlp</a></code> function.</p>
</td></tr>
<tr><td><code id="plotPreferencePDF_+3A_typecolumn">typeColumn</code></td>
<td>
<p>The name of the column specifying the evaluation type</p>
</td></tr>
<tr><td><code id="plotPreferencePDF_+3A_savelocation">saveLocation</code></td>
<td>
<p>Directory to save plot (if NULL plot is not saved)</p>
</td></tr>
<tr><td><code id="plotPreferencePDF_+3A_filename">fileName</code></td>
<td>
<p>Name of the file to save to plot, for example
'plot.png'. See the function <code>ggsave</code> in the ggplot2 package for
supported file formats.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Create a plot showing the preference score probability density function, showing prediction overlap between true and false cases
#'
</p>


<h3>Value</h3>

<p>A ggplot object. Use the <code><a href="ggplot2.html#topic+ggsave">ggsave</a></code> function to save to file in a different
format.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
 
data("simulationProfile")
plpData &lt;- simulatePlpData(simulationProfile, n=1000)
saveLoc &lt;- file.path(tempdir(), "plotPreferencePDF")
results &lt;- runPlp(plpData, outcomeId = 3, saveDirectory = saveLoc)
plotPreferencePDF(results)
# clean up
unlink(saveLoc, recursive = TRUE)


</code></pre>

<hr>
<h2 id='plotSmoothCalibration'>Plot the smooth calibration as detailed in Calster et al. &quot;A calibration heirarchy for risk models
was defined: from utopia to empirical data&quot; (2016)</h2><span id='topic+plotSmoothCalibration'></span>

<h3>Description</h3>

<p>Plot the smooth calibration as detailed in Calster et al. &quot;A calibration heirarchy for risk models
was defined: from utopia to empirical data&quot; (2016)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotSmoothCalibration(
  plpResult,
  smooth = "loess",
  span = 0.75,
  nKnots = 5,
  scatter = FALSE,
  bins = 20,
  sample = TRUE,
  typeColumn = "evaluation",
  saveLocation = NULL,
  fileName = "smoothCalibration.pdf"
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plotSmoothCalibration_+3A_plpresult">plpResult</code></td>
<td>
<p>The result of running <code><a href="#topic+runPlp">runPlp</a></code> function. An object containing the
model or location where the model is save, the data selection settings, the
preprocessing and training settings as well as various performance measures
obtained by the model.</p>
</td></tr>
<tr><td><code id="plotSmoothCalibration_+3A_smooth">smooth</code></td>
<td>
<p>options: 'loess' or 'rcs'</p>
</td></tr>
<tr><td><code id="plotSmoothCalibration_+3A_span">span</code></td>
<td>
<p>This specifies the width of span used for loess. This will allow for faster
computing and lower memory usage.</p>
</td></tr>
<tr><td><code id="plotSmoothCalibration_+3A_nknots">nKnots</code></td>
<td>
<p>The number of knots to be used by the rcs evaluation. Default is 5</p>
</td></tr>
<tr><td><code id="plotSmoothCalibration_+3A_scatter">scatter</code></td>
<td>
<p>plot the decile calibrations as points on the graph. Default is False</p>
</td></tr>
<tr><td><code id="plotSmoothCalibration_+3A_bins">bins</code></td>
<td>
<p>The number of bins for the histogram. Default is 20.</p>
</td></tr>
<tr><td><code id="plotSmoothCalibration_+3A_sample">sample</code></td>
<td>
<p>If using loess then by default 20,000 patients will be sampled to save time</p>
</td></tr>
<tr><td><code id="plotSmoothCalibration_+3A_typecolumn">typeColumn</code></td>
<td>
<p>The name of the column specifying the evaluation type</p>
</td></tr>
<tr><td><code id="plotSmoothCalibration_+3A_savelocation">saveLocation</code></td>
<td>
<p>Directory to save plot (if NULL plot is not saved)</p>
</td></tr>
<tr><td><code id="plotSmoothCalibration_+3A_filename">fileName</code></td>
<td>
<p>Name of the file to save to plot, for example
'plot.png'. See the function <code>ggsave</code> in the ggplot2 package for
supported file formats.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Create a plot showing the smoothed calibration
</p>


<h3>Value</h3>

<p>A ggplot object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# generate prediction dataaframe with 1000 patients
predictedRisk &lt;- stats::runif(1000)
# overconfident for high risk patients
actualRisk &lt;- ifelse(predictedRisk &lt; 0.5, predictedRisk, 0.5 + 0.5 * (predictedRisk - 0.5))
outcomeCount &lt;- stats::rbinom(1000, 1, actualRisk)
# mock data frame
prediction &lt;- data.frame(rowId = 1:1000,
                         value = predictedRisk, 
                         outcomeCount = outcomeCount,
                         evaluationType = "Test")
attr(prediction, "modelType") &lt;- "binary"
calibrationSummary &lt;- getCalibrationSummary(prediction, "binary",
                                            numberOfStrata = 10,
                                            typeColumn = "evaluationType")
plpResults &lt;- list()
plpResults$performanceEvaluation$calibrationSummary &lt;- calibrationSummary
plpResults$prediction &lt;- prediction
plotSmoothCalibration(plpResults)

</code></pre>

<hr>
<h2 id='plotSparseCalibration'>Plot the calibration</h2><span id='topic+plotSparseCalibration'></span>

<h3>Description</h3>

<p>Plot the calibration
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotSparseCalibration(
  plpResult,
  typeColumn = "evaluation",
  saveLocation = NULL,
  fileName = "roc.png"
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plotSparseCalibration_+3A_plpresult">plpResult</code></td>
<td>
<p>A plp result object as generated using the <code><a href="#topic+runPlp">runPlp</a></code> function.</p>
</td></tr>
<tr><td><code id="plotSparseCalibration_+3A_typecolumn">typeColumn</code></td>
<td>
<p>The name of the column specifying the evaluation type</p>
</td></tr>
<tr><td><code id="plotSparseCalibration_+3A_savelocation">saveLocation</code></td>
<td>
<p>Directory to save plot (if NULL plot is not saved)</p>
</td></tr>
<tr><td><code id="plotSparseCalibration_+3A_filename">fileName</code></td>
<td>
<p>Name of the file to save to plot, for example
'plot.png'. See the function <code>ggsave</code> in the ggplot2 package for
supported file formats.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Create a plot showing the calibration
#'
</p>


<h3>Value</h3>

<p>A ggplot object. Use the <code><a href="ggplot2.html#topic+ggsave">ggsave</a></code> function to save to file in a different
format.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
 
data("simulationProfile")
plpData &lt;- simulatePlpData(simulationProfile, n=1000)
saveLoc &lt;- file.path(tempdir(), "plotSparseCalibration")
results &lt;- runPlp(plpData, outcomeId = 3, saveDirectory = saveLoc)
plotSparseCalibration(results)
# clean up 
unlink(saveLoc, recursive = TRUE)


</code></pre>

<hr>
<h2 id='plotSparseCalibration2'>Plot the conventional calibration</h2><span id='topic+plotSparseCalibration2'></span>

<h3>Description</h3>

<p>Plot the conventional calibration
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotSparseCalibration2(
  plpResult,
  typeColumn = "evaluation",
  saveLocation = NULL,
  fileName = "roc.png"
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plotSparseCalibration2_+3A_plpresult">plpResult</code></td>
<td>
<p>A plp result object as generated using the <code><a href="#topic+runPlp">runPlp</a></code> function.</p>
</td></tr>
<tr><td><code id="plotSparseCalibration2_+3A_typecolumn">typeColumn</code></td>
<td>
<p>The name of the column specifying the evaluation type</p>
</td></tr>
<tr><td><code id="plotSparseCalibration2_+3A_savelocation">saveLocation</code></td>
<td>
<p>Directory to save plot (if NULL plot is not saved)</p>
</td></tr>
<tr><td><code id="plotSparseCalibration2_+3A_filename">fileName</code></td>
<td>
<p>Name of the file to save to plot, for example
'plot.png'. See the function <code>ggsave</code> in the ggplot2 package for
supported file formats.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Create a plot showing the calibration
#'
</p>


<h3>Value</h3>

<p>A ggplot object. Use the <code><a href="ggplot2.html#topic+ggsave">ggsave</a></code> function to save to file in a different
format.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
 
data("simulationProfile")
plpData &lt;- simulatePlpData(simulationProfile, n=1000)
saveLoc &lt;- file.path(tempdir(), "plotSparseCalibration2")
results &lt;- runPlp(plpData, outcomeId = 3, saveDirectory = saveLoc)
plotSparseCalibration2(results)
# clean up
unlink(saveLoc, recursive = TRUE)


</code></pre>

<hr>
<h2 id='plotSparseRoc'>Plot the ROC curve using the sparse thresholdSummary data frame</h2><span id='topic+plotSparseRoc'></span>

<h3>Description</h3>

<p>Plot the ROC curve using the sparse thresholdSummary data frame
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotSparseRoc(
  plpResult,
  typeColumn = "evaluation",
  saveLocation = NULL,
  fileName = "roc.png"
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plotSparseRoc_+3A_plpresult">plpResult</code></td>
<td>
<p>A plp result object as generated using the <code><a href="#topic+runPlp">runPlp</a></code> function.</p>
</td></tr>
<tr><td><code id="plotSparseRoc_+3A_typecolumn">typeColumn</code></td>
<td>
<p>The name of the column specifying the evaluation type</p>
</td></tr>
<tr><td><code id="plotSparseRoc_+3A_savelocation">saveLocation</code></td>
<td>
<p>Directory to save plot (if NULL plot is not saved)</p>
</td></tr>
<tr><td><code id="plotSparseRoc_+3A_filename">fileName</code></td>
<td>
<p>Name of the file to save to plot, for example
'plot.png'. See the function <code>ggsave</code> in the ggplot2 package for
supported file formats.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Create a plot showing the Receiver Operator Characteristics (ROC) curve.
</p>


<h3>Value</h3>

<p>A ggplot object. Use the <code><a href="ggplot2.html#topic+ggsave">ggsave</a></code> function to save to file in a different
format.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
 
data("simulationProfile")
plpData &lt;- simulatePlpData(simulationProfile, n=1000)
saveLoc &lt;- file.path(tempdir(), "plotSparseRoc")
results &lt;- runPlp(plpData, outcomeId = 3, saveDirectory = saveLoc)
plotSparseRoc(results)
# clean up
unlink(saveLoc, recursive = TRUE)


</code></pre>

<hr>
<h2 id='plotVariableScatterplot'>Plot the variable importance scatterplot</h2><span id='topic+plotVariableScatterplot'></span>

<h3>Description</h3>

<p>Plot the variable importance scatterplot
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotVariableScatterplot(
  covariateSummary,
  saveLocation = NULL,
  fileName = "VariableScatterplot.png"
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plotVariableScatterplot_+3A_covariatesummary">covariateSummary</code></td>
<td>
<p>A prediction object as generated using the
<code><a href="#topic+runPlp">runPlp</a></code> function.</p>
</td></tr>
<tr><td><code id="plotVariableScatterplot_+3A_savelocation">saveLocation</code></td>
<td>
<p>Directory to save plot (if NULL plot is not saved)</p>
</td></tr>
<tr><td><code id="plotVariableScatterplot_+3A_filename">fileName</code></td>
<td>
<p>Name of the file to save to plot, for example
'plot.png'. See the function <code>ggsave</code> in the ggplot2 package for
supported file formats.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Create a plot showing the variable importance scatterplot
#'
</p>


<h3>Value</h3>

<p>A ggplot object. Use the <code><a href="ggplot2.html#topic+ggsave">ggsave</a></code> function to save to file in a different
format.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
 
data("simulationProfile")
plpData &lt;- simulatePlpData(simulationProfile, n=1000)
saveLoc &lt;- file.path(tempdir(), "plotVariableScatterplot")
results &lt;- runPlp(plpData, outcomeId = 3, saveDirectory = saveLoc)
plotVariableScatterplot(results$covariateSummary)
# clean up


</code></pre>

<hr>
<h2 id='pmmFit'>Predictive mean matching using lasso</h2><span id='topic+pmmFit'></span>

<h3>Description</h3>

<p>Predictive mean matching using lasso
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pmmFit(data, k = 5)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="pmmFit_+3A_data">data</code></td>
<td>
<p>An andromeda object with the following fields:
xObs: covariates table for observed data
xMiss: covariates table for missing data
yObs: outcome variable that we want to impute</p>
</td></tr>
<tr><td><code id="pmmFit_+3A_k">k</code></td>
<td>
<p>The number of donors to use for matching (default 5)</p>
</td></tr>
</table>

<hr>
<h2 id='predictCyclops'>Create predictive probabilities</h2><span id='topic+predictCyclops'></span>

<h3>Description</h3>

<p>Create predictive probabilities
</p>


<h3>Usage</h3>

<pre><code class='language-R'>predictCyclops(plpModel, data, cohort)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predictCyclops_+3A_plpmodel">plpModel</code></td>
<td>
<p>An object of type <code>predictiveModel</code> as generated using
<code><a href="#topic+fitPlp">fitPlp</a></code>.</p>
</td></tr>
<tr><td><code id="predictCyclops_+3A_data">data</code></td>
<td>
<p>The new plpData containing the covariateData for the new population</p>
</td></tr>
<tr><td><code id="predictCyclops_+3A_cohort">cohort</code></td>
<td>
<p>The cohort to calculate the prediction for</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Generates predictions for the population specified in plpData given the model.
</p>


<h3>Value</h3>

<p>The value column in the result data.frame is: logistic: probabilities of the outcome, poisson:
Poisson rate (per day) of the outome, survival: hazard rate (per day) of the outcome.
</p>


<h3>Examples</h3>

<pre><code class='language-R'> 
data("simulationProfile")
plpData &lt;- simulatePlpData(simulationProfile, n = 1000)
population &lt;- createStudyPopulation(plpData, outcomeId = 3)
data &lt;- splitData(plpData, population)
plpModel &lt;- fitPlp(data$Train, modelSettings = setLassoLogisticRegression(),
                   analysisId = "test", analysisPath = NULL)
prediction &lt;- predictCyclops(plpModel, data$Test, data$Test$labels)
# view prediction dataframe
head(prediction)

</code></pre>

<hr>
<h2 id='predictGlm'>predict using a logistic regression model</h2><span id='topic+predictGlm'></span>

<h3>Description</h3>

<p>Predict risk with a given plpModel containing a generalized linear model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>predictGlm(plpModel, data, cohort)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predictGlm_+3A_plpmodel">plpModel</code></td>
<td>
<p>An object of type <code>plpModel</code> - a patient level
prediction model</p>
</td></tr>
<tr><td><code id="predictGlm_+3A_data">data</code></td>
<td>
<p>An object of type <code>plpData</code> - the patient level prediction
data extracted from the CDM.</p>
</td></tr>
<tr><td><code id="predictGlm_+3A_cohort">cohort</code></td>
<td>
<p>The population dataframe created using
<code>createStudyPopulation</code> who will have their risks predicted or a cohort
without the outcome known</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A dataframe containing the prediction for each person in the
population
</p>


<h3>Examples</h3>

<pre><code class='language-R'>coefficients &lt;- data.frame(
  covariateId = c(1002),
  coefficient = c(0.05))
model &lt;- createGlmModel(coefficients, intercept = -2.5)
data("simulationProfile")
plpData &lt;- simulatePlpData(simulationProfile, n=50)
prediction &lt;- predictGlm(model, plpData, plpData$cohorts)
# see the predicted risk values
head(prediction)
</code></pre>

<hr>
<h2 id='predictPlp'>predictPlp</h2><span id='topic+predictPlp'></span>

<h3>Description</h3>

<p>Predict the risk of the outcome using the input plpModel for the input plpData
</p>


<h3>Usage</h3>

<pre><code class='language-R'>predictPlp(plpModel, plpData, population, timepoint)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predictPlp_+3A_plpmodel">plpModel</code></td>
<td>
<p>An object of type <code>plpModel</code> - a patient level prediction model</p>
</td></tr>
<tr><td><code id="predictPlp_+3A_plpdata">plpData</code></td>
<td>
<p>An object of type <code>plpData</code> - the patient level prediction
data extracted from the CDM.</p>
</td></tr>
<tr><td><code id="predictPlp_+3A_population">population</code></td>
<td>
<p>The population created using createStudyPopulation() who will have their risks predicted or a cohort without the outcome known</p>
</td></tr>
<tr><td><code id="predictPlp_+3A_timepoint">timepoint</code></td>
<td>
<p>The timepoint to predict risk (survival models only)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function applied the trained model on the plpData to make predictions
</p>


<h3>Value</h3>

<p>A data frame containing the predicted risk values
</p>


<h3>Examples</h3>

<pre><code class='language-R'>coefficients &lt;- data.frame(
  covariateId = c(1002),
  coefficient = c(0.05)
)
model &lt;- createGlmModel(coefficients, intercept = -2.5)
data("simulationProfile")
plpData &lt;- simulatePlpData(simulationProfile, n = 50)
prediction &lt;- predictPlp(model, plpData, plpData$cohorts)
# see the predicted risk values
head(prediction)
</code></pre>

<hr>
<h2 id='preprocessData'>A function that wraps around FeatureExtraction::tidyCovariateData to normalise
the data and remove rare or redundant features</h2><span id='topic+preprocessData'></span>

<h3>Description</h3>

<p>A function that wraps around FeatureExtraction::tidyCovariateData to normalise
the data and remove rare or redundant features
</p>


<h3>Usage</h3>

<pre><code class='language-R'>preprocessData(covariateData, preprocessSettings = createPreprocessSettings())
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="preprocessData_+3A_covariatedata">covariateData</code></td>
<td>
<p>The covariate part of the training data created by <code>splitData</code> after being sampled and having
any required feature engineering</p>
</td></tr>
<tr><td><code id="preprocessData_+3A_preprocesssettings">preprocessSettings</code></td>
<td>
<p>The settings for the preprocessing created by <code>createPreprocessSettings</code>
The data processed</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Returns an object of class <code>covariateData</code> that has been processed.
This includes normalising the data and removing rare or redundant features.
Redundant features are features that within an analysisId together cover
all obervations.
</p>


<h3>Value</h3>

<p>The covariateData object with the processed covariates
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(dplyr)
data("simulationProfile")
plpData &lt;- simulatePlpData(simulationProfile, n=1000)
preProcessedData &lt;- preprocessData(plpData$covariateData, createPreprocessSettings())
# check age is normalized by max value
preProcessedData$covariates %&gt;% dplyr::filter(.data$covariateId == 1002)
</code></pre>

<hr>
<h2 id='print.plpData'>Print a plpData object</h2><span id='topic+print.plpData'></span>

<h3>Description</h3>

<p>Print a plpData object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'plpData'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print.plpData_+3A_x">x</code></td>
<td>
<p>The plpData object to print</p>
</td></tr>
<tr><td><code id="print.plpData_+3A_...">...</code></td>
<td>
<p>Additional arguments</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A message describing the object
</p>


<h3>Examples</h3>

<pre><code class='language-R'> 
data("simulationProfile")
plpData &lt;- simulatePlpData(simulationProfile, n=10)
print(plpData)
</code></pre>

<hr>
<h2 id='print.summary.plpData'>Print a summary.plpData object</h2><span id='topic+print.summary.plpData'></span>

<h3>Description</h3>

<p>Print a summary.plpData object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'summary.plpData'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print.summary.plpData_+3A_x">x</code></td>
<td>
<p>The summary.plpData object to print</p>
</td></tr>
<tr><td><code id="print.summary.plpData_+3A_...">...</code></td>
<td>
<p>Additional arguments</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A message describing the object
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("simulationProfile")
plpData &lt;- simulatePlpData(simulationProfile, n=10)
summary &lt;- summary(plpData)
print(summary)
</code></pre>

<hr>
<h2 id='recalibratePlp'>recalibratePlp</h2><span id='topic+recalibratePlp'></span>

<h3>Description</h3>

<p>Recalibrating a model using the recalibrationInTheLarge or weakRecalibration methods
</p>


<h3>Usage</h3>

<pre><code class='language-R'>recalibratePlp(
  prediction,
  analysisId,
  typeColumn = "evaluationType",
  method = c("recalibrationInTheLarge", "weakRecalibration")
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="recalibratePlp_+3A_prediction">prediction</code></td>
<td>
<p>A prediction dataframe</p>
</td></tr>
<tr><td><code id="recalibratePlp_+3A_analysisid">analysisId</code></td>
<td>
<p>The model analysisId</p>
</td></tr>
<tr><td><code id="recalibratePlp_+3A_typecolumn">typeColumn</code></td>
<td>
<p>The column name where the strata types are specified</p>
</td></tr>
<tr><td><code id="recalibratePlp_+3A_method">method</code></td>
<td>
<p>Method used to recalibrate ('recalibrationInTheLarge' or 'weakRecalibration' )</p>
</td></tr>
</table>


<h3>Details</h3>

<p>'recalibrationInTheLarge' calculates a single correction factor for the
average predicted risks to match the average observed risks.
'weakRecalibration' fits a glm model to the logit of the predicted risks,
also known as Platt scaling/logistic recalibration.
</p>


<h3>Value</h3>

<p>A prediction dataframe with the recalibrated predictions added
</p>


<h3>Examples</h3>

<pre><code class='language-R'>prediction &lt;- data.frame(rowId = 1:100,
                         value = runif(100),
                         outcomeCount = stats::rbinom(100, 1, 0.1),
                         evaluationType = rep("validation", 100))
attr(prediction, "metaData") &lt;- list(modelType = "binary")
# since value is unformally distributed but outcomeCount is not (prob &lt;- 0.1)
# the predictions are mis-calibrated
outcomeRate &lt;- mean(prediction$outcomeCount)
observedRisk &lt;- mean(prediction$value)
message("outcome rate is: ", outcomeRate)
message("observed risk is: ", observedRisk)
# lets recalibrate the predictions
prediction &lt;- recalibratePlp(prediction, 
                             analysisId = "recalibration", 
                             method = "recalibrationInTheLarge")
recalibratedRisk &lt;- mean(prediction$value)
message("recalibrated risk with recalibration in the large is: ", recalibratedRisk)
prediction &lt;- recalibratePlp(prediction, 
                             analysisId = "recalibration", 
                             method = "weakRecalibration")
recalibratedRisk &lt;- mean(prediction$value)
message("recalibrated risk with weak recalibration is: ", recalibratedRisk)
</code></pre>

<hr>
<h2 id='recalibratePlpRefit'>recalibratePlpRefit</h2><span id='topic+recalibratePlpRefit'></span>

<h3>Description</h3>

<p>Recalibrating a model by refitting it
</p>


<h3>Usage</h3>

<pre><code class='language-R'>recalibratePlpRefit(plpModel, newPopulation, newData, returnModel = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="recalibratePlpRefit_+3A_plpmodel">plpModel</code></td>
<td>
<p>The trained plpModel (runPlp$model)</p>
</td></tr>
<tr><td><code id="recalibratePlpRefit_+3A_newpopulation">newPopulation</code></td>
<td>
<p>The population created using createStudyPopulation() who will have their risks predicted</p>
</td></tr>
<tr><td><code id="recalibratePlpRefit_+3A_newdata">newData</code></td>
<td>
<p>An object of type <code>plpData</code> - the patient level prediction
data extracted from the CDM.</p>
</td></tr>
<tr><td><code id="recalibratePlpRefit_+3A_returnmodel">returnModel</code></td>
<td>
<p>Logical: return the refitted model</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An prediction dataframe with the predictions of the recalibrated model added
</p>


<h3>Examples</h3>

<pre><code class='language-R'> 
data("simulationProfile")
plpData &lt;- simulatePlpData(simulationProfile, n = 1000)
saveLoc &lt;- file.path(tempdir(), "recalibratePlpRefit")
plpResults &lt;- runPlp(plpData, outcomeId = 3, saveDirectory = saveLoc)
newData &lt;- simulatePlpData(simulationProfile, n = 1000)
newPopulation &lt;- createStudyPopulation(newData, outcomeId = 3)
predictions &lt;- recalibratePlpRefit(plpModel = plpResults$model, 
                                   newPopulation = newPopulation, 
                                   newData = newData)
# clean up
unlink(saveLoc, recursive = TRUE)

</code></pre>

<hr>
<h2 id='removeRareFeatures'>A function that removes rare features from the data</h2><span id='topic+removeRareFeatures'></span>

<h3>Description</h3>

<p>A function that removes rare features from the data
</p>


<h3>Usage</h3>

<pre><code class='language-R'>removeRareFeatures(trainData, featureEngineeringSettings, done = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="removeRareFeatures_+3A_traindata">trainData</code></td>
<td>
<p>The data to be normalized</p>
</td></tr>
<tr><td><code id="removeRareFeatures_+3A_featureengineeringsettings">featureEngineeringSettings</code></td>
<td>
<p>The settings for the normalization</p>
</td></tr>
<tr><td><code id="removeRareFeatures_+3A_done">done</code></td>
<td>
<p>Whether to find and remove rare features or remove them only (bool)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>removes features that are present in less than a certain fraction of the population
</p>


<h3>Value</h3>

<p>The data with rare features removed
</p>

<hr>
<h2 id='robustNormalize'>A function that normalizes continous by the interquartile range and
optionally forces the resulting values to be between -3 and 3 with
f(x) = x / sqrt(1 + (x/3)^2)
'@details uses (value - median) / iqr to normalize the data and then can
applies the function f(x) = x / sqrt(1 + (x/3)^2) to the normalized values.
This forces the values to be between -3 and 3 while preserving the relative
ordering of the values.
based on https://arxiv.org/abs/2407.04491 for more details</h2><span id='topic+robustNormalize'></span>

<h3>Description</h3>

<p>A function that normalizes continous by the interquartile range and
optionally forces the resulting values to be between -3 and 3 with
f(x) = x / sqrt(1 + (x/3)^2)
'@details uses (value - median) / iqr to normalize the data and then can
applies the function f(x) = x / sqrt(1 + (x/3)^2) to the normalized values.
This forces the values to be between -3 and 3 while preserving the relative
ordering of the values.
based on https://arxiv.org/abs/2407.04491 for more details
</p>


<h3>Usage</h3>

<pre><code class='language-R'>robustNormalize(trainData, featureEngineeringSettings, done = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="robustNormalize_+3A_traindata">trainData</code></td>
<td>
<p>The training data to be normalized</p>
</td></tr>
<tr><td><code id="robustNormalize_+3A_featureengineeringsettings">featureEngineeringSettings</code></td>
<td>
<p>The settings for the normalization</p>
</td></tr>
<tr><td><code id="robustNormalize_+3A_done">done</code></td>
<td>
<p>Whether the data has already been normalized (bool)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The <code>trainData</code> object with normalized data
</p>

<hr>
<h2 id='runMultiplePlp'>Run a list of predictions analyses</h2><span id='topic+runMultiplePlp'></span>

<h3>Description</h3>

<p>Run a list of predictions analyses
</p>


<h3>Usage</h3>

<pre><code class='language-R'>runMultiplePlp(
  databaseDetails = createDatabaseDetails(),
  modelDesignList = list(createModelDesign(targetId = 1, outcomeId = 2, modelSettings =
    setLassoLogisticRegression()), createModelDesign(targetId = 1, outcomeId = 3,
    modelSettings = setLassoLogisticRegression())),
  onlyFetchData = FALSE,
  cohortDefinitions = NULL,
  logSettings = createLogSettings(verbosity = "DEBUG", timeStamp = TRUE, logName =
    "runPlp Log"),
  saveDirectory = NULL,
  sqliteLocation = file.path(saveDirectory, "sqlite")
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="runMultiplePlp_+3A_databasedetails">databaseDetails</code></td>
<td>
<p>The database settings created using <code>createDatabaseDetails()</code></p>
</td></tr>
<tr><td><code id="runMultiplePlp_+3A_modeldesignlist">modelDesignList</code></td>
<td>
<p>A list of model designs created using <code>createModelDesign()</code></p>
</td></tr>
<tr><td><code id="runMultiplePlp_+3A_onlyfetchdata">onlyFetchData</code></td>
<td>
<p>Only fetches and saves the data object to the output folder without running the analysis.</p>
</td></tr>
<tr><td><code id="runMultiplePlp_+3A_cohortdefinitions">cohortDefinitions</code></td>
<td>
<p>A list of cohort definitions for the target and outcome cohorts</p>
</td></tr>
<tr><td><code id="runMultiplePlp_+3A_logsettings">logSettings</code></td>
<td>
<p>The setting specifying the logging for the analyses created using <code>createLogSettings()</code></p>
</td></tr>
<tr><td><code id="runMultiplePlp_+3A_savedirectory">saveDirectory</code></td>
<td>
<p>Name of the folder where all the outputs will written to.</p>
</td></tr>
<tr><td><code id="runMultiplePlp_+3A_sqlitelocation">sqliteLocation</code></td>
<td>
<p>(optional) The location of the sqlite database with the results</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function will run all specified predictions as defined using .
</p>


<h3>Value</h3>

<p>A data frame with the following columns: </p>

<table>
<tr>
 <td style="text-align: left;"> <code style="white-space: pre;">&#8288;analysisId&#8288;</code> </td><td style="text-align: left;"> The unique identifier
for a set of analysis choices.</td>
</tr>
<tr>
 <td style="text-align: left;"> <code style="white-space: pre;">&#8288;targetId&#8288;</code> </td><td style="text-align: left;"> The ID of the target cohort populations.</td>
</tr>
<tr>
 <td style="text-align: left;">
<code style="white-space: pre;">&#8288;outcomeId&#8288;</code> </td><td style="text-align: left;"> The ID of the outcomeId.</td>
</tr>
<tr>
 <td style="text-align: left;"> <code style="white-space: pre;">&#8288;dataLocation&#8288;</code> </td><td style="text-align: left;"> The location where the plpData was saved
</td>
</tr>
<tr>
 <td style="text-align: left;"> <code style="white-space: pre;">&#8288;the settings ids&#8288;</code> </td><td style="text-align: left;"> The ids for all other settings used for model development.</td>
</tr>
<tr>
 <td style="text-align: left;"> </td>
</tr>

</table>



<h3>Examples</h3>

<pre><code class='language-R'>
 
connectionDetails &lt;- Eunomia::getEunomiaConnectionDetails()
databaseDetails &lt;- createDatabaseDetails(connectionDetails = connectionDetails,
                                         cdmDatabaseSchema = "main",
                                         cohortDatabaseSchema = "main",
                                         cohortTable = "cohort",
                                         outcomeDatabaseSchema = "main",
                                         outcomeTable = "cohort",
                                         targetId = 1,
                                         outcomeIds = 2)
Eunomia::createCohorts(connectionDetails = connectionDetails)
covariateSettings &lt;- 
 FeatureExtraction::createCovariateSettings(useDemographicsGender = TRUE,
                                            useDemographicsAge = TRUE,
                                            useConditionOccurrenceLongTerm = TRUE)
# GI Bleed in users of celecoxib
modelDesign &lt;- createModelDesign(targetId = 1, 
                                 outcomeId = 3, 
                                 modelSettings = setLassoLogisticRegression(seed = 42),
                                 populationSettings = createStudyPopulationSettings(),
                                 restrictPlpDataSettings = createRestrictPlpDataSettings(),
                                 covariateSettings = covariateSettings,
                                 splitSettings = createDefaultSplitSetting(splitSeed = 42),
                                 preprocessSettings = createPreprocessSettings())
# GI Bleed in users of NSAIDs
modelDesign2 &lt;- createModelDesign(targetId = 4,
                                  outcomeId = 3,
                                  modelSettings = setLassoLogisticRegression(seed = 42),
                                  populationSettings = createStudyPopulationSettings(),
                                  restrictPlpDataSettings = createRestrictPlpDataSettings(),
                                  covariateSettings = covariateSettings,
                                  splitSettings = createDefaultSplitSetting(splitSeed = 42),
                                  preprocessSettings = createPreprocessSettings())
saveLoc &lt;- file.path(tempdir(), "runMultiplePlp")
multipleResults &lt;- runMultiplePlp(databaseDetails = databaseDetails,
                                  modelDesignList = list(modelDesign, modelDesign2),
                                  saveDirectory = saveLoc)
# You should see results for two developed models in the ouutput. The output is as well
# uploaded to a sqlite database in the saveLoc/sqlite folder, 
dir(saveLoc)
# The dir output should show two Analysis_ folders with the results, 
# two targetId_ folders with th extracted data, and a sqlite folder with the database
# The results can be explored in the shiny app by calling viewMultiplePlp(saveLoc)

# clean up (viewing the results in the shiny app is won't work after this)
unlink(saveLoc, recursive = TRUE)


</code></pre>

<hr>
<h2 id='runPlp'>runPlp - Develop and internally evaluate a model using specified settings</h2><span id='topic+runPlp'></span>

<h3>Description</h3>

<p>This provides a general framework for training patient level prediction models.  The user can select
various default feature selection methods or incorporate their own,  The user can also select from
a range of default classifiers or incorporate their own.  There are three types of evaluations for the model
patient (randomly splits people into train/validation sets) or year (randomly splits data into train/validation sets
based on index year - older in training, newer in validation) or both (same as year spliting but checks there are
no overlaps in patients within training set and validaiton set - any overlaps are removed from validation set)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>runPlp(
  plpData,
  outcomeId = plpData$metaData$databaseDetails$outcomeIds[1],
  analysisId = paste(Sys.Date(), outcomeId, sep = "-"),
  analysisName = "Study details",
  populationSettings = createStudyPopulationSettings(),
  splitSettings = createDefaultSplitSetting(type = "stratified", testFraction = 0.25,
    trainFraction = 0.75, splitSeed = 123, nfold = 3),
  sampleSettings = createSampleSettings(type = "none"),
  featureEngineeringSettings = createFeatureEngineeringSettings(type = "none"),
  preprocessSettings = createPreprocessSettings(minFraction = 0.001, normalize = TRUE),
  modelSettings = setLassoLogisticRegression(),
  logSettings = createLogSettings(verbosity = "DEBUG", timeStamp = TRUE, logName =
    "runPlp Log"),
  executeSettings = createDefaultExecuteSettings(),
  saveDirectory = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="runPlp_+3A_plpdata">plpData</code></td>
<td>
<p>An object of type <code>plpData</code> - the patient level prediction
data extracted from the CDM.  Can also include an initial population as
plpData$popualtion.</p>
</td></tr>
<tr><td><code id="runPlp_+3A_outcomeid">outcomeId</code></td>
<td>
<p>(integer) The ID of the outcome.</p>
</td></tr>
<tr><td><code id="runPlp_+3A_analysisid">analysisId</code></td>
<td>
<p>(integer) Identifier for the analysis. It is used to create, e.g., the result folder. Default is a timestamp.</p>
</td></tr>
<tr><td><code id="runPlp_+3A_analysisname">analysisName</code></td>
<td>
<p>(character) Name for the analysis</p>
</td></tr>
<tr><td><code id="runPlp_+3A_populationsettings">populationSettings</code></td>
<td>
<p>An object of type <code>populationSettings</code> created using <code>createStudyPopulationSettings</code> that
specifies how the data class labels are defined and addition any exclusions to apply to the
plpData cohort</p>
</td></tr>
<tr><td><code id="runPlp_+3A_splitsettings">splitSettings</code></td>
<td>
<p>An object of type <code>splitSettings</code> that specifies how to split the data into train/validation/test.
The default settings can be created using <code>createDefaultSplitSetting</code>.</p>
</td></tr>
<tr><td><code id="runPlp_+3A_samplesettings">sampleSettings</code></td>
<td>
<p>An object of type <code>sampleSettings</code> that specifies any under/over sampling to be done.
The default is none.</p>
</td></tr>
<tr><td><code id="runPlp_+3A_featureengineeringsettings">featureEngineeringSettings</code></td>
<td>
<p>An object of <code>featureEngineeringSettings</code> specifying any feature engineering to be learned (using the train data)</p>
</td></tr>
<tr><td><code id="runPlp_+3A_preprocesssettings">preprocessSettings</code></td>
<td>
<p>An object of <code>preprocessSettings</code>. This setting specifies the minimum fraction of
target population who must have a covariate for it to be included in the model training
and whether to normalise the covariates before training</p>
</td></tr>
<tr><td><code id="runPlp_+3A_modelsettings">modelSettings</code></td>
<td>
<p>An object of class <code>modelSettings</code> created using one of the function:
</p>

<ul>
<li><p> setLassoLogisticRegression() A lasso logistic regression model
</p>
</li>
<li><p> setGradientBoostingMachine() A gradient boosting machine
</p>
</li>
<li><p> setAdaBoost() An ada boost model
</p>
</li>
<li><p> setRandomForest() A random forest model
</p>
</li>
<li><p> setDecisionTree() A decision tree model
</p>
</li>
<li><p> setKNN() A KNN model
</p>
</li></ul>
</td></tr>
<tr><td><code id="runPlp_+3A_logsettings">logSettings</code></td>
<td>
<p>An object of <code>logSettings</code> created using <code>createLogSettings</code>
specifying how the logging is done</p>
</td></tr>
<tr><td><code id="runPlp_+3A_executesettings">executeSettings</code></td>
<td>
<p>An object of <code>executeSettings</code> specifying which parts of the analysis to run</p>
</td></tr>
<tr><td><code id="runPlp_+3A_savedirectory">saveDirectory</code></td>
<td>
<p>The path to the directory where the results will be saved (if NULL uses working directory)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function takes as input the plpData extracted from an OMOP CDM database and follows the specified settings to
develop and internally validate a model for the specified outcomeId.
</p>


<h3>Value</h3>

<p>An plpResults object containing the following:
</p>

<ul>
<li><p> model The developed model of class <code>plpModel</code>
</p>
</li>
<li><p> executionSummary A list containing the hardward details, R package details and execution time
</p>
</li>
<li><p> performanceEvaluation Various internal performance metrics in sparse format
</p>
</li>
<li><p> prediction The plpData cohort table with the predicted risks added as a column (named value)
</p>
</li>
<li><p> covariateSummary A characterization of the features for patients with and without the outcome during the time at risk
</p>
</li>
<li><p> analysisRef A list with details about the analysis
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'> 
# simulate some data
data('simulationProfile')
plpData &lt;- simulatePlpData(simulationProfile, n = 1000)
# develop a model with the default settings
saveLoc &lt;- file.path(tempdir(), "runPlp")
results &lt;- runPlp(plpData = plpData, outcomeId = 3, analysisId = 1,
                  saveDirectory = saveLoc) 
# to check the results you can view the log file at saveLoc/1/plpLog.txt
# or view with shiny app using viewPlp(results)
# clean up
unlink(saveLoc, recursive = TRUE)

</code></pre>

<hr>
<h2 id='savePlpAnalysesJson'>Save the modelDesignList to a json file</h2><span id='topic+savePlpAnalysesJson'></span>

<h3>Description</h3>

<p>Save the modelDesignList to a json file
</p>


<h3>Usage</h3>

<pre><code class='language-R'>savePlpAnalysesJson(
  modelDesignList = list(createModelDesign(targetId = 1, outcomeId = 2, modelSettings =
    setLassoLogisticRegression()), createModelDesign(targetId = 1, outcomeId = 3,
    modelSettings = setLassoLogisticRegression())),
  cohortDefinitions = NULL,
  saveDirectory = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="savePlpAnalysesJson_+3A_modeldesignlist">modelDesignList</code></td>
<td>
<p>A list of modelDesigns created using <code>createModelDesign()</code></p>
</td></tr>
<tr><td><code id="savePlpAnalysesJson_+3A_cohortdefinitions">cohortDefinitions</code></td>
<td>
<p>A list of the cohortDefinitions (generally extracted from ATLAS)</p>
</td></tr>
<tr><td><code id="savePlpAnalysesJson_+3A_savedirectory">saveDirectory</code></td>
<td>
<p>The directory to save the modelDesignList settings</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function creates a json file with the modelDesignList saved
</p>


<h3>Value</h3>

<p>The json string of the ModelDesignList
</p>


<h3>Examples</h3>

<pre><code class='language-R'>modelDesign &lt;- createModelDesign(targetId = 1, 
                                 outcomeId = 2,
                                 modelSettings = setLassoLogisticRegression())
saveLoc &lt;- file.path(tempdir(), "loadPlpAnalysesJson")
jsonFile &lt;- savePlpAnalysesJson(modelDesignList = modelDesign, saveDirectory = saveLoc)
# clean up
unlink(saveLoc, recursive = TRUE)
</code></pre>

<hr>
<h2 id='savePlpData'>Save the plpData to folder</h2><span id='topic+savePlpData'></span>

<h3>Description</h3>

<p><code>savePlpData</code> saves an object of type plpData to folder.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>savePlpData(plpData, file, envir = NULL, overwrite = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="savePlpData_+3A_plpdata">plpData</code></td>
<td>
<p>An object of type <code>plpData</code> as generated using
<code>getPlpData</code>.</p>
</td></tr>
<tr><td><code id="savePlpData_+3A_file">file</code></td>
<td>
<p>The name of the folder where the data will be written. The folder should
not yet exist.</p>
</td></tr>
<tr><td><code id="savePlpData_+3A_envir">envir</code></td>
<td>
<p>The environment for to evaluate variables when saving</p>
</td></tr>
<tr><td><code id="savePlpData_+3A_overwrite">overwrite</code></td>
<td>
<p>Whether to force overwrite an existing file</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Called for its side effect, the data will be written to a set of files in the
folder specified by the user.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("simulationProfile")
plpData &lt;- simulatePlpData(simulationProfile, n = 500)
saveLoc &lt;- file.path(tempdir(), "savePlpData")
savePlpData(plpData, saveLoc)
dir(saveLoc, full.names = TRUE)

# clean up
unlink(saveLoc, recursive = TRUE)
</code></pre>

<hr>
<h2 id='savePlpModel'>Saves the plp model</h2><span id='topic+savePlpModel'></span>

<h3>Description</h3>

<p>Saves the plp model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>savePlpModel(plpModel, dirPath)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="savePlpModel_+3A_plpmodel">plpModel</code></td>
<td>
<p>A trained classifier returned by running <code>runPlp()$model</code></p>
</td></tr>
<tr><td><code id="savePlpModel_+3A_dirpath">dirPath</code></td>
<td>
<p>A location to save the model to</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Saves the plp model to a user specificed folder
</p>


<h3>Value</h3>

<div class="sourceCode"><pre>                    The directory path where the model was saved
</pre></div>


<h3>Examples</h3>

<pre><code class='language-R'> 
data("simulationProfile")
plpData &lt;- simulatePlpData(simulationProfile, n = 1000)
saveLoc &lt;- file.path(tempdir(), "savePlpModel")
plpResult &lt;- runPlp(plpData, outcomeId = 3, saveDirectory = saveLoc)
path &lt;- savePlpModel(plpResult$model, file.path(saveLoc, "savedModel"))
# show the saved model
dir(path, full.names = TRUE)

# clean up
unlink(saveLoc, recursive = TRUE)

</code></pre>

<hr>
<h2 id='savePlpResult'>Saves the result from runPlp into the location directory</h2><span id='topic+savePlpResult'></span>

<h3>Description</h3>

<p>Saves the result from runPlp into the location directory
</p>


<h3>Usage</h3>

<pre><code class='language-R'>savePlpResult(result, dirPath)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="savePlpResult_+3A_result">result</code></td>
<td>
<p>The result of running runPlp()</p>
</td></tr>
<tr><td><code id="savePlpResult_+3A_dirpath">dirPath</code></td>
<td>
<p>The directory to save the csv</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Saves the result from runPlp into the location directory
</p>


<h3>Value</h3>

<div class="sourceCode"><pre>                       The directory path where the results were saved
</pre></div>


<h3>Examples</h3>

<pre><code class='language-R'> 
data("simulationProfile")
plpData &lt;- simulatePlpData(simulationProfile, n = 1000)
saveLoc &lt;- file.path(tempdir(), "savePlpResult")
results &lt;- runPlp(plpData, outcomeId = 3, saveDirectory = saveLoc)
# save the results
newSaveLoc &lt;- file.path(tempdir(), "savePlpResult", "saved")
savePlpResult(results, newSaveLoc)
# show the saved results
dir(newSaveLoc, recursive = TRUE, full.names = TRUE)

# clean up
unlink(saveLoc, recursive = TRUE)
unlink(newSaveLoc, recursive = TRUE)

</code></pre>

<hr>
<h2 id='savePlpShareable'>Save the plp result as json files and csv files for transparent sharing</h2><span id='topic+savePlpShareable'></span>

<h3>Description</h3>

<p>Save the plp result as json files and csv files for transparent sharing
</p>


<h3>Usage</h3>

<pre><code class='language-R'>savePlpShareable(result, saveDirectory, minCellCount = 10)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="savePlpShareable_+3A_result">result</code></td>
<td>
<p>An object of class runPlp with development or validation results</p>
</td></tr>
<tr><td><code id="savePlpShareable_+3A_savedirectory">saveDirectory</code></td>
<td>
<p>The directory the save the results as csv files</p>
</td></tr>
<tr><td><code id="savePlpShareable_+3A_mincellcount">minCellCount</code></td>
<td>
<p>Minimum cell count for the covariateSummary and certain evaluation results</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Saves the main results json/csv files (these files can be read by the shiny app)
</p>


<h3>Value</h3>

<div class="sourceCode"><pre>                       The directory path where the results were saved
</pre></div>


<h3>Examples</h3>

<pre><code class='language-R'> 
data("simulationProfile")
plpData &lt;- simulatePlpData(simulationProfile, n = 1000)
saveLoc &lt;- file.path(tempdir(), "savePlpShareable")
results &lt;- runPlp(plpData, saveDirectory = saveLoc)
newSaveLoc &lt;- file.path(tempdir(), "savePlpShareable", "saved")
path &lt;- savePlpShareable(results, newSaveLoc)
# show the saved result
dir(newSaveLoc, full.names = TRUE, recursive = TRUE)

# clean up
unlink(saveLoc, recursive = TRUE)
unlink(newSaveLoc, recursive = TRUE)

</code></pre>

<hr>
<h2 id='savePrediction'>Saves the prediction dataframe to a json file</h2><span id='topic+savePrediction'></span>

<h3>Description</h3>

<p>Saves the prediction dataframe to a json file
</p>


<h3>Usage</h3>

<pre><code class='language-R'>savePrediction(prediction, dirPath, fileName = "prediction.json")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="savePrediction_+3A_prediction">prediction</code></td>
<td>
<p>The prediciton data.frame</p>
</td></tr>
<tr><td><code id="savePrediction_+3A_dirpath">dirPath</code></td>
<td>
<p>The directory to save the prediction json</p>
</td></tr>
<tr><td><code id="savePrediction_+3A_filename">fileName</code></td>
<td>
<p>The name of the json file that will be saved</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Saves the prediction data frame returned by predict.R to an json file and
returns the fileLocation where the prediction is saved
</p>


<h3>Value</h3>

<div class="sourceCode"><pre>                       The file location where the prediction was saved
</pre></div>


<h3>Examples</h3>

<pre><code class='language-R'>prediction &lt;- data.frame(
  rowIds = c(1, 2, 3),
  outcomeCount = c(0, 1, 0),
  value = c(0.1, 0.9, 0.2)
)
saveLoc &lt;- file.path(tempdir())
savePrediction(prediction, saveLoc)
dir(saveLoc)

# clean up
unlink(file.path(saveLoc, "prediction.json"))
</code></pre>

<hr>
<h2 id='setAdaBoost'>Create setting for AdaBoost with python DecisionTreeClassifier base estimator</h2><span id='topic+setAdaBoost'></span>

<h3>Description</h3>

<p>Create setting for AdaBoost with python DecisionTreeClassifier base estimator
</p>


<h3>Usage</h3>

<pre><code class='language-R'>setAdaBoost(
  nEstimators = list(10, 50, 200),
  learningRate = list(1, 0.5, 0.1),
  algorithm = list("SAMME"),
  seed = sample(1e+06, 1)
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="setAdaBoost_+3A_nestimators">nEstimators</code></td>
<td>
<p>(list) The maximum number of estimators at which boosting is terminated. In case of perfect fit, the learning procedure is stopped early.</p>
</td></tr>
<tr><td><code id="setAdaBoost_+3A_learningrate">learningRate</code></td>
<td>
<p>(list) Weight applied to each classifier at each boosting iteration. A higher learning rate increases the contribution of each classifier. There is a trade-off between the learningRate and nEstimators parameters
There is a trade-off between learningRate and nEstimators.</p>
</td></tr>
<tr><td><code id="setAdaBoost_+3A_algorithm">algorithm</code></td>
<td>
<p>Only ‘SAMME’ can be provided. The 'algorithm' argument will be deprecated in scikit-learn 1.8.</p>
</td></tr>
<tr><td><code id="setAdaBoost_+3A_seed">seed</code></td>
<td>
<p>A seed for the model</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a modelSettings object
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run:  
model &lt;- setAdaBoost(nEstimators = list(10),
                     learningRate = list(0.1),
                     seed = 42)

## End(Not run)
</code></pre>

<hr>
<h2 id='setCoxModel'>Create setting for lasso Cox model</h2><span id='topic+setCoxModel'></span>

<h3>Description</h3>

<p>Create setting for lasso Cox model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>setCoxModel(
  variance = 0.01,
  seed = NULL,
  includeCovariateIds = c(),
  noShrinkage = c(),
  threads = -1,
  upperLimit = 20,
  lowerLimit = 0.01,
  tolerance = 2e-07,
  maxIterations = 3000
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="setCoxModel_+3A_variance">variance</code></td>
<td>
<p>Numeric: prior distribution starting variance</p>
</td></tr>
<tr><td><code id="setCoxModel_+3A_seed">seed</code></td>
<td>
<p>An option to add a seed when training the model</p>
</td></tr>
<tr><td><code id="setCoxModel_+3A_includecovariateids">includeCovariateIds</code></td>
<td>
<p>a set of covariate IDS to limit the analysis to</p>
</td></tr>
<tr><td><code id="setCoxModel_+3A_noshrinkage">noShrinkage</code></td>
<td>
<p>a set of covariates whcih are to be forced to be included in the final model. default is the intercept</p>
</td></tr>
<tr><td><code id="setCoxModel_+3A_threads">threads</code></td>
<td>
<p>An option to set number of threads when training model</p>
</td></tr>
<tr><td><code id="setCoxModel_+3A_upperlimit">upperLimit</code></td>
<td>
<p>Numeric: Upper prior variance limit for grid-search</p>
</td></tr>
<tr><td><code id="setCoxModel_+3A_lowerlimit">lowerLimit</code></td>
<td>
<p>Numeric: Lower prior variance limit for grid-search</p>
</td></tr>
<tr><td><code id="setCoxModel_+3A_tolerance">tolerance</code></td>
<td>
<p>Numeric: maximum relative change in convergence criterion from successive iterations to achieve convergence</p>
</td></tr>
<tr><td><code id="setCoxModel_+3A_maxiterations">maxIterations</code></td>
<td>
<p>Integer: maximum iterations of Cyclops to attempt before returning a failed-to-converge error</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>modelSettings</code> object
</p>


<h3>Examples</h3>

<pre><code class='language-R'>coxL1 &lt;- setCoxModel()
</code></pre>

<hr>
<h2 id='setDecisionTree'>Create setting for the scikit-learn DecisionTree with python</h2><span id='topic+setDecisionTree'></span>

<h3>Description</h3>

<p>Create setting for the scikit-learn DecisionTree with python
</p>


<h3>Usage</h3>

<pre><code class='language-R'>setDecisionTree(
  criterion = list("gini"),
  splitter = list("best"),
  maxDepth = list(as.integer(4), as.integer(10), NULL),
  minSamplesSplit = list(2, 10),
  minSamplesLeaf = list(10, 50),
  minWeightFractionLeaf = list(0),
  maxFeatures = list(100, "sqrt", NULL),
  maxLeafNodes = list(NULL),
  minImpurityDecrease = list(10^-7),
  classWeight = list(NULL),
  seed = sample(1e+06, 1)
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="setDecisionTree_+3A_criterion">criterion</code></td>
<td>
<p>The function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “entropy” for the information gain.</p>
</td></tr>
<tr><td><code id="setDecisionTree_+3A_splitter">splitter</code></td>
<td>
<p>The strategy used to choose the split at each node. Supported strategies are “best” to choose the best split and “random” to choose the best random split.</p>
</td></tr>
<tr><td><code id="setDecisionTree_+3A_maxdepth">maxDepth</code></td>
<td>
<p>(list) The maximum depth of the tree. If NULL, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.</p>
</td></tr>
<tr><td><code id="setDecisionTree_+3A_minsamplessplit">minSamplesSplit</code></td>
<td>
<p>The minimum number of samples required to split an internal node</p>
</td></tr>
<tr><td><code id="setDecisionTree_+3A_minsamplesleaf">minSamplesLeaf</code></td>
<td>
<p>The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least minSamplesLeaf training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.</p>
</td></tr>
<tr><td><code id="setDecisionTree_+3A_minweightfractionleaf">minWeightFractionLeaf</code></td>
<td>
<p>The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sampleWeight is not provided.</p>
</td></tr>
<tr><td><code id="setDecisionTree_+3A_maxfeatures">maxFeatures</code></td>
<td>
<p>(list) The number of features to consider when looking for the best split (int/'sqrt'/NULL)</p>
</td></tr>
<tr><td><code id="setDecisionTree_+3A_maxleafnodes">maxLeafNodes</code></td>
<td>
<p>(list) Grow a tree with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes. (int/NULL)</p>
</td></tr>
<tr><td><code id="setDecisionTree_+3A_minimpuritydecrease">minImpurityDecrease</code></td>
<td>
<p>Threshold for early stopping in tree growth. A node will split if its impurity is above the threshold, otherwise it is a leaf.</p>
</td></tr>
<tr><td><code id="setDecisionTree_+3A_classweight">classWeight</code></td>
<td>
<p>(list) Weights associated with classes 'balance' or NULL</p>
</td></tr>
<tr><td><code id="setDecisionTree_+3A_seed">seed</code></td>
<td>
<p>The random state seed</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a modelSettings object
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run:  
model &lt;- setDecisionTree(criterion = list("gini"),
                         maxDepth = list(4),
                         minSamplesSplit = list(2),
                         minSamplesLeaf = list(10),
                         seed = 42)

## End(Not run)
</code></pre>

<hr>
<h2 id='setGradientBoostingMachine'>Create setting for gradient boosting machine model using gbm_xgboost implementation</h2><span id='topic+setGradientBoostingMachine'></span>

<h3>Description</h3>

<p>Create setting for gradient boosting machine model using gbm_xgboost implementation
</p>


<h3>Usage</h3>

<pre><code class='language-R'>setGradientBoostingMachine(
  ntrees = c(100, 300),
  nthread = 20,
  earlyStopRound = 25,
  maxDepth = c(4, 6, 8),
  minChildWeight = 1,
  learnRate = c(0.05, 0.1, 0.3),
  scalePosWeight = 1,
  lambda = 1,
  alpha = 0,
  seed = sample(1e+07, 1)
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="setGradientBoostingMachine_+3A_ntrees">ntrees</code></td>
<td>
<p>The number of trees to build</p>
</td></tr>
<tr><td><code id="setGradientBoostingMachine_+3A_nthread">nthread</code></td>
<td>
<p>The number of computer threads to use (how many cores do you have?)</p>
</td></tr>
<tr><td><code id="setGradientBoostingMachine_+3A_earlystopround">earlyStopRound</code></td>
<td>
<p>If the performance does not increase over earlyStopRound number of trees then training stops (this prevents overfitting)</p>
</td></tr>
<tr><td><code id="setGradientBoostingMachine_+3A_maxdepth">maxDepth</code></td>
<td>
<p>Maximum depth of each tree - a large value will lead to slow model training</p>
</td></tr>
<tr><td><code id="setGradientBoostingMachine_+3A_minchildweight">minChildWeight</code></td>
<td>
<p>Minimum sum of of instance weight in a child node - larger values are more conservative</p>
</td></tr>
<tr><td><code id="setGradientBoostingMachine_+3A_learnrate">learnRate</code></td>
<td>
<p>The boosting learn rate</p>
</td></tr>
<tr><td><code id="setGradientBoostingMachine_+3A_scaleposweight">scalePosWeight</code></td>
<td>
<p>Controls weight of positive class in loss - useful for imbalanced classes</p>
</td></tr>
<tr><td><code id="setGradientBoostingMachine_+3A_lambda">lambda</code></td>
<td>
<p>L2 regularization on weights - larger is more conservative</p>
</td></tr>
<tr><td><code id="setGradientBoostingMachine_+3A_alpha">alpha</code></td>
<td>
<p>L1 regularization on weights - larger is more conservative</p>
</td></tr>
<tr><td><code id="setGradientBoostingMachine_+3A_seed">seed</code></td>
<td>
<p>An option to add a seed when training the final model</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A modelSettings object that can be used to fit the model
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
modelGbm &lt;- setGradientBoostingMachine(
  ntrees = c(10, 100), nthread = 20,
  maxDepth = c(4, 6), learnRate = c(0.1, 0.3)
)

</code></pre>

<hr>
<h2 id='setIterativeHardThresholding'>Create setting for Iterative Hard Thresholding model</h2><span id='topic+setIterativeHardThresholding'></span>

<h3>Description</h3>

<p>Create setting for Iterative Hard Thresholding model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>setIterativeHardThresholding(
  K = 10,
  penalty = "bic",
  seed = sample(1e+05, 1),
  exclude = c(),
  forceIntercept = FALSE,
  fitBestSubset = FALSE,
  initialRidgeVariance = 0.1,
  tolerance = 1e-08,
  maxIterations = 10000,
  threshold = 1e-06,
  delta = 0
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="setIterativeHardThresholding_+3A_k">K</code></td>
<td>
<p>The maximum number of non-zero predictors</p>
</td></tr>
<tr><td><code id="setIterativeHardThresholding_+3A_penalty">penalty</code></td>
<td>
<p>Specifies the IHT penalty; possible values are <code>BIC</code> or <code>AIC</code> or a numeric value</p>
</td></tr>
<tr><td><code id="setIterativeHardThresholding_+3A_seed">seed</code></td>
<td>
<p>An option to add a seed when training the model</p>
</td></tr>
<tr><td><code id="setIterativeHardThresholding_+3A_exclude">exclude</code></td>
<td>
<p>A vector of numbers or covariateId names to exclude from prior</p>
</td></tr>
<tr><td><code id="setIterativeHardThresholding_+3A_forceintercept">forceIntercept</code></td>
<td>
<p>Logical: Force intercept coefficient into regularization</p>
</td></tr>
<tr><td><code id="setIterativeHardThresholding_+3A_fitbestsubset">fitBestSubset</code></td>
<td>
<p>Logical: Fit final subset with no regularization</p>
</td></tr>
<tr><td><code id="setIterativeHardThresholding_+3A_initialridgevariance">initialRidgeVariance</code></td>
<td>
<p>integer</p>
</td></tr>
<tr><td><code id="setIterativeHardThresholding_+3A_tolerance">tolerance</code></td>
<td>
<p>numeric</p>
</td></tr>
<tr><td><code id="setIterativeHardThresholding_+3A_maxiterations">maxIterations</code></td>
<td>
<p>integer</p>
</td></tr>
<tr><td><code id="setIterativeHardThresholding_+3A_threshold">threshold</code></td>
<td>
<p>numeric</p>
</td></tr>
<tr><td><code id="setIterativeHardThresholding_+3A_delta">delta</code></td>
<td>
<p>numeric</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>modelSettings</code> object
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
modelIht &lt;- setIterativeHardThresholding(K = 5, seed = 42)

</code></pre>

<hr>
<h2 id='setLassoLogisticRegression'>Create modelSettings for lasso logistic regression</h2><span id='topic+setLassoLogisticRegression'></span>

<h3>Description</h3>

<p>Create modelSettings for lasso logistic regression
</p>


<h3>Usage</h3>

<pre><code class='language-R'>setLassoLogisticRegression(
  variance = 0.01,
  seed = NULL,
  includeCovariateIds = c(),
  noShrinkage = c(0),
  threads = -1,
  forceIntercept = FALSE,
  upperLimit = 20,
  lowerLimit = 0.01,
  tolerance = 2e-06,
  maxIterations = 3000,
  priorCoefs = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="setLassoLogisticRegression_+3A_variance">variance</code></td>
<td>
<p>Numeric: prior distribution starting variance</p>
</td></tr>
<tr><td><code id="setLassoLogisticRegression_+3A_seed">seed</code></td>
<td>
<p>An option to add a seed when training the model</p>
</td></tr>
<tr><td><code id="setLassoLogisticRegression_+3A_includecovariateids">includeCovariateIds</code></td>
<td>
<p>a set of covariateIds to limit the analysis to</p>
</td></tr>
<tr><td><code id="setLassoLogisticRegression_+3A_noshrinkage">noShrinkage</code></td>
<td>
<p>a set of covariates whcih are to be forced to be included in
in the final model. Default is the intercept</p>
</td></tr>
<tr><td><code id="setLassoLogisticRegression_+3A_threads">threads</code></td>
<td>
<p>An option to set number of threads when training model.</p>
</td></tr>
<tr><td><code id="setLassoLogisticRegression_+3A_forceintercept">forceIntercept</code></td>
<td>
<p>Logical: Force intercept coefficient into prior</p>
</td></tr>
<tr><td><code id="setLassoLogisticRegression_+3A_upperlimit">upperLimit</code></td>
<td>
<p>Numeric: Upper prior variance limit for grid-search</p>
</td></tr>
<tr><td><code id="setLassoLogisticRegression_+3A_lowerlimit">lowerLimit</code></td>
<td>
<p>Numeric: Lower prior variance limit for grid-search</p>
</td></tr>
<tr><td><code id="setLassoLogisticRegression_+3A_tolerance">tolerance</code></td>
<td>
<p>Numeric: maximum relative change in convergence criterion from
from successive iterations to achieve convergence</p>
</td></tr>
<tr><td><code id="setLassoLogisticRegression_+3A_maxiterations">maxIterations</code></td>
<td>
<p>Integer: maximum iterations of Cyclops to attempt
before returning a failed-to-converge error</p>
</td></tr>
<tr><td><code id="setLassoLogisticRegression_+3A_priorcoefs">priorCoefs</code></td>
<td>
<p>Use coefficients from a previous model as starting
points for model fit (transfer learning)</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>modelSettings</code> object
</p>


<h3>Examples</h3>

<pre><code class='language-R'>modelLasso &lt;- setLassoLogisticRegression(seed=42)
</code></pre>

<hr>
<h2 id='setLightGBM'>Create setting for gradient boosting machine model using lightGBM
(https://github.com/microsoft/LightGBM/tree/master/R-package).</h2><span id='topic+setLightGBM'></span>

<h3>Description</h3>

<p>Create setting for gradient boosting machine model using lightGBM
(https://github.com/microsoft/LightGBM/tree/master/R-package).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>setLightGBM(
  nthread = 20,
  earlyStopRound = 25,
  numIterations = c(100),
  numLeaves = c(31),
  maxDepth = c(5, 10),
  minDataInLeaf = c(20),
  learningRate = c(0.05, 0.1, 0.3),
  lambdaL1 = c(0),
  lambdaL2 = c(0),
  scalePosWeight = 1,
  isUnbalance = FALSE,
  seed = sample(1e+07, 1)
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="setLightGBM_+3A_nthread">nthread</code></td>
<td>
<p>The number of computer threads to use (how many cores do you have?)</p>
</td></tr>
<tr><td><code id="setLightGBM_+3A_earlystopround">earlyStopRound</code></td>
<td>
<p>If the performance does not increase over earlyStopRound number of trees then training stops (this prevents overfitting)</p>
</td></tr>
<tr><td><code id="setLightGBM_+3A_numiterations">numIterations</code></td>
<td>
<p>Number of boosting iterations.</p>
</td></tr>
<tr><td><code id="setLightGBM_+3A_numleaves">numLeaves</code></td>
<td>
<p>This hyperparameter sets the maximum number of leaves. Increasing this parameter can lead to higher model complexity and potential overfitting.</p>
</td></tr>
<tr><td><code id="setLightGBM_+3A_maxdepth">maxDepth</code></td>
<td>
<p>This hyperparameter sets the maximum depth . Increasing this parameter can also lead to higher model complexity and potential overfitting.</p>
</td></tr>
<tr><td><code id="setLightGBM_+3A_mindatainleaf">minDataInLeaf</code></td>
<td>
<p>This hyperparameter sets the minimum number of data points that must be present in a leaf node. Increasing this parameter can help to reduce overfitting</p>
</td></tr>
<tr><td><code id="setLightGBM_+3A_learningrate">learningRate</code></td>
<td>
<p>This hyperparameter controls the step size at each iteration of the gradient descent algorithm. Lower values can lead to slower convergence but may result in better performance.</p>
</td></tr>
<tr><td><code id="setLightGBM_+3A_lambdal1">lambdaL1</code></td>
<td>
<p>This hyperparameter controls L1 regularization, which can help to reduce overfitting by encouraging sparse models.</p>
</td></tr>
<tr><td><code id="setLightGBM_+3A_lambdal2">lambdaL2</code></td>
<td>
<p>This hyperparameter controls L2 regularization, which can also help to reduce overfitting by discouraging large weights in the model.</p>
</td></tr>
<tr><td><code id="setLightGBM_+3A_scaleposweight">scalePosWeight</code></td>
<td>
<p>Controls weight of positive class in loss - useful for imbalanced classes</p>
</td></tr>
<tr><td><code id="setLightGBM_+3A_isunbalance">isUnbalance</code></td>
<td>
<p>This parameter cannot be used at the same time with scalePosWeight, choose only one of them. While enabling this should increase the overall performance metric of your model, it will also result in poor estimates of the individual class probabilities.</p>
</td></tr>
<tr><td><code id="setLightGBM_+3A_seed">seed</code></td>
<td>
<p>An option to add a seed when training the final model</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of settings that can be used to train a model with <code>runPlp</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
modelLightGbm &lt;- setLightGBM(
  numLeaves = c(20, 31, 50), maxDepth = c(-1, 5, 10),
  minDataInLeaf = c(10, 20, 30), learningRate = c(0.05, 0.1, 0.3)
)

</code></pre>

<hr>
<h2 id='setMLP'>Create setting for neural network model with python's scikit-learn. For
bigger models, consider using <code>DeepPatientLevelPrediction</code> package.</h2><span id='topic+setMLP'></span>

<h3>Description</h3>

<p>Create setting for neural network model with python's scikit-learn. For
bigger models, consider using <code>DeepPatientLevelPrediction</code> package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>setMLP(
  hiddenLayerSizes = list(c(100), c(20)),
  activation = list("relu"),
  solver = list("adam"),
  alpha = list(0.3, 0.01, 1e-04, 1e-06),
  batchSize = list("auto"),
  learningRate = list("constant"),
  learningRateInit = list(0.001),
  powerT = list(0.5),
  maxIter = list(200, 100),
  shuffle = list(TRUE),
  tol = list(1e-04),
  warmStart = list(TRUE),
  momentum = list(0.9),
  nesterovsMomentum = list(TRUE),
  earlyStopping = list(FALSE),
  validationFraction = list(0.1),
  beta1 = list(0.9),
  beta2 = list(0.999),
  epsilon = list(1e-08),
  nIterNoChange = list(10),
  seed = sample(1e+05, 1)
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="setMLP_+3A_hiddenlayersizes">hiddenLayerSizes</code></td>
<td>
<p>(list of vectors) The ith element represents the number of neurons in the ith hidden layer.</p>
</td></tr>
<tr><td><code id="setMLP_+3A_activation">activation</code></td>
<td>
<p>(list) Activation function for the hidden layer.
</p>

<ul>
<li><p> &quot;identity&quot;: no-op activation, useful to implement linear bottleneck, returns f(x) = x
</p>
</li>
<li><p> &quot;logistic&quot;: the logistic sigmoid function, returns f(x) = 1 / (1 + exp(-x)).
</p>
</li>
<li><p> &quot;tanh&quot;: the hyperbolic tan function, returns f(x) = tanh(x).
</p>
</li>
<li><p> &quot;relu&quot;: the rectified linear unit function, returns f(x) = max(0, x)
</p>
</li></ul>
</td></tr>
<tr><td><code id="setMLP_+3A_solver">solver</code></td>
<td>
<p>(list) The solver for weight optimization. (‘lbfgs’, ‘sgd’, ‘adam’)</p>
</td></tr>
<tr><td><code id="setMLP_+3A_alpha">alpha</code></td>
<td>
<p>(list) L2 penalty (regularization term) parameter.</p>
</td></tr>
<tr><td><code id="setMLP_+3A_batchsize">batchSize</code></td>
<td>
<p>(list) Size of minibatches for stochastic optimizers. If the solver is ‘lbfgs’, the classifier will not use minibatch. When set to “auto”, batchSize=min(200, n_samples).</p>
</td></tr>
<tr><td><code id="setMLP_+3A_learningrate">learningRate</code></td>
<td>
<p>(list) Only used when solver='sgd' Learning rate schedule for weight updates. ‘constant’, ‘invscaling’, ‘adaptive’, default=’constant’</p>
</td></tr>
<tr><td><code id="setMLP_+3A_learningrateinit">learningRateInit</code></td>
<td>
<p>(list) Only used when solver=’sgd’ or ‘adam’. The initial learning rate used. It controls the step-size in updating the weights.</p>
</td></tr>
<tr><td><code id="setMLP_+3A_powert">powerT</code></td>
<td>
<p>(list) Only used when solver=’sgd’.  The exponent for inverse scaling learning rate. It is used in updating effective learning rate when the learning_rate is set to ‘invscaling’.</p>
</td></tr>
<tr><td><code id="setMLP_+3A_maxiter">maxIter</code></td>
<td>
<p>(list)  Maximum number of iterations. The solver iterates until convergence (determined by ‘tol’) or this number of iterations. For stochastic solvers (‘sgd’, ‘adam’), note that this determines the number of epochs (how many times each data point will be used), not the number of gradient steps.</p>
</td></tr>
<tr><td><code id="setMLP_+3A_shuffle">shuffle</code></td>
<td>
<p>(list) boolean: Whether to shuffle samples in each iteration. Only used when solver=’sgd’ or ‘adam’.</p>
</td></tr>
<tr><td><code id="setMLP_+3A_tol">tol</code></td>
<td>
<p>(list) Tolerance for the optimization. When the loss or score is not improving by at least tol for nIterNoChange consecutive iterations, unless learning_rate is set to ‘adaptive’, convergence is considered to be reached and training stops.</p>
</td></tr>
<tr><td><code id="setMLP_+3A_warmstart">warmStart</code></td>
<td>
<p>(list) When set to True, reuse the solution of the previous call to fit as initialization, otherwise, just erase the previous solution.</p>
</td></tr>
<tr><td><code id="setMLP_+3A_momentum">momentum</code></td>
<td>
<p>(list) Momentum for gradient descent update. Should be between 0 and 1. Only used when solver=’sgd’.</p>
</td></tr>
<tr><td><code id="setMLP_+3A_nesterovsmomentum">nesterovsMomentum</code></td>
<td>
<p>(list) Whether to use Nesterov’s momentum. Only used when solver=’sgd’ and momentum &gt; 0.</p>
</td></tr>
<tr><td><code id="setMLP_+3A_earlystopping">earlyStopping</code></td>
<td>
<p>(list) boolean Whether to use early stopping to terminate training when validation score is not improving. If set to true, it will automatically set aside 10 percent of training data as validation and terminate training when validation score is not improving by at least tol for n_iter_no_change consecutive epochs.</p>
</td></tr>
<tr><td><code id="setMLP_+3A_validationfraction">validationFraction</code></td>
<td>
<p>(list) The proportion of training data to set aside as validation set for early stopping. Must be between 0 and 1. Only used if earlyStopping is True.</p>
</td></tr>
<tr><td><code id="setMLP_+3A_beta1">beta1</code></td>
<td>
<p>(list) Exponential decay rate for estimates of first moment vector in adam, should be in 0 to 1.</p>
</td></tr>
<tr><td><code id="setMLP_+3A_beta2">beta2</code></td>
<td>
<p>(list) Exponential decay rate for estimates of second moment vector in adam, should be in 0 to 1.</p>
</td></tr>
<tr><td><code id="setMLP_+3A_epsilon">epsilon</code></td>
<td>
<p>(list) Value for numerical stability in adam.</p>
</td></tr>
<tr><td><code id="setMLP_+3A_niternochange">nIterNoChange</code></td>
<td>
<p>(list) Maximum number of epochs to not meet tol improvement. Only effective when solver=’sgd’ or ‘adam’.</p>
</td></tr>
<tr><td><code id="setMLP_+3A_seed">seed</code></td>
<td>
<p>A seed for the model</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a modelSettings object
</p>


<h3>Examples</h3>

<pre><code class='language-R'> 
## Not run:  
model &lt;- setMLP(hiddenLayerSizes = list(c(20)), alpha=list(3e-4), seed = 42)

## End(Not run)
</code></pre>

<hr>
<h2 id='setNaiveBayes'>Create setting for naive bayes model with python</h2><span id='topic+setNaiveBayes'></span>

<h3>Description</h3>

<p>Create setting for naive bayes model with python
</p>


<h3>Usage</h3>

<pre><code class='language-R'>setNaiveBayes()
</code></pre>


<h3>Value</h3>

<p>a modelSettings object
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run:  
plpData &lt;- getEunomiaPlpData()
model &lt;- setNaiveBayes()
analysisId &lt;- "naiveBayes"
saveLocation &lt;- file.path(tempdir(), analysisId)
results &lt;- runPlp(plpData, modelSettings = model,
                  saveDirectory = saveLocation,
                  analysisId = analysisId)
# clean up
unlink(saveLocation, recursive = TRUE)

## End(Not run)
</code></pre>

<hr>
<h2 id='setPythonEnvironment'>Use the python environment created using configurePython()</h2><span id='topic+setPythonEnvironment'></span>

<h3>Description</h3>

<p>Use the python environment created using configurePython()
</p>


<h3>Usage</h3>

<pre><code class='language-R'>setPythonEnvironment(envname = "PLP", envtype = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="setPythonEnvironment_+3A_envname">envname</code></td>
<td>
<p>A string for the name of the virtual environment (default is 'PLP')</p>
</td></tr>
<tr><td><code id="setPythonEnvironment_+3A_envtype">envtype</code></td>
<td>
<p>An option for specifying the environment as'conda' or 'python'.  If NULL then the default is 'conda' for windows users and 'python' for non-windows users</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function sets PatientLevelPrediction to use a python environment
</p>


<h3>Value</h3>

<p>A string indicating the which python environment will be used
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run:  #' # create a conda environment named PLP
configurePython(envname="PLP", envtype="conda")

## End(Not run)
</code></pre>

<hr>
<h2 id='setRandomForest'>Create setting for random forest model using sklearn</h2><span id='topic+setRandomForest'></span>

<h3>Description</h3>

<p>Create setting for random forest model using sklearn
</p>


<h3>Usage</h3>

<pre><code class='language-R'>setRandomForest(
  ntrees = list(100, 500),
  criterion = list("gini"),
  maxDepth = list(4, 10, 17),
  minSamplesSplit = list(2, 5),
  minSamplesLeaf = list(1, 10),
  minWeightFractionLeaf = list(0),
  mtries = list("sqrt", "log2"),
  maxLeafNodes = list(NULL),
  minImpurityDecrease = list(0),
  bootstrap = list(TRUE),
  maxSamples = list(NULL, 0.9),
  oobScore = list(FALSE),
  nJobs = list(NULL),
  classWeight = list(NULL),
  seed = sample(1e+05, 1)
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="setRandomForest_+3A_ntrees">ntrees</code></td>
<td>
<p>(list) The number of trees to build</p>
</td></tr>
<tr><td><code id="setRandomForest_+3A_criterion">criterion</code></td>
<td>
<p>(list) The function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “entropy” for the information gain. Note: this parameter is tree-specific.</p>
</td></tr>
<tr><td><code id="setRandomForest_+3A_maxdepth">maxDepth</code></td>
<td>
<p>(list) The maximum depth of the tree. If NULL, then nodes are expanded until all leaves are pure or until all leaves contain less than minSamplesSplit samples.</p>
</td></tr>
<tr><td><code id="setRandomForest_+3A_minsamplessplit">minSamplesSplit</code></td>
<td>
<p>(list) The minimum number of samples required to split an internal node</p>
</td></tr>
<tr><td><code id="setRandomForest_+3A_minsamplesleaf">minSamplesLeaf</code></td>
<td>
<p>(list) The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least minSamplesLeaf training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.</p>
</td></tr>
<tr><td><code id="setRandomForest_+3A_minweightfractionleaf">minWeightFractionLeaf</code></td>
<td>
<p>(list) The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sampleWeight is not provided.</p>
</td></tr>
<tr><td><code id="setRandomForest_+3A_mtries">mtries</code></td>
<td>
<p>(list) The number of features to consider when looking for the best split:
</p>

<ul>
<li><p> int then consider max_features features at each split.
</p>
</li>
<li><p> float then max_features is a fraction and round(max_features * n_features) features are considered at each split
</p>
</li>
<li><p> 'sqrt' then max_features=sqrt(n_features)
</p>
</li>
<li><p> 'log2' then max_features=log2(n_features)
</p>
</li>
<li><p> NULL then max_features=n_features
</p>
</li></ul>
</td></tr>
<tr><td><code id="setRandomForest_+3A_maxleafnodes">maxLeafNodes</code></td>
<td>
<p>(list) Grow trees with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.</p>
</td></tr>
<tr><td><code id="setRandomForest_+3A_minimpuritydecrease">minImpurityDecrease</code></td>
<td>
<p>(list) A node will be split if this split induces a decrease of the impurity greater than or equal to this value.</p>
</td></tr>
<tr><td><code id="setRandomForest_+3A_bootstrap">bootstrap</code></td>
<td>
<p>(list) Whether bootstrap samples are used when building trees. If False, the whole dataset is used to build each tree.</p>
</td></tr>
<tr><td><code id="setRandomForest_+3A_maxsamples">maxSamples</code></td>
<td>
<p>(list) If bootstrap is True, the number of samples to draw from X to train each base estimator.</p>
</td></tr>
<tr><td><code id="setRandomForest_+3A_oobscore">oobScore</code></td>
<td>
<p>(list) Whether to use out-of-bag samples to estimate the generalization score. Only available if bootstrap=True.</p>
</td></tr>
<tr><td><code id="setRandomForest_+3A_njobs">nJobs</code></td>
<td>
<p>The number of jobs to run in parallel.</p>
</td></tr>
<tr><td><code id="setRandomForest_+3A_classweight">classWeight</code></td>
<td>
<p>(list) Weights associated with classes. If not given, all classes are supposed to have weight one. NULL, “balanced”, “balanced_subsample”</p>
</td></tr>
<tr><td><code id="setRandomForest_+3A_seed">seed</code></td>
<td>
<p>A seed when training the final model</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a modelSettings object
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run:  
plpData &lt;- getEunomiaPlpData()
model &lt;- setRandomForest(ntrees = list(100),
                          maxDepth = list(4),
                          minSamplesSplit = list(2),
                          minSamplesLeaf = list(10),
                          maxSamples = list(0.9),
                          seed = 42)
saveLoc &lt;- file.path(tempdir(), "randomForest")
results &lt;- runPlp(plpData, modelSettings = model, saveDirectory = saveLoc)
# clean up
unlink(saveLoc, recursive = TRUE)

## End(Not run)
</code></pre>

<hr>
<h2 id='setSVM'>Create setting for the python sklearn SVM (SVC function)</h2><span id='topic+setSVM'></span>

<h3>Description</h3>

<p>Create setting for the python sklearn SVM (SVC function)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>setSVM(
  C = list(1, 0.9, 2, 0.1),
  kernel = list("rbf"),
  degree = list(1, 3, 5),
  gamma = list("scale", 1e-04, 3e-05, 0.001, 0.01, 0.25),
  coef0 = list(0),
  shrinking = list(TRUE),
  tol = list(0.001),
  classWeight = list(NULL),
  cacheSize = 500,
  seed = sample(1e+05, 1)
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="setSVM_+3A_c">C</code></td>
<td>
<p>(list) Regularization parameter. The strength of the regularization is inversely proportional to C. Must be strictly positive. The penalty is a squared l2 penalty.</p>
</td></tr>
<tr><td><code id="setSVM_+3A_kernel">kernel</code></td>
<td>
<p>(list) Specifies the kernel type to be used in the algorithm. one of ‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’. If none is given ‘rbf’ will be used.</p>
</td></tr>
<tr><td><code id="setSVM_+3A_degree">degree</code></td>
<td>
<p>(list) degree of kernel function is significant only in poly, rbf, sigmoid</p>
</td></tr>
<tr><td><code id="setSVM_+3A_gamma">gamma</code></td>
<td>
<p>(list) kernel coefficient for rbf and poly, by default 1/n_features will be taken. ‘scale’, ‘auto’ or float, default=’scale’</p>
</td></tr>
<tr><td><code id="setSVM_+3A_coef0">coef0</code></td>
<td>
<p>(list) independent term in kernel function. It is only significant in poly/sigmoid.</p>
</td></tr>
<tr><td><code id="setSVM_+3A_shrinking">shrinking</code></td>
<td>
<p>(list) whether to use the shrinking heuristic.</p>
</td></tr>
<tr><td><code id="setSVM_+3A_tol">tol</code></td>
<td>
<p>(list) Tolerance for stopping criterion.</p>
</td></tr>
<tr><td><code id="setSVM_+3A_classweight">classWeight</code></td>
<td>
<p>(list) Class weight based on imbalance either 'balanced' or NULL</p>
</td></tr>
<tr><td><code id="setSVM_+3A_cachesize">cacheSize</code></td>
<td>
<p>Specify the size of the kernel cache (in MB).</p>
</td></tr>
<tr><td><code id="setSVM_+3A_seed">seed</code></td>
<td>
<p>A seed for the model</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a modelSettings object
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run:  
plpData &lt;- getEunomiaPlpData()
model &lt;- setSVM(C = list(1), gamma = list("scale"), seed = 42)
saveLoc &lt;- file.path(tempdir(), "svm")
results &lt;- runPlp(plpData, modelSettings = model, saveDirectory = saveLoc)
# clean up
unlink(saveLoc, recursive = TRUE)

## End(Not run)
</code></pre>

<hr>
<h2 id='simpleImpute'>Simple Imputation</h2><span id='topic+simpleImpute'></span>

<h3>Description</h3>

<p>This function does single imputation with the mean or median
</p>


<h3>Usage</h3>

<pre><code class='language-R'>simpleImpute(trainData, featureEngineeringSettings, done = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="simpleImpute_+3A_traindata">trainData</code></td>
<td>
<p>The data to be imputed</p>
</td></tr>
<tr><td><code id="simpleImpute_+3A_featureengineeringsettings">featureEngineeringSettings</code></td>
<td>
<p>The settings for the imputation</p>
</td></tr>
<tr><td><code id="simpleImpute_+3A_done">done</code></td>
<td>
<p>Whether the imputation has already been done (bool)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The imputed data
</p>

<hr>
<h2 id='simulatePlpData'>Generate simulated data</h2><span id='topic+simulatePlpData'></span>

<h3>Description</h3>

<p><code>simulateplpData</code> creates a plpData object with simulated data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>simulatePlpData(plpDataSimulationProfile, n = 10000)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="simulatePlpData_+3A_plpdatasimulationprofile">plpDataSimulationProfile</code></td>
<td>
<p>An object of type <code>plpDataSimulationProfile</code> as generated
using the <br /><code>createplpDataSimulationProfile</code> function.</p>
</td></tr>
<tr><td><code id="simulatePlpData_+3A_n">n</code></td>
<td>
<p>The size of the population to be generated.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function generates simulated data that is in many ways similar to the original data on which
the simulation profile is based.
</p>


<h3>Value</h3>

<p>An object of type <code>plpData</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># first load the simulation profile to use
data("simulationProfile")
# then generate the simulated data
plpData &lt;- simulatePlpData(simulationProfile, n = 100)
nrow(plpData$cohorts)
</code></pre>

<hr>
<h2 id='simulationProfile'>A simulation profile for generating synthetic patient level prediction data</h2><span id='topic+simulationProfile'></span>

<h3>Description</h3>

<p>A simulation profile for generating synthetic patient level prediction data
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(simulationProfile)
</code></pre>


<h3>Format</h3>

<p>A data frame containing the following elements:
</p>

<dl>
<dt>covariatePrevalence</dt><dd><p>prevalence of all covariates</p>
</dd>
<dt>outcomeModels</dt><dd><p>regression model parameters to simulate outcomes</p>
</dd>
<dt>metaData</dt><dd><p>settings used to simulate the profile</p>
</dd>
<dt>covariateRef</dt><dd><p>covariateIds and covariateNames</p>
</dd>
<dt>timePrevalence</dt><dd><p>time window</p>
</dd>
<dt>exclusionPrevalence</dt><dd><p>prevalence of exclusion of covariates</p>
</dd>
</dl>


<hr>
<h2 id='sklearnFromJson'>Loads sklearn python model from json</h2><span id='topic+sklearnFromJson'></span>

<h3>Description</h3>

<p>Loads sklearn python model from json
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sklearnFromJson(path)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sklearnFromJson_+3A_path">path</code></td>
<td>
<p>path to the model json file</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a sklearn python model object
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run:  
plpData &lt;- getEunomiaPlpData()
modelSettings &lt;- setDecisionTree(maxDepth = list(3), minSamplesSplit = list(2),
                                  minSamplesLeaf = list(1), maxFeatures = list(100))
saveLocation &lt;- file.path(tempdir(), "sklearnFromJson")
results &lt;- runPlp(plpData, modelSettings = modelSettings, saveDirectory = saveLocation)
# view save model
dir(results$model$model, full.names = TRUE)
# load into a sklearn object
model &lt;- sklearnFromJson(file.path(results$model$model, "model.json"))
# max depth is 3 as we set in beginning
model$max_depth
# clean up
unlink(saveLocation, recursive = TRUE)

## End(Not run)
</code></pre>

<hr>
<h2 id='sklearnToJson'>Saves sklearn python model object to json in path</h2><span id='topic+sklearnToJson'></span>

<h3>Description</h3>

<p>Saves sklearn python model object to json in path
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sklearnToJson(model, path)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sklearnToJson_+3A_model">model</code></td>
<td>
<p>a fitted sklearn python model object</p>
</td></tr>
<tr><td><code id="sklearnToJson_+3A_path">path</code></td>
<td>
<p>path to the saved model file</p>
</td></tr>
</table>


<h3>Value</h3>

<p>nothing, saves the model to the path as json
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run:  
sklearn &lt;- reticulate::import("sklearn", convert = FALSE)
model &lt;- sklearn$tree$DecisionTreeClassifier()
model$fit(sklearn$datasets$load_iris()$data, sklearn$datasets$load_iris()$target)
saveLoc &lt;- file.path(tempdir(), "model.json")
sklearnToJson(model, saveLoc)
# the model.json is saved in the tempdir
dir(tempdir())
# clean up
unlink(saveLoc)

## End(Not run)
</code></pre>

<hr>
<h2 id='splitData'>Split the plpData into test/train sets using a splitting settings of class
<code>splitSettings</code></h2><span id='topic+splitData'></span>

<h3>Description</h3>

<p>Split the plpData into test/train sets using a splitting settings of class
<code>splitSettings</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>splitData(
  plpData = plpData,
  population = population,
  splitSettings = createDefaultSplitSetting(splitSeed = 42)
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="splitData_+3A_plpdata">plpData</code></td>
<td>
<p>An object of type <code>plpData</code> - the patient level
prediction data extracted from the CDM.</p>
</td></tr>
<tr><td><code id="splitData_+3A_population">population</code></td>
<td>
<p>The population created using <code>createStudyPopulation</code>
that define who will be used to develop the model</p>
</td></tr>
<tr><td><code id="splitData_+3A_splitsettings">splitSettings</code></td>
<td>
<p>An object of type <code>splitSettings</code> specifying the
split - the default can be created using <code>createDefaultSplitSetting</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a list containing the training data (Train) and optionally the test
data (Test). Train is an Andromeda object containing
</p>
<ul>
<li><p> covariates: a table (rowId, covariateId, covariateValue)
containing the covariates for each data point in the train data
</p>
</li>
<li><p> covariateRef: a table with the covariate information
</p>
</li>
<li><p> labels: a table (rowId, outcomeCount, ...) for each data point
in the train data (outcomeCount is the class label)
</p>
</li>
<li><p> folds: a table (rowId, index) specifying which training
fold each data point is in.
</p>
</li></ul>

<p>Test is an Andromeda object containing
</p>
<ul>
<li><p> covariates: a table (rowId, covariateId, covariateValue)
containing the covariates for each data point in the test data
</p>
</li>
<li><p> covariateRef: a table with the covariate information
</p>
</li>
<li><p> labels: a table (rowId, outcomeCount, ...) for each data
point in the test data (outcomeCount is the class label)
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>data("simulationProfile")
plpData &lt;- simulatePlpData(simulationProfile, n = 1000)
population &lt;- createStudyPopulation(plpData)
splitSettings &lt;- createDefaultSplitSetting(testFraction = 0.50, 
                                           trainFraction = 0.50, nfold = 5)
data = splitData(plpData, population, splitSettings)
# test data should be ~500 rows (changes because of study population)
nrow(data$Test$labels)
# train data should be ~500 rows
nrow(data$Train$labels)
# should be five fold in the train data
length(unique(data$Train$folds$index))
</code></pre>

<hr>
<h2 id='summary.plpData'>Summarize a plpData object</h2><span id='topic+summary.plpData'></span>

<h3>Description</h3>

<p>Summarize a plpData object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'plpData'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="summary.plpData_+3A_object">object</code></td>
<td>
<p>The plpData object to summarize</p>
</td></tr>
<tr><td><code id="summary.plpData_+3A_...">...</code></td>
<td>
<p>Additional arguments</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A summary of the object containing the number of people, outcomes and covariates
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("simulationProfile")
plpData &lt;- simulatePlpData(simulationProfile, n=10)
summary(plpData)
</code></pre>

<hr>
<h2 id='toSparseM'>Convert the plpData in COO format into a sparse R matrix</h2><span id='topic+toSparseM'></span>

<h3>Description</h3>

<p>Converts the standard plpData to a sparse matrix
</p>


<h3>Usage</h3>

<pre><code class='language-R'>toSparseM(plpData, cohort = NULL, map = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="toSparseM_+3A_plpdata">plpData</code></td>
<td>
<p>An object of type <code>plpData</code> with covariate in coo format - the patient level prediction
data extracted from the CDM.</p>
</td></tr>
<tr><td><code id="toSparseM_+3A_cohort">cohort</code></td>
<td>
<p>If specified the plpData is restricted to the rowIds in the cohort (otherwise plpData$labels is used)</p>
</td></tr>
<tr><td><code id="toSparseM_+3A_map">map</code></td>
<td>
<p>A covariate map (telling us the column number for covariates)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function converts the covariates <code>Andromeda</code> table in COO format into a sparse matrix from
the package Matrix
</p>


<h3>Value</h3>

<p>Returns a list, containing the data as a sparse matrix, the plpData covariateRef
and a data.frame named map that tells us what covariate corresponds to each column
This object is a list with the following components: </p>

<dl>
<dt>data</dt><dd><p>A sparse matrix with the rows corresponding to each person in the plpData and the columns corresponding to the covariates.</p>
</dd>
<dt>covariateRef</dt><dd><p>The plpData covariateRef.</p>
</dd>
<dt>map</dt><dd><p>A data.frame containing the data column ids and the corresponding covariateId from covariateRef.</p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'> 
library(dplyr)
data("simulationProfile")
plpData &lt;- simulatePlpData(simulationProfile, n=100)
# how many covariates are there before we convert to sparse matrix
plpData$covariateData$covariates %&gt;% 
 dplyr::group_by(.data$covariateId) %&gt;% 
 dplyr::summarise(n = n()) %&gt;% 
 dplyr::collect() %&gt;% nrow()
sparseData &lt;- toSparseM(plpData, cohort=plpData$cohorts)
# how many covariates are there after we convert to sparse matrix'
sparseData$dataMatrix@Dim[2]

</code></pre>

<hr>
<h2 id='validateExternal'>validateExternal - Validate model performance on new data</h2><span id='topic+validateExternal'></span>

<h3>Description</h3>

<p>validateExternal - Validate model performance on new data
</p>


<h3>Usage</h3>

<pre><code class='language-R'>validateExternal(
  validationDesignList,
  databaseDetails,
  logSettings = createLogSettings(verbosity = "INFO", logName = "validatePLP"),
  outputFolder
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="validateExternal_+3A_validationdesignlist">validationDesignList</code></td>
<td>
<p>A list of objects created with <code>createValidationDesign</code></p>
</td></tr>
<tr><td><code id="validateExternal_+3A_databasedetails">databaseDetails</code></td>
<td>
<p>A list of objects of class
<code>databaseDetails</code> created using <code>createDatabaseDetails</code></p>
</td></tr>
<tr><td><code id="validateExternal_+3A_logsettings">logSettings</code></td>
<td>
<p>An object of <code>logSettings</code> created
using <code>createLogSettings</code></p>
</td></tr>
<tr><td><code id="validateExternal_+3A_outputfolder">outputFolder</code></td>
<td>
<p>The directory to save the validation results to
(subfolders are created per database in validationDatabaseDetails)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of results
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
 
data("simulationProfile")
plpData &lt;- simulatePlpData(simulationProfile, n=1000)
# first fit a model on some data, default is a L1 logistic regression
saveLoc &lt;- file.path(tempdir(), "development")
results &lt;- runPlp(plpData, saveDirectory = saveLoc)
# then create my validation design
validationDesign &lt;- createValidationDesign(1, 3, plpModelList = list(results$model))
# I will validate on Eunomia example database
connectionDetails &lt;- Eunomia::getEunomiaConnectionDetails()
Eunomia::createCohorts(connectionDetails)
databaseDetails &lt;- createDatabaseDetails(connectionDetails = connectionDetails,
cdmDatabaseSchema = "main", cdmDatabaseName = "Eunomia", cdmDatabaseId = 1,
targetId = 1, outcomeIds = 3)
path &lt;- file.path(tempdir(), "validation")
validateExternal(validationDesign, databaseDetails, outputFolder = path)
# see generated result files
dir(path, recursive = TRUE)
# clean up
unlink(saveLoc, recursive = TRUE)
unlink(path, recursive = TRUE)


</code></pre>

<hr>
<h2 id='validateMultiplePlp'>externally validate the multiple plp models across new datasets</h2><span id='topic+validateMultiplePlp'></span>

<h3>Description</h3>

<p>This function loads all the models in a multiple plp analysis folder and
validates the models on new data
</p>


<h3>Usage</h3>

<pre><code class='language-R'>validateMultiplePlp(
  analysesLocation,
  validationDatabaseDetails,
  validationRestrictPlpDataSettings = createRestrictPlpDataSettings(),
  recalibrate = NULL,
  cohortDefinitions = NULL,
  saveDirectory = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="validateMultiplePlp_+3A_analyseslocation">analysesLocation</code></td>
<td>
<p>The location where the multiple plp analyses are</p>
</td></tr>
<tr><td><code id="validateMultiplePlp_+3A_validationdatabasedetails">validationDatabaseDetails</code></td>
<td>
<p>A single or list of validation database settings created using <code>createDatabaseDetails()</code></p>
</td></tr>
<tr><td><code id="validateMultiplePlp_+3A_validationrestrictplpdatasettings">validationRestrictPlpDataSettings</code></td>
<td>
<p>The settings specifying the extra restriction settings when extracting the data created using <code>createRestrictPlpDataSettings()</code>.</p>
</td></tr>
<tr><td><code id="validateMultiplePlp_+3A_recalibrate">recalibrate</code></td>
<td>
<p>A vector of recalibration methods (currently supports 'RecalibrationintheLarge' and/or 'weakRecalibration')</p>
</td></tr>
<tr><td><code id="validateMultiplePlp_+3A_cohortdefinitions">cohortDefinitions</code></td>
<td>
<p>A list of cohortDefinitions</p>
</td></tr>
<tr><td><code id="validateMultiplePlp_+3A_savedirectory">saveDirectory</code></td>
<td>
<p>The location to save to validation results</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Users need to input a location where the results of the multiple plp analyses
are found and the connection and database settings for the new data
</p>


<h3>Value</h3>

<p>Nothing. The results are saved to the saveDirectory
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
 
# first develop a model using runMultiplePlp
connectionDetails &lt;- Eunomia::getEunomiaConnectionDetails()
Eunomia::createCohorts(connectionDetails = connectionDetails)
databaseDetails &lt;- createDatabaseDetails(connectionDetails = connectionDetails,
                                         cdmDatabaseId = "1",
                                         cdmDatabaseName = "Eunomia",
                                         cdmDatabaseSchema = "main",
                                         targetId = 1,
                                         outcomeIds = 3)
covariateSettings &lt;- 
 FeatureExtraction::createCovariateSettings(useDemographicsGender = TRUE,
   useDemographicsAge = TRUE, useConditionOccurrenceLongTerm = TRUE)
modelDesign &lt;- createModelDesign(targetId = 1, 
                                 outcomeId = 3,
                                 modelSettings = setLassoLogisticRegression(seed = 42),
                                 covariateSettings = covariateSettings)
saveLoc &lt;- file.path(tempdir(), "valiateMultiplePlp", "development")
results &lt;- runMultiplePlp(databaseDetails = databaseDetails,
               modelDesignList = list(modelDesign),
               saveDirectory = saveLoc)
# now validate the model on a Eunomia but with a different target
analysesLocation &lt;- saveLoc
validationDatabaseDetails &lt;- createDatabaseDetails(connectionDetails = connectionDetails,
                                                   cdmDatabaseId = "2",
                                                   cdmDatabaseName = "EunomiaNew",
                                                   cdmDatabaseSchema = "main",
                                                   targetId = 4,
                                                   outcomeIds = 3)
newSaveLoc &lt;- file.path(tempdir(), "valiateMultiplePlp", "validation")
validateMultiplePlp(analysesLocation = analysesLocation,
                    validationDatabaseDetails = validationDatabaseDetails,
                    saveDirectory = newSaveLoc)
# the results could now be viewed in the shiny app with viewMultiplePlp(newSaveLoc)


</code></pre>

<hr>
<h2 id='viewDatabaseResultPlp'>open a local shiny app for viewing the result of a PLP analyses from a database</h2><span id='topic+viewDatabaseResultPlp'></span>

<h3>Description</h3>

<p>open a local shiny app for viewing the result of a PLP analyses from a database
</p>


<h3>Usage</h3>

<pre><code class='language-R'>viewDatabaseResultPlp(
  mySchema,
  myServer,
  myUser,
  myPassword,
  myDbms,
  myPort = NULL,
  myTableAppend
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="viewDatabaseResultPlp_+3A_myschema">mySchema</code></td>
<td>
<p>Database result schema containing the result tables</p>
</td></tr>
<tr><td><code id="viewDatabaseResultPlp_+3A_myserver">myServer</code></td>
<td>
<p>server with the result database</p>
</td></tr>
<tr><td><code id="viewDatabaseResultPlp_+3A_myuser">myUser</code></td>
<td>
<p>Username for the connection to the result database</p>
</td></tr>
<tr><td><code id="viewDatabaseResultPlp_+3A_mypassword">myPassword</code></td>
<td>
<p>Password for the connection to the result database</p>
</td></tr>
<tr><td><code id="viewDatabaseResultPlp_+3A_mydbms">myDbms</code></td>
<td>
<p>database management system for the result database</p>
</td></tr>
<tr><td><code id="viewDatabaseResultPlp_+3A_myport">myPort</code></td>
<td>
<p>Port for the connection to the result database</p>
</td></tr>
<tr><td><code id="viewDatabaseResultPlp_+3A_mytableappend">myTableAppend</code></td>
<td>
<p>A string appended to the results tables (optional)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Opens a shiny app for viewing the results of the models from a database
</p>


<h3>Value</h3>

<p>Opens a shiny app for interactively viewing the results
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
  
connectionDetails &lt;- Eunomia::getEunomiaConnectionDetails()
Eunomia::createCohorts(connectionDetails)
databaseDetails &lt;- createDatabaseDetails(connectionDetails = connectionDetails,
                                          cdmDatabaseSchema = "main",
                                          cdmDatabaseName = "Eunomia",
                                          cdmDatabaseId = "1",
                                          targetId = 1,
                                          outcomeIds = 3)
modelDesign &lt;- createModelDesign(targetId = 1,
                                 outcomeId = 3,
                                 modelSettings = setLassoLogisticRegression())
saveLoc &lt;- file.path(tempdir(), "viewDatabaseResultPlp", "developement")
runMultiplePlp(databaseDetails = databaseDetails, modelDesignList = list(modelDesign),
               saveDirectory = saveLoc)
# view result files
dir(saveLoc, recursive = TRUE)
viewDatabaseResultPlp(myDbms = "sqlite", 
                      mySchema = "main", 
                      myServer = file.path(saveLoc, "sqlite", "databaseFile.sqlite"),
                      myUser = NULL,
                      myPassword = NULL,
                      myTableAppend = "")
# clean up, shiny app can't be opened after the following has been run
unlink(saveLoc, recursive = TRUE)


</code></pre>

<hr>
<h2 id='viewMultiplePlp'>open a local shiny app for viewing the result of a multiple PLP analyses</h2><span id='topic+viewMultiplePlp'></span>

<h3>Description</h3>

<p>open a local shiny app for viewing the result of a multiple PLP analyses
</p>


<h3>Usage</h3>

<pre><code class='language-R'>viewMultiplePlp(analysesLocation)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="viewMultiplePlp_+3A_analyseslocation">analysesLocation</code></td>
<td>
<p>The directory containing the results (with the analysis_x folders)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Opens a shiny app for viewing the results of the models from various T,O, Tar and settings
settings.
</p>


<h3>Value</h3>

<p>Opens a shiny app for interactively viewing the results
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
 
connectionDetails &lt;- Eunomia::getEunomiaConnectionDetails()
Eunomia::createCohorts(connectionDetails)
databaseDetails &lt;- createDatabaseDetails(connectionDetails = connectionDetails,
                                          cdmDatabaseSchema = "main",
                                          cdmDatabaseName = "Eunomia",
                                          cdmDatabaseId = "1",
                                          targetId = 1,
                                          outcomeIds = 3)
modelDesign &lt;- createModelDesign(targetId = 1, 
                                 outcomeId = 3, 
                                 modelSettings = setLassoLogisticRegression())
saveLoc &lt;- file.path(tempdir(), "viewMultiplePlp", "development")
runMultiplePlp(databaseDetails = databaseDetails, modelDesignList = list(modelDesign),
               saveDirectory = saveLoc)
# view result files
dir(saveLoc, recursive = TRUE)
# open shiny app
viewMultiplePlp(analysesLocation = saveLoc)
# clean up, shiny app can't be opened after the following has been run
unlink(saveLoc, recursive = TRUE)


</code></pre>

<hr>
<h2 id='viewPlp'>viewPlp - Interactively view the performance and model settings</h2><span id='topic+viewPlp'></span>

<h3>Description</h3>

<p>This is a shiny app for viewing interactive plots of the performance and the settings
</p>


<h3>Usage</h3>

<pre><code class='language-R'>viewPlp(runPlp, validatePlp = NULL, diagnosePlp = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="viewPlp_+3A_runplp">runPlp</code></td>
<td>
<p>The output of runPlp() (an object of class 'runPlp')</p>
</td></tr>
<tr><td><code id="viewPlp_+3A_validateplp">validatePlp</code></td>
<td>
<p>The output of externalValidatePlp (on object of class 'validatePlp')</p>
</td></tr>
<tr><td><code id="viewPlp_+3A_diagnoseplp">diagnosePlp</code></td>
<td>
<p>The output of diagnosePlp()</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Either the result of runPlp and view the plots
</p>


<h3>Value</h3>

<p>Opens a shiny app for interactively viewing the results
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
 
data("simulationProfile")
plpData &lt;- simulatePlpData(simulationProfile, n= 1000)
saveLoc &lt;- file.path(tempdir(), "viewPlp", "development")
results &lt;- runPlp(plpData, saveDirectory = saveLoc)
# view result files
dir(saveLoc, recursive = TRUE)
# open shiny app
viewPlp(results)
# clean up, shiny app can't be opened after the following has been run
unlink(saveLoc, recursive = TRUE)


</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
