<!DOCTYPE html><html><head><title>Help for package neuralnet</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {neuralnet}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#compute'><p>Deprecated function</p></a></li>
<li><a href='#confidence.interval'><p>Calculates confidence intervals of the weights</p></a></li>
<li><a href='#gwplot'><p>Plot method for generalized weights</p></a></li>
<li><a href='#neuralnet'><p>Training of neural networks</p></a></li>
<li><a href='#neuralnet-package'><p>Training of Neural Networks</p></a></li>
<li><a href='#plot.nn'><p>Plot method for neural networks</p></a></li>
<li><a href='#predict.nn'><p>Neural network prediction</p></a></li>
<li><a href='#prediction'><p>Summarizes the output of the neural network, the data and the fitted values</p>
of glm objects (if available)</a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Training of Neural Networks</td>
</tr>
<tr>
<td>Version:</td>
<td>1.44.2</td>
</tr>
<tr>
<td>Date:</td>
<td>2019-02-07</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.9.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>grid, MASS, grDevices, stats, utils, Deriv</td>
</tr>
<tr>
<td>Description:</td>
<td>Training of neural networks using backpropagation,
    resilient backpropagation with (Riedmiller, 1994) or without
    weight backtracking (Riedmiller and Braun, 1993) or the
    modified globally convergent version by Anastasiadis et al.
    (2005). The package allows flexible settings through
    custom-choice of error and activation function. Furthermore,
    the calculation of generalized weights (Intrator O &amp; Intrator
    N, 1993) is implemented.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/bips-hb/neuralnet">https://github.com/bips-hb/neuralnet</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/bips-hb/neuralnet/issues">https://github.com/bips-hb/neuralnet/issues</a></td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>6.1.0</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2019-02-07 21:47:31 UTC; wright</td>
</tr>
<tr>
<td>Author:</td>
<td>Stefan Fritsch [aut],
  Frauke Guenther [aut],
  Marvin N. Wright [aut, cre],
  Marc Suling [ctb],
  Sebastian M. Mueller [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Marvin N. Wright &lt;wright@leibniz-bips.de&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2019-02-07 22:20:07 UTC</td>
</tr>
</table>
<hr>
<h2 id='compute'>Deprecated function</h2><span id='topic+compute'></span>

<h3>Description</h3>

<p>The function <code>compute</code> is deprecated. Please refer to the new function <code><a href="#topic+predict.nn">predict.nn</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>compute(x, covariate, rep = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="compute_+3A_x">x</code></td>
<td>
<p>an object of class <code>nn</code>.</p>
</td></tr>
<tr><td><code id="compute_+3A_covariate">covariate</code></td>
<td>
<p>a dataframe or matrix containing the variables that had
been used to train the neural network.</p>
</td></tr>
<tr><td><code id="compute_+3A_rep">rep</code></td>
<td>
<p>an integer indicating the neural network's repetition which
should be used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>compute</code> returns a list containing the following components:
</p>
<table>
<tr><td><code>neurons</code></td>
<td>
<p>a list of the neurons' output for each layer of the neural
network.</p>
</td></tr> <tr><td><code>net.result</code></td>
<td>
<p>a matrix containing the overall result of the
neural network.</p>
</td></tr>
</table>

<hr>
<h2 id='confidence.interval'>Calculates confidence intervals of the weights</h2><span id='topic+confidence.interval'></span>

<h3>Description</h3>

<p><code>confidence.interval</code>, a method for objects of class <code>nn</code>,
typically produced by <code>neuralnet</code>.  Calculates confidence intervals of
the weights (White, 1989) and the network information criteria NIC (Murata
et al. 1994). All confidence intervals are calculated under the assumption
of a local identification of the given neural network.  If this assumption
is violated, the results will not be reasonable. Please make also sure that
the chosen error function equals the negative log-likelihood function,
otherwise the results are not meaningfull, too.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>confidence.interval(x, alpha = 0.05)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="confidence.interval_+3A_x">x</code></td>
<td>
<p>neural network</p>
</td></tr>
<tr><td><code id="confidence.interval_+3A_alpha">alpha</code></td>
<td>
<p>numerical. Sets the confidence level to (1-alpha).</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>confidence.interval</code> returns a list containing the following
components:
</p>
<table>
<tr><td><code>lower.ci</code></td>
<td>
<p>a list containing the lower confidence bounds of all
weights of the neural network differentiated by the repetitions.</p>
</td></tr> <tr><td><code>upper.ci</code></td>
<td>
<p>a list containing the upper confidence bounds of all weights of
the neural network differentiated by the repetitions.</p>
</td></tr> <tr><td><code>nic</code></td>
<td>
<p>a vector
containg the information criteria NIC for every repetition.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Stefan Fritsch, Frauke Guenther <a href="mailto:guenther@leibniz-bips.de">guenther@leibniz-bips.de</a>
</p>


<h3>References</h3>

<p>White (1989) <em>Learning in artificial neural networks. A
statistical perspective.</em> Neural Computation (1), pages 425-464
</p>
<p>Murata et al. (1994) <em>Network information criterion - determining the
number of hidden units for an artificial neural network model.</em> IEEE
Transactions on Neural Networks 5 (6), pages 865-871
</p>


<h3>See Also</h3>

<p><code><a href="#topic+neuralnet">neuralnet</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
    data(infert, package="datasets")
    print(net.infert &lt;- neuralnet(case~parity+induced+spontaneous,  
                        infert, err.fct="ce", linear.output=FALSE))
    confidence.interval(net.infert)

</code></pre>

<hr>
<h2 id='gwplot'>Plot method for generalized weights</h2><span id='topic+gwplot'></span>

<h3>Description</h3>

<p><code>gwplot</code>, a method for objects of class <code>nn</code>, typically produced
by <code>neuralnet</code>.  Plots the generalized weights (Intrator and Intrator,
1993) for one specific covariate and one response variable.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gwplot(x, rep = NULL, max = NULL, min = NULL, file = NULL,
  selected.covariate = 1, selected.response = 1, highlight = FALSE,
  type = "p", col = "black", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gwplot_+3A_x">x</code></td>
<td>
<p>an object of class <code>nn</code></p>
</td></tr>
<tr><td><code id="gwplot_+3A_rep">rep</code></td>
<td>
<p>an integer indicating the repetition to plot. If rep=&quot;best&quot;, the
repetition with the smallest error will be plotted. If not stated all
repetitions will be plotted.</p>
</td></tr>
<tr><td><code id="gwplot_+3A_max">max</code></td>
<td>
<p>maximum of the y axis. In default, max is set to the highest
y-value.</p>
</td></tr>
<tr><td><code id="gwplot_+3A_min">min</code></td>
<td>
<p>minimum of the y axis. In default, min is set to the smallest
y-value.</p>
</td></tr>
<tr><td><code id="gwplot_+3A_file">file</code></td>
<td>
<p>a character string naming the plot to write to. If not stated,
the plot will not be saved.</p>
</td></tr>
<tr><td><code id="gwplot_+3A_selected.covariate">selected.covariate</code></td>
<td>
<p>either a string of the covariate's name or an
integer of the ordered covariates, indicating the reference covariate in the
generalized weights plot. Defaulting to the first covariate.</p>
</td></tr>
<tr><td><code id="gwplot_+3A_selected.response">selected.response</code></td>
<td>
<p>either a string of the response variable's name or
an integer of the ordered response variables, indicating the reference
response in the generalized weights plot. Defaulting to the first response
variable.</p>
</td></tr>
<tr><td><code id="gwplot_+3A_highlight">highlight</code></td>
<td>
<p>a logical value, indicating whether to highlight (red
color) the best repetition (smallest error). Only reasonable if rep=NULL.
Default is FALSE</p>
</td></tr>
<tr><td><code id="gwplot_+3A_type">type</code></td>
<td>
<p>a character indicating the type of plotting; actually any of the
types as in <code><a href="graphics.html#topic+plot.default">plot.default</a></code>.</p>
</td></tr>
<tr><td><code id="gwplot_+3A_col">col</code></td>
<td>
<p>a color of the generalized weights.</p>
</td></tr>
<tr><td><code id="gwplot_+3A_...">...</code></td>
<td>
<p>Arguments to be passed to methods, such as graphical parameters
(see <code><a href="graphics.html#topic+par">par</a></code>).</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Stefan Fritsch, Frauke Guenther <a href="mailto:guenther@leibniz-bips.de">guenther@leibniz-bips.de</a>
</p>


<h3>References</h3>

<p>Intrator O. and Intrator N. (1993) <em>Using Neural Nets for
Interpretation of Nonlinear Models.</em> Proceedings of the Statistical
Computing Section, 244-249 San Francisco: American Statistical Society
(eds.)
</p>


<h3>See Also</h3>

<p><code><a href="#topic+neuralnet">neuralnet</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(infert, package="datasets")
print(net.infert &lt;- neuralnet(case~parity+induced+spontaneous, infert, 
		                err.fct="ce", linear.output=FALSE, likelihood=TRUE))
gwplot(net.infert, selected.covariate="parity")
gwplot(net.infert, selected.covariate="induced")
gwplot(net.infert, selected.covariate="spontaneous")

</code></pre>

<hr>
<h2 id='neuralnet'>Training of neural networks</h2><span id='topic+neuralnet'></span><span id='topic+print.nn'></span>

<h3>Description</h3>

<p>Train neural networks using backpropagation,
resilient backpropagation (RPROP) with (Riedmiller, 1994) or without weight
backtracking (Riedmiller and Braun, 1993) or the modified globally
convergent version (GRPROP) by Anastasiadis et al. (2005). The function
allows flexible settings through custom-choice of error and activation
function. Furthermore, the calculation of generalized weights (Intrator O.
and Intrator N., 1993) is implemented.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>neuralnet(formula, data, hidden = 1, threshold = 0.01,
  stepmax = 1e+05, rep = 1, startweights = NULL,
  learningrate.limit = NULL, learningrate.factor = list(minus = 0.5,
  plus = 1.2), learningrate = NULL, lifesign = "none",
  lifesign.step = 1000, algorithm = "rprop+", err.fct = "sse",
  act.fct = "logistic", linear.output = TRUE, exclude = NULL,
  constant.weights = NULL, likelihood = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="neuralnet_+3A_formula">formula</code></td>
<td>
<p>a symbolic description of the model to be fitted.</p>
</td></tr>
<tr><td><code id="neuralnet_+3A_data">data</code></td>
<td>
<p>a data frame containing the variables specified in
<code>formula</code>.</p>
</td></tr>
<tr><td><code id="neuralnet_+3A_hidden">hidden</code></td>
<td>
<p>a vector of integers specifying the number of hidden neurons
(vertices) in each layer.</p>
</td></tr>
<tr><td><code id="neuralnet_+3A_threshold">threshold</code></td>
<td>
<p>a numeric value specifying the threshold for the partial
derivatives of the error function as stopping criteria.</p>
</td></tr>
<tr><td><code id="neuralnet_+3A_stepmax">stepmax</code></td>
<td>
<p>the maximum steps for the training of the neural network.
Reaching this maximum leads to a stop of the neural network's training
process.</p>
</td></tr>
<tr><td><code id="neuralnet_+3A_rep">rep</code></td>
<td>
<p>the number of repetitions for the neural network's training.</p>
</td></tr>
<tr><td><code id="neuralnet_+3A_startweights">startweights</code></td>
<td>
<p>a vector containing starting values for the weights. 
Set to <code>NULL</code> for random initialization.</p>
</td></tr>
<tr><td><code id="neuralnet_+3A_learningrate.limit">learningrate.limit</code></td>
<td>
<p>a vector or a list containing the lowest and
highest limit for the learning rate. Used only for RPROP and GRPROP.</p>
</td></tr>
<tr><td><code id="neuralnet_+3A_learningrate.factor">learningrate.factor</code></td>
<td>
<p>a vector or a list containing the multiplication
factors for the upper and lower learning rate. Used only for RPROP and
GRPROP.</p>
</td></tr>
<tr><td><code id="neuralnet_+3A_learningrate">learningrate</code></td>
<td>
<p>a numeric value specifying the learning rate used by
traditional backpropagation. Used only for traditional backpropagation.</p>
</td></tr>
<tr><td><code id="neuralnet_+3A_lifesign">lifesign</code></td>
<td>
<p>a string specifying how much the function will print during
the calculation of the neural network. 'none', 'minimal' or 'full'.</p>
</td></tr>
<tr><td><code id="neuralnet_+3A_lifesign.step">lifesign.step</code></td>
<td>
<p>an integer specifying the stepsize to print the minimal
threshold in full lifesign mode.</p>
</td></tr>
<tr><td><code id="neuralnet_+3A_algorithm">algorithm</code></td>
<td>
<p>a string containing the algorithm type to calculate the
neural network. The following types are possible: 'backprop', 'rprop+',
'rprop-', 'sag', or 'slr'. 'backprop' refers to backpropagation, 'rprop+'
and 'rprop-' refer to the resilient backpropagation with and without weight
backtracking, while 'sag' and 'slr' induce the usage of the modified
globally convergent algorithm (grprop). See Details for more information.</p>
</td></tr>
<tr><td><code id="neuralnet_+3A_err.fct">err.fct</code></td>
<td>
<p>a differentiable function that is used for the calculation of
the error. Alternatively, the strings 'sse' and 'ce' which stand for the sum
of squared errors and the cross-entropy can be used.</p>
</td></tr>
<tr><td><code id="neuralnet_+3A_act.fct">act.fct</code></td>
<td>
<p>a differentiable function that is used for smoothing the
result of the cross product of the covariate or neurons and the weights.
Additionally the strings, 'logistic' and 'tanh' are possible for the
logistic function and tangent hyperbolicus.</p>
</td></tr>
<tr><td><code id="neuralnet_+3A_linear.output">linear.output</code></td>
<td>
<p>logical. If act.fct should not be applied to the output
neurons set linear output to TRUE, otherwise to FALSE.</p>
</td></tr>
<tr><td><code id="neuralnet_+3A_exclude">exclude</code></td>
<td>
<p>a vector or a matrix specifying the weights, that are
excluded from the calculation. If given as a vector, the exact positions of
the weights must be known. A matrix with n-rows and 3 columns will exclude n
weights, where the first column stands for the layer, the second column for
the input neuron and the third column for the output neuron of the weight.</p>
</td></tr>
<tr><td><code id="neuralnet_+3A_constant.weights">constant.weights</code></td>
<td>
<p>a vector specifying the values of the weights that
are excluded from the training process and treated as fix.</p>
</td></tr>
<tr><td><code id="neuralnet_+3A_likelihood">likelihood</code></td>
<td>
<p>logical. If the error function is equal to the negative
log-likelihood function, the information criteria AIC and BIC will be
calculated. Furthermore the usage of confidence.interval is meaningfull.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The globally convergent algorithm is based on the resilient backpropagation
without weight backtracking and additionally modifies one learning rate,
either the learningrate associated with the smallest absolute gradient (sag)
or the smallest learningrate (slr) itself. The learning rates in the grprop
algorithm are limited to the boundaries defined in learningrate.limit.
</p>


<h3>Value</h3>

<p><code>neuralnet</code> returns an object of class <code>nn</code>.  An object of
class <code>nn</code> is a list containing at most the following components:
</p>
<table>
<tr><td><code>call</code></td>
<td>
<p> the matched call. </p>
</td></tr> 
<tr><td><code>response</code></td>
<td>
<p> extracted from the <code>data argument</code>.  </p>
</td></tr> 
<tr><td><code>covariate</code></td>
<td>
<p> the variables extracted from the <code>data argument</code>. </p>
</td></tr> 
<tr><td><code>model.list</code></td>
<td>
<p> a list containing the covariates and the response variables extracted from the <code>formula argument</code>. </p>
</td></tr> 
<tr><td><code>err.fct</code></td>
<td>
<p> the error function. </p>
</td></tr> 
<tr><td><code>act.fct</code></td>
<td>
<p> the activation function. </p>
</td></tr> 
<tr><td><code>data</code></td>
<td>
<p> the <code>data argument</code>.</p>
</td></tr> 
<tr><td><code>net.result</code></td>
<td>
<p> a list containing the overall result of the neural network for every repetition.</p>
</td></tr> 
<tr><td><code>weights</code></td>
<td>
<p> a list containing the fitted weights of the neural network for every repetition. </p>
</td></tr> 
<tr><td><code>generalized.weights</code></td>
<td>
<p> a list containing the generalized weights of the neural network for every repetition. </p>
</td></tr> 
<tr><td><code>result.matrix</code></td>
<td>
<p> a matrix containing the reached threshold, needed steps, error, AIC and BIC (if computed) and weights for every repetition. Each column represents one repetition. </p>
</td></tr> 
<tr><td><code>startweights</code></td>
<td>
<p> a list containing the startweights of the neural network for every repetition. </p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Stefan Fritsch, Frauke Guenther, Marvin N. Wright
</p>


<h3>References</h3>

<p>Riedmiller M. (1994) <em>Rprop - Description and
Implementation Details.</em> Technical Report. University of Karlsruhe.
</p>
<p>Riedmiller M. and Braun H. (1993) <em>A direct adaptive method for faster
backpropagation learning: The RPROP algorithm.</em> Proceedings of the IEEE
International Conference on Neural Networks (ICNN), pages 586-591.  San
Francisco.
</p>
<p>Anastasiadis A. et. al. (2005) <em>New globally convergent training scheme
based on the resilient propagation algorithm.</em> Neurocomputing 64, pages
253-270.
</p>
<p>Intrator O. and Intrator N. (1993) <em>Using Neural Nets for
Interpretation of Nonlinear Models.</em> Proceedings of the Statistical
Computing Section, 244-249 San Francisco: American Statistical Society
(eds).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+plot.nn">plot.nn</a></code> for plotting the neural network.
</p>
<p><code><a href="#topic+gwplot">gwplot</a></code> for plotting the generalized weights.
</p>
<p><code><a href="#topic+predict.nn">predict.nn</a></code> for computation of a given neural network for given
covariate vectors (formerly <code>compute</code>).
</p>
<p><code><a href="#topic+confidence.interval">confidence.interval</a></code> for calculation of confidence intervals of
the weights.
</p>
<p><code><a href="#topic+prediction">prediction</a></code> for a summary of the output of the neural network.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(neuralnet)

# Binary classification
nn &lt;- neuralnet(Species == "setosa" ~ Petal.Length + Petal.Width, iris, linear.output = FALSE)
## Not run: print(nn)
## Not run: plot(nn)

# Multiclass classification
nn &lt;- neuralnet(Species ~ Petal.Length + Petal.Width, iris, linear.output = FALSE)
## Not run: print(nn)
## Not run: plot(nn)

# Custom activation function
softplus &lt;- function(x) log(1 + exp(x))
nn &lt;- neuralnet((Species == "setosa") ~ Petal.Length + Petal.Width, iris, 
                linear.output = FALSE, hidden = c(3, 2), act.fct = softplus)
## Not run: print(nn)
## Not run: plot(nn)

</code></pre>

<hr>
<h2 id='neuralnet-package'>Training of Neural Networks</h2><span id='topic+neuralnet-package'></span>

<h3>Description</h3>

<p>Training of neural networks using the backpropagation, resilient
backpropagation with (Riedmiller, 1994) or without weight backtracking
(Riedmiller, 1993) or the modified globally convergent version by
Anastasiadis et al. (2005). The package allows flexible settings through
custom-choice of error and activation function. Furthermore, the calculation
of generalized weights (Intrator O &amp; Intrator N, 1993) is implemented.
</p>


<h3>Note</h3>

<p>This work has been supported by the German Research Foundation<br />
(DFG: <a href="http://www.dfg.de">http://www.dfg.de</a>) under grant scheme PI 345/3-1.
</p>


<h3>Author(s)</h3>

<p>Stefan Fritsch, Frauke Guenther <a href="mailto:guenther@leibniz-bips.de">guenther@leibniz-bips.de</a>,
</p>
<p>Maintainer: Frauke Guenther <a href="mailto:guenther@leibniz-bips.de">guenther@leibniz-bips.de</a>
</p>


<h3>References</h3>

<p>Riedmiller M. (1994) <em>Rprop - Description and
Implementation Details.</em> Technical Report. University of Karlsruhe.
</p>
<p>Riedmiller M. and Braun H. (1993) <em>A direct adaptive method for faster
backpropagation learning: The RPROP algorithm.</em> Proceedings of the IEEE
International Conference on Neural Networks (ICNN), pages 586-591.  San
Francisco.
</p>
<p>Anastasiadis A. et. al. (2005) <em>New globally convergent training scheme
based on the resilient propagation algorithm.</em> Neurocomputing 64, pages
253-270.
</p>
<p>Intrator O. and Intrator N. (1993) <em>Using Neural Nets for
Interpretation of Nonlinear Models.</em> Proceedings of the Statistical
Computing Section, 244-249 San Francisco: American Statistical Society
(eds).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+plot.nn">plot.nn</a></code> for plotting of the neural network.
</p>
<p><code><a href="#topic+gwplot">gwplot</a></code> for plotting of the generalized weights.
</p>
<p><code><a href="#topic+compute">compute</a></code> for computation of the calculated network.
</p>
<p><code><a href="#topic+confidence.interval">confidence.interval</a></code> for calculation of a confidence interval
for the weights.
</p>
<p><code><a href="#topic+prediction">prediction</a></code> for calculation of a prediction.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
AND &lt;- c(rep(0,7),1)
OR &lt;- c(0,rep(1,7))
binary.data &lt;- data.frame(expand.grid(c(0,1), c(0,1), c(0,1)), AND, OR)
print(net &lt;- neuralnet(AND+OR~Var1+Var2+Var3,  binary.data, hidden=0, 
             rep=10, err.fct="ce", linear.output=FALSE))

XOR &lt;- c(0,1,1,0)
xor.data &lt;- data.frame(expand.grid(c(0,1), c(0,1)), XOR)
print(net.xor &lt;- neuralnet(XOR~Var1+Var2, xor.data, hidden=2, rep=5))
plot(net.xor, rep="best")

data(infert, package="datasets")
print(net.infert &lt;- neuralnet(case~parity+induced+spontaneous,  infert, 
                    err.fct="ce", linear.output=FALSE, likelihood=TRUE))
gwplot(net.infert, selected.covariate="parity")
gwplot(net.infert, selected.covariate="induced")
gwplot(net.infert, selected.covariate="spontaneous")
confidence.interval(net.infert)

Var1 &lt;- runif(50, 0, 100) 
sqrt.data &lt;- data.frame(Var1, Sqrt=sqrt(Var1))
print(net.sqrt &lt;- neuralnet(Sqrt~Var1, sqrt.data, hidden=10, 
                  threshold=0.01))
predict(net.sqrt, data.frame(Var1 = (1:10)^2))

Var1 &lt;- rpois(100,0.5)
Var2 &lt;- rbinom(100,2,0.6)
Var3 &lt;- rbinom(100,1,0.5)
SUM &lt;- as.integer(abs(Var1+Var2+Var3+(rnorm(100))))
sum.data &lt;- data.frame(Var1,Var2,Var3, SUM)
print(net.sum &lt;- neuralnet(SUM~Var1+Var2+Var3, sum.data, hidden=1, 
                 act.fct="tanh"))
prediction(net.sum)

</code></pre>

<hr>
<h2 id='plot.nn'>Plot method for neural networks</h2><span id='topic+plot.nn'></span>

<h3>Description</h3>

<p><code>plot.nn</code>, a method for the <code>plot</code> generic. It is designed for an
inspection of the weights for objects of class <code>nn</code>, typically produced
by <code>neuralnet</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'nn'
plot(x, rep = NULL, x.entry = NULL, x.out = NULL,
  radius = 0.15, arrow.length = 0.2, intercept = TRUE,
  intercept.factor = 0.4, information = TRUE, information.pos = 0.1,
  col.entry.synapse = "black", col.entry = "black",
  col.hidden = "black", col.hidden.synapse = "black",
  col.out = "black", col.out.synapse = "black",
  col.intercept = "blue", fontsize = 12, dimension = 6,
  show.weights = TRUE, file = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.nn_+3A_x">x</code></td>
<td>
<p>an object of class <code>nn</code></p>
</td></tr>
<tr><td><code id="plot.nn_+3A_rep">rep</code></td>
<td>
<p>repetition of the neural network. If rep=&quot;best&quot;, the repetition
with the smallest error will be plotted. If not stated all repetitions will
be plotted, each in a separate window.</p>
</td></tr>
<tr><td><code id="plot.nn_+3A_x.entry">x.entry</code></td>
<td>
<p>x-coordinate of the entry layer. Depends on the arrow.length
in default.</p>
</td></tr>
<tr><td><code id="plot.nn_+3A_x.out">x.out</code></td>
<td>
<p>x-coordinate of the output layer.</p>
</td></tr>
<tr><td><code id="plot.nn_+3A_radius">radius</code></td>
<td>
<p>radius of the neurons.</p>
</td></tr>
<tr><td><code id="plot.nn_+3A_arrow.length">arrow.length</code></td>
<td>
<p>length of the entry and out arrows.</p>
</td></tr>
<tr><td><code id="plot.nn_+3A_intercept">intercept</code></td>
<td>
<p>a logical value indicating whether to plot the intercept.</p>
</td></tr>
<tr><td><code id="plot.nn_+3A_intercept.factor">intercept.factor</code></td>
<td>
<p>x-position factor of the intercept. The closer the
factor is to 0, the closer the intercept is to its left neuron.</p>
</td></tr>
<tr><td><code id="plot.nn_+3A_information">information</code></td>
<td>
<p>a logical value indicating whether to add the error and
steps to the plot.</p>
</td></tr>
<tr><td><code id="plot.nn_+3A_information.pos">information.pos</code></td>
<td>
<p>y-position of the information.</p>
</td></tr>
<tr><td><code id="plot.nn_+3A_col.entry.synapse">col.entry.synapse</code></td>
<td>
<p>color of the synapses leading to the input neurons.</p>
</td></tr>
<tr><td><code id="plot.nn_+3A_col.entry">col.entry</code></td>
<td>
<p>color of the input neurons.</p>
</td></tr>
<tr><td><code id="plot.nn_+3A_col.hidden">col.hidden</code></td>
<td>
<p>color of the neurons in the hidden layer.</p>
</td></tr>
<tr><td><code id="plot.nn_+3A_col.hidden.synapse">col.hidden.synapse</code></td>
<td>
<p>color of the weighted synapses.</p>
</td></tr>
<tr><td><code id="plot.nn_+3A_col.out">col.out</code></td>
<td>
<p>color of the output neurons.</p>
</td></tr>
<tr><td><code id="plot.nn_+3A_col.out.synapse">col.out.synapse</code></td>
<td>
<p>color of the synapses leading away from the output
neurons.</p>
</td></tr>
<tr><td><code id="plot.nn_+3A_col.intercept">col.intercept</code></td>
<td>
<p>color of the intercept.</p>
</td></tr>
<tr><td><code id="plot.nn_+3A_fontsize">fontsize</code></td>
<td>
<p>fontsize of the text.</p>
</td></tr>
<tr><td><code id="plot.nn_+3A_dimension">dimension</code></td>
<td>
<p>size of the plot in inches.</p>
</td></tr>
<tr><td><code id="plot.nn_+3A_show.weights">show.weights</code></td>
<td>
<p>a logical value indicating whether to print the
calculated weights above the synapses.</p>
</td></tr>
<tr><td><code id="plot.nn_+3A_file">file</code></td>
<td>
<p>a character string naming the plot to write to. If not stated,
the plot will not be saved.</p>
</td></tr>
<tr><td><code id="plot.nn_+3A_...">...</code></td>
<td>
<p>arguments to be passed to methods, such as graphical parameters
(see <code><a href="graphics.html#topic+par">par</a></code>).</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Stefan Fritsch, Frauke Guenther <a href="mailto:guenther@leibniz-bips.de">guenther@leibniz-bips.de</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+neuralnet">neuralnet</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
XOR &lt;- c(0,1,1,0)
xor.data &lt;- data.frame(expand.grid(c(0,1), c(0,1)), XOR)
print(net.xor &lt;- neuralnet( XOR~Var1+Var2, xor.data, hidden=2, rep=5))
plot(net.xor, rep="best")

</code></pre>

<hr>
<h2 id='predict.nn'>Neural network prediction</h2><span id='topic+predict.nn'></span>

<h3>Description</h3>

<p>Prediction of artificial neural network of class <code>nn</code>, produced by <code>neuralnet()</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'nn'
predict(object, newdata, rep = 1, all.units = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.nn_+3A_object">object</code></td>
<td>
<p>Neural network of class <code>nn</code>.</p>
</td></tr>
<tr><td><code id="predict.nn_+3A_newdata">newdata</code></td>
<td>
<p>New data of class <code>data.frame</code> or <code>matrix</code>.</p>
</td></tr>
<tr><td><code id="predict.nn_+3A_rep">rep</code></td>
<td>
<p>Integer indicating the neural network's repetition which should be used.</p>
</td></tr>
<tr><td><code id="predict.nn_+3A_all.units">all.units</code></td>
<td>
<p>Return output for all units instead of final output only.</p>
</td></tr>
<tr><td><code id="predict.nn_+3A_...">...</code></td>
<td>
<p>further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Matrix of predictions. Each column represents one output unit. 
If <code>all.units=TRUE</code>, a list of matrices with output for each unit.
</p>


<h3>Author(s)</h3>

<p>Marvin N. Wright
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(neuralnet)

# Split data
train_idx &lt;- sample(nrow(iris), 2/3 * nrow(iris))
iris_train &lt;- iris[train_idx, ]
iris_test &lt;- iris[-train_idx, ]

# Binary classification
nn &lt;- neuralnet(Species == "setosa" ~ Petal.Length + Petal.Width, iris_train, linear.output = FALSE)
pred &lt;- predict(nn, iris_test)
table(iris_test$Species == "setosa", pred[, 1] &gt; 0.5)

# Multiclass classification
nn &lt;- neuralnet((Species == "setosa") + (Species == "versicolor") + (Species == "virginica")
                 ~ Petal.Length + Petal.Width, iris_train, linear.output = FALSE)
pred &lt;- predict(nn, iris_test)
table(iris_test$Species, apply(pred, 1, which.max))

</code></pre>

<hr>
<h2 id='prediction'>Summarizes the output of the neural network, the data and the fitted values
of glm objects (if available)</h2><span id='topic+prediction'></span>

<h3>Description</h3>

<p><code>prediction</code>, a method for objects of class <code>nn</code>, typically
produced by <code>neuralnet</code>.  In a first step, the dataframe will be
amended by a mean response, the mean of all responses corresponding to the
same covariate-vector.  The calculated data.error is the error function
between the original response and the new mean response.  In a second step,
all duplicate rows will be erased to get a quick overview of the data.  To
obtain an overview of the results of the neural network and the glm objects,
the covariate matrix will be bound to the output of the neural network and
the fitted values of the glm object(if available) and will be reduced by all
duplicate rows.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prediction(x, list.glm = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="prediction_+3A_x">x</code></td>
<td>
<p>neural network</p>
</td></tr>
<tr><td><code id="prediction_+3A_list.glm">list.glm</code></td>
<td>
<p>an optional list of glm objects</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list of the summaries of the repetitions of the neural networks,
the data and the glm objects (if available).
</p>


<h3>Author(s)</h3>

<p>Stefan Fritsch, Frauke Guenther <a href="mailto:guenther@leibniz-bips.de">guenther@leibniz-bips.de</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+neuralnet">neuralnet</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
Var1 &lt;- rpois(100,0.5)
Var2 &lt;- rbinom(100,2,0.6)
Var3 &lt;- rbinom(100,1,0.5)
SUM &lt;- as.integer(abs(Var1+Var2+Var3+(rnorm(100))))
sum.data &lt;- data.frame(Var1,Var2,Var3, SUM)
print(net.sum &lt;- neuralnet( SUM~Var1+Var2+Var3,  sum.data, hidden=1, 
                 act.fct="tanh"))
main &lt;- glm(SUM~Var1+Var2+Var3, sum.data, family=poisson())
full &lt;- glm(SUM~Var1*Var2*Var3, sum.data, family=poisson())
prediction(net.sum, list.glm=list(main=main, full=full))

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
