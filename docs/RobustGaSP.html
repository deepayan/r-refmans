<!DOCTYPE html><html><head><title>Help for package RobustGaSP</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {RobustGaSP}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#RobustGaSP-package'>
<p>Robust Gaussian Stochastic Process Emulation</p></a></li>
<li><a href='#as.S3prediction'><p> Convert a <code>rgasp</code> or <code>ppgasp</code> <code>S4 object</code> prediction into a <code>S3 object</code></p></a></li>
<li><a href='#as.S4prediction'><p> Convert a <code>RobustGaSP</code> <code>S3 object</code> prediction into a <code>S4 object</code></p></a></li>
<li><a href='#Borehole'><p> Borehole function</p></a></li>
<li><a href='#construct_ppgasp'><p>PP GaSP constructor after estimating the parameters</p></a></li>
<li><a href='#construct_rgasp'><p>Robust GaSP constructor after estimating the parameters</p></a></li>
<li><a href='#dettepepel.3.data'><p>Dette &amp; Pepelyshev (2010) Curved Function</p></a></li>
<li><a href='#environ.4.data'><p> Environmental model function</p></a></li>
<li><a href='#euclidean_distance'><p>Euclidean distance matrix between two input matrices</p></a></li>
<li><a href='#findInertInputs'>
<p>find inert inputs with the posterior mode</p></a></li>
<li><a href='#friedman.5.data'><p>Friedman Function</p></a></li>
<li><a href='#generate_predictive_mean_cov'>
<p>A C++ function to generate predictive mean and cholesky decomposition of the scaled covariance function.</p></a></li>
<li><a href='#higdon.1.data'>
<p>Higdon (2002) Function</p></a></li>
<li><a href='#humanity_model'>
<p>data from the humanity model</p></a></li>
<li><a href='#leave_one_out_rgasp'>
<p>leave-one-out fitted values and standard deviation for robust GaSP model</p></a></li>
<li><a href='#limetal.2.data'><p> Lim et at. (2002) nonpolynomial function</p></a></li>
<li><a href='#log_approx_ref_prior'>
<p>The natural logarithm of the jointly robust prior (up to a normalizing constant)</p></a></li>
<li><a href='#log_approx_ref_prior_deriv'>
<p>Derivative of the jointly robust prior</p></a></li>
<li><a href='#log_marginal_lik'>
<p>Natural logarithm of marginal likelihood of the robust GaSP model</p></a></li>
<li><a href='#log_marginal_lik_deriv'>
<p>Derivative of natural logarithm of the marginal likelihood</p></a></li>
<li><a href='#log_marginal_lik_deriv_ppgasp'>
<p>Derivative of natural logarithm of the marginal likelihood</p></a></li>
<li><a href='#log_marginal_lik_ppgasp'>
<p>Natural logarithm of marginal likelihood of the PP GaSP model</p></a></li>
<li><a href='#log_profile_lik'>
<p>Natural logarithm of profile likelihood of the robust GaSP model</p></a></li>
<li><a href='#log_profile_lik_deriv'>
<p>Derivative of natural logarithm of the profile likelihood</p></a></li>
<li><a href='#log_profile_lik_deriv_ppgasp'>
<p>Derivative of natural logarithm of the profile likelihood</p></a></li>
<li><a href='#log_profile_lik_ppgasp'>
<p>Natural logarithm of profile likelihood of the PP GaSP model</p></a></li>
<li><a href='#log_ref_marginal_post'>
<p>Natural logarithm of reference marginal posterior density of the robust GaSP model</p></a></li>
<li><a href='#log_ref_marginal_post_ppgasp'>
<p>Natural logarithm of reference marginal posterior density of the PP GaSP model</p></a></li>
<li><a href='#matern_3_2_deriv'>
<p>The derivative of matern correlation function (roughness parameter equal to 3/2) with regard to the inverse range parameter</p></a></li>
<li><a href='#matern_3_2_funct'>
<p>Matern correlation function with roughness parameter equal to 3/2.</p></a></li>
<li><a href='#matern_5_2_deriv'>
<p>The derivative of matern correlation function with regard to the inverse range parameter</p></a></li>
<li><a href='#matern_5_2_funct'>
<p>Matern correlation function with roughness parameter equal to 5/2.</p></a></li>
<li><a href='#neg_log_marginal_post_approx_ref'><p> Natural logarithm of approximate reference marginal posterior density of the robust GaSP model</p></a></li>
<li><a href='#neg_log_marginal_post_approx_ref_deriv'>
<p>Derivative of negative natural logarithm of approximate reference marginal posterior density</p></a></li>
<li><a href='#neg_log_marginal_post_approx_ref_deriv_ppgasp'>
<p>Derivative of negative natural logarithm of approximate reference marginal posterior density of the PP GaSP model</p></a></li>
<li><a href='#neg_log_marginal_post_approx_ref_ppgasp'><p>Natural logarithm of approximate reference marginal posterior density of the PP GaSP model</p></a></li>
<li><a href='#neg_log_marginal_post_ref'>
<p>Negative natural logarithm of reference marginal posterior density of the robust GaSP model with regard to a specific parameterization.</p></a></li>
<li><a href='#neg_log_marginal_post_ref_ppgasp'>
<p>Negative natural logarithm of reference marginal posterior density of the PP GaSP model with regard to a specific parameterization.</p></a></li>
<li><a href='#periodic_exp_deriv'>
<p>The derivative of periodic exponential correlation function with regard to the inverse range parameter</p></a></li>
<li><a href='#periodic_exp_funct'>
<p>Periodic folding of the exponential correlation function .</p></a></li>
<li><a href='#periodic_gauss_deriv'>
<p>The derivative of periodic Gaussian correlation function with regard to the inverse range parameter</p></a></li>
<li><a href='#periodic_gauss_funct'>
<p>Periodic folding of the Gaussian correlation function .</p></a></li>
<li><a href='#plot'>
<p>Plot for Robust GaSP model</p></a></li>
<li><a href='#pow_exp_deriv'>
<p>The derivative of power exponential correlation function with regard to the inverse range parameter</p></a></li>
<li><a href='#pow_exp_funct'>
<p>Pow exponential correlation function.</p></a></li>
<li><a href='#ppgasp'><p> Setting up the parallel partial GaSP model</p></a></li>
<li><a href='#ppgasp-class'><p>PP GaSP class</p></a></li>
<li><a href='#pred_rgasp'>
<p>Prediction for robust GaSP model</p></a></li>
<li><a href='#predict.ppgasp'>
<p>Prediction for PP GaSP model</p></a></li>
<li><a href='#predict.rgasp'>
<p>Prediction for Robust GaSP model</p></a></li>
<li><a href='#predppgasp-class'><p> Predicted PP GaSP class</p></a></li>
<li><a href='#predrgasp-class'><p> Predictive robust GaSP class</p></a></li>
<li><a href='#rgasp'><p> Setting up the robust GaSP model</p></a></li>
<li><a href='#rgasp-class'><p> Robust GaSP class</p></a></li>
<li><a href='#Sample'>
<p>Sample for Robust GaSP model</p></a></li>
<li><a href='#search_LB_prob'>
<p>Search for the default lower bound of range parameters.</p></a></li>
<li><a href='#separable_kernel'><p>Product correlation matrix with the product form</p></a></li>
<li><a href='#separable_multi_kernel'><p>Product correlation matrix with the product form</p></a></li>
<li><a href='#show'>
<p>Show Robust GaSP object</p></a></li>
<li><a href='#show.ppgasp'>
<p>Show parllel partial Gaussian stochastic process (PP GaSP) object</p></a></li>
<li><a href='#simulate'>
<p>Sample for Robust GaSP model</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Robust Gaussian Stochastic Process Emulation</td>
</tr>
<tr>
<td>Version:</td>
<td>0.6.6</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-02-09 00:30:22 UTC</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;</td>
</tr>
<tr>
<td>Author:</td>
<td>Mengyang Gu [aut, cre],
  Jesus Palomo [aut],
  James Berger [aut]</td>
</tr>
<tr>
<td>Description:</td>
<td>Robust parameter estimation and prediction of Gaussian stochastic 
 process emulators. It allows for robust parameter estimation and prediction using 
 Gaussian stochastic process emulator. It also implements the parallel partial 
 Gaussian stochastic process emulator for computer model with massive outputs 
 See the reference: Mengyang Gu and Jim Berger, 2016, Annals of Applied Statistics; 
 Mengyang Gu, Xiaojing Wang and Jim Berger, 2018, Annals of Statistics. </td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0), methods</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp (&ge; 0.12.3), nloptr (&ge; 1.0.4)</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppEigen</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-02-08 19:55:04 UTC; mengyanggu</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>5.0.1</td>
</tr>
</table>
<hr>
<h2 id='RobustGaSP-package'>
Robust Gaussian Stochastic Process Emulation
</h2><span id='topic+RobustGaSP-package'></span><span id='topic+RobustGaSP'></span>

<h3>Description</h3>

<p>Robust parameter estimation and prediction of Gaussian stochastic 
 process emulators. It allows for robust parameter estimation and prediction using 
 Gaussian stochastic process emulator. It also implements the parallel partial 
 Gaussian stochastic process emulator for computer model with massive outputs 
 See the reference: Mengyang Gu and Jim Berger, 2016, Annals of Applied Statistics; 
 Mengyang Gu, Xiaojing Wang and Jim Berger, 2018, Annals of Statistics. 
</p>


<h3>Details</h3>

<p>The DESCRIPTION file:
</p>

<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> RobustGaSP</td>
</tr>
<tr>
 <td style="text-align: left;">
Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
Title: </td><td style="text-align: left;"> Robust Gaussian Stochastic Process Emulation</td>
</tr>
<tr>
 <td style="text-align: left;">
Version: </td><td style="text-align: left;"> 0.6.6</td>
</tr>
<tr>
 <td style="text-align: left;">
Date/Publication: </td><td style="text-align: left;"> 2024-01-14 08:10:03 UTC</td>
</tr>
<tr>
 <td style="text-align: left;">
Authors@R: </td><td style="text-align: left;"> c(person(given="Mengyang",family="Gu",role=c("aut","cre"),
    email="mengyang@pstat.ucsb.edu"), 
    person(given="Jesus",family="Palomo", role=c("aut"),
    email="jesus.palomo@urjc.es"),
    person(given="James",family="Berger", role="aut"))</td>
</tr>
<tr>
 <td style="text-align: left;">
Maintainer: </td><td style="text-align: left;"> Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;</td>
</tr>
<tr>
 <td style="text-align: left;">
Author: </td><td style="text-align: left;"> Mengyang Gu [aut, cre],
  Jesus Palomo [aut],
  James Berger [aut]</td>
</tr>
<tr>
 <td style="text-align: left;">
Description: </td><td style="text-align: left;"> Robust parameter estimation and prediction of Gaussian stochastic 
 process emulators. It allows for robust parameter estimation and prediction using 
 Gaussian stochastic process emulator. It also implements the parallel partial 
 Gaussian stochastic process emulator for computer model with massive outputs 
 See the reference: Mengyang Gu and Jim Berger, 2016, Annals of Applied Statistics; 
 Mengyang Gu, Xiaojing Wang and Jim Berger, 2018, Annals of Statistics. </td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> GPL-2 | GPL-3</td>
</tr>
<tr>
 <td style="text-align: left;">
LazyData: </td><td style="text-align: left;"> true</td>
</tr>
<tr>
 <td style="text-align: left;">
Depends: </td><td style="text-align: left;"> R (&gt;= 3.5.0), methods</td>
</tr>
<tr>
 <td style="text-align: left;">
Imports: </td><td style="text-align: left;"> Rcpp (&gt;= 0.12.3), nloptr (&gt;= 1.0.4)</td>
</tr>
<tr>
 <td style="text-align: left;">
LinkingTo: </td><td style="text-align: left;"> Rcpp, RcppEigen</td>
</tr>
<tr>
 <td style="text-align: left;">
NeedsCompilation: </td><td style="text-align: left;"> yes</td>
</tr>
<tr>
 <td style="text-align: left;">
Repository: </td><td style="text-align: left;"> CRAN</td>
</tr>
<tr>
 <td style="text-align: left;">
Packaged: </td><td style="text-align: left;"> 2019-06-05 02:09:17 UTC; gumengyang</td>
</tr>
<tr>
 <td style="text-align: left;">
RoxygenNote: </td><td style="text-align: left;"> 5.0.1</td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>


<p>Index of help topics:
</p>
<pre>
RobustGaSP-package      Robust Gaussian Stochastic Process Emulation
findInertInputs         find inert inputs with the posterior mode
humanity.X              data from the humanity model
leave_one_out_rgasp     leave-one-out fitted values and standard
                        deviation for robust GaSP model
plot                    Plot for Robust GaSP model
ppgasp                  Setting up the parallel partial GaSP model
ppgasp-class            PP GaSP class
predict.ppgasp          Prediction for PP GaSP model
predict.rgasp           Prediction for Robust GaSP model
predppgasp-class        Predicted PP GaSP class
predrgasp-class         Predictive robust GaSP class
rgasp                   Setting up the robust GaSP model
rgasp-class             Robust GaSP class
show                    Show Robust GaSP object
show.ppgasp             Show parllel partial Gaussian stochastic
                        process (PP GaSP) object
simulate                Sample for Robust GaSP model
</pre>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Jesus Palomo [aut],
  James Berger [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>

<p>J.O. Berger, V. De Oliveira and B. Sanso (2001), <em>Objective Bayesian analysis of spatially correlated data</em>, <em>Journal of the American Statistical Association</em>, 96, 1361-1374.
</p>
<p>M. Gu. and J.O. Berger (2016). Parallel partial Gaussian process emulation for computer models with massive output. <em>Annals of Applied Statistics</em>, 10(3), 1317-1347.
</p>
<p>M. Gu. (2016). Robust uncertainty quantification and scalable computation for computer models with massive output. Ph.D. thesis. Duke University.
</p>
<p>M. Gu, X. Wang and J.O. Berger (2018), <em>Robust Gaussian stochastic process emulation</em>, <em>Annals of Statistics</em>, 46(6A), 3038-3066.
</p>
<p>M. Gu (2018), <em>Jointly robust prior for Gaussian stochastic process in emulation, calibration and variable selection</em>, arXiv:1804.09329.
</p>
<p>R. Paulo (2005), <em>Default priors for Gaussian processes</em>, <em>Annals of statistics</em>, 33(2), 556-582.
</p>
<p>J. Sacks, W.J. Welch, T.J. Mitchell, and H.P. Wynn (1989), <em>Design and analysis of computer experiments</em>, <em>Statistical Science</em>, <b>4</b>, 409-435.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  #------------------------
  # a 3 dimensional example
  #------------------------
  # dimensional of the inputs
  dim_inputs &lt;- 3    
  # number of the inputs
  num_obs &lt;- 30       
  # uniform samples of design
  input &lt;- matrix(runif(num_obs*dim_inputs), num_obs,dim_inputs) 
  
  # Following codes use maximin Latin Hypercube Design, which is typically better than uniform
  # library(lhs)
  # input &lt;- maximinLHS(n=num_obs, k=dim_inputs)  ##maximin lhd sample
  
  ####
  # outputs from the 3 dim dettepepel.3.data function
  
  output = matrix(0,num_obs,1)
  for(i in 1:num_obs){
    output[i]&lt;-dettepepel.3.data(input[i,])
  }
  
  # use constant mean basis, with no constraint on optimization
  m1&lt;- rgasp(design = input, response = output, lower_bound=FALSE)
  
  # the following use constraints on optimization
  # m1&lt;- rgasp(design = input, response = output, lower_bound=TRUE)
  
  # the following use a single start on optimization
  # m1&lt;- rgasp(design = input, response = output, lower_bound=FALSE)
  
  # number of points to be predicted 
  num_testing_input &lt;- 5000    
  # generate points to be predicted
  testing_input &lt;- matrix(runif(num_testing_input*dim_inputs),num_testing_input,dim_inputs)
  # Perform prediction
  m1.predict&lt;-predict(m1, testing_input, outasS3 = FALSE)
  # Predictive mean
  #m1.predict@mean  
  
  # The following tests how good the prediction is 
  testing_output &lt;- matrix(0,num_testing_input,1)
  for(i in 1:num_testing_input){
    testing_output[i]&lt;-dettepepel.3.data(testing_input[i,])
  }
  
  # compute the MSE, average coverage and average length
  # out of sample MSE
  MSE_emulator &lt;- sum((m1.predict@mean-testing_output)^2)/(num_testing_input)  
  
  # proportion covered by 95% posterior predictive credible interval
  prop_emulator &lt;- length(which((m1.predict@lower95&lt;=testing_output)
                   &amp;(m1.predict@upper95&gt;=testing_output)))/num_testing_input
  
  # average length of  posterior predictive credible interval
  length_emulator &lt;- sum(m1.predict@upper95-m1.predict@lower95)/num_testing_input
  
  # output of prediction
  MSE_emulator
  prop_emulator
  length_emulator  
  # normalized RMSE
  sqrt(MSE_emulator/mean((testing_output-mean(output))^2 ))
</code></pre>

<hr>
<h2 id='as.S3prediction'> Convert a <code>rgasp</code> or <code>ppgasp</code> <code>S4 object</code> prediction into a <code>S3 object</code>
</h2><span id='topic+as.S3prediction'></span><span id='topic+as.S3prediction.predrgasp-class'></span><span id='topic+as.S3prediction+2Cpredrgasp-method'></span><span id='topic+as.S3prediction.predppgasp-class'></span><span id='topic+as.S3prediction+2Cpredppgasp-method'></span>

<h3>Description</h3>

<p>This function converts the default <code>S4 object</code> prediction into a <code>S3 object</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as.S3prediction(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="as.S3prediction_+3A_object">object</code></td>
<td>
<p> an object of class <code><a href="#topic+predrgasp-class">predrgasp-class</a></code> or <code><a href="#topic+ppgasp-class">ppgasp-class</a></code>  is converted into a <code>S3 object</code>
</p>
</td></tr>
<tr><td><code id="as.S3prediction_+3A_...">...</code></td>
<td>
<p> Extra arguments to be passed to the function (not implemented yet).
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The returned value is a <code>list</code> with 
</p>
<table>
<tr><td><code>mean</code></td>
<td>
<p> predictive mean for the testing inputs.</p>
</td></tr>
<tr><td><code>lower95</code></td>
<td>
<p>lower bound of the 95% posterior credible interval.</p>
</td></tr>
<tr><td><code>upper95</code></td>
<td>
<p>upper bound of the 95% posterior credible interval.</p>
</td></tr>
<tr><td><code>sd</code></td>
<td>
<p>standard deviation of each <code>testing_input</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Jesus Palomo [aut],
  James Berger [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  #------------------------
  # a 3 dimensional example
  #------------------------
  # dimensional of the inputs
  dim_inputs &lt;- 3    
  # number of the inputs
  num_obs &lt;- 30       
  # uniform samples of design
  input &lt;- matrix(runif(num_obs*dim_inputs), num_obs,dim_inputs) 
  
  # Following codes use maximin Latin Hypercube Design, which is typically better than uniform
  # library(lhs)
  # input &lt;- maximinLHS(n=num_obs, k=dim_inputs)  ##maximin lhd sample
  
  # outputs from the 3 dim dettepepel.3.data function
  
  output = matrix(0,num_obs,1)
  for(i in 1:num_obs){
    output[i]&lt;-dettepepel.3.data (input[i,])
  }
  
  # use constant mean basis, with no constraint on optimization
  m1&lt;- rgasp(design = input, response = output, lower_bound=FALSE)
  
  # the following use constraints on optimization
  # m1&lt;- rgasp(design = input, response = output, lower_bound=TRUE)
  
  # the following use a single start on optimization
  # m1&lt;- rgasp(design = input, response = output, lower_bound=FALSE, multiple_starts=FALSE)
  
  # number of points to be predicted 
  num_testing_input &lt;- 5000    
  # generate points to be predicted
  testing_input &lt;- matrix(runif(num_testing_input*dim_inputs),num_testing_input,dim_inputs)
  # Perform prediction
  m1.predict&lt;-predict(m1, testing_input, outasS3 = FALSE)
  
  # The returned object is of predrgasp-class
  str(m1.predict)
  # To have the prediction as a list
  m1.predict.aslist &lt;- as.S3prediction(m1.predict)
  str(m1.predict.aslist)
  
</code></pre>

<hr>
<h2 id='as.S4prediction'> Convert a <code>RobustGaSP</code> <code>S3 object</code> prediction into a <code>S4 object</code>
</h2><span id='topic+as.S4prediction'></span><span id='topic+as.S4prediction.predict'></span><span id='topic+as.S4prediction.predrgasp-class'></span><span id='topic+as.S4prediction+2Cpredrgasp-method'></span>

<h3>Description</h3>

<p>This function converts a <code>S3 object</code> prediction into the default <code>S4 object</code> prediction
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as.S4prediction(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="as.S4prediction_+3A_object">object</code></td>
<td>
<p>  an object of type list obtained by <code><a href="#topic+predict.rgasp">predict.rgasp</a></code> contains the prediction and it will be converted into a <code>S4 object</code>
</p>
</td></tr>
<tr><td><code id="as.S4prediction_+3A_...">...</code></td>
<td>
<p> Extra arguments to be passed to the function (not implemented yet).
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The returned value is a <code>S4 object</code> of class <code><a href="#topic+predrgasp-class">predrgasp-class</a></code> with 
</p>

<dl>
<dt><code>call</code>:</dt><dd> <p><code>call</code> to the function.</p>
</dd>
<dt><code>mean</code>:</dt><dd><p> predictive mean for the testing inputs.</p>
</dd>
<dt><code>lower95</code>:</dt><dd><p>lower bound of the 95% posterior credible interval.</p>
</dd>
<dt><code>upper95</code>:</dt><dd><p>upper bound of the 95% posterior credible interval.</p>
</dd>
<dt><code>sd</code>:</dt><dd><p>standard deviation of each <code>testing_input</code>.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Jesus Palomo [aut],
  James Berger [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  #------------------------
  # a 3 dimensional example
  #------------------------
  # dimensional of the inputs
  dim_inputs &lt;- 3    
  # number of the inputs
  num_obs &lt;- 30       
  # uniform samples of design
  input &lt;- matrix(runif(num_obs*dim_inputs), num_obs,dim_inputs) 
  
  # Following codes use maximin Latin Hypercube Design, which is typically better than uniform
  # library(lhs)
  # input &lt;- maximinLHS(n=num_obs, k=dim_inputs)  ##maximin lhd sample
  
  # outputs from the 3 dim dettepepel.3.data function
  
  output = matrix(0,num_obs,1)
  for(i in 1:num_obs){
    output[i]&lt;-dettepepel.3.data (input[i,])
  }
  
  # use constant mean basis, with no constraint on optimization
  m1&lt;- rgasp(design = input, response = output, lower_bound=FALSE)
  
  # the following use constraints on optimization
  # m1&lt;- rgasp(design = input, response = output, lower_bound=TRUE)
  
  # the following use a single start on optimization
  # m1&lt;- rgasp(design = input, response = output, lower_bound=FALSE, multiple_starts=FALSE)
  
  # number of points to be predicted 
  num_testing_input &lt;- 5000    
  # generate points to be predicted
  testing_input &lt;- matrix(runif(num_testing_input*dim_inputs),num_testing_input,dim_inputs)
  # Perform prediction
  m1.predict&lt;-predict(m1, testing_input, outasS3 = FALSE)
  # Notice the call slot of the object
  print(m1.predict@call)
  
  # To convert the prediction to a S3 object 
  m1.predict.aslist &lt;- as.S3prediction(m1.predict)
  # To recover back the prediction as a predrgasp-class object
  m1.predict.aspredgasp &lt;- as.S4prediction.predict(m1.predict.aslist)
  str(m1.predict.aslist)
  # Notice that in this case the @call slot is different than the initial
  print(m1.predict.aspredgasp@call)
  
</code></pre>

<hr>
<h2 id='Borehole'> Borehole function
</h2><span id='topic+borehole'></span><span id='topic+Borehole'></span>

<h3>Description</h3>

<p>It is an 8-dimensional test function that models water flow through a borehole. Its simplicity and quick evaluation makes it a commonly used function for testing a wide variety of methods in computer experiments. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>borehole(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Borehole_+3A_x">x</code></td>
<td>
<p> an 8-dimensional vector specifying the location where the function is to be evaluated. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>For more details, see Worley, B. A. (1987). Deterministic uncertainty analysis (No. CONF-871101-30). Oak Ridge National Lab., TN (USA).
</p>


<h3>Value</h3>

<p>The response is given by
</p>
<p style="text-align: center;"><code class="reqn">
f(x) = \frac{2\pi T_u (H_u - H_l)}{\ln(\frac{r}{r_w})\Big(1+\frac{2LT_u}{\ln(\frac{r}{r_w})r^2_w K_w}+\frac{T_u}{T_l}\Big)}</code>
</p>

<p>where <code>x</code> is an 8-dimensional vector with <code>rw &lt;- x[1]</code>, <code>r  &lt;- x[2]</code>, <code>Tu &lt;- x[3]</code>, <code>Hu &lt;- x[4]</code>, <code>Tl &lt;- x[5]</code>, <code>Hl &lt;- x[6]</code>, <code>L  &lt;- x[7]</code>, <code>Kw &lt;- x[8]</code>.
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Jesus Palomo [aut],
  James Berger [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>

<p>Harper, W. V., &amp; Gupta, S. K. (1983). Sensitivity/uncertainty analysis of a borehole scenario comparing Latin Hypercube Sampling and deterministic sensitivity approaches (No. BMI/ONWI-516). Battelle Memorial Inst., Columbus, OH (USA). Office of Nuclear Waste Isolation.
</p>
<p>Worley, B. A. (1987). Deterministic uncertainty analysis (No. CONF-871101-30). Oak Ridge National Lab., TN (USA).
</p>
<p>S. Surjanovic, D. Bingham, Virtual Library of Simulation Experiments:  Test Functions
and Datasets, retrieved March 29, 2016, from <a href="http://www.sfu.ca/~ssurjano/borehole.html">http://www.sfu.ca/~ssurjano/borehole.html</a>.
</p>

<hr>
<h2 id='construct_ppgasp'>PP GaSP constructor after estimating the parameters</h2><span id='topic+construct_ppgasp'></span>

<h3>Description</h3>

<p>This function constructs the PP GaSP model if the range and noise-variance ratio parameters are given or have been estimated.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>construct_ppgasp(beta, nu, R0, X, zero_mean, output, kernel_type, alpha)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="construct_ppgasp_+3A_beta">beta</code></td>
<td>

<p>inverse-range parameters.
</p>
</td></tr>
<tr><td><code id="construct_ppgasp_+3A_nu">nu</code></td>
<td>

<p>noise-variance ratio parameter.
</p>
</td></tr>
<tr><td><code id="construct_ppgasp_+3A_r0">R0</code></td>
<td>

<p>A List of matrix where the j-th matrix is an absolute difference matrix of the j-th input vector.
</p>
</td></tr>
<tr><td><code id="construct_ppgasp_+3A_x">X</code></td>
<td>

<p>The mean basis function i.e. the trend function.
</p>
</td></tr>
<tr><td><code id="construct_ppgasp_+3A_zero_mean">zero_mean</code></td>
<td>

<p>The mean basis function is zero or not.
</p>
</td></tr>
<tr><td><code id="construct_ppgasp_+3A_output">output</code></td>
<td>

<p>the output matrix.
</p>
</td></tr>
<tr><td><code id="construct_ppgasp_+3A_kernel_type">kernel_type</code></td>
<td>

<p>Type of kernel. <code>matern_3_2</code> and <code>matern_5_2</code> are <code>Matern kernel</code> with roughness parameter 3/2 and 5/2 respectively. <code>pow_exp</code> is power exponential kernel with roughness parameter alpha. If <code>pow_exp</code> is to be used, one needs to specify its roughness parameter alpha.
</p>
</td></tr>
<tr><td><code id="construct_ppgasp_+3A_alpha">alpha</code></td>
<td>

<p>Roughness parameters in the kernel functions.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list. The first element is a lower triangular matrix <code>L</code>, a cholesky decomposition of <code>R</code>, i.e. LL^t=R
and <code>R</code> is the correlation matrix. The second element is  lower triangular matrix <code>LX</code> a cholesky decomposition of X^tR^{-1}X. The third element is a matrix of <code>theta_hat</code>, the mean (trend) parameters. The last element is a vector of <code>sigma2_hat</code>, which is the estimated variance parameter on each function.  
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Jesus Palomo [aut],
  James Berger [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>

<hr>
<h2 id='construct_rgasp'>Robust GaSP constructor after estimating the parameters</h2><span id='topic+construct_rgasp'></span>

<h3>Description</h3>

<p>This function constructs the Robust GaSP model if the range and noise-variance ratio parameters are given or have been estimated.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>construct_rgasp(beta, nu, R0, X, zero_mean, output, kernel_type, alpha)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="construct_rgasp_+3A_beta">beta</code></td>
<td>

<p>inverse-range parameters.
</p>
</td></tr>
<tr><td><code id="construct_rgasp_+3A_nu">nu</code></td>
<td>

<p>noise-variance ratio parameter.
</p>
</td></tr>
<tr><td><code id="construct_rgasp_+3A_r0">R0</code></td>
<td>

<p>A List of matrix where the j-th matrix is an absolute difference matrix of the j-th input vector.
</p>
</td></tr>
<tr><td><code id="construct_rgasp_+3A_x">X</code></td>
<td>

<p>The mean basis function i.e. the trend function.
</p>
</td></tr>
<tr><td><code id="construct_rgasp_+3A_zero_mean">zero_mean</code></td>
<td>

<p>The mean basis function is zero or not.
</p>
</td></tr>
<tr><td><code id="construct_rgasp_+3A_output">output</code></td>
<td>

<p>the output vector.
</p>
</td></tr>
<tr><td><code id="construct_rgasp_+3A_kernel_type">kernel_type</code></td>
<td>

<p>Type of kernel. <code>matern_3_2</code> and <code>matern_5_2</code> are <code>Matern kernel</code> with roughness parameter 3/2 and 5/2 respectively. <code>pow_exp</code> is power exponential kernel with roughness parameter alpha. If <code>pow_exp</code> is to be used, one needs to specify its roughness parameter alpha.
</p>
</td></tr>
<tr><td><code id="construct_rgasp_+3A_alpha">alpha</code></td>
<td>

<p>Roughness parameters in the kernel functions.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list. The first element is a lower triangular matrix <code>L</code>, a cholesky decomposition of <code>R</code>, i.e. LL^t=R
and <code>R</code> is the correlation matrix. The second element is  lower triangular matrix <code>LX</code> a cholesky decomposition of X^tR^{-1}X
. The third element is a vector <code>theta_hat</code>, the mean (trend) parameters. The last element is numeric valued <code>sigma2_hat</code>, which is the estimated variance parameter.  
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Jesus Palomo [aut],
  James Berger [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>

<hr>
<h2 id='dettepepel.3.data'>Dette &amp; Pepelyshev (2010) Curved Function
</h2><span id='topic+dettepepel.3.data'></span><span id='topic+detpep10curv'></span>

<h3>Description</h3>

<p>A 3-dimensional test function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dettepepel.3.data(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dettepepel.3.data_+3A_x">x</code></td>
<td>
<p> a 3-dimensional vector specifying the location where the function is to be evaluated.  
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For more details see Dette, H., &amp; Pepelyshev, A. (2010). Generalized Latin hypercube design for computer experiments. Technometrics, 52(4).
</p>


<h3>Value</h3>

<p>A real value equal to the function value evaluated at x.
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Jesus Palomo [aut],
  James Berger [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>

<p>Dette, H., &amp; Pepelyshev, A. (2010). Generalized Latin hypercube design for computer experiments. Technometrics, 52(4).
</p>
<p>S. Surjanovic, D. Bingham, Virtual Library of Simulation Experiments:  Test Functions
and Datasets, retrieved March 29, 2016, from <a href="http://www.sfu.ca/~ssurjano/detpep10curv.html">http://www.sfu.ca/~ssurjano/detpep10curv.html</a>.
</p>

<hr>
<h2 id='environ.4.data'> Environmental model function 
</h2><span id='topic+environ.4.data'></span><span id='topic+environ'></span>

<h3>Description</h3>

<p>This function is the environmental model where the output is the concentration of the pollutant at the space-time grid used in in Bliznyuk et al. (2008). The physical inputs are 4 dimensions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>environ.4.data(x, s=c(0.5, 1, 1.5, 2, 2.5), t=seq(from=0.3, to=60, by=0.3))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="environ.4.data_+3A_x">x</code></td>
<td>
<p> a 4-dimensional vector specifying input parameters.</p>
</td></tr>
<tr><td><code id="environ.4.data_+3A_s">s</code></td>
<td>
<p>  spatial location to be evaluated.</p>
</td></tr>

<tr><td><code id="environ.4.data_+3A_t">t</code></td>
<td>
<p>  time point to be evaluated.</p>
</td></tr>

</table>


<h3>Details</h3>

<p>Bliznyuk, N., Ruppert, D., Shoemaker, C., Regis, R., Wild, S., &amp; Mugunthan, P. (2008). Bayesian calibration and uncertainty analysis for computationally expensive models using optimization and radial basis function approximation. <em>Journal of Computational and Graphical Statistics</em>, <b>17</b>(2), 270-294.
</p>


<h3>Value</h3>

<p>A real value of this function evaluated at <code>x</code>.
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Jesus Palomo [aut],
  James Berger [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>

<p>N. Bliznyuk, D. Ruppert, C. Shoemaker, R. Regis, S. Wild, &amp; P. Mugunthan (2008). Bayesian calibration and uncertainty analysis for computationally expensive models using optimization and radial basis function approximation. <em>Journal of Computational and Graphical Statistics</em>, <b>17</b>(2), 270-294.
</p>
<p>S. Surjanovic, D. Bingham, Virtual Library of Simulation Experiments:  Test Functions
and Datasets, retrieved March 29, 2016, from <a href="http://www.sfu.ca/~ssurjano/fried.html">http://www.sfu.ca/~ssurjano/fried.html</a>.
</p>

<hr>
<h2 id='euclidean_distance'>Euclidean distance matrix between two input matrices
</h2><span id='topic+euclidean_distance'></span>

<h3>Description</h3>

<p>Function to construct the euclidean distance matrix with the two input matrices.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>euclidean_distance(input1, input2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="euclidean_distance_+3A_input1">input1</code></td>
<td>

<p>A matrix of input with the number of rows being the number of observations and the number of columns being the number of variables
</p>
</td></tr>
<tr><td><code id="euclidean_distance_+3A_input2">input2</code></td>
<td>

<p>A matrix of input with the number of rows being the number of observations and the number of columns being the number of variables
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The  euclidean distance matrix with the number of rows and the number of columns being the number of rows in the first and second input matrices.
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Jesus Palomo [aut],
  James Berger [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  # dimensional of the inputs
  dim_inputs &lt;- 8    
  
  # number of the inputs
  num_obs &lt;- 30       
  
  # uniform samples of design
  input &lt;-matrix(runif(num_obs*dim_inputs), num_obs,dim_inputs) 

  # the Euclidean distance matrix 
  R0=euclidean_distance(input, input)
</code></pre>

<hr>
<h2 id='findInertInputs'>
find inert inputs with the posterior mode
</h2><span id='topic+findInertInputs'></span>

<h3>Description</h3>

<p>The function tests for inert inputs (inputs that barely affect the outputs) using the posterior mode. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>findInertInputs(object,threshold=0.1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="findInertInputs_+3A_object">object</code></td>
<td>

<p>an object of  class <code>rgasp</code> or the <code>ppgasp</code>.
</p>
</td></tr>
<tr><td><code id="findInertInputs_+3A_threshold">threshold</code></td>
<td>

<p>a threshold between 0 to 1. If the normalized inverse parameter of an input is smaller this value, it is classified as inert inputs. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function utilizes the following quantity 
</p>
<p>object@p*object@beta_hat*object@CL/sum(object@beta_hat*object@CL)
</p>
<p>for each input to identify the inert outputs. The average estimated normalized inverse range parameters will be 1. If the estimated normalized inverse range parameters of an input is close to 0, it means this input might be an inert input. 
</p>
<p>In this method, a prior that has shrinkage effects is suggested to use, .e.g the jointly robust prior (i.e. one should set <code>prior_choice='ref_approx'</code> in <code>rgasp()</code> to obtain the use <code>rgasp</code> object before using this function). Moreover, one may not add a lower bound of the range parameters to perform this method, i.e. one should set <code>lower_bound=F</code> in <code>rgasp()</code>.  For more details see Chapter 4 in the reference below.
</p>
<p>Mengyang Gu. (2016). Robust Uncertainty Quantification and Scalable Computation for Computer Models with Massive Output. Ph.D. thesis. Duke University.
</p>


<h3>Value</h3>

<p>A vector that has the same dimension of the number of inputs indicating how likely the inputs are inerts. The average value is 1. When a value is very close to zero, it tends to be an inert inputs. 
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Jesus Palomo [aut],
  James Berger [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>

<p>Mengyang Gu. (2016). Robust Uncertainty Quantification and Scalable Computation for Computer Models with Massive Output. Ph.D. thesis. Duke University.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  #-----------------------------------------------
  # test for inert inputs in the Borehole function
  #-----------------------------------------------
# dimensional of the inputs
dim_inputs &lt;- 8    
# number of the inputs
num_obs &lt;- 40       

# uniform samples of design
set.seed(0)
input &lt;-matrix(runif(num_obs*dim_inputs), num_obs,dim_inputs) 
# Following codes use maximin Latin Hypercube Design, which is typically better than uniform
# library(lhs)
# input &lt;- maximinLHS(n=num_obs, k=dim_inputs)  # maximin lhd sample

# rescale the design to the domain
input[,1]&lt;-0.05+(0.15-0.05)*input[,1];
input[,2]&lt;-100+(50000-100)*input[,2];
input[,3]&lt;-63070+(115600-63070)*input[,3];
input[,4]&lt;-990+(1110-990)*input[,4];
input[,5]&lt;-63.1+(116-63.1)*input[,5];
input[,6]&lt;-700+(820-700)*input[,6];
input[,7]&lt;-1120+(1680-1120)*input[,7];
input[,8]&lt;-9855+(12045-9855)*input[,8];

# outputs from the 8 dim Borehole function

output=matrix(0,num_obs,1)
for(i in 1:num_obs){
  output[i]=borehole(input[i,])
}





# use constant mean basis with trend, with no constraint on optimization
m3&lt;- rgasp(design = input, response = output, lower_bound=FALSE)

P=findInertInputs(m3)


</code></pre>

<hr>
<h2 id='friedman.5.data'>Friedman Function
</h2><span id='topic+friedman.5.data'></span><span id='topic+friedman'></span><span id='topic+fried'></span>

<h3>Description</h3>

<p>A 5-dimensional test function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>friedman.5.data(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="friedman.5.data_+3A_x">x</code></td>
<td>
<p> a 5-dimensional vector specifying the location where the function is to be evaluated.  
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For more details see Friedman, J. H. (1991). Multivariate adaptive regression splines. The annals of statistics, 19(1), 1-67.
</p>


<h3>Value</h3>

<p>A real value equal to the function value evaluated at x.
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Jesus Palomo [aut],
  James Berger [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>

<p>Friedman, J. H. (1991). Multivariate adaptive regression splines. The annals of statistics, 19(1), 1-67.
</p>
<p>S. Surjanovic, D. Bingham, Virtual Library of Simulation Experiments:  Test Functions
and Datasets, retrieved March 29, 2016, from <a href="http://www.sfu.ca/~ssurjano/fried.html">http://www.sfu.ca/~ssurjano/fried.html</a>.
</p>

<hr>
<h2 id='generate_predictive_mean_cov'>
A C++ function to generate predictive mean and cholesky decomposition of the scaled covariance function.
</h2><span id='topic+generate_predictive_mean_cov'></span>

<h3>Description</h3>

<p>The function computes predictive mean and cholesky decomposition of the scaled covariance function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>generate_predictive_mean_cov(beta, nu, input, X,zero_mean,output,  
testing_input,X_testing, L, LX, theta_hat, sigma2_hat,rr0,r0, 
kernel_type,alpha,method,sample_data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="generate_predictive_mean_cov_+3A_beta">beta</code></td>
<td>

<p>inverse-range parameters.
</p>
</td></tr>
<tr><td><code id="generate_predictive_mean_cov_+3A_nu">nu</code></td>
<td>

<p>noise-variance ratio parameter.
</p>
</td></tr>
<tr><td><code id="generate_predictive_mean_cov_+3A_input">input</code></td>
<td>

<p>input matrix.
</p>
</td></tr>
<tr><td><code id="generate_predictive_mean_cov_+3A_x">X</code></td>
<td>

<p>the mean basis function i.e. the trend function.
</p>
</td></tr>
<tr><td><code id="generate_predictive_mean_cov_+3A_zero_mean">zero_mean</code></td>
<td>

<p>The mean basis function is zero or not.
</p>
</td></tr>
<tr><td><code id="generate_predictive_mean_cov_+3A_output">output</code></td>
<td>

<p>output matrix.
</p>
</td></tr>
<tr><td><code id="generate_predictive_mean_cov_+3A_testing_input">testing_input</code></td>
<td>

<p>testing input matrix.
</p>
</td></tr>
<tr><td><code id="generate_predictive_mean_cov_+3A_x_testing">X_testing</code></td>
<td>

<p>mean/trend matrix of testing inputs.
</p>
</td></tr>
<tr><td><code id="generate_predictive_mean_cov_+3A_l">L</code></td>
<td>

<p>a lower triangular matrix for the cholesky decomposition of <code>R</code>, the correlation matrix.
</p>
</td></tr>
<tr><td><code id="generate_predictive_mean_cov_+3A_lx">LX</code></td>
<td>

<p>a lower triangular matrix for the cholesky decomposition of X^tR^{-1}X.
</p>
</td></tr>
<tr><td><code id="generate_predictive_mean_cov_+3A_theta_hat">theta_hat</code></td>
<td>

<p>estimated mean/trend parameters.
</p>
</td></tr>
<tr><td><code id="generate_predictive_mean_cov_+3A_sigma2_hat">sigma2_hat</code></td>
<td>

<p>estimated variance parameter.
</p>
</td></tr>
<tr><td><code id="generate_predictive_mean_cov_+3A_rr0">rr0</code></td>
<td>

<p>a matrix of absolute difference between testing inputs and testing inputs.
</p>
</td></tr>
<tr><td><code id="generate_predictive_mean_cov_+3A_r0">r0</code></td>
<td>

<p>a matrix of absolute difference between inputs and testing inputs.
</p>
</td></tr>
<tr><td><code id="generate_predictive_mean_cov_+3A_kernel_type">kernel_type</code></td>
<td>

<p>Type of kernel. <code>matern_3_2</code> and <code>matern_5_2</code> are <code>Matern kernel</code> with roughness parameter 3/2 and 5/2 respectively. <code>pow_exp</code> is power exponential kernel with roughness parameter alpha. If <code>pow_exp</code> is to be used, one needs to specify its roughness parameter alpha.
</p>
</td></tr>
<tr><td><code id="generate_predictive_mean_cov_+3A_alpha">alpha</code></td>
<td>

<p>Roughness parameters in the kernel functions.
</p>
</td></tr>
<tr><td><code id="generate_predictive_mean_cov_+3A_method">method</code></td>
<td>
<p>  method of parameter estimation. <code>post_mode</code> means the marginal posterior mode is used for estimation. <code>mle</code> means the maximum likelihood estimation is used. <code>mmle</code> means the maximum marginal likelihood estimation is used. The <code>post_mode</code>  is the default method. </p>
</td></tr>
<tr><td><code id="generate_predictive_mean_cov_+3A_sample_data">sample_data</code></td>
<td>

<p>a boolean value. If true, the data (which may contain noise) is sampled. If false, the the mean of the data is sampled.  
</p>
</td></tr></table>


<h3>Value</h3>

<p>A list of 2 elements. The first is a vector for predictive mean for testing inputs. The second is a scaled covariance matrix of the predictive distribution. 
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Jesus Palomo [aut],
  James Berger [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>

<p>Mengyang Gu. (2016). Robust Uncertainty Quantification and Scalable Computation for Computer Models with Massive Output. Ph.D. thesis. Duke University.
</p>

<hr>
<h2 id='higdon.1.data'>
Higdon (2002) Function
</h2><span id='topic+higdon.1.data'></span><span id='topic+higdon'></span><span id='topic+hig02'></span>

<h3>Description</h3>

<p>Higdon (2002) 1-dimensional test function.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>higdon.1.data(s)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="higdon.1.data_+3A_s">s</code></td>
<td>
<p> a 1-dimensional vector specifying the location where the function is to be evaluated.  
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For more details, see Higdon D. (2002) Space and Space-Time Modeling using Process Convolutions. In: Anderson C.W., Barnett V., Chatwin P.C., El-Shaarawi A.H. (eds) Quantitative Methods for Current Environmental Issues. Springer, London. 
</p>


<h3>Value</h3>

<p>A real number equal to the Higdon (2002) function value at <code>s</code>.
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Jesus Palomo [aut],
  James Berger [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>

<p>Higdon, D. (2002). Space and space-time modeling using process convolutions. In Quantitative methods for current environmental issues (pp. 37-56). Springer London.
</p>
<p>S. Surjanovic, D. Bingham, Virtual Library of Simulation Experiments:  Test Functions
and Datasets, retrieved March 29, 2016, from <a href="https://www.sfu.ca/~ssurjano/hig02.html">https://www.sfu.ca/~ssurjano/hig02.html</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
s &lt;- seq(0,10,0.01)
y &lt;- higdon.1.data(s)
plot(s,y, xlab='s',ylab='y',type='l',main='Higdon (2002) function')

</code></pre>

<hr>
<h2 id='humanity_model'>
data from the humanity model
</h2><span id='topic+humanity.X'></span><span id='topic+humanity.Y'></span><span id='topic+humanity.Xt'></span><span id='topic+humanity.Yt'></span>

<h3>Description</h3>

<p>This data set provides the training data and testing data from the 'diplomatic and military operations in a non-warfighting
domain' (DIAMOND) simulator. It porduces the number of casualties during the second day to sixth day after the earthquake and
volcanic eruption in Giarre and Catania. See  (Overstall and Woods (2016)) for details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>	data(humanity_model)
</code></pre>


<h3>Format</h3>

<p>Four data frame with observations on the following variables.
</p>

<dl>
<dt><code>humanity.X</code></dt><dd><p>A matrix of the training inputs.</p>
</dd>
<dt><code>humanity.Y</code></dt><dd><p>A matrix of the output of the calsualties from the second to sixth day after the the earthquake and
volcanic eruption for each set of training inputs.</p>
</dd>
<dt><code>humanity.Xt</code></dt><dd><p>A matrix of the test inputs.</p>
</dd>
<dt><code>humanity.Yt</code></dt><dd><p>A matrix of the test output of the calsualties.</p>
</dd>
</dl>



<h3>References</h3>

<p>A. M. Overstall and D. C. Woods (2016). Multivariate emulation of computer simulators: model selection and diagnostics with application to a humanitarian relief model. Journal of the Royal Statistical Society: Series C (Applied Statistics), 65(4):483-505.
</p>
<p>B. Taylor and A. Lane. Development of a novel family of military campaign simulation models. Journal of the Operational Research Society, 55(4):333-339, 2004.
</p>

<hr>
<h2 id='leave_one_out_rgasp'>
leave-one-out fitted values and standard deviation for robust GaSP model
</h2><span id='topic+leave_one_out_rgasp'></span>

<h3>Description</h3>

<p>A function to calculate leave-one-out fitted values and the standard deviation of the prediction on robust GaSP models after the robust GaSP model has been constructed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>leave_one_out_rgasp(object)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="leave_one_out_rgasp_+3A_object">object</code></td>
<td>
<p> an object of  class <code>rgasp</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of 2 elements with 
</p>
<table>
<tr><td><code>mean</code></td>
<td>
<p>leave one out fitted values.</p>
</td></tr>
<tr><td><code>sd</code></td>
<td>
<p>standard deviation of each prediction.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Jesus Palomo [aut],
  James Berger [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>

<p>Mengyang Gu. (2016). Robust Uncertainty Quantification and Scalable Computation for Computer Models with Massive Output. Ph.D. thesis. Duke University.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rgasp">rgasp</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(RobustGaSP)
 #------------------------
  # a 3 dimensional example
  #------------------------
  # dimensional of the inputs
  dim_inputs &lt;- 3    
  # number of the inputs
  num_obs &lt;- 30       
  # uniform samples of design
  input &lt;- matrix(runif(num_obs*dim_inputs), num_obs,dim_inputs) 
  
  # Following codes use maximin Latin Hypercube Design, which is typically better than uniform
  # library(lhs)
  # input &lt;- maximinLHS(n=num_obs, k=dim_inputs)  ##maximin lhd sample
  
  ####
  # outputs from the 3 dim dettepepel.3.data function
  
  output = matrix(0,num_obs,1)
  for(i in 1:num_obs){
    output[i]&lt;-dettepepel.3.data (input[i,])
  }
  
  # use constant mean basis, with no constraint on optimization
  m1&lt;- rgasp(design = input, response = output, lower_bound=FALSE)
  
  ##leave one out predict
  leave_one_out_m1=leave_one_out_rgasp(m1)
  
  ##predictive mean 
  leave_one_out_m1$mean
  ##standard deviation
  leave_one_out_m1$sd
  ##standardized error
  (leave_one_out_m1$mean-output)/leave_one_out_m1$sd

  
</code></pre>

<hr>
<h2 id='limetal.2.data'> Lim et at. (2002) nonpolynomial function
</h2><span id='topic+limetal.2.data'></span><span id='topic+limetal02non'></span>

<h3>Description</h3>

<p>This function is 2-dimensional test function. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>limetal.2.data(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="limetal.2.data_+3A_x">x</code></td>
<td>
<p> a 2-dimensional vector specifying the location where the function is to be evaluated.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For more details, see Lim, Y. B., Sacks, J., Studden, W. J., &amp; Welch, W. J. (2002). Design and analysis of computer experiments when the output is highly correlated over the input space. Canadian Journal of Statistics, 30(1), 109-126.
</p>
<p>Welch, W. J., Buck, R. J., Sacks, J., Wynn, H. P., Mitchell, T. J., &amp; Morris, M. D. (1992). Screening, predicting, and computer experiments. Technometrics, 34(1), 15-25.
</p>


<h3>Value</h3>

<p>A real value of this function evaluated at <code>x</code>.
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Jesus Palomo [aut],
  James Berger [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>

<p>Lim, Y. B., Sacks, J., Studden, W. J., &amp; Welch, W. J. (2002). Design and analysis of computer experiments when the output is highly correlated over the input space. , <em>Canadian Journal of Statistics</em>, <b>30</b>(1), 109-126.
</p>
<p>Welch, W. J., Buck, R. J., Sacks, J., Wynn, H. P., Mitchell, T. J., &amp; Morris, M. D. (1992). Screening, predicting, and computer experiments. Technometrics, 34(1), 15-25.
</p>
<p>S. Surjanovic, D. Bingham, Virtual Library of Simulation Experiments:  Test Functions
and Datasets, retrieved March 29, 2016, from <a href="http://www.sfu.ca/~ssurjano/limetal02non.html">http://www.sfu.ca/~ssurjano/limetal02non.html</a>.
</p>

<hr>
<h2 id='log_approx_ref_prior'>
The natural logarithm of the jointly robust prior (up to a normalizing constant)
</h2><span id='topic+log_approx_ref_prior'></span>

<h3>Description</h3>

<p>A function to the natural logarithm of the jointly robust prior (up to a normalizing constant).</p>


<h3>Usage</h3>

<pre><code class='language-R'>log_approx_ref_prior(param, nugget, nugget_est, CL, a, b)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="log_approx_ref_prior_+3A_param">param</code></td>
<td>

<p>A vector of natural logarithm of inverse-range parameters and natural logarithm of the nugget-variance ratio parameter.
</p>
</td></tr>
<tr><td><code id="log_approx_ref_prior_+3A_nugget">nugget</code></td>
<td>

<p>The nugget-variance ratio parameter if this parameter is fixed.
</p>
</td></tr>
<tr><td><code id="log_approx_ref_prior_+3A_nugget_est">nugget_est</code></td>
<td>

<p>Boolean value of whether the nugget is estimated or fixed.
</p>
</td></tr>
<tr><td><code id="log_approx_ref_prior_+3A_cl">CL</code></td>
<td>

<p>Prior parameter in the jointly robust prior.
</p>
</td></tr>
<tr><td><code id="log_approx_ref_prior_+3A_a">a</code></td>
<td>

<p>Prior parameter in the jointly robust prior.
</p>
</td></tr>
<tr><td><code id="log_approx_ref_prior_+3A_b">b</code></td>
<td>

<p>Prior parameter in the jointly robust prior.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The numerical value of the derivative of the approximate reference prior with regard to inverse-range parameters and the nugget-variance ratio parameter. When the nugget is fixed, the derivative is on inverse-range parameters.
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Jesus Palomo [aut],
  James Berger [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>

<p>M. Gu. (2016). Robust uncertainty quantification and scalable computation for computer models with massive output. Ph.D. thesis. Duke University.
</p>
<p>M. Gu (2018), <em>Jointly robust prior for Gaussian stochastic process in emulation, calibration and variable selection</em>, arXiv:1804.09329.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rgasp">rgasp</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'># inputs
x&lt;-runif(10);
n&lt;-length(x);

# default prior parameters
a&lt;-0.2
b&lt;-n^{-1}*(a+1)
R0&lt;-as.matrix(abs(outer(x,x, "-")))
CL&lt;- mean(R0[which(R0&gt;0)])

# compute the density of log reference prior up to a normalizing constant
param &lt;- seq(-10,10,0.01)
prior &lt;- rep(0,length(param))
for(i in 1:length(param)){
  prior[i] &lt;- exp(log_approx_ref_prior(param[i],nugget=0,nugget_est=FALSE,CL,a,b) )
}
# plot
plot(param,prior,type='l',
                xlab='Logarithm of inverse range parameters',
                ylab='Prior density up to a normalizing constant')
</code></pre>

<hr>
<h2 id='log_approx_ref_prior_deriv'>
Derivative of the jointly robust prior
</h2><span id='topic+log_approx_ref_prior_deriv'></span>

<h3>Description</h3>

<p>The function computes the derivative of the approximate reference prior with regard to inverse range parameter and the nugget-noise ratio parameter (if not fixed). When the nugget is fixed, it only compute the derivative with regard to the inverse range parameter; otherwise it produces derivative with regard to inverse range parameter and noise ratio parameter.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>log_approx_ref_prior_deriv(param, nugget, nugget_est, CL, a, b)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="log_approx_ref_prior_deriv_+3A_param">param</code></td>
<td>

<p>A vector of natural logarithm of inverse-range parameters and natural logarithm of the nugget-variance ratio parameter.
</p>
</td></tr>
<tr><td><code id="log_approx_ref_prior_deriv_+3A_nugget">nugget</code></td>
<td>

<p>The nugget-variance ratio parameter if this parameter is fixed.
</p>
</td></tr>
<tr><td><code id="log_approx_ref_prior_deriv_+3A_nugget_est">nugget_est</code></td>
<td>

<p>Boolean value of whether the nugget is estimated or fixed.
</p>
</td></tr>
<tr><td><code id="log_approx_ref_prior_deriv_+3A_cl">CL</code></td>
<td>

<p>Prior parameter in the jointly robust prior.
</p>
</td></tr>
<tr><td><code id="log_approx_ref_prior_deriv_+3A_a">a</code></td>
<td>

<p>Prior parameter in the jointly robust prior.
</p>
</td></tr>
<tr><td><code id="log_approx_ref_prior_deriv_+3A_b">b</code></td>
<td>

<p>Prior parameter in the jointly robust prior.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The numerical value of the derivative of the jointly robust prior with regard to <code>beta</code> (the inverse-range parameters) and <code>nugget</code> (the nugget-variance ratio parameter). When the nugget is fixed, the derivative is on inverse-range parameters.
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Jesus Palomo [aut],
  James Berger [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>

<p>M. Gu. (2016). Robust uncertainty quantification and scalable computation for computer models with massive output. Ph.D. thesis. Duke University.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+log_approx_ref_prior">log_approx_ref_prior</a></code>,<code><a href="#topic+rgasp">rgasp</a></code> </p>

<hr>
<h2 id='log_marginal_lik'>
Natural logarithm of marginal likelihood of the robust GaSP model
</h2><span id='topic+log_marginal_lik'></span>

<h3>Description</h3>

<p>This function computes the natural logarithm of marginal likelihood after marginalizing out the mean (trend) and variance parameters by the location-scale prior.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>log_marginal_lik(param, nugget, nugget_est, R0, X, zero_mean,output, kernel_type, alpha)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="log_marginal_lik_+3A_param">param</code></td>
<td>

<p>a vector of natural logarithm of inverse-range parameters and natural logarithm of the nugget-variance ratio parameter.
</p>
</td></tr>
<tr><td><code id="log_marginal_lik_+3A_nugget">nugget</code></td>
<td>

<p>the nugget-variance ratio parameter if this parameter is fixed.
</p>
</td></tr>
<tr><td><code id="log_marginal_lik_+3A_nugget_est">nugget_est</code></td>
<td>

<p>Boolean value of whether the nugget is estimated or fixed.
</p>
</td></tr>
<tr><td><code id="log_marginal_lik_+3A_r0">R0</code></td>
<td>

<p>a list of matrix where the j-th matrix is an absolute difference matrix of the j-th input vector.
</p>
</td></tr>
<tr><td><code id="log_marginal_lik_+3A_x">X</code></td>
<td>

<p>the mean basis function i.e. the trend function.
</p>
</td></tr>
<tr><td><code id="log_marginal_lik_+3A_zero_mean">zero_mean</code></td>
<td>

<p>the mean basis function is zero or not.
</p>
</td></tr>
<tr><td><code id="log_marginal_lik_+3A_output">output</code></td>
<td>

<p>the output vector.
</p>
</td></tr>
<tr><td><code id="log_marginal_lik_+3A_kernel_type">kernel_type</code></td>
<td>

<p>A vector of <code>integer</code> specifying the type of kernels of each coordinate of the input. 
In each coordinate of the vector, 1 means the <code>pow_exp</code> kernel with roughness parameter specified in alpha; 2 means  <code>matern_3_2</code>  kernel; 3 means <code>matern_5_2</code>  kernel; 5 means <code>periodic_gauss</code> kernel; 5 means <code>periodic_exp</code> kernel. 
</p>
</td></tr>
<tr><td><code id="log_marginal_lik_+3A_alpha">alpha</code></td>
<td>

<p>Roughness parameters in the kernel functions.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The numerical value of natural logarithm of the marginal likelihood.
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Jesus Palomo [aut],
  James Berger [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>

<p>M. Gu. (2016). Robust uncertainty quantification and scalable computation for computer models with massive output. Ph.D. thesis. Duke University.
</p>

<hr>
<h2 id='log_marginal_lik_deriv'>
Derivative of natural logarithm of the marginal likelihood
</h2><span id='topic+log_marginal_lik_deriv'></span>

<h3>Description</h3>

<p>The derivative of natural logarithm of marginal likelihood of the Robust GaSP model with regard to inverse range parameters and nugget-variance ratio parameter after marginalizing out the mean (trend) and variance parameters the location-scale prior. When the nugget is fixed, it only computes the derivative with regard to the inverse range parameter; otherwise it produces derivative with regard to inverse range parameter and nugget-variance ratio parameter.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>log_marginal_lik_deriv(param, nugget, nugget_est, R0, X, zero_mean, 
output, kernel_type, alpha)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="log_marginal_lik_deriv_+3A_param">param</code></td>
<td>

<p>A vector of natural logarithm of inverse-range parameters and natural logarithm of the nugget-variance ratio parameter.
</p>
</td></tr>
<tr><td><code id="log_marginal_lik_deriv_+3A_nugget">nugget</code></td>
<td>

<p>The nugget-variance ratio parameter if this parameter is fixed.
</p>
</td></tr>
<tr><td><code id="log_marginal_lik_deriv_+3A_nugget_est">nugget_est</code></td>
<td>

<p>Boolean value of whether the nugget is estimated or fixed.
</p>
</td></tr>
<tr><td><code id="log_marginal_lik_deriv_+3A_r0">R0</code></td>
<td>

<p>A list of matrix where the j-th matrix is an absolute difference matrix of the j-th input vector.
</p>
</td></tr>
<tr><td><code id="log_marginal_lik_deriv_+3A_x">X</code></td>
<td>

<p>The mean basis function i.e. the trend function.
</p>
</td></tr>
<tr><td><code id="log_marginal_lik_deriv_+3A_zero_mean">zero_mean</code></td>
<td>

<p>The mean basis function is zero or not.
</p>
</td></tr>
<tr><td><code id="log_marginal_lik_deriv_+3A_output">output</code></td>
<td>

<p>The output vector.
</p>
</td></tr>
<tr><td><code id="log_marginal_lik_deriv_+3A_kernel_type">kernel_type</code></td>
<td>

<p>A vector of <code>integer</code> specifying the type of kernels of each coordinate of the input. 
In each coordinate of the vector, 1 means the <code>pow_exp</code> kernel with roughness parameter specified in alpha; 2 means  <code>matern_3_2</code>  kernel; 3 means <code>matern_5_2</code>  kernel; 5 means <code>periodic_gauss</code> kernel; 5 means <code>periodic_exp</code> kernel. 
</p>
</td></tr>
<tr><td><code id="log_marginal_lik_deriv_+3A_alpha">alpha</code></td>
<td>

<p>Roughness parameters in the kernel functions.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The numerical value of the derivative of natural logarithm of marginal likelihood with regard to range and nugget-variance ratio parameter (if not fixed). When the nugget is fixed, the derivative is on inverse-range parameters.
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Jesus Palomo [aut],
  James Berger [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>

<p>M. Gu. (2016). Robust uncertainty quantification and scalable computation for computer models with massive output. Ph.D. thesis. Duke University.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+log_marginal_lik">log_marginal_lik</a></code>.
</p>

<hr>
<h2 id='log_marginal_lik_deriv_ppgasp'>
Derivative of natural logarithm of the marginal likelihood
</h2><span id='topic+log_marginal_lik_deriv_ppgasp'></span>

<h3>Description</h3>

<p>The derivative of natural logarithm of marginal likelihood of the PP GaSP model with regard to inverse range parameters and nugget-variance ratio parameter after marginalizing out the mean (trend) and variance parameters by the location-scale prior. When the nugget is fixed, it only compute the derivative with regard to the inverse range parameter; otherwise it produces derivative with regard to inverse range parameter and nugget-variance ratio parameter.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>log_marginal_lik_deriv_ppgasp(param, nugget, nugget_est, R0, X, zero_mean, 
output, kernel_type, alpha)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="log_marginal_lik_deriv_ppgasp_+3A_param">param</code></td>
<td>

<p>a vector of natural logarithm of inverse-range parameters and natural logarithm of the nugget-variance ratio parameter.
</p>
</td></tr>
<tr><td><code id="log_marginal_lik_deriv_ppgasp_+3A_nugget">nugget</code></td>
<td>

<p>the nugget-variance ratio parameter if this parameter is fixed.
</p>
</td></tr>
<tr><td><code id="log_marginal_lik_deriv_ppgasp_+3A_nugget_est">nugget_est</code></td>
<td>

<p>Boolean value of whether the nugget is estimated or fixed.
</p>
</td></tr>
<tr><td><code id="log_marginal_lik_deriv_ppgasp_+3A_r0">R0</code></td>
<td>

<p>a list of matrix where the j-th matrix is an absolute difference matrix of the j-th input vector.
</p>
</td></tr>
<tr><td><code id="log_marginal_lik_deriv_ppgasp_+3A_x">X</code></td>
<td>

<p>the mean basis function i.e. the trend function.
</p>
</td></tr>
<tr><td><code id="log_marginal_lik_deriv_ppgasp_+3A_zero_mean">zero_mean</code></td>
<td>

<p>the mean basis function is zero or not.
</p>
</td></tr>
<tr><td><code id="log_marginal_lik_deriv_ppgasp_+3A_output">output</code></td>
<td>

<p>a matrix where each row is one runs of the computer model output.
</p>
</td></tr>
<tr><td><code id="log_marginal_lik_deriv_ppgasp_+3A_kernel_type">kernel_type</code></td>
<td>

<p>A vector of <code>integer</code> specifying the type of kernels of each coordinate of the input. 
In each coordinate of the vector, 1 means the <code>pow_exp</code> kernel with roughness parameter specified in alpha; 2 means  <code>matern_3_2</code>  kernel; 3 means <code>matern_5_2</code>  kernel; 5 means <code>periodic_gauss</code> kernel; 5 means <code>periodic_exp</code> kernel. 
</p>
</td></tr>
<tr><td><code id="log_marginal_lik_deriv_ppgasp_+3A_alpha">alpha</code></td>
<td>

<p>roughness parameters in the kernel functions.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The numerical value of the derivative of natural logarithm of marginal likelihood with regard to range and nugget-variance ratio parameter (if not fixed). When the nugget is fixed, the derivative is on inverse-range parameters.
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Jesus Palomo [aut],
  James Berger [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>

<p>M. Gu. and J.O. Berger (2016). Parallel partial Gaussian process emulation for computer models with massive output. <em>Annals of Applied Statistics</em>, 10(3), 1317-1347.
</p>
<p>M. Gu. (2016). Robust uncertainty quantification and scalable computation for computer models with massive output. Ph.D. thesis. Duke University.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+log_marginal_lik_ppgasp">log_marginal_lik_ppgasp</a></code>.
</p>

<hr>
<h2 id='log_marginal_lik_ppgasp'>
Natural logarithm of marginal likelihood of the PP GaSP model
</h2><span id='topic+log_marginal_lik_ppgasp'></span>

<h3>Description</h3>

<p>This function computes the natural logarithm of marginal likelihood of the PP GaSP model after marginalizing out the mean (trend) and variance parameters by the location-scale prior.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>log_marginal_lik_ppgasp(param, nugget, nugget_est, R0, X, zero_mean,output,
kernel_type, alpha)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="log_marginal_lik_ppgasp_+3A_param">param</code></td>
<td>

<p>a vector of natural logarithm of inverse-range parameters and natural logarithm of the nugget-variance ratio parameter.
</p>
</td></tr>
<tr><td><code id="log_marginal_lik_ppgasp_+3A_nugget">nugget</code></td>
<td>

<p>the nugget-variance ratio parameter if this parameter is fixed.
</p>
</td></tr>
<tr><td><code id="log_marginal_lik_ppgasp_+3A_nugget_est">nugget_est</code></td>
<td>

<p>Boolean value of whether the nugget is estimated or fixed.
</p>
</td></tr>
<tr><td><code id="log_marginal_lik_ppgasp_+3A_r0">R0</code></td>
<td>

<p>a List of matrix where the j-th matrix is an absolute difference matrix of the j-th input vector.
</p>
</td></tr>
<tr><td><code id="log_marginal_lik_ppgasp_+3A_x">X</code></td>
<td>

<p>the mean basis function i.e. the trend function.
</p>
</td></tr>
<tr><td><code id="log_marginal_lik_ppgasp_+3A_zero_mean">zero_mean</code></td>
<td>

<p>the mean basis function is zero or not.
</p>
</td></tr>
<tr><td><code id="log_marginal_lik_ppgasp_+3A_output">output</code></td>
<td>

<p>a matrix where each row is one runs of the computer model output.
</p>
</td></tr>
<tr><td><code id="log_marginal_lik_ppgasp_+3A_kernel_type">kernel_type</code></td>
<td>

<p>type of kernel. <code>matern_3_2</code> and <code>matern_5_2</code> are <code>Matern kernel</code> with roughness parameter 3/2 and 5/2 respectively. <code>pow_exp</code> is power exponential kernel with roughness parameter alpha. If <code>pow_exp</code> is to be used, one needs to specify its roughness parameter alpha.
</p>
</td></tr>
<tr><td><code id="log_marginal_lik_ppgasp_+3A_alpha">alpha</code></td>
<td>

<p>roughness parameters in the kernel functions.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The numerical value of natural logarithm of the marginal likelihood.
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Jesus Palomo [aut],
  James Berger [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>

<p>M. Gu. and J.O. Berger (2016). Parallel partial Gaussian process emulation for computer models with massive output. <em>Annals of Applied Statistics</em>, 10(3), 1317-1347.
</p>
<p>M. Gu. (2016). Robust Uncertainty Quantification and Scalable Computation for Computer Models with Massive Output. Ph.D. thesis. Duke University.
</p>

<hr>
<h2 id='log_profile_lik'>
Natural logarithm of profile likelihood of the robust GaSP model
</h2><span id='topic+log_profile_lik'></span>

<h3>Description</h3>

<p>This function computes the natural logarithm of profile likelihood after  plugging in the maximum likelihood estimator of the mean (trend) and variance parameters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>log_profile_lik(param, nugget, nugget_est, R0, X, zero_mean,output, kernel_type, alpha)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="log_profile_lik_+3A_param">param</code></td>
<td>

<p>a vector of natural logarithm of inverse-range parameters and natural logarithm of the nugget-variance ratio parameter.
</p>
</td></tr>
<tr><td><code id="log_profile_lik_+3A_nugget">nugget</code></td>
<td>

<p>the nugget-variance ratio parameter if this parameter is fixed.
</p>
</td></tr>
<tr><td><code id="log_profile_lik_+3A_nugget_est">nugget_est</code></td>
<td>

<p>Boolean value of whether the nugget is estimated or fixed.
</p>
</td></tr>
<tr><td><code id="log_profile_lik_+3A_r0">R0</code></td>
<td>

<p>a list of matrix where the j-th matrix is an absolute difference matrix of the j-th input vector.
</p>
</td></tr>
<tr><td><code id="log_profile_lik_+3A_x">X</code></td>
<td>

<p>the mean basis function i.e. the trend function.
</p>
</td></tr>
<tr><td><code id="log_profile_lik_+3A_zero_mean">zero_mean</code></td>
<td>

<p>the mean basis function is zero or not.
</p>
</td></tr>
<tr><td><code id="log_profile_lik_+3A_output">output</code></td>
<td>

<p>the output vector.
</p>
</td></tr>
<tr><td><code id="log_profile_lik_+3A_kernel_type">kernel_type</code></td>
<td>

<p>A vector of <code>integer</code> specifying the type of kernels of each coordinate of the input. 
In each coordinate of the vector, 1 means the <code>pow_exp</code> kernel with roughness parameter specified in alpha; 2 means  <code>matern_3_2</code>  kernel; 3 means <code>matern_5_2</code>  kernel; 5 means <code>periodic_gauss</code> kernel; 5 means <code>periodic_exp</code> kernel. 
</p>
</td></tr>
<tr><td><code id="log_profile_lik_+3A_alpha">alpha</code></td>
<td>

<p>Roughness parameters in the kernel functions.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The numerical value of natural logarithm of the profile likelihood.
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Jesus Palomo [aut],
  James Berger [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>

<p>M. Gu. (2016). Robust uncertainty quantification and scalable computation for computer models with massive output. Ph.D. thesis. Duke University.
</p>

<hr>
<h2 id='log_profile_lik_deriv'>
Derivative of natural logarithm of the profile likelihood
</h2><span id='topic+log_profile_lik_deriv'></span>

<h3>Description</h3>

<p>The derivative of natural logarithm of profile likelihood of the Robust GaSP model with regard to inverse range parameters and nugget-variance ratio parameter after plugging in the maximum likelihood estimator of the mean (trend) and variance parameters. When the nugget parameter is fixed, it only computes the derivative with regard to the inverse range parameter; otherwise it produces derivative with regard to inverse range parameter and nugget-variance ratio parameter.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>log_profile_lik_deriv(param, nugget, nugget_est, R0, X, zero_mean, 
output, kernel_type, alpha)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="log_profile_lik_deriv_+3A_param">param</code></td>
<td>

<p>A vector of natural logarithm of inverse-range parameters and natural logarithm of the nugget-variance ratio parameter.
</p>
</td></tr>
<tr><td><code id="log_profile_lik_deriv_+3A_nugget">nugget</code></td>
<td>

<p>The nugget-variance ratio parameter if this parameter is fixed.
</p>
</td></tr>
<tr><td><code id="log_profile_lik_deriv_+3A_nugget_est">nugget_est</code></td>
<td>

<p>Boolean value of whether the nugget is estimated or fixed.
</p>
</td></tr>
<tr><td><code id="log_profile_lik_deriv_+3A_r0">R0</code></td>
<td>

<p>A list of matrix where the j-th matrix is an absolute difference matrix of the j-th input vector.
</p>
</td></tr>
<tr><td><code id="log_profile_lik_deriv_+3A_x">X</code></td>
<td>

<p>The mean basis function i.e. the trend function.
</p>
</td></tr>
<tr><td><code id="log_profile_lik_deriv_+3A_zero_mean">zero_mean</code></td>
<td>

<p>The mean basis function is zero or not.
</p>
</td></tr>
<tr><td><code id="log_profile_lik_deriv_+3A_output">output</code></td>
<td>

<p>The output vector.
</p>
</td></tr>
<tr><td><code id="log_profile_lik_deriv_+3A_kernel_type">kernel_type</code></td>
<td>

<p>A vector of <code>integer</code> specifying the type of kernels of each coordinate of the input. 
In each coordinate of the vector, 1 means the <code>pow_exp</code> kernel with roughness parameter specified in alpha; 2 means  <code>matern_3_2</code>  kernel; 3 means <code>matern_5_2</code>  kernel; 5 means <code>periodic_gauss</code> kernel; 5 means <code>periodic_exp</code> kernel. 
</p>
</td></tr>
<tr><td><code id="log_profile_lik_deriv_+3A_alpha">alpha</code></td>
<td>

<p>Roughness parameters in the kernel functions.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The numerical value of the derivative of natural logarithm of marginal likelihood with regard to range and nugget-variance ratio parameter (if not fixed). When the nugget is fixed, the derivative is on inverse-range parameters.
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Jesus Palomo [aut],
  James Berger [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>

<p>M. Gu. (2016). Robust uncertainty quantification and scalable computation for computer models with massive output. Ph.D. thesis. Duke University.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+log_profile_lik">log_profile_lik</a></code>.
</p>

<hr>
<h2 id='log_profile_lik_deriv_ppgasp'>
Derivative of natural logarithm of the profile likelihood
</h2><span id='topic+log_profile_lik_deriv_ppgasp'></span>

<h3>Description</h3>

<p>The derivative of natural logarithm of profile likelihood of the PP GaSP model with regard to inverse range parameters and nugget-variance ratio parameter after plugging in the maximum likelihood estimator of the mean (trend) and variance parameters. When the nugget is fixed, it only compute the derivative with regard to the inverse range parameter; otherwise it produces derivative with regard to inverse range parameter and nugget-variance ratio parameter.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>log_profile_lik_deriv_ppgasp(param, nugget, nugget_est, R0, X, zero_mean, 
output, kernel_type, alpha)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="log_profile_lik_deriv_ppgasp_+3A_param">param</code></td>
<td>

<p>a vector of natural logarithm of inverse-range parameters and natural logarithm of the nugget-variance ratio parameter.
</p>
</td></tr>
<tr><td><code id="log_profile_lik_deriv_ppgasp_+3A_nugget">nugget</code></td>
<td>

<p>the nugget-variance ratio parameter if this parameter is fixed.
</p>
</td></tr>
<tr><td><code id="log_profile_lik_deriv_ppgasp_+3A_nugget_est">nugget_est</code></td>
<td>

<p>Boolean value of whether the nugget is estimated or fixed.
</p>
</td></tr>
<tr><td><code id="log_profile_lik_deriv_ppgasp_+3A_r0">R0</code></td>
<td>

<p>a list of matrix where the j-th matrix is an absolute difference matrix of the j-th input vector.
</p>
</td></tr>
<tr><td><code id="log_profile_lik_deriv_ppgasp_+3A_x">X</code></td>
<td>

<p>the mean basis function i.e. the trend function.
</p>
</td></tr>
<tr><td><code id="log_profile_lik_deriv_ppgasp_+3A_zero_mean">zero_mean</code></td>
<td>

<p>the mean basis function is zero or not.
</p>
</td></tr>
<tr><td><code id="log_profile_lik_deriv_ppgasp_+3A_output">output</code></td>
<td>

<p>a matrix where each row is one runs of the computer model output.
</p>
</td></tr>
<tr><td><code id="log_profile_lik_deriv_ppgasp_+3A_kernel_type">kernel_type</code></td>
<td>

<p>A vector of <code>integer</code> specifying the type of kernels of each coordinate of the input. 
In each coordinate of the vector, 1 means the <code>pow_exp</code> kernel with roughness parameter specified in alpha; 2 means  <code>matern_3_2</code>  kernel; 3 means <code>matern_5_2</code>  kernel; 5 means <code>periodic_gauss</code> kernel; 5 means <code>periodic_exp</code> kernel. 
</p>
</td></tr>
<tr><td><code id="log_profile_lik_deriv_ppgasp_+3A_alpha">alpha</code></td>
<td>

<p>roughness parameters in the kernel functions.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The numerical value of the derivative of natural logarithm of profile likelihood with regard to range and nugget-variance ratio parameter (if not fixed). When the nugget is fixed, the derivative is on inverse-range parameters.
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Jesus Palomo [aut],
  James Berger [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>

<p>M. Gu. and J.O. Berger (2016). Parallel partial Gaussian process emulation for computer models with massive output. <em>Annals of Applied Statistics</em>, 10(3), 1317-1347.
</p>
<p>M. Gu. (2016). Robust uncertainty quantification and scalable computation for computer models with massive output. Ph.D. thesis. Duke University.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+log_profile_lik_ppgasp">log_profile_lik_ppgasp</a></code>.
</p>

<hr>
<h2 id='log_profile_lik_ppgasp'>
Natural logarithm of profile likelihood of the PP GaSP model
</h2><span id='topic+log_profile_lik_ppgasp'></span>

<h3>Description</h3>

<p>This function computes the natural logarithm of profile likelihood of the PP GaSP model after plugging in the maximum likelihood estimator of the mean (trend) and variance parameters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>log_profile_lik_ppgasp(param, nugget, nugget_est, R0, X, zero_mean
,output,kernel_type, alpha)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="log_profile_lik_ppgasp_+3A_param">param</code></td>
<td>

<p>a vector of natural logarithm of inverse-range parameters and natural logarithm of the nugget-variance ratio parameter.
</p>
</td></tr>
<tr><td><code id="log_profile_lik_ppgasp_+3A_nugget">nugget</code></td>
<td>

<p>the nugget-variance ratio parameter if this parameter is fixed.
</p>
</td></tr>
<tr><td><code id="log_profile_lik_ppgasp_+3A_nugget_est">nugget_est</code></td>
<td>

<p>Boolean value of whether the nugget is estimated or fixed.
</p>
</td></tr>
<tr><td><code id="log_profile_lik_ppgasp_+3A_r0">R0</code></td>
<td>

<p>a List of matrix where the j-th matrix is an absolute difference matrix of the j-th input vector.
</p>
</td></tr>
<tr><td><code id="log_profile_lik_ppgasp_+3A_x">X</code></td>
<td>

<p>the mean basis function i.e. the trend function.
</p>
</td></tr>
<tr><td><code id="log_profile_lik_ppgasp_+3A_zero_mean">zero_mean</code></td>
<td>

<p>the mean basis function is zero or not.
</p>
</td></tr>
<tr><td><code id="log_profile_lik_ppgasp_+3A_output">output</code></td>
<td>

<p>a matrix where each row is one runs of the computer model output.
</p>
</td></tr>
<tr><td><code id="log_profile_lik_ppgasp_+3A_kernel_type">kernel_type</code></td>
<td>

<p>A vector of <code>integer</code> specifying the type of kernels of each coordinate of the input. 
In each coordinate of the vector, 1 means the <code>pow_exp</code> kernel with roughness parameter specified in alpha; 2 means  <code>matern_3_2</code>  kernel; 3 means <code>matern_5_2</code>  kernel; 5 means <code>periodic_gauss</code> kernel; 5 means <code>periodic_exp</code> kernel. 
</p>
</td></tr>
<tr><td><code id="log_profile_lik_ppgasp_+3A_alpha">alpha</code></td>
<td>

<p>roughness parameters in the kernel functions.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The numerical value of natural logarithm of the profile likelihood.
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Jesus Palomo [aut],
  James Berger [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>

<p>M. Gu. and J.O. Berger (2016). Parallel partial Gaussian process emulation for computer models with massive output. <em>Annals of Applied Statistics</em>, 10(3), 1317-1347.
</p>
<p>M. Gu. (2016). Robust Uncertainty Quantification and Scalable Computation for Computer Models with Massive Output. Ph.D. thesis. Duke University.
</p>

<hr>
<h2 id='log_ref_marginal_post'>
Natural logarithm of reference marginal posterior density of the robust GaSP model
</h2><span id='topic+log_ref_marginal_post'></span>

<h3>Description</h3>

<p>This function computes the natural logarithm of marginal posterior density with reference prior of inverse range parameter (beta parameterization) after marginalizing out the mean (trend) and variance parameters  by the location-scale prior.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>log_ref_marginal_post(param, nugget, nugget_est, R0, X, zero_mean,
output, kernel_type, alpha)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="log_ref_marginal_post_+3A_param">param</code></td>
<td>

<p>a vector of natural logarithm of inverse-range parameters and natural logarithm of the nugget-variance ratio parameter.
</p>
</td></tr>
<tr><td><code id="log_ref_marginal_post_+3A_nugget">nugget</code></td>
<td>

<p>the nugget-variance ratio parameter if this parameter is fixed.
</p>
</td></tr>
<tr><td><code id="log_ref_marginal_post_+3A_nugget_est">nugget_est</code></td>
<td>

<p>Boolean value of whether the nugget is estimated or fixed.
</p>
</td></tr>
<tr><td><code id="log_ref_marginal_post_+3A_r0">R0</code></td>
<td>

<p>a list of matrix where the j-th matrix is an absolute difference matrix of the j-th input vector.
</p>
</td></tr>
<tr><td><code id="log_ref_marginal_post_+3A_x">X</code></td>
<td>

<p>the mean basis function i.e. the trend function.
</p>
</td></tr>
<tr><td><code id="log_ref_marginal_post_+3A_zero_mean">zero_mean</code></td>
<td>

<p>the mean basis function is zero or not.
</p>
</td></tr>
<tr><td><code id="log_ref_marginal_post_+3A_output">output</code></td>
<td>

<p>the output vector.
</p>
</td></tr>
<tr><td><code id="log_ref_marginal_post_+3A_kernel_type">kernel_type</code></td>
<td>

<p>type of kernel. <code>matern_3_2</code> and <code>matern_5_2</code> are <code>Matern kernel</code> with roughness parameter 3/2 and 5/2 respectively. <code>pow_exp</code> is power exponential kernel with roughness parameter alpha. If <code>pow_exp</code> is to be used, one needs to specify its roughness parameter alpha.
</p>
</td></tr>
<tr><td><code id="log_ref_marginal_post_+3A_alpha">alpha</code></td>
<td>

<p>roughness parameters in the kernel functions.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The natural logarithm of marginal posterior density with reference prior of inverse range parameter (beta parameterization).
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Jesus Palomo [aut],
  James Berger [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>

<p>Mengyang Gu. (2016). Robust Uncertainty Quantification and Scalable Computation for Computer Models with Massive Output. Ph.D. thesis. Duke University.
</p>

<hr>
<h2 id='log_ref_marginal_post_ppgasp'>
Natural logarithm of reference marginal posterior density of the PP GaSP model
</h2><span id='topic+log_ref_marginal_post_ppgasp'></span>

<h3>Description</h3>

<p>This function computes the natural logarithm of marginal posterior density of the PP GaSP model with reference prior of inverse range parameter (beta parameterization) after marginalizing out the mean (trend) and variance parameters  by the location-scale prior.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>log_ref_marginal_post_ppgasp(param, nugget, nugget_est, R0, X, zero_mean,
output, kernel_type, alpha)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="log_ref_marginal_post_ppgasp_+3A_param">param</code></td>
<td>

<p>a vector of natural logarithm of inverse-range parameters and natural logarithm of the nugget-variance ratio parameter.
</p>
</td></tr>
<tr><td><code id="log_ref_marginal_post_ppgasp_+3A_nugget">nugget</code></td>
<td>

<p>the nugget-variance ratio parameter if this parameter is fixed.
</p>
</td></tr>
<tr><td><code id="log_ref_marginal_post_ppgasp_+3A_nugget_est">nugget_est</code></td>
<td>

<p>Boolean value of whether the nugget is estimated or fixed.
</p>
</td></tr>
<tr><td><code id="log_ref_marginal_post_ppgasp_+3A_r0">R0</code></td>
<td>

<p>a list of matrix where the j-th matrix is an absolute difference matrix of the j-th input vector.
</p>
</td></tr>
<tr><td><code id="log_ref_marginal_post_ppgasp_+3A_x">X</code></td>
<td>

<p>the mean basis function i.e. the trend function.
</p>
</td></tr>
<tr><td><code id="log_ref_marginal_post_ppgasp_+3A_zero_mean">zero_mean</code></td>
<td>

<p>the mean basis function is zero or not.
</p>
</td></tr>
<tr><td><code id="log_ref_marginal_post_ppgasp_+3A_output">output</code></td>
<td>

<p>the output matrix.
</p>
</td></tr>
<tr><td><code id="log_ref_marginal_post_ppgasp_+3A_kernel_type">kernel_type</code></td>
<td>

<p>type of kernel. <code>matern_3_2</code> and <code>matern_5_2</code> are <code>Matern kernel</code> with roughness parameter 3/2 and 5/2 respectively. <code>pow_exp</code> is power exponential kernel with roughness parameter alpha. If <code>pow_exp</code> is to be used, one needs to specify its roughness parameter alpha.
</p>
</td></tr>
<tr><td><code id="log_ref_marginal_post_ppgasp_+3A_alpha">alpha</code></td>
<td>

<p>roughness parameters in the kernel functions.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The natural logarithm of marginal posterior density with reference prior of inverse range parameter (beta parameterization).
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Jesus Palomo [aut],
  James Berger [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>

<p>M. Gu. and J.O. Berger (2016). Parallel partial Gaussian process emulation for computer models with massive output. <em>Annals of Applied Statistics</em>, 10(3), 1317-1347.
</p>
<p>M. Gu. (2016). Robust Uncertainty Quantification and Scalable Computation for Computer Models with Massive Output. Ph.D. thesis. Duke University.
</p>

<hr>
<h2 id='matern_3_2_deriv'>
The derivative of matern correlation function (roughness parameter equal to 3/2) with regard to the inverse range parameter </h2><span id='topic+matern_3_2_deriv'></span>

<h3>Description</h3>

<p>This function computes the derivative of a correlation matrix using 1-dimensional matern correlation function (roughness parameter equal to 3/2), with regard to the inverse range parameter.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>matern_3_2_deriv(R0_i, R, beta_i)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="matern_3_2_deriv_+3A_r0_i">R0_i</code></td>
<td>

<p>an absolute difference matrix of the i-th input vector.
</p>
</td></tr>
<tr><td><code id="matern_3_2_deriv_+3A_r">R</code></td>
<td>

<p>the correlation matrix. 
</p>
</td></tr>
<tr><td><code id="matern_3_2_deriv_+3A_beta_i">beta_i</code></td>
<td>

<p>the inverse range parameter.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A matrix in which each element is the derivative of 1-dimensional matern correlation function with regard to the inverse range parameter.
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Jesus Palomo [aut],
  James Berger [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>See Also</h3>

<p><code><a href="#topic+matern_3_2_funct">matern_3_2_funct</a></code>.
</p>

<hr>
<h2 id='matern_3_2_funct'>
Matern correlation function with roughness parameter equal to 3/2. </h2><span id='topic+matern_3_2_funct'></span>

<h3>Description</h3>

<p>This function computes the values of Matern correlation function with roughness parameter equal to 3/2.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>matern_3_2_funct(d, beta_i)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="matern_3_2_funct_+3A_d">d</code></td>
<td>

<p>locations the Matern correlation function are to be evaluated.
</p>
</td></tr>
<tr><td><code id="matern_3_2_funct_+3A_beta_i">beta_i</code></td>
<td>

<p>the inverse range parameter.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A matrix in which each element is the value of the Matern correlation function with roughness parameter equal to 1.5 evaluated at that location. 
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Jesus Palomo [aut],
  James Berger [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>

<hr>
<h2 id='matern_5_2_deriv'>
The derivative of matern correlation function with regard to the inverse range parameter </h2><span id='topic+matern_5_2_deriv'></span>

<h3>Description</h3>

<p>This function computes the derivative of a correlation matrix using 1-dimensional matern correlation function, with regard to the inverse range parameter.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>matern_5_2_deriv(R0_i, R, beta_i)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="matern_5_2_deriv_+3A_r0_i">R0_i</code></td>
<td>

<p>an absolute difference matrix of the i-th input vector.
</p>
</td></tr>
<tr><td><code id="matern_5_2_deriv_+3A_r">R</code></td>
<td>

<p>the correlation matrix. 
</p>
</td></tr>
<tr><td><code id="matern_5_2_deriv_+3A_beta_i">beta_i</code></td>
<td>

<p>the inverse range parameter.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A matrix in which each element is the derivative of 1-dimensional matern correlation function with regard to the inverse range parameter.
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Jesus Palomo [aut],
  James Berger [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>See Also</h3>

<p><code><a href="#topic+matern_5_2_funct">matern_5_2_funct</a></code>.
</p>

<hr>
<h2 id='matern_5_2_funct'>
Matern correlation function with roughness parameter equal to 5/2. </h2><span id='topic+matern_5_2_funct'></span>

<h3>Description</h3>

<p>This function computes the values of Matern correlation function with roughness parameter equal to 5/2.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>matern_5_2_funct(d, beta_i)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="matern_5_2_funct_+3A_d">d</code></td>
<td>

<p>locations the Matern correlation function are to be evaluated.
</p>
</td></tr>
<tr><td><code id="matern_5_2_funct_+3A_beta_i">beta_i</code></td>
<td>

<p>the inverse range parameter.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A matrix in which each element is the value of the Matern correlation function with roughness parameter equal to 2.5 evaluated at that location. 
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Jesus Palomo [aut],
  James Berger [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>

<hr>
<h2 id='neg_log_marginal_post_approx_ref'> Natural logarithm of approximate reference marginal posterior density of the robust GaSP model
</h2><span id='topic+neg_log_marginal_post_approx_ref'></span>

<h3>Description</h3>

<p>Natural logarithm of marginal posterior density with the approximate reference prior of inverse range parameter after marginalizing out the mean (trend) and variance parameters  by the location-scale prior.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>neg_log_marginal_post_approx_ref(param, nugget, nugget.est
,R0, X, zero_mean,output, CL, a, b,kernel_type, alpha)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="neg_log_marginal_post_approx_ref_+3A_param">param</code></td>
<td>

<p>a vector of natural logarithm of inverse-range parameters and natural logarithm of the nugget-variance ratio parameter.
</p>
</td></tr>
<tr><td><code id="neg_log_marginal_post_approx_ref_+3A_nugget">nugget</code></td>
<td>

<p>the nugget-variance ratio parameter if this parameter is fixed.
</p>
</td></tr>
<tr><td><code id="neg_log_marginal_post_approx_ref_+3A_nugget.est">nugget.est</code></td>
<td>

<p>Boolean value of whether the nugget is estimated or fixed.
</p>
</td></tr>
<tr><td><code id="neg_log_marginal_post_approx_ref_+3A_r0">R0</code></td>
<td>

<p>a list of matrix where the j-th matrix is an absolute difference matrix of the j-th input vector.
</p>
</td></tr>
<tr><td><code id="neg_log_marginal_post_approx_ref_+3A_x">X</code></td>
<td>

<p>the mean basis function i.e. the trend function.
</p>
</td></tr>
<tr><td><code id="neg_log_marginal_post_approx_ref_+3A_zero_mean">zero_mean</code></td>
<td>

<p>the mean basis function is zero or not.
</p>
</td></tr>
<tr><td><code id="neg_log_marginal_post_approx_ref_+3A_output">output</code></td>
<td>

<p>the output vector.
</p>
</td></tr>
<tr><td><code id="neg_log_marginal_post_approx_ref_+3A_cl">CL</code></td>
<td>

<p>prior parameters in the approximate reference prior.
</p>
</td></tr>
<tr><td><code id="neg_log_marginal_post_approx_ref_+3A_a">a</code></td>
<td>

<p>prior parameter in the approximate reference prior.
</p>
</td></tr>
<tr><td><code id="neg_log_marginal_post_approx_ref_+3A_b">b</code></td>
<td>

<p>prior parameter in the approximate reference prior.
</p>
</td></tr>
<tr><td><code id="neg_log_marginal_post_approx_ref_+3A_kernel_type">kernel_type</code></td>
<td>

<p>type of kernel. <code>matern_3_2</code> and <code>matern_5_2</code> are <code>Matern kernel</code> with roughness parameter 3/2 and 5/2 respectively. <code>pow_exp</code> is power exponential kernel with roughness parameter alpha. If <code>pow_exp</code> is to be used, one needs to specify its roughness parameter alpha.
</p>
</td></tr>
<tr><td><code id="neg_log_marginal_post_approx_ref_+3A_alpha">alpha</code></td>
<td>

<p>roughness parameters in the kernel functions.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The natural logarithm of the marginal posterior density with approximate reference prior of inverse range parameter (beta parameterization). 
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Jesus Palomo [aut],
  James Berger [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>

<p>Mengyang Gu. (2016). Robust Uncertainty Quantification and Scalable Computation for Computer Models with Massive Output. Ph.D. thesis. Duke University.
</p>

<hr>
<h2 id='neg_log_marginal_post_approx_ref_deriv'>
Derivative of negative natural logarithm of approximate reference marginal posterior density
</h2><span id='topic+neg_log_marginal_post_approx_ref_deriv'></span>

<h3>Description</h3>

<p>The function computes the derivative (with regard to log of inverse range parameter) of natural logarithm of marginal posterior density with the jointly robust prior prior after marginalizing out the mean (trend) and variance parameters  by the location-scale prior.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>neg_log_marginal_post_approx_ref_deriv(param, nugget, nugget.est, 
                                       R0, X, zero_mean,output, CL, a, b, 
                                      kernel_type, alpha)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="neg_log_marginal_post_approx_ref_deriv_+3A_param">param</code></td>
<td>

<p>A vector of natural logarithm of inverse-range parameters and natural logarithm of the nugget-variance ratio parameter.
</p>
</td></tr>
<tr><td><code id="neg_log_marginal_post_approx_ref_deriv_+3A_nugget">nugget</code></td>
<td>

<p>The nugget-variance ratio parameter if this parameter is fixed.
</p>
</td></tr>
<tr><td><code id="neg_log_marginal_post_approx_ref_deriv_+3A_nugget.est">nugget.est</code></td>
<td>

<p>Boolean value of whether the nugget is estimated or fixed. 
</p>
</td></tr>
<tr><td><code id="neg_log_marginal_post_approx_ref_deriv_+3A_r0">R0</code></td>
<td>

<p>A List of matrix where the j-th matrix is an absolute difference matrix of the j-th input vector.
</p>
</td></tr>
<tr><td><code id="neg_log_marginal_post_approx_ref_deriv_+3A_x">X</code></td>
<td>

<p>The mean basis function i.e. the trend function.
</p>
</td></tr>
<tr><td><code id="neg_log_marginal_post_approx_ref_deriv_+3A_zero_mean">zero_mean</code></td>
<td>

<p>The mean basis function is zero or not.
</p>
</td></tr>
<tr><td><code id="neg_log_marginal_post_approx_ref_deriv_+3A_output">output</code></td>
<td>

<p>The output vector.
</p>
</td></tr>
<tr><td><code id="neg_log_marginal_post_approx_ref_deriv_+3A_cl">CL</code></td>
<td>

<p>Pseudoparameter in the approximate reference prior.
</p>
</td></tr>
<tr><td><code id="neg_log_marginal_post_approx_ref_deriv_+3A_a">a</code></td>
<td>

<p>Pseudoparameter in the approximate reference prior.
</p>
</td></tr>
<tr><td><code id="neg_log_marginal_post_approx_ref_deriv_+3A_b">b</code></td>
<td>

<p>Pseudoparameter in the approximate reference prior.
</p>
</td></tr>
<tr><td><code id="neg_log_marginal_post_approx_ref_deriv_+3A_kernel_type">kernel_type</code></td>
<td>

<p>Type of kernel. <code>matern_3_2</code> and <code>matern_5_2</code> are <code>Matern kernel</code> with roughness parameter 3/2 and 5/2 respectively. <code>pow_exp</code> is power exponential kernel with roughness parameter alpha. If <code>pow_exp</code> is to be used, one needs to specify its roughness parameter alpha.
</p>
</td></tr>
<tr><td><code id="neg_log_marginal_post_approx_ref_deriv_+3A_alpha">alpha</code></td>
<td>

<p>Roughness parameters in the kernel functions.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The derivative of natural logarithm of marginal posterior density with jointly robust prior prior.
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Jesus Palomo [aut],
  James Berger [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>

<p>Mengyang Gu. (2016). Robust Uncertainty Quantification and Scalable Computation for Computer Models with Massive Output. Ph.D. thesis. Duke University.
</p>
<p>M. Gu (2018), <em>Jointly Robust Prior for Gaussian Stochastic Process in Emulation, Calibration and Variable Selection</em>, arXiv:1804.09329.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+neg_log_marginal_post_approx_ref">neg_log_marginal_post_approx_ref</a></code>.
</p>

<hr>
<h2 id='neg_log_marginal_post_approx_ref_deriv_ppgasp'>
Derivative of negative natural logarithm of approximate reference marginal posterior density of the PP GaSP model
</h2><span id='topic+neg_log_marginal_post_approx_ref_deriv_ppgasp'></span>

<h3>Description</h3>

<p>The function computes the derivative (with regard to log of inverse range parameter) of natural logarithm of marginal posterior density of the PP GaSP model with the jointly robust  prior after marginalizing out the mean (trend) and variance parameters  by the location-scale prior.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>neg_log_marginal_post_approx_ref_deriv_ppgasp(param, nugget, nugget.est, 
                                       R0, X, zero_mean,output, CL, a, b, 
                                      kernel_type, alpha)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="neg_log_marginal_post_approx_ref_deriv_ppgasp_+3A_param">param</code></td>
<td>

<p>A vector of natural logarithm of inverse-range parameters and natural logarithm of the nugget-variance ratio parameter.
</p>
</td></tr>
<tr><td><code id="neg_log_marginal_post_approx_ref_deriv_ppgasp_+3A_nugget">nugget</code></td>
<td>

<p>The nugget-variance ratio parameter if this parameter is fixed.
</p>
</td></tr>
<tr><td><code id="neg_log_marginal_post_approx_ref_deriv_ppgasp_+3A_nugget.est">nugget.est</code></td>
<td>

<p>Boolean value of whether the nugget is estimated or fixed. 
</p>
</td></tr>
<tr><td><code id="neg_log_marginal_post_approx_ref_deriv_ppgasp_+3A_r0">R0</code></td>
<td>

<p>A List of matrix where the j-th matrix is an absolute difference matrix of the j-th input vector.
</p>
</td></tr>
<tr><td><code id="neg_log_marginal_post_approx_ref_deriv_ppgasp_+3A_x">X</code></td>
<td>

<p>The mean basis function i.e. the trend function.
</p>
</td></tr>
<tr><td><code id="neg_log_marginal_post_approx_ref_deriv_ppgasp_+3A_zero_mean">zero_mean</code></td>
<td>

<p>The mean basis function is zero or not.
</p>
</td></tr>
<tr><td><code id="neg_log_marginal_post_approx_ref_deriv_ppgasp_+3A_output">output</code></td>
<td>

<p>The output matrix.
</p>
</td></tr>
<tr><td><code id="neg_log_marginal_post_approx_ref_deriv_ppgasp_+3A_cl">CL</code></td>
<td>

<p>Pseudoparameter in the approximate reference prior.
</p>
</td></tr>
<tr><td><code id="neg_log_marginal_post_approx_ref_deriv_ppgasp_+3A_a">a</code></td>
<td>

<p>Pseudoparameter in the approximate reference prior.
</p>
</td></tr>
<tr><td><code id="neg_log_marginal_post_approx_ref_deriv_ppgasp_+3A_b">b</code></td>
<td>

<p>Pseudoparameter in the approximate reference prior.
</p>
</td></tr>
<tr><td><code id="neg_log_marginal_post_approx_ref_deriv_ppgasp_+3A_kernel_type">kernel_type</code></td>
<td>

<p>A vector of <code>integer</code> specifying the type of kernels of each coordinate of the input. 
In each coordinate of the vector, 1 means the <code>pow_exp</code> kernel with roughness parameter specified in alpha; 2 means  <code>matern_3_2</code>  kernel; 3 means <code>matern_5_2</code>  kernel; 5 means <code>periodic_gauss</code> kernel; 5 means <code>periodic_exp</code> kernel. 
</p>
</td></tr>
<tr><td><code id="neg_log_marginal_post_approx_ref_deriv_ppgasp_+3A_alpha">alpha</code></td>
<td>

<p>Roughness parameters in the kernel functions.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The derivative of natural logarithm of marginal posterior density with the jointly robust prior.
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Jesus Palomo [aut],
  James Berger [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>

<p>M. Gu. and J.O. Berger (2016). Parallel partial Gaussian process emulation for computer models with massive output. <em>Annals of Applied Statistics</em>, 10(3), 1317-1347.
</p>
<p>M. Gu. (2016). Robust uncertainty quantification and scalable computation for computer models with massive output. Ph.D. thesis. Duke University.
</p>
<p>M. Gu (2018), <em>Jointly robust prior for Gaussian stochastic process in emulation, calibration and variable selection</em>, arXiv:1804.09329.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+neg_log_marginal_post_approx_ref_ppgasp">neg_log_marginal_post_approx_ref_ppgasp</a></code>.
</p>

<hr>
<h2 id='neg_log_marginal_post_approx_ref_ppgasp'>Natural logarithm of approximate reference marginal posterior density of the PP GaSP model
</h2><span id='topic+neg_log_marginal_post_approx_ref_ppgasp'></span>

<h3>Description</h3>

<p>Natural logarithm of marginal posterior density with the jointly robust prior of inverse range parameter of the PP GaSP model after marginalizing out the mean (trend) and variance parameters  by the location-scale prior.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>neg_log_marginal_post_approx_ref_ppgasp(param, nugget, 
nugget.est, R0, X, zero_mean,output, CL, a, b,kernel_type, alpha)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="neg_log_marginal_post_approx_ref_ppgasp_+3A_param">param</code></td>
<td>

<p>A vector of natural logarithm of inverse-range parameters and natural logarithm of the nugget-variance ratio parameter.
</p>
</td></tr>
<tr><td><code id="neg_log_marginal_post_approx_ref_ppgasp_+3A_nugget">nugget</code></td>
<td>

<p>The nugget-variance ratio parameter if this parameter is fixed.
</p>
</td></tr>
<tr><td><code id="neg_log_marginal_post_approx_ref_ppgasp_+3A_nugget.est">nugget.est</code></td>
<td>

<p>Boolean value of whether the nugget is estimated or fixed.
</p>
</td></tr>
<tr><td><code id="neg_log_marginal_post_approx_ref_ppgasp_+3A_r0">R0</code></td>
<td>

<p>A List of matrix where the j-th matrix is an absolute difference matrix of the j-th input vector.
</p>
</td></tr>
<tr><td><code id="neg_log_marginal_post_approx_ref_ppgasp_+3A_x">X</code></td>
<td>

<p>The mean basis function i.e. the trend function.
</p>
</td></tr>
<tr><td><code id="neg_log_marginal_post_approx_ref_ppgasp_+3A_zero_mean">zero_mean</code></td>
<td>

<p>The mean basis function is zero or not.
</p>
</td></tr>
<tr><td><code id="neg_log_marginal_post_approx_ref_ppgasp_+3A_output">output</code></td>
<td>

<p>a matrix where each row is one runs of the computer model output.
</p>
</td></tr>
<tr><td><code id="neg_log_marginal_post_approx_ref_ppgasp_+3A_cl">CL</code></td>
<td>

<p>prior parameters in the approximate reference prior.
</p>
</td></tr>
<tr><td><code id="neg_log_marginal_post_approx_ref_ppgasp_+3A_a">a</code></td>
<td>

<p>prior parameter in the approximate reference prior.
</p>
</td></tr>
<tr><td><code id="neg_log_marginal_post_approx_ref_ppgasp_+3A_b">b</code></td>
<td>

<p>prior parameter in the approximate reference prior.
</p>
</td></tr>
<tr><td><code id="neg_log_marginal_post_approx_ref_ppgasp_+3A_kernel_type">kernel_type</code></td>
<td>

<p>A vector of <code>integer</code> specifying the type of kernels of each coordinate of the input. 
In each coordinate of the vector, 1 means the <code>pow_exp</code> kernel with roughness parameter specified in alpha; 2 means  <code>matern_3_2</code>  kernel; 3 means <code>matern_5_2</code>  kernel; 5 means <code>periodic_gauss</code> kernel; 5 means <code>periodic_exp</code> kernel. 
</p>
</td></tr>
<tr><td><code id="neg_log_marginal_post_approx_ref_ppgasp_+3A_alpha">alpha</code></td>
<td>

<p>Roughness parameters in the kernel functions.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The natural logarithm of the marginal posterior density with the jointly robust prior prior of inverse range parameter (beta parameterization). 
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Jesus Palomo [aut],
  James Berger [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>

<p>M. Gu. and J.O. Berger (2016). Parallel partial Gaussian process emulation for computer models with massive output. <em>Annals of Applied Statistics</em>, 10(3), 1317-1347.
</p>
<p>M. Gu. (2016). Robust uncertainty quantification and scalable computation for computer models with massive output. Ph.D. thesis. Duke University.
</p>
<p>M. Gu (2018), <em>Jointly Robust Prior for Gaussian Stochastic Process in Emulation, Calibration and Variable Selection</em>, arXiv:1804.09329.
</p>

<hr>
<h2 id='neg_log_marginal_post_ref'>
Negative natural logarithm of reference marginal posterior density of the robust GaSP model with regard to a specific parameterization.
</h2><span id='topic+neg_log_marginal_post_ref'></span>

<h3>Description</h3>

<p>Negative natural logarithm of marginal posterior density (with regard to a specific parameterization) with reference prior of inverse range parameter (beta parameterization) after marginalizing out the mean (trend) and variance parameters  by the location-scale prior.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>neg_log_marginal_post_ref(param, nugget, nugget.est, 
                          R0, X, zero_mean,output, prior_choice, 
                          kernel_type, alpha)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="neg_log_marginal_post_ref_+3A_param">param</code></td>
<td>

<p>a vector of natural logarithm of inverse-range parameters and natural logarithm of the noise-variance ratio parameter.
</p>
</td></tr>
<tr><td><code id="neg_log_marginal_post_ref_+3A_nugget">nugget</code></td>
<td>

<p>the noise-variance ratio parameter if this parameter is fixed.
</p>
</td></tr>
<tr><td><code id="neg_log_marginal_post_ref_+3A_nugget.est">nugget.est</code></td>
<td>

<p>Boolean value of whether the nugget is estimated or fixed.
</p>
</td></tr>
<tr><td><code id="neg_log_marginal_post_ref_+3A_r0">R0</code></td>
<td>

<p>a list of matrix where the j-th matrix is an absolute difference matrix of the j-th input vector.
</p>
</td></tr>
<tr><td><code id="neg_log_marginal_post_ref_+3A_x">X</code></td>
<td>

<p>the mean basis function i.e. the trend function.
</p>
</td></tr>
<tr><td><code id="neg_log_marginal_post_ref_+3A_zero_mean">zero_mean</code></td>
<td>

<p>the mean basis function is zero or not.
</p>
</td></tr>
<tr><td><code id="neg_log_marginal_post_ref_+3A_output">output</code></td>
<td>

<p>the output vector.
</p>
</td></tr>
<tr><td><code id="neg_log_marginal_post_ref_+3A_prior_choice">prior_choice</code></td>
<td>

<p>parameterization: <code>ref_xi</code> for log inverse range parameterization or <code>ref_gamma</code> for range parameterization.
</p>
</td></tr>
<tr><td><code id="neg_log_marginal_post_ref_+3A_kernel_type">kernel_type</code></td>
<td>

<p>type of kernel. <code>matern_3_2</code> and <code>matern_5_2</code> are <code>Matern kernel</code> with roughness parameter 3/2 and 5/2 respectively. <code>pow_exp</code> is power exponential kernel with roughness parameter alpha. If <code>pow_exp</code> is to be used, one needs to specify its roughness parameter alpha.
</p>
</td></tr>
<tr><td><code id="neg_log_marginal_post_ref_+3A_alpha">alpha</code></td>
<td>

<p>roughness parameters in the kernel functions.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The negative natural logarithm of marginal posterior density with reference prior with regard to a specific parameterization.
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Jesus Palomo [aut],
  James Berger [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>

<p>Mengyang Gu. (2016). Robust Uncertainty Quantification and Scalable Computation for Computer Models with Massive Output. Ph.D. thesis. Duke University.
</p>

<hr>
<h2 id='neg_log_marginal_post_ref_ppgasp'>
Negative natural logarithm of reference marginal posterior density of the PP GaSP model with regard to a specific parameterization.
</h2><span id='topic+neg_log_marginal_post_ref_ppgasp'></span>

<h3>Description</h3>

<p>Negative natural logarithm of marginal posterior density (with regard to a specific 
parameterization) of the PP GaSP model with reference prior of inverse range parameter 
(beta parameterization) after marginalizing out the mean (trend) and variance parameters  
by the location-scale prior.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>neg_log_marginal_post_ref_ppgasp(param, nugget, nugget.est
,R0, X, zero_mean,output, prior_choice,kernel_type, alpha)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="neg_log_marginal_post_ref_ppgasp_+3A_param">param</code></td>
<td>

<p>a vector of natural logarithm of inverse-range parameters and natural logarithm of the noise-variance ratio parameter.
</p>
</td></tr>
<tr><td><code id="neg_log_marginal_post_ref_ppgasp_+3A_nugget">nugget</code></td>
<td>

<p>the noise-variance ratio parameter if this parameter is fixed.
</p>
</td></tr>
<tr><td><code id="neg_log_marginal_post_ref_ppgasp_+3A_nugget.est">nugget.est</code></td>
<td>

<p>Boolean value of whether the nugget is estimated or fixed.
</p>
</td></tr>
<tr><td><code id="neg_log_marginal_post_ref_ppgasp_+3A_r0">R0</code></td>
<td>

<p>a list of matrix where the j-th matrix is an absolute difference matrix of the j-th input vector.
</p>
</td></tr>
<tr><td><code id="neg_log_marginal_post_ref_ppgasp_+3A_x">X</code></td>
<td>

<p>the mean basis function i.e. the trend function.
</p>
</td></tr>
<tr><td><code id="neg_log_marginal_post_ref_ppgasp_+3A_zero_mean">zero_mean</code></td>
<td>

<p>the mean basis function is zero or not.
</p>
</td></tr>
<tr><td><code id="neg_log_marginal_post_ref_ppgasp_+3A_output">output</code></td>
<td>

<p>the output matrix.
</p>
</td></tr>
<tr><td><code id="neg_log_marginal_post_ref_ppgasp_+3A_prior_choice">prior_choice</code></td>
<td>

<p>parameterization: <code>ref_xi</code> for log inverse range parameterization or <code>ref_gamma</code> for range parameterization.
</p>
</td></tr>
<tr><td><code id="neg_log_marginal_post_ref_ppgasp_+3A_kernel_type">kernel_type</code></td>
<td>

<p>type of kernel. <code>matern_3_2</code> and <code>matern_5_2</code> are <code>Matern kernel</code> with roughness parameter 3/2 and 5/2 respectively. <code>pow_exp</code> is power exponential kernel with roughness parameter alpha. If <code>pow_exp</code> is to be used, one needs to specify its roughness parameter alpha.
</p>
</td></tr>
<tr><td><code id="neg_log_marginal_post_ref_ppgasp_+3A_alpha">alpha</code></td>
<td>

<p>roughness parameters in the kernel functions.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The negative natural logarithm of marginal posterior density with reference prior with regard to a specific parameterization.
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Jesus Palomo [aut],
  James Berger [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>

<p>M. Gu. and J.O. Berger (2016). Parallel partial Gaussian process emulation for computer models with massive output. <em>Annals of Applied Statistics</em>, 10(3), 1317-1347.
</p>
<p>M. Gu. (2016). Robust Uncertainty Quantification and Scalable Computation for Computer Models with Massive Output. Ph.D. thesis. Duke University.
</p>
<p>M. Gu (2018), <em>Jointly Robust Prior for Gaussian Stochastic Process in Emulation, Calibration and Variable Selection</em>, arXiv:1804.09329.
</p>

<hr>
<h2 id='periodic_exp_deriv'>
The derivative of periodic exponential correlation function with regard to the inverse range parameter </h2><span id='topic+periodic_exp_deriv'></span>

<h3>Description</h3>

<p>The function computes the derivative of a correlation matrix parameterized by the periodic exponential correlation function, with regard to the inverse range parameter.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>periodic_exp_deriv(R0_i, R, beta_i)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="periodic_exp_deriv_+3A_r0_i">R0_i</code></td>
<td>

<p>an absolute difference matrix of the i-th input vector.
</p>
</td></tr>
<tr><td><code id="periodic_exp_deriv_+3A_r">R</code></td>
<td>

<p>the correlation matrix. 
</p>
</td></tr>
<tr><td><code id="periodic_exp_deriv_+3A_beta_i">beta_i</code></td>
<td>

</td></tr>
</table>
<p>the inverse range parameter.
</p>


<h3>Value</h3>

<p>A matrix in which each element is the derivative of periodic exponential correlation function with regard to the inverse range parameter.
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Jesus Palomo [aut],
  James Berger [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>See Also</h3>

<p><code><a href="#topic+periodic_exp_funct">periodic_exp_funct</a></code>.
</p>

<hr>
<h2 id='periodic_exp_funct'>
Periodic folding of the exponential correlation function . </h2><span id='topic+periodic_exp_funct'></span>

<h3>Description</h3>

<p>This function computes the values of periodic folding of the exponential correlation function (i.e. the power expoential correlation with roughness parameter being 2).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>periodic_exp_funct(d, beta_i)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="periodic_exp_funct_+3A_d">d</code></td>
<td>

<p>locations the periodic exponential correlation function are to be evaluated.
</p>
</td></tr>
<tr><td><code id="periodic_exp_funct_+3A_beta_i">beta_i</code></td>
<td>

<p>the inverse range parameter.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A matrix in which each element is the value of the periodic folding of the exponential correlation function  evaluated at that location. 
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Jesus Palomo [aut],
  James Berger [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>

<hr>
<h2 id='periodic_gauss_deriv'>
The derivative of periodic Gaussian correlation function with regard to the inverse range parameter </h2><span id='topic+periodic_gauss_deriv'></span>

<h3>Description</h3>

<p>The function computes the derivative of a correlation matrix parameterized by the periodic Gaussian correlation function, with regard to the inverse range parameter.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>periodic_gauss_deriv(R0_i, R, beta_i)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="periodic_gauss_deriv_+3A_r0_i">R0_i</code></td>
<td>

<p>an absolute difference matrix of the i-th input vector.
</p>
</td></tr>
<tr><td><code id="periodic_gauss_deriv_+3A_r">R</code></td>
<td>

<p>the correlation matrix. 
</p>
</td></tr>
<tr><td><code id="periodic_gauss_deriv_+3A_beta_i">beta_i</code></td>
<td>

</td></tr>
</table>
<p>the inverse range parameter.
</p>


<h3>Value</h3>

<p>A matrix in which each element is the derivative of periodic Gaussian correlation function with regard to the inverse range parameter.
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Jesus Palomo [aut],
  James Berger [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>See Also</h3>

<p><code><a href="#topic+periodic_gauss_funct">periodic_gauss_funct</a></code>.
</p>

<hr>
<h2 id='periodic_gauss_funct'>
Periodic folding of the Gaussian correlation function . </h2><span id='topic+periodic_gauss_funct'></span>

<h3>Description</h3>

<p>This function computes the values of periodic folding of the Gaussian correlation function (i.e. the power expoential correlation with roughness parameter being 2).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>periodic_gauss_funct(d, beta_i)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="periodic_gauss_funct_+3A_d">d</code></td>
<td>

<p>locations the periodic Gaussian correlation function are to be evaluated.
</p>
</td></tr>
<tr><td><code id="periodic_gauss_funct_+3A_beta_i">beta_i</code></td>
<td>

<p>the inverse range parameter.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A matrix in which each element is the value of the periodic folding of the Gaussian correlation function  evaluated at that location. 
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Jesus Palomo [aut],
  James Berger [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>

<hr>
<h2 id='plot'>
Plot for Robust GaSP model
</h2><span id='topic+plot'></span><span id='topic+plot.rgasp'></span><span id='topic+plot+2Crgasp-method'></span>

<h3>Description</h3>

<p>Function to make plots on Robust GaSP models after the Robust GaSP model has been constructed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'rgasp'
plot(x,y, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_+3A_x">x</code></td>
<td>
<p> an object of  class <code>rgasp</code>.</p>
</td></tr>
<tr><td><code id="plot_+3A_y">y</code></td>
<td>
<p> not used.  </p>
</td></tr>
<tr><td><code id="plot_+3A_...">...</code></td>
<td>
<p> additional arguments not implemented yet.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Three plots: the leave-one-out fitted values versus exact values, standardized residuals and QQ plot.
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Jesus Palomo [aut],
  James Berger [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>

<p>M. Gu. (2016). Robust uncertainty quantification and scalable computation for computer models with massive output. Ph.D. thesis. Duke University.
</p>


<h3>Examples</h3>

<pre><code class='language-R'> library(RobustGaSP)
  #------------------------
  # a 3 dimensional example
  #------------------------
  # dimensional of the inputs
  dim_inputs &lt;- 3    
  # number of the inputs
  num_obs &lt;- 30       
  # uniform samples of design
  input &lt;- matrix(runif(num_obs*dim_inputs), num_obs,dim_inputs) 
  
  # Following codes use maximin Latin Hypercube Design, which is typically better than uniform
  # library(lhs)
  # input &lt;- maximinLHS(n=num_obs, k=dim_inputs)  ##maximin lhd sample
  
  # outputs from the 3 dim dettepepel.3.data function
  
  output = matrix(0,num_obs,1)
  for(i in 1:num_obs){
    output[i]&lt;-dettepepel.3.data (input[i,])
  }
  
  # use constant mean basis, with no constraint on optimization
  m1&lt;- rgasp(design = input, response = output, lower_bound=FALSE)
 
  # plot
  plot(m1)
</code></pre>

<hr>
<h2 id='pow_exp_deriv'>
The derivative of power exponential correlation function with regard to the inverse range parameter </h2><span id='topic+pow_exp_deriv'></span>

<h3>Description</h3>

<p>The function computes the derivative of a correlation matrix using 1-dimensional power exponential correlation function, with regard to the inverse range parameter.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pow_exp_deriv(R0_i, R, beta_i, alpha_i)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pow_exp_deriv_+3A_r0_i">R0_i</code></td>
<td>

<p>an absolute difference matrix of the i-th input vector.
</p>
</td></tr>
<tr><td><code id="pow_exp_deriv_+3A_r">R</code></td>
<td>

<p>the correlation matrix. 
</p>
</td></tr>
<tr><td><code id="pow_exp_deriv_+3A_beta_i">beta_i</code></td>
<td>

</td></tr>
</table>
<p>the inverse range parameter.
</p>
<table>
<tr><td><code id="pow_exp_deriv_+3A_alpha_i">alpha_i</code></td>
<td>

<p>the  roughness parameter.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A matrix in which each element is the derivative of 1-dimensional power exponential correlation function with regard to the inverse range parameter.
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Jesus Palomo [aut],
  James Berger [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pow_exp_funct">pow_exp_funct</a></code>.
</p>

<hr>
<h2 id='pow_exp_funct'>
Pow exponential correlation function. </h2><span id='topic+pow_exp_funct'></span>

<h3>Description</h3>

<p>This function computes the values of power exponential correlation function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pow_exp_funct(d, beta_i, alpha_i)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pow_exp_funct_+3A_d">d</code></td>
<td>

<p>locations the power exponential correlation function are to be evaluated.
</p>
</td></tr>
<tr><td><code id="pow_exp_funct_+3A_beta_i">beta_i</code></td>
<td>

<p>the inverse range parameter.
</p>
</td></tr>
<tr><td><code id="pow_exp_funct_+3A_alpha_i">alpha_i</code></td>
<td>

<p>the roughness parameter.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A matrix in which each element is the value of the power exponential function  evaluated at that location. 
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Jesus Palomo [aut],
  James Berger [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>

<hr>
<h2 id='ppgasp'> Setting up the parallel partial GaSP model
</h2><span id='topic+ppgasp'></span><span id='topic+ppgasp-method'></span>

<h3>Description</h3>

<p>Setting up the parallel partial GaSP model for estimating the parameters (if the parameters are not given). 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>   ppgasp(design,response,trend=matrix(1,dim(response)[1],1),zero.mean="No",nugget=0,
    nugget.est=F,range.par=NA,method='post_mode',prior_choice='ref_approx',a=0.2,
    b=1/(length(response))^{1/dim(as.matrix(design))[2]}*(a+dim(as.matrix(design))[2]),
    kernel_type='matern_5_2',isotropic=F,R0=NA,
    optimization='lbfgs',
    alpha=rep(1.9,dim(as.matrix(design))[2]),lower_bound=T,
    max_eval=max(30,20+5*dim(design)[2]),initial_values=NA,num_initial_values=2)
 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ppgasp_+3A_design">design</code></td>
<td>
<p> a matrix of inputs.
</p>
</td></tr>
<tr><td><code id="ppgasp_+3A_response">response</code></td>
<td>
<p> a matrix of outputs where each row is one runs of the computer model  output.
</p>
</td></tr>
<tr><td><code id="ppgasp_+3A_trend">trend</code></td>
<td>
<p> the mean/trend matrix of inputs. The default value is a  vector of ones. 
</p>
</td></tr>
<tr><td><code id="ppgasp_+3A_zero.mean">zero.mean</code></td>
<td>
<p> it has zero mean or not. The default value is FALSE meaning the mean is not zero. TRUE means the mean is zero.
</p>
</td></tr>
<tr><td><code id="ppgasp_+3A_nugget">nugget</code></td>
<td>
<p> numerical value of the nugget variance ratio. If nugget is equal to 0, it means there is either no nugget or the nugget is estimated. If the nugget is not equal to 0, it means a fixed nugget. The default value is 0. 
</p>
</td></tr>
<tr><td><code id="ppgasp_+3A_nugget.est">nugget.est</code></td>
<td>
<p> boolean value. <code>T</code> means nugget should be estimated and <code>F</code> means nugget is fixed
or not estimated. The default value is <code>F</code>.
</p>
</td></tr>
<tr><td><code id="ppgasp_+3A_range.par">range.par</code></td>
<td>
<p> either <code>NA</code> or a <code>vector</code>. If it is <code>NA</code>, it means range parameters are estimated; otherwise range parameters are given. The default value is <code>NA</code>.
</p>
</td></tr>
<tr><td><code id="ppgasp_+3A_method">method</code></td>
<td>
<p>  method of parameter estimation. <code>post_mode</code> means the marginal posterior mode is used for estimation. <code>mle</code> means the maximum likelihood estimation is used. <code>mmle</code> means the maximum marginal likelihood estimation is used. The <code>post_mode</code>  is the default method. 
</p>
</td></tr>
<tr><td><code id="ppgasp_+3A_prior_choice">prior_choice</code></td>
<td>
<p> the choice of prior for range parameters and noise-variance parameters. <code>ref_xi</code> and <code>ref_gamma</code> means the reference prior with reference prior with the log of inverse range parameterization &xi; or range parameterization &gamma;. <code>ref_approx</code> uses the jointly robust prior to approximate the reference prior. The default choice is <code>ref_approx</code>.
</p>
</td></tr>
<tr><td><code id="ppgasp_+3A_a">a</code></td>
<td>

<p>prior parameters in the jointly robust prior. The default value is 0.2. 
</p>
</td></tr>
<tr><td><code id="ppgasp_+3A_b">b</code></td>
<td>

<p>prior parameters in the jointly robust prior. The default value is <code>n^{-1/p}(a+p)</code> where n is the number of runs and p is the dimension of the input vector. 
</p>
</td></tr>
<tr><td><code id="ppgasp_+3A_kernel_type">kernel_type</code></td>
<td>

<p>A vector specifying the type of kernels of each coordinate of the input. <code>matern_3_2</code> and <code>matern_5_2</code> are <code>Matern correlation</code> with roughness parameter 3/2 and 5/2 respectively. <code>pow_exp</code> is power exponential correlation with roughness parameter alpha. If <code>pow_exp</code> is to be used, one needs to specify its roughness parameter alpha. The default choice is <code>matern_5_2</code>.
</p>
</td></tr>
<tr><td><code id="ppgasp_+3A_isotropic">isotropic</code></td>
<td>

<p>a boolean value. <code>T</code> means the isotropic kernel will be used and <code>F</code> means the separable kernel will be used. The default choice is  the separable kernel. 
</p>
</td></tr>
<tr><td><code id="ppgasp_+3A_r0">R0</code></td>
<td>

<p>the distance between inputs. If the value is <code>NA</code>, it will be calculated later. It can also be specified by the user. If specified by user, it is either a <code>matrix</code> or <code>list</code>. The default value is <code>NA</code>.
</p>
</td></tr>
<tr><td><code id="ppgasp_+3A_optimization">optimization</code></td>
<td>

<p>the method for numerically optimization of the kernel parameters.  Currently three methods are implemented. <code>lbfgs</code> is the low-storage version of the Broyden-Fletcher-Goldfarb-Shanno method. <code>nelder-mead</code> is the  Nelder and Mead method.  <code>brent</code> is the Brent method for one-dimensional problems. 
</p>
</td></tr>
<tr><td><code id="ppgasp_+3A_alpha">alpha</code></td>
<td>

<p>roughness parameters in the <code>pow_exp</code> correlation functions. The default choice is a vector with each entry being 1.9. 
</p>
</td></tr>

<tr><td><code id="ppgasp_+3A_lower_bound">lower_bound</code></td>
<td>

<p>boolean value.  <code>T</code> means the default lower bounds of the inverse range parameters are used to constrained the optimization and <code>F</code> means the optimization is unconstrained. The default value is <code>T</code> and we also suggest to use <code>F</code> in various scenarios. 
</p>
</td></tr>
<tr><td><code id="ppgasp_+3A_max_eval">max_eval</code></td>
<td>

<p>the maximum number of steps to estimate the range and nugget parameters.  
</p>
</td></tr>
<tr><td><code id="ppgasp_+3A_initial_values">initial_values</code></td>
<td>

<p>a matrix of initial values of the kernel parameters to be optimized numerically, where each row of the matrix contains a set of the log inverse range parameters and the log nugget parameter. 
</p>
</td></tr>
<tr><td><code id="ppgasp_+3A_num_initial_values">num_initial_values</code></td>
<td>

<p>the number of initial values of the kernel parameters in optimization.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>ppgasp</code> returns a S4 object of class <code>ppgasp</code> (see <code>ppgasp-class</code>).
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Jesus Palomo [aut],
  James Berger [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>

<p>M. Gu. and J.O. Berger (2016). Parallel partial Gaussian process emulation for computer models with massive output. <em>Annals of Applied Statistics</em>, 10(3), 1317-1347.
</p>
<p>M. Gu, X. Wang and J.O. Berger (2018), <em>Robust Gaussian stochastic process emulation</em>, <em>Annals of Statistics</em>, 46(6A), 3038-3066.
</p>
<p>M. Gu (2018), <em>Jointly robust prior for Gaussian stochastic process in emulation, calibration and variable selection</em>, arXiv:1804.09329.
</p>
<p>J. Nocedal (1980), Updating quasi-Newton matrices with limited storage, <em>Math. Comput.</em>, 35, 773-782.
</p>
<p>D. C. Liu and J. Nocedal (1989), On the limited memory BFGS method for large scale optimization, <em>Math. Programming</em>, 45, p. 503-528.
</p>
<p>Brent, R. (1973), Algorithms for Minimization without Derivatives. Englewood Cliffs N.J.: Prentice-Hall.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
  library(RobustGaSP)
  
  ###parallel partial Gaussian stochastic process (PP GaSP) model 
  ##for the humanity model
  data(humanity_model)
  ##120 runs. The input has 13 variables and output is 5 dimensional.
  ##PP GaSP Emulator
  m.ppgasp=ppgasp(design=humanity.X,response=humanity.Y,nugget.est= TRUE)
  show(m.ppgasp)

  ##make predictions
  m_pred=predict(m.ppgasp,humanity.Xt)
  sqrt(mean((m_pred$mean-humanity.Yt)^2))
  mean(m_pred$upper95&gt;humanity.Yt &amp; humanity.Yt&gt;m_pred$lower95)
  mean(m_pred$upper95-m_pred$lower95)
  sqrt( mean( (mean(humanity.Y)-humanity.Yt)^2 ))

  ##with a linear trend on the selected input performs better
  ## Not run: 
    ###PP GaSP Emulation with a linear trend for the humanity model
    data(humanity_model)
    ##pp gasp with trend
    n&lt;-dim(humanity.Y)[1]
    n_testing=dim(humanity.Yt)[1]
    H=cbind(matrix(1,n,1),humanity.X$foodC)
    H_testing=cbind(matrix(1,n_testing,1),humanity.Xt$foodC)
    m.ppgasp_trend=ppgasp(design=humanity.X,response=humanity.Y,trend=H, 
    nugget.est= TRUE)
    
    show(m.ppgasp_trend)
    
    ##make predictions
    m_pred_trend=predict(m.ppgasp_trend,humanity.Xt,testing_trend=H_testing)
    sqrt(mean((m_pred_trend$mean-humanity.Yt)^2))
    mean(m_pred_trend$upper95&gt;humanity.Yt &amp; humanity.Yt&gt;m_pred_trend$lower95)
    mean(m_pred_trend$upper95-m_pred_trend$lower95)
  
## End(Not run)

</code></pre>

<hr>
<h2 id='ppgasp-class'>PP GaSP class </h2><span id='topic+ppgasp-class'></span>

<h3>Description</h3>

<p> S4 class for PP GaSP model if the range and noise-variance ratio parameters are given and/or have been estimated.</p>


<h3>Objects from the Class</h3>

<p>Objects of this class are created and initialized with the function <code><a href="#topic+ppgasp">ppgasp</a></code> that computes the calculations needed for setting up the analysis.</p>


<h3>Slots</h3>


<dl>
<dt><code>p</code>:</dt><dd><p>Object of class <code>integer</code>. The dimensions of the inputs.</p>
</dd>
<dt><code>num_obs</code>:</dt><dd><p>Object of class <code>integer</code>. The number of observations.</p>
</dd>
<dt><code>k</code>:</dt><dd><p>Object of class <code>integer</code>. The number of outputs in each computer model run.</p>
</dd>
<dt><code>input</code>:</dt><dd><p>Object of class <code>matrix</code> with dimension n x p. The design of experiments.</p>
</dd>
<dt><code>output</code>:</dt><dd><p>Object of class <code>matrix</code> with dimension n x k. Each row denotes a output vector in each run of the computer model.</p>
</dd>
<dt><code>X</code>:</dt><dd><p>Object of class <code>matrix</code> of with dimension n x q. The mean basis function, i.e. the trend function.</p>
</dd>
<dt><code>zero_mean</code>:</dt><dd><p>A <code>character</code> to specify whether the mean is zero or not. &quot;Yes&quot; means it has zero mean and &quot;No&quot;&quot; means the mean is not zero. </p>
</dd>
<dt><code>q</code>:</dt><dd><p>Object of class <code>integer</code>. The number of mean basis.</p>
</dd>
<dt><code>LB</code>:</dt><dd><p>Object of class <code>vector</code> with dimension p x 1. The lower bound for inverse range parameters beta.</p>
</dd>
<dt><code>beta_initial</code>:</dt><dd><p>Object of class <code>vector</code> with the initial values of inverse range parameters p x 1.</p>
</dd>
<dt><code>beta_hat</code>:</dt><dd><p>Object of class <code>vector</code> with dimension p x 1. The inverse-range parameters.</p>
</dd>
<dt><code>log_post</code>:</dt><dd><p>Object of class <code>numeric</code> with the logarithm of marginal posterior.</p>
</dd>
<dt><code>R0</code>:</dt><dd><p>Object of class <code>list</code> of matrices where the j-th matrix is an absolute difference matrix of the j-th input vector.</p>
</dd>
<dt><code>theta_hat</code>:</dt><dd><p>Object of class <code>vector</code> with dimension q x 1. The the mean (trend) parameter.</p>
</dd>
<dt><code>L</code>:</dt><dd><p>Object of class <code>matrix</code> with dimension n x n. The Cholesky decomposition of the correlation matrix <code>R</code>, i.e. </p>
<p style="text-align: center;"><code class="reqn">L\%*\%t(L)=R</code>
</p>
</dd>
<dt><code>sigma2_hat</code>:</dt><dd><p>Object of the class <code>matrix</code>. The estimated variance parameter of each output.</p>
</dd>
<dt><code>LX</code>:</dt><dd><p>Object of the class <code>matrix</code> with dimension q x q. The Cholesky decomposition of the correlation matrix </p>
<p style="text-align: center;"><code class="reqn">t(X)\%*\%R^{-1}\%*\%X</code>
</p>
</dd>
<dt><code>CL</code>:</dt><dd><p>Object of the class <code>vector</code> used for the lower bound and the prior.</p>
</dd>
<dt><code>nugget</code>:</dt><dd><p>A <code>numeric</code> object used for the noise-variance ratio parameter.</p>
</dd>
<dt><code>nugget.est</code>:</dt><dd><p>A <code>logical</code> object of whether the nugget is estimated (T) or fixed (F).</p>
</dd>
<dt><code>kernel_type</code>:</dt><dd><p>A <code>vector</code> of <code>character</code> to specify the type of kernel to use.</p>
</dd>
<dt><code>alpha</code>:</dt><dd><p>Object of class <code>vector</code> with dimension p x 1 for the roughness parameters in the kernel.</p>
</dd>
<dt><code>method</code>:</dt><dd><p>Object of class <code>character</code> to specify the method of parameter estimation. There are three values: <code>post_mode</code>, <code>mle</code> and <code>mmle</code>.</p>
</dd>
<dt><code>isotropic</code>:</dt><dd><p>Object of class <code>logical</code> to specify whether the kernel is isotropic. </p>
</dd>
<dt><code>call</code>:</dt><dd><p>The <code>call</code> to <code>ppgasp</code> function to create the object.</p>
</dd>
</dl>



<h3>Methods</h3>


<dl>
<dt>show</dt><dd><p>Prints the main slots of the object. </p>
</dd>
<dt>predict</dt><dd><p>See <code><a href="#topic+predict.ppgasp">predict</a></code>.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Jesus Palomo [aut],
  James Berger [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+RobustGaSP">RobustGaSP</a></code> for more details about how to create a <code>RobustGaSP</code> object.
</p>

<hr>
<h2 id='pred_rgasp'>
Prediction for robust GaSP model
</h2><span id='topic+pred_rgasp'></span>

<h3>Description</h3>

<p>A function to make prediction on robust GaSP models after the robust GaSP model has been constructed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pred_rgasp(beta, nu, input, X, zero_mean,output, testing_input,
           X_testing, L, LX, theta_hat, sigma2_hat, 
           q_025, q_975, r0, kernel_type, alpha,method,interval_data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pred_rgasp_+3A_beta">beta</code></td>
<td>

<p>inverse-range parameters.
</p>
</td></tr>
<tr><td><code id="pred_rgasp_+3A_nu">nu</code></td>
<td>

<p>noise-variance ratio parameter.
</p>
</td></tr>
<tr><td><code id="pred_rgasp_+3A_input">input</code></td>
<td>

<p>input matrix.
</p>
</td></tr>
<tr><td><code id="pred_rgasp_+3A_x">X</code></td>
<td>

<p>the mean basis function i.e. the trend function.
</p>
</td></tr>
<tr><td><code id="pred_rgasp_+3A_zero_mean">zero_mean</code></td>
<td>

<p>The mean basis function is zero or not.
</p>
</td></tr>
<tr><td><code id="pred_rgasp_+3A_output">output</code></td>
<td>

<p>output matrix.
</p>
</td></tr>
<tr><td><code id="pred_rgasp_+3A_testing_input">testing_input</code></td>
<td>

<p>testing input matrix.
</p>
</td></tr>
<tr><td><code id="pred_rgasp_+3A_x_testing">X_testing</code></td>
<td>

<p>mean/trend matrix of testing inputs.
</p>
</td></tr>
<tr><td><code id="pred_rgasp_+3A_l">L</code></td>
<td>

<p>a lower triangular matrix for the cholesky decomposition of <code>R</code>, the correlation matrix.
</p>
</td></tr>
<tr><td><code id="pred_rgasp_+3A_lx">LX</code></td>
<td>

<p>a lower triangular matrix for the cholesky decomposition of X^tR^{-1}X.
</p>
</td></tr>
<tr><td><code id="pred_rgasp_+3A_theta_hat">theta_hat</code></td>
<td>

<p>estimated mean/trend parameters.
</p>
</td></tr>
<tr><td><code id="pred_rgasp_+3A_sigma2_hat">sigma2_hat</code></td>
<td>

<p>estimated variance parameter.
</p>
</td></tr>
<tr><td><code id="pred_rgasp_+3A_q_025">q_025</code></td>
<td>

<p>0.025 quantile of <code>t</code> distribution.
</p>
</td></tr>
<tr><td><code id="pred_rgasp_+3A_q_975">q_975</code></td>
<td>

<p>0.975 quantile of <code>t</code> distribution.
</p>
</td></tr>
<tr><td><code id="pred_rgasp_+3A_r0">r0</code></td>
<td>

<p>a matrix of absolute difference between inputs and testing inputs.
</p>
</td></tr>
<tr><td><code id="pred_rgasp_+3A_kernel_type">kernel_type</code></td>
<td>

<p>type of kernel. <code>matern_3_2</code> and <code>matern_5_2</code> are <code>Matern kernel</code> with roughness parameter 3/2 and 5/2 respectively. <code>pow_exp</code> is power exponential kernel with roughness parameter alpha. If <code>pow_exp</code> is to be used, one needs to specify its roughness parameter alpha.
</p>
</td></tr>
<tr><td><code id="pred_rgasp_+3A_alpha">alpha</code></td>
<td>

<p>Roughness parameters in the kernel functions.
</p>
</td></tr>
<tr><td><code id="pred_rgasp_+3A_method">method</code></td>
<td>
  
<p>method of parameter estimation. <code>post_mode</code> means the marginal posterior mode is used for estimation. <code>mle</code> means the maximum likelihood estimation is used. <code>mmle</code> means the maximum marginal likelihood estimation is used. The <code>post_mode</code>  is the default method. 
</p>
</td></tr>
<tr><td><code id="pred_rgasp_+3A_interval_data">interval_data</code></td>
<td>

<p>a boolean value. If <code>T</code>, the interval of the data will be calculated. If <code>F</code>, the  interval of the mean of the data will be calculated. 
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of 4 elements. The first is a vector for predictive mean for testing inputs. The second is a vector for lower quantile for 95% posterior credible interval and the  third is the upper quantile for 95% posterior credible interval for these testing inputs. The last is a vector of standard deviation of each testing inputs.
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Jesus Palomo [aut],
  James Berger [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>

<p>Mengyang Gu. (2016). Robust Uncertainty Quantification and Scalable Computation for Computer Models with Massive Output. Ph.D. thesis. Duke University.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.rgasp">predict.rgasp</a></code>
</p>

<hr>
<h2 id='predict.ppgasp'>
Prediction for PP GaSP model
</h2><span id='topic+predict.ppgasp'></span><span id='topic+predict.ppgasp-class'></span><span id='topic+predict+2Cppgasp-method'></span>

<h3>Description</h3>

<p>Function to make prediction on the PP GaSP model after the PP GaSP model has been constructed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'ppgasp'
predict(object, testing_input, 
testing_trend= matrix(1,dim(testing_input)[1],1),r0=NA, 
interval_data=T,
outasS3 = T,loc_index=NA, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.ppgasp_+3A_object">object</code></td>
<td>
<p> an object of  class <code>ppgasp</code>.</p>
</td></tr>
<tr><td><code id="predict.ppgasp_+3A_testing_input">testing_input</code></td>
<td>
<p>a matrix containing the inputs where the <code>rgasp</code> is to perform prediction.</p>
</td></tr>
<tr><td><code id="predict.ppgasp_+3A_testing_trend">testing_trend</code></td>
<td>
<p>a matrix of mean/trend for prediction.</p>
</td></tr>
<tr><td><code id="predict.ppgasp_+3A_r0">r0</code></td>
<td>

<p>the distance between input and testing input. If the value 
is <code>NA</code>, it will be calculated later. It can also be 
specified by the user. If specified by user, it is either a 
<code>matrix</code> or <code>list</code>. The default value is <code>NA</code>.
</p>
</td></tr>
<tr><td><code id="predict.ppgasp_+3A_interval_data">interval_data</code></td>
<td>

<p>a boolean value. If <code>T</code>, the interval of the data will be   
calculated. Otherwise, the interval of the mean of the data will 
be calculted.
</p>
</td></tr>
<tr><td><code id="predict.ppgasp_+3A_outass3">outasS3</code></td>
<td>
<p>a boolean parameter indicating whether the output of the function should be as an <code>S3 object</code>.</p>
</td></tr>
<tr><td><code id="predict.ppgasp_+3A_loc_index">loc_index</code></td>
<td>
<p>specified coodinate index of the prediction. The default value is <code>NA</code> and prediction will be computed for all coordinates. If e.g. <code>loc_index=c(3,5)</code>, it means the prediction will be computed on only the third and fifth coordinates, corresponding the coordinates of the third and fifth columns of the output matrix. </p>
</td></tr>
<tr><td><code id="predict.ppgasp_+3A_...">...</code></td>
<td>
<p>Extra arguments to be passed to the function (not implemented yet).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>If the parameter <code>outasS3=F</code>, then the returned value is a <code>S4 object</code> of class <code><a href="#topic+predppgasp-class">predppgasp-class</a></code> with 
</p>
<table>
<tr><td><code>call:</code></td>
<td>
 <p><code>call</code> to <code>predict.ppgasp</code> function where the returned object has been created.</p>
</td></tr>
<tr><td><code>mean:</code></td>
<td>
<p> predictive mean for the testing inputs.</p>
</td></tr>
<tr><td><code>lower95:</code></td>
<td>
<p>lower bound of the 95% posterior credible interval.</p>
</td></tr>
<tr><td><code>upper95:</code></td>
<td>
<p>upper bound of the 95% posterior credible interval.</p>
</td></tr>
<tr><td><code>sd:</code></td>
<td>
<p>standard deviation of each <code>testing_input</code>.</p>
</td></tr>
</table>
<p>If the parameter <code>outasS3=T</code>, then the returned value is a <code>list</code> with 
</p>
<table>
<tr><td><code>mean</code></td>
<td>
<p> predictive mean for the testing inputs.</p>
</td></tr>
<tr><td><code>lower95</code></td>
<td>
<p>lower bound of the 95% posterior credible interval.</p>
</td></tr>
<tr><td><code>upper95</code></td>
<td>
<p>upper bound of the 95% posterior credible interval.</p>
</td></tr>
<tr><td><code>sd</code></td>
<td>
<p>standard deviation of each <code>testing_input</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Jesus Palomo [aut],
  James Berger [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>

<p>M. Gu. and J.O. Berger (2016). Parallel partial Gaussian process emulation for computer models with massive output. <em>Annals of Applied Statistics</em>, 10(3), 1317-1347.
</p>
<p>M. Gu. (2016). Robust Uncertainty Quantification and Scalable Computation for Computer Models with Massive Output. Ph.D. thesis. Duke University.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  library(RobustGaSP)
  #----------------------------------
  # an example of environmental model
  #----------------------------------
  
  set.seed(1)
  #Here the sample size is very small. Consider to use more observations 
  n=80
  p=4
  ##using the latin hypercube will be better
  #library(lhs)
  #input_samples=maximinLHS(n,p)
  input_samples=matrix(runif(n*p),n,p)
  input=matrix(0,n,p)
  input[,1]=7+input_samples[,1]*6
  input[,2]=0.02+input_samples[,2]*1
  input[,3]=0.01+input_samples[,3]*2.99
  input[,4]=30.01+input_samples[,4]*0.285
  
  k=300
  output=matrix(0,n,k)
  ##environ.4.data is an environmental model on a spatial-time vector
  ##? environ.4.data
  for(i in 1:n){
    output[i,]=environ.4.data(input[i,],s=seq(0.15,3,0.15),t=seq(4,60,4)  )
  }
  
  ##samples some test inputs
  n_star=1000
  sample_unif=matrix(runif(n_star*p),n_star,p)
  
  testing_input=matrix(0,n_star,p)
  testing_input[,1]=7+sample_unif[,1]*6
  testing_input[,2]=0.02+sample_unif[,2]*1
  testing_input[,3]=0.01+sample_unif[,3]*2.99
  testing_input[,4]=30.01+sample_unif[,4]*0.285
  
  
  testing_output=matrix(0,n_star,k)
  
  s=seq(0.15,3,0.15)
  t=seq(4,60,4) 
  
  for(i in 1:n_star){
    testing_output[i,]=environ.4.data(testing_input[i,],s=s,t=t )
  }
  
  ##we do a transformation of the output 
  ##one can change the number of initial values to test
  log_output_1=log(output+1)
  #since we have lots of output, we use 'nelder-mead' for optimization
  m.ppgasp=ppgasp(design=input,response=log_output_1,kernel_type
                  ='pow_exp',num_initial_values=2,optimization='nelder-mead')
  
  m_pred.ppgasp=predict(m.ppgasp,testing_input)
  ##we transform back for the prediction
  m_pred_ppgasp_median=exp(m_pred.ppgasp$mean)-1
  ##mean squared error
  mean( (m_pred_ppgasp_median-testing_output)^2)
  ##variance of the testing outputs
  var(as.numeric(testing_output))
  
  ##makes plots for the testing 
  par(mfrow=c(1,2))
  testing_plot_1=matrix(testing_output[1,],  length(t), length(s) )
  
  max_testing_plot_1=max(testing_plot_1)
  min_testing_plot_1=min(testing_plot_1)
  
  image(x=t,y=s,testing_plot_1,  col = hcl.colors(100, "terrain"),main='test outputs')
  contour(x=t,y=s,testing_plot_1, levels = seq(min_testing_plot_1, max_testing_plot_1,
                                               by = (max_testing_plot_1-min_testing_plot_1)/5),
          add = TRUE, col = "brown")
  
  ppgasp_plot_1=matrix(m_pred_ppgasp_median[1,],  length(t), length(s) )
  max_ppgasp_plot_1=max(ppgasp_plot_1)
  min_ppgasp_plot_1=min(ppgasp_plot_1)
  
  image(x=t,y=s,ppgasp_plot_1,  col = hcl.colors(100, "terrain"),main='prediction')
  contour(x=t,y=s,ppgasp_plot_1, levels = seq(min_testing_plot_1, max_ppgasp_plot_1,
                                              by = (max_ppgasp_plot_1-min_ppgasp_plot_1)/5),
          add = TRUE, col = "brown")
  dev.off()
  

</code></pre>

<hr>
<h2 id='predict.rgasp'>
Prediction for Robust GaSP model
</h2><span id='topic+predict'></span><span id='topic+predict.rgasp'></span><span id='topic+predict.rgasp-class'></span><span id='topic+predict+2Crgasp-method'></span>

<h3>Description</h3>

<p>Function to make prediction on the robust GaSP model after the robust GaSP model has been constructed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'rgasp'
predict(object,testing_input,testing_trend= matrix(1,dim(testing_input)[1],1),
r0=NA,interval_data=T,
outasS3 = T,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.rgasp_+3A_object">object</code></td>
<td>
<p> an object of  class <code>rgasp</code>.</p>
</td></tr>
<tr><td><code id="predict.rgasp_+3A_testing_input">testing_input</code></td>
<td>
<p>a matrix containing the inputs where the <code>rgasp</code> is to perform prediction.</p>
</td></tr>
<tr><td><code id="predict.rgasp_+3A_testing_trend">testing_trend</code></td>
<td>
<p>a matrix of mean/trend for prediction.</p>
</td></tr>
<tr><td><code id="predict.rgasp_+3A_r0">r0</code></td>
<td>

<p>the distance between input and testing input. If the value is <code>NA</code>, it will be calculated later. It can also be specified by the user. If specified by user, it is either a <code>matrix</code> or <code>list</code>. The default value is <code>NA</code>.
</p>
</td></tr>
<tr><td><code id="predict.rgasp_+3A_interval_data">interval_data</code></td>
<td>

<p>a boolean value. If <code>T</code>, the interval of the data will be calculated. Otherwise, the interval of the mean of the data will be calculted.
</p>
</td></tr>
<tr><td><code id="predict.rgasp_+3A_outass3">outasS3</code></td>
<td>
<p>a boolean parameter indicating whether the output of the function should be as an <code>S3 object</code>.</p>
</td></tr>
<tr><td><code id="predict.rgasp_+3A_...">...</code></td>
<td>
<p>Extra arguments to be passed to the function (not implemented yet).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>If the parameter <code>outasS3=F</code>, then the returned value is a <code>S4 object</code> of class <code><a href="#topic+predrgasp-class">predrgasp-class</a></code> with 
</p>

<dl>
<dt><code>call</code>:</dt><dd> <p><code>call</code> to <code>predict.rgasp</code> function where the returned object has been created.</p>
</dd>
<dt><code>mean</code>:</dt><dd><p> predictive mean for the testing inputs.</p>
</dd>
<dt><code>lower95</code>:</dt><dd><p>lower bound of the 95% posterior credible interval.</p>
</dd>
<dt><code>upper95</code>:</dt><dd><p>upper bound of the 95% posterior credible interval.</p>
</dd>
<dt><code>sd</code>:</dt><dd><p>standard deviation of each <code>testing_input</code>.</p>
</dd>
</dl>

<p>If the parameter <code>outasS3=T</code>, then the returned value is a <code>list</code> with 
</p>
<table>
<tr><td><code>mean</code></td>
<td>
<p> predictive mean for the testing inputs.</p>
</td></tr>
<tr><td><code>lower95</code></td>
<td>
<p>lower bound of the 95% posterior credible interval.</p>
</td></tr>
<tr><td><code>upper95</code></td>
<td>
<p>upper bound of the 95% posterior credible interval.</p>
</td></tr>
<tr><td><code>sd</code></td>
<td>
<p>standard deviation of each <code>testing_input</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Jesus Palomo [aut],
  James Berger [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>

<p>M. Gu. (2016). Robust Uncertainty Quantification and Scalable Computation for Computer Models with Massive Output. Ph.D. thesis. Duke University.
</p>
<p>M. Gu. and J.O. Berger (2016). Parallel partial Gaussian process emulation for computer models with massive output. <em>Annals of Applied Statistics</em>, 10(3), 1317-1347.
</p>
<p>M. Gu, X. Wang and J.O. Berger (2018), <em>Robust Gaussian Stochastic Process Emulation</em>, <em>Annals of Statistics</em>, 46(6A), 3038-3066.
</p>
<p>M. Gu (2018), <em>Jointly Robust Prior for Gaussian Stochastic Process in Emulation, Calibration and Variable Selection</em>, arXiv:1804.09329.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  #------------------------
  # a 3 dimensional example
  #------------------------
  # dimensional of the inputs
  dim_inputs &lt;- 3    
  # number of the inputs
  num_obs &lt;- 30       
  # uniform samples of design
  input &lt;- matrix(runif(num_obs*dim_inputs), num_obs,dim_inputs) 
  
  # Following codes use maximin Latin Hypercube Design, which is typically better than uniform
  # library(lhs)
  # input &lt;- maximinLHS(n=num_obs, k=dim_inputs)  ##maximin lhd sample
  
  # outputs from the 3 dim dettepepel.3.data function
  
  output = matrix(0,num_obs,1)
  for(i in 1:num_obs){
    output[i]&lt;-dettepepel.3.data (input[i,])
  }
  
  # use constant mean basis, with no constraint on optimization
  m1&lt;- rgasp(design = input, response = output, lower_bound=FALSE)
  
  # the following use constraints on optimization
  # m1&lt;- rgasp(design = input, response = output, lower_bound=TRUE)
  
  # the following use a single start on optimization
  # m1&lt;- rgasp(design = input, response = output, lower_bound=FALS)
  
  # number of points to be predicted 
  num_testing_input &lt;- 5000    
  # generate points to be predicted
  testing_input &lt;- matrix(runif(num_testing_input*dim_inputs),num_testing_input,dim_inputs)
  # Perform prediction
  m1.predict&lt;-predict(m1, testing_input)
  # Predictive mean
  # m1.predict$mean  
  
  # The following tests how good the prediction is 
  testing_output &lt;- matrix(0,num_testing_input,1)
  for(i in 1:num_testing_input){
    testing_output[i]&lt;-dettepepel.3.data(testing_input[i,])
  }
  
  # compute the MSE, average coverage and average length
  # out of sample MSE
  MSE_emulator &lt;- sum((m1.predict$mean-testing_output)^2)/(num_testing_input)  
  
  # proportion covered by 95% posterior predictive credible interval
  prop_emulator &lt;- length(which((m1.predict$lower95&lt;=testing_output)
                   &amp;(m1.predict$upper95&gt;=testing_output)))/num_testing_input
  
  # average length of  posterior predictive credible interval
  length_emulator &lt;- sum(m1.predict$upper95-m1.predict$lower95)/num_testing_input
  
  # output of prediction
  MSE_emulator
  prop_emulator
  length_emulator  
  # normalized RMSE
  sqrt(MSE_emulator/mean((testing_output-mean(output))^2 ))


  #-----------------------------------
  # a 2 dimensional example with trend
  #-----------------------------------
  # dimensional of the inputs
  dim_inputs &lt;- 2    
  # number of the inputs
  num_obs &lt;- 20       
  
  # uniform samples of design
  input &lt;-matrix(runif(num_obs*dim_inputs), num_obs,dim_inputs) 
  # Following codes use maximin Latin Hypercube Design, which is typically better than uniform
  # library(lhs)
  # input &lt;- maximinLHS(n=num_obs, k=dim_inputs)  ##maximin lhd sample
  
  # outputs from the 2 dim Brainin function
  
  output &lt;- matrix(0,num_obs,1)
  for(i in 1:num_obs){
    output[i]&lt;-limetal.2.data (input[i,])
  }
  
  #mean basis (trend)
  X&lt;-cbind(rep(1,num_obs), input )
  
  
  # use constant mean basis with trend, with no constraint on optimization
  m2&lt;- rgasp(design = input, response = output,trend =X,  lower_bound=FALSE)
  
  
  # number of points to be predicted 
  num_testing_input &lt;- 5000    
  # generate points to be predicted
  testing_input &lt;- matrix(runif(num_testing_input*dim_inputs),num_testing_input,dim_inputs)
  
  # trend of testing
  testing_X&lt;-cbind(rep(1,num_testing_input), testing_input )
  
  
  # Perform prediction
  m2.predict&lt;-predict(m2, testing_input,testing_trend=testing_X)
  # Predictive mean
  #m2.predict$mean  
  
  # The following tests how good the prediction is 
  testing_output &lt;- matrix(0,num_testing_input,1)
  for(i in 1:num_testing_input){
    testing_output[i]&lt;-limetal.2.data(testing_input[i,])
  }
  
  # compute the MSE, average coverage and average length
  # out of sample MSE
  MSE_emulator &lt;- sum((m2.predict$mean-testing_output)^2)/(num_testing_input)  
  
  # proportion covered by 95% posterior predictive credible interval
  prop_emulator &lt;- length(which((m2.predict$lower95&lt;=testing_output)
                   &amp;(m2.predict$upper95&gt;=testing_output)))/num_testing_input
  
  # average length of  posterior predictive credible interval
  length_emulator &lt;- sum(m2.predict$upper95-m2.predict$lower95)/num_testing_input
  
  # output of prediction
  MSE_emulator
  prop_emulator
  length_emulator  
  # normalized RMSE
  sqrt(MSE_emulator/mean((testing_output-mean(output))^2 ))


    ###here try the isotropic kernel (a function of Euclidean distance)
  m2_isotropic&lt;- rgasp(design = input, response = output,trend =X,  
             lower_bound=FALSE,isotropic=TRUE)
  
  m2_isotropic.predict&lt;-predict(m2_isotropic, testing_input,testing_trend=testing_X)
  
  # compute the MSE, average coverage and average length
  # out of sample MSE
  MSE_emulator_isotropic &lt;- sum((m2_isotropic.predict$mean-testing_output)^2)/(num_testing_input)
  
  # proportion covered by 95% posterior predictive credible interval
  prop_emulator_isotropic &lt;- length(which((m2_isotropic.predict$lower95&lt;=testing_output)
                                &amp;(m2_isotropic.predict$upper95&gt;=testing_output)))/num_testing_input
  
  # average length of  posterior predictive credible interval
  length_emulator_isotropic &lt;- sum(m2_isotropic.predict$upper95-
  m2_isotropic.predict$lower95)/num_testing_input
  
  MSE_emulator_isotropic
  prop_emulator_isotropic
  length_emulator_isotropic
  ##the result of isotropic kernel is not as good as the product kernel for this example


  #--------------------------------------------------------------------------------------
  # an 8 dimensional example using only a subset inputs and a noise with unknown variance
  #--------------------------------------------------------------------------------------
  set.seed(1)
  # dimensional of the inputs
  dim_inputs &lt;- 8    
  # number of the inputs
  num_obs &lt;- 50       
  
  # uniform samples of design
  input &lt;-matrix(runif(num_obs*dim_inputs), num_obs,dim_inputs) 
  # Following codes use maximin Latin Hypercube Design, which is typically better than uniform
  # library(lhs)
  # input &lt;- maximinLHS(n=num_obs, k=dim_inputs)  # maximin lhd sample
  
  # rescale the design to the domain
  input[,1]&lt;-0.05+(0.15-0.05)*input[,1];
  input[,2]&lt;-100+(50000-100)*input[,2];
  input[,3]&lt;-63070+(115600-63070)*input[,3];
  input[,4]&lt;-990+(1110-990)*input[,4];
  input[,5]&lt;-63.1+(116-63.1)*input[,5];
  input[,6]&lt;-700+(820-700)*input[,6];
  input[,7]&lt;-1120+(1680-1120)*input[,7];
  input[,8]&lt;-9855+(12045-9855)*input[,8];
  
  # outputs from the 8 dim Borehole function
  
  output=matrix(0,num_obs,1)
  for(i in 1:num_obs){
    output[i]=borehole(input[i,])
  }
  
  
    
    
  
  # use constant mean basis with trend, with no constraint on optimization
  m3&lt;- rgasp(design = input[,c(1,4,6,7,8)], response = output,
             nugget.est=TRUE, lower_bound=FALSE)
  
  
  # number of points to be predicted 
  num_testing_input &lt;- 5000    
  # generate points to be predicted
  testing_input &lt;- matrix(runif(num_testing_input*dim_inputs),num_testing_input,dim_inputs)
  
  # resale the points to the region to be predict
  testing_input[,1]&lt;-0.05+(0.15-0.05)*testing_input[,1];
  testing_input[,2]&lt;-100+(50000-100)*testing_input[,2];
  testing_input[,3]&lt;-63070+(115600-63070)*testing_input[,3];
  testing_input[,4]&lt;-990+(1110-990)*testing_input[,4];
  testing_input[,5]&lt;-63.1+(116-63.1)*testing_input[,5];
  testing_input[,6]&lt;-700+(820-700)*testing_input[,6];
  testing_input[,7]&lt;-1120+(1680-1120)*testing_input[,7];
  testing_input[,8]&lt;-9855+(12045-9855)*testing_input[,8];
  
  
  # Perform prediction
  m3.predict&lt;-predict(m3, testing_input[,c(1,4,6,7,8)])
  # Predictive mean
  #m3.predict$mean  
  
  # The following tests how good the prediction is 
  testing_output &lt;- matrix(0,num_testing_input,1)
  for(i in 1:num_testing_input){
    testing_output[i]&lt;-borehole(testing_input[i,])
  }
  
  # compute the MSE, average coverage and average length
  # out of sample MSE
  MSE_emulator &lt;- sum((m3.predict$mean-testing_output)^2)/(num_testing_input)  
  
  # proportion covered by 95% posterior predictive credible interval
  prop_emulator &lt;- length(which((m3.predict$lower95&lt;=testing_output)
                   &amp;(m3.predict$upper95&gt;=testing_output)))/num_testing_input
  
  # average length of  posterior predictive credible interval
  length_emulator &lt;- sum(m3.predict$upper95-m3.predict$lower95)/num_testing_input
  
  # output of sample prediction
  MSE_emulator
  prop_emulator
  length_emulator  
  # normalized RMSE
  sqrt(MSE_emulator/mean((testing_output-mean(output))^2 ))

</code></pre>

<hr>
<h2 id='predppgasp-class'> Predicted PP GaSP class </h2><span id='topic+predppgasp-class'></span>

<h3>Description</h3>

<p> S4 class for the prediction of a PP GaSP model</p>


<h3>Objects from the Class</h3>

<p>Objects of this class are created and initialized with the function <code><a href="#topic+predict.ppgasp">predict.ppgasp</a></code> that computes the prediction on the PP GaSP model after the PP GaSP model has been constructed.</p>


<h3>Slots</h3>


<dl>
<dt><code>call</code>:</dt><dd> <p><code>call</code> to <code>predict.ppgasp</code> function where the returned object has been created.</p>
</dd>
<dt><code>mean</code>:</dt><dd><p> predictive mean for the testing inputs.</p>
</dd>
<dt><code>lower95</code>:</dt><dd><p>lower bound of the 95% posterior credible interval.</p>
</dd>
<dt><code>upper95</code>:</dt><dd><p>upper bound of the 95% posterior credible interval.</p>
</dd>
<dt><code>sd</code>:</dt><dd><p>standard deviation of each <code>testing_input</code>.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Jesus Palomo [aut],
  James Berger [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+predict.ppgasp">predict.ppgasp</a></code> for more details about how to make predictions based on a <code>ppgasp</code> object.
</p>

<hr>
<h2 id='predrgasp-class'> Predictive robust GaSP class </h2><span id='topic+predrgasp-class'></span>

<h3>Description</h3>

<p> S4 class for the prediction of a Robust GaSP</p>


<h3>Objects from the Class</h3>

<p>Objects of this class are created and initialized with the function <code><a href="#topic+predict.rgasp">predict.rgasp</a></code> that computes the prediction on Robust GaSP models after the Robust GaSP model has been constructed.</p>


<h3>Slots</h3>


<dl>
<dt><code>call</code>:</dt><dd> <p><code>call</code> to <code>predict.rgasp</code> function where the returned object has been created.</p>
</dd>
<dt><code>mean</code>:</dt><dd><p> predictive mean for the testing inputs.</p>
</dd>
<dt><code>lower95</code>:</dt><dd><p>lower bound of the 95% posterior credible interval.</p>
</dd>
<dt><code>upper95</code>:</dt><dd><p>upper bound of the 95% posterior credible interval.</p>
</dd>
<dt><code>sd</code>:</dt><dd><p>standard deviation of each <code>testing_input</code>.</p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Jesus Palomo [aut],
  James Berger [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+predict.rgasp">predict.rgasp</a></code> for more details about how to make predictions based on a <code>rgasp</code> object.
</p>

<hr>
<h2 id='rgasp'> Setting up the robust GaSP model
</h2><span id='topic+rgasp'></span><span id='topic+rgasp-method'></span>

<h3>Description</h3>

<p>Setting up the robust GaSP model for estimating the parameters (if the parameters are not given). 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  rgasp(design, response,trend=matrix(1,length(response),1),zero.mean="No",nugget=0,
    nugget.est=F,range.par=NA,method='post_mode',prior_choice='ref_approx',a=0.2,
    b=1/(length(response))^{1/dim(as.matrix(design))[2]}*(a+dim(as.matrix(design))[2]),
    kernel_type='matern_5_2',isotropic=F,R0=NA, 
    optimization='lbfgs', alpha=rep(1.9,dim(as.matrix(design))[2]),
    lower_bound=T,max_eval=max(30,20+5*dim(design)[2]),
    initial_values=NA,num_initial_values=2)
 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rgasp_+3A_design">design</code></td>
<td>
<p> a matrix of inputs.
</p>
</td></tr>
<tr><td><code id="rgasp_+3A_response">response</code></td>
<td>
<p> a matrix of outputs.
</p>
</td></tr>
<tr><td><code id="rgasp_+3A_trend">trend</code></td>
<td>
<p> the mean/trend matrix of inputs. The default value is a  vector of ones. 
</p>
</td></tr>
<tr><td><code id="rgasp_+3A_zero.mean">zero.mean</code></td>
<td>
<p> it has zero mean or not. The default value is <code>NO</code> meaning the mean is not zero. <code>Yes</code> means the mean is zero.
</p>
</td></tr>
<tr><td><code id="rgasp_+3A_nugget">nugget</code></td>
<td>
<p> numerical value of the nugget variance ratio. If nugget is equal to 0, it means there is either no nugget or the nugget is estimated. If the nugget is not equal to 0, it means a fixed nugget. The default value is 0. 
</p>
</td></tr>
<tr><td><code id="rgasp_+3A_nugget.est">nugget.est</code></td>
<td>
<p> boolean value. <code>T</code> means nugget should be estimated and <code>F</code> means nugget is fixed
or not estimated. The default value is F <code>F</code>.
</p>
</td></tr>
<tr><td><code id="rgasp_+3A_range.par">range.par</code></td>
<td>
<p> either <code>NA</code> or a <code>vector</code>. If it is <code>NA</code>, it means range parameters are estimated; otherwise range parameters are given. The default value is <code>NA</code>.
</p>
</td></tr>
<tr><td><code id="rgasp_+3A_method">method</code></td>
<td>
<p>  method of parameter estimation. <code>post_mode</code> means the marginal posterior mode is used for estimation. <code>mle</code> means the maximum likelihood estimation is used. <code>mmle</code> means the maximum marginal likelihood estimation is used. The <code>post_mode</code>  is the default method. 
</p>
</td></tr>
<tr><td><code id="rgasp_+3A_prior_choice">prior_choice</code></td>
<td>
<p>the choice of prior for range parameters and noise-variance parameters. <code>ref_xi</code> and <code>ref_gamma</code> means the reference prior with reference prior with the log of inverse range parameterization &xi; or range parameterization &gamma;. <code>ref_approx</code> uses the jointly robust prior to approximate the reference prior. The default choice is <code>ref_approx</code>.
</p>
</td></tr>
<tr><td><code id="rgasp_+3A_a">a</code></td>
<td>

<p>prior parameters in the jointly robust prior. The default value is 0.2. 
</p>
</td></tr>
<tr><td><code id="rgasp_+3A_b">b</code></td>
<td>

<p>prior parameters in the jointly robust prior. The default value is <code>n^{-1/p}(a+p)</code> where n is the number of runs and p is the dimension of the input vector. 
</p>
</td></tr>
<tr><td><code id="rgasp_+3A_kernel_type">kernel_type</code></td>
<td>

<p>A vector specifying the type of kernels of each coordinate of the input. <code>matern_3_2</code> and <code>matern_5_2</code> are <code>Matern correlation</code> with roughness parameter 3/2 and 5/2 respectively. <code>pow_exp</code> is power exponential correlation with roughness parameter alpha. If <code>pow_exp</code> is to be used, one needs to specify its roughness parameter alpha. The default choice is <code>matern_5_2</code>. The <code>periodic_gauss</code> means the Gaussian kernel with periodic folding method with be used. The <code>periodic_exp</code> means the exponential kernel with periodic folding method will be used.
</p>
</td></tr>
<tr><td><code id="rgasp_+3A_isotropic">isotropic</code></td>
<td>

<p>a boolean value. <code>T</code> means the isotropic kernel will be used and <code>F</code> means the separable kernel will be used. The default choice is  the separable kernel. 
</p>
</td></tr>
<tr><td><code id="rgasp_+3A_r0">R0</code></td>
<td>

<p>the distance between inputs. If the value is <code>NA</code>, it will be calculated later. It can also be specified by the user. If specified by user, it is either a <code>matrix</code> or <code>list</code>. The default value is <code>NA</code>.
</p>
</td></tr>
<tr><td><code id="rgasp_+3A_optimization">optimization</code></td>
<td>

<p>the method for numerically optimization of the kernel parameters.  Currently three methods are implemented. <code>lbfgs</code> is the low-storage version of the Broyden-Fletcher-Goldfarb-Shanno method. <code>nelder-mead</code> is the  Nelder and Mead method.  <code>brent</code> is the Brent method for one-dimensional problems. 
</p>
</td></tr>
<tr><td><code id="rgasp_+3A_alpha">alpha</code></td>
<td>

<p>roughness parameters in the <code>pow_exp</code> correlation functions. The default choice is a vector with each entry being 1.9. 
</p>
</td></tr>

<tr><td><code id="rgasp_+3A_lower_bound">lower_bound</code></td>
<td>

<p>boolean value.  <code>T</code> means the default lower bounds of the inverse range parameters are used to constrained the optimization and <code>F</code> means the optimization is unconstrained. The default value is <code>T</code> and we also suggest to use <code>F</code> in various scenarios. 
</p>
</td></tr>
<tr><td><code id="rgasp_+3A_max_eval">max_eval</code></td>
<td>

<p>the maximum number of steps to estimate the range and nugget parameters.  
</p>
</td></tr>
<tr><td><code id="rgasp_+3A_initial_values">initial_values</code></td>
<td>

<p>a matrix of initial values of the kernel parameters to be optimized numerically, where each row of the matrix contains a set of the log inverse range parameters and the log nugget parameter. 
</p>
</td></tr>
<tr><td><code id="rgasp_+3A_num_initial_values">num_initial_values</code></td>
<td>

<p>the number of initial values of the kernel parameters in optimization.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>rgasp</code> returns a S4 object of class <code>rgasp</code> (see <code>rgasp-class</code>).
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Jesus Palomo [aut],
  James Berger [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>

<p>M. Gu, X. Wang and J.O. Berger (2018), Robust Gaussian stochastic process emulation, <em>Annals of Statistics</em>, 46(6A), 3038-3066.
</p>
<p>M. Gu (2018), Jointly robust prior for Gaussian stochastic process in emulation, calibration and variable selection, arXiv:1804.09329.
</p>
<p>M. Gu. (2016). Robust uncertainty quantification and scalable computation for computer models with massive output. Ph.D. thesis. Duke University.
</p>
<p>M. Gu. and J.O. Berger (2016). Parallel partial Gaussian process emulation for computer models with massive output. <em>Annals of Applied Statistics</em>, 10(3), 1317-1347.
</p>
<p>E.T. Spiller, M.J. Bayarri, J.O. Berger and E.S. Calder and A.K. Patra and E.B. Pitman, and R.L. Wolpert (2014), Automating emulator construction for geophysical hazard maps. <em>SIAM/ASA Journal on Uncertainty Quantification</em>, 2(1), 126-152.
</p>
<p>J. Nocedal (1980), Updating quasi-Newton matrices with limited storage, <em>Math. Comput.</em>, 35, 773-782.
</p>
<p>D. C. Liu and J. Nocedal (1989), On the limited memory BFGS method for large scale optimization, <em>Math. Programming</em>, 45, p. 503-528.
</p>
<p>Brent, R. (1973), Algorithms for Minimization without Derivatives. Englewood Cliffs N.J.: Prentice-Hall.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  library(RobustGaSP)
  #------------------------
  # a 3 dimensional example
  #------------------------
  # dimensional of the inputs
  dim_inputs &lt;- 3    
  # number of the inputs
  num_obs &lt;- 50       
  # uniform samples of design
  input &lt;- matrix(runif(num_obs*dim_inputs), num_obs,dim_inputs) 
  
  # Following codes use maximin Latin Hypercube Design, which is typically better than uniform
  # library(lhs)
  # input &lt;- maximinLHS(n=num_obs, k=dim_inputs)  ##maximin lhd sample
  
  ####
  # outputs from the 3 dim dettepepel.3.data function
  
  output = matrix(0,num_obs,1)
  for(i in 1:num_obs){
    output[i]&lt;-dettepepel.3.data (input[i,])
  }
  
  # use constant mean basis, with no constraint on optimization
  # and marginal posterior mode estimation
  m1&lt;- rgasp(design = input, response = output, lower_bound=FALSE)
  
  # you can use specify the estimation as maximum likelihood estimation (MLE)
  m2&lt;- rgasp(design = input, response = output, method='mle',lower_bound=FALSE)
  
  ##let's do some comparison on prediction
  n_testing=1000
  testing_input=matrix(runif(n_testing*dim_inputs),n_testing,dim_inputs)
  
  m1_pred=predict(m1,testing_input=testing_input)
  m2_pred=predict(m2,testing_input=testing_input)
  
  
  ##root of mean square error and interval
  test_output = matrix(0,n_testing,1)
  for(i in 1:n_testing){
    test_output[i]&lt;-dettepepel.3.data (testing_input[i,])
  }
  
  ##root of mean square error
  sqrt(mean( (m1_pred$mean-test_output)^2))
  sqrt(mean( (m2_pred$mean-test_output)^2))
  sd(test_output)
  #---------------------------------------
  # a 1 dimensional example with zero mean
  #---------------------------------------


  input=10*seq(0,1,1/14)
  output&lt;-higdon.1.data(input)
  #the following code fit a GaSP with zero mean by setting zero.mean="Yes"
  model&lt;- rgasp(design = input, response = output, zero.mean="Yes")
  model
  
  testing_input = as.matrix(seq(0,10,1/100))
  model.predict&lt;-predict(model,testing_input)
  names(model.predict)
  
  #########plot predictive distribution
  testing_output=higdon.1.data(testing_input)
  plot(testing_input,model.predict$mean,type='l',col='blue',
       xlab='input',ylab='output')
  polygon( c(testing_input,rev(testing_input)),c(model.predict$lower95,
        rev(model.predict$upper95)),col =  "grey80", border = FALSE)
  lines(testing_input, testing_output)
  lines(testing_input,model.predict$mean,type='l',col='blue')
  lines(input, output,type='p')
  
  ## mean square erros
  mean((model.predict$mean-testing_output)^2)


  #-----------------------------------
  # a 2 dimensional example with trend
  #-----------------------------------
  # dimensional of the inputs
  dim_inputs &lt;- 2    
  # number of the inputs
  num_obs &lt;- 20       
  
  # uniform samples of design
  input &lt;-matrix(runif(num_obs*dim_inputs), num_obs,dim_inputs) 
  # Following codes use maximin Latin Hypercube Design, which is typically better than uniform
  # library(lhs)
  # input &lt;- maximinLHS(n=num_obs, k=dim_inputs)  # maximin lhd sample
  
  # outputs from a 2 dim function
  
  output &lt;- matrix(0,num_obs,1)
  for(i in 1:num_obs){
    output[i]&lt;-limetal.2.data (input[i,])
  }
  
  ####trend or mean basis
  X&lt;-cbind(rep(1,num_obs), input )
  
  
  # use constant mean basis with trend, with no constraint on optimization
  m2&lt;- rgasp(design = input, response = output,trend =X,  lower_bound=FALSE)

  show(m2)      # show this rgasp object 
  
  m2@beta_hat       # estimated inverse range parameters
  m2@theta_hat      # estimated trend parameters

  #--------------------------------------------------------------------------------------
  # an 8 dimensional example using only a subset inputs and a noise with unknown variance
  #--------------------------------------------------------------------------------------
  set.seed(1)
  # dimensional of the inputs
  dim_inputs &lt;- 8    
  # number of the inputs
  num_obs &lt;- 50       
  
  # uniform samples of design
  input &lt;-matrix(runif(num_obs*dim_inputs), num_obs,dim_inputs) 
  # Following codes use maximin Latin Hypercube Design, which is typically better than uniform
  # library(lhs)
  # input &lt;- maximinLHS(n=num_obs, k=dim_inputs)  # maximin lhd sample
  
  # rescale the design to the domain
  input[,1]&lt;-0.05+(0.15-0.05)*input[,1];
  input[,2]&lt;-100+(50000-100)*input[,2];
  input[,3]&lt;-63070+(115600-63070)*input[,3];
  input[,4]&lt;-990+(1110-990)*input[,4];
  input[,5]&lt;-63.1+(116-63.1)*input[,5];
  input[,6]&lt;-700+(820-700)*input[,6];
  input[,7]&lt;-1120+(1680-1120)*input[,7];
  input[,8]&lt;-9855+(12045-9855)*input[,8];
  
  # outputs from the 8 dim Borehole function
  
  output=matrix(0,num_obs,1)
  for(i in 1:num_obs){
    output[i]=borehole(input[i,])
  }
  
  
    
    
  
  # use constant mean basis with trend, with no constraint on optimization
  m3&lt;- rgasp(design = input[,c(1,4,6,7,8)], response = output, 
            nugget.est=TRUE, lower_bound=FALSE)

  m3@beta_hat       # estimated inverse range parameters
  m3@nugget     


</code></pre>

<hr>
<h2 id='rgasp-class'> Robust GaSP class </h2><span id='topic+rgasp-class'></span>

<h3>Description</h3>

<p> S4 class for Robust GaSP if the range and noise-variance ratio parameters are given and/or have been estimated.</p>


<h3>Objects from the Class</h3>

<p>Objects of this class are created and initialized with the function <code><a href="#topic+rgasp">rgasp</a></code> that computes the calculations needed for setting up the analysis.</p>


<h3>Slots</h3>


<dl>
<dt><code>p</code>:</dt><dd><p>Object of class <code>integer</code>. The dimensions of the inputs.</p>
</dd>
<dt><code>num_obs</code>:</dt><dd><p>Object of class <code>integer</code>. The number of observations.</p>
</dd>
<dt><code>input</code>:</dt><dd><p>Object of class <code>matrix</code> with dimension n x p. The design of experiments.</p>
</dd>
<dt><code>output</code>:</dt><dd><p>Object of class <code>matrix</code> with dimension n x 1. The Observations or output vector.</p>
</dd>
<dt><code>X</code>:</dt><dd><p>Object of class <code>matrix</code> of with dimension n x q. The mean basis function, i.e. the trend function.</p>
</dd>
<dt><code>zero_mean</code>:</dt><dd><p>A <code>character</code> to specify whether the mean is zero or not. &quot;Yes&quot; means it has zero mean and &quot;No&quot;&quot; means the mean is not zero. </p>
</dd>
<dt><code>q</code>:</dt><dd><p>Object of class <code>integer</code>. The number of mean basis.</p>
</dd>
<dt><code>LB</code>:</dt><dd><p>Object of class <code>vector</code> with dimension p x 1. The lower bound for inverse range parameters beta.</p>
</dd>
<dt><code>beta_initial</code>:</dt><dd><p>Object of class <code>vector</code> with the initial values of inverse range parameters p x 1.</p>
</dd>
<dt><code>beta_hat</code>:</dt><dd><p>Object of class <code>vector</code> with dimension p x 1. The inverse-range parameters.</p>
</dd>
<dt><code>log_post</code>:</dt><dd><p>Object of class <code>numeric</code> with the logarithm of marginal posterior.</p>
</dd>
<dt><code>R0</code>:</dt><dd><p>Object of class <code>list</code> of matrices where the j-th matrix is an absolute difference matrix of the j-th input vector.</p>
</dd>
<dt><code>theta_hat</code>:</dt><dd><p>Object of class <code>vector</code> with dimension q x 1. The the mean (trend) parameter.</p>
</dd>
<dt><code>L</code>:</dt><dd><p>Object of class <code>matrix</code> with dimension n x n. The Cholesky decomposition of the correlation matrix <code>R</code>, i.e. </p>
<p style="text-align: center;"><code class="reqn">L\%*\%t(L)=R</code>
</p>
</dd>
<dt><code>sigma2_hat</code>:</dt><dd><p>Object of the class <code>numeric</code>. The estimated variance parameter.</p>
</dd>
<dt><code>LX</code>:</dt><dd><p>Object of the class <code>matrix</code> with dimension q x q. The Cholesky decomposition of the correlation matrix </p>
<p style="text-align: center;"><code class="reqn">t(X)\%*\%R^{-1}\%*\%X</code>
</p>
</dd>
<dt><code>CL</code>:</dt><dd><p>Object of the class <code>vector</code> used for the lower bound and the prior.</p>
</dd>
<dt><code>nugget</code>:</dt><dd><p>A <code>numeric</code> object used for the noise-variance ratio parameter.</p>
</dd>
<dt><code>nugget.est</code>:</dt><dd><p>A <code>logical</code> object of whether the nugget is estimated (T) or fixed (F).</p>
</dd>
<dt><code>kernel_type</code>:</dt><dd><p>A <code>vector</code> of <code>character</code> to specify the type of kernel to use.</p>
</dd>
<dt><code>alpha</code>:</dt><dd><p>Object of class <code>vector</code> with dimension p x 1 for the roughness parameters in the kernel.</p>
</dd>
<dt><code>method</code>:</dt><dd><p>Object of class <code>character</code> to specify the method of parameter estimation. There are three values: <code>post_mode</code>, <code>mle</code> and <code>mmle</code>.</p>
</dd>
<dt><code>isotropic</code>:</dt><dd><p>Object of class <code>logical</code> to specify whether the kernel is isotropic. </p>
</dd>
<dt><code>call</code>:</dt><dd><p>The <code>call</code> to <code>rgasp</code> function to create the object.</p>
</dd>
</dl>



<h3>Methods</h3>


<dl>
<dt>show</dt><dd><p>Prints the main slots of the object. </p>
</dd>
<dt>predict</dt><dd><p>See <code><a href="#topic+predict.rgasp">predict</a></code>.</p>
</dd>
</dl>



<h3>Note</h3>

<p>The response <code>output</code> must have one dimension.
The number of observations in <code>input</code> must be equal to the number of experiments <code>output</code>.
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Jesus Palomo [aut],
  James Berger [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+RobustGaSP">RobustGaSP</a></code> for more details about how to create a <code>RobustGaSP</code> object.
</p>

<hr>
<h2 id='Sample'>
Sample for Robust GaSP model
</h2><span id='topic+Sample'></span><span id='topic+Sample.rgasp'></span><span id='topic+Sample.rgasp-class'></span><span id='topic+Sample+2Crgasp-method'></span>

<h3>Description</h3>

<p>Function to sample Robust GaSP after the Robust GaSP model has been constructed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'rgasp'
Sample(object, testing_input, num_sample=1,
testing_trend= matrix(1,dim(testing_input)[1],1),
r0=NA, rr0=NA, sample_data=T,
...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Sample_+3A_object">object</code></td>
<td>
<p> an object of  class <code>rgasp</code>.</p>
</td></tr>
<tr><td><code id="Sample_+3A_testing_input">testing_input</code></td>
<td>
<p>a matrix containing the inputs where the <code>rgasp</code> is to sample.</p>
</td></tr>
<tr><td><code id="Sample_+3A_num_sample">num_sample</code></td>
<td>
<p>number of samples one wants. </p>
</td></tr>
<tr><td><code id="Sample_+3A_testing_trend">testing_trend</code></td>
<td>
<p>a matrix of mean/trend for prediction.</p>
</td></tr>
<tr><td><code id="Sample_+3A_r0">r0</code></td>
<td>

<p>the distance between input and testing input. If the value is <code>NA</code>, it will be calculated later. It can also be specified by the user. If specified by user, it is either a <code>matrix</code> or <code>list</code>. The default value is <code>NA</code>.
</p>
</td></tr>
<tr><td><code id="Sample_+3A_rr0">rr0</code></td>
<td>

<p>the distance between testing input and testing input. If the value is <code>NA</code>, it will be calculated later. It can also be specified by the user. If specified by user, it is either a <code>matrix</code> or <code>list</code>. The default value is <code>NA</code>.
</p>
</td></tr>
<tr><td><code id="Sample_+3A_...">...</code></td>
<td>
<p>Extra arguments to be passed to the function (not implemented yet).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The returned value is a <code>matrix</code> where each column is a sample on the prespecified inputs. 
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Jesus Palomo [aut],
  James Berger [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>

<p>Mengyang Gu. (2016). Robust Uncertainty Quantification and Scalable Computation for Computer Models with Massive Output. Ph.D. thesis. Duke University.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  #------------------------
  # a 1 dimensional example
  #------------------------
  
###########1dim higdon.1.data 
p1 = 1     ###dimensional of the inputs
dim_inputs1 &lt;- p1
n1 = 15   ###sample size or number of training computer runs you have 
num_obs1 &lt;- n1
input1 = 10*matrix(runif(num_obs1*dim_inputs1), num_obs1,dim_inputs1) ##uniform
#####lhs is better
#library(lhs)
#input1 = 10*maximinLHS(n=num_obs1, k=dim_inputs1)  ##maximin lhd sample
output1 = matrix(0,num_obs1,1)
for(i in 1:num_obs1){
  output1[i]=higdon.1.data (input1[i])
}





m1&lt;- rgasp(design = input1, response = output1, lower_bound=FALSE)

#####locations to samples
testing_input1 = seq(0,10,1/50) 
testing_input1=as.matrix(testing_input1)
#####draw 10 samples
m1_sample=Sample(m1,testing_input1,num_sample=10)

#####plot these samples
matplot(testing_input1,m1_sample, type='l',xlab='input',ylab='output')
lines(input1,output1,type='p')


</code></pre>

<hr>
<h2 id='search_LB_prob'>
Search for the default lower bound of range parameters. 
</h2><span id='topic+search_LB_prob'></span>

<h3>Description</h3>

<p>Function to find the values to construct the default lower bound of range parameters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>search_LB_prob(param, R0, COND_NUM_UB, p, kernel_type, alpha, nugget)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="search_LB_prob_+3A_param">param</code></td>
<td>

<p>A vector of natural logarithm of inverse-range parameters and natural logarithm of the nugget-variance ratio parameter.
</p>
</td></tr>
<tr><td><code id="search_LB_prob_+3A_r0">R0</code></td>
<td>

<p>A List of matrix where the j-th matrix is an absolute difference matrix of the j-th input vector.
</p>
</td></tr>
<tr><td><code id="search_LB_prob_+3A_cond_num_ub">COND_NUM_UB</code></td>
<td>

<p>The maximum condition number of the correlation matrix.
</p>
</td></tr>
<tr><td><code id="search_LB_prob_+3A_p">p</code></td>
<td>

</td></tr>
<tr><td><code id="search_LB_prob_+3A_kernel_type">kernel_type</code></td>
<td>

<p>Type of kernel. <code>matern_3_2</code> and <code>matern_5_2</code> are <code>Matern kernel</code> with roughness parameter 3/2 and 5/2 respectively. <code>pow_exp</code> is power exponential kernel with roughness parameter alpha. If <code>pow_exp</code> is to be used, one needs to specify its roughness parameter alpha.
</p>
</td></tr>
<tr><td><code id="search_LB_prob_+3A_alpha">alpha</code></td>
<td>

<p>Roughness parameters in the kernel functions.
</p>
</td></tr>
<tr><td><code id="search_LB_prob_+3A_nugget">nugget</code></td>
<td>

<p>The nugget-variance ratio parameter if this parameter is fixed.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>vector</code> of values used in constructing the default lower bound of range parameters.
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Jesus Palomo [aut],
  James Berger [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>

<p>Mengyang Gu. (2016). Robust Uncertainty Quantification and Scalable Computation for Computer Models with Massive Output. Ph.D. thesis. Duke University.
</p>

<hr>
<h2 id='separable_kernel'>Product correlation matrix with the product form
</h2><span id='topic+separable_kernel'></span>

<h3>Description</h3>

<p>Function to construct the product correlation matrix with the product form.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>separable_kernel(R0, beta, kernel_type, alpha)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="separable_kernel_+3A_r0">R0</code></td>
<td>

<p>A List of matrix where the j-th matrix is an absolute difference matrix of the j-th input vector.
</p>
</td></tr>
<tr><td><code id="separable_kernel_+3A_beta">beta</code></td>
<td>

<p>The range parameters.
</p>
</td></tr>
<tr><td><code id="separable_kernel_+3A_kernel_type">kernel_type</code></td>
<td>

<p>A vector specifying the type of kernels of each coordinate of the input. <code>matern_3_2</code> and <code>matern_5_2</code> are <code>Matern correlation</code> with roughness parameter 3/2 and 5/2 respectively. <code>pow_exp</code> is power exponential correlation with roughness parameter alpha. If <code>pow_exp</code> is to be used, one needs to specify its roughness parameter alpha. The default choice is <code>matern_5_2</code>. The <code>periodic_gauss</code> means the Gaussian kernel with periodic folding method with be used. The <code>periodic_exp</code> means the exponential kernel with periodic folding method will be used.
</p>
</td></tr>
<tr><td><code id="separable_kernel_+3A_alpha">alpha</code></td>
<td>

<p>Roughness parameters in the kernel functions.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The product correlation matrix with the product form.
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Jesus Palomo [aut],
  James Berger [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>

<hr>
<h2 id='separable_multi_kernel'>Product correlation matrix with the product form
</h2><span id='topic+separable_multi_kernel'></span>

<h3>Description</h3>

<p>Function to construct the product correlation matrix with the product form. The kernel can be different for each coordinate of the input. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>separable_multi_kernel(R0, beta, kernel_type, alpha)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="separable_multi_kernel_+3A_r0">R0</code></td>
<td>

<p>A List of matrix where the j-th matrix is an absolute difference matrix of the j-th input vector.
</p>
</td></tr>
<tr><td><code id="separable_multi_kernel_+3A_beta">beta</code></td>
<td>

<p>The range parameters.
</p>
</td></tr>
<tr><td><code id="separable_multi_kernel_+3A_kernel_type">kernel_type</code></td>
<td>

<p>A vector of <code>integer</code> specifying the type of kernels of each coordinate of the input. 
In each coordinate of the vector, 1 means the <code>pow_exp</code> kernel with roughness parameter specified in alpha; 2 means  <code>matern_3_2</code>  kernel; 3 means <code>matern_5_2</code>  kernel; 5 means <code>periodic_gauss</code> kernel; 5 means <code>periodic_exp</code> kernel. 
</p>
</td></tr>
<tr><td><code id="separable_multi_kernel_+3A_alpha">alpha</code></td>
<td>

<p>Roughness parameters in the kernel functions.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The product correlation matrix with the product form.
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Jesus Palomo [aut],
  James Berger [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>

<hr>
<h2 id='show'>
Show Robust GaSP object
</h2><span id='topic+show'></span><span id='topic+show.rgasp'></span><span id='topic+show.rgasp-class'></span><span id='topic+show+2Crgasp-method'></span>

<h3>Description</h3>

<p>Function to print Robust GaSP models after the Robust GaSP model has been constructed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'rgasp'
show(object)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="show_+3A_object">object</code></td>
<td>
<p> an object of  class <code>rgasp</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Jesus Palomo [aut],
  James Berger [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  #------------------------
  # a 3 dimensional example
  #------------------------
  # dimensional of the inputs
  dim_inputs &lt;- 3    
  # number of the inputs
  num_obs &lt;- 30       
  # uniform samples of design
  input &lt;- matrix(runif(num_obs*dim_inputs), num_obs,dim_inputs) 
  
  # Following codes use maximin Latin Hypercube Design, which is typically better than uniform
  # library(lhs)
  # input &lt;- maximinLHS(n=num_obs, k=dim_inputs)  ##maximin lhd sample
  
  ####
  # outputs from the 3 dim dettepepel.3.data function
  
  output = matrix(0,num_obs,1)
  for(i in 1:num_obs){
    output[i]&lt;-dettepepel.3.data (input[i,])
  }
  
  # use constant mean basis, with no constraint on optimization
  m1&lt;- rgasp(design = input, response = output, lower_bound=FALSE)
  
  # the following use constraints on optimization
  # m1&lt;- rgasp(design = input, response = output, lower_bound=TRUE)
  
  # the following use a single start on optimization
  # m1&lt;- rgasp(design = input, response = output, lower_bound=FALSE)
  
  
  show(m1)
</code></pre>

<hr>
<h2 id='show.ppgasp'>
Show parllel partial Gaussian stochastic process (PP GaSP) object
</h2><span id='topic+show.ppgasp'></span><span id='topic+show.ppgasp-class'></span><span id='topic+show+2Cppgasp-method'></span>

<h3>Description</h3>

<p>Function to print the PP GaSP model after the PP GaSP model has been constructed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'ppgasp'
show(object)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="show.ppgasp_+3A_object">object</code></td>
<td>
<p> an object of  class <code>ppgasp</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Jesus Palomo [aut],
  James Berger [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
  library(RobustGaSP)
  
  ###PP GaSP model for the humanity model
  data(humanity_model)
  ##pp gasp
  m.ppgasp=ppgasp(design=humanity.X,response=humanity.Y,nugget.est= TRUE)
  show(m.ppgasp)
</code></pre>

<hr>
<h2 id='simulate'>
Sample for Robust GaSP model
</h2><span id='topic+simulate'></span><span id='topic+simulate.rgasp'></span><span id='topic+simulate.rgasp-class'></span><span id='topic+simulate+2Crgasp-method'></span>

<h3>Description</h3>

<p>Function to sample Robust GaSP after the Robust GaSP model has been constructed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'rgasp'
simulate(object, testing_input, num_sample=1,
testing_trend= matrix(1,dim(testing_input)[1],1),
r0=NA,rr0=NA,sample_data=T,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="simulate_+3A_object">object</code></td>
<td>
<p> an object of  class <code>rgasp</code>.</p>
</td></tr>
<tr><td><code id="simulate_+3A_testing_input">testing_input</code></td>
<td>
<p>a matrix containing the inputs where the <code>rgasp</code> is to sample.</p>
</td></tr>
<tr><td><code id="simulate_+3A_num_sample">num_sample</code></td>
<td>
<p>number of samples one wants. </p>
</td></tr>
<tr><td><code id="simulate_+3A_testing_trend">testing_trend</code></td>
<td>
<p>a matrix of mean/trend for prediction.</p>
</td></tr>
<tr><td><code id="simulate_+3A_r0">r0</code></td>
<td>

<p>the distance between input and testing input. If the value is <code>NA</code>, it will be calculated later. It can also be specified by the user. If specified by user, it is either a <code>matrix</code> or <code>list</code>. The default value is <code>NA</code>.
</p>
</td></tr>
<tr><td><code id="simulate_+3A_rr0">rr0</code></td>
<td>

<p>the distance between testing input and testing input. If the value is <code>NA</code>, it will be calculated later. It can also be specified by the user. If specified by user, it is either a <code>matrix</code> or <code>list</code>. The default value is <code>NA</code>.
</p>
</td></tr>
<tr><td><code id="simulate_+3A_sample_data">sample_data</code></td>
<td>

<p>a boolean value. If <code>T</code>, the interval of the data will be calculated. Otherwise, the interval of the mean of the data will be calculted.
</p>
</td></tr>
<tr><td><code id="simulate_+3A_...">...</code></td>
<td>
<p>Extra arguments to be passed to the function (not implemented yet).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The returned value is a <code>matrix</code> where each column is a sample on the prespecified inputs. 
</p>


<h3>Author(s)</h3>

<p>Mengyang Gu [aut, cre],
  Jesus Palomo [aut],
  James Berger [aut]
</p>
<p>Maintainer: Mengyang Gu &lt;mengyang@pstat.ucsb.edu&gt;
</p>


<h3>References</h3>

<p>M. Gu. (2016). Robust uncertainty quantification and scalable computation for computer models with massive output. Ph.D. thesis. Duke University.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  #------------------------
  # a 1 dimensional example
  #------------------------
  
###########1dim higdon.1.data 
p1 = 1     ###dimensional of the inputs
dim_inputs1 &lt;- p1
n1 = 15   ###sample size or number of training computer runs you have 
num_obs1 &lt;- n1
input1 = 10*matrix(runif(num_obs1*dim_inputs1), num_obs1,dim_inputs1) ##uniform
#####lhs is better
#library(lhs)
#input1 = 10*maximinLHS(n=num_obs1, k=dim_inputs1)  ##maximin lhd sample
output1 = matrix(0,num_obs1,1)
for(i in 1:num_obs1){
  output1[i]=higdon.1.data (input1[i])
}




m1&lt;- rgasp(design = input1, response = output1, lower_bound=FALSE)

#####locations to samples
testing_input1 = seq(0,10,1/50) 
testing_input1=as.matrix(testing_input1)
#####draw 10 samples
m1_sample=simulate(m1,testing_input1,num_sample=10)

#####plot these samples
matplot(testing_input1,m1_sample, type='l',xlab='input',ylab='output')
lines(input1,output1,type='p')


</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
