<!DOCTYPE html><html><head><title>Help for package NonProbEst</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {NonProbEst}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#calib_weights'><p>Weights of the calibration estimator</p></a></li>
<li><a href='#confidence_interval'><p>Confidence interval</p></a></li>
<li><a href='#fast_jackknife_variance'><p>Calculates Jackknife variance without reweighting</p></a></li>
<li><a href='#generic_jackknife_variance'><p>Calculates Jackknife variance with reweighting for an arbitrary estimator</p></a></li>
<li><a href='#jackknife_variance'><p>Calculates Jackknife variance with reweighting for PSA</p></a></li>
<li><a href='#lee_weights'><p>Calculates Lee weights</p></a></li>
<li><a href='#matching'><p>Predicts unknown responses by matching</p></a></li>
<li><a href='#mean_estimation'><p>Estimates the population means</p></a></li>
<li><a href='#model_assisted'><p>Calculates a model assisted estimation</p></a></li>
<li><a href='#model_based'><p>Calculates a model based estimation</p></a></li>
<li><a href='#model_calibrated'><p>Calculates a model calibrated estimation</p></a></li>
<li><a href='#population'><p>A full population</p></a></li>
<li><a href='#prop_estimation'><p>Estimates the population proportion</p></a></li>
<li><a href='#propensities'><p>Calculates sample propensities</p></a></li>
<li><a href='#sampleNP'><p>A non-probabilistic sample</p></a></li>
<li><a href='#sampleP'><p>A probabilistic sample</p></a></li>
<li><a href='#sc_weights'><p>Calculates Schonlau and Couper weights</p></a></li>
<li><a href='#total_estimation'><p>Estimates the population totals</p></a></li>
<li><a href='#valliant_weights'><p>Calculates Valliant weights</p></a></li>
<li><a href='#vd_weights'><p>Calculates Valliant and Dever weights</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Estimation in Nonprobability Sampling</td>
</tr>
<tr>
<td>Version:</td>
<td>0.2.4</td>
</tr>
<tr>
<td>Author:</td>
<td>Luis Castro Martín &lt;luiscastro193@gmail.com&gt;, Ramón Ferri García &lt;rferri@ugr.es&gt; and María del Mar Rueda &lt;mrueda@ugr.es&gt;</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Luis Castro Martín &lt;luiscastro193@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Different inference procedures are proposed in the literature to correct for selection bias that might be introduced with non-random selection mechanisms. A class of methods to correct for selection bias is to apply a statistical model to predict the units not in the sample (super-population modeling). Other studies use calibration or Statistical Matching (statistically match nonprobability and probability samples). To date, the more relevant methods are weighting by Propensity Score Adjustment (PSA).
    The Propensity Score Adjustment method was originally developed to construct weights by estimating response probabilities and using them in Horvitz–Thompson type estimators. This method is usually used by combining a non-probability sample with a reference sample to construct propensity models for the non-probability sample. Calibration can be used in a posterior way to adding information of auxiliary variables.
    Propensity scores in PSA are usually estimated using logistic regression models. Machine learning classification algorithms can be used as alternatives for logistic regression as a technique to estimate propensities.
    The package 'NonProbEst' implements some of these methods and thus provides a wide options to work with data coming from a non-probabilistic sample.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>Imports:</td>
<td>caret, sampling, e1071, glmnet, Matrix</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.0</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2020-06-03 11:56:47 UTC; luis</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2020-06-03 12:10:03 UTC</td>
</tr>
</table>
<hr>
<h2 id='calib_weights'>Weights of the calibration estimator</h2><span id='topic+calib_weights'></span>

<h3>Description</h3>

<p>Calculates the calibration weights from a disjunct matrix of covariates, a vector of population totals and a vector of initial weights.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calib_weights(Xs, totals, initial_weights, N, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="calib_weights_+3A_xs">Xs</code></td>
<td>
<p>Matrix of calibration variables.</p>
</td></tr>
<tr><td><code id="calib_weights_+3A_totals">totals</code></td>
<td>
<p>A vector containing population totals for each column (class) of the calibration variables matrix.</p>
</td></tr>
<tr><td><code id="calib_weights_+3A_initial_weights">initial_weights</code></td>
<td>
<p>A vector containing the initial weights for each individual.</p>
</td></tr>
<tr><td><code id="calib_weights_+3A_n">N</code></td>
<td>
<p>Integer indicating the population size.</p>
</td></tr>
<tr><td><code id="calib_weights_+3A_...">...</code></td>
<td>
<p>Further arguments to be passed to the 'calib' function from the 'sampling' package.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function uses the 'calib' function from the 'sampling' package for the estimation of g-weights, which are multiplied by the initial weights to obtain the final calibration weights. The initial weights can be calculated previously from the propensities for any of the implemented methods (see functions <code>lee_weights</code>, <code>sc_weights</code>, <code>valliant_weights</code>, <code>vd_weights</code>). The population size is used to scale said initial weights so they are easier to calibrate.
</p>


<h3>Value</h3>

<p>A vector with the corresponding weights.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n = nrow(sampleNP)
N = 50000
language_total = 45429
covariates = c("education_primaria", "education_secundaria",
   "age", "sex")
pi = propensities(sampleNP, sampleP, covariates, algorithm = "glm", smooth = FALSE)
wi = sc_weights(pi$convenience)
calib_weights(sampleNP$language, language_total, wi, N, method = "raking")
</code></pre>

<hr>
<h2 id='confidence_interval'>Confidence interval</h2><span id='topic+confidence_interval'></span>

<h3>Description</h3>

<p>Calculates the confidence interval for the estimator considered.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>confidence_interval(estimation, std_dev, confidence = 0.95)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="confidence_interval_+3A_estimation">estimation</code></td>
<td>
<p>A numeric value specifying the point estimation.</p>
</td></tr>
<tr><td><code id="confidence_interval_+3A_std_dev">std_dev</code></td>
<td>
<p>A numeric value specifying the standard deviation of the point estimation.</p>
</td></tr>
<tr><td><code id="confidence_interval_+3A_confidence">confidence</code></td>
<td>
<p>A numeric value between 0 and 1 specifying the confidence level, taken as 1 - alpha (1 - Type I error). By default, its value is 0.95.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector containing the lower and upper bounds.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>covariates = c("education_primaria","education_secundaria",
"age", "sex")
pi = propensities(sampleNP, sampleP, covariates, algorithm = "glm", smooth = FALSE)
psa_weights = sc_weights(pi$convenience)
N = 50000
Y_est = total_estimation(sampleNP, psa_weights, estimated_vars = "vote_pens", N = N)
VY_est = fast_jackknife_variance(sampleNP, psa_weights,
   estimated_vars = "vote_pens") * N^2
confidence_interval(Y_est, sqrt(VY_est), confidence = 0.90)
</code></pre>

<hr>
<h2 id='fast_jackknife_variance'>Calculates Jackknife variance without reweighting</h2><span id='topic+fast_jackknife_variance'></span>

<h3>Description</h3>

<p>Calculates the variance of a given estimator by Leave-One-Out Jackknife (Quenouille, 1956) with the original adjusted weights.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fast_jackknife_variance(sample, weights, estimated_vars, N = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fast_jackknife_variance_+3A_sample">sample</code></td>
<td>
<p>A data frame containing the sample.</p>
</td></tr>
<tr><td><code id="fast_jackknife_variance_+3A_weights">weights</code></td>
<td>
<p>A vector containing the pre-calculated weights.</p>
</td></tr>
<tr><td><code id="fast_jackknife_variance_+3A_estimated_vars">estimated_vars</code></td>
<td>
<p>A string vector specifying the variables for which the estimators' variance are to be estimated.</p>
</td></tr>
<tr><td><code id="fast_jackknife_variance_+3A_n">N</code></td>
<td>
<p>Integer indicating the population size. Optional.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The variance estimation is performed by eliminating an individual at each iteration with its corresponding weight and estimating the mean of the corresponding subsample, which is further used in the Jackknife formula as the usual procedure. The calculation of variance estimates through this procedure might take less computation time but also might not take into account the variance of the weighting method.
</p>


<h3>Value</h3>

<p>A vector containing the resulting variance for each variable.
</p>


<h3>References</h3>

<p>Quenouille, M. H. (1956). <em>Notes on bias in estimation.</em> Biometrika, 43(3/4), 353-360.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>covariates = c("education_primaria", "education_secundaria")
data_propensities = propensities(sampleNP, sampleP, covariates)
psa_weights = sc_weights(data_propensities$convenience)
fast_jackknife_variance(sampleNP, psa_weights, c("vote_pens"), 50000)
</code></pre>

<hr>
<h2 id='generic_jackknife_variance'>Calculates Jackknife variance with reweighting for an arbitrary estimator</h2><span id='topic+generic_jackknife_variance'></span>

<h3>Description</h3>

<p>Calculates the variance of a given estimator by Leave-One-Out Jackknife (Quenouille, 1956) with reweighting in each iteration.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>generic_jackknife_variance(sample, estimator, N = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="generic_jackknife_variance_+3A_sample">sample</code></td>
<td>
<p>Data frame containing the non-probabilistic sample.</p>
</td></tr>
<tr><td><code id="generic_jackknife_variance_+3A_estimator">estimator</code></td>
<td>
<p>Function that, given a sample as a parameter, returns an estimation.</p>
</td></tr>
<tr><td><code id="generic_jackknife_variance_+3A_n">N</code></td>
<td>
<p>Integer indicating the population size. Optional.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The estimation of the variance requires a recalculation of the estimates in each iteration which might involve weighting adjustments, leading to an increase in computation time. It is expected that the estimated variance captures the weighting adjustments' variability and the estimator's variability.
</p>


<h3>Value</h3>

<p>The resulting variance.
</p>


<h3>References</h3>

<p>Quenouille, M. H. (1956). <em>Notes on bias in estimation.</em> Biometrika, 43(3/4), 353-360.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
covariates = c("education_primaria", "education_secundaria",
   "age", "sex", "language")
if (is.numeric(sampleNP$vote_gen))
   sampleNP$vote_gen = factor(sampleNP$vote_gen, c(0, 1), c('F', 'T'))
vote_gen_estimator = function(sample) {
   model_based(sample, population, covariates,
      "vote_gen", positive_label = 'T', algorithm = 'glmnet')
}
generic_jackknife_variance(sampleNP, vote_gen_estimator)

</code></pre>

<hr>
<h2 id='jackknife_variance'>Calculates Jackknife variance with reweighting for PSA</h2><span id='topic+jackknife_variance'></span>

<h3>Description</h3>

<p>Calculates the variance of PSA by Leave-One-Out Jackknife (Quenouille, 1956) with reweighting in each iteration.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>jackknife_variance(
  estimated_vars,
  convenience_sample,
  reference_sample,
  covariates,
  N = NULL,
  algorithm = "glm",
  smooth = FALSE,
  proc = NULL,
  trControl = trainControl(classProbs = TRUE),
  weighting.func = "sc",
  g = 5,
  calib = FALSE,
  calib_vars = NULL,
  totals = NULL,
  args.calib = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="jackknife_variance_+3A_estimated_vars">estimated_vars</code></td>
<td>
<p>A string vector specifying the variables for which the estimators' variance are to be estimated.</p>
</td></tr>
<tr><td><code id="jackknife_variance_+3A_convenience_sample">convenience_sample</code></td>
<td>
<p>Data frame containing the non-probabilistic sample.</p>
</td></tr>
<tr><td><code id="jackknife_variance_+3A_reference_sample">reference_sample</code></td>
<td>
<p>Data frame containing the probabilistic sample.</p>
</td></tr>
<tr><td><code id="jackknife_variance_+3A_covariates">covariates</code></td>
<td>
<p>String vector specifying the common variables to use for training.</p>
</td></tr>
<tr><td><code id="jackknife_variance_+3A_n">N</code></td>
<td>
<p>Integer indicating the population size. Optional.</p>
</td></tr>
<tr><td><code id="jackknife_variance_+3A_algorithm">algorithm</code></td>
<td>
<p>A string specifying which classification or regression model to use (same as caret's method). By default, its value is &quot;glm&quot; (logistic regression).</p>
</td></tr>
<tr><td><code id="jackknife_variance_+3A_smooth">smooth</code></td>
<td>
<p>A logical value; if TRUE, propensity estimates pi_i are smoothed applying the formula (1000*pi_i + 0.5)/1001</p>
</td></tr>
<tr><td><code id="jackknife_variance_+3A_proc">proc</code></td>
<td>
<p>A string or vector of strings specifying if any of the data preprocessing techniques available in <a href="caret.html#topic+train">train</a> function from 'caret' package should be applied to data prior to the propensity estimation. By default, its value is NULL and no preprocessing is applied.</p>
</td></tr>
<tr><td><code id="jackknife_variance_+3A_trcontrol">trControl</code></td>
<td>
<p>A trainControl specifying the computational nuances of the <a href="caret.html#topic+train">train</a> function.</p>
</td></tr>
<tr><td><code id="jackknife_variance_+3A_weighting.func">weighting.func</code></td>
<td>
<p>A string specifying which function should be used to compute weights from propensity scores. Available functions are the following: </p>
 <ul>
<li> <p><code>sc</code> calls <a href="#topic+sc_weights">sc_weights</a>. </p>
</li>
<li> <p><code>valliant</code> calls <a href="#topic+valliant_weights">valliant_weights</a>. </p>
</li>
<li> <p><code>lee</code> calls <a href="#topic+lee_weights">lee_weights</a>. </p>
</li>
<li> <p><code>vd</code> calls <a href="#topic+vd_weights">vd_weights</a>. </p>
</li></ul>
</td></tr>
<tr><td><code id="jackknife_variance_+3A_g">g</code></td>
<td>
<p>If <code>weighting.func = "lee"</code> or <code>weighting.func = "vd"</code>, this element specifies the number of strata to use; by default, its value is 5.</p>
</td></tr>
<tr><td><code id="jackknife_variance_+3A_calib">calib</code></td>
<td>
<p>A logical value; if TRUE, PSA weights are used as initial weights for calibration. By default, its value is FALSE.</p>
</td></tr>
<tr><td><code id="jackknife_variance_+3A_calib_vars">calib_vars</code></td>
<td>
<p>A string or vector of strings specifying the variables to be used for calibration. By default, its value is NULL.</p>
</td></tr>
<tr><td><code id="jackknife_variance_+3A_totals">totals</code></td>
<td>
<p>A vector containing population totals for each column (class) of the calibration variables matrix. Ignored if <code>calib</code> is set to FALSE.</p>
</td></tr>
<tr><td><code id="jackknife_variance_+3A_args.calib">args.calib</code></td>
<td>
<p>A list containing further arguments to be passed to the <a href="#topic+calib_weights">calib_weights</a> function.</p>
</td></tr>
<tr><td><code id="jackknife_variance_+3A_...">...</code></td>
<td>
<p>Further parameters to be passed to the <a href="caret.html#topic+train">train</a> function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The estimation of the variance requires a recalculation of the estimates in each iteration which might involve weighting adjustments, leading to an increase in computation time. It is expected that the estimated variance captures the weighting adjustments' variability and the estimator's variability.
</p>


<h3>Value</h3>

<p>The resulting variance.
</p>


<h3>References</h3>

<p>Quenouille, M. H. (1956). <em>Notes on bias in estimation.</em> Biometrika, 43(3/4), 353-360.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
#A simple example without calibration and default parameters
covariates = c("education_primaria", "education_secundaria")
jackknife_variance("vote_pens",sampleNP, sampleP, covariates)

#An example with linear calibration and default parameters
covariates = c("education_primaria", "education_secundaria")
calib_vars = c("age", "sex")
totals = c(2544377, 24284)

jackknife_variance("vote_pens",sampleNP, sampleP, covariates,
calib = T, calib_vars, totals, args.calib = list(method = "linear"))

</code></pre>

<hr>
<h2 id='lee_weights'>Calculates Lee weights</h2><span id='topic+lee_weights'></span>

<h3>Description</h3>

<p>Computes weights from propensity estimates using the propensity stratification design weights averaging formula introduced in Lee (2006) and Lee and Valliant (2009).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lee_weights(convenience_propensities, reference_propensities, g = 5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lee_weights_+3A_convenience_propensities">convenience_propensities</code></td>
<td>
<p>A vector with the propensities associated with the convenience sample.</p>
</td></tr>
<tr><td><code id="lee_weights_+3A_reference_propensities">reference_propensities</code></td>
<td>
<p>A vector with the propensities associated with the reference sample.</p>
</td></tr>
<tr><td><code id="lee_weights_+3A_g">g</code></td>
<td>
<p>The number of strata to use; by default, its value is 5.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function takes the vector of propensities <code class="reqn">\pi(x)</code> and calculates the weights to be applied in the Horvitz-Thompson estimator using the formula that can be found in Lee (2006) and Lee and Valliant (2009). The vector of propensities is divided in <em>g</em> strata (ideally five according to Cochran, 1968) aiming to have individuals with similar propensities in each strata. After the stratification, weight is calculated as follows for an individual <em>i</em>:
</p>
<p style="text-align: center;"><code class="reqn">w_i = \frac{n_r(g_i) / n_r}{n_v(g_i) / n_v}</code>
</p>

<p>where <code class="reqn">g_i</code> represents the strata to which <em>i</em> belongs, <code class="reqn">n_r (g_i)</code> and <code class="reqn">n_v (g_i)</code> are the number of individuals in the <code class="reqn">g_i</code> strata from the reference and the convenience sample respectively, and <code class="reqn">n_r</code> and <code class="reqn">n_v</code> are the sample sizes for the reference and the convenience sample respectively.
</p>


<h3>Value</h3>

<p>A vector with the corresponding weights.
</p>


<h3>References</h3>

<p>Lee, S. (2006). <em>Propensity score adjustment as a weighting scheme for volunteer panel web surveys.</em> Journal of official statistics, 22(2), 329.
</p>
<p>Lee, S., &amp; Valliant, R. (2009). <em>Estimation for volunteer panel web surveys using propensity score adjustment and calibration adjustment.</em> Sociological Methods &amp; Research, 37(3), 319-343.
</p>
<p>Cochran, W. G. (1968). <em>The Effectiveness of Adjustment by Subclassification in Removing Bias in Observational Studies.</em> Biometrics, 24(2), 295-313
</p>


<h3>Examples</h3>

<pre><code class='language-R'>covariates = c("education_primaria", "education_secundaria")
data_propensities = propensities(sampleNP, sampleP, covariates)
lee_weights(data_propensities$convenience, data_propensities$reference)
</code></pre>

<hr>
<h2 id='matching'>Predicts unknown responses by matching</h2><span id='topic+matching'></span>

<h3>Description</h3>

<p>It uses the matching method introduced by Rivers (2007). The idea is to model the relationship between y_k and x_k using the convenience sample in order to predict y_k for the reference sample. You can then predict the total using the 'total_estimation' method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>matching(
  convenience_sample,
  reference_sample,
  covariates,
  estimated_var,
  positive_label = NULL,
  algorithm = "glm",
  proc = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="matching_+3A_convenience_sample">convenience_sample</code></td>
<td>
<p>Data frame containing the non-probabilistic sample.</p>
</td></tr>
<tr><td><code id="matching_+3A_reference_sample">reference_sample</code></td>
<td>
<p>Data frame containing the probabilistic sample.</p>
</td></tr>
<tr><td><code id="matching_+3A_covariates">covariates</code></td>
<td>
<p>String vector specifying the common variables to use for training.</p>
</td></tr>
<tr><td><code id="matching_+3A_estimated_var">estimated_var</code></td>
<td>
<p>String specifying the variable to estimate.</p>
</td></tr>
<tr><td><code id="matching_+3A_positive_label">positive_label</code></td>
<td>
<p>String specifying the label to be considered positive if the estimated variable is categorical. Leave it as the default NULL otherwise.</p>
</td></tr>
<tr><td><code id="matching_+3A_algorithm">algorithm</code></td>
<td>
<p>A string specifying which classification or regression model to use (same as caret's method).</p>
</td></tr>
<tr><td><code id="matching_+3A_proc">proc</code></td>
<td>
<p>A string or vector of strings specifying if any of the data preprocessing techniques available in <a href="caret.html#topic+train">train</a> function from 'caret' package should be applied to data prior to the propensity estimation. By default, its value is NULL and no preprocessing is applied.</p>
</td></tr>
<tr><td><code id="matching_+3A_...">...</code></td>
<td>
<p>Further parameters to be passed to the <a href="caret.html#topic+train">train</a> function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Training of the models is done via the 'caret' package. The algorithm specified in <code>algorithm</code> must match one of the names in the list of algorithms supported by 'caret'. If the estimated variable is categorical, probabilities are returned.
</p>


<h3>Value</h3>

<p>A vector containing the estimated responses for the reference sample.
</p>


<h3>References</h3>

<p>Rivers, D. (2007). <em>Sampling for Web Surveys</em>. Presented in Joint Statistical Meetings, Salt Lake City, UT.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#Simple example with default parameters
N = 50000
covariates = c("education_primaria", "education_secundaria")
if (is.numeric(sampleNP$vote_gen))
   sampleNP$vote_gen = factor(sampleNP$vote_gen, c(0, 1), c('F', 'T'))
estimated_votes = data.frame(
   vote_gen = matching(sampleNP, sampleP, covariates, "vote_gen", 'T')
)
total_estimation(estimated_votes, N / nrow(estimated_votes), c("vote_gen"), N)
</code></pre>

<hr>
<h2 id='mean_estimation'>Estimates the population means</h2><span id='topic+mean_estimation'></span>

<h3>Description</h3>

<p>Estimates the means for the specified variables measured in a sample given some pre-calculated weights.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mean_estimation(sample, weights, estimated_vars, N = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mean_estimation_+3A_sample">sample</code></td>
<td>
<p>A data frame containing the sample with the variables for which the means are to be calculated.</p>
</td></tr>
<tr><td><code id="mean_estimation_+3A_weights">weights</code></td>
<td>
<p>A vector of pre-calculated weights.</p>
</td></tr>
<tr><td><code id="mean_estimation_+3A_estimated_vars">estimated_vars</code></td>
<td>
<p>String vector specifying the variables in the sample to be estimated.</p>
</td></tr>
<tr><td><code id="mean_estimation_+3A_n">N</code></td>
<td>
<p>An integer specifying the population size (optional).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector with the corresponding estimations.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>covariates = c("education_primaria", "education_secundaria")
data_propensities = propensities(sampleNP, sampleP, covariates)
psa_weights = sc_weights(data_propensities$convenience)
mean_estimation(sampleNP, psa_weights, c("vote_pens"))
</code></pre>

<hr>
<h2 id='model_assisted'>Calculates a model assisted estimation</h2><span id='topic+model_assisted'></span>

<h3>Description</h3>

<p>It uses the model assisted estimator introduced by Särndal et al. (1992).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>model_assisted(
  sample_data,
  weights,
  full_data,
  covariates,
  estimated_var,
  estimate_mean = FALSE,
  positive_label = NULL,
  algorithm = "glm",
  proc = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="model_assisted_+3A_sample_data">sample_data</code></td>
<td>
<p>Data frame containing the sample.</p>
</td></tr>
<tr><td><code id="model_assisted_+3A_weights">weights</code></td>
<td>
<p>Vector containing the sample weights.</p>
</td></tr>
<tr><td><code id="model_assisted_+3A_full_data">full_data</code></td>
<td>
<p>Data frame containing all the individuals contained in the population.</p>
</td></tr>
<tr><td><code id="model_assisted_+3A_covariates">covariates</code></td>
<td>
<p>String vector specifying the common variables to use for training.</p>
</td></tr>
<tr><td><code id="model_assisted_+3A_estimated_var">estimated_var</code></td>
<td>
<p>String specifying the variable to estimate.</p>
</td></tr>
<tr><td><code id="model_assisted_+3A_estimate_mean">estimate_mean</code></td>
<td>
<p>Boolean specifying whether the mean estimation should be returned. Otherwise, the total estimation is returned by default.</p>
</td></tr>
<tr><td><code id="model_assisted_+3A_positive_label">positive_label</code></td>
<td>
<p>String specifying the label to be considered positive if the estimated variable is categorical. Leave it as the default NULL otherwise.</p>
</td></tr>
<tr><td><code id="model_assisted_+3A_algorithm">algorithm</code></td>
<td>
<p>A string specifying which classification or regression model to use (same as caret's method).</p>
</td></tr>
<tr><td><code id="model_assisted_+3A_proc">proc</code></td>
<td>
<p>A string or vector of strings specifying if any of the data preprocessing techniques available in <a href="caret.html#topic+train">train</a> function from 'caret' package should be applied to data prior to the propensity estimation. By default, its value is NULL and no preprocessing is applied.</p>
</td></tr>
<tr><td><code id="model_assisted_+3A_...">...</code></td>
<td>
<p>Further parameters to be passed to the <a href="caret.html#topic+train">train</a> function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Training of the models is done via the 'caret' package. The algorithm specified in <code>algorithm</code> must match one of the names in the list of algorithms supported by 'caret'.
</p>


<h3>Value</h3>

<p>The population total estimation (or mean if specified by the 'estimate_mean' parameter).
</p>


<h3>References</h3>

<p>Särndal, C. E., Swensson, B., &amp; Wretman, J. (1992). <em>Model assisted survey sampling.</em> Springer, New York.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#Simple example
covariates = c("education_primaria", "education_secundaria",
   "age", "sex", "language")
if (is.numeric(sampleNP$vote_gen))
   sampleNP$vote_gen = factor(sampleNP$vote_gen, c(0, 1), c('F', 'T'))
model_assisted(sampleNP, nrow(population) / nrow(sampleNP),
   population, covariates, "vote_gen", positive_label = 'T', algorithm = 'glmnet')
</code></pre>

<hr>
<h2 id='model_based'>Calculates a model based estimation</h2><span id='topic+model_based'></span>

<h3>Description</h3>

<p>It uses the model based estimator. The idea in order to estimate the population total is to add the sample responses and the predicted responses for the individuals not contained in the sample. See for example Valliant et al. (2000).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>model_based(
  sample_data,
  full_data,
  covariates,
  estimated_var,
  estimate_mean = FALSE,
  positive_label = NULL,
  algorithm = "glm",
  proc = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="model_based_+3A_sample_data">sample_data</code></td>
<td>
<p>Data frame containing the sample.</p>
</td></tr>
<tr><td><code id="model_based_+3A_full_data">full_data</code></td>
<td>
<p>Data frame containing all the individuals contained in the population.</p>
</td></tr>
<tr><td><code id="model_based_+3A_covariates">covariates</code></td>
<td>
<p>String vector specifying the common variables to use for training.</p>
</td></tr>
<tr><td><code id="model_based_+3A_estimated_var">estimated_var</code></td>
<td>
<p>String specifying the variable to estimate.</p>
</td></tr>
<tr><td><code id="model_based_+3A_estimate_mean">estimate_mean</code></td>
<td>
<p>Boolean specifying whether the mean estimation should be returned. Otherwise, the total estimation is returned by default.</p>
</td></tr>
<tr><td><code id="model_based_+3A_positive_label">positive_label</code></td>
<td>
<p>String specifying the label to be considered positive if the estimated variable is categorical. Leave it as the default NULL otherwise.</p>
</td></tr>
<tr><td><code id="model_based_+3A_algorithm">algorithm</code></td>
<td>
<p>A string specifying which classification or regression model to use (same as caret's method).</p>
</td></tr>
<tr><td><code id="model_based_+3A_proc">proc</code></td>
<td>
<p>A string or vector of strings specifying if any of the data preprocessing techniques available in <a href="caret.html#topic+train">train</a> function from 'caret' package should be applied to data prior to the propensity estimation. By default, its value is NULL and no preprocessing is applied.</p>
</td></tr>
<tr><td><code id="model_based_+3A_...">...</code></td>
<td>
<p>Further parameters to be passed to the <a href="caret.html#topic+train">train</a> function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Training of the models is done via the 'caret' package. The algorithm specified in <code>algorithm</code> must match one of the names in the list of algorithms supported by 'caret'.
</p>


<h3>Value</h3>

<p>The population total estimation (or mean if specified by the 'estimate_mean' parameter).
</p>


<h3>References</h3>

<p>Valliant, R., Dorfman, A. H., &amp; Royall, R. M. (2000) <em>Finite population sampling and inference: a prediction approach.</em> Wiley, New York.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#Simple example
covariates = c("education_primaria", "education_secundaria",
   "age", "sex", "language")
if (is.numeric(sampleNP$vote_gen))
   sampleNP$vote_gen = factor(sampleNP$vote_gen, c(0, 1), c('F', 'T'))
model_based(sampleNP, population, covariates,
   "vote_gen", positive_label = 'T', algorithm = 'glmnet')
</code></pre>

<hr>
<h2 id='model_calibrated'>Calculates a model calibrated estimation</h2><span id='topic+model_calibrated'></span>

<h3>Description</h3>

<p>It uses the model calibrated estimator introduced by Wu et al. (2001).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>model_calibrated(
  sample_data,
  weights,
  full_data,
  covariates,
  estimated_var,
  estimate_mean = FALSE,
  positive_label = NULL,
  algorithm = "glm",
  proc = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="model_calibrated_+3A_sample_data">sample_data</code></td>
<td>
<p>Data frame containing the sample.</p>
</td></tr>
<tr><td><code id="model_calibrated_+3A_weights">weights</code></td>
<td>
<p>Vector containing the sample weights.</p>
</td></tr>
<tr><td><code id="model_calibrated_+3A_full_data">full_data</code></td>
<td>
<p>Data frame containing all the individuals contained in the population.</p>
</td></tr>
<tr><td><code id="model_calibrated_+3A_covariates">covariates</code></td>
<td>
<p>String vector specifying the common variables to use for training.</p>
</td></tr>
<tr><td><code id="model_calibrated_+3A_estimated_var">estimated_var</code></td>
<td>
<p>String specifying the variable to estimate.</p>
</td></tr>
<tr><td><code id="model_calibrated_+3A_estimate_mean">estimate_mean</code></td>
<td>
<p>Boolean specifying whether the mean estimation should be returned. Otherwise, the total estimation is returned by default.</p>
</td></tr>
<tr><td><code id="model_calibrated_+3A_positive_label">positive_label</code></td>
<td>
<p>String specifying the label to be considered positive if the estimated variable is categorical. Leave it as the default NULL otherwise.</p>
</td></tr>
<tr><td><code id="model_calibrated_+3A_algorithm">algorithm</code></td>
<td>
<p>A string specifying which classification or regression model to use (same as caret's method).</p>
</td></tr>
<tr><td><code id="model_calibrated_+3A_proc">proc</code></td>
<td>
<p>A string or vector of strings specifying if any of the data preprocessing techniques available in <a href="caret.html#topic+train">train</a> function from 'caret' package should be applied to data prior to the propensity estimation. By default, its value is NULL and no preprocessing is applied.</p>
</td></tr>
<tr><td><code id="model_calibrated_+3A_...">...</code></td>
<td>
<p>Further parameters to be passed to the <a href="caret.html#topic+train">train</a> function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Training of the models is done via the 'caret' package. The algorithm specified in <code>algorithm</code> must match one of the names in the list of algorithms supported by 'caret'.
</p>


<h3>Value</h3>

<p>The population total estimation (or mean if specified by the 'estimate_mean' parameter).
</p>


<h3>References</h3>

<p>Wu, C., &amp; Sitter, R. R. (2001). <em>A model-calibration approach to using complete auxiliary information from survey data.</em> Journal of the American Statistical Association, 96(453), 185-193.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#Simple example
covariates = c("education_primaria", "education_secundaria",
   "age", "sex", "language")
if (is.numeric(sampleNP$vote_gen))
   sampleNP$vote_gen = factor(sampleNP$vote_gen, c(0, 1), c('F', 'T'))
model_calibrated(sampleNP, nrow(population) / nrow(sampleNP),
   population, covariates, "vote_gen", positive_label = 'T', algorithm = 'glmnet')
</code></pre>

<hr>
<h2 id='population'>A full population</h2><span id='topic+population'></span>

<h3>Description</h3>

<p>A dataset of a simulated fictitious population of 50,000 individuals. Further details on the generation of the dataset can be found in Ferri-García and Rueda (2018). The variables present in the dataset are the following:
</p>

<ul>
<li><p> education_primaria. A binary variable indicating if the highest academic level achieved by the individual is Primary Education.
</p>
</li>
<li><p> education_secundaria. A binary variable indicating if the highest academic level achieved by the individual is Secondary Education.
</p>
</li>
<li><p> education_terciaria. A binary variable indicating if the highest academic level achieved by the individual is Tertiary Education.
</p>
</li>
<li><p> age. A numeric variable, with values ranging from 18 to 100, indicating the age of the individual.
</p>
</li>
<li><p> sex. A binary variable indicating if the individual is a man.
</p>
</li>
<li><p> language. A binary variable indicating if the individual is a native.
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>population
</code></pre>


<h3>Format</h3>

<p>An object of class <code>data.frame</code> with 50000 rows and 6 columns.
</p>


<h3>References</h3>

<p>Ferri-García, R., &amp; Rueda, M. (2018). <em>Efficiency of propensity score adjustment and calibration on the estimation from non-probabilistic online surveys</em>. SORT-Statistics and Operations Research Transactions, 1(2), 159-162.
</p>

<hr>
<h2 id='prop_estimation'>Estimates the population proportion</h2><span id='topic+prop_estimation'></span>

<h3>Description</h3>

<p>Estimates the proportion of a given class or classes for the specified variables measured in a sample given some pre-calculated weights.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prop_estimation(sample, weights, estimated_vars, class, N = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="prop_estimation_+3A_sample">sample</code></td>
<td>
<p>A data frame containing the sample with the variables for which the means are to be calculated.</p>
</td></tr>
<tr><td><code id="prop_estimation_+3A_weights">weights</code></td>
<td>
<p>A vector of pre-calculated weights.</p>
</td></tr>
<tr><td><code id="prop_estimation_+3A_estimated_vars">estimated_vars</code></td>
<td>
<p>String vector specifying the variables in the sample to be estimated.</p>
</td></tr>
<tr><td><code id="prop_estimation_+3A_class">class</code></td>
<td>
<p>String vector specifying which class (value) proportion is to be estimated in each variable. The <em>i</em>-th element of this vector corresponds to the class of which proportion is desired to estimate of the <em>i</em>-th variable of the vector specified in <code>estimated_vars</code>.</p>
</td></tr>
<tr><td><code id="prop_estimation_+3A_n">N</code></td>
<td>
<p>An integer specifying the population size (optional).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector with the corresponding estimations.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>covariates = c("education_primaria", "education_secundaria")
data_propensities = propensities(sampleNP, sampleP, covariates)
psa_weights = sc_weights(data_propensities$convenience)

#The function will estimate the proportion of individuals
#with the 0 value in vote_pens and the 1 value in vote_pir
prop_estimation(sampleNP, psa_weights, c("vote_pens", "vote_pir"), c(0, 1))
</code></pre>

<hr>
<h2 id='propensities'>Calculates sample propensities</h2><span id='topic+propensities'></span>

<h3>Description</h3>

<p>Given a convenience sample and a reference sample, computes estimates on the propensity to participate in the convenience sample based on classification models to be selected by the user.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>propensities(
  convenience_sample,
  reference_sample,
  covariates,
  algorithm = "glm",
  smooth = FALSE,
  proc = NULL,
  trControl = trainControl(classProbs = TRUE),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="propensities_+3A_convenience_sample">convenience_sample</code></td>
<td>
<p>Data frame containing the non-probabilistic sample.</p>
</td></tr>
<tr><td><code id="propensities_+3A_reference_sample">reference_sample</code></td>
<td>
<p>Data frame containing the probabilistic sample.</p>
</td></tr>
<tr><td><code id="propensities_+3A_covariates">covariates</code></td>
<td>
<p>String vector specifying the common variables to use for training.</p>
</td></tr>
<tr><td><code id="propensities_+3A_algorithm">algorithm</code></td>
<td>
<p>A string specifying which classification or regression model to use (same as caret's method).</p>
</td></tr>
<tr><td><code id="propensities_+3A_smooth">smooth</code></td>
<td>
<p>A logical value; if TRUE, propensity estimates pi_i are smoothed applying the formula (1000*pi_i + 0.5)/1001</p>
</td></tr>
<tr><td><code id="propensities_+3A_proc">proc</code></td>
<td>
<p>A string or vector of strings specifying if any of the data preprocessing techniques available in <a href="caret.html#topic+train">train</a> function from 'caret' package should be applied to data prior to the propensity estimation. By default, its value is NULL and no preprocessing is applied.</p>
</td></tr>
<tr><td><code id="propensities_+3A_trcontrol">trControl</code></td>
<td>
<p>A trainControl specifying the computational nuances of the <a href="caret.html#topic+train">train</a> function.</p>
</td></tr>
<tr><td><code id="propensities_+3A_...">...</code></td>
<td>
<p>Further parameters to be passed to the <a href="caret.html#topic+train">train</a> function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Training of the propensity estimation models is done via the 'caret' package. The algorithm specified in <code>algorithm</code> must match one of the names in the list of algorithms supported by 'caret'. Case weights are used to balance classes (for models that accept them).
The smoothing formula for propensities avoids mathematical irregularities in the calculation of sample weight when an estimated propensity is 0 or 1. Further details can be found in Buskirk and Kolenikov (2015).
</p>


<h3>Value</h3>

<p>A list containing 'convenience' propensities and 'reference' propensities.
</p>


<h3>References</h3>

<p>Buskirk, T. D., &amp; Kolenikov, S. (2015). <em>Finding respondents in the forest: A comparison of logistic regression and random forest models for response propensity weighting and stratification.</em> Survey Methods: Insights from the Field, 17.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#Simple example with default parameters
covariates = c("education_primaria", "education_secundaria")
propensities(sampleNP, sampleP, covariates)
</code></pre>

<hr>
<h2 id='sampleNP'>A non-probabilistic sample</h2><span id='topic+sampleNP'></span>

<h3>Description</h3>

<p>A dataset of 1000 individuals extracted from the subpopulation of individuals with internet access in a simulated fictitious population of 50,000 individuals. This sample attempts to reproduce a case of nonprobability sampling with selection bias, as there are important differences between the potentially covered population, the covered population and the full target population. Further details on the generation of the dataset can be found in Ferri-García and Rueda (2018). The variables present in the dataset are the following:
</p>

<ul>
<li><p> vote_gen. A binary variable indicating if the individual vote preferences are for Party 1. This variable is related to gender.
</p>
</li>
<li><p> vote_pens. A binary variable indicating if the individual vote preferences are for Party 2. This variable is related to age.
</p>
</li>
<li><p> vote_pir. A binary variable indicating if the individual vote preferences are for Party 3. This variable is related to age and internet access.
</p>
</li>
<li><p> education_primaria. A binary variable indicating if the highest academic level achieved by the individual is Primary Education.
</p>
</li>
<li><p> education_secundaria. A binary variable indicating if the highest academic level achieved by the individual is Secondary Education.
</p>
</li>
<li><p> education_terciaria. A binary variable indicating if the highest academic level achieved by the individual is Tertiary Education.
</p>
</li>
<li><p> age. A numeric variable, with values ranging from 18 to 100, indicating the age of the individual.
</p>
</li>
<li><p> sex. A binary variable indicating if the individual is a man.
</p>
</li>
<li><p> language. A binary variable indicating if the individual is a native.
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>sampleNP
</code></pre>


<h3>Format</h3>

<p>An object of class <code>data.frame</code> with 1000 rows and 9 columns.
</p>


<h3>References</h3>

<p>Ferri-García, R., &amp; Rueda, M. (2018). <em>Efficiency of propensity score adjustment and calibration on the estimation from non-probabilistic online surveys</em>. SORT-Statistics and Operations Research Transactions, 1(2), 159-162.
</p>

<hr>
<h2 id='sampleP'>A probabilistic sample</h2><span id='topic+sampleP'></span>

<h3>Description</h3>

<p>A dataset of 500 individuals extracted with simple random sampling from a simulated fictitious population of 50,000 individuals. Further details on the generation of the dataset can be found in Ferri-García and Rueda (2018). The variables present in the dataset are the following:
</p>

<ul>
<li><p> education_primaria. A binary variable indicating if the highest academic level achieved by the individual is Primary Education.
</p>
</li>
<li><p> education_secundaria. A binary variable indicating if the highest academic level achieved by the individual is Secondary Education.
</p>
</li>
<li><p> education_terciaria. A binary variable indicating if the highest academic level achieved by the individual is Tertiary Education.
</p>
</li>
<li><p> age. A numeric variable, with values ranging from 18 to 100, indicating the age of the individual.
</p>
</li>
<li><p> sex. A binary variable indicating if the individual is a man.
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>sampleP
</code></pre>


<h3>Format</h3>

<p>An object of class <code>data.frame</code> with 500 rows and 5 columns.
</p>


<h3>References</h3>

<p>Ferri-García, R., &amp; Rueda, M. (2018). <em>Efficiency of propensity score adjustment and calibration on the estimation from non-probabilistic online surveys</em>. SORT-Statistics and Operations Research Transactions, 1(2), 159-162.
</p>

<hr>
<h2 id='sc_weights'>Calculates Schonlau and Couper weights</h2><span id='topic+sc_weights'></span>

<h3>Description</h3>

<p>Computes weights from propensity estimates using the (1 - pi_i)/pi_i formula introduced in Schonlau and Couper (2017).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sc_weights(propensities)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sc_weights_+3A_propensities">propensities</code></td>
<td>
<p>A vector with the propensities associated to the elements of the convenience sample.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function takes the vector of propensities <code class="reqn">\pi(x)</code> and calculates the weights to be applied in the Hajek estimator using the formula that can be found in Schonlau and Couper (2017). For an individual <em>i</em>, weight is calculated as follows:
</p>
<p style="text-align: center;"><code class="reqn">w_i = \frac{1 - \pi_i (x)}{\pi_i (x)}</code>
</p>



<h3>Value</h3>

<p>A vector with the corresponding weights.
</p>


<h3>References</h3>

<p>Schonlau, M., &amp; Couper, M. P. (2017). <em>Options for conducting web surveys.</em> Statistical Science, 32(2), 279-292.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>covariates = c("education_primaria", "education_secundaria")
data_propensities = propensities(sampleNP, sampleP, covariates)
sc_weights(data_propensities$convenience)
</code></pre>

<hr>
<h2 id='total_estimation'>Estimates the population totals</h2><span id='topic+total_estimation'></span>

<h3>Description</h3>

<p>Estimates the population totals for the specified variables measured in a sample given some pre-calculated weights.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>total_estimation(sample, weights, estimated_vars, N)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="total_estimation_+3A_sample">sample</code></td>
<td>
<p>A data frame containing the sample with the variables for which the estimated population totals are to be calculated.</p>
</td></tr>
<tr><td><code id="total_estimation_+3A_weights">weights</code></td>
<td>
<p>A vector of pre-calculated weights.</p>
</td></tr>
<tr><td><code id="total_estimation_+3A_estimated_vars">estimated_vars</code></td>
<td>
<p>String vector specifying the variables in the sample to be estimated.</p>
</td></tr>
<tr><td><code id="total_estimation_+3A_n">N</code></td>
<td>
<p>An integer specifying the population size.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector with the corresponding estimations.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>covariates = c("education_primaria", "education_secundaria")
data_propensities = propensities(sampleNP, sampleP, covariates)
psa_weights = sc_weights(data_propensities$convenience)
total_estimation(sampleNP, psa_weights, c("vote_pens"), 50000)
</code></pre>

<hr>
<h2 id='valliant_weights'>Calculates Valliant weights</h2><span id='topic+valliant_weights'></span>

<h3>Description</h3>

<p>Computes weights from propensity estimates using the 1/pi_i formula introduced in Valliant (2019).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>valliant_weights(propensities)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="valliant_weights_+3A_propensities">propensities</code></td>
<td>
<p>A vector with the propensities associated to the elements of the convenience sample.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function takes the vector of propensities <code class="reqn">\pi(x)</code> and calculates the weights to be applied in the Hajek estimator using the formula that can be found in Valliant (2019). For an individual <em>i</em>, weight is calculated as follows:
</p>
<p style="text-align: center;"><code class="reqn">w_i = 1/\pi_i (x)</code>
</p>



<h3>Value</h3>

<p>A vector with the corresponding weights.
</p>


<h3>References</h3>

<p>Valliant, R. (2019). <em>Comparing Alternatives for Estimation from Nonprobability Samples</em>. Journal of Survey Statistics and Methodology, smz003, <a href="https://doi.org/10.1093/jssam/smz003">https://doi.org/10.1093/jssam/smz003</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>covariates = c("education_primaria", "education_secundaria")
data_propensities = propensities(sampleNP, sampleP, covariates)
valliant_weights(data_propensities$convenience)
</code></pre>

<hr>
<h2 id='vd_weights'>Calculates Valliant and Dever weights</h2><span id='topic+vd_weights'></span>

<h3>Description</h3>

<p>Computes weights from propensity estimates using the propensity stratification 1/p_i averaging formula introduced in Valliant and Dever (2011).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vd_weights(convenience_propensities, reference_propensities, g = 5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vd_weights_+3A_convenience_propensities">convenience_propensities</code></td>
<td>
<p>A vector with the propensities associated with the convenience sample.</p>
</td></tr>
<tr><td><code id="vd_weights_+3A_reference_propensities">reference_propensities</code></td>
<td>
<p>A vector with the propensities associated with the reference sample.</p>
</td></tr>
<tr><td><code id="vd_weights_+3A_g">g</code></td>
<td>
<p>The number of strata to use; by default, its value is 5.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function takes the vector of propensities <code class="reqn">\pi(x)</code> and calculates the weights to be applied in the Horvitz-Thompson estimator using the formula that can be found in Valliant and Dever (2019). The vector of propensities is divided in <em>g</em> strata (ideally five according to Cochran, 1968) aiming to have individuals with similar propensities in each strata. After the stratification, weight is calculated as follows for an individual <em>i</em>:
</p>
<p style="text-align: center;"><code class="reqn">w_i = \frac{n(g_i)}{ \sum_{k \in g_i} \pi_k (x)}</code>
</p>

<p>where <code class="reqn">g_i</code> represents the strata to which <em>i</em> belongs, and <code class="reqn">n(g_i)</code> is the number of individuals in the <code class="reqn">g_i</code> strata.
</p>


<h3>Value</h3>

<p>A vector with the corresponding weights.
</p>


<h3>References</h3>

<p>Valliant, R., &amp; Dever, J. A. (2011). <em>Estimating propensity adjustments for volunteer web surveys.</em> Sociological Methods &amp; Research, 40(1), 105-137.
</p>
<p>Cochran, W. G. (1968). <em>The Effectiveness of Adjustment by Subclassification in Removing Bias in Observational Studies.</em> Biometrics, 24(2), 295-313
</p>


<h3>Examples</h3>

<pre><code class='language-R'>covariates = c("education_primaria", "education_secundaria")
data_propensities = propensities(sampleNP, sampleP, covariates)
vd_weights(data_propensities$convenience, data_propensities$reference)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
