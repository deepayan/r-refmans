<!DOCTYPE html><html><head><title>Help for package archiveRetriever</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {archiveRetriever}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#archive_overview'><p>archive_overview: Getting a first glimpse of mementos available in the Internet Archive</p></a></li>
<li><a href='#retrieve_links'><p>retrieve_links: Retrieving Links of Lower-level web pages of mementos from the Internet Archive</p></a></li>
<li><a href='#retrieve_urls'><p>retrieve_urls: Retrieving Urls from the Internet Archive</p></a></li>
<li><a href='#scrape_urls'><p>scrape_urls: Scraping Urls from the Internet Archive</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Retrieve Archived Web Pages from the 'Internet Archive'</td>
</tr>
<tr>
<td>Version:</td>
<td>0.4.0</td>
</tr>
<tr>
<td>Description:</td>
<td>Scraping content from archived web pages stored in
    the 'Internet Archive' (<a href="https://archive.org">https://archive.org</a>) using a systematic
    workflow.  Get an overview of the mementos available from the
    respective homepage, retrieve the Urls and links of the page and
    finally scrape the content. The final output is stored in tibbles,
    which can be then easily used for further analysis.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.apache.org/licenses/LICENSE-2.0">Apache License (&ge; 2.0)</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/liserman/archiveRetriever/">https://github.com/liserman/archiveRetriever/</a></td>
</tr>
<tr>
<td>Imports:</td>
<td>anytime, dplyr, ggplot2, gridExtra, httr, jsonlite, lubridate,
rvest, stringr, tibble, tidyr, utils, xml2</td>
</tr>
<tr>
<td>Suggests:</td>
<td>vcr (&ge; 1.0.0), testthat, webmockr</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.1</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-06-11 09:28:22 UTC; liserman</td>
</tr>
<tr>
<td>Author:</td>
<td>Lukas Isermann <a href="https://orcid.org/0000-0002-7195-9302"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre],
  Konstantin Gavras <a href="https://orcid.org/0000-0002-9222-0101"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Lukas Isermann &lt;lukas.isermann@uni-mannheim.de&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-06-11 09:40:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='archive_overview'>archive_overview: Getting a first glimpse of mementos available in the Internet Archive</h2><span id='topic+archive_overview'></span>

<h3>Description</h3>

<p><code>archive_overview</code> provides an overview of available mementos of the homepage from the Internet Archive
</p>


<h3>Usage</h3>

<pre><code class='language-R'>archive_overview(homepage, startDate, endDate)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="archive_overview_+3A_homepage">homepage</code></td>
<td>
<p>A character vector of the homepage, including the top-level-domain</p>
</td></tr>
<tr><td><code id="archive_overview_+3A_startdate">startDate</code></td>
<td>
<p>A character vector of the starting date of the overview. Accepts a large variety of date formats (see <a href="anytime.html#topic+anytime">anytime</a>)</p>
</td></tr>
<tr><td><code id="archive_overview_+3A_enddate">endDate</code></td>
<td>
<p>A character vector of the ending date of the overview. Accepts a large variety of date formats (see <a href="anytime.html#topic+anytime">anytime</a>)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>This function provides an overview of mementos available from the Internet Archive. It returns a calendar indicating all dates in which mementos of the homepage have been stored in the Internet Archive at least once. However, a memento being stored in the Internet Archive does not guarantee that the information from the homepage can be actually scraped. As the Internet Archive is an internet resource, it is always possible that a request fails due to connectivity problems. One easy and obvious solution is to re-try the function.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
archive_overview(homepage = "www.spiegel.de", startDate = "20180601", endDate = "20190615")
archive_overview(homepage = "nytimes.com", startDate = "2018-06-01", endDate = "2019-05-01")

## End(Not run)
</code></pre>

<hr>
<h2 id='retrieve_links'>retrieve_links: Retrieving Links of Lower-level web pages of mementos from the Internet Archive</h2><span id='topic+retrieve_links'></span>

<h3>Description</h3>

<p><code>retrieve_links</code> retrieves the Urls of mementos stored in the Internet Archive
</p>


<h3>Usage</h3>

<pre><code class='language-R'>retrieve_links(
  ArchiveUrls,
  encoding = "UTF-8",
  ignoreErrors = FALSE,
  filter = TRUE,
  pattern = NULL,
  nonArchive = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="retrieve_links_+3A_archiveurls">ArchiveUrls</code></td>
<td>
<p>A string of the memento of the Internet Archive</p>
</td></tr>
<tr><td><code id="retrieve_links_+3A_encoding">encoding</code></td>
<td>
<p>Specify a encoding for the homepage. Default is 'UTF-8'</p>
</td></tr>
<tr><td><code id="retrieve_links_+3A_ignoreerrors">ignoreErrors</code></td>
<td>
<p>Ignore errors for some Urls and proceed scraping</p>
</td></tr>
<tr><td><code id="retrieve_links_+3A_filter">filter</code></td>
<td>
<p>Filter links by top-level domain. Only sub-domains of top-level domain will be returned. Default is TRUE.</p>
</td></tr>
<tr><td><code id="retrieve_links_+3A_pattern">pattern</code></td>
<td>
<p>Filter links by custom pattern instead of top-level domains. Default is NULL.</p>
</td></tr>
<tr><td><code id="retrieve_links_+3A_nonarchive">nonArchive</code></td>
<td>
<p>Logical input. Can be set to TRUE if you want to use the archiveRetriever to scrape web pages outside the Internet Archive.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>This function retrieves the links of all lower-level web pages of mementos of a homepage available from the Internet Archive. It returns a tibble including the baseUrl and all links of lower-level web pages. However, a memento being stored in the Internet Archive does not guarantee that the information from the homepage can be actually scraped. As the Internet Archive is an internet resource, it is always possible that a request fails due to connectivity problems. One easy and obvious solution is to re-try the function.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
retrieve_links("http://web.archive.org/web/20190801001228/https://www.spiegel.de/")

## End(Not run)
</code></pre>

<hr>
<h2 id='retrieve_urls'>retrieve_urls: Retrieving Urls from the Internet Archive</h2><span id='topic+retrieve_urls'></span>

<h3>Description</h3>

<p><code>retrieve_urls</code> retrieves the Urls of mementos stored in the Internet Archive
</p>


<h3>Usage</h3>

<pre><code class='language-R'>retrieve_urls(homepage, startDate, endDate, collapseDate = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="retrieve_urls_+3A_homepage">homepage</code></td>
<td>
<p>A character vector of the homepage, including the top-level-domain</p>
</td></tr>
<tr><td><code id="retrieve_urls_+3A_startdate">startDate</code></td>
<td>
<p>A character vector of the starting date of the overview. Accepts a large variety of date formats (see <a href="anytime.html#topic+anytime">anytime</a>)</p>
</td></tr>
<tr><td><code id="retrieve_urls_+3A_enddate">endDate</code></td>
<td>
<p>A character vector of the ending date of the overview. Accepts a large variety of date formats (see <a href="anytime.html#topic+anytime">anytime</a>)</p>
</td></tr>
<tr><td><code id="retrieve_urls_+3A_collapsedate">collapseDate</code></td>
<td>
<p>A logical value indicating whether the output should be limited to one memento per day</p>
</td></tr>
</table>


<h3>Value</h3>

<p>This function retrieves the mementos of a homepage available from the Internet Archive. It returns a vector of strings of all mementos stored in the Internet Archive in the respective time frame. The mementos only refer to the homepage being retrieved and not its lower level web pages. However, a memento being stored in the Internet Archive does not guarantee that the information from the homepage can be actually scraped.  As the Internet Archive is an internet resource, it is always possible that a request fails due to connectivity problems. One easy and obvious solution is to re-try the function.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
retrieve_urls("www.spiegel.de", "20190801", "20190901")
retrieve_urls("nytimes.com", startDate = "2018-01-01", endDate = "01/02/2018")
retrieve_urls("nytimes.com", startDate = "2018-01-01", endDate = "2018-01-02", collapseDate = FALSE)

## End(Not run)
</code></pre>

<hr>
<h2 id='scrape_urls'>scrape_urls: Scraping Urls from the Internet Archive</h2><span id='topic+scrape_urls'></span>

<h3>Description</h3>

<p><code>scrape_urls</code> scrapes Urls of mementos and lower-level web pages stored in the Internet Archive using XPaths as default
</p>


<h3>Usage</h3>

<pre><code class='language-R'>scrape_urls(
  Urls,
  Paths,
  collapse = TRUE,
  startnum = 1,
  attachto = NULL,
  CSS = FALSE,
  archiveDate = FALSE,
  ignoreErrors = FALSE,
  stopatempty = TRUE,
  emptylim = 10,
  encoding = "UTF-8",
  lengthwarning = TRUE,
  nonArchive = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="scrape_urls_+3A_urls">Urls</code></td>
<td>
<p>A character vector of the memento of the Internet Archive</p>
</td></tr>
<tr><td><code id="scrape_urls_+3A_paths">Paths</code></td>
<td>
<p>A named character vector of the content to be scraped from the memento. Takes XPath expressions as default.</p>
</td></tr>
<tr><td><code id="scrape_urls_+3A_collapse">collapse</code></td>
<td>
<p>Logical value indicating whether to collapse matching html nodes, or character input of xpath by which matches are supposed to be collapsed. Structuring Xpaths can only be used with Xpath selectors as Paths input and CSS = FALSE. If a Xpath is given, the Paths argument only refers to children of the structure given in collapse.</p>
</td></tr>
<tr><td><code id="scrape_urls_+3A_startnum">startnum</code></td>
<td>
<p>Specify the starting number for scraping the Urls. Important when scraping breaks during process.</p>
</td></tr>
<tr><td><code id="scrape_urls_+3A_attachto">attachto</code></td>
<td>
<p>Scraper attaches new content to existing object in working memory. Object should stem from same scraping process.</p>
</td></tr>
<tr><td><code id="scrape_urls_+3A_css">CSS</code></td>
<td>
<p>Use CSS selectors as input for the Paths</p>
</td></tr>
<tr><td><code id="scrape_urls_+3A_archivedate">archiveDate</code></td>
<td>
<p>Retrieve the archiving date</p>
</td></tr>
<tr><td><code id="scrape_urls_+3A_ignoreerrors">ignoreErrors</code></td>
<td>
<p>Ignore errors for some Urls and proceed scraping</p>
</td></tr>
<tr><td><code id="scrape_urls_+3A_stopatempty">stopatempty</code></td>
<td>
<p>Stop if scraping does not succeed</p>
</td></tr>
<tr><td><code id="scrape_urls_+3A_emptylim">emptylim</code></td>
<td>
<p>Specify the number of Urls not being scraped until break-off</p>
</td></tr>
<tr><td><code id="scrape_urls_+3A_encoding">encoding</code></td>
<td>
<p>Specify a default encoding for the homepage. Default is 'UTF-8'</p>
</td></tr>
<tr><td><code id="scrape_urls_+3A_lengthwarning">lengthwarning</code></td>
<td>
<p>Warning function for large number of URLs appears. Set FALSE to disable default warning.</p>
</td></tr>
<tr><td><code id="scrape_urls_+3A_nonarchive">nonArchive</code></td>
<td>
<p>Logical input. Can be set to TRUE if you want to use the archiveRetriever to scrape web pages outside the Internet Archive. Cannot be used in combination with archiveDate.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>This function scrapes the content of mementos or lower-level web pages from the Internet Archive. It returns a tibble including Urls and the scraped content. However, a memento being stored in the Internet Archive does not guarantee that the information from the homepage can be actually scraped. As the Internet Archive is an internet resource, it is always possible that a request fails due to connectivity problems. One easy and obvious solution is to re-try the function.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
scrape_urls(
Urls = "https://web.archive.org/web/20201001000859/https://www.nytimes.com/section/politics",
Paths = c(title = "//article/div/h2//text()", teaser = "//article/div/p/text()"),
collapse = FALSE, archiveDate = TRUE)

scrape_urls(
 Urls = "https://stackoverflow.com/questions/21167159/css-nth-match-doesnt-work",
 Paths = c(ans="//div[@itemprop='text']/*", aut="//div[@itemprop='author']/span[@itemprop='name']"),
 collapse = "//div[@id='answers']/div[contains(@class, 'answer')]",
 nonArchive = TRUE,
 encoding = "bytes")

## End(Not run)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
