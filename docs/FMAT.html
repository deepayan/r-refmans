<!DOCTYPE html><html><head><title>Help for package FMAT</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {FMAT}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#.'><p>A simple function equivalent to <code>list</code>.</p></a></li>
<li><a href='#BERT_download'><p>Download and save BERT models to local cache folder.</p></a></li>
<li><a href='#BERT_info'><p>Get basic information of BERT models.</p></a></li>
<li><a href='#BERT_vocab'><p>Check if mask words are in the model vocabulary.</p></a></li>
<li><a href='#FMAT_load'><p>[Deprecated] Load BERT models (useless for GPU).</p></a></li>
<li><a href='#FMAT_query'><p>Prepare a data.table of queries and variables for the FMAT.</p></a></li>
<li><a href='#FMAT_query_bind'><p>Combine multiple query data.tables and renumber query ids.</p></a></li>
<li><a href='#FMAT_run'><p>Run the fill-mask pipeline on multiple models (CPU / GPU).</p></a></li>
<li><a href='#ICC_models'><p>Intraclass correlation coefficient (ICC) of BERT models.</p></a></li>
<li><a href='#LPR_reliability'><p>Reliability analysis (Cronbach's <code class="reqn">\alpha</code>) of LPR.</p></a></li>
<li><a href='#summary.fmat'><p>[S3 method] Summarize the results for the FMAT.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>The Fill-Mask Association Test</td>
</tr>
<tr>
<td>Version:</td>
<td>2024.6</td>
</tr>
<tr>
<td>Date:</td>
<td>2024-06-12</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Han-Wu-Shuang Bao &lt;baohws@foxmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>
    The Fill-Mask Association Test ('FMAT')
    is an integrative and probability-based method using
    Masked Language Models to measure conceptual associations
    (e.g., attitudes, biases, stereotypes, social norms, cultural values)
    as propositions in natural language.
    Supported language models include 'BERT'
    &lt;<a href="https://doi.org/10.48550%2FarXiv.1810.04805">doi:10.48550/arXiv.1810.04805</a>&gt; and its variants available at 'Hugging Face'
    <a href="https://huggingface.co/models?pipeline_tag=fill-mask">https://huggingface.co/models?pipeline_tag=fill-mask</a>.
    Methodological references and installation guidance are provided at
    <a href="https://psychbruce.github.io/FMAT/">https://psychbruce.github.io/FMAT/</a>.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://psychbruce.github.io/FMAT/">https://psychbruce.github.io/FMAT/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/psychbruce/FMAT/issues">https://github.com/psychbruce/FMAT/issues</a></td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>Python (&gt;= 3.9.0)</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.0.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>reticulate, data.table, stringr, forcats, psych, irr, glue,
crayon, cli, purrr, plyr, dplyr, tidyr</td>
</tr>
<tr>
<td>Suggests:</td>
<td>bruceR, PsychWordVec, text, sweater, nlme</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.1</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-06-12 14:37:57 UTC; Bruce</td>
</tr>
<tr>
<td>Author:</td>
<td>Han-Wu-Shuang Bao <a href="https://orcid.org/0000-0003-3043-710X"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-06-12 14:50:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='.'>A simple function equivalent to <code>list</code>.</h2><span id='topic+.'></span>

<h3>Description</h3>

<p>A simple function equivalent to <code>list</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.(...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="._+3A_...">...</code></td>
<td>
<p>Named objects (usually character vectors for this package).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of named objects.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>.(Male=c("he", "his"), Female=c("she", "her"))

</code></pre>

<hr>
<h2 id='BERT_download'>Download and save BERT models to local cache folder.</h2><span id='topic+BERT_download'></span>

<h3>Description</h3>

<p>Download and save BERT models to local cache folder &quot;%USERPROFILE%/.cache/huggingface&quot;.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>BERT_download(models = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="BERT_download_+3A_models">models</code></td>
<td>
<p>Model names at
<a href="https://huggingface.co/models?pipeline_tag=fill-mask&amp;library=transformers">HuggingFace</a>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>No return value.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+BERT_info">BERT_info</a></code>
</p>
<p><code><a href="#topic+BERT_vocab">BERT_vocab</a></code>
</p>
<p><code><a href="#topic+FMAT_load">FMAT_load</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
models = c("bert-base-uncased", "bert-base-cased")
BERT_download(models)

BERT_download()  # check downloaded models

BERT_info()  # information of all downloaded models

## End(Not run)

</code></pre>

<hr>
<h2 id='BERT_info'>Get basic information of BERT models.</h2><span id='topic+BERT_info'></span>

<h3>Description</h3>

<p>Get basic information of BERT models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>BERT_info(models = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="BERT_info_+3A_models">models</code></td>
<td>
<p>Model names at
<a href="https://huggingface.co/models?pipeline_tag=fill-mask&amp;library=transformers">HuggingFace</a>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data.table of model name, model file size,
vocabulary size (of word/token embeddings),
embedding dimensions (of word/token embeddings),
and [MASK] token.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+BERT_download">BERT_download</a></code>
</p>
<p><code><a href="#topic+BERT_vocab">BERT_vocab</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
models = c("bert-base-uncased", "bert-base-cased")
BERT_info(models)

BERT_info()  # information of all downloaded models

## End(Not run)

</code></pre>

<hr>
<h2 id='BERT_vocab'>Check if mask words are in the model vocabulary.</h2><span id='topic+BERT_vocab'></span>

<h3>Description</h3>

<p>Check if mask words are in the model vocabulary.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>BERT_vocab(
  models,
  mask.words,
  add.tokens = FALSE,
  add.method = c("sum", "mean")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="BERT_vocab_+3A_models">models</code></td>
<td>
<p>Model names at
<a href="https://huggingface.co/models?pipeline_tag=fill-mask&amp;library=transformers">HuggingFace</a>.</p>
</td></tr>
<tr><td><code id="BERT_vocab_+3A_mask.words">mask.words</code></td>
<td>
<p>Option words filling in the mask.</p>
</td></tr>
<tr><td><code id="BERT_vocab_+3A_add.tokens">add.tokens</code></td>
<td>
<p>Add new tokens (for out-of-vocabulary words or even phrases) to model vocabulary?
Defaults to <code>FALSE</code>. It only temporarily adds tokens for tasks but does not change the raw model file.</p>
</td></tr>
<tr><td><code id="BERT_vocab_+3A_add.method">add.method</code></td>
<td>
<p>Method used to produce the token embeddings of new added tokens.
Can be <code>"sum"</code> (default) or <code>"mean"</code> of subword token embeddings.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data.table of model name, mask word, real token (replaced if out of vocabulary),
and token id (0~N).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+BERT_download">BERT_download</a></code>
</p>
<p><code><a href="#topic+BERT_info">BERT_info</a></code>
</p>
<p><code><a href="#topic+FMAT_run">FMAT_run</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
models = c("bert-base-uncased", "bert-base-cased")
BERT_info(models)

BERT_vocab(models, c("bruce", "Bruce"))

BERT_vocab(models, 2020:2025)  # some are out-of-vocabulary
BERT_vocab(models, 2020:2025, add.tokens=TRUE)  # add vocab

BERT_vocab(models,
           c("individualism", "artificial intelligence"),
           add.tokens=TRUE)

## End(Not run)

</code></pre>

<hr>
<h2 id='FMAT_load'>[Deprecated] Load BERT models (useless for GPU).</h2><span id='topic+FMAT_load'></span>

<h3>Description</h3>

<p>Load BERT models from local cache folder &quot;%USERPROFILE%/.cache/huggingface&quot;.
For <a href="https://psychbruce.github.io/FMAT/#guidance-for-gpu-acceleration">GPU Acceleration</a>,
please directly use <code><a href="#topic+FMAT_run">FMAT_run</a></code>.
In general, <code><a href="#topic+FMAT_run">FMAT_run</a></code> is always preferred than <code><a href="#topic+FMAT_load">FMAT_load</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>FMAT_load(models)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="FMAT_load_+3A_models">models</code></td>
<td>
<p>Model names at
<a href="https://huggingface.co/models?pipeline_tag=fill-mask&amp;library=transformers">HuggingFace</a>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A named list of fill-mask pipelines obtained from the models.
The returned object <em>cannot</em> be saved as any RData.
You will need to <em>rerun</em> this function if you <em>restart</em> the R session.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+BERT_download">BERT_download</a></code>
</p>
<p><code><a href="#topic+FMAT_query">FMAT_query</a></code>
</p>
<p><code><a href="#topic+FMAT_query_bind">FMAT_query_bind</a></code>
</p>
<p><code><a href="#topic+FMAT_run">FMAT_run</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
models = c("bert-base-uncased", "bert-base-cased")
models = FMAT_load(models)  # load models from cache

## End(Not run)

</code></pre>

<hr>
<h2 id='FMAT_query'>Prepare a data.table of queries and variables for the FMAT.</h2><span id='topic+FMAT_query'></span>

<h3>Description</h3>

<p>Prepare a data.table of queries and variables for the FMAT.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>FMAT_query(
  query = "Text with [MASK], optionally with {TARGET} and/or {ATTRIB}.",
  MASK = .(),
  TARGET = .(),
  ATTRIB = .()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="FMAT_query_+3A_query">query</code></td>
<td>
<p>Query text (should be a character string/vector
with at least one <code style="white-space: pre;">&#8288;[MASK]&#8288;</code> token).
Multiple queries share the same set of
<code>MASK</code>, <code>TARGET</code>, and <code>ATTRIB</code>.
For multiple queries with different
<code>MASK</code>, <code>TARGET</code>, and/or <code>ATTRIB</code>,
please use <code><a href="#topic+FMAT_query_bind">FMAT_query_bind</a></code> to combine them.</p>
</td></tr>
<tr><td><code id="FMAT_query_+3A_mask">MASK</code></td>
<td>
<p>A named list of <code style="white-space: pre;">&#8288;[MASK]&#8288;</code> target words.
Must be single words in the vocabulary of a certain masked language model.
</p>
<p>For model vocabulary, see, e.g.,
<a href="https://huggingface.co/bert-base-uncased/raw/main/vocab.txt">https://huggingface.co/bert-base-uncased/raw/main/vocab.txt</a>
</p>
<p>Infrequent words may be not included in a model's vocabulary,
and in this case you may insert the words into the context by
specifying either <code>TARGET</code> or <code>ATTRIB</code>.</p>
</td></tr>
<tr><td><code id="FMAT_query_+3A_target">TARGET</code>, <code id="FMAT_query_+3A_attrib">ATTRIB</code></td>
<td>
<p>A named list of Target/Attribute words or phrases.
If specified, then <code>query</code> must contain
<code>{TARGET}</code> and/or <code>{ATTRIB}</code> (in all uppercase and in braces)
to be replaced by the words/phrases.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data.table of queries and variables.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+FMAT_load">FMAT_load</a></code>
</p>
<p><code><a href="#topic+FMAT_query_bind">FMAT_query_bind</a></code>
</p>
<p><code><a href="#topic+FMAT_run">FMAT_run</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>FMAT_query("[MASK] is a nurse.", MASK = .(Male="He", Female="She"))

FMAT_query(
  c("[MASK] is {TARGET}.", "[MASK] works as {TARGET}."),
  MASK = .(Male="He", Female="She"),
  TARGET = .(Occupation=c("a doctor", "a nurse", "an artist"))
)

FMAT_query(
  "The [MASK] {ATTRIB}.",
  MASK = .(Male=c("man", "boy"),
           Female=c("woman", "girl")),
  ATTRIB = .(Masc=c("is masculine", "has a masculine personality"),
             Femi=c("is feminine", "has a feminine personality"))
)

</code></pre>

<hr>
<h2 id='FMAT_query_bind'>Combine multiple query data.tables and renumber query ids.</h2><span id='topic+FMAT_query_bind'></span>

<h3>Description</h3>

<p>Combine multiple query data.tables and renumber query ids.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>FMAT_query_bind(...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="FMAT_query_bind_+3A_...">...</code></td>
<td>
<p>Query data.tables returned from <code><a href="#topic+FMAT_query">FMAT_query</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data.table of queries and variables.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+FMAT_load">FMAT_load</a></code>
</p>
<p><code><a href="#topic+FMAT_query">FMAT_query</a></code>
</p>
<p><code><a href="#topic+FMAT_run">FMAT_run</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>FMAT_query_bind(
  FMAT_query(
    "[MASK] is {TARGET}.",
    MASK = .(Male="He", Female="She"),
    TARGET = .(Occupation=c("a doctor", "a nurse", "an artist"))
  ),
  FMAT_query(
    "[MASK] occupation is {TARGET}.",
    MASK = .(Male="His", Female="Her"),
    TARGET = .(Occupation=c("doctor", "nurse", "artist"))
  )
)

</code></pre>

<hr>
<h2 id='FMAT_run'>Run the fill-mask pipeline on multiple models (CPU / GPU).</h2><span id='topic+FMAT_run'></span>

<h3>Description</h3>

<p>Run the fill-mask pipeline on multiple models with CPU or GPU
(faster but requiring an NVIDIA GPU device).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>FMAT_run(
  models,
  data,
  gpu,
  add.tokens = FALSE,
  add.method = c("sum", "mean"),
  file = NULL,
  progress = TRUE,
  warning = TRUE,
  na.out = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="FMAT_run_+3A_models">models</code></td>
<td>
<p>Options:
</p>

<ul>
<li><p> A character vector of model names at
<a href="https://huggingface.co/models?pipeline_tag=fill-mask&amp;library=transformers">HuggingFace</a>.
</p>

<ul>
<li><p> Can be used for both CPU and GPU.
</p>
</li></ul>

</li>
<li><p> A returned object from <code><a href="#topic+FMAT_load">FMAT_load</a></code>.
</p>

<ul>
<li><p> Can ONLY be used for CPU.
</p>
</li>
<li><p> If you <em>restart</em> the R session,
you will need to <em>rerun</em> <code><a href="#topic+FMAT_load">FMAT_load</a></code>.
</p>
</li></ul>

</li></ul>
</td></tr>
<tr><td><code id="FMAT_run_+3A_data">data</code></td>
<td>
<p>A data.table returned from <code><a href="#topic+FMAT_query">FMAT_query</a></code> or <code><a href="#topic+FMAT_query_bind">FMAT_query_bind</a></code>.</p>
</td></tr>
<tr><td><code id="FMAT_run_+3A_gpu">gpu</code></td>
<td>
<p>Use GPU (3x faster than CPU) to run the fill-mask pipeline?
Defaults to missing value that will <em>automatically</em> use available GPU
(if not available, then use CPU).
An NVIDIA GPU device (e.g., GeForce RTX Series) is required to use GPU.
See <a href="https://psychbruce.github.io/FMAT/#guidance-for-gpu-acceleration">Guidance for GPU Acceleration</a>.
</p>
<p>Options passing to the <code>device</code> parameter in Python:
</p>

<ul>
<li> <p><code>FALSE</code>: CPU (<code>device = -1</code>).
</p>
</li>
<li> <p><code>TRUE</code>: GPU (<code>device = 0</code>).
</p>
</li>
<li><p> Any other value: passing to
<a href="https://huggingface.co/docs/transformers/main_classes/pipelines#transformers.pipeline.device">transformers.pipeline(device=...)</a>
which defines the device (e.g.,
<code>"cpu"</code>, <code>"cuda:0"</code>, or a GPU device id like <code>1</code>)
on which the pipeline will be allocated.
</p>
</li></ul>
</td></tr>
<tr><td><code id="FMAT_run_+3A_add.tokens">add.tokens</code></td>
<td>
<p>Add new tokens (for out-of-vocabulary words or even phrases) to model vocabulary?
Defaults to <code>FALSE</code>. It only temporarily adds tokens for tasks but does not change the raw model file.</p>
</td></tr>
<tr><td><code id="FMAT_run_+3A_add.method">add.method</code></td>
<td>
<p>Method used to produce the token embeddings of new added tokens.
Can be <code>"sum"</code> (default) or <code>"mean"</code> of subword token embeddings.</p>
</td></tr>
<tr><td><code id="FMAT_run_+3A_file">file</code></td>
<td>
<p>File name of <code>.RData</code> to save the returned data.</p>
</td></tr>
<tr><td><code id="FMAT_run_+3A_progress">progress</code></td>
<td>
<p>Show a progress bar? Defaults to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="FMAT_run_+3A_warning">warning</code></td>
<td>
<p>Alert warning of out-of-vocabulary word(s)? Defaults to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="FMAT_run_+3A_na.out">na.out</code></td>
<td>
<p>Replace probabilities of out-of-vocabulary word(s) with <code>NA</code>? Defaults to <code>TRUE</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function automatically adjusts for
the compatibility of tokens used in certain models:
(1) for uncased models (e.g., ALBERT), it turns tokens to lowercase;
(2) for models that use <code style="white-space: pre;">&#8288;&lt;mask&gt;&#8288;</code> rather than <code style="white-space: pre;">&#8288;[MASK]&#8288;</code>,
it automatically uses the corrected mask token;
(3) for models that require a prefix to estimate whole words than subwords
(e.g., ALBERT, RoBERTa), it adds a certain prefix (usually a white space;
\u2581 for ALBERT and XLM-RoBERTa, \u0120 for RoBERTa and DistilRoBERTa).
</p>
<p>Note that these changes only affect the <code>token</code> variable
in the returned data, but will not affect the <code>M_word</code> variable.
Thus, users may analyze data based on the unchanged <code>M_word</code>
rather than the <code>token</code>.
</p>
<p>Note also that there may be extremely trivial differences
(after 5~6 significant digits) in the
raw probability estimates between using CPU and GPU,
but these differences would have little impact on main results.
</p>


<h3>Value</h3>

<p>A data.table (of new class <code>fmat</code>) appending <code>data</code> with these new variables:
</p>

<ul>
<li> <p><code>model</code>: model name.
</p>
</li>
<li> <p><code>output</code>: complete sentence output with unmasked token.
</p>
</li>
<li> <p><code>token</code>: actual token to be filled in the blank mask
(a note &quot;out-of-vocabulary&quot; will be added
if the original word is not found in the model vocabulary).
</p>
</li>
<li> <p><code>prob</code>: (raw) conditional probability of the unmasked token
given the provided context, estimated by the masked language model.
</p>

<ul>
<li><p> It is NOT SUGGESTED to directly interpret the raw probabilities
because the <em>contrast</em> between a pair of probabilities
is more interpretable. See <code><a href="#topic+summary.fmat">summary.fmat</a></code>.
</p>
</li></ul>

</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+BERT_download">BERT_download</a></code>
</p>
<p><code><a href="#topic+BERT_vocab">BERT_vocab</a></code>
</p>
<p><code><a href="#topic+FMAT_load">FMAT_load</a></code> (deprecated)
</p>
<p><code><a href="#topic+FMAT_query">FMAT_query</a></code>
</p>
<p><code><a href="#topic+FMAT_query_bind">FMAT_query_bind</a></code>
</p>
<p><code><a href="#topic+summary.fmat">summary.fmat</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Running the examples requires the models downloaded

## Not run: 
models = c("bert-base-uncased", "bert-base-cased")

query1 = FMAT_query(
  c("[MASK] is {TARGET}.", "[MASK] works as {TARGET}."),
  MASK = .(Male="He", Female="She"),
  TARGET = .(Occupation=c("a doctor", "a nurse", "an artist"))
)
data1 = FMAT_run(models, query1)
summary(data1, target.pair=FALSE)

query2 = FMAT_query(
  "The [MASK] {ATTRIB}.",
  MASK = .(Male=c("man", "boy"),
           Female=c("woman", "girl")),
  ATTRIB = .(Masc=c("is masculine", "has a masculine personality"),
             Femi=c("is feminine", "has a feminine personality"))
)
data2 = FMAT_run(models, query2)
summary(data2, mask.pair=FALSE)
summary(data2)

## End(Not run)

</code></pre>

<hr>
<h2 id='ICC_models'>Intraclass correlation coefficient (ICC) of BERT models.</h2><span id='topic+ICC_models'></span>

<h3>Description</h3>

<p>Interrater agreement of log probabilities (treated as &quot;ratings&quot;/rows)
among BERT language models (treated as &quot;raters&quot;/columns),
with both row and column as (&quot;two-way&quot;) random effects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ICC_models(data, type = "agreement", unit = "average")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ICC_models_+3A_data">data</code></td>
<td>
<p>Raw data returned from <code><a href="#topic+FMAT_run">FMAT_run</a></code>.</p>
</td></tr>
<tr><td><code id="ICC_models_+3A_type">type</code></td>
<td>
<p>Interrater <code>"agreement"</code> (default) or <code>"consistency"</code>.</p>
</td></tr>
<tr><td><code id="ICC_models_+3A_unit">unit</code></td>
<td>
<p>Reliability of <code>"average"</code> scores (default) or <code>"single"</code> scores.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data.table of ICC.
</p>

<hr>
<h2 id='LPR_reliability'>Reliability analysis (Cronbach's <code class="reqn">\alpha</code>) of LPR.</h2><span id='topic+LPR_reliability'></span>

<h3>Description</h3>

<p>Reliability analysis (Cronbach's <code class="reqn">\alpha</code>) of LPR.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>LPR_reliability(fmat, item = c("query", "T_word", "A_word"), by = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="LPR_reliability_+3A_fmat">fmat</code></td>
<td>
<p>A data.table returned from <code><a href="#topic+summary.fmat">summary.fmat</a></code>.</p>
</td></tr>
<tr><td><code id="LPR_reliability_+3A_item">item</code></td>
<td>
<p>Reliability of multiple <code>"query"</code> (default),
<code>"T_word"</code>, or <code>"A_word"</code>.</p>
</td></tr>
<tr><td><code id="LPR_reliability_+3A_by">by</code></td>
<td>
<p>Variable(s) to split data by.
Options can be <code>"model"</code>, <code>"TARGET"</code>, <code>"ATTRIB"</code>,
or any combination of them.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data.table of Cronbach's <code class="reqn">\alpha</code>.
</p>

<hr>
<h2 id='summary.fmat'>[S3 method] Summarize the results for the FMAT.</h2><span id='topic+summary.fmat'></span>

<h3>Description</h3>

<p>Summarize the results of <em>Log Probability Ratio</em> (LPR),
which indicates the <em>relative</em> (vs. <em>absolute</em>)
association between concepts.
</p>
<p>The LPR of just one contrast (e.g., only between a pair of attributes)
may <em>not</em> be sufficient for a proper interpretation of the results,
and may further require a second contrast (e.g., between a pair of targets).
</p>
<p>Users are suggested to use linear mixed models
(with the R packages <code>nlme</code> or <code>lme4</code>/<code>lmerTest</code>)
to perform the formal analyses and hypothesis tests based on the LPR.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'fmat'
summary(
  object,
  mask.pair = TRUE,
  target.pair = TRUE,
  attrib.pair = TRUE,
  warning = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.fmat_+3A_object">object</code></td>
<td>
<p>A data.table (of new class <code>fmat</code>)
returned from <code><a href="#topic+FMAT_run">FMAT_run</a></code>.</p>
</td></tr>
<tr><td><code id="summary.fmat_+3A_mask.pair">mask.pair</code>, <code id="summary.fmat_+3A_target.pair">target.pair</code>, <code id="summary.fmat_+3A_attrib.pair">attrib.pair</code></td>
<td>
<p>Pairwise contrast of
<code style="white-space: pre;">&#8288;[MASK]&#8288;</code>, <code>TARGET</code>, <code>ATTRIB</code>?
Defaults to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="summary.fmat_+3A_warning">warning</code></td>
<td>
<p>Alert warning of out-of-vocabulary word(s)? Defaults to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="summary.fmat_+3A_...">...</code></td>
<td>
<p>Other arguments (currently not used).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data.table of the summarized results with Log Probability Ratio (LPR).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+FMAT_run">FMAT_run</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># see examples in `FMAT_run`

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
