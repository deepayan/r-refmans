<!DOCTYPE html><html lang="en"><head><title>Help for package nn2poly</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {nn2poly}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#add_constraints'><p>Add constraints to a neural network</p></a></li>
<li><a href='#eval_poly'><p>Polynomial evaluation</p></a></li>
<li><a href='#luz_model_sequential'><p>Build a <code>luz</code> model composed of a linear stack of layers</p></a></li>
<li><a href='#nn2poly'><p>Obtain polynomial representation</p></a></li>
<li><a href='#plot_diagonal'><p>Plots a comparison between two sets of points.</p></a></li>
<li><a href='#plot_taylor_and_activation_potentials'><p>Plots activation potentials and Taylor expansion.</p></a></li>
<li><a href='#plot.nn2poly'><p>Plot method for <code>nn2poly</code> objects.</p></a></li>
<li><a href='#predict.nn2poly'><p>Predict method for <code>nn2poly</code> objects.</p></a></li>
<li><a href='#reexports'><p>Objects exported from other packages</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Title:</td>
<td>Neural Network Weights Transformation into Polynomial
Coefficients</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.2</td>
</tr>
<tr>
<td>Description:</td>
<td>Implements a method that builds the coefficients of a polynomial
    model that performs almost equivalently as a given neural network
    (densely connected). This is achieved using Taylor expansion at the
    activation functions.  The obtained polynomial coefficients can be used
    to explain features (and their interactions) importance  in the neural network,
    therefore working as a tool for interpretability or eXplainable Artificial 
    Intelligence (XAI). See Morala et al. 2021 &lt;<a href="https://doi.org/10.1016%2Fj.neunet.2021.04.036">doi:10.1016/j.neunet.2021.04.036</a>&gt;,
    and 2023 &lt;<a href="https://doi.org/10.1109%2FTNNLS.2023.3330328">doi:10.1109/TNNLS.2023.3330328</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp, generics, matrixStats, pracma</td>
</tr>
<tr>
<td>Suggests:</td>
<td>keras, tensorflow, reticulate, luz, torch, cowplot, ggplot2,
patchwork, testthat (&ge; 3.0.0), vdiffr, knitr, rmarkdown</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://ibidat.github.io/nn2poly/">https://ibidat.github.io/nn2poly/</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-10-23 09:21:38 UTC; pablo</td>
</tr>
<tr>
<td>Author:</td>
<td>Pablo Morala <a href="https://orcid.org/0000-0002-4109-2330"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre],
  IÃ±aki Ucar <a href="https://orcid.org/0000-0001-6403-5550"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut],
  Jose Ignacio Diez [ctr]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Pablo Morala &lt;moralapablo@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-11-11 15:20:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='add_constraints'>Add constraints to a neural network</h2><span id='topic+add_constraints'></span>

<h3>Description</h3>

<p>This function sets up a neural network object with the constraints required
by the <code><a href="#topic+nn2poly">nn2poly</a></code> algorithm. Currently supported neural network
frameworks are <code>keras/tensorflow</code> and <code>luz/torch</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>add_constraints(object, type = c("l1_norm", "l2_norm"), ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="add_constraints_+3A_object">object</code></td>
<td>
<p>A neural network object in sequential form from one of the
supported frameworks.</p>
</td></tr>
<tr><td><code id="add_constraints_+3A_type">type</code></td>
<td>
<p>Constraint type. Currently, <code>l1_norm</code> and <code>l2_norm</code> are supported.</p>
</td></tr>
<tr><td><code id="add_constraints_+3A_...">...</code></td>
<td>
<p>Additional arguments (unused).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Constraints are added to the model object using callbacks in their specific
framework. These callbacks are used during training when calling fit on the
model. Specifically we are using callbacks that are applied at the end of
each train batch.
</p>
<p>Models in <code>luz/torch</code> need to use the <code><a href="#topic+luz_model_sequential">luz_model_sequential</a></code>
helper in order to have a sequential model in the appropriate form.
</p>


<h3>Value</h3>

<p>A <code>nn2poly</code> neural network object.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+luz_model_sequential">luz_model_sequential()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
if (requireNamespace("keras", quietly=TRUE)) {
  # ---- Example with a keras/tensorflow network ----
  # Build a small nn:
  nn &lt;- keras::keras_model_sequential()
  nn &lt;- keras::layer_dense(nn, units = 10, activation = "tanh", input_shape = 2)
  nn &lt;- keras::layer_dense(nn, units = 1, activation = "linear")

  # Add constraints
  nn_constrained &lt;- add_constraints(nn, constraint_type = "l1_norm")

  # Check that class of the constrained nn is "nn2poly"
  class(nn_constrained)[1]
}

if (requireNamespace("luz", quietly=TRUE)) {
  # ---- Example with a luz/torch network ----

  # Build a small nn
  nn &lt;- luz_model_sequential(
    torch::nn_linear(2,10),
    torch::nn_tanh(),
    torch::nn_linear(10,1)
  )

  # With luz/torch we need to setup the nn before adding the constraints
  nn &lt;- luz::setup(module = nn,
    loss = torch::nn_mse_loss(),
    optimizer = torch::optim_adam,
  )

  # Add constraints
  nn &lt;- add_constraints(nn)

  # Check that class of the constrained nn is "nn2poly"
  class(nn)[1]
}

## End(Not run)

</code></pre>

<hr>
<h2 id='eval_poly'>Polynomial evaluation</h2><span id='topic+eval_poly'></span>

<h3>Description</h3>

<p>Evaluates one or several polynomials on the given data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>eval_poly(poly, newdata)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="eval_poly_+3A_poly">poly</code></td>
<td>
<p>List containing 2 items: <code>labels</code> and <code>values</code>.
</p>

<ul>
<li> <p><code>labels</code>: List of integer vectors with same length (or number of cols)
as <code>values</code>, where each integer vector denotes the combination of
variables associated to the coefficient value stored at the same position in
<code>values</code>. That is, the monomials in the polynomial. Note that the
variables are numbered from 1 to p, with the intercept is represented by 0.
</p>
</li>
<li> <p><code>values</code>: Matrix (can also be a vector if single polynomial), where
each column represents a polynomial, with same number of rows as the length
of <code>labels</code>, containing at each row the value of the coefficient
of the monomial given by the equivalent label in that same position.
</p>
</li></ul>

<p>Example: If <code>labels</code> contains the integer vector c(1,1,3) at position
5, then the value stored in <code>values</code> at row 5 is the coefficient
associated with the term x_1^2*x_3.</p>
</td></tr>
<tr><td><code id="eval_poly_+3A_newdata">newdata</code></td>
<td>
<p>Input data as matrix, vector or dataframe.
Number of columns (or elements in vector) should be the number of variables
in the polynomial (dimension p). Response variable to be predicted should
not be included.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note that this function is unstable and subject to change. Therefore it is
not exported but this documentations is left available so users can use it if
needed to simulate data by using <code>nn2poly:::eval_poly()</code>
</p>


<h3>Value</h3>

<p>Returns a matrix containing the evaluation of the polynomials.
Each column corresponds to each polynomial used and each row to each
observation, meaning that each column vector corresponds to the results of
evaluating all the given data for each polynomial.
</p>


<h3>See Also</h3>

<p><code>eval_poly()</code> is also used in <code><a href="#topic+predict.nn2poly">predict.nn2poly()</a></code>.
</p>

<hr>
<h2 id='luz_model_sequential'>Build a <code>luz</code> model composed of a linear stack of layers</h2><span id='topic+luz_model_sequential'></span>

<h3>Description</h3>

<p>Helper function to build <code>luz</code> models as a sequential model, by feeding
it a stack of <code>luz</code> layers.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>luz_model_sequential(...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="luz_model_sequential_+3A_...">...</code></td>
<td>
<p>Sequence of modules to be added.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This step is needed so we can get the activation functions and
layers and neurons architecture easily with <code>nn2poly:::get_parameters()</code>.
Furthermore, this step is also needed to be able to impose the needed
constraints when using the <code>luz/torch</code> framework.
</p>


<h3>Value</h3>

<p>A <code>nn_sequential</code> module.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+add_constraints">add_constraints()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
if (requireNamespace("luz", quietly=TRUE)) {
# Create a NN using luz/torch as a sequential model
# with 3 fully connected linear layers,
# the first one with input = 5 variables,
# 100 neurons and tanh activation function, the second
# one with 50 neurons and softplus activation function
# and the last one with 1 linear output.
nn &lt;- luz_model_sequential(
  torch::nn_linear(5,100),
  torch::nn_tanh(),
  torch::nn_linear(100,50),
  torch::nn_softplus(),
  torch::nn_linear(50,1)
)

nn

# Check that the nn is of class nn_squential
class(nn)
}

## End(Not run)

</code></pre>

<hr>
<h2 id='nn2poly'>Obtain polynomial representation</h2><span id='topic+nn2poly'></span>

<h3>Description</h3>

<p>Implements the main NN2Poly algorithm to obtain a polynomial representation
of a trained neural network using its weights and Taylor expansion of its
activation functions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nn2poly(
  object,
  max_order = 2,
  keep_layers = FALSE,
  taylor_orders = 8,
  ...,
  all_partitions = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="nn2poly_+3A_object">object</code></td>
<td>
<p>An object for which the computation of the NN2Poly algorithm is
desired. Currently supports models from the following deep learning frameworks:
</p>

<ul>
<li> <p><code>tensorflow</code>/<code>keras</code> models built as a sequential model.
</p>
</li>
<li> <p><code>torch</code>/<code>luz</code> models built as a sequential model.
</p>
</li></ul>

<p>It also supports a named <code>list</code> as input which allows to introduce by
hand a model from any other source. This <code>list</code> should be of length L
(number of hidden layers + 1) containing the weights matrix for each layer.
Each element of the list should be named as the activation function used at
each layer. Currently supported activation functions are <code>"tanh"</code>,
<code>"softplus"</code>, <code>"sigmoid"</code> and <code>"linear"</code>.
</p>
<p>At any layer <code class="reqn">l</code>, the expected shape of such matrices is of the form
<code class="reqn">(h_{(l-1)} + 1)*(h_l)</code>, that is, the number of rows is the number of
neurons in the previous layer plus the bias vector, and the number of columns
is the number of neurons in the current layer L. Therefore, each column
corresponds to the weight vector affecting each neuron in that layer.
The bias vector should be in the first row.</p>
</td></tr>
<tr><td><code id="nn2poly_+3A_max_order">max_order</code></td>
<td>
<p><code>integer</code> that determines the maximum order
that will be forced in the final polynomial, discarding terms of higher order
that would naturally arise when considering all Taylor expansions allowed by
<code>taylor_orders</code>.</p>
</td></tr>
<tr><td><code id="nn2poly_+3A_keep_layers">keep_layers</code></td>
<td>
<p>Boolean that determines if all polynomials computed in
the internal layers have to be stored and given in the output (<code>TRUE</code>),
or if only the polynomials from the last layer are needed (<code>FALSE</code>).
Default set to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="nn2poly_+3A_taylor_orders">taylor_orders</code></td>
<td>
<p><code>integer</code> or <code>vector</code> of length L that sets the
degree at which Taylor expansion is truncated at each layer. If a single
value is used, that value is set for each non linear layer and 1 for linear
at each layer activation function. Default set to <code>8</code>.</p>
</td></tr>
<tr><td><code id="nn2poly_+3A_...">...</code></td>
<td>
<p>Ignored.</p>
</td></tr>
<tr><td><code id="nn2poly_+3A_all_partitions">all_partitions</code></td>
<td>
<p>Optional argument containing the needed multipartitions
as list of lists of lists. If set to <code>NULL</code>, nn2poly will compute said
multipartitions. This step can be computationally expensive when the chosen
polynomial order or the dimension are too high. In such cases, it is
encouraged that the multipartitions are stored and reused when possible.
Default set to <code>NULL</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns an object of class <code>nn2poly</code>.
</p>
<p>If <code>keep_layers = FALSE</code> (default case), it returns a list with two
items:
</p>

<ul>
<li><p> An item named <code>labels</code> that is a list of integer vectors. Those vectors
represent each monomial in the polynomial, where each integer in the vector
represents each time one of the original variables appears in that term.
As an example, vector c(1,1,2) represents the term <code class="reqn">x_1^2x_2</code>. Note that
the variables are numbered from 1 to p, with the intercept is represented by
zero.
</p>
</li>
<li><p> An item named <code>values</code> which contains a matrix in which each column contains
the coefficients of the polynomial associated with an output neuron. That is,
if the neural network has a single output unit, the matrix <code>values</code> will have
a single column and if it has multiple output units, the matrix <code>values</code> will
have several columns. Each row will be the coefficient associated with the
label in the same position in the labels list.
</p>
</li></ul>

<p>If <code>keep_layers = TRUE</code>, it returns a list of length the number of
layers (represented by <code>layer_i</code>), where each one is another list with
<code>input</code> and <code>output</code> elements. Each of those elements contains an
item as explained before. The last layer output item will be the same element
as if <code>keep_layers = FALSE</code>.
</p>
<p>The polynomials obtained at the hidden layers are not needed to represent the
NN but can be used to explore other insights from the NN.
</p>


<h3>See Also</h3>

<p>Predict method for <code>nn2poly</code> output <code><a href="#topic+predict.nn2poly">predict.nn2poly()</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Build a NN estructure with random weights, with 2 (+ bias) inputs,
# 4 (+bias) neurons in the first hidden layer with "tanh" activation
# function, 4 (+bias) neurons in the second hidden layer with "softplus",
# and 1 "linear" output unit

weights_layer_1 &lt;- matrix(rnorm(12), nrow = 3, ncol = 4)
weights_layer_2 &lt;- matrix(rnorm(20), nrow = 5, ncol = 4)
weights_layer_3 &lt;- matrix(rnorm(5), nrow = 5, ncol = 1)

# Set it as a list with activation functions as names
nn_object = list("tanh" = weights_layer_1,
                 "softplus" = weights_layer_2,
                 "linear" = weights_layer_3)

# Obtain the polynomial representation (order = 3) of that neural network
final_poly &lt;- nn2poly(nn_object, max_order = 3)

# Change the last layer to have 3 outputs (as in a multiclass classification)
# problem
weights_layer_4 &lt;- matrix(rnorm(20), nrow = 5, ncol = 4)

# Set it as a list with activation functions as names
nn_object = list("tanh" = weights_layer_1,
                 "softplus" = weights_layer_2,
                 "linear" = weights_layer_4)
# Obtain the polynomial representation of that neural network
# In this case the output is formed by several polynomials with the same
# structure but different coefficient values
final_poly &lt;- nn2poly(nn_object, max_order = 3)

# Polynomial representation of each hidden neuron is given by
final_poly &lt;- nn2poly(nn_object, max_order = 3, keep_layers = TRUE)

</code></pre>

<hr>
<h2 id='plot_diagonal'>Plots a comparison between two sets of points.</h2><span id='topic+plot_diagonal'></span>

<h3>Description</h3>

<p>If the points come from the predictions of an NN and a PM and the line
<code>(plot.line = TRUE)</code> is displayed, in case the method does exhibit
asymptotic behavior, the points should not fall in the line.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_diagonal(
  x_axis,
  y_axis,
  xlab = NULL,
  ylab = NULL,
  title = NULL,
  plot.line = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot_diagonal_+3A_x_axis">x_axis</code></td>
<td>
<p>Values to plot in the <code>x</code> axis.</p>
</td></tr>
<tr><td><code id="plot_diagonal_+3A_y_axis">y_axis</code></td>
<td>
<p>Values to plot in the <code>y</code> axis.</p>
</td></tr>
<tr><td><code id="plot_diagonal_+3A_xlab">xlab</code></td>
<td>
<p>Lab of the <code>x</code> axis</p>
</td></tr>
<tr><td><code id="plot_diagonal_+3A_ylab">ylab</code></td>
<td>
<p>Lab of the <code>y</code> axis.</p>
</td></tr>
<tr><td><code id="plot_diagonal_+3A_title">title</code></td>
<td>
<p>Title of the plot.</p>
</td></tr>
<tr><td><code id="plot_diagonal_+3A_plot.line">plot.line</code></td>
<td>
<p>If a red line with <code>slope = 1</code> and <code>intercept = 0</code> should
be plotted.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Plot (ggplot object).
</p>

<hr>
<h2 id='plot_taylor_and_activation_potentials'>Plots activation potentials and Taylor expansion.</h2><span id='topic+plot_taylor_and_activation_potentials'></span>

<h3>Description</h3>

<p>Function that allows to take a NN and the data input values
and plot the distribution of data activation potentials
(sum of input values * weights) at all neurons together at each layer
with the Taylor expansion used in the activation functions. If any layer
is <code>'linear'</code> (usually will be the output), then that layer will not
be an approximation as Taylor expansion is not needed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_taylor_and_activation_potentials(
  object,
  data,
  max_order,
  taylor_orders = 8,
  constraints,
  taylor_interval = 1.5,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot_taylor_and_activation_potentials_+3A_object">object</code></td>
<td>
<p>An object for which the computation of the NN2Poly algorithm is
desired. Currently supports models from the following deep learning frameworks:
</p>

<ul>
<li> <p><code>tensorflow</code>/<code>keras</code> models built as a sequential model.
</p>
</li>
<li> <p><code>torch</code>/<code>luz</code> models built as a sequential model.
</p>
</li></ul>

<p>It also supports a named <code>list</code> as input which allows to introduce by
hand a model from any other source. This <code>list</code> should be of length L
(number of hidden layers + 1) containing the weights matrix for each layer.
Each element of the list should be named as the activation function used at
each layer. Currently supported activation functions are <code>"tanh"</code>,
<code>"softplus"</code>, <code>"sigmoid"</code> and <code>"linear"</code>.
</p>
<p>At any layer <code class="reqn">l</code>, the expected shape of such matrices is of the form
<code class="reqn">(h_{(l-1)} + 1)*(h_l)</code>, that is, the number of rows is the number of
neurons in the previous layer plus the bias vector, and the number of columns
is the number of neurons in the current layer L. Therefore, each column
corresponds to the weight vector affecting each neuron in that layer.
The bias vector should be in the first row.</p>
</td></tr>
<tr><td><code id="plot_taylor_and_activation_potentials_+3A_data">data</code></td>
<td>
<p>Matrix or data frame containing the predictor variables (X)
to be used as input to compute their activation potentials. The response
variable column should not be included.</p>
</td></tr>
<tr><td><code id="plot_taylor_and_activation_potentials_+3A_max_order">max_order</code></td>
<td>
<p><code>integer</code> that determines the maximum order
that will be forced in the final polynomial, discarding terms of higher order
that would naturally arise when considering all Taylor expansions allowed by
<code>taylor_orders</code>.</p>
</td></tr>
<tr><td><code id="plot_taylor_and_activation_potentials_+3A_taylor_orders">taylor_orders</code></td>
<td>
<p><code>integer</code> or <code>vector</code> of length L that sets the
degree at which Taylor expansion is truncated at each layer. If a single
value is used, that value is set for each non linear layer and 1 for linear
at each layer activation function. Default set to <code>8</code>.</p>
</td></tr>
<tr><td><code id="plot_taylor_and_activation_potentials_+3A_constraints">constraints</code></td>
<td>
<p>Boolean parameter determining if the NN is constrained
(TRUE) or not (FALSE). This only modifies the plots title to show
&quot;constrained&quot; or &quot;unconstrained&quot; respectively.</p>
</td></tr>
<tr><td><code id="plot_taylor_and_activation_potentials_+3A_taylor_interval">taylor_interval</code></td>
<td>
<p>optional parameter determining the interval in which
the Taylor expansion is represented. Default is 1.5.</p>
</td></tr>
<tr><td><code id="plot_taylor_and_activation_potentials_+3A_...">...</code></td>
<td>
<p>Additional parameters.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of plots.
</p>

<hr>
<h2 id='plot.nn2poly'>Plot method for <code>nn2poly</code> objects.</h2><span id='topic+plot.nn2poly'></span>

<h3>Description</h3>

<p>A function that takes a polynomial (or several ones) as given by the
<span class="pkg">nn2poly</span> algorithm, and then plots their absolute magnitude as barplots
to be able to compare the most important coefficients.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'nn2poly'
plot(x, ..., n = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.nn2poly_+3A_x">x</code></td>
<td>
<p>A <code>nn2poly</code> object, as returned by the <span class="pkg">nn2poly</span> algorithm.</p>
</td></tr>
<tr><td><code id="plot.nn2poly_+3A_...">...</code></td>
<td>
<p>Ignored.</p>
</td></tr>
<tr><td><code id="plot.nn2poly_+3A_n">n</code></td>
<td>
<p>An integer denoting the number of coefficients to be plotted,
after ordering them by absolute magnitude.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The plot method represents only the polynomials at the final layer, even if
<code>x</code> is generated using <code>nn2poly()</code> with <code>keep_layers=TRUE</code>.
</p>


<h3>Value</h3>

<p>A plot showing the <code>n</code> most important coefficients.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># --- Single polynomial output ---
# Build a NN structure with random weights, with 2 (+ bias) inputs,
# 4 (+bias) neurons in the first hidden layer with "tanh" activation
# function, 4 (+bias) neurons in the second hidden layer with "softplus",
# and 2 "linear" output units

weights_layer_1 &lt;- matrix(rnorm(12), nrow = 3, ncol = 4)
weights_layer_2 &lt;- matrix(rnorm(20), nrow = 5, ncol = 4)
weights_layer_3 &lt;- matrix(rnorm(5), nrow = 5, ncol = 1)

# Set it as a list with activation functions as names
nn_object = list("tanh" = weights_layer_1,
                 "softplus" = weights_layer_2,
                 "linear" = weights_layer_3)

# Obtain the polynomial representation (order = 3) of that neural network
final_poly &lt;- nn2poly(nn_object, max_order = 3)

# Plot all the coefficients, one plot per output unit
plot(final_poly)

# Plot only the 5 most important coeffcients (by absolute magnitude)
# one plot per output unit
plot(final_poly, n = 5)

# --- Multiple output polynomials ---
# Build a NN structure with random weights, with 2 (+ bias) inputs,
# 4 (+bias) neurons in the first hidden layer with "tanh" activation
# function, 4 (+bias) neurons in the second hidden layer with "softplus",
# and 2 "linear" output units

weights_layer_1 &lt;- matrix(rnorm(12), nrow = 3, ncol = 4)
weights_layer_2 &lt;- matrix(rnorm(20), nrow = 5, ncol = 4)
weights_layer_3 &lt;- matrix(rnorm(10), nrow = 5, ncol = 2)

# Set it as a list with activation functions as names
nn_object = list("tanh" = weights_layer_1,
                 "softplus" = weights_layer_2,
                 "linear" = weights_layer_3)

# Obtain the polynomial representation (order = 3) of that neural network
final_poly &lt;- nn2poly(nn_object, max_order = 3)

# Plot all the coefficients, one plot per output unit
plot(final_poly)

# Plot only the 5 most important coeffcients (by absolute magnitude)
# one plot per output unit
plot(final_poly, n = 5)

</code></pre>

<hr>
<h2 id='predict.nn2poly'>Predict method for <code>nn2poly</code> objects.</h2><span id='topic+predict.nn2poly'></span>

<h3>Description</h3>

<p>Predicted values obtained with a <code>nn2poly</code> object on given data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'nn2poly'
predict(object, newdata, layers = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.nn2poly_+3A_object">object</code></td>
<td>
<p>Object of class inheriting from 'nn2poly'.</p>
</td></tr>
<tr><td><code id="predict.nn2poly_+3A_newdata">newdata</code></td>
<td>
<p>Input data as matrix, vector or dataframe.
Number of columns (or elements in vector) should be the number of variables
in the polynomial (dimension p). Response variable to be predicted should
not be included.</p>
</td></tr>
<tr><td><code id="predict.nn2poly_+3A_layers">layers</code></td>
<td>
<p>Vector containing the chosen layers from <code>object</code> to be
evaluated. If set to <code>NULL</code>, all layers are computed. Default is set
to <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="predict.nn2poly_+3A_...">...</code></td>
<td>
<p>Further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Internally uses <code>eval_poly()</code> to obtain the predictions. However, this only
works with a objects of class <code>nn2poly</code> while <code>eval_poly()</code> can be used
with a manually created polynomial in list form.
</p>
<p>When <code>object</code> contains all the internal polynomials also, as given by
<code>nn2poly(object, keep_layers = TRUE)</code>, it is important to note that there
are two polynomial items per layer (input/output). These polynomial items will
also contain several polynomials of the same structure, one per neuron in the
layer, stored as matrix rows in <code>$values</code>. Please see the NN2Poly
original paper for more details.
</p>
<p>Note also that &quot;linear&quot; layers will contain the same input and output results
as Taylor expansion is not used and thus the polynomials are also the same.
Because of this, in the situation of evaluating multiple layers we provide
the final layer with &quot;input&quot; and &quot;output&quot; even if they are the same, for
consistency.
</p>


<h3>Value</h3>

<p>Returns a matrix or list of matrices with the evaluation of each
polynomial at each layer as given by the provided <code>object</code> of class
<code>nn2poly</code>.
</p>
<p>If <code>object</code> contains the polynomials of the last layer, as given by
<code>nn2poly(object, keep_layers = FALSE)</code>, then the output is a matrix with
the evaluation of each data point on each polynomial. In this matrix, each
column represents the evaluation of a polynomial and each column corresponds
to each point in the new data to be evaluated.
</p>
<p>If <code>object</code> contains all the internal polynomials also, as given by
<code>nn2poly(object, keep_layers = TRUE)</code>, then the output is a list of
layers (represented by <code>layer_i</code>), where each one is another list with
<code>input</code> and <code>output</code> elements, where each one contains a matrix
with the evaluation of the &quot;input&quot; or &quot;output&quot; polynomial at the given layer,
as explained in the case without internal polynomials.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+nn2poly">nn2poly()</a></code>: function that obtains the <code>nn2poly</code> polynomial
object, <code><a href="#topic+eval_poly">eval_poly()</a></code>: function that can evaluate polynomials in general,
<code><a href="stats.html#topic+predict">stats::predict()</a></code>: generic predict function.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Build a NN structure with random weights, with 2 (+ bias) inputs,
# 4 (+bias) neurons in the first hidden layer with "tanh" activation
# function, 4 (+bias) neurons in the second hidden layer with "softplus",
# and 1 "linear" output unit

weights_layer_1 &lt;- matrix(rnorm(12), nrow = 3, ncol = 4)
weights_layer_2 &lt;- matrix(rnorm(20), nrow = 5, ncol = 4)
weights_layer_3 &lt;- matrix(rnorm(5), nrow = 5, ncol = 1)

# Set it as a list with activation functions as names
nn_object = list("tanh" = weights_layer_1,
                 "softplus" = weights_layer_2,
                 "linear" = weights_layer_3)

# Obtain the polynomial representation (order = 3) of that neural network
final_poly &lt;- nn2poly(nn_object, max_order = 3)

# Define some new data, it can be vector, matrix or dataframe
newdata &lt;- matrix(rnorm(10), ncol = 2, nrow = 5)

# Predict using the obtained polynomial
predict(object = final_poly, newdata = newdata)

# Change the last layer to have 3 outputs (as in a multiclass classification)
# problem
weights_layer_4 &lt;- matrix(rnorm(20), nrow = 5, ncol = 4)

# Set it as a list with activation functions as names
nn_object = list("tanh" = weights_layer_1,
                 "softplus" = weights_layer_2,
                 "linear" = weights_layer_4)

# Obtain the polynomial representation of that neural network
# Polynomial representation of each hidden neuron is given by
final_poly &lt;- nn2poly(nn_object, max_order = 3, keep_layers = TRUE)

# Define some new data, it can be vector, matrix or dataframe
newdata &lt;- matrix(rnorm(10), ncol = 2, nrow = 5)

# Predict using the obtained polynomials (for all layers)
predict(object = final_poly, newdata = newdata)

# Predict using the obtained polynomials (for chosen layers)
predict(object = final_poly, newdata = newdata, layers = c(2,3))

</code></pre>

<hr>
<h2 id='reexports'>Objects exported from other packages</h2><span id='topic+reexports'></span><span id='topic+fit'></span>

<h3>Description</h3>

<p>These objects are imported from other packages. Follow the links
below to see their documentation.
</p>

<dl>
<dt>generics</dt><dd><p><code><a href="generics.html#topic+fit">fit</a></code></p>
</dd>
</dl>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
