<!DOCTYPE html><html lang="en"><head><title>Help for package WeightIt</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {WeightIt}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#WeightIt-package'><p>WeightIt: Weighting for Covariate Balance in Observational Studies</p></a></li>
<li><a href='#.weightit_methods'><p>Weighting methods</p></a></li>
<li><a href='#anova.glm_weightit'><p>Methods for <code>glm_weightit()</code> objects</p></a></li>
<li><a href='#as.weightit'><p>Create a <code>weightit</code> object manually</p></a></li>
<li><a href='#calibrate'><p>Calibrate Propensity Score Weights</p></a></li>
<li><a href='#ESS'><p>Compute effective sample size of weighted sample</p></a></li>
<li><a href='#get_w_from_ps'><p>Compute weights from propensity scores</p></a></li>
<li><a href='#glm_weightit'><p>Fitting Weighted Generalized Linear Models</p></a></li>
<li><a href='#glm_weightit-methods'><p>Methods for <code>glm_weightit()</code> objects</p></a></li>
<li><a href='#make_full_rank'><p>Make a design matrix full rank</p></a></li>
<li><a href='#method_bart'><p>Propensity Score Weighting Using BART</p></a></li>
<li><a href='#method_cbps'><p>Covariate Balancing Propensity Score Weighting</p></a></li>
<li><a href='#method_ebal'><p>Entropy Balancing</p></a></li>
<li><a href='#method_energy'><p>Energy Balancing</p></a></li>
<li><a href='#method_gbm'><p>Propensity Score Weighting Using Generalized Boosted Models</p></a></li>
<li><a href='#method_glm'><p>Propensity Score Weighting Using Generalized Linear Models</p></a></li>
<li><a href='#method_ipt'><p>Inverse Probability Tilting</p></a></li>
<li><a href='#method_npcbps'><p>Nonparametric Covariate Balancing Propensity Score Weighting</p></a></li>
<li><a href='#method_optweight'><p>Optimization-Based Weighting</p></a></li>
<li><a href='#method_super'><p>Propensity Score Weighting Using SuperLearner</p></a></li>
<li><a href='#method_user'><p>User-Defined Functions for Estimating Weights</p></a></li>
<li><a href='#msmdata'><p>Simulated data for a 3 time point sequential study</p></a></li>
<li><a href='#plot.weightit'><p>Plot information about the weight estimation process</p></a></li>
<li><a href='#predict.glm_weightit'><p>Predictions for <code>glm_weightit</code> objects</p></a></li>
<li><a href='#sbps'><p>Subgroup Balancing Propensity Score</p></a></li>
<li><a href='#summary.weightit'><p>Print and Summarize Output</p></a></li>
<li><a href='#trim'><p>Trim (Winsorize) Large Weights</p></a></li>
<li><a href='#weightit'><p>Estimate Balancing Weights</p></a></li>
<li><a href='#weightit.fit'><p>Generate Balancing Weights with Minimal Input Processing</p></a></li>
<li><a href='#weightitMSM'><p>Generate Balancing Weights for Longitudinal Treatments</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Weighting for Covariate Balance in Observational Studies</td>
</tr>
<tr>
<td>Version:</td>
<td>1.4.0</td>
</tr>
<tr>
<td>Description:</td>
<td>Generates balancing weights for causal effect estimation in observational studies with
             binary, multi-category, or continuous point or longitudinal treatments by easing and
             extending the functionality of several R packages and providing in-house estimation methods.
             Available methods include those that rely on parametric modeling, optimization, and machine learning. Also
             allows for assessment of weights and checking of covariate balance by interfacing directly
             with the 'cobalt' package. Methods for estimating weighted regression models that take into account 
             uncertainty in the estimation of the weights via M-estimation or bootstrapping are available. See the vignette "Installing Supporting Packages" for instructions on how
             to install any package 'WeightIt' uses, including those that may not be on CRAN.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.1.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>cobalt (&ge; 4.5.1), ggplot2 (&ge; 3.3.0), chk (&ge; 0.9.2), rlang
(&ge; 1.1.0), crayon (&ge; 1.3.4), sandwich, generics, utils, stats</td>
</tr>
<tr>
<td>Suggests:</td>
<td>rootSolve (&ge; 1.8.2.4), CBPS (&ge; 0.18), optweight (&ge; 0.2.4),
SuperLearner (&ge; 2.0-25), mclogit, MNP (&ge; 3.1-4), brglm2 (&ge;
0.5.2), enrichwith (&ge; 0.3.1), logistf (&ge; 1.26.0), osqp (&ge;
0.6.3.3), survival (&ge; 3.6-2), fwb (&ge; 0.2.0), splines,
marginaleffects (&ge; 0.19.0), MASS, gbm (&ge; 2.1.9), dbarts (&ge;
0.9-29), misaem (&ge; 1.0.1), mlogit, dfidx, broom, knitr,
rmarkdown, testthat (&ge; 3.0.0), waldo (&ge; 0.6.0)</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://ngreifer.github.io/WeightIt/">https://ngreifer.github.io/WeightIt/</a>,
<a href="https://github.com/ngreifer/WeightIt">https://github.com/ngreifer/WeightIt</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/ngreifer/WeightIt/issues">https://github.com/ngreifer/WeightIt/issues</a></td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.2</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2025-02-24 17:09:30 UTC; NoahGreifer</td>
</tr>
<tr>
<td>Author:</td>
<td>Noah Greifer <a href="https://orcid.org/0000-0003-3067-7154"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Noah Greifer &lt;noah.greifer@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2025-02-24 18:00:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='WeightIt-package'>WeightIt: Weighting for Covariate Balance in Observational Studies</h2><span id='topic+WeightIt'></span><span id='topic+WeightIt-package'></span>

<h3>Description</h3>

<p><img src="../help/figures/logo.png" style='float: right' alt='logo' width='120' />
</p>
<p>Generates balancing weights for causal effect estimation in observational studies with binary, multi-category, or continuous point or longitudinal treatments by easing and extending the functionality of several R packages and providing in-house estimation methods. Available methods include those that rely on parametric modeling, optimization, and machine learning. Also allows for assessment of weights and checking of covariate balance by interfacing directly with the 'cobalt' package. Methods for estimating weighted regression models that take into account uncertainty in the estimation of the weights via M-estimation or bootstrapping are available. See the vignette &quot;Installing Supporting Packages&quot; for instructions on how to install any package 'WeightIt' uses, including those that may not be on CRAN.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Noah Greifer <a href="mailto:noah.greifer@gmail.com">noah.greifer@gmail.com</a> (<a href="https://orcid.org/0000-0003-3067-7154">ORCID</a>)
</p>


<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://ngreifer.github.io/WeightIt/">https://ngreifer.github.io/WeightIt/</a>
</p>
</li>
<li> <p><a href="https://github.com/ngreifer/WeightIt">https://github.com/ngreifer/WeightIt</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/ngreifer/WeightIt/issues">https://github.com/ngreifer/WeightIt/issues</a>
</p>
</li></ul>


<hr>
<h2 id='.weightit_methods'>Weighting methods</h2><span id='topic+.weightit_methods'></span>

<h3>Description</h3>

<p><code>.weightit_methods</code> is a list containing the allowable weighting
methods that can be supplied by name to the <code>method</code> argument of
<code><a href="#topic+weightit">weightit()</a></code>, <code><a href="#topic+weightitMSM">weightitMSM()</a></code>, and <code><a href="#topic+weightit.fit">weightit.fit()</a></code>. Each entry corresponds
to an allowed method and contains information about what options are and
are not allowed for each method. While this list is primarily for internal
use by checking functions in <span class="pkg">WeightIt</span>, it might be of use for package
authors that want to support different weighting methods.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.weightit_methods
</code></pre>


<h3>Format</h3>

<p>An object of class <code>list</code> of length 10.
</p>


<h3>Details</h3>

<p>Each component is itself a list containing the following components:
</p>

<dl>
<dt><code>treat_type</code></dt><dd><p>at least one of <code>"binary"</code>, <code>"multi-category"</code>, or
<code>"continuous"</code> indicating which treatment types are
available for this method.</p>
</dd>
<dt><code>estimand</code></dt><dd><p>which estimands are available for this method. All methods that support binary and multi-category treatments accept <code>"ATE"</code>, <code>"ATT"</code>, and <code>"ATC"</code>, as well as some other estimands depending on the method. See <code><a href="#topic+get_w_from_ps">get_w_from_ps()</a></code> for more details about what each estimand means.</p>
</dd>
<dt><code>alias</code></dt><dd><p>a character vector of aliases for the method. When an alias is supplied, the corresponding method will still be dispatched. For example, the canonical method to request entropy balancing is <code>"ebal"</code>, but <code>"ebalance"</code> and <code>"entropy"</code> also work. The first value is the canonical name.</p>
</dd>
<dt><code>description</code></dt><dd><p>a string containing the description of the name in English.</p>
</dd>
<dt><code>ps</code></dt><dd><p>a logical for whether propensity scores are returned by the method for binary treatments. Propensity scores are never returned for multi-category or continuous treatments.</p>
</dd>
<dt><code>msm_valid</code></dt><dd><p>a logical for whether the method can be validly used with longitudinal treatments.</p>
</dd>
<dt><code>msm_method_available</code></dt><dd><p>a logical for whether a version of the method can be used that estimates weights using a single model rather than multiplying the weights across time points. This is related to the <code>is.MSM.method</code> argument of <code>weightitMSM()</code>.</p>
</dd>
<dt><code>subclass_ok</code></dt><dd><p>a logical for whether <code>subclass</code> can be supplied to compute subclassification weights from the propensity scores.</p>
</dd>
<dt><code>packages_needed</code></dt><dd><p>a character vector of the minimal packages required to use the method. Some methods may require additional packages for certain options.</p>
</dd>
<dt><code>s.weights_ok</code></dt><dd><p>a logical for whether sampling weights can be used with the method.</p>
</dd>
<dt><code>missing</code></dt><dd><p>a character vector of the allowed options that can be supplied to <code>missing</code> when missing data is present. All methods accept <code>"ind"</code> for the missingness indicator approach; some other methods accept additional values.</p>
</dd>
<dt><code>moments_int_ok</code></dt><dd><p>a logical for whether <code>moments</code>, <code>int</code>, and <code>quantile</code> can be used with the method.</p>
</dd>
<dt><code>moments_default</code></dt><dd><p>when <code>moments_int_ok</code> is <code>TRUE</code>, the default value of <code>moments</code> used with the method. For most methods, this is 1.</p>
</dd>
<dt><code>density_ok</code></dt><dd><p>a logical for whether arguments that control the density can be used with the method when used with a continuous treatment.</p>
</dd>
<dt><code>stabilize_ok</code></dt><dd><p>a logical for whether the <code>stabilize</code> argument (and <code>num.formula</code> for longitudinal treatments) can be used with the method.</p>
</dd>
<dt><code>plot.weightit_ok</code></dt><dd><p>a logical for whether <code>plot()</code> can be used on the <code>weightit</code> output with the method.</p>
</dd>
</dl>



<h3>See Also</h3>

<p><code><a href="#topic+weightit">weightit()</a></code> and <code><a href="#topic+weightitMSM">weightitMSM()</a></code> for how the methods are used. Also
see the individual methods pages for information on whether and how each
option can be used.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Get all acceptable names
names(.weightit_methods)

# Get all acceptable names and aliases
lapply(.weightit_methods, `[[`, "alias")

# Which estimands are allowed with `method = "bart"`
.weightit_methods[["bart"]]$estimand

# All methods that support continuous treatments
supp &lt;- sapply(.weightit_methods, function(x) {
  "continuous" %in% x$treat_type
})
names(.weightit_methods)[supp]

# All methods that return propensity scores (for
# binary treatments only)
supp &lt;- sapply(.weightit_methods, `[[`, "ps")
names(.weightit_methods)[supp]
</code></pre>

<hr>
<h2 id='anova.glm_weightit'>Methods for <code>glm_weightit()</code> objects</h2><span id='topic+anova.glm_weightit'></span>

<h3>Description</h3>

<p><code>anova()</code> is used to compare nested models fit with
<code>glm_weightit()</code>, <code>mutinom_weightit()</code>, <code>ordinal_weightit()</code>, or
<code>coxph_weightit()</code> using a Wald test that incorporates uncertainty in
estimating the weights (if any).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'glm_weightit'
anova(
  object,
  object2,
  test = "Chisq",
  method = "Wald",
  tolerance = 1e-07,
  vcov = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="anova.glm_weightit_+3A_object">object</code>, <code id="anova.glm_weightit_+3A_object2">object2</code></td>
<td>
<p>an output from one of the above modeling functions.
<code>object2</code> is required.</p>
</td></tr>
<tr><td><code id="anova.glm_weightit_+3A_test">test</code></td>
<td>
<p>the type of test statistic used to compare models. Currently only
<code>"Chisq"</code> (the chi-square statistic) is allowed.</p>
</td></tr>
<tr><td><code id="anova.glm_weightit_+3A_method">method</code></td>
<td>
<p>the kind of test used to compare models. Currently only
<code>"Wald"</code> is allowed.</p>
</td></tr>
<tr><td><code id="anova.glm_weightit_+3A_tolerance">tolerance</code></td>
<td>
<p>for the Wald test, the tolerance used to determine if models
are symbolically nested.</p>
</td></tr>
<tr><td><code id="anova.glm_weightit_+3A_vcov">vcov</code></td>
<td>
<p>either a string indicating the method used to compute the
variance of the estimated parameters for <code>object</code>, a function used to
extract the variance, or the variance matrix itself. Default is to use the
variance matrix already present in <code>object</code>. If a string or function,
arguments passed to <code>...</code> are supplied to the method or function. (Note:
for <code>vcov()</code>, can also be supplied as <code>type</code>.)</p>
</td></tr>
<tr><td><code id="anova.glm_weightit_+3A_...">...</code></td>
<td>
<p>other arguments passed to the function used for computing the
parameter variance matrix, if supplied as a string or function, e.g.,
<code>cluster</code>, <code>R</code>, or <code>fwb.args</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>anova()</code> performs a Wald test to compare two fitted models. The
models must be nested, but they don't have to be nested symbolically (i.e.,
the names of the coefficients of the smaller model do not have to be a subset
of the names of the coefficients of the larger model). The larger model must
be supplied to <code>object</code> and the smaller to <code>object2</code>. Both models must
contain the same units, weights (if any), and outcomes. The
variance-covariance matrix of the coefficients of the smaller model is not
used.
</p>


<h3>Value</h3>

<p>An object of class <code>"anova"</code> inheriting from class <code>"data.frame"</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+glm_weightit">glm_weightit()</a></code> for the page documenting <code>glm_weightit()</code>,
<code>lm_weightit()</code>, <code>ordinal_weightit()</code>, <code>multinom_weightit()</code>, and
<code>coxph_weightit()</code>. <code><a href="stats.html#topic+anova.glm">anova.glm()</a></code> for model comparison of <code>glm</code> objects.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("lalonde", package = "cobalt")

# Model comparison for any relationship between `treat`
# and `re78` (not the same as testing for the ATE)
fit1 &lt;- glm_weightit(
  re78 ~ treat * (age + educ + race + married + nodegree +
                    re74 + re75), data = lalonde
)

fit2 &lt;- glm_weightit(
  re78 ~ age + educ + race + married + nodegree +
    re74 + re75, data = lalonde
)

anova(fit1, fit2)

# Using the usual maximum likelihood variance matrix
anova(fit1, fit2, vcov = "const")

# Using a bootstrapped variance matrix
anova(fit1, fit2, vcov = "BS", R = 100)


# Model comparison between spline model and linear
# model; note they are nested but not symbolically
# nested
fit_s &lt;- glm_weightit(re78 ~ splines::ns(age, df =4),
                      data = lalonde )

fit_l &lt;- glm_weightit( re78 ~ age, data = lalonde )

anova(fit_s, fit_l)

</code></pre>

<hr>
<h2 id='as.weightit'>Create a <code>weightit</code> object manually</h2><span id='topic+as.weightit'></span><span id='topic+as.weightit.weightit.fit'></span><span id='topic+as.weightit.default'></span><span id='topic+as.weightitMSM'></span><span id='topic+as.weightitMSM.default'></span>

<h3>Description</h3>

<p>This function allows users to get the benefits of a <code>weightit</code>
object when using weights not estimated with <code><a href="#topic+weightit">weightit()</a></code> or <code><a href="#topic+weightitMSM">weightitMSM()</a></code>.
These benefits include diagnostics, plots, and direct compatibility with
<span class="pkg">cobalt</span> for assessing balance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as.weightit(x, ...)

## S3 method for class 'weightit.fit'
as.weightit(x, covs = NULL, ...)

## Default S3 method:
as.weightit(
  x,
  treat,
  covs = NULL,
  estimand = NULL,
  s.weights = NULL,
  ps = NULL,
  ...
)

as.weightitMSM(x, ...)

## Default S3 method:
as.weightitMSM(
  x,
  treat.list,
  covs.list = NULL,
  estimand = NULL,
  s.weights = NULL,
  ps.list = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="as.weightit_+3A_x">x</code></td>
<td>
<p>required; a <code>numeric</code> vector of weights, one for each unit, or a
<code>weightit.fit</code> object from <code><a href="#topic+weightit.fit">weightit.fit()</a></code>.</p>
</td></tr>
<tr><td><code id="as.weightit_+3A_...">...</code></td>
<td>
<p>additional arguments. These must be named. They will be included
in the output object.</p>
</td></tr>
<tr><td><code id="as.weightit_+3A_covs">covs</code></td>
<td>
<p>an optional <code>data.frame</code> of covariates. For using <span class="pkg">WeightIt</span>
functions, this is not necessary, but for use with <span class="pkg">cobalt</span> it is. Note
that when using with a <code>weightit.fit</code> object, this should not be the matrix
supplied to the <code>covs</code> argument of <code>weightit.fit()</code> unless there are no
factor/character variables in it. Ideally this is the original, unprocessed
covariate data frame with factor variables included.</p>
</td></tr>
<tr><td><code id="as.weightit_+3A_treat">treat</code></td>
<td>
<p>a vector of treatment statuses, one for each unit. Required when
<code>x</code> is a vector of weights.</p>
</td></tr>
<tr><td><code id="as.weightit_+3A_estimand">estimand</code></td>
<td>
<p>an optional <code>character</code> of length 1 giving the estimand. The
text is not checked.</p>
</td></tr>
<tr><td><code id="as.weightit_+3A_s.weights">s.weights</code></td>
<td>
<p>an optional <code>numeric</code> vector of sampling weights, one for
each unit.</p>
</td></tr>
<tr><td><code id="as.weightit_+3A_ps">ps</code></td>
<td>
<p>an optional <code>numeric</code> vector of propensity scores, one for each
unit.</p>
</td></tr>
<tr><td><code id="as.weightit_+3A_treat.list">treat.list</code></td>
<td>
<p>a list of treatment statuses at each time point.</p>
</td></tr>
<tr><td><code id="as.weightit_+3A_covs.list">covs.list</code></td>
<td>
<p>an optional list of <code>data.frame</code>s of covariates of
covariates at each time point. For using <span class="pkg">WeightIt</span> functions, this is
not necessary, but for use with <span class="pkg">cobalt</span> it is.</p>
</td></tr>
<tr><td><code id="as.weightit_+3A_ps.list">ps.list</code></td>
<td>
<p>an optional list of <code>numeric</code> vectors of propensity scores at
each time point.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>weightit</code> (for <code>as.weightit()</code>) or <code>weightitMSM</code>
(for <code>as.weightitMSM()</code>).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
treat &lt;- rbinom(500, 1, .3)
weights &lt;- rchisq(500, df = 2)

W &lt;- as.weightit(weights, treat = treat, estimand = "ATE")
summary(W)

# See ?weightit.fit for using as.weightit() with a
# weightit.fit object.

</code></pre>

<hr>
<h2 id='calibrate'>Calibrate Propensity Score Weights</h2><span id='topic+calibrate'></span><span id='topic+calibrate.default'></span><span id='topic+calibrate.weightit'></span>

<h3>Description</h3>

<p><code>calibrate()</code> calibrates propensity scores used in weights. This
involves fitting a new propensity score model using logistic or isotonic
regression with the previously estimated propensity score as the sole
predictor. Weights are computed using this new propensity score.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calibrate(x, ...)

## Default S3 method:
calibrate(x, treat, s.weights = NULL, data = NULL, method = "platt", ...)

## S3 method for class 'weightit'
calibrate(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="calibrate_+3A_x">x</code></td>
<td>
<p>A <code>weightit</code> object or a vector of propensity scores. Only binary
treatments are supported.</p>
</td></tr>
<tr><td><code id="calibrate_+3A_...">...</code></td>
<td>
<p>Not used.</p>
</td></tr>
<tr><td><code id="calibrate_+3A_treat">treat</code></td>
<td>
<p>A vector of treatment status for each unit. Only binary
treatments are supported.</p>
</td></tr>
<tr><td><code id="calibrate_+3A_s.weights">s.weights</code></td>
<td>
<p>A vector of sampling weights or the name of a variable in
<code>data</code> that contains sampling weights.</p>
</td></tr>
<tr><td><code id="calibrate_+3A_data">data</code></td>
<td>
<p>An optional data frame containing the variable named in
<code>s.weights</code> when supplied as a string.</p>
</td></tr>
<tr><td><code id="calibrate_+3A_method">method</code></td>
<td>
<p><code>character</code>; the method of calibration used. Allowable options
include <code>"platt"</code> (default) for Platt scaling as described by Gutman et al.
(2024) and <code>"isoreg"</code> for isotonic regression as described by van der Laan
et al. (2024) and implemented in <code><a href="stats.html#topic+isoreg">isoreg()</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>If the input is a <code>weightit</code> object, the output will be a <code>weightit</code>
object with the propensity scores replaced with the calibrated propensity
scores and the weights replaced by weights computed from the calibrated
propensity scores.
</p>
<p>If the input is a numeric vector of weights, the output will be a numeric
vector of the calibrated propensity scores.
</p>


<h3>References</h3>

<p>Gutman, R., Karavani, E., &amp; Shimoni, Y. (2024). Improving Inverse
Probability Weighting by Post-calibrating Its Propensity Scores.
<em>Epidemiology</em>, 35(4). <a href="https://doi.org/10.1097/EDE.0000000000001733">doi:10.1097/EDE.0000000000001733</a>
</p>
<p>van der Laan, L., Lin, Z., Carone, M., &amp; Luedtke, A. (2024). Stabilized
Inverse Probability Weighting via Isotonic Calibration (arXiv:2411.06342).
arXiv. <a href="http://arxiv.org/abs/2411.06342">http://arxiv.org/abs/2411.06342</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+weightit">weightit()</a></code>, <code><a href="#topic+weightitMSM">weightitMSM()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library("cobalt")
data("lalonde", package = "cobalt")

#Using GBM to estimate weights
(W &lt;- weightit(treat ~ age + educ + married +
                 nodegree + re74, data = lalonde,
               method = "gbm", estimand = "ATT",
               criterion = "smd.max"))
summary(W)

#Calibrating the GBM propensity scores
Wc &lt;- calibrate(W)

#Calibrating propensity scores directly
PSc &lt;- calibrate(W$ps, treat = lalonde$treat)

</code></pre>

<hr>
<h2 id='ESS'>Compute effective sample size of weighted sample</h2><span id='topic+ESS'></span>

<h3>Description</h3>

<p>Computes the effective sample size (ESS) of a weighted sample,
which represents the size of an unweighted sample with approximately the same
amount of precision as the weighted sample under consideration.
</p>
<p>The ESS is calculated as <code class="reqn">(\sum w)^2/\sum w^2</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ESS(w)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ESS_+3A_w">w</code></td>
<td>
<p>a vector of weights</p>
</td></tr>
</table>


<h3>References</h3>

<p>McCaffrey, D. F., Ridgeway, G., &amp; Morral, A. R. (2004).
Propensity Score Estimation With Boosted Regression for Evaluating Causal
Effects in Observational Studies. Psychological Methods, 9(4), 403–425.
<a href="https://doi.org/10.1037/1082-989X.9.4.403">doi:10.1037/1082-989X.9.4.403</a>
</p>
<p>Shook‐Sa, B. E., &amp; Hudgens, M. G. (2020). Power and sample size for
observational studies of point exposure effects. Biometrics, biom.13405.
<a href="https://doi.org/10.1111/biom.13405">doi:10.1111/biom.13405</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+summary.weightit">summary.weightit()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library("cobalt")
data("lalonde", package = "cobalt")

#Balancing covariates between treatment groups (binary)
(W1 &lt;- weightit(treat ~ age + educ + married +
                  nodegree + re74, data = lalonde,
                method = "glm", estimand = "ATE"))
summary(W1)
ESS(W1$weights[W1$treat == 0])
ESS(W1$weights[W1$treat == 1])
</code></pre>

<hr>
<h2 id='get_w_from_ps'>Compute weights from propensity scores</h2><span id='topic+get_w_from_ps'></span>

<h3>Description</h3>

<p>Given a vector or matrix of propensity scores, outputs a vector
of weights that target the provided estimand.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_w_from_ps(
  ps,
  treat,
  estimand = "ATE",
  focal = NULL,
  treated = NULL,
  subclass = NULL,
  stabilize = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_w_from_ps_+3A_ps">ps</code></td>
<td>
<p>a vector, matrix, or data frame of propensity scores. See Details.</p>
</td></tr>
<tr><td><code id="get_w_from_ps_+3A_treat">treat</code></td>
<td>
<p>a vector of treatment status for each individual. See Details.</p>
</td></tr>
<tr><td><code id="get_w_from_ps_+3A_estimand">estimand</code></td>
<td>
<p>the desired estimand that the weights should target. Current
options include <code>"ATE"</code> (average treatment effect), <code>"ATT"</code> (average
treatment effect on the treated), <code>"ATC"</code> (average treatment effect on the
control), <code>"ATO"</code> (average treatment effect in the overlap), <code>"ATM"</code>
(average treatment effect in the matched sample), and <code>"ATOS"</code> (average
treatment effect in the optimal subset). See Details.</p>
</td></tr>
<tr><td><code id="get_w_from_ps_+3A_focal">focal</code></td>
<td>
<p>when <code>estimand</code> is <code>"ATT"</code> or <code>"ATC"</code>, which group should be
consider the (focal) &quot;treated&quot; or &quot;control&quot; group, respectively. If not
<code>NULL</code> and <code>estimand</code> is not <code>"ATT"</code> or <code>"ATC"</code>, <code>estimand</code> will
automatically be set to <code>"ATT"</code>.</p>
</td></tr>
<tr><td><code id="get_w_from_ps_+3A_treated">treated</code></td>
<td>
<p>when treatment is binary, the value of <code>treat</code> that is
considered the &quot;treated&quot; group (i.e., the group for which the propensity
scores are the probability of being in). If <code>NULL</code>, <code>get_w_from_ps()</code> will
attempt to figure it out on its own using some heuristics. This really only
matters when <code>treat</code> has values other than 0 and 1 and when <code>ps</code> is given
as a vector or an unnamed single-column matrix or data frame.</p>
</td></tr>
<tr><td><code id="get_w_from_ps_+3A_subclass">subclass</code></td>
<td>
<p><code>numeric</code>; the number of subclasses to use when computing
weights using marginal mean weighting through stratification (also known as
fine stratification). If <code>NULL</code>, standard inverse probability weights (and
their extensions) will be computed; if a number greater than 1, subclasses
will be formed and weights will be computed based on subclass membership.
<code>estimand</code> must be <code>"ATE"</code>, <code>"ATT"</code>, or <code>"ATC"</code> if <code>subclass</code> is
non-<code>NULL</code>. See Details.</p>
</td></tr>
<tr><td><code id="get_w_from_ps_+3A_stabilize">stabilize</code></td>
<td>
<p><code>logical</code>; whether to compute stabilized weights or not.
This simply involves multiplying each unit's weight by the proportion of
units in their treatment group. For saturated outcome models and in balance
checking, this won't make a difference; otherwise, this can improve
performance.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>get_w_from_ps()</code> applies the formula for computing weights from
propensity scores for the desired estimand. The formula for each estimand is
below, with <code class="reqn">A_i</code> the treatment value for unit <code class="reqn">i</code> taking on values
<code class="reqn">\mathcal{A} = (1, \ldots, g)</code>, <code class="reqn">p_{a, i}</code> the probability of
receiving treatment level <code class="reqn">a</code> for unit <code class="reqn">i</code>, and <code class="reqn">f</code> is the focal
group (the treated group for the ATT and the control group for the ATC):
</p>
<p style="text-align: center;"><code class="reqn">
\begin{aligned}
w^{ATE}_i &amp;= 1 / p_{A_i, i} \\
w^{ATT}_i &amp;= w^{ATE}_i \times p_{f, i} \\
w^{ATO}_i &amp;= w^{ATE}_i / \sum_{a \in \mathcal{A}}{1/p_{a, i}} \\
w^{ATM}_i &amp;= w^{ATE}_i \times \min(p_{1, i}, \ldots, p_{g, i}) \\
w^{ATOS}_i &amp;= w^{ATE}_i \times \mathbb{1}\left(\alpha &lt; p_{2, i} &lt; 1 - \alpha\right)
\end{aligned}
</code>
</p>

<p><code>get_w_from_ps()</code> can only be used with binary and multi-category treatments.
</p>


<h4>Supplying the <code>ps</code> argument</h4>

<p>The <code>ps</code> argument can be entered in two ways:
</p>

<ul>
<li><p> A numeric matrix with a row for each unit and a (named) column for each treatment level, with each cell corresponding to the probability of receiving the corresponding treatment level
</p>
</li>
<li><p> A numeric vector with a value for each unit corresponding to the probability of being &quot;treated&quot; (only allowed for binary treatments)
</p>
</li></ul>

<p>When supplied as a vector, <code>get_w_from_ps()</code> has to know which value of
<code>treat</code> corresponds to the &quot;treated&quot; group. For 0/1 variables, 1 will be
considered treated. For other types of variables, <code>get_w_from_ps()</code> will try
to figure it out using heuristics, but it's safer to supply an argument to
<code>treated</code>. When <code>estimand</code> is <code>"ATT"</code> or <code>"ATC"</code>, supplying a value to
<code>focal</code> is sufficient (for ATT, <code>focal</code> is the treated group, and for ATC,
<code>focal</code> is the control group).
</p>
<p>When supplied as a matrix, the columns must be named with the levels of the
treatment, and it is assumed that each column corresponds to the probability
of being in that treatment group. This is the safest way to supply <code>ps</code>
unless <code>treat</code> is a 0/1 variable. When <code>estimand</code> is <code>"ATT"</code> or <code>"ATC"</code>, a
value for <code>focal</code> must be specified.
</p>



<h4>Marginal mean weighting through stratification (MMWS)</h4>

<p>When <code>subclass</code> is not <code>NULL</code>, MMWS weights are computed. The implementation
differs slightly from that described in Hong (2010, 2012). First, subclasses
are formed by finding the quantiles of the propensity scores in the target
group (for the ATE, all units; for the ATT or ATC, just the units in the
focal group). Any subclasses lacking members of a treatment group will be
filled in with them from neighboring subclasses so each subclass will always
have at least one member of each treatment group. A new subclass-propensity
score matrix is formed, where each unit's subclass-propensity score for each
treatment value is computed as the proportion of units with that treatment
value in the unit's subclass. For example, if a subclass had 10 treated units
and 90 control units in it, the subclass-propensity score for being treated
would be .1 and the subclass-propensity score for being control would be .9
for all units in the subclass.
</p>
<p>For multi-category treatments, the propensity scores for each treatment are
stratified separately as described in Hong (2012); for binary treatments,
only one set of propensity scores are stratified and the subclass-propensity
scores for the other treatment are computed as the complement of the
propensity scores for the stratified treatment.
</p>
<p>After the subclass-propensity scores have been computed, the standard
propensity score weighting formulas are used to compute the unstabilized MMWS
weights. To estimate MMWS weights equivalent to those described in Hong
(2010, 2012), <code>stabilize</code> must be set to <code>TRUE</code>, but, as with standard
propensity score weights, this is optional. Note that MMWS weights are also
known as fine stratification weights and described by Desai et al. (2017).
</p>



<h3>Value</h3>

<p>A vector of weights. When <code>subclass</code> is not <code>NULL</code>, the subclasses
are returned as the <code>"subclass"</code> attribute. When <code>estimand = "ATOS"</code>, the
chosen value of <code>alpha</code> (the smallest propensity score allowed to remain in
the sample) is returned in the <code>"alpha"</code> attribute.
</p>


<h3>References</h3>



<h4>Binary treatments</h4>


<ul>
<li> <p><code>estimand = "ATO"</code>
</p>
</li></ul>

<p>Li, F., Morgan, K. L., &amp; Zaslavsky, A. M. (2018). Balancing covariates via
propensity score weighting. Journal of the American Statistical Association,
113(521), 390–400. <a href="https://doi.org/10.1080/01621459.2016.1260466">doi:10.1080/01621459.2016.1260466</a>
</p>

<ul>
<li> <p><code>estimand = "ATM"</code>
</p>
</li></ul>

<p>Li, L., &amp; Greene, T. (2013). A Weighting Analogue to Pair Matching in
Propensity Score Analysis. The International Journal of Biostatistics, 9(2).
<a href="https://doi.org/10.1515/ijb-2012-0030">doi:10.1515/ijb-2012-0030</a>
</p>

<ul>
<li> <p><code>estimand = "ATOS"</code>
</p>
</li></ul>

<p>Crump, R. K., Hotz, V. J., Imbens, G. W., &amp; Mitnik, O. A. (2009). Dealing
with limited overlap in estimation of average treatment effects. Biometrika,
96(1), 187–199. <a href="https://doi.org/10.1093/biomet/asn055">doi:10.1093/biomet/asn055</a>
</p>

<ul>
<li><p> Other estimands
</p>
</li></ul>

<p>Austin, P. C. (2011). An Introduction to Propensity Score Methods for
Reducing the Effects of Confounding in Observational Studies. Multivariate
Behavioral Research, 46(3), 399–424. <a href="https://doi.org/10.1080/00273171.2011.568786">doi:10.1080/00273171.2011.568786</a>
</p>

<ul>
<li><p> Marginal mean weighting through stratification (MMWS)
</p>
</li></ul>

<p>Hong, G. (2010). Marginal mean weighting through stratification: Adjustment
for selection bias in multilevel data. Journal of Educational and Behavioral
Statistics, 35(5), 499–531. <a href="https://doi.org/10.3102/1076998609359785">doi:10.3102/1076998609359785</a>
</p>
<p>Desai, R. J., Rothman, K. J., Bateman, B. . T., Hernandez-Diaz, S., &amp;
Huybrechts, K. F. (2017). A Propensity-score-based Fine Stratification
Approach for Confounding Adjustment When Exposure Is Infrequent:
Epidemiology, 28(2), 249–257. <a href="https://doi.org/10.1097/EDE.0000000000000595">doi:10.1097/EDE.0000000000000595</a>
</p>



<h4>Multi-Category Treatments</h4>


<ul>
<li> <p><code>estimand = "ATO"</code>
</p>
</li></ul>

<p>Li, F., &amp; Li, F. (2019). Propensity score weighting for causal inference with
multiple treatments. The Annals of Applied Statistics, 13(4), 2389–2415.
<a href="https://doi.org/10.1214/19-AOAS1282">doi:10.1214/19-AOAS1282</a>
</p>

<ul>
<li> <p><code>estimand = "ATM"</code>
</p>
</li></ul>

<p>Yoshida, K., Hernández-Díaz, S., Solomon, D. H., Jackson, J. W., Gagne, J.
J., Glynn, R. J., &amp; Franklin, J. M. (2017). Matching weights to
simultaneously compare three treatment groups: Comparison to three-way
matching. Epidemiology (Cambridge, Mass.), 28(3), 387–395.
<a href="https://doi.org/10.1097/EDE.0000000000000627">doi:10.1097/EDE.0000000000000627</a>
</p>

<ul>
<li><p> Other estimands
</p>
</li></ul>

<p>McCaffrey, D. F., Griffin, B. A., Almirall, D., Slaughter, M. E., Ramchand,
R., &amp; Burgette, L. F. (2013). A Tutorial on Propensity Score Estimation for
Multiple Treatments Using Generalized Boosted Models. Statistics in Medicine,
32(19), 3388–3414. <a href="https://doi.org/10.1002/sim.5753">doi:10.1002/sim.5753</a>
</p>

<ul>
<li><p> Marginal mean weighting through stratification
</p>
</li></ul>

<p>Hong, G. (2012). Marginal mean weighting through stratification: A
generalized method for evaluating multivalued and multiple treatments with
nonexperimental data. <em>Psychological Methods</em>, 17(1), 44–60.
<a href="https://doi.org/10.1037/a0024918">doi:10.1037/a0024918</a>
</p>



<h3>See Also</h3>

<p><code><a href="#topic+method_glm">method_glm</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library("cobalt")
data("lalonde", package = "cobalt")

ps.fit &lt;- glm(treat ~ age + educ + race + married +
                nodegree + re74 + re75, data = lalonde,
              family = binomial)
ps &lt;- ps.fit$fitted.values

w1 &lt;- get_w_from_ps(ps, treat = lalonde$treat,
                    estimand = "ATT")

treatAB &lt;- factor(ifelse(lalonde$treat == 1, "A", "B"))
w2 &lt;- get_w_from_ps(ps, treat = treatAB,
                    estimand = "ATT", focal = "A")
all.equal(w1, w2)
w3 &lt;- get_w_from_ps(ps, treat = treatAB,
                    estimand = "ATT", treated = "A")
all.equal(w1, w3)

# Using MMWS
w4 &lt;- get_w_from_ps(ps, treat = lalonde$treat,
                    estimand = "ATE", subclass = 20,
                    stabilize = TRUE)

# A multi-category example using predicted probabilities
# from multinomial logistic regression
T3 &lt;- factor(sample(c("A", "B", "C"), nrow(lalonde),
                    replace = TRUE))

multi.fit &lt;- multinom_weightit(
  T3 ~ age + educ + race + married +
    nodegree + re74 + re75, data = lalonde,
  vcov = "none"
)

ps.multi &lt;- fitted(multi.fit)
head(ps.multi)

w5 &lt;- get_w_from_ps(ps.multi, treat = T3,
                    estimand = "ATE")
</code></pre>

<hr>
<h2 id='glm_weightit'>Fitting Weighted Generalized Linear Models</h2><span id='topic+glm_weightit'></span><span id='topic+lm_weightit'></span><span id='topic+ordinal_weightit'></span><span id='topic+multinom_weightit'></span><span id='topic+coxph_weightit'></span>

<h3>Description</h3>

<p><code>glm_weightit()</code> is used to fit generalized linear models with a
covariance matrix that accounts for estimation of weights, if supplied.
<code>lm_weightit()</code> is a wrapper for <code>glm_weightit()</code> with the Gaussian family
and identity link (i.e., a linear model). <code>ordinal_weightit()</code> fits
proportional odds ordinal regression models. <code>multinom_weightit()</code> fits
multinomial logistic regression models. <code>coxph_weightit()</code> fits a Cox
proportional hazards model and is a wrapper for <code><a href="survival.html#topic+coxph">survival::coxph()</a></code>. By
default, these functions use M-estimation to construct a robust covariance
matrix using the estimating equations for the weighting model and the outcome
model when available.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>glm_weightit(
  formula,
  data,
  family = gaussian,
  weightit = NULL,
  vcov = NULL,
  cluster,
  R = 500L,
  offset,
  start = NULL,
  etastart,
  mustart,
  control = list(...),
  x = FALSE,
  y = TRUE,
  contrasts = NULL,
  fwb.args = list(),
  br = FALSE,
  ...
)

lm_weightit(
  formula,
  data,
  weightit = NULL,
  vcov = NULL,
  cluster,
  R = 500L,
  offset,
  x = FALSE,
  y = TRUE,
  contrasts = NULL,
  fwb.args = list(),
  ...
)

ordinal_weightit(
  formula,
  data,
  link = "logit",
  weightit = NULL,
  vcov = NULL,
  cluster,
  R = 500L,
  offset,
  start = NULL,
  control = list(...),
  x = FALSE,
  y = TRUE,
  contrasts = NULL,
  fwb.args = list(),
  ...
)

multinom_weightit(
  formula,
  data,
  link = "logit",
  weightit = NULL,
  vcov = NULL,
  cluster,
  R = 500L,
  offset,
  start = NULL,
  control = list(...),
  x = FALSE,
  y = TRUE,
  contrasts = NULL,
  fwb.args = list(),
  ...
)

coxph_weightit(
  formula,
  data,
  weightit = NULL,
  vcov = NULL,
  cluster,
  R = 500L,
  x = FALSE,
  y = TRUE,
  fwb.args = list(),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="glm_weightit_+3A_formula">formula</code></td>
<td>
<p>an object of class &quot;formula&quot; (or one that can be coerced to
that class): a symbolic description of the model to be fitted. For
<code>coxph_weightit()</code>, see <code><a href="survival.html#topic+coxph">survival::coxph()</a></code> for how this should be
specified.</p>
</td></tr>
<tr><td><code id="glm_weightit_+3A_data">data</code></td>
<td>
<p>a data frame containing the variables in the model. If not found
in data, the variables are taken from <code>environment(formula)</code>, typically the
environment from which the function is called.</p>
</td></tr>
<tr><td><code id="glm_weightit_+3A_family">family</code></td>
<td>
<p>a description of the error distribution and link function to be
used in the model. This can be a character string naming a family function,
a family function or the result of a call to a family function. See
<a href="stats.html#topic+family">family</a> for details of family functions.</p>
</td></tr>
<tr><td><code id="glm_weightit_+3A_weightit">weightit</code></td>
<td>
<p>a <code>weightit</code> or <code>weightitMSM</code> object; the output of a call to
<code><a href="#topic+weightit">weightit()</a></code> or <code><a href="#topic+weightitMSM">weightitMSM()</a></code>. If not supplied, an unweighted model will
be fit.</p>
</td></tr>
<tr><td><code id="glm_weightit_+3A_vcov">vcov</code></td>
<td>
<p>string; the method used to compute the variance of the estimated
parameters. Allowable options include <code>"asympt"</code>, which uses the
asymptotically correct M-estimation-based method that accounts for
estimation of the weights when available; <code>"const"</code>, which uses the usual
maximum likelihood estimates (only available when <code>weightit</code> is not
supplied); <code>"HC0"</code>, which computes the robust sandwich variance treating
weights (if supplied) as fixed; <code>"BS"</code>, which uses the traditional
bootstrap (including re-estimation of the weights, if supplied); <code>"FWB"</code>,
which uses the fractional weighted bootstrap as implemented in
<code><a href="fwb.html#topic+fwb">fwb::fwb()</a></code> (including re-estimation of the weights, if supplied);
and <code>"none"</code> to omit calculation of a variance matrix. If <code>NULL</code> (the
default), will use <code>"asympt"</code> if <code>weightit</code> is supplied and M-estimation is
available and <code>"HC0"</code> otherwise. See the <code>vcov_type</code> component of the
outcome object to see which was used.</p>
</td></tr>
<tr><td><code id="glm_weightit_+3A_cluster">cluster</code></td>
<td>
<p>optional; for computing a cluster-robust variance matrix, a
variable indicating the clustering of observations, a list (or data frame)
thereof, or a one-sided formula specifying which variable(s) from the
fitted model should be used. Note the cluster-robust variance matrix uses a
correction for small samples, as is done in <code>sandwich::vcovCL()</code> by
default. Cluster-robust variance calculations are available only when
<code>vcov</code> is <code>"asympt"</code>, <code>"HC0"</code>, <code>"BS"</code>, or <code>"FWB"</code>.</p>
</td></tr>
<tr><td><code id="glm_weightit_+3A_r">R</code></td>
<td>
<p>the number of bootstrap replications when <code>vcov</code> is <code>"BS"</code> or
<code>"FWB"</code>. Default is 500. Ignored otherwise.</p>
</td></tr>
<tr><td><code id="glm_weightit_+3A_offset">offset</code></td>
<td>
<p>optional; a numeric vector containing the model offset. See
<code><a href="stats.html#topic+offset">offset()</a></code>. An offset can also be preset in the model formula.</p>
</td></tr>
<tr><td><code id="glm_weightit_+3A_start">start</code></td>
<td>
<p>optional starting values for the coefficients.</p>
</td></tr>
<tr><td><code id="glm_weightit_+3A_etastart">etastart</code>, <code id="glm_weightit_+3A_mustart">mustart</code></td>
<td>
<p>optional starting values for the linear predictor and
vector of means. Passed to <code><a href="stats.html#topic+glm">glm()</a></code>.</p>
</td></tr>
<tr><td><code id="glm_weightit_+3A_control">control</code></td>
<td>
<p>a list of parameters for controlling the fitting process.</p>
</td></tr>
<tr><td><code id="glm_weightit_+3A_x">x</code>, <code id="glm_weightit_+3A_y">y</code></td>
<td>
<p>logical values indicating whether the response vector and model
matrix used in the fitting process should be returned as components of the
returned value.</p>
</td></tr>
<tr><td><code id="glm_weightit_+3A_contrasts">contrasts</code></td>
<td>
<p>an optional list defining contrasts for factor variables.
See <code><a href="stats.html#topic+model.matrix">model.matrix()</a></code>.</p>
</td></tr>
<tr><td><code id="glm_weightit_+3A_fwb.args">fwb.args</code></td>
<td>
<p>an optional list of further arguments to supply to
<code><a href="fwb.html#topic+fwb">fwb::fwb()</a></code> when <code>vcov = "FWB"</code>.</p>
</td></tr>
<tr><td><code id="glm_weightit_+3A_br">br</code></td>
<td>
<p><code>logical</code>; whether to use bias-reduced regression as implemented by
<code><a href="brglm2.html#topic+brglmFit">brglm2::brglmFit()</a></code>. If <code>TRUE</code>, arguments passed to <code>control</code> or
... will be passed to <code><a href="brglm2.html#topic+brglmControl">brglm2::brglmControl()</a></code>.</p>
</td></tr>
<tr><td><code id="glm_weightit_+3A_...">...</code></td>
<td>
<p>arguments to be used to form the default control argument if it
is not supplied directly.</p>
</td></tr>
<tr><td><code id="glm_weightit_+3A_link">link</code></td>
<td>
<p>for <code>plor_weightit()</code> and <code>multinom_weightit()</code>, a string
corresponding to the desired link function. For <code>ordinal_weightit()</code>, any
allowed by <code><a href="stats.html#topic+binomial">binomial()</a></code> are accepted; for <code>multinom_weightit()</code>, only
<code>"logit"</code> is allowed. Default is <code>"logit"</code> for ordinal or multinomial
logistic regression, respectively.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code><a href="#topic+glm_weightit">glm_weightit()</a></code> is essentially a wrapper for <code><a href="stats.html#topic+glm">glm()</a></code> that
optionally computes a coefficient variance matrix that can be adjusted to
account for estimation of the weights if a <code>weightit</code> or <code>weightitMSM</code> object
is supplied to the <code>weightit</code> argument. When no argument is supplied to
<code>weightit</code> or there is no <code>"Mparts"</code> attribute in the supplied object, the
default variance matrix returned will be the &quot;HC0&quot; sandwich variance matrix,
which is robust to misspecification of the outcome family (including
heteroscedasticity). Otherwise, the default variance matrix uses M-estimation
to additionally adjust for estimation of the weights. When possible, this
often yields smaller (and more accurate) standard errors. See the individual
methods pages to see whether and when an <code>"Mparts"</code> attribute is included in
the supplied object. To request that a variance matrix be computed that
doesn't account for estimation of the weights even when a compatible
<code>weightit</code> object is supplied, set <code>vcov = "HC0"</code>, which treats the weights
as fixed.
</p>
<p>Bootstrapping can also be used to compute the coefficient variance matrix;
when <code>vcov = "BS"</code> or <code>vcov = "FWB"</code>, which implement the traditional
resampling-based and fractional weighted bootstrap, respectively, the entire
process of estimating the weights and fitting the outcome model is repeated
in bootstrap samples (if a <code>weightit</code> object is supplied). This accounts for
estimation of the weights and can be used with any weighting method. It is
important to set a seed using <code>set.seed()</code> to ensure replicability of the
results. The fractional weighted bootstrap is more reliable but requires the
weighting method to accept sampling weights (which most do, and you'll get an
error if it doesn't). Setting <code>vcov = "FWB"</code> and supplying <code>fwb.args = list(wtype = "multinom")</code> also performs the resampling-based bootstrap but
with the additional features <span class="pkg">fwb</span> provides (e.g., a progress bar and
parallelization) at the expense of needing to have <span class="pkg">fwb</span> installed.
</p>
<p><code>multinom_weightit()</code> implements multinomial logistic regression using a
custom function in <span class="pkg">WeightIt</span>. This implementation is less robust to
failures than other multinomial logistic regression solvers and should be
used with caution. Estimation of coefficients should align with that from
<code>mlogit::mlogit()</code> and <code>mclogit::mblogit()</code>.
</p>
<p><code>ordinal_weightit()</code> implements proportional odds ordinal regression using a
custom function in <span class="pkg">WeightIt</span>. Estimation of coefficients should align
with that from <code>MASS::polr()</code>.
</p>
<p><code>coxph_weightit()</code> is a wrapper for <code><a href="survival.html#topic+coxph">survival::coxph()</a></code> to fit weighted
survival models. It differs from <code>coxph()</code> in that the <code>cluster</code> argument (if
used) should be specified as a one-sided formula (which can include multiple
clustering variables) and uses a small sample correction for cluster variance
estimates when specified. Currently, M-estimation is not supported, so
bootstrapping (i.e., <code>vcov = "BS"</code> or <code>"FWB"</code>) is the only way to correctly
adjust for estimation of the weights. By default, the robust variance is
estimated treating weights as fixed, which is the same variance returned when
<code>robust = TRUE</code> in <code>coxph()</code>.
</p>
<p>Functions in the <span class="pkg">sandwich</span> package can be to compute standard errors
after fitting, regardless of how <code>vcov</code> was specified, though these will
ignore estimation of the weights, if any. When no adjustment is done for
estimation of the weights (i.e., because no <code>weightit</code> argument was supplied
or there was no <code>"Mparts"</code> component in the supplied object), the default
variance matrix produced by <code>glm_weightit()</code> should align with that from
<code style="white-space: pre;">&#8288;sandwich::vcovHC(. type = "HC0")&#8288;</code> or <code>sandwich::vcovCL(., type = "HC0", cluster = cluster)</code> when <code>cluster</code> is supplied. Not all types are available
for all models.
</p>


<h3>Value</h3>

<p>For <code>lm_weightit()</code> and <code>glm_weightit()</code>, a <code>glm_weightit</code> object,
which inherits from <code>glm</code>. For <code>ordinal_weightit()</code> and
<code>multinom_weightit()</code>, an <code>ordinal_weightit</code> or <code>multinom_weightit</code>,
respectively. For <code>coxph_weightit()</code>, a <code>coxph_weightit</code> object, which
inherits from <code>coxph</code>. See <code><a href="survival.html#topic+coxph">survival::coxph()</a></code> for details.
</p>
<p>Unless <code>vcov = "none"</code>, the <code>vcov</code> component contains the covariance matrix
adjusted for the estimation of the weights if requested and a compatible
<code>weightit</code> object was supplied. The <code>vcov_type</code> component contains the type
of variance matrix requested. If <code>cluster</code> is supplied, it will be stored in
the <code>"cluster"</code> attribute of the output object, even if not used.
</p>
<p>The <code>model</code> component of the output object (also the <code>model.frame()</code> output)
will include two extra columns when <code>weightit</code> is supplied: <code>(weights)</code>
containing the weights used in the model (the product of the estimated
weights and the sampling weights, if any) and <code>(s.weights)</code> containing the
sampling weights, which will be 1 if <code>s.weights</code> is not supplied in the
original <code>weightit()</code> call.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+lm">lm()</a></code> and <code><a href="stats.html#topic+glm">glm()</a></code> for fitting generalized linear models without
adjusting standard errors for estimation of the weights. <code><a href="survival.html#topic+coxph">survival::coxph()</a></code>
for fitting Cox proportional hazards models without adjusting standard errors
for estimation of the weights.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data("lalonde", package = "cobalt")

# Logistic regression ATT weights
w.out &lt;- weightit(treat ~ age + educ + married + re74,
                  data = lalonde, method = "glm",
                  estimand = "ATT")

# Linear regression outcome model that adjusts
# for estimation of weights
fit1 &lt;- lm_weightit(re78 ~ treat, data = lalonde,
                    weightit = w.out)

summary(fit1)

# Linear regression outcome model that treats weights
# as fixed
fit2 &lt;- lm_weightit(re78 ~ treat, data = lalonde,
                    weightit = w.out, vcov = "HC0")

summary(fit2)

# example code

# Linear regression outcome model that bootstraps
# estimation of weights and outcome model fitting
# using fractional weighted bootstrap with "Mammen"
# weights
set.seed(123)
fit3 &lt;- lm_weightit(re78 ~ treat, data = lalonde,
                    weightit = w.out,
                    vcov = "FWB",
                    R = 50, #should use way more
                    fwb.args = list(wtype = "mammen"))

summary(fit3)

# Multinomial logistic regression outcome model
# that adjusts for estimation of weights
lalonde$re78_3 &lt;- factor(findInterval(lalonde$re78,
                                      c(0, 5e3, 1e4)))

fit4 &lt;- multinom_weightit(re78_3 ~ treat,
                          data = lalonde,
                          weightit = w.out)

summary(fit4)

# Ordinal probit regression that adjusts for estimation
# of weights
fit5 &lt;- ordinal_weightit(ordered(re78_3) ~ treat,
                         data = lalonde,
                         link = "probit",
                         weightit = w.out)

summary(fit5)
</code></pre>

<hr>
<h2 id='glm_weightit-methods'>Methods for <code>glm_weightit()</code> objects</h2><span id='topic+glm_weightit-methods'></span><span id='topic+summary.glm_weightit'></span><span id='topic+summary.multinom_weightit'></span><span id='topic+summary.ordinal_weightit'></span><span id='topic+summary.coxph_weightit'></span><span id='topic+print.glm_weightit'></span><span id='topic+vcov.glm_weightit'></span><span id='topic+estfun.glm_weightit'></span><span id='topic+update.glm_weightit'></span>

<h3>Description</h3>

<p>This page documents methods for objects returned by
<code><a href="#topic+glm_weightit">glm_weightit()</a></code>, <code>lm_weightit()</code>, <code>ordinal_weightit()</code>,
<code>multinom_weightit()</code>, and <code>coxph_weightit()</code>. <code>predict()</code> methods are
described at <code><a href="#topic+predict.glm_weightit">predict.glm_weightit()</a></code> and <code>anova()</code> methods are described at
<code><a href="#topic+anova.glm_weightit">anova.glm_weightit()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'glm_weightit'
summary(object, ci = FALSE, level = 0.95, transform = NULL, vcov = NULL, ...)

## S3 method for class 'multinom_weightit'
summary(object, ci = FALSE, level = 0.95, transform = NULL, vcov = NULL, ...)

## S3 method for class 'ordinal_weightit'
summary(
  object,
  ci = FALSE,
  level = 0.95,
  transform = NULL,
  thresholds = TRUE,
  vcov = NULL,
  ...
)

## S3 method for class 'coxph_weightit'
summary(object, ci = FALSE, level = 0.95, transform = NULL, vcov = NULL, ...)

## S3 method for class 'glm_weightit'
print(x, digits = max(3L, getOption("digits") - 3L), ...)

## S3 method for class 'glm_weightit'
vcov(object, complete = TRUE, vcov = NULL, ...)

## S3 method for class 'glm_weightit'
estfun(x, asympt = TRUE, ...)

## S3 method for class 'glm_weightit'
update(object, formula. = NULL, ..., evaluate = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="glm_weightit-methods_+3A_object">object</code>, <code id="glm_weightit-methods_+3A_x">x</code></td>
<td>
<p>an output from one of the above modeling functions.</p>
</td></tr>
<tr><td><code id="glm_weightit-methods_+3A_ci">ci</code></td>
<td>
<p><code>logical</code>; whether to display Wald confidence intervals for
estimated coefficients. Default is <code>FALSE</code>. (Note: this argument can also
be supplied as <code>conf.int</code>.)</p>
</td></tr>
<tr><td><code id="glm_weightit-methods_+3A_level">level</code></td>
<td>
<p>when <code>ci = TRUE</code>, the desired confidence level.</p>
</td></tr>
<tr><td><code id="glm_weightit-methods_+3A_transform">transform</code></td>
<td>
<p>the function used to transform the coefficients, e.g., <code>exp</code>
(which can also be supplied as a string, e.g., <code>"exp"</code>); passed to
<code><a href="base.html#topic+match.fun">match.fun()</a></code> before being used on the coefficients. When <code>ci = TRUE</code>, this
is also applied to the confidence interval bounds. If specified, the
standard error will be omitted from the output. Default is no
transformation.</p>
</td></tr>
<tr><td><code id="glm_weightit-methods_+3A_vcov">vcov</code></td>
<td>
<p>either a string indicating the method used to compute the
variance of the estimated parameters for <code>object</code>, a function used to
extract the variance, or the variance matrix itself. Default is to use the
variance matrix already present in <code>object</code>. If a string or function,
arguments passed to <code>...</code> are supplied to the method or function. (Note:
for <code>vcov()</code>, can also be supplied as <code>type</code>.)</p>
</td></tr>
<tr><td><code id="glm_weightit-methods_+3A_...">...</code></td>
<td>
<p>for <code>vcov()</code> or <code>summary()</code> or <code>confint()</code> with <code>vcov</code> supplied,
other arguments used to compute the variance matrix depending on the method
supplied to <code>vcov</code>, e.g., <code>cluster</code>, <code>R</code>, or <code>fwb.args</code>. For <code>update()</code>,
additional arguments to the call or arguments with changed values. See
<code><a href="#topic+glm_weightit">glm_weightit()</a></code> for details.</p>
</td></tr>
<tr><td><code id="glm_weightit-methods_+3A_thresholds">thresholds</code></td>
<td>
<p><code>logical</code>; whether to include thresholds in the <code>summary()</code>
output for <code>ordinal_weightit</code> objects. Default is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="glm_weightit-methods_+3A_digits">digits</code></td>
<td>
<p>the number of <em>significant</em> digits to be
passed to <code><a href="base.html#topic+format">format</a>(<a href="stats.html#topic+coef">coef</a>(x), .)</code> when
<code><a href="base.html#topic+print">print</a>()</code>ing.</p>
</td></tr>
<tr><td><code id="glm_weightit-methods_+3A_complete">complete</code></td>
<td>
<p><code>logical</code>; whether the full variance-covariance matrix should
be returned also in case of an over-determined system where some
coefficients are undefined and <code>coef(.)</code> contains <code>NA</code>s correspondingly.
When <code>complete = TRUE</code>, <code>vcov()</code> is compatible with <code>coef()</code> also in this
singular case.</p>
</td></tr>
<tr><td><code id="glm_weightit-methods_+3A_asympt">asympt</code></td>
<td>
<p><code>logical</code>; for <code>estfun()</code>, whether to use the asymptotic
empirical estimating functions that account for estimation of the weights
(when <code>Mparts</code> is available). Default is <code>TRUE</code>. Set to <code>FALSE</code> to ignore
estimation of the weights. Ignored when <code>Mparts</code> is not available or no
argument was supplied to <code>weightit</code> in the fitting function.</p>
</td></tr>
<tr><td><code id="glm_weightit-methods_+3A_formula.">formula.</code></td>
<td>
<p>changes to the model formula, passed to the <code>new</code> argument of
<code><a href="stats.html#topic+update.formula">update.formula()</a></code>.</p>
</td></tr>
<tr><td><code id="glm_weightit-methods_+3A_evaluate">evaluate</code></td>
<td>
<p>whether to evaluate the call (<code>TRUE</code>, the default) or just
return it.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>vcov()</code> by default extracts the parameter covariance matrix already
computed by the fitting function, and <code>summary()</code> and <code>confint()</code> uses this
covariance matrix to compute standard errors and Wald confidence intervals
(internally calling <code><a href="stats.html#topic+confint.lm">confint.lm()</a></code>), respectively. Supplying arguments to
<code>vcov</code> or <code>...</code> will compute a new covariance matrix. If <code>cluster</code> was
supplied to the original fitting function, it will be incorporated into any
newly computed covariance matrix unless <code>cluster = NULL</code> is specified in
<code>vcov()</code>, <code>summary()</code>, or <code>confint()</code>. For other arguments (e.g., <code>R</code> and
<code>fwb.args</code>), the defaults are those used by <code><a href="#topic+glm_weightit">glm_weightit()</a></code>. Note that for
<code>vcov = "BS"</code> and <code>vcov = "FWB"</code> (and <code>vcov = "const"</code> for
<code>multinom_weightit</code> or <code>ordinal_weightit</code> objects), the environment for the
fitting function is used, so any changes to that environment may affect
calculation. It is always safer to simply recompute the fitted object with a
new covariance matrix than to modify it with the <code>vcov</code> argument, but it can
be quicker to just request a new covariance matrix when refitting the model
is slow.
</p>
<p><code>update()</code> updates a fitted model object with new arguments, e.g., a new
model formula, dataset, or variance matrix. When only arguments that control
the computation of the variance are supplied, only the variance will be
recalculated (i.e., the parameters will not be re-estimated). When <code>data</code> is
supplied, <code>weightit</code> is not supplied, and a <code>weightit</code> object was originally
passed to the model fitting function, the <code>weightit</code> object will be re-fit
with the new dataset before the model is refit using the new weights and new
data. That is, calling <code>update(obj, data = d)</code> is equivalent to calling
<code>update(obj, data = d, weightit = update(obj$weightit, data = d))</code> when a
<code>weightit</code> object was supplied to the model fitting function. Similarly,
supplying <code>s.weights</code> or <code>weights</code> passes the argument through to
<code>weightit()</code> to be refit. When <code>s.weights</code> or <code>weights</code> are supplied and no
<code>weightit</code> object is present, a fake one containing just the supplied weights
will be created.
</p>
<p><code>estfun()</code> extracts the empirical estimating functions for the fitted model, optionally accounting for the estimation of the weights (if available). This, along with <code>bread()</code>, is used by <code><a href="sandwich.html#topic+sandwich">sandwich::sandwich()</a></code> to compute the robust covariance matrix of the estimated coefficients. See <code><a href="#topic+glm_weightit">glm_weightit()</a></code> and <code>vcov()</code> above for more details.
</p>


<h3>Value</h3>

<p><code>summary()</code> returns a <code>summary.glm_weightit()</code> object, which has its
own <code>print()</code> method. For <code>coxph_weightit()</code> objects, the <code>print()</code> and
<code>summary()</code> methods are more like those for <code>glm</code> objects than for <code>coxph</code>
objects.
</p>
<p>Otherwise, all methods return the same type of object as their generics.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+glm_weightit">glm_weightit()</a></code> for the page documenting <code>glm_weightit()</code>,
<code>lm_weightit()</code>, <code>ordinal_weightit()</code>, <code>multinom_weightit()</code>, and
<code>coxph_weightit()</code>. <code><a href="stats.html#topic+summary.glm">summary.glm()</a></code>, <code><a href="stats.html#topic+vcov">vcov()</a></code>, <code><a href="stats.html#topic+confint">confint()</a></code> for the relevant
methods pages. <code><a href="#topic+predict.glm_weightit">predict.glm_weightit()</a></code> for computing predictions from the
models. <code><a href="#topic+anova.glm_weightit">anova.glm_weightit()</a></code> for comparing models using a Wald test.
</p>
<p><code><a href="sandwich.html#topic+estfun">sandwich::estfun()</a></code> and <code><a href="sandwich.html#topic+bread">sandwich::bread()</a></code> for the <code>estfun()</code> and <code>bread()</code> generics.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## See more examples at ?glm_weightit
</code></pre>

<hr>
<h2 id='make_full_rank'>Make a design matrix full rank</h2><span id='topic+make_full_rank'></span>

<h3>Description</h3>

<p>When writing <a href="#topic+method_user">user-defined methods</a> for use with
<code><a href="#topic+weightit">weightit()</a></code>, it may be necessary to take the potentially non-full rank
<code>covs</code> data frame and make it full rank for use in a downstream function.
This function performs that operation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>make_full_rank(mat, with.intercept = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="make_full_rank_+3A_mat">mat</code></td>
<td>
<p>a numeric matrix or data frame to be transformed. Typically this
contains covariates. <code>NA</code>s are not allowed.</p>
</td></tr>
<tr><td><code id="make_full_rank_+3A_with.intercept">with.intercept</code></td>
<td>
<p>whether an intercept (i.e., a vector of 1s) should be
added to <code>mat</code> before making it full rank. If <code>TRUE</code>, the intercept will be
used in determining whether a column is linearly dependent on others.
Regardless, no intercept will be included in the output.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>make_full_rank()</code> calls <code><a href="base.html#topic+qr">qr()</a></code> to find the rank and linearly
independent columns of <code>mat</code>, which are retained while others are dropped. If
<code>with.intercept</code> is set to <code>TRUE</code>, an intercept column is added to the matrix
before calling <code>qr()</code>. Note that dependent columns that appear later in <code>mat</code>
will be dropped first.
</p>
<p>See example at <code><a href="#topic+method_user">method_user</a></code>.
</p>


<h3>Value</h3>

<p>An object of the same type as <code>mat</code> containing only linearly
independent columns.
</p>


<h3>Note</h3>

<p>Older versions would drop all columns that only had one value. With
<code>with.intercept = FALSE</code>, if only one column has only one value, it will not
be removed, and it will function as though there was an intercept present; if
more than only column has only one value, only the first one will remain.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+method_user">method_user</a></code>, <code><a href="stats.html#topic+model.matrix">model.matrix()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
set.seed(1000)
c1 &lt;- rbinom(10, 1, .4)
c2 &lt;- 1-c1
c3 &lt;- rnorm(10)
c4 &lt;- 10*c3
mat &lt;- data.frame(c1, c2, c3, c4)

make_full_rank(mat) #leaves c2 and c4

make_full_rank(mat, with.intercept = FALSE) #leaves c1, c2, and c4
</code></pre>

<hr>
<h2 id='method_bart'>Propensity Score Weighting Using BART</h2><span id='topic+method_bart'></span>

<h3>Description</h3>

<p>This page explains the details of estimating weights from
Bayesian additive regression trees (BART)-based propensity scores by setting
<code>method = "bart"</code> in the call to <code><a href="#topic+weightit">weightit()</a></code> or <code><a href="#topic+weightitMSM">weightitMSM()</a></code>. This method
can be used with binary, multi-category, and continuous treatments.
</p>
<p>In general, this method relies on estimating propensity scores using BART and
then converting those propensity scores into weights using a formula that
depends on the desired estimand. This method relies on <code><a href="dbarts.html#topic+bart2">dbarts::bart2()</a></code>
from the <a href="https://CRAN.R-project.org/package=dbarts"><span class="pkg">dbarts</span></a> package.
</p>


<h4>Binary Treatments</h4>

<p>For binary treatments, this method estimates the propensity scores using
<code><a href="dbarts.html#topic+bart2">dbarts::bart2()</a></code>. The following estimands are allowed: ATE, ATT, ATC,
ATO, ATM, and ATOS. Weights can also be computed using marginal mean
weighting through stratification for the ATE, ATT, and ATC. See
<code><a href="#topic+get_w_from_ps">get_w_from_ps()</a></code> for details.
</p>



<h4>Multi-Category Treatments</h4>

<p>For multi-category treatments, the propensity scores are estimated using
several calls to <code><a href="dbarts.html#topic+bart2">dbarts::bart2()</a></code>, one for each treatment group; the
treatment probabilities are not normalized to sum to 1. The following
estimands are allowed: ATE, ATT, ATC, ATO, and ATM. The weights for each
estimand are computed using the standard formulas or those mentioned above.
Weights can also be computed using marginal mean weighting through
stratification for the ATE, ATT, and ATC. See <code><a href="#topic+get_w_from_ps">get_w_from_ps()</a></code> for details.
</p>



<h4>Continuous Treatments For continuous treatments, weights are estimated as</h4>

<p><code class="reqn">w_i = f_A(a_i) / f_{A|X}(a_i)</code>, where <code class="reqn">f_A(a_i)</code> (known as the
stabilization factor) is the unconditional density of treatment evaluated the
observed treatment value and <code class="reqn">f_{A|X}(a_i)</code> (known as the generalized
propensity score) is the conditional density of treatment given the
covariates evaluated at the observed value of treatment. The shape of
<code class="reqn">f_A(.)</code> and <code class="reqn">f_{A|X}(.)</code> is controlled by the <code>density</code> argument
described below (normal distributions by default), and the predicted values
used for the mean of the conditional density are estimated using BART as
implemented in <code><a href="dbarts.html#topic+bart2">dbarts::bart2()</a></code>. Kernel density estimation can be used
instead of assuming a specific density for the numerator and denominator by
setting <code>density = "kernel"</code>. Other arguments to <code><a href="stats.html#topic+density">density()</a></code> can be specified
to refine the density estimation parameters.
</p>



<h4>Longitudinal Treatments</h4>

<p>For longitudinal treatments, the weights are the product of the weights
estimated at each time point.
</p>



<h4>Sampling Weights</h4>

<p>Sampling weights are not supported.
</p>



<h4>Missing Data</h4>

<p>In the presence of missing data, the following value(s) for <code>missing</code> are
allowed:
</p>

<dl>
<dt><code>"ind"</code> (default)</dt><dd>
<p>First, for each variable with missingness, a new missingness indicator variable is created which takes the value 1 if the original covariate is <code>NA</code> and 0 otherwise. The missingness indicators are added to the model formula as main effects. The missing values in the covariates are then replaced with the covariate medians. The weight estimation then proceeds with this new formula and set of covariates. The covariates output in the resulting <code>weightit</code> object will be the original covariates with the <code>NA</code>s.
</p>
</dd>
</dl>




<h4>M-estimation</h4>

<p>M-estimation is not supported.
</p>



<h3>Details</h3>

<p>BART works by fitting a sum-of-trees model for the treatment or
probability of treatment. The number of trees is determined by the <code>n.trees</code>
argument. Bayesian priors are used for the hyperparameters, so the result is
a posterior distribution of predicted values for each unit. The mean of these
for each unit is taken for use in computing the (generalized) propensity
score. Although the hyperparameters governing the priors can be modified by
supplying arguments to <code>weightit()</code> that are passed to the BART fitting
function, the default values tend to work well and require little
modification (though the defaults differ for continuous and categorical
treatments; see the <code><a href="dbarts.html#topic+bart2">dbarts::bart2()</a></code> documentation for details). Unlike
many other machine learning methods, no loss function is optimized and the
hyperparameters do not need to be tuned (e.g., using cross-validation),
though performance can benefit from tuning. BART tends to balance sparseness
with flexibility by using very weak learners as the trees, which makes it
suitable for capturing complex functions without specifying a particular
functional form and without overfitting.
</p>


<h4>Reproducibility</h4>

<p>BART has a random component, so some work must be done to ensure
reproducibility across runs. See the <em>Reproducibility</em> section at
<code><a href="dbarts.html#topic+bart2">dbarts::bart2()</a></code> for more details. To ensure reproducibility, one can
do one of two things: 1) supply an argument to <code>seed</code>, which is passed to
<code>dbarts::bart2()</code> and sets the seed for single- and multi-threaded uses, or
2) call <code><a href="base.html#topic+set.seed">set.seed()</a></code>, though this only ensures reproducibility when using
single-threading, which can be requested by setting <code>n.threads = 1</code>. Note
that to ensure reproducibility on any machine, regardless of the number of
cores available, one should use single-threading and either supply <code>seed</code> or
call <code>set.seed()</code>.
</p>



<h3>Additional Arguments</h3>

<p>All arguments to <code><a href="dbarts.html#topic+bart2">dbarts::bart2()</a></code> can be passed through <code>weightit()</code>
or <code>weightitMSM()</code>, with the following exceptions:
</p>

<ul>
<li> <p><code>test</code>, <code>weights</code>,<code>subset</code>, <code>offset.test</code> are ignored
</p>
</li>
<li> <p><code>combine.chains</code> is always set to <code>TRUE</code>
</p>
</li>
<li> <p><code>sampleronly</code> is always set to <code>FALSE</code>
</p>
</li></ul>

<p>For continuous treatments only, the following arguments may be supplied:
</p>

<dl>
<dt><code>density</code></dt><dd><p>A function corresponding to the conditional density of the treatment. The standardized residuals of the treatment model will be fed through this function to produce the numerator and denominator of the generalized propensity score weights. If blank, <code><a href="stats.html#topic+dnorm">dnorm()</a></code> is used as recommended by Robins et al. (2000). This can also be supplied as a string containing the name of the function to be called. If the string contains underscores, the call will be split by the underscores and the latter splits will be supplied as arguments to the second argument and beyond. For example, if <code>density = "dt_2"</code> is specified, the density used will be that of a t-distribution with 2 degrees of freedom. Using a t-distribution can be useful when extreme outcome values are observed (Naimi et al., 2014).
</p>
<p>Can also be <code>"kernel"</code> to use kernel density estimation, which calls <code><a href="stats.html#topic+density">density()</a></code> to estimate the numerator and denominator densities for the weights. (This used to be requested by setting <code>use.kernel = TRUE</code>, which is now deprecated.)</p>
</dd>
<dt><code>bw</code>, <code>adjust</code>, <code>kernel</code>, <code>n</code></dt><dd><p>If <code>density = "kernel"</code>, the arguments to <code><a href="stats.html#topic+density">density()</a></code>. The defaults are the same as those in <code>density()</code> except that <code>n</code> is 10 times the number of units in the sample.</p>
</dd>
<dt><code>plot</code></dt><dd><p>If <code>density = "kernel"</code>, whether to plot the estimated densities.</p>
</dd>
</dl>



<h3>Additional Outputs</h3>


<dl>
<dt><code>obj</code></dt><dd>
<p>When <code>include.obj = TRUE</code>, the <code>bart2</code> fit(s) used to generate the predicted values. With multi-category treatments, this will be a list of the fits; otherwise, it will be a single fit. The predicted probabilities used to compute the propensity scores can be extracted using <code><a href="dbarts.html#topic+bart">fitted()</a></code>.
</p>
</dd>
</dl>



<h3>References</h3>

<p>Hill, J., Weiss, C., &amp; Zhai, F. (2011). Challenges With
Propensity Score Strategies in a High-Dimensional Setting and a Potential
Alternative. <em>Multivariate Behavioral Research</em>, 46(3), 477–513.
<a href="https://doi.org/10.1080/00273171.2011.570161">doi:10.1080/00273171.2011.570161</a>
</p>
<p>Chipman, H. A., George, E. I., &amp; McCulloch, R. E. (2010). BART: Bayesian
additive regression trees. <em>The Annals of Applied Statistics</em>, 4(1), 266–298.
<a href="https://doi.org/10.1214/09-AOAS285">doi:10.1214/09-AOAS285</a>
</p>
<p>Note that many references that deal with BART for causal inference focus on
estimating potential outcomes with BART, not the propensity scores, and so
are not directly relevant when using BART to estimate propensity scores for
weights.
</p>
<p>See <code><a href="#topic+method_glm">method_glm</a></code> for additional references on propensity score weighting
more generally.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+weightit">weightit()</a></code>, <code><a href="#topic+weightitMSM">weightitMSM()</a></code>, <code><a href="#topic+get_w_from_ps">get_w_from_ps()</a></code>
</p>
<p><code><a href="#topic+method_super">method_super</a></code> for stacking predictions from several machine learning
methods, including BART.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library("cobalt")
data("lalonde", package = "cobalt")

#Balancing covariates between treatment groups (binary)
(W1 &lt;- weightit(treat ~ age + educ + married +
                  nodegree + re74, data = lalonde,
                method = "bart", estimand = "ATT"))
summary(W1)
bal.tab(W1)

#Balancing covariates with respect to race (multi-category)
(W2 &lt;- weightit(race ~ age + educ + married +
                nodegree + re74, data = lalonde,
                method = "bart", estimand = "ATE"))
summary(W2)
bal.tab(W2)

#Balancing covariates with respect to re75 (continuous)
#assuming t(3) conditional density for treatment
(W3 &lt;- weightit(re75 ~ age + educ + married +
                  nodegree + re74, data = lalonde,
                method = "bart", density = "dt_3"))
 summary(W3)
 bal.tab(W3)


</code></pre>

<hr>
<h2 id='method_cbps'>Covariate Balancing Propensity Score Weighting</h2><span id='topic+method_cbps'></span>

<h3>Description</h3>

<p>This page explains the details of estimating weights from covariate balancing
propensity scores by setting <code>method = "cbps"</code> in the call to <code><a href="#topic+weightit">weightit()</a></code> or
<code><a href="#topic+weightitMSM">weightitMSM()</a></code>. This method can be used with binary, multi-category, and
continuous treatments.
</p>
<p>In general, this method relies on estimating propensity scores using
generalized method of moments and then converting those propensity scores
into weights using a formula that depends on the desired estimand. This
method relies on code written for <span class="pkg">WeightIt</span> using <code><a href="stats.html#topic+optim">optim()</a></code>.
</p>


<h4>Binary Treatments</h4>

<p>For binary treatments, this method estimates the propensity scores and
weights using <code>optim()</code> using formulas described by Imai and Ratkovic (2014).
The following estimands are allowed: ATE, ATT, ATC, and ATO.
</p>



<h4>Multi-Category Treatments</h4>

<p>For multi-category treatments, this method estimates the generalized
propensity scores and weights using <code>optim()</code> using formulas described by
Imai and Ratkovic (2014). The following estimands are allowed: ATE and ATT.
</p>



<h4>Continuous Treatments</h4>

<p>For continuous treatments, this method estimates the generalized propensity
scores and weights using <code>optim()</code> using a modification of the formulas
described by Fong, Hazlett, and Imai (2018). See Details.
</p>



<h4>Longitudinal Treatments</h4>

<p>For longitudinal treatments, the weights are computed using methods similar
to those described by Huffman and van Gameren (2018). This involves
specifying moment conditions for the models at each time point as with
single-time point treatments but using the product of the time-specific
weights as the weights that are used in the balance moment conditions. This
yields weights that balance the covariate at each time point. This is not the
same implementation as is implemented in <code>CBPS::CBMSM()</code>, and results should
not be expected to align between the two methods. Any combination of
treatment types is supported.
</p>
<p>For the over-identified version (i.e., setting <code>over = TRUE</code>), the empirical
variance is used in the objective function, whereas the expected variance
averaging over the treatment is used with binary and multi-category point
treatments.
</p>



<h4>Sampling Weights</h4>

<p>Sampling weights are supported through <code>s.weights</code> in all scenarios.
</p>



<h4>Missing Data</h4>

<p>In the presence of missing data, the following value(s) for <code>missing</code> are
allowed:
</p>

<dl>
<dt><code>"ind"</code> (default)</dt><dd>
<p>First, for each variable with missingness, a new missingness indicator variable is created which takes the value 1 if the original covariate is <code>NA</code> and 0 otherwise. The missingness indicators are added to the model formula as main effects. The missing values in the covariates are then replaced with the covariate medians (this value is arbitrary and does not affect estimation). The weight estimation then proceeds with this new formula and set of covariates. The covariates output in the resulting <code>weightit</code> object will be the original covariates with the <code>NA</code>s.
</p>
</dd>
</dl>




<h4>M-estimation</h4>

<p>M-estimation is supported for the just-identified CBPS (the default, setting
<code>over = FALSE</code>) for binary and multi-category treatments. Otherwise (i.e.,
for continuous or longitudinal treatments or when <code>over = TRUE</code>),
M-estimation is not supported. See <code><a href="#topic+glm_weightit">glm_weightit()</a></code> and
<code>vignette("estimating-effects")</code> for details.
</p>



<h3>Details</h3>

<p>CBPS estimates the coefficients of a generalized linear model (for
binary treatments), multinomial logistic regression model (for multi-category
treatments), or linear regression model (for continuous treatments) that is
used to compute (generalized) propensity scores, from which the weights are
computed. It involves replacing (or augmenting, in the case of the
over-identified version) the standard regression score equations with the
balance constraints in a generalized method of moments estimation. The idea
is to nudge the estimation of the coefficients toward those that produce
balance in the weighted sample. The just-identified version (with <code>exact = FALSE</code>) does away with the score equations for the coefficients so that only
the balance constraints are used. The just-identified version will therefore
produce superior balance on the means (i.e., corresponding to the balance
constraints) for binary and multi-category treatments and linear terms for
continuous treatments than will the over-identified version.
</p>
<p>Just-identified CBPS is very similar to entropy balancing and inverse
probability tilting. For the ATT, all three methods will yield identical
estimates. For other estimands, the results will differ.
</p>
<p>Note that <span class="pkg">WeightIt</span> provides different functionality from the <span class="pkg">CBPS</span>
package in terms of the versions of CBPS available; for extensions to CBPS
(e.g., optimal CBPS and CBPS for instrumental variables), the <span class="pkg">CBPS</span>
package may be preferred. Note that for longitudinal treatments,
<code>CBPS::CBMSM()</code> uses different methods and produces different results from
<code>weightitMSM()</code> called with <code>method = "cbps"</code>.
</p>


<h3>Additional Arguments</h3>

<p><code>moments</code> and <code>int</code> are accepted. See
<code><a href="#topic+weightit">weightit()</a></code> for details.
</p>
<p>The following additional arguments can be specified:
</p>

<dl>
<dt><code>over</code></dt><dd><p><code>logical</code>; whether to request the over-identified CBPS, which combines the generalized linear model regression score equations (for binary treatments), multinomial logistic regression score equations (for multi-category treatments), or linear regression score equations (for continuous treatments) to the balance moment conditions. Default is <code>FALSE</code> to use the just-identified CBPS.
</p>
</dd>
<dt><code>twostep</code></dt><dd><p><code>logical</code>; when <code>over = TRUE</code>, whether to use the two-step approximation to the generalized method of moments variance. Default is <code>TRUE</code>. Setting to <code>FALSE</code> increases computation time but may improve estimation. Ignored with a warning when <code>over = FALSE</code>.
</p>
</dd>
<dt><code>link</code></dt><dd><p>the link used in the generalized linear model for the propensity scores when treatment is binary. Default is <code>"logit"</code> for logistic regression, which is used in the original description of the method by Imai and Ratkovic (2014), but others are allowed, including <code>"probit"</code>, <code>"cauchit"</code>, <code>"cloglog"</code>, <code>"loglog"</code>, <code>"log"</code>, <code>"clog"</code>, and <code>"identity"</code>. Note that negative weights are possible with these last three and they should be used with caution. An object of class <code>"link-glm"</code> can also be supplied. The argument is passed to <code><a href="stats.html#topic+quasibinomial">quasibinomial()</a></code>. Ignored for multi-category, continuous, and longitudinal treatments.
</p>
</dd>
<dt><code>reltol</code></dt><dd><p>the relative tolerance for convergence of the optimization. Passed to the <code>control</code> argument of <code>optim()</code>. Default is <code>1e-10</code>.
</p>
</dd>
<dt><code>maxit</code></dt><dd><p>the maximum number of iterations for convergence of the optimization. Passed to the <code>control</code> argument of <code>optim()</code>. Default is 1000 for binary and multi-category treatments and 10000 for continuous and longitudinal treatments.
</p>
</dd>
<dt><code>solver</code></dt><dd><p>the solver to use to estimate the parameters of the just-identified CBPS. Allowable options include <code>"multiroot"</code> to use <code><a href="rootSolve.html#topic+multiroot">rootSolve::multiroot()</a></code> and <code>"optim"</code> to use <code><a href="stats.html#topic+optim">stats::optim()</a></code>. <code>"multiroot"</code> is the default when <span class="pkg">rootSolve</span> is installed, as it tends to be much faster and more accurate; otherwise, <code>"optim"</code> is the default and requires no dependencies. Regardless of <code>solver</code>, the output of <code>optim()</code> is returned when <code>include.obj = TRUE</code> (see below). When <code>over = TRUE</code>, the parameter estimates of the just-identified CBPS are used as starting values for the over-identified CBPS.
</p>
</dd>
<dt><code>quantile</code></dt><dd>
<p>A named list of quantiles (values between 0 and 1) for each continuous covariate, which are used to create additional variables that when balanced ensure balance on the corresponding quantile of the variable. For example, setting <code style="white-space: pre;">&#8288;quantile = list(x1 = c(.25, .5. , .75))&#8288;</code> ensures the 25th, 50th, and 75th percentiles of <code>x1</code> in each treatment group will be balanced in the weighted sample. Can also be a single number (e.g., <code>.5</code>) or an unnamed list of length 1 (e.g., <code>list(c(.25, .5, .75))</code>) to request the same quantile(s) for all continuous covariates, or a named vector (e.g., <code>c(x1 = .5, x2 = .75)</code> to request one quantile for each covariate. Only allowed with binary and multi-category treatments.
</p>
</dd>
</dl>



<h3>Additional Outputs</h3>


<dl>
<dt><code>obj</code></dt><dd><p>When <code>include.obj = TRUE</code>, the output of the final call to <code>optim()</code> used to produce the model parameters. Note that because of variable transformations, the resulting parameter estimates may not be interpretable.
</p>
</dd>
</dl>



<h3>Note</h3>

<p>This method used to rely on functionality in the <span class="pkg">CBPS</span> package,
but no longer does. Slight differences may be found between the two packages
in some cases due to numerical imprecision (or, for continuous and
longitudinal treatments, due to a difference in the estimator).
<span class="pkg">WeightIt</span> supports arbitrary numbers of groups for the multi-category
CBPS and any estimand, whereas <span class="pkg">CBPS</span> only supports up to four groups and
only the ATE. The implementation of the just-identified CBPS for continuous
treatments also differs from that of <span class="pkg">CBPS</span>, and departs slightly from
that described by Fong et al. (2018). The treatment mean and treatment
variance are treated as random parameters to be estimated and are included in
the balance moment conditions. In Fong et al. (2018), the treatment mean and
variance are fixed to their empirical counterparts. For continuous treatments
with the over-identified CBPS, <span class="pkg">WeightIt</span> and <span class="pkg">CBPS</span> use different
methods of specifying the GMM variance matrix, which may lead to differing
results.
</p>
<p>Note that the default method differs between the two implementations; by
default <span class="pkg">WeightIt</span> uses the just-identified CBPS, which is faster to fit,
yields better balance, and is compatible with M-estimation for estimating the
standard error of the treatment effect, whereas <span class="pkg">CBPS</span> uses the
over-identified CBPS by default. However, both the just-identified and
over-identified versions are available in both packages.
</p>
<p>When the <span class="pkg">rootSolve</span> package is installed, the optimization process will
be slightly faster and more accurate because starting values are provided by
an initial call to <code><a href="rootSolve.html#topic+multiroot">rootSolve::multiroot()</a></code>. However, the package is not
required.
</p>


<h3>References</h3>



<h4>Binary treatments</h4>

<p>Imai, K., &amp; Ratkovic, M. (2014). Covariate balancing propensity score.
<em>Journal of the Royal Statistical Society: Series B (Statistical
Methodology)</em>, 76(1), 243–263.
</p>



<h4>Multi-Category treatments</h4>

<p>Imai, K., &amp; Ratkovic, M. (2014). Covariate balancing propensity score.
<em>Journal of the Royal Statistical Society: Series B (Statistical
Methodology)</em>, 76(1), 243–263.
</p>



<h4>Continuous treatments</h4>

<p>Fong, C., Hazlett, C., &amp; Imai, K. (2018). Covariate balancing propensity
score for a continuous treatment: Application to the efficacy of political
advertisements. <em>The Annals of Applied Statistics</em>, 12(1), 156–177.
<a href="https://doi.org/10.1214/17-AOAS1101">doi:10.1214/17-AOAS1101</a>
</p>



<h4>Longitudinal treatments</h4>

<p>Huffman, C., &amp; van Gameren, E. (2018). Covariate Balancing Inverse
Probability Weights for Time-Varying Continuous Interventions. <em>Journal of
Causal Inference</em>, 6(2). <a href="https://doi.org/10.1515/jci-2017-0002">doi:10.1515/jci-2017-0002</a>
</p>
<p>Note: one should not cite Imai &amp; Ratkovic (2015) when using CBPS for
longitudinal treatments.
</p>
<p>Some of the code was inspired by the source code of the <span class="pkg">CBPS</span> package.
</p>



<h3>See Also</h3>

<p><code><a href="#topic+weightit">weightit()</a></code>, <code><a href="#topic+weightitMSM">weightitMSM()</a></code>
</p>
<p><a href="#topic+method_ebal">method_ebal</a> and <a href="#topic+method_ipt">method_ipt</a> for entropy balancing and inverse probability
tilting, which work similarly.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("lalonde", package = "cobalt")

#Balancing covariates between treatment groups (binary)
(W1a &lt;- weightit(treat ~ age + educ + married +
                  nodegree + re74, data = lalonde,
                method = "cbps", estimand = "ATT"))
summary(W1a)
cobalt::bal.tab(W1a)

#Balancing covariates between treatment groups (binary)
#using over-identified CBPS with probit link
(W1b &lt;- weightit(treat ~ age + educ + married +
                  nodegree + re74, data = lalonde,
                method = "cbps", estimand = "ATT",
                over = TRUE, link = "probit"))
summary(W1b)
cobalt::bal.tab(W1b)

#Balancing covariates with respect to race (multi-category)
(W2 &lt;- weightit(race ~ age + educ + married +
                  nodegree + re74, data = lalonde,
                method = "cbps", estimand = "ATE"))
summary(W2)
cobalt::bal.tab(W2)

#Balancing covariates with respect to re75 (continuous)
(W3 &lt;- weightit(re75 ~ age + educ + married +
                  nodegree + re74, data = lalonde,
                method = "cbps"))
summary(W3)
cobalt::bal.tab(W3)

#Longitudinal treatments
data("msmdata")
(W4 &lt;- weightitMSM(list(A_1 ~ X1_0 + X2_0,
                        A_2 ~ X1_1 + X2_1 +
                          A_1 + X1_0 + X2_0),
                   data = msmdata,
                   method = "cbps"))
summary(W4)
cobalt::bal.tab(W4)

</code></pre>

<hr>
<h2 id='method_ebal'>Entropy Balancing</h2><span id='topic+method_ebal'></span>

<h3>Description</h3>

<p>This page explains the details of estimating weights using
entropy balancing by setting <code>method = "ebal"</code> in the call to <code><a href="#topic+weightit">weightit()</a></code> or
<code><a href="#topic+weightitMSM">weightitMSM()</a></code>. This method can be used with binary, multi-category, and
continuous treatments.
</p>
<p>In general, this method relies on estimating weights by minimizing the
negative entropy of the weights subject to exact moment balancing
constraints. This method relies on code written for <span class="pkg">WeightIt</span> using
<code><a href="stats.html#topic+optim">optim()</a></code>.
</p>


<h4>Binary Treatments</h4>

<p>For binary treatments, this method estimates the weights using <code>optim()</code>
using formulas described by Hainmueller (2012). The following estimands are
allowed: ATE, ATT, and ATC. When the ATE is requested, the optimization is
run twice, once for each treatment group.
</p>



<h4>Multi-Category Treatments</h4>

<p>For multi-category treatments, this method estimates the weights using
<code>optim()</code>. The following estimands are allowed: ATE and ATT. When the ATE is
requested, <code>optim()</code> is run once for each treatment group. When the ATT is
requested, <code>optim()</code> is run once for each non-focal (i.e., control) group.
</p>



<h4>Continuous Treatments</h4>

<p>For continuous treatments, this method estimates the weights using <code>optim()</code>
using formulas described by Tübbicke (2022) and Vegetabile et al. (2021).
</p>



<h4>Longitudinal Treatments</h4>

<p>For longitudinal treatments, the weights are the product of the weights
estimated at each time point. This method is not guaranteed to yield exact
balance at each time point. NOTE: the use of entropy balancing with
longitudinal treatments has not been validated!
</p>



<h4>Sampling Weights</h4>

<p>Sampling weights are supported through <code>s.weights</code> in all scenarios.
</p>



<h4>Missing Data</h4>

<p>In the presence of missing data, the following value(s) for <code>missing</code> are
allowed:
</p>

<dl>
<dt><code>"ind"</code> (default)</dt><dd>
<p>First, for each variable with missingness, a new missingness indicator variable is created which takes the value 1 if the original covariate is <code>NA</code> and 0 otherwise. The missingness indicators are added to the model formula as main effects. The missing values in the covariates are then replaced with the covariate medians (this value is arbitrary and does not affect estimation). The weight estimation then proceeds with this new formula and set of covariates. The covariates output in the resulting <code>weightit</code> object will be the original covariates with the <code>NA</code>s.
</p>
</dd>
</dl>




<h4>M-estimation</h4>

<p>M-estimation is supported for all scenarios. See <code><a href="#topic+glm_weightit">glm_weightit()</a></code> and
<code>vignette("estimating-effects")</code> for details.
</p>



<h3>Details</h3>

<p>Entropy balancing involves the specification of an optimization
problem, the solution to which is then used to compute the weights. The
constraints of the primal optimization problem correspond to covariate
balance on the means (for binary and multi-category treatments) or
treatment-covariate covariances (for continuous treatments), positivity of
the weights, and that the weights sum to a certain value. It turns out that
the dual optimization problem is much easier to solve because it is over only
as many variables as there are balance constraints rather than over the
weights for each unit and it is unconstrained. Zhao and Percival (2017) found
that entropy balancing for the ATT of a binary treatment actually involves
the estimation of the coefficients of a logistic regression propensity score
model but using a specialized loss function different from that optimized
with maximum likelihood. Entropy balancing is doubly robust (for the ATT) in
the sense that it is consistent either when the true propensity score model
is a logistic regression of the treatment on the covariates or when the true
outcome model for the control units is a linear regression of the outcome on
the covariates, and it attains a semi-parametric efficiency bound when both
are true. Entropy balancing will always yield exact mean balance on the
included terms.
</p>


<h3>Additional Arguments</h3>

<p><code>moments</code> and <code>int</code> are accepted. See
<code><a href="#topic+weightit">weightit()</a></code> for details.
</p>

<dl>
<dt><code>base.weights</code></dt><dd>
<p>A vector of base weights, one for each unit. These correspond to the base weights $q$ in Hainmueller (2012). The estimated weights minimize the Kullback entropy divergence from the base weights, defined as <code class="reqn">\sum w \log(w/q)</code>, subject to exact balance constraints. These can be used to supply previously estimated weights so that the newly estimated weights retain the some of the properties of the original weights while ensuring the balance constraints are met. Sampling weights should not be passed to <code>base.weights</code> but can be included in a <code>weightit()</code> call that includes <code>s.weights</code>.
</p>
</dd>
<dt><code>reltol</code></dt><dd><p>the relative tolerance for convergence of the optimization. Passed to the <code>control</code> argument of <code>optim()</code>. Default is <code>1e-10</code>.
</p>
</dd>
<dt><code>maxit</code></dt><dd><p>the maximum number of iterations for convergence of the optimization. Passed to the <code>control</code> argument of <code>optim()</code>. Default is 1000 for binary and multi-category treatments and 10000 for continuous and longitudinal treatments.
</p>
</dd>
<dt><code>solver</code></dt><dd><p>the solver to use to estimate the parameters of the just-identified CBPS. Allowable options include <code>"multiroot"</code> to use <code><a href="rootSolve.html#topic+multiroot">rootSolve::multiroot()</a></code> and <code>"optim"</code> to use <code><a href="stats.html#topic+optim">stats::optim()</a></code>. <code>"multiroot"</code> is the default when <span class="pkg">rootSolve</span> is installed, as it tends to be much faster and more accurate; otherwise, <code>"optim"</code> is the default and requires no dependencies. Regardless of <code>solver</code>, the output of <code>optim()</code> is returned when <code>include.obj = TRUE</code> (see below). When <code>over = TRUE</code>, the parameter estimates of the just-identified CBPS are used as starting values for the over-identified CBPS.
</p>
</dd>
<dt><code>quantile</code></dt><dd>
<p>A named list of quantiles (values between 0 and 1) for each continuous covariate, which are used to create additional variables that when balanced ensure balance on the corresponding quantile of the variable. For example, setting <code style="white-space: pre;">&#8288;quantile = list(x1 = c(.25, .5. , .75))&#8288;</code> ensures the 25th, 50th, and 75th percentiles of <code>x1</code> in each treatment group will be balanced in the weighted sample. Can also be a single number (e.g., <code>.5</code>) or an unnamed list of length 1 (e.g., <code>list(c(.25, .5, .75))</code>) to request the same quantile(s) for all continuous covariates, or a named vector (e.g., <code>c(x1 = .5, x2 = .75)</code> to request one quantile for each covariate. Only allowed with binary and multi-category treatments.
</p>
</dd>
<dt><code>d.moments</code></dt><dd>
<p>With continuous treatments, the number of moments of the treatment and covariate distributions that are constrained to be the same in the weighted sample as in the original sample. For example, setting <code>d.moments = 3</code> ensures that the mean, variance, and skew of the treatment and covariates are the same in the weighted sample as in the unweighted sample. <code>d.moments</code> should be greater than or equal to <code>moments</code> and will be automatically set accordingly if not (or if not specified). Vegetabile et al. (2021) recommend setting <code>d.moments = 3</code>, even if <code>moments</code> is less than 3. This argument corresponds to the tuning parameters $r$ and $s$ in Vegetabile et al. (2021) (which here are set to be equal). Ignored for binary and multi-category treatments.
</p>
</dd>
</dl>

<p>The <code>stabilize</code> argument is ignored; in the past it would reduce the
variability of the weights through an iterative process. If you want to
minimize the variance of the weights subject to balance constraints, use
<code>method = "optweight"</code>.
</p>


<h3>Additional Outputs</h3>


<dl>
<dt><code>obj</code></dt><dd><p>When <code>include.obj = TRUE</code>, the output of the call to <code><a href="stats.html#topic+optim">optim()</a></code>, which contains the dual variables and convergence information. For ATE fits or with multi-category treatments, a list of <code>optim()</code> outputs, one for each weighted group.
</p>
</dd>
</dl>



<h3>References</h3>



<h4>Binary Treatments</h4>



<h5><code>estimand = "ATT"</code> Hainmueller, J. (2012). Entropy Balancing for Causal</h5>

<p>Effects: A Multivariate Reweighting Method to Produce Balanced Samples in
Observational Studies. <em>Political Analysis</em>, 20(1), 25–46.
<a href="https://doi.org/10.1093/pan/mpr025">doi:10.1093/pan/mpr025</a>
</p>
<p>Zhao, Q., &amp; Percival, D. (2017). Entropy balancing is doubly robust. <em>Journal
of Causal Inference</em>, 5(1). <a href="https://doi.org/10.1515/jci-2016-0010">doi:10.1515/jci-2016-0010</a>
</p>



<h5><code>estimand = "ATE"</code></h5>

<p>Källberg, D., &amp; Waernbaum, I. (2023). Large Sample Properties of Entropy
Balancing Estimators of Average Causal Effects. <em>Econometrics and
Statistics</em>. <a href="https://doi.org/10.1016/j.ecosta.2023.11.004">doi:10.1016/j.ecosta.2023.11.004</a>
</p>




<h4>Continuous Treatments</h4>

<p>Tübbicke, S. (2022). Entropy Balancing for Continuous Treatments. <em>Journal of
Econometric Methods</em>, 11(1), 71–89. <a href="https://doi.org/10.1515/jem-2021-0002">doi:10.1515/jem-2021-0002</a>
</p>
<p>Vegetabile, B. G., Griffin, B. A., Coffman, D. L., Cefalu, M., Robbins, M.
W., &amp; McCaffrey, D. F. (2021). Nonparametric estimation of population average
dose-response curves using entropy balancing weights for continuous
exposures. <em>Health Services and Outcomes Research Methodology</em>, 21(1),
69–110. <a href="https://doi.org/10.1007/s10742-020-00236-2">doi:10.1007/s10742-020-00236-2</a>
</p>



<h3>See Also</h3>

<p><code><a href="#topic+weightit">weightit()</a></code>, <code><a href="#topic+weightitMSM">weightitMSM()</a></code>
</p>
<p><a href="#topic+method_ipt">method_ipt</a> and <a href="#topic+method_cbps">method_cbps</a> for inverse probability tilting and CBPS,
which work similarly.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("lalonde", package = "cobalt")

#Balancing covariates between treatment groups (binary)
(W1 &lt;- weightit(treat ~ age + educ + married +
                  nodegree + re74, data = lalonde,
                method = "ebal", estimand = "ATT"))
summary(W1)
cobalt::bal.tab(W1)

#Balancing covariates with respect to race (multi-category)
(W2 &lt;- weightit(race ~ age + educ + married +
                  nodegree + re74, data = lalonde,
                method = "ebal", estimand = "ATE"))
summary(W2)
cobalt::bal.tab(W2)

#Balancing covariates and squares with respect to
#re75 (continuous), maintaining 3 moments of the
#covariate and treatment distributions
(W3 &lt;- weightit(re75 ~ age + educ + married +
                  nodegree + re74, data = lalonde,
                method = "ebal", moments = 2,
                d.moments = 3))
summary(W3)
cobalt::bal.tab(W3)
</code></pre>

<hr>
<h2 id='method_energy'>Energy Balancing</h2><span id='topic+method_energy'></span>

<h3>Description</h3>

<p>This page explains the details of estimating weights using
energy balancing by setting <code>method = "energy"</code> in the call to <code><a href="#topic+weightit">weightit()</a></code>
or <code><a href="#topic+weightitMSM">weightitMSM()</a></code>. This method can be used with binary, multi-category, and
continuous treatments.
</p>
<p>In general, this method relies on estimating weights by minimizing an energy
statistic related to covariate balance. For binary and multi-category
treatments, this is the energy distance, which is a multivariate distance
between distributions, between treatment groups. For continuous treatments,
this is the sum of the distance covariance between the treatment variable and
the covariates and the energy distances between the treatment and covariates
in the weighted sample and their distributions in the original sample. This
method relies on code written for <span class="pkg">WeightIt</span> using <code><a href="osqp.html#topic+osqp">osqp::osqp()</a></code>
from the <a href="https://CRAN.R-project.org/package=osqp"><span class="pkg">osqp</span></a> package to perform the optimization. This method may
be slow or memory-intensive for large datasets.
</p>


<h4>Binary Treatments</h4>

<p>For binary treatments, this method estimates the weights using <code>osqp()</code> using
formulas described by Huling and Mak (2024). The following estimands are
allowed: ATE, ATT, and ATC.
</p>



<h4>Multi-Category Treatments</h4>

<p>For multi-category treatments, this method estimates the weights using
<code>osqp()</code> using formulas described by Huling and Mak (2024). The following
estimands are allowed: ATE and ATT.
</p>



<h4>Continuous Treatments</h4>

<p>For continuous treatments, this method estimates the weights using <code>osqp()</code>
using formulas described by Huling, Greifer, and Chen (2023).
</p>



<h4>Longitudinal Treatments</h4>

<p>For longitudinal treatments, the weights are the product of the weights
estimated at each time point. This method is not guaranteed to yield optimal
balance at each time point. NOTE: the use of energy balancing with
longitudinal treatments has not been validated!
</p>



<h4>Sampling Weights</h4>

<p>Sampling weights are supported through <code>s.weights</code> in all scenarios. In some
cases, sampling weights will cause the optimization to fail due to lack of
convexity or infeasible constraints.
</p>



<h4>Missing Data</h4>

<p>In the presence of missing data, the following value(s) for <code>missing</code> are
allowed:
</p>

<dl>
<dt><code>"ind"</code> (default)</dt><dd>
<p>First, for each variable with missingness, a new missingness indicator variable is created which takes the value 1 if the original covariate is <code>NA</code> and 0 otherwise. The missingness indicators are added to the model formula as main effects. The missing values in the covariates are then replaced with the covariate medians (this value is arbitrary and does not affect estimation). The weight estimation then proceeds with this new formula and set of covariates. The covariates output in the resulting <code>weightit</code> object will be the original covariates with the <code>NA</code>s.
</p>
</dd>
</dl>




<h4>M-estimation</h4>

<p>M-estimation is not supported.
</p>



<h3>Details</h3>

<p>Energy balancing is a method of estimating weights using
optimization without a propensity score. The weights are the solution to a
constrain quadratic optimization problem where the objective function
concerns covariate balance as measured by the energy distance and (for
continuous treatments) the distance covariance.
</p>
<p>Energy balancing for binary and multi-category treatments involves minimizing
the energy distance between the treatment groups and between each treatment
group and a target group (e.g., the full sample for the ATE). The energy
distance is a scalar measure of the difference between two multivariate
distributions and is equal to 0 when the two distributions are identical.
</p>
<p>Energy balancing for continuous treatments involves minimizing the distance
covariance between the treatment and the covariates; the distance covariance
is a scalar measure of the association between two (possibly multivariate)
distributions that is equal to 0 when the two distributions are independent.
In addition, the energy distances between the treatment and covariate
distributions in the weighted sample and the treatment and covariate
distributions in the original sample are minimized.
</p>
<p>The primary benefit of energy balancing is that all features of the covariate
distribution are balanced, not just means, as with other optimization-based
methods like entropy balancing. Still, it is possible to add additional
balance constraints to require balance on individual terms using the
<code>moments</code> argument, just like with entropy balancing. Energy balancing can
sometimes yield weights with high variability; the <code>lambda</code> argument can be
supplied to penalize highly variable weights to increase the effective sample
size at the expense of balance.
</p>


<h4>Reproducibility</h4>

<p>Although there are no stochastic components to the optimization, a feature
turned off by default is to update the optimization based on how long the
optimization has been running, which will vary across runs even when a seed
is set and no parameters have been changed. See the discussion
<a href="https://github.com/osqp/osqp-r/issues/19">here</a> for more details. To ensure
reproducibility by default, <code>adaptive_rho_interval</code> is set to 10. See
<code><a href="osqp.html#topic+osqpSettings">osqp::osqpSettings()</a></code> for details.
</p>



<h3>Additional Arguments</h3>

<p>The following following additional arguments
can be specified:
</p>

<dl>
<dt><code>dist.mat</code></dt><dd><p>the name of the method used to compute the distance matrix of the covariates or the numeric distance matrix itself. Allowable options include <code>"scaled_euclidean"</code> for the Euclidean (L2) distance on the scaled covariates (the default), <code>"mahalanobis"</code> for the Mahalanobis distance, and <code>"euclidean"</code> for the raw Euclidean distance. Abbreviations allowed. Note that some user-supplied distance matrices can cause the R session to abort due to a bug within <span class="pkg">osqp</span>, so this argument should be used with caution. A distance matrix must be a square, symmetric, numeric matrix with zeros along the diagonal and a row and column for each unit. Can also be supplied as the output of a call to <code><a href="stats.html#topic+dist">dist()</a></code>.
</p>
</dd>
<dt><code>lambda</code></dt><dd><p>a positive numeric scalar used to penalize the square of the weights. This value divided by the square of the total sample size is added to the diagonal of the quadratic part of the loss function. Higher values favor weights with less variability. Note this is distinct from the lambda value described in Huling and Mak (2024), which penalizes the complexity of individual treatment rules rather than the weights, but does correspond to lambda from Huling et al. (2023). Default is .0001, which is essentially 0.
</p>
</dd>
</dl>

<p>For binary and multi-category treatments, the following additional
arguments can be specified:
</p>

<dl>
<dt><code>improved</code></dt><dd><p><code>logical</code>; whether to use the improved energy balancing weights as described by Huling and Mak (2024) when <code>estimand = "ATE"</code>. This involves optimizing balance not only between each treatment group and the overall sample, but also between each pair of treatment groups. Huling and Mak (2024) found that the improved energy balancing weights generally outperformed standard energy balancing. Default is <code>TRUE</code>; set to <code>FALSE</code> to use the standard energy balancing weights instead (not recommended).
</p>
</dd>
<dt><code>quantile</code></dt><dd>
<p>A named list of quantiles (values between 0 and 1) for each continuous covariate, which are used to create additional variables that when balanced ensure balance on the corresponding quantile of the variable. For example, setting <code style="white-space: pre;">&#8288;quantile = list(x1 = c(.25, .5. , .75))&#8288;</code> ensures the 25th, 50th, and 75th percentiles of <code>x1</code> in each treatment group will be balanced in the weighted sample. Can also be a single number (e.g., <code>.5</code>) or an unnamed list of length 1 (e.g., <code>list(c(.25, .5, .75))</code>) to request the same quantile(s) for all continuous covariates, or a named vector (e.g., <code>c(x1 = .5, x2 = .75)</code> to request one quantile for each covariate.
</p>
</dd>
</dl>

<p>For continuous treatments, the following additional arguments can be
specified:
</p>

<dl>
<dt><code>d.moments</code></dt><dd>
<p>The number of moments of the treatment and covariate distributions that are constrained to be the same in the weighted sample as in the original sample. For example, setting <code>d.moments = 3</code> ensures that the mean, variance, and skew of the treatment and covariates are the same in the weighted sample as in the unweighted sample. <code>d.moments</code> should be greater than or equal to <code>moments</code> and will be automatically set accordingly if not (or if not specified).
</p>
</dd>
<dt><code>dimension.adj</code></dt><dd>
<p><code>logical</code>; whether to include the dimensionality adjustment described by Huling et al. (2023). If <code>TRUE</code>, the default, the energy distance for the covariates is weighted <code class="reqn">\sqrt{p}</code> times as much as the energy distance for the treatment, where <code class="reqn">p</code> is the number of covariates. If <code>FALSE</code>, the two energy distances are given equal weights. Default is <code>TRUE</code>.
</p>
</dd>
</dl>

<p>The <code>moments</code> argument functions differently for <code>method = "energy"</code> from
how it does with other methods. When unspecified or set to zero, energy
balancing weights are estimated as described by Huling and Mak (2024) for
binary and multi-category treatments or by Huling et al. (2023) for
continuous treatments. When <code>moments</code> is set to an integer larger than 0,
additional balance constraints on the requested moments of the covariates
are also included, guaranteeing exact moment balance on these covariates
while minimizing the energy distance of the weighted sample. For binary and
multi-category treatments, this involves exact balance on the means of the
entered covariates; for continuous treatments, this involves exact balance
on the treatment-covariate correlations of the entered covariates.
</p>
<p>Any other arguments will be passed to <code><a href="osqp.html#topic+osqpSettings">osqp::osqpSettings()</a></code>. Some
defaults differ from those in <code>osqpSettings()</code>; see <em>Reproducibility</em>
below.
</p>


<h3>Additional Outputs</h3>


<dl>
<dt><code>obj</code></dt><dd><p>When <code>include.obj = TRUE</code>, the output of the call to <code><a href="osqp.html#topic+solve_osqp">osqp::solve_osqp()</a></code>, which contains the dual variables and convergence information.
</p>
</dd>
</dl>



<h3>Note</h3>

<p>Sometimes the optimization can fail to converge because the problem is
not convex. A warning will be displayed if so. In these cases, try simply
re-fitting the weights without changing anything (but see the
<em>Reproducibility</em> section above). If the method repeatedly fails, you should
try another method or change the supplied parameters (though this is
uncommon). Increasing <code>max_iter</code> or changing <code>adaptive_rho_interval</code> might
help.
</p>
<p>If it seems like the weights are balancing the covariates but you still get a
failure to converge, this usually indicates that more iterations are needs to
find the optimal solutions. This can occur when <code>moments</code> or <code>int</code> are
specified. <code>max_iter</code> should be increased, and setting <code>verbose = TRUE</code>
allows you to monitor the process and examine if the optimization is
approaching convergence.
</p>


<h3>Author(s)</h3>

<p>Noah Greifer, using code from Jared Huling's
<a href="https://CRAN.R-project.org/package=independenceWeights"><span class="pkg">independenceWeights</span></a> package for continuous treatments.
</p>


<h3>References</h3>



<h4>Binary and multi-category treatments</h4>

<p>Huling, J. D., &amp; Mak, S. (2024). Energy balancing of covariate distributions.
<em>Journal of Causal Inference</em>, 12(1). <a href="https://doi.org/10.1515/jci-2022-0029">doi:10.1515/jci-2022-0029</a>
</p>



<h4>Continuous treatments</h4>

<p>Huling, J. D., Greifer, N., &amp; Chen, G. (2023). Independence weights for
causal inference with continuous treatments. <em>Journal of the American
Statistical Association</em>, 0(ja), 1–25. <a href="https://doi.org/10.1080/01621459.2023.2213485">doi:10.1080/01621459.2023.2213485</a>
</p>



<h3>See Also</h3>

<p><code><a href="#topic+weightit">weightit()</a></code>, <code><a href="#topic+weightitMSM">weightitMSM()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library("cobalt")
data("lalonde", package = "cobalt")

#Balancing covariates between treatment groups (binary)
(W1 &lt;- weightit(treat ~ age + educ + married +
                  nodegree + re74, data = lalonde,
                method = "energy", estimand = "ATE"))
summary(W1)
bal.tab(W1)

#Balancing covariates with respect to race (multi-category)
(W2 &lt;- weightit(race ~ age + educ + married +
                  nodegree + re74, data = lalonde,
                method = "energy", estimand = "ATT",
                focal = "black"))
summary(W2)
bal.tab(W2)

  #Balancing covariates with respect to re75 (continuous)
  (W3 &lt;- weightit(re75 ~ age + educ + married +
                    nodegree + re74, data = lalonde,
                  method = "energy", moments = 1))
  summary(W3)
  bal.tab(W3, poly = 2)


</code></pre>

<hr>
<h2 id='method_gbm'>Propensity Score Weighting Using Generalized Boosted Models</h2><span id='topic+method_gbm'></span>

<h3>Description</h3>

<p>This page explains the details of estimating weights from
generalized boosted model-based propensity scores by setting <code>method = "gbm"</code>
in the call to <code><a href="#topic+weightit">weightit()</a></code> or <code><a href="#topic+weightitMSM">weightitMSM()</a></code>. This method can be used with
binary, multi-category, and continuous treatments.
</p>
<p>In general, this method relies on estimating propensity scores using
generalized boosted modeling and then converting those propensity scores into
weights using a formula that depends on the desired estimand. The algorithm
involves using a balance-based or prediction-based criterion to optimize in
choosing the value of tuning parameters (the number of trees and possibly
others). The method relies on the <a href="https://CRAN.R-project.org/package=gbm"><span class="pkg">gbm</span></a> package.
</p>
<p>This method mimics the functionality of functions in the <span class="pkg">twang</span> package,
but has improved performance and more flexible options. See Details section
for more details.
</p>


<h4>Binary Treatments</h4>

<p>For binary treatments, this method estimates the propensity scores using
<code><a href="gbm.html#topic+gbm.fit">gbm::gbm.fit()</a></code> and then selects the optimal tuning parameter values
using the method specified in the <code>criterion</code> argument. The following
estimands are allowed: ATE, ATT, ATC, ATO, and ATM. The weights are computed
from the estimated propensity scores using <code><a href="#topic+get_w_from_ps">get_w_from_ps()</a></code>, which
implements the standard formulas. Weights can also be computed using marginal
mean weighting through stratification for the ATE, ATT, and ATC. See
<code><a href="#topic+get_w_from_ps">get_w_from_ps()</a></code> for details.
</p>



<h4>Multi-Category Treatments</h4>

<p>For binary treatments, this method estimates the propensity scores using
<code><a href="gbm.html#topic+gbm.fit">gbm::gbm.fit()</a></code> and then selects the optimal tuning parameter values
using the method specified in the <code>criterion</code> argument. The following
estimands are allowed: ATE, ATT, ATC, ATO, and ATM. The weights are computed
from the estimated propensity scores using <code><a href="#topic+get_w_from_ps">get_w_from_ps()</a></code>, which
implements the standard formulas. Weights can also be computed using marginal
mean weighting through stratification for the ATE, ATT, and ATC. See
<code><a href="#topic+get_w_from_ps">get_w_from_ps()</a></code> for details.
</p>



<h4>Continuous Treatments</h4>

<p>For continuous treatments, this method estimates the generalized propensity
score using <code><a href="gbm.html#topic+gbm.fit">gbm::gbm.fit()</a></code> and then selects the optimal tuning
parameter values using the method specified in the <code>criterion</code> argument.
</p>



<h4>Longitudinal Treatments</h4>

<p>For longitudinal treatments, the weights are the product of the weights
estimated at each time point.
</p>



<h4>Sampling Weights</h4>

<p>Sampling weights are supported through <code>s.weights</code> in all scenarios.
</p>



<h4>Missing Data</h4>

<p>In the presence of missing data, the following value(s) for <code>missing</code> are
allowed:
</p>

<dl>
<dt><code>"ind"</code> (default)</dt><dd><p>First, for each variable with missingness, a new missingness indicator variable is created which takes the value 1 if the original covariate is <code>NA</code> and 0 otherwise. The missingness indicators are added to the model formula as main effects. The missing values in the covariates are then replaced with the covariate medians (this value is arbitrary and does not affect estimation). The weight estimation then proceeds with this new formula and set of covariates. The covariates output in the resulting <code>weightit</code> object will be the original covariates with the <code>NA</code>s.</p>
</dd>
<dt><code>"surr"</code></dt><dd><p>Surrogate splitting is used to process <code>NA</code>s. No missingness indicators are created. Nodes are split using only the non-missing values of each variable. To generate predicted values for each unit, a non-missing variable that operates similarly to the variable with missingness is used as a surrogate. Missing values are ignored when calculating balance statistics to choose the optimal tree.</p>
</dd>
</dl>




<h4>M-estimation</h4>

<p>M-estimation is not supported.
</p>



<h3>Details</h3>

<p>Generalized boosted modeling (GBM, also known as gradient boosting
machines) is a machine learning method that generates predicted values from a
flexible regression of the treatment on the covariates, which are treated as
propensity scores and used to compute weights. It does this by building a
series of regression trees, each fit to the residuals of the last, minimizing
a loss function that depends on the distribution chosen. The optimal number
of trees is a tuning parameter that must be chosen; McCaffrey et al. (2004)
were innovative in using covariate balance to select this value rather than
traditional machine learning performance metrics such as cross-validation
accuracy. GBM is particularly effective for fitting nonlinear treatment
models characterized by curves and interactions, but performs worse for
simpler treatment models. It is unclear which balance measure should be used
to select the number of trees, though research has indicated that balance
measures tend to perform better than cross-validation accuracy for estimating
effective propensity score weights.
</p>
<p><span class="pkg">WeightIt</span> offers almost identical functionality to <span class="pkg">twang</span>, the
first package to implement this method. Compared to the current version of
<span class="pkg">twang</span>, <span class="pkg">WeightIt</span> offers more options for the measure of balance
used to select the number of trees, improved performance, tuning of
hyperparameters, more estimands, and support for continuous treatments.
<span class="pkg">WeightIt</span> computes weights for multi-category treatments differently
from how <span class="pkg">twang</span> does; rather than fitting a separate binary GBM for each
pair of treatments, <span class="pkg">WeightIt</span> fits a single multi-class GBM model and
uses balance measures appropriate for multi-category treatments.
</p>
<p><code>plot()</code> can be used on the output of <code>weightit()</code> with <code>method = "gbm"</code> to
display the results of the tuning process; see Examples and <code><a href="#topic+plot.weightit">plot.weightit()</a></code>
for more details.
</p>


<h3>Additional Arguments</h3>

<p>The following additional arguments can be
specified:
</p>

<dl>
<dt><code>criterion</code></dt><dd><p>A string describing the balance criterion used to select the best weights. See <code><a href="cobalt.html#topic+bal.compute">cobalt::bal.compute()</a></code> for allowable options for each treatment type. In addition, to optimize the cross-validation error instead of balance, <code>criterion</code> can be set as <code style="white-space: pre;">&#8288;"cv{#}&#8288;</code>&quot;, where <code style="white-space: pre;">&#8288;{#}&#8288;</code> is replaced by a number representing the number of cross-validation folds used (e.g., <code>"cv5"</code> for 5-fold cross-validation). For binary and multi-category treatments, the default is <code>"smd.mean"</code>, which minimizes the average absolute standard mean difference among the covariates between treatment groups. For continuous treatments, the default is <code>"p.mean"</code>, which minimizes the average absolute Pearson correlation between the treatment and covariates.
</p>
</dd>
<dt><code>trim.at</code></dt><dd><p>A number supplied to <code>at</code> in <code><a href="#topic+trim">trim()</a></code> which trims the weights from all the trees before choosing the best tree. This can be valuable when some weights are extreme, which occurs especially with continuous treatments. The default is 0 (i.e., no trimming).
</p>
</dd>
<dt><code>distribution</code></dt><dd><p>A string with the distribution used in the loss function of the boosted model. This is supplied to the <code>distribution</code> argument in <code><a href="gbm.html#topic+gbm.fit">gbm::gbm.fit()</a></code>. For binary treatments, <code>"bernoulli"</code> and <code>"adaboost"</code> are available, with <code>"bernoulli"</code> the default. For multi-category treatments, only <code>"multinomial"</code> is allowed. For continuous treatments <code>"gaussian"</code>, <code>"laplace"</code>, and <code>"tdist"</code> are available, with <code>"gaussian"</code> the default. This argument is tunable.
</p>
</dd>
<dt><code>n.trees</code></dt><dd><p>The maximum number of trees used. This is passed onto the <code>n.trees</code> argument in <code>gbm.fit()</code>. The default is 10000 for binary and multi-category treatments and 20000 for continuous treatments.
</p>
</dd>
<dt><code>start.tree</code></dt><dd><p>The tree at which to start balance checking. If you know the best balance isn't in the first 100 trees, for example, you can set <code>start.tree = 101</code> so that balance statistics are not computed on the first 100 trees. This can save some time since balance checking takes up the bulk of the run time for some balance-based stopping methods, and is especially useful when running the same model adding more and more trees. The default is 1, i.e., to start from the very first tree in assessing balance.
</p>
</dd>
<dt><code>interaction.depth</code></dt><dd><p>The depth of the trees. This is passed onto the <code>interaction.depth</code> argument in <code>gbm.fit()</code>. Higher values indicate better ability to capture nonlinear and nonadditive relationships. The default is 3 for binary and multi-category treatments and 4 for continuous treatments. This argument is tunable.
</p>
</dd>
<dt><code>shrinkage</code></dt><dd><p>The shrinkage parameter applied to the trees. This is passed onto the <code>shrinkage</code> argument in <code>gbm.fit()</code>. The default is .01 for binary and multi-category treatments and .0005 for continuous treatments. The lower this value is, the more trees one may have to include to reach the optimum. This argument is tunable.
</p>
</dd>
<dt><code>bag.fraction</code></dt><dd><p>The fraction of the units randomly selected to propose the next tree in the expansion. This is passed onto the <code>bag.fraction</code> argument in <code>gbm.fit()</code>. The default is 1, but smaller values should be tried. For values less then 1, subsequent runs with the same parameters will yield different results due to random sampling; be sure to seed the seed using <code><a href="base.html#topic+set.seed">set.seed()</a></code> to ensure replicability of results.
</p>
</dd>
<dt><code>use.offset</code></dt><dd><p><code>logical</code>; whether to use the linear predictor resulting from a generalized linear model as an offset to the GBM model. If <code>TRUE</code>, this fits a logistic regression model (for binary treatments) or a linear regression model (for continuous treatments) and supplies the linear predict to the <code>offset</code> argument of <code>gbm.fit()</code>. This often improves performance generally but especially when the true propensity score model is well approximated by a GLM, and this yields uniformly superior performance over <code>method = "glm"</code> with respect to <code>criterion</code>. Default is <code>FALSE</code> to omit the offset. Only allowed for binary and continuous treatments. This argument is tunable.
</p>
</dd>
</dl>

<p>All other arguments take on the defaults of those in <code><a href="gbm.html#topic+gbm.fit">gbm::gbm.fit()</a></code>,
and some are not used at all. For binary and multi-category treatments with
a with cross-validation used as the criterion, <code>class.stratify.cv</code> is set
to <code>TRUE</code> by default.
</p>
<p>The <code>w</code> argument in <code>gbm.fit()</code> is ignored because sampling weights are
passed using <code>s.weights</code>.
</p>
<p>For continuous treatments only, the following arguments may be supplied:
</p>

<dl>
<dt><code>density</code></dt><dd><p>A function corresponding to the conditional density of the treatment. The standardized residuals of the treatment model will be fed through this function to produce the numerator and denominator of the generalized propensity score weights. This can also be supplied as a string containing the name of the function to be called. If the string contains underscores, the call will be split by the underscores and the latter splits will be supplied as arguments to the second argument and beyond. For example, if <code>density = "dt_2"</code> is specified, the density used will be that of a t-distribution with 2 degrees of freedom. Using a t-distribution can be useful when extreme outcome values are observed (Naimi et al., 2014).
</p>
<p>Can also be <code>"kernel"</code> to use kernel density estimation, which calls <code><a href="stats.html#topic+density">density()</a></code> to estimate the numerator and denominator densities for the weights. (This used to be requested by setting <code>use.kernel = TRUE</code>, which is now deprecated.)
</p>
<p>If unspecified, a density corresponding to the argument passed to <code>distribution</code>. If <code>"gaussian"</code> (the default), <code><a href="stats.html#topic+dnorm">dnorm()</a></code> is used. If <code>"tdist"</code>, a t-distribution with 4 degrees of freedom is used. If <code>"laplace"</code>, a laplace distribution is used.</p>
</dd>
<dt><code>bw</code>, <code>adjust</code>, <code>kernel</code>, <code>n</code></dt><dd><p>If <code>density = "kernel"</code>, the arguments to <code><a href="stats.html#topic+density">density()</a></code>. The defaults are the same as those in <code>density()</code> except that <code>n</code> is 10 times the number of units in the sample.</p>
</dd>
<dt><code>plot</code></dt><dd><p>If <code>density = "kernel"</code>, whether to plot the estimated densities.</p>
</dd>
</dl>

<p>For tunable arguments, multiple entries may be supplied, and <code>weightit()</code>
will choose the best value by optimizing the criterion specified in
<code>criterion</code>. See below for additional outputs that are included when
arguments are supplied to be tuned. See Examples for an example of tuning.
The same seed is used for every run to ensure any variation in performance
across tuning parameters is due to the specification and not to using a
random seed. This only matters when <code>bag.fraction</code> differs from 1 (its
default) or cross-validation is used as the criterion; otherwise, there are
no random components in the model.
</p>


<h3>Additional Outputs</h3>


<dl>
<dt><code>info</code></dt><dd>
<p>A list with the following entries:
</p>

<dl>
<dt><code>best.tree</code></dt><dd>
<p>The number of trees at the optimum. If this is close to <code>n.trees</code>, <code>weightit()</code> should be rerun with a larger value for <code>n.trees</code>, and <code>start.tree</code> can be set to just below <code>best.tree</code>. When other parameters are tuned, this is the best tree value in the best combination of tuned parameters. See example.</p>
</dd>
<dt><code>tree.val</code></dt><dd>
<p>A data frame with two columns: the first is the number of trees and the second is the value of the criterion corresponding to that tree. Running <code><a href="base.html#topic+plot">plot()</a></code> on this object will plot the criterion by the number of trees and is a good way to see patterns in the relationship between them and to determine if more trees are needed. When other parameters are tuned, these are the number of trees and the criterion values in the best combination of tuned parameters. See example.</p>
</dd>
</dl>

<p>If any arguments are to be tuned (i.e., they have been supplied more than one value), the following two additional components are included in <code>info</code>:
</p>

<dl>
<dt><code>tune</code></dt><dd>
<p>A data frame with a column for each argument being tuned, the best value of the balance criterion for the given combination of parameters, and the number of trees at which the best value was reached.</p>
</dd>
<dt><code>best.tune</code></dt><dd>
<p>A one-row data frame containing the values of the arguments being tuned that were ultimately selected to estimate the returned weights.</p>
</dd>
</dl>

</dd>
<dt><code>obj</code></dt><dd>
<p>When <code>include.obj = TRUE</code>, the <code>gbm</code> fit used to generate the predicted values.
</p>
</dd>
</dl>



<h3>Note</h3>

<p>The <code>criterion</code> argument used to be called <code>stop.method</code>, which is its
name in <span class="pkg">twang</span>. <code>stop.method</code> still works for backward compatibility.
Additionally, the criteria formerly named as <code>"es.mean"</code>, <code>"es.max"</code>, and
<code>"es.rms"</code> have been renamed to <code>"smd.mean"</code>, <code>"smd.max"</code>, and <code>"smd.rms"</code>.
The former are used in <span class="pkg">twang</span> and will still work with <code>weightit()</code> for
backward compatibility.
</p>
<p>Estimated propensity scores are trimmed to <code class="reqn">10^{-8}</code> and <code class="reqn">1 - 10^{-8}</code> to ensure balance statistics can be computed.
</p>


<h3>References</h3>



<h4>Binary treatments</h4>

<p>McCaffrey, D. F., Ridgeway, G., &amp; Morral, A. R. (2004). Propensity Score
Estimation With Boosted Regression for Evaluating Causal Effects in
Observational Studies. <em>Psychological Methods</em>, 9(4), 403–425.
<a href="https://doi.org/10.1037/1082-989X.9.4.403">doi:10.1037/1082-989X.9.4.403</a>
</p>



<h4>Multi-Category Treatments</h4>

<p>McCaffrey, D. F., Griffin, B. A., Almirall, D., Slaughter, M. E., Ramchand,
R., &amp; Burgette, L. F. (2013). A Tutorial on Propensity Score Estimation for
Multiple Treatments Using Generalized Boosted Models. <em>Statistics in
Medicine</em>, 32(19), 3388–3414. <a href="https://doi.org/10.1002/sim.5753">doi:10.1002/sim.5753</a>
</p>



<h4>Continuous treatments</h4>

<p>Zhu, Y., Coffman, D. L., &amp; Ghosh, D. (2015). A Boosting Algorithm for
Estimating Generalized Propensity Scores with Continuous Treatments. <em>Journal
of Causal Inference</em>, 3(1). <a href="https://doi.org/10.1515/jci-2014-0022">doi:10.1515/jci-2014-0022</a>
</p>



<h3>See Also</h3>

<p><code><a href="#topic+weightit">weightit()</a></code>, <code><a href="#topic+weightitMSM">weightitMSM()</a></code>
</p>
<p><code><a href="gbm.html#topic+gbm.fit">gbm::gbm.fit()</a></code> for the fitting function.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library("cobalt")
data("lalonde", package = "cobalt")

#Balancing covariates between treatment groups (binary)
(W1 &lt;- weightit(treat ~ age + educ + married +
                  nodegree + re74, data = lalonde,
                method = "gbm", estimand = "ATE",
                criterion = "smd.max",
                use.offset = TRUE))
summary(W1)
bal.tab(W1)

# View information about the fitting process
W1$info$best.tree #best tree
plot(W1) #plot of criterion value against number of trees


  #Balancing covariates with respect to race (multi-category)
  (W2 &lt;- weightit(race ~ age + educ + married +
                    nodegree + re74, data = lalonde,
                  method = "gbm", estimand = "ATT",
                  focal = "hispan", criterion = "ks.mean"))
  summary(W2)
  bal.tab(W2, stats = c("m", "ks"))

  #Balancing covariates with respect to re75 (continuous)
  (W3 &lt;- weightit(re75 ~ age + educ + married +
                    nodegree + re74, data = lalonde,
                  method = "gbm", density = "kernel",
                  criterion = "p.rms", trim.at = .97))
  summary(W3)
  bal.tab(W3)

  #Using a t(3) density and illustrating the search for
  #more trees.
  W4a &lt;- weightit(re75 ~ age + educ + married +
                    nodegree + re74, data = lalonde,
                  method = "gbm", density = "dt_3",
                  criterion = "p.max",
                  n.trees = 10000)

  W4a$info$best.tree #10000; optimum hasn't been found
  plot(W4a) #decreasing at right edge

  W4b &lt;- weightit(re75 ~ age + educ + married +
                    nodegree + re74, data = lalonde,
                  method = "gbm", density = "dt_3",
                  criterion = "p.max",
                  start.tree = 10000,
                  n.trees = 20000)

  W4b$info$best.tree #13417; optimum has been found
  plot(W4b) #increasing at right edge

  bal.tab(W4b)

  #Tuning hyperparameters
  (W5 &lt;- weightit(treat ~ age + educ + married +
                    nodegree + re74, data = lalonde,
                  method = "gbm", estimand = "ATT",
                  criterion = "ks.max",
                  interaction.depth = 2:4,
                  distribution = c("bernoulli", "adaboost")))

  W5$info$tune

  W5$info$best.tune #Best values of tuned parameters
  plot(W5) #plot criterion values against number of trees

  bal.tab(W5, stats = c("m", "ks"))


</code></pre>

<hr>
<h2 id='method_glm'>Propensity Score Weighting Using Generalized Linear Models</h2><span id='topic+method_glm'></span>

<h3>Description</h3>

<p>This page explains the details of estimating weights from
generalized linear model-based propensity scores by setting <code>method = "glm"</code>
in the call to <code><a href="#topic+weightit">weightit()</a></code> or <code><a href="#topic+weightitMSM">weightitMSM()</a></code>. This method can be used with
binary, multi-category, and continuous treatments.
</p>
<p>In general, this method relies on estimating propensity scores with a
parametric generalized linear model and then converting those propensity
scores into weights using a formula that depends on the desired estimand. For
binary and multi-category treatments, a binomial or multinomial regression
model is used to estimate the propensity scores as the predicted probability
of being in each treatment given the covariates. For ordinal treatments, an
ordinal regression model is used to estimate generalized propensity scores.
For continuous treatments, a generalized linear model is used to estimate
generalized propensity scores as the conditional density of treatment given
the covariates.
</p>


<h4>Binary Treatments</h4>

<p>For binary treatments, this method estimates the propensity scores using
<code><a href="stats.html#topic+glm">glm()</a></code>. An additional argument is <code>link</code>, which uses the same options as
<code>link</code> in <code><a href="stats.html#topic+family">family()</a></code>. The default link is <code>"logit"</code>, but others, including
<code>"probit"</code>, are allowed. The following estimands are allowed: ATE, ATT, ATC,
ATO, ATM, and ATOS. Weights can also be computed using marginal mean
weighting through stratification for the ATE, ATT, and ATC. See
<code><a href="#topic+get_w_from_ps">get_w_from_ps()</a></code> for details.
</p>



<h4>Multi-Category Treatments</h4>

<p>For multi-category treatments, the propensity scores are estimated using
multinomial regression from one of a few functions depending on the argument
supplied to <code>multi.method</code> (see Additional Arguments below). The following
estimands are allowed: ATE, ATT, ATC, ATO, and ATM. The weights for each
estimand are computed using the standard formulas or those mentioned above.
Weights can also be computed using marginal mean weighting through
stratification for the ATE, ATT, and ATC. See <code><a href="#topic+get_w_from_ps">get_w_from_ps()</a></code> for details.
Ordinal treatments are treated exactly the same as non-order multi-category
treatments except that additional models are available to estimate the
generalized propensity score (e.g., ordinal logistic regression).
</p>



<h4>Continuous Treatments</h4>

<p>For continuous treatments, weights are estimated as
<code class="reqn">w_i = f_A(a_i) / f_{A|X}(a_i)</code>, where <code class="reqn">f_A(a_i)</code> (known as the
stabilization factor) is
the unconditional density of treatment evaluated the observed treatment value
and <code class="reqn">f_{A|X}(a_i)</code> (known as the generalized propensity score) is the
conditional density of treatment given the covariates evaluated at the
observed value of treatment. The shape of <code class="reqn">f_A(.)</code> and <code class="reqn">f_{A|X}(.)</code>
is controlled by the <code>density</code> argument described below (normal distributions
by default), and the predicted values used for the mean of the conditional
density are estimated using linear regression. Kernel density estimation can
be used instead of assuming a specific density for the numerator and
denominator by setting <code>density = "kernel"</code>. Other arguments to <code><a href="stats.html#topic+density">density()</a></code>
can be specified to refine the density estimation parameters.
</p>



<h4>Longitudinal Treatments</h4>

<p>For longitudinal treatments, the weights are the product of the weights
estimated at each time point.
</p>



<h4>Sampling Weights</h4>

<p>Sampling weights are supported through <code>s.weights</code> in all scenarios except
for multi-category treatments with <code>link = "bayes.probit"</code> and for binary and
continuous treatments with <code>missing = "saem"</code> (see below). Warning messages
may appear otherwise about non-integer successes, and these can be ignored.
</p>



<h4>Missing Data</h4>

<p>In the presence of missing data, the following value(s) for <code>missing</code> are
allowed:
</p>

<dl>
<dt><code>"ind"</code> (default)</dt><dd><p>First, for each variable with missingness, a new missingness indicator variable is created which takes the value 1 if the original covariate is <code>NA</code> and 0 otherwise. The missingness indicators are added to the model formula as main effects. The missing values in the covariates are then replaced with the covariate medians (this value is arbitrary and does not affect estimation). The weight estimation then proceeds with this new formula and set of covariates. The covariates output in the resulting <code>weightit</code> object will be the original covariates with the <code>NA</code>s.
</p>
</dd>
<dt><code>"saem"</code></dt><dd><p>For binary treatments with <code>link = "logit"</code> or continuous treatments, a stochastic approximation version of the EM algorithm (SAEM) is used via the <a href="https://CRAN.R-project.org/package=misaem"><span class="pkg">misaem</span></a> package. No additional covariates are created. See Jiang et al. (2019) for information on this method. In some cases, this is a suitable alternative to multiple imputation.
</p>
</dd>
</dl>




<h4>M-estimation</h4>

<p>For binary treatments, M-estimation is supported when <code>link</code> is neither
<code>"flic"</code> nor <code>"flac"</code> (see below). For multi-category treatments,
M-estimation is supported when <code>multi.method</code> is <code>"weightit"</code> (the default)
or <code>"glm"</code>. For continuous treatments, M-estimation is supported when
<code>density</code> is not <code>"kernel"</code>. The conditional treatment variance and
unconditional treatment mean and variance are included as parameters to
estimate, as these all go into calculation of the weights. For all treatment
types, M-estimation is not supported when <code>missing = "saem"</code>. See
<code><a href="#topic+glm_weightit">glm_weightit()</a></code> and <code>vignette("estimating-effects")</code> for details. For
longitudinal treatments, M-estimation is supported whenever the underlying
methods are.
</p>



<h3>Additional Arguments</h3>

<p>For binary treatments, the following
additional argument can be specified:
</p>

<dl>
<dt><code>link</code></dt><dd><p>the link used in the generalized linear model for the propensity scores. <code>link</code> can be any of those allowed by <code><a href="stats.html#topic+binomial">binomial()</a></code> as well as <code>"loglog"</code> and <code>"clog"</code>. A <code>br.</code> prefix can be added (e.g., <code>"br.logit"</code>); this changes the fitting method to the bias-corrected generalized linear models implemented in the <a href="https://CRAN.R-project.org/package=brglm2"><span class="pkg">brglm2</span></a> package. <code>link</code> can also be either <code>"flic"</code> or <code>"flac"</code> to fit the corresponding Firth corrected logistic regression models implemented in the <a href="https://CRAN.R-project.org/package=logistf"><span class="pkg">logistf</span></a> package.</p>
</dd>
</dl>

<p>For multi-category treatments, the following additional arguments can be
specified:
</p>

<dl>
<dt><code>multi.method</code></dt><dd><p>the method used to estimate the generalized propensity scores. Allowable options include <code>"weightit"</code> (the default) to use multinomial logistic regression implemented in <span class="pkg">WeightIt</span>, <code>"glm"</code> to use a series of binomial models using <code><a href="stats.html#topic+glm">glm()</a></code>, <code>"mclogit"</code> to use multinomial logistic regression as implemented in <code><a href="mclogit.html#topic+mblogit">mclogit::mblogit()</a></code>, <code>"mnp"</code> to use Bayesian multinomial probit regression as implemented in <code><a href="MNP.html#topic+MNP">MNP::MNP()</a></code>, and <code>"brmultinom"</code> to use bias-reduced multinomial logistic regression as implemented in <code><a href="brglm2.html#topic+brmultinom">brglm2::brmultinom()</a></code>. <code>"weightit"</code> and <code>"mclogit"</code> should give near-identical results, the main difference being increased robustness and customizability when using <code>"mclogit"</code> at the expense of not being able to use M-estimation to compute standard errors after weighting. For ordered treatments, allowable options include <code>"weightit"</code> (the default) to use ordinal regression implemented in <span class="pkg">WeightIt</span> or <code>"polr"</code> to use ordinal regression implemented in <code><a href="MASS.html#topic+polr">MASS::polr()</a></code>, unless <code>link</code> is <code>"br.logit"</code>, in which case bias-reduce ordinal logistic regression as implemented in <code><a href="brglm2.html#topic+bracl">brglm2::bracl()</a></code> is used. Ignored when <code>missing = "saem"</code>. Using the defaults allows for the use of M-estimation and requires no additional dependencies, but other packages may provide benefits such as speed and flexibility.</p>
</dd>
<dt><code>link</code></dt><dd><p>The link used in the multinomial, binomial, or ordered regression model for the generalized propensity scores depending on the argument supplied to <code>multi.method</code>. When <code>multi.method = "glm"</code>, <code>link</code> can be any of those allowed by <code><a href="stats.html#topic+binomial">binomial()</a></code>. When treatment is ordered and <code>multi.method</code> is <code>"weightit"</code> or <code>"polr"</code>, <code>link</code> can be any of those allowed by <code>MASS::polr()</code> or <code>"br.logit"</code>. Otherwise, <code>link</code> should be <code>"logit"</code> or not specified.</p>
</dd>
</dl>

<p>For continuous treatments, the following additional arguments may be
supplied:
</p>

<dl>
<dt><code>density</code></dt><dd><p>A function corresponding the conditional density of the treatment. The standardized residuals of the treatment model will be fed through this function to produce the numerator and denominator of the generalized propensity score weights. If blank, <code><a href="stats.html#topic+dnorm">dnorm()</a></code> is used as recommended by Robins et al. (2000). This can also be supplied as a string containing the name of the function to be called. If the string contains underscores, the call will be split by the underscores and the latter splits will be supplied as arguments to the second argument and beyond. For example, if <code>density = "dt_2"</code> is specified, the density used will be that of a t-distribution with 2 degrees of freedom. Using a t-distribution can be useful when extreme outcome values are observed (Naimi et al., 2014).
</p>
<p>Can also be <code>"kernel"</code> to use kernel density estimation, which calls <code><a href="stats.html#topic+density">density()</a></code> to estimate the numerator and denominator densities for the weights. (This used to be requested by setting <code>use.kernel = TRUE</code>, which is now deprecated.)</p>
</dd>
<dt><code>bw</code>, <code>adjust</code>, <code>kernel</code>, <code>n</code></dt><dd><p>If <code>density = "kernel"</code>, the arguments to <code><a href="stats.html#topic+density">density()</a></code>. The defaults are the same as those in <code>density()</code> except that <code>n</code> is 10 times the number of units in the sample.</p>
</dd>
<dt><code>plot</code></dt><dd><p>If <code>density = "kernel"</code>, whether to plot the estimated densities.</p>
</dd>
<dt><code>link</code></dt><dd><p>The link used to fit the linear model for the generalized propensity score. Can be any allowed by <code><a href="stats.html#topic+gaussian">gaussian()</a></code>.
</p>
</dd>
</dl>

<p>Additional arguments to <code>glm()</code> can be specified as well when it is used
for fitting. The <code>method</code> argument in <code>glm()</code> is renamed to <code>glm.method</code>.
This can be used to supply alternative fitting functions, such as those
implemented in the <a href="https://CRAN.R-project.org/package=glm2"><span class="pkg">glm2</span></a> package. Other arguments to <code>weightit()</code>
are passed to <code>...</code> in <code>glm()</code>. In the presence of missing data with <code>link = "logit"</code> and <code>missing = "saem"</code>, additional arguments are passed to
<code><a href="misaem.html#topic+miss.glm">misaem::miss.glm()</a></code> and <code><a href="misaem.html#topic+predict.miss.glm">misaem::predict.miss.glm()</a></code>, except the
<code>method</code> argument in <code><a href="misaem.html#topic+predict.miss.glm">misaem::predict.miss.glm()</a></code> is replaced with
<code>saem.method</code>.
</p>
<p>For continuous treatments in the presence of missing data with <code>missing = "saem"</code>, additional arguments are passed to <code><a href="misaem.html#topic+miss.lm">misaem::miss.lm()</a></code> and
<code><a href="misaem.html#topic+predict.miss.lm">misaem::predict.miss.lm()</a></code>.
</p>


<h3>Additional Outputs</h3>


<dl>
<dt><code>obj</code></dt><dd><p>When <code>include.obj = TRUE</code>, the (generalized) propensity score model fit. For binary treatments, the output of the call to <code><a href="stats.html#topic+glm">glm()</a></code> or the requested fitting function. For multi-category treatments, the output of the call to the fitting function (or a list thereof if <code>multi.method = "glm"</code>). For continuous treatments, the output of the call to <code>glm()</code> for the predicted values in the denominator density.
</p>
</dd>
</dl>



<h3>References</h3>



<h4>Binary treatments</h4>


<ul>
<li> <p><code>estimand = "ATO"</code>
</p>
</li></ul>

<p>Li, F., Morgan, K. L., &amp; Zaslavsky, A. M. (2018). Balancing covariates via
propensity score weighting. <em>Journal of the American Statistical
Association</em>, 113(521), 390–400. <a href="https://doi.org/10.1080/01621459.2016.1260466">doi:10.1080/01621459.2016.1260466</a>
</p>

<ul>
<li> <p><code>estimand = "ATM"</code>
</p>
</li></ul>

<p>Li, L., &amp; Greene, T. (2013). A Weighting Analogue to Pair Matching in
Propensity Score Analysis. <em>The International Journal of Biostatistics</em>,
9(2). <a href="https://doi.org/10.1515/ijb-2012-0030">doi:10.1515/ijb-2012-0030</a>
</p>

<ul>
<li> <p><code>estimand = "ATOS"</code>
</p>
</li></ul>

<p>Crump, R. K., Hotz, V. J., Imbens, G. W., &amp; Mitnik, O. A. (2009). Dealing
with limited overlap in estimation of average treatment effects.
<em>Biometrika</em>, 96(1), 187–199. <a href="https://doi.org/10.1093/biomet/asn055">doi:10.1093/biomet/asn055</a>
</p>

<ul>
<li><p> Other estimands
</p>
</li></ul>

<p>Austin, P. C. (2011). An Introduction to Propensity Score Methods for
Reducing the Effects of Confounding in Observational Studies. <em>Multivariate
Behavioral Research</em>, 46(3), 399–424. <a href="https://doi.org/10.1080/00273171.2011.568786">doi:10.1080/00273171.2011.568786</a>
</p>

<ul>
<li><p> Marginal mean weighting through stratification
</p>
</li></ul>

<p>Hong, G. (2010). Marginal mean weighting through stratification: Adjustment
for selection bias in multilevel data. <em>Journal of Educational and Behavioral
Statistics</em>, 35(5), 499–531. <a href="https://doi.org/10.3102/1076998609359785">doi:10.3102/1076998609359785</a>
</p>

<ul>
<li><p> Bias-reduced logistic regression
</p>
</li></ul>

<p>See references for the <a href="https://CRAN.R-project.org/package=brglm2"><span class="pkg">brglm2</span></a> package.
</p>

<ul>
<li><p> Firth corrected logistic regression
</p>
</li></ul>

<p>Puhr, R., Heinze, G., Nold, M., Lusa, L., &amp; Geroldinger, A. (2017). Firth’s
logistic regression with rare events: Accurate effect estimates and
predictions? <em>Statistics in Medicine</em>, 36(14), 2302–2317.
<a href="https://doi.org/10.1002/sim.7273">doi:10.1002/sim.7273</a>
</p>

<ul>
<li><p> SAEM logistic regression for missing data
</p>
</li></ul>

<p>Jiang, W., Josse, J., &amp; Lavielle, M. (2019). Logistic regression with missing
covariates — Parameter estimation, model selection and prediction within a
joint-modeling framework. <em>Computational Statistics &amp; Data Analysis</em>, 106907.
<a href="https://doi.org/10.1016/j.csda.2019.106907">doi:10.1016/j.csda.2019.106907</a>
</p>



<h4>Multi-Category Treatments</h4>


<ul>
<li> <p><code>estimand = "ATO"</code>
</p>
</li></ul>

<p>Li, F., &amp; Li, F. (2019). Propensity score weighting for causal inference with
multiple treatments. <em>The Annals of Applied Statistics</em>, 13(4), 2389–2415.
<a href="https://doi.org/10.1214/19-AOAS1282">doi:10.1214/19-AOAS1282</a>
</p>

<ul>
<li> <p><code>estimand = "ATM"</code>
</p>
</li></ul>

<p>Yoshida, K., Hernández-Díaz, S., Solomon, D. H., Jackson, J. W., Gagne, J.
J., Glynn, R. J., &amp; Franklin, J. M. (2017). Matching weights to
simultaneously compare three treatment groups: Comparison to three-way
matching. <em>Epidemiology</em> (Cambridge, Mass.), 28(3), 387–395.
<a href="https://doi.org/10.1097/EDE.0000000000000627">doi:10.1097/EDE.0000000000000627</a>
</p>

<ul>
<li><p> Other estimands
</p>
</li></ul>

<p>McCaffrey, D. F., Griffin, B. A., Almirall, D., Slaughter, M. E., Ramchand,
R., &amp; Burgette, L. F. (2013). A Tutorial on Propensity Score Estimation for
Multiple Treatments Using Generalized Boosted Models. <em>Statistics in
Medicine</em>, 32(19), 3388–3414. <a href="https://doi.org/10.1002/sim.5753">doi:10.1002/sim.5753</a>
</p>

<ul>
<li><p> Marginal mean weighting through stratification
</p>
</li></ul>

<p>Hong, G. (2012). Marginal mean weighting through stratification: A
generalized method for evaluating multivalued and multiple treatments with
nonexperimental data. <em>Psychological Methods</em>, 17(1), 44–60.
<a href="https://doi.org/10.1037/a0024918">doi:10.1037/a0024918</a>
</p>



<h4>Continuous treatments</h4>

<p>Robins, J. M., Hernán, M. Á., &amp; Brumback, B. (2000). Marginal Structural
Models and Causal Inference in Epidemiology. <em>Epidemiology</em>, 11(5), 550–560.
</p>

<ul>
<li><p> Using non-normal conditional densities
</p>
</li></ul>

<p>Naimi, A. I., Moodie, E. E. M., Auger, N., &amp; Kaufman, J. S. (2014).
Constructing Inverse Probability Weights for Continuous Exposures: A
Comparison of Methods. <em>Epidemiology</em>, 25(2), 292–299.
<a href="https://doi.org/10.1097/EDE.0000000000000053">doi:10.1097/EDE.0000000000000053</a>
</p>

<ul>
<li><p> SAEM linear regression for missing data
</p>
</li></ul>

<p>Jiang, W., Josse, J., &amp; Lavielle, M. (2019). Logistic regression with missing
covariates — Parameter estimation, model selection and prediction within a
joint-modeling framework. <em>Computational Statistics &amp; Data Analysis</em>, 106907.
<a href="https://doi.org/10.1016/j.csda.2019.106907">doi:10.1016/j.csda.2019.106907</a>
</p>



<h3>See Also</h3>

<p><code><a href="#topic+weightit">weightit()</a></code>, <code><a href="#topic+weightitMSM">weightitMSM()</a></code>, <code><a href="#topic+get_w_from_ps">get_w_from_ps()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library("cobalt")
data("lalonde", package = "cobalt")

#Balancing covariates between treatment groups (binary)
(W1 &lt;- weightit(treat ~ age + educ + married +
                  nodegree + re74, data = lalonde,
                method = "glm", estimand = "ATT",
                link = "probit"))
summary(W1)
bal.tab(W1)

#Balancing covariates with respect to race (multi-category)
(W2 &lt;- weightit(race ~ age + educ + married +
                  nodegree + re74, data = lalonde,
                method = "glm", estimand = "ATE"))
summary(W2)
bal.tab(W2)

#Balancing covariates with respect to re75 (continuous)
#with kernel density estimate
(W3 &lt;- weightit(re75 ~ age + educ + married +
                  nodegree + re74, data = lalonde,
                method = "glm", density = "kernel"))
summary(W3)
bal.tab(W3)
</code></pre>

<hr>
<h2 id='method_ipt'>Inverse Probability Tilting</h2><span id='topic+method_ipt'></span>

<h3>Description</h3>

<p>This page explains the details of estimating weights using
inverse probability tilting by setting <code>method = "ipt"</code> in the call to
<code><a href="#topic+weightit">weightit()</a></code> or <code><a href="#topic+weightitMSM">weightitMSM()</a></code>. This method can be used with binary and
multi-category treatments.
</p>
<p>In general, this method relies on estimating propensity scores using a
modification of the usual generalized linear model score equations to enforce
balance and then converting those propensity scores into weights using a
formula that depends on the desired estimand. This method relies on code
written for <span class="pkg">WeightIt</span> using <code><a href="rootSolve.html#topic+multiroot">rootSolve::multiroot()</a></code>.
</p>


<h4>Binary Treatments</h4>

<p>For binary treatments, this method estimates the weights using formulas
described by Graham, Pinto, and Egel (2012). The following estimands are
allowed: ATE, ATT, and ATC. When the ATE is requested, the optimization is
run twice, once for each treatment group.
</p>



<h4>Multi-Category Treatments</h4>

<p>For multi-category treatments, this method estimates the weights using
modifications of the formulas described by Graham, Pinto, and Egel (2012).
The following estimands are allowed: ATE and ATT. When the ATE is requested,
estimation is performed once for each treatment group. When the ATT is
requested, estimation is performed once for each non-focal (i.e., control)
group.
</p>



<h4>Continuous Treatments</h4>

<p>Inverse probability tilting is not compatible with continuous treatments.
</p>



<h4>Longitudinal Treatments</h4>

<p>For longitudinal treatments, the weights are the product of the weights
estimated at each time point. This method is not guaranteed to yield exact
balance at each time point. NOTE: the use of inverse probability tilting with
longitudinal treatments has not been validated!
</p>



<h4>Sampling Weights</h4>

<p>Sampling weights are supported through <code>s.weights</code> in all scenarios.
</p>



<h4>Missing Data</h4>

<p>In the presence of missing data, the following value(s) for <code>missing</code> are
allowed:
</p>

<dl>
<dt><code>"ind"</code> (default)</dt><dd>
<p>First, for each variable with missingness, a new missingness indicator variable is created which takes the value 1 if the original covariate is <code>NA</code> and 0 otherwise. The missingness indicators are added to the model formula as main effects. The missing values in the covariates are then replaced with the covariate medians (this value is arbitrary and does not affect estimation). The weight estimation then proceeds with this new formula and set of covariates. The covariates output in the resulting <code>weightit</code> object will be the original covariates with the <code>NA</code>s.
</p>
</dd>
</dl>




<h4>M-estimation</h4>

<p>M-estimation is supported for all scenarios. See <code><a href="#topic+glm_weightit">glm_weightit()</a></code> and
<code>vignette("estimating-effects")</code> for details.
</p>



<h3>Details</h3>

<p>Inverse probability tilting (IPT) involves specifying estimating
equations that fit the parameters of two or more generalized linear models
with a modification that ensures exact balance on the covariate means. These
estimating equations are solved, and the estimated parameters are used in the
(generalized) propensity score, which is used to compute the weights.
Conceptually and mathematically, IPT is very similar to entropy balancing and
just-identified CBPS. For the ATT and ATC, entropy balancing, just-identified
CBPS, and IPT will yield identical results. For the ATE or when <code>link</code> is
specified as something other than <code>"logit"</code>, the three methods differ.
</p>
<p>Treatment effect estimates for binary treatments are consistent if the true
propensity score is a logistic regression or the outcome model is linear in
the covariates and their interaction with treatments. For entropy balancing,
this is only true for the ATT, and for just-identified CBPS, this is only
true if there is no effect modification by covariates. In this way, IPT
provides additional theoretical guarantees over the other two methods, though
potentially with some cost in precision.
</p>


<h3>Additional Arguments</h3>

<p><code>moments</code> and <code>int</code> are accepted. See
<code><a href="#topic+weightit">weightit()</a></code> for details.
</p>

<dl>
<dt><code>quantile</code></dt><dd>
<p>A named list of quantiles (values between 0 and 1) for each continuous covariate, which are used to create additional variables that when balanced ensure balance on the corresponding quantile of the variable. For example, setting <code style="white-space: pre;">&#8288;quantile = list(x1 = c(.25, .5. , .75))&#8288;</code> ensures the 25th, 50th, and 75th percentiles of <code>x1</code> in each treatment group will be balanced in the weighted sample. Can also be a single number (e.g., <code>.5</code>) or an unnamed list of length 1 (e.g., <code>list(c(.25, .5, .75))</code>) to request the same quantile(s) for all continuous covariates, or a named vector (e.g., <code>c(x1 = .5, x2 = .75)</code> to request one quantile for each covariate.
</p>
</dd>
<dt><code>link</code></dt><dd><p>the link used to determine the inverse link for computing the (generalized) propensity scores. Default is <code>"logit"</code>, which is used in the original description of the method by Graham, Pinto, and Egel (2012), but <code>"probit"</code>, <code>"cauchit"</code>, <code>"cloglog"</code>, <code>"loglog"</code>, <code>"log"</code>, and <code>"clog"</code> are also allowed. Note that negative weights are possible with these last two and they should be used with caution. An object of class <code>"link-glm"</code> can also be supplied. The argument is passed to <code><a href="stats.html#topic+quasibinomial">quasibinomial()</a></code>.
</p>
</dd>
</dl>

<p>The <code>stabilize</code> argument is ignored.
</p>


<h3>Additional Outputs</h3>


<dl>
<dt><code>obj</code></dt><dd><p>When <code>include.obj = TRUE</code>, the output of the call to <code><a href="stats.html#topic+optim">optim()</a></code>, which contains the coefficient estimates and convergence information. For ATE fits or with multi-category treatments, a list of <code>rootSolve::multiroot()</code> outputs, one for each weighted group.
</p>
</dd>
</dl>



<h3>References</h3>



<h4><code>estimand = "ATE"</code></h4>

<p>Graham, B. S., De Xavier Pinto, C. C., &amp; Egel, D. (2012). Inverse Probability
Tilting for Moment Condition Models with Missing Data. <em>The Review of
Economic Studies</em>, 79(3), 1053–1079. <a href="https://doi.org/10.1093/restud/rdr047">doi:10.1093/restud/rdr047</a>
</p>



<h4><code>estimand = "ATT"</code></h4>

<p>Sant'Anna, P. H. C., &amp; Zhao, J. (2020). Doubly robust
difference-in-differences estimators. <em>Journal of Econometrics</em>, 219(1),
101–122. <a href="https://doi.org/10.1016/j.jeconom.2020.06.003">doi:10.1016/j.jeconom.2020.06.003</a>
</p>



<h3>See Also</h3>

<p><code><a href="#topic+weightit">weightit()</a></code>, <code><a href="#topic+weightitMSM">weightitMSM()</a></code>
</p>
<p><a href="#topic+method_ebal">method_ebal</a> and <a href="#topic+method_cbps">method_cbps</a> for entropy balancing and CBPS, which work
similarly.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data("lalonde", package = "cobalt")

#Balancing covariates between treatment groups (binary)
(W1 &lt;- weightit(treat ~ age + educ + married +
                  nodegree + re74, data = lalonde,
                method = "ipt", estimand = "ATT"))
summary(W1)
cobalt::bal.tab(W1)

#Balancing covariates with respect to race (multi-category)
(W2 &lt;- weightit(race ~ age + educ + married +
                  nodegree + re74, data = lalonde,
                method = "ipt", estimand = "ATE"))
summary(W2)
cobalt::bal.tab(W2)

</code></pre>

<hr>
<h2 id='method_npcbps'>Nonparametric Covariate Balancing Propensity Score Weighting</h2><span id='topic+method_npcbps'></span>

<h3>Description</h3>

<p>This page explains the details of estimating weights from
nonparametric covariate balancing propensity scores by setting <code>method = "npcbps"</code> in the call to <code><a href="#topic+weightit">weightit()</a></code> or <code><a href="#topic+weightitMSM">weightitMSM()</a></code>. This method can be
used with binary, multi-category, and continuous treatments.
</p>
<p>In general, this method relies on estimating weights by maximizing the
empirical likelihood of the data subject to balance constraints. This method
relies on <code><a href="CBPS.html#topic+npCBPS">CBPS::npCBPS()</a></code> from the <a href="https://CRAN.R-project.org/package=CBPS"><span class="pkg">CBPS</span></a> package.
</p>


<h4>Binary Treatments</h4>

<p>For binary treatments, this method estimates the weights using
<code><a href="CBPS.html#topic+npCBPS">CBPS::npCBPS()</a></code>. The ATE is the only estimand allowed. The weights are
taken from the output of the <code>npCBPS</code> fit object.
</p>



<h4>Multi-Category Treatments</h4>

<p>For multi-category treatments, this method estimates the weights using
<code><a href="CBPS.html#topic+npCBPS">CBPS::npCBPS()</a></code>. The ATE is the only estimand allowed. The weights are
taken from the output of the <code>npCBPS</code> fit object.
</p>



<h4>Continuous Treatments</h4>

<p>For continuous treatments, this method estimates the weights using
<code><a href="CBPS.html#topic+npCBPS">CBPS::npCBPS()</a></code>. The weights are taken from the output of the <code>npCBPS</code>
fit object.
</p>



<h4>Longitudinal Treatments</h4>

<p>For longitudinal treatments, the weights are the product of the weights
estimated at each time point. This is not how <code><a href="CBPS.html#topic+CBMSM">CBPS::CBMSM()</a></code> estimates
weights for longitudinal treatments.
</p>



<h4>Sampling Weights</h4>

<p>Sampling weights are <b>not</b> supported with <code>method = "npcbps"</code>.
</p>



<h4>Missing Data</h4>

<p>In the presence of missing data, the following value(s) for <code>missing</code> are
allowed:
</p>

<dl>
<dt><code>"ind"</code> (default)</dt><dd><p>First, for each variable with missingness, a new missingness indicator variable is created which takes the value 1 if the original covariate is <code>NA</code> and 0 otherwise. The missingness indicators are added to the model formula as main effects. The missing values in the covariates are then replaced with the covariate medians (this value is arbitrary and does not affect estimation). The weight estimation then proceeds with this new formula and set of covariates. The covariates output in the resulting <code>weightit</code> object will be the original covariates with the <code>NA</code>s.
</p>
</dd>
</dl>




<h4>M-estimation</h4>

<p>M-estimation is not supported.
</p>



<h3>Details</h3>

<p>Nonparametric CBPS involves the specification of a constrained
optimization problem over the weights. The constraints correspond to
covariate balance, and the loss function is the empirical likelihood of the
data given the weights. npCBPS is similar to <a href="#topic+method_ebal">entropy
balancing</a> and will generally produce similar results. Because the
optimization problem of npCBPS is not convex it can be slow to converge or
not converge at all, so approximate balance is allowed instead using the
<code>cor.prior</code> argument, which controls the average deviation from zero
correlation between the treatment and covariates allowed.
</p>


<h3>Additional Arguments</h3>

<p><code>moments</code> and <code>int</code> are accepted. See
<code><a href="#topic+weightit">weightit()</a></code> for details.
</p>

<dl>
<dt><code>quantile</code></dt><dd>
<p>A named list of quantiles (values between 0 and 1) for each continuous covariate, which are used to create additional variables that when balanced ensure balance on the corresponding quantile of the variable. For example, setting <code style="white-space: pre;">&#8288;quantile = list(x1 = c(.25, .5. , .75))&#8288;</code> ensures the 25th, 50th, and 75th percentiles of <code>x1</code> in each treatment group will be balanced in the weighted sample. Can also be a single number (e.g., <code>.5</code>) or an unnamed list of length 1 (e.g., <code>list(c(.25, .5, .75))</code>) to request the same quantile(s) for all continuous covariates, or a named vector (e.g., <code>c(x1 = .5, x2 = .75)</code> to request one quantile for each covariate. Only allowed with binary and multi-category treatments.
</p>
</dd>
</dl>

<p>All arguments to <code>npCBPS()</code> can be passed through <code>weightit()</code> or
<code>weightitMSM()</code>.
</p>
<p>All arguments take on the defaults of those in <code>npCBPS()</code>.
</p>


<h3>Additional Outputs</h3>


<dl>
<dt><code>obj</code></dt><dd><p>When <code>include.obj = TRUE</code>, the nonparametric CB(G)PS model fit. The output of the call to <code><a href="CBPS.html#topic+npCBPS">CBPS::npCBPS()</a></code>.
</p>
</dd>
</dl>



<h3>References</h3>

<p>Fong, C., Hazlett, C., &amp; Imai, K. (2018). Covariate balancing
propensity score for a continuous treatment: Application to the efficacy of
political advertisements. <em>The Annals of Applied Statistics</em>, 12(1), 156–177.
<a href="https://doi.org/10.1214/17-AOAS1101">doi:10.1214/17-AOAS1101</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+weightit">weightit()</a></code>, <code><a href="#topic+weightitMSM">weightitMSM()</a></code>, <code><a href="#topic+method_cbps">method_cbps</a></code>
</p>
<p><code><a href="CBPS.html#topic+npCBPS">CBPS::npCBPS()</a></code> for the fitting function
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Examples take a long time to run
library("cobalt")
data("lalonde", package = "cobalt")

  #Balancing covariates between treatment groups (binary)
  (W1 &lt;- weightit(treat ~ age + educ + married +
                    nodegree + re74, data = lalonde,
                  method = "npcbps", estimand = "ATE"))
  summary(W1)
  bal.tab(W1)

  #Balancing covariates with respect to race (multi-category)
  (W2 &lt;- weightit(race ~ age + educ + married +
                    nodegree + re74, data = lalonde,
                  method = "npcbps", estimand = "ATE"))
  summary(W2)
  bal.tab(W2)


</code></pre>

<hr>
<h2 id='method_optweight'>Optimization-Based Weighting</h2><span id='topic+method_optweight'></span>

<h3>Description</h3>

<p>This page explains the details of estimating optimization-based
weights (also known as stable balancing weights) by setting <code>method = "optweight"</code> in the call to <code><a href="#topic+weightit">weightit()</a></code> or <code><a href="#topic+weightitMSM">weightitMSM()</a></code>. This method can
be used with binary, multi-category, and continuous treatments.
</p>
<p>In general, this method relies on estimating weights by solving a quadratic
programming problem subject to approximate or exact balance constraints. This
method relies on <code><a href="optweight.html#topic+optweight">optweight::optweight()</a></code> from the <a href="https://CRAN.R-project.org/package=optweight"><span class="pkg">optweight</span></a>
package.
</p>
<p>Because <code>optweight()</code> offers finer control and uses the same syntax as
<code>weightit()</code>, it is recommended that <code><a href="optweight.html#topic+optweight">optweight::optweight()</a></code> be used
instead of <code>weightit()</code> with <code>method = "optweight"</code>.
</p>


<h4>Binary Treatments</h4>

<p>For binary treatments, this method estimates the weights using
<code><a href="optweight.html#topic+optweight">optweight::optweight()</a></code>. The following estimands are allowed: ATE, ATT,
and ATC. The weights are taken from the output of the <code>optweight</code> fit object.
</p>



<h4>Multi-Category Treatments</h4>

<p>For multi-category treatments, this method estimates the weights using
<code><a href="optweight.html#topic+optweight">optweight::optweight()</a></code>. The following estimands are allowed: ATE and
ATT. The weights are taken from the output of the <code>optweight</code> fit object.
</p>



<h4>Continuous Treatments</h4>

<p>For binary treatments, this method estimates the weights using
<code><a href="optweight.html#topic+optweight">optweight::optweight()</a></code>. The weights are taken from the output of the
<code>optweight</code> fit object.
</p>



<h4>Longitudinal Treatments</h4>

<p>For longitudinal treatments, <code>optweight()</code> estimates weights that
simultaneously satisfy balance constraints at all time points, so only one
model is fit to obtain the weights. Using <code>method = "optweight"</code> in
<code>weightitMSM()</code> causes <code>is.MSM.method</code> to be set to <code>TRUE</code> by default. Setting
it to <code>FALSE</code> will run one model for each time point and multiply the weights
together, a method that is not recommended. NOTE: neither use of
optimization-based weights with longitudinal treatments has been validated!
</p>



<h4>Sampling Weights</h4>

<p>Sampling weights are supported through <code>s.weights</code> in all scenarios, but only
for versions of <span class="pkg">optweight</span> after 0.2.5.
</p>



<h4>Missing Data</h4>

<p>In the presence of missing data, the following value(s) for <code>missing</code> are
allowed:
</p>

<dl>
<dt><code>"ind"</code> (default)</dt><dd><p>First, for each variable with missingness, a new missingness indicator variable is created which takes the value 1 if the original covariate is <code>NA</code> and 0 otherwise. The missingness indicators are added to the model formula as main effects. The missing values in the covariates are then replaced with the covariate medians (this value is arbitrary and does not affect estimation). The weight estimation then proceeds with this new formula and set of covariates. The covariates output in the resulting <code>weightit</code> object will be the original covariates with the <code>NA</code>s.
</p>
</dd>
</dl>




<h4>M-estimation</h4>

<p>M-estimation is not supported.
</p>



<h3>Details</h3>

<p>Stable balancing weights are weights that solve a constrained
optimization problem, where the constraints correspond to covariate balance
and the loss function is the variance (or other norm) of the weights. These
weights maximize the effective sample size of the weighted sample subject to
user-supplied balance constraints. An advantage of this method over entropy
balancing is the ability to allow approximate, rather than exact, balance
through the <code>tols</code> argument, which can increase precision even for slight
relaxations of the constraints.
</p>
<p><code>plot()</code> can be used on the output of <code>weightit()</code> with <code>method = "optweight"</code>
to display the dual variables; see Examples and <code><a href="#topic+plot.weightit">plot.weightit()</a></code> for more
details.
</p>


<h3>Additional Arguments</h3>

<p><code>moments</code> and <code>int</code> are accepted. See
<code><a href="#topic+weightit">weightit()</a></code> for details.
</p>

<dl>
<dt><code>quantile</code></dt><dd>
<p>A named list of quantiles (values between 0 and 1) for each continuous covariate, which are used to create additional variables that when balanced ensure balance on the corresponding quantile of the variable. For example, setting <code style="white-space: pre;">&#8288;quantile = list(x1 = c(.25, .5. , .75))&#8288;</code> ensures the 25th, 50th, and 75th percentiles of <code>x1</code> in each treatment group will be balanced in the weighted sample. Can also be a single number (e.g., <code>.5</code>) or an unnamed list of length 1 (e.g., <code>list(c(.25, .5, .75))</code>) to request the same quantile(s) for all continuous covariates, or a named vector (e.g., <code>c(x1 = .5, x2 = .75)</code> to request one quantile for each covariate. Only allowed with binary and multi-category treatments.
</p>
</dd>
</dl>

<p>All arguments to <code>optweight()</code> can be passed through <code>weightit()</code> or
<code>weightitMSM()</code>, with the following exception:
</p>

<ul>
<li> <p><code>targets</code> cannot be used and is ignored.
</p>
</li></ul>

<p>All arguments take on the defaults of those in <code>optweight()</code>.
</p>


<h3>Additional Outputs</h3>


<dl>
<dt><code>info</code></dt><dd>
<p>A list with one entry:
</p>

<dl>
<dt><code>duals</code></dt><dd><p>A data frame of dual variables for each balance constraint.</p>
</dd>
</dl>

</dd>
<dt><code>obj</code></dt><dd><p>When <code>include.obj = TRUE</code>, the output of the call to <code><a href="optweight.html#topic+optweight">optweight::optweight()</a></code>.</p>
</dd>
</dl>



<h3>Note</h3>

<p>The specification of <code>tols</code> differs between <code>weightit()</code> and
<code>optweight()</code>. In <code>weightit()</code>, one tolerance value should be included per
level of each factor variable, whereas in <code>optweight()</code>, all levels of a
factor are given the same tolerance, and only one value needs to be supplied
for a factor variable. Because of the potential for confusion and ambiguity,
it is recommended to only supply one value for <code>tols</code> in <code>weightit()</code> that
applies to all variables. For finer control, use <code>optweight()</code> directly.
</p>
<p>Seriously, just use <code><a href="optweight.html#topic+optweight">optweight::optweight()</a></code>. The syntax is almost
identical and it's compatible with <span class="pkg">cobalt</span>, too.
</p>


<h3>References</h3>



<h4>Binary treatments</h4>

<p>Wang, Y., &amp; Zubizarreta, J. R. (2020). Minimal dispersion approximately
balancing weights: Asymptotic properties and practical considerations.
<em>Biometrika</em>, 107(1), 93–105. <a href="https://doi.org/10.1093/biomet/asz050">doi:10.1093/biomet/asz050</a>
</p>
<p>Zubizarreta, J. R. (2015). Stable Weights that Balance Covariates for
Estimation With Incomplete Outcome Data. <em>Journal of the American Statistical
Association</em>, 110(511), 910–922. <a href="https://doi.org/10.1080/01621459.2015.1023805">doi:10.1080/01621459.2015.1023805</a>
</p>



<h4>Multi-Category Treatments</h4>

<p>de los Angeles Resa, M., &amp; Zubizarreta, J. R. (2020). Direct and stable weight
adjustment in non-experimental studies with multivalued treatments: Analysis
of the effect of an earthquake on post-traumatic stress. <em>Journal of the Royal
Statistical Society: Series A (Statistics in Society)</em>, n/a(n/a).
<a href="https://doi.org/10.1111/rssa.12561">doi:10.1111/rssa.12561</a>
</p>



<h4>Continuous treatments</h4>

<p>Greifer, N. (2020). <em>Estimating Balancing Weights for Continuous Treatments
Using Constrained Optimization</em>. <a href="https://doi.org/10.17615/DYSS-B342">doi:10.17615/DYSS-B342</a>
</p>



<h3>See Also</h3>

<p><code><a href="#topic+weightit">weightit()</a></code>, <code><a href="#topic+weightitMSM">weightitMSM()</a></code>
</p>
<p><code><a href="optweight.html#topic+optweight">optweight::optweight()</a></code> for the fitting function
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data("lalonde", package = "cobalt")

#Balancing covariates between treatment groups (binary)
(W1 &lt;- weightit(treat ~ age + educ + married +
                  nodegree + re74, data = lalonde,
                method = "optweight", estimand = "ATT",
                tols = 0))
summary(W1)
cobalt::bal.tab(W1)
plot(W1)

#Balancing covariates with respect to race (multi-category)
(W2 &lt;- weightit(race ~ age + educ + married +
                  nodegree + re74, data = lalonde,
                method = "optweight", estimand = "ATE",
                tols = .01))
summary(W2)
cobalt::bal.tab(W2)
plot(W2)

#Balancing covariates with respect to re75 (continuous)

</code></pre>

<hr>
<h2 id='method_super'>Propensity Score Weighting Using SuperLearner</h2><span id='topic+method_super'></span>

<h3>Description</h3>

<p>This page explains the details of estimating weights from
SuperLearner-based propensity scores by setting <code>method = "super"</code> in the
call to <code><a href="#topic+weightit">weightit()</a></code> or <code><a href="#topic+weightitMSM">weightitMSM()</a></code>. This method can be used with binary,
multi-category, and continuous treatments.
</p>
<p>In general, this method relies on estimating propensity scores using the
SuperLearner algorithm for stacking predictions and then converting those
propensity scores into weights using a formula that depends on the desired
estimand. For binary and multi-category treatments, one or more binary
classification algorithms are used to estimate the propensity scores as the
predicted probability of being in each treatment given the covariates. For
continuous treatments, regression algorithms are used to estimate generalized
propensity scores as the conditional density of treatment given the
covariates. This method relies on <code><a href="SuperLearner.html#topic+SuperLearner">SuperLearner::SuperLearner()</a></code> from
the <a href="https://CRAN.R-project.org/package=SuperLearner"><span class="pkg">SuperLearner</span></a> package.
</p>


<h4>Binary Treatments</h4>

<p>For binary treatments, this method estimates the propensity scores using
<code><a href="SuperLearner.html#topic+SuperLearner">SuperLearner::SuperLearner()</a></code>. The following estimands are allowed:
ATE, ATT, ATC, ATO, ATM, and ATOS. Weights can also be computed using
marginal mean weighting through stratification for the ATE, ATT, and ATC. See
<code><a href="#topic+get_w_from_ps">get_w_from_ps()</a></code> for details.
</p>



<h4>Multi-Category Treatments</h4>

<p>For multi-category treatments, the propensity scores are estimated using
several calls to <code><a href="SuperLearner.html#topic+SuperLearner">SuperLearner::SuperLearner()</a></code>, one for each treatment
group; the treatment probabilities are not normalized to sum to 1. The
following estimands are allowed: ATE, ATT, ATC, ATO, and ATM. The weights for
each estimand are computed using the standard formulas or those mentioned
above. Weights can also be computed using marginal mean weighting through
stratification for the ATE, ATT, and ATC. See <code><a href="#topic+get_w_from_ps">get_w_from_ps()</a></code> for details.
</p>



<h4>Continuous Treatments</h4>

<p>For continuous treatments, the generalized propensity score is estimated
using <code><a href="SuperLearner.html#topic+SuperLearner">SuperLearner::SuperLearner()</a></code>. In addition, kernel density
estimation can be used instead of assuming a normal density for the numerator
and denominator of the generalized propensity score by setting <code>density = "kernel"</code>. Other arguments to <code><a href="stats.html#topic+density">density()</a></code> can be specified to refine the
density estimation parameters. <code>plot = TRUE</code> can be specified to plot the
density for the numerator and denominator, which can be helpful in diagnosing
extreme weights.
</p>



<h4>Longitudinal Treatments</h4>

<p>For longitudinal treatments, the weights are the product of the weights
estimated at each time point.
</p>



<h4>Sampling Weights</h4>

<p>Sampling weights are supported through <code>s.weights</code> in all scenarios.
</p>



<h4>Missing Data</h4>

<p>In the presence of missing data, the following value(s) for <code>missing</code> are
allowed:
</p>

<dl>
<dt><code>"ind"</code> (default)</dt><dd><p>First, for each variable with missingness, a new missingness indicator variable is created which takes the value 1 if the original covariate is <code>NA</code> and 0 otherwise. The missingness indicators are added to the model formula as main effects. The missing values in the covariates are then replaced with the covariate medians (this value is arbitrary and does not affect estimation). The weight estimation then proceeds with this new formula and set of covariates. The covariates output in the resulting <code>weightit</code> object will be the original covariates with the <code>NA</code>s.
</p>
</dd>
</dl>




<h4>M-estimation</h4>

<p>M-estimation is not supported.
</p>



<h3>Details</h3>

<p>SuperLearner works by fitting several machine learning models to the
treatment and covariates and then taking a weighted combination of the
generated predicted values to use as the propensity scores, which are then
used to construct weights. The machine learning models used are supplied
using the <code>SL.library</code> argument; the more models are supplied, the higher the
chance of correctly modeling the propensity score. It is a good idea to
include parameteric models, flexible and tree-based models, and regularized
models among the models selected. The predicted values are combined using the
method supplied in the <code>SL.method</code> argument (which is nonnegative least
squares by default). A benefit of SuperLearner is that, asymptotically, it is
guaranteed to perform as well as or better than the best-performing method
included in the library. Using Balance SuperLearner by setting <code>SL.method = "method.balance"</code> works by selecting the combination of predicted values that
minimizes an imbalance measure.
</p>


<h3>Additional Arguments</h3>


<dl>
<dt><code>discrete</code></dt><dd><p>if <code>TRUE</code>, uses discrete SuperLearner, which simply selects the best performing method. Default <code>FALSE</code>, which finds the optimal combination of predictions for the libraries using <code>SL.method</code>.</p>
</dd>
</dl>

<p>An argument to <code>SL.library</code> <strong>must</strong> be supplied. To see a list of
available entries, use <code><a href="SuperLearner.html#topic+listWrappers">SuperLearner::listWrappers()</a></code>.
</p>
<p>All arguments to <code><a href="SuperLearner.html#topic+SuperLearner">SuperLearner::SuperLearner()</a></code> can be passed through
<code>weightit()</code> or <code>weightitMSM()</code>, with the following exceptions:
</p>

<ul>
<li> <p><code>obsWeights</code> is ignored because sampling weights are passed using <code>s.weights</code>.
</p>
</li>
<li> <p><code>method</code> in <code>SuperLearner()</code> is replaced with the argument <code>SL.method</code> in <code>weightit()</code>.
</p>
</li></ul>

<p>For continuous treatments only, the following arguments may be supplied:
</p>

<dl>
<dt><code>density</code></dt><dd><p>A function corresponding to the conditional density of the treatment. The standardized residuals of the treatment model will be fed through this function to produce the numerator and denominator of the generalized propensity score weights. If blank, <code><a href="stats.html#topic+dnorm">dnorm()</a></code> is used as recommended by Robins et al. (2000). This can also be supplied as a string containing the name of the function to be called. If the string contains underscores, the call will be split by the underscores and the latter splits will be supplied as arguments to the second argument and beyond. For example, if <code>density = "dt_2"</code> is specified, the density used will be that of a t-distribution with 2 degrees of freedom. Using a t-distribution can be useful when extreme outcome values are observed (Naimi et al., 2014).
</p>
<p>Can also be <code>"kernel"</code> to use kernel density estimation, which calls <code><a href="stats.html#topic+density">density()</a></code> to estimate the numerator and denominator densities for the weights. (This used to be requested by setting <code>use.kernel = TRUE</code>, which is now deprecated.)</p>
</dd>
<dt><code>bw</code>, <code>adjust</code>, <code>kernel</code>, <code>n</code></dt><dd><p>If <code>density = "kernel"</code>, the arguments to <code><a href="stats.html#topic+density">density()</a></code>. The defaults are the same as those in <code>density()</code> except that <code>n</code> is 10 times the number of units in the sample.</p>
</dd>
<dt><code>plot</code></dt><dd><p>If <code>density = "kernel"</code>, whether to plot the estimated densities.</p>
</dd>
</dl>



<h4>Balance SuperLearner</h4>

<p>In addition to the methods allowed by <code>SuperLearner()</code>, one can specify
<code>SL.method = "method.balance"</code> to use &quot;Balance SuperLearner&quot; as described
by Pirracchio and Carone (2018), wherein covariate balance is used to
choose the optimal combination of the predictions from the methods
specified with <code>SL.library</code>. Coefficients are chosen (one for each
prediction method) so that the weights generated from the weighted
combination of the predictions optimize a balance criterion, which must be
set with the <code>criterion</code> argument, described below.
</p>

<dl>
<dt><code>criterion</code></dt><dd><p>A string describing the balance criterion used to select the best weights. See <code><a href="cobalt.html#topic+bal.compute">cobalt::bal.compute()</a></code> for allowable options for each treatment type. For binary and multi-category treatments, the default is <code>"smd.mean"</code>, which minimizes the average absolute standard mean difference among the covariates between treatment groups. For continuous treatments, the default is <code>"p.mean"</code>, which minimizes the average absolute Pearson correlation between the treatment and covariates.
</p>
</dd>
</dl>

<p>Note that this implementation differs from that of Pirracchio and Carone
(2018) in that here, balance is measured only on the terms included in the
model formula (i.e., and not their interactions unless specifically
included), and balance results from a sample weighted using the estimated
predicted values as propensity scores, not a sample matched using
propensity score matching on the predicted values. Binary and continuous
treatments are supported, but currently multi-category treatments are not.
</p>



<h3>Additional Outputs</h3>


<dl>
<dt><code>info</code></dt><dd>
<p>For binary and continuous treatments, a list with two entries, <code>coef</code> and <code>cvRisk</code>. For multi-category treatments, a list of lists with these two entries, one for each treatment level.
</p>

<dl>
<dt><code>coef</code></dt><dd>
<p>The coefficients in the linear combination of the predictions from each method in <code>SL.library</code>. Higher values indicate that the corresponding method plays a larger role in determining the resulting predicted value, and values close to zero indicate that the method plays little role in determining the predicted value. When <code>discrete = TRUE</code>, these correspond to the coefficients that would have been estimated had <code>discrete</code> been <code>FALSE</code>.
</p>
</dd>
<dt><code>cvRisk</code></dt><dd>
<p>The cross-validation risk for each method in <code>SL.library</code>. Higher values indicate that the method has worse cross-validation accuracy. When <code>SL.method = "method.balance"</code>, the sample weighted balance statistic requested with <code>criterion</code>. Higher values indicate worse balance.
</p>
</dd>
</dl>

</dd>
<dt><code>obj</code></dt><dd>
<p>When <code>include.obj = TRUE</code>, the SuperLearner fit(s) used to generate the predicted values. For binary and continuous treatments, the output of the call to <code><a href="SuperLearner.html#topic+SuperLearner">SuperLearner::SuperLearner()</a></code>. For multi-category treatments, a list of outputs to calls to <code>SuperLearner::SuperLearner()</code>.
</p>
</dd>
</dl>



<h3>Note</h3>

<p>Some methods formerly available in <span class="pkg">SuperLearner</span> are now in
<span class="pkg">SuperLearnerExtra</span>, which can be found on GitHub at
<a href="https://github.com/ecpolley/SuperLearnerExtra">https://github.com/ecpolley/SuperLearnerExtra</a>.
</p>
<p>The <code>criterion</code> argument used to be called <code>stop.method</code>, which is its name
in <span class="pkg">twang</span>. <code>stop.method</code> still works for backward compatibility.
Additionally, the criteria formerly named as <code>es.mean</code>, <code>es.max</code>, and
<code>es.rms</code> have been renamed to <code>smd.mean</code>, <code>smd.max</code>, and <code>smd.rms</code>. The
former are used in <span class="pkg">twang</span> and will still work with <code>weightit()</code> for
backward compatibility.
</p>
<p>As of version 1.2.0, the default behavior for binary and multi-category
treatments is to stratify on the treatment when performing cross-validation
to ensure all treatment groups are represented in cross-validation. To
recover previous behavior, set <code>cvControl = list(stratifyCV = FALSE)</code>.
</p>


<h3>References</h3>



<h4>Binary treatments</h4>

<p>Pirracchio, R., Petersen, M. L., &amp; van der Laan, M. (2015). Improving
Propensity Score Estimators’ Robustness to Model Misspecification Using Super
Learner. <em>American Journal of Epidemiology</em>, 181(2), 108–119.
<a href="https://doi.org/10.1093/aje/kwu253">doi:10.1093/aje/kwu253</a>
</p>



<h4>Continuous treatments</h4>

<p>Kreif, N., Grieve, R., Díaz, I., &amp; Harrison, D. (2015). Evaluation of the
Effect of a Continuous Treatment: A Machine Learning Approach with an
Application to Treatment for Traumatic Brain Injury. <em>Health Economics</em>,
24(9), 1213–1228. <a href="https://doi.org/10.1002/hec.3189">doi:10.1002/hec.3189</a>
</p>



<h4>Balance SuperLearner (<code>SL.method = "method.balance"</code>)</h4>

<p>Pirracchio, R., &amp; Carone, M. (2018). The Balance Super Learner: A robust
adaptation of the Super Learner to improve estimation of the average
treatment effect in the treated based on propensity score matching.
<em>Statistical Methods in Medical Research</em>, 27(8), 2504–2518.
<a href="https://doi.org/10.1177/0962280216682055">doi:10.1177/0962280216682055</a>
</p>
<p>See <code><a href="#topic+method_glm">method_glm</a></code> for additional references.
</p>



<h3>See Also</h3>

<p><code><a href="#topic+weightit">weightit()</a></code>, <code><a href="#topic+weightitMSM">weightitMSM()</a></code>, <code><a href="#topic+get_w_from_ps">get_w_from_ps()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library("cobalt")
data("lalonde", package = "cobalt")

#Note: for time, all exmaples use a small set of
#      learners. Many more should be added if
#      possible, including a variety of model
#      types (e.g., parametric, flexible, tree-
#.     based, regularized, etc.)

#Balancing covariates between treatment groups (binary)
(W1 &lt;- weightit(treat ~ age + educ + married +
                  nodegree + re74, data = lalonde,
                method = "super", estimand = "ATT",
                SL.library = c("SL.glm", "SL.stepAIC",
                               "SL.glm.interaction")))
summary(W1)
bal.tab(W1)

#Balancing covariates with respect to race (multi-category)
(W2 &lt;- weightit(race ~ age + educ + married +
                  nodegree + re74, data = lalonde,
                method = "super", estimand = "ATE",
                SL.library = c("SL.glm", "SL.stepAIC",
                               "SL.glm.interaction")))
summary(W2)
bal.tab(W2)

#Balancing covariates with respect to re75 (continuous)
#assuming t(8) conditional density for treatment
(W3 &lt;- weightit(re75 ~ age + educ + married +
                  nodegree + re74, data = lalonde,
                method = "super", density = "dt_8",
                SL.library = c("SL.glm", "SL.ridge",
                               "SL.glm.interaction")))
summary(W3)
bal.tab(W3)

#Balancing covariates between treatment groups (binary)
# using balance SuperLearner to minimize the maximum
# KS statistic
(W4 &lt;- weightit(treat ~ age + educ + married +
                  nodegree + re74, data = lalonde,
                method = "super", estimand = "ATT",
                SL.library = c("SL.glm", "SL.stepAIC",
                               "SL.lda"),
                SL.method = "method.balance",
                criterion = "ks.max"))
summary(W4)
bal.tab(W4, stats = c("m", "ks"))

</code></pre>

<hr>
<h2 id='method_user'>User-Defined Functions for Estimating Weights</h2><span id='topic+method_user'></span>

<h3>Description</h3>

<p>This page explains the details of estimating weights using a user-defined function. The function must take in arguments that are passed to it by <code><a href="#topic+weightit">weightit()</a></code> or <code><a href="#topic+weightitMSM">weightitMSM()</a></code> and return a vector of weights or a list containing the weights.
</p>
<p>To supply a user-defined function, the function object should be entered directly to <code>method</code>; for example, for a function <code>fun</code>, <code>method = fun</code>.
</p>


<h4>Point Treatments</h4>

<p>The following arguments are automatically passed to the user-defined function, which should have named parameters corresponding to them:
</p>

<ul>
<li><p><code>treat</code>: a vector of treatment status for each unit. This comes directly from the left hand side of the formula passed to <code>weightit()</code> and so will have it's type (e.g., numeric, factor, etc.), which may need to be converted.
</p>
</li>
<li><p><code>covs</code>: a data frame of covariate values for each unit. This comes directly from the right hand side of the formula passed to <code>weightit()</code>. The covariates are processed so that all columns are numeric; all factor variables are split into dummies and all interactions are evaluated. All levels of factor variables are given dummies, so the matrix of the covariates is not full rank. Users can use <code><a href="#topic+make_full_rank">make_full_rank()</a></code>, which accepts a numeric matrix or data frame and removes columns to make it full rank, if a full rank covariate matrix is desired.
</p>
</li>
<li><p><code>s.weights</code>: a numeric vector of sampling weights, one for each unit.
</p>
</li>
<li><p><code>ps</code>: a numeric vector of propensity scores.
</p>
</li>
<li><p><code>subset</code>: a logical vector the same length as <code>treat</code> that is <code>TRUE</code> for units to be included in the estimation and <code>FALSE</code> otherwise. This is used to subset the input objects when <code>exact</code> is used. <code>treat</code>, <code>covs</code>, <code>s.weights</code>, and <code>ps</code>, if supplied, will already have been subsetted by <code>subset</code>.
</p>
</li>
<li><p><code>estimand</code>: a character vector of length 1 containing the desired estimand. The characters will have been converted to uppercase. If &quot;ATC&quot; was supplied to estimand, <code>weightit()</code> sets <code>focal</code> to the control level (usually 0 or the lowest level of <code>treat</code>) and sets <code>estimand</code> to &quot;ATT&quot;.
</p>
</li>
<li><p><code>focal</code>: a character vector of length 1 containing the focal level of the treatment when the estimand is the ATT (or the ATC as detailed above). <code>weightit()</code> ensures the value of focal is a level of <code>treat</code>.
</p>
</li>
<li><p><code>stabilize</code>: a logical vector of length 1. It is not processed by <code>weightit()</code> before it reaches the fitting function.
</p>
</li>
<li><p><code>moments</code>: a numeric vector of length 1. It is not processed by <code>weightit()</code> before it reaches the fitting function except that <code>as.integer()</code> is applied to it. This is used in other methods to determine whether polynomials of the entered covariates are to be used in the weight estimation.
</p>
</li>
<li><p><code>int</code>: a logical vector of length 1. It is not processed by <code>weightit()</code> before it reaches the fitting function. This is used in other methods to determine whether interactions of the entered covariates are to be used in the weight estimation.
</p>
</li></ul>

<p>None of these parameters are required to be in the fitting function. These are simply those that are automatically available.
</p>
<p>In addition, any additional arguments supplied to <code>weightit()</code> will be passed on to the fitting function. <code>weightit()</code> ensures the arguments correspond to the parameters of the fitting function and throws an error if an incorrectly named argument is supplied and the fitting function doesn't include <code style="white-space: pre;">&#8288;\dots&#8288;</code> as a parameter.
</p>
<p>The fitting function must output either a numeric vector of weights or a list (or list-like object) with an entry named wither &quot;w&quot; or &quot;weights&quot;. If a list, the list can contain other named entries, but only entries named &quot;w&quot;, &quot;weights&quot;, &quot;ps&quot;, and &quot;fit.obj&quot; will be processed. &quot;ps&quot; is a vector of propensity scores and &quot;fit.obj&quot; should be an object used in the fitting process that a user may want to examine and that is included in the <code>weightit</code> output object as &quot;obj&quot; when <code>include.obj = TRUE</code>. The &quot;ps&quot; and &quot;fit.obj&quot; components are optional, but &quot;weights&quot; or &quot;w&quot; is required.
</p>



<h4>Longitudinal Treatments</h4>

<p>Longitudinal treatments can be handled either by running the fitting function for point treatments for each time point and multiplying the resulting weights together or by running a method that accommodates multiple time points and outputs a single set of weights. For the former, <code>weightitMSM()</code> can be used with the user-defined function just as it is with <code>weightit()</code>. The latter method is not yet accommodated by <code>weightitMSM()</code>, but will be someday, maybe.
</p>



<h3>See Also</h3>

<p><code><a href="#topic+weightit">weightit()</a></code>, <code><a href="#topic+weightitMSM">weightitMSM()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library("cobalt")
data("lalonde", package = "cobalt")

#A user-defined version of method = "ps"
my.ps &lt;- function(treat, covs, estimand, focal = NULL) {
  covs &lt;- make_full_rank(covs)
  d &lt;- data.frame(treat, covs)
  f &lt;- formula(d)
  ps &lt;- glm(f, data = d, family = "binomial")$fitted
  w &lt;- get_w_from_ps(ps, treat = treat, estimand = estimand,
                     focal = focal)

  list(w = w, ps = ps)
}

#Balancing covariates between treatment groups (binary)
(W1 &lt;- weightit(treat ~ age + educ + married +
                  nodegree + re74, data = lalonde,
                method = my.ps, estimand = "ATT"))
summary(W1)
bal.tab(W1)

data("msmdata")
(W2 &lt;- weightitMSM(list(A_1 ~ X1_0 + X2_0,
                        A_2 ~ X1_1 + X2_1 +
                          A_1 + X1_0 + X2_0,
                        A_3 ~ X1_2 + X2_2 +
                          A_2 + X1_1 + X2_1 +
                          A_1 + X1_0 + X2_0),
                   data = msmdata,
                   method = my.ps))

summary(W2)
bal.tab(W2)

# Kernel balancing using the `kbal` package, available
# using `pak::pak_install("chadhazlett/KBAL")`.
# Only the ATT and ATC are available.

## Not run: 
  kbal.fun &lt;- function(treat, covs, estimand, focal, verbose, ...) {
    args &lt;- list(...)

    if (!estimand %in% c("ATT", "ATC"))
      stop('`estimand` must be "ATT" or "ATC".', call. = FALSE)

    treat &lt;- as.numeric(treat == focal)

    args &lt;- args[names(args) %in% names(formals(kbal::kbal))]
    args$allx &lt;- covs
    args$treatment &lt;- treat
    args$printprogress &lt;- verbose

    cat_cols &lt;- apply(covs, 2, function(x) length(unique(x)) &lt;= 2)

    if (all(cat_cols)) {
      args$cat_data &lt;- TRUE
      args$mixed_data &lt;- FALSE
      args$scale_data &lt;- FALSE
      args$linkernel &lt;- FALSE
      args$drop_MC &lt;- FALSE
    }
    else if (any(cat_cols)) {
      args$cat_data &lt;- FALSE
      args$mixed_data &lt;- TRUE
      args$cat_columns &lt;- colnames(covs)[cat_cols]
      args$allx[,!cat_cols] &lt;- scale(args$allx[,!cat_cols])
      args$cont_scale &lt;- 1
    }
    else {
      args$cat_data &lt;- FALSE
      args$mixed_data &lt;- FALSE
    }

    k.out &lt;- do.call(kbal::kbal, args)
    w &lt;- k.out$w

    list(w = w, fit.obj = k.out)
  }

  (Wk &lt;- weightit(treat ~ age + educ + married +
                    nodegree + re74, data = lalonde,
                  method = kbal.fun, estimand = "ATT",
                  include.obj = TRUE))
  summary(Wk)
  bal.tab(Wk, stats = c("m", "ks"))

## End(Not run)

</code></pre>

<hr>
<h2 id='msmdata'>Simulated data for a 3 time point sequential study</h2><span id='topic+msmdata'></span>

<h3>Description</h3>

<p>This is a simulated dataset of 7500 units with covariates and treatment
measured three times and the outcome measured at the end from a
hypothetical observational study examining the effect of treatment
delivered at each time point on an adverse event.
</p>
<p>The data were generated using a simple simulation mechanism.
For further details on how the dataset was built, see the code
at <a href="https://github.com/ngreifer/Weightit/blob/master/data-raw/msmdata.R">data-raw/msmdata.R</a>.
</p>
<p>The dataset is provided to illustrate the features of
<code>weightitMSM()</code> and is not based on a realistic data-generating
process, so it should not be used as a benchmark.
</p>
<p>For simulating realistic data with a
known data-generating mechanism, consider using the <span class="pkg">simcausal</span> package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>msmdata
</code></pre>


<h3>Format</h3>

<p>A data frame with 7500 observations on the following 10 variables.
</p>

<dl>
<dt><code>X1_0</code></dt><dd><p>a count covariate measured at baseline</p>
</dd>
<dt><code>X2_0</code></dt><dd><p>a binary covariate measured at baseline</p>
</dd>
<dt><code>A_1</code></dt><dd><p>a binary indicator of treatment status at the first time point</p>
</dd>
<dt><code>X1_1</code></dt><dd><p>a count covariate measured at the first time point (after the first treatment)</p>
</dd>
<dt><code>X2_1</code></dt><dd><p>a binary covariate measured at the first time point (after the first treatment)</p>
</dd>
<dt><code>A_2</code></dt><dd><p>a binary indicator of treatment status at the second time point</p>
</dd>
<dt><code>X1_2</code></dt><dd><p>a count covariate measured at the second time point (after the second treatment)</p>
</dd>
<dt><code>X2_2</code></dt><dd><p>a binary covariate measured at the first time point (after the first treatment)</p>
</dd>
<dt><code>A_3</code></dt><dd><p>a binary indicator of treatment status at the third time point</p>
</dd>
<dt><code>Y_B</code></dt><dd><p>a binary indicator of the outcome event (e.g., death)</p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'>
data("msmdata")

</code></pre>

<hr>
<h2 id='plot.weightit'>Plot information about the weight estimation process</h2><span id='topic+plot.weightit'></span>

<h3>Description</h3>

<p><code>plot.weightit()</code> plots information about the weights depending
on how they were estimated. Currently, only weighting using <code>method = "gbm"</code>
or <code>"optweight"</code> is supported. To plot the distribution of weights, see
<code><a href="#topic+plot.summary.weightit">plot.summary.weightit()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'weightit'
plot(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.weightit_+3A_x">x</code></td>
<td>
<p>a <code>weightit</code> object; the output of a call to <code><a href="#topic+weightit">weightit()</a></code>.</p>
</td></tr>
<tr><td><code id="plot.weightit_+3A_...">...</code></td>
<td>
<p>Unused.</p>
</td></tr>
</table>


<h3>Details</h3>



<h4><code>method = "gbm"</code></h4>

<p>After weighting with generalized boosted modeling, <code>plot()</code> displays the
results of the tuning process used to find the optimal number of trees (and
tuning parameter values, if modified) that are used in the final weights. The
plot produced has the number of trees on the x-axis and the value of the
criterion on the y axis with a diamond at the optimal point. When multiple
parameters are selected by tuning, a separate line is displayed on the plot
for each combination of tuning parameters. When <code>by</code> is used in the call to
<code>weightit()</code>, the plot is faceted by the <code>by</code> variable. See <code><a href="#topic+method_gbm">method_gbm</a></code>
for more information on selecting tuning parameters.
</p>



<h4><code>method = "optweight"</code></h4>

<p>After estimating stable balancing weights, <code>plot()</code> displays the values of
the dual variables for each balance constraint in a bar graph. Large values
of the dual variables indicate the covariates for which the balance
constraint is causing increases in the variability of the weights, i.e., the
covariates for which relaxing the imbalance tolerance would yield the
greatest gains in effective sample size. For continuous treatments, the dual
variables are split into those for the target (i.e., ensuring the mean of
each covariate after weighting is equal to its unweighted mean) and those for
balance (i.e., ensuring the treatment-covariate correlations are no larger
than the imbalance tolerance). This is essentially a wrapper for
<code><a href="optweight.html#topic+plot.optweight">optweight::plot.optweight()</a></code>. See <code><a href="#topic+method_optweight">method_optweight</a></code> for details.
</p>



<h3>Value</h3>

<p>A <code>ggplot</code> object.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+weightit">weightit()</a></code>, <code><a href="#topic+plot.summary.weightit">plot.summary.weightit()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# See example at the corresponding methods page

</code></pre>

<hr>
<h2 id='predict.glm_weightit'>Predictions for <code>glm_weightit</code> objects</h2><span id='topic+predict.glm_weightit'></span><span id='topic+predict.ordinal_weightit'></span><span id='topic+predict.multinom_weightit'></span>

<h3>Description</h3>

<p><code>predict()</code> generates predictions for models fit using
<code>glm_weightit()</code>, <code>ordinal_weightit()</code>, <code>multinom_weightit()</code>, or
<code>coxph_weightit()</code>. This page only details the <code>predict()</code> methods after
using <code>glm_weightit()</code>, <code>ordinal_weightit()</code>, or <code>multinom_weightit()</code>. See
<code><a href="survival.html#topic+predict.coxph">survival::predict.coxph()</a></code> for predictions when fitting Cox proportional
hazards models using <code>coxph_weightit()</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'glm_weightit'
predict(object, newdata = NULL, type = "response", na.action = na.pass, ...)

## S3 method for class 'ordinal_weightit'
predict(
  object,
  newdata = NULL,
  type = "response",
  na.action = na.pass,
  values = NULL,
  ...
)

## S3 method for class 'multinom_weightit'
predict(
  object,
  newdata = NULL,
  type = "response",
  na.action = na.pass,
  values = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.glm_weightit_+3A_object">object</code></td>
<td>
<p>a <code>glm_weightit</code> object.</p>
</td></tr>
<tr><td><code id="predict.glm_weightit_+3A_newdata">newdata</code></td>
<td>
<p>optionally, a data frame in which to look for variables with
which to predict. If omitted, the fitted values applied to the original
dataset are used.</p>
</td></tr>
<tr><td><code id="predict.glm_weightit_+3A_type">type</code></td>
<td>
<p>the type of prediction desired. Allowable options include
<code>"response"</code>, predictions on the scale of the original response variable
(also <code>"probs"</code>); <code>"link"</code>, predictions on the scale of the linear
predictor (also <code>"lp"</code>); <code>"class"</code>, the modal predicted category for
ordinal and multinomial models; and <code>"mean"</code>, the expected value of the
outcome for ordinal and multinomial models. See Details for more
information. The default is <code>"response"</code> for all models, which differs from
<code><a href="stats.html#topic+predict.glm">stats::predict.glm()</a></code>.</p>
</td></tr>
<tr><td><code id="predict.glm_weightit_+3A_na.action">na.action</code></td>
<td>
<p>function determining what should be done with missing values
in <code>newdata</code>. The default is to predict <code>NA</code>.</p>
</td></tr>
<tr><td><code id="predict.glm_weightit_+3A_...">...</code></td>
<td>
<p>further arguments passed to or from other methods.</p>
</td></tr>
<tr><td><code id="predict.glm_weightit_+3A_values">values</code></td>
<td>
<p>when <code>type = "mean"</code>, the numeric values each level corresponds
to. Should be supplied as a named vector with outcome levels as the names.
If <code>NULL</code> and the outcome levels can be converted to numeric, those will be
used. See Details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For generalized linear models other than ordinal and multinomial
models, see <code><a href="stats.html#topic+predict.glm">stats::predict.glm()</a></code> for more information on how predictions
are computed and which arguments can be specified. Note that standard errors
cannot be computed for the predictions using <code>predict.glm_weightit()</code>.
</p>
<p>For ordinal and multinomial models, setting <code>type = "mean"</code> computes the
expected value of the outcome for each unit; this corresponds to the sum of
the values supplied in <code>values</code> weighted by the predicted probability of
those values. If <code>values</code> is omitted, <code>predict()</code> will attempt to convert the
outcome levels to numeric values, and if this cannot be done, an error will
be thrown. <code>values</code> should be specified as a named vector, e.g., <code>values = c(one = 1, two = 2, three = 3)</code>, where <code>"one"</code>, <code>"two"</code>, and <code>"three"</code> are
the original outcome levels and 1, 2, and 3 are the numeric values they
correspond to. This method only makes sense to use if the outcome levels
meaningfully correspond to numeric values.
</p>
<p>For ordinal models, setting <code>type = "link"</code> (also <code>"lp"</code>) computes the linear
predictor without including the thresholds. This can be interpreted as the
prediction of the latent variable underlying the ordinal response. This
cannot be used with multinomial models.
</p>


<h3>Value</h3>

<p>A numeric vector containing the desired predictions, except for the
following circumstances when an ordinal or multinomial model was fit:
</p>

<ul>
<li><p> when <code>type = "response"</code>, a numeric matrix with a row for each unit and
a column for each level of the outcome with the predicted probability of
the corresponding outcome in the cells
</p>
</li>
<li><p> when <code>type = "class"</code>, a factor with the model predicted class for each
unit; for ordinal models, this will be an ordered factor.
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="stats.html#topic+predict.glm">stats::predict.glm()</a></code> for predictions from generalized linear
models. <code><a href="#topic+glm_weightit">glm_weightit()</a></code> for the fitting function.
<code><a href="survival.html#topic+predict.coxph">survival::predict.coxph()</a></code> for predictions from Cox proportional hazards
models.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("lalonde", package = "cobalt")

# Logistic regression model
fit1 &lt;- glm_weightit(
  re78 &gt; 0 ~ treat * (age + educ + race + married +
                        re74 + re75),
  data = lalonde, family = binomial, vcov = "none")

summary(predict(fit1))

# G-computation using predicted probabilities
p0 &lt;- predict(fit1, type = "response",
              newdata = transform(lalonde,
                                  treat = 0))

p1 &lt;- predict(fit1, type = "response",
              newdata = transform(lalonde,
                                  treat = 1))

mean(p1) - mean(p0)

# Multinomial logistic regression model
lalonde$re78_3 &lt;- factor(findInterval(lalonde$re78,
                                      c(0, 5e3, 1e4)),
                         labels = c("low", "med", "high"))

fit2 &lt;- multinom_weightit(
  re78_3 ~ treat * (age + educ + race + married +
                      re74 + re75),
  data = lalonde, vcov = "none")

# Predicted probabilities
head(predict(fit2))

# Class assignment accuracy
mean(predict(fit2, type = "class") == lalonde$re78_3)

# G-computation using expected value of the outcome
values &lt;- c("low" = 2500,
            "med" = 7500,
            "high" = 12500)

p0 &lt;- predict(fit2, type = "mean", values = values,
              newdata = transform(lalonde,
                                  treat = 0))

p1 &lt;- predict(fit2, type = "mean", values = values,
              newdata = transform(lalonde,
                                  treat = 1))

mean(p1) - mean(p0)

# Ordinal logistic regression
fit3 &lt;- ordinal_weightit(
  re78 ~ treat * (age + educ + race + married +
                    re74 + re75),
  data = lalonde, vcov = "none")

# G-computation using expected value of the outcome;
# using original outcome values
p0 &lt;- predict(fit3, type = "mean",
              newdata = transform(lalonde,
                                  treat = 0))

p1 &lt;- predict(fit3, type = "mean",
              newdata = transform(lalonde,
                                  treat = 1))

mean(p1) - mean(p0)

</code></pre>

<hr>
<h2 id='sbps'>Subgroup Balancing Propensity Score</h2><span id='topic+sbps'></span>

<h3>Description</h3>

<p>Implements the subgroup balancing propensity score (SBPS), which
is an algorithm that attempts to achieve balance in subgroups by sharing
information from the overall sample and subgroups (Dong, Zhang, Zeng, &amp; Li,
2020; DZZL). Each subgroup can use either weights estimated using the whole
sample, weights estimated using just that subgroup, or a combination of the
two. The optimal combination is chosen as that which minimizes an imbalance
criterion that includes subgroup as well as overall balance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sbps(
  obj,
  obj2 = NULL,
  moderator = NULL,
  formula = NULL,
  data = NULL,
  smooth = FALSE,
  full.search
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="sbps_+3A_obj">obj</code></td>
<td>
<p>a <code>weightit</code> object containing weights estimated in the overall
sample.</p>
</td></tr>
<tr><td><code id="sbps_+3A_obj2">obj2</code></td>
<td>
<p>a <code>weightit</code> object containing weights estimated in the
subgroups. Typically this has been estimated by including <code>by</code> in the call
to <code><a href="#topic+weightit">weightit()</a></code>. Either <code>obj2</code> or <code>moderator</code> must be specified.</p>
</td></tr>
<tr><td><code id="sbps_+3A_moderator">moderator</code></td>
<td>
<p>optional; a string containing the name of the variable in
<code>data</code> for which weighting is to be done within subgroups or a one-sided
formula with the subgrouping variable on the right-hand side. This argument
is analogous to the <code>by</code> argument in <code>weightit()</code>, and in fact it is passed
on to <code>by</code>. Either <code>obj2</code> or <code>moderator</code> must be specified.</p>
</td></tr>
<tr><td><code id="sbps_+3A_formula">formula</code></td>
<td>
<p>an optional formula with the covariates for which balance is
to be optimized. If not specified, the formula in <code>obj$call</code> will be used.</p>
</td></tr>
<tr><td><code id="sbps_+3A_data">data</code></td>
<td>
<p>an optional data set in the form of a data frame that contains
the variables in <code>formula</code> or <code>moderator</code>.</p>
</td></tr>
<tr><td><code id="sbps_+3A_smooth">smooth</code></td>
<td>
<p><code>logical</code>; whether the smooth version of the SBPS should be
used. This is only compatible with <code>weightit</code> methods that return a
propensity score.</p>
</td></tr>
<tr><td><code id="sbps_+3A_full.search">full.search</code></td>
<td>
<p><code>logical</code>; when <code>smooth = FALSE</code>, whether every
combination of subgroup and overall weights should be evaluated. If
<code>FALSE</code>, a stochastic search as described in DZZL will be used instead. If
<code>TRUE</code>, all <code class="reqn">2^R</code> combinations will be checked, where <code class="reqn">R</code> is the
number of subgroups, which can take a long time with many subgroups. If
unspecified, will default to <code>TRUE</code> if <code class="reqn">R &lt;= 8</code> and <code>FALSE</code> otherwise.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The SBPS relies on two sets of weights: one estimated in the overall
sample and one estimated within each subgroup. The algorithm decides whether
each subgroup should use the weights estimated in the overall sample or those
estimated in the subgroup. There are 2^R permutations of overall and subgroup
weights, where R is the number of subgroups. The optimal permutation is
chosen as that which minimizes a balance criterion as described in DZZL. The
balance criterion used here is, for binary and multi-category treatments, the
sum of the squared standardized mean differences within subgroups and
overall, which are computed using <code><a href="cobalt.html#topic+balance-summary">cobalt::col_w_smd()</a></code>, and for continuous
treatments, the sum of the squared correlations between each covariate and
treatment within subgroups and overall, which are computed using
<code><a href="cobalt.html#topic+balance-summary">cobalt::col_w_corr()</a></code>.
</p>
<p>The smooth version estimates weights that determine the relative contribution
of the overall and subgroup propensity scores to a weighted average
propensity score for each subgroup. If P_O are the propensity scores
estimated in the overall sample and P_S are the propensity scores estimated
in each subgroup, the smooth SBPS finds R coefficients C so that for each
subgroup, the ultimate propensity score is <code class="reqn">C*P_S + (1-C)*P_O</code>, and
weights are computed from this propensity score. The coefficients are
estimated using <code><a href="stats.html#topic+optim">optim()</a></code> with <code>method = "L-BFGS-B"</code>. When C is estimated to
be 1 or 0 for each subgroup, the smooth SBPS coincides with the standard
SBPS.
</p>
<p>If <code>obj2</code> is not specified and <code>moderator</code> is, <code>sbps()</code> will attempt to refit
the model specified in <code>obj</code> with the <code>moderator</code> in the <code>by</code> argument. This
relies on the environment in which <code>obj</code> was created to be intact and can
take some time if <code>obj</code> was hard to fit. It's safer to estimate <code>obj</code> and
<code>obj2</code> (the latter simply by including the moderator in the <code>by</code> argument)
and supply these to <code>sbps()</code>.
</p>


<h3>Value</h3>

<p>A <code>weightit.sbps</code> object, which inherits from <code>weightit</code>. This
contains all the information in <code>obj</code> with the weights, propensity scores,
call, and possibly covariates updated from <code>sbps()</code>. In addition, the
<code>prop.subgroup</code> component contains the values of the coefficients C for the
subgroups (which are either 0 or 1 for the standard SBPS), and the
<code>moderator</code> component contains a data.frame with the moderator.
</p>
<p>This object has its own summary method and is compatible with <span class="pkg">cobalt</span>
functions. The <code>cluster</code> argument should be used with <span class="pkg">cobalt</span> functions
to accurately reflect the performance of the weights in balancing the
subgroups.
</p>


<h3>References</h3>

<p>Dong, J., Zhang, J. L., Zeng, S., &amp; Li, F. (2020). Subgroup
balancing propensity score. Statistical Methods in Medical Research, 29(3),
659–676. <a href="https://doi.org/10.1177/0962280219870836">doi:10.1177/0962280219870836</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+weightit">weightit()</a></code>, <code><a href="#topic+summary.weightit">summary.weightit()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library("cobalt")
data("lalonde", package = "cobalt")

#Balancing covariates between treatment groups within races
(W1 &lt;- weightit(treat ~ age + educ + married +
                nodegree + race + re74, data = lalonde,
                method = "glm", estimand = "ATT"))

(W2 &lt;- weightit(treat ~ age + educ + married +
                nodegree + race + re74, data = lalonde,
                method = "glm", estimand = "ATT",
                by = "race"))
S &lt;- sbps(W1, W2)
print(S)
summary(S)
bal.tab(S, cluster = "race")

#Could also have run
#  sbps(W1, moderator = "race")

S_ &lt;- sbps(W1, W2, smooth = TRUE)
print(S_)
summary(S_)
bal.tab(S_, cluster = "race")
</code></pre>

<hr>
<h2 id='summary.weightit'>Print and Summarize Output</h2><span id='topic+summary.weightit'></span><span id='topic+plot.summary.weightit'></span><span id='topic+summary.weightitMSM'></span><span id='topic+plot.summary.weightitMSM'></span>

<h3>Description</h3>

<p><code>summary()</code> generates a summary of the <code>weightit</code> or
<code>weightitMSM</code> object to evaluate the properties of the estimated weights.
<code>plot()</code> plots the distribution of the weights. <code>nobs()</code> extracts the number
of observations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'weightit'
summary(object, top = 5L, ignore.s.weights = FALSE, ...)

## S3 method for class 'summary.weightit'
plot(x, binwidth = NULL, bins = NULL, ...)

## S3 method for class 'weightitMSM'
summary(object, top = 5L, ignore.s.weights = FALSE, ...)

## S3 method for class 'summary.weightitMSM'
plot(x, binwidth = NULL, bins = NULL, time = 1, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="summary.weightit_+3A_object">object</code></td>
<td>
<p>a <code>weightit</code> or <code>weightitMSM</code> object; the output of a call to
<code><a href="#topic+weightit">weightit()</a></code> or <code><a href="#topic+weightitMSM">weightitMSM()</a></code>.</p>
</td></tr>
<tr><td><code id="summary.weightit_+3A_top">top</code></td>
<td>
<p>how many of the largest and smallest weights to display. Default
is 5.</p>
</td></tr>
<tr><td><code id="summary.weightit_+3A_ignore.s.weights">ignore.s.weights</code></td>
<td>
<p>whether or not to ignore sampling weights when
computing the weight summary. If <code>FALSE</code>, the default, the estimated
weights will be multiplied by the sampling weights (if any) before values
are computed.</p>
</td></tr>
<tr><td><code id="summary.weightit_+3A_...">...</code></td>
<td>
<p>For <code>plot()</code>, additional arguments passed to <code><a href="graphics.html#topic+hist">graphics::hist()</a></code> to
determine the number of bins, though <code><a href="ggplot2.html#topic+geom_histogram">ggplot2::geom_histogram()</a></code> is
actually used to create the plot.</p>
</td></tr>
<tr><td><code id="summary.weightit_+3A_x">x</code></td>
<td>
<p>a <code>summary.weightit</code> or <code>summary.weightitMSM</code> object; the output of
a call to <code>summary.weightit()</code> or <code>summary.weightitMSM()</code>.</p>
</td></tr>
<tr><td><code id="summary.weightit_+3A_binwidth">binwidth</code>, <code id="summary.weightit_+3A_bins">bins</code></td>
<td>
<p>arguments passed to <code><a href="ggplot2.html#topic+geom_histogram">ggplot2::geom_histogram()</a></code> to
control the size and/or number of bins.</p>
</td></tr>
<tr><td><code id="summary.weightit_+3A_time">time</code></td>
<td>
<p><code>numeric</code>; the time point for which to display the distribution
of weights. Default is to plot the distribution for the first time points.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>For point treatments (i.e., <code>weightit</code> objects), <code>summary()</code> returns
a <code>summary.weightit</code> object with the following elements:
</p>
<table role = "presentation">
<tr><td><code>weight.range</code></td>
<td>
<p>The range (minimum and maximum) weight for each treatment
group.</p>
</td></tr> <tr><td><code>weight.top</code></td>
<td>
<p>The units with the greatest weights in each
treatment group; how many are included is determined by <code>top</code>.</p>
</td></tr>
<tr><td><code>coef.of.var (Coef of Var)</code></td>
<td>
<p>The coefficient of variation (standard
deviation divided by mean) of the weights in each treatment group and
overall.</p>
</td></tr> <tr><td><code>scaled.mad (MAD)</code></td>
<td>
<p>The mean absolute deviation of the weights
in each treatment group and overall divided by the mean of the weights in the
corresponding group.</p>
</td></tr> <tr><td><code>negative entropy (Entropy)</code></td>
<td>
<p>The negative entropy
(<code class="reqn">\sum w log(w)</code>) of the weights in each treatment group and overall
divided by the mean of the weights in the corresponding group.</p>
</td></tr>
<tr><td><code>num.zeros</code></td>
<td>
<p>The number of weights equal to zero.</p>
</td></tr>
<tr><td><code>effective.sample.size</code></td>
<td>
<p>The effective sample size for each treatment
group before and after weighting. See <code><a href="#topic+ESS">ESS()</a></code>.</p>
</td></tr>
</table>
<p>For longitudinal treatments (i.e., <code>weightitMSM</code> objects), <code>summary()</code>
returns a list of the above elements for each treatment period.
</p>
<p><code>plot()</code> returns a <code>ggplot</code> object with a histogram displaying the
distribution of the estimated weights. If the estimand is the ATT or ATC,
only the weights for the non-focal group(s) will be displayed (since the
weights for the focal group are all 1). A dotted line is displayed at the
mean of the weights.
</p>
<p><code>nobs()</code> returns a single number. Note that even units with <code>weights</code> or
<code>s.weights</code> of 0 are included.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+weightit">weightit()</a></code>, <code><a href="#topic+weightitMSM">weightitMSM()</a></code>, <code><a href="base.html#topic+summary">summary()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# See example at ?weightit or ?weightitMSM

</code></pre>

<hr>
<h2 id='trim'>Trim (Winsorize) Large Weights</h2><span id='topic+trim'></span><span id='topic+trim.weightit'></span><span id='topic+trim.default'></span>

<h3>Description</h3>

<p>Trims (i.e., winsorizes) large weights by setting all weights
higher than that at a given quantile to the weight at the quantile or to 0.
This can be useful in controlling extreme weights, which can reduce effective
sample size by enlarging the variability of the weights. Note that by
default, no observations are fully discarded when using <code>trim()</code>, which may
differ from the some uses of the word &quot;trim&quot; (see the <code>drop</code> argument below).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>trim(x, ...)

## S3 method for class 'weightit'
trim(x, at = 0, lower = FALSE, drop = FALSE, ...)

## Default S3 method:
trim(x, at = 0, lower = FALSE, treat = NULL, drop = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="trim_+3A_x">x</code></td>
<td>
<p>A <code>weightit</code> object or a vector of weights.</p>
</td></tr>
<tr><td><code id="trim_+3A_...">...</code></td>
<td>
<p>Not used.</p>
</td></tr>
<tr><td><code id="trim_+3A_at">at</code></td>
<td>
<p><code>numeric</code>; either the quantile of the weights above which weights
are to be trimmed. A single number between .5 and 1, or the number of
weights to be trimmed (e.g., <code>at = 3</code> for the top 3 weights to be set to
the 4th largest weight).</p>
</td></tr>
<tr><td><code id="trim_+3A_lower">lower</code></td>
<td>
<p><code>logical</code>; whether also to trim at the lower quantile (e.g., for
<code>at = .9</code>, trimming at both .1 and .9, or for <code>at = 3</code>, trimming the top
and bottom 3 weights). Default is <code>FALSE</code> to only trim the higher weights.</p>
</td></tr>
<tr><td><code id="trim_+3A_drop">drop</code></td>
<td>
<p><code>logical</code>; whether to set the weights of the trimmed units to 0
or not. Default is <code>FALSE</code> to retain all trimmed units. Setting to <code>TRUE</code>
may change the original targeted estimand when not the ATT or ATC.</p>
</td></tr>
<tr><td><code id="trim_+3A_treat">treat</code></td>
<td>
<p>A vector of treatment status for each unit. This should always
be included when <code>x</code> is numeric, but you can get away with leaving it out
if the treatment is continuous or the estimand is the ATE for binary or
multi-category treatments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>trim()</code> takes in a <code>weightit</code> object (the output of a call to
<code><a href="#topic+weightit">weightit()</a></code> or <code><a href="#topic+weightitMSM">weightitMSM()</a></code>) or a numeric vector of weights and trims
(winsorizes) them to the specified quantile. All weights above that quantile
are set to the weight at that quantile unless <code>drop = TRUE</code>, in which case
they are set to 0. If <code>lower = TRUE</code>, all weights below 1 minus the quantile
are trimmed. In general, trimming weights can decrease balance but also
decreases the variability of the weights, improving precision at the
potential expense of unbiasedness (Cole &amp; Hernán, 2008). See Lee, Lessler,
and Stuart (2011) and Thoemmes and Ong (2015) for discussions and simulation
results of trimming weights at various quantiles. Note that trimming weights
can also change the target population and therefore the estimand.
</p>
<p>When using <code>trim()</code> on a numeric vector of weights, it is helpful to include
the treatment vector as well. The helps determine the type of treatment and
estimand, which are used to specify how trimming is performed. In particular,
if the estimand is determined to be the ATT or ATC, the weights of the target
(i.e., focal) group are ignored, since they should all be equal to 1.
Otherwise, if the estimand is the ATE or the treatment is continuous, all
weights are considered for trimming. In general, weights for any group for
which all the weights are the same will not be considered in the trimming.
</p>


<h3>Value</h3>

<p>If the input is a <code>weightit</code> object, the output will be a <code>weightit</code>
object with the weights replaced by the trimmed weights (or 0) and will have
an additional attribute, <code>"trim"</code>, equal to the quantile of trimming.
</p>
<p>If the input is a numeric vector of weights, the output will be a numeric
vector of the trimmed weights, again with the aforementioned attribute.
</p>


<h3>References</h3>

<p>Cole, S. R., &amp; Hernán, M. Á. (2008). Constructing Inverse
Probability Weights for Marginal Structural Models. American Journal of
Epidemiology, 168(6), 656–664.
</p>
<p>Lee, B. K., Lessler, J., &amp; Stuart, E. A. (2011). Weight Trimming and
Propensity Score Weighting. PLoS ONE, 6(3), e18174.
</p>
<p>Thoemmes, F., &amp; Ong, A. D. (2016). A Primer on Inverse Probability of
Treatment Weighting and Marginal Structural Models. Emerging Adulthood, 4(1),
40–59.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+weightit">weightit()</a></code>, <code><a href="#topic+weightitMSM">weightitMSM()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library("cobalt")
data("lalonde", package = "cobalt")

(W &lt;- weightit(treat ~ age + educ + married +
                 nodegree + re74, data = lalonde,
               method = "glm", estimand = "ATT"))
summary(W)

#Trimming the top and bottom 5 weights
trim(W, at = 5, lower = TRUE)

#Trimming at 90th percentile
(W.trim &lt;- trim(W, at = .9))

summary(W.trim)
#Note that only the control weights were trimmed

#Trimming a numeric vector of weights
all.equal(trim(W$weights, at = .9, treat = lalonde$treat),
          W.trim$weights)

#Dropping trimmed units
(W.trim &lt;- trim(W, at = .9, drop = TRUE))

summary(W.trim)
#Note that we now have zeros in the control group

#Using made up data and as.weightit()
treat &lt;- rbinom(500, 1, .3)
weights &lt;- rchisq(500, df = 2)
W &lt;- as.weightit(weights, treat = treat,
                 estimand = "ATE")
summary(W)
summary(trim(W, at = .95))
</code></pre>

<hr>
<h2 id='weightit'>Estimate Balancing Weights</h2><span id='topic+weightit'></span>

<h3>Description</h3>

<p><code>weightit()</code> allows for the easy generation of balancing weights
using a variety of available methods for binary, continuous, and
multi-category treatments. Many of these methods exist in other packages,
which <code>weightit()</code> calls; these packages must be installed to use the desired
method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>weightit(
  formula,
  data = NULL,
  method = "glm",
  estimand = "ATE",
  stabilize = FALSE,
  focal = NULL,
  by = NULL,
  s.weights = NULL,
  ps = NULL,
  moments = NULL,
  int = FALSE,
  subclass = NULL,
  missing = NULL,
  verbose = FALSE,
  include.obj = FALSE,
  keep.mparts = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="weightit_+3A_formula">formula</code></td>
<td>
<p>a formula with a treatment variable on the left hand side and
the covariates to be balanced on the right hand side. See <code><a href="stats.html#topic+glm">glm()</a></code> for more
details. Interactions and functions of covariates are allowed.</p>
</td></tr>
<tr><td><code id="weightit_+3A_data">data</code></td>
<td>
<p>an optional data set in the form of a data frame that contains
the variables in <code>formula</code>.</p>
</td></tr>
<tr><td><code id="weightit_+3A_method">method</code></td>
<td>
<p>a string of length 1 containing the name of the method that
will be used to estimate weights. See Details below for allowable options.
The default is <code>"glm"</code> for propensity score weighting using a generalized
linear model to estimate the propensity score.</p>
</td></tr>
<tr><td><code id="weightit_+3A_estimand">estimand</code></td>
<td>
<p>the desired estimand. For binary and multi-category
treatments, can be <code>"ATE"</code>, <code>"ATT"</code>, <code>"ATC"</code>, and, for some methods,
<code>"ATO"</code>, <code>"ATM"</code>, or <code>"ATOS"</code>. The default for both is <code>"ATE"</code>. This
argument is ignored for continuous treatments. See the individual pages for
each method for more information on which estimands are allowed with each
method and what literature to read to interpret these estimands.</p>
</td></tr>
<tr><td><code id="weightit_+3A_stabilize">stabilize</code></td>
<td>
<p>whether or not and how to stabilize the weights. If <code>TRUE</code>,
each unit's weight will be multiplied by a standardization factor, which is
the the unconditional probability (or density) of each unit's observed
treatment value. If a formula, a generalized linear model will be fit with
the included predictors, and the inverse of the corresponding weight will
be used as the standardization factor. Can only be used with continuous
treatments or when <code>estimand = "ATE"</code>. Default is <code>FALSE</code> for no
standardization. See also the <code>num.formula</code> argument at <code><a href="#topic+weightitMSM">weightitMSM()</a></code>.
For continuous treatments, weights are already stabilized, so setting
<code>stabilize = TRUE</code> will be ignored with a warning (supplying a formula
still works).</p>
</td></tr>
<tr><td><code id="weightit_+3A_focal">focal</code></td>
<td>
<p>when <code>estimand</code> is set to <code>"ATT"</code> or <code>"ATC"</code>, which group to
consider the &quot;treated&quot; or &quot;control&quot; group. This group will not be weighted,
and the other groups will be weighted to resemble the focal group. If
specified, <code>estimand</code> will automatically be set to <code>"ATT"</code> (with a warning
if <code>estimand</code> is not <code>"ATT"</code> or <code>"ATC"</code>). See section <em><code>estimand</code> and
<code>focal</code></em> in Details below.</p>
</td></tr>
<tr><td><code id="weightit_+3A_by">by</code></td>
<td>
<p>a string containing the name of the variable in <code>data</code> for which
weighting is to be done within categories or a one-sided formula with the
stratifying variable on the right-hand side. For example, if <code>by = "gender"</code> or <code>by = ~gender</code>, a separate propensity score model or
optimization will occur within each level of the variable <code>"gender"</code>. Only
one <code>by</code> variable is allowed; to stratify by multiply variables
simultaneously, create a new variable that is a full cross of those
variables using <code><a href="base.html#topic+interaction">interaction()</a></code>.</p>
</td></tr>
<tr><td><code id="weightit_+3A_s.weights">s.weights</code></td>
<td>
<p>A vector of sampling weights or the name of a variable in
<code>data</code> that contains sampling weights. These can also be matching weights
if weighting is to be used on matched data. See the individual pages for
each method for information on whether sampling weights can be supplied.</p>
</td></tr>
<tr><td><code id="weightit_+3A_ps">ps</code></td>
<td>
<p>A vector of propensity scores or the name of a variable in <code>data</code>
containing propensity scores. If not <code>NULL</code>, <code>method</code> is ignored unless it
is a user-supplied function, and the propensity scores will be used to
create weights. <code>formula</code> must include the treatment variable in <code>data</code>,
but the listed covariates will play no role in the weight estimation. Using
<code>ps</code> is similar to calling <code><a href="#topic+get_w_from_ps">get_w_from_ps()</a></code> directly, but produces a full
<code>weightit</code> object rather than just producing weights.</p>
</td></tr>
<tr><td><code id="weightit_+3A_moments">moments</code></td>
<td>
<p><code>numeric</code>; for some methods, the greatest power of each
covariate to be balanced. For example, if <code>moments = 3</code>, for each
non-categorical covariate, the covariate, its square, and its cube will be
balanced. This argument is ignored for other methods; to balance powers of
the covariates, appropriate functions must be entered in <code>formula</code>. See the
individual pages for each method for information on whether they accept
<code>moments</code>.</p>
</td></tr>
<tr><td><code id="weightit_+3A_int">int</code></td>
<td>
<p><code>logical</code>; for some methods, whether first-order interactions of
the covariates are to be balanced. This argument is ignored for other
methods; to balance interactions between the variables, appropriate
functions must be entered in <code>formula</code>. See the individual pages for each
method for information on whether they accept <code>int</code>.</p>
</td></tr>
<tr><td><code id="weightit_+3A_subclass">subclass</code></td>
<td>
<p><code>numeric</code>; the number of subclasses to use for computing
weights using marginal mean weighting with subclasses (MMWS). If <code>NULL</code>,
standard inverse probability weights (and their extensions) will be
computed; if a number greater than 1, subclasses will be formed and weights
will be computed based on subclass membership. Attempting to set a
non-<code>NULL</code> value for methods that don't compute a propensity score will
result in an error; see each method's help page for information on whether
MMWS weights are compatible with the method. See <code><a href="#topic+get_w_from_ps">get_w_from_ps()</a></code> for
details and references.</p>
</td></tr>
<tr><td><code id="weightit_+3A_missing">missing</code></td>
<td>
<p><code>character</code>; how missing data should be handled. The options
and defaults depend on the <code>method</code> used. Ignored if no missing data is
present. It should be noted that multiple imputation outperforms all
available missingness methods available in <code>weightit()</code> and should probably
be used instead. Consider the <a href="https://CRAN.R-project.org/package=MatchThem"><span class="pkg">MatchThem</span></a> package for the use of
<code>weightit()</code> with multiply imputed data.</p>
</td></tr>
<tr><td><code id="weightit_+3A_verbose">verbose</code></td>
<td>
<p><code>logical</code>; whether to print additional information output by
the fitting function.</p>
</td></tr>
<tr><td><code id="weightit_+3A_include.obj">include.obj</code></td>
<td>
<p><code>logical</code>; whether to include in the output any fit
objects created in the process of estimating the weights. For example, with
<code>method = "glm"</code>, the <code>glm</code> objects containing the propensity score model
will be included. See the individual pages for each method for information
on what object will be included if <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="weightit_+3A_keep.mparts">keep.mparts</code></td>
<td>
<p><code>logical</code>; whether to include in the output components
necessary to estimate standard errors that account for estimation of the
weights in <code><a href="#topic+glm_weightit">glm_weightit()</a></code>. Default is <code>TRUE</code> if such parts are present.
See the individual pages for each method for whether these components are
produced. Set to <code>FALSE</code> to keep the output object smaller, e.g., if
standard errors will not be computed using <code>glm_weightit()</code>.</p>
</td></tr>
<tr><td><code id="weightit_+3A_...">...</code></td>
<td>
<p>other arguments for functions called by <code>weightit()</code> that control
aspects of fitting that are not covered by the above arguments. See
Details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The primary purpose of <code>weightit()</code> is as a dispatcher to functions
that perform the estimation of balancing weights using the requested
<code>method</code>. Below are the methods allowed and links to pages containing more
information about them, including additional arguments and outputs (e.g.,
when <code>include.obj = TRUE</code>), how missing values are treated, which estimands
are allowed, and whether sampling weights are allowed.</p>

<table>
<tr>
 <td style="text-align: left;">
   <code><a href="#topic+method_glm">&quot;glm&quot;</a></code> </td><td style="text-align: left;"> Propensity score weighting using generalized linear models </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code><a href="#topic+method_gbm">&quot;gbm&quot;</a></code> </td><td style="text-align: left;"> Propensity score weighting using generalized boosted modeling </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code><a href="#topic+method_cbps">&quot;cbps&quot;</a></code> </td><td style="text-align: left;"> Covariate Balancing Propensity Score weighting </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code><a href="#topic+method_npcbps">&quot;npcbps&quot;</a></code> </td><td style="text-align: left;"> Non-parametric Covariate Balancing Propensity Score weighting </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code><a href="#topic+method_ebal">&quot;ebal&quot;</a></code> </td><td style="text-align: left;"> Entropy balancing </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code><a href="#topic+method_ipt">&quot;ipt&quot;</a></code> </td><td style="text-align: left;"> Inverse probability tilting </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code><a href="#topic+method_optweight">&quot;optweight&quot;</a></code> </td><td style="text-align: left;"> Optimization-based weighting </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code><a href="#topic+method_super">&quot;super&quot;</a></code> </td><td style="text-align: left;"> Propensity score weighting using SuperLearner </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code><a href="#topic+method_bart">&quot;bart&quot;</a></code> </td><td style="text-align: left;"> Propensity score weighting using Bayesian additive regression trees (BART) </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code><a href="#topic+method_energy">&quot;energy&quot;</a></code> </td><td style="text-align: left;"> Energy balancing </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>

<p><code>method</code> can also be supplied as a user-defined function; see <code><a href="#topic+method_user">method_user</a></code>
for instructions and examples. Setting <code>method = NULL</code> computes unit weights.
</p>


<h4><code>estimand</code> and <code>focal</code> For binary and multi-category treatments, the</h4>

<p>argument to <code>estimand</code> determines what distribution the weighted sample
should resemble. When set to <code>"ATE"</code>, this requests that each group resemble
the full sample. When set to <code>"ATO"</code>, <code>"ATM"</code>, or <code>"ATOS"</code> (for the methods
that allow them), this requests that each group resemble an &quot;overlap&quot; sample.
When set to <code>"ATT"</code> or <code>"ATC"</code>, this requests that each group resemble the
treated or control group, respectively (termed the &quot;focal&quot; group). Weights
are set to 1 for the focal group.
</p>
<p>How does <code>weightit()</code> decide which group is the treated and which group is
the control? For binary treatments, several heuristics are used. The first is
by checking whether a valid argument to <code>focal</code> was supplied containing the
name of the focal group, which is the treated group when <code>estimand = "ATT"</code>
and the control group when <code>estimand = "ATC"</code>. If <code>focal</code> is not supplied,
guesses are made using the following criteria, evaluated in order:
</p>

<ul>
<li><p> If the treatment variable is <code>logical</code>, <code>TRUE</code> is considered treated and <code>FALSE</code> control.
</p>
</li>
<li><p> If the treatment is numeric (or a string or factor with values that can be coerced to numeric values), if 0 is one of the values, it is considered the control, and otherwise, the lower value is considered the control (with the other considered treated).
</p>
</li>
<li><p> If exactly one of the treatment values is <code>"t"</code>, <code>"tr"</code>, <code>"treat"</code>, <code>"treated"</code>, or <code>"exposed"</code>, it is considered the treated (and the other control).
</p>
</li>
<li><p> If exactly one of the treatment values is <code>"c"</code>, <code>"co"</code>, <code>"ctrl"</code>, <code>"control"</code>, or <code>"unexposed"</code>, it is considered the control (and the other treated).
</p>
</li>
<li><p> If the treatment variable is a factor, the first level is considered control and the second treated.
</p>
</li>
<li><p> The lowest value after sorting with <code><a href="base.html#topic+sort">sort()</a></code> is considered control and the other treated.
To be safe, it is best to code your binary treatment variable as <code>0</code> for
control and <code>1</code> for treated. Otherwise, <code>focal</code> should be supplied when
requesting the ATT or ATC. For multi-category treatments, <code>focal</code> is required
when requesting the ATT or ATC; none of the heuristics above are used.
</p>
</li></ul>




<h4>Citing <span class="pkg">WeightIt</span> When using <code>weightit()</code>, please cite both the</h4>

<p><span class="pkg">WeightIt</span> package (using <code>citation("WeightIt")</code>) and the paper(s) in the
references section of the method used.
</p>



<h3>Value</h3>

<p>A <code>weightit</code> object with the following elements: </p>
<table role = "presentation">
<tr><td><code>weights</code></td>
<td>
<p>The
estimated weights, one for each unit.</p>
</td></tr> <tr><td><code>treat</code></td>
<td>
<p>The values of the
treatment variable.</p>
</td></tr>
<tr><td><code>covs</code></td>
<td>
<p>The covariates used in the fitting. Only includes the raw covariates, which may have been altered in
the fitting process.</p>
</td></tr>
<tr><td><code>estimand</code></td>
<td>
<p>The estimand requested.</p>
</td></tr> <tr><td><code>method</code></td>
<td>
<p>The weight estimation
method specified.</p>
</td></tr>
<tr><td><code>ps</code></td>
<td>
<p>The estimated or provided propensity scores. Estimated propensity scores are
returned for binary treatments and only when <code>method</code> is <code>"glm"</code>, <code>"gbm"</code>, <code>"cbps"</code>, <code>"ipt"</code>, <code>"super"</code>, or <code>"bart"</code>. The propensity score corresponds to the predicted probability of being treated; see section <em><code>estimand</code> and <code>focal</code></em> in Details for how the treated group is determined.</p>
</td></tr>
<tr><td><code>s.weights</code></td>
<td>
<p>The provided sampling weights.</p>
</td></tr> <tr><td><code>focal</code></td>
<td>
<p>The focal
treatment level if the ATT or ATC was requested.</p>
</td></tr> <tr><td><code>by</code></td>
<td>
<p>A data.frame
containing the <code>by</code> variable when specified.</p>
</td></tr> <tr><td><code>obj</code></td>
<td>
<p>When <code>include.obj = TRUE</code>, the fit object.</p>
</td></tr>
<tr><td><code>info</code></td>
<td>
<p>Additional information about the fitting. See the individual
methods pages for what is included.</p>
</td></tr>
</table>
<p>When <code>keep.mparts</code> is <code>TRUE</code> (the default) and the chosen method is
compatible with M-estimation, the components related to M-estimation for use
in <code><a href="#topic+glm_weightit">glm_weightit()</a></code> are stored in the <code>"Mparts"</code> attribute. When <code>by</code> is
specified, <code>keep.mparts</code> is set to <code>FALSE</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+weightitMSM">weightitMSM()</a></code> for estimating weights with sequential (i.e.,
longitudinal) treatments for use in estimating marginal structural models
(MSMs).
</p>
<p><code><a href="#topic+weightit.fit">weightit.fit()</a></code>, which is a lower-level dispatcher function that accepts a
matrix of covariates and a vector of treatment statuses rather than a formula
and data frame and performs minimal argument checking and processing. It may
be useful for speeding up simulation studies for which the correct arguments
are known. In general, <code>weightit()</code> should be used.
</p>
<p><code><a href="#topic+summary.weightit">summary.weightit()</a></code> for summarizing the weights
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library("cobalt")
data("lalonde", package = "cobalt")

#Balancing covariates between treatment groups (binary)
(W1 &lt;- weightit(treat ~ age + educ + married +
                  nodegree + re74, data = lalonde,
                method = "glm", estimand = "ATT"))
summary(W1)
bal.tab(W1)

#Balancing covariates with respect to race (multi-category)
(W2 &lt;- weightit(race ~ age + educ + married +
                  nodegree + re74, data = lalonde,
                method = "ebal", estimand = "ATE"))
summary(W2)
bal.tab(W2)

#Balancing covariates with respect to re75 (continuous)
(W3 &lt;- weightit(re75 ~ age + educ + married +
                  nodegree + re74, data = lalonde,
                method = "cbps"))
summary(W3)
bal.tab(W3)
</code></pre>

<hr>
<h2 id='weightit.fit'>Generate Balancing Weights with Minimal Input Processing</h2><span id='topic+weightit.fit'></span>

<h3>Description</h3>

<p><code>weightit.fit()</code> dispatches one of the weight estimation methods
determined by <code>method</code>. It is an internal function called by <code><a href="#topic+weightit">weightit()</a></code> and
should probably not be used except in special cases. Unlike <code>weightit()</code>,
<code>weightit.fit()</code> does not accept a formula and data frame interface and
instead requires the covariates and treatment to be supplied as a numeric
matrix and atomic vector, respectively. In this way, <code>weightit.fit()</code> is to
<code>weightit()</code> what <code><a href="stats.html#topic+lm.fit">lm.fit()</a></code> is to <code><a href="stats.html#topic+lm">lm()</a></code> - a thinner, slightly faster
interface that performs minimal argument checking.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>weightit.fit(
  covs,
  treat,
  method = "glm",
  s.weights = NULL,
  by.factor = NULL,
  estimand = "ATE",
  focal = NULL,
  stabilize = FALSE,
  ps = NULL,
  moments = NULL,
  int = FALSE,
  subclass = NULL,
  missing = NULL,
  verbose = FALSE,
  include.obj = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="weightit.fit_+3A_covs">covs</code></td>
<td>
<p>a numeric matrix of covariates.</p>
</td></tr>
<tr><td><code id="weightit.fit_+3A_treat">treat</code></td>
<td>
<p>a vector of treatment statuses.</p>
</td></tr>
<tr><td><code id="weightit.fit_+3A_method">method</code></td>
<td>
<p>a string containing the name of the method that will be used to
estimate weights. See <code><a href="#topic+weightit">weightit()</a></code> for allowable options. The default is
<code>"glm"</code> for propensity score weighting using a generalized linear model to
estimate the propensity score.</p>
</td></tr>
<tr><td><code id="weightit.fit_+3A_s.weights">s.weights</code></td>
<td>
<p>a numeric vector of sampling weights. See the individual
pages for each method for information on whether sampling weights can be
supplied.</p>
</td></tr>
<tr><td><code id="weightit.fit_+3A_by.factor">by.factor</code></td>
<td>
<p>a factor variable for which weighting is to be done within
levels. Corresponds to the <code>by</code> argument in <code><a href="#topic+weightit">weightit()</a></code>.</p>
</td></tr>
<tr><td><code id="weightit.fit_+3A_estimand">estimand</code></td>
<td>
<p>the desired estimand. For binary and multi-category
treatments, can be <code>"ATE"</code>, <code>"ATT"</code>, <code>"ATC"</code>, and, for some methods,
<code>"ATO"</code>, <code>"ATM"</code>, or <code>"ATOS"</code>. The default for both is <code>"ATE"</code>. This
argument is ignored for continuous treatments. See the individual pages for
each method for more information on which estimands are allowed with each
method and what literature to read to interpret these estimands.</p>
</td></tr>
<tr><td><code id="weightit.fit_+3A_focal">focal</code></td>
<td>
<p>when <code>estimand</code> is set to <code>"ATT"</code> or <code>"ATC"</code>, which group to
consider the &quot;treated&quot; or &quot;control&quot; group. This group will not be weighted,
and the other groups will be weighted to resemble the focal group. If
specified, <code>estimand</code> will automatically be set to <code>"ATT"</code> (with a warning
if <code>estimand</code> is not <code>"ATT"</code> or <code>"ATC"</code>). See section <em><code>estimand</code> and
<code>focal</code></em> in Details at <code><a href="#topic+weightit">weightit()</a></code>.</p>
</td></tr>
<tr><td><code id="weightit.fit_+3A_stabilize">stabilize</code></td>
<td>
<p><code>logical</code>; whether or not to stabilize the weights. For the
methods that involve estimating propensity scores, this involves
multiplying each unit's weight by the proportion of units in their
treatment group. Default is <code>FALSE</code>. Note this differs from its use with
<code><a href="#topic+weightit">weightit()</a></code>.</p>
</td></tr>
<tr><td><code id="weightit.fit_+3A_ps">ps</code></td>
<td>
<p>a vector of propensity scores. If specified, <code>method</code> will be
ignored and set to <code>"glm"</code>.</p>
</td></tr>
<tr><td><code id="weightit.fit_+3A_moments">moments</code>, <code id="weightit.fit_+3A_int">int</code>, <code id="weightit.fit_+3A_subclass">subclass</code></td>
<td>
<p>arguments to customize the weight estimation. See
<code><a href="#topic+weightit">weightit()</a></code> for details.</p>
</td></tr>
<tr><td><code id="weightit.fit_+3A_missing">missing</code></td>
<td>
<p><code>character</code>; how missing data should be handled. The options
depend on the <code>method</code> used. If <code>NULL</code>, <code>covs</code> will be checked for <code>NA</code>
values, and if present, <code>missing</code> will be set to <code>"ind"</code>. If <code>""</code>, <code>covs</code>
will not be checked for <code>NA</code> values; this can be faster when it is known
there are none.</p>
</td></tr>
<tr><td><code id="weightit.fit_+3A_verbose">verbose</code></td>
<td>
<p><code>logical</code>; whether to print additional information output by
the fitting function.</p>
</td></tr>
<tr><td><code id="weightit.fit_+3A_include.obj">include.obj</code></td>
<td>
<p><code>logical</code>; whether to include in the output any fit
objects created in the process of estimating the weights. For example, with
<code>method = "glm"</code>, the <code>glm</code> objects containing the propensity score model
will be included. See the individual pages for each method for information
on what object will be included if <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="weightit.fit_+3A_...">...</code></td>
<td>
<p>other arguments for functions called by <code>weightit.fit()</code> that
control aspects of fitting that are not covered by the above arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>weightit.fit()</code> is called by <code><a href="#topic+weightit">weightit()</a></code> after the arguments to
<code>weightit()</code> have been checked and processed. <code>weightit.fit()</code> dispatches the
function used to actually estimate the weights, passing on the supplied
arguments directly. <code>weightit.fit()</code> is not meant to be used by anyone other
than experienced users who have a specific use case in mind. The returned
object contains limited information about the supplied arguments or details
of the estimation method; all that is processed by <code>weightit()</code>.
</p>
<p>Less argument checking or processing occurs in <code>weightit.fit()</code> than does in
<code>weightit()</code>, which means supplying incorrect arguments can result in errors,
crashes, and invalid weights, and error and warning messages may not be
helpful in diagnosing the problem. <code>weightit.fit()</code> does check to make sure
weights were actually estimated, though.
</p>
<p><code>weightit.fit()</code> may be most useful in speeding up simulation simulation
studies that use <code>weightit()</code> because the covariates can be supplied as a
numeric matrix, which is often how they are generated in simulations, without
having to go through the potentially slow process of extracting the
covariates and treatment from a formula and data frame. If the user is
certain the arguments are valid (e.g., by ensuring the estimated weights are
consistent with those estimated from <code>weightit()</code> with the same arguments),
less time needs to be spent on processing the arguments. Also, the returned
object is much smaller than a <code>weightit</code> object because the covariates are
not returned alongside the weights.
</p>


<h3>Value</h3>

<p>A <code>weightit.fit</code> object with the following elements:
</p>
<table role = "presentation">
<tr><td><code>weights</code></td>
<td>
<p>The estimated weights, one for each unit.</p>
</td></tr> <tr><td><code>treat</code></td>
<td>
<p>The
values of the treatment variable.</p>
</td></tr> <tr><td><code>estimand</code></td>
<td>
<p>The estimand requested.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>The weight estimation method specified.</p>
</td></tr>
<tr><td><code>ps</code></td>
<td>
<p>The estimated or provided propensity scores. Estimated propensity scores are
returned for binary treatments and only when <code>method</code> is <code>"glm"</code>, <code>"gbm"</code>, <code>"cbps"</code>, <code>"ipt"</code>, <code>"super"</code>, or <code>"bart"</code>. The propensity score corresponds to the predicted probability of being treated; see section <em><code>estimand</code> and <code>focal</code></em> in Details at <code><a href="#topic+weightit">weightit()</a></code> for how the treated group is determined.</p>
</td></tr>
<tr><td><code>s.weights</code></td>
<td>
<p>The provided sampling weights.</p>
</td></tr> <tr><td><code>focal</code></td>
<td>
<p>The focal
treatment level if the ATT or ATC was requested.</p>
</td></tr> <tr><td><code>fit.obj</code></td>
<td>
<p>When
<code>include.obj = TRUE</code>, the fit object.</p>
</td></tr> <tr><td><code>info</code></td>
<td>
<p>Additional information
about the fitting. See the individual methods pages for what is included.</p>
</td></tr>
</table>
<p>The <code>weightit.fit</code> object does not have specialized <code>print()</code>, <code>summary()</code>,
or <code>plot()</code> methods. It is simply a list containing the above components. Use
<code><a href="#topic+as.weightit">as.weightit()</a></code> to convert it to a <code>weightit</code> object, which does have these
methods. See Examples.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+weightit">weightit()</a></code>, which you should use for estimating weights unless you
know better.
</p>
<p><code><a href="#topic+as.weightit">as.weightit()</a></code> for converting a <code>weightit.fit</code> object to a <code>weightit</code>
object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library("cobalt")
data("lalonde", package = "cobalt")

# Balancing covariates between treatment groups (binary)
covs &lt;- lalonde[c("age", "educ", "race", "married",
                  "nodegree", "re74", "re75")]
## Create covs matrix, splitting any factors using
## cobalt::splitfactor()
covs_mat &lt;- as.matrix(splitfactor(covs))

WF1 &lt;- weightit.fit(covs_mat, treat = lalonde$treat,
                    method = "glm", estimand = "ATT")
str(WF1)

# Converting to a weightit object for use with
# summary() and bal.tab()
W1 &lt;- as.weightit(WF1, covs = covs)
W1
summary(W1)
bal.tab(W1)

</code></pre>

<hr>
<h2 id='weightitMSM'>Generate Balancing Weights for Longitudinal Treatments</h2><span id='topic+weightitMSM'></span>

<h3>Description</h3>

<p><code>weightitMSM()</code> allows for the easy generation of balancing
weights for marginal structural models for time-varying treatments using a
variety of available methods for binary, continuous, and multi-category
treatments. Many of these methods exist in other packages, which <code><a href="#topic+weightit">weightit()</a></code>
calls; these packages must be installed to use the desired method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>weightitMSM(
  formula.list,
  data = NULL,
  method = "glm",
  stabilize = FALSE,
  by = NULL,
  s.weights = NULL,
  num.formula = NULL,
  moments = NULL,
  int = FALSE,
  missing = NULL,
  verbose = FALSE,
  include.obj = FALSE,
  keep.mparts = TRUE,
  is.MSM.method,
  weightit.force = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="weightitMSM_+3A_formula.list">formula.list</code></td>
<td>
<p>a list of formulas corresponding to each time point with
the time-specific treatment variable on the left hand side and
pre-treatment covariates to be balanced on the right hand side. The
formulas must be in temporal order, and must contain all covariates to be
balanced at that time point (i.e., treatments and covariates featured in
early formulas should appear in later ones). Interactions and functions of
covariates are allowed.</p>
</td></tr>
<tr><td><code id="weightitMSM_+3A_data">data</code></td>
<td>
<p>an optional data set in the form of a data frame that contains
the variables in the formulas in <code>formula.list</code>. This must be a wide data
set with exactly one row per unit.</p>
</td></tr>
<tr><td><code id="weightitMSM_+3A_method">method</code></td>
<td>
<p>a string of length 1 containing the name of the method that
will be used to estimate weights. See <code><a href="#topic+weightit">weightit()</a></code> for allowable options.
The default is <code>"glm"</code>, which estimates the weights using generalized
linear models.</p>
</td></tr>
<tr><td><code id="weightitMSM_+3A_stabilize">stabilize</code></td>
<td>
<p><code>logical</code>; whether or not to stabilize the weights.
Stabilizing the weights involves fitting a model predicting treatment at
each time point from treatment status at prior time points. If <code>TRUE</code>, a
fully saturated model will be fit (i.e., all interactions between all
treatments up to each time point), essentially using the observed treatment
probabilities in the numerator (for binary and multi-category treatments).
This may yield an error if some combinations are not observed. Default is
<code>FALSE</code>. To manually specify stabilization model formulas, e.g., to specify
non-saturated models, use <code>num.formula</code>. With many time points, saturated
models may be time-consuming or impossible to fit.</p>
</td></tr>
<tr><td><code id="weightitMSM_+3A_by">by</code></td>
<td>
<p>a string containing the name of the variable in <code>data</code> for which
weighting is to be done within categories or a one-sided formula with the
stratifying variable on the right-hand side. For example, if <code>by = "gender"</code> or <code>by = ~gender</code>, a separate propensity score model or
optimization will occur within each level of the variable <code>"gender"</code>. Only
one <code>by</code> variable is allowed; to stratify by multiply variables
simultaneously, create a new variable that is a full cross of those
variables using <code><a href="base.html#topic+interaction">interaction()</a></code>.</p>
</td></tr>
<tr><td><code id="weightitMSM_+3A_s.weights">s.weights</code></td>
<td>
<p>A vector of sampling weights or the name of a variable in
<code>data</code> that contains sampling weights. These can also be matching weights
if weighting is to be used on matched data. See the individual pages for
each method for information on whether sampling weights can be supplied.</p>
</td></tr>
<tr><td><code id="weightitMSM_+3A_num.formula">num.formula</code></td>
<td>
<p>optional; a one-sided formula with the stabilization
factors (other than the previous treatments) on the right hand side, which
adds, for each time point, the stabilization factors to a model saturated
with previous treatments. See Cole &amp; Hernán (2008) for a discussion of how
to specify this model; including stabilization factors can change the
estimand without proper adjustment, and should be done with caution. Can
also be a list of one-sided formulas, one for each time point. Unless you
know what you are doing, we recommend setting <code>stabilize = TRUE</code> and
ignoring <code>num.formula</code>.</p>
</td></tr>
<tr><td><code id="weightitMSM_+3A_moments">moments</code></td>
<td>
<p><code>numeric</code>; for some methods, the greatest power of each
covariate to be balanced. For example, if <code>moments = 3</code>, for each
non-categorical covariate, the covariate, its square, and its cube will be
balanced. This argument is ignored for other methods; to balance powers of
the covariates, appropriate functions must be entered in <code>formula</code>. See the
individual pages for each method for information on whether they accept
<code>moments</code>.</p>
</td></tr>
<tr><td><code id="weightitMSM_+3A_int">int</code></td>
<td>
<p><code>logical</code>; for some methods, whether first-order interactions of
the covariates are to be balanced. This argument is ignored for other
methods; to balance interactions between the variables, appropriate
functions must be entered in <code>formula</code>. See the individual pages for each
method for information on whether they accept <code>int</code>.</p>
</td></tr>
<tr><td><code id="weightitMSM_+3A_missing">missing</code></td>
<td>
<p><code>character</code>; how missing data should be handled. The options
and defaults depend on the <code>method</code> used. Ignored if no missing data is
present. It should be noted that multiple imputation outperforms all
available missingness methods available in <code>weightit()</code> and should probably
be used instead. Consider the <a href="https://CRAN.R-project.org/package=MatchThem"><span class="pkg">MatchThem</span></a> package for the use of
<code>weightit()</code> with multiply imputed data.</p>
</td></tr>
<tr><td><code id="weightitMSM_+3A_verbose">verbose</code></td>
<td>
<p><code>logical</code>; whether to print additional information output by
the fitting function.</p>
</td></tr>
<tr><td><code id="weightitMSM_+3A_include.obj">include.obj</code></td>
<td>
<p>whether to include in the output a list of the fit objects
created in the process of estimating the weights at each time point. For
example, with <code>method = "glm"</code>, a list of the <code>glm</code> objects containing the
propensity score models at each time point will be included. See the help
pages for each method for information on what object will be included if
<code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="weightitMSM_+3A_keep.mparts">keep.mparts</code></td>
<td>
<p><code>logical</code>; whether to include in the output components
necessary to estimate standard errors that account for estimation of the
weights in <code><a href="#topic+glm_weightit">glm_weightit()</a></code>. Default is <code>TRUE</code> if such parts are present.
See the individual pages for each method for whether these components are
produced. Set to <code>FALSE</code> to keep the output object smaller, e.g., if
standard errors will not be computed using <code>glm_weightit()</code>.</p>
</td></tr>
<tr><td><code id="weightitMSM_+3A_is.msm.method">is.MSM.method</code></td>
<td>
<p>whether the method estimates weights for multiple time
points all at once (<code>TRUE</code>) or by estimating weights at each time point and
then multiplying them together (<code>FALSE</code>). This is only relevant for
user-specified functions.</p>
</td></tr>
<tr><td><code id="weightitMSM_+3A_weightit.force">weightit.force</code></td>
<td>
<p>several methods are not valid for estimating weights
with longitudinal treatments, and will produce an error message if
attempted. Set to <code>TRUE</code> to bypass this error message.</p>
</td></tr>
<tr><td><code id="weightitMSM_+3A_...">...</code></td>
<td>
<p>other arguments for functions called by <code>weightit()</code> that control
aspects of fitting that are not covered by the above arguments. See Details
at <code><a href="#topic+weightit">weightit()</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Currently only &quot;wide&quot; data sets, where each row corresponds to a
unit's entire variable history, are supported. You can use <code><a href="stats.html#topic+reshape">reshape()</a></code> or
other functions to transform your data into this format; see example below.
</p>
<p>In general, <code>weightitMSM()</code> works by separating the estimation of weights
into separate procedures for each time period based on the formulas provided.
For each formula, <code>weightitMSM()</code> simply calls <code>weightit()</code> to that formula,
collects the weights for each time period, and multiplies them together to
arrive at longitudinal balancing weights.
</p>
<p>Each formula should contain all the covariates to be balanced on. For
example, the formula corresponding to the second time period should contain
all the baseline covariates, the treatment variable at the first time period,
and the time-varying covariates that took on values after the first treatment
and before the second. Currently, only wide data sets are supported, where
each unit is represented by exactly one row that contains the covariate and
treatment history encoded in separate variables.
</p>
<p>The <code>"cbps"</code> method, which calls <code>CBPS()</code> in <span class="pkg">CBPS</span>, will yield different
results from <code>CBMSM()</code> in <span class="pkg">CBPS</span> because <code>CBMSM()</code> takes a different
approach to generating weights than simply estimating several time-specific
models.
</p>


<h3>Value</h3>

<p>A <code>weightitMSM</code> object with the following elements:
</p>
<table role = "presentation">
<tr><td><code>weights</code></td>
<td>
<p>The estimated weights, one for each unit.</p>
</td></tr> <tr><td><code>treat.list</code></td>
<td>
<p>A
list of the values of the time-varying treatment variables.</p>
</td></tr>
<tr><td><code>covs.list</code></td>
<td>
<p>A list of the covariates used in the fitting at each time
point. Only includes the raw covariates, which may have been altered in the
fitting process.</p>
</td></tr> <tr><td><code>data</code></td>
<td>
<p>The data.frame originally entered to
<code>weightitMSM()</code>.</p>
</td></tr> <tr><td><code>estimand</code></td>
<td>
<p>&quot;ATE&quot;, currently the only estimand for MSMs
with binary or multi-category treatments.</p>
</td></tr> <tr><td><code>method</code></td>
<td>
<p>The weight
estimation method specified.</p>
</td></tr> <tr><td><code>ps.list</code></td>
<td>
<p>A list of the estimated
propensity scores (if any) at each time point.</p>
</td></tr> <tr><td><code>s.weights</code></td>
<td>
<p>The provided
sampling weights.</p>
</td></tr> <tr><td><code>by</code></td>
<td>
<p>A data.frame containing the <code>by</code> variable when
specified.</p>
</td></tr> <tr><td><code>stabilization</code></td>
<td>
<p>The stabilization factors, if any.</p>
</td></tr>
</table>
<p>When <code>keep.mparts</code> is <code>TRUE</code> (the default) and the chosen method is
compatible with M-estimation, the components related to M-estimation for use
in <code><a href="#topic+glm_weightit">glm_weightit()</a></code> are stored in the <code>"Mparts.list"</code> attribute. When <code>by</code> is
specified, <code>keep.mparts</code> is set to <code>FALSE</code>.
</p>


<h3>References</h3>

<p>Cole, S. R., &amp; Hernán, M. A. (2008). Constructing Inverse
Probability Weights for Marginal Structural Models. American Journal of
Epidemiology, 168(6), 656–664. <a href="https://doi.org/10.1093/aje/kwn164">doi:10.1093/aje/kwn164</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+weightit">weightit()</a></code> for information on the allowable methods
</p>
<p><code><a href="#topic+summary.weightitMSM">summary.weightitMSM()</a></code> for summarizing the weights
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library("cobalt")

data("msmdata")
(W1 &lt;- weightitMSM(list(A_1 ~ X1_0 + X2_0,
                        A_2 ~ X1_1 + X2_1 +
                          A_1 + X1_0 + X2_0,
                        A_3 ~ X1_2 + X2_2 +
                          A_2 + X1_1 + X2_1 +
                          A_1 + X1_0 + X2_0),
                   data = msmdata,
                   method = "glm"))
summary(W1)
bal.tab(W1)

#Using stabilization factors
W2 &lt;- weightitMSM(list(A_1 ~ X1_0 + X2_0,
                        A_2 ~ X1_1 + X2_1 +
                          A_1 + X1_0 + X2_0,
                        A_3 ~ X1_2 + X2_2 +
                          A_2 + X1_1 + X2_1 +
                          A_1 + X1_0 + X2_0),
                   data = msmdata,
                   method = "glm",
                   stabilize = TRUE,
                   num.formula = list(~ 1,
                                      ~ A_1,
                                      ~ A_1 + A_2))

#Same as above but with fully saturated stabilization factors
#(i.e., making the last entry in 'num.formula' A_1*A_2)
W3 &lt;- weightitMSM(list(A_1 ~ X1_0 + X2_0,
                        A_2 ~ X1_1 + X2_1 +
                          A_1 + X1_0 + X2_0,
                        A_3 ~ X1_2 + X2_2 +
                          A_2 + X1_1 + X2_1 +
                          A_1 + X1_0 + X2_0),
                   data = msmdata,
                   method = "glm",
                   stabilize = TRUE)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
