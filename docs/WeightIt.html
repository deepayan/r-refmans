<!DOCTYPE html><html><head><title>Help for package WeightIt</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {WeightIt}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#WeightIt-package'><p>WeightIt: Weighting for Covariate Balance in Observational Studies</p></a></li>
<li><a href='#as.weightit'><p>Create a <code>weightit</code> object manually</p></a></li>
<li><a href='#calibrate'><p>Calibrate Propensity Score Weights</p></a></li>
<li><a href='#ESS'><p>Compute effective sample size of weighted sample</p></a></li>
<li><a href='#get_w_from_ps'><p>Compute weights from propensity scores</p></a></li>
<li><a href='#glm_weightit'><p>Fitting Weighted Generalized Linear Models</p></a></li>
<li><a href='#make_full_rank'><p>Make a design matrix full rank</p></a></li>
<li><a href='#method_bart'><p>Propensity Score Weighting Using BART</p></a></li>
<li><a href='#method_cbps'><p>Covariate Balancing Propensity Score Weighting</p></a></li>
<li><a href='#method_ebal'><p>Entropy Balancing</p></a></li>
<li><a href='#method_energy'><p>Energy Balancing</p></a></li>
<li><a href='#method_gbm'><p>Propensity Score Weighting Using Generalized Boosted Models</p></a></li>
<li><a href='#method_glm'><p>Propensity Score Weighting Using Generalized Linear Models</p></a></li>
<li><a href='#method_ipt'><p>Inverse Probability Tilting</p></a></li>
<li><a href='#method_npcbps'><p>Nonparametric Covariate Balancing Propensity Score Weighting</p></a></li>
<li><a href='#method_optweight'><p>Optimization-Based Weighting</p></a></li>
<li><a href='#method_super'><p>Propensity Score Weighting Using SuperLearner</p></a></li>
<li><a href='#method_user'><p>User-Defined Functions for Estimating Weights</p></a></li>
<li><a href='#msmdata'><p>Simulated data for a 3 time point sequential study</p></a></li>
<li><a href='#sbps'><p>Subgroup Balancing Propensity Score</p></a></li>
<li><a href='#summary.weightit'><p>Print and Summarize Output</p></a></li>
<li><a href='#trim'><p>Trim (Winsorize) Large Weights</p></a></li>
<li><a href='#weightit'><p>Estimate Balancing Weights</p></a></li>
<li><a href='#weightit.fit'><p>Generate Balancing Weights with Minimal Input Processing</p></a></li>
<li><a href='#weightitMSM'><p>Generate Balancing Weights for Longitudinal Treatments</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Weighting for Covariate Balance in Observational Studies</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.0</td>
</tr>
<tr>
<td>Description:</td>
<td>Generates balancing weights for causal effect estimation in observational studies with
             binary, multi-category, or continuous point or longitudinal treatments by easing and
             extending the functionality of several R packages and providing in-house estimation methods.
             Available methods include propensity score weighting using generalized linear models, gradient
             boosting machines, the covariate balancing propensity score algorithm, inverse probability tilting, Bayesian additive regression trees, and
             SuperLearner, and directly estimating balancing weights using entropy balancing, energy balancing, and optimization-based weights. Also
             allows for assessment of weights and checking of covariate balance by interfacing directly
             with the 'cobalt' package. See the vignette "Installing Supporting Packages" for instructions on how
             to install any package 'WeightIt' uses, including those that may not be on CRAN.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.3.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>cobalt (&ge; 4.5.1), ggplot2 (&ge; 3.3.0), chk (&ge; 0.8.1), rlang
(&ge; 1.1.0), crayon, backports (&ge; 1.4.1), utils, stats</td>
</tr>
<tr>
<td>Suggests:</td>
<td>rootSolve (&ge; 1.8.2.4), CBPS (&ge; 0.18), optweight (&ge; 0.2.4),
SuperLearner (&ge; 2.0-25), mclogit, MNP (&ge; 3.1-4), brglm2 (&ge;
0.5.2), osqp (&ge; 0.6.0.5), survival, fwb (&ge; 0.2.0), splines,
marginaleffects (&ge; 0.11.1), sandwich, MASS, gbm (&ge; 2.1.3),
dbarts (&ge; 0.9-20), misaem (&ge; 1.0.1), knitr, rmarkdown</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://ngreifer.github.io/WeightIt/">https://ngreifer.github.io/WeightIt/</a>,
<a href="https://github.com/ngreifer/WeightIt">https://github.com/ngreifer/WeightIt</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/ngreifer/WeightIt/issues">https://github.com/ngreifer/WeightIt/issues</a></td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.1</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-03-22 22:57:47 UTC; NoahGreifer</td>
</tr>
<tr>
<td>Author:</td>
<td>Noah Greifer <a href="https://orcid.org/0000-0003-3067-7154"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Noah Greifer &lt;noah.greifer@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-03-23 11:20:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='WeightIt-package'>WeightIt: Weighting for Covariate Balance in Observational Studies</h2><span id='topic+WeightIt'></span><span id='topic+WeightIt-package'></span>

<h3>Description</h3>

<p><img src="../help/figures/logo.png" style='float: right' alt='logo' width='120' />
</p>
<p>Generates balancing weights for causal effect estimation in observational studies with binary, multi-category, or continuous point or longitudinal treatments by easing and extending the functionality of several R packages and providing in-house estimation methods. Available methods include propensity score weighting using generalized linear models, gradient boosting machines, the covariate balancing propensity score algorithm, inverse probability tilting, Bayesian additive regression trees, and SuperLearner, and directly estimating balancing weights using entropy balancing, energy balancing, and optimization-based weights. Also allows for assessment of weights and checking of covariate balance by interfacing directly with the 'cobalt' package. See the vignette &quot;Installing Supporting Packages&quot; for instructions on how to install any package 'WeightIt' uses, including those that may not be on CRAN.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Noah Greifer <a href="mailto:noah.greifer@gmail.com">noah.greifer@gmail.com</a> (<a href="https://orcid.org/0000-0003-3067-7154">ORCID</a>)
</p>


<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://ngreifer.github.io/WeightIt/">https://ngreifer.github.io/WeightIt/</a>
</p>
</li>
<li> <p><a href="https://github.com/ngreifer/WeightIt">https://github.com/ngreifer/WeightIt</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/ngreifer/WeightIt/issues">https://github.com/ngreifer/WeightIt/issues</a>
</p>
</li></ul>


<hr>
<h2 id='as.weightit'>Create a <code>weightit</code> object manually</h2><span id='topic+as.weightit'></span><span id='topic+as.weightit.weightit.fit'></span><span id='topic+as.weightit.default'></span><span id='topic+as.weightitMSM'></span><span id='topic+as.weightitMSM.default'></span>

<h3>Description</h3>

<p>This function allows users to get the benefits of a <code>weightit</code> object
when using weights not estimated with <code><a href="#topic+weightit">weightit()</a></code> or <code><a href="#topic+weightitMSM">weightitMSM()</a></code>. These
benefits include diagnostics, plots, and direct compatibility with
<span class="pkg">cobalt</span> for assessing balance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as.weightit(x, ...)

## S3 method for class 'weightit.fit'
as.weightit(x, covs = NULL, ...)

## Default S3 method:
as.weightit(
  x,
  treat,
  covs = NULL,
  estimand = NULL,
  s.weights = NULL,
  ps = NULL,
  ...
)

as.weightitMSM(x, ...)

## Default S3 method:
as.weightitMSM(
  x,
  treat.list,
  covs.list = NULL,
  estimand = NULL,
  s.weights = NULL,
  ps.list = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="as.weightit_+3A_x">x</code></td>
<td>
<p>required; a <code>numeric</code> vector of weights, one for each
unit, or a <code>weightit.fit</code> object from <code><a href="#topic+weightit.fit">weightit.fit()</a></code>.</p>
</td></tr>
<tr><td><code id="as.weightit_+3A_...">...</code></td>
<td>
<p>additional arguments. These must be named. They will be included
in the output object.</p>
</td></tr>
<tr><td><code id="as.weightit_+3A_covs">covs</code></td>
<td>
<p>an optional <code>data.frame</code> of covariates. For using
<span class="pkg">WeightIt</span> functions, this is not necessary, but for use with
<span class="pkg">cobalt</span> it is. Note that when using with a <code>weightit.fit</code> object, this should not be the matrix supplied to the <code>covs</code> argument of <code>weightit.fit()</code> unless there are no factor/character variables in it. Ideally this is the original, unprocessed covariate data frame with factor variables included.</p>
</td></tr>
<tr><td><code id="as.weightit_+3A_treat">treat</code></td>
<td>
<p>a vector of treatment statuses, one for each unit. Required when <code>x</code> is a vector of weights.</p>
</td></tr>
<tr><td><code id="as.weightit_+3A_estimand">estimand</code></td>
<td>
<p>an optional <code>character</code> of length 1 giving the
estimand. The text is not checked.</p>
</td></tr>
<tr><td><code id="as.weightit_+3A_s.weights">s.weights</code></td>
<td>
<p>an optional <code>numeric</code> vector of sampling weights, one
for each unit.</p>
</td></tr>
<tr><td><code id="as.weightit_+3A_ps">ps</code></td>
<td>
<p>an optional <code>numeric</code> vector of propensity scores, one for
each unit.</p>
</td></tr>
<tr><td><code id="as.weightit_+3A_treat.list">treat.list</code></td>
<td>
<p>a list of treatment statuses at each time point.</p>
</td></tr>
<tr><td><code id="as.weightit_+3A_covs.list">covs.list</code></td>
<td>
<p>an optional list of <code>data.frame</code>s of covariates of
covariates at each time point. For using <span class="pkg">WeightIt</span> functions, this is
not necessary, but for use with <span class="pkg">cobalt</span> it is.</p>
</td></tr>
<tr><td><code id="as.weightit_+3A_ps.list">ps.list</code></td>
<td>
<p>an optional list of <code>numeric</code> vectors of propensity
scores at each time point.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>weightit</code> (for <code>as.weightit()</code>) or <code>weightitMSM</code> (for <code>as.weightitMSM()</code>).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
treat &lt;- rbinom(500, 1, .3)
weights &lt;- rchisq(500, df = 2)

W &lt;- as.weightit(weights, treat = treat, estimand = "ATE")
summary(W)

# See ?weightit.fit for using as.weightit() with a
# weightit.fit object.

</code></pre>

<hr>
<h2 id='calibrate'>Calibrate Propensity Score Weights</h2><span id='topic+calibrate'></span><span id='topic+calibrate.default'></span><span id='topic+calibrate.weightit'></span>

<h3>Description</h3>

<p><code>calibrate()</code> performs Platt scaling to calibrate propensity scores as recommended by Gutman et al. (2022). This involves fitting a new propensity score model using logistic regression with the previously estimated propensity score as the sole predictor. Weights are computed using this new propensity score.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calibrate(x, ...)

## Default S3 method:
calibrate(x, treat, s.weights = NULL, data = NULL, ...)

## S3 method for class 'weightit'
calibrate(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="calibrate_+3A_x">x</code></td>
<td>
<p>A <code>weightit</code> object or a vector of propensity scores. Only binary treatments are supported.</p>
</td></tr>
<tr><td><code id="calibrate_+3A_...">...</code></td>
<td>
<p>Not used.</p>
</td></tr>
<tr><td><code id="calibrate_+3A_treat">treat</code></td>
<td>
<p>A vector of treatment status for each unit. Only binary treatments are supported.</p>
</td></tr>
<tr><td><code id="calibrate_+3A_s.weights">s.weights</code></td>
<td>
<p>A vector of sampling weights or the name of a variable in
<code>data</code> that contains sampling weights.</p>
</td></tr>
<tr><td><code id="calibrate_+3A_data">data</code></td>
<td>
<p>An optional data frame containing the variable named in <code>s.weights</code> when supplied as a string.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>If the input is a <code>weightit</code> object, the output will be a
<code>weightit</code> object with the propensity scores replaced with the calibrated propensity scores and the weights replaced by weights computed from the calibrated propensity scores.
</p>
<p>If the input is a numeric vector of weights, the output will be a numeric
vector of the calibrated propensity scores.
</p>


<h3>References</h3>

<p>Gutman, R., Karavani, E., &amp; Shimoni, Y. (2022). Propensity score models are better when post-calibrated (arXiv:2211.01221). arXiv. <a href="http://arxiv.org/abs/2211.01221">http://arxiv.org/abs/2211.01221</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+weightit">weightit()</a></code>, <code><a href="#topic+weightitMSM">weightitMSM()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library("cobalt")
data("lalonde", package = "cobalt")

#Using GBM to estimate weights
(W &lt;- weightit(treat ~ age + educ + married +
                 nodegree + re74, data = lalonde,
               method = "gbm", estimand = "ATT",
               criterion = "smd.max"))
summary(W)

#Calibrating the GBM propensity scores
Wc &lt;- calibrate(W)

#Calibrating propensity scores directly
PSc &lt;- calibrate(W$ps, treat = lalonde$treat)

</code></pre>

<hr>
<h2 id='ESS'>Compute effective sample size of weighted sample</h2><span id='topic+ESS'></span>

<h3>Description</h3>

<p>Computes the effective sample size (ESS) of a weighted sample, which
represents the size of an unweighted sample with approximately the same
amount of precision as the weighted sample under consideration.
</p>
<p>The ESS is calculated as <code class="reqn">(\sum w)^2/\sum w^2</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ESS(w)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ESS_+3A_w">w</code></td>
<td>
<p>a vector of weights</p>
</td></tr>
</table>


<h3>References</h3>

<p>McCaffrey, D. F., Ridgeway, G., &amp; Morral, A. R. (2004).
Propensity Score Estimation With Boosted Regression for Evaluating Causal
Effects in Observational Studies. Psychological Methods, 9(4), 403–425. <a href="https://doi.org/10.1037/1082-989X.9.4.403">doi:10.1037/1082-989X.9.4.403</a>
</p>
<p>Shook‐Sa, B. E., &amp; Hudgens, M. G. (2020). Power and sample size for
observational studies of point exposure effects. Biometrics, biom.13405. <a href="https://doi.org/10.1111/biom.13405">doi:10.1111/biom.13405</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+summary.weightit">summary.weightit()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library("cobalt")
data("lalonde", package = "cobalt")

#Balancing covariates between treatment groups (binary)
(W1 &lt;- weightit(treat ~ age + educ + married +
                  nodegree + re74, data = lalonde,
                method = "glm", estimand = "ATE"))
summary(W1)
ESS(W1$weights[W1$treat == 0])
ESS(W1$weights[W1$treat == 1])
</code></pre>

<hr>
<h2 id='get_w_from_ps'>Compute weights from propensity scores</h2><span id='topic+get_w_from_ps'></span>

<h3>Description</h3>

<p>Given a vector or matrix of propensity scores, outputs a vector of weights
that target the provided estimand.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_w_from_ps(
  ps,
  treat,
  estimand = "ATE",
  focal = NULL,
  treated = NULL,
  subclass = NULL,
  stabilize = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_w_from_ps_+3A_ps">ps</code></td>
<td>
<p>A vector, matrix, or data frame of propensity scores. See Details.</p>
</td></tr>
<tr><td><code id="get_w_from_ps_+3A_treat">treat</code></td>
<td>
<p>A vector of treatment status for each individual. See Details.</p>
</td></tr>
<tr><td><code id="get_w_from_ps_+3A_estimand">estimand</code></td>
<td>
<p>The desired estimand that the weights should target. Current
options include &quot;ATE&quot; (average treatment effect), &quot;ATT&quot; (average treatment
effect on the treated), &quot;ATC&quot; (average treatment effect on the control),
&quot;ATO&quot; (average treatment effect in the overlap), &quot;ATM&quot; (average treatment
effect in the matched sample), and &quot;ATOS&quot; (average treatment effect in the
optimal subset).</p>
</td></tr>
<tr><td><code id="get_w_from_ps_+3A_focal">focal</code></td>
<td>
<p>When the estimand is the ATT or ATC, which group should be
consider the (focal) &quot;treated&quot; or &quot;control&quot; group, respectively. If not
<code>NULL</code> and <code>estimand</code> is not &quot;ATT&quot; or &quot;ATC&quot;, <code>estimand</code> will
automatically be set to &quot;ATT&quot;.</p>
</td></tr>
<tr><td><code id="get_w_from_ps_+3A_treated">treated</code></td>
<td>
<p>When treatment is binary, the value of <code>treat</code> that is
considered the &quot;treated&quot; group (i.e., the group for which the propensity
scores are the probability of being in). If <code>NULL</code>,
<code>get_w_from_ps()</code> will attempt to figure it out on its own using some
heuristics. This really only matters when <code>treat</code> has values other than
0 and 1 and when <code>ps</code> is given as a vector or an unnamed single-column
matrix or data frame.</p>
</td></tr>
<tr><td><code id="get_w_from_ps_+3A_subclass">subclass</code></td>
<td>
<p><code>numeric</code>; the number of subclasses to use when
computing weights using marginal mean weighting through stratification (also
known as fine stratification). If <code>NULL</code>, standard inverse probability
weights (and their extensions) will be computed; if a number greater than 1,
subclasses will be formed and weights will be computed based on subclass
membership. <code>estimand</code> must be ATE, ATT, or ATC if <code>subclass</code> is
non-<code>NULL</code>. See Details.</p>
</td></tr>
<tr><td><code id="get_w_from_ps_+3A_stabilize">stabilize</code></td>
<td>
<p><code>logical</code>; whether to compute stabilized weights or
not. This simply involves multiplying each unit's weight by the proportion
of units in their treatment group. For saturated outcome models and in
balance checking, this won't make a difference; otherwise, this can improve
performance.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>get_w_from_ps()</code> applies the formula for computing weights from
propensity scores for the desired estimand. See the References section for
information on these estimands and the formulas.
</p>
<p><code>ps</code> can be entered a variety of ways. For binary treatments, when
<code>ps</code> is entered as a vector or unnamed single-column matrix or data
frame, <code>get_w_from_ps()</code> has to know which value of <code>treat</code>
corresponds to the &quot;treated&quot; group. For 0/1 variables, 1 will be considered
treated. For other types of variables, <code>get_w_from_ps()</code> will try to
figure it out using heuristics, but it's safer to supply an argument to
<code>treated</code>. When <code>estimand</code> is &quot;ATT&quot; or &quot;ATC&quot;, supplying a value to
<code>focal</code> is sufficient (for ATT, <code>focal</code> is the treated group, and
for ATC, <code>focal</code> is the control group). When entered as a matrix or
data frame, the columns must be named with the levels of the treatment, and
it is assumed that each column corresponds to the probability of being in
that treatment group. This is the safest way to supply <code>ps</code> unless
<code>treat</code> is a 0/1 variable.
</p>
<p>For multi-category treatments, <code>ps</code> can be entered as a vector or a
matrix or data frame. When entered as a vector, it is assumed the value
corresponds to the probability of being in the treatment actually received;
this is only possible when the estimand is &quot;ATE&quot;. Otherwise, <code>ps</code> must
be entered as a named matrix or data frame as described above for binary
treatments. When the estimand is &quot;ATT&quot; or &quot;ATC&quot;, a value for <code>focal</code>
must be specified.
</p>
<p>When <code>subclass</code> is not <code>NULL</code>, marginal mean weighting through
stratification (MMWS) weights are computed. The implementation differs
slightly from that described in Hong (2010, 2012). First, subclasses are
formed by finding the quantiles of the propensity scores in the target group
(for the ATE, all units; for the ATT or ATC, just the units in the focal
group). Any subclasses lacking members of a treatment group will be filled
in with them from neighboring subclasses so each subclass will always have
at least one member of each treatment group. A new subclass-propensity score
matrix is formed, where each unit's subclass-propensity score for each
treatment value is computed as the proportion of units with that treatment
value in the unit's subclass. For example, if a subclass had 10 treated
units and 90 control units in it, the subclass-propensity score for being
treated would be .1 and the subclass-propensity score for being control
would be .9 for all units in the subclass. For multi-category treatments,
the propensity scores for each treatment are stratified separately as
described in Hong (2012); for binary treatments, only one set of propensity
scores are stratified and the subclass-propensity scores for the other
treatment are computed as the complement of the propensity scores for the
stratified treatment. After the subclass-propensity scores have been
computed, the standard propensity score weighting formulas are used to
compute the unstabilized MMWS weights. To estimate MMWS weights equivalent
to those described in Hong (2010, 2012), <code>stabilize</code> must be set to
<code>TRUE</code>, but, as with standard propensity score weights, this is
optional. Note that MMWS weights are also known as fine stratification
weights and described by Desai et al. (2017).
</p>
<p><code>get_w_from_ps()</code> is not compatible with continuous treatments.
</p>


<h3>Value</h3>

<p>A vector of weights. When <code>subclass</code> is not <code>NULL</code>, the
subclasses are returned as the <code>"subclass"</code> attribute. When
<code>estimand = "ATOS"</code>, the chosen value of <code>alpha</code> (the smallest
propensity score allowed to remain in the sample) is returned in the
<code>"alpha"</code> attribute.
</p>


<h3>References</h3>



<h4>Binary treatments</h4>


<ul>
<li> <p><code>estimand = "ATO"</code>
</p>
</li></ul>

<p>Li, F., Morgan, K. L., &amp; Zaslavsky, A. M. (2018). Balancing covariates via
propensity score weighting. Journal of the American Statistical Association,
113(521), 390–400. <a href="https://doi.org/10.1080/01621459.2016.1260466">doi:10.1080/01621459.2016.1260466</a>
</p>

<ul>
<li> <p><code>estimand = "ATM"</code>
</p>
</li></ul>

<p>Li, L., &amp; Greene, T. (2013). A Weighting Analogue to Pair Matching in
Propensity Score Analysis. The International Journal of Biostatistics, 9(2). <a href="https://doi.org/10.1515/ijb-2012-0030">doi:10.1515/ijb-2012-0030</a>
</p>

<ul>
<li> <p><code>estimand = "ATOS"</code>
</p>
</li></ul>

<p>Crump, R. K., Hotz, V. J., Imbens, G. W., &amp; Mitnik, O. A. (2009). Dealing
with limited overlap in estimation of average treatment effects. Biometrika,
96(1), 187–199. <a href="https://doi.org/10.1093/biomet/asn055">doi:10.1093/biomet/asn055</a>
</p>

<ul>
<li><p> Other estimands
</p>
</li></ul>

<p>Austin, P. C. (2011). An Introduction to Propensity Score Methods for
Reducing the Effects of Confounding in Observational Studies. Multivariate
Behavioral Research, 46(3), 399–424. <a href="https://doi.org/10.1080/00273171.2011.568786">doi:10.1080/00273171.2011.568786</a>
</p>

<ul>
<li><p> Marginal mean weighting through stratification (MMWS)
</p>
</li></ul>

<p>Hong, G. (2010). Marginal mean weighting through stratification: Adjustment
for selection bias in multilevel data. Journal of Educational and Behavioral
Statistics, 35(5), 499–531. <a href="https://doi.org/10.3102/1076998609359785">doi:10.3102/1076998609359785</a>
</p>
<p>Desai, R. J., Rothman, K. J., Bateman, B. . T., Hernandez-Diaz, S., &amp;
Huybrechts, K. F. (2017). A Propensity-score-based Fine Stratification
Approach for Confounding Adjustment When Exposure Is Infrequent:
Epidemiology, 28(2), 249–257. <a href="https://doi.org/10.1097/EDE.0000000000000595">doi:10.1097/EDE.0000000000000595</a>
</p>



<h4>Multi-Category Treatments</h4>


<ul>
<li> <p><code>estimand = "ATO"</code>
</p>
</li></ul>

<p>Li, F., &amp; Li, F. (2019). Propensity score weighting for causal inference
with multiple treatments. The Annals of Applied Statistics, 13(4),
2389–2415. <a href="https://doi.org/10.1214/19-AOAS1282">doi:10.1214/19-AOAS1282</a>
</p>

<ul>
<li> <p><code>estimand = "ATM"</code>
</p>
</li></ul>

<p>Yoshida, K., Hernández-Díaz, S., Solomon, D. H., Jackson, J. W., Gagne, J.
J., Glynn, R. J., &amp; Franklin, J. M. (2017). Matching weights to
simultaneously compare three treatment groups: Comparison to three-way
matching. Epidemiology (Cambridge, Mass.), 28(3), 387–395. <a href="https://doi.org/10.1097/EDE.0000000000000627">doi:10.1097/EDE.0000000000000627</a>
</p>

<ul>
<li><p> Other estimands
</p>
</li></ul>

<p>McCaffrey, D. F., Griffin, B. A., Almirall, D., Slaughter, M. E., Ramchand,
R., &amp; Burgette, L. F. (2013). A Tutorial on Propensity Score Estimation for
Multiple Treatments Using Generalized Boosted Models. Statistics in
Medicine, 32(19), 3388–3414. <a href="https://doi.org/10.1002/sim.5753">doi:10.1002/sim.5753</a>
</p>

<ul>
<li><p> Marginal mean weighting through stratification
</p>
</li></ul>

<p>Hong, G. (2012). Marginal mean weighting through stratification: A
generalized method for evaluating multivalued and multiple treatments with
nonexperimental data. Psychological Methods, 17(1), 44–60. <a href="https://doi.org/10.1037/a0024918">doi:10.1037/a0024918</a>
</p>



<h3>See Also</h3>

<p><code><a href="#topic+method_glm">method_glm</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library("cobalt")
data("lalonde", package = "cobalt")

ps.fit &lt;- glm(treat ~ age + educ + race + married +
                nodegree + re74 + re75, data = lalonde,
              family = binomial)
ps &lt;- ps.fit$fitted.values

w1 &lt;- get_w_from_ps(ps, treat = lalonde$treat,
                    estimand = "ATT")

treatAB &lt;- factor(ifelse(lalonde$treat == 1, "A", "B"))
w2 &lt;- get_w_from_ps(ps, treat = treatAB,
                    estimand = "ATT", focal = "A")
all.equal(w1, w2)
w3 &lt;- get_w_from_ps(ps, treat = treatAB,
                    estimand = "ATT", treated = "A")
all.equal(w1, w3)

#Using MMWS
w4 &lt;- get_w_from_ps(ps, treat = lalonde$treat,
                    estimand = "ATE", subclass = 20,
                    stabilize = TRUE)

#A multi-category example using GBM predicted probabilities
library(gbm)
T3 &lt;- factor(sample(c("A", "B", "C"), nrow(lalonde), replace = TRUE))

gbm.fit &lt;- gbm(T3 ~ age + educ + race + married +
                 nodegree + re74 + re75, data = lalonde,
               distribution = "multinomial", n.trees = 200,
               interaction.depth = 3)
ps.multi &lt;- drop(predict(gbm.fit, type = "response",
                         n.trees = 200))
w &lt;- get_w_from_ps(ps.multi, T3, estimand = "ATE")

</code></pre>

<hr>
<h2 id='glm_weightit'>Fitting Weighted Generalized Linear Models</h2><span id='topic+glm_weightit'></span><span id='topic+lm_weightit'></span><span id='topic+summary.glm_weightit'></span>

<h3>Description</h3>

<p><code>lm_weightit()</code> and <code>glm_weightit()</code> are used to fit (generalized) linear models with a variance matrix that accounts for estimation of weights, if supplied. By default, these functions use M-estimation to construct a robust covariance matrix using the estimation equations for the weighting model and the outcome model. <code>lm_weightit()</code> is a wrapper for <code>glm_weightit()</code> with the Gaussian family and identity link (i.e., a linear model).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>glm_weightit(
  formula,
  data,
  family = gaussian,
  weightit,
  vcov = NULL,
  cluster,
  R = 500,
  offset,
  start = NULL,
  etastart,
  mustart,
  control = list(...),
  x = FALSE,
  y = TRUE,
  contrasts = NULL,
  fwb.args = list(),
  ...
)

lm_weightit(
  formula,
  data,
  weightit,
  vcov = NULL,
  cluster,
  R = 500,
  offset,
  start = NULL,
  etastart,
  mustart,
  control = list(...),
  x = FALSE,
  y = TRUE,
  contrasts = NULL,
  ...
)

## S3 method for class 'glm_weightit'
summary(object, ci = FALSE, level = 0.95, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="glm_weightit_+3A_formula">formula</code></td>
<td>
<p>an object of class &quot;formula&quot; (or one that can be coerced to that class): a symbolic description of the model to be fitted.</p>
</td></tr>
<tr><td><code id="glm_weightit_+3A_data">data</code></td>
<td>
<p>a data frame containing the variables in the model. If not found in data, the variables are taken from <code>environment(formula)</code>, typically the environment from which the function is called.</p>
</td></tr>
<tr><td><code id="glm_weightit_+3A_family">family</code></td>
<td>
<p>a description of the error distribution and link function to be used in the model. This can be a character string naming a family function, a family function or the result of a call to a family function. See <a href="stats.html#topic+family">family</a> for details of family functions. Can also be the string <code>"multinomial"</code> for multinomial logistic regression.</p>
</td></tr>
<tr><td><code id="glm_weightit_+3A_weightit">weightit</code></td>
<td>
<p>a <code>weightit</code> or <code>weightitMSM</code> object; the output of a call to <code><a href="#topic+weightit">weightit()</a></code> or <code><a href="#topic+weightitMSM">weightitMSM()</a></code>. If not supplied, an unweighted model will be fit.</p>
</td></tr>
<tr><td><code id="glm_weightit_+3A_vcov">vcov</code></td>
<td>
<p>string; the method used to compute the variance of the estimated parameters. Allowable options include <code>"asympt"</code>, which uses the asymptotically correct M-estimation-based method that accounts for estimation of the weights when available; <code>"const"</code>, which uses the usual maximum likelihood estimates (only available when <code>weightit</code> is not supplied); <code>"HC0"</code>, which computes the robust sandwich variance treating weights (if supplied) as fixed; <code>"BS"</code>, which uses the traditional bootstrap (including re-estimation of the weights, if supplied); <code>"FWB"</code>, which uses the fractional weighted bootstrap as implemented in <code><a href="fwb.html#topic+fwb">fwb::fwb()</a></code> (including re-estimation of the weights, if supplied); and <code>"none"</code> to omit calculation of a variance matrix. If <code>NULL</code> (the default), will use <code>"asympt"</code> if <code>weightit</code> is supplied and M-estimation is available and <code>"HC0"</code> otherwise. See the <code>vcov_type</code> component of the outcome object to see which was used.</p>
</td></tr>
<tr><td><code id="glm_weightit_+3A_cluster">cluster</code></td>
<td>
<p>optional; for computing a cluster-robust variance matrix, a variable indicating the clustering of observations, a list (or data frame) thereof, or a one-sided formula specifying which variable(s) from the fitted model should be used. Note the cluster-robust variance matrix uses a correction for small samples, as is done in <code>sadwich::vcovCL()</code> by default. Cluster-robust variance calculations are available only when <code>vcov</code> is <code>"asympt"</code>, <code>"HC0"</code>, <code>"BS"</code>, or <code>"FWB"</code>.</p>
</td></tr>
<tr><td><code id="glm_weightit_+3A_r">R</code></td>
<td>
<p>the number of bootstrap replications when <code>vcov</code> is <code>"BS"</code> or <code>"FWB"</code>. Default is 500. Ignored otherwise.</p>
</td></tr>
<tr><td><code id="glm_weightit_+3A_offset">offset</code></td>
<td>
<p>optional; a numeric vector contain the model offset. See <code><a href="stats.html#topic+offset">offset()</a></code>. An offset can also be preset in the model formula.</p>
</td></tr>
<tr><td><code id="glm_weightit_+3A_start">start</code></td>
<td>
<p>optional starting values for the coefficients.</p>
</td></tr>
<tr><td><code id="glm_weightit_+3A_etastart">etastart</code>, <code id="glm_weightit_+3A_mustart">mustart</code></td>
<td>
<p>optional starting values for the linear predictor and vector of means when <code>family</code> is not <code>"multinomial"</code>. Passed to <code><a href="stats.html#topic+glm">glm()</a></code>.</p>
</td></tr>
<tr><td><code id="glm_weightit_+3A_control">control</code></td>
<td>
<p>a list of parameters for controlling the fitting process.</p>
</td></tr>
<tr><td><code id="glm_weightit_+3A_x">x</code>, <code id="glm_weightit_+3A_y">y</code></td>
<td>
<p>logical values indicating whether the response vector and model matrix used in the fitting process should be returned as components of the returned value.</p>
</td></tr>
<tr><td><code id="glm_weightit_+3A_contrasts">contrasts</code></td>
<td>
<p>an optional list define contrasts for factor variables. See <code><a href="stats.html#topic+model.matrix">model.matrix()</a></code>.</p>
</td></tr>
<tr><td><code id="glm_weightit_+3A_fwb.args">fwb.args</code></td>
<td>
<p>an optional list of further arguments to supply to <code><a href="fwb.html#topic+fwb">fwb::fwb()</a></code> when <code>vcov = "FWB"</code>.</p>
</td></tr>
<tr><td><code id="glm_weightit_+3A_...">...</code></td>
<td>
<p>for <code>glm_weightit()</code> and <code>lm_weightit()</code>, arguments to be used to form the default control argument if it is not supplied directly. Ignored otherwise.</p>
</td></tr>
<tr><td><code id="glm_weightit_+3A_object">object</code></td>
<td>
<p>a <code>glm_weightit</code> object.</p>
</td></tr>
<tr><td><code id="glm_weightit_+3A_ci">ci</code></td>
<td>
<p><code>logical</code> whether to display Wald confidence intervals for estimated coefficients. Default is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="glm_weightit_+3A_level">level</code></td>
<td>
<p>when <code>ci = TRUE</code>, the desired confidence level.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code><a href="#topic+glm_weightit">glm_weightit()</a></code> is essentially a wrapper for <code><a href="stats.html#topic+glm">glm()</a></code> that optionally computes a coefficient variance matrix that can be adjusted to account for estimation of the weights if a <code>weightit</code> or <code>weightitMSM</code> object is supplied to the <code>weightit</code> argument. When no argument is supplied to <code>weightit</code> or there is no <code>"Mparts"</code> attribute in the supplied object, the default variance matrix returned will be the &quot;HC0&quot; sandwich variance matrix, which is robust to misspecification of the outcome family (including heteroscedasticity). Otherwise, the default variance matrix uses M-estimation to additionally adjust for estimation of the weights. When possible, this often yields smaller (and more accurate) standard errors. See the individual methods pages to see whether and when an <code>"Mparts"</code> attribute is included in the supplied object. To request that a variance matrix be computed that doesn't account for estimation of the weights even when a compatible <code>weightit</code> object is supplied, set <code>vcov = "HC0"</code>, which treats the weights as fixed.
</p>
<p>Bootstrapping can also be used to compute the coefficient variance matrix; when <code>vcov = "BS"</code> or <code>vcov = "FWB"</code>, which implement the traditional resampling-based and fractional weighted bootstrap, respectively, the entire process of estimating the weights and fitting the outcome model is repeated in bootstrap samples (if a <code>weightit</code> object is supplied). This accounts for estimation of the weights and can be used with any weighting method. It is important to set a seed using <code>set.seed()</code> to ensure replicability of the results. The fractional weighted bootstrap is more reliable but requires the weighting method to accept sampling weights (which most do, and you'll get an error if it doesn't). Setting <code>vcov = "FWB"</code> and supplying <code>fwb.args = list(wtype = "multinom")</code> also performs the resampling-based bootstrap but with the additional features <span class="pkg">fwb</span> provides (e.g., a progress and parallelization) at the expense of needing to have <span class="pkg">fwb</span> installed.
</p>
<p>When <code>family = "multinomial"</code>, multinomial logistic regression is fit using a custom function in <span class="pkg">WeightIt</span> that uses M-estimation to estimate the model coefficients. This implementation is less robust to failures than other multinomial logistic regression solvers and should be used with caution. Estimation of coefficients should align with that from <code>mlogit::mlogit()</code> and <code>mclogit::mblogit()</code>.
</p>
<p>Functions in the <span class="pkg">sandwich</span> package can be to compute standard errors after fitting, regardless of how <code>vcov</code> was specified, though these will ignore estimation of the weights, if any. When no adjustment is done for estimation of the weights (i.e., because no <code>weightit</code> argument was supplied or there was no <code>"Mparts"</code> component in the supplied object), the default variance matrix produced by <code>glm_weightit()</code> should align with that from <code style="white-space: pre;">&#8288;sandwich::vcovHC(. type = "HC0")&#8288;</code> or <code>sandwich::vcovCL(., type = "HC0", cluster = cluster)</code> when <code>cluster</code> is supplied.
</p>


<h3>Value</h3>

<p>A <code>glm_weightit</code> object, which inherits from <code>glm</code>. Unless <code>vcov = "none"</code>, the <code>vcov</code> component contains the covariance matrix adjusted for the estimation of the weights if requested and a compatible <code>weightit</code> object was supplied. The <code>vcov_type</code> component contains the type of variance matrix requested. If <code>cluster</code> is supplied, it will be stored in the <code>"cluster"</code> attribute of the output object, even if not used.
</p>
<p><code>print()</code>, <code>vcov()</code>, <code>predict()</code>, and <code>confint()</code> methods are also available; these generally follow the same pattern as the respect method for <code>glm</code> objects. <code>confint()</code> uses Wald confidence intervals (internally calling <code><a href="stats.html#topic+confint.lm">confint.lm()</a></code>). When <code>family = "multinomial"</code>, predict() produces a matrix of predicted probabilities, one for each level of the outcome, and the <code>type</code> argument is ignored. <code>model.frame()</code> output (also the <code>model</code> component of the output object) will include two extra column when <code>weightit</code> is supplied: <code>(weights)</code> containing the weights used in the model (the product of the estimated weights and the sampling weights, if any) and <code>(s.weights)</code> containing the sampling weights, which will be 1 if <code>s.weights</code> is not supplied in the original <code>weightit()</code> call.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+lm">lm()</a></code> and <code><a href="stats.html#topic+glm">glm()</a></code> for fitting generalized linear models without adjusting standard errors for estimation of the weights.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data("lalonde", package = "cobalt")

# Logistic regression ATT weights
w.out &lt;- weightit(treat ~ age + educ + married + re74,
                  data = lalonde, method = "glm",
                  estimand = "ATT")

# Linear regression outcome model that adjusts
# for estimation of weights
fit1 &lt;- lm_weightit(re78 ~ treat, data = lalonde,
                    weightit = w.out)

summary(fit1)

# Linear regression outcome model that treats weights
# as fixed
fit2 &lt;- lm_weightit(re78 ~ treat, data = lalonde,
                    weightit = w.out, vcov = "HC0")

summary(fit2)

# Linear regression outcome model that bootstraps
# estimation of weights and outcome model fitting
# using fractional weighted bootstrap with "Mammen"
# weights
set.seed(123)
fit3 &lt;- lm_weightit(re78 ~ treat, data = lalonde,
                    weightit = w.out,
                    vcov = "FWB",
                    R = 50,
                    fwb.args = list(wtype = "mammen"))

summary(fit3)

# Multinomial logistic regression outcome model
# that adjusts for estimation of weights
lalonde$re78_3 &lt;- factor(findInterval(lalonde$re78,
                                      c(0, 5e3, 1e4)))

fit4 &lt;- glm_weightit(re78_3 ~ treat, data = lalonde,
                     weightit = w.out,
                     family = "multinomial")

summary(fit4)
</code></pre>

<hr>
<h2 id='make_full_rank'>Make a design matrix full rank</h2><span id='topic+make_full_rank'></span>

<h3>Description</h3>

<p>When writing <a href="#topic+method_user">user-defined methods</a> for use with
<code><a href="#topic+weightit">weightit()</a></code>, it may be necessary to take the potentially non-full rank
<code>covs</code> data frame and make it full rank for use in a downstream
function. This function performs that operation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>make_full_rank(mat, with.intercept = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="make_full_rank_+3A_mat">mat</code></td>
<td>
<p>a numeric matrix or data frame to be transformed. Typically this
contains covariates. <code>NA</code>s are not allowed.</p>
</td></tr>
<tr><td><code id="make_full_rank_+3A_with.intercept">with.intercept</code></td>
<td>
<p>whether an intercept (i.e., a vector of 1s) should be
added to <code>mat</code> before making it full rank. If <code>TRUE</code>, the
intercept will be used in determining whether a column is linearly dependent
on others. Regardless, no intercept will be included in the output.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>make_full_rank()</code> calls <code><a href="base.html#topic+qr">qr()</a></code> to find the rank and linearly
independent columns of <code>mat</code>, which are retained while others are
dropped. If <code>with.intercept</code> is set to <code>TRUE</code>, an intercept column
is added to the matrix before calling <code>qr()</code>. Note that dependent
columns that appear later in <code>mat</code> will be dropped first.
</p>
<p>See example at <code><a href="#topic+method_user">method_user</a></code>.
</p>


<h3>Value</h3>

<p>An object of the same type as <code>mat</code> containing only linearly
independent columns.
</p>


<h3>Note</h3>

<p>Older versions would drop all columns that only had one value. With
<code>with.intercept = FALSE</code>, if only one column has only one value, it
will not be removed, and it will function as though there was an intercept
present; if more than only column has only one value, only the first one
will remain.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+method_user">method_user</a></code>, <code><a href="stats.html#topic+model.matrix">model.matrix()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
set.seed(1000)
c1 &lt;- rbinom(10, 1, .4)
c2 &lt;- 1-c1
c3 &lt;- rnorm(10)
c4 &lt;- 10*c3
mat &lt;- data.frame(c1, c2, c3, c4)

make_full_rank(mat) #leaves c2 and c4

make_full_rank(mat, with.intercept = FALSE) #leaves c1, c2, and c4
</code></pre>

<hr>
<h2 id='method_bart'>Propensity Score Weighting Using BART</h2><span id='topic+method_bart'></span>

<h3>Description</h3>

<p>This page explains the details of estimating weights from Bayesian additive regression trees (BART)-based propensity scores by setting <code>method = "bart"</code> in the call to <code><a href="#topic+weightit">weightit()</a></code> or <code><a href="#topic+weightitMSM">weightitMSM()</a></code>. This method can be used with binary, multi-category, and continuous treatments.
</p>
<p>In general, this method relies on estimating propensity scores using BART and then converting those propensity scores into weights using a formula that depends on the desired estimand. This method relies on <code><a href="dbarts.html#topic+bart">dbarts::bart2()</a></code> from the <a href="https://CRAN.R-project.org/package=dbarts"><span class="pkg">dbarts</span></a> package.
</p>


<h4>Binary Treatments</h4>

<p>For binary treatments, this method estimates the propensity scores using <code><a href="dbarts.html#topic+bart">dbarts::bart2()</a></code>. The following estimands are allowed: ATE, ATT, ATC, ATO, ATM, and ATOS. Weights can also be computed using marginal mean weighting through stratification for the ATE, ATT, and ATC. See <code><a href="#topic+get_w_from_ps">get_w_from_ps()</a></code> for details.
</p>



<h4>Multi-Category Treatments</h4>

<p>For multi-category treatments, the propensity scores are estimated using several calls to <code><a href="dbarts.html#topic+bart">dbarts::bart2()</a></code>, one for each treatment group; the treatment probabilities are not normalized to sum to 1. The following estimands are allowed: ATE, ATT, ATC, ATO, and ATM. The weights for each estimand are computed using the standard formulas or those mentioned above. Weights can also be computed using marginal mean weighting through stratification for the ATE, ATT, and ATC. See <code><a href="#topic+get_w_from_ps">get_w_from_ps()</a></code> for details.
</p>



<h4>Continuous Treatments</h4>

<p>For continuous treatments, the generalized propensity score is estimated using <code><a href="dbarts.html#topic+bart">dbarts::bart2()</a></code>. In addition, kernel density estimation can be used instead of assuming a normal density for the numerator and denominator of the generalized propensity score by setting <code>use.kernel = TRUE</code>. Other arguments to <code><a href="stats.html#topic+density">density()</a></code> can be specified to refine the density estimation parameters. <code>plot = TRUE</code> can be specified to plot the density for the numerator and denominator, which can be helpful in diagnosing extreme weights.
</p>



<h4>Longitudinal Treatments</h4>

<p>For longitudinal treatments, the weights are the product of the weights estimated at each time point.
</p>



<h4>Sampling Weights</h4>

<p>Sampling weights are not supported.
</p>



<h4>Missing Data</h4>

<p>In the presence of missing data, the following value(s) for <code>missing</code> are allowed:
</p>

<dl>
<dt><code>"ind"</code> (default)</dt><dd>
<p>First, for each variable with missingness, a new missingness indicator variable is created which takes the value 1 if the original covariate is <code>NA</code> and 0 otherwise. The missingness indicators are added to the model formula as main effects. The missing values in the covariates are then replaced with the covariate medians. The weight estimation then proceeds with this new formula and set of covariates. The covariates output in the resulting <code>weightit</code> object will be the original covariates with the <code>NA</code>s.
</p>
</dd>
</dl>




<h4>M-estimation</h4>

<p>M-estimation is not supported.
</p>



<h3>Details</h3>

<p>BART works by fitting a sum-of-trees model for the treatment or probability of treatment. The number of trees is determined by the <code>n.trees</code> argument. Bayesian priors are used for the hyperparameters, so the result is a posterior distribution of predicted values for each unit. The mean of these for each unit is taken for use in computing the (generalized) propensity score. Although the hyperparameters governing the priors can be modified by supplying arguments to <code>weightit()</code> that are passed to the BART fitting function, the default values tend to work well and require little modification (though the defaults differ for continuous and categorical treatments; see the <code><a href="dbarts.html#topic+bart">dbarts::bart2()</a></code> documentation for details). Unlike many other machine learning methods, no loss function is optimized and the hyperparameters do not need to be tuned (e.g., using cross-validation), though performance can benefit from tuning. BART tends to balance sparseness with flexibility by using very weak learners as the trees, which makes it suitable for capturing complex functions without specifying a particular functional form and without overfitting.
</p>


<h3>Additional Arguments</h3>

<p>All arguments to <code><a href="dbarts.html#topic+bart">dbarts::bart2()</a></code> can be passed through <code>weightit()</code> or <code>weightitMSM()</code>, with the following exceptions:
</p>

<ul>
<li> <p><code>test</code>, <code>weights</code>,<code>subset</code>, <code>offset.test</code> are ignored
</p>
</li>
<li> <p><code>combine.chains</code> is always set to <code>TRUE</code>
</p>
</li>
<li> <p><code>sampleronly</code> is always set to <code>FALSE</code>
</p>
</li></ul>

<p>For continuous treatments only, the following arguments may be supplied:
</p>

<dl>
<dt><code>density</code></dt><dd><p>A function corresponding to the conditional density of the treatment. The standardized residuals of the treatment model will be fed through this function to produce the numerator and denominator of the generalized propensity score weights. If blank, <code><a href="stats.html#topic+dnorm">dnorm()</a></code> is used as recommended by Robins et al. (2000). This can also be supplied as a string containing the name of the function to be called. If the string contains underscores, the call will be split by the underscores and the latter splits will be supplied as arguments to the second argument and beyond. For example, if <code>density = "dt_2"</code> is specified, the density used will be that of a t-distribution with 2 degrees of freedom. Using a t-distribution can be useful when extreme outcome values are observed (Naimi et al., 2014). Ignored if <code>use.kernel = TRUE</code> (described below).
</p>
</dd>
<dt><code>use.kernel</code></dt><dd><p>If <code>TRUE</code>, uses kernel density estimation through <code><a href="stats.html#topic+density">density()</a></code> to estimate the numerator and denominator densities for the weights. If <code>FALSE</code>, the argument to the <code>density</code> parameter is used instead.
</p>
</dd>
<dt><code>bw</code>, <code>adjust</code>, <code>kernel</code>, <code>n</code></dt><dd><p>If <code>use.kernel = TRUE</code>, the arguments to the <code><a href="stats.html#topic+density">density()</a></code> function. The defaults are the same as those in <code>density()</code> except that <code>n</code> is 10 times the number of units in the sample.
</p>
</dd>
<dt><code>plot</code></dt><dd><p>If <code>use.kernel = TRUE</code>, whether to plot the estimated density.
</p>
</dd>
</dl>



<h3>Additional Outputs</h3>


<dl>
<dt><code>obj</code></dt><dd>
<p>When <code>include.obj = TRUE</code>, the <code>bart2</code> fit(s) used to generate the predicted values. With multi-category treatments, this will be a list of the fits; otherwise, it will be a single fit. The predicted probabilities used to compute the propensity scores can be extracted using <code><a href="dbarts.html#topic+bart">fitted()</a></code>.
</p>
</dd>
</dl>



<h3>References</h3>

<p>Hill, J., Weiss, C., &amp; Zhai, F. (2011). Challenges With Propensity Score Strategies in a High-Dimensional Setting and a Potential Alternative. Multivariate Behavioral Research, 46(3), 477–513. <a href="https://doi.org/10.1080/00273171.2011.570161">doi:10.1080/00273171.2011.570161</a>
</p>
<p>Chipman, H. A., George, E. I., &amp; McCulloch, R. E. (2010). BART: Bayesian additive regression trees. The Annals of Applied Statistics, 4(1), 266–298. <a href="https://doi.org/10.1214/09-AOAS285">doi:10.1214/09-AOAS285</a>
</p>
<p>Note that many references that deal with BART for causal inference focus on estimating potential outcomes with BART, not the propensity scores, and so are not directly relevant when using BART to estimate propensity scores for weights.
</p>
<p>See <code><a href="#topic+method_glm">method_glm</a></code> for additional references on propensity score weighting more generally.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+weightit">weightit()</a></code>, <code><a href="#topic+weightitMSM">weightitMSM()</a></code>, <code><a href="#topic+get_w_from_ps">get_w_from_ps()</a></code>
</p>
<p><code><a href="#topic+method_super">method_super</a></code> for stacking predictions from several machine learning methods, including BART.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library("cobalt")
data("lalonde", package = "cobalt")

#Balancing covariates between treatment groups (binary)
(W1 &lt;- weightit(treat ~ age + educ + married +
                  nodegree + re74, data = lalonde,
                method = "bart", estimand = "ATT"))
summary(W1)
bal.tab(W1)

#Balancing covariates with respect to race (multi-category)
(W2 &lt;- weightit(race ~ age + educ + married +
                nodegree + re74, data = lalonde,
                method = "bart", estimand = "ATE"))
summary(W2)
bal.tab(W2)

#Balancing covariates with respect to re75 (continuous)
#assuming t(3) conditional density for treatment
(W3 &lt;- weightit(re75 ~ age + educ + married +
                  nodegree + re74, data = lalonde,
                method = "bart", density = "dt_3"))
 summary(W3)
 bal.tab(W3)


</code></pre>

<hr>
<h2 id='method_cbps'>Covariate Balancing Propensity Score Weighting</h2><span id='topic+method_cbps'></span>

<h3>Description</h3>

<p>This page explains the details of estimating weights from covariate balancing propensity scores by setting <code>method = "cbps"</code> in the call to <code><a href="#topic+weightit">weightit()</a></code> or <code><a href="#topic+weightitMSM">weightitMSM()</a></code>. This method can be used with binary, multi-category, and continuous treatments.
</p>
<p>In general, this method relies on estimating propensity scores using generalized method of moments and then converting those propensity scores into weights using a formula that depends on the desired estimand. This method relies on code written for <span class="pkg">WeightIt</span> using <code><a href="stats.html#topic+optim">optim()</a></code>.
</p>


<h4>Binary Treatments</h4>

<p>For binary treatments, this method estimates the propensity scores and weights using <code>optim()</code> using formulas described by Imai and Ratkovic (2014). The following estimands are allowed: ATE, ATT, and ATC.
</p>



<h4>Multi-Category Treatments</h4>

<p>For multi-category treatments, this method estimates the generalized propensity scores and weights using <code>optim()</code> using formulas described by Imai and Ratkovic (2014). The following estimands are allowed: ATE and ATT.
</p>



<h4>Continuous Treatments</h4>

<p>For continuous treatments, this method estimates the generalized propensity scores and weights using <code>optim()</code> using formulas described by Fong, Hazelett, and Imai (2018).
</p>



<h4>Longitudinal Treatments</h4>

<p>For longitudinal treatments, the weights are the product of the weights estimated at each time point. This is not how <code><a href="CBPS.html#topic+CBMSM">CBPS::CBMSM()</a></code> in the <span class="pkg">CBPS</span> package estimates weights for longitudinal treatments.
</p>



<h4>Sampling Weights</h4>

<p>Sampling weights are supported through <code>s.weights</code> in all scenarios.
</p>



<h4>Missing Data</h4>

<p>In the presence of missing data, the following value(s) for <code>missing</code> are allowed:
</p>

<dl>
<dt><code>"ind"</code> (default)</dt><dd>
<p>First, for each variable with missingness, a new missingness indicator variable is created which takes the value 1 if the original covariate is <code>NA</code> and 0 otherwise. The missingness indicators are added to the model formula as main effects. The missing values in the covariates are then replaced with the covariate medians (this value is arbitrary and does not affect estimation). The weight estimation then proceeds with this new formula and set of covariates. The covariates output in the resulting <code>weightit</code> object will be the original covariates with the <code>NA</code>s.
</p>
</dd>
</dl>




<h4>M-estimation</h4>

<p>M-estimation is supported for the just-identified CBPS (the default, setting <code>over = FALSE</code>) for all scenarios. See <code><a href="#topic+glm_weightit">glm_weightit()</a></code> and <code>vignette("estimating-effects")</code> for details.
</p>



<h3>Details</h3>

<p>CBPS estimates the coefficients of a generalized linear model (for binary treatments), multinomial logistic regression model (for multi-category treatments), or linear regression model (for continuous treatments) that is used to compute (generalized) propensity scores, from which the weights are computed. It involves replacing (or augmenting, in the case of the over-identified version) the standard regression score equations with the balance constraints in a generalized method of moments estimation. The idea is to nudge the estimation of the coefficients toward those that produce balance in the weighted sample. The just-identified version (with <code>exact = FALSE</code>) does away with the score equations for the coefficients so that only the balance constraints (and the score equation for the variance of the error with a continuous treatment) are used. The just-identified version will therefore produce superior balance on the means (i.e., corresponding to the balance constraints) for binary and multi-category treatments and linear terms for continuous treatments than will the over-identified version.
</p>
<p>Just-identified CBPS is very similar to entropy balancing and inverse probability tilting. For the ATT, all three methods will yield identical estimates. For other estimands, the results will differ.
</p>
<p>Note that <span class="pkg">WeightIt</span> provides different functionality from the <span class="pkg">CBPS</span> package in terms of the versions of CBPS available; for extensions to CBPS (e.g., optimal CBPS, CBPS for instrumental variables, and jointly estimated CBPS for longitudinal treatments), the <span class="pkg">CBPS</span> package may be preferred.
</p>


<h3>Additional Arguments</h3>

<p>The following additional arguments can be specified:
</p>

<dl>
<dt><code>over</code></dt><dd><p><code>logical</code>; whether to request the over-identified CBPS, which combines the generalized linear model regression score equations (for binary treatments), multinomial logistic regression score equations (for multi-category treatments), or linear regression score equations (for continuous treatments) to the balance moment conditions. Default is <code>FALSE</code> to use the just-identified CBPS.
</p>
</dd>
<dt><code>twostep</code></dt><dd><p><code>logical</code>; when <code>over = TRUE</code>, whether to use the two-step approximation to the generalized method of moments variance. Default is <code>TRUE</code>. Ignored when <code>over = FALSE</code>.
</p>
</dd>
<dt><code>link</code></dt><dd><p><code>"string"</code>; the link used in the generalized linear model for the propensity scores when treatment is binary. Default is <code>"logit"</code> for logistic regression, which is used in the original description of the method by Imai and Ratkovic (2014), but others are allowed: <code>"logit"</code>, <code>"probit"</code>, <code>"cauchit"</code>, and <code>"cloglog"</code> all use the binomial likelihood, <code>"log"</code> uses the Poisson likelihood, and <code>"identity"</code> uses the Gaussian likelihood (i.e., the linear probability model). Note that negative weights are possible with these last two and they should be used with caution. Ignored for multi-category and continuous treatments.
</p>
</dd>
<dt><code>reltol</code></dt><dd><p>the relative tolerance for convergence of the optimization. Passed to the <code>control</code> argument of <code>optim()</code>. Default is <code>sqrt(.Machine$double.eps)</code>.
</p>
</dd>
<dt><code>maxit</code></dt><dd><p>the maximum number of iterations for convergence of the optimization. Passed to the <code>control</code> argument of <code>optim()</code>. Default is 1000.
</p>
</dd>
</dl>



<h3>Additional Outputs</h3>


<dl>
<dt><code>obj</code></dt><dd><p>When <code>include.obj = TRUE</code>, the output of the final call to <code>optim()</code> used to produce the model parameters. Note that because of variable transformations, the resulting parameter estimates may not be interpretable.
</p>
</dd>
</dl>



<h3>Note</h3>

<p>This method used to rely on functionality in the <span class="pkg">CBPS</span> package, but no longer does. Slight differences may be found between the two packages in some cases due to numerical imprecision. <span class="pkg">WeightIt</span> supports arbitrary numbers of groups for the multi-category CBPS and any estimand, whereas <span class="pkg">CBPS</span> only supports up to four groups and only the ATE. For continuous treatments with the over-identified CBPS, <span class="pkg">WeightIt</span> and <span class="pkg">CBPS</span> use different methods of specifying the GMM variance matrix, which may lead to differing results. Note that the default method differs between the two implementations; by default <span class="pkg">WeightIt</span> uses the just-identified CBPS, which is faster to fit, yields better balance, and is compatible with M-estimation for estimating the standard error of the treatment effect, whereas <span class="pkg">CBPS</span> uses the over-identified CBPS by default. However, both the just-identified and over-identified versions are available in both packages.
</p>


<h3>References</h3>



<h4>Binary treatments</h4>

<p>Imai, K., &amp; Ratkovic, M. (2014). Covariate balancing propensity score. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 76(1), 243–263.
</p>



<h4>Multi-Category Treatments</h4>

<p>Imai, K., &amp; Ratkovic, M. (2014). Covariate balancing propensity score. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 76(1), 243–263.
</p>



<h4>Continuous treatments</h4>

<p>Fong, C., Hazlett, C., &amp; Imai, K. (2018). Covariate balancing propensity score for a continuous treatment: Application to the efficacy of political advertisements. The Annals of Applied Statistics, 12(1), 156–177. <a href="https://doi.org/10.1214/17-AOAS1101">doi:10.1214/17-AOAS1101</a>
</p>
<p>Some of the code was inspired by the source code of the <span class="pkg">CBPS</span> package.
</p>



<h3>See Also</h3>

<p><code><a href="#topic+weightit">weightit()</a></code>, <code><a href="#topic+weightitMSM">weightitMSM()</a></code>
</p>
<p><a href="#topic+method_ebal">method_ebal</a> and <a href="#topic+method_ipt">method_ipt</a> for entropy balancing and inverse probability tilting, which work similarly.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("lalonde", package = "cobalt")

#Balancing covariates between treatment groups (binary)
(W1a &lt;- weightit(treat ~ age + educ + married +
                  nodegree + re74, data = lalonde,
                method = "cbps", estimand = "ATT"))
summary(W1a)
cobalt::bal.tab(W1a)

#Balancing covariates between treatment groups (binary)
#using over-identified CBPS with probit link
(W1b &lt;- weightit(treat ~ age + educ + married +
                  nodegree + re74, data = lalonde,
                method = "cbps", estimand = "ATT",
                over = TRUE, link = "probit"))
summary(W1b)
cobalt::bal.tab(W1b)

#Balancing covariates with respect to race (multi-category)
(W2 &lt;- weightit(race ~ age + educ + married +
                  nodegree + re74, data = lalonde,
                method = "cbps", estimand = "ATE"))
summary(W2)
cobalt::bal.tab(W2)

#Balancing covariates with respect to re75 (continuous)
(W3 &lt;- weightit(re75 ~ age + educ + married +
                  nodegree + re74, data = lalonde,
                method = "cbps"))
summary(W3)
cobalt::bal.tab(W3)
</code></pre>

<hr>
<h2 id='method_ebal'>Entropy Balancing</h2><span id='topic+method_ebal'></span>

<h3>Description</h3>

<p>This page explains the details of estimating weights using entropy balancing by setting <code>method = "ebal"</code> in the call to <code><a href="#topic+weightit">weightit()</a></code> or <code><a href="#topic+weightitMSM">weightitMSM()</a></code>. This method can be used with binary, multi-category, and continuous treatments.
</p>
<p>In general, this method relies on estimating weights by minimizing the negative entropy of the weights subject to exact moment balancing constraints. This method relies on code written for <span class="pkg">WeightIt</span> using <code><a href="stats.html#topic+optim">optim()</a></code>.
</p>


<h4>Binary Treatments</h4>

<p>For binary treatments, this method estimates the weights using <code>optim()</code> using formulas described by Hainmueller (2012). The following estimands are allowed: ATE, ATT, and ATC. When the ATE is requested, the optimization is run twice, once for each treatment group.
</p>



<h4>Multi-Category Treatments</h4>

<p>For multi-category treatments, this method estimates the weights using <code>optim()</code>. The following estimands are allowed: ATE and ATT. When the ATE is requested, <code>optim()</code> is run once for each treatment group. When the ATT is requested, <code>optim()</code> is run once for each non-focal (i.e., control) group.
</p>



<h4>Continuous Treatments</h4>

<p>For continuous treatments, this method estimates the weights using <code>optim()</code> using formulas described by Tübbicke (2022) and Vegetabile et al. (2021).
</p>



<h4>Longitudinal Treatments</h4>

<p>For longitudinal treatments, the weights are the product of the weights estimated at each time point. This method is not guaranteed to yield exact balance at each time point. NOTE: the use of entropy balancing with longitudinal treatments has not been validated!
</p>



<h4>Sampling Weights</h4>

<p>Sampling weights are supported through <code>s.weights</code> in all scenarios.
</p>



<h4>Missing Data</h4>

<p>In the presence of missing data, the following value(s) for <code>missing</code> are allowed:
</p>

<dl>
<dt><code>"ind"</code> (default)</dt><dd>
<p>First, for each variable with missingness, a new missingness indicator variable is created which takes the value 1 if the original covariate is <code>NA</code> and 0 otherwise. The missingness indicators are added to the model formula as main effects. The missing values in the covariates are then replaced with the covariate medians (this value is arbitrary and does not affect estimation). The weight estimation then proceeds with this new formula and set of covariates. The covariates output in the resulting <code>weightit</code> object will be the original covariates with the <code>NA</code>s.
</p>
</dd>
</dl>




<h4>M-estimation</h4>

<p>M-estimation is supported for all scenarios. See <code><a href="#topic+glm_weightit">glm_weightit()</a></code> and <code>vignette("estimating-effects")</code> for details.
</p>



<h3>Details</h3>

<p>Entropy balancing involves the specification of an optimization problem, the solution to which is then used to compute the weights. The constraints of the primal optimization problem correspond to covariate balance on the means (for binary and multi-category treatments) or treatment-covariate covariances (for continuous treatments), positivity of the weights, and that the weights sum to a certain value. It turns out that the dual optimization problem is much easier to solve because it is over only as many variables as there are balance constraints rather than over the weights for each unit and it is unconstrained. Zhao and Percival (2017) found that entropy balancing for the ATT of a binary treatment actually involves the estimation of the coefficients of a logistic regression propensity score model but using a specialized loss function different from that optimized with maximum likelihood. Entropy balancing is doubly robust (for the ATT) in the sense that it is consistent either when the true propensity score model is a logistic regression of the treatment on the covariates or when the true outcome model for the control units is a linear regression of the outcome on the covariates, and it attains a semi-parametric efficiency bound when both are true. Entropy balancing will always yield exact mean balance on the included terms.
</p>


<h3>Additional Arguments</h3>

<p><code>moments</code> and <code>int</code> are accepted. See <code><a href="#topic+weightit">weightit()</a></code> for details.
</p>

<dl>
<dt><code>base.weights</code></dt><dd>
<p>A vector of base weights, one for each unit. These correspond to the base weights $q$ in Hainmueller (2012). The estimated weights minimize the Kullback entropy divergence from the base weights, defined as <code class="reqn">\sum w \log(w/q)</code>, subject to exact balance constraints. These can be used to supply previously estimated weights so that the newly estimated weights retain the some of the properties of the original weights while ensuring the balance constraints are met. Sampling weights should not be passed to <code>base.weights</code> but can be included in a <code>weightit()</code> call that includes <code>s.weights</code>.
</p>
</dd>
<dt><code>quantile</code></dt><dd>
<p>A named list of quantiles (values between 0 and 1) for each continuous covariate, which are used to create additional variables that when balanced ensure balance on the corresponding quantile of the variable. For example, setting <code style="white-space: pre;">&#8288;quantile = list(x1 = c(.25, .5. , .75))&#8288;</code> ensures the 25th, 50th, and 75th percentiles of <code>x1</code> in each treatment group will be balanced in the weighted sample. Can also be a single number (e.g., <code>.5</code>) or an unnamed list of length 1 (e.g., <code>list(c(.25, .5, .75))</code>) to request the same quantile(s) for all continuous covariates, or a named vector (e.g., <code style="white-space: pre;">&#8288;c(x1 = .5, x2 = .75&#8288;</code>) to request one quantile for each covariate. Only allowed with binary and multi-category treatments.
</p>
</dd>
<dt><code>d.moments</code></dt><dd>
<p>With continuous treatments, the number of moments of the treatment and covariate distributions that are constrained to be the same in the weighted sample as in the original sample. For example, setting <code>d.moments = 3</code> ensures that the mean, variance, and skew of the treatment and covariates are the same in the weighted sample as in the unweighted sample. <code>d.moments</code> should be greater than or equal to <code>moments</code> and will be automatically set accordingly if not (or if not specified). Vegetabile et al. (2021) recommend setting <code>d.moments = 3</code>, even if <code>moments</code> is less than 3. This argument corresponds to the tuning parameters $r$ and $s$ in Vegetabile et al. (2021) (which here are set to be equal). Ignored for binary and multi-category treatments.
</p>
</dd>
</dl>

<p>The arguments <code>maxit</code> and <code>reltol</code> can be supplied and are passed to the <code>control</code> argument of <code><a href="stats.html#topic+optim">optim()</a></code>. The <code>"BFGS"</code> method is used, so the defaults correspond to this.
</p>
<p>The <code>stabilize</code> argument is ignored; in the past it would reduce the variability of the weights through an iterative process. If you want to minimize the variance of the weights subject to balance constraints, use <code>method = "optweight"</code>.
</p>


<h3>Additional Outputs</h3>


<dl>
<dt><code>obj</code></dt><dd><p>When <code>include.obj = TRUE</code>, the output of the call to <code><a href="stats.html#topic+optim">optim()</a></code>, which contains the dual variables and convergence information. For ATE fits or with multi-category treatments, a list of <code>optim()</code> outputs, one for each weighted group.
</p>
</dd>
</dl>



<h3>References</h3>



<h4>Binary Treatments</h4>

<p>Hainmueller, J. (2012). Entropy Balancing for Causal Effects: A Multivariate Reweighting Method to Produce Balanced Samples in Observational Studies. Political Analysis, 20(1), 25–46. <a href="https://doi.org/10.1093/pan/mpr025">doi:10.1093/pan/mpr025</a>
</p>
<p>Källberg, D., &amp; Waernbaum, I. (2022). Large Sample Properties of Entropy Balancing Estimators of Average Causal Effects. ArXiv:2204.10623 <a href="ggplot2.html#topic+Stat">Stat</a>. <a href="https://arxiv.org/abs/2204.10623">https://arxiv.org/abs/2204.10623</a>
</p>
<p>Zhao, Q., &amp; Percival, D. (2017). Entropy balancing is doubly robust. Journal of Causal Inference, 5(1). <a href="https://doi.org/10.1515/jci-2016-0010">doi:10.1515/jci-2016-0010</a>
</p>



<h4>Continuous Treatments</h4>

<p>Tübbicke, S. (2022). Entropy Balancing for Continuous Treatments. Journal of Econometric Methods, 11(1), 71–89. <a href="https://doi.org/10.1515/jem-2021-0002">doi:10.1515/jem-2021-0002</a>
</p>
<p>Vegetabile, B. G., Griffin, B. A., Coffman, D. L., Cefalu, M., Robbins, M. W., &amp; McCaffrey, D. F. (2021). Nonparametric estimation of population average dose-response curves using entropy balancing weights for continuous exposures. Health Services and Outcomes Research Methodology, 21(1), 69–110. <a href="https://doi.org/10.1007/s10742-020-00236-2">doi:10.1007/s10742-020-00236-2</a>
</p>



<h3>See Also</h3>

<p><code><a href="#topic+weightit">weightit()</a></code>, <code><a href="#topic+weightitMSM">weightitMSM()</a></code>
</p>
<p><a href="#topic+method_ipt">method_ipt</a> and <a href="#topic+method_cbps">method_cbps</a> for inverse probability tilting and CBPS, which work similarly.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("lalonde", package = "cobalt")

#Balancing covariates between treatment groups (binary)
(W1 &lt;- weightit(treat ~ age + educ + married +
                  nodegree + re74, data = lalonde,
                method = "ebal", estimand = "ATT"))
summary(W1)
cobalt::bal.tab(W1)

#Balancing covariates with respect to race (multi-category)
(W2 &lt;- weightit(race ~ age + educ + married +
                  nodegree + re74, data = lalonde,
                method = "ebal", estimand = "ATE"))
summary(W2)
cobalt::bal.tab(W2)

#Balancing covariates and squares with respect to
#re75 (continuous), maintaining 3 moments of the
#covariate and treatment distributions
(W3 &lt;- weightit(re75 ~ age + educ + married +
                  nodegree + re74, data = lalonde,
                method = "ebal", moments = 2,
                d.moments = 3))
summary(W3)
cobalt::bal.tab(W3)
</code></pre>

<hr>
<h2 id='method_energy'>Energy Balancing</h2><span id='topic+method_energy'></span>

<h3>Description</h3>

<p>This page explains the details of estimating weights using energy balancing by setting <code>method = "energy"</code> in the call to <code><a href="#topic+weightit">weightit()</a></code> or <code><a href="#topic+weightitMSM">weightitMSM()</a></code>. This method can be used with binary, multi-category, and continuous treatments.
</p>
<p>In general, this method relies on estimating weights by minimizing an energy statistic related to covariate balance. For binary and multi-category treatments, this is the energy distance, which is a multivariate distance between distributions, between treatment groups. For continuous treatments, this is the sum of the distance covariance between the treatment variable and the covariates and the energy distances between the treatment and covariates in the weighted sample and their distributions in the original sample. This method relies on code written for <span class="pkg">WeightIt</span> using <code><a href="osqp.html#topic+osqp">osqp::osqp()</a></code> from the <a href="https://CRAN.R-project.org/package=osqp"><span class="pkg">osqp</span></a> package to perform the optimization. This method may be slow or memory-intensive for large datasets.
</p>


<h4>Binary Treatments</h4>

<p>For binary treatments, this method estimates the weights using <code>osqp()</code> using formulas described by Huling and Mak (2022). The following estimands are allowed: ATE, ATT, and ATC.
</p>



<h4>Multi-Category Treatments</h4>

<p>For multi-category treatments, this method estimates the weights using <code>osqp()</code> using formulas described by Huling and Mak (2022). The following estimands are allowed: ATE and ATT.
</p>



<h4>Continuous Treatments</h4>

<p>For continuous treatments, this method estimates the weights using <code>osqp()</code> using formulas described by Huling, Greifer, and Chen (2023).
</p>



<h4>Longitudinal Treatments</h4>

<p>For longitudinal treatments, the weights are the product of the weights estimated at each time point. This method is not guaranteed to yield optimal balance at each time point. NOTE: the use of energy balancing with longitudinal treatments has not been validated!
</p>



<h4>Sampling Weights</h4>

<p>Sampling weights are supported through <code>s.weights</code> in all scenarios. In some cases, sampling weights will cause the optimization to fail due to lack of convexity or infeasible constraints.
</p>



<h4>Missing Data</h4>

<p>In the presence of missing data, the following value(s) for <code>missing</code> are allowed:
</p>

<dl>
<dt><code>"ind"</code> (default)</dt><dd>
<p>First, for each variable with missingness, a new missingness indicator variable is created which takes the value 1 if the original covariate is <code>NA</code> and 0 otherwise. The missingness indicators are added to the model formula as main effects. The missing values in the covariates are then replaced with the covariate medians (this value is arbitrary and does not affect estimation). The weight estimation then proceeds with this new formula and set of covariates. The covariates output in the resulting <code>weightit</code> object will be the original covariates with the <code>NA</code>s.
</p>
</dd>
</dl>




<h4>M-estimation</h4>

<p>M-estimation is not supported.
</p>



<h3>Details</h3>

<p>Energy balancing is a method of estimating weights using optimization without a propensity score. The weights are the solution to a constrain quadratic optimization problem where the objective function concerns covariate balance as measured by the energy distance and (for continuous treatments) the distance covariance.
</p>
<p>Energy balancing for binary and multi-category treatments involves minimizing the energy distance between the treatment groups and between each treatment group and a target group (e.g., the full sample for the ATE). The energy distance is a scalar measure of the difference between two multivariate distributions and is equal to 0 when the two distributions are identical.
</p>
<p>Energy balancing for continuous treatments involves minimizing the distance covariance between the treatment and the covariates; the distance covariance is a scalar measure of the association between two (possibly multivariate) distributions that is equal to 0 when the two distributions are independent. In addition, the energy distances between the treatment and covariate distributions in the weighted sample and the treatment and covariate distributions in the original sample are minimized.
</p>
<p>The primary benefit of energy balancing is that all features of the covariate distribution are balanced, not just means, as with other optimization-based methods like entropy balancing. Still, it is possible to add additional balance constraints to require balance on individual terms using the <code>moments</code> argument, just like with entropy balancing. Energy balancing can sometimes yield weights with high variability; the <code>lambda</code> argument can be supplied to penalize highly variable weights to increase the effective sample size at the expense of balance.
</p>


<h3>Additional Arguments</h3>

<p>The following following additional arguments can be specified:
</p>

<dl>
<dt><code>dist.mat</code></dt><dd><p>the name of the method used to compute the distance matrix of the covariates or the numeric distance matrix itself. Allowable options include <code>"scaled_euclidean"</code> for the Euclidean (L2) distance on the scaled covariates (the default), <code>"mahalanobis"</code> for the Mahalanobis distance, and <code>"euclidean"</code> for the raw Euclidean distance. Abbreviations allowed. Note that some user-supplied distance matrices can cause the R session to abort due to a bug within <span class="pkg">osqp</span>, so this argument should be used with caution. A distance matrix must be a square, symmetric, numeric matrix with zeros along the diagonal and a row and column for each unit. Can also be supplied as the output of a call to <code><a href="stats.html#topic+dist">dist()</a></code>.
</p>
</dd>
<dt><code>lambda</code></dt><dd><p>a positive numeric scalar used to penalize the square of the weights. This value divided by the square of the total sample size is added to the diagonal of the quadratic part of the loss function. Higher values favor weights with less variability. Note this is distinct from the lambda value described in Huling and Mak (2022), which penalizes the complexity of individual treatment rules rather than the weights, but does correspond to lambda from Huling et al. (2021). Default is .0001, which is essentially 0.
</p>
</dd>
</dl>

<p>For binary and multi-category treatments, the following additional arguments can be specified:
</p>

<dl>
<dt><code>improved</code></dt><dd><p><code>logical</code>; whether to use the improved energy balancing weights as described by Huling and Mak (2022) when <code>estimand = "ATE"</code>. This involves optimizing balance not only between each treatment group and the overall sample, but also between each pair of treatment groups. Huling and Mak (2022) found that the improved energy balancing weights generally outperformed standard energy balancing. Default is <code>TRUE</code>; set to <code>FALSE</code> to use the standard energy balancing weights instead (not recommended).
</p>
</dd>
<dt><code>quantile</code></dt><dd>
<p>A named list of quantiles (values between 0 and 1) for each continuous covariate, which are used to create additional variables that when balanced ensure balance on the corresponding quantile of the variable. For example, setting <code style="white-space: pre;">&#8288;quantile = list(x1 = c(.25, .5. , .75))&#8288;</code> ensures the 25th, 50th, and 75th percentiles of <code>x1</code> in each treatment group will be balanced in the weighted sample. Can also be a single number (e.g., <code>.5</code>) or an unnamed list of length 1 (e.g., <code>list(c(.25, .5, .75))</code>) to request the same quantile(s) for all continuous covariates, or a named vector (e.g., <code style="white-space: pre;">&#8288;c(x1 = .5, x2 = .75&#8288;</code>) to request one quantile for each covariate.
</p>
</dd>
</dl>

<p>For continuous treatments, the following additional arguments can be specified:
</p>

<dl>
<dt><code>d.moments</code></dt><dd>
<p>The number of moments of the treatment and covariate distributions that are constrained to be the same in the weighted sample as in the original sample. For example, setting <code>d.moments = 3</code> ensures that the mean, variance, and skew of the treatment and covariates are the same in the weighted sample as in the unweighted sample. <code>d.moments</code> should be greater than or equal to <code>moments</code> and will be automatically set accordingly if not (or if not specified).
</p>
</dd>
<dt><code>dimension.adj</code></dt><dd>
<p><code>logical</code>; whether to include the dimensionality adjustment described by Huling et al. (2021). If <code>TRUE</code>, the default, the energy distance for the covariates is weighted <code class="reqn">\sqrt{p}</code> times as much as the energy distance for the treatment, where <code class="reqn">p</code> is the number of covariates. If <code>FALSE</code>, the two energy distances are given equal weights. Default is <code>TRUE</code>.
</p>
</dd>
</dl>

<p>The <code>moments</code> argument functions differently for <code>method = "energy"</code> from how it does with other methods. When unspecified or set to zero, energy balancing weights are estimated as described by Huling and Mak (2022) for binary and multi-category treatments or by Huling et al. (2023) for continuous treatments. When <code>moments</code> is set to an integer larger than 0, additional balance constraints on the requested moments of the covariates are also included, guaranteeing exact moment balance on these covariates while minimizing the energy distance of the weighted sample. For binary and multi-category treatments, this involves exact balance on the means of the entered covariates; for continuous treatments, this involves exact balance on the treatment-covariate correlations of the entered covariates.
</p>


<h3>Additional Outputs</h3>


<dl>
<dt><code>obj</code></dt><dd><p>When <code>include.obj = TRUE</code>, the output of the call to <code><a href="osqp.html#topic+solve_osqp">osqp::solve_osqp()</a></code>, which contains the dual variables and convergence information.
</p>
</dd>
</dl>



<h3>Note</h3>

<p>Sometimes the optimization can fail to converge because the problem is not convex. A warning will be displayed if so. In these cases, try simply re-fitting the weights without changing anything. If the method repeatedly fails, you should try another method or change the supplied parameters (though this is uncommon). Increasing <code>max_iter</code> might help.
</p>
<p>If it seems like the weights are balancing the covariates but you still get a failure to converge, this usually indicates that more iterations are needs to find the optimal solutions. This can occur when <code>moments</code> or <code>int</code> are specified. <code>max_iter</code> should be increased, and setting <code>verbose = TRUE</code> allows you to monitor the process and examine if the optimization is approaching convergence.
</p>


<h3>Author(s)</h3>

<p>Noah Greifer, using code from Jared Huling's <a href="https://CRAN.R-project.org/package=independenceWeights"><span class="pkg">independenceWeights</span></a> package for continuous treatments.
</p>


<h3>References</h3>



<h4>Binary and multi-category treatments</h4>

<p>Huling, J. D., &amp; Mak, S. (2022). Energy Balancing of Covariate Distributions (arXiv:2004.13962). arXiv. <a href="https://doi.org/10.48550/arXiv.2004.13962">doi:10.48550/arXiv.2004.13962</a>
</p>



<h4>Continuous treatments</h4>

<p>Huling, J. D., Greifer, N., &amp; Chen, G. (2023). Independence weights for causal inference with continuous treatments. Journal of the American Statistical Association, 0(ja), 1–25. <a href="https://doi.org/10.1080/01621459.2023.2213485">doi:10.1080/01621459.2023.2213485</a>
</p>



<h3>See Also</h3>

<p><code><a href="#topic+weightit">weightit()</a></code>, <code><a href="#topic+weightitMSM">weightitMSM()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library("cobalt")
data("lalonde", package = "cobalt")

#Examples may not converge, but may after several runs
## Not run: 
  #Balancing covariates between treatment groups (binary)
  (W1 &lt;- weightit(treat ~ age + educ + married +
                    nodegree + re74, data = lalonde,
                  method = "energy", estimand = "ATE"))
  summary(W1)
  bal.tab(W1)

  #Balancing covariates with respect to race (multi-category)
  (W2 &lt;- weightit(race ~ age + educ + married +
                    nodegree + re74, data = lalonde,
                  method = "energy", estimand = "ATT",
                  focal = "black"))
  summary(W2)
  bal.tab(W2)

  #Balancing covariates with respect to re75 (continuous)
  (W3 &lt;- weightit(re75 ~ age + educ + married +
                    nodegree + re74, data = lalonde,
                  method = "energy", moments = 1))
  summary(W3)
  bal.tab(W3, poly = 2)

## End(Not run)

</code></pre>

<hr>
<h2 id='method_gbm'>Propensity Score Weighting Using Generalized Boosted Models</h2><span id='topic+method_gbm'></span>

<h3>Description</h3>

<p>This page explains the details of estimating weights from generalized boosted model-based propensity scores by setting <code>method = "gbm"</code> in the call to <code><a href="#topic+weightit">weightit()</a></code> or <code><a href="#topic+weightitMSM">weightitMSM()</a></code>. This method can be used with binary, multi-category, and continuous treatments.
</p>
<p>In general, this method relies on estimating propensity scores using generalized boosted modeling and then converting those propensity scores into weights using a formula that depends on the desired estimand. The algorithm involves using a balance-based or prediction-based criterion to optimize in choosing the value of tuning parameters (the number of trees and possibly others). The method relies on the <a href="https://CRAN.R-project.org/package=gbm"><span class="pkg">gbm</span></a> package.
</p>
<p>This method mimics the functionality of functions in the <span class="pkg">twang</span> package, but has improved performance and more flexible options. See Details section for more details.
</p>


<h4>Binary Treatments</h4>

<p>For binary treatments, this method estimates the propensity scores using <code><a href="gbm.html#topic+gbm.fit">gbm::gbm.fit()</a></code> and then selects the optimal tuning parameter values using the method specified in the <code>criterion</code> argument. The following estimands are allowed: ATE, ATT, ATC, ATO, and ATM. The weights are computed from the estimated propensity scores using <code><a href="#topic+get_w_from_ps">get_w_from_ps()</a></code>, which implements the standard formulas. Weights can also be computed using marginal mean weighting through stratification for the ATE, ATT, and ATC. See <code><a href="#topic+get_w_from_ps">get_w_from_ps()</a></code> for details.
</p>



<h4>Multi-Category Treatments</h4>

<p>For binary treatments, this method estimates the propensity scores using <code><a href="gbm.html#topic+gbm.fit">gbm::gbm.fit()</a></code> and then selects the optimal tuning parameter values using the method specified in the <code>criterion</code> argument. The following estimands are allowed: ATE, ATT, ATC, ATO, and ATM. The weights are computed from the estimated propensity scores using <code><a href="#topic+get_w_from_ps">get_w_from_ps()</a></code>, which implements the standard formulas. Weights can also be computed using marginal mean weighting through stratification for the ATE, ATT, and ATC. See <code><a href="#topic+get_w_from_ps">get_w_from_ps()</a></code> for details.
</p>



<h4>Continuous Treatments</h4>

<p>For continuous treatments, this method estimates the generalized propensity score using <code><a href="gbm.html#topic+gbm.fit">gbm::gbm.fit()</a></code> and then selects the optimal tuning parameter values using the method specified in the <code>criterion</code> argument.
</p>



<h4>Longitudinal Treatments</h4>

<p>For longitudinal treatments, the weights are the product of the weights estimated at each time point.
</p>



<h4>Sampling Weights</h4>

<p>Sampling weights are supported through <code>s.weights</code> in all scenarios.
</p>



<h4>Missing Data</h4>

<p>In the presence of missing data, the following value(s) for <code>missing</code> are allowed:
</p>

<dl>
<dt><code>"ind"</code> (default)</dt><dd><p>First, for each variable with missingness, a new missingness indicator variable is created which takes the value 1 if the original covariate is <code>NA</code> and 0 otherwise. The missingness indicators are added to the model formula as main effects. The missing values in the covariates are then replaced with the covariate medians (this value is arbitrary and does not affect estimation). The weight estimation then proceeds with this new formula and set of covariates. The covariates output in the resulting <code>weightit</code> object will be the original covariates with the <code>NA</code>s.</p>
</dd>
<dt><code>"surr"</code></dt><dd><p>Surrogate splitting is used to process <code>NA</code>s. No missingness indicators are created. Nodes are split using only the non-missing values of each variable. To generate predicted values for each unit, a non-missing variable that operates similarly to the variable with missingness is used as a surrogate. Missing values are ignored when calculating balance statistics to choose the optimal tree.</p>
</dd>
</dl>




<h4>M-estimation</h4>

<p>M-estimation is not supported.
</p>



<h3>Details</h3>

<p>Generalized boosted modeling (GBM, also known as gradient boosting machines) is a machine learning method that generates predicted values from a flexible regression of the treatment on the covariates, which are treated as propensity scores and used to compute weights. It does this by building a series of regression trees, each fit to the residuals of the last, minimizing a loss function that depends on the distribution chosen. The optimal number of trees is a tuning parameter that must be chosen; McCaffrey et al. (2004) were innovative in using covariate balance to select this value rather than traditional machine learning performance metrics such as cross-validation accuracy. GBM is particularly effective for fitting nonlinear treatment models characterized by curves and interactions, but performs worse for simpler treatment models. It is unclear which balance measure should be used to select the number of trees, though research has indicated that balance measures tend to perform better than cross-validation accuracy for estimating effective propensity score weights.
</p>
<p><span class="pkg">WeightIt</span> offers almost identical functionality to <span class="pkg">twang</span>, the first package to implement this method. Compared to the current version of <span class="pkg">twang</span>, <span class="pkg">WeightIt</span> offers more options for the measure of balance used to select the number of trees, improved performance, tuning of hyperparameters, more estimands, and support for continuous treatments. <span class="pkg">WeightIt</span> computes weights for multi-category treatments differently from how <span class="pkg">twang</span> does; rather than fitting a separate binary GBM for each pair of treatments, <span class="pkg">WeightIt</span> fits a single multi-class GBM model and uses balance measures appropriate for multi-category treatments.
</p>


<h3>Additional Arguments</h3>

<p>The following additional arguments can be specified:
</p>

<dl>
<dt><code>criterion</code></dt><dd><p>A string describing the balance criterion used to select the best weights. See <code><a href="cobalt.html#topic+bal.compute">cobalt::bal.compute()</a></code> for allowable options for each treatment type. In addition, to optimize the cross-validation error instead of balance, <code>criterion</code> can be set as <code style="white-space: pre;">&#8288;"cv{#}&#8288;</code>&quot;, where <code style="white-space: pre;">&#8288;{#}&#8288;</code> is replaced by a number representing the number of cross-validation folds used (e.g., <code>"cv5"</code> for 5-fold cross-validation). For binary and multi-category treatments, the default is <code>"smd.mean"</code>, which minimizes the average absolute standard mean difference among the covariates between treatment groups. For continuous treatments, the default is <code>"p.mean"</code>, which minimizes the average absolute Pearson correlation between the treatment and covariates.
</p>
</dd>
<dt><code>trim.at</code></dt><dd><p>A number supplied to <code>at</code> in <code><a href="#topic+trim">trim()</a></code> which trims the weights from all the trees before choosing the best tree. This can be valuable when some weights are extreme, which occurs especially with continuous treatments. The default is 0 (i.e., no trimming).
</p>
</dd>
<dt><code>distribution</code></dt><dd><p>A string with the distribution used in the loss function of the boosted model. This is supplied to the <code>distribution</code> argument in <code><a href="gbm.html#topic+gbm.fit">gbm::gbm.fit()</a></code>. For binary treatments, <code>"bernoulli"</code> and <code>"adaboost"</code> are available, with <code>"bernoulli"</code> the default. For multi-category treatments, only <code>"multinomial"</code> is allowed. For continuous treatments <code>"gaussian"</code>, <code>"laplace"</code>, and <code>"tdist"</code> are available, with <code>"gaussian"</code> the default. This argument is tunable.
</p>
</dd>
<dt><code>n.trees</code></dt><dd><p>The maximum number of trees used. This is passed onto the <code>n.trees</code> argument in <code>gbm.fit()</code>. The default is 10000 for binary and multi-category treatments and 20000 for continuous treatments.
</p>
</dd>
<dt><code>start.tree</code></dt><dd><p>The tree at which to start balance checking. If you know the best balance isn't in the first 100 trees, for example, you can set <code>start.tree = 101</code> so that balance statistics are not computed on the first 100 trees. This can save some time since balance checking takes up the bulk of the run time for some balance-based stopping methods, and is especially useful when running the same model adding more and more trees. The default is 1, i.e., to start from the very first tree in assessing balance.
</p>
</dd>
<dt><code>interaction.depth</code></dt><dd><p>The depth of the trees. This is passed onto the <code>interaction.depth</code> argument in <code>gbm.fit()</code>. Higher values indicate better ability to capture nonlinear and nonadditive relationships. The default is 3 for binary and multi-category treatments and 4 for continuous treatments. This argument is tunable.
</p>
</dd>
<dt><code>shrinkage</code></dt><dd><p>The shrinkage parameter applied to the trees. This is passed onto the <code>shrinkage</code> argument in <code>gbm.fit()</code>. The default is .01 for binary and multi-category treatments and .0005 for continuous treatments. The lower this value is, the more trees one may have to include to reach the optimum. This argument is tunable.
</p>
</dd>
<dt><code>bag.fraction</code></dt><dd><p>The fraction of the units randomly selected to propose the next tree in the expansion. This is passed onto the <code>bag.fraction</code> argument in <code>gbm.fit()</code>. The default is 1, but smaller values should be tried. For values less then 1, subsequent runs with the same parameters will yield different results due to random sampling; be sure to seed the seed using <code><a href="base.html#topic+set.seed">set.seed()</a></code> to ensure replicability of results.
</p>
</dd>
</dl>

<p>All other arguments take on the defaults of those in <code><a href="gbm.html#topic+gbm.fit">gbm::gbm.fit()</a></code>, and some are not used at all.
</p>
<p>The <code>w</code> argument in <code>gbm.fit()</code> is ignored because sampling weights are passed using <code>s.weights</code>.
</p>
<p>For continuous treatments only, the following arguments may be supplied:
</p>

<dl>
<dt><code>density</code></dt><dd><p>A function corresponding to the conditional density of the treatment. The standardized residuals of the treatment model will be fed through this function to produce the numerator and denominator of the generalized propensity score weights. If blank, <code><a href="stats.html#topic+dnorm">dnorm()</a></code> is used as recommended by Robins et al. (2000). This can also be supplied as a string containing the name of the function to be called. If the string contains underscores, the call will be split by the underscores and the latter splits will be supplied as arguments to the second argument and beyond. For example, if <code>density = "dt_2"</code> is specified, the density used will be that of a t-distribution with 2 degrees of freedom. Using a t-distribution can be useful when extreme outcome values are observed (Naimi et al., 2014). Ignored if <code>use.kernel = TRUE</code> (described below).
</p>
</dd>
<dt><code>use.kernel</code></dt><dd><p>If <code>TRUE</code>, uses kernel density estimation through the <code><a href="stats.html#topic+density">density()</a></code> function to estimate the numerator and denominator densities for the weights. If <code>FALSE</code> (the default), the argument to the <code>density</code> parameter is used instead.
</p>
</dd>
<dt><code>bw</code>, <code>adjust</code>, <code>kernel</code>, <code>n</code></dt><dd><p>If <code>use.kernel = TRUE</code>, the arguments to <code><a href="stats.html#topic+density">density()</a></code>. The defaults are the same as those in <code>density</code> except that <code>n</code> is 10 times the number of units in the sample.
</p>
</dd>
<dt><code>plot</code></dt><dd><p>If <code>use.kernel = TRUE</code> with continuous treatments, whether to plot the estimated density.
</p>
</dd>
</dl>

<p>For tunable arguments, multiple entries may be supplied, and <code>weightit()</code> will choose the best value by optimizing the criterion specified in <code>criterion</code>. See below for additional outputs that are included when arguments are supplied to be tuned. See Examples for an example of tuning.
</p>


<h3>Additional Outputs</h3>


<dl>
<dt><code>info</code></dt><dd>
<p>A list with the following entries:
</p>

<dl>
<dt><code>best.tree</code></dt><dd>
<p>The number of trees at the optimum. If this is close to <code>n.trees</code>, <code>weightit()</code> should be rerun with a larger value for <code>n.trees</code>, and <code>start.tree</code> can be set to just below <code>best.tree</code>. When other parameters are tuned, this is the best tree value in the best combination of tuned parameters. See example.
</p>
</dd>
<dt><code>tree.val</code></dt><dd>
<p>A data frame with two columns: the first is the number of trees and the second is the value of the criterion corresponding to that tree. Running <code><a href="base.html#topic+plot">plot()</a></code> on this object will plot the criterion by the number of trees and is a good way to see patterns in the relationship between them and to determine if more trees are needed. When other parameters are tuned, these are the number of trees and the criterion values in the best combination of tuned parameters. See example.
</p>
</dd>
</dl>

<p>If any arguments are to be tuned (i.e., they have been supplied more than one value), the following two additional components are included in <code>info</code>:
</p>

<dl>
<dt><code>tune</code></dt><dd>
<p>A data frame with a column for each argument being tuned, the best value of the balance criterion for the given combination of parameters, and the number of trees at which the best value was reached.
</p>
</dd>
<dt><code>best.tune</code></dt><dd>
<p>A one-row data frame containing the values of the arguments being tuned that were ultimately selected to estimate the returned weights.
</p>
</dd>
</dl>

</dd>
<dt><code>obj</code></dt><dd>
<p>When <code>include.obj = TRUE</code>, the <code>gbm</code> fit used to generate the predicted values.
</p>
</dd>
</dl>



<h3>Note</h3>

<p>The <code>criterion</code> argument used to be called <code>stop.method</code>, which is its name in <span class="pkg">twang</span>. <code>stop.method</code> still works for backward compatibility. Additionally, the criteria formerly named as <code>es.mean</code>, <code>es.max</code>, and <code>es.rms</code> have been renamed to <code>smd.mean</code>, <code>smd.max</code>, and <code>smd.rms</code>. The former are used in <span class="pkg">twang</span> and will still work with <code>weightit()</code> for backward compatibility.
</p>


<h3>References</h3>



<h4>Binary treatments</h4>

<p>McCaffrey, D. F., Ridgeway, G., &amp; Morral, A. R. (2004). Propensity Score Estimation With Boosted Regression for Evaluating Causal Effects in Observational Studies. Psychological Methods, 9(4), 403–425. <a href="https://doi.org/10.1037/1082-989X.9.4.403">doi:10.1037/1082-989X.9.4.403</a>
</p>



<h4>Multi-Category Treatments</h4>

<p>McCaffrey, D. F., Griffin, B. A., Almirall, D., Slaughter, M. E., Ramchand, R., &amp; Burgette, L. F. (2013). A Tutorial on Propensity Score Estimation for Multiple Treatments Using Generalized Boosted Models. Statistics in Medicine, 32(19), 3388–3414. <a href="https://doi.org/10.1002/sim.5753">doi:10.1002/sim.5753</a>
</p>



<h4>Continuous treatments</h4>

<p>Zhu, Y., Coffman, D. L., &amp; Ghosh, D. (2015). A Boosting Algorithm for Estimating Generalized Propensity Scores with Continuous Treatments. Journal of Causal Inference, 3(1). <a href="https://doi.org/10.1515/jci-2014-0022">doi:10.1515/jci-2014-0022</a>
</p>



<h3>See Also</h3>

<p><code><a href="#topic+weightit">weightit()</a></code>, <code><a href="#topic+weightitMSM">weightitMSM()</a></code>
</p>
<p><code><a href="gbm.html#topic+gbm.fit">gbm::gbm.fit()</a></code> for the fitting function.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library("cobalt")
data("lalonde", package = "cobalt")

#Balancing covariates between treatment groups (binary)
(W1 &lt;- weightit(treat ~ age + educ + married +
                  nodegree + re74, data = lalonde,
                method = "gbm", estimand = "ATE",
                criterion = "smd.max"))
summary(W1)
bal.tab(W1)

## Not run: 
  #Balancing covariates with respect to race (multi-category)
  (W2 &lt;- weightit(race ~ age + educ + married +
                    nodegree + re74, data = lalonde,
                  method = "gbm", estimand = "ATT",
                  focal = "hispan", criterion = "ks.mean"))
  summary(W2)
  bal.tab(W2, stats = c("m", "ks"))

  #Balancing covariates with respect to re75 (continuous)
  (W3 &lt;- weightit(re75 ~ age + educ + married +
                    nodegree + re74, data = lalonde,
                  method = "gbm", use.kernel = TRUE,
                  criterion = "p.rms", trim.at = .97))
  summary(W3)
  bal.tab(W3)

  #Using a t(3) density and illustrating the search for
  #more trees.
  W4a &lt;- weightit(re75 ~ age + educ + married +
                    nodegree + re74, data = lalonde,
                  method = "gbm", density = "dt_3",
                  criterion = "p.max",
                  n.trees = 10000)

  W4a$info$best.tree #10000; optimum hasn't been found
  plot(W4a$info$tree.val, type = "l") #decreasing at right edge

  W4b &lt;- weightit(re75 ~ age + educ + married +
                    nodegree + re74, data = lalonde,
                  method = "gbm", density = "dt_3",
                  criterion = "p.max",
                  start.tree = 10000,
                  n.trees = 20000)

  W4b$info$best.tree #13417; optimum has been found
  plot(W4b$info$tree.val, type = "l") #increasing at right edge

  bal.tab(W4b)

  #Tuning hyperparameters
  (W5 &lt;- weightit(treat ~ age + educ + married +
                    nodegree + re74, data = lalonde,
                  method = "gbm", estimand = "ATT",
                  criterion = "ks.max",
                  interaction.depth = 2:4,
                  distribution = c("bernoulli", "adaboost")))

  W5$info$tune

  W5$info$best.tune #Best values of tuned parameters

  bal.tab(W5, stats = c("m", "ks"))

## End(Not run)

</code></pre>

<hr>
<h2 id='method_glm'>Propensity Score Weighting Using Generalized Linear Models</h2><span id='topic+method_glm'></span>

<h3>Description</h3>

<p>This page explains the details of estimating weights from generalized linear model-based propensity scores by setting <code>method = "glm"</code> in the call to <code><a href="#topic+weightit">weightit()</a></code> or <code><a href="#topic+weightitMSM">weightitMSM()</a></code>. This method can be used with binary, multi-category, and continuous treatments.
</p>
<p>In general, this method relies on estimating propensity scores with a parametric generalized linear model and then converting those propensity scores into weights using a formula that depends on the desired estimand. For binary and multi-category treatments, a binomial or multinomial regression model is used to estimate the propensity scores as the predicted probability of being in each treatment given the covariates. For ordinal treatments, an ordinal regression model is used to estimate generalized propensity scores. For continuous treatments, a generalized linear model is used to estimate generalized propensity scores as the conditional density of treatment given the covariates.
</p>


<h4>Binary Treatments</h4>

<p>For binary treatments, this method estimates the propensity scores using <code><a href="stats.html#topic+glm">glm()</a></code>. An additional argument is <code>link</code>, which uses the same options as <code>link</code> in <code><a href="stats.html#topic+family">family()</a></code>. The default link is &quot;logit&quot;, but others, including &quot;probit&quot;, are allowed. The following estimands are allowed: ATE, ATT, ATC, ATO, ATM, and ATOS. Weights can also be computed using marginal mean weighting through stratification for the ATE, ATT, and ATC. See <code><a href="#topic+get_w_from_ps">get_w_from_ps()</a></code> for details.
</p>



<h4>Multi-Category Treatments</h4>

<p>For multi-category treatments, the propensity scores are estimated using multinomial regression from one of a few functions depending on the argument supplied to <code>multi.method</code> (see Additional Arguments below). The following estimands are allowed: ATE, ATT, ATC, ATO, and ATM. The weights for each estimand are computed using the standard formulas or those mentioned above. Weights can also be computed using marginal mean weighting through stratification for the ATE, ATT, and ATC. See <code><a href="#topic+get_w_from_ps">get_w_from_ps()</a></code> for details.
</p>



<h4>Continuous Treatments</h4>

<p>For continuous treatments, the generalized propensity score is estimated using linear regression. The conditional density can be specified as normal or another distribution. In addition, kernel density estimation can be used instead of assuming a specific density for the numerator and denominator of the generalized propensity score by setting <code>use.kernel = TRUE</code>. Other arguments to <code><a href="stats.html#topic+density">density()</a></code> can be specified to refine the density estimation parameters. <code>plot = TRUE</code> can be specified to plot the density for the numerator and denominator, which can be helpful in diagnosing extreme weights.
</p>



<h4>Longitudinal Treatments</h4>

<p>For longitudinal treatments, the weights are the product of the weights estimated at each time point.
</p>



<h4>Sampling Weights</h4>

<p>Sampling weights are supported through <code>s.weights</code> in all scenarios except for multi-category treatments with <code>link = "bayes.probit"</code> and for binary and continuous treatments with <code>missing = "saem"</code> (see below). Warning messages may appear otherwise about non-integer successes, and these can be ignored.
</p>



<h4>Missing Data</h4>

<p>In the presence of missing data, the following value(s) for <code>missing</code> are allowed:
</p>

<dl>
<dt><code>"ind"</code> (default)</dt><dd><p>First, for each variable with missingness, a new missingness indicator variable is created which takes the value 1 if the original covariate is <code>NA</code> and 0 otherwise. The missingness indicators are added to the model formula as main effects. The missing values in the covariates are then replaced with the covariate medians (this value is arbitrary and does not affect estimation). The weight estimation then proceeds with this new formula and set of covariates. The covariates output in the resulting <code>weightit</code> object will be the original covariates with the <code>NA</code>s.
</p>
</dd>
<dt><code>"saem"</code></dt><dd><p>For binary treatments with <code>link = "logit"</code> or continuous treatments, a stochastic approximation version of the EM algorithm (SAEM) is used via the <a href="https://CRAN.R-project.org/package=misaem"><span class="pkg">misaem</span></a> package. No additional covariates are created. See Jiang et al. (2019) for information on this method. In some cases, this is a suitable alternative to multiple imputation.
</p>
</dd>
</dl>




<h4>M-estimation</h4>

<p>For binary treatments, M-estimation is supported when <code>missing</code> is not <code>"saem"</code> and <code>link</code> does not start with <code>"br."</code>. For multi-category treatments, M-estimation is supported when <code>missing</code> is not <code>"saem"</code> and <code>multi-method</code> is <code>"weightit"</code> (the default for non-ordered treatments) or <code>"glm"</code>. For continuous treatments, M-estimation is not supported. See <code><a href="#topic+glm_weightit">glm_weightit()</a></code> and <code>vignette("estimating-effects")</code> for details. For longitudinal treatments, M-estimation is supported whenever the underlying methods are.
</p>



<h3>Additional Arguments</h3>

<p>For binary treatments, the following additional argument can be specified:
</p>

<dl>
<dt><code>link</code></dt><dd><p>the link used in the generalized linear model for the propensity scores. <code>link</code> can be any of those allowed by <code><a href="stats.html#topic+binomial">binomial()</a></code>. A <code>br.</code> prefix can be added (e.g., <code>"br.logit"</code>); this changes the fitting method to the bias-corrected generalized linear models implemented in the <a href="https://CRAN.R-project.org/package=brglm2"><span class="pkg">brglm2</span></a> package.</p>
</dd>
</dl>

<p>For multi-category treatments, the following additional arguments can be specified:
</p>

<dl>
<dt><code>multi.method</code></dt><dd><p>the method used to estimate the generalized propensity scores. Allowable options include <code>"weightit"</code> to use an M-estimation-based method of multinomial logistic regression implemented in <span class="pkg">WeightIt</span>, <code>"glm"</code> to use a series of binomial models using <code><a href="stats.html#topic+glm">glm()</a></code>, <code>"mclogit"</code> to use multinomial logistic regression as implemented in <code><a href="mclogit.html#topic+mblogit">mclogit::mblogit()</a></code>, <code>"mnp"</code> to use Bayesian multinomial probit regression as implemented in <code><a href="MNP.html#topic+MNP">MNP::MNP()</a></code>, and <code>"brmultinom"</code> to use bias-reduced multinomial logistic regression as implemented in <code><a href="brglm2.html#topic+brmultinom">brglm2::brmultinom()</a></code>. For ordered treatments, <code>"polr"</code> can be supplied to use ordinal regression as implemented in <code><a href="MASS.html#topic+polr">MASS::polr()</a></code> unless <code>link</code> is <code>"br.logit"</code>, in which case bias-reduce ordinal logistic regression as implemented in <code><a href="brglm2.html#topic+bracl">brglm2::bracl()</a></code> is used. <code>"weightit"</code> and <code>"mclogit"</code> should give near-identical results, the main difference being increased robustness and customizability when using <code>"mclogit"</code> at the expense of not being able to use M-estimation to compute standard errors after weighting. The default is <code>"weightit"</code> for un-ordered treatments and <code>"polr"</code> for ordered treatments. Ignored when <code>missing = "saem"</code>.</p>
</dd>
<dt><code>link</code></dt><dd><p>The link used in the multinomial, binomial, or ordered regression model for the generalized propensity scores depending on the argument supplied to <code>multi.method</code>. When <code>multi.method = "glm"</code>, <code>link</code> can be any of those allowed by <code><a href="stats.html#topic+binomial">binomial()</a></code>. When treatment is ordered and <code>multi.method = "polr"</code>, <code>link</code> can be any of those allowed by <code>MASS::polr()</code> or <code>"br.logit"</code>. Otherwise, <code>link</code> should be <code>"logit"</code> or not specified.</p>
</dd>
</dl>

<p>For continuous treatments, the following additional arguments may be supplied:
</p>

<dl>
<dt><code>density</code></dt><dd><p>A function corresponding the conditional density of the treatment. The standardized residuals of the treatment model will be fed through this function to produce the numerator and denominator of the generalized propensity score weights. If blank, <code><a href="stats.html#topic+dnorm">dnorm()</a></code> is used as recommended by Robins et al. (2000). This can also be supplied as a string containing the name of the function to be called. If the string contains underscores, the call will be split by the underscores and the latter splits will be supplied as arguments to the second argument and beyond. For example, if <code>density = "dt_2"</code> is specified, the density used will be that of a t-distribution with 2 degrees of freedom. Using a t-distribution can be useful when extreme outcome values are observed (Naimi et al., 2014). Ignored if <code>use.kernel = TRUE</code> (described below).
</p>
</dd>
<dt><code>use.kernel</code></dt><dd><p>If <code>TRUE</code>, uses kernel density estimation through the <code><a href="stats.html#topic+density">density()</a></code> function to estimate the numerator and denominator densities for the weights. If <code>FALSE</code>, the argument to the <code>density</code> parameter is used instead.
</p>
</dd>
<dt><code>bw</code>, <code>adjust</code>, <code>kernel</code>, <code>n</code></dt><dd><p>If <code>use.kernel = TRUE</code>, the arguments to the <code><a href="stats.html#topic+density">density()</a></code> function. The defaults are the same as those in <code>density</code> except that <code>n</code> is 10 times the number of units in the sample.
</p>
</dd>
<dt><code>plot</code></dt><dd><p>If <code>use.kernel = TRUE</code> with continuous treatments, whether to plot the estimated density.
</p>
</dd>
<dt><code>link</code></dt><dd><p>The link used to fit the linear model for the generalized propensity score. Can be any allowed by <code><a href="stats.html#topic+gaussian">gaussian()</a></code>.
</p>
</dd>
</dl>

<p>Additional arguments to <code>glm()</code> can be specified as well when it is used for fitting. The <code>method</code> argument in <code>glm()</code> is renamed to <code>glm.method</code>. This can be used to supply alternative fitting functions, such as those implemented in the <a href="https://CRAN.R-project.org/package=glm2"><span class="pkg">glm2</span></a> package. Other arguments to <code>weightit()</code> are passed to <code>...</code> in <code>glm()</code>. In the presence of missing data with <code>link = "logit"</code> and <code>missing = "saem"</code>, additional arguments are passed to <code><a href="misaem.html#topic+miss.glm">misaem::miss.glm()</a></code> and <code><a href="misaem.html#topic+predict.miss.glm">misaem::predict.miss.glm()</a></code>, except the <code>method</code> argument in <code><a href="misaem.html#topic+predict.miss.glm">misaem::predict.miss.glm()</a></code> is replaced with <code>saem.method</code>.
</p>
<p>For continuous treatments in the presence of missing data with <code>missing = "saem"</code>, additional arguments are passed to <code><a href="misaem.html#topic+miss.lm">misaem::miss.lm()</a></code> and <code><a href="misaem.html#topic+predict.miss.lm">misaem::predict.miss.lm()</a></code>.
</p>


<h3>Additional Outputs</h3>


<dl>
<dt><code>obj</code></dt><dd><p>When <code>include.obj = TRUE</code>, the (generalized) propensity score model fit. For binary treatments, the output of the call to <code><a href="stats.html#topic+glm">glm()</a></code>. For multi-category treatments, the output of the call to the fitting function (or a list thereof if <code>multi.method = "glm"</code>. For continuous treatments, the output of the call to <code>glm()</code> for the predicted values in the denominator density.
</p>
</dd>
</dl>



<h3>References</h3>



<h4>Binary treatments</h4>


<ul>
<li> <p><code>estimand = "ATO"</code>
</p>
</li></ul>

<p>Li, F., Morgan, K. L., &amp; Zaslavsky, A. M. (2018). Balancing covariates via propensity score weighting. Journal of the American Statistical Association, 113(521), 390–400. <a href="https://doi.org/10.1080/01621459.2016.1260466">doi:10.1080/01621459.2016.1260466</a>
</p>

<ul>
<li> <p><code>estimand = "ATM"</code>
</p>
</li></ul>

<p>Li, L., &amp; Greene, T. (2013). A Weighting Analogue to Pair Matching in Propensity Score Analysis. The International Journal of Biostatistics, 9(2). <a href="https://doi.org/10.1515/ijb-2012-0030">doi:10.1515/ijb-2012-0030</a>
</p>

<ul>
<li> <p><code>estimand = "ATOS"</code>
</p>
</li></ul>

<p>Crump, R. K., Hotz, V. J., Imbens, G. W., &amp; Mitnik, O. A. (2009). Dealing with limited overlap in estimation of average treatment effects. Biometrika, 96(1), 187–199. <a href="https://doi.org/10.1093/biomet/asn055">doi:10.1093/biomet/asn055</a>
</p>

<ul>
<li><p> Other estimands
</p>
</li></ul>

<p>Austin, P. C. (2011). An Introduction to Propensity Score Methods for Reducing the Effects of Confounding in Observational Studies. Multivariate Behavioral Research, 46(3), 399–424. <a href="https://doi.org/10.1080/00273171.2011.568786">doi:10.1080/00273171.2011.568786</a>
</p>

<ul>
<li><p> Marginal mean weighting through stratification
</p>
</li></ul>

<p>Hong, G. (2010). Marginal mean weighting through stratification: Adjustment for selection bias in multilevel data. Journal of Educational and Behavioral Statistics, 35(5), 499–531. <a href="https://doi.org/10.3102/1076998609359785">doi:10.3102/1076998609359785</a>
</p>

<ul>
<li><p> Bias-reduced logistic regression
</p>
</li></ul>

<p>See references for the <span class="pkg">brglm2</span> <code><a href="brglm2.html#topic+brglm2">package()</a></code>.
</p>

<ul>
<li><p> SAEM logistic regression for missing data
</p>
</li></ul>

<p>Jiang, W., Josse, J., &amp; Lavielle, M. (2019). Logistic regression with missing covariates — Parameter estimation, model selection and prediction within a joint-modeling framework. Computational Statistics &amp; Data Analysis, 106907. <a href="https://doi.org/10.1016/j.csda.2019.106907">doi:10.1016/j.csda.2019.106907</a>
</p>



<h4>Multi-Category Treatments</h4>


<ul>
<li> <p><code>estimand = "ATO"</code>
</p>
</li></ul>

<p>Li, F., &amp; Li, F. (2019). Propensity score weighting for causal inference with multiple treatments. The Annals of Applied Statistics, 13(4), 2389–2415. <a href="https://doi.org/10.1214/19-AOAS1282">doi:10.1214/19-AOAS1282</a>
</p>

<ul>
<li> <p><code>estimand = "ATM"</code>
</p>
</li></ul>

<p>Yoshida, K., Hernández-Díaz, S., Solomon, D. H., Jackson, J. W., Gagne, J. J., Glynn, R. J., &amp; Franklin, J. M. (2017). Matching weights to simultaneously compare three treatment groups: Comparison to three-way matching. Epidemiology (Cambridge, Mass.), 28(3), 387–395. <a href="https://doi.org/10.1097/EDE.0000000000000627">doi:10.1097/EDE.0000000000000627</a>
</p>

<ul>
<li><p> Other estimands
</p>
</li></ul>

<p>McCaffrey, D. F., Griffin, B. A., Almirall, D., Slaughter, M. E., Ramchand, R., &amp; Burgette, L. F. (2013). A Tutorial on Propensity Score Estimation for Multiple Treatments Using Generalized Boosted Models. Statistics in Medicine, 32(19), 3388–3414. <a href="https://doi.org/10.1002/sim.5753">doi:10.1002/sim.5753</a>
</p>

<ul>
<li><p> Marginal mean weighting through stratification
</p>
</li></ul>

<p>Hong, G. (2012). Marginal mean weighting through stratification: A generalized method for evaluating multivalued and multiple treatments with nonexperimental data. Psychological Methods, 17(1), 44–60. <a href="https://doi.org/10.1037/a0024918">doi:10.1037/a0024918</a>
</p>



<h4>Continuous treatments</h4>

<p>Robins, J. M., Hernán, M. Á., &amp; Brumback, B. (2000). Marginal Structural Models and Causal Inference in Epidemiology. Epidemiology, 11(5), 550–560.
</p>

<ul>
<li><p> Using non-normal conditional densities
</p>
</li></ul>

<p>Naimi, A. I., Moodie, E. E. M., Auger, N., &amp; Kaufman, J. S. (2014). Constructing Inverse Probability Weights for Continuous Exposures: A Comparison of Methods. Epidemiology, 25(2), 292–299. <a href="https://doi.org/10.1097/EDE.0000000000000053">doi:10.1097/EDE.0000000000000053</a>
</p>

<ul>
<li><p> SAEM linear regression for missing data
</p>
</li></ul>

<p>Jiang, W., Josse, J., &amp; Lavielle, M. (2019). Logistic regression with missing covariates — Parameter estimation, model selection and prediction within a joint-modeling framework. Computational Statistics &amp; Data Analysis, 106907. <a href="https://doi.org/10.1016/j.csda.2019.106907">doi:10.1016/j.csda.2019.106907</a>
</p>



<h3>See Also</h3>

<p><code><a href="#topic+weightit">weightit()</a></code>, <code><a href="#topic+weightitMSM">weightitMSM()</a></code>, <code><a href="#topic+get_w_from_ps">get_w_from_ps()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library("cobalt")
data("lalonde", package = "cobalt")

#Balancing covariates between treatment groups (binary)
(W1 &lt;- weightit(treat ~ age + educ + married +
                  nodegree + re74, data = lalonde,
                method = "glm", estimand = "ATT",
                link = "probit"))
summary(W1)
bal.tab(W1)

#Balancing covariates with respect to race (multi-category)
(W2 &lt;- weightit(race ~ age + educ + married +
                  nodegree + re74, data = lalonde,
                method = "glm", estimand = "ATE"))
summary(W2)
bal.tab(W2)

#Balancing covariates with respect to re75 (continuous)
(W3 &lt;- weightit(re75 ~ age + educ + married +
                  nodegree + re74, data = lalonde,
                method = "glm", use.kernel = TRUE))
summary(W3)
bal.tab(W3)
</code></pre>

<hr>
<h2 id='method_ipt'>Inverse Probability Tilting</h2><span id='topic+method_ipt'></span>

<h3>Description</h3>

<p>This page explains the details of estimating weights using inverse probability tilting by setting <code>method = "ipt"</code> in the call to <code><a href="#topic+weightit">weightit()</a></code> or <code><a href="#topic+weightitMSM">weightitMSM()</a></code>. This method can be used with binary and multi-category treatments.
</p>
<p>In general, this method relies on estimating propensity scores using a modification of the usual generalized linear model score equations to enforce balance and then converting those propensity scores into weights using a formula that depends on the desired estimand. This method relies on code written for <span class="pkg">WeightIt</span> using <code><a href="rootSolve.html#topic+multiroot">rootSolve::multiroot()</a></code>.
</p>


<h4>Binary Treatments</h4>

<p>For binary treatments, this method estimates the weights using formulas described by Graham, Pinto, and Egel (2012). The following estimands are allowed: ATE, ATT, and ATC. When the ATE is requested, the optimization is run twice, once for each treatment group.
</p>



<h4>Multi-Category Treatments</h4>

<p>For multi-category treatments, this method estimates the weights using modifications of the formulas described by Graham, Pinto, and Egel (2012). The following estimands are allowed: ATE and ATT. When the ATE is requested, estimation is performed once for each treatment group. When the ATT is requested, estimation is performed once for each non-focal (i.e., control) group.
</p>



<h4>Continuous Treatments</h4>

<p>Inverse probability tilting is not compatible with continuous treatments.
</p>



<h4>Longitudinal Treatments</h4>

<p>For longitudinal treatments, the weights are the product of the weights estimated at each time point. This method is not guaranteed to yield exact balance at each time point. NOTE: the use of inverse probability tilting with longitudinal treatments has not been validated!
</p>



<h4>Sampling Weights</h4>

<p>Sampling weights are supported through <code>s.weights</code> in all scenarios.
</p>



<h4>Missing Data</h4>

<p>In the presence of missing data, the following value(s) for <code>missing</code> are allowed:
</p>

<dl>
<dt><code>"ind"</code> (default)</dt><dd>
<p>First, for each variable with missingness, a new missingness indicator variable is created which takes the value 1 if the original covariate is <code>NA</code> and 0 otherwise. The missingness indicators are added to the model formula as main effects. The missing values in the covariates are then replaced with the covariate medians (this value is arbitrary and does not affect estimation). The weight estimation then proceeds with this new formula and set of covariates. The covariates output in the resulting <code>weightit</code> object will be the original covariates with the <code>NA</code>s.
</p>
</dd>
</dl>




<h4>M-estimation</h4>

<p>M-estimation is supported for all scenarios. See <code><a href="#topic+glm_weightit">glm_weightit()</a></code> and <code>vignette("estimating-effects")</code> for details.
</p>



<h3>Details</h3>

<p>Inverse probability tilting (IPT) involves specifying estimating equations that fit the parameters of one or more generalized linear models with a modification that ensures exact balance on the covariate means. These estimating equations are solved, and the estimated parameters are used in the (generalized) propensity score, which is used to compute the weights. Conceptually and mathematically, IPT is very similar to entropy balancing and just-identified CBPS. For the ATT and ATC, entropy balancing, just-identified CBPS, and IPT will yield identical results. For the ATE or when <code>link</code> is specified as something other than <code>"logit"</code>, the three methods differ.
</p>
<p>Treatment effect estimates for binary treatments are consistent if the true propensity score is a logistic regression or the outcome model is linear in the covariates and their interaction with treatments. For entropy balancing, this is only true for the ATT, and for just-identified CBPS, this is only true if there is no effect modification by covariates. In this way, IPT provides additional theoretical guarantees over the other two methods, though potentially with some cost in precision.
</p>


<h3>Additional Arguments</h3>

<p><code>moments</code> and <code>int</code> are accepted. See <code><a href="#topic+weightit">weightit()</a></code> for details.
</p>

<dl>
<dt><code>quantile</code></dt><dd>
<p>A named list of quantiles (values between 0 and 1) for each continuous covariate, which are used to create additional variables that when balanced ensure balance on the corresponding quantile of the variable. For example, setting <code style="white-space: pre;">&#8288;quantile = list(x1 = c(.25, .5. , .75))&#8288;</code> ensures the 25th, 50th, and 75th percentiles of <code>x1</code> in each treatment group will be balanced in the weighted sample. Can also be a single number (e.g., <code>.5</code>) or an unnamed list of length 1 (e.g., <code>list(c(.25, .5, .75))</code>) to request the same quantile(s) for all continuous covariates, or a named vector (e.g., <code style="white-space: pre;">&#8288;c(x1 = .5, x2 = .75&#8288;</code>) to request one quantile for each covariate.
</p>
</dd>
<dt><code>link</code></dt><dd><p><code>"string"</code>; the link used to determine the inverse link for computing the (generalized) propensity scores. Default is <code>"logit"</code>, which is used in the original description of the method by Graham, Pinto, and Egel (2012), but <code>"probit"</code>, <code>"cauchit"</code>, and <code>"cloglog"</code> are also allowed.
</p>
</dd>
</dl>

<p>The <code>stabilize</code> argument is ignored.
</p>


<h3>Additional Outputs</h3>


<dl>
<dt><code>obj</code></dt><dd><p>When <code>include.obj = TRUE</code>, the output of the call to <code><a href="stats.html#topic+optim">optim()</a></code>, which contains the coefficient estimates and convergence information. For ATE fits or with multi-category treatments, a list of <code>rootSolve::multiroot()</code> outputs, one for each weighted group.
</p>
</dd>
</dl>



<h3>References</h3>



<h4><code>estimand = "ATE"</code></h4>

<p>Graham, B. S., De Xavier Pinto, C. C., &amp; Egel, D. (2012). Inverse Probability Tilting for Moment Condition Models with Missing Data. <em>The Review of Economic Studies</em>, 79(3), 1053–1079. <a href="https://doi.org/10.1093/restud/rdr047">doi:10.1093/restud/rdr047</a>
</p>



<h4><code>estimand = "ATT"</code></h4>

<p>Sant’Anna, P. H. C., &amp; Zhao, J. (2020). Doubly robust difference-in-differences estimators. <em>Journal of Econometrics</em>, 219(1), 101–122. <a href="https://doi.org/10.1016/j.jeconom.2020.06.003">doi:10.1016/j.jeconom.2020.06.003</a>
</p>



<h3>See Also</h3>

<p><code><a href="#topic+weightit">weightit()</a></code>, <code><a href="#topic+weightitMSM">weightitMSM()</a></code>
</p>
<p><a href="#topic+method_ebal">method_ebal</a> and <a href="#topic+method_cbps">method_cbps</a> for entropy balancing and CBPS, which work similarly.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("lalonde", package = "cobalt")

#Balancing covariates between treatment groups (binary)
(W1 &lt;- weightit(treat ~ age + educ + married +
                  nodegree + re74, data = lalonde,
                method = "ipt", estimand = "ATT"))
summary(W1)
cobalt::bal.tab(W1)

#Balancing covariates with respect to race (multi-category)
(W2 &lt;- weightit(race ~ age + educ + married +
                  nodegree + re74, data = lalonde,
                method = "ipt", estimand = "ATE"))
summary(W2)
cobalt::bal.tab(W2)

</code></pre>

<hr>
<h2 id='method_npcbps'>Nonparametric Covariate Balancing Propensity Score Weighting</h2><span id='topic+method_npcbps'></span>

<h3>Description</h3>

<p>This page explains the details of estimating weights from nonparametric covariate balancing propensity scores by setting <code>method = "npcbps"</code> in the call to <code><a href="#topic+weightit">weightit()</a></code> or <code><a href="#topic+weightitMSM">weightitMSM()</a></code>. This method can be used with binary, multi-category, and continuous treatments.
</p>
<p>In general, this method relies on estimating weights by maximizing the empirical likelihood of the data subject to balance constraints. This method relies on <code><a href="CBPS.html#topic+npCBPS">CBPS::npCBPS()</a></code> from the <a href="https://CRAN.R-project.org/package=CBPS"><span class="pkg">CBPS</span></a> package.
</p>


<h4>Binary Treatments</h4>

<p>For binary treatments, this method estimates the weights using <code><a href="CBPS.html#topic+npCBPS">CBPS::npCBPS()</a></code>. The ATE is the only estimand allowed. The weights are taken from the output of the <code>npCBPS</code> fit object.
</p>



<h4>Multi-Category Treatments</h4>

<p>For multi-category treatments, this method estimates the weights using <code><a href="CBPS.html#topic+npCBPS">CBPS::npCBPS()</a></code>. The ATE is the only estimand allowed. The weights are taken from the output of the <code>npCBPS</code> fit object.
</p>



<h4>Continuous Treatments</h4>

<p>For continuous treatments, this method estimates the weights using <code><a href="CBPS.html#topic+npCBPS">CBPS::npCBPS()</a></code>. The weights are taken from the output of the <code>npCBPS</code> fit object.
</p>



<h4>Longitudinal Treatments</h4>

<p>For longitudinal treatments, the weights are the product of the weights estimated at each time point. This is not how <code><a href="CBPS.html#topic+CBMSM">CBPS::CBMSM()</a></code> estimates weights for longitudinal treatments.
</p>



<h4>Sampling Weights</h4>

<p>Sampling weights are <b>not</b> supported with <code>method = "npcbps"</code>.
</p>



<h4>Missing Data</h4>

<p>In the presence of missing data, the following value(s) for <code>missing</code> are allowed:
</p>

<dl>
<dt><code>"ind"</code> (default)</dt><dd><p>First, for each variable with missingness, a new missingness indicator variable is created which takes the value 1 if the original covariate is <code>NA</code> and 0 otherwise. The missingness indicators are added to the model formula as main effects. The missing values in the covariates are then replaced with the covariate medians (this value is arbitrary and does not affect estimation). The weight estimation then proceeds with this new formula and set of covariates. The covariates output in the resulting <code>weightit</code> object will be the original covariates with the <code>NA</code>s.
</p>
</dd>
</dl>




<h4>M-estimation</h4>

<p>M-estimation is not supported.
</p>



<h3>Details</h3>

<p>Nonparametric CBPS involves the specification of a constrained optimization problem over the weights. The constraints correspond to covariate balance, and the loss function is the empirical likelihood of the data given the weights. npCBPS is similar to <a href="#topic+method_ebal">entropy balancing</a> and will generally produce similar results. Because the optimization problem of npCBPS is not convex it can be slow to converge or not converge at all, so approximate balance is allowed instead using the <code>cor.prior</code> argument, which controls the average deviation from zero correlation between the treatment and covariates allowed.
</p>


<h3>Additional Arguments</h3>

<p><code>moments</code> and <code>int</code> are accepted. See <code><a href="#topic+weightit">weightit()</a></code> for details.
</p>

<dl>
<dt><code>quantile</code></dt><dd>
<p>A named list of quantiles (values between 0 and 1) for each continuous covariate, which are used to create additional variables that when balanced ensure balance on the corresponding quantile of the variable. For example, setting <code style="white-space: pre;">&#8288;quantile = list(x1 = c(.25, .5. , .75))&#8288;</code> ensures the 25th, 50th, and 75th percentiles of <code>x1</code> in each treatment group will be balanced in the weighted sample. Can also be a single number (e.g., <code>.5</code>) or an unnamed list of length 1 (e.g., <code>list(c(.25, .5, .75))</code>) to request the same quantile(s) for all continuous covariates, or a named vector (e.g., <code style="white-space: pre;">&#8288;c(x1 = .5, x2 = .75&#8288;</code>) to request one quantile for each covariate. Only allowed with binary and multi-category treatments.
</p>
</dd>
</dl>

<p>All arguments to <code>npCBPS()</code> can be passed through <code>weightit()</code> or <code>weightitMSM()</code>.
</p>
<p>All arguments take on the defaults of those in <code>npCBPS()</code>.
</p>


<h3>Additional Outputs</h3>


<dl>
<dt><code>obj</code></dt><dd><p>When <code>include.obj = TRUE</code>, the nonparametric CB(G)PS model fit. The output of the call to <code><a href="CBPS.html#topic+npCBPS">CBPS::npCBPS()</a></code>.
</p>
</dd>
</dl>



<h3>Note</h3>

<p>When sampling weights are used with <code>CBPS::CBPS()</code>, the estimated weights already incorporate the sampling weights. When <code>weightit()</code> is used with <code>method = "cbps"</code>, the estimated weights are separated from the sampling weights, as they are with all other methods.
</p>


<h3>References</h3>

<p>Fong, C., Hazlett, C., &amp; Imai, K. (2018). Covariate balancing propensity score for a continuous treatment: Application to the efficacy of political advertisements. The Annals of Applied Statistics, 12(1), 156–177. <a href="https://doi.org/10.1214/17-AOAS1101">doi:10.1214/17-AOAS1101</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+weightit">weightit()</a></code>, <code><a href="#topic+weightitMSM">weightitMSM()</a></code>, <code><a href="#topic+method_cbps">method_cbps</a></code>
</p>
<p><code><a href="CBPS.html#topic+npCBPS">CBPS::npCBPS()</a></code> for the fitting function
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Examples take a long time to run
library("cobalt")
data("lalonde", package = "cobalt")
## Not run: 
  #Balancing covariates between treatment groups (binary)
  (W1 &lt;- weightit(treat ~ age + educ + married +
                    nodegree + re74, data = lalonde,
                  method = "npcbps", estimand = "ATE"))
  summary(W1)
  bal.tab(W1)

  #Balancing covariates with respect to race (multi-category)
  (W2 &lt;- weightit(race ~ age + educ + married +
                    nodegree + re74, data = lalonde,
                  method = "npcbps", estimand = "ATE"))
  summary(W2)
  bal.tab(W2)

## End(Not run)

</code></pre>

<hr>
<h2 id='method_optweight'>Optimization-Based Weighting</h2><span id='topic+method_optweight'></span><span id='topic+method_sbw'></span>

<h3>Description</h3>

<p>This page explains the details of estimating optimization-based weights (also known as stable balancing weights) by setting <code>method = "optweight"</code> in the call to <code><a href="#topic+weightit">weightit()</a></code> or <code><a href="#topic+weightitMSM">weightitMSM()</a></code>. This method can be used with binary, multi-category, and continuous treatments.
</p>
<p>In general, this method relies on estimating weights by solving a quadratic programming problem subject to approximate or exact balance constraints. This method relies on <code><a href="optweight.html#topic+optweight">optweight::optweight()</a></code> from the <a href="https://CRAN.R-project.org/package=optweight"><span class="pkg">optweight</span></a> package.
</p>
<p>Because <code>optweight()</code> offers finer control and uses the same syntax as <code>weightit()</code>, it is recommended that <code><a href="optweight.html#topic+optweight">optweight::optweight()</a></code> be used instead of <code>weightit</code> with <code>method = "optweight"</code>.
</p>


<h4>Binary Treatments</h4>

<p>For binary treatments, this method estimates the weights using <code><a href="optweight.html#topic+optweight">optweight::optweight()</a></code>. The following estimands are allowed: ATE, ATT, and ATC. The weights are taken from the output of the <code>optweight</code> fit object.
</p>



<h4>Multi-Category Treatments</h4>

<p>For multi-category treatments, this method estimates the weights using <code><a href="optweight.html#topic+optweight">optweight::optweight()</a></code>. The following estimands are allowed: ATE and ATT. The weights are taken from the output of the <code>optweight</code> fit object.
</p>



<h4>Continuous Treatments</h4>

<p>For binary treatments, this method estimates the weights using <code><a href="optweight.html#topic+optweight">optweight::optweight()</a></code>. The weights are taken from the output of the <code>optweight</code> fit object.
</p>



<h4>Longitudinal Treatments</h4>

<p>For longitudinal treatments, <code>optweight()</code> estimates weights that simultaneously satisfy balance constraints at all time points, so only one model is fit to obtain the weights. Using <code>method = "optweight"</code> in <code>weightitMSM()</code> causes <code>is.MSM.method</code> to be set to <code>TRUE</code> by default. Setting it to <code>FALSE</code> will run one model for each time point and multiply the weights together, a method that is not recommended. NOTE: neither use of optimization-based weights with longitudinal treatments has been validated!
</p>



<h4>Sampling Weights</h4>

<p>Sampling weights are supported through <code>s.weights</code> in all scenarios.
</p>



<h4>Missing Data</h4>

<p>In the presence of missing data, the following value(s) for <code>missing</code> are allowed:
</p>

<dl>
<dt><code>"ind"</code> (default)</dt><dd><p>First, for each variable with missingness, a new missingness indicator variable is created which takes the value 1 if the original covariate is <code>NA</code> and 0 otherwise. The missingness indicators are added to the model formula as main effects. The missing values in the covariates are then replaced with the covariate medians (this value is arbitrary and does not affect estimation). The weight estimation then proceeds with this new formula and set of covariates. The covariates output in the resulting <code>weightit</code> object will be the original covariates with the <code>NA</code>s.
</p>
</dd>
</dl>




<h4>M-estimation</h4>

<p>M-estimation is not supported.
</p>



<h3>Details</h3>

<p>Stable balancing weights are weights that solve a constrained optimization problem, where the constraints correspond to covariate balance and the loss function is the variance (or other norm) of the weights. These weights maximize the effective sample size of the weighted sample subject to user-supplied balance constraints. An advantage of this method over entropy balancing is the ability to allow approximate, rather than exact, balance through the <code>tols</code> argument, which can increase precision even for slight relaxations of the constraints.
</p>


<h3>Additional Arguments</h3>

<p><code>moments</code> and <code>int</code> are accepted. See <code><a href="#topic+weightit">weightit()</a></code> for details.
</p>

<dl>
<dt><code>quantile</code></dt><dd>
<p>A named list of quantiles (values between 0 and 1) for each continuous covariate, which are used to create additional variables that when balanced ensure balance on the corresponding quantile of the variable. For example, setting <code style="white-space: pre;">&#8288;quantile = list(x1 = c(.25, .5. , .75))&#8288;</code> ensures the 25th, 50th, and 75th percentiles of <code>x1</code> in each treatment group will be balanced in the weighted sample. Can also be a single number (e.g., <code>.5</code>) or an unnamed list of length 1 (e.g., <code>list(c(.25, .5, .75))</code>) to request the same quantile(s) for all continuous covariates, or a named vector (e.g., <code style="white-space: pre;">&#8288;c(x1 = .5, x2 = .75&#8288;</code>) to request one quantile for each covariate. Only allowed with binary and multi-category treatments.
</p>
</dd>
</dl>

<p>All arguments to <code>optweight()</code> can be passed through <code>weightit()</code> or <code>weightitMSM()</code>, with the following exception:
</p>

<ul>
<li> <p><code>targets</code> cannot be used and is ignored.
</p>
</li></ul>

<p>All arguments take on the defaults of those in <code>optweight()</code>.
</p>


<h3>Additional Outputs</h3>


<dl>
<dt><code>info</code></dt><dd>
<p>A list with one entry:
</p>

<dl>
<dt><code>duals</code></dt><dd><p>A data frame of dual variables for each balance constraint.</p>
</dd>
</dl>

</dd>
<dt><code>obj</code></dt><dd><p>When <code>include.obj = TRUE</code>, the output of the call to <code><a href="optweight.html#topic+optweight">optweight::optweight()</a></code>.</p>
</dd>
</dl>



<h3>Note</h3>

<p>The specification of <code>tols</code> differs between <code>weightit()</code> and <code>optweight()</code>. In <code>weightit()</code>, one tolerance value should be included per level of each factor variable, whereas in <code>optweight()</code>, all levels of a factor are given the same tolerance, and only one value needs to be supplied for a factor variable. Because of the potential for confusion and ambiguity, it is recommended to only supply one value for <code>tols</code> in <code>weightit()</code> that applies to all variables. For finer control, use <code>optweight()</code> directly.
</p>
<p>Seriously, just use <code><a href="optweight.html#topic+optweight">optweight::optweight()</a></code>. The syntax is almost identical and it's compatible with <span class="pkg">cobalt</span>, too.
</p>


<h3>References</h3>



<h4>Binary treatments</h4>

<p>Wang, Y., &amp; Zubizarreta, J. R. (2020). Minimal dispersion approximately balancing weights: Asymptotic properties and practical considerations. Biometrika, 107(1), 93–105. <a href="https://doi.org/10.1093/biomet/asz050">doi:10.1093/biomet/asz050</a>
</p>
<p>Zubizarreta, J. R. (2015). Stable Weights that Balance Covariates for Estimation With Incomplete Outcome Data. Journal of the American Statistical Association, 110(511), 910–922. <a href="https://doi.org/10.1080/01621459.2015.1023805">doi:10.1080/01621459.2015.1023805</a>
</p>



<h4>Multi-Category Treatments</h4>

<p>de los Angeles Resa, M., &amp; Zubizarreta, J. R. (2020). Direct and stable weight adjustment in non-experimental studies with multivalued treatments: Analysis of the effect of an earthquake on post-traumatic stress. Journal of the Royal Statistical Society: Series A (Statistics in Society), n/a(n/a). <a href="https://doi.org/10.1111/rssa.12561">doi:10.1111/rssa.12561</a>
</p>



<h4>Continuous treatments</h4>

<p>Greifer, N. (2020). Estimating Balancing Weights for Continuous Treatments Using Constrained Optimization. <a href="https://doi.org/10.17615/DYSS-B342">doi:10.17615/DYSS-B342</a>
</p>



<h3>See Also</h3>

<p><code><a href="#topic+weightit">weightit()</a></code>, <code><a href="#topic+weightitMSM">weightitMSM()</a></code>
</p>
<p><code><a href="optweight.html#topic+optweight">optweight::optweight()</a></code> for the fitting function
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data("lalonde", package = "cobalt")

#Balancing covariates between treatment groups (binary)
(W1 &lt;- weightit(treat ~ age + educ + married +
                  nodegree + re74, data = lalonde,
                method = "optweight", estimand = "ATT",
                tols = 0))
summary(W1)
cobalt::bal.tab(W1)

#Balancing covariates with respect to race (multi-category)
(W2 &lt;- weightit(race ~ age + educ + married +
                  nodegree + re74, data = lalonde,
                method = "optweight", estimand = "ATE",
                tols = .01))
summary(W2)
cobalt::bal.tab(W2)

#Balancing covariates with respect to re75 (continuous)
(W3 &lt;- weightit(re75 ~ age + educ + married +
                  nodegree + re74, data = lalonde,
                method = "optweight", tols = .05))
summary(W3)
cobalt::bal.tab(W3)

</code></pre>

<hr>
<h2 id='method_super'>Propensity Score Weighting Using SuperLearner</h2><span id='topic+method_super'></span>

<h3>Description</h3>

<p>This page explains the details of estimating weights from SuperLearner-based propensity scores by setting <code>method = "super"</code> in the call to <code><a href="#topic+weightit">weightit()</a></code> or <code><a href="#topic+weightitMSM">weightitMSM()</a></code>. This method can be used with binary, multi-category, and continuous treatments.
</p>
<p>In general, this method relies on estimating propensity scores using the SuperLearner algorithm for stacking predictions and then converting those propensity scores into weights using a formula that depends on the desired estimand. For binary and multi-category treatments, one or more binary classification algorithms are used to estimate the propensity scores as the predicted probability of being in each treatment given the covariates. For continuous treatments, regression algorithms are used to estimate generalized propensity scores as the conditional density of treatment given the covariates. This method relies on <code><a href="SuperLearner.html#topic+SuperLearner">SuperLearner::SuperLearner()</a></code> from the <a href="https://CRAN.R-project.org/package=SuperLearner"><span class="pkg">SuperLearner</span></a> package.
</p>


<h4>Binary Treatments</h4>

<p>For binary treatments, this method estimates the propensity scores using <code><a href="SuperLearner.html#topic+SuperLearner">SuperLearner::SuperLearner()</a></code>. The following estimands are allowed: ATE, ATT, ATC, ATO, ATM, and ATOS. Weights can also be computed using marginal mean weighting through stratification for the ATE, ATT, and ATC. See <code><a href="#topic+get_w_from_ps">get_w_from_ps()</a></code> for details.
</p>



<h4>Multi-Category Treatments</h4>

<p>For multi-category treatments, the propensity scores are estimated using several calls to <code><a href="SuperLearner.html#topic+SuperLearner">SuperLearner::SuperLearner()</a></code>, one for each treatment group; the treatment probabilities are not normalized to sum to 1. The following estimands are allowed: ATE, ATT, ATC, ATO, and ATM. The weights for each estimand are computed using the standard formulas or those mentioned above. Weights can also be computed using marginal mean weighting through stratification for the ATE, ATT, and ATC. See <code><a href="#topic+get_w_from_ps">get_w_from_ps()</a></code> for details.
</p>



<h4>Continuous Treatments</h4>

<p>For continuous treatments, the generalized propensity score is estimated using <code><a href="SuperLearner.html#topic+SuperLearner">SuperLearner::SuperLearner()</a></code>. In addition, kernel density estimation can be used instead of assuming a normal density for the numerator and denominator of the generalized propensity score by setting <code>use.kernel = TRUE</code>. Other arguments to <code><a href="stats.html#topic+density">density()</a></code> can be specified to refine the density estimation parameters. <code>plot = TRUE</code> can be specified to plot the density for the numerator and denominator, which can be helpful in diagnosing extreme weights.
</p>



<h4>Longitudinal Treatments</h4>

<p>For longitudinal treatments, the weights are the product of the weights estimated at each time point.
</p>



<h4>Sampling Weights</h4>

<p>Sampling weights are supported through <code>s.weights</code> in all scenarios.
</p>



<h4>Missing Data</h4>

<p>In the presence of missing data, the following value(s) for <code>missing</code> are allowed:
</p>

<dl>
<dt><code>"ind"</code> (default)</dt><dd><p>First, for each variable with missingness, a new missingness indicator variable is created which takes the value 1 if the original covariate is <code>NA</code> and 0 otherwise. The missingness indicators are added to the model formula as main effects. The missing values in the covariates are then replaced with the covariate medians (this value is arbitrary and does not affect estimation). The weight estimation then proceeds with this new formula and set of covariates. The covariates output in the resulting <code>weightit</code> object will be the original covariates with the <code>NA</code>s.
</p>
</dd>
</dl>




<h4>M-estimation</h4>

<p>M-estimation is not supported.
</p>



<h3>Details</h3>

<p>SuperLearner works by fitting several machine learning models to the treatment and covariates and then taking a weighted combination of the generated predicted values to use as the propensity scores, which are then used to construct weights. The machine learning models used are supplied using the <code>SL.library</code> argument; the more models are supplied, the higher the chance of correctly modeling the propensity score. The predicted values are combined using the method supplied in the <code>SL.method</code> argument (which is nonnegative least squares by default). A benefit of SuperLearner is that, asymptotically, it is guaranteed to perform as well as or better than the best-performing method included in the library. Using Balance SuperLearner by setting <code>SL.method = "method.balance"</code> works by selecting the combination of predicted values that minimizes an imbalance measure.
</p>


<h3>Additional Arguments</h3>


<dl>
<dt><code>discrete</code></dt><dd><p>if <code>TRUE</code>, uses discrete SuperLearner, which simply selects the best performing method. Default <code>FALSE</code>, which finds the optimal combination of predictions for the libraries using <code>SL.method</code>.</p>
</dd>
</dl>

<p>An argument to <code>SL.library</code> <strong>must</strong> be supplied. To see a list of available entries, use <code><a href="SuperLearner.html#topic+listWrappers">SuperLearner::listWrappers()</a></code>.
</p>
<p>All arguments to <code><a href="SuperLearner.html#topic+SuperLearner">SuperLearner::SuperLearner()</a></code> can be passed through <code>weightit()</code> or <code>weightitMSM()</code>, with the following exceptions:
</p>

<ul>
<li> <p><code>obsWeights</code> is ignored because sampling weights are passed using <code>s.weights</code>.
</p>
</li>
<li> <p><code>method</code> in <code>SuperLearner()</code> is replaced with the argument <code>SL.method</code> in <code>weightit()</code>.
</p>
</li></ul>

<p>For continuous treatments only, the following arguments may be supplied:
</p>

<dl>
<dt><code>density</code></dt><dd><p>A function corresponding to the conditional density of the treatment. The standardized residuals of the treatment model will be fed through this function to produce the numerator and denominator of the generalized propensity score weights. If blank, <code><a href="stats.html#topic+dnorm">dnorm()</a></code> is used as recommended by Robins et al. (2000). This can also be supplied as a string containing the name of the function to be called. If the string contains underscores, the call will be split by the underscores and the latter splits will be supplied as arguments to the second argument and beyond. For example, if <code>density = "dt_2"</code> is specified, the density used will be that of a t-distribution with 2 degrees of freedom. Using a t-distribution can be useful when extreme outcome values are observed (Naimi et al., 2014). Ignored if <code>use.kernel = TRUE</code> (described below).
</p>
</dd>
<dt><code>use.kernel</code></dt><dd><p>If <code>TRUE</code>, uses kernel density estimation through the <code><a href="stats.html#topic+density">density()</a></code> function to estimate the numerator and denominator densities for the weights. If <code>FALSE</code>, the argument to the <code>density</code> parameter is used instead.
</p>
</dd>
<dt><code>bw</code>, <code>adjust</code>, <code>kernel</code>, <code>n</code></dt><dd><p>If <code>use.kernel = TRUE</code>, the arguments to the <code><a href="stats.html#topic+density">density()</a></code> function. The defaults are the same as those in <code>density</code> except that <code>n</code> is 10 times the number of units in the sample.
</p>
</dd>
<dt><code>plot</code></dt><dd><p>If <code>use.kernel = TRUE</code>, whether to plot the estimated density.
</p>
</dd>
</dl>



<h4>Balance SuperLearner</h4>

<p>In addition to the methods allowed by <code>SuperLearner()</code>, one can specify <code>SL.method = "method.balance"</code> to use &quot;Balance SuperLearner&quot; as described by Pirracchio and Carone (2018), wherein covariate balance is used to choose the optimal combination of the predictions from the methods specified with <code>SL.library</code>. Coefficients are chosen (one for each prediction method) so that the weights generated from the weighted combination of the predictions optimize a balance criterion, which must be set with the <code>criterion</code> argument, described below.
</p>

<dl>
<dt><code>criterion</code></dt><dd><p>A string describing the balance criterion used to select the best weights. See <code><a href="cobalt.html#topic+bal.compute">cobalt::bal.compute()</a></code> for allowable options for each treatment type. For binary and multi-category treatments, the default is <code>"smd.mean"</code>, which minimizes the average absolute standard mean difference among the covariates between treatment groups. For continuous treatments, the default is <code>"p.mean"</code>, which minimizes the average absolute Pearson correlation between the treatment and covariates.
</p>
</dd>
</dl>

<p>Note that this implementation differs from that of Pirracchio and Carone (2018) in that here, balance is measured only on the terms included in the model formula (i.e., and not their interactions unless specifically included), and balance results from a sample weighted using the estimated predicted values as propensity scores, not a sample matched using propensity score matching on the predicted values. Binary and continuous treatments are supported, but currently multi-category treatments are not.
</p>



<h3>Additional Outputs</h3>


<dl>
<dt><code>info</code></dt><dd>
<p>For binary and continuous treatments, a list with two entries, <code>coef</code> and <code>cvRisk</code>. For multi-category treatments, a list of lists with these two entries, one for each treatment level.
</p>

<dl>
<dt><code>coef</code></dt><dd>
<p>The coefficients in the linear combination of the predictions from each method in <code>SL.library</code>. Higher values indicate that the corresponding method plays a larger role in determining the resulting predicted value, and values close to zero indicate that the method plays little role in determining the predicted value. When <code>discrete = TRUE</code>, these correspond to the coefficients that would have been estimated had <code>discrete</code> been <code>FALSE</code>.
</p>
</dd>
<dt><code>cvRisk</code></dt><dd>
<p>The cross-validation risk for each method in <code>SL.library</code>. Higher values indicate that the method has worse cross-validation accuracy. When <code>SL.method = "method.balance"</code>, the sample weighted balance statistic requested with <code>criterion</code>. Higher values indicate worse balance.
</p>
</dd>
</dl>

</dd>
<dt><code>obj</code></dt><dd>
<p>When <code>include.obj = TRUE</code>, the SuperLearner fit(s) used to generate the predicted values. For binary and continuous treatments, the output of the call to <code><a href="SuperLearner.html#topic+SuperLearner">SuperLearner::SuperLearner()</a></code>. For multi-category treatments, a list of outputs to calls to <code>SuperLearner::SuperLearner()</code>.
</p>
</dd>
</dl>



<h3>Note</h3>

<p>Some methods formerly available in <span class="pkg">SuperLearner</span> are now in <span class="pkg">SuperLearnerExtra</span>, which can be found on GitHub at <a href="https://github.com/ecpolley/SuperLearnerExtra">https://github.com/ecpolley/SuperLearnerExtra</a>.
</p>
<p>The <code>criterion</code> argument used to be called <code>stop.method</code>, which is its name in <span class="pkg">twang</span>. <code>stop.method</code> still works for backward compatibility. Additionally, the criteria formerly named as <code>es.mean</code>, <code>es.max</code>, and <code>es.rms</code> have been renamed to <code>smd.mean</code>, <code>smd.max</code>, and <code>smd.rms</code>. The former are used in <span class="pkg">twang</span> and will still work with <code>weightit()</code> for backward compatibility.
</p>


<h3>References</h3>



<h4>Binary treatments</h4>

<p>Pirracchio, R., Petersen, M. L., &amp; van der Laan, M. (2015). Improving Propensity Score Estimators’ Robustness to Model Misspecification Using Super Learner. American Journal of Epidemiology, 181(2), 108–119. <a href="https://doi.org/10.1093/aje/kwu253">doi:10.1093/aje/kwu253</a>
</p>



<h4>Multi-Category Treatments</h4>

<p>Imai, K., &amp; Ratkovic, M. (2014). Covariate balancing propensity score. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 76(1), 243–263.
</p>



<h4>Continuous treatments</h4>

<p>Kreif, N., Grieve, R., Díaz, I., &amp; Harrison, D. (2015). Evaluation of the Effect of a Continuous Treatment: A Machine Learning Approach with an Application to Treatment for Traumatic Brain Injury. Health Economics, 24(9), 1213–1228. <a href="https://doi.org/10.1002/hec.3189">doi:10.1002/hec.3189</a>
</p>



<h4>Balance SuperLearner (<code>SL.method = "method.balance"</code>)</h4>

<p>Pirracchio, R., &amp; Carone, M. (2018). The Balance Super Learner: A robust adaptation of the Super Learner to improve estimation of the average treatment effect in the treated based on propensity score matching. Statistical Methods in Medical Research, 27(8), 2504–2518. <a href="https://doi.org/10.1177/0962280216682055">doi:10.1177/0962280216682055</a>
</p>
<p>See <code><a href="#topic+method_glm">method_glm</a></code> for additional references.
</p>



<h3>See Also</h3>

<p><code><a href="#topic+weightit">weightit()</a></code>, <code><a href="#topic+weightitMSM">weightitMSM()</a></code>, <code><a href="#topic+get_w_from_ps">get_w_from_ps()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library("cobalt")
data("lalonde", package = "cobalt")

#Balancing covariates between treatment groups (binary)
(W1 &lt;- weightit(treat ~ age + educ + married +
                  nodegree + re74, data = lalonde,
                method = "super", estimand = "ATT",
                SL.library = c("SL.glm", "SL.stepAIC",
                               "SL.glm.interaction")))
summary(W1)
bal.tab(W1)

  #Balancing covariates with respect to race (multi-category)
  (W2 &lt;- weightit(race ~ age + educ + married +
                    nodegree + re74, data = lalonde,
                  method = "super", estimand = "ATE",
                  SL.library = c("SL.glm", "SL.stepAIC",
                                 "SL.glm.interaction")))
  summary(W2)
  bal.tab(W2)

  #Balancing covariates with respect to re75 (continuous)
  #assuming t(8) conditional density for treatment
  (W3 &lt;- weightit(re75 ~ age + educ + married +
                    nodegree + re74, data = lalonde,
                  method = "super", density = "dt_8",
                  SL.library = c("SL.glm", "SL.ridge",
                                 "SL.glm.interaction")))
  summary(W3)
  bal.tab(W3)

#Balancing covariates between treatment groups (binary)
# using balance SuperLearner to minimize the maximum
# KS statistic
(W4 &lt;- weightit(treat ~ age + educ + married +
                  nodegree + re74, data = lalonde,
                method = "super", estimand = "ATT",
                SL.library = c("SL.glm", "SL.stepAIC",
                               "SL.lda"),
                SL.method = "method.balance",
                criterion = "ks.max"))
summary(W4)
bal.tab(W4, stats = c("m", "ks"))

</code></pre>

<hr>
<h2 id='method_user'>User-Defined Functions for Estimating Weights</h2><span id='topic+method_user'></span>

<h3>Description</h3>

<p>This page explains the details of estimating weights using a user-defined function. The function must take in arguments that are passed to it by <code><a href="#topic+weightit">weightit()</a></code> or <code><a href="#topic+weightitMSM">weightitMSM()</a></code> and return a vector of weights or a list containing the weights.
</p>
<p>To supply a user-defined function, the function object should be entered directly to <code>method</code>; for example, for a function <code>fun</code>, <code>method = fun</code>.
</p>


<h4>Point Treatments</h4>

<p>The following arguments are automatically passed to the user-defined function, which should have named parameters corresponding to them:
</p>

<ul>
<li><p><code>treat</code>: a vector of treatment status for each unit. This comes directly from the left hand side of the formula passed to <code>weightit()</code> and so will have it's type (e.g., numeric, factor, etc.), which may need to be converted.
</p>
</li>
<li><p><code>covs</code>: a data frame of covariate values for each unit. This comes directly from the right hand side of the formula passed to <code>weightit()</code>. The covariates are processed so that all columns are numeric; all factor variables are split into dummies and all interactions are evaluated. All levels of factor variables are given dummies, so the matrix of the covariates is not full rank. Users can use <code><a href="#topic+make_full_rank">make_full_rank()</a></code>, which accepts a numeric matrix or data frame and removes columns to make it full rank, if a full rank covariate matrix is desired.
</p>
</li>
<li><p><code>s.weights</code>: a numeric vector of sampling weights, one for each unit.
</p>
</li>
<li><p><code>ps</code>: a numeric vector of propensity scores.
</p>
</li>
<li><p><code>subset</code>: a logical vector the same length as <code>treat</code> that is <code>TRUE</code> for units to be included in the estimation and <code>FALSE</code> otherwise. This is used to subset the input objects when <code>exact</code> is used. <code>treat</code>, <code>covs</code>, <code>s.weights</code>, and <code>ps</code>, if supplied, will already have been subsetted by <code>subset</code>.
</p>
</li>
<li><p><code>estimand</code>: a character vector of length 1 containing the desired estimand. The characters will have been converted to uppercase. If &quot;ATC&quot; was supplied to estimand, <code>weightit()</code> sets <code>focal</code> to the control level (usually 0 or the lowest level of <code>treat</code>) and sets <code>estimand</code> to &quot;ATT&quot;.
</p>
</li>
<li><p><code>focal</code>: a character vector of length 1 containing the focal level of the treatment when the estimand is the ATT (or the ATC as detailed above). <code>weightit()</code> ensures the value of focal is a level of <code>treat</code>.
</p>
</li>
<li><p><code>stabilize</code>: a logical vector of length 1. It is not processed by <code>weightit()</code> before it reaches the fitting function.
</p>
</li>
<li><p><code>moments</code>: a numeric vector of length 1. It is not processed by <code>weightit()</code> before it reaches the fitting function except that <code>as.integer()</code> is applied to it. This is used in other methods to determine whether polynomials of the entered covariates are to be used in the weight estimation.
</p>
</li>
<li><p><code>int</code>: a logical vector of length 1. It is not processed by <code>weightit()</code> before it reaches the fitting function. This is used in other methods to determine whether interactions of the entered covariates are to be used in the weight estimation.
</p>
</li></ul>

<p>None of these parameters are required to be in the fitting function. These are simply those that are automatically available.
</p>
<p>In addition, any additional arguments supplied to <code>weightit()</code> will be passed on to the fitting function. <code>weightit()</code> ensures the arguments correspond to the parameters of the fitting function and throws an error if an incorrectly named argument is supplied and the fitting function doesn't include <code style="white-space: pre;">&#8288;\dots&#8288;</code> as a parameter.
</p>
<p>The fitting function must output either a numeric vector of weights or a list (or list-like object) with an entry named wither &quot;w&quot; or &quot;weights&quot;. If a list, the list can contain other named entries, but only entries named &quot;w&quot;, &quot;weights&quot;, &quot;ps&quot;, and &quot;fit.obj&quot; will be processed. &quot;ps&quot; is a vector of propensity scores and &quot;fit.obj&quot; should be an object used in the fitting process that a user may want to examine and that is included in the <code>weightit</code> output object as &quot;obj&quot; when <code>include.obj = TRUE</code>. The &quot;ps&quot; and &quot;fit.obj&quot; components are optional, but &quot;weights&quot; or &quot;w&quot; is required.
</p>



<h4>Longitudinal Treatments</h4>

<p>Longitudinal treatments can be handled either by running the fitting function for point treatments for each time point and multiplying the resulting weights together or by running a method that accommodates multiple time points and outputs a single set of weights. For the former, <code>weightitMSM()</code> can be used with the user-defined function just as it is with <code>weightit()</code>. The latter method is not yet accommodated by <code>weightitMSM()</code>, but will be someday, maybe.
</p>



<h3>See Also</h3>

<p><code><a href="#topic+weightit">weightit()</a></code>, <code><a href="#topic+weightitMSM">weightitMSM()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library("cobalt")
data("lalonde", package = "cobalt")

#A user-defined version of method = "ps"
my.ps &lt;- function(treat, covs, estimand, focal = NULL) {
  covs &lt;- make_full_rank(covs)
  d &lt;- data.frame(treat, covs)
  f &lt;- formula(d)
  ps &lt;- glm(f, data = d, family = "binomial")$fitted
  w &lt;- get_w_from_ps(ps, treat = treat, estimand = estimand,
                     focal = focal)

  list(w = w, ps = ps)
}

#Balancing covariates between treatment groups (binary)
(W1 &lt;- weightit(treat ~ age + educ + married +
                  nodegree + re74, data = lalonde,
                method = my.ps, estimand = "ATT"))
summary(W1)
bal.tab(W1)

data("msmdata")
(W2 &lt;- weightitMSM(list(A_1 ~ X1_0 + X2_0,
                        A_2 ~ X1_1 + X2_1 +
                          A_1 + X1_0 + X2_0,
                        A_3 ~ X1_2 + X2_2 +
                          A_2 + X1_1 + X2_1 +
                          A_1 + X1_0 + X2_0),
                   data = msmdata,
                   method = my.ps))

summary(W2)
bal.tab(W2)

# Kernel balancing using the KBAL package, available
# using devtools::install_github("chadhazlett/KBAL").
# Only the ATT and ATC are available. Use 'kbal.method'
# instead of 'method' in weightit() to choose between
# "ebal" and "el".

## Not run: 
  kbal.fun &lt;- function(treat, covs, estimand, focal, ...) {
    args &lt;- list(...)
    if (is_not_null(focal))
      treat &lt;- as.numeric(treat == focal)
    else if (estimand != "ATT")
      stop("estimand must be 'ATT' or 'ATC'.", call. = FALSE)
    if ("kbal.method" %in% names(args)) {
      names(args)[names(args) == "kbal.method"] &lt;- "method"
    }
    args[!names(args) %in% setdiff(names(formals(KBAL::kbal)),
                                     c("X", "D"))] &lt;- NULL
    k.out &lt;- do.call(KBAL::kbal, c(list(X = covs, D = treat),
                                   args))
    w &lt;- k.out$w

    list(w = w)
  }

  (Wk &lt;- weightit(treat ~ age + educ + married +
                    nodegree + re74, data = lalonde,
                  method = kbal.fun, estimand = "ATT",
                  kbal.method = "ebal"))
  summary(Wk)
  bal.tab(Wk, disp.ks = TRUE)

## End(Not run)

</code></pre>

<hr>
<h2 id='msmdata'>Simulated data for a 3 time point sequential study</h2><span id='topic+msmdata'></span>

<h3>Description</h3>

<p>This is a simulated dataset of 7500 units with covariates and treatment
measured three times and the outcome measured at the end from a hypothetical
observational study examining the effect of treatment delivered at each time
point on an adverse event.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>msmdata
</code></pre>


<h3>Format</h3>

<p>A data frame with 7500 observations on the following 10 variables.
</p>

<dl>
<dt><code>X1_0</code></dt><dd><p>a count covariate measured at baseline</p>
</dd>
<dt><code>X2_0</code></dt><dd><p>a binary covariate measured at baseline</p>
</dd>
<dt><code>A_1</code></dt><dd><p>a binary indicator of treatment status at the first time point</p>
</dd>
<dt><code>X1_1</code></dt><dd><p>a count covariate measured at the first time point (after the first treatment)</p>
</dd>
<dt><code>X2_1</code></dt><dd><p>a binary covariate measured at the first time point (after the first treatment)</p>
</dd>
<dt><code>A_2</code></dt><dd><p>a binary indicator of treatment status at the second time point</p>
</dd>
<dt><code>X1_2</code></dt><dd><p>a count covariate measured at the second time point (after the second treatment)</p>
</dd>
<dt><code>X2_2</code></dt><dd><p>a binary covariate measured at the first time point (after the first treatment)</p>
</dd>
<dt><code>A_3</code></dt><dd><p>a binary indicator of treatment status at the third time point</p>
</dd>
<dt><code>Y_B</code></dt><dd><p>a binary indicator of the outcome event (e.g., death)</p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'>
data("msmdata")

</code></pre>

<hr>
<h2 id='sbps'>Subgroup Balancing Propensity Score</h2><span id='topic+sbps'></span>

<h3>Description</h3>

<p>Implements the subgroup balancing propensity score (SBPS), which is an
algorithm that attempts to achieve balance in subgroups by sharing
information from the overall sample and subgroups (Dong, Zhang, Zeng, &amp; Li,
2020; DZZL). Each subgroup can use either weights estimated using the whole
sample, weights estimated using just that subgroup, or a combination of the
two. The optimal combination is chosen as that which minimizes an imbalance
criterion that includes subgroup as well as overall balance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sbps(
  obj,
  obj2 = NULL,
  moderator = NULL,
  formula = NULL,
  data = NULL,
  smooth = FALSE,
  full.search
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sbps_+3A_obj">obj</code></td>
<td>
<p>a <code>weightit</code> object containing weights estimated in the
overall sample.</p>
</td></tr>
<tr><td><code id="sbps_+3A_obj2">obj2</code></td>
<td>
<p>a <code>weightit</code> object containing weights estimated in the
subgroups. Typically this has been estimated by including <code>by</code> in the
call to <code><a href="#topic+weightit">weightit()</a></code>. Either <code>obj2</code> or <code>moderator</code> must be
specified.</p>
</td></tr>
<tr><td><code id="sbps_+3A_moderator">moderator</code></td>
<td>
<p>optional; a string containing the name of the variable in
<code>data</code> for which weighting is to be done within subgroups or a
one-sided formula with the subgrouping variable on the right-hand side. This
argument is analogous to the <code>by</code> argument in <code>weightit()</code>, and in
fact it is passed on to <code>by</code>. Either <code>obj2</code> or <code>moderator</code>
must be specified.</p>
</td></tr>
<tr><td><code id="sbps_+3A_formula">formula</code></td>
<td>
<p>an optional formula with the covariates for which balance is
to be optimized. If not specified, the formula in <code>obj$call</code> will be
used.</p>
</td></tr>
<tr><td><code id="sbps_+3A_data">data</code></td>
<td>
<p>an optional data set in the form of a data frame that contains
the variables in <code>formula</code> or <code>moderator</code>.</p>
</td></tr>
<tr><td><code id="sbps_+3A_smooth">smooth</code></td>
<td>
<p><code>logical</code>; whether the smooth version of the SBPS should
be used. This is only compatible with <code>weightit</code> methods that return a
propensity score.</p>
</td></tr>
<tr><td><code id="sbps_+3A_full.search">full.search</code></td>
<td>
<p><code>logical</code>; when <code>smooth = FALSE</code>, whether every
combination of subgroup and overall weights should be evaluated. If
<code>FALSE</code>, a stochastic search as described in DZZL will be used instead.
If <code>TRUE</code>, all <code class="reqn">2^R</code> combinations will be checked, where <code class="reqn">R</code> is
the number of subgroups, which can take a long time with many subgroups. If
unspecified, will default to <code>TRUE</code> if <code class="reqn">R &lt;= 8</code> and <code>FALSE</code>
otherwise.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The SBPS relies on two sets of weights: one estimated in the overall sample
and one estimated within each subgroup. The algorithm decides whether each
subgroup should use the weights estimated in the overall sample or those
estimated in the subgroup. There are 2^R permutations of overall and
subgroup weights, where R is the number of subgroups. The optimal
permutation is chosen as that which minimizes a balance criterion as
described in DZZL. The balance criterion used here is, for binary and
multi-category treatments, the sum of the squared standardized mean differences
within subgroups and overall, which are computed using
<code><a href="cobalt.html#topic+balance-summary">cobalt::col_w_smd()</a></code>, and for continuous treatments, the
sum of the squared correlations between each covariate and treatment within
subgroups and overall, which are computed using <code><a href="cobalt.html#topic+balance-summary">cobalt::col_w_corr()</a></code>.
</p>
<p>The smooth version estimates weights that determine the relative
contribution of the overall and subgroup propensity scores to a weighted
average propensity score for each subgroup. If P_O are the propensity scores
estimated in the overall sample and P_S are the propensity scores estimated
in each subgroup, the smooth SBPS finds R coefficients C so that for each
subgroup, the ultimate propensity score is <code class="reqn">C*P_S + (1-C)*P_O</code>, and
weights are computed from this propensity score. The coefficients are
estimated using <code><a href="stats.html#topic+optim">optim()</a></code> with <code>method = "L-BFGS-B"</code>. When C is
estimated to be 1 or 0 for each subgroup, the smooth SBPS coincides with the
standard SBPS.
</p>
<p>If <code>obj2</code> is not specified and <code>moderator</code> is, <code>sbps()</code> will
attempt to refit the model specified in <code>obj</code> with the <code>moderator</code>
in the <code>by</code> argument. This relies on the environment in which
<code>obj</code> was created to be intact and can take some time if <code>obj</code> was
hard to fit. It's safer to estimate <code>obj</code> and <code>obj2</code> (the latter
simply by including the moderator in the <code>by</code> argument) and supply
these to <code>sbps()</code>.
</p>


<h3>Value</h3>

<p>A <code>weightit.sbps</code> object, which inherits from <code>weightit</code>.
This contains all the information in <code>obj</code> with the weights, propensity
scores, call, and possibly covariates updated from <code>sbps()</code>. In
addition, the <code>prop.subgroup</code> component contains the values of the
coefficients C for the subgroups (which are either 0 or 1 for the standard
SBPS), and the <code>moderator</code> component contains a data.frame with the
moderator.
</p>
<p>This object has its own summary method and is compatible with <span class="pkg">cobalt</span>
functions. The <code>cluster</code> argument should be used with <span class="pkg">cobalt</span>
functions to accurately reflect the performance of the weights in balancing
the subgroups.
</p>


<h3>References</h3>

<p>Dong, J., Zhang, J. L., Zeng, S., &amp; Li, F. (2020). Subgroup
balancing propensity score. Statistical Methods in Medical Research, 29(3),
659–676. <a href="https://doi.org/10.1177/0962280219870836">doi:10.1177/0962280219870836</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+weightit">weightit()</a></code>, <code><a href="#topic+summary.weightit">summary.weightit()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library("cobalt")
data("lalonde", package = "cobalt")

#Balancing covariates between treatment groups within races
(W1 &lt;- weightit(treat ~ age + educ + married +
                nodegree + race + re74, data = lalonde,
                method = "glm", estimand = "ATT"))

(W2 &lt;- weightit(treat ~ age + educ + married +
                nodegree + race + re74, data = lalonde,
                method = "glm", estimand = "ATT",
                by = "race"))
S &lt;- sbps(W1, W2)
print(S)
summary(S)
bal.tab(S, cluster = "race")

#Could also have run
#  sbps(W1, moderator = "race")

S_ &lt;- sbps(W1, W2, smooth = TRUE)
print(S_)
summary(S_)
bal.tab(S_, cluster = "race")
</code></pre>

<hr>
<h2 id='summary.weightit'>Print and Summarize Output</h2><span id='topic+summary.weightit'></span><span id='topic+plot.summary.weightit'></span><span id='topic+summary.weightitMSM'></span><span id='topic+plot.summary.weightitMSM'></span>

<h3>Description</h3>

<p><code>summary()</code> generates a summary of the <code>weightit</code> or
<code>weightitMSM</code> object to evaluate the properties of the estimated
weights. <code>plot()</code> plots the distribution of the weights.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'weightit'
summary(object, top = 5, ignore.s.weights = FALSE, ...)

## S3 method for class 'summary.weightit'
plot(x, binwidth = NULL, bins = NULL, ...)

## S3 method for class 'weightitMSM'
summary(object, top = 5, ignore.s.weights = FALSE, ...)

## S3 method for class 'summary.weightitMSM'
plot(x, binwidth = NULL, bins = NULL, time = 1, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.weightit_+3A_object">object</code></td>
<td>
<p>a <code>weightit</code> or <code>weightitMSM</code> object; the output of
a call to <code><a href="#topic+weightit">weightit()</a></code> or <code><a href="#topic+weightitMSM">weightitMSM()</a></code>.</p>
</td></tr>
<tr><td><code id="summary.weightit_+3A_top">top</code></td>
<td>
<p>how many of the largest and smallest weights to display. Default
is 5.</p>
</td></tr>
<tr><td><code id="summary.weightit_+3A_ignore.s.weights">ignore.s.weights</code></td>
<td>
<p>whether or not to ignore sampling weights when
computing the weight summary. If <code>FALSE</code>, the default, the estimated
weights will be multiplied by the sampling weights (if any) before values
are computed.</p>
</td></tr>
<tr><td><code id="summary.weightit_+3A_...">...</code></td>
<td>
<p>For <code>plot()</code>, additional arguments passed to <code><a href="graphics.html#topic+hist">graphics::hist()</a></code> to
determine the number of bins, though <code><a href="ggplot2.html#topic+geom_histogram">ggplot2::geom_histogram()</a></code> is actually used to create the plot.</p>
</td></tr>
<tr><td><code id="summary.weightit_+3A_x">x</code></td>
<td>
<p>a <code>summary.weightit</code> or <code>summary.weightitMSM</code> object; the
output of a call to <code>summary.weightit()</code> or
<code>summary.weightitMSM()</code>.</p>
</td></tr>
<tr><td><code id="summary.weightit_+3A_binwidth">binwidth</code>, <code id="summary.weightit_+3A_bins">bins</code></td>
<td>
<p>arguments passed to <code><a href="ggplot2.html#topic+geom_histogram">ggplot2::geom_histogram()</a></code> to
control the size and/or number of bins.</p>
</td></tr>
<tr><td><code id="summary.weightit_+3A_time">time</code></td>
<td>
<p><code>numeric</code>; the time point for which to display the distribution of weights. Default is to plot the distribution for the first time points.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>For point treatments (i.e., <code>weightit</code> objects), a <code>summary.weightit</code> object with the following elements:
</p>
<table>
<tr><td><code>weight.range</code></td>
<td>
<p>The range (minimum and maximum) weight for each treatment group.</p>
</td></tr>
<tr><td><code>weight.top</code></td>
<td>
<p>The units with the greatest weights in each treatment group; how many are included is determined by <code>top</code>.</p>
</td></tr>
<tr><td><code>coef.of.var (Coef of Var)</code></td>
<td>
<p>The coefficient of variation (standard deviation divided by mean) of the weights in each treatment group and overall.</p>
</td></tr>
<tr><td><code>scaled.mad (MAD)</code></td>
<td>
<p>The mean absolute deviation of the weights in each treatment group and overall divided by the mean of the weights in the corresponding group.</p>
</td></tr>
<tr><td><code>negative entropy (Entropy)</code></td>
<td>
<p>The negative entropy (<code class="reqn">\sum w log(w)</code>) of the weights in each treatment group and overall divided by the mean of the weights in the corresponding group.</p>
</td></tr>
<tr><td><code>num.zeros</code></td>
<td>
<p>The number of weights equal to zero.</p>
</td></tr>
<tr><td><code>effective.sample.size</code></td>
<td>
<p>The effective sample size for each treatment group before and after weighting. See <code><a href="#topic+ESS">ESS()</a></code>.</p>
</td></tr>
</table>
<p>For longitudinal treatments (i.e., <code>weightitMSM</code> objects), a list of
the above elements for each treatment period.
</p>
<p><code>plot()</code> returns a <code>ggplot</code> object with a histogram displaying the
distribution of the estimated weights. If the estimand is the ATT or ATC,
only the weights for the non-focal group(s) will be displayed (since the
weights for the focal group are all 1). A dotted line is displayed at the
mean of the weights.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+weightit">weightit()</a></code>, <code><a href="#topic+weightitMSM">weightitMSM()</a></code>, <code><a href="base.html#topic+summary">summary()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# See example at ?weightit or ?weightitMSM

</code></pre>

<hr>
<h2 id='trim'>Trim (Winsorize) Large Weights</h2><span id='topic+trim'></span><span id='topic+trim.weightit'></span><span id='topic+trim.default'></span>

<h3>Description</h3>

<p>Trims (i.e., winsorizes) large weights by setting all weights higher than
that at a given quantile to the weight at the quantile or to 0. This can be useful
in controlling extreme weights, which can reduce effective sample size by
enlarging the variability of the weights. Note that by default, no observations are
fully discarded when using <code>trim()</code>, which may differ from the some
uses of the word &quot;trim&quot; (see the <code>drop</code> argument below).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>trim(x, ...)

## S3 method for class 'weightit'
trim(x, at = 0, lower = FALSE, drop = FALSE, ...)

## Default S3 method:
trim(x, at = 0, lower = FALSE, treat = NULL, drop = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="trim_+3A_x">x</code></td>
<td>
<p>A <code>weightit</code> object or a vector of weights.</p>
</td></tr>
<tr><td><code id="trim_+3A_...">...</code></td>
<td>
<p>Not used.</p>
</td></tr>
<tr><td><code id="trim_+3A_at">at</code></td>
<td>
<p><code>numeric</code>; either the quantile of the weights above which
weights are to be trimmed. A single number between .5 and 1, or the number
of weights to be trimmed (e.g., <code>at = 3</code> for the top 3 weights to be
set to the 4th largest weight).</p>
</td></tr>
<tr><td><code id="trim_+3A_lower">lower</code></td>
<td>
<p><code>logical</code>; whether also to trim at the lower quantile
(e.g., for <code>at = .9</code>, trimming at both .1 and .9, or for <code>at = 3</code>,
trimming the top and bottom 3 weights). Default is <code>FALSE</code> to only trim the higher weights.</p>
</td></tr>
<tr><td><code id="trim_+3A_drop">drop</code></td>
<td>
<p><code>logical</code>; whether to set the weights of the trimmed units to 0 or not. Default is <code>FALSE</code> to retain all trimmed units. Setting to <code>TRUE</code> may change the original targeted estimand when not the ATT or ATC.</p>
</td></tr>
<tr><td><code id="trim_+3A_treat">treat</code></td>
<td>
<p>A vector of treatment status for each unit. This should always
be included when <code>x</code> is numeric, but you can get away with leaving it
out if the treatment is continuous or the estimand is the ATE for binary or
multi-category treatments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>trim()</code> takes in a <code>weightit</code> object (the output of a call to
<code><a href="#topic+weightit">weightit()</a></code> or <code><a href="#topic+weightitMSM">weightitMSM()</a></code>) or a numeric vector of weights and trims
(winsorizes) them to the specified quantile. All weights above that quantile
are set to the weight at that quantile unless <code>drop = TRUE</code>, in which case they are set to 0. If <code>lower = TRUE</code>, all weights
below 1 minus the quantile are trimmed. In
general, trimming weights can decrease balance but also decreases the
variability of the weights, improving precision at the potential expense of
unbiasedness (Cole &amp; Hernán, 2008). See Lee, Lessler, and Stuart (2011) and
Thoemmes and Ong (2015) for discussions and simulation results of trimming
weights at various quantiles. Note that trimming weights can also change the
target population and therefore the estimand.
</p>
<p>When using <code>trim()</code> on a numeric vector of weights, it is helpful to
include the treatment vector as well. The helps determine the type of
treatment and estimand, which are used to specify how trimming is performed.
In particular, if the estimand is determined to be the ATT or ATC, the
weights of the target (i.e., focal) group are ignored, since they should all
be equal to 1. Otherwise, if the estimand is the ATE or the treatment is
continuous, all weights are considered for trimming. In general, weights for
any group for which all the weights are the same will not be considered in
the trimming.
</p>


<h3>Value</h3>

<p>If the input is a <code>weightit</code> object, the output will be a
<code>weightit</code> object with the weights replaced by the trimmed weights (or 0) and
will have an additional attribute, <code>"trim"</code>, equal to the quantile of
trimming.
</p>
<p>If the input is a numeric vector of weights, the output will be a numeric
vector of the trimmed weights, again with the aforementioned attribute.
</p>


<h3>References</h3>

<p>Cole, S. R., &amp; Hernán, M. Á. (2008). Constructing Inverse
Probability Weights for Marginal Structural Models. American Journal of
Epidemiology, 168(6), 656–664.
</p>
<p>Lee, B. K., Lessler, J., &amp; Stuart, E. A. (2011). Weight Trimming and
Propensity Score Weighting. PLoS ONE, 6(3), e18174.
</p>
<p>Thoemmes, F., &amp; Ong, A. D. (2016). A Primer on Inverse Probability of
Treatment Weighting and Marginal Structural Models. Emerging Adulthood,
4(1), 40–59.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+weightit">weightit()</a></code>, <code><a href="#topic+weightitMSM">weightitMSM()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library("cobalt")
data("lalonde", package = "cobalt")

(W &lt;- weightit(treat ~ age + educ + married +
                 nodegree + re74, data = lalonde,
               method = "glm", estimand = "ATT"))
summary(W)

#Trimming the top and bottom 5 weights
trim(W, at = 5, lower = TRUE)

#Trimming at 90th percentile
(W.trim &lt;- trim(W, at = .9))

summary(W.trim)
#Note that only the control weights were trimmed

#Trimming a numeric vector of weights
all.equal(trim(W$weights, at = .9, treat = lalonde$treat),
          W.trim$weights)

#Dropping trimmed units
(W.trim &lt;- trim(W, at = .9, drop = TRUE))

summary(W.trim)
#Note that we now have zeros in the control group

#Using made up data and as.weightit()
treat &lt;- rbinom(500, 1, .3)
weights &lt;- rchisq(500, df = 2)
W &lt;- as.weightit(weights, treat = treat,
                 estimand = "ATE")
summary(W)
summary(trim(W, at = .95))
</code></pre>

<hr>
<h2 id='weightit'>Estimate Balancing Weights</h2><span id='topic+weightit'></span>

<h3>Description</h3>

<p><code>weightit()</code> allows for the easy generation of balancing weights using
a variety of available methods for binary, continuous, and multi-category
treatments. Many of these methods exist in other packages, which
<code>weightit()</code> calls; these packages must be installed to use the desired
method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>weightit(
  formula,
  data = NULL,
  method = "glm",
  estimand = "ATE",
  stabilize = FALSE,
  focal = NULL,
  by = NULL,
  s.weights = NULL,
  ps = NULL,
  moments = NULL,
  int = FALSE,
  subclass = NULL,
  missing = NULL,
  verbose = FALSE,
  include.obj = FALSE,
  keep.mparts = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="weightit_+3A_formula">formula</code></td>
<td>
<p>a formula with a treatment variable on the left hand side and
the covariates to be balanced on the right hand side. See <code><a href="stats.html#topic+glm">glm()</a></code> for more
details. Interactions and functions of covariates are allowed.</p>
</td></tr>
<tr><td><code id="weightit_+3A_data">data</code></td>
<td>
<p>an optional data set in the form of a data frame that contains
the variables in <code>formula</code>.</p>
</td></tr>
<tr><td><code id="weightit_+3A_method">method</code></td>
<td>
<p>a string of length 1 containing the name of the method that
will be used to estimate weights. See Details below for allowable options.
The default is <code>"glm"</code> for propensity score weighting using a
generalized linear model to estimate the propensity score.</p>
</td></tr>
<tr><td><code id="weightit_+3A_estimand">estimand</code></td>
<td>
<p>the desired estimand. For binary and multi-category
treatments, can be <code>"ATE"</code>, <code>"ATT"</code>, <code>"ATC"</code>, and, for some
methods, <code>"ATO"</code>, <code>"ATM"</code>, or <code>"ATOS"</code>. The default for both
is <code>"ATE"</code>. This argument is ignored for continuous treatments. See the
individual pages for each method for more information on which estimands are
allowed with each method and what literature to read to interpret these
estimands.</p>
</td></tr>
<tr><td><code id="weightit_+3A_stabilize">stabilize</code></td>
<td>
<p><code>logical</code>; whether or not to stabilize the weights.
For the methods that involve estimating propensity scores, this involves
multiplying each unit's weight by the proportion of units in their treatment
group. Default is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="weightit_+3A_focal">focal</code></td>
<td>
<p>when multi-category treatments are used and ATT weights are
requested, which group to consider the &quot;treated&quot; or focal group. This group
will not be weighted, and the other groups will be weighted to be more like
the focal group. If specified, <code>estimand</code> will automatically be set to
<code>"ATT"</code>.</p>
</td></tr>
<tr><td><code id="weightit_+3A_by">by</code></td>
<td>
<p>a string containing the name of the variable in <code>data</code> for
which weighting is to be done within categories or a one-sided formula with
the stratifying variable on the right-hand side. For example, if <code>by = "gender"</code> or <code>by = ~gender</code>, a separate propensity score model or optimization will occur within each level of the variable <code>"gender"</code>. (The argument used to be
called <code>exact</code>, which will still work but with a message.) Only one
<code>by</code> variable is allowed; to stratify by multiply variables
simultaneously, create a new variable that is a full cross of those
variables using <code><a href="base.html#topic+interaction">interaction()</a></code>.</p>
</td></tr>
<tr><td><code id="weightit_+3A_s.weights">s.weights</code></td>
<td>
<p>A vector of sampling weights or the name of a variable in
<code>data</code> that contains sampling weights. These can also be matching
weights if weighting is to be used on matched data. See the individual pages
for each method for information on whether sampling weights can be supplied.</p>
</td></tr>
<tr><td><code id="weightit_+3A_ps">ps</code></td>
<td>
<p>A vector of propensity scores or the name of a variable in
<code>data</code> containing propensity scores. If not <code>NULL</code>, <code>method</code>
is ignored unless it is a user-supplied function, and the propensity scores will be used to create weights.
<code>formula</code> must include the treatment variable in <code>data</code>, but the
listed covariates will play no role in the weight estimation. Using
<code>ps</code> is similar to calling <code><a href="#topic+get_w_from_ps">get_w_from_ps()</a></code> directly, but produces a
full <code>weightit</code> object rather than just producing weights.</p>
</td></tr>
<tr><td><code id="weightit_+3A_moments">moments</code></td>
<td>
<p><code>numeric</code>; for some methods, the greatest power of each
covariate to be balanced. For example, if <code>moments = 3</code>, for each
non-categorical covariate, the covariate, its square, and its cube will be
balanced. This argument is ignored for other methods; to balance powers of
the covariates, appropriate functions must be entered in <code>formula</code>. See
the individual pages for each method for information on whether they accept
<code>moments</code>.</p>
</td></tr>
<tr><td><code id="weightit_+3A_int">int</code></td>
<td>
<p><code>logical</code>; for some methods, whether first-order
interactions of the covariates are to be balanced. This argument is ignored
for other methods; to balance interactions between the variables,
appropriate functions must be entered in <code>formula</code>. See the individual
pages for each method for information on whether they accept <code>int</code>.</p>
</td></tr>
<tr><td><code id="weightit_+3A_subclass">subclass</code></td>
<td>
<p><code>numeric</code>; the number of subclasses to use for
computing weights using marginal mean weighting with subclasses (MMWS). If
<code>NULL</code>, standard inverse probability weights (and their extensions)
will be computed; if a number greater than 1, subclasses will be formed and
weights will be computed based on subclass membership. Attempting to set a
non-<code>NULL</code> value for methods that don't compute a propensity score will
result in an error; see each method's help page for information on whether
MMWS weights are compatible with the method. See <code><a href="#topic+get_w_from_ps">get_w_from_ps()</a></code> for
details and references.</p>
</td></tr>
<tr><td><code id="weightit_+3A_missing">missing</code></td>
<td>
<p><code>character</code>; how missing data should be handled. The
options and defaults depend on the <code>method</code> used. Ignored if no missing
data is present. It should be noted that multiple imputation outperforms all
available missingness methods available in <code>weightit()</code> and should
probably be used instead. Consider the <a href="https://CRAN.R-project.org/package=MatchThem"><span class="pkg">MatchThem</span></a>
package for the use of <code>weightit()</code> with multiply imputed data.</p>
</td></tr>
<tr><td><code id="weightit_+3A_verbose">verbose</code></td>
<td>
<p><code>logical</code>; whether to print additional information
output by the fitting function.</p>
</td></tr>
<tr><td><code id="weightit_+3A_include.obj">include.obj</code></td>
<td>
<p><code>logical</code>; whether to include in the output any fit
objects created in the process of estimating the weights. For example, with
<code>method = "glm"</code>, the <code>glm</code> objects containing the propensity
score model will be included. See the individual pages for each method for
information on what object will be included if <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="weightit_+3A_keep.mparts">keep.mparts</code></td>
<td>
<p><code>logical</code>; whether to include in the output components necessary to estimate standard errors that account for estimation of the weights in <code><a href="#topic+glm_weightit">glm_weightit()</a></code>. Default is <code>TRUE</code> if such parts are present. See the individual pages for each method for whether these components are produced. Set to <code>FALSE</code> to keep the output object smaller, e.g., if standard errors will not be computed using <code>glm_weightit()</code>.</p>
</td></tr>
<tr><td><code id="weightit_+3A_...">...</code></td>
<td>
<p>other arguments for functions called by <code>weightit()</code> that
control aspects of fitting that are not covered by the above arguments. See Details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The primary purpose of <code>weightit()</code> is as a dispatcher to functions
that perform the estimation of balancing weights using the requested
<code>method</code>. Below are the methods allowed and links to pages containing
more information about them, including additional arguments and outputs
(e.g., when <code>include.obj = TRUE</code>), how missing values are treated,
which estimands are allowed, and whether sampling weights are allowed.</p>

<table>
<tr>
 <td style="text-align: left;">
   <code><a href="#topic+method_glm">&quot;glm&quot;</a></code> </td><td style="text-align: left;"> Propensity score weighting using generalized linear models </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code><a href="#topic+method_gbm">&quot;gbm&quot;</a></code> </td><td style="text-align: left;"> Propensity score weighting using generalized boosted modeling </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code><a href="#topic+method_cbps">&quot;cbps&quot;</a></code> </td><td style="text-align: left;"> Covariate Balancing Propensity Score weighting </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code><a href="#topic+method_npcbps">&quot;npcbps&quot;</a></code> </td><td style="text-align: left;"> Non-parametric Covariate Balancing Propensity Score weighting </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code><a href="#topic+method_ebal">&quot;ebal&quot;</a></code> </td><td style="text-align: left;"> Entropy balancing </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code><a href="#topic+method_ipt">&quot;ipt&quot;</a></code> </td><td style="text-align: left;"> Inverse probability tilting </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code><a href="#topic+method_optweight">&quot;optweight&quot;</a></code> </td><td style="text-align: left;"> Optimization-based weighting </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code><a href="#topic+method_super">&quot;super&quot;</a></code> </td><td style="text-align: left;"> Propensity score weighting using SuperLearner </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code><a href="#topic+method_bart">&quot;bart&quot;</a></code> </td><td style="text-align: left;"> Propensity score weighting using Bayesian additive regression trees (BART) </td>
</tr>
<tr>
 <td style="text-align: left;">
   <code><a href="#topic+method_energy">&quot;energy&quot;</a></code> </td><td style="text-align: left;"> Energy balancing </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>

<p><code>method</code> can also be supplied as a user-defined function; see
<code><a href="#topic+method_user">method_user</a></code> for instructions and examples.
</p>
<p>When using <code>weightit()</code>, please cite both the <span class="pkg">WeightIt</span> package
(using <code>citation("WeightIt")</code>) and the paper(s) in the references
section of the method used.
</p>


<h3>Value</h3>

<p>A <code>weightit</code> object with the following elements:
</p>
<table>
<tr><td><code>weights</code></td>
<td>
<p>The estimated weights, one for each unit.</p>
</td></tr>
<tr><td><code>treat</code></td>
<td>
<p>The values of the treatment variable.</p>
</td></tr>
<tr><td><code>covs</code></td>
<td>
<p>The covariates used in the fitting. Only includes the raw covariates, which may have been altered in
the fitting process.</p>
</td></tr>
<tr><td><code>estimand</code></td>
<td>
<p>The estimand requested.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>The weight estimation method specified.</p>
</td></tr>
<tr><td><code>ps</code></td>
<td>
<p>The estimated or provided propensity scores. Estimated propensity scores are
returned for binary treatments and only when <code>method</code> is <code>"glm"</code>, <code>"gbm"</code>, <code>"cbps"</code>, <code>"ipt"</code>, <code>"super"</code>, or <code>"bart"</code>.</p>
</td></tr>
<tr><td><code>s.weights</code></td>
<td>
<p>The provided sampling weights.</p>
</td></tr>
<tr><td><code>focal</code></td>
<td>
<p>The focal treatment level if the ATT or ATC was requested.</p>
</td></tr>
<tr><td><code>by</code></td>
<td>
<p>A data.frame containing the <code>by</code> variable when specified.</p>
</td></tr>
<tr><td><code>obj</code></td>
<td>
<p>When <code>include.obj = TRUE</code>, the fit object.</p>
</td></tr>
<tr><td><code>info</code></td>
<td>
<p>Additional information about the fitting. See the individual
methods pages for what is included.</p>
</td></tr>
</table>
<p>When <code>keep.mparts</code> is <code>TRUE</code> (the default) and the chosen method is compatible with M-estimation, the components related to M-estimation for use in <code><a href="#topic+glm_weightit">glm_weightit()</a></code> are stored in the <code>"Mparts"</code> attribute. When <code>by</code> is specified, <code>keep.mparts</code> is set to <code>FALSE</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+weightitMSM">weightitMSM()</a></code> for estimating weights with sequential (i.e.,
longitudinal) treatments for use in estimating marginal structural models
(MSMs).
</p>
<p><code><a href="#topic+weightit.fit">weightit.fit()</a></code>, which is a lower-level dispatcher function that accepts a
matrix of covariates and a vector of treatment statuses rather than a
formula and data frame and performs minimal argument checking and
processing. It may be useful for speeding up simulation studies for which
the correct arguments are known. In general <code>weightit()</code> should be
used.
</p>
<p><code><a href="#topic+summary.weightit">summary.weightit()</a></code> for summarizing the weights
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library("cobalt")
data("lalonde", package = "cobalt")

#Balancing covariates between treatment groups (binary)
(W1 &lt;- weightit(treat ~ age + educ + married +
                  nodegree + re74, data = lalonde,
                method = "glm", estimand = "ATT"))
summary(W1)
bal.tab(W1)

#Balancing covariates with respect to race (multi-category)
(W2 &lt;- weightit(race ~ age + educ + married +
                  nodegree + re74, data = lalonde,
                method = "ebal", estimand = "ATE"))
summary(W2)
bal.tab(W2)


#Balancing covariates with respect to re75 (continuous)
(W3 &lt;- weightit(re75 ~ age + educ + married +
                  nodegree + re74, data = lalonde,
                method = "cbps", over = FALSE))
summary(W3)
bal.tab(W3)

</code></pre>

<hr>
<h2 id='weightit.fit'>Generate Balancing Weights with Minimal Input Processing</h2><span id='topic+weightit.fit'></span>

<h3>Description</h3>

<p><code>weightit.fit()</code> dispatches one of the weight estimation methods
determined by <code>method</code>. It is an internal function called by
<code><a href="#topic+weightit">weightit()</a></code> and should probably not be used except in special cases. Unlike
<code>weightit()</code>, <code>weightit.fit()</code> does not accept a formula and data
frame interface and instead requires the covariates and treatment to be
supplied as a numeric matrix and atomic vector, respectively. In this way,
<code>weightit.fit()</code> is to <code>weightit()</code> what <code><a href="stats.html#topic+lm.fit">lm.fit()</a></code> is to <code><a href="stats.html#topic+lm">lm()</a></code> -
a thinner, slightly faster interface that performs minimal argument
checking.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>weightit.fit(
  covs,
  treat,
  method = "glm",
  s.weights = NULL,
  by.factor = NULL,
  estimand = "ATE",
  focal = NULL,
  stabilize = FALSE,
  ps = NULL,
  moments = NULL,
  int = FALSE,
  subclass = NULL,
  is.MSM.method = FALSE,
  missing = NULL,
  verbose = FALSE,
  include.obj = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="weightit.fit_+3A_covs">covs</code></td>
<td>
<p>a numeric matrix of covariates.</p>
</td></tr>
<tr><td><code id="weightit.fit_+3A_treat">treat</code></td>
<td>
<p>a vector of treatment statuses.</p>
</td></tr>
<tr><td><code id="weightit.fit_+3A_method">method</code></td>
<td>
<p>a string of length 1 containing the name of the method that
will be used to estimate weights. See <code><a href="#topic+weightit">weightit()</a></code> for allowable options.
The default is <code>"glm"</code> for propensity score weighting using a
generalized linear model to estimate the propensity score.</p>
</td></tr>
<tr><td><code id="weightit.fit_+3A_s.weights">s.weights</code></td>
<td>
<p>a numeric vector of sampling weights. See the individual
pages for each method for information on whether sampling weights can be
supplied.</p>
</td></tr>
<tr><td><code id="weightit.fit_+3A_by.factor">by.factor</code></td>
<td>
<p>a factor variable for which weighting is to be done within
levels. Corresponds to the <code>by</code> argument in <code><a href="#topic+weightit">weightit()</a></code>.</p>
</td></tr>
<tr><td><code id="weightit.fit_+3A_estimand">estimand</code></td>
<td>
<p>the desired estimand. For binary and multi-category
treatments, can be &quot;ATE&quot;, &quot;ATT&quot;, &quot;ATC&quot;, and, for some methods, &quot;ATO&quot;, &quot;ATM&quot;,
or &quot;ATOS&quot;. The default for both is &quot;ATE&quot;. This argument is ignored for
continuous treatments. See the individual pages for each method for more
information on which estimands are allowed with each method and what
literature to read to interpret these estimands.</p>
</td></tr>
<tr><td><code id="weightit.fit_+3A_focal">focal</code></td>
<td>
<p>when multi-category treatments are used and ATT weights are
requested, which group to consider the &quot;treated&quot; or focal group. This group
will not be weighted, and the other groups will be weighted to be more like
the focal group. Must be non-<code>NULL</code> if <code>estimand = "ATT"</code> or
<code>"ATC"</code>.</p>
</td></tr>
<tr><td><code id="weightit.fit_+3A_stabilize">stabilize</code></td>
<td>
<p><code>logical</code>; whether or not to stabilize the weights.
For the methods that involve estimating propensity scores, this involves
multiplying each unit's weight by the proportion of units in their treatment
group. Default is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="weightit.fit_+3A_ps">ps</code></td>
<td>
<p>a vector of propensity scores. If specified, <code>method</code> will be
ignored and set to <code>"glm"</code>.</p>
</td></tr>
<tr><td><code id="weightit.fit_+3A_moments">moments</code>, <code id="weightit.fit_+3A_int">int</code>, <code id="weightit.fit_+3A_subclass">subclass</code></td>
<td>
<p>arguments to customize the weight estimation.
See <code><a href="#topic+weightit">weightit()</a></code> for details.</p>
</td></tr>
<tr><td><code id="weightit.fit_+3A_is.msm.method">is.MSM.method</code></td>
<td>
<p>see <code><a href="#topic+weightitMSM">weightitMSM()</a></code>. Typically can be ignored.</p>
</td></tr>
<tr><td><code id="weightit.fit_+3A_missing">missing</code></td>
<td>
<p><code>character</code>; how missing data should be handled. The
options depend on the <code>method</code> used. If <code>NULL</code>, <code>covs</code> covs
will be checked for <code>NA</code> values, and if present, <code>missing</code> will be
set to <code>"ind"</code>. If <code>""</code>, <code>covs</code> covs will not be checked for
<code>NA</code> values; this can be faster when it is known there are none.</p>
</td></tr>
<tr><td><code id="weightit.fit_+3A_verbose">verbose</code></td>
<td>
<p>whether to print additional information output by the fitting
function.</p>
</td></tr>
<tr><td><code id="weightit.fit_+3A_include.obj">include.obj</code></td>
<td>
<p>whether to include in the output any fit objects created
in the process of estimating the weights. For example, with <code>method = "glm"</code>, the <code>glm</code> objects containing the propensity score model will be
included. See the individual pages for each method for information on what
object will be included if <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="weightit.fit_+3A_...">...</code></td>
<td>
<p>other arguments for functions called by <code>weightit.fit()</code>
that control aspects of fitting that are not covered by the above arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>weightit.fit()</code> is called by <code><a href="#topic+weightit">weightit()</a></code> after the arguments to
<code>weightit()</code> have been checked and processed. <code>weightit.fit()</code>
dispatches the function used to actually estimate the weights, passing on
the supplied arguments directly. <code>weightit.fit()</code> is not meant to be
used by anyone other than experienced users who have a specific use case in
mind. The returned object contains limited information about the supplied
arguments or details of the estimation method; all that is processed by
<code>weightit()</code>.
</p>
<p>Less argument checking or processing occurs in <code>weightit.fit()</code> than
does in <code>weightit()</code>, which means supplying incorrect arguments can
result in errors, crashes, and invalid weights, and error and warning
messages may not be helpful in diagnosing the problem. <code>weightit.fit()</code>
does check to make sure weights were actually estimated, though.
</p>
<p><code>weightit.fit()</code> may be most useful in speeding up simulation
simulation studies that use <code>weightit()</code> because the covariates can be
supplied as a numeric matrix, which is often how they are generated in
simulations, without having to go through the potentially slow process of
extracting the covariates and treatment from a formula and data frame. If
the user is certain the arguments are valid (e.g., by ensuring the estimated
weights are consistent with those estimated from <code>weightit()</code> with the
same arguments), less time needs to be spent on processing the arguments.
Also, the returned object is much smaller than a <code>weightit</code> object
because the covariates are not returned alongside the weights.
</p>


<h3>Value</h3>

<p>A <code>weightit.fit</code> object with the following elements:
</p>
<table>
<tr><td><code>weights</code></td>
<td>
<p>The estimated weights, one for each unit.</p>
</td></tr>
<tr><td><code>treat</code></td>
<td>
<p>The values of the treatment variable.</p>
</td></tr>
<tr><td><code>estimand</code></td>
<td>
<p>The estimand requested. ATC is recoded as ATT.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>The weight estimation method specified.</p>
</td></tr>
<tr><td><code>ps</code></td>
<td>
<p>The estimated or provided propensity scores. Estimated propensity scores are returned for binary treatments and only when <code>method</code> is <code>"glm"</code>, <code>"gbm"</code>, <code>"cbps"</code>, <code>"super"</code>, or <code>"bart"</code>.</p>
</td></tr>
<tr><td><code>s.weights</code></td>
<td>
<p>The provided sampling weights.</p>
</td></tr>
<tr><td><code>focal</code></td>
<td>
<p>The focal treatment level if the ATT or ATC was requested.</p>
</td></tr>
<tr><td><code>fit.obj</code></td>
<td>
<p>When <code>include.obj = TRUE</code>, the fit object.</p>
</td></tr>
<tr><td><code>info</code></td>
<td>
<p>Additional information about the fitting. See the individual methods pages for what is included.</p>
</td></tr>
</table>
<p>The <code>weightit.fit</code> object does not have specialized <code>print()</code>,
<code>summary()</code>, or <code>plot()</code> methods. It is simply a list containing
the above components. Use <code><a href="#topic+as.weightit">as.weightit()</a></code> to convert it to a <code>weightit</code> object, which does have these methods. See Examples.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+weightit">weightit()</a></code>, which you should use for estimating weights unless
you know better.
</p>
<p><code><a href="#topic+as.weightit">as.weightit()</a></code> for converting a <code>weightit.fit</code> object to a <code>weightit</code> object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library("cobalt")
data("lalonde", package = "cobalt")

# Balancing covariates between treatment groups (binary)
covs &lt;- lalonde[c("age", "educ", "race", "married",
                  "nodegree", "re74", "re75")]
## Create covs matrix, splitting any factors using
## cobalt::splitfactor()
covs_mat &lt;- as.matrix(splitfactor(covs))

WF1 &lt;- weightit.fit(covs_mat, treat = lalonde$treat,
                    method = "glm", estimand = "ATT")
str(WF1)

# Converting to a weightit object for use with
# summary() and bal.tab()
W1 &lt;- as.weightit(WF1, covs = covs)
W1
summary(W1)
bal.tab(W1)

</code></pre>

<hr>
<h2 id='weightitMSM'>Generate Balancing Weights for Longitudinal Treatments</h2><span id='topic+weightitMSM'></span>

<h3>Description</h3>

<p><code>weightitMSM()</code> allows for the easy generation of balancing weights for
marginal structural models for time-varying treatments using a variety of
available methods for binary, continuous, and multi-category treatments. Many
of these methods exist in other packages, which <code><a href="#topic+weightit">weightit()</a></code> calls; these
packages must be installed to use the desired method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>weightitMSM(
  formula.list,
  data = NULL,
  method = "glm",
  stabilize = FALSE,
  by = NULL,
  s.weights = NULL,
  num.formula = NULL,
  moments = NULL,
  int = FALSE,
  missing = NULL,
  verbose = FALSE,
  include.obj = FALSE,
  keep.mparts = TRUE,
  is.MSM.method,
  weightit.force = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="weightitMSM_+3A_formula.list">formula.list</code></td>
<td>
<p>a list of formulas corresponding to each time point with
the time-specific treatment variable on the left hand side and pre-treatment
covariates to be balanced on the right hand side. The formulas must be in
temporal order, and must contain all covariates to be balanced at that time
point (i.e., treatments and covariates featured in early formulas should
appear in later ones). Interactions and functions of covariates are allowed.</p>
</td></tr>
<tr><td><code id="weightitMSM_+3A_data">data</code></td>
<td>
<p>an optional data set in the form of a data frame that contains
the variables in the formulas in <code>formula.list</code>. This must be a wide
data set with exactly one row per unit.</p>
</td></tr>
<tr><td><code id="weightitMSM_+3A_method">method</code></td>
<td>
<p>a string of length 1 containing the name of the method that
will be used to estimate weights. See <code><a href="#topic+weightit">weightit()</a></code> for allowable options.
The default is <code>"glm"</code>, which estimates the weights using generalized
linear models.</p>
</td></tr>
<tr><td><code id="weightitMSM_+3A_stabilize">stabilize</code></td>
<td>
<p><code>logical</code>; whether or not to stabilize the weights.
Stabilizing the weights involves fitting a model predicting treatment at
each time point from treatment status at prior time points. If <code>TRUE</code>,
a fully saturated model will be fit (i.e., all interactions between all
treatments up to each time point), essentially using the observed treatment
probabilities in the numerator (for binary and multi-category treatments). This
may yield an error if some combinations are not observed. Default is
<code>FALSE</code>. To manually specify stabilization model formulas, e.g., to
specify non-saturated models, use <code>num.formula</code>. With many time points,
saturated models may be time-consuming or impossible to fit.</p>
</td></tr>
<tr><td><code id="weightitMSM_+3A_by">by</code></td>
<td>
<p>a string containing the name of the variable in <code>data</code> for
which weighting is to be done within categories or a one-sided formula with
the stratifying variable on the right-hand side. For example, if <code>by = "gender"</code> or <code>by = ~gender</code>, a separate propensity score model or optimization will occur within each level of the variable <code>"gender"</code>. (The argument used to be
called <code>exact</code>, which will still work but with a message.) Only one
<code>by</code> variable is allowed; to stratify by multiply variables
simultaneously, create a new variable that is a full cross of those
variables using <code><a href="base.html#topic+interaction">interaction()</a></code>.</p>
</td></tr>
<tr><td><code id="weightitMSM_+3A_s.weights">s.weights</code></td>
<td>
<p>A vector of sampling weights or the name of a variable in
<code>data</code> that contains sampling weights. These can also be matching
weights if weighting is to be used on matched data. See the individual pages
for each method for information on whether sampling weights can be supplied.</p>
</td></tr>
<tr><td><code id="weightitMSM_+3A_num.formula">num.formula</code></td>
<td>
<p>optional; a one-sided formula with the stabilization
factors (other than the previous treatments) on the right hand side, which
adds, for each time point, the stabilization factors to a model saturated
with previous treatments. See Cole &amp; Hernán (2008) for a discussion of how
to specify this model; including stabilization factors can change the
estimand without proper adjustment, and should be done with caution. Can
also be a list of one-sided formulas, one for each time point. Unless you
know what you are doing, we recommend setting <code>stabilize = TRUE</code> and
ignoring <code>num.formula</code>.</p>
</td></tr>
<tr><td><code id="weightitMSM_+3A_moments">moments</code></td>
<td>
<p><code>numeric</code>; for some methods, the greatest power of each
covariate to be balanced. For example, if <code>moments = 3</code>, for each
non-categorical covariate, the covariate, its square, and its cube will be
balanced. This argument is ignored for other methods; to balance powers of
the covariates, appropriate functions must be entered in <code>formula</code>. See
the individual pages for each method for information on whether they accept
<code>moments</code>.</p>
</td></tr>
<tr><td><code id="weightitMSM_+3A_int">int</code></td>
<td>
<p><code>logical</code>; for some methods, whether first-order
interactions of the covariates are to be balanced. This argument is ignored
for other methods; to balance interactions between the variables,
appropriate functions must be entered in <code>formula</code>. See the individual
pages for each method for information on whether they accept <code>int</code>.</p>
</td></tr>
<tr><td><code id="weightitMSM_+3A_missing">missing</code></td>
<td>
<p><code>character</code>; how missing data should be handled. The
options and defaults depend on the <code>method</code> used. Ignored if no missing
data is present. It should be noted that multiple imputation outperforms all
available missingness methods available in <code>weightit()</code> and should
probably be used instead. Consider the <a href="https://CRAN.R-project.org/package=MatchThem"><span class="pkg">MatchThem</span></a>
package for the use of <code>weightit()</code> with multiply imputed data.</p>
</td></tr>
<tr><td><code id="weightitMSM_+3A_verbose">verbose</code></td>
<td>
<p><code>logical</code>; whether to print additional information
output by the fitting function.</p>
</td></tr>
<tr><td><code id="weightitMSM_+3A_include.obj">include.obj</code></td>
<td>
<p>whether to include in the output a list of the fit
objects created in the process of estimating the weights at each time point.
For example, with <code>method = "glm"</code>, a list of the <code>glm</code> objects
containing the propensity score models at each time point will be included.
See the help pages for each method for information on what object will be
included if <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="weightitMSM_+3A_keep.mparts">keep.mparts</code></td>
<td>
<p><code>logical</code>; whether to include in the output components necessary to estimate standard errors that account for estimation of the weights in <code><a href="#topic+glm_weightit">glm_weightit()</a></code>. Default is <code>TRUE</code> if such parts are present. See the individual pages for each method for whether these components are produced. Set to <code>FALSE</code> to keep the output object smaller, e.g., if standard errors will not be computed using <code>glm_weightit()</code>.</p>
</td></tr>
<tr><td><code id="weightitMSM_+3A_is.msm.method">is.MSM.method</code></td>
<td>
<p>whether the method estimates weights for multiple time
points all at once (<code>TRUE</code>) or by estimating weights at each time point
and then multiplying them together (<code>FALSE</code>). This is only relevant for
<code>method = "optweight"</code>, which estimates weights for longitudinal
treatments all at once, and for user-specified functions.</p>
</td></tr>
<tr><td><code id="weightitMSM_+3A_weightit.force">weightit.force</code></td>
<td>
<p>several methods are not valid for estimating weights
with longitudinal treatments, and will produce an error message if
attempted. Set to <code>TRUE</code> to bypass this error message.</p>
</td></tr>
<tr><td><code id="weightitMSM_+3A_...">...</code></td>
<td>
<p>other arguments for functions called by <code>weightit()</code> that
control aspects of fitting that are not covered by the above arguments. See
Details at <code><a href="#topic+weightit">weightit()</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Currently only &quot;wide&quot; data sets, where each row corresponds to a unit's
entire variable history, are supported. You can use <code><a href="stats.html#topic+reshape">reshape()</a></code> or other
functions to transform your data into this format; see example below.
</p>
<p>In general, <code>weightitMSM()</code> works by separating the estimation of
weights into separate procedures for each time period based on the formulas
provided. For each formula, <code>weightitMSM()</code> simply calls
<code>weightit()</code> to that formula, collects the weights for each time
period, and multiplies them together to arrive at longitudinal balancing
weights.
</p>
<p>Each formula should contain all the covariates to be balanced on. For
example, the formula corresponding to the second time period should contain
all the baseline covariates, the treatment variable at the first time
period, and the time-varying covariates that took on values after the first
treatment and before the second. Currently, only wide data sets are
supported, where each unit is represented by exactly one row that contains
the covariate and treatment history encoded in separate variables.
</p>
<p>The <code>"cbps"</code> method, which calls <code>CBPS()</code> in <span class="pkg">CBPS</span>, will
yield different results from <code>CBMSM()</code> in <span class="pkg">CBPS</span> because
<code>CBMSM()</code> takes a different approach to generating weights than simply
estimating several time-specific models.
</p>


<h3>Value</h3>

<p>A <code>weightitMSM</code> object with the following elements:
</p>
<table>
<tr><td><code>weights</code></td>
<td>
<p>The estimated weights, one for each unit.</p>
</td></tr>
<tr><td><code>treat.list</code></td>
<td>
<p>A list of the values of the time-varying treatment variables.</p>
</td></tr>
<tr><td><code>covs.list</code></td>
<td>
<p>A list of the covariates used in the fitting at each time point. Only includes the raw covariates, which may have been altered in the fitting process.</p>
</td></tr>
<tr><td><code>data</code></td>
<td>
<p>The data.frame originally entered to <code>weightitMSM()</code>.</p>
</td></tr>
<tr><td><code>estimand</code></td>
<td>
<p>&quot;ATE&quot;, currently the only estimand for MSMs with binary or multi-category treatments.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>The weight estimation method specified.</p>
</td></tr>
<tr><td><code>ps.list</code></td>
<td>
<p>A list of the estimated propensity scores (if any) at each time point.</p>
</td></tr>
<tr><td><code>s.weights</code></td>
<td>
<p>The provided sampling weights.</p>
</td></tr>
<tr><td><code>by</code></td>
<td>
<p>A data.frame containing the <code>by</code> variable when specified.</p>
</td></tr>
<tr><td><code>stabilization</code></td>
<td>
<p>The stabilization factors, if any.</p>
</td></tr>
</table>
<p>When <code>keep.mparts</code> is <code>TRUE</code> (the default) and the chosen method is compatible with M-estimation, the components related to M-estimation for use in <code><a href="#topic+glm_weightit">glm_weightit()</a></code> are stored in the <code>"Mparts.list"</code> attribute. When <code>by</code> is specified, <code>keep.mparts</code> is set to <code>FALSE</code>.
</p>


<h3>References</h3>

<p>Cole, S. R., &amp; Hernán, M. A. (2008). Constructing Inverse
Probability Weights for Marginal Structural Models. American Journal of
Epidemiology, 168(6), 656–664. <a href="https://doi.org/10.1093/aje/kwn164">doi:10.1093/aje/kwn164</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+weightit">weightit()</a></code> for information on the allowable methods
</p>
<p><code><a href="#topic+summary.weightitMSM">summary.weightitMSM()</a></code> for summarizing the weights
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library("cobalt")

data("msmdata")
(W1 &lt;- weightitMSM(list(A_1 ~ X1_0 + X2_0,
                        A_2 ~ X1_1 + X2_1 +
                          A_1 + X1_0 + X2_0,
                        A_3 ~ X1_2 + X2_2 +
                          A_2 + X1_1 + X2_1 +
                          A_1 + X1_0 + X2_0),
                   data = msmdata,
                   method = "glm"))
summary(W1)
bal.tab(W1)

#Using stabilization factors
W2 &lt;- weightitMSM(list(A_1 ~ X1_0 + X2_0,
                        A_2 ~ X1_1 + X2_1 +
                          A_1 + X1_0 + X2_0,
                        A_3 ~ X1_2 + X2_2 +
                          A_2 + X1_1 + X2_1 +
                          A_1 + X1_0 + X2_0),
                   data = msmdata,
                   method = "glm",
                   stabilize = TRUE,
                   num.formula = list(~ 1,
                                      ~ A_1,
                                      ~ A_1 + A_2))

#Same as above but with fully saturated stabilization factors
#(i.e., making the last entry in 'num.formula' A_1*A_2)
W3 &lt;- weightitMSM(list(A_1 ~ X1_0 + X2_0,
                        A_2 ~ X1_1 + X2_1 +
                          A_1 + X1_0 + X2_0,
                        A_3 ~ X1_2 + X2_2 +
                          A_2 + X1_1 + X2_1 +
                          A_1 + X1_0 + X2_0),
                   data = msmdata,
                   method = "glm",
                   stabilize = TRUE)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
