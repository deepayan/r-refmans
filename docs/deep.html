<!DOCTYPE html><html><head><title>Help for package deep</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {deep}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#deep'><p>deep: A Neural Networks Framework</p></a></li>
<li><a href='#McCullochPitts-class'><p>The McCullochPitts neuron class, that implements the logic of the</p>
McCullochPitts neuron model.</a></li>
<li><a href='#McCullochPittsLayer-class'><p>The McCullochPittsLayer class, that implements a layer of McCullochPitts</p>
neurons.</a></li>
<li><a href='#NeuralNetwork-class'><p>The main NeuralNetwork class, that holds the layers.</p></a></li>
<li><a href='#Perceptron-class'><p>The Perceptron neuron class, that implements the logic of the perceptron</p>
model.</a></li>
<li><a href='#PerceptronLayer-class'><p>The PerceptronLayer class, that implements a layer of Perceptron neurons.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>A Neural Networks Framework</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.0</td>
</tr>
<tr>
<td>Author:</td>
<td>Brian Lee Mayer</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Brian &lt;bleemayer@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Explore neural networks in a layer oriented way, the framework
    is intended to give the user total control of the
    internals of a net without much effort. Use classes like PerceptronLayer
    to create a layer of Percetron neurons, and specify how many you want. The
    package does all the tricky stuff internally leaving you focused in what
    you want. I wrote this package during a neural networks course to help me
    with the problem set.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>Imports:</td>
<td>methods</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>6.1.1</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2019-12-16 00:43:56 UTC; rstudio</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2019-12-20 11:50:03 UTC</td>
</tr>
</table>
<hr>
<h2 id='deep'>deep: A Neural Networks Framework</h2><span id='topic+deep'></span><span id='topic+deep-package'></span>

<h3>Description</h3>

<p>The deep package provides classes for layers, types of neurons and the
neural network as a whole.
</p>

<hr>
<h2 id='McCullochPitts-class'>The McCullochPitts neuron class, that implements the logic of the
McCullochPitts neuron model.</h2><span id='topic+McCullochPitts-class'></span><span id='topic+mcCullochPitts'></span>

<h3>Description</h3>

<p>The McCullochPitts neuron class, that implements the logic of the
McCullochPitts neuron model.
</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="McCullochPitts-class_+3A_inputs">inputs</code></td>
<td>
<p>The actual data to be fed to the nuron, this input's
dimentions vary with the chosen weights dimentions.</p>
</td></tr>
<tr><td><code id="McCullochPitts-class_+3A_ins">ins</code></td>
<td>
<p>The list of vectors of inputs to the first layer in the network</p>
</td></tr>
<tr><td><code id="McCullochPitts-class_+3A_outs">outs</code></td>
<td>
<p>The list of vectors of outputs of the last layer in the network</p>
</td></tr>
<tr><td><code id="McCullochPitts-class_+3A_epochs">epochs</code></td>
<td>
<p>How many rounds of training to run</p>
</td></tr>
<tr><td><code id="McCullochPitts-class_+3A_tax">tax</code></td>
<td>
<p>This is the learning rate, aka eta</p>
</td></tr>
<tr><td><code id="McCullochPitts-class_+3A_maxerr">maxErr</code></td>
<td>
<p>A contition to early stop the training process</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The computed value using the McCullochPitts model.
</p>
<p>Vector of computed values of the same size of the last layer
</p>


<h3>Fields</h3>


<dl>
<dt><code>ws</code></dt><dd><p>The matrix of weights that multiply the input vector, it can be a
vector, a matrix or an array.</p>
</dd>
<dt><code>bias</code></dt><dd><p>The bias value.</p>
</dd>
</dl>


<h3>Examples</h3>

<pre><code class='language-R'># Create a dataset
dataset &lt;- iris
dataset$Petal.Length &lt;- NULL
dataset$Petal.Width &lt;- NULL
dataset &lt;- dataset[dataset$Species != "versicolor",]
dataset$Code &lt;- as.integer(dataset$Species == "virginica")
dataset &lt;- dataset[sample(20),]

# Create the neuron
neuron &lt;- mcCullochPitts(c(1,1), 1)

# Train the neuron, takes a while
neuron$train(dataset[,c(1,2)], dataset[,'Code', drop=FALSE], epochs = 10)

# Check the output
neuron$output(c(1,2))

# See accuracy
dataset$Calc &lt;- sapply(1:nrow(dataset), function(x) {
    as.integer(neuron$output(dataset[x,c(1,2)]))
})
length(which(dataset$Code==dataset$Calc))/nrow(dataset)

</code></pre>

<hr>
<h2 id='McCullochPittsLayer-class'>The McCullochPittsLayer class, that implements a layer of McCullochPitts
neurons.</h2><span id='topic+McCullochPittsLayer-class'></span><span id='topic+mcCullochPittsLayer'></span>

<h3>Description</h3>

<p>The McCullochPittsLayer class, that implements a layer of McCullochPitts
neurons.
</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="McCullochPittsLayer-class_+3A_input">input</code></td>
<td>
<p>The actual data to be fed to the layer, this input's
dimentions vary with the chosen <code>n</code>.</p>
</td></tr>
<tr><td><code id="McCullochPittsLayer-class_+3A_ins">ins</code></td>
<td>
<p>The list of vectors of inputs to the first layer in the network</p>
</td></tr>
<tr><td><code id="McCullochPittsLayer-class_+3A_outs">outs</code></td>
<td>
<p>The list of vectors of outputs of the last layer in the network</p>
</td></tr>
<tr><td><code id="McCullochPittsLayer-class_+3A_epochs">epochs</code></td>
<td>
<p>How many rounds of training to run</p>
</td></tr>
<tr><td><code id="McCullochPittsLayer-class_+3A_tax">tax</code></td>
<td>
<p>This is the learning rate, aka eta</p>
</td></tr>
<tr><td><code id="McCullochPittsLayer-class_+3A_maxerr">maxErr</code></td>
<td>
<p>A contition to early stop the training process</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The computed value using the McCullochPittsLayer model.
</p>
<p>Vector of computed values of the same size of the last layer
</p>


<h3>Fields</h3>


<dl>
<dt><code>n</code></dt><dd><p>The number of neurons to create in the layer</p>
</dd>
<dt><code>dims</code></dt><dd><p>A vector of dimensions of the inputs to the layer</p>
</dd>
<dt><code>neurons</code></dt><dd><p>A list with the internal neurons</p>
</dd>
</dl>

<hr>
<h2 id='NeuralNetwork-class'>The main NeuralNetwork class, that holds the layers.</h2><span id='topic+NeuralNetwork-class'></span><span id='topic+neuralNet'></span>

<h3>Description</h3>

<p>The main NeuralNetwork class, that holds the layers.
</p>


<h3>Fields</h3>


<dl>
<dt><code>eta</code></dt><dd><p>The learning tax, representes the size of the weight adjustment
between each epoch of training.</p>
</dd>
<dt><code>layers</code></dt><dd><p>This field is a list of the layers of the network, you can use
subsetting to inspect them.</p>
</dd>
</dl>


<h3>Examples</h3>

<pre><code class='language-R'># Create a dataset
dataset &lt;- iris
dataset$Petal.Length &lt;- NULL
dataset$Petal.Width &lt;- NULL
dataset &lt;- dataset[dataset$Species != "versicolor",]
dataset$Code &lt;- as.integer(dataset$Species == "virginica")
dataset &lt;- dataset[sample(20),]

# Create the network
net &lt;- neuralNet(2, perceptronLayer(1))

# Train the network, takes a while
net$train(dataset[,c(1,2), drop=FALSE], dataset[,'Code', drop=FALSE], epochs = 10)

# Check the output
net$compute(c(1,2))

# See accuracy
net$validationScore(dataset[,c(1,2), drop=FALSE], dataset[,'Code', drop=FALSE])

</code></pre>

<hr>
<h2 id='Perceptron-class'>The Perceptron neuron class, that implements the logic of the perceptron
model.</h2><span id='topic+Perceptron-class'></span><span id='topic+perceptron'></span>

<h3>Description</h3>

<p>The Perceptron neuron class, that implements the logic of the perceptron
model.
</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="Perceptron-class_+3A_inputs">inputs</code></td>
<td>
<p>The actual data to be fed to the neuron, this input's
dimentions vary with the chosen weights dimentions.</p>
</td></tr>
<tr><td><code id="Perceptron-class_+3A_ins">ins</code></td>
<td>
<p>The list of vectors of inputs to the first layer in the network</p>
</td></tr>
<tr><td><code id="Perceptron-class_+3A_outs">outs</code></td>
<td>
<p>The list of vectors of outputs of the last layer in the network</p>
</td></tr>
<tr><td><code id="Perceptron-class_+3A_epochs">epochs</code></td>
<td>
<p>How many rounds of training to run</p>
</td></tr>
<tr><td><code id="Perceptron-class_+3A_tax">tax</code></td>
<td>
<p>This is the learning rate, aka eta</p>
</td></tr>
<tr><td><code id="Perceptron-class_+3A_maxerr">maxErr</code></td>
<td>
<p>A contition to early stop the training process</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The computed value using the Perceptron model.
</p>
<p>Vector of computed values of the same size of the last layer
</p>


<h3>Fields</h3>


<dl>
<dt><code>ws</code></dt><dd><p>The matrix of weights that multiply the input vector, it can be a
vector, a matrix or an array.</p>
</dd>
<dt><code>bias</code></dt><dd><p>The bias value.</p>
</dd>
</dl>


<h3>Examples</h3>

<pre><code class='language-R'># Create a dataset
dataset &lt;- iris
dataset$Petal.Length &lt;- NULL
dataset$Petal.Width &lt;- NULL
dataset &lt;- dataset[dataset$Species != "versicolor",]
dataset$Code &lt;- as.integer(dataset$Species == "virginica")
dataset &lt;- dataset[sample(20),]

# Create the neuron
neuron &lt;- perceptron(c(1,1), 1)

# Train the neuron, takes a while
neuron$train(dataset[,c(1,2), drop=FALSE], dataset[,'Code', drop=FALSE], epochs = 10)

# Check the output
neuron$output(c(1,2))

# See accuracy
dataset$Calc &lt;- sapply(1:nrow(dataset), function(x) neuron$output(dataset[x,c(1,2)]))
length(which(dataset$Code==dataset$Calc))/nrow(dataset)

</code></pre>

<hr>
<h2 id='PerceptronLayer-class'>The PerceptronLayer class, that implements a layer of Perceptron neurons.</h2><span id='topic+PerceptronLayer-class'></span><span id='topic+perceptronLayer'></span>

<h3>Description</h3>

<p>The PerceptronLayer class, that implements a layer of Perceptron neurons.
</p>


<h3>Arguments</h3>

<table>
<tr><td><code id="PerceptronLayer-class_+3A_input">input</code></td>
<td>
<p>The actual data to be fed to the layer, this input's
dimentions vary with the chosen <code>n</code>.</p>
</td></tr>
<tr><td><code id="PerceptronLayer-class_+3A_ins">ins</code></td>
<td>
<p>The list of vectors of inputs to the first layer in the network</p>
</td></tr>
<tr><td><code id="PerceptronLayer-class_+3A_outs">outs</code></td>
<td>
<p>The list of vectors of outputs of the last layer in the network</p>
</td></tr>
<tr><td><code id="PerceptronLayer-class_+3A_epochs">epochs</code></td>
<td>
<p>How many rounds of training to run</p>
</td></tr>
<tr><td><code id="PerceptronLayer-class_+3A_tax">tax</code></td>
<td>
<p>This is the learning rate, aka eta</p>
</td></tr>
<tr><td><code id="PerceptronLayer-class_+3A_maxerr">maxErr</code></td>
<td>
<p>A contition to early stop the training process</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The computed value using the Perceptron model.
</p>
<p>Vector of computed values of the same size of the last layer
</p>


<h3>Fields</h3>


<dl>
<dt><code>n</code></dt><dd><p>The number of neurons to create in the layer</p>
</dd>
<dt><code>dims</code></dt><dd><p>A vector of dimensions of the inputs to the layer</p>
</dd>
<dt><code>neurons</code></dt><dd><p>A list with the internal neurons</p>
</dd>
</dl>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
