<!DOCTYPE html><html lang="en"><head><title>Help for package chatRater</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {chatRater}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#chatRater-package'><p>Multi-Model LLM API Wrapper and Cognitive Experiment Utilities</p></a></li>
<li><a href='#generate_ratings'><p>Generate Ratings for a Stimulus Using LLM APIs</p></a></li>
<li><a href='#generate_ratings_for_all'><p>Generate Ratings for Multiple Stimuli</p></a></li>
<li><a href='#get_levenshtein_d'><p>Get Levenshtein Distance (D)</p></a></li>
<li><a href='#get_lexical_coverage'><p>Get Lexical Coverage with Specified Vocabulary</p></a></li>
<li><a href='#get_semantic_transparency'><p>Get Semantic Transparency Rating</p></a></li>
<li><a href='#get_word_frequency'><p>Get Word Frequency Information</p></a></li>
<li><a href='#get_zipf_metric'><p>Get Zipf Metric</p></a></li>
<li><a href='#llm_api_call'><p>Base LLM API Call Wrapper</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Rating and Evaluating Texts Using Large Language Models</td>
</tr>
<tr>
<td>Version:</td>
<td>1.1.0</td>
</tr>
<tr>
<td>Date:</td>
<td>2025-02-18</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Shiyang Zheng &lt;Shiyang.Zheng@nottingham.ac.uk&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Generates ratings and psycholinguistic metrics for textual stimuli using large language models.
    It enables users to evaluate idioms and other language materials by combining context, prompts, and stimulus inputs.
    It supports multiple LLM APIs (such as 'OpenAI', 'DeepSeek', 'Anthropic', 'Cohere', 'Google PaLM', and 'Ollama')
    by allowing users to switch models with a single parameter. In addition to generating numeric ratings,
    'chatRater' provides functions for obtaining detailed psycholinguistic metrics including word frequency (with optional corpus input),
    lexical coverage (with customizable vocabulary size and test basis), Zipf metric, Levenshtein distance, and semantic transparency.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Imports:</td>
<td>openai, httr, jsonlite</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2025-03-03 19:42:42 UTC; admin</td>
</tr>
<tr>
<td>Author:</td>
<td>Shiyang Zheng [aut, cre]</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2025-03-03 20:00:07 UTC</td>
</tr>
</table>
<hr>
<h2 id='chatRater-package'>Multi-Model LLM API Wrapper and Cognitive Experiment Utilities</h2><span id='topic+chatRater-package'></span>

<h3>Description</h3>

<p>This package provides functions to interact with multiple LLM APIs
(e.g., 'OpenAI', 'DeepSeek', 'Anthropic', 'Cohere', 'Google PaLM', 'Ollama').
</p>
<p>Additionally, several functions are provided that encapsulate LLM prompts to obtain
various psycholinguistic metrics:
</p>

<ul>
<li> <p><strong>Word Frequency</strong>: Number of occurrences (often log-transformed) of a word in a corpus.
</p>
</li>
<li> <p><strong>Lexical Coverage</strong>: Proportion of words in the stimulus that are found in a target vocabulary.
</p>
</li>
<li> <p><strong>Zipf Metric</strong>: The slope of the log-log frequency vs. rank distribution, per Zipf's law.
</p>
</li>
<li> <p><strong>Levenshtein Distance (D)</strong>: Minimum number of single-character edits required to transform one stimulus into another.
</p>
</li>
<li> <p><strong>Semantic Transparency</strong>: The degree to which the meaning of a compound or phrase is inferable from its parts.
</p>
</li></ul>



<h3>Details</h3>

<p>The LLM API functions allow users to generate ratings using various large language models.
The cognitive experiment functions.</p>

<hr>
<h2 id='generate_ratings'>Generate Ratings for a Stimulus Using LLM APIs</h2><span id='topic+generate_ratings'></span>

<h3>Description</h3>

<p>Generates ratings for a given stimulus by calling one of several LLM APIs.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>generate_ratings(
  model = "gpt-3.5-turbo",
  stim = "kick the bucket",
  prompt = "You are a native English speaker.",
  question = "Please rate the following stim:",
  top_p = 1,
  temp = 0,
  n_iterations = 30,
  api_key = "",
  debug = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="generate_ratings_+3A_model">model</code></td>
<td>
<p>A character string specifying the LLM model (e.g., &quot;gpt-3.5-turbo&quot;, &quot;deepseek-chat&quot;).</p>
</td></tr>
<tr><td><code id="generate_ratings_+3A_stim">stim</code></td>
<td>
<p>A character string representing the stimulus (e.g., an idiom).</p>
</td></tr>
<tr><td><code id="generate_ratings_+3A_prompt">prompt</code></td>
<td>
<p>A character string for the system prompt (e.g., &quot;You are a native English speaker.&quot;).</p>
</td></tr>
<tr><td><code id="generate_ratings_+3A_question">question</code></td>
<td>
<p>A character string for the user prompt (e.g., &quot;Please rate the following stim:&quot;).</p>
</td></tr>
<tr><td><code id="generate_ratings_+3A_top_p">top_p</code></td>
<td>
<p>Numeric value for the probability mass (default 1).</p>
</td></tr>
<tr><td><code id="generate_ratings_+3A_temp">temp</code></td>
<td>
<p>Numeric value for the temperature (default 0).</p>
</td></tr>
<tr><td><code id="generate_ratings_+3A_n_iterations">n_iterations</code></td>
<td>
<p>An integer indicating how many times to query the API.</p>
</td></tr>
<tr><td><code id="generate_ratings_+3A_api_key">api_key</code></td>
<td>
<p>API key as a character string.</p>
</td></tr>
<tr><td><code id="generate_ratings_+3A_debug">debug</code></td>
<td>
<p>Logical; if TRUE, debug information is printed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function supports multiple APIs. Branching is based on the <code>model</code> parameter.
</p>


<h3>Value</h3>

<p>A data frame containing the stimulus, the rating, and the iteration number.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  ratings &lt;- generate_ratings(
    model = "gpt-3.5-turbo",
    stim = "kick the bucket",
    prompt = "You are a native English speaker.",
    question = "Please rate the following stim:",
    n_iterations = 5,
    api_key = "your_api_key"
  )
  print(ratings)

## End(Not run)
</code></pre>

<hr>
<h2 id='generate_ratings_for_all'>Generate Ratings for Multiple Stimuli</h2><span id='topic+generate_ratings_for_all'></span>

<h3>Description</h3>

<p>Applies the <code>generate_ratings</code> function to a list of stimuli.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>generate_ratings_for_all(
  model = "gpt-3.5-turbo",
  stim_list,
  prompt = "You are a native English speaker.",
  question = "Please rate the following stim:",
  top_p = 1,
  temp = 0,
  n_iterations = 30,
  api_key = "",
  debug = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="generate_ratings_for_all_+3A_model">model</code></td>
<td>
<p>A character string specifying the LLM model.</p>
</td></tr>
<tr><td><code id="generate_ratings_for_all_+3A_stim_list">stim_list</code></td>
<td>
<p>A character vector of stimuli.</p>
</td></tr>
<tr><td><code id="generate_ratings_for_all_+3A_prompt">prompt</code></td>
<td>
<p>A character string for the system prompt.</p>
</td></tr>
<tr><td><code id="generate_ratings_for_all_+3A_question">question</code></td>
<td>
<p>A character string for the user prompt.</p>
</td></tr>
<tr><td><code id="generate_ratings_for_all_+3A_top_p">top_p</code></td>
<td>
<p>Numeric value for the probability mass.</p>
</td></tr>
<tr><td><code id="generate_ratings_for_all_+3A_temp">temp</code></td>
<td>
<p>Numeric value for the temperature.</p>
</td></tr>
<tr><td><code id="generate_ratings_for_all_+3A_n_iterations">n_iterations</code></td>
<td>
<p>An integer indicating how many iterations per stimulus.</p>
</td></tr>
<tr><td><code id="generate_ratings_for_all_+3A_api_key">api_key</code></td>
<td>
<p>API key as a character string.</p>
</td></tr>
<tr><td><code id="generate_ratings_for_all_+3A_debug">debug</code></td>
<td>
<p>Logical; if TRUE, debug information is printed.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame with stimuli, ratings, and iteration numbers.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  all_ratings &lt;- generate_ratings_for_all(
    model = "gpt-3.5-turbo",
    stim_list = c("kick the bucket", "spill the beans"),
    prompt = "You are a native English speaker.",
    question = "Please rate the following stim:",
    n_iterations = 5,
    api_key = "your_api_key"
  )
  print(all_ratings)

## End(Not run)
</code></pre>

<hr>
<h2 id='get_levenshtein_d'>Get Levenshtein Distance (D)</h2><span id='topic+get_levenshtein_d'></span>

<h3>Description</h3>

<p>Uses an LLM to compute the Levenshtein distance (D) between two linguistic stimuli.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_levenshtein_d(
  stimulus1,
  stimulus2,
  model = "gpt-3.5-turbo",
  api_key = "",
  top_p = 1,
  temp = 0
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_levenshtein_d_+3A_stimulus1">stimulus1</code></td>
<td>
<p>A character string representing the first text.</p>
</td></tr>
<tr><td><code id="get_levenshtein_d_+3A_stimulus2">stimulus2</code></td>
<td>
<p>A character string representing the second text.</p>
</td></tr>
<tr><td><code id="get_levenshtein_d_+3A_model">model</code></td>
<td>
<p>A character string specifying the LLM model (default &quot;gpt-3.5-turbo&quot;).</p>
</td></tr>
<tr><td><code id="get_levenshtein_d_+3A_api_key">api_key</code></td>
<td>
<p>API key as a character string.</p>
</td></tr>
<tr><td><code id="get_levenshtein_d_+3A_top_p">top_p</code></td>
<td>
<p>Numeric value (default 1).</p>
</td></tr>
<tr><td><code id="get_levenshtein_d_+3A_temp">temp</code></td>
<td>
<p>Numeric value (default 0).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Default definition: &quot;Levenshtein distance is defined as the minimum number of single-character edits
(insertions, deletions, or substitutions) required to transform one string into another.&quot;
</p>


<h3>Value</h3>

<p>A numeric value representing the Levenshtein distance.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  lev_dist &lt;- get_levenshtein_d("kitten", "sitting",
                                model = "gpt-3.5-turbo",
                                api_key = "your_api_key")
  cat("Levenshtein Distance:", lev_dist, "\n")

## End(Not run)
</code></pre>

<hr>
<h2 id='get_lexical_coverage'>Get Lexical Coverage with Specified Vocabulary</h2><span id='topic+get_lexical_coverage'></span>

<h3>Description</h3>

<p>Uses an LLM to obtain the lexical coverage (percentage) of a given text,
taking into account a specified vocabulary size and the vocabulary test basis.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_lexical_coverage(
  stimulus,
  vocab_size = 2000,
  vocab_test = "Vocabulary Levels Test",
  model = "gpt-3.5-turbo",
  api_key = "",
  top_p = 1,
  temp = 0
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_lexical_coverage_+3A_stimulus">stimulus</code></td>
<td>
<p>A character string representing the language material.</p>
</td></tr>
<tr><td><code id="get_lexical_coverage_+3A_vocab_size">vocab_size</code></td>
<td>
<p>A numeric value indicating the size of the target vocabulary (e.g., 1000, 2000, 3000).</p>
</td></tr>
<tr><td><code id="get_lexical_coverage_+3A_vocab_test">vocab_test</code></td>
<td>
<p>A character string specifying the vocabulary test used (e.g., &quot;Vocabulary Levels Test&quot;, &quot;LexTALE&quot;).
Users may provide any test name.</p>
</td></tr>
<tr><td><code id="get_lexical_coverage_+3A_model">model</code></td>
<td>
<p>A character string specifying the LLM model (default &quot;gpt-3.5-turbo&quot;).</p>
</td></tr>
<tr><td><code id="get_lexical_coverage_+3A_api_key">api_key</code></td>
<td>
<p>API key as a character string.</p>
</td></tr>
<tr><td><code id="get_lexical_coverage_+3A_top_p">top_p</code></td>
<td>
<p>Numeric value for probability mass (default 1).</p>
</td></tr>
<tr><td><code id="get_lexical_coverage_+3A_temp">temp</code></td>
<td>
<p>Numeric value for temperature (default 0).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Default definition: &quot;Lexical coverage is the proportion of words in a text that are included in a given vocabulary list.
For this evaluation, assume a target vocabulary size of <code>vocab_size</code> words based on the <code>vocab_test</code>.&quot;
</p>


<h3>Value</h3>

<p>A numeric value indicating the lexical coverage percentage.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  coverage &lt;- get_lexical_coverage("The quick brown fox jumps over the lazy dog",
                                   vocab_size = 2000,
                                   vocab_test = "Vocabulary Levels Test",
                                   model = "gpt-3.5-turbo",
                                   api_key = "your_api_key")
  cat("Lexical Coverage (%):", coverage, "\n")

## End(Not run)
</code></pre>

<hr>
<h2 id='get_semantic_transparency'>Get Semantic Transparency Rating</h2><span id='topic+get_semantic_transparency'></span>

<h3>Description</h3>

<p>Uses an LLM to obtain a semantic transparency rating for the given linguistic stimulus.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_semantic_transparency(
  stimulus,
  model = "gpt-3.5-turbo",
  api_key = "",
  top_p = 1,
  temp = 0
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_semantic_transparency_+3A_stimulus">stimulus</code></td>
<td>
<p>A character string representing the language material.</p>
</td></tr>
<tr><td><code id="get_semantic_transparency_+3A_model">model</code></td>
<td>
<p>A character string specifying the LLM model (default &quot;gpt-3.5-turbo&quot;).</p>
</td></tr>
<tr><td><code id="get_semantic_transparency_+3A_api_key">api_key</code></td>
<td>
<p>API key as a character string.</p>
</td></tr>
<tr><td><code id="get_semantic_transparency_+3A_top_p">top_p</code></td>
<td>
<p>Numeric value (default 1).</p>
</td></tr>
<tr><td><code id="get_semantic_transparency_+3A_temp">temp</code></td>
<td>
<p>Numeric value (default 0).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Default definition: &quot;Semantic transparency is the degree to which the meaning of a compound or phrase
can be inferred from its constituent parts.&quot;
</p>


<h3>Value</h3>

<p>An integer rating (1-7) indicating the semantic transparency.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  sem_trans &lt;- get_semantic_transparency("blackbird",
                                          model = "gpt-3.5-turbo",
                                          api_key = "your_api_key")
  cat("Semantic Transparency Rating:", sem_trans, "\n")

## End(Not run)
</code></pre>

<hr>
<h2 id='get_word_frequency'>Get Word Frequency Information</h2><span id='topic+get_word_frequency'></span>

<h3>Description</h3>

<p>Uses an LLM to obtain frequency information for a specified word position
in the stimulus. The user can specify a corpus; if none is provided and corpus_source is &quot;llm&quot;,
the LLM will generate or assume a representative corpus.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_word_frequency(
  stimulus,
  position = "first",
  corpus = "",
  corpus_source = ifelse(corpus != "", "provided", "llm"),
  model = "gpt-3.5-turbo",
  api_key = "",
  top_p = 1,
  temp = 0
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_word_frequency_+3A_stimulus">stimulus</code></td>
<td>
<p>A character string representing the language material.</p>
</td></tr>
<tr><td><code id="get_word_frequency_+3A_position">position</code></td>
<td>
<p>A character string indicating which word to analyze (&quot;first&quot;, &quot;last&quot;, &quot;each&quot;, or &quot;total&quot;).</p>
</td></tr>
<tr><td><code id="get_word_frequency_+3A_corpus">corpus</code></td>
<td>
<p>An optional character string representing the corpus to use for frequency analysis.</p>
</td></tr>
<tr><td><code id="get_word_frequency_+3A_corpus_source">corpus_source</code></td>
<td>
<p>A character string, either &quot;provided&quot; or &quot;llm&quot;. Default is &quot;provided&quot; if corpus is given, otherwise &quot;llm&quot;.</p>
</td></tr>
<tr><td><code id="get_word_frequency_+3A_model">model</code></td>
<td>
<p>A character string specifying the LLM model (default &quot;gpt-3.5-turbo&quot;).</p>
</td></tr>
<tr><td><code id="get_word_frequency_+3A_api_key">api_key</code></td>
<td>
<p>API key as a character string.</p>
</td></tr>
<tr><td><code id="get_word_frequency_+3A_top_p">top_p</code></td>
<td>
<p>Numeric value for probability mass (default 1).</p>
</td></tr>
<tr><td><code id="get_word_frequency_+3A_temp">temp</code></td>
<td>
<p>Numeric value for temperature (default 0).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Default definition: &quot;Word frequency is defined as the number of times a word appears in a corpus (often log-transformed).&quot;
</p>


<h3>Value</h3>

<p>A numeric value representing the frequency (or a JSON string if &quot;each&quot; is specified).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  freq_first &lt;- get_word_frequency("The quick brown fox jumps over the lazy dog",
                                   position = "first",
                                   corpus = "A sample corpus text with everyday language.",
                                   corpus_source = "provided",
                                   model = "gpt-3.5-turbo",
                                   api_key = "your_api_key")
  cat("Frequency (first word):", freq_first, "\n")

## End(Not run)
</code></pre>

<hr>
<h2 id='get_zipf_metric'>Get Zipf Metric</h2><span id='topic+get_zipf_metric'></span>

<h3>Description</h3>

<p>Uses an LLM to estimate a Zipf-based metric (slope) for the given stimulus.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_zipf_metric(
  stimulus,
  model = "gpt-3.5-turbo",
  api_key = "",
  top_p = 1,
  temp = 0
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_zipf_metric_+3A_stimulus">stimulus</code></td>
<td>
<p>A character string representing the language material.</p>
</td></tr>
<tr><td><code id="get_zipf_metric_+3A_model">model</code></td>
<td>
<p>A character string specifying the LLM model (default &quot;gpt-3.5-turbo&quot;).</p>
</td></tr>
<tr><td><code id="get_zipf_metric_+3A_api_key">api_key</code></td>
<td>
<p>API key as a character string.</p>
</td></tr>
<tr><td><code id="get_zipf_metric_+3A_top_p">top_p</code></td>
<td>
<p>Numeric value (default 1).</p>
</td></tr>
<tr><td><code id="get_zipf_metric_+3A_temp">temp</code></td>
<td>
<p>Numeric value (default 0).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Default definition: &quot;Zipf's law states that word frequency is inversely proportional to its rank;
the Zipf metric is the slope of the log-log frequency vs. rank plot.&quot;
</p>


<h3>Value</h3>

<p>A numeric value representing the Zipf metric.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  zipf_metric &lt;- get_zipf_metric("The quick brown fox jumps over the lazy dog",
                                 model = "gpt-3.5-turbo",
                                 api_key = "your_api_key")
  cat("Zipf Metric:", zipf_metric, "\n")

## End(Not run)
</code></pre>

<hr>
<h2 id='llm_api_call'>Base LLM API Call Wrapper</h2><span id='topic+llm_api_call'></span>

<h3>Description</h3>

<p>Sends a prompt (with background academic definitions) to an LLM API
(defaulting to 'OpenAI') and returns the LLM response.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>llm_api_call(
  prompt_text,
  model = "gpt-3.5-turbo",
  api_key = "",
  top_p = 1,
  temp = 0
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="llm_api_call_+3A_prompt_text">prompt_text</code></td>
<td>
<p>A character string containing the user prompt.</p>
</td></tr>
<tr><td><code id="llm_api_call_+3A_model">model</code></td>
<td>
<p>A character string specifying the LLM model (default &quot;gpt-3.5-turbo&quot;).</p>
</td></tr>
<tr><td><code id="llm_api_call_+3A_api_key">api_key</code></td>
<td>
<p>API key as a character string.</p>
</td></tr>
<tr><td><code id="llm_api_call_+3A_top_p">top_p</code></td>
<td>
<p>Numeric value for the probability mass (default 1).</p>
</td></tr>
<tr><td><code id="llm_api_call_+3A_temp">temp</code></td>
<td>
<p>Numeric value for the sampling temperature (default 0).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The system prompt includes academic definitions for word frequency, lexical coverage,
Zipf's law, Levenshtein distance, and semantic transparency.
</p>


<h3>Value</h3>

<p>A character string containing the LLM's response.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
  response &lt;- llm_api_call("Please provide a rating for the stimulus 'apple'.")
  cat(response)

## End(Not run)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
