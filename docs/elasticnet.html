<!DOCTYPE html><html><head><title>Help for package elasticnet</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {elasticnet}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#arrayspc'><p>Sparse PCs of Microarrays</p></a></li>
<li><a href='#cv.enet'><p>Computes K-fold cross-validated error curve for elastic net</p></a></li>
<li><a href='#diabetes'><p>Blood and other measurements in diabetics</p></a></li>
<li><a href='#elasticnet-internal'><p>Internal elasticnet functions</p></a></li>
<li><a href='#enet'>
<p>Fits Elastic Net regression models</p></a></li>
<li><a href='#pitprops'><p>Pitprops correlation data</p></a></li>
<li><a href='#plot.enet'><p>Plot method for enet objects</p></a></li>
<li><a href='#predict.enet'>
<p>Make predictions or extract coefficients from a fitted elastic net model</p></a></li>
<li><a href='#print.arrayspc'><p>Print method for arrayspc objects</p></a></li>
<li><a href='#print.enet'><p>Print method for enet objects</p></a></li>
<li><a href='#print.spca'><p>Print method for spca objects</p></a></li>
<li><a href='#spca'><p>Sparse Principal Components Analysis</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Version:</td>
<td>1.3</td>
</tr>
<tr>
<td>Date:</td>
<td>2020-05-15</td>
</tr>
<tr>
<td>Title:</td>
<td>Elastic-Net for Sparse Estimation and Sparse PCA</td>
</tr>
<tr>
<td>Author:</td>
<td>Hui Zou &lt;zouxx019@umn.edu&gt; and Trevor Hastie
        &lt;hastie@stanford.edu&gt;</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Hui Zou &lt;zouxx019@umn.edu&gt;</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.10), lars</td>
</tr>
<tr>
<td>Description:</td>
<td>Provides functions for fitting the entire
        solution path of the Elastic-Net and also provides functions
        for doing sparse PCA.  </td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="http://users.stat.umn.edu/~zouxx019/">http://users.stat.umn.edu/~zouxx019/</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2020-05-15 16:06:02 UTC; hzou</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2020-05-15 17:10:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='arrayspc'>Sparse PCs of Microarrays</h2><span id='topic+arrayspc'></span>

<h3>Description</h3>

<p>Sparse PC by iterative SVD and soft-thresholding</p>


<h3>Usage</h3>

<pre><code class='language-R'>arrayspc(x,K=1,para,use.corr=FALSE, max.iter=200,trace=FALSE,eps=1e-3)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="arrayspc_+3A_x">x</code></td>
<td>

<p>The microarray matrix.
</p>
</td></tr>
<tr><td><code id="arrayspc_+3A_k">K</code></td>
<td>

<p>Number of components. Default is 1.
</p>
</td></tr>
<tr><td><code id="arrayspc_+3A_para">para</code></td>
<td>
<p>The thresholding parameters. A vector of length K. 
</p>
</td></tr>
<tr><td><code id="arrayspc_+3A_use.corr">use.corr</code></td>
<td>
<p>Perform PCA on the correlation matrix? This option is
only effective when the argument type is set &quot;data&quot;.</p>
</td></tr>
<tr><td><code id="arrayspc_+3A_max.iter">max.iter</code></td>
<td>
<p>Maximum number of iterations.</p>
</td></tr>
<tr><td><code id="arrayspc_+3A_trace">trace</code></td>
<td>
<p>If TRUE, prints out its progress.</p>
</td></tr>
<tr><td><code id="arrayspc_+3A_eps">eps</code></td>
<td>
<p>Convergence criterion.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function is equivalent to a special case of spca() with the quadratic
penalty=infinity. It is specifically designed for the case p&gt;&gt;n, like microarrays.
</p>


<h3>Value</h3>

<p>A &quot;arrayspc&quot; object is returned.</p>


<h3>Author(s)</h3>

<p>Hui Zou and Trevor Hastie</p>


<h3>References</h3>

<p>Zou, H., Hastie, T. and Tibshirani, R. (2006) &quot;Sparse principal component
analysis&quot; <em>Journal of Computational and Graphical Statistics</em>, 15 (2), 265&ndash;286.
</p>


<h3>See Also</h3>

<p>spca, princomp
</p>

<hr>
<h2 id='cv.enet'>Computes K-fold cross-validated error curve for elastic net</h2><span id='topic+cv.enet'></span>

<h3>Description</h3>

<p>Computes the K-fold cross-validated mean squared prediction error for
elastic net.</p>


<h3>Usage</h3>

<pre><code class='language-R'>cv.enet(x, y, K = 10, lambda, s, mode,trace = FALSE, plot.it = TRUE, se = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cv.enet_+3A_x">x</code></td>
<td>
<p>Input to lars</p>
</td></tr>
<tr><td><code id="cv.enet_+3A_y">y</code></td>
<td>
<p>Input to lars</p>
</td></tr>
<tr><td><code id="cv.enet_+3A_k">K</code></td>
<td>
<p>Number of folds</p>
</td></tr>
<tr><td><code id="cv.enet_+3A_lambda">lambda</code></td>
<td>
<p>Quadratic penalty parameter</p>
</td></tr>
<tr><td><code id="cv.enet_+3A_s">s</code></td>
<td>
<p>Abscissa values at which CV curve should be computed.
A value, or vector of values, indexing the path. Its values depends on the mode= argument</p>
</td></tr>
<tr><td><code id="cv.enet_+3A_mode">mode</code></td>
<td>
<p>Mode=&quot;step&quot; means the s= argument indexes the LARS-EN step number. If mode=&quot;fraction&quot;, then s should be a number
between 0 and 1, and it refers to the ratio of the L1 norm of the
coefficient vector, relative to the norm at the full LS solution.
Mode=&quot;norm&quot; means s refers to the L1 norm of the coefficient vector.
Abbreviations allowed. If mode=&quot;norm&quot;, then s should be the L1 norm of
the coefficient vector. If mode=&quot;penalty&quot;, then s should be the 1-norm
penalty parameter.</p>
</td></tr>
<tr><td><code id="cv.enet_+3A_trace">trace</code></td>
<td>
<p>Show computations?</p>
</td></tr>
<tr><td><code id="cv.enet_+3A_plot.it">plot.it</code></td>
<td>
<p>Plot it?</p>
</td></tr>
<tr><td><code id="cv.enet_+3A_se">se</code></td>
<td>
<p>Include standard error bands?</p>
</td></tr>
<tr><td><code id="cv.enet_+3A_...">...</code></td>
<td>
<p>Additional arguments to <code>enet</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>Invisibly returns a list with components (which can be plotted using <code>plotCVLars</code>)
</p>
<table>
<tr><td><code>fraction</code></td>
<td>
<p>Values of s</p>
</td></tr>
<tr><td><code>cv</code></td>
<td>
<p>The CV curve at each value of fraction</p>
</td></tr>
<tr><td><code>cv.error</code></td>
<td>
<p>The standard error of the CV curve</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Hui Zou and Trevor Hastie</p>


<h3>References</h3>

<p>Zou and Hastie (2005) &quot;Regularization and
Variable Selection via the Elastic Net&quot;
<em>Journal of the Royal Statistical Society, Series B,76,301-320</em>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(diabetes)
attach(diabetes)
## use the L1 fraction norm as the tuning parameter
cv.enet(x2,y,lambda=0.05,s=seq(0,1,length=100),mode="fraction",trace=TRUE,max.steps=80)
## use the number of steps as the tuning parameter
cv.enet(x2,y,lambda=0.05,s=1:50,mode="step")
detach(diabetes)
</code></pre>

<hr>
<h2 id='diabetes'>Blood and other measurements in diabetics</h2><span id='topic+diabetes'></span>

<h3>Description</h3>

<p>The <code>diabetes</code> data frame has 442 rows and 3 columns.
These are the data used in the Efron et al &quot;Least Angle Regression&quot; paper.
</p>


<h3>Format</h3>

<p>This data frame contains the following columns:
</p>

<dl>
<dt>x</dt><dd><p>a matrix with 10 columns</p>
</dd>
<dt>y</dt><dd><p>a numeric vector</p>
</dd>
<dt>x2</dt><dd><p>a matrix with 64 columns</p>
</dd>
</dl>



<h3>Details</h3>

<p>The x matrix has been standardized to have unit L2 norm in each column
and zero mean. The matrix x2 consists of x plus certain interactions.
</p>


<h3>Source</h3>

<p><a href="http://www-stat.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.ps">http://www-stat.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.ps</a>
</p>


<h3>References</h3>

<p>Efron, Hastie, Johnstone and Tibshirani (2003) &quot;Least Angle Regression&quot;
(with discussion) <em>Annals of Statistics</em>
</p>

<hr>
<h2 id='elasticnet-internal'>Internal elasticnet functions</h2><span id='topic+updateRR'></span><span id='topic+solvebeta'></span><span id='topic+rootmatrix'></span><span id='topic+soft'></span><span id='topic+convcheck'></span>

<h3>Description</h3>

<p>Internal elasticnet functions</p>


<h3>Usage</h3>

<pre><code class='language-R'>updateRR(xnew, R = NULL, xold, lambda, eps = .Machine$double.eps)
solvebeta(x, y, paras, max.steps, sparse=c("penalty","varnum"), eps = .Machine$double.eps)
rootmatrix(x)
soft(a,para)
convcheck(beta1,beta2)
</code></pre>


<h3>Details</h3>

<p>These are not to be called by the user.
</p>


<h3>Author(s)</h3>

<p>Hui Zou and Trevor Hastie</p>

<hr>
<h2 id='enet'>
Fits Elastic Net regression models
</h2><span id='topic+enet'></span>

<h3>Description</h3>

<p>Starting from zero, the LARS-EN algorithm provides the entire sequence of coefficients and fits.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>enet(x, y, lambda, max.steps, normalize=TRUE, intercept=TRUE,
     trace = FALSE, eps = .Machine$double.eps)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="enet_+3A_x">x</code></td>
<td>

<p>matrix of predictors
</p>
</td></tr>
<tr><td><code id="enet_+3A_y">y</code></td>
<td>

<p>response 
</p>
</td></tr>
<tr><td><code id="enet_+3A_lambda">lambda</code></td>
<td>

<p>Quadratic penalty parameter. lambda=0 performs the Lasso fit.
</p>
</td></tr>
<tr><td><code id="enet_+3A_max.steps">max.steps</code></td>
<td>

<p>Limit the number of steps taken; the default is <code>50 * min(m,
    n-1)</code>, with m the number of variables, and n the number of samples.
One can use this option to perform early stopping.
</p>
</td></tr>
<tr><td><code id="enet_+3A_trace">trace</code></td>
<td>

<p>If TRUE, prints out its progress
</p>
</td></tr>
<tr><td><code id="enet_+3A_normalize">normalize</code></td>
<td>
<p> Standardize the predictors?
</p>
</td></tr>
<tr><td><code id="enet_+3A_intercept">intercept</code></td>
<td>
<p> Center the predictors?
</p>
</td></tr>
<tr><td><code id="enet_+3A_eps">eps</code></td>
<td>

<p>An effective zero
</p>
</td></tr></table>


<h3>Details</h3>

<p>The Elastic Net methodology is described in detail in Zou and Hastie (2004).
The LARS-EN algorithm computes the complete elastic net
solution simultaneously for ALL values of the shrinkage parameter in
the same computational cost as a least squares fit. 
The structure of enet() is based on lars() coded by Efron and Hastie.  
Some internel functions from the lars package are called. 
The user should install lars before using elasticnet functions.
</p>


<h3>Value</h3>

<p>An &quot;enet&quot; object is returned, for which print, plot and predict methods exist.
</p>


<h3>Author(s)</h3>

<p>Hui Zou and Trevor Hastie</p>


<h3>References</h3>

<p>Zou and Hastie (2005) &quot;Regularization and
Variable Selection via the Elastic Net&quot;
<em>Journal of the Royal Statistical Society, Series B, 67, 301-320</em>.
</p>


<h3>See Also</h3>

<p>print, plot, and predict methods for enet
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(diabetes)
attach(diabetes)
##fit the lasso model (treated as a special case of the elastic net)
object1 &lt;- enet(x,y,lambda=0)
plot(object1)
##fit the elastic net model with lambda=1.
object2 &lt;- enet(x,y,lambda=1) 
plot(object2)
##early stopping after 50 LARS-EN steps
object4 &lt;- enet(x2,y,lambda=0.5,max.steps=50)
plot(object4)
detach(diabetes)
</code></pre>

<hr>
<h2 id='pitprops'>Pitprops correlation data</h2><span id='topic+pitprops'></span>

<h3>Description</h3>

<p>The <code>pitprops</code> data is a correlation matrix that was calculated from 180 observations. There are 13 
explanatory variables.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(pitprops)
</code></pre>


<h3>Details</h3>

<p>Jeffers (1967) tried to interpret the first six PCs. This is a classical example showing the
difficulty of interpreting principal components.
</p>


<h3>References</h3>

<p>Jeffers, J. (1967) &quot;Two case studies in the application of principal component&quot;,    
<em>Applied Statistics</em>, 16, 225-236.</p>

<hr>
<h2 id='plot.enet'>Plot method for enet objects</h2><span id='topic+plot.enet'></span>

<h3>Description</h3>

<p>Produce a plot of an enet fit. The default is a complete coefficient path.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'enet'
plot(x, xvar = c("fraction", "penalty", "L1norm", "step"),
use.color = FALSE, ...) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.enet_+3A_x">x</code></td>
<td>
<p>enet object</p>
</td></tr>
<tr><td><code id="plot.enet_+3A_xvar">xvar</code></td>
<td>
<p>The type of x variable against which to
plot. <code>xvar=fraction</code> plots agains the fraction of the L1 norm of the coefficient vector (default).
<code>xvar=penalty</code> plots against the 1-norm penalty parameter.
<code>xvar=L1norm</code> plots against the L1 norm of the coefficient
vector. <code>xvar=step</code> plots against the LARS-EN step number. 
</p>
</td></tr>
<tr><td><code id="plot.enet_+3A_use.color">use.color</code></td>
<td>
<p>a colorful plot?</p>
</td></tr>
<tr><td><code id="plot.enet_+3A_...">...</code></td>
<td>
<p>Additonal arguments for generic plot.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>NULL</p>


<h3>Author(s)</h3>

<p>Hui Zou and Trevor Hastie</p>


<h3>References</h3>

<p>Zou and Hastie (2005) &quot;Regularization and Variable Selection via the Elastic Net&quot;
<em>Journal of the Royal Statistical Society, Series B,67,301-320</em>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(diabetes)
attach(diabetes)
object &lt;- enet(x,y,lambda=1)
par(mfrow=c(2,2))
plot(object)
plot(object,xvar="step")
detach(diabetes)
</code></pre>

<hr>
<h2 id='predict.enet'>
Make predictions or extract coefficients from a fitted elastic net model
</h2><span id='topic+predict.enet'></span>

<h3>Description</h3>

<p>While enet() produces the entire path of solutions, predict.enet
allows one to extract a prediction at a particular point along the path.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'enet'
predict(object, newx, s, type = c("fit", "coefficients"), mode =
c("step","fraction", "norm", "penalty"),naive=FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.enet_+3A_object">object</code></td>
<td>

<p>A fitted enet object
</p>
</td></tr>
<tr><td><code id="predict.enet_+3A_newx">newx</code></td>
<td>

<p>If type=&quot;fit&quot;, then newx should be the x values at which the fit is
required. If type=&quot;coefficients&quot;, then newx can be omitted.
</p>
</td></tr>
<tr><td><code id="predict.enet_+3A_s">s</code></td>
<td>

<p>a value, or vector of values, indexing the path. Its values depends on the mode= argument. By
default (mode=&quot;step&quot;).
</p>
</td></tr>
<tr><td><code id="predict.enet_+3A_type">type</code></td>
<td>

<p>If type=&quot;fit&quot;, predict returns the fitted values. If
type=&quot;coefficients&quot;, predict returns the coefficients.
Abbreviations allowed.
</p>
</td></tr>
<tr><td><code id="predict.enet_+3A_mode">mode</code></td>
<td>

<p>Mode=&quot;step&quot; means the s= argument indexes the LARS-EN step number, and
the coefficients will be returned corresponding to the values
corresponding to step s. If mode=&quot;fraction&quot;, then s should be a number
between 0 and 1, and it refers to the ratio of the L1 norm of the
coefficient vector, relative to the norm at the full LS solution.
Mode=&quot;norm&quot; means s refers to the L1 norm of the coefficient vector.
Abbreviations allowed. If mode=&quot;norm&quot;, then s should be the L1 norm of
the coefficient vector. If mode=&quot;penalty&quot;, then s should be the 1-norm
penalty parameter.
</p>
</td></tr>
<tr><td><code id="predict.enet_+3A_naive">naive</code></td>
<td>
<p>IF naive is True, then the naive elastic net fit is
returned.</p>
</td></tr>
<tr><td><code id="predict.enet_+3A_...">...</code></td>
<td>
<p>Additonal arguments for generic print.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Starting from zero, the LARS-EN algorithm provides the entire sequence of coefficients and fits.
</p>


<h3>Value</h3>

<p>Either a vector/matrix of fitted values, or a vector/matrix of coefficients.
</p>


<h3>Author(s)</h3>

<p>Hui Zou and Trevor Hastie</p>


<h3>References</h3>

<p>Zou and Hastie (2005) &quot;Regularization and
Variable Selection via the Elastic Net&quot;
<em>Journal of the Royal Statistical Society, Series B,67,301-320</em>.
</p>


<h3>See Also</h3>

<p>print, plot,  enet
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(diabetes)
attach(diabetes)
object &lt;- enet(x,y,lambda=0.1)
### make predictions at the values in x, at each of the
### steps produced in object
fits &lt;- predict.enet(object, x, type="fit")
### extract the coefficient vector with L1 norm=2000
coef2000 &lt;- predict(object, s=2000, type="coef", mode="norm")
### extract the coefficient vector with L1 norm fraction=0.45
coef.45 &lt;- predict(object, s=0.45, type="coef", mode="fraction")
detach(diabetes)
</code></pre>

<hr>
<h2 id='print.arrayspc'>Print method for arrayspc objects</h2><span id='topic+print.arrayspc'></span>

<h3>Description</h3>

<p>Print out an arrayspc fit. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'arrayspc'
print(x, ...)  
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.arrayspc_+3A_x">x</code></td>
<td>
<p>arrayspc object</p>
</td></tr>
<tr><td><code id="print.arrayspc_+3A_...">...</code></td>
<td>
<p>Additonal arguments for generic print.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>NULL</p>


<h3>Author(s)</h3>

<p>Hui Zou and Trevor Hastie</p>


<h3>References</h3>

<p>Zou, H., Hastie, T. and Tibshirani, R. (2006) &quot;Sparse principal component
analysis&quot; <em>Journal of Computational and Graphical Statistics</em>, 15 (2), 265&ndash;286.
</p>

<hr>
<h2 id='print.enet'>Print method for enet objects</h2><span id='topic+print.enet'></span>

<h3>Description</h3>

<p>Print out an enet fit. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'enet'
print(x, ...)  
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.enet_+3A_x">x</code></td>
<td>
<p>enet object</p>
</td></tr>
<tr><td><code id="print.enet_+3A_...">...</code></td>
<td>
<p>Additonal arguments for generic print.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>NULL</p>


<h3>Author(s)</h3>

<p>Hui Zou and Trevor Hastie</p>


<h3>References</h3>

<p>Zou and Hastie (2005) &quot;Regularization and Variable Selection via the Elastic Net&quot;
<em>Journal of the Royal Statistical Society, Series B,67,301-320</em>.
</p>

<hr>
<h2 id='print.spca'>Print method for spca objects</h2><span id='topic+print.spca'></span>

<h3>Description</h3>

<p>Print out a spca fit. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'spca'
print(x, ...)  
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.spca_+3A_x">x</code></td>
<td>
<p>spca object</p>
</td></tr>
<tr><td><code id="print.spca_+3A_...">...</code></td>
<td>
<p>Additonal arguments for generic print.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>NULL</p>


<h3>Author(s)</h3>

<p>Hui Zou and Trevor Hastie</p>


<h3>References</h3>

<p>Zou, H., Hastie, T. and Tibshirani, R. (2006) &quot;Sparse principal component
analysis&quot; <em>Journal of Computational and Graphical Statistics</em>, 15 (2), 265&ndash;286.
</p>

<hr>
<h2 id='spca'>Sparse Principal Components Analysis</h2><span id='topic+spca'></span>

<h3>Description</h3>

<p>Using an alternating minimization algorithm to minimize the SPCA criterion.</p>


<h3>Usage</h3>

<pre><code class='language-R'>spca(x, K, para, type=c("predictor","Gram"),
     sparse=c("penalty","varnum"), use.corr=FALSE, lambda=1e-6,
     max.iter=200, trace=FALSE, eps.conv=1e-3)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="spca_+3A_x">x</code></td>
<td>

<p>A matrix. It can be the predictor matrix or the sample
covariance/correlation matrix. 
</p>
</td></tr>
<tr><td><code id="spca_+3A_k">K</code></td>
<td>

<p>Number of components
</p>
</td></tr>
<tr><td><code id="spca_+3A_para">para</code></td>
<td>
<p>A vector of length K. All elements should be positive. If sparse=&quot;varnum&quot;, the elements integers.
</p>
</td></tr>
<tr><td><code id="spca_+3A_type">type</code></td>
<td>

<p>If type=&quot;predictor&quot;, x is the predictor matrix.
If type=&quot;Gram&quot;, the function asks the user to provide the sample covariance or correlation matrix.  
</p>
</td></tr>
<tr><td><code id="spca_+3A_sparse">sparse</code></td>
<td>
<p>If sparse=&quot;penalty&quot;, para is a vector of 1-norm
penalty parameters. If sparse=&quot;varnum&quot;, para defines the number of
sparse loadings to be obtained. This option is not discussed in the
paper given below, but it is convenient in practice.</p>
</td></tr>
<tr><td><code id="spca_+3A_lambda">lambda</code></td>
<td>
<p>Quadratic penalty parameter. Default value is 1e-6.</p>
</td></tr>
<tr><td><code id="spca_+3A_use.corr">use.corr</code></td>
<td>
<p>Perform PCA on the correlation matrix? This option is
only effective when the argument type is set &quot;data&quot;.</p>
</td></tr>
<tr><td><code id="spca_+3A_max.iter">max.iter</code></td>
<td>
<p>Maximum number of iterations.</p>
</td></tr>
<tr><td><code id="spca_+3A_trace">trace</code></td>
<td>
<p>If TRUE, prints out its progress.</p>
</td></tr>
<tr><td><code id="spca_+3A_eps.conv">eps.conv</code></td>
<td>
<p>Convergence criterion.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>PCA is shown to be equivalent to a regression-type optimization problem,
then sparse loadings are obtained by imposing the 1-norm constraint on
the regression coefficients. If x is a microarray matrix, use arrayspc().
</p>


<h3>Value</h3>

<p>A &quot;spca&quot; object is returned. 
The below are some quantities which the user may be interested in: 
</p>
<table>
<tr><td><code>loadings</code></td>
<td>

<p>The loadings of the sparse PCs
</p>
</td></tr>
<tr><td><code>pev</code></td>
<td>
 
<p>Percentage of explained variance
</p>
</td></tr>
<tr><td><code>var.all</code></td>
<td>
 
<p>Total variance of the predictors
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Hui Zou and Trevor Hastie</p>


<h3>References</h3>

<p>Zou, H., Hastie, T. and Tibshirani, R. (2006) &quot;Sparse principal component
analysis&quot; <em>Journal of Computational and Graphical Statistics</em>, 15 (2), 265&ndash;286.
</p>


<h3>See Also</h3>

<p>princomp, arrayspc
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(pitprops)
out1&lt;-spca(pitprops,K=6,type="Gram",sparse="penalty",trace=TRUE,para=c(0.06,0.16,0.1,0.5,0.5,0.5))
## print the object out1
out1
out2&lt;-spca(pitprops,K=6,type="Gram",sparse="varnum",trace=TRUE,para=c(7,4,4,1,1,1))
out2
## to see the contents of out2
names(out2) 
## to get the loadings
out2$loadings
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
