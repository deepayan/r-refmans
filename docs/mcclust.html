<!DOCTYPE html><html lang="en"><head><title>Help for package mcclust</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {mcclust}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#mcclust-package'>
<p>Process MCMC Sample of Clusterings.</p></a></li>
<li><a href='#arandi'><p> (Adjusted) Rand Index for Clusterings</p></a></li>
<li><a href='#cls.draw1.5'><p>Sample of Clusterings from Posterior Distribution of Bayesian Cluster Model</p></a></li>
<li><a href='#cls.draw2'><p>Sample of Clusterings from Posterior Distribution of Bayesian Cluster Model</p></a></li>
<li><a href='#cltoSim'><p>Compute Similarity Matrix for a Clustering and vice versa</p></a></li>
<li><a href='#comp.psm'><p>Estimate Posterior Similarity Matrix</p></a></li>
<li><a href='#maxpear'><p>Maximize/Compute Posterior Expected Adjusted Rand Index</p></a></li>
<li><a href='#medv'><p>Clustering Method of Medvedovic</p></a></li>
<li><a href='#minbinder'><p> Minimize/Compute Posterior Expectation of Binders Loss Function</p></a></li>
<li><a href='#norm.label'><p> Norm Labelling of a Clustering</p></a></li>
<li><a href='#relabel'><p> Stephens' Relabelling Algorithm for Clusterings</p></a></li>
<li><a href='#vi.dist'><p> Variation of Information Distance for Clusterings</p></a></li>
<li><a href='#Ysim1.5'><p>Simulated 3-dimensional Normal Data Containing 8 Clusters</p></a></li>
<li><a href='#Ysim2'><p>Simulated 3-dimensional Normal Data Containing 8 Clusters</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Process an MCMC Sample of Clusterings</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.1</td>
</tr>
<tr>
<td>Date:</td>
<td>2009-05-22</td>
</tr>
<tr>
<td>Author:</td>
<td>Arno Fritsch</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.10), lpSolve</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Arno Fritsch &lt;arno.fritsch@tu-dortmund.de&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Implements methods for processing a sample of (hard)
        clusterings, e.g. the MCMC output of a Bayesian clustering
        model. Among them are methods that find a single best
        clustering to represent the sample, which are based on the
        posterior similarity matrix or a relabelling algorithm.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>LazyLoad:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-05-02 10:08:07 UTC; hornik</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-05-02 13:01:54 UTC</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
</table>
<hr>
<h2 id='mcclust-package'>
Process MCMC Sample of Clusterings.
</h2><span id='topic+mcclust-package'></span><span id='topic+mcclust'></span>

<h3>Description</h3>

<p>Implements methods for processing a sample of (hard) clusterings, e.g. the MCMC output of a Bayesian clustering model.
Among them are methods that find a single best clustering to represent the sample, which are based on the posterior 
similarity matrix or a relabelling algorithm.
</p>


<h3>Details</h3>


<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> mcclust</td>
</tr>
<tr>
 <td style="text-align: left;">
Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
Version: </td><td style="text-align: left;"> 1.0</td>
</tr>
<tr>
 <td style="text-align: left;">
Date: </td><td style="text-align: left;"> 2009-03-12</td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> GPL (&gt;= 2)</td>
</tr>
<tr>
 <td style="text-align: left;">
LazyLoad: </td><td style="text-align: left;"> yes</td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>

<p>Most important functions:<br />
</p>
<p><code>comp.psm</code> for computing posterior similarity matrix (PSM). Based on the PSM <code>maxpear</code> and <code>minbinder</code> provide
several optimization methods to find a clustering with maximal posterior expected adjusted Rand index with the true clustering or 
one that minimizes the posterior expectation of a loss function by Binder (1978). <code>minbinder</code> provides the optimization algorithm of 
Lau and Green.<br />
</p>
<p><code>relabel</code> contains the relabelling algorithm of Stephens (2000).<br /> 
</p>
<p><code>arandi</code> and <code>vi.dist</code> compute distance functions for clusterings, the (adjusted) Rand index and the entropy-based variation of 
information distance.
</p>


<h3>Author(s)</h3>

<p>Arno Fritsch
</p>
<p>Maintainer: Arno Fritsch &lt;arno.fritsch@tu-dortmund.de&gt;
</p>


<h3>References</h3>

<p>Binder, D.A. (1978) Bayesian cluster analysis, <em>Biometrika</em> <b>65</b>, 31&ndash;38.
</p>
<p>Fritsch, A. and Ickstadt, K. (2009) An improved criterion for clustering based on the
posterior similarity matrix, <em>Bayesian Analysis</em>, accepted.
</p>
<p>Lau, J.W. and Green, P.J. (2007) Bayesian model based clustering
procedures, <em>Journal of Computational and Graphical Statistics</em> <b>16</b>, 526&ndash;558.
</p>
<p>Stephens, M. (2000) Dealing with label switching in mixture models. 
<em>Journal of the Royal Statistical Society Series B</em>, <b>62</b>, 795&ndash;809.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(cls.draw2) 
# sample of 500 clusterings from a Bayesian cluster model 
tru.class &lt;- rep(1:8,each=50) 
# the true grouping of the observations
psm2 &lt;- comp.psm(cls.draw2)
# posterior similarity matrix

# optimize criteria based on PSM
mbind2 &lt;- minbinder(psm2)
mpear2 &lt;- maxpear(psm2)

# Relabelling  
k &lt;- apply(cls.draw2,1, function(cl) length(table(cl)))
max.k &lt;- as.numeric(names(table(k))[which.max(table(k))])
relab2 &lt;- relabel(cls.draw2[k==max.k,])

# compare clusterings found by different methods with true grouping
arandi(mpear2$cl, tru.class)
arandi(mbind2$cl, tru.class)
arandi(relab2$cl, tru.class)

</code></pre>

<hr>
<h2 id='arandi'> (Adjusted) Rand Index for Clusterings </h2><span id='topic+arandi'></span>

<h3>Description</h3>

<p>Computes the adjusted or unadjusted Rand index between two clusterings/partitions of the same objects.   
</p>


<h3>Usage</h3>

<pre><code class='language-R'>arandi(cl1, cl2, adjust = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="arandi_+3A_cl1">cl1</code>, <code id="arandi_+3A_cl2">cl2</code></td>
<td>
<p>vectors of cluster memberships (need to have the same lengths).</p>
</td></tr>
<tr><td><code id="arandi_+3A_adjust">adjust</code></td>
<td>
<p>logical. Should index be adjusted? Defaults to TRUE.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Rand index is based on how often the two clusterings agree in the treatment of pairs of observations,
where agreement means that two observations are in/not in the same cluster in both clusterings.<br />
</p>
<p>The adjusted Rand index adjusts for the expected number of chance agreements.<br />
</p>
<p>Formulas of Hubert and Arabie (1985) are used for the computation.</p>


<h3>Author(s)</h3>

<p> Arno Fritsch, <a href="mailto:arno.fritsch@tu-dortmund.de">arno.fritsch@tu-dortmund.de</a></p>


<h3>References</h3>

<p> Hubert, L. and Arabie, P. (1985): Comparing
partitions. <em>Journal of Classification</em>, <b>2</b>, 193&ndash;218. </p>


<h3>See Also</h3>

<p><code><a href="#topic+vi.dist">vi.dist</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'> cl1 &lt;- sample(1:3,10,replace=TRUE)
 cl2 &lt;- c(cl1[1:5], sample(1:3,5,replace=TRUE))
 arandi(cl1,cl2)
 arandi(cl1,cl2,adjust=FALSE)</code></pre>

<hr>
<h2 id='cls.draw1.5'>Sample of Clusterings from Posterior Distribution of Bayesian Cluster Model</h2><span id='topic+cls.draw1.5'></span>

<h3>Description</h3>

<p>Output of a Dirichlet process mixture model with normal components fitted to the data set <code>Ysim1.5</code>.
True clusters are given by <code>rep(1:8,each =50).</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(cls.draw1.5)
</code></pre>


<h3>Format</h3>

<p>matrix with 500 rows and 400 columns. Each row contains a clustering of the 400 observations.
</p>


<h3>Source</h3>

<p>Fritsch, A. and Ickstadt, K. (2009) An improved criterion for clustering based on the
posterior similarity matrix, <em>Bayesian Analysis</em>, accepted.</p>

<hr>
<h2 id='cls.draw2'>Sample of Clusterings from Posterior Distribution of Bayesian Cluster Model</h2><span id='topic+cls.draw2'></span>

<h3>Description</h3>

<p>Output of a Dirichlet process mixture model with normal components fitted to the data set <code>Ysim2</code>.
True clusters are given by <code>rep(1:8,each =50).</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(cls.draw2)
</code></pre>


<h3>Format</h3>

<p>matrix with 500 rows and 400 columns. Each row contains a clustering of the 400 observations.
</p>


<h3>Source</h3>

<p>Fritsch, A. and Ickstadt, K. (2009) An improved criterion for clustering based on the
posterior similarity matrix, <em>Bayesian Analysis</em>, accepted.</p>

<hr>
<h2 id='cltoSim'>Compute Similarity Matrix for a Clustering and vice versa</h2><span id='topic+cltoSim'></span><span id='topic+Simtocl'></span>

<h3>Description</h3>

<p>A similarity matrix is a symmetric matrix whose entry <code class="reqn">[i,j]</code> is 1 if observation <code>i</code>
and <code>j</code> are in the same cluster and 0 otherwise.   
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cltoSim(cl)
Simtocl(Sim)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cltoSim_+3A_cl">cl</code></td>
<td>
<p> vector of cluster memberships</p>
</td></tr>
<tr><td><code id="cltoSim_+3A_sim">Sim</code></td>
<td>
<p> similarity matrix</p>
</td></tr>
</table>


<h3>Warning </h3>

<p><code>Simtocl</code> does <b>not</b> check whether <code>Sim</code> is a valid similarity matrix,
e.g. that <code>Sim[i,j]==1</code> if <code>Sim[i,k]==1</code> and <code>Sim[j,k]==1.</code></p>


<h3>Author(s)</h3>

<p> Arno Fritsch, <a href="mailto:arno.fritsch@tu-dortmund.de">arno.fritsch@tu-dortmund.de</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+comp.psm">comp.psm</a></code> for an average similarity matrix.</p>


<h3>Examples</h3>

<pre><code class='language-R'>cl &lt;- c(3,3,1,2,2)
(Sim &lt;- cltoSim(cl))
Simtocl(Sim) 

# not a valid similarity matrix
(Sim2 &lt;- matrix(c(1,0,1,0,1,1,1,1,1), ncol=3))
Simtocl(Sim2) # no warning
</code></pre>

<hr>
<h2 id='comp.psm'>Estimate Posterior Similarity Matrix</h2><span id='topic+comp.psm'></span>

<h3>Description</h3>

<p>For a sample of clusterings of the same objects the proportion of clusterings in which observation 
<code class="reqn">i</code> and <code class="reqn">j</code> are together in a cluster is computed and a matrix containing all proportions is given out. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>comp.psm(cls)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="comp.psm_+3A_cls">cls</code></td>
<td>
<p> a matrix in which every row corresponds to a clustering of the <code>ncol(cls)</code> objects</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In Bayesian cluster analysis the posterior similarity matrix is a matrix whose entry 
<code class="reqn">[i,j]</code> contains the posterior probability that observation <code class="reqn">i</code> and <code class="reqn">j</code> are together in a cluster.
It is estimated by the proportion of a posteriori clusterings in which <code class="reqn">i</code> and <code class="reqn">j</code> cluster together.
</p>


<h3>Value</h3>

<p>a symmetric <code>ncol(cls)*ncol(cls)</code> matrix 
</p>


<h3>Author(s)</h3>

<p> Arno Fritsch, <a href="mailto:arno.fritsch@tu-dortmund.de">arno.fritsch@tu-dortmund.de</a></p>


<h3>See Also</h3>

 <p><code><a href="#topic+cltoSim">cltoSim</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>(cls &lt;- rbind(c(1,1,2,2),c(1,1,2,2),c(1,2,2,2),c(2,2,1,1)))
comp.psm(cls)
</code></pre>

<hr>
<h2 id='maxpear'>Maximize/Compute Posterior Expected Adjusted Rand Index</h2><span id='topic+maxpear'></span><span id='topic+pear'></span>

<h3>Description</h3>

<p>Based on a posterior similarity matrix of a sample of clusterings <code>maxpear</code> finds the clustering that maximizes the 
posterior expected Rand adjusted index (PEAR) with the true clustering, while <code>pear</code> computes PEAR for several provided clusterings. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>maxpear(psm, cls.draw = NULL, method = c("avg", "comp", "draws",
         "all"), max.k = NULL)

pear(cls,psm)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="maxpear_+3A_psm">psm</code></td>
<td>
<p> a posterior similarity matrix, usually obtained from a call to <code>comp.psm</code>.</p>
</td></tr>
<tr><td><code id="maxpear_+3A_cls">cls</code>, <code id="maxpear_+3A_cls.draw">cls.draw</code></td>
<td>
<p>a matrix in which every row corresponds to a clustering of the <code>ncol(cls)</code> objects.
<code>cls.draw</code> refers to the clusterings that have been used to compute <code>psm</code>, <code>cls.draw</code> has to be provided if 
<code>method="draw"</code> or <code>"all"</code>.</p>
</td></tr>
<tr><td><code id="maxpear_+3A_method">method</code></td>
<td>
<p>the maximization method used. Should be one of <code>"avg"</code>, <code>"comp"</code>, <code>"draws"</code> or <code>"all"</code>. The default is <code>"avg"</code>.</p>
</td></tr>
<tr><td><code id="maxpear_+3A_max.k">max.k</code></td>
<td>
<p> integer, if <code>method="avg"</code> or <code>"comp"</code> the maximum number of clusters up to which the hierarchical clustering is cut.
Defaults to <code>ceiling(nrow(psm)/8)</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For <code>method="avg"</code> and <code>"comp"</code> <code>1-psm</code> is used as a distance matrix for hierarchical clustering with average/complete linkage.
The hierachical clustering is cut for the cluster sizes <code>1:max.k</code> and PEAR computed for these clusterings.<br />
Method <code>"draws"</code> simply computes PEAR for each row of <code>cls.draw</code> and takes the maximum.<br />
If <code>method="all"</code> all maximization methods are applied. 
</p>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>cl</code></td>
<td>
<p>clustering with maximal value of PEAR. If <code>method="all"</code> a matrix containing the clustering with the higest value
of PEAR over all methods in the first row and the clusterings of the individual methods in the next rows.</p>
</td></tr>
<tr><td><code>value</code></td>
<td>
<p>value of PEAR. A vector corresponding to the rows of <code>cl</code> if <code>method="all"</code>.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>the maximization method used.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Arno Fritsch, <a href="mailto:arno.fritsch@tu-dortmund.de">arno.fritsch@tu-dortmund.de</a></p>


<h3>References</h3>

<p>Fritsch, A. and Ickstadt, K. (2009) An improved criterion for clustering based on the
posterior similarity matrix, <em>Bayesian Analysis</em>, accepted.</p>


<h3>See Also</h3>

 <p><code><a href="#topic+comp.psm">comp.psm</a></code> for computing posterior similarity matrix, <code><a href="#topic+minbinder">minbinder</a></code>, <code><a href="#topic+medv">medv</a></code>, <code><a href="#topic+relabel">relabel</a></code>
for other possibilities for processing a sample of clusterings.</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(cls.draw1.5) 
# sample of 500 clusterings from a Bayesian cluster model 
tru.class &lt;- rep(1:8,each=50) 
# the true grouping of the observations
psm1.5 &lt;- comp.psm(cls.draw1.5)
mpear1.5 &lt;- maxpear(psm1.5)
table(mpear1.5$cl, tru.class)

# Does hierachical clustering with Ward's method lead 
# to a better value of PEAR?
hclust.ward &lt;- hclust(as.dist(1-psm1.5), method="ward")
cls.ward &lt;- t(apply(matrix(1:20),1, function(k) cutree(hclust.ward,k=k)))
ward1.5 &lt;- pear(cls.ward, psm1.5)
max(ward1.5) &gt; mpear1.5$value

</code></pre>

<hr>
<h2 id='medv'>Clustering Method of Medvedovic</h2><span id='topic+medv'></span>

<h3>Description</h3>

<p>Based on a posterior similarity matrix of a sample of clusterings <code>medv</code> obtains a clustering by using <code>1-psm</code> as distance 
matrix for hierarchical clustering with complete linkage. The dendrogram is cut at a value <code>h</code> close to 1. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>medv(psm, h=0.99)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="medv_+3A_psm">psm</code></td>
<td>
<p>a posterior similarity matrix, usually obtained from a call to <code>comp.psm</code>.</p>
</td></tr>
<tr><td><code id="medv_+3A_h">h</code></td>
<td>
<p>The height at which the dendrogram is cut.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>vector of cluster memberships. 
</p>


<h3>Author(s)</h3>

<p> Arno Fritsch, <a href="mailto:arno.fritsch@tu-dortmund.de">arno.fritsch@tu-dortmund.de</a></p>


<h3>References</h3>

<p>Medvedovic, M. Yeung, K. and Bumgarner, R. (2004) Bayesian mixture model based clustering
of replicated microarray data, <em>Bioinformatics</em>, <b>20</b>, 1222-1232.</p>


<h3>See Also</h3>

 <p><code><a href="#topic+comp.psm">comp.psm</a></code> for computing posterior similarity matrix, <code><a href="#topic+maxpear">maxpear</a></code>, <code><a href="#topic+minbinder">minbinder</a></code>, <code><a href="#topic+relabel">relabel</a></code>
for other possibilities for processing a sample of clusterings.</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(cls.draw1.5) 
# sample of 500 clusterings from a Bayesian cluster model 
tru.class &lt;- rep(1:8,each=50) 
# the true grouping of the observations
psm1.5 &lt;- comp.psm(cls.draw1.5)
medv1.5 &lt;- medv(psm1.5)
table(medv1.5, tru.class)


</code></pre>

<hr>
<h2 id='minbinder'> Minimize/Compute Posterior Expectation of Binders Loss Function</h2><span id='topic+minbinder'></span><span id='topic+binder'></span><span id='topic+laugreen'></span>

<h3>Description</h3>

<p>Based on a posterior similarity matrix of a sample of clusterings <code>minbinder</code> finds the clustering that minimizes the 
posterior expectation of Binders loss function, while <code>binder</code> computes the posterior expected loss for several provided clusterings. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>minbinder(psm, cls.draw = NULL, method = c("avg", "comp", "draws", 
          "laugreen","all"), max.k = NULL, include.lg = FALSE, 
          start.cl = NULL, tol = 0.001)

binder(cls,psm)

laugreen(psm, start.cl, tol=0.001)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="minbinder_+3A_psm">psm</code></td>
<td>
<p> a posterior similarity matrix, usually obtained from a call to <code>comp.psm</code>.</p>
</td></tr>
<tr><td><code id="minbinder_+3A_cls">cls</code>, <code id="minbinder_+3A_cls.draw">cls.draw</code></td>
<td>
<p>a matrix in which every row corresponds to a clustering of the <code>ncol(cls)</code> objects.
<code>cls.draw</code> refers to the clusterings that have been used to compute <code>psm</code>, <code>cls.draw</code> has to be provided if 
<code>method="draw"</code> or <code>"all"</code>.</p>
</td></tr>
<tr><td><code id="minbinder_+3A_method">method</code></td>
<td>
<p>the maximization method used. Should be one of <code>"avg"</code>, <code>"comp"</code>, <code>"draws"</code>, <code>"laugreen"</code> or <code>"all"</code>.
The default is <code>"avg"</code>.</p>
</td></tr>
<tr><td><code id="minbinder_+3A_max.k">max.k</code></td>
<td>
<p> integer, if <code>method="avg"</code> or <code>"comp"</code> the maximum number of clusters up to which the hierarchical clustering is cut.
Defaults to <code>ceiling(nrow(psm)/4)</code>. </p>
</td></tr>
<tr><td><code id="minbinder_+3A_include.lg">include.lg</code></td>
<td>
<p>logical, should method <code>"laugreen"</code> be included when <code>method="all"</code>? Defaults to FALSE.</p>
</td></tr>
<tr><td><code id="minbinder_+3A_start.cl">start.cl</code></td>
<td>
<p>clustering used as starting point for <code>method="laugreen"</code>. If <code>NULL</code> <code>start.cl= 1:nrow(psm)</code> is used.</p>
</td></tr>
<tr><td><code id="minbinder_+3A_tol">tol</code></td>
<td>
<p>convergence tolerance for <code>method="laugreen"</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The posterior expected loss is the sum of the absolute differences of the indicator function of observation 
<code class="reqn">i</code> and <code class="reqn">j</code> clustering together and the posterior probability that they are in one cluster.
</p>
<p>For <code>method="avg"</code> and <code>"comp"</code> <code>1-psm</code> is used as a distance matrix for hierarchical clustering with average/complete linkage.
The hierachical clustering is cut for the cluster sizes <code>1:max.k</code> and the posterior expected loss is computed for these clusterings.<br />
Method <code>"draws"</code> simply computes the posterior expected loss for each row of <code>cls.draw</code> and takes the minimum.<br />
Method <code>"laugreen"</code> implements the algorithm of Lau and Green (2007), which is based on binary integer programming. Since the method can 
take some time to converge it is only used if explicitly demanded with <code>method="laugreen"</code> or <code>method="all"</code> <em>and</em> <code>include.lg=TRUE</code>.
If <code>method="all"</code> all minimization methods except <code>"laugreen"</code> are applied.  
</p>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>cl</code></td>
<td>
<p>clustering with minimal value of expected loss. If <code>method="all"</code> a matrix containing the clustering with the smallest value
of the expected loss over all methods in the first row and the clusterings of the individual methods in the next rows.</p>
</td></tr>
<tr><td><code>value</code></td>
<td>
<p>value of posterior expected loss. A vector corresponding to the rows of <code>cl</code> if <code>method="all"</code>.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>the maximization method used.</p>
</td></tr>
<tr><td><code>iter.lg</code></td>
<td>
<p>if <code>method="laugreen"</code> the number of iterations the method needed to converge.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p> Arno Fritsch, <a href="mailto:arno.fritsch@tu-dortmund.de">arno.fritsch@tu-dortmund.de</a></p>


<h3>References</h3>

<p>Binder, D.A. (1978) Bayesian cluster analysis, <em>Biometrika</em> <b>65</b>, 31&ndash;38.
</p>
<p>Fritsch, A. and Ickstadt, K. (2009) An improved criterion for clustering based on the
posterior similarity matrix, <em>Bayesian Analysis</em>, accepted.
</p>
<p>Lau, J.W. and Green, P.J. (2007) Bayesian model based clustering
procedures, <em>Journal of Computational and Graphical Statistics</em> <b>16</b>, 526&ndash;558.
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+comp.psm">comp.psm</a></code> for computing posterior similarity matrix, <code><a href="#topic+maxpear">maxpear</a></code>, <code><a href="#topic+medv">medv</a></code>, <code><a href="#topic+relabel">relabel</a></code>
for other possibilities for processing a sample of clusterings. <code><a href="lpSolve.html#topic+lp">lp</a></code> for the linear programming.</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(cls.draw2) 
# sample of 500 clusterings from a Bayesian cluster model 
tru.class &lt;- rep(1:8,each=50) 
# the true grouping of the observations
psm2 &lt;- comp.psm(cls.draw2)
mbind2 &lt;- minbinder(psm2)
table(mbind2$cl, tru.class)

# Does hierachical clustering with Ward's method lead 
# to a lower value of Binders loss?
hclust.ward &lt;- hclust(as.dist(1-psm2), method="ward")
cls.ward &lt;- t(apply(matrix(1:20),1, function(k) cutree(hclust.ward,k=k)))
ward2 &lt;- binder(cls.ward, psm2)
min(ward2) &lt; mbind2$value

# Method laugreen is applied to 40 randomly selected observations
ind &lt;- sample(1:400, 40)
mbind.lg &lt;- minbinder(psm2[ind, ind],cls.draw2[,ind], method="all",
                        include.lg=TRUE)
mbind.lg$value

</code></pre>

<hr>
<h2 id='norm.label'> Norm Labelling of a Clustering </h2><span id='topic+norm.label'></span>

<h3>Description</h3>

<p>Cluster labels of a clusterings are replaced by <code>1:length(table(cl))</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>norm.label(cl)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="norm.label_+3A_cl">cl</code></td>
<td>
<p> vector of cluster memberships </p>
</td></tr>
</table>


<h3>Value</h3>

<p>the clustering with normed labels.</p>


<h3>Author(s)</h3>

<p> Arno Fritsch, <a href="mailto:arno.fritsch@tu-dortmund.de">arno.fritsch@tu-dortmund.de</a></p>


<h3>See Also</h3>

 <p><code><a href="#topic+relabel">relabel</a></code> for labelling a sample of clusterings the same way </p>


<h3>Examples</h3>

<pre><code class='language-R'>(cl &lt;- sample(c(13,12,34), 13, replace=TRUE))
norm.label(cl)

(cl &lt;- sample(c("a","b","f31"), 13, replace=TRUE))
norm.label(cl)
</code></pre>

<hr>
<h2 id='relabel'> Stephens' Relabelling Algorithm for Clusterings</h2><span id='topic+relabel'></span>

<h3>Description</h3>

<p>For a sample of clusterings in which corresponding clusters have different labels the algorithm attempts to bring 
the clusterings to a unique labelling.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>relabel(cls, print.loss = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="relabel_+3A_cls">cls</code></td>
<td>
<p>a matrix in which every row corresponds to a clustering of the <code>ncol(cls)</code> objects.</p>
</td></tr>
<tr><td><code id="relabel_+3A_print.loss">print.loss</code></td>
<td>
<p>logical, should current value of loss function be printed after each iteration? Defaults to TRUE.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The algorithm minimizes the loss function </p>
<p style="text-align: center;"><code class="reqn">\sum_{m=1}^M\sum_{i=1}^n\sum_{j=1}^K-\log\hat{p}_{ij} \cdot I_{\{z_i^{(m)}=j\}}</code>
</p>

<p>over the <code class="reqn">M</code> clusterings, <code class="reqn">n</code> observations and <code class="reqn">K</code> clusters, where <code class="reqn">\hat{p}_{ij}</code> is the
estimated probability that observation <code class="reqn">i</code> belongs to cluster <code class="reqn">j</code> and <code class="reqn">z_i^{(m)}</code> indicates to which cluster
observation <code class="reqn">i</code> belongs in clustering <code class="reqn">m</code>. <code class="reqn">I_{\{.\}}</code> is an indicator function.
</p>
<p>Minimization is achieved by iterating the estimation of <code class="reqn">\hat{p}_{ij}</code> over all clusterings and the
minimization of the loss function in each clustering by permuting the cluster labels. The latter is
done by linear programming.   
</p>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>cls</code></td>
<td>
<p>the input <code>cls</code> with unified labelling.</p>
</td></tr>
<tr><td><code>P</code></td>
<td>
<p>an <code class="reqn">n \times K</code> matrix, where entry <code class="reqn">[i,j]</code> contains the estimated probability that observation
<code class="reqn">i</code> belongs to cluster <code class="reqn">j</code>.</p>
</td></tr>
<tr><td><code>loss.val</code></td>
<td>
<p>value of the loss function.</p>
</td></tr>
<tr><td><code>cl</code></td>
<td>
<p>vector of cluster memberships that have the highest probabilities <code class="reqn">\hat{p}_{ij}</code>.</p>
</td></tr> 
</table>


<h3>Warning</h3>

<p>The algorithm assumes that the number of clusters <code class="reqn">K</code> is fixed. If this is not the case
<code class="reqn">K</code> is taken to be the most common number of clusters. Clusterings with other numbers of clusters are discarded
and a warning is issued.   
</p>


<h3>Note</h3>

 
<p>The implementation is a variant of the algorithm of Stephens which is originally applied to draws of parameters 
for each observation, not to cluster labels.   
</p>


<h3>Author(s)</h3>

<p> Arno Fritsch, <a href="mailto:arno.fritsch@tu-dortmund.de">arno.fritsch@tu-dortmund.de</a></p>


<h3>References</h3>

<p>Stephens, M. (2000) Dealing with label switching in mixture models. 
<em>Journal of the Royal Statistical Society Series B</em>, <b>62</b>, 795&ndash;809. </p>


<h3>See Also</h3>

 <p><code><a href="lpSolve.html#topic+lp.transport">lp.transport</a></code> for the linear programming, <code><a href="#topic+maxpear">maxpear</a></code>, <code><a href="#topic+minbinder">minbinder</a></code>, <code><a href="#topic+medv">medv</a></code>
for other possibilities of processing a sample of clusterings.</p>


<h3>Examples</h3>

<pre><code class='language-R'>(cls &lt;- rbind(c(1,1,2,2),c(1,1,2,2),c(1,2,2,2),c(2,2,1,1)))
# group 2 in clustering 4 corresponds to group 1 in clustering 1-3.
cls.relab &lt;- relabel(cls)
cls.relab$cls
</code></pre>

<hr>
<h2 id='vi.dist'> Variation of Information Distance for Clusterings </h2><span id='topic+vi.dist'></span>

<h3>Description</h3>

<p>Computes the 'variation of information' distance of Meila (2007) between two clusterings/partitions of the same objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vi.dist(cl1, cl2, parts = FALSE, base = 2)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="vi.dist_+3A_cl1">cl1</code>, <code id="vi.dist_+3A_cl2">cl2</code></td>
<td>
<p> vectors of cluster memberships (need to have the same lengths).</p>
</td></tr>
<tr><td><code id="vi.dist_+3A_parts">parts</code></td>
<td>
<p> logical; should the two conditional entropies also be returned?</p>
</td></tr>
<tr><td><code id="vi.dist_+3A_base">base</code></td>
<td>
<p> base of logarithm used for computation of entropy and mutual information.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The variation of information distance is the sum of the two conditional entropies of one clustering given the other.
For details see Meila (2007).
</p>


<h3>Value</h3>

<p>The VI distance. If <code>parts=TRUE</code> the two conditional entropies are appended. 
</p>


<h3>Author(s)</h3>

<p> Arno Fritsch, <a href="mailto:arno.fritsch@tu-dortmund.de">arno.fritsch@tu-dortmund.de</a></p>


<h3>References</h3>

<p>Meila, M. (2007) Comparing Clusterings - an Information Based Distance.
<em>Journal of Multivariate Analysis</em>, <b>98</b>, 873 &ndash; 895.</p>


<h3>See Also</h3>

 <p><code><a href="#topic+arandi">arandi</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'> cl1 &lt;- sample(1:3,10,replace=TRUE)
 cl2 &lt;- c(cl1[1:5], sample(1:3,5,replace=TRUE))
 vi.dist(cl1,cl2)
 vi.dist(cl1,cl2, parts=TRUE)
</code></pre>

<hr>
<h2 id='Ysim1.5'>Simulated 3-dimensional Normal Data Containing 8 Clusters</h2><span id='topic+Ysim1.5'></span>

<h3>Description</h3>

<p>Cluster means are given by the 8 possible values of <code class="reqn">(\pm 1.5,\pm 1.5, \pm 1.5)</code> to which standard normal
noise was added. True clusters are given by <code>rep(1:8,each =50).</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(Ysim1.5)
</code></pre>


<h3>Format</h3>

<p>matrix with 400 rows and 3 columns.
</p>


<h3>Source</h3>

<p>Simulated by <br />
<code>
    1.5 * matrix(c(rep(c(1,1,1),50), rep(c(1,1,-1),50), rep(c(1,-1,1),50), rep(c(-1,1,1),50),
    rep(c(1,-1,-1),50), rep(c(-1,1,-1),50), rep(c(-1,-1,1),50), rep(c(-1,-1,-1),50)), 
    byrow=TRUE, ncol=3) + matrix(rnorm( 400*3),ncol=3)
    </code>
</p>


<h3>References</h3>

<p>Fritsch, A. and Ickstadt, K. (2008) An improved criterion for clustering based on the
posterior similarity matrix, <em>Bayesian Analysis</em>, accepted.</p>

<hr>
<h2 id='Ysim2'>Simulated 3-dimensional Normal Data Containing 8 Clusters</h2><span id='topic+Ysim2'></span>

<h3>Description</h3>

<p>Cluster means are given by the 8 possible values of <code class="reqn">(\pm 2,\pm 2, \pm 2)</code> to which standard normal
noise was added. True clusters are given by <code>rep(1:8,each =50).</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(Ysim2)
</code></pre>


<h3>Format</h3>

<p>matrix with 400 rows and 3 columns.
</p>


<h3>Source</h3>

<p>Simulated by<br /> 
<code>
     2 * matrix(c(rep(c(1,1,1),50), rep(c(1,1,-1),50), rep(c(1,-1,1),50), rep(c(-1,1,1),50),
     rep(c(1,-1,-1),50), rep(c(-1,1,-1),50), rep(c(-1,-1,1),50), rep(c(-1,-1,-1),50)),
     byrow=TRUE, ncol=3) + matrix(rnorm( 400*3),ncol=3)
    </code></p>


<h3>References</h3>

<p>Fritsch, A. and Ickstadt, K. (2009) An improved criterion for clustering based on the
posterior similarity matrix, <em>Bayesian Analysis</em>, accepted.</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
