<!DOCTYPE html><html><head><title>Help for package scoringutils</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {scoringutils}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#scoringutils-package'><p>scoringutils: Utilities for Scoring and Assessing Predictions</p></a></li>
<li><a href='#abs_error'><p>Absolute Error</p></a></li>
<li><a href='#add_coverage'><p>Add coverage of central prediction intervals</p></a></li>
<li><a href='#ae_median_quantile'><p>Absolute Error of the Median (Quantile-based Version)</p></a></li>
<li><a href='#ae_median_sample'><p>Absolute Error of the Median (Sample-based Version)</p></a></li>
<li><a href='#avail_forecasts'><p>Display Number of Forecasts Available</p></a></li>
<li><a href='#available_metrics'><p>Available metrics in scoringutils</p></a></li>
<li><a href='#bias_quantile'><p>Determines Bias of Quantile Forecasts</p></a></li>
<li><a href='#bias_range'><p>Determines Bias of Quantile Forecasts based on the range of the</p>
prediction intervals</a></li>
<li><a href='#bias_sample'><p>Determines bias of forecasts</p></a></li>
<li><a href='#brier_score'><p>Brier Score</p></a></li>
<li><a href='#check_equal_length'><p>Check Length</p></a></li>
<li><a href='#check_forecasts'><p>Check forecasts</p></a></li>
<li><a href='#check_metrics'><p>Check whether the desired metrics are available in scoringutils</p></a></li>
<li><a href='#check_not_null'><p>Check Variable is not NULL</p></a></li>
<li><a href='#check_predictions'><p>Check Prediction Input For Lower-level Scoring Functions</p></a></li>
<li><a href='#check_quantiles'><p>Check that quantiles are valid</p></a></li>
<li><a href='#check_summary_params'><p>Check input parameters for <code>summarise_scores()</code></p></a></li>
<li><a href='#check_true_values'><p>Check Observed Value Input For Lower-level Scoring Functions</p></a></li>
<li><a href='#collapse_messages'><p>Collapse several messages to one</p></a></li>
<li><a href='#compare_two_models'><p>Compare Two Models Based on Subset of Common Forecasts</p></a></li>
<li><a href='#correlation'><p>Correlation Between Metrics</p></a></li>
<li><a href='#crps_sample'><p>Ranked Probability Score</p></a></li>
<li><a href='#delete_columns'><p>Delete Columns From a Data.table</p></a></li>
<li><a href='#dss_sample'><p>Dawid-Sebastiani Score</p></a></li>
<li><a href='#example_binary'><p>Binary Forecast Example Data</p></a></li>
<li><a href='#example_continuous'><p>Continuous Forecast Example Data</p></a></li>
<li><a href='#example_integer'><p>Integer Forecast Example Data</p></a></li>
<li><a href='#example_point'><p>Point Forecast Example Data</p></a></li>
<li><a href='#example_quantile'><p>Quantile Example Data</p></a></li>
<li><a href='#example_quantile_forecasts_only'><p>Quantile Example Data - Forecasts only</p></a></li>
<li><a href='#example_truth_only'><p>Truth data only</p></a></li>
<li><a href='#find_duplicates'><p>Find duplicate forecasts</p></a></li>
<li><a href='#geom_mean_helper'><p>Calculate Geometric Mean</p></a></li>
<li><a href='#get_forecast_unit'><p>Get unit of a single forecast</p></a></li>
<li><a href='#get_prediction_type'><p>Get prediction type of a forecast</p></a></li>
<li><a href='#get_protected_columns'><p>Get protected columns from a data frame</p></a></li>
<li><a href='#get_target_type'><p>Get type of the target true values of a forecast</p></a></li>
<li><a href='#infer_rel_skill_metric'><p>Infer metric for pairwise comparisons</p></a></li>
<li><a href='#interval_score'><p>Interval Score</p></a></li>
<li><a href='#is_scoringutils_check'><p>Check whether object has been checked with check_forecasts()</p></a></li>
<li><a href='#log_shift'><p>Log transformation with an additive shift</p></a></li>
<li><a href='#logs_binary'><p>Log Score for Binary outcomes</p></a></li>
<li><a href='#logs_sample'><p>Logarithmic score</p></a></li>
<li><a href='#mad_sample'><p>Determine dispersion of a probabilistic forecast</p></a></li>
<li><a href='#make_NA'><p>Make Rows NA in Data for Plotting</p></a></li>
<li><a href='#merge_pred_and_obs'><p>Merge Forecast Data And Observations</p></a></li>
<li><a href='#metrics'><p>Summary information for selected metrics</p></a></li>
<li><a href='#pairwise_comparison'><p>Do Pairwise Comparisons of Scores</p></a></li>
<li><a href='#pairwise_comparison_one_group'><p>Do Pairwise Comparison for one Set of Forecasts</p></a></li>
<li><a href='#permutation_test'><p>Simple permutation test</p></a></li>
<li><a href='#pit'><p>Probability Integral Transformation (data.frame Format)</p></a></li>
<li><a href='#pit_sample'><p>Probability Integral Transformation (sample-based version)</p></a></li>
<li><a href='#plot_avail_forecasts'><p>Visualise Where Forecasts Are Available</p></a></li>
<li><a href='#plot_correlation'><p>Plot Correlation Between Metrics</p></a></li>
<li><a href='#plot_heatmap'><p>Create a Heatmap of a Scoring Metric</p></a></li>
<li><a href='#plot_interval_coverage'><p>Plot Interval Coverage</p></a></li>
<li><a href='#plot_pairwise_comparison'><p>Plot Heatmap of Pairwise Comparisons</p></a></li>
<li><a href='#plot_pit'><p>PIT Histogram</p></a></li>
<li><a href='#plot_predictions'><p>Plot Predictions vs True Values</p></a></li>
<li><a href='#plot_quantile_coverage'><p>Plot Quantile Coverage</p></a></li>
<li><a href='#plot_ranges'><p>Plot Metrics by Range of the Prediction Interval</p></a></li>
<li><a href='#plot_score_table'><p>Plot Coloured Score Table</p></a></li>
<li><a href='#plot_wis'><p>Plot Contributions to the Weighted Interval Score</p></a></li>
<li><a href='#prediction_is_quantile'><p>Check if predictions are quantile forecasts</p></a></li>
<li><a href='#print.scoringutils_check'><p>Print output from <code>check_forecasts()</code></p></a></li>
<li><a href='#quantile_score'><p>Quantile Score</p></a></li>
<li><a href='#quantile_to_range_long'><p>Change Data from a Plain Quantile Format to a Long Range Format</p></a></li>
<li><a href='#range_long_to_quantile'><p>Change Data from a Range Format to a Quantile Format</p></a></li>
<li><a href='#sample_to_quantile'><p>Change Data from a Sample Based Format to a Quantile Format</p></a></li>
<li><a href='#sample_to_range_long'><p>Change Data from a Sample Based Format to a Long Interval Range Format</p></a></li>
<li><a href='#score'><p>Evaluate forecasts</p></a></li>
<li><a href='#score_binary'><p>Evaluate forecasts in a Binary Format</p></a></li>
<li><a href='#score_quantile'><p>Evaluate forecasts in a Quantile-Based Format</p></a></li>
<li><a href='#score_sample'><p>Evaluate forecasts in a Sample-Based Format (Integer or Continuous)</p></a></li>
<li><a href='#se_mean_sample'><p>Squared Error of the Mean (Sample-based Version)</p></a></li>
<li><a href='#set_forecast_unit'><p>Set unit of a single forecast manually</p></a></li>
<li><a href='#squared_error'><p>Squared Error</p></a></li>
<li><a href='#summarise_scores'><p>Summarise scores as produced by <code>score()</code></p></a></li>
<li><a href='#theme_scoringutils'><p>Scoringutils ggplot2 theme</p></a></li>
<li><a href='#transform_forecasts'><p>Transform forecasts and observed values</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Utilities for Scoring and Assessing Predictions</td>
</tr>
<tr>
<td>Version:</td>
<td>1.2.2</td>
</tr>
<tr>
<td>Language:</td>
<td>en-GB</td>
</tr>
<tr>
<td>Description:</td>
<td>
    Provides a collection of metrics and proper scoring rules 
    (Tilmann Gneiting &amp; Adrian E Raftery (2007) 
    &lt;<a href="https://doi.org/10.1198%2F016214506000001437">doi:10.1198/016214506000001437</a>&gt;, Jordan, A., Kr√ºger, F., &amp; Lerch, S. (2019)
    &lt;<a href="https://doi.org/10.18637%2Fjss.v090.i12">doi:10.18637/jss.v090.i12</a>&gt;) within a consistent framework for 
    evaluation, comparison and visualisation of forecasts. 
    In addition to proper scoring rules, functions are provided to assess 
    bias, sharpness and calibration 
    (Sebastian Funk, Anton Camacho, Adam J. Kucharski, Rachel Lowe, Rosalind
    M. Eggo, W. John Edmunds (2019) &lt;<a href="https://doi.org/10.1371%2Fjournal.pcbi.1006785">doi:10.1371/journal.pcbi.1006785</a>&gt;) of 
    forecasts. 
    Several types of predictions (e.g. binary, discrete, continuous) which may 
    come in different formats (e.g. forecasts represented by predictive samples 
    or by quantiles of the predictive distribution) can be evaluated. 
    Scoring metrics can be used either through a convenient data.frame format, 
    or can be applied as individual functions in a vector / matrix format. 
    All functionality has been implemented with a focus on performance and is 
    robustly tested. Find more information about the package in the 
    accompanying paper (&lt;<a href="https://doi.org/10.48550%2FarXiv.2205.07090">doi:10.48550/arXiv.2205.07090</a>&gt;).</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>Imports:</td>
<td>data.table, ggdist (&ge; 3.2.0), ggplot2 (&ge; 3.4.0), lifecycle,
methods, rlang, scoringRules, stats</td>
</tr>
<tr>
<td>Suggests:</td>
<td>kableExtra, knitr, magrittr, rmarkdown, testthat, vdiffr</td>
</tr>
<tr>
<td>Config/Needs/website:</td>
<td>r-lib/pkgdown, amirmasoudabdol/preferably</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://doi.org/10.48550/arXiv.2205.07090">https://doi.org/10.48550/arXiv.2205.07090</a>,
<a href="https://epiforecasts.io/scoringutils/">https://epiforecasts.io/scoringutils/</a>,
<a href="https://github.com/epiforecasts/scoringutils">https://github.com/epiforecasts/scoringutils</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/epiforecasts/scoringutils/issues">https://github.com/epiforecasts/scoringutils/issues</a></td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.6)</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-11-29 14:54:39 UTC; nikos</td>
</tr>
<tr>
<td>Author:</td>
<td>Nikos Bosse <a href="https://orcid.org/0000-0002-7750-5280"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut,
    cre],
  Sam Abbott <a href="https://orcid.org/0000-0001-8057-8037"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut],
  Hugo Gruson <a href="https://orcid.org/0000-0002-4094-1476"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut],
  Johannes Bracher <a href="https://orcid.org/0000-0002-3777-1410"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [ctb],
  Sebastian Funk [aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Nikos Bosse &lt;nikosbosse@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-11-29 15:50:10 UTC</td>
</tr>
</table>
<hr>
<h2 id='scoringutils-package'>scoringutils: Utilities for Scoring and Assessing Predictions</h2><span id='topic+scoringutils'></span><span id='topic+scoringutils-package'></span>

<h3>Description</h3>

<p>Provides a collection of metrics and proper scoring rules (Tilmann Gneiting &amp; Adrian E Raftery (2007) <a href="https://doi.org/10.1198/016214506000001437">doi:10.1198/016214506000001437</a>, Jordan, A., Kr√ºger, F., &amp; Lerch, S. (2019) <a href="https://doi.org/10.18637/jss.v090.i12">doi:10.18637/jss.v090.i12</a>) within a consistent framework for evaluation, comparison and visualisation of forecasts. In addition to proper scoring rules, functions are provided to assess bias, sharpness and calibration (Sebastian Funk, Anton Camacho, Adam J. Kucharski, Rachel Lowe, Rosalind M. Eggo, W. John Edmunds (2019) <a href="https://doi.org/10.1371/journal.pcbi.1006785">doi:10.1371/journal.pcbi.1006785</a>) of forecasts. Several types of predictions (e.g. binary, discrete, continuous) which may come in different formats (e.g. forecasts represented by predictive samples or by quantiles of the predictive distribution) can be evaluated. Scoring metrics can be used either through a convenient data.frame format, or can be applied as individual functions in a vector / matrix format. All functionality has been implemented with a focus on performance and is robustly tested. Find more information about the package in the accompanying paper (<a href="https://doi.org/10.48550/arXiv.2205.07090">doi:10.48550/arXiv.2205.07090</a>).
</p>
<p>Provides a collection of metrics and proper scoring rules (Tilmann Gneiting &amp; Adrian E Raftery (2007) <a href="https://doi.org/10.1198/016214506000001437">doi:10.1198/016214506000001437</a>, Jordan, A., Kr√ºger, F., &amp; Lerch, S. (2019) <a href="https://doi.org/10.18637/jss.v090.i12">doi:10.18637/jss.v090.i12</a>) within a consistent framework for evaluation, comparison and visualisation of forecasts. In addition to proper scoring rules, functions are provided to assess bias, sharpness and calibration (Sebastian Funk, Anton Camacho, Adam J. Kucharski, Rachel Lowe, Rosalind M. Eggo, W. John Edmunds (2019) <a href="https://doi.org/10.1371/journal.pcbi.1006785">doi:10.1371/journal.pcbi.1006785</a>) of forecasts. Several types of predictions (e.g. binary, discrete, continuous) which may come in different formats (e.g. forecasts represented by predictive samples or by quantiles of the predictive distribution) can be evaluated. Scoring metrics can be used either through a convenient data.frame format, or can be applied as individual functions in a vector / matrix format. All functionality has been implemented with a focus on performance and is robustly tested. Find more information about the package in the accompanying paper (<a href="https://doi.org/10.48550/arXiv.2205.07090">doi:10.48550/arXiv.2205.07090</a>).
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Nikos Bosse <a href="mailto:nikosbosse@gmail.com">nikosbosse@gmail.com</a> (<a href="https://orcid.org/0000-0002-7750-5280">ORCID</a>)
</p>
<p>Authors:
</p>

<ul>
<li><p> Sam Abbott <a href="mailto:contact@samabbott.co.uk">contact@samabbott.co.uk</a> (<a href="https://orcid.org/0000-0001-8057-8037">ORCID</a>)
</p>
</li>
<li><p> Hugo Gruson <a href="mailto:hugo.gruson@lshtm.ac.uk">hugo.gruson@lshtm.ac.uk</a> (<a href="https://orcid.org/0000-0002-4094-1476">ORCID</a>)
</p>
</li>
<li><p> Sebastian Funk <a href="mailto:sebastian.funk@lshtm.ac.uk">sebastian.funk@lshtm.ac.uk</a>
</p>
</li></ul>

<p>Other contributors:
</p>

<ul>
<li><p> Johannes Bracher <a href="mailto:johannes.bracher@kit.edu">johannes.bracher@kit.edu</a> (<a href="https://orcid.org/0000-0002-3777-1410">ORCID</a>) [contributor]
</p>
</li></ul>



<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://doi.org/10.48550/arXiv.2205.07090">doi:10.48550/arXiv.2205.07090</a>
</p>
</li>
<li> <p><a href="https://epiforecasts.io/scoringutils/">https://epiforecasts.io/scoringutils/</a>
</p>
</li>
<li> <p><a href="https://github.com/epiforecasts/scoringutils">https://github.com/epiforecasts/scoringutils</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/epiforecasts/scoringutils/issues">https://github.com/epiforecasts/scoringutils/issues</a>
</p>
</li></ul>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://doi.org/10.48550/arXiv.2205.07090">doi:10.48550/arXiv.2205.07090</a>
</p>
</li>
<li> <p><a href="https://epiforecasts.io/scoringutils/">https://epiforecasts.io/scoringutils/</a>
</p>
</li>
<li> <p><a href="https://github.com/epiforecasts/scoringutils">https://github.com/epiforecasts/scoringutils</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/epiforecasts/scoringutils/issues">https://github.com/epiforecasts/scoringutils/issues</a>
</p>
</li></ul>


<hr>
<h2 id='abs_error'>Absolute Error</h2><span id='topic+abs_error'></span>

<h3>Description</h3>

<p>Calculate absolute error as
</p>
<p style="text-align: center;"><code class="reqn">
  \textrm{abs}(\textrm{true\_value} - \textrm{median\_prediction})
</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>abs_error(true_values, predictions)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="abs_error_+3A_true_values">true_values</code></td>
<td>
<p>A vector with the true observed values of size n</p>
</td></tr>
<tr><td><code id="abs_error_+3A_predictions">predictions</code></td>
<td>
<p>numeric vector with predictions, corresponding to the
quantiles in a second vector, <code>quantiles</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>vector with the absolute error
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ae_median_sample">ae_median_sample()</a></code>, <code><a href="#topic+ae_median_quantile">ae_median_quantile()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>true_values &lt;- rnorm(30, mean = 1:30)
predicted_values &lt;- rnorm(30, mean = 1:30)
abs_error(true_values, predicted_values)
</code></pre>

<hr>
<h2 id='add_coverage'>Add coverage of central prediction intervals</h2><span id='topic+add_coverage'></span>

<h3>Description</h3>

<p>Adds a column with the coverage of central prediction intervals
to unsummarised scores as produced by <code><a href="#topic+score">score()</a></code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>add_coverage(scores, by, ranges = c(50, 90))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="add_coverage_+3A_scores">scores</code></td>
<td>
<p>A data.table of scores as produced by <code><a href="#topic+score">score()</a></code>.</p>
</td></tr>
<tr><td><code id="add_coverage_+3A_by">by</code></td>
<td>
<p>character vector with column names to add the coverage for.</p>
</td></tr>
<tr><td><code id="add_coverage_+3A_ranges">ranges</code></td>
<td>
<p>numeric vector of the ranges of the central prediction intervals
for which coverage values shall be added.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The coverage values that are added are computed according to the values
specified in <code>by</code>. If, for example, <code>by = "model"</code>, then there will be one
coverage value for every model and <code><a href="#topic+add_coverage">add_coverage()</a></code> will compute the coverage
for every model across the values present in all other columns which define
the unit of a single forecast.
</p>


<h3>Value</h3>

<p>a data.table with unsummarised scores with columns added for the
coverage of the central prediction intervals. While the overall data.table
is still unsummarised, note that for the coverage columns some level of
summary is present according to the value specified in <code>by</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(magrittr) # pipe operator

score(example_quantile) %&gt;%
  add_coverage(by = c("model", "target_type")) %&gt;%
  summarise_scores(by = c("model", "target_type")) %&gt;%
  summarise_scores(fun = signif, digits = 2)
</code></pre>

<hr>
<h2 id='ae_median_quantile'>Absolute Error of the Median (Quantile-based Version)</h2><span id='topic+ae_median_quantile'></span>

<h3>Description</h3>

<p>Absolute error of the median calculated as
</p>
<p style="text-align: center;"><code class="reqn">
  \textrm{abs}(\textrm{true\_value} - \textrm{prediction})
</code>
</p>

<p>The function was created for internal use within <code><a href="#topic+score">score()</a></code>, but can also
used as a standalone function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ae_median_quantile(true_values, predictions, quantiles = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ae_median_quantile_+3A_true_values">true_values</code></td>
<td>
<p>A vector with the true observed values of size n</p>
</td></tr>
<tr><td><code id="ae_median_quantile_+3A_predictions">predictions</code></td>
<td>
<p>numeric vector with predictions, corresponding to the
quantiles in a second vector, <code>quantiles</code>.</p>
</td></tr>
<tr><td><code id="ae_median_quantile_+3A_quantiles">quantiles</code></td>
<td>
<p>numeric vector that denotes the quantile for the values
in <code>predictions</code>. Only those predictions where <code>quantiles == 0.5</code> will
be kept. If <code>quantiles</code> is <code>NULL</code>, then all <code>predictions</code> and
<code>true_values</code> will be used (this is then the same as <code><a href="#topic+abs_error">abs_error()</a></code>)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>vector with the scoring values
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ae_median_sample">ae_median_sample()</a></code>, <code><a href="#topic+abs_error">abs_error()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>true_values &lt;- rnorm(30, mean = 1:30)
predicted_values &lt;- rnorm(30, mean = 1:30)
ae_median_quantile(true_values, predicted_values, quantiles = 0.5)
</code></pre>

<hr>
<h2 id='ae_median_sample'>Absolute Error of the Median (Sample-based Version)</h2><span id='topic+ae_median_sample'></span>

<h3>Description</h3>

<p>Absolute error of the median calculated as
</p>
<p style="text-align: center;"><code class="reqn">%
  \textrm{abs}(\textrm{true\_value} - \textrm{median\_prediction})
</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>ae_median_sample(true_values, predictions)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ae_median_sample_+3A_true_values">true_values</code></td>
<td>
<p>A vector with the true observed values of size n</p>
</td></tr>
<tr><td><code id="ae_median_sample_+3A_predictions">predictions</code></td>
<td>
<p>nxN matrix of predictive samples, n (number of rows) being
the number of data points and N (number of columns) the number of Monte
Carlo samples. Alternatively, predictions can just be a vector of size n.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>vector with the scoring values
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ae_median_quantile">ae_median_quantile()</a></code>, <code><a href="#topic+abs_error">abs_error()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>true_values &lt;- rnorm(30, mean = 1:30)
predicted_values &lt;- rnorm(30, mean = 1:30)
ae_median_sample(true_values, predicted_values)
</code></pre>

<hr>
<h2 id='avail_forecasts'>Display Number of Forecasts Available</h2><span id='topic+avail_forecasts'></span>

<h3>Description</h3>

<p>Given a data set with forecasts, count the number of available forecasts
for arbitrary grouping (e.g. the number of forecasts per model, or the
number of forecasts per model and location).
This is useful to determine whether there are any missing forecasts.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>avail_forecasts(data, by = NULL, collapse = c("quantile", "sample"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="avail_forecasts_+3A_data">data</code></td>
<td>
<p>A data.frame or data.table with the predictions and observations.
For scoring using <code><a href="#topic+score">score()</a></code>, the following columns need to be present:
</p>

<ul>
<li> <p><code>true_value</code> - the true observed values
</p>
</li>
<li> <p><code>prediction</code> - predictions or predictive samples for one
true value. (You only don't need to provide a prediction column if
you want to score quantile forecasts in a wide range format.)</p>
</li></ul>

<p>For scoring integer and continuous forecasts a <code>sample</code> column is needed:
</p>

<ul>
<li> <p><code>sample</code> - an index to identify the predictive samples in the
prediction column generated by one model for one true value. Only
necessary for continuous and integer forecasts, not for
binary predictions.</p>
</li></ul>

<p>For scoring predictions in a quantile-format forecast you should provide
a column called <code>quantile</code>:
</p>

<ul>
<li> <p><code>quantile</code>: quantile to which the prediction corresponds
</p>
</li></ul>

<p>In addition a <code>model</code> column is suggested and if not present this will be
flagged and added to the input data with all forecasts assigned as an
&quot;unspecified model&quot;).
</p>
<p>You can check the format of your data using <code><a href="#topic+check_forecasts">check_forecasts()</a></code> and there
are examples for each format (<a href="#topic+example_quantile">example_quantile</a>, <a href="#topic+example_continuous">example_continuous</a>,
<a href="#topic+example_integer">example_integer</a>, and <a href="#topic+example_binary">example_binary</a>).</p>
</td></tr>
<tr><td><code id="avail_forecasts_+3A_by">by</code></td>
<td>
<p>character vector or <code>NULL</code> (the default) that denotes the
categories over which the number of forecasts should be counted.
By default (<code>by = NULL</code>) this will be the unit of a single forecast (i.e.
all available columns (apart from a few &quot;protected&quot; columns such as
'prediction' and 'true value') plus &quot;quantile&quot; or &quot;sample&quot; where present).</p>
</td></tr>
<tr><td><code id="avail_forecasts_+3A_collapse">collapse</code></td>
<td>
<p>character vector (default is <code style="white-space: pre;">&#8288;c("quantile", "sample"&#8288;</code>) with
names of categories for which the number of rows should be collapsed to one
when counting. For example, a single forecast is usually represented by a
set of several quantiles or samples and collapsing these to one makes sure
that a single forecast only gets counted once.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data.table with columns as specified in <code>by</code> and an additional
column with the number of forecasts.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

avail_forecasts(example_quantile,
  collapse = c("quantile"),
  by = c("model", "target_type")
)
</code></pre>

<hr>
<h2 id='available_metrics'>Available metrics in scoringutils</h2><span id='topic+available_metrics'></span>

<h3>Description</h3>

<p>Available metrics in scoringutils
</p>


<h3>Usage</h3>

<pre><code class='language-R'>available_metrics()
</code></pre>


<h3>Value</h3>

<p>A vector with the name of all available metrics
</p>

<hr>
<h2 id='bias_quantile'>Determines Bias of Quantile Forecasts</h2><span id='topic+bias_quantile'></span>

<h3>Description</h3>

<p>Determines bias from quantile forecasts. For an increasing number of
quantiles this measure converges against the sample based bias version
for integer and continuous forecasts.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bias_quantile(predictions, quantiles, true_value)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bias_quantile_+3A_predictions">predictions</code></td>
<td>
<p>vector of length corresponding to the number of quantiles
that holds predictions</p>
</td></tr>
<tr><td><code id="bias_quantile_+3A_quantiles">quantiles</code></td>
<td>
<p>vector of corresponding size with the quantiles for which
predictions were made. If this does not contain the median (0.5) then the
median is imputed as being the mean of the two innermost quantiles.</p>
</td></tr>
<tr><td><code id="bias_quantile_+3A_true_value">true_value</code></td>
<td>
<p>a single true value</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For quantile forecasts, bias is measured as
</p>
<p style="text-align: center;"><code class="reqn">
B_t = (1 - 2 \cdot \max \{i | q_{t,i} \in Q_t \land q_{t,i} \leq x_t\})
 \mathbf{1}( x_t \leq q_{t, 0.5}) \\
+ (1 - 2 \cdot \min \{i | q_{t,i} \in Q_t \land q_{t,i} \geq x_t\})
 1( x_t \geq q_{t, 0.5}),</code>
</p>

<p>where <code class="reqn">Q_t</code> is the set of quantiles that form the predictive
distribution at time <code class="reqn">t</code>. They represent our
belief about what the true value $x_t$ will be. For consistency, we define
<code class="reqn">Q_t</code> such that it always includes the element
<code class="reqn">q_{t, 0} = - \infty</code> and <code class="reqn">q_{t,1} = \infty</code>.
<code class="reqn">1()</code> is the indicator function that is <code class="reqn">1</code> if the
condition is satisfied and $0$ otherwise. In clearer terms, <code class="reqn">B_t</code> is
defined as the maximum percentile rank for which the corresponding quantile
is still below the true value, if the true value is smaller than the
median of the predictive distribution. If the true value is above the
median of the predictive distribution, then $B_t$ is the minimum percentile
rank for which the corresponding quantile is still larger than the true
value. If the true value is exactly the median, both terms cancel out and
<code class="reqn">B_t</code> is zero. For a large enough number of quantiles, the
percentile rank will equal the proportion of predictive samples below the
observed true value, and this metric coincides with the one for
continuous forecasts.
</p>
<p>Bias can assume values between
-1 and 1 and is 0 ideally (i.e. unbiased).
</p>


<h3>Value</h3>

<p>scalar with the quantile bias for a single quantile prediction
</p>


<h3>Author(s)</h3>

<p>Nikos Bosse <a href="mailto:nikosbosse@gmail.com">nikosbosse@gmail.com</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
predictions &lt;- c(
  705.500, 1127.000, 4006.250, 4341.500, 4709.000, 4821.996,
  5340.500, 5451.000, 5703.500, 6087.014, 6329.500, 6341.000,
  6352.500, 6594.986, 6978.500, 7231.000, 7341.500, 7860.004,
  7973.000, 8340.500, 8675.750, 11555.000, 11976.500
)

quantiles &lt;- c(0.01, 0.025, seq(0.05, 0.95, 0.05), 0.975, 0.99)

true_value &lt;- 8062

bias_quantile(predictions, quantiles, true_value = true_value)
</code></pre>

<hr>
<h2 id='bias_range'>Determines Bias of Quantile Forecasts based on the range of the
prediction intervals</h2><span id='topic+bias_range'></span>

<h3>Description</h3>

<p>Determines bias from quantile forecasts based on the range of the
prediction intervals. For an increasing number of quantiles this measure
converges against the sample based bias version for integer and continuous
forecasts.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bias_range(lower, upper, range, true_value)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bias_range_+3A_lower">lower</code></td>
<td>
<p>vector of length corresponding to the number of central
prediction intervals that holds predictions for the lower bounds of a
prediction interval</p>
</td></tr>
<tr><td><code id="bias_range_+3A_upper">upper</code></td>
<td>
<p>vector of length corresponding to the number of central
prediction intervals that holds predictions for the upper bounds of a
prediction interval</p>
</td></tr>
<tr><td><code id="bias_range_+3A_range">range</code></td>
<td>
<p>vector of corresponding size with information about the width
of the central prediction interval</p>
</td></tr>
<tr><td><code id="bias_range_+3A_true_value">true_value</code></td>
<td>
<p>a single true value</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For quantile forecasts, bias is measured as
</p>
<p style="text-align: center;"><code class="reqn">
B_t = (1 - 2 \cdot \max \{i | q_{t,i} \in Q_t \land q_{t,i} \leq x_t\})
\mathbf{1}( x_t \leq q_{t, 0.5}) \\
+ (1 - 2 \cdot \min \{i | q_{t,i} \in Q_t \land q_{t,i} \geq x_t\})
 \mathbf{1}( x_t \geq q_{t, 0.5}),
</code>
</p>

<p>where <code class="reqn">Q_t</code> is the set of quantiles that form the predictive
distribution at time <code class="reqn">t</code>. They represent our
belief about what the true value <code class="reqn">x_t</code> will be. For consistency, we
define
<code class="reqn">Q_t</code> such that it always includes the element
<code class="reqn">q_{t, 0} = - \infty</code> and <code class="reqn">q_{t,1} = \infty</code>.
<code class="reqn">\mathbf{1}()</code> is the indicator function that is <code class="reqn">1</code> if the
condition is satisfied and $0$ otherwise. In clearer terms, <code class="reqn">B_t</code> is
defined as the maximum percentile rank for which the corresponding quantile
is still below the true value, if the true value is smaller than the
median of the predictive distribution. If the true value is above the
median of the predictive distribution, then $B_t$ is the minimum percentile
rank for which the corresponding quantile is still larger than the true
value. If the true value is exactly the median, both terms cancel out and
<code class="reqn">B_t</code> is zero. For a large enough number of quantiles, the
percentile rank will equal the proportion of predictive samples below the
observed true value, and this metric coincides with the one for
continuous forecasts.
</p>
<p>Bias can assume values between
-1 and 1 and is 0 ideally.
</p>


<h3>Value</h3>

<p>scalar with the quantile bias for a single quantile prediction
</p>


<h3>Author(s)</h3>

<p>Nikos Bosse <a href="mailto:nikosbosse@gmail.com">nikosbosse@gmail.com</a>
</p>


<h3>See Also</h3>

<p>bias_quantile bias_sample
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
lower &lt;- c(
  6341.000, 6329.500, 6087.014, 5703.500,
  5451.000, 5340.500, 4821.996, 4709.000,
  4341.500, 4006.250, 1127.000, 705.500
)

upper &lt;- c(
  6341.000, 6352.500, 6594.986, 6978.500,
  7231.000, 7341.500, 7860.004, 7973.000,
  8340.500, 8675.750, 11555.000, 11976.500
)

range &lt;- c(0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 95, 98)

true_value &lt;- 8062

bias_range(
  lower = lower, upper = upper,
  range = range, true_value = true_value
)
</code></pre>

<hr>
<h2 id='bias_sample'>Determines bias of forecasts</h2><span id='topic+bias_sample'></span>

<h3>Description</h3>

<p>Determines bias from predictive Monte-Carlo samples. The function
automatically recognises, whether forecasts are continuous or
integer valued and adapts the Bias function accordingly.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bias_sample(true_values, predictions)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bias_sample_+3A_true_values">true_values</code></td>
<td>
<p>A vector with the true observed values of size n</p>
</td></tr>
<tr><td><code id="bias_sample_+3A_predictions">predictions</code></td>
<td>
<p>nxN matrix of predictive samples, n (number of rows) being
the number of data points and N (number of columns) the number of Monte
Carlo samples. Alternatively, predictions can just be a vector of size n.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For continuous forecasts, Bias is measured as
</p>
<p style="text-align: center;"><code class="reqn">
B_t (P_t, x_t) = 1 - 2 * (P_t (x_t))
</code>
</p>

<p>where <code class="reqn">P_t</code> is the empirical cumulative distribution function of the
prediction for the true value <code class="reqn">x_t</code>. Computationally, <code class="reqn">P_t (x_t)</code> is
just calculated as the fraction of predictive samples for <code class="reqn">x_t</code>
that are smaller than <code class="reqn">x_t</code>.
</p>
<p>For integer valued forecasts, Bias is measured as
</p>
<p style="text-align: center;"><code class="reqn">
B_t (P_t, x_t) = 1 - (P_t (x_t) + P_t (x_t + 1))
</code>
</p>

<p>to adjust for the integer nature of the forecasts.
</p>
<p>In both cases, Bias can assume values between
-1 and 1 and is 0 ideally.
</p>


<h3>Value</h3>

<p>vector of length n with the biases of the predictive samples with
respect to the true values.
</p>


<h3>Author(s)</h3>

<p>Nikos Bosse <a href="mailto:nikosbosse@gmail.com">nikosbosse@gmail.com</a>
</p>


<h3>References</h3>

<p>The integer valued Bias function is discussed in
Assessing the performance of real-time epidemic forecasts: A case study of
Ebola in the Western Area region of Sierra Leone, 2014-15 Funk S, Camacho A,
Kucharski AJ, Lowe R, Eggo RM, et al. (2019) Assessing the performance of
real-time epidemic forecasts: A case study of Ebola in the Western Area
region of Sierra Leone, 2014-15. PLOS Computational Biology 15(2): e1006785.
<a href="https://doi.org/10.1371/journal.pcbi.1006785">doi:10.1371/journal.pcbi.1006785</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## integer valued forecasts
true_values &lt;- rpois(30, lambda = 1:30)
predictions &lt;- replicate(200, rpois(n = 30, lambda = 1:30))
bias_sample(true_values, predictions)

## continuous forecasts
true_values &lt;- rnorm(30, mean = 1:30)
predictions &lt;- replicate(200, rnorm(30, mean = 1:30))
bias_sample(true_values, predictions)
</code></pre>

<hr>
<h2 id='brier_score'>Brier Score</h2><span id='topic+brier_score'></span>

<h3>Description</h3>

<p>Computes the Brier Score for probabilistic forecasts of binary outcomes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>brier_score(true_values, predictions)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="brier_score_+3A_true_values">true_values</code></td>
<td>
<p>A vector with the true observed values of size n with
all values equal to either 0 or 1</p>
</td></tr>
<tr><td><code id="brier_score_+3A_predictions">predictions</code></td>
<td>
<p>A vector with a predicted probability
that true_value = 1.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Brier score is a proper score rule that assesses the accuracy of
probabilistic binary predictions. The outcomes can be either 0 or 1,
the predictions must be a probability that the true outcome will be 1.
</p>
<p>The Brier Score is then computed as the mean squared error between the
probabilistic prediction and the true outcome.
</p>
<p style="text-align: center;"><code class="reqn">
  \textrm{Brier\_Score} = \frac{1}{N} \sum_{t = 1}^{n} (\textrm{prediction}_t -
  \textrm{outcome}_t)^2
</code>
</p>



<h3>Value</h3>

<p>A numeric value with the Brier Score, i.e. the mean squared
error of the given probability forecasts
</p>


<h3>Examples</h3>

<pre><code class='language-R'>true_values &lt;- sample(c(0, 1), size = 30, replace = TRUE)
predictions &lt;- runif(n = 30, min = 0, max = 1)

brier_score(true_values, predictions)
</code></pre>

<hr>
<h2 id='check_equal_length'>Check Length</h2><span id='topic+check_equal_length'></span>

<h3>Description</h3>

<p>Check whether variables all have the same length
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_equal_length(..., one_allowed = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="check_equal_length_+3A_...">...</code></td>
<td>
<p>The variables to check</p>
</td></tr>
<tr><td><code id="check_equal_length_+3A_one_allowed">one_allowed</code></td>
<td>
<p>logical, allow arguments of length one that can be
recycled</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The function returns <code>NULL</code>, but throws an error if variable lengths
differ
</p>

<hr>
<h2 id='check_forecasts'>Check forecasts</h2><span id='topic+check_forecasts'></span>

<h3>Description</h3>

<p>Function to check the input data before running
<code><a href="#topic+score">score()</a></code>.
</p>
<p>The data should come in one of three different formats:
</p>

<ul>
<li><p> A format for binary predictions (see <a href="#topic+example_binary">example_binary</a>)
</p>
</li>
<li><p> A sample-based format for discrete or continuous predictions
(see <a href="#topic+example_continuous">example_continuous</a> and <a href="#topic+example_integer">example_integer</a>)
</p>
</li>
<li><p> A quantile-based format (see <a href="#topic+example_quantile">example_quantile</a>)
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>check_forecasts(data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="check_forecasts_+3A_data">data</code></td>
<td>
<p>A data.frame or data.table with the predictions and observations.
For scoring using <code><a href="#topic+score">score()</a></code>, the following columns need to be present:
</p>

<ul>
<li> <p><code>true_value</code> - the true observed values
</p>
</li>
<li> <p><code>prediction</code> - predictions or predictive samples for one
true value. (You only don't need to provide a prediction column if
you want to score quantile forecasts in a wide range format.)</p>
</li></ul>

<p>For scoring integer and continuous forecasts a <code>sample</code> column is needed:
</p>

<ul>
<li> <p><code>sample</code> - an index to identify the predictive samples in the
prediction column generated by one model for one true value. Only
necessary for continuous and integer forecasts, not for
binary predictions.</p>
</li></ul>

<p>For scoring predictions in a quantile-format forecast you should provide
a column called <code>quantile</code>:
</p>

<ul>
<li> <p><code>quantile</code>: quantile to which the prediction corresponds
</p>
</li></ul>

<p>In addition a <code>model</code> column is suggested and if not present this will be
flagged and added to the input data with all forecasts assigned as an
&quot;unspecified model&quot;).
</p>
<p>You can check the format of your data using <code><a href="#topic+check_forecasts">check_forecasts()</a></code> and there
are examples for each format (<a href="#topic+example_quantile">example_quantile</a>, <a href="#topic+example_continuous">example_continuous</a>,
<a href="#topic+example_integer">example_integer</a>, and <a href="#topic+example_binary">example_binary</a>).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with elements that give information about what <code>scoringutils</code>
thinks you are trying to do and potential issues.
</p>

<ul>
<li> <p><code>target_type</code> the type of the prediction target as inferred from the
input: 'binary', if all values in <code>true_value</code> are either 0 or 1 and values
in <code>prediction</code> are between 0 and 1, 'discrete' if all true values are
integers.
and 'continuous' if not.
</p>
</li>
<li> <p><code>prediction_type</code> inferred type of the prediction. 'quantile', if there is
a column called 'quantile', else 'discrete' if all values in <code>prediction</code>
are integer, else 'continuous.
</p>
</li>
<li> <p><code>forecast_unit</code> unit of a single forecast, i.e. the grouping that uniquely
defines a single forecast. This is assumed to be all
present columns apart from the following protected columns:
<code>c("prediction", "true_value", "sample", "quantile","range", "boundary")</code>.
It is important that you remove all unnecessary columns before scoring.
</p>
</li>
<li> <p><code>rows_per_forecast</code> a data.frame that shows how many rows (usually
quantiles or samples there are available per forecast. If a forecast model
has several entries, then there a forecasts with differing numbers of
quantiles / samples.
</p>
</li>
<li> <p><code>unique_values</code> A data.frame that shows how many unique values there are
present per model and column in the data. This doesn't directly show missing
values, but rather the maximum number of unique values across the whole data.
</p>
</li>
<li> <p><code>warnings</code> A vector with warnings. These can be ignored if you know what
you are doing.
</p>
</li>
<li> <p><code>errors</code> A vector with issues that will cause an error when running
<code><a href="#topic+score">score()</a></code>.
</p>
</li>
<li> <p><code>messages</code> A verbal explanation of the information provided above.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Nikos Bosse <a href="mailto:nikosbosse@gmail.com">nikosbosse@gmail.com</a>
</p>


<h3>See Also</h3>

<p>Function to move from sample-based to quantile format:
<code><a href="#topic+sample_to_quantile">sample_to_quantile()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>check &lt;- check_forecasts(example_quantile)
print(check)
check_forecasts(example_binary)
</code></pre>

<hr>
<h2 id='check_metrics'>Check whether the desired metrics are available in scoringutils</h2><span id='topic+check_metrics'></span>

<h3>Description</h3>

<p>Helper function to check whether desired metrics are
available. If the input is <code>NULL</code>, all metrics will be returned.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_metrics(metrics)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="check_metrics_+3A_metrics">metrics</code></td>
<td>
<p>character vector with desired metrics</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A character vector with metrics that can be used for downstream
computation
</p>

<hr>
<h2 id='check_not_null'>Check Variable is not NULL</h2><span id='topic+check_not_null'></span>

<h3>Description</h3>

<p>Check whether a certain variable is not <code>NULL</code> and return the name of that
variable and the function call where the variable is missing. This function
is a helper function that should only be called within other functions
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_not_null(...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="check_not_null_+3A_...">...</code></td>
<td>
<p>The variables to check</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The function returns <code>NULL</code>, but throws an error if the variable is
missing.
</p>

<hr>
<h2 id='check_predictions'>Check Prediction Input For Lower-level Scoring Functions</h2><span id='topic+check_predictions'></span>

<h3>Description</h3>

<p>Helper function to check inputs for lower-level score functions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_predictions(
  predictions,
  true_values = NULL,
  type = c("continuous", "integer", "binary"),
  class = c("vector", "matrix")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="check_predictions_+3A_predictions">predictions</code></td>
<td>
<p>an object with predictions. Depending on whether
<code>class = vector</code> or <code>class = "matrix"</code> this can be either a vector of length
n (corresponding to the length of the true_values) or a nxN matrix of
predictive samples, n (number of rows) being the number of data points and
N (number of columns) the number of Monte Carlo samples</p>
</td></tr>
<tr><td><code id="check_predictions_+3A_true_values">true_values</code></td>
<td>
<p>A vector with the true observed values of size n</p>
</td></tr>
<tr><td><code id="check_predictions_+3A_type">type</code></td>
<td>
<p>character, one of &quot;continuous&quot; (default), &quot;integer&quot; or &quot;binary&quot; that
defines the type of the forecast</p>
</td></tr>
<tr><td><code id="check_predictions_+3A_class">class</code></td>
<td>
<p>character, either &quot;vector&quot; (default) or &quot;matrix&quot; that determines the
class the input has to correspond to</p>
</td></tr>
</table>

<hr>
<h2 id='check_quantiles'>Check that quantiles are valid</h2><span id='topic+check_quantiles'></span>

<h3>Description</h3>

<p>Helper function to check that input quantiles are valid.
Quantiles must be in the range specified, increase monotonically,
and contain no duplicates.
</p>
<p>This is used in <code><a href="#topic+bias_range">bias_range()</a></code> and <code><a href="#topic+bias_quantile">bias_quantile()</a></code> to
provide informative errors to users.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_quantiles(quantiles, name = "quantiles", range = c(0, 1))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="check_quantiles_+3A_quantiles">quantiles</code></td>
<td>
<p>Numeric vector of quantiles to check</p>
</td></tr>
<tr><td><code id="check_quantiles_+3A_name">name</code></td>
<td>
<p>Character name to use in error messages</p>
</td></tr>
<tr><td><code id="check_quantiles_+3A_range">range</code></td>
<td>
<p>Numeric vector giving allowed range</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None. Function errors if quantiles are invalid.
</p>

<hr>
<h2 id='check_summary_params'>Check input parameters for <code><a href="#topic+summarise_scores">summarise_scores()</a></code></h2><span id='topic+check_summary_params'></span>

<h3>Description</h3>

<p>A helper function to check the input parameters for
<code><a href="#topic+score">score()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_summary_params(scores, by, relative_skill, baseline, metric)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="check_summary_params_+3A_scores">scores</code></td>
<td>
<p>A data.table of scores as produced by <code><a href="#topic+score">score()</a></code>.</p>
</td></tr>
<tr><td><code id="check_summary_params_+3A_by">by</code></td>
<td>
<p>character vector with column names to summarise scores by. Default
is <code>NULL</code>, meaning that the only summary that takes is place is summarising
over samples or quantiles (in case of quantile-based forecasts), such that
there is one score per forecast as defined by the <em>unit of a single forecast</em>
(rather than one score for every sample or quantile).
The <em>unit of a single forecast</em> is determined by the columns present in the
input data that do not correspond to a metric produced by <code><a href="#topic+score">score()</a></code>, which
indicate indicate a grouping of forecasts (for example there may be one
forecast per day, location and model). Adding additional, unrelated, columns
may alter results in an unpredictable way.</p>
</td></tr>
<tr><td><code id="check_summary_params_+3A_relative_skill">relative_skill</code></td>
<td>
<p>logical, whether or not to compute relative
performance between models based on pairwise comparisons.
If <code>TRUE</code> (default is <code>FALSE</code>), then a column called
'model' must be present in the input data. For more information on
the computation of relative skill, see <code><a href="#topic+pairwise_comparison">pairwise_comparison()</a></code>.
Relative skill will be calculated for the aggregation level specified in
<code>by</code>.</p>
</td></tr>
<tr><td><code id="check_summary_params_+3A_baseline">baseline</code></td>
<td>
<p>character string with the name of a model. If a baseline is
given, then a scaled relative skill with respect to the baseline will be
returned. By default (<code>NULL</code>), relative skill will not be scaled with
respect to a baseline model.</p>
</td></tr>
<tr><td><code id="check_summary_params_+3A_metric">metric</code></td>
<td>
<p><a href="https://lifecycle.r-lib.org/articles/stages.html#deprecated"><img src="../help/figures/lifecycle-deprecated.svg" alt='[Deprecated]' /></a> Deprecated in 1.1.0. Use
<code>relative_skill_metric</code> instead.</p>
</td></tr>
</table>

<hr>
<h2 id='check_true_values'>Check Observed Value Input For Lower-level Scoring Functions</h2><span id='topic+check_true_values'></span>

<h3>Description</h3>

<p>Helper function to check inputs for lower-level score functions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_true_values(true_values, type = c("continuous", "integer", "binary"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="check_true_values_+3A_true_values">true_values</code></td>
<td>
<p>A vector with the true observed values of size n</p>
</td></tr>
<tr><td><code id="check_true_values_+3A_type">type</code></td>
<td>
<p>character, one of &quot;continuous&quot; (default), &quot;integer&quot; or &quot;binary&quot; that
defines the type of the forecast</p>
</td></tr>
</table>

<hr>
<h2 id='collapse_messages'>Collapse several messages to one</h2><span id='topic+collapse_messages'></span>

<h3>Description</h3>

<p>Internal helper function to facilitate generating messages
and warnings in <code><a href="#topic+check_forecasts">check_forecasts()</a></code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>collapse_messages(type = "messages", messages)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="collapse_messages_+3A_type">type</code></td>
<td>
<p>character, should be either &quot;messages&quot;, &quot;warnings&quot; or &quot;errors&quot;</p>
</td></tr>
<tr><td><code id="collapse_messages_+3A_messages">messages</code></td>
<td>
<p>the messages or warnings to collapse</p>
</td></tr>
</table>


<h3>Value</h3>

<p>string with the message or warning
</p>

<hr>
<h2 id='compare_two_models'>Compare Two Models Based on Subset of Common Forecasts</h2><span id='topic+compare_two_models'></span>

<h3>Description</h3>

<p>This function compares two models based on the subset of forecasts for which
both models have made a prediction. It gets called
from <code><a href="#topic+pairwise_comparison_one_group">pairwise_comparison_one_group()</a></code>, which handles the
comparison of multiple models on a single set of forecasts (there are no
subsets of forecasts to be distinguished). <code><a href="#topic+pairwise_comparison_one_group">pairwise_comparison_one_group()</a></code>
in turn gets called from from <code><a href="#topic+pairwise_comparison">pairwise_comparison()</a></code> which can handle
pairwise comparisons for a set of forecasts with multiple subsets, e.g.
pairwise comparisons for one set of forecasts, but done separately for two
different forecast targets.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>compare_two_models(
  scores,
  name_model1,
  name_model2,
  metric,
  one_sided = FALSE,
  test_type = c("non_parametric", "permutation"),
  n_permutations = 999
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="compare_two_models_+3A_scores">scores</code></td>
<td>
<p>A data.table of scores as produced by <code><a href="#topic+score">score()</a></code>.</p>
</td></tr>
<tr><td><code id="compare_two_models_+3A_name_model1">name_model1</code></td>
<td>
<p>character, name of the first model</p>
</td></tr>
<tr><td><code id="compare_two_models_+3A_name_model2">name_model2</code></td>
<td>
<p>character, name of the model to compare against</p>
</td></tr>
<tr><td><code id="compare_two_models_+3A_metric">metric</code></td>
<td>
<p>A character vector of length one with the metric to do the
comparison on. The default is &quot;auto&quot;, meaning that either &quot;interval_score&quot;,
&quot;crps&quot;, or &quot;brier_score&quot; will be selected where available.
See <code><a href="#topic+available_metrics">available_metrics()</a></code> for available metrics.</p>
</td></tr>
<tr><td><code id="compare_two_models_+3A_one_sided">one_sided</code></td>
<td>
<p>Boolean, default is <code>FALSE</code>, whether two conduct a one-sided
instead of a two-sided test to determine significance in a pairwise
comparison.</p>
</td></tr>
<tr><td><code id="compare_two_models_+3A_test_type">test_type</code></td>
<td>
<p>character, either &quot;non_parametric&quot; (the default) or
&quot;permutation&quot;. This determines which kind of test shall be conducted to
determine p-values.</p>
</td></tr>
<tr><td><code id="compare_two_models_+3A_n_permutations">n_permutations</code></td>
<td>
<p>numeric, the number of permutations for a
permutation test. Default is 999.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Johannes Bracher, <a href="mailto:johannes.bracher@kit.edu">johannes.bracher@kit.edu</a>
</p>
<p>Nikos Bosse <a href="mailto:nikosbosse@gmail.com">nikosbosse@gmail.com</a>
</p>

<hr>
<h2 id='correlation'>Correlation Between Metrics</h2><span id='topic+correlation'></span>

<h3>Description</h3>

<p>Calculate the correlation between different metrics for a data.frame of
scores as produced by <code><a href="#topic+score">score()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>correlation(scores, metrics = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="correlation_+3A_scores">scores</code></td>
<td>
<p>A data.table of scores as produced by <code><a href="#topic+score">score()</a></code>.</p>
</td></tr>
<tr><td><code id="correlation_+3A_metrics">metrics</code></td>
<td>
<p>A character vector with the metrics to show. If set to
<code>NULL</code> (default), all metrics present in <code>scores</code> will
be shown</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data.table with correlations for the different metrics
</p>


<h3>Examples</h3>

<pre><code class='language-R'>scores &lt;- score(example_quantile)
correlation(scores)
</code></pre>

<hr>
<h2 id='crps_sample'>Ranked Probability Score</h2><span id='topic+crps_sample'></span>

<h3>Description</h3>

<p>Wrapper around the <code><a href="scoringRules.html#topic+scores_sample_univ">crps_sample()</a></code>
function from the
<span class="pkg">scoringRules</span> package. Can be used for continuous as well as integer
valued forecasts
</p>


<h3>Usage</h3>

<pre><code class='language-R'>crps_sample(true_values, predictions)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="crps_sample_+3A_true_values">true_values</code></td>
<td>
<p>A vector with the true observed values of size n</p>
</td></tr>
<tr><td><code id="crps_sample_+3A_predictions">predictions</code></td>
<td>
<p>nxN matrix of predictive samples, n (number of rows) being
the number of data points and N (number of columns) the number of Monte
Carlo samples. Alternatively, predictions can just be a vector of size n.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>vector with the scoring values
</p>


<h3>References</h3>

<p>Alexander Jordan, Fabian Kr√ºger, Sebastian Lerch, Evaluating Probabilistic
Forecasts with scoringRules, <a href="https://www.jstatsoft.org/article/view/v090i12">https://www.jstatsoft.org/article/view/v090i12</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>true_values &lt;- rpois(30, lambda = 1:30)
predictions &lt;- replicate(200, rpois(n = 30, lambda = 1:30))
crps_sample(true_values, predictions)
</code></pre>

<hr>
<h2 id='delete_columns'>Delete Columns From a Data.table</h2><span id='topic+delete_columns'></span>

<h3>Description</h3>

<p>take a vector of column names and delete the columns if they
are present in the data.table
</p>


<h3>Usage</h3>

<pre><code class='language-R'>delete_columns(df, cols_to_delete, make_unique = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="delete_columns_+3A_df">df</code></td>
<td>
<p>A data.table or data.frame from which columns shall be deleted</p>
</td></tr>
<tr><td><code id="delete_columns_+3A_cols_to_delete">cols_to_delete</code></td>
<td>
<p>character vector with names of columns to be deleted</p>
</td></tr>
<tr><td><code id="delete_columns_+3A_make_unique">make_unique</code></td>
<td>
<p>whether to make the data set unique after removing columns</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data.table
</p>

<hr>
<h2 id='dss_sample'>Dawid-Sebastiani Score</h2><span id='topic+dss_sample'></span>

<h3>Description</h3>

<p>Wrapper around the <code><a href="scoringRules.html#topic+scores_sample_univ">dss_sample()</a></code>
function from the
<span class="pkg">scoringRules</span> package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dss_sample(true_values, predictions)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dss_sample_+3A_true_values">true_values</code></td>
<td>
<p>A vector with the true observed values of size n</p>
</td></tr>
<tr><td><code id="dss_sample_+3A_predictions">predictions</code></td>
<td>
<p>nxN matrix of predictive samples, n (number of rows) being
the number of data points and N (number of columns) the number of Monte
Carlo samples. Alternatively, predictions can just be a vector of size n.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>vector with scoring values
</p>


<h3>References</h3>

<p>Alexander Jordan, Fabian Kr√ºger, Sebastian Lerch, Evaluating Probabilistic
Forecasts with scoringRules, <a href="https://www.jstatsoft.org/article/view/v090i12">https://www.jstatsoft.org/article/view/v090i12</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>true_values &lt;- rpois(30, lambda = 1:30)
predictions &lt;- replicate(200, rpois(n = 30, lambda = 1:30))
dss_sample(true_values, predictions)
</code></pre>

<hr>
<h2 id='example_binary'>Binary Forecast Example Data</h2><span id='topic+example_binary'></span>

<h3>Description</h3>

<p>A data set with binary predictions for COVID-19 cases and deaths constructed
from data submitted to the European Forecast Hub.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>example_binary
</code></pre>


<h3>Format</h3>

<p>A data frame with 346 rows and 10 columns:
</p>

<dl>
<dt>location</dt><dd><p>the country for which a prediction was made</p>
</dd>
<dt>location_name</dt><dd><p>name of the country for which a prediction was made</p>
</dd>
<dt>target_end_date</dt><dd><p>the date for which a prediction was made</p>
</dd>
<dt>target_type</dt><dd><p>the target to be predicted (cases or deaths)</p>
</dd>
<dt>true_value</dt><dd><p>true observed values</p>
</dd>
<dt>forecast_date</dt><dd><p>the date on which a prediction was made</p>
</dd>
<dt>model</dt><dd><p>name of the model that generated the forecasts</p>
</dd>
<dt>horizon</dt><dd><p>forecast horizon in weeks</p>
</dd>
<dt>prediction</dt><dd><p>predicted value</p>
</dd>
</dl>



<h3>Details</h3>

<p>Predictions in the data set were constructed based on the continuous example
data by looking at the number of samples below the mean prediction.
The outcome was constructed as whether or not the actually
observed value was below or above that mean prediction.
This should not be understood as sound statistical practice, but rather
as a practical way to create an example data set.
</p>
<p>The data was created using the script create-example-data.R in the inst/
folder (or the top level folder in a compiled package).
</p>


<h3>Source</h3>

<p><a href="https://github.com/european-modelling-hubs/covid19-forecast-hub-europe/commit/a42867b1ea152c57e25b04f9faa26cfd4bfd8fa6/">https://github.com/european-modelling-hubs/covid19-forecast-hub-europe/commit/a42867b1ea152c57e25b04f9faa26cfd4bfd8fa6/</a> # nolint
</p>

<hr>
<h2 id='example_continuous'>Continuous Forecast Example Data</h2><span id='topic+example_continuous'></span>

<h3>Description</h3>

<p>A data set with continuous predictions for COVID-19 cases and deaths
constructed from data submitted to the European Forecast Hub.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>example_continuous
</code></pre>


<h3>Format</h3>

<p>A data frame with 13,429 rows and 10 columns:
</p>

<dl>
<dt>location</dt><dd><p>the country for which a prediction was made</p>
</dd>
<dt>target_end_date</dt><dd><p>the date for which a prediction was made</p>
</dd>
<dt>target_type</dt><dd><p>the target to be predicted (cases or deaths)</p>
</dd>
<dt>true_value</dt><dd><p>true observed values</p>
</dd>
<dt>location_name</dt><dd><p>name of the country for which a prediction was made</p>
</dd>
<dt>forecast_date</dt><dd><p>the date on which a prediction was made</p>
</dd>
<dt>model</dt><dd><p>name of the model that generated the forecasts</p>
</dd>
<dt>horizon</dt><dd><p>forecast horizon in weeks</p>
</dd>
<dt>prediction</dt><dd><p>predicted value</p>
</dd>
<dt>sample</dt><dd><p>id for the corresponding sample</p>
</dd>
</dl>



<h3>Details</h3>

<p>The data was created using the script create-example-data.R in the inst/
folder (or the top level folder in a compiled package).
</p>


<h3>Source</h3>

<p><a href="https://github.com/european-modelling-hubs/covid19-forecast-hub-europe/commit/a42867b1ea152c57e25b04f9faa26cfd4bfd8fa6/">https://github.com/european-modelling-hubs/covid19-forecast-hub-europe/commit/a42867b1ea152c57e25b04f9faa26cfd4bfd8fa6/</a> # nolint
</p>

<hr>
<h2 id='example_integer'>Integer Forecast Example Data</h2><span id='topic+example_integer'></span>

<h3>Description</h3>

<p>A data set with integer predictions for COVID-19 cases and deaths
constructed from data submitted to the European Forecast Hub.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>example_integer
</code></pre>


<h3>Format</h3>

<p>A data frame with 13,429 rows and 10 columns:
</p>

<dl>
<dt>location</dt><dd><p>the country for which a prediction was made</p>
</dd>
<dt>target_end_date</dt><dd><p>the date for which a prediction was made</p>
</dd>
<dt>target_type</dt><dd><p>the target to be predicted (cases or deaths)</p>
</dd>
<dt>true_value</dt><dd><p>true observed values</p>
</dd>
<dt>location_name</dt><dd><p>name of the country for which a prediction was made</p>
</dd>
<dt>forecast_date</dt><dd><p>the date on which a prediction was made</p>
</dd>
<dt>model</dt><dd><p>name of the model that generated the forecasts</p>
</dd>
<dt>horizon</dt><dd><p>forecast horizon in weeks</p>
</dd>
<dt>prediction</dt><dd><p>predicted value</p>
</dd>
<dt>sample</dt><dd><p>id for the corresponding sample</p>
</dd>
</dl>



<h3>Details</h3>

<p>The data was created using the script create-example-data.R in the inst/
folder (or the top level folder in a compiled package).
</p>

<hr>
<h2 id='example_point'>Point Forecast Example Data</h2><span id='topic+example_point'></span>

<h3>Description</h3>

<p>A data set with predictions for COVID-19 cases and deaths submitted to the
European Forecast Hub. This data set is like the quantile example data, only
that the median has been replaced by a point forecast.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>example_point
</code></pre>


<h3>Format</h3>

<p>A data frame with
</p>

<dl>
<dt>location</dt><dd><p>the country for which a prediction was made</p>
</dd>
<dt>target_end_date</dt><dd><p>the date for which a prediction was made</p>
</dd>
<dt>target_type</dt><dd><p>the target to be predicted (cases or deaths)</p>
</dd>
<dt>true_value</dt><dd><p>true observed values</p>
</dd>
<dt>location_name</dt><dd><p>name of the country for which a prediction was made</p>
</dd>
<dt>forecast_date</dt><dd><p>the date on which a prediction was made</p>
</dd>
<dt>quantile</dt><dd><p>quantile of the corresponding prediction</p>
</dd>
<dt>prediction</dt><dd><p>predicted value</p>
</dd>
<dt>model</dt><dd><p>name of the model that generated the forecasts</p>
</dd>
<dt>horizon</dt><dd><p>forecast horizon in weeks</p>
</dd>
</dl>



<h3>Details</h3>

<p>The data was created using the script create-example-data.R in the inst/
folder (or the top level folder in a compiled package).
</p>


<h3>Source</h3>

<p><a href="https://github.com/european-modelling-hubs/covid19-forecast-hub-europe/commit/a42867b1ea152c57e25b04f9faa26cfd4bfd8fa6/">https://github.com/european-modelling-hubs/covid19-forecast-hub-europe/commit/a42867b1ea152c57e25b04f9faa26cfd4bfd8fa6/</a> # nolint
</p>

<hr>
<h2 id='example_quantile'>Quantile Example Data</h2><span id='topic+example_quantile'></span>

<h3>Description</h3>

<p>A data set with predictions for COVID-19 cases and deaths submitted to the
European Forecast Hub.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>example_quantile
</code></pre>


<h3>Format</h3>

<p>A data frame with
</p>

<dl>
<dt>location</dt><dd><p>the country for which a prediction was made</p>
</dd>
<dt>target_end_date</dt><dd><p>the date for which a prediction was made</p>
</dd>
<dt>target_type</dt><dd><p>the target to be predicted (cases or deaths)</p>
</dd>
<dt>true_value</dt><dd><p>true observed values</p>
</dd>
<dt>location_name</dt><dd><p>name of the country for which a prediction was made</p>
</dd>
<dt>forecast_date</dt><dd><p>the date on which a prediction was made</p>
</dd>
<dt>quantile</dt><dd><p>quantile of the corresponding prediction</p>
</dd>
<dt>prediction</dt><dd><p>predicted value</p>
</dd>
<dt>model</dt><dd><p>name of the model that generated the forecasts</p>
</dd>
<dt>horizon</dt><dd><p>forecast horizon in weeks</p>
</dd>
</dl>



<h3>Details</h3>

<p>The data was created using the script create-example-data.R in the inst/
folder (or the top level folder in a compiled package).
</p>


<h3>Source</h3>

<p><a href="https://github.com/european-modelling-hubs/covid19-forecast-hub-europe/commit/a42867b1ea152c57e25b04f9faa26cfd4bfd8fa6/">https://github.com/european-modelling-hubs/covid19-forecast-hub-europe/commit/a42867b1ea152c57e25b04f9faa26cfd4bfd8fa6/</a> # nolint
</p>

<hr>
<h2 id='example_quantile_forecasts_only'>Quantile Example Data - Forecasts only</h2><span id='topic+example_quantile_forecasts_only'></span>

<h3>Description</h3>

<p>A data set with quantile predictions for COVID-19 cases and deaths
submitted to the European Forecast Hub.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>example_quantile_forecasts_only
</code></pre>


<h3>Format</h3>

<p>A data frame with 7,581 rows and 9 columns:
</p>

<dl>
<dt>location</dt><dd><p>the country for which a prediction was made</p>
</dd>
<dt>target_end_date</dt><dd><p>the date for which a prediction was made</p>
</dd>
<dt>target_type</dt><dd><p>the target to be predicted (cases or deaths)</p>
</dd>
<dt>forecast_date</dt><dd><p>the date on which a prediction was made</p>
</dd>
<dt>quantile</dt><dd><p>quantile of the corresponding prediction</p>
</dd>
<dt>prediction</dt><dd><p>predicted value</p>
</dd>
<dt>model</dt><dd><p>name of the model that generated the forecasts</p>
</dd>
<dt>horizon</dt><dd><p>forecast horizon in weeks</p>
</dd>
</dl>



<h3>Details</h3>

<p>The data was created using the script create-example-data.R in the inst/
folder (or the top level folder in a compiled package).
</p>


<h3>Source</h3>

<p><a href="https://github.com/european-modelling-hubs/covid19-forecast-hub-europe/commit/a42867b1ea152c57e25b04f9faa26cfd4bfd8fa6/">https://github.com/european-modelling-hubs/covid19-forecast-hub-europe/commit/a42867b1ea152c57e25b04f9faa26cfd4bfd8fa6/</a> # nolint
</p>

<hr>
<h2 id='example_truth_only'>Truth data only</h2><span id='topic+example_truth_only'></span>

<h3>Description</h3>

<p>A data set with truth values for COVID-19 cases and deaths
submitted to the European Forecast Hub.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>example_truth_only
</code></pre>


<h3>Format</h3>

<p>A data frame with 140 rows and 5 columns:
</p>

<dl>
<dt>location</dt><dd><p>the country for which a prediction was made</p>
</dd>
<dt>target_end_date</dt><dd><p>the date for which a prediction was made</p>
</dd>
<dt>target_type</dt><dd><p>the target to be predicted (cases or deaths)</p>
</dd>
<dt>true_value</dt><dd><p>true observed values</p>
</dd>
<dt>location_name</dt><dd><p>name of the country for which a prediction was made</p>
</dd>
</dl>



<h3>Details</h3>

<p>The data was created using the script create-example-data.R in the inst/
folder (or the top level folder in a compiled package).
</p>


<h3>Source</h3>

<p><a href="https://github.com/european-modelling-hubs/covid19-forecast-hub-europe/commit/a42867b1ea152c57e25b04f9faa26cfd4bfd8fa6/">https://github.com/european-modelling-hubs/covid19-forecast-hub-europe/commit/a42867b1ea152c57e25b04f9faa26cfd4bfd8fa6/</a> # nolint
</p>

<hr>
<h2 id='find_duplicates'>Find duplicate forecasts</h2><span id='topic+find_duplicates'></span>

<h3>Description</h3>

<p>Helper function to identify duplicate forecasts, i.e.
instances where there is more than one forecast for the same prediction
target.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>find_duplicates(data, forecast_unit)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="find_duplicates_+3A_data">data</code></td>
<td>
<p>A data.frame as used for <code><a href="#topic+score">score()</a></code></p>
</td></tr>
<tr><td><code id="find_duplicates_+3A_forecast_unit">forecast_unit</code></td>
<td>
<p>A character vector with the column names that define
the unit of a single forecast. If missing the function tries to infer the
unit of a single forecast.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data.frame with all rows for which a duplicate forecast was found
</p>


<h3>Examples</h3>

<pre><code class='language-R'>example &lt;- rbind(example_quantile, example_quantile[1000:1010])
find_duplicates(example)
</code></pre>

<hr>
<h2 id='geom_mean_helper'>Calculate Geometric Mean</h2><span id='topic+geom_mean_helper'></span>

<h3>Description</h3>

<p>Calculate Geometric Mean
</p>


<h3>Usage</h3>

<pre><code class='language-R'>geom_mean_helper(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="geom_mean_helper_+3A_x">x</code></td>
<td>
<p>numeric vector of values for which to calculate the geometric mean</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the geometric mean of the values in <code>x</code>
</p>

<hr>
<h2 id='get_forecast_unit'>Get unit of a single forecast</h2><span id='topic+get_forecast_unit'></span>

<h3>Description</h3>

<p>Helper function to get the unit of a single forecast, i.e.
the column names that define where a single forecast was made for
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_forecast_unit(data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_forecast_unit_+3A_data">data</code></td>
<td>
<p>A data.frame or data.table with the predictions and observations.
For scoring using <code><a href="#topic+score">score()</a></code>, the following columns need to be present:
</p>

<ul>
<li> <p><code>true_value</code> - the true observed values
</p>
</li>
<li> <p><code>prediction</code> - predictions or predictive samples for one
true value. (You only don't need to provide a prediction column if
you want to score quantile forecasts in a wide range format.)</p>
</li></ul>

<p>For scoring integer and continuous forecasts a <code>sample</code> column is needed:
</p>

<ul>
<li> <p><code>sample</code> - an index to identify the predictive samples in the
prediction column generated by one model for one true value. Only
necessary for continuous and integer forecasts, not for
binary predictions.</p>
</li></ul>

<p>For scoring predictions in a quantile-format forecast you should provide
a column called <code>quantile</code>:
</p>

<ul>
<li> <p><code>quantile</code>: quantile to which the prediction corresponds
</p>
</li></ul>

<p>In addition a <code>model</code> column is suggested and if not present this will be
flagged and added to the input data with all forecasts assigned as an
&quot;unspecified model&quot;).
</p>
<p>You can check the format of your data using <code><a href="#topic+check_forecasts">check_forecasts()</a></code> and there
are examples for each format (<a href="#topic+example_quantile">example_quantile</a>, <a href="#topic+example_continuous">example_continuous</a>,
<a href="#topic+example_integer">example_integer</a>, and <a href="#topic+example_binary">example_binary</a>).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A character vector with the column names that define the unit of
a single forecast
</p>

<hr>
<h2 id='get_prediction_type'>Get prediction type of a forecast</h2><span id='topic+get_prediction_type'></span>

<h3>Description</h3>

<p>Internal helper function to get the prediction type of a
forecast. That is inferred based on the properties of the values in the
<code>prediction</code> column.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_prediction_type(data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_prediction_type_+3A_data">data</code></td>
<td>
<p>A data.frame or data.table with the predictions and observations.
For scoring using <code><a href="#topic+score">score()</a></code>, the following columns need to be present:
</p>

<ul>
<li> <p><code>true_value</code> - the true observed values
</p>
</li>
<li> <p><code>prediction</code> - predictions or predictive samples for one
true value. (You only don't need to provide a prediction column if
you want to score quantile forecasts in a wide range format.)</p>
</li></ul>

<p>For scoring integer and continuous forecasts a <code>sample</code> column is needed:
</p>

<ul>
<li> <p><code>sample</code> - an index to identify the predictive samples in the
prediction column generated by one model for one true value. Only
necessary for continuous and integer forecasts, not for
binary predictions.</p>
</li></ul>

<p>For scoring predictions in a quantile-format forecast you should provide
a column called <code>quantile</code>:
</p>

<ul>
<li> <p><code>quantile</code>: quantile to which the prediction corresponds
</p>
</li></ul>

<p>In addition a <code>model</code> column is suggested and if not present this will be
flagged and added to the input data with all forecasts assigned as an
&quot;unspecified model&quot;).
</p>
<p>You can check the format of your data using <code><a href="#topic+check_forecasts">check_forecasts()</a></code> and there
are examples for each format (<a href="#topic+example_quantile">example_quantile</a>, <a href="#topic+example_continuous">example_continuous</a>,
<a href="#topic+example_integer">example_integer</a>, and <a href="#topic+example_binary">example_binary</a>).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Character vector of length one with either &quot;quantile&quot;, &quot;integer&quot;, or
&quot;continuous&quot;.
</p>

<hr>
<h2 id='get_protected_columns'>Get protected columns from a data frame</h2><span id='topic+get_protected_columns'></span>

<h3>Description</h3>

<p>Helper function to get the names of all columns in a data frame
that are protected columns.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_protected_columns(data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_protected_columns_+3A_data">data</code></td>
<td>
<p>A data.frame or data.table with the predictions and observations.
For scoring using <code><a href="#topic+score">score()</a></code>, the following columns need to be present:
</p>

<ul>
<li> <p><code>true_value</code> - the true observed values
</p>
</li>
<li> <p><code>prediction</code> - predictions or predictive samples for one
true value. (You only don't need to provide a prediction column if
you want to score quantile forecasts in a wide range format.)</p>
</li></ul>

<p>For scoring integer and continuous forecasts a <code>sample</code> column is needed:
</p>

<ul>
<li> <p><code>sample</code> - an index to identify the predictive samples in the
prediction column generated by one model for one true value. Only
necessary for continuous and integer forecasts, not for
binary predictions.</p>
</li></ul>

<p>For scoring predictions in a quantile-format forecast you should provide
a column called <code>quantile</code>:
</p>

<ul>
<li> <p><code>quantile</code>: quantile to which the prediction corresponds
</p>
</li></ul>

<p>In addition a <code>model</code> column is suggested and if not present this will be
flagged and added to the input data with all forecasts assigned as an
&quot;unspecified model&quot;).
</p>
<p>You can check the format of your data using <code><a href="#topic+check_forecasts">check_forecasts()</a></code> and there
are examples for each format (<a href="#topic+example_quantile">example_quantile</a>, <a href="#topic+example_continuous">example_continuous</a>,
<a href="#topic+example_integer">example_integer</a>, and <a href="#topic+example_binary">example_binary</a>).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A character vector with the names of protected columns in the data
</p>

<hr>
<h2 id='get_target_type'>Get type of the target true values of a forecast</h2><span id='topic+get_target_type'></span>

<h3>Description</h3>

<p>Internal helper function to get the type of the target
true values of a forecast. That is inferred based on the which columns
are present in the data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_target_type(data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_target_type_+3A_data">data</code></td>
<td>
<p>A data.frame or data.table with the predictions and observations.
For scoring using <code><a href="#topic+score">score()</a></code>, the following columns need to be present:
</p>

<ul>
<li> <p><code>true_value</code> - the true observed values
</p>
</li>
<li> <p><code>prediction</code> - predictions or predictive samples for one
true value. (You only don't need to provide a prediction column if
you want to score quantile forecasts in a wide range format.)</p>
</li></ul>

<p>For scoring integer and continuous forecasts a <code>sample</code> column is needed:
</p>

<ul>
<li> <p><code>sample</code> - an index to identify the predictive samples in the
prediction column generated by one model for one true value. Only
necessary for continuous and integer forecasts, not for
binary predictions.</p>
</li></ul>

<p>For scoring predictions in a quantile-format forecast you should provide
a column called <code>quantile</code>:
</p>

<ul>
<li> <p><code>quantile</code>: quantile to which the prediction corresponds
</p>
</li></ul>

<p>In addition a <code>model</code> column is suggested and if not present this will be
flagged and added to the input data with all forecasts assigned as an
&quot;unspecified model&quot;).
</p>
<p>You can check the format of your data using <code><a href="#topic+check_forecasts">check_forecasts()</a></code> and there
are examples for each format (<a href="#topic+example_quantile">example_quantile</a>, <a href="#topic+example_continuous">example_continuous</a>,
<a href="#topic+example_integer">example_integer</a>, and <a href="#topic+example_binary">example_binary</a>).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Character vector of length one with either &quot;binary&quot;, &quot;integer&quot;, or
&quot;continuous&quot;
</p>

<hr>
<h2 id='infer_rel_skill_metric'>Infer metric for pairwise comparisons</h2><span id='topic+infer_rel_skill_metric'></span>

<h3>Description</h3>

<p>Helper function to infer the metric for which pairwise comparisons shall
be made. The function simply checks the names of the available columns and
chooses the most widely used metric.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>infer_rel_skill_metric(scores)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="infer_rel_skill_metric_+3A_scores">scores</code></td>
<td>
<p>A data.table of scores as produced by <code><a href="#topic+score">score()</a></code>.</p>
</td></tr>
</table>

<hr>
<h2 id='interval_score'>Interval Score</h2><span id='topic+interval_score'></span>

<h3>Description</h3>

<p>Proper Scoring Rule to score quantile predictions, following Gneiting
and Raftery (2007). Smaller values are better.
</p>
<p>The score is computed as
</p>
<p style="text-align: center;"><code class="reqn">
\textrm{score} = (\textrm{upper} - \textrm{lower}) + \frac{2}{\alpha}(\textrm{lower}
 - \textrm{true\_value}) *
\mathbf{1}(\textrm{true\_value} &lt; \textrm{lower}) +
\frac{2}{\alpha}(\textrm{true\_value} - \textrm{upper}) *
\mathbf{1}(\textrm{true\_value} &gt; \textrm{upper})
</code>
</p>

<p>where <code class="reqn">\mathbf{1}()</code> is the indicator function and
indicates how much is outside the prediction interval.
<code class="reqn">\alpha</code> is the decimal value that indicates how much is outside
the prediction interval.
</p>
<p>To improve usability, the user is asked to provide an interval range in
percentage terms, i.e. interval_range = 90 (percent) for a 90 percent
prediction interval. Correspondingly, the user would have to provide the
5% and 95% quantiles (the corresponding alpha would then be 0.1).
No specific distribution is assumed,
but the range has to be symmetric (i.e you can't use the 0.1 quantile
as the lower bound and the 0.7 quantile as the upper).
Non-symmetric quantiles can be scored using the function <code><a href="#topic+quantile_score">quantile_score()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>interval_score(
  true_values,
  lower,
  upper,
  interval_range,
  weigh = TRUE,
  separate_results = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="interval_score_+3A_true_values">true_values</code></td>
<td>
<p>A vector with the true observed values of size n</p>
</td></tr>
<tr><td><code id="interval_score_+3A_lower">lower</code></td>
<td>
<p>vector of size n with the prediction for the lower quantile
of the given range</p>
</td></tr>
<tr><td><code id="interval_score_+3A_upper">upper</code></td>
<td>
<p>vector of size n with the prediction for the upper quantile
of the given range</p>
</td></tr>
<tr><td><code id="interval_score_+3A_interval_range">interval_range</code></td>
<td>
<p>the range of the prediction intervals. i.e. if you're
forecasting the 0.05 and 0.95 quantile, the interval_range would be 90.
Can be either a single number or a vector of size n, if the range changes
for different forecasts to be scored. This corresponds to (100-alpha)/100
in Gneiting and Raftery (2007). Internally, the range will be transformed
to alpha.</p>
</td></tr>
<tr><td><code id="interval_score_+3A_weigh">weigh</code></td>
<td>
<p>if TRUE, weigh the score by alpha / 2, so it can be averaged
into an interval score that, in the limit, corresponds to CRPS. Alpha is the
decimal value that  represents how much is outside a central prediction
interval (e.g. for a 90 percent central prediction interval, alpha is 0.1)
Default: <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="interval_score_+3A_separate_results">separate_results</code></td>
<td>
<p>if <code>TRUE</code> (default is <code>FALSE</code>), then the separate
parts of the interval score (dispersion penalty, penalties for over- and
under-prediction get returned as separate elements of a list). If you want a
<code>data.frame</code> instead, simply call <code><a href="base.html#topic+as.data.frame">as.data.frame()</a></code> on the output.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>vector with the scoring values, or a list with separate entries if
<code>separate_results</code> is <code>TRUE</code>.
</p>


<h3>References</h3>

<p>Strictly Proper Scoring Rules, Prediction,and Estimation,
Tilmann Gneiting and Adrian E. Raftery, 2007, Journal of the American
Statistical Association, Volume 102, 2007 - Issue 477
</p>
<p>Evaluating epidemic forecasts in an interval format,
Johannes Bracher, Evan L. Ray, Tilmann Gneiting and Nicholas G. Reich,
<a href="https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008618">https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008618</a> # nolint
</p>


<h3>Examples</h3>

<pre><code class='language-R'>true_values &lt;- rnorm(30, mean = 1:30)
interval_range &lt;- rep(90, 30)
alpha &lt;- (100 - interval_range) / 100
lower &lt;- qnorm(alpha / 2, rnorm(30, mean = 1:30))
upper &lt;- qnorm((1 - alpha / 2), rnorm(30, mean = 1:30))

interval_score(
  true_values = true_values,
  lower = lower,
  upper = upper,
  interval_range = interval_range
)

# gives a warning, as the interval_range should likely be 50 instead of 0.5
interval_score(true_value = 4, upper = 2, lower = 8, interval_range = 0.5)

# example with missing values and separate results
interval_score(
  true_values = c(true_values, NA),
  lower = c(lower, NA),
  upper = c(NA, upper),
  separate_results = TRUE,
  interval_range = 90
)
</code></pre>

<hr>
<h2 id='is_scoringutils_check'>Check whether object has been checked with check_forecasts()</h2><span id='topic+is_scoringutils_check'></span>

<h3>Description</h3>

<p>Helper function to determine whether an object has been checked
by and passed <code><a href="#topic+check_forecasts">check_forecasts()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is_scoringutils_check(data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="is_scoringutils_check_+3A_data">data</code></td>
<td>
<p>An object of class <code>scoringutils_check()</code> as produced by
<code><a href="#topic+check_forecasts">check_forecasts()</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Logical, either TRUE or FALSE
</p>

<hr>
<h2 id='log_shift'>Log transformation with an additive shift</h2><span id='topic+log_shift'></span>

<h3>Description</h3>

<p>Function that shifts a value by some offset and then applies the
natural logarithm to it.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>log_shift(x, offset = 0, base = exp(1))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="log_shift_+3A_x">x</code></td>
<td>
<p>vector of input values to be transformed</p>
</td></tr>
<tr><td><code id="log_shift_+3A_offset">offset</code></td>
<td>
<p>number to add to the input value before taking the natural
logarithm</p>
</td></tr>
<tr><td><code id="log_shift_+3A_base">base</code></td>
<td>
<p>a positive or complex number: the base with respect to which
logarithms are computed. Defaults to e = exp(1).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The output is computed as log(x + offset)
</p>


<h3>Value</h3>

<p>A numeric vector with transformed values
</p>


<h3>References</h3>

<p>Transformation of forecasts for evaluating predictive
performance in an epidemiological context
Nikos I. Bosse, Sam Abbott, Anne Cori, Edwin van Leeuwen, Johannes Bracher,
Sebastian Funk
medRxiv 2023.01.23.23284722
<a href="https://doi.org/10.1101/2023.01.23.23284722">doi:10.1101/2023.01.23.23284722</a>
<a href="https://www.medrxiv.org/content/10.1101/2023.01.23.23284722v1">https://www.medrxiv.org/content/10.1101/2023.01.23.23284722v1</a> # nolint
</p>


<h3>Examples</h3>

<pre><code class='language-R'>log_shift(1:10)
log_shift(0:9, offset = 1)

transform_forecasts(
  example_quantile[true_value &gt; 0, ],
  fun = log_shift,
  offset = 1
 )
</code></pre>

<hr>
<h2 id='logs_binary'>Log Score for Binary outcomes</h2><span id='topic+logs_binary'></span>

<h3>Description</h3>

<p>Computes the Log Score for probabilistic forecasts of binary outcomes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>logs_binary(true_values, predictions)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="logs_binary_+3A_true_values">true_values</code></td>
<td>
<p>A vector with the true observed values of size n with
all values equal to either 0 or 1</p>
</td></tr>
<tr><td><code id="logs_binary_+3A_predictions">predictions</code></td>
<td>
<p>A vector with a predicted probability
that true_value = 1.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Log Score is a proper score rule suited to assessing the accuracy of
probabilistic binary predictions. The outcomes can be either 0 or 1,
the predictions must be a probability that the true outcome will be 1.
</p>
<p>The Log Score is then computed as the negative logarithm of the probability
assigned to the true outcome. Reporting the negative logarithm means that
smaller values are better.
</p>


<h3>Value</h3>

<p>A numeric value with the Log Score, i.e. the mean squared
error of the given probability forecasts
</p>


<h3>Examples</h3>

<pre><code class='language-R'>true_values &lt;- sample(c(0, 1), size = 30, replace = TRUE)
predictions &lt;- runif(n = 30, min = 0, max = 1)
logs_binary(true_values, predictions)
</code></pre>

<hr>
<h2 id='logs_sample'>Logarithmic score</h2><span id='topic+logs_sample'></span>

<h3>Description</h3>

<p>Wrapper around the <code><a href="scoringRules.html#topic+scores_sample_univ">logs_sample()</a></code>
function from the
<span class="pkg">scoringRules</span> package. Used to score continuous predictions.
While the Log Score is in theory also applicable
to integer forecasts, the problem lies in the implementation: The Log Score
needs a kernel density estimation, which is not well defined with
integer-valued Monte Carlo Samples. The Log Score can be used for specific
integer valued probability distributions. See the scoringRules package for
more details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>logs_sample(true_values, predictions)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="logs_sample_+3A_true_values">true_values</code></td>
<td>
<p>A vector with the true observed values of size n</p>
</td></tr>
<tr><td><code id="logs_sample_+3A_predictions">predictions</code></td>
<td>
<p>nxN matrix of predictive samples, n (number of rows) being
the number of data points and N (number of columns) the number of Monte
Carlo samples. Alternatively, predictions can just be a vector of size n.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>vector with the scoring values
</p>


<h3>References</h3>

<p>Alexander Jordan, Fabian Kr√ºger, Sebastian Lerch, Evaluating Probabilistic
Forecasts with scoringRules, <a href="https://www.jstatsoft.org/article/view/v090i12">https://www.jstatsoft.org/article/view/v090i12</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>true_values &lt;- rpois(30, lambda = 1:30)
predictions &lt;- replicate(200, rpois(n = 30, lambda = 1:30))
logs_sample(true_values, predictions)
</code></pre>

<hr>
<h2 id='mad_sample'>Determine dispersion of a probabilistic forecast</h2><span id='topic+mad_sample'></span>

<h3>Description</h3>

<p>Determine dispersion of a probabilistic forecast
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mad_sample(predictions)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mad_sample_+3A_predictions">predictions</code></td>
<td>
<p>nxN matrix of predictive samples, n (number of rows) being
the number of data points and N (number of columns) the number of Monte
Carlo samples. Alternatively, predictions can just be a vector of size n.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Sharpness is the ability of the model to generate predictions within a
narrow range and dispersion is the lack thereof.
It is a data-independent measure, and is purely a feature
of the forecasts themselves.
</p>
<p>Dispersion of predictive samples corresponding to one single true value is
measured as the normalised median of the absolute deviation from
the median of the predictive samples. For details, see <a href="stats.html#topic+mad">mad()</a>
and the explanations given in Funk et al. (2019)
</p>


<h3>Value</h3>

<p>vector with dispersion values
</p>


<h3>References</h3>

<p>Funk S, Camacho A, Kucharski AJ, Lowe R, Eggo RM, Edmunds WJ (2019)
Assessing the performance of real-time epidemic forecasts: A case study of
Ebola in the Western Area region of Sierra Leone, 2014-15.
PLoS Comput Biol 15(2): e1006785. <a href="https://doi.org/10.1371/journal.pcbi.1006785">doi:10.1371/journal.pcbi.1006785</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>predictions &lt;- replicate(200, rpois(n = 30, lambda = 1:30))
mad_sample(predictions)
</code></pre>

<hr>
<h2 id='make_NA'>Make Rows NA in Data for Plotting</h2><span id='topic+make_NA'></span><span id='topic+make_na'></span>

<h3>Description</h3>

<p>Filters the data and turns values into <code>NA</code> before the data gets passed to
<code><a href="#topic+plot_predictions">plot_predictions()</a></code>. The reason to do this is to this is that it allows to
'filter' prediction and truth data separately. Any value that is NA will then
be removed in the subsequent call to <code><a href="#topic+plot_predictions">plot_predictions()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>make_NA(data = NULL, what = c("truth", "forecast", "both"), ...)

make_na(data = NULL, what = c("truth", "forecast", "both"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="make_NA_+3A_data">data</code></td>
<td>
<p>A data.frame or data.table with the predictions and observations.
For scoring using <code><a href="#topic+score">score()</a></code>, the following columns need to be present:
</p>

<ul>
<li> <p><code>true_value</code> - the true observed values
</p>
</li>
<li> <p><code>prediction</code> - predictions or predictive samples for one
true value. (You only don't need to provide a prediction column if
you want to score quantile forecasts in a wide range format.)</p>
</li></ul>

<p>For scoring integer and continuous forecasts a <code>sample</code> column is needed:
</p>

<ul>
<li> <p><code>sample</code> - an index to identify the predictive samples in the
prediction column generated by one model for one true value. Only
necessary for continuous and integer forecasts, not for
binary predictions.</p>
</li></ul>

<p>For scoring predictions in a quantile-format forecast you should provide
a column called <code>quantile</code>:
</p>

<ul>
<li> <p><code>quantile</code>: quantile to which the prediction corresponds
</p>
</li></ul>

<p>In addition a <code>model</code> column is suggested and if not present this will be
flagged and added to the input data with all forecasts assigned as an
&quot;unspecified model&quot;).
</p>
<p>You can check the format of your data using <code><a href="#topic+check_forecasts">check_forecasts()</a></code> and there
are examples for each format (<a href="#topic+example_quantile">example_quantile</a>, <a href="#topic+example_continuous">example_continuous</a>,
<a href="#topic+example_integer">example_integer</a>, and <a href="#topic+example_binary">example_binary</a>).</p>
</td></tr>
<tr><td><code id="make_NA_+3A_what">what</code></td>
<td>
<p>character vector that determines which values should be turned
into <code>NA</code>. If <code>what = "truth"</code>, values in the column 'true_value' will be
turned into <code>NA</code>. If <code>what = "forecast"</code>, values in the column 'prediction'
will be turned into <code>NA</code>. If <code>what = "both"</code>, values in both column will be
turned into <code>NA</code>.</p>
</td></tr>
<tr><td><code id="make_NA_+3A_...">...</code></td>
<td>
<p>logical statements used to filter the data</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data.table
</p>


<h3>Examples</h3>

<pre><code class='language-R'>make_NA (
    example_continuous,
    what = "truth",
    target_end_date &gt;= "2021-07-22",
    target_end_date &lt; "2021-05-01"
  )
</code></pre>

<hr>
<h2 id='merge_pred_and_obs'>Merge Forecast Data And Observations</h2><span id='topic+merge_pred_and_obs'></span>

<h3>Description</h3>

<p>The function more or less provides a wrapper around <code>merge</code> that
aims to handle the merging well if additional columns are present
in one or both data sets. If in doubt, you should probably merge the
data sets manually.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>merge_pred_and_obs(
  forecasts,
  observations,
  join = c("left", "full", "right"),
  by = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="merge_pred_and_obs_+3A_forecasts">forecasts</code></td>
<td>
<p>data.frame with the forecast data (as can be passed to
<code><a href="#topic+score">score()</a></code>).</p>
</td></tr>
<tr><td><code id="merge_pred_and_obs_+3A_observations">observations</code></td>
<td>
<p>data.frame with the observations</p>
</td></tr>
<tr><td><code id="merge_pred_and_obs_+3A_join">join</code></td>
<td>
<p>character, one of <code>c("left", "full", "right")</code>. Determines the
type of the join. Usually, a left join is appropriate, but sometimes you
may want to do a full join to keep dates for which there is a forecast, but
no ground truth data.</p>
</td></tr>
<tr><td><code id="merge_pred_and_obs_+3A_by">by</code></td>
<td>
<p>character vector that denotes the columns by which to merge. Any
value that is not a column in observations will be removed.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a data.frame with forecasts and observations
</p>


<h3>Examples</h3>

<pre><code class='language-R'>forecasts &lt;- example_quantile_forecasts_only
observations &lt;- example_truth_only
merge_pred_and_obs(forecasts, observations)
</code></pre>

<hr>
<h2 id='metrics'>Summary information for selected metrics</h2><span id='topic+metrics'></span>

<h3>Description</h3>

<p>A data set with summary information on selected metrics implemented in
<span class="pkg">scoringutils</span>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>metrics
</code></pre>


<h3>Format</h3>

<p>An object of class <code>data.table</code> (inherits from <code>data.frame</code>) with 22 rows and 8 columns.
</p>


<h3>Details</h3>

<p>The data was created using the script create-metric-tables.R in the inst/
folder (or the top level folder in a compiled package).
</p>

<hr>
<h2 id='pairwise_comparison'>Do Pairwise Comparisons of Scores</h2><span id='topic+pairwise_comparison'></span>

<h3>Description</h3>

<p>Compute relative scores between different models making pairwise
comparisons. Pairwise comparisons are a sort of pairwise tournament where all
combinations of two models are compared against each other based on the
overlapping set of available forecasts common to both models.
Internally, a ratio of the mean scores of both models is computed.
The relative score of a model is then the geometric mean of all mean score
ratios which involve that model. When a baseline is provided, then that
baseline is excluded from the relative scores for individual models
(which therefore differ slightly from relative scores without a baseline)
and all relative scores are scaled by (i.e. divided by) the relative score of
the baseline model.
Usually, the function input should be unsummarised scores as
produced by <code><a href="#topic+score">score()</a></code>.
Note that the function internally infers the <em>unit of a single forecast</em> by
determining all columns in the input that do not correspond to metrics
computed by <code><a href="#topic+score">score()</a></code>. Adding unrelated columns will change results in an
unpredictable way.
</p>
<p>The code for the pairwise comparisons is inspired by an implementation by
Johannes Bracher.
The implementation of the permutation test follows the function
<code>permutationTest</code> from the <code>surveillance</code> package by Michael H√∂hle,
Andrea Riebler and Michaela Paul.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pairwise_comparison(
  scores,
  by = "model",
  metric = "auto",
  baseline = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pairwise_comparison_+3A_scores">scores</code></td>
<td>
<p>A data.table of scores as produced by <code><a href="#topic+score">score()</a></code>.</p>
</td></tr>
<tr><td><code id="pairwise_comparison_+3A_by">by</code></td>
<td>
<p>character vector with names of columns present in the input
data.frame. <code>by</code> determines how pairwise comparisons will be computed.
You will get a relative skill score for every grouping level determined in
<code>by</code>. If, for example, <code>by = c("model", "location")</code>. Then you will get a
separate relative skill score for every model in every location. Internally,
the data.frame will be split according <code>by</code> (but removing &quot;model&quot; before
splitting) and the pairwise comparisons will be computed separately for the
split data.frames.</p>
</td></tr>
<tr><td><code id="pairwise_comparison_+3A_metric">metric</code></td>
<td>
<p>A character vector of length one with the metric to do the
comparison on. The default is &quot;auto&quot;, meaning that either &quot;interval_score&quot;,
&quot;crps&quot;, or &quot;brier_score&quot; will be selected where available.
See <code><a href="#topic+available_metrics">available_metrics()</a></code> for available metrics.</p>
</td></tr>
<tr><td><code id="pairwise_comparison_+3A_baseline">baseline</code></td>
<td>
<p>character vector of length one that denotes the baseline
model against which to compare other models.</p>
</td></tr>
<tr><td><code id="pairwise_comparison_+3A_...">...</code></td>
<td>
<p>additional arguments for the comparison between two models. See
<code><a href="#topic+compare_two_models">compare_two_models()</a></code> for more information.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A ggplot2 object with a coloured table of summarised scores
</p>


<h3>Author(s)</h3>

<p>Nikos Bosse <a href="mailto:nikosbosse@gmail.com">nikosbosse@gmail.com</a>
</p>
<p>Johannes Bracher, <a href="mailto:johannes.bracher@kit.edu">johannes.bracher@kit.edu</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

scores &lt;- score(example_quantile)
pairwise &lt;- pairwise_comparison(scores, by = "target_type")

library(ggplot2)
plot_pairwise_comparison(pairwise, type = "mean_scores_ratio") +
  facet_wrap(~target_type)
</code></pre>

<hr>
<h2 id='pairwise_comparison_one_group'>Do Pairwise Comparison for one Set of Forecasts</h2><span id='topic+pairwise_comparison_one_group'></span>

<h3>Description</h3>

<p>This function does the pairwise comparison for one set of forecasts, but
multiple models involved. It gets called from <code><a href="#topic+pairwise_comparison">pairwise_comparison()</a></code>.
<code><a href="#topic+pairwise_comparison">pairwise_comparison()</a></code> splits the data into arbitrary subgroups specified
by the user (e.g. if pairwise comparison should be done separately for
different forecast targets) and then the actual pairwise comparison for that
subgroup is managed from <code><a href="#topic+pairwise_comparison_one_group">pairwise_comparison_one_group()</a></code>. In order to
actually do the comparison between two models over a subset of common
forecasts it calls <code><a href="#topic+compare_two_models">compare_two_models()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pairwise_comparison_one_group(scores, metric, baseline, by, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pairwise_comparison_one_group_+3A_scores">scores</code></td>
<td>
<p>A data.table of scores as produced by <code><a href="#topic+score">score()</a></code>.</p>
</td></tr>
<tr><td><code id="pairwise_comparison_one_group_+3A_metric">metric</code></td>
<td>
<p>A character vector of length one with the metric to do the
comparison on. The default is &quot;auto&quot;, meaning that either &quot;interval_score&quot;,
&quot;crps&quot;, or &quot;brier_score&quot; will be selected where available.
See <code><a href="#topic+available_metrics">available_metrics()</a></code> for available metrics.</p>
</td></tr>
<tr><td><code id="pairwise_comparison_one_group_+3A_baseline">baseline</code></td>
<td>
<p>character vector of length one that denotes the baseline
model against which to compare other models.</p>
</td></tr>
<tr><td><code id="pairwise_comparison_one_group_+3A_by">by</code></td>
<td>
<p>character vector with names of columns present in the input
data.frame. <code>by</code> determines how pairwise comparisons will be computed.
You will get a relative skill score for every grouping level determined in
<code>by</code>. If, for example, <code>by = c("model", "location")</code>. Then you will get a
separate relative skill score for every model in every location. Internally,
the data.frame will be split according <code>by</code> (but removing &quot;model&quot; before
splitting) and the pairwise comparisons will be computed separately for the
split data.frames.</p>
</td></tr>
<tr><td><code id="pairwise_comparison_one_group_+3A_...">...</code></td>
<td>
<p>additional arguments for the comparison between two models. See
<code><a href="#topic+compare_two_models">compare_two_models()</a></code> for more information.</p>
</td></tr>
</table>

<hr>
<h2 id='permutation_test'>Simple permutation test</h2><span id='topic+permutation_test'></span>

<h3>Description</h3>

<p>#' The implementation of the permutation test follows the
function
<code>permutationTest</code> from the <code>surveillance</code> package by Michael H√∂hle,
Andrea Riebler and Michaela Paul.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>permutation_test(
  scores1,
  scores2,
  n_permutation = 999,
  one_sided = FALSE,
  comparison_mode = c("difference", "ratio")
)
</code></pre>


<h3>Value</h3>

<p>p-value of the permutation test
</p>

<hr>
<h2 id='pit'>Probability Integral Transformation (data.frame Format)</h2><span id='topic+pit'></span>

<h3>Description</h3>

<p>Wrapper around <code>pit()</code> for use in data.frames
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pit(data, by, n_replicates = 100)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pit_+3A_data">data</code></td>
<td>
<p>a data.frame with the following columns: <code>true_value</code>,
<code>prediction</code>, <code>sample</code>.</p>
</td></tr>
<tr><td><code id="pit_+3A_by">by</code></td>
<td>
<p>Character vector with the columns according to which the
PIT values shall be grouped. If you e.g. have the columns 'model' and
'location' in the data and want to have a PIT histogram for
every model and location, specify <code>by = c("model", "location")</code>.</p>
</td></tr>
<tr><td><code id="pit_+3A_n_replicates">n_replicates</code></td>
<td>
<p>the number of draws for the randomised PIT for
integer predictions.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>see <code><a href="#topic+pit">pit()</a></code>
</p>


<h3>Value</h3>

<p>a data.table with PIT values according to the grouping specified in
<code>by</code>
</p>


<h3>References</h3>

<p>Sebastian Funk, Anton Camacho, Adam J. Kucharski, Rachel Lowe,
Rosalind M. Eggo, W. John Edmunds (2019) Assessing the performance of
real-time epidemic forecasts: A case study of Ebola in the Western Area
region of Sierra Leone, 2014-15, <a href="https://doi.org/10.1371/journal.pcbi.1006785">doi:10.1371/journal.pcbi.1006785</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>result &lt;- pit(example_continuous, by = "model")
plot_pit(result)

# example with quantile data
result &lt;- pit(example_quantile, by = "model")
plot_pit(result)
</code></pre>

<hr>
<h2 id='pit_sample'>Probability Integral Transformation (sample-based version)</h2><span id='topic+pit_sample'></span>

<h3>Description</h3>

<p>Uses a Probability Integral Transformation (PIT) (or a
randomised PIT for integer forecasts) to
assess the calibration of predictive Monte Carlo samples. Returns a
p-values resulting from an Anderson-Darling test for uniformity
of the (randomised) PIT as well as a PIT histogram if specified.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pit_sample(true_values, predictions, n_replicates = 100)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pit_sample_+3A_true_values">true_values</code></td>
<td>
<p>A vector with the true observed values of size n</p>
</td></tr>
<tr><td><code id="pit_sample_+3A_predictions">predictions</code></td>
<td>
<p>nxN matrix of predictive samples, n (number of rows) being
the number of data points and N (number of columns) the number of Monte
Carlo samples. Alternatively, predictions can just be a vector of size n.</p>
</td></tr>
<tr><td><code id="pit_sample_+3A_n_replicates">n_replicates</code></td>
<td>
<p>the number of draws for the randomised PIT for
integer predictions.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Calibration or reliability of forecasts is the ability of a model to
correctly identify its own uncertainty in making predictions. In a model
with perfect calibration, the observed data at each time point look as if
they came from the predictive probability distribution at that time.
</p>
<p>Equivalently, one can inspect the probability integral transform of the
predictive distribution at time t,
</p>
<p style="text-align: center;"><code class="reqn">
u_t = F_t (x_t)
</code>
</p>

<p>where <code class="reqn">x_t</code> is the observed data point at time <code class="reqn">t \textrm{ in } t_1,
‚Ä¶, t_n</code>, n being the number of forecasts, and <code class="reqn">F_t</code> is
the (continuous) predictive cumulative probability distribution at time t. If
the true probability distribution of outcomes at time t is <code class="reqn">G_t</code> then the
forecasts <code class="reqn">F_t</code> are said to be ideal if <code class="reqn">F_t = G_t</code> at all times t.
In that case, the probabilities <code class="reqn">u_t</code> are distributed uniformly.
</p>
<p>In the case of discrete outcomes such as incidence counts,
the PIT is no longer uniform even when forecasts are ideal.
In that case a randomised PIT can be used instead:
</p>
<p style="text-align: center;"><code class="reqn">
u_t = P_t(k_t) + v * (P_t(k_t) - P_t(k_t - 1) )
</code>
</p>

<p>where <code class="reqn">k_t</code> is the observed count, <code class="reqn">P_t(x)</code> is the predictive
cumulative probability of observing incidence k at time t,
<code class="reqn">P_t (-1) = 0</code> by definition and v is standard uniform and independent
of k. If <code class="reqn">P_t</code> is the true cumulative
probability distribution, then <code class="reqn">u_t</code> is standard uniform.
</p>
<p>The function checks whether integer or continuous forecasts were provided.
It then applies the (randomised) probability integral and tests
the values <code class="reqn">u_t</code> for uniformity using the
Anderson-Darling test.
</p>
<p>As a rule of thumb, there is no evidence to suggest a forecasting model is
miscalibrated if the p-value found was greater than a threshold of p &gt;= 0.1,
some evidence that it was miscalibrated if 0.01 &lt; p &lt; 0.1, and good
evidence that it was miscalibrated if p &lt;= 0.01. However, the AD-p-values
may be overly strict and there actual usefulness may be questionable.
In this context it should be noted, though, that uniformity of the
PIT is a necessary but not sufficient condition of calibration.
</p>


<h3>Value</h3>

<p>A vector with PIT-values. For continuous forecasts, the vector will
correspond to the length of <code>true_values</code>. For integer forecasts, a
randomised PIT will be returned of length
<code>length(true_values) * n_replicates</code>
</p>


<h3>References</h3>

<p>Sebastian Funk, Anton Camacho, Adam J. Kucharski, Rachel Lowe,
Rosalind M. Eggo, W. John Edmunds (2019) Assessing the performance of
real-time epidemic forecasts: A case study of Ebola in the Western Area
region of Sierra Leone, 2014-15, <a href="https://doi.org/10.1371/journal.pcbi.1006785">doi:10.1371/journal.pcbi.1006785</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pit">pit()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

## continuous predictions
true_values &lt;- rnorm(20, mean = 1:20)
predictions &lt;- replicate(100, rnorm(n = 20, mean = 1:20))
pit &lt;- pit_sample(true_values, predictions)
plot_pit(pit)

## integer predictions
true_values &lt;- rpois(50, lambda = 1:50)
predictions &lt;- replicate(2000, rpois(n = 50, lambda = 1:50))
pit &lt;- pit_sample(true_values, predictions, n_replicates = 30)
plot_pit(pit)
</code></pre>

<hr>
<h2 id='plot_avail_forecasts'>Visualise Where Forecasts Are Available</h2><span id='topic+plot_avail_forecasts'></span>

<h3>Description</h3>

<p>Visualise Where Forecasts Are Available
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_avail_forecasts(
  avail_forecasts,
  y = "model",
  x = "forecast_date",
  make_x_factor = TRUE,
  show_numbers = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_avail_forecasts_+3A_avail_forecasts">avail_forecasts</code></td>
<td>
<p>data.frame with a column called <code style="white-space: pre;">&#8288;Number forecasts&#8288;</code> as
produced by <code><a href="#topic+avail_forecasts">avail_forecasts()</a></code></p>
</td></tr>
<tr><td><code id="plot_avail_forecasts_+3A_y">y</code></td>
<td>
<p>character vector of length one that denotes the name of the column
to appear on the y-axis of the plot. Default is &quot;model&quot;.</p>
</td></tr>
<tr><td><code id="plot_avail_forecasts_+3A_x">x</code></td>
<td>
<p>character vector of length one that denotes the name of the column
to appear on the x-axis of the plot. Default is &quot;forecast_date&quot;.</p>
</td></tr>
<tr><td><code id="plot_avail_forecasts_+3A_make_x_factor">make_x_factor</code></td>
<td>
<p>logical (default is TRUE). Whether or not to convert
the variable on the x-axis to a factor. This has an effect e.g. if dates
are shown on the x-axis.</p>
</td></tr>
<tr><td><code id="plot_avail_forecasts_+3A_show_numbers">show_numbers</code></td>
<td>
<p>logical (default is <code>TRUE</code>) that indicates whether
or not to show the actual count numbers on the plot</p>
</td></tr>
</table>


<h3>Value</h3>

<p>ggplot object with a plot of interval coverage
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(ggplot2)
avail_forecasts &lt;- avail_forecasts(
  example_quantile, by = c("model", "target_type", "target_end_date")
)
plot_avail_forecasts(
 avail_forecasts, x = "target_end_date", show_numbers = FALSE
) +
 facet_wrap("target_type")
</code></pre>

<hr>
<h2 id='plot_correlation'>Plot Correlation Between Metrics</h2><span id='topic+plot_correlation'></span>

<h3>Description</h3>

<p>Plots a heatmap of correlations between different metrics
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_correlation(correlations)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_correlation_+3A_correlations">correlations</code></td>
<td>
<p>A data.table of correlations between scores as produced
by <code><a href="#topic+correlation">correlation()</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A ggplot2 object showing a coloured matrix of correlations
between metrics
</p>


<h3>Examples</h3>

<pre><code class='language-R'>scores &lt;- score(example_quantile)
correlations &lt;- correlation(
 summarise_scores(scores)
)
plot_correlation(correlations)
</code></pre>

<hr>
<h2 id='plot_heatmap'>Create a Heatmap of a Scoring Metric</h2><span id='topic+plot_heatmap'></span>

<h3>Description</h3>

<p>This function can be used to create a heatmap of one metric across different
groups, e.g. the interval score obtained by several forecasting models in
different locations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_heatmap(scores, y = "model", x, metric)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_heatmap_+3A_scores">scores</code></td>
<td>
<p>A data.frame of scores based on quantile forecasts as
produced by <code><a href="#topic+score">score()</a></code>.</p>
</td></tr>
<tr><td><code id="plot_heatmap_+3A_y">y</code></td>
<td>
<p>The variable from the scores you want to show on the y-Axis. The
default for this is &quot;model&quot;</p>
</td></tr>
<tr><td><code id="plot_heatmap_+3A_x">x</code></td>
<td>
<p>The variable from the scores you want to show on the x-Axis. This
could be something like &quot;horizon&quot;, or &quot;location&quot;</p>
</td></tr>
<tr><td><code id="plot_heatmap_+3A_metric">metric</code></td>
<td>
<p>the metric that determines the value and colour shown in the
tiles of the heatmap</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A ggplot2 object showing a heatmap of the desired metric
</p>


<h3>Examples</h3>

<pre><code class='language-R'>scores &lt;- score(example_quantile)
scores &lt;- summarise_scores(scores, by = c("model", "target_type", "range"))

plot_heatmap(scores, x = "target_type", metric = "bias")
</code></pre>

<hr>
<h2 id='plot_interval_coverage'>Plot Interval Coverage</h2><span id='topic+plot_interval_coverage'></span>

<h3>Description</h3>

<p>Plot interval coverage
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_interval_coverage(scores, colour = "model")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_interval_coverage_+3A_scores">scores</code></td>
<td>
<p>A data.frame of scores based on quantile forecasts as
produced by <code><a href="#topic+score">score()</a></code> or <code><a href="#topic+summarise_scores">summarise_scores()</a></code>. Note that &quot;range&quot; must be included
in the <code>by</code> argument when running <code><a href="#topic+summarise_scores">summarise_scores()</a></code></p>
</td></tr>
<tr><td><code id="plot_interval_coverage_+3A_colour">colour</code></td>
<td>
<p>According to which variable shall the graphs be coloured?
Default is &quot;model&quot;.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>ggplot object with a plot of interval coverage
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
scores &lt;- score(example_quantile)
scores &lt;- summarise_scores(scores, by = c("model", "range"))
plot_interval_coverage(scores)
</code></pre>

<hr>
<h2 id='plot_pairwise_comparison'>Plot Heatmap of Pairwise Comparisons</h2><span id='topic+plot_pairwise_comparison'></span>

<h3>Description</h3>

<p>Creates a heatmap of the ratios or pvalues from a pairwise comparison
between models
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_pairwise_comparison(
  comparison_result,
  type = c("mean_scores_ratio", "pval")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_pairwise_comparison_+3A_comparison_result">comparison_result</code></td>
<td>
<p>A data.frame as produced by
<code><a href="#topic+pairwise_comparison">pairwise_comparison()</a></code></p>
</td></tr>
<tr><td><code id="plot_pairwise_comparison_+3A_type">type</code></td>
<td>
<p>character vector of length one that is either
&quot;mean_scores_ratio&quot; or &quot;pval&quot;. This denotes whether to
visualise the ratio or the p-value of the pairwise comparison.
Default is &quot;mean_scores_ratio&quot;.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>library(ggplot2)
scores &lt;- score(example_quantile)
pairwise &lt;- pairwise_comparison(scores, by = "target_type")
plot_pairwise_comparison(pairwise, type = "mean_scores_ratio") +
  facet_wrap(~target_type)
</code></pre>

<hr>
<h2 id='plot_pit'>PIT Histogram</h2><span id='topic+plot_pit'></span>

<h3>Description</h3>

<p>Make a simple histogram of the probability integral transformed values to
visually check whether a uniform distribution seems likely.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_pit(pit, num_bins = "auto", breaks = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_pit_+3A_pit">pit</code></td>
<td>
<p>either a vector with the PIT values of size n, or a data.frame as
produced by <code><a href="#topic+pit">pit()</a></code></p>
</td></tr>
<tr><td><code id="plot_pit_+3A_num_bins">num_bins</code></td>
<td>
<p>the number of bins in the PIT histogram, default is &quot;auto&quot;.
When <code>num_bins == "auto"</code>, <code><a href="#topic+plot_pit">plot_pit()</a></code> will either display 10 bins, or it
will display a bin for each available quantile in case you passed in data in
a quantile-based format.
You can control the number of bins by supplying a number. This is fine for
sample-based pit histograms, but may fail for quantile-based formats. In this
case it is preferred to supply explicit breaks points using the <code>breaks</code>
argument.</p>
</td></tr>
<tr><td><code id="plot_pit_+3A_breaks">breaks</code></td>
<td>
<p>numeric vector with the break points for the bins in the
PIT histogram. This is preferred when creating a PIT histogram based on
quantile-based data. Default is <code>NULL</code> and breaks will be determined by
<code>num_bins</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>vector with the scoring values
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

# PIT histogram in vector based format
true_values &lt;- rnorm(30, mean = 1:30)
predictions &lt;- replicate(200, rnorm(n = 30, mean = 1:30))
pit &lt;- pit_sample(true_values, predictions)
plot_pit(pit)

# quantile-based pit
pit &lt;- pit(example_quantile,by = "model")
plot_pit(pit, breaks = seq(0.1, 1, 0.1))

# sample-based pit
pit &lt;- pit(example_integer,by = "model")
plot_pit(pit)
</code></pre>

<hr>
<h2 id='plot_predictions'>Plot Predictions vs True Values</h2><span id='topic+plot_predictions'></span>

<h3>Description</h3>

<p>Make a plot of observed and predicted values
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_predictions(data, by = NULL, x = "date", range = c(0, 50, 90))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_predictions_+3A_data">data</code></td>
<td>
<p>a data.frame that follows the same specifications outlined in
<code><a href="#topic+score">score()</a></code>. To customise your plotting, you can filter your data using the
function <code><a href="#topic+make_NA">make_NA()</a></code>.</p>
</td></tr>
<tr><td><code id="plot_predictions_+3A_by">by</code></td>
<td>
<p>character vector with column names that denote categories by which
the plot should be stratified. If for example you want to have a facetted
plot, this should be a character vector with the columns used in facetting
(note that the facetting still needs to be done outside of the function call)</p>
</td></tr>
<tr><td><code id="plot_predictions_+3A_x">x</code></td>
<td>
<p>character vector of length one that denotes the name of the variable</p>
</td></tr>
<tr><td><code id="plot_predictions_+3A_range">range</code></td>
<td>
<p>numeric vector indicating the interval ranges to plot. If 0 is
included in range, the median prediction will be shown.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>ggplot object with a plot of true vs predicted values
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(ggplot2)
library(magrittr)

example_continuous %&gt;%
  make_NA (
    what = "truth",
    target_end_date &gt;= "2021-07-22",
    target_end_date &lt; "2021-05-01"
  ) %&gt;%
  make_NA (
    what = "forecast",
    model != "EuroCOVIDhub-ensemble",
    forecast_date != "2021-06-07"
  ) %&gt;%
  plot_predictions (
    x = "target_end_date",
    by = c("target_type", "location"),
    range = c(0, 50, 90, 95)
  ) +
  facet_wrap(~ location + target_type, scales = "free_y") +
  aes(fill = model, color = model)

example_continuous %&gt;%
  make_NA (
    what = "truth",
    target_end_date &gt;= "2021-07-22",
    target_end_date &lt; "2021-05-01"
  ) %&gt;%
  make_NA (
    what = "forecast",
    forecast_date != "2021-06-07"
  ) %&gt;%
  plot_predictions (
    x = "target_end_date",
    by = c("target_type", "location"),
    range = c(0)
  ) +
  facet_wrap(~ location + target_type, scales = "free_y") +
  aes(fill = model, color = model)
</code></pre>

<hr>
<h2 id='plot_quantile_coverage'>Plot Quantile Coverage</h2><span id='topic+plot_quantile_coverage'></span>

<h3>Description</h3>

<p>Plot quantile coverage
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_quantile_coverage(scores, colour = "model")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_quantile_coverage_+3A_scores">scores</code></td>
<td>
<p>A data.frame of scores based on quantile forecasts as
produced by <code><a href="#topic+score">score()</a></code> or <code><a href="#topic+summarise_scores">summarise_scores()</a></code>. Note that &quot;range&quot; must be included
in the <code>by</code> argument when running <code><a href="#topic+summarise_scores">summarise_scores()</a></code></p>
</td></tr>
<tr><td><code id="plot_quantile_coverage_+3A_colour">colour</code></td>
<td>
<p>According to which variable shall the graphs be coloured?
Default is &quot;model&quot;.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>ggplot object with a plot of interval coverage
</p>


<h3>Examples</h3>

<pre><code class='language-R'>scores &lt;- score(example_quantile)
scores &lt;- summarise_scores(scores, by = c("model", "quantile"))
plot_quantile_coverage(scores)
</code></pre>

<hr>
<h2 id='plot_ranges'>Plot Metrics by Range of the Prediction Interval</h2><span id='topic+plot_ranges'></span>

<h3>Description</h3>

<p>Visualise the metrics by range, e.g. if you are interested how different
interval ranges contribute to the overall interval score, or how
sharpness / dispersion changes by range.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_ranges(scores, y = "interval_score", x = "model", colour = "range")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_ranges_+3A_scores">scores</code></td>
<td>
<p>A data.frame of scores based on quantile forecasts as
produced by <code><a href="#topic+score">score()</a></code> or <code><a href="#topic+summarise_scores">summarise_scores()</a></code>. Note that &quot;range&quot; must be included
in the <code>by</code> argument when running <code><a href="#topic+summarise_scores">summarise_scores()</a></code></p>
</td></tr>
<tr><td><code id="plot_ranges_+3A_y">y</code></td>
<td>
<p>The variable from the scores you want to show on the y-Axis.
This could be something like &quot;interval_score&quot; (the default) or &quot;dispersion&quot;</p>
</td></tr>
<tr><td><code id="plot_ranges_+3A_x">x</code></td>
<td>
<p>The variable from the scores you want to show on the x-Axis.
Usually this will be &quot;model&quot;</p>
</td></tr>
<tr><td><code id="plot_ranges_+3A_colour">colour</code></td>
<td>
<p>Character vector of length one used to determine a variable
for colouring dots. The Default is &quot;range&quot;.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A ggplot2 object showing a contributions from the three components of
the weighted interval score
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(ggplot2)
scores &lt;- score(example_quantile)
scores &lt;- summarise_scores(scores, by = c("model", "target_type", "range"))

plot_ranges(scores, x = "model") +
  facet_wrap(~target_type, scales = "free")

# visualise dispersion instead of interval score
plot_ranges(scores, y = "dispersion", x = "model") +
  facet_wrap(~target_type)
</code></pre>

<hr>
<h2 id='plot_score_table'>Plot Coloured Score Table</h2><span id='topic+plot_score_table'></span>

<h3>Description</h3>

<p>Plots a coloured table of summarised scores obtained using
<code><a href="#topic+score">score()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_score_table(scores, y = "model", by = NULL, metrics = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_score_table_+3A_scores">scores</code></td>
<td>
<p>A data.table of scores as produced by <code><a href="#topic+score">score()</a></code>.</p>
</td></tr>
<tr><td><code id="plot_score_table_+3A_y">y</code></td>
<td>
<p>the variable to be shown on the y-axis. Instead of a single character string,
you can also specify a vector with column names, e.g.
<code>y = c("model", "location")</code>. These column names will be concatenated
to create a unique row identifier (e.g. &quot;model1_location1&quot;).</p>
</td></tr>
<tr><td><code id="plot_score_table_+3A_by">by</code></td>
<td>
<p>A character vector that determines how the colour shading for the
plot gets computed. By default (<code>NULL</code>), shading will be determined per
metric, but you can provide additional column names (see examples).</p>
</td></tr>
<tr><td><code id="plot_score_table_+3A_metrics">metrics</code></td>
<td>
<p>A character vector with the metrics to show. If set to
<code>NULL</code> (default), all metrics present in <code>scores</code> will be shown.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A ggplot2 object with a coloured table of summarised scores
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(ggplot2)
library(magrittr) # pipe operator


scores &lt;- score(example_quantile) %&gt;%
  summarise_scores(by = c("model", "target_type")) %&gt;%
  summarise_scores(fun = signif, digits = 2)

plot_score_table(scores, y = "model", by = "target_type") +
  facet_wrap(~target_type, ncol = 1)

# can also put target description on the y-axis
plot_score_table(scores,
                 y = c("model", "target_type"),
                 by = "target_type")
</code></pre>

<hr>
<h2 id='plot_wis'>Plot Contributions to the Weighted Interval Score</h2><span id='topic+plot_wis'></span>

<h3>Description</h3>

<p>Visualise the components of the weighted interval score: penalties for
over-prediction, under-prediction and for high dispersion (lack of sharpness)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_wis(scores, x = "model", relative_contributions = FALSE, flip = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_wis_+3A_scores">scores</code></td>
<td>
<p>A data.frame of scores based on quantile forecasts as
produced by <code><a href="#topic+score">score()</a></code> and summarised using <code><a href="#topic+summarise_scores">summarise_scores()</a></code></p>
</td></tr>
<tr><td><code id="plot_wis_+3A_x">x</code></td>
<td>
<p>The variable from the scores you want to show on the x-Axis.
Usually this will be &quot;model&quot;.</p>
</td></tr>
<tr><td><code id="plot_wis_+3A_relative_contributions">relative_contributions</code></td>
<td>
<p>show relative contributions instead of absolute
contributions. Default is FALSE and this functionality is not available yet.</p>
</td></tr>
<tr><td><code id="plot_wis_+3A_flip">flip</code></td>
<td>
<p>boolean (default is <code>FALSE</code>), whether or not to flip the axes.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A ggplot2 object showing a contributions from the three components of
the weighted interval score
</p>


<h3>References</h3>

<p>Bracher J, Ray E, Gneiting T, Reich, N (2020) Evaluating epidemic forecasts
in an interval format. <a href="https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008618">https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008618</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(ggplot2)
scores &lt;- score(example_quantile)
scores &lt;- summarise_scores(scores, by = c("model", "target_type"))

plot_wis(scores,
  x = "model",
  relative_contributions = TRUE
) +
  facet_wrap(~target_type)
plot_wis(scores,
  x = "model",
  relative_contributions = FALSE
) +
  facet_wrap(~target_type, scales = "free_x")
</code></pre>

<hr>
<h2 id='prediction_is_quantile'>Check if predictions are quantile forecasts</h2><span id='topic+prediction_is_quantile'></span>

<h3>Description</h3>

<p>Internal helper function to check if a data frame contains
quantile forecast predictions. This is determined by checking if the
&quot;quantile&quot; column is present.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prediction_is_quantile(data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="prediction_is_quantile_+3A_data">data</code></td>
<td>
<p>Data frame containing forecast predictions</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Logical indicating whether predictions are quantile forecasts
</p>

<hr>
<h2 id='print.scoringutils_check'>Print output from <code>check_forecasts()</code></h2><span id='topic+print.scoringutils_check'></span>

<h3>Description</h3>

<p>Helper function that prints the output generated by
<code><a href="#topic+check_forecasts">check_forecasts()</a></code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'scoringutils_check'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.scoringutils_check_+3A_x">x</code></td>
<td>
<p>An object of class 'scoringutils_check' as produced by
<code><a href="#topic+check_forecasts">check_forecasts()</a></code></p>
</td></tr>
<tr><td><code id="print.scoringutils_check_+3A_...">...</code></td>
<td>
<p>additional arguments (not used here)</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>check &lt;- check_forecasts(example_quantile)
print(check)
</code></pre>

<hr>
<h2 id='quantile_score'>Quantile Score</h2><span id='topic+quantile_score'></span>

<h3>Description</h3>

<p>Proper Scoring Rule to score quantile predictions. Smaller values are better.
The quantile score is
closely related to the Interval score (see <code><a href="#topic+interval_score">interval_score()</a></code>) and is
the quantile equivalent that works with single quantiles instead of
central prediction intervals.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>quantile_score(true_values, predictions, quantiles, weigh = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="quantile_score_+3A_true_values">true_values</code></td>
<td>
<p>A vector with the true observed values of size n</p>
</td></tr>
<tr><td><code id="quantile_score_+3A_predictions">predictions</code></td>
<td>
<p>nxN matrix of predictive samples, n (number of rows) being
the number of data points and N (number of columns) the number of Monte
Carlo samples. Alternatively, predictions can just be a vector of size n.</p>
</td></tr>
<tr><td><code id="quantile_score_+3A_quantiles">quantiles</code></td>
<td>
<p>vector of size n with the quantile values of the
corresponding predictions.</p>
</td></tr>
<tr><td><code id="quantile_score_+3A_weigh">weigh</code></td>
<td>
<p>if TRUE, weigh the score by alpha / 2, so it can be averaged
into an interval score that, in the limit, corresponds to CRPS. Alpha is the
value that corresponds to the (alpha/2) or (1 - alpha/2) quantiles provided
and will be computed from the quantile. Alpha is the decimal value that
represents how much is outside a central prediction interval (E.g. for a
90 percent central prediction interval, alpha is 0.1). Default: <code>TRUE</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>vector with the scoring values
</p>


<h3>References</h3>

<p>Strictly Proper Scoring Rules, Prediction,and Estimation,
Tilmann Gneiting and Adrian E. Raftery, 2007, Journal of the American
Statistical Association, Volume 102, 2007 - Issue 477
</p>
<p>Evaluating epidemic forecasts in an interval format,
Johannes Bracher, Evan L. Ray, Tilmann Gneiting and Nicholas G. Reich,
<a href="https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008618">https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008618</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>true_values &lt;- rnorm(10, mean = 1:10)
alpha &lt;- 0.5

lower &lt;- qnorm(alpha / 2, rnorm(10, mean = 1:10))
upper &lt;- qnorm((1 - alpha / 2), rnorm(10, mean = 1:10))

qs_lower &lt;- quantile_score(true_values,
  predictions = lower,
  quantiles = alpha / 2
)
qs_upper &lt;- quantile_score(true_values,
  predictions = upper,
  quantiles = 1 - alpha / 2
)
interval_score &lt;- (qs_lower + qs_upper) / 2
</code></pre>

<hr>
<h2 id='quantile_to_range_long'>Change Data from a Plain Quantile Format to a Long Range Format</h2><span id='topic+quantile_to_range_long'></span>

<h3>Description</h3>

<p>Transform data from a format that uses quantiles only to one that uses
interval ranges to denote quantiles.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>quantile_to_range_long(data, keep_quantile_col = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="quantile_to_range_long_+3A_data">data</code></td>
<td>
<p>a data.frame in quantile format</p>
</td></tr>
<tr><td><code id="quantile_to_range_long_+3A_keep_quantile_col">keep_quantile_col</code></td>
<td>
<p>keep the quantile column in the final
output after transformation (default is FALSE)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a data.frame in a long interval range format
</p>

<hr>
<h2 id='range_long_to_quantile'>Change Data from a Range Format to a Quantile Format</h2><span id='topic+range_long_to_quantile'></span>

<h3>Description</h3>

<p>Transform data from a format that uses interval ranges to denote quantiles
to a format that uses quantiles only.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>range_long_to_quantile(data, keep_range_col = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="range_long_to_quantile_+3A_data">data</code></td>
<td>
<p>a data.frame following the specifications from
<code><a href="#topic+score">score()</a></code>) for quantile forecasts.</p>
</td></tr>
<tr><td><code id="range_long_to_quantile_+3A_keep_range_col">keep_range_col</code></td>
<td>
<p>keep the range and boundary columns after
transformation (default is FALSE)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a data.frame in a plain quantile format
</p>

<hr>
<h2 id='sample_to_quantile'>Change Data from a Sample Based Format to a Quantile Format</h2><span id='topic+sample_to_quantile'></span>

<h3>Description</h3>

<p>Transform data from a format that is based on predictive samples to a format
based on plain quantiles.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sample_to_quantile(data, quantiles = c(0.05, 0.25, 0.5, 0.75, 0.95), type = 7)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sample_to_quantile_+3A_data">data</code></td>
<td>
<p>a data.frame with samples</p>
</td></tr>
<tr><td><code id="sample_to_quantile_+3A_quantiles">quantiles</code></td>
<td>
<p>a numeric vector of quantiles to extract</p>
</td></tr>
<tr><td><code id="sample_to_quantile_+3A_type">type</code></td>
<td>
<p>type argument passed down to the quantile function. For more
information, see <code><a href="stats.html#topic+quantile">quantile()</a></code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>a data.frame in a long interval range format
</p>


<h3>Examples</h3>

<pre><code class='language-R'>sample_to_quantile(example_integer)
</code></pre>

<hr>
<h2 id='sample_to_range_long'>Change Data from a Sample Based Format to a Long Interval Range Format</h2><span id='topic+sample_to_range_long'></span>

<h3>Description</h3>

<p>Transform data from a format that is based on predictive samples to a format
based on interval ranges
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sample_to_range_long(
  data,
  range = c(0, 50, 90),
  type = 7,
  keep_quantile_col = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sample_to_range_long_+3A_data">data</code></td>
<td>
<p>a data.frame with samples</p>
</td></tr>
<tr><td><code id="sample_to_range_long_+3A_range">range</code></td>
<td>
<p>a numeric vector of interval ranges to extract
(e.g. <code>c(0, 50, 90)</code>)</p>
</td></tr>
<tr><td><code id="sample_to_range_long_+3A_type">type</code></td>
<td>
<p>type argument passed down to the quantile function. For more
information, see <code><a href="stats.html#topic+quantile">quantile()</a></code></p>
</td></tr>
<tr><td><code id="sample_to_range_long_+3A_keep_quantile_col">keep_quantile_col</code></td>
<td>
<p>keep quantile column, default is TRUE</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a data.frame in a long interval range format
</p>

<hr>
<h2 id='score'>Evaluate forecasts</h2><span id='topic+score'></span>

<h3>Description</h3>

<p>This function allows automatic scoring of forecasts using a
range of metrics. For most users it will be the workhorse for
scoring forecasts as it wraps the lower level functions package functions.
However, these functions are also available if you wish to make use of them
independently.
</p>
<p>A range of forecasts formats are supported, including quantile-based,
sample-based, binary forecasts. Prior to scoring, users may wish to make use
of <code><a href="#topic+check_forecasts">check_forecasts()</a></code> to ensure that the input data is in a supported
format though this will also be run internally by <code><a href="#topic+score">score()</a></code>. Examples for
each format are also provided (see the documentation for <code>data</code> below or in
<code><a href="#topic+check_forecasts">check_forecasts()</a></code>).
</p>
<p>Each format has a set of required columns (see below). Additional columns may
be present to indicate a grouping of forecasts. For example, we could have
forecasts made by different models in various locations at different time
points, each for several weeks into the future. It is important, that there
are only columns present which are relevant in order to group forecasts.
A combination of different columns should uniquely define the
<em>unit of a single forecast</em>, meaning that a single forecast is defined by the
values in the other columns. Adding additional unrelated columns may alter
results.
</p>
<p>To obtain a quick overview of the currently supported evaluation metrics,
have a look at the <a href="#topic+metrics">metrics</a> data included in the package. The column
<code>metrics$Name</code> gives an overview of all available metric names that can be
computed. If interested in an unsupported metric please open a <a href="https://github.com/epiforecasts/scoringutils/issues">feature request</a> or consider
contributing a pull request.
</p>
<p>For additional help and examples, check out the <a href="https://epiforecasts.io/scoringutils/articles/scoringutils.html">Getting Started Vignette</a>
as well as the paper <a href="https://arxiv.org/abs/2205.07090">Evaluating Forecasts with scoringutils in R</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>score(data, metrics = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="score_+3A_data">data</code></td>
<td>
<p>A data.frame or data.table with the predictions and observations.
For scoring using <code><a href="#topic+score">score()</a></code>, the following columns need to be present:
</p>

<ul>
<li> <p><code>true_value</code> - the true observed values
</p>
</li>
<li> <p><code>prediction</code> - predictions or predictive samples for one
true value. (You only don't need to provide a prediction column if
you want to score quantile forecasts in a wide range format.)</p>
</li></ul>

<p>For scoring integer and continuous forecasts a <code>sample</code> column is needed:
</p>

<ul>
<li> <p><code>sample</code> - an index to identify the predictive samples in the
prediction column generated by one model for one true value. Only
necessary for continuous and integer forecasts, not for
binary predictions.</p>
</li></ul>

<p>For scoring predictions in a quantile-format forecast you should provide
a column called <code>quantile</code>:
</p>

<ul>
<li> <p><code>quantile</code>: quantile to which the prediction corresponds
</p>
</li></ul>

<p>In addition a <code>model</code> column is suggested and if not present this will be
flagged and added to the input data with all forecasts assigned as an
&quot;unspecified model&quot;).
</p>
<p>You can check the format of your data using <code><a href="#topic+check_forecasts">check_forecasts()</a></code> and there
are examples for each format (<a href="#topic+example_quantile">example_quantile</a>, <a href="#topic+example_continuous">example_continuous</a>,
<a href="#topic+example_integer">example_integer</a>, and <a href="#topic+example_binary">example_binary</a>).</p>
</td></tr>
<tr><td><code id="score_+3A_metrics">metrics</code></td>
<td>
<p>the metrics you want to have in the output. If <code>NULL</code> (the
default), all available metrics will be computed. For a list of available
metrics see <code><a href="#topic+available_metrics">available_metrics()</a></code>, or  check the <a href="#topic+metrics">metrics</a> data set.</p>
</td></tr>
<tr><td><code id="score_+3A_...">...</code></td>
<td>
<p>additional parameters passed down to <code><a href="#topic+score_quantile">score_quantile()</a></code> (internal
function used for scoring forecasts in a quantile-based format).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data.table with unsummarised scores. There will be one score per
quantile or sample, which is usually not desired, so you should almost
always run <code><a href="#topic+summarise_scores">summarise_scores()</a></code> on the unsummarised scores.
</p>


<h3>Author(s)</h3>

<p>Nikos Bosse <a href="mailto:nikosbosse@gmail.com">nikosbosse@gmail.com</a>
</p>


<h3>References</h3>

<p>Funk S, Camacho A, Kucharski AJ, Lowe R, Eggo RM, Edmunds WJ
(2019) Assessing the performance of real-time epidemic forecasts: A
case study of Ebola in the Western Area region of Sierra Leone, 2014-15.
PLoS Comput Biol 15(2): e1006785. <a href="https://doi.org/10.1371/journal.pcbi.1006785">doi:10.1371/journal.pcbi.1006785</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(magrittr) # pipe operator


check_forecasts(example_quantile)
score(example_quantile) %&gt;%
  add_coverage(by = c("model", "target_type")) %&gt;%
  summarise_scores(by = c("model", "target_type"))

# set forecast unit manually (to avoid issues with scoringutils trying to
# determine the forecast unit automatically), check forecasts before scoring
example_quantile %&gt;%
  set_forecast_unit(
    c("location", "target_end_date", "target_type", "horizon", "model")
  ) %&gt;%
  check_forecasts() %&gt;%
  score()

# forecast formats with different metrics
## Not run: 
score(example_binary)
score(example_quantile)
score(example_integer)
score(example_continuous)

## End(Not run)

# score point forecasts (marked by 'NA' in the quantile column)
score(example_point) %&gt;%
  summarise_scores(by = "model", na.rm = TRUE)

</code></pre>

<hr>
<h2 id='score_binary'>Evaluate forecasts in a Binary Format</h2><span id='topic+score_binary'></span>

<h3>Description</h3>

<p>Evaluate forecasts in a Binary Format
</p>


<h3>Usage</h3>

<pre><code class='language-R'>score_binary(data, forecast_unit, metrics)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="score_binary_+3A_data">data</code></td>
<td>
<p>A data.frame or data.table with the predictions and observations.
For scoring using <code><a href="#topic+score">score()</a></code>, the following columns need to be present:
</p>

<ul>
<li> <p><code>true_value</code> - the true observed values
</p>
</li>
<li> <p><code>prediction</code> - predictions or predictive samples for one
true value. (You only don't need to provide a prediction column if
you want to score quantile forecasts in a wide range format.)</p>
</li></ul>

<p>For scoring integer and continuous forecasts a <code>sample</code> column is needed:
</p>

<ul>
<li> <p><code>sample</code> - an index to identify the predictive samples in the
prediction column generated by one model for one true value. Only
necessary for continuous and integer forecasts, not for
binary predictions.</p>
</li></ul>

<p>For scoring predictions in a quantile-format forecast you should provide
a column called <code>quantile</code>:
</p>

<ul>
<li> <p><code>quantile</code>: quantile to which the prediction corresponds
</p>
</li></ul>

<p>In addition a <code>model</code> column is suggested and if not present this will be
flagged and added to the input data with all forecasts assigned as an
&quot;unspecified model&quot;).
</p>
<p>You can check the format of your data using <code><a href="#topic+check_forecasts">check_forecasts()</a></code> and there
are examples for each format (<a href="#topic+example_quantile">example_quantile</a>, <a href="#topic+example_continuous">example_continuous</a>,
<a href="#topic+example_integer">example_integer</a>, and <a href="#topic+example_binary">example_binary</a>).</p>
</td></tr>
<tr><td><code id="score_binary_+3A_forecast_unit">forecast_unit</code></td>
<td>
<p>A character vector with the column names that define
the unit of a single forecast, i.e. a forecast was made for a combination
of the values in <code>forecast_unit</code>.</p>
</td></tr>
<tr><td><code id="score_binary_+3A_metrics">metrics</code></td>
<td>
<p>the metrics you want to have in the output. If <code>NULL</code> (the
default), all available metrics will be computed. For a list of available
metrics see <code><a href="#topic+available_metrics">available_metrics()</a></code>, or  check the <a href="#topic+metrics">metrics</a> data set.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data.table with appropriate scores. For more information see
<code><a href="#topic+score">score()</a></code>.
</p>


<h3>Author(s)</h3>

<p>Nikos Bosse <a href="mailto:nikosbosse@gmail.com">nikosbosse@gmail.com</a>
</p>

<hr>
<h2 id='score_quantile'>Evaluate forecasts in a Quantile-Based Format</h2><span id='topic+score_quantile'></span>

<h3>Description</h3>

<p>Evaluate forecasts in a Quantile-Based Format
</p>


<h3>Usage</h3>

<pre><code class='language-R'>score_quantile(
  data,
  forecast_unit,
  metrics,
  weigh = TRUE,
  count_median_twice = FALSE,
  separate_results = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="score_quantile_+3A_data">data</code></td>
<td>
<p>A data.frame or data.table with the predictions and observations.
For scoring using <code><a href="#topic+score">score()</a></code>, the following columns need to be present:
</p>

<ul>
<li> <p><code>true_value</code> - the true observed values
</p>
</li>
<li> <p><code>prediction</code> - predictions or predictive samples for one
true value. (You only don't need to provide a prediction column if
you want to score quantile forecasts in a wide range format.)</p>
</li></ul>

<p>For scoring integer and continuous forecasts a <code>sample</code> column is needed:
</p>

<ul>
<li> <p><code>sample</code> - an index to identify the predictive samples in the
prediction column generated by one model for one true value. Only
necessary for continuous and integer forecasts, not for
binary predictions.</p>
</li></ul>

<p>For scoring predictions in a quantile-format forecast you should provide
a column called <code>quantile</code>:
</p>

<ul>
<li> <p><code>quantile</code>: quantile to which the prediction corresponds
</p>
</li></ul>

<p>In addition a <code>model</code> column is suggested and if not present this will be
flagged and added to the input data with all forecasts assigned as an
&quot;unspecified model&quot;).
</p>
<p>You can check the format of your data using <code><a href="#topic+check_forecasts">check_forecasts()</a></code> and there
are examples for each format (<a href="#topic+example_quantile">example_quantile</a>, <a href="#topic+example_continuous">example_continuous</a>,
<a href="#topic+example_integer">example_integer</a>, and <a href="#topic+example_binary">example_binary</a>).</p>
</td></tr>
<tr><td><code id="score_quantile_+3A_forecast_unit">forecast_unit</code></td>
<td>
<p>A character vector with the column names that define
the unit of a single forecast, i.e. a forecast was made for a combination
of the values in <code>forecast_unit</code></p>
</td></tr>
<tr><td><code id="score_quantile_+3A_metrics">metrics</code></td>
<td>
<p>the metrics you want to have in the output. If <code>NULL</code> (the
default), all available metrics will be computed. For a list of available
metrics see <code><a href="#topic+available_metrics">available_metrics()</a></code>, or  check the <a href="#topic+metrics">metrics</a> data set.</p>
</td></tr>
<tr><td><code id="score_quantile_+3A_weigh">weigh</code></td>
<td>
<p>if TRUE, weigh the score by alpha / 2, so it can be averaged
into an interval score that, in the limit, corresponds to CRPS. Alpha is the
decimal value that  represents how much is outside a central prediction
interval (e.g. for a 90 percent central prediction interval, alpha is 0.1)
Default: <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="score_quantile_+3A_count_median_twice">count_median_twice</code></td>
<td>
<p>logical that controls whether or not to count the
median twice when summarising (default is <code>FALSE</code>). Counting the
median twice would conceptually treat it as a 0\
the median is the lower as well as the upper bound. The alternative is to
treat the median as a single quantile forecast instead of an interval. The
interval score would then be better understood as an average of quantile
scores.</p>
</td></tr>
<tr><td><code id="score_quantile_+3A_separate_results">separate_results</code></td>
<td>
<p>if <code>TRUE</code> (default is <code>FALSE</code>), then the separate
parts of the interval score (dispersion penalty, penalties for over- and
under-prediction get returned as separate elements of a list). If you want a
<code>data.frame</code> instead, simply call <code><a href="base.html#topic+as.data.frame">as.data.frame()</a></code> on the output.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data.table with appropriate scores. For more information see
<code><a href="#topic+score">score()</a></code>
</p>


<h3>Author(s)</h3>

<p>Nikos Bosse <a href="mailto:nikosbosse@gmail.com">nikosbosse@gmail.com</a>
</p>


<h3>References</h3>

<p>Funk S, Camacho A, Kucharski AJ, Lowe R, Eggo RM, Edmunds WJ
(2019) Assessing the performance of real-time epidemic forecasts: A
case study of Ebola in the Western Area region of Sierra Leone, 2014-15.
PLoS Comput Biol 15(2): e1006785. <a href="https://doi.org/10.1371/journal.pcbi.1006785">doi:10.1371/journal.pcbi.1006785</a>
</p>

<hr>
<h2 id='score_sample'>Evaluate forecasts in a Sample-Based Format (Integer or Continuous)</h2><span id='topic+score_sample'></span>

<h3>Description</h3>

<p>Evaluate forecasts in a Sample-Based Format (Integer or Continuous)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>score_sample(data, forecast_unit, metrics, prediction_type)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="score_sample_+3A_data">data</code></td>
<td>
<p>A data.frame or data.table with the predictions and observations.
For scoring using <code><a href="#topic+score">score()</a></code>, the following columns need to be present:
</p>

<ul>
<li> <p><code>true_value</code> - the true observed values
</p>
</li>
<li> <p><code>prediction</code> - predictions or predictive samples for one
true value. (You only don't need to provide a prediction column if
you want to score quantile forecasts in a wide range format.)</p>
</li></ul>

<p>For scoring integer and continuous forecasts a <code>sample</code> column is needed:
</p>

<ul>
<li> <p><code>sample</code> - an index to identify the predictive samples in the
prediction column generated by one model for one true value. Only
necessary for continuous and integer forecasts, not for
binary predictions.</p>
</li></ul>

<p>For scoring predictions in a quantile-format forecast you should provide
a column called <code>quantile</code>:
</p>

<ul>
<li> <p><code>quantile</code>: quantile to which the prediction corresponds
</p>
</li></ul>

<p>In addition a <code>model</code> column is suggested and if not present this will be
flagged and added to the input data with all forecasts assigned as an
&quot;unspecified model&quot;).
</p>
<p>You can check the format of your data using <code><a href="#topic+check_forecasts">check_forecasts()</a></code> and there
are examples for each format (<a href="#topic+example_quantile">example_quantile</a>, <a href="#topic+example_continuous">example_continuous</a>,
<a href="#topic+example_integer">example_integer</a>, and <a href="#topic+example_binary">example_binary</a>).</p>
</td></tr>
<tr><td><code id="score_sample_+3A_forecast_unit">forecast_unit</code></td>
<td>
<p>A character vector with the column names that define
the unit of a single forecast, i.e. a forecast was made for a combination
of the values in <code>forecast_unit</code></p>
</td></tr>
<tr><td><code id="score_sample_+3A_metrics">metrics</code></td>
<td>
<p>the metrics you want to have in the output. If <code>NULL</code> (the
default), all available metrics will be computed. For a list of available
metrics see <code><a href="#topic+available_metrics">available_metrics()</a></code>, or  check the <a href="#topic+metrics">metrics</a> data set.</p>
</td></tr>
<tr><td><code id="score_sample_+3A_prediction_type">prediction_type</code></td>
<td>
<p>character, should be either &quot;continuous&quot; or &quot;integer&quot;</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data.table with appropriate scores. For more information see
<code><a href="#topic+score">score()</a></code>
</p>


<h3>Author(s)</h3>

<p>Nikos Bosse <a href="mailto:nikosbosse@gmail.com">nikosbosse@gmail.com</a>
</p>


<h3>References</h3>

<p>Funk S, Camacho A, Kucharski AJ, Lowe R, Eggo RM, Edmunds WJ
(2019) Assessing the performance of real-time epidemic forecasts: A
case study of Ebola in the Western Area region of Sierra Leone, 2014-15.
PLoS Comput Biol 15(2): e1006785. <a href="https://doi.org/10.1371/journal.pcbi.1006785">doi:10.1371/journal.pcbi.1006785</a>
</p>

<hr>
<h2 id='se_mean_sample'>Squared Error of the Mean (Sample-based Version)</h2><span id='topic+se_mean_sample'></span>

<h3>Description</h3>

<p>Squared error of the mean calculated as
</p>
<p style="text-align: center;"><code class="reqn">
  \textrm{mean}(\textrm{true\_value} - \textrm{prediction})^2
</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>se_mean_sample(true_values, predictions)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="se_mean_sample_+3A_true_values">true_values</code></td>
<td>
<p>A vector with the true observed values of size n</p>
</td></tr>
<tr><td><code id="se_mean_sample_+3A_predictions">predictions</code></td>
<td>
<p>nxN matrix of predictive samples, n (number of rows) being
the number of data points and N (number of columns) the number of Monte
Carlo samples. Alternatively, predictions can just be a vector of size n.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>vector with the scoring values
</p>


<h3>See Also</h3>

<p><code><a href="#topic+squared_error">squared_error()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>true_values &lt;- rnorm(30, mean = 1:30)
predicted_values &lt;- rnorm(30, mean = 1:30)
se_mean_sample(true_values, predicted_values)
</code></pre>

<hr>
<h2 id='set_forecast_unit'>Set unit of a single forecast manually</h2><span id='topic+set_forecast_unit'></span>

<h3>Description</h3>

<p>Helper function to set the unit of a single forecast (i.e. the
combination of columns that uniquely define a single forecast) manually.
This simple function keeps the columns specified in <code>forecast_unit</code> (plus
additional protected columns, e.g. for true values, predictions or quantile
levels) and removes duplicate rows.
If not done manually, <code>scoringutils</code> attempts to determine the unit
of a single forecast automatically by simply assuming that all column names
are relevant to determine the forecast unit. This may lead to unexpected
behaviour, so setting the forecast unit explicitly can help make the code
easier to debug and easier to read. When used as part of a workflow,
<code>set_forecast_unit()</code> can be directly piped into <code>check_forecasts()</code> to
check everything is in order.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>set_forecast_unit(data, forecast_unit)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="set_forecast_unit_+3A_data">data</code></td>
<td>
<p>A data.frame or data.table with the predictions and observations.
For scoring using <code><a href="#topic+score">score()</a></code>, the following columns need to be present:
</p>

<ul>
<li> <p><code>true_value</code> - the true observed values
</p>
</li>
<li> <p><code>prediction</code> - predictions or predictive samples for one
true value. (You only don't need to provide a prediction column if
you want to score quantile forecasts in a wide range format.)</p>
</li></ul>

<p>For scoring integer and continuous forecasts a <code>sample</code> column is needed:
</p>

<ul>
<li> <p><code>sample</code> - an index to identify the predictive samples in the
prediction column generated by one model for one true value. Only
necessary for continuous and integer forecasts, not for
binary predictions.</p>
</li></ul>

<p>For scoring predictions in a quantile-format forecast you should provide
a column called <code>quantile</code>:
</p>

<ul>
<li> <p><code>quantile</code>: quantile to which the prediction corresponds
</p>
</li></ul>

<p>In addition a <code>model</code> column is suggested and if not present this will be
flagged and added to the input data with all forecasts assigned as an
&quot;unspecified model&quot;).
</p>
<p>You can check the format of your data using <code><a href="#topic+check_forecasts">check_forecasts()</a></code> and there
are examples for each format (<a href="#topic+example_quantile">example_quantile</a>, <a href="#topic+example_continuous">example_continuous</a>,
<a href="#topic+example_integer">example_integer</a>, and <a href="#topic+example_binary">example_binary</a>).</p>
</td></tr>
<tr><td><code id="set_forecast_unit_+3A_forecast_unit">forecast_unit</code></td>
<td>
<p>Character vector with the names of the columns that
uniquely identify a single forecast.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data.table with only those columns kept that are relevant to
scoring or denote the unit of a single forecast as specified by the user.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set_forecast_unit(
  example_quantile,
  c("location", "target_end_date", "target_type", "horizon", "model")
)
</code></pre>

<hr>
<h2 id='squared_error'>Squared Error</h2><span id='topic+squared_error'></span>

<h3>Description</h3>

<p>Squared Error SE calculated as
</p>
<p style="text-align: center;"><code class="reqn">
  (\textrm{true\_values} - \textrm{predicted\_values})^2
</code>
</p>



<h3>Usage</h3>

<pre><code class='language-R'>squared_error(true_values, predictions)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="squared_error_+3A_true_values">true_values</code></td>
<td>
<p>A vector with the true observed values of size n</p>
</td></tr>
<tr><td><code id="squared_error_+3A_predictions">predictions</code></td>
<td>
<p>A vector with predicted values of size n</p>
</td></tr>
</table>


<h3>Value</h3>

<p>vector with the scoring values
</p>


<h3>Examples</h3>

<pre><code class='language-R'>true_values &lt;- rnorm(30, mean = 1:30)
predicted_values &lt;- rnorm(30, mean = 1:30)
squared_error(true_values, predicted_values)
</code></pre>

<hr>
<h2 id='summarise_scores'>Summarise scores as produced by <code><a href="#topic+score">score()</a></code></h2><span id='topic+summarise_scores'></span><span id='topic+summarize_scores'></span>

<h3>Description</h3>

<p>Summarise scores as produced by <code><a href="#topic+score">score()</a></code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>summarise_scores(
  scores,
  by = NULL,
  across = NULL,
  fun = mean,
  relative_skill = FALSE,
  relative_skill_metric = "auto",
  metric = deprecated(),
  baseline = NULL,
  ...
)

summarize_scores(
  scores,
  by = NULL,
  across = NULL,
  fun = mean,
  relative_skill = FALSE,
  relative_skill_metric = "auto",
  metric = deprecated(),
  baseline = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summarise_scores_+3A_scores">scores</code></td>
<td>
<p>A data.table of scores as produced by <code><a href="#topic+score">score()</a></code>.</p>
</td></tr>
<tr><td><code id="summarise_scores_+3A_by">by</code></td>
<td>
<p>character vector with column names to summarise scores by. Default
is <code>NULL</code>, meaning that the only summary that takes is place is summarising
over samples or quantiles (in case of quantile-based forecasts), such that
there is one score per forecast as defined by the <em>unit of a single forecast</em>
(rather than one score for every sample or quantile).
The <em>unit of a single forecast</em> is determined by the columns present in the
input data that do not correspond to a metric produced by <code><a href="#topic+score">score()</a></code>, which
indicate indicate a grouping of forecasts (for example there may be one
forecast per day, location and model). Adding additional, unrelated, columns
may alter results in an unpredictable way.</p>
</td></tr>
<tr><td><code id="summarise_scores_+3A_across">across</code></td>
<td>
<p>character vector with column names from the vector of variables
that define the <em>unit of a single forecast</em> (see above) to summarise scores
across (meaning that the specified columns will be dropped). This is an
alternative to specifying <code>by</code> directly. If <code>NULL</code> (default), then <code>by</code> will
be used or inferred internally if also not specified. Only  one of <code>across</code>
and <code>by</code>  may be used at a time.</p>
</td></tr>
<tr><td><code id="summarise_scores_+3A_fun">fun</code></td>
<td>
<p>a function used for summarising scores. Default is <code>mean</code>.</p>
</td></tr>
<tr><td><code id="summarise_scores_+3A_relative_skill">relative_skill</code></td>
<td>
<p>logical, whether or not to compute relative
performance between models based on pairwise comparisons.
If <code>TRUE</code> (default is <code>FALSE</code>), then a column called
'model' must be present in the input data. For more information on
the computation of relative skill, see <code><a href="#topic+pairwise_comparison">pairwise_comparison()</a></code>.
Relative skill will be calculated for the aggregation level specified in
<code>by</code>.</p>
</td></tr>
<tr><td><code id="summarise_scores_+3A_relative_skill_metric">relative_skill_metric</code></td>
<td>
<p>character with the name of the metric for which
a relative skill shall be computed. If equal to 'auto' (the default), then
this will be either interval score, CRPS or Brier score (depending on which
of these is available in the input data)</p>
</td></tr>
<tr><td><code id="summarise_scores_+3A_metric">metric</code></td>
<td>
<p><a href="https://lifecycle.r-lib.org/articles/stages.html#deprecated"><img src="../help/figures/lifecycle-deprecated.svg" alt='[Deprecated]' /></a> Deprecated in 1.1.0. Use
<code>relative_skill_metric</code> instead.</p>
</td></tr>
<tr><td><code id="summarise_scores_+3A_baseline">baseline</code></td>
<td>
<p>character string with the name of a model. If a baseline is
given, then a scaled relative skill with respect to the baseline will be
returned. By default (<code>NULL</code>), relative skill will not be scaled with
respect to a baseline model.</p>
</td></tr>
<tr><td><code id="summarise_scores_+3A_...">...</code></td>
<td>
<p>additional parameters that can be passed to the summary function
provided to <code>fun</code>. For more information see the documentation of the
respective function.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>
library(magrittr) # pipe operator

scores &lt;- score(example_continuous)
summarise_scores(scores)


# summarise over samples or quantiles to get one score per forecast
scores &lt;- score(example_quantile)
summarise_scores(scores)

# get scores by model
summarise_scores(scores,by = "model")

# get scores by model and target type
summarise_scores(scores, by = c("model", "target_type"))

# Get scores summarised across horizon, forecast date, and target end date
summarise_scores(
 scores, across = c("horizon", "forecast_date", "target_end_date")
)

# get standard deviation
summarise_scores(scores, by = "model", fun = sd)

# round digits
summarise_scores(scores,by = "model") %&gt;%
  summarise_scores(fun = signif, digits = 2)

# get quantiles of scores
# make sure to aggregate over ranges first
summarise_scores(scores,
  by = "model", fun = quantile,
  probs = c(0.25, 0.5, 0.75)
)

# get ranges
# summarise_scores(scores, by = "range")
</code></pre>

<hr>
<h2 id='theme_scoringutils'>Scoringutils ggplot2 theme</h2><span id='topic+theme_scoringutils'></span>

<h3>Description</h3>

<p>A theme for ggplot2 plots used in scoringutils
</p>


<h3>Usage</h3>

<pre><code class='language-R'>theme_scoringutils()
</code></pre>


<h3>Value</h3>

<p>A ggplot2 theme
</p>

<hr>
<h2 id='transform_forecasts'>Transform forecasts and observed values</h2><span id='topic+transform_forecasts'></span>

<h3>Description</h3>

<p>Function to transform forecasts and true values before scoring.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>transform_forecasts(data, fun = log_shift, append = TRUE, label = "log", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="transform_forecasts_+3A_data">data</code></td>
<td>
<p>A data.frame or data.table with the predictions and observations.
For scoring using <code><a href="#topic+score">score()</a></code>, the following columns need to be present:
</p>

<ul>
<li> <p><code>true_value</code> - the true observed values
</p>
</li>
<li> <p><code>prediction</code> - predictions or predictive samples for one
true value. (You only don't need to provide a prediction column if
you want to score quantile forecasts in a wide range format.)</p>
</li></ul>

<p>For scoring integer and continuous forecasts a <code>sample</code> column is needed:
</p>

<ul>
<li> <p><code>sample</code> - an index to identify the predictive samples in the
prediction column generated by one model for one true value. Only
necessary for continuous and integer forecasts, not for
binary predictions.</p>
</li></ul>

<p>For scoring predictions in a quantile-format forecast you should provide
a column called <code>quantile</code>:
</p>

<ul>
<li> <p><code>quantile</code>: quantile to which the prediction corresponds
</p>
</li></ul>

<p>In addition a <code>model</code> column is suggested and if not present this will be
flagged and added to the input data with all forecasts assigned as an
&quot;unspecified model&quot;).
</p>
<p>You can check the format of your data using <code><a href="#topic+check_forecasts">check_forecasts()</a></code> and there
are examples for each format (<a href="#topic+example_quantile">example_quantile</a>, <a href="#topic+example_continuous">example_continuous</a>,
<a href="#topic+example_integer">example_integer</a>, and <a href="#topic+example_binary">example_binary</a>).</p>
</td></tr>
<tr><td><code id="transform_forecasts_+3A_fun">fun</code></td>
<td>
<p>A function used to transform both true values and predictions.
The default function is <code><a href="#topic+log_shift">log_shift()</a></code>, a custom function that is essentially
the same as <code><a href="base.html#topic+log">log()</a></code>, but has an additional arguments (<code>offset</code>)
that allows you add an offset before applying the logarithm. This is often
helpful as the natural log transformation is not defined at zero. A common,
and pragmatic solution, is to add a small offset to the data before applying
the log transformation. In our work we have often used an offset of 1 but
the precise value will depend on your application.</p>
</td></tr>
<tr><td><code id="transform_forecasts_+3A_append">append</code></td>
<td>
<p>Logical, defaults to <code>TRUE</code>. Whether or not to append a
transformed version of the data to the currently existing data (<code>TRUE</code>). If
selected, the data gets transformed and appended to the existing data frame,
making it possible to use the outcome directly in <code><a href="#topic+score">score()</a></code>. An additional
column, 'scale', gets created that denotes which rows or untransformed
('scale' has the value &quot;natural&quot;) and which have been transformed ('scale'
has the value passed to the argument <code>label</code>).</p>
</td></tr>
<tr><td><code id="transform_forecasts_+3A_label">label</code></td>
<td>
<p>A string for the newly created 'scale' column to denote the
newly transformed values. Only relevant if <code>append = TRUE</code>.</p>
</td></tr>
<tr><td><code id="transform_forecasts_+3A_...">...</code></td>
<td>
<p>Additional parameters to pass to the function you supplied. For
the default option of <code><a href="#topic+log_shift">log_shift()</a></code> this could be the <code>offset</code> argument.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>There are a few reasons, depending on the circumstances, for
why this might be desirable (check out the linked reference for more info).
In epidemiology, for example, it may be useful to log-transform incidence
counts before evaluating forecasts using scores such as the weighted interval
score (WIS) or the continuous ranked probability score (CRPS).
Log-transforming forecasts and observations changes the interpretation of
the score from a measure of absolute distance between forecast and
observation to a score that evaluates a forecast of the exponential growth
rate. Another motivation can be to apply a variance-stabilising
transformation or to standardise incidence counts by population.
</p>
<p>Note that if you want to apply a transformation, it is important to transform
the forecasts and observations and then apply the score. Applying a
transformation after the score risks losing propriety of the proper scoring
rule.
</p>


<h3>Value</h3>

<p>A <code>data.table</code> with either a transformed version of the data, or one
with both the untransformed and the transformed data. includes the original
data as well as a transformation of the original data. There will be one
additional column, &lsquo;scale&rsquo;, present which will be set to &quot;natural&quot; for the
untransformed forecasts.
</p>


<h3>Author(s)</h3>

<p>Nikos Bosse <a href="mailto:nikosbosse@gmail.com">nikosbosse@gmail.com</a>
</p>


<h3>References</h3>

<p>Transformation of forecasts for evaluating predictive
performance in an epidemiological context
Nikos I. Bosse, Sam Abbott, Anne Cori, Edwin van Leeuwen, Johannes Bracher,
Sebastian Funk
medRxiv 2023.01.23.23284722
<a href="https://doi.org/10.1101/2023.01.23.23284722">doi:10.1101/2023.01.23.23284722</a>
<a href="https://www.medrxiv.org/content/10.1101/2023.01.23.23284722v1">https://www.medrxiv.org/content/10.1101/2023.01.23.23284722v1</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(magrittr) # pipe operator

# transform forecasts using the natural logarithm
# negative values need to be handled (here by replacing them with 0)
example_quantile %&gt;%
  .[, true_value := ifelse(true_value &lt; 0, 0, true_value)] %&gt;%
# Here we use the default function log_shift() which is essentially the same
# as log(), but has an additional arguments (offset) that allows you add an
# offset before applying the logarithm.
  transform_forecasts(append = FALSE) %&gt;%
  head()

# alternatively, integrating the truncation in the transformation function:
example_quantile %&gt;%
 transform_forecasts(
   fun = function(x) {log_shift(pmax(0, x))}, append = FALSE
 ) %&gt;%
 head()

# specifying an offset for the log transformation removes the
# warning caused by zeros in the data
example_quantile %&gt;%
  .[, true_value := ifelse(true_value &lt; 0, 0, true_value)] %&gt;%
  transform_forecasts(offset = 1, append = FALSE) %&gt;%
  head()

# adding square root transformed forecasts to the original ones
example_quantile %&gt;%
  .[, true_value := ifelse(true_value &lt; 0, 0, true_value)] %&gt;%
  transform_forecasts(fun = sqrt, label = "sqrt") %&gt;%
  score() %&gt;%
  summarise_scores(by = c("model", "scale"))

# adding multiple transformations
example_quantile %&gt;%
  .[, true_value := ifelse(true_value &lt; 0, 0, true_value)] %&gt;%
  transform_forecasts(fun = log_shift, offset = 1) %&gt;%
  transform_forecasts(fun = sqrt, label = "sqrt") %&gt;%
  head()
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
