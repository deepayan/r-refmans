<!DOCTYPE html><html><head><title>Help for package tfio</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {tfio}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#%&gt;%'><p>Pipe operator</p></a></li>
<li><a href='#arrow_feather_dataset'><p>Creates a <code>ArrowFeatherDataset</code>.</p></a></li>
<li><a href='#arrow_stream_dataset'><p>Creates a <code>ArrowStreamDataset</code>.</p></a></li>
<li><a href='#from_schema'><p>Create an Arrow Dataset from the given Arrow schema.</p></a></li>
<li><a href='#from_schema.arrow_feather_dataset'><p>Create an Arrow Dataset for reading record batches from Arrow feather files,</p>
inferring output types and shapes from the given Arrow schema.</a></li>
<li><a href='#from_schema.arrow_stream_dataset'><p>Create an Arrow Dataset from an input stream, inferring output types and</p>
shapes from the given Arrow schema.</a></li>
<li><a href='#ignite_dataset'><p>Create a <code>IgniteDataset</code>.</p></a></li>
<li><a href='#kafka_dataset'><p>Creates a <code>KafkaDataset</code>.</p></a></li>
<li><a href='#kinesis_dataset'><p>Creates a <code>KinesisDataset</code>.</p></a></li>
<li><a href='#lmdb_dataset'><p>Create a <code>LMDBDataset</code>.</p></a></li>
<li><a href='#make_libsvm_dataset'><p>Create a Dataset from LibSVM files.</p></a></li>
<li><a href='#mnist_image_dataset'><p>Creates a <code>MNISTImageDataset</code>.</p></a></li>
<li><a href='#mnist_label_dataset'><p>Creates a <code>MNISTLabelDataset</code>.</p></a></li>
<li><a href='#parquet_dataset'><p>Create a <code>ParquetDataset</code>.</p></a></li>
<li><a href='#pubsub_dataset'><p>Creates a <code>PubSubDataset</code>.</p></a></li>
<li><a href='#reexports'><p>Objects exported from other packages</p></a></li>
<li><a href='#sequence_file_dataset'><p>Create a <code>SequenceFileDataset</code>.</p></a></li>
<li><a href='#tfio'><p>TensorFlow IO API for R</p></a></li>
<li><a href='#tiff_dataset'><p>Create a <code>TIFFDataset</code>.</p></a></li>
<li><a href='#video_dataset'><p>Create a <code>VideoDataset</code> that reads the video file.</p></a></li>
<li><a href='#webp_dataset'><p>Create a <code>WebPDataset</code>.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Interface to 'TensorFlow IO'</td>
</tr>
<tr>
<td>Version:</td>
<td>0.4.1</td>
</tr>
<tr>
<td>Description:</td>
<td>Interface to 'TensorFlow IO', Datasets and filesystem extensions maintained by 'TensorFlow SIG-IO' <a href="https://github.com/tensorflow/community/blob/master/sigs/io/CHARTER.md">https://github.com/tensorflow/community/blob/master/sigs/io/CHARTER.md</a>.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.apache.org/licenses/LICENSE-2.0">Apache License 2.0</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/tensorflow/io">https://github.com/tensorflow/io</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/tensorflow/io/issues">https://github.com/tensorflow/io/issues</a></td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>TensorFlow &gt;= 1.13.0 (https://www.tensorflow.org/)
and TensorFlow IO &gt;= 0.4.0 (https://github.com/tensorflow/io)</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.1)</td>
</tr>
<tr>
<td>Imports:</td>
<td>reticulate (&ge; 1.10), tensorflow (&ge; 1.9), tfdatasets (&ge;
1.9), forge, magrittr</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.0.2</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat, knitr</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2019-12-19 14:00:25 UTC; yuan.tang</td>
</tr>
<tr>
<td>Author:</td>
<td>TensorFlow IO Contributors [aut, cph] (Full list of contributors can be
    found at &lt;https://github.com/tensorflow/io/graphs/contributors&gt;),
  Yuan Tang <a href="https://orcid.org/0000-0001-5243-233X"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut,
    cre],
  TensorFlow Authors [cph],
  Ant Financial [cph],
  RStudio [cph]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Yuan Tang &lt;terrytangyuan@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2019-12-19 16:50:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='+25+26gt+3B+25'>Pipe operator</h2><span id='topic++25+3E+25'></span>

<h3>Description</h3>

<p>See <code><a href="magrittr.html#topic++25+3E+25">%&gt;%</a></code> for more details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lhs %&gt;% rhs
</code></pre>

<hr>
<h2 id='arrow_feather_dataset'>Creates a <code>ArrowFeatherDataset</code>.</h2><span id='topic+arrow_feather_dataset'></span>

<h3>Description</h3>

<p>An Arrow Dataset for reading record batches from Arrow feather files. Feather
is a light-weight columnar format ideal for simple writing of Pandas
DataFrames.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>arrow_feather_dataset(filenames, columns, output_types, output_shapes = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="arrow_feather_dataset_+3A_filenames">filenames</code></td>
<td>
<p>A <code>tf.string</code> tensor, list or scalar containing files in
Arrow Feather format.</p>
</td></tr>
<tr><td><code id="arrow_feather_dataset_+3A_columns">columns</code></td>
<td>
<p>A list of column indices to be used in the Dataset.</p>
</td></tr>
<tr><td><code id="arrow_feather_dataset_+3A_output_types">output_types</code></td>
<td>
<p>Tensor dtypes of the output tensors.</p>
</td></tr>
<tr><td><code id="arrow_feather_dataset_+3A_output_shapes">output_shapes</code></td>
<td>
<p>TensorShapes of the output tensors or <code>NULL</code> to infer
partial.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
dataset &lt;- arrow_feather_dataset(
    list('/path/to/a.feather', '/path/to/b.feather'),
    columns = reticulate::tuple(0L, 1L),
    output_types = reticulate::tuple(tf$int32, tf$float32),
    output_shapes = reticulate::tuple(list(), list())) %&gt;%
  dataset_repeat(1)

sess &lt;- tf$Session()
iterator &lt;- make_iterator_one_shot(dataset)
next_batch &lt;- iterator_get_next(iterator)

until_out_of_range({
  batch &lt;- sess$run(next_batch)
  print(batch)
})

## End(Not run)

</code></pre>

<hr>
<h2 id='arrow_stream_dataset'>Creates a <code>ArrowStreamDataset</code>.</h2><span id='topic+arrow_stream_dataset'></span>

<h3>Description</h3>

<p>An Arrow Dataset for reading record batches from an input stream. Currently
supported input streams are a socket client or stdin.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>arrow_stream_dataset(host, columns, output_types, output_shapes = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="arrow_stream_dataset_+3A_host">host</code></td>
<td>
<p>A <code>tf.string</code> tensor or string defining the input stream.
For a socket client, use &quot;&lt;HOST_IP&gt;:&lt;PORT&gt;&quot;, for stdin use &quot;STDIN&quot;.</p>
</td></tr>
<tr><td><code id="arrow_stream_dataset_+3A_columns">columns</code></td>
<td>
<p>A list of column indices to be used in the Dataset.</p>
</td></tr>
<tr><td><code id="arrow_stream_dataset_+3A_output_types">output_types</code></td>
<td>
<p>Tensor dtypes of the output tensors.</p>
</td></tr>
<tr><td><code id="arrow_stream_dataset_+3A_output_shapes">output_shapes</code></td>
<td>
<p>TensorShapes of the output tensors or <code>NULL</code> to infer
partial.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
dataset &lt;- arrow_stream_dataset(
    host,
    columns = reticulate::tuple(0L, 1L),
    output_types = reticulate::tuple(tf$int32, tf$float32),
    output_shapes = reticulate::tuple(list(), list())) %&gt;%
  dataset_repeat(1)

sess &lt;- tf$Session()
iterator &lt;- make_iterator_one_shot(dataset)
next_batch &lt;- iterator_get_next(iterator)

until_out_of_range({
  batch &lt;- sess$run(next_batch)
  print(batch)
})

## End(Not run)

</code></pre>

<hr>
<h2 id='from_schema'>Create an Arrow Dataset from the given Arrow schema.</h2><span id='topic+from_schema'></span>

<h3>Description</h3>

<p>Infer output types and shapes from the given Arrow schema and create an Arrow
Dataset.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>from_schema(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="from_schema_+3A_object">object</code></td>
<td>
<p>An <span class="rlang"><b>R</b></span> object.</p>
</td></tr>
<tr><td><code id="from_schema_+3A_...">...</code></td>
<td>
<p>Optional arguments passed on to implementing methods.</p>
</td></tr>
</table>

<hr>
<h2 id='from_schema.arrow_feather_dataset'>Create an Arrow Dataset for reading record batches from Arrow feather files,
inferring output types and shapes from the given Arrow schema.</h2><span id='topic+from_schema.arrow_feather_dataset'></span>

<h3>Description</h3>

<p>Create an Arrow Dataset for reading record batches from Arrow feather files,
inferring output types and shapes from the given Arrow schema.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'arrow_feather_dataset'
from_schema(object, schema, columns = NULL, host = NULL, filenames = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="from_schema.arrow_feather_dataset_+3A_object">object</code></td>
<td>
<p>An <span class="rlang"><b>R</b></span> object.</p>
</td></tr>
<tr><td><code id="from_schema.arrow_feather_dataset_+3A_schema">schema</code></td>
<td>
<p>Arrow schema defining the record batch data in the stream.</p>
</td></tr>
<tr><td><code id="from_schema.arrow_feather_dataset_+3A_columns">columns</code></td>
<td>
<p>A list of column indices to be used in the Dataset.</p>
</td></tr>
<tr><td><code id="from_schema.arrow_feather_dataset_+3A_host">host</code></td>
<td>
<p>Not used.</p>
</td></tr>
<tr><td><code id="from_schema.arrow_feather_dataset_+3A_filenames">filenames</code></td>
<td>
<p>A <code>tf.string</code> tensor, list or scalar containing files in
Arrow Feather format.</p>
</td></tr>
<tr><td><code id="from_schema.arrow_feather_dataset_+3A_...">...</code></td>
<td>
<p>Optional arguments passed on to implementing methods.</p>
</td></tr>
</table>

<hr>
<h2 id='from_schema.arrow_stream_dataset'>Create an Arrow Dataset from an input stream, inferring output types and
shapes from the given Arrow schema.</h2><span id='topic+from_schema.arrow_stream_dataset'></span>

<h3>Description</h3>

<p>Create an Arrow Dataset from an input stream, inferring output types and
shapes from the given Arrow schema.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'arrow_stream_dataset'
from_schema(object, schema, columns = NULL, host = NULL, filenames = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="from_schema.arrow_stream_dataset_+3A_object">object</code></td>
<td>
<p>An <span class="rlang"><b>R</b></span> object.</p>
</td></tr>
<tr><td><code id="from_schema.arrow_stream_dataset_+3A_schema">schema</code></td>
<td>
<p>Arrow schema defining the record batch data in the stream.</p>
</td></tr>
<tr><td><code id="from_schema.arrow_stream_dataset_+3A_columns">columns</code></td>
<td>
<p>A list of column indices to be used in the Dataset.</p>
</td></tr>
<tr><td><code id="from_schema.arrow_stream_dataset_+3A_host">host</code></td>
<td>
<p>A <code>tf.string</code> tensor or string defining the input stream.
For a socket client, use &quot;&lt;HOST_IP&gt;:&lt;PORT&gt;&quot;, for stdin use &quot;STDIN&quot;.</p>
</td></tr>
<tr><td><code id="from_schema.arrow_stream_dataset_+3A_filenames">filenames</code></td>
<td>
<p>Not used.</p>
</td></tr>
<tr><td><code id="from_schema.arrow_stream_dataset_+3A_...">...</code></td>
<td>
<p>Optional arguments passed on to implementing methods.</p>
</td></tr>
</table>

<hr>
<h2 id='ignite_dataset'>Create a <code>IgniteDataset</code>.</h2><span id='topic+ignite_dataset'></span>

<h3>Description</h3>

<p>Apache Ignite is a memory-centric distributed database, caching, and
processing platform for transactional, analytical, and streaming workloads,
delivering in-memory speeds at petabyte scale. This contrib package
contains an integration between Apache Ignite and TensorFlow. The
integration is based on tf.data from TensorFlow side and Binary Client
Protocol from Apache Ignite side. It allows to use Apache Ignite as a
datasource for neural network training, inference and all other
computations supported by TensorFlow. Ignite Dataset is based on Apache
Ignite Binary Client Protocol.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ignite_dataset(
  cache_name,
  host = "localhost",
  port = 10800,
  local = FALSE,
  part = -1,
  page_size = 100,
  username = NULL,
  password = NULL,
  certfile = NULL,
  keyfile = NULL,
  cert_password = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ignite_dataset_+3A_cache_name">cache_name</code></td>
<td>
<p>Cache name to be used as datasource.</p>
</td></tr>
<tr><td><code id="ignite_dataset_+3A_host">host</code></td>
<td>
<p>Apache Ignite Thin Client host to be connected.</p>
</td></tr>
<tr><td><code id="ignite_dataset_+3A_port">port</code></td>
<td>
<p>Apache Ignite Thin Client port to be connected.</p>
</td></tr>
<tr><td><code id="ignite_dataset_+3A_local">local</code></td>
<td>
<p>Local flag that defines to query only local data.</p>
</td></tr>
<tr><td><code id="ignite_dataset_+3A_part">part</code></td>
<td>
<p>Number of partitions to be queried.</p>
</td></tr>
<tr><td><code id="ignite_dataset_+3A_page_size">page_size</code></td>
<td>
<p>Apache Ignite Thin Client page size.</p>
</td></tr>
<tr><td><code id="ignite_dataset_+3A_username">username</code></td>
<td>
<p>Apache Ignite Thin Client authentication username.</p>
</td></tr>
<tr><td><code id="ignite_dataset_+3A_password">password</code></td>
<td>
<p>Apache Ignite Thin Client authentication password.</p>
</td></tr>
<tr><td><code id="ignite_dataset_+3A_certfile">certfile</code></td>
<td>
<p>File in PEM format containing the certificate as well as any
number of CA certificates needed to establish the certificate's
authenticity.</p>
</td></tr>
<tr><td><code id="ignite_dataset_+3A_keyfile">keyfile</code></td>
<td>
<p>File containing the private key (otherwise the private key
will be taken from certfile as well).</p>
</td></tr>
<tr><td><code id="ignite_dataset_+3A_cert_password">cert_password</code></td>
<td>
<p>Password to be used if the private key is encrypted and
a password is necessary.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
dataset &lt;- ignite_dataset(
    cache_name = "SQL_PUBLIC_TEST_CACHE", port = 10800) %&gt;%
  dataset_repeat(1)

sess &lt;- tf$Session()
iterator &lt;- make_iterator_one_shot(dataset)
next_batch &lt;- iterator_get_next(iterator)

until_out_of_range({
  batch &lt;- sess$run(next_batch)
  print(batch)
})

## End(Not run)

</code></pre>

<hr>
<h2 id='kafka_dataset'>Creates a <code>KafkaDataset</code>.</h2><span id='topic+kafka_dataset'></span>

<h3>Description</h3>

<p>Creates a <code>KafkaDataset</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kafka_dataset(
  topics,
  servers = "localhost",
  group = "",
  eof = FALSE,
  timeout = 1000
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kafka_dataset_+3A_topics">topics</code></td>
<td>
<p>A <code>tf.string</code> tensor containing one or more subscriptions, in
the format of <code style="white-space: pre;">&#8288;[topic:partition:offset:length]&#8288;</code>, by default length is -1
for unlimited.</p>
</td></tr>
<tr><td><code id="kafka_dataset_+3A_servers">servers</code></td>
<td>
<p>A list of bootstrap servers.</p>
</td></tr>
<tr><td><code id="kafka_dataset_+3A_group">group</code></td>
<td>
<p>The consumer group id.</p>
</td></tr>
<tr><td><code id="kafka_dataset_+3A_eof">eof</code></td>
<td>
<p>If True, the kafka reader will stop on EOF.</p>
</td></tr>
<tr><td><code id="kafka_dataset_+3A_timeout">timeout</code></td>
<td>
<p>The timeout value for the Kafka Consumer to wait (in
millisecond).</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
dataset &lt;- kafka_dataset(
    topics = list("test:0:0:4"), group = "test", eof = TRUE) %&gt;%
  dataset_repeat(1)

sess &lt;- tf$Session()
iterator &lt;- make_iterator_one_shot(dataset)
next_batch &lt;- iterator_get_next(iterator)

until_out_of_range({
  batch &lt;- sess$run(next_batch)
  print(batch)
})

## End(Not run)

</code></pre>

<hr>
<h2 id='kinesis_dataset'>Creates a <code>KinesisDataset</code>.</h2><span id='topic+kinesis_dataset'></span>

<h3>Description</h3>

<p>Kinesis is a managed service provided by AWS for data streaming.
This dataset reads messages from Kinesis with each message presented
as a <code>tf.string</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kinesis_dataset(stream, shard = "", read_indefinitely = TRUE, interval = 1e+05)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kinesis_dataset_+3A_stream">stream</code></td>
<td>
<p>A <code>tf.string</code> tensor containing the name of the stream.</p>
</td></tr>
<tr><td><code id="kinesis_dataset_+3A_shard">shard</code></td>
<td>
<p>A <code>tf.string</code> tensor containing the id of the shard.</p>
</td></tr>
<tr><td><code id="kinesis_dataset_+3A_read_indefinitely">read_indefinitely</code></td>
<td>
<p>If <code>True</code>, the Kinesis dataset will keep retry again
on <code>EOF</code> after the <code>interval</code> period. If <code>False</code>, then the dataset will
stop on <code>EOF</code>. The default value is <code>True</code>.</p>
</td></tr>
<tr><td><code id="kinesis_dataset_+3A_interval">interval</code></td>
<td>
<p>The interval for the Kinesis Client to wait before it tries
to get records again (in millisecond).</p>
</td></tr>
</table>

<hr>
<h2 id='lmdb_dataset'>Create a <code>LMDBDataset</code>.</h2><span id='topic+lmdb_dataset'></span>

<h3>Description</h3>

<p>This function allows a user to read data from a LMDB
file. A lmdb file consists of (key value) pairs sequentially.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lmdb_dataset(filenames)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lmdb_dataset_+3A_filenames">filenames</code></td>
<td>
<p>A <code>tf.string</code> tensor containing one or more filenames.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
dataset &lt;- sequence_file_dataset("testdata/data.mdb") %&gt;%
  dataset_repeat(1)

sess &lt;- tf$Session()
iterator &lt;- make_iterator_one_shot(dataset)
next_batch &lt;- iterator_get_next(iterator)

until_out_of_range({
  batch &lt;- sess$run(next_batch)
  print(batch)
})

## End(Not run)

</code></pre>

<hr>
<h2 id='make_libsvm_dataset'>Create a Dataset from LibSVM files.</h2><span id='topic+make_libsvm_dataset'></span>

<h3>Description</h3>

<p>Create a Dataset from LibSVM files.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>make_libsvm_dataset(
  file_names,
  num_features,
  dtype = NULL,
  label_dtype = NULL,
  batch_size = 1,
  compression_type = "",
  buffer_size = NULL,
  num_parallel_parser_calls = NULL,
  drop_final_batch = FALSE,
  prefetch_buffer_size = 0
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="make_libsvm_dataset_+3A_file_names">file_names</code></td>
<td>
<p>A <code>tf.string</code> tensor containing one or more filenames.</p>
</td></tr>
<tr><td><code id="make_libsvm_dataset_+3A_num_features">num_features</code></td>
<td>
<p>The number of features.</p>
</td></tr>
<tr><td><code id="make_libsvm_dataset_+3A_dtype">dtype</code></td>
<td>
<p>The type of the output feature tensor. Default to <code>tf.float32</code>.</p>
</td></tr>
<tr><td><code id="make_libsvm_dataset_+3A_label_dtype">label_dtype</code></td>
<td>
<p>The type of the output label tensor. Default to
<code>tf.int64</code>.</p>
</td></tr>
<tr><td><code id="make_libsvm_dataset_+3A_batch_size">batch_size</code></td>
<td>
<p>An integer representing the number of records to combine in
a single batch, default 1.</p>
</td></tr>
<tr><td><code id="make_libsvm_dataset_+3A_compression_type">compression_type</code></td>
<td>
<p>A <code>tf.string</code> scalar evaluating to one of <code>""</code> (no
compression), <code>"ZLIB"</code>, or <code>"GZIP"</code>.</p>
</td></tr>
<tr><td><code id="make_libsvm_dataset_+3A_buffer_size">buffer_size</code></td>
<td>
<p>A <code>tf.int64</code> scalar denoting the number of bytes to
buffer. A value of 0 results in the default buffering values chosen based
on the compression type.</p>
</td></tr>
<tr><td><code id="make_libsvm_dataset_+3A_num_parallel_parser_calls">num_parallel_parser_calls</code></td>
<td>
<p>Number of parallel records to parse in
parallel. Defaults to an automatic selection.</p>
</td></tr>
<tr><td><code id="make_libsvm_dataset_+3A_drop_final_batch">drop_final_batch</code></td>
<td>
<p>Whether the last batch should be dropped in case its
size is smaller than <code>batch_size</code>; the default behavior is not to drop the
smaller batch.</p>
</td></tr>
<tr><td><code id="make_libsvm_dataset_+3A_prefetch_buffer_size">prefetch_buffer_size</code></td>
<td>
<p>An integer specifying the number of feature
batches to prefetch for performance improvement. Defaults to auto-tune. Set
to 0 to disable prefetching.</p>
</td></tr>
</table>

<hr>
<h2 id='mnist_image_dataset'>Creates a <code>MNISTImageDataset</code>.</h2><span id='topic+mnist_image_dataset'></span>

<h3>Description</h3>

<p>This creates a dataset for MNIST images.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mnist_image_dataset(filenames, compression_type = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mnist_image_dataset_+3A_filenames">filenames</code></td>
<td>
<p>A <code>tf.string</code> tensor containing one or more filenames.</p>
</td></tr>
<tr><td><code id="mnist_image_dataset_+3A_compression_type">compression_type</code></td>
<td>
<p>A <code>tf.string</code> scalar evaluating to one
of <code>""</code> (no compression), <code>"ZLIB"</code>, or <code>"GZIP"</code>.</p>
</td></tr>
</table>

<hr>
<h2 id='mnist_label_dataset'>Creates a <code>MNISTLabelDataset</code>.</h2><span id='topic+mnist_label_dataset'></span>

<h3>Description</h3>

<p>This creates a dataset for MNIST labels.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mnist_label_dataset(filenames, compression_type = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mnist_label_dataset_+3A_filenames">filenames</code></td>
<td>
<p>A <code>tf.string</code> tensor containing one or more filenames.</p>
</td></tr>
<tr><td><code id="mnist_label_dataset_+3A_compression_type">compression_type</code></td>
<td>
<p>A <code>tf.string</code> scalar evaluating to one
of <code>""</code> (no compression), <code>"ZLIB"</code>, or <code>"GZIP"</code>.</p>
</td></tr>
</table>

<hr>
<h2 id='parquet_dataset'>Create a <code>ParquetDataset</code>.</h2><span id='topic+parquet_dataset'></span>

<h3>Description</h3>

<p>This allows a user to read data from a parquet file.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>parquet_dataset(filenames, columns, output_types)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="parquet_dataset_+3A_filenames">filenames</code></td>
<td>
<p>A 0-D or 1-D <code>tf.string</code> tensor containing one or more
filenames.</p>
</td></tr>
<tr><td><code id="parquet_dataset_+3A_columns">columns</code></td>
<td>
<p>A 0-D or 1-D <code>tf.int32</code> tensor containing the columns to
extract.</p>
</td></tr>
<tr><td><code id="parquet_dataset_+3A_output_types">output_types</code></td>
<td>
<p>A tuple of <code>tf.DType</code> objects representing the types of
the columns returned.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
dtypes &lt;- tf$python$framework$dtypes
output_types &lt;- reticulate::tuple(
  dtypes$bool, dtypes$int32, dtypes$int64, dtypes$float32, dtypes$float64)
dataset &lt;- parquet_dataset(
    filenames = list("testdata/parquet_cpp_example.parquet"),
    columns = list(0, 1, 2, 4, 5),
    output_types = output_types) %&gt;%
  dataset_repeat(2)

sess &lt;- tf$Session()
iterator &lt;- make_iterator_one_shot(dataset)
next_batch &lt;- iterator_get_next(iterator)

until_out_of_range({
  batch &lt;- sess$run(next_batch)
  print(batch)
})

## End(Not run)

</code></pre>

<hr>
<h2 id='pubsub_dataset'>Creates a <code>PubSubDataset</code>.</h2><span id='topic+pubsub_dataset'></span>

<h3>Description</h3>

<p>This creates a dataset for consuming PubSub messages.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pubsub_dataset(subscriptions, server = NULL, eof = FALSE, timeout = 1000)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pubsub_dataset_+3A_subscriptions">subscriptions</code></td>
<td>
<p>A <code>tf.string</code> tensor containing one or more
subscriptions.</p>
</td></tr>
<tr><td><code id="pubsub_dataset_+3A_server">server</code></td>
<td>
<p>The pubsub server.</p>
</td></tr>
<tr><td><code id="pubsub_dataset_+3A_eof">eof</code></td>
<td>
<p>If True, the pubsub reader will stop on EOF.</p>
</td></tr>
<tr><td><code id="pubsub_dataset_+3A_timeout">timeout</code></td>
<td>
<p>The timeout value for the PubSub to wait (in millisecond).</p>
</td></tr>
</table>

<hr>
<h2 id='reexports'>Objects exported from other packages</h2><span id='topic+reexports'></span><span id='topic+tf'></span><span id='topic+install_tensorflow'></span><span id='topic+tf_config'></span><span id='topic+tf_version'></span><span id='topic+next_batch'></span><span id='topic+with_dataset'></span><span id='topic+until_out_of_range'></span><span id='topic+make_iterator_one_shot'></span><span id='topic+make_iterator_initializable'></span><span id='topic+make_iterator_from_structure'></span><span id='topic+make_iterator_from_string_handle'></span><span id='topic+iterator_get_next'></span><span id='topic+iterator_initializer'></span><span id='topic+iterator_string_handle'></span><span id='topic+iterator_make_initializer'></span><span id='topic+out_of_range_handler'></span><span id='topic+dataset_repeat'></span><span id='topic+dataset_shuffle'></span><span id='topic+dataset_shuffle_and_repeat'></span><span id='topic+dataset_batch'></span><span id='topic+dataset_cache'></span><span id='topic+dataset_concatenate'></span><span id='topic+dataset_take'></span><span id='topic+dataset_map'></span><span id='topic+dataset_map_and_batch'></span><span id='topic+dataset_flat_map'></span><span id='topic+dataset_prefetch'></span><span id='topic+dataset_prefetch_to_device'></span><span id='topic+dataset_filter'></span><span id='topic+dataset_skip'></span><span id='topic+dataset_interleave'></span><span id='topic+dataset_shard'></span><span id='topic+dataset_padded_batch'></span><span id='topic+dataset_prepare'></span><span id='topic+output_types'></span>

<h3>Description</h3>

<p>These objects are imported from other packages. Follow the links
below to see their documentation.
</p>

<dl>
<dt>tensorflow</dt><dd><p><code><a href="tensorflow.html#topic+install_tensorflow">install_tensorflow</a></code>, <code><a href="tensorflow.html#topic+tf">tf</a></code>, <code><a href="tensorflow.html#topic+tf_config">tf_config</a></code>, <code><a href="tensorflow.html#topic+tf_version">tf_version</a></code></p>
</dd>
<dt>tfdatasets</dt><dd><p><code><a href="tfdatasets.html#topic+dataset_batch">dataset_batch</a></code>, <code><a href="tfdatasets.html#topic+dataset_cache">dataset_cache</a></code>, <code><a href="tfdatasets.html#topic+dataset_concatenate">dataset_concatenate</a></code>, <code><a href="tfdatasets.html#topic+dataset_filter">dataset_filter</a></code>, <code><a href="tfdatasets.html#topic+dataset_flat_map">dataset_flat_map</a></code>, <code><a href="tfdatasets.html#topic+dataset_interleave">dataset_interleave</a></code>, <code><a href="tfdatasets.html#topic+dataset_map">dataset_map</a></code>, <code><a href="tfdatasets.html#topic+dataset_map_and_batch">dataset_map_and_batch</a></code>, <code><a href="tfdatasets.html#topic+dataset_padded_batch">dataset_padded_batch</a></code>, <code><a href="tfdatasets.html#topic+dataset_prefetch">dataset_prefetch</a></code>, <code><a href="tfdatasets.html#topic+dataset_prefetch">dataset_prefetch</a></code>, <code><a href="tfdatasets.html#topic+dataset_prefetch_to_device">dataset_prefetch_to_device</a></code>, <code><a href="tfdatasets.html#topic+dataset_prepare">dataset_prepare</a></code>, <code><a href="tfdatasets.html#topic+dataset_repeat">dataset_repeat</a></code>, <code><a href="tfdatasets.html#topic+dataset_shard">dataset_shard</a></code>, <code><a href="tfdatasets.html#topic+dataset_shuffle">dataset_shuffle</a></code>, <code><a href="tfdatasets.html#topic+dataset_shuffle_and_repeat">dataset_shuffle_and_repeat</a></code>, <code><a href="tfdatasets.html#topic+dataset_skip">dataset_skip</a></code>, <code><a href="tfdatasets.html#topic+dataset_take">dataset_take</a></code>, <code><a href="tfdatasets.html#topic+iterator_get_next">iterator_get_next</a></code>, <code><a href="tfdatasets.html#topic+iterator_initializer">iterator_initializer</a></code>, <code><a href="tfdatasets.html#topic+iterator_make_initializer">iterator_make_initializer</a></code>, <code><a href="tfdatasets.html#topic+iterator_string_handle">iterator_string_handle</a></code>, <code><a href="tfdatasets.html#topic+make_iterator_from_string_handle">make_iterator_from_string_handle</a></code>, <code><a href="tfdatasets.html#topic+make_iterator_from_structure">make_iterator_from_structure</a></code>, <code><a href="tfdatasets.html#topic+make_iterator_initializable">make_iterator_initializable</a></code>, <code><a href="tfdatasets.html#topic+make_iterator_one_shot">make_iterator_one_shot</a></code>, <code><a href="tfdatasets.html#topic+next_batch">next_batch</a></code>, <code><a href="tfdatasets.html#topic+out_of_range_handler">out_of_range_handler</a></code>, <code><a href="tfdatasets.html#topic+output_types">output_types</a></code>, <code><a href="tfdatasets.html#topic+output_types">output_types</a></code>, <code><a href="tfdatasets.html#topic+until_out_of_range">until_out_of_range</a></code>, <code><a href="tfdatasets.html#topic+with_dataset">with_dataset</a></code></p>
</dd>
</dl>

<hr>
<h2 id='sequence_file_dataset'>Create a <code>SequenceFileDataset</code>.</h2><span id='topic+sequence_file_dataset'></span>

<h3>Description</h3>

<p>This function allows a user to read data from a hadoop sequence
file. A sequence file consists of (key value) pairs sequentially. At
the moment, <code>org.apache.hadoop.io.Text</code> is the only serialization type
being supported, and there is no compression support.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sequence_file_dataset(filenames)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sequence_file_dataset_+3A_filenames">filenames</code></td>
<td>
<p>A <code>tf.string</code> tensor containing one or more filenames.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
dataset &lt;- sequence_file_dataset("testdata/string.seq") %&gt;%
  dataset_repeat(1)

sess &lt;- tf$Session()
iterator &lt;- make_iterator_one_shot(dataset)
next_batch &lt;- iterator_get_next(iterator)

until_out_of_range({
  batch &lt;- sess$run(next_batch)
  print(batch)
})

## End(Not run)

</code></pre>

<hr>
<h2 id='tfio'>TensorFlow IO API for R</h2><span id='topic+tfio'></span>

<h3>Description</h3>

<p>This library provides an R interface to the
<a href="https://github.com/tensorflow/io">TensorFlow IO</a> API
that provides datasets and filesystem extensions maintained by SIG-IO.
</p>

<hr>
<h2 id='tiff_dataset'>Create a <code>TIFFDataset</code>.</h2><span id='topic+tiff_dataset'></span>

<h3>Description</h3>

<p>A TIFF Image File Dataset that reads the TIFF file.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tiff_dataset(filenames)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tiff_dataset_+3A_filenames">filenames</code></td>
<td>
<p>A <code>tf.string</code> tensor containing one or more filenames.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
dataset &lt;- tiff_dataset(
    filenames = list("testdata/small.tiff")) %&gt;%
  dataset_repeat(1)

sess &lt;- tf$Session()
iterator &lt;- make_iterator_one_shot(dataset)
next_batch &lt;- iterator_get_next(iterator)

until_out_of_range({
  batch &lt;- sess$run(next_batch)
  print(batch)
})

## End(Not run)

</code></pre>

<hr>
<h2 id='video_dataset'>Create a <code>VideoDataset</code> that reads the video file.</h2><span id='topic+video_dataset'></span>

<h3>Description</h3>

<p>This allows a user to read data from a video file with ffmpeg. The output of
VideoDataset is a sequence of (height, weight, 3) tensor in rgb24 format.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>video_dataset(filenames)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="video_dataset_+3A_filenames">filenames</code></td>
<td>
<p>A <code>tf.string</code> tensor containing one or more filenames.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
dataset &lt;- video_dataset(
    filenames = list("testdata/small.mp4")) %&gt;%
  dataset_repeat(2)

sess &lt;- tf$Session()
iterator &lt;- make_iterator_one_shot(dataset)
next_batch &lt;- iterator_get_next(iterator)

until_out_of_range({
  batch &lt;- sess$run(next_batch)
  print(batch)
})

## End(Not run)

</code></pre>

<hr>
<h2 id='webp_dataset'>Create a <code>WebPDataset</code>.</h2><span id='topic+webp_dataset'></span>

<h3>Description</h3>

<p>A WebP Image File Dataset that reads the WebP file.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>webp_dataset(filenames)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="webp_dataset_+3A_filenames">filenames</code></td>
<td>
<p>A <code>tf.string</code> tensor containing one or more filenames.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
dataset &lt;- webp_dataset(
    filenames = list("testdata/sample.webp")) %&gt;%
  dataset_repeat(1)

sess &lt;- tf$Session()
iterator &lt;- make_iterator_one_shot(dataset)
next_batch &lt;- iterator_get_next(iterator)

until_out_of_range({
  batch &lt;- sess$run(next_batch)
  print(batch)
})

## End(Not run)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
