<!DOCTYPE html><html><head><title>Help for package akc</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {akc}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#akc-package'><p>akc: Automatic Knowledge Classification</p></a></li>
<li><a href='#bibli_data_table'><p>A selected dataset of bibliometric data on the topic of &quot;Library science&quot;</p></a></li>
<li><a href='#doc_group'><p>Construct network of documents based on keyword co-occurrence</p></a></li>
<li><a href='#keyword_clean'><p>Automatic keyword cleaning and transfer to tidy format</p></a></li>
<li><a href='#keyword_cloud'><p>Draw word cloud for grouped keywords</p></a></li>
<li><a href='#keyword_extract'><p>Extract keywords from raw text</p></a></li>
<li><a href='#keyword_group'><p>Construct network from a tidy table and divide them into groups</p></a></li>
<li><a href='#keyword_merge'><p>Merge keywords that supposed to have same meanings</p></a></li>
<li><a href='#keyword_network'><p>Flexiable visualization of network (alternative to 'keyword_vis')</p></a></li>
<li><a href='#keyword_table'><p>Display the table with different groups of keywords</p></a></li>
<li><a href='#keyword_vis'><p>Visualization of grouped keyword co-occurrence network</p></a></li>
<li><a href='#make_dict'><p>Making one's own dictionary</p></a></li>
<li><a href='#reexports'><p>Objects exported from other packages</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Automatic Knowledge Classification</td>
</tr>
<tr>
<td>Version:</td>
<td>0.9.9</td>
</tr>
<tr>
<td>Description:</td>
<td>A tidy framework for automatic knowledge classification and visualization. Currently, the core functionality of the framework is mainly supported by modularity-based clustering (community detection) in keyword co-occurrence network, and focuses on co-word analysis of bibliometric research. However, the designed functions in 'akc' are general, and could be extended to solve other tasks in text mining as well.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.0.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>igraph, dplyr, ggplot2, stringr, ggraph (&ge; 1.0.2), tidygraph
(&ge; 1.1.2), ggforce, textstem, tibble, tidytext, rlang,
magrittr, data.table (&ge; 1.13.0), ggwordcloud (&ge; 0.5.0),
tidyfst</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/hope-data-science/akc">https://github.com/hope-data-science/akc</a></td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.1</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat (&ge; 3.0.0), knitr, rmarkdown</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-01-06 07:41:38 UTC; DELL</td>
</tr>
<tr>
<td>Author:</td>
<td>Tian-Yuan Huang <a href="https://orcid.org/0000-0002-4151-3764"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Tian-Yuan Huang &lt;huang.tian-yuan@qq.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-01-06 08:00:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='akc-package'>akc: Automatic Knowledge Classification</h2><span id='topic+akc'></span><span id='topic+akc-package'></span>

<h3>Description</h3>

<p><img src="../help/figures/logo.png" style='float: right' alt='logo' width='120' />
</p>
<p>A tidy framework for automatic knowledge classification and visualization. Currently, the core functionality of the framework is mainly supported by modularity-based clustering (community detection) in keyword co-occurrence network, and focuses on co-word analysis of bibliometric research. However, the designed functions in 'akc' are general, and could be extended to solve other tasks in text mining as well.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Tian-Yuan Huang <a href="mailto:huang.tian-yuan@qq.com">huang.tian-yuan@qq.com</a> (<a href="https://orcid.org/0000-0002-4151-3764">ORCID</a>)
</p>


<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://github.com/hope-data-science/akc">https://github.com/hope-data-science/akc</a>
</p>
</li></ul>


<hr>
<h2 id='bibli_data_table'>A selected dataset of bibliometric data on the topic of &quot;Library science&quot;</h2><span id='topic+bibli_data_table'></span>

<h3>Description</h3>

<p>A selected sample of bibliometric data about topics on &quot;Library science&quot;.
</p>
<p>Period: 2019
</p>
<p>Database: Web of Science Core Collection
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bibli_data_table
</code></pre>


<h3>Format</h3>

<p>A data frame with 1448 rows and 4 variables:
</p>

<dl>
<dt>id</dt><dd><p>Unique article identifier for each article</p>
</dd>
<dt>title</dt><dd><p>Title of the article</p>
</dd>
<dt>keyword</dt><dd><p>Keyword list of the article</p>
</dd>
<dt>abstract</dt><dd><p>Abstract of the article</p>
</dd>
</dl>


<hr>
<h2 id='doc_group'>Construct network of documents based on keyword co-occurrence</h2><span id='topic+doc_group'></span>

<h3>Description</h3>

<p>Create a <code>tbl_graph</code>(a class provided by <span class="pkg">tidygraph</span>) from the tidy table with document ID and keyword.
Each entry(row) should contain only one document and keyword in the tidy format.This function would
group the documents.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>doc_group(
  dt,
  id = "id",
  keyword = "keyword",
  com_detect_fun = group_fast_greedy
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="doc_group_+3A_dt">dt</code></td>
<td>
<p>A data.frame containing at least two columns with document ID and keyword.</p>
</td></tr>
<tr><td><code id="doc_group_+3A_id">id</code></td>
<td>
<p>Quoted characters specifying the column name of document ID.Default uses &quot;id&quot;.</p>
</td></tr>
<tr><td><code id="doc_group_+3A_keyword">keyword</code></td>
<td>
<p>Quoted characters specifying the column name of keyword.Default uses &quot;keyword&quot;.</p>
</td></tr>
<tr><td><code id="doc_group_+3A_com_detect_fun">com_detect_fun</code></td>
<td>
<p>Community detection function,provided by <span class="pkg">tidygraph</span>(wrappers around clustering
functions provided by <span class="pkg">igraph</span>), see <code><a href="tidygraph.html#topic+group_graph">group_graph</a></code> to find other optional algorithms.
Default uses <code><a href="tidygraph.html#topic+group_fast_greedy">group_fast_greedy</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>As we could classify keywords using document ID, we could also
classify documents with keywords. In the output network, the nodes are documents
and the edges mean the two documents share same keywords with each other.
</p>


<h3>Value</h3>

<p>A tbl_graph, representing the document relation network based on
keyword co-occurrence.
</p>


<h3>Examples</h3>

<pre><code class='language-R'> library(akc)
 bibli_data_table %&gt;%
   keyword_clean(id = "id",keyword = "keyword") %&gt;%
   doc_group(id = "id",keyword = "keyword") -&gt; grouped_doc

 grouped_doc
</code></pre>

<hr>
<h2 id='keyword_clean'>Automatic keyword cleaning and transfer to tidy format</h2><span id='topic+keyword_clean'></span>

<h3>Description</h3>

<p>Carry out several keyword cleaning processes automatically and return a tidy table with
document ID and keywords.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>keyword_clean(
  df,
  id = "id",
  keyword = "keyword",
  sep = ";",
  rmParentheses = TRUE,
  rmNumber = TRUE,
  lemmatize = FALSE,
  lemmatize_dict = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="keyword_clean_+3A_df">df</code></td>
<td>
<p>A data.frame containing at least two columns with document ID and keyword strings with separators.</p>
</td></tr>
<tr><td><code id="keyword_clean_+3A_id">id</code></td>
<td>
<p>Quoted characters specifying the column name of document ID.Default uses &quot;id&quot;.</p>
</td></tr>
<tr><td><code id="keyword_clean_+3A_keyword">keyword</code></td>
<td>
<p>Quoted characters specifying the column name of keywords.Default uses &quot;keyword&quot;.</p>
</td></tr>
<tr><td><code id="keyword_clean_+3A_sep">sep</code></td>
<td>
<p>Separator(s) of keywords. Default uses &quot;;&quot;.</p>
</td></tr>
<tr><td><code id="keyword_clean_+3A_rmparentheses">rmParentheses</code></td>
<td>
<p>Remove the contents in the parentheses (including the parentheses) or not. Default
uses TRUE.</p>
</td></tr>
<tr><td><code id="keyword_clean_+3A_rmnumber">rmNumber</code></td>
<td>
<p>Remove the pure number sequence or no. Default uses TRUE.</p>
</td></tr>
<tr><td><code id="keyword_clean_+3A_lemmatize">lemmatize</code></td>
<td>
<p>Lemmatize the keywords or not. Lemmatization is supported by 'lemmatize_strings' function
in 'textstem' package.Default uses FALSE.</p>
</td></tr>
<tr><td><code id="keyword_clean_+3A_lemmatize_dict">lemmatize_dict</code></td>
<td>
<p>A dictionary of base terms and lemmas to use for replacement.
Only used when the <b>lemmatize</b> parameter is <code>TRUE</code>.
The first column should be the full word form in lower case
while the second column is the corresponding replacement lemma.
Default uses <code>NULL</code>, this would apply the default dictionary used in
<code><a href="textstem.html#topic+lemmatize_strings">lemmatize_strings</a></code> function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The entire cleaning processes include:
1.Split the text with separators;
2.Remove the contents in the parentheses (including the parentheses);
3.Remove white spaces from start and end of string and reduces repeated white spaces inside a string;
4.Remove all the null character string and pure number sequences;
5.Convert all letters to lower case;
6.Lemmatization.
Some of the procedures could be suppressed or activated with parameter adjustments.
Default setting did not use lemmatization, it is suggested to use <code><a href="#topic+keyword_merge">keyword_merge</a></code> to
merge the keywords afterward.
</p>


<h3>Value</h3>

<p>A tbl with two columns, namely document ID and cleaned keywords.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+keyword_merge">keyword_merge</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(akc)

bibli_data_table

bibli_data_table %&gt;%
  keyword_clean(id = "id",keyword = "keyword")
</code></pre>

<hr>
<h2 id='keyword_cloud'>Draw word cloud for grouped keywords</h2><span id='topic+keyword_cloud'></span>

<h3>Description</h3>

<p>This function should be used to plot the object exported by
<code><a href="#topic+keyword_group">keyword_group</a></code>. It could draw a robust word cloud of keywords.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>keyword_cloud(tibble_graph, group_no = NULL, top = 50, max_size = 20)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="keyword_cloud_+3A_tibble_graph">tibble_graph</code></td>
<td>
<p>A <code>tbl_graph</code> output by <code><a href="#topic+keyword_group">keyword_group</a></code>.</p>
</td></tr>
<tr><td><code id="keyword_cloud_+3A_group_no">group_no</code></td>
<td>
<p>If one wants to visualize a specific group, gives the group number.
Default uses <code>NULL</code>,which returns all the groups.</p>
</td></tr>
<tr><td><code id="keyword_cloud_+3A_top">top</code></td>
<td>
<p>How many top keywords (by frequency) should be plot? Default uses 50.</p>
</td></tr>
<tr><td><code id="keyword_cloud_+3A_max_size">max_size</code></td>
<td>
<p>Size of largest keyword.Default uses 20.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In the output graph, the size of keywords is proportional to the keyword
frequency, keywords in different colours belong to different group. For advanced
usage of word cloud, use <span class="pkg">ggwordcloud</span> directly with the grouped keywords
yielded by <code><a href="#topic+keyword_group">keyword_group</a></code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+keyword_group">keyword_group</a></code>,
<code><a href="ggwordcloud.html#topic+geom_text_wordcloud_area">geom_text_wordcloud_area</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(dplyr)
library(akc)


  bibli_data_table %&gt;%
    keyword_clean(id = "id",keyword = "keyword") %&gt;%
    keyword_group(id = "id",keyword = "keyword") -&gt; grouped_keyword

  grouped_keyword %&gt;%
    keyword_cloud()

  grouped_keyword %&gt;%
    keyword_cloud(group_no = 1)

</code></pre>

<hr>
<h2 id='keyword_extract'>Extract keywords from raw text</h2><span id='topic+keyword_extract'></span>

<h3>Description</h3>

<p>When we have raw text like abstract or article but not keywords, we might prefer extracting
keywords first. The least prerequisite data to be provided are a data.frame with document id and raw text,
and a user defined dictionary should be provided. One could use <code><a href="#topic+make_dict">make_dict</a></code> function to construct his(her)
own dictionary with a character vector containing the vocabularies. If the dictionary is not provided,
the function would return all the ngram tokens without filtering (not recommended).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>keyword_extract(
  dt,
  id = "id",
  text,
  dict = NULL,
  stopword = NULL,
  n_max = 4,
  n_min = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="keyword_extract_+3A_dt">dt</code></td>
<td>
<p>A data.frame containing at least two columns with document ID and text strings for extraction.</p>
</td></tr>
<tr><td><code id="keyword_extract_+3A_id">id</code></td>
<td>
<p>Quoted characters specifying the column name of document ID.Default uses &quot;id&quot;.</p>
</td></tr>
<tr><td><code id="keyword_extract_+3A_text">text</code></td>
<td>
<p>Quoted characters specifying the column name of raw text for extraction.</p>
</td></tr>
<tr><td><code id="keyword_extract_+3A_dict">dict</code></td>
<td>
<p>A data.table with two columns,namely &quot;id&quot; and &quot;keyword&quot;(set as key).
This should be exported by <code><a href="#topic+make_dict">make_dict</a></code> function. The default uses <code>NULL</code>,
which means the output keywords are not filtered by the dictionary (usually not recommended).</p>
</td></tr>
<tr><td><code id="keyword_extract_+3A_stopword">stopword</code></td>
<td>
<p>A vector containing the stop words to be used. Default uses <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="keyword_extract_+3A_n_max">n_max</code></td>
<td>
<p>The number of words in the n-gram. This must be an integer greater than or equal to 1.
Default uses 4.</p>
</td></tr>
<tr><td><code id="keyword_extract_+3A_n_min">n_min</code></td>
<td>
<p>This must be an integer greater than or equal to 1, and less than or equal to n_max.
Default uses 1.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In the procedure of keyword extraction from <span class="pkg">akc</span>,first the raw text would be split
into independent clause (namely split by puctuations of <code>[,;!?.]</code>). Then the ngrams of the
clauses would be extracted. Finally, the phrases represented by ngrams should be in the dictionary
created by the user (using <code>make_dict</code>).The user could also specify the <em>n</em> of ngrams.
</p>
<p>This function could take some time if the sample size is large, it is suggested to use system.time to do
some test first. Nonetheless, it has been optimized by data.table codes already and has good performance for big data.
</p>


<h3>Value</h3>

<p>A data.frame(tibble) with two columns, namely document ID and extracted keyword.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+make_dict">make_dict</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
 library(akc)
 library(dplyr)

  bibli_data_table %&gt;%
    keyword_clean(id = "id",keyword = "keyword") %&gt;%
    pull(keyword) %&gt;%
    make_dict -&gt; my_dict

  tidytext::stop_words %&gt;%
    pull(word) %&gt;%
    unique() -&gt; my_stopword

 
  bibli_data_table %&gt;%
    keyword_extract(id = "id",text = "abstract",
    dict = my_dict,stopword = my_stopword)
 
</code></pre>

<hr>
<h2 id='keyword_group'>Construct network from a tidy table and divide them into groups</h2><span id='topic+keyword_group'></span>

<h3>Description</h3>

<p>Create a <code>tbl_graph</code>(a class provided by <span class="pkg">tidygraph</span>) from the tidy table with document ID and keyword.
Each entry(row) should contain only one keyword in the tidy format.This function would automatically computes
the frequency and classification group number of nodes representing keywords.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>keyword_group(
  dt,
  id = "id",
  keyword = "keyword",
  top = 200,
  min_freq = 1,
  com_detect_fun = group_fast_greedy
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="keyword_group_+3A_dt">dt</code></td>
<td>
<p>A data.frame containing at least two columns with document ID and keyword.</p>
</td></tr>
<tr><td><code id="keyword_group_+3A_id">id</code></td>
<td>
<p>Quoted characters specifying the column name of document ID.Default uses &quot;id&quot;.</p>
</td></tr>
<tr><td><code id="keyword_group_+3A_keyword">keyword</code></td>
<td>
<p>Quoted characters specifying the column name of keyword.Default uses &quot;keyword&quot;.</p>
</td></tr>
<tr><td><code id="keyword_group_+3A_top">top</code></td>
<td>
<p>The number of keywords selected with the largest frequency.
If there is a tie,more than <em>top</em> entries would be selected.</p>
</td></tr>
<tr><td><code id="keyword_group_+3A_min_freq">min_freq</code></td>
<td>
<p>Minimum occurrence of selected keywords.Default uses 1.</p>
</td></tr>
<tr><td><code id="keyword_group_+3A_com_detect_fun">com_detect_fun</code></td>
<td>
<p>Community detection function,provided by <span class="pkg">tidygraph</span>(wrappers around clustering
functions provided by <span class="pkg">igraph</span>), see <code><a href="tidygraph.html#topic+group_graph">group_graph</a></code> to find other optional algorithms.
Default uses <code><a href="tidygraph.html#topic+group_fast_greedy">group_fast_greedy</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function receives a tidy table with document ID and keyword.Only top keywords with
largest frequency would be selected and the minimum occurrence of keywords could be specified.
For suggestions of community detection algorithm, see the references provided below.
</p>


<h3>Value</h3>

<p>A tbl_graph, representing the keyword co-occurence network with frequency and group
number of the keywords.
</p>


<h3>References</h3>

<p>de Sousa, Fabiano Berardo, and Liang Zhao. &quot;Evaluating and comparing the igraph community detection algorithms.&quot; 2014 Brazilian Conference on Intelligent Systems. IEEE, 2014.
</p>
<p>Yang, Z., Algesheimer, R., &amp; Tessone, C. J. (2016). A comparative analysis of community detection algorithms on artificial networks. Scientific reports, 6, 30750.
</p>


<h3>See Also</h3>

<p><code><a href="tidygraph.html#topic+tbl_graph">tbl_graph</a></code>, <code><a href="tidygraph.html#topic+group_graph">group_graph</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(akc)

bibli_data_table %&gt;%
  keyword_clean(id = "id",keyword = "keyword") %&gt;%
  keyword_group(id = "id",keyword = "keyword")

# use 'louvain' algorithm for community detection

bibli_data_table %&gt;%
  keyword_clean(id = "id",keyword = "keyword") %&gt;%
  keyword_group(id = "id",keyword = "keyword",
  com_detect_fun = group_louvain)

# get more alternatives by searching '?tidygraph::group_graph'

</code></pre>

<hr>
<h2 id='keyword_merge'>Merge keywords that supposed to have same meanings</h2><span id='topic+keyword_merge'></span>

<h3>Description</h3>

<p>Merge keywords that have common stem or lemma, and return the majority form of the word. This function
recieves a tidy table (data.frame) with document ID and keyword waiting to be merged.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>keyword_merge(
  dt,
  id = "id",
  keyword = "keyword",
  reduce_form = "lemma",
  lemmatize_dict = NULL,
  stem_lang = "porter"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="keyword_merge_+3A_dt">dt</code></td>
<td>
<p>A data.frame containing at least two columns with document ID and keyword.</p>
</td></tr>
<tr><td><code id="keyword_merge_+3A_id">id</code></td>
<td>
<p>Quoted characters specifying the column name of document ID.Default uses &quot;id&quot;.</p>
</td></tr>
<tr><td><code id="keyword_merge_+3A_keyword">keyword</code></td>
<td>
<p>Quoted characters specifying the column name of keyword.Default uses &quot;keyword&quot;.</p>
</td></tr>
<tr><td><code id="keyword_merge_+3A_reduce_form">reduce_form</code></td>
<td>
<p>Merge keywords with the same stem(&quot;stem&quot;) or lemma(&quot;lemma&quot;). See details.
Default uses &quot;lemma&quot;. Another advanced option is &quot;partof&quot;. If a non-unigram (A) is part (subset) of
another non-unigram (B), then the longer one(B) would be replaced by the shorter one(A).</p>
</td></tr>
<tr><td><code id="keyword_merge_+3A_lemmatize_dict">lemmatize_dict</code></td>
<td>
<p>A dictionary of base terms and lemmas to use for replacement.
Only used when the <b>lemmatize</b> parameter is <code>TRUE</code>.
The first column should be the full word form in lower case
while the second column is the corresponding replacement lemma.
Default uses <code>NULL</code>, this would apply the default dictionary used in
<code><a href="textstem.html#topic+lemmatize_strings">lemmatize_strings</a></code> function. Applicable when <b>reduce_form</b> takes &quot;lemma&quot;.</p>
</td></tr>
<tr><td><code id="keyword_merge_+3A_stem_lang">stem_lang</code></td>
<td>
<p>The name of a recognized language.
The list of supported languages could be found at <code><a href="SnowballC.html#topic+getStemLanguages">getStemLanguages</a></code>.
Applicable when <b>reduce_form</b> takes &quot;stem&quot;.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>While <code>keyword_clean</code> has provided a robust way to lemmatize the keywords, the returned token
might not be the most common way to use.This function first gets the stem or lemma of
every keyword using <code><a href="textstem.html#topic+stem_strings">stem_strings</a></code> or <code><a href="textstem.html#topic+lemmatize_strings">lemmatize_strings</a></code> from <span class="pkg">textstem</span> package,
then find the most frequent form (if more than 1,randomly select one)
for each stem or lemma. Last, every keyword
would be replaced by the most frequent keyword which share the same stem or lemma with it.
</p>
<p>When the 'reduce_form' is set to &quot;partof&quot;, then for non-unigrams in the same document,
if one non-unigram is the subset of another, then they would be merged into the shorter one,
which is considered to be more general (e.g. &quot;time series&quot; and &quot;time series analysis&quot; would be
merged into &quot;time series&quot; if they co-occur in the same document). This could reduce the redundant
information. This is only applied to multi-word phrases, because using it for one word would
oversimplify the token and cause information loss (therefore, &quot;time series&quot; and &quot;time&quot; would not be
merged into &quot;time&quot;). This is an advanced option that should be used with caution (A trade-off between
information generalization and detailed information retention).
</p>


<h3>Value</h3>

<p>A tbl, namely a tidy table with document ID and merged keyword.
</p>


<h3>See Also</h3>

<p><code><a href="textstem.html#topic+stem_strings">stem_strings</a></code>, <code><a href="textstem.html#topic+lemmatize_strings">lemmatize_strings</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(akc)


bibli_data_table %&gt;%
  keyword_clean(lemmatize = FALSE) %&gt;%
  keyword_merge(reduce_form = "stem")

bibli_data_table %&gt;%
  keyword_clean(lemmatize = FALSE) %&gt;%
  keyword_merge(reduce_form = "lemma")


</code></pre>

<hr>
<h2 id='keyword_network'>Flexiable visualization of network (alternative to 'keyword_vis')</h2><span id='topic+keyword_network'></span>

<h3>Description</h3>

<p>Providing flexible visualization of <code><a href="#topic+keyword_vis">keyword_vis</a></code>. The
group size would be showed, and user could extract specific group to visualize.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>keyword_network(
  tibble_graph,
  group_no = NULL,
  facet = TRUE,
  max_nodes = 10,
  alpha = 0.7
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="keyword_network_+3A_tibble_graph">tibble_graph</code></td>
<td>
<p>A <code>tbl_graph</code> output by <code><a href="#topic+keyword_group">keyword_group</a></code>.</p>
</td></tr>
<tr><td><code id="keyword_network_+3A_group_no">group_no</code></td>
<td>
<p>If one wants to visualize a specific group, gives the group number.
Default uses <code>NULL</code>,which returns all the groups.</p>
</td></tr>
<tr><td><code id="keyword_network_+3A_facet">facet</code></td>
<td>
<p>Whether the figure should use facet or not.</p>
</td></tr>
<tr><td><code id="keyword_network_+3A_max_nodes">max_nodes</code></td>
<td>
<p>The maximum number of nodes displayed in each group.</p>
</td></tr>
<tr><td><code id="keyword_network_+3A_alpha">alpha</code></td>
<td>
<p>The transparency of label. Must lie between 0 and 1. Default uses 0.7.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If the <code>group_no</code> is not specified, when <code>facet == TRUE</code>,
the function returns a faceted figure with limited number of nodes
(adjuseted by <code>max_nodes</code> parameter). The &quot;N=&quot; shows the total size of the group.
</p>
<p>When <code>facet == FALSE</code>,all the nodes would be displayed in one
network.Colors are used to specify the groups, the size of nodes is proportional to the keyword frequency,
while the alpha of edges is proportional to the co-occurrence relationship between keywords.
</p>
<p>If the <code>group_no</code> is specified, returns the network visualization of the group.
If you want to display all the nodes, set <code>max_nodes</code> to <code>Inf</code>.
</p>


<h3>Value</h3>

<p>An object yielded by <code><a href="ggraph.html#topic+ggraph">ggraph</a></code>
</p>


<h3>See Also</h3>

<p><code><a href="ggraph.html#topic+ggraph">ggraph</a></code>,<code><a href="#topic+keyword_vis">keyword_vis</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
 library(akc)

 bibli_data_table %&gt;%
   keyword_clean(id = "id",keyword = "keyword") %&gt;%
   keyword_group(id = "id",keyword = "keyword") %&gt;%
   keyword_network()

# use color with `scale_fill_`
 bibli_data_table %&gt;%
   keyword_clean(id = "id",keyword = "keyword") %&gt;%
   keyword_group(id = "id",keyword = "keyword") %&gt;%
   keyword_network() + ggplot2::scale_fill_viridis_d()

 # without facet
 bibli_data_table %&gt;%
   keyword_clean(id = "id",keyword = "keyword") %&gt;%
   keyword_group(id = "id",keyword = "keyword") %&gt;%
   keyword_network(facet = FALSE)

# get Group 5
 bibli_data_table %&gt;%
   keyword_clean(id = "id",keyword = "keyword") %&gt;%
   keyword_group(id = "id",keyword = "keyword") %&gt;%
   keyword_network(group_no = 5)

</code></pre>

<hr>
<h2 id='keyword_table'>Display the table with different groups of keywords</h2><span id='topic+keyword_table'></span>

<h3>Description</h3>

<p>Display the result of network-based keyword clustering, with frequency information attached.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>keyword_table(tibble_graph, top = 10)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="keyword_table_+3A_tibble_graph">tibble_graph</code></td>
<td>
<p>A <code>tbl_graph</code> output by <code><a href="#topic+keyword_group">keyword_group</a></code>.</p>
</td></tr>
<tr><td><code id="keyword_table_+3A_top">top</code></td>
<td>
<p>How many keywords should be displayed in the table for each group.
Default uses 10.If there is a tie,more than <em>top</em> keywords would be selected.
To show all the keywords, use <em>Inf</em>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A tibble with two columns, namely group and keywords with frequency attached.
Different keywords are separated by semicolon(';').
</p>


<h3>See Also</h3>

<p><code><a href="#topic+keyword_group">keyword_group</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(akc)

bibli_data_table %&gt;%
  keyword_clean(id = "id",keyword = "keyword") %&gt;%
  keyword_group(id = "id",keyword = "keyword") %&gt;%
  keyword_table()
</code></pre>

<hr>
<h2 id='keyword_vis'>Visualization of grouped keyword co-occurrence network</h2><span id='topic+keyword_vis'></span>

<h3>Description</h3>

<p>Visualization of network-based keyword clustering, with frequency and co-occurrence information attached.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>keyword_vis(tibble_graph, facet = TRUE, max_nodes = 10, alpha = 0.7)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="keyword_vis_+3A_tibble_graph">tibble_graph</code></td>
<td>
<p>A <code>tbl_graph</code> output by <code><a href="#topic+keyword_group">keyword_group</a></code>.</p>
</td></tr>
<tr><td><code id="keyword_vis_+3A_facet">facet</code></td>
<td>
<p>Whether the figure should use facet or not.</p>
</td></tr>
<tr><td><code id="keyword_vis_+3A_max_nodes">max_nodes</code></td>
<td>
<p>The maximum number of nodes displayed in each group.</p>
</td></tr>
<tr><td><code id="keyword_vis_+3A_alpha">alpha</code></td>
<td>
<p>The transparency of label. Must lie between 0 and 1. Default uses 0.7.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When <code>facet == TRUE</code>,the function returns a faceted figure with limited number of nodes
(adjuseted by <code>max_nodes</code> parameter).When <code>facet == FALSE</code>,all the nodes would be displayed in one
network.Colors are used to specify the groups, the size of nodes is proportional to the keyword frequency,
while the alpha of edges is proportional to the co-occurrence relationship between keywords.
</p>


<h3>Value</h3>

<p>An object yielded by <code><a href="ggraph.html#topic+ggraph">ggraph</a></code>
</p>


<h3>See Also</h3>

<p><code><a href="ggraph.html#topic+ggraph">ggraph</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(akc)

bibli_data_table %&gt;%
  keyword_clean(id = "id",keyword = "keyword") %&gt;%
  keyword_group(id = "id",keyword = "keyword") %&gt;%
  keyword_vis()

# without facet
bibli_data_table %&gt;%
  keyword_clean(id = "id",keyword = "keyword") %&gt;%
  keyword_group(id = "id",keyword = "keyword") %&gt;%
  keyword_vis(facet = FALSE)


</code></pre>

<hr>
<h2 id='make_dict'>Making one's own dictionary</h2><span id='topic+make_dict'></span>

<h3>Description</h3>

<p>Construting a dictionary using a string vector with user defined vocabulary.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>make_dict(dict_vacabulary_vector)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="make_dict_+3A_dict_vacabulary_vector">dict_vacabulary_vector</code></td>
<td>
<p>A character vector containing the user defined professional vocabulary.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Build a user defined vocabulary for keyword extraction (<code><a href="#topic+keyword_extract">keyword_extract</a></code>).
</p>


<h3>Value</h3>

<p>A data.table with document id and keyword,using keyword as the key.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+keyword_extract">keyword_extract</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(akc)
library(dplyr)

bibli_data_table %&gt;%
  keyword_clean() %&gt;%
  pull(keyword) %&gt;%
  make_dict() -&gt; dict

</code></pre>

<hr>
<h2 id='reexports'>Objects exported from other packages</h2><span id='topic+reexports'></span><span id='topic++25+3E+25'></span><span id='topic+group_components'></span><span id='topic+group_edge_betweenness'></span><span id='topic+group_fast_greedy'></span><span id='topic+group_infomap'></span><span id='topic+group_label_prop'></span><span id='topic+group_leading_eigen'></span><span id='topic+group_louvain'></span><span id='topic+group_optimal'></span><span id='topic+group_spinglass'></span><span id='topic+group_walktrap'></span><span id='topic+group_biconnected_component'></span>

<h3>Description</h3>

<p>These objects are imported from other packages. Follow the links
below to see their documentation.
</p>

<dl>
<dt>magrittr</dt><dd><p><code><a href="magrittr.html#topic+pipe">%&gt;%</a></code></p>
</dd>
<dt>tidygraph</dt><dd><p><code><a href="tidygraph.html#topic+group_graph">group_biconnected_component</a></code>, <code><a href="tidygraph.html#topic+group_graph">group_components</a></code>, <code><a href="tidygraph.html#topic+group_graph">group_edge_betweenness</a></code>, <code><a href="tidygraph.html#topic+group_graph">group_fast_greedy</a></code>, <code><a href="tidygraph.html#topic+group_graph">group_infomap</a></code>, <code><a href="tidygraph.html#topic+group_graph">group_label_prop</a></code>, <code><a href="tidygraph.html#topic+group_graph">group_leading_eigen</a></code>, <code><a href="tidygraph.html#topic+group_graph">group_louvain</a></code>, <code><a href="tidygraph.html#topic+group_graph">group_optimal</a></code>, <code><a href="tidygraph.html#topic+group_graph">group_spinglass</a></code>, <code><a href="tidygraph.html#topic+group_graph">group_walktrap</a></code></p>
</dd>
</dl>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
