<!DOCTYPE html><html><head><title>Help for package BayesS5</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {BayesS5}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#Bernoulli_Uniform'>
<p>Bernoulli-Uniform model prior</p></a></li>
<li><a href='#hyper_par'>
<p>Tuning parameter selection for nonlocal priors</p></a></li>
<li><a href='#ind_fun_g'>
<p>Zellner's g-prior</p></a></li>
<li><a href='#ind_fun_NLfP'>
<p>the log-marginal likelhood function based on the invers moment functional priors and inverse gamma prior (0.01,0.01)</p></a></li>
<li><a href='#ind_fun_pemom'>
<p>the log-marginal likelhood function based on peMoM priors and inverse gamma prior (0.01,0.01)</p></a></li>
<li><a href='#ind_fun_pimom'>
<p>the log-marginal likelhood function based on piMoM priors</p></a></li>
<li><a href='#obj_fun_g'>
<p>the log posterior distribution based on g-priors and inverse gamma prior (0.01,0.01)</p></a></li>
<li><a href='#obj_fun_pemom'>
<p>the log posterior distribution based on peMoM priors and inverse gamma prior (0.01,0.01)</p></a></li>
<li><a href='#obj_fun_pimom'>
<p>the log posterior distribution based on piMoM priors and inverse gamma prior (0.01,0.01)</p></a></li>
<li><a href='#result'>
<p>Posterior inference results from the object of S5</p></a></li>
<li><a href='#result_est_LS'>
<p>Posterior inference results from the object of S5</p></a></li>
<li><a href='#result_est_MAP'>
<p>Posterior inference results from the object of S5</p></a></li>
<li><a href='#S5'>
<p>Simplified shotgun stochastic search algorithm with screening (S5)</p></a></li>
<li><a href='#S5_additive'>
<p>Simplified shotgun stochastic search algorithm with screening (S5) for additive models</p></a></li>
<li><a href='#S5_parallel'>
<p>Parallel version of S5</p></a></li>
<li><a href='#SSS'>
<p>Shotgun stochastic search algorithm (SSS)</p></a></li>
<li><a href='#Uniform'>
<p>Uniform model prior</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Bayesian Variable Selection Using Simplified Shotgun Stochastic
Search with Screening (S5)</td>
</tr>
<tr>
<td>Version:</td>
<td>1.41</td>
</tr>
<tr>
<td>Date:</td>
<td>2020-03-20</td>
</tr>
<tr>
<td>Author:</td>
<td>Minsuk Shin and Ruoxuan Tian</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Minsuk Shin &lt;minsuk000@gmail.com&gt;</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.4.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>Matrix, stats, snowfall, abind, splines2</td>
</tr>
<tr>
<td>Description:</td>
<td>In p &gt;&gt; n settings, full posterior sampling using existing Markov chain Monte
    Carlo (MCMC) algorithms is highly inefficient and often not feasible from a practical
    perspective. To overcome this problem, we propose a scalable stochastic search algorithm that is called the Simplified Shotgun Stochastic Search (S5) and aimed at rapidly explore interesting regions of model space and finding the maximum a posteriori(MAP) model. Also, the S5 provides an approximation of posterior probability of each model (including the marginal inclusion probabilities). This algorithm is a part of an article titled "Scalable Bayesian Variable Selection Using Nonlocal Prior Densities in Ultrahigh-dimensional Settings" (2018) by Minsuk Shin, Anirban Bhattacharya, and Valen E. Johnson and "Nonlocal Functional Priors for Nonparametric Hypothesis Testing and High-dimensional Model Selection" (2020+) by Minsuk Shin and Anirban Bhattacharya. </td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://arxiv.org/abs/1507.07106v4">https://arxiv.org/abs/1507.07106v4</a></td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2020-03-23 16:44:09 UTC; mshin</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2020-03-24 07:40:14 UTC</td>
</tr>
</table>
<hr>
<h2 id='Bernoulli_Uniform'>
Bernoulli-Uniform model prior 
</h2><span id='topic+Bernoulli_Uniform'></span>

<h3>Description</h3>

<p>A mixture model prior with Bernoulli and uniform densities. See Scott and Berger (2010) for details. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Bernoulli_Uniform(ind,p)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Bernoulli_Uniform_+3A_ind">ind</code></td>
<td>

<p>an index set of variables in a model
</p>
</td></tr>
<tr><td><code id="Bernoulli_Uniform_+3A_p">p</code></td>
<td>

<p>the total number of covariates
</p>
</td></tr>
</table>


<h3>References</h3>

<p>Scott, James G., and James O. Berger. &quot;Bayes and empirical-Bayes multiplicity adjustment in the variable-selection problem.&quot; The Annals of Statistics 38.5 (2010): 2587-2619. </p>


<h3>See Also</h3>

<p><code><a href="#topic+Uniform">Uniform</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>p = 5000
ind = 1:3 
m = Bernoulli_Uniform(ind,p)
print(m)
</code></pre>

<hr>
<h2 id='hyper_par'>
Tuning parameter selection for nonlocal priors </h2><span id='topic+hyper_par'></span>

<h3>Description</h3>

<p>Hyper parameter tau selection for nonlocal priors using random sampling from the null distribution (Nikooienejad et al, 2016). 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hyper_par(type, X, y, thre)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hyper_par_+3A_type">type</code></td>
<td>

<p>a type of nonlocal priors; 'pimom' or 'pemom'.
</p>
</td></tr>
<tr><td><code id="hyper_par_+3A_x">X</code></td>
<td>

<p>a covariate matrix (a standardization is recommneded for nonlocal priors).
</p>
</td></tr>
<tr><td><code id="hyper_par_+3A_y">y</code></td>
<td>

<p>a response variable. 
</p>
</td></tr>
<tr><td><code id="hyper_par_+3A_thre">thre</code></td>
<td>

<p>a threshold; for details, see below. The default is p^-0.5.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Nikooienejad et al. (2016) proposed a novel approach to choose the hyperparameter tau for nonlocal priors. They first derive the null distribution of the regression coefficient by randomly sampling the covariates, and shuffle the index of the samples in the covariates. Then, they calculate the MLE from the sampled covariates that are shuffled. This process is repeated large enough times to approximate the null distribution of the MLE under the situation where all true regression coefficients are zero. They compare the nonlocal density with different values of the parameter to the null distribution so that the overlap of these densities falls below the threshold; see Nikooienejad et al. (2016) for further details. 
</p>


<h3>Value</h3>



<table>
<tr><td><code>tau</code></td>
<td>
<p>  : the choosen hyper parameter tau</p>
</td></tr>

</table>


<h3>Author(s)</h3>

<p>Shin Minsuk and Ruoxuan Tian
</p>


<h3>References</h3>

<p>Shin, M., Bhattacharya, A., Johnson V. E. (2018) A Scalable Bayesian Variable Selection Using Nonlocal Prior Densities in Ultrahigh-dimensional Settings, Statistica Sinica. 
</p>
<p>Nikooienejad,A., Wang, W., and Johnson V.E. (2016). Bayesian variable selection for binary outcomes in high dimensional genomic studies using non-local priors. Bioinformatics, 32(9), 1338-45.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ind_fun_pimom">ind_fun_pimom</a></code>, <code><a href="#topic+ind_fun_pemom">ind_fun_pemom</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>p=50
n = 200

indx.beta = 1:5
xd0 = rep(0,p);xd0[indx.beta]=1
bt0 = rep(0,p); 
bt0[1:5]=c(1,1.25,1.5,1.75,2)*sample(c(1,-1),5,replace=TRUE)
xd=xd0
bt=bt0
X = matrix(rnorm(n*p),n,p)
y = crossprod(t(X),bt0) + rnorm(n)*sqrt(1.5) 
X = scale(X)
y = y-mean(y)
y = as.vector(y)

# piMoM  
C0 = 1 # the number of repetitions of S5 algorithms to explore the model space
tuning = 10 # tuning parameter
#tuning = hyper_par(type="pimom",X,y,thre = p^-0.5)
print(tuning)
</code></pre>

<hr>
<h2 id='ind_fun_g'>
Zellner's g-prior 
</h2><span id='topic+ind_fun_g'></span>

<h3>Description</h3>

<p>a log-marginal likelhood value of a model, based on the  Zellner's g-prior on the regression coefficients.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ind_fun_g(X.ind,y,n,p,tuning)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ind_fun_g_+3A_x.ind">X.ind</code></td>
<td>

<p>the subset of covariates in a model
</p>
</td></tr>
<tr><td><code id="ind_fun_g_+3A_y">y</code></td>
<td>

<p>the response variable
</p>
</td></tr>   
<tr><td><code id="ind_fun_g_+3A_n">n</code></td>
<td>

<p>the sample size   
</p>
</td></tr>   
<tr><td><code id="ind_fun_g_+3A_p">p</code></td>
<td>

<p>the total number of covariates
</p>
</td></tr>   
<tr><td><code id="ind_fun_g_+3A_tuning">tuning</code></td>
<td>

<p>a value of the tuning parameter
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Shin Minsuk and Ruoxuan Tian
</p>


<h3>References</h3>

<p>Zellner, Arnold. &quot;On assessing prior distributions and Bayesian regression analysis with g-prior distributions.&quot; Bayesian inference and decision techniques: Essays in Honor of Bruno De Finetti 6 (1986): 233-243.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ind_fun_pimom">ind_fun_pimom</a></code>, <code><a href="#topic+ind_fun_g">ind_fun_g</a></code>

</p>


<h3>Examples</h3>

<pre><code class='language-R'>#p=5000
p = 10
n = 200

indx.beta = 1:5
xd0 = rep(0,p);xd0[indx.beta]=1
bt0 = rep(0,p); 
bt0[1:5]=c(1,1.25,1.5,1.75,2)*sample(c(1,-1),5,replace=TRUE)
xd=xd0
bt=bt0
X = matrix(rnorm(n*p),n,p)
y = crossprod(t(X),bt0) + rnorm(n)*sqrt(1.5)
X = scale(X)
y = y-mean(y)
y = as.vector(y)

C0 = 1 # the number of repetitions of S5 algorithms to explore the model space
tuning = p^2 # tuning parameter g for g-prior
ind_fun = ind_fun_g # choose the pror on the regression coefficients (g-prior in this case)
model = Uniform #choose the model prior (Uniform prior in this cases)
tem =  seq(0.4,1,length.out=20)^2 # the sequence of the temperatures

fit_g = S5(X,y,ind_fun=ind_fun,model=model, tuning=tuning,tem=tem,C0=C0)
</code></pre>

<hr>
<h2 id='ind_fun_NLfP'>
the log-marginal likelhood function based on the invers moment functional priors and inverse gamma prior (0.01,0.01)
</h2><span id='topic+ind_fun_NLfP'></span>

<h3>Description</h3>

<p>a log-marginal likelhood value of a model, based on the  peMoM prior on the regression coefficients  and inverse gamma prior (0.01,0.01) on the variance.</p>


<h3>Usage</h3>

<pre><code class='language-R'>ind_fun_NLfP(ind2, y, phi, n, p, K, IP.phi, C.prior1, tuning)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ind_fun_NLfP_+3A_ind2">ind2</code></td>
<td>

<p>the index of covariates in a model
</p>
</td></tr>
<tr><td><code id="ind_fun_NLfP_+3A_y">y</code></td>
<td>

<p>the response variable
</p>
</td></tr>
<tr><td><code id="ind_fun_NLfP_+3A_phi">phi</code></td>
<td>

<p>the B-spline basis
</p>
</td></tr>
<tr><td><code id="ind_fun_NLfP_+3A_n">n</code></td>
<td>

<p>the sample size   
</p>
</td></tr>   
<tr><td><code id="ind_fun_NLfP_+3A_p">p</code></td>
<td>

<p>the total number of covariates
</p>
</td></tr>
<tr><td><code id="ind_fun_NLfP_+3A_k">K</code></td>
<td>

<p>the degree of freedom for the B-spline basis
</p>
</td></tr>
<tr><td><code id="ind_fun_NLfP_+3A_ip.phi">IP.phi</code></td>
<td>

<p>the projection matrix on the null space; Q matrix in Shin and Bhattacharya (2020+)
</p>
</td></tr>
<tr><td><code id="ind_fun_NLfP_+3A_c.prior1">C.prior1</code></td>
<td>

<p>the logarithm of the normalizing constant of the nonlocal funcitonal prior
</p>
</td></tr>
<tr><td><code id="ind_fun_NLfP_+3A_tuning">tuning</code></td>
<td>

<p>a value of the tuning parameter
</p>
</td></tr>
</table>


<h3>References</h3>

<p>Shin, M. and  Bhattacharya, A.(2020) Nonlocal Functional Priors for Nonparametric Hypothesis Testing and High-dimensional  Model Selection. 
</p>

<hr>
<h2 id='ind_fun_pemom'>
the log-marginal likelhood function based on peMoM priors and inverse gamma prior (0.01,0.01)
</h2><span id='topic+ind_fun_pemom'></span>

<h3>Description</h3>

<p>a log-marginal likelhood value of a model, based on the  peMoM prior on the regression coefficients  and inverse gamma prior (0.01,0.01) on the variance.</p>


<h3>Usage</h3>

<pre><code class='language-R'>ind_fun_pemom(X.ind,y,n,p,tuning)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ind_fun_pemom_+3A_x.ind">X.ind</code></td>
<td>

<p>the subset of covariates in a model
</p>
</td></tr>
<tr><td><code id="ind_fun_pemom_+3A_y">y</code></td>
<td>

<p>the response variable
</p>
</td></tr>   
<tr><td><code id="ind_fun_pemom_+3A_n">n</code></td>
<td>

<p>the sample size   
</p>
</td></tr>   
<tr><td><code id="ind_fun_pemom_+3A_p">p</code></td>
<td>

<p>the total number of covariates
</p>
</td></tr>   
<tr><td><code id="ind_fun_pemom_+3A_tuning">tuning</code></td>
<td>

<p>a value of the tuning parameter
</p>
</td></tr>
</table>


<h3>References</h3>

<p>Shin, M., Bhattacharya, A., Johnson V. E. (2018) A Scalable Bayesian Variable Selection Using Nonlocal Prior Densities in Ultrahigh-dimensional Settings, Statistica Sinica. 
</p>
<p>Rossell, D., Telesca, D., and Johnson, V. E. (2013) High-dimensional Bayesian classifiers using non-local priors, Statistical Models for Data Analysis, 305-313.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ind_fun_g">ind_fun_g</a></code>, <code><a href="#topic+ind_fun_pimom">ind_fun_pimom</a></code>
</p>

<hr>
<h2 id='ind_fun_pimom'>
the log-marginal likelhood function based on piMoM priors
</h2><span id='topic+ind_fun_pimom'></span>

<h3>Description</h3>

<p>a log-marginal likelhood value of a model, based on the  piMoM prior on the regression coefficients and inverse gamma prior (0.01,0.01) on the variance.</p>


<h3>Usage</h3>

<pre><code class='language-R'>ind_fun_pimom(X.ind,y,n,p,tuning)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ind_fun_pimom_+3A_x.ind">X.ind</code></td>
<td>

<p>the subset of covariates in a model
</p>
</td></tr>
<tr><td><code id="ind_fun_pimom_+3A_y">y</code></td>
<td>

<p>the response variable
</p>
</td></tr>   
<tr><td><code id="ind_fun_pimom_+3A_n">n</code></td>
<td>

<p>the sample size   
</p>
</td></tr>   
<tr><td><code id="ind_fun_pimom_+3A_p">p</code></td>
<td>

<p>the total number of covariates
</p>
</td></tr>   
<tr><td><code id="ind_fun_pimom_+3A_tuning">tuning</code></td>
<td>

<p>a value of the tuning parameter
</p>
</td></tr>
</table>


<h3>References</h3>

<p>Shin, M., Bhattacharya, A., Johnson V. E. (2018) A Scalable Bayesian Variable Selection Using Nonlocal Prior Densities in Ultrahigh-dimensional Settings, Statistica Sinica. 
</p>
<p>Johnson, V. E. and Rossell, D. (2012) Bayesian model selection in high-dimensional settings  , David, Journal of the American Statistical Association,  107 (498), 649-660.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ind_fun_g">ind_fun_g</a></code>, <code><a href="#topic+ind_fun_pemom">ind_fun_pemom</a></code>
</p>

<hr>
<h2 id='obj_fun_g'>
the log posterior distribution based on g-priors and inverse gamma prior (0.01,0.01)
</h2><span id='topic+obj_fun_g'></span>

<h3>Description</h3>

<p>a log posterior density value at regression coefficients of a model, based on the g-prior on the regression coefficients  and inverse gamma prior (0.01,0.01) on the variance.</p>


<h3>Usage</h3>

<pre><code class='language-R'>obj_fun_g(ind,X,y,n,p,tuning)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="obj_fun_g_+3A_ind">ind</code></td>
<td>

<p>the index set of  a model
</p>
</td></tr>
<tr><td><code id="obj_fun_g_+3A_x">X</code></td>
<td>

<p>the covariates
</p>
</td></tr>   
<tr><td><code id="obj_fun_g_+3A_y">y</code></td>
<td>

<p>the response variable
</p>
</td></tr>
<tr><td><code id="obj_fun_g_+3A_n">n</code></td>
<td>

<p>the sample size   
</p>
</td></tr>   
<tr><td><code id="obj_fun_g_+3A_p">p</code></td>
<td>

<p>the total number of covariates
</p>
</td></tr>   
<tr><td><code id="obj_fun_g_+3A_tuning">tuning</code></td>
<td>

<p>a value of the tuning parameter
</p>
</td></tr>
</table>


<h3>References</h3>

<p>Shin, M., Bhattacharya, A., Johnson V. E. (2018) A Scalable Bayesian Variable Selection Using Nonlocal Prior Densities in Ultrahigh-dimensional Settings, Statistica Sinica. 
</p>
<p>Rossell, D., Telesca, D., and Johnson, V. E. (2013) High-dimensional Bayesian classifiers using non-local priors, Statistical Models for Data Analysis, 305-313.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+obj_fun_pimom">obj_fun_pimom</a></code>, <code><a href="#topic+obj_fun_pemom">obj_fun_pemom</a></code>
</p>

<hr>
<h2 id='obj_fun_pemom'>
the log posterior distribution based on peMoM priors and inverse gamma prior (0.01,0.01)
</h2><span id='topic+obj_fun_pemom'></span>

<h3>Description</h3>

<p>a log posterior density value at regression coefficients of a model, based on the  peMoM prior on the regression coefficients  and inverse gamma prior (0.01,0.01) on the variance.</p>


<h3>Usage</h3>

<pre><code class='language-R'>obj_fun_pemom(ind,X,y,n,p,tuning)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="obj_fun_pemom_+3A_ind">ind</code></td>
<td>

<p>the index set of  a model
</p>
</td></tr>
<tr><td><code id="obj_fun_pemom_+3A_x">X</code></td>
<td>

<p>the covariates
</p>
</td></tr>   
<tr><td><code id="obj_fun_pemom_+3A_y">y</code></td>
<td>

<p>the response variable
</p>
</td></tr>
<tr><td><code id="obj_fun_pemom_+3A_n">n</code></td>
<td>

<p>the sample size   
</p>
</td></tr>   
<tr><td><code id="obj_fun_pemom_+3A_p">p</code></td>
<td>

<p>the total number of covariates
</p>
</td></tr>   
<tr><td><code id="obj_fun_pemom_+3A_tuning">tuning</code></td>
<td>

<p>a value of the tuning parameter
</p>
</td></tr>
</table>


<h3>References</h3>

<p>Shin, M., Bhattacharya, A., Johnson V. E. (2018) A Scalable Bayesian Variable Selection Using Nonlocal Prior Densities in Ultrahigh-dimensional Settings, Statistica Sinica. 
</p>
<p>Rossell, D., Telesca, D., and Johnson, V. E. (2013) High-dimensional Bayesian classifiers using non-local priors, Statistical Models for Data Analysis, 305-313.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+obj_fun_g">obj_fun_g</a></code>, <code><a href="#topic+obj_fun_pimom">obj_fun_pimom</a></code>
</p>

<hr>
<h2 id='obj_fun_pimom'>
the log posterior distribution based on piMoM priors and inverse gamma prior (0.01,0.01)
</h2><span id='topic+obj_fun_pimom'></span>

<h3>Description</h3>

<p>a log posterior density value at regression coefficients of a model, based on the  piMoM prior on the regression coefficients  and inverse gamma prior (0.01,0.01) on the variance.</p>


<h3>Usage</h3>

<pre><code class='language-R'>obj_fun_pimom(ind,X,y,n,p,tuning)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="obj_fun_pimom_+3A_ind">ind</code></td>
<td>

<p>the index set of  a model
</p>
</td></tr>
<tr><td><code id="obj_fun_pimom_+3A_x">X</code></td>
<td>

<p>the covariates
</p>
</td></tr>   
<tr><td><code id="obj_fun_pimom_+3A_y">y</code></td>
<td>

<p>the response variable
</p>
</td></tr>
<tr><td><code id="obj_fun_pimom_+3A_n">n</code></td>
<td>

<p>the sample size   
</p>
</td></tr>   
<tr><td><code id="obj_fun_pimom_+3A_p">p</code></td>
<td>

<p>the total number of covariates
</p>
</td></tr>   
<tr><td><code id="obj_fun_pimom_+3A_tuning">tuning</code></td>
<td>

<p>a value of the tuning parameter
</p>
</td></tr>
</table>


<h3>References</h3>

<p>Shin, M., Bhattacharya, A., Johnson V. E. (2018) A Scalable Bayesian Variable Selection Using Nonlocal Prior Densities in Ultrahigh-dimensional Settings, Statistica Sinica. 
</p>
<p>Rossell, D., Telesca, D., and Johnson, V. E. (2013) High-dimensional Bayesian classifiers using non-local priors, Statistical Models for Data Analysis, 305-313.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+obj_fun_g">obj_fun_g</a></code>, <code><a href="#topic+obj_fun_pemom">obj_fun_pemom</a></code>
</p>

<hr>
<h2 id='result'>
Posterior inference results from the object of S5
</h2><span id='topic+result'></span>

<h3>Description</h3>

<p>Using the object of S5, the  maximum a posteriori (MAP) model, its posterior probability, and the marginal inclusion probabilities are provided.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>result(fit)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="result_+3A_fit">fit</code></td>
<td>

<p>an object of the 'S5' function.
</p>
</td></tr>
</table>


<h3>Value</h3>



<table>
<tr><td><code>hppm</code></td>
<td>
<p>the MAP model </p>
</td></tr>
<tr><td><code>hppm.prob</code></td>
<td>
<p>the posterior probability of the MAP model</p>
</td></tr>
<tr><td><code>marg.prob</code></td>
<td>
<p>the marginal inclusion probabilities </p>
</td></tr>
<tr><td><code>gam</code></td>
<td>
<p>the binary vaiables of searched models by S5</p>
</td></tr>
<tr><td><code>obj</code></td>
<td>
<p>the corresponding log (unnormalized) posterior model probabilities</p>
</td></tr>
<tr><td><code>post</code></td>
<td>
<p>the corresponding (normalized) posterior model probabilities</p>
</td></tr>
<tr><td><code>tuning</code></td>
<td>
<p>the tuning parameter used in the model selection</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Shin Minsuk and Ruoxuan Tian
</p>


<h3>References</h3>

<p>Shin, M., Bhattacharya, A., Johnson V. E. (2018) A Scalable Bayesian Variable Selection Using Nonlocal Prior Densities in Ultrahigh-dimensional Settings, Statistica Sinica. 
</p>
<p>Hans, C., Dobra, A., and West, M. (2007). Shotgun stochastic search for large p regression. Journal of the American Statistical Association, 102, 507-516.
</p>
<p>Nikooienejad,A., Wang, W., and Johnson V.E. (2016). Bayesian variable selection for binary outcomes in high dimensional genomic studies using non-local priors. Bioinformatics, 32(9), 1338-45.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>p=5000
n = 200

indx.beta = 1:5
xd0 = rep(0,p);xd0[indx.beta]=1
bt0 = rep(0,p); 
bt0[1:5]=c(1,1.25,1.5,1.75,2)*sample(c(1,-1),5,replace=TRUE)
xd=xd0
bt=bt0
X = matrix(rnorm(n*p),n,p)
y = X%*%bt0 + rnorm(n)*sqrt(1.5)
X = scale(X)
y = y-mean(y)
y = as.vector(y)

### piMoM  
#C0 = 2 # the number of repetitions of S5 algorithms to explore the model space
#tuning = 10 # tuning parameter
#tuning = hyper_par(type="pimom",X,y,thre = p^-0.5)
#print(tuning)
#ind_fun = ind_fun_pimom # choose the prior on the regression coefficients (pimom in this case)
#model = Bernoulli_Uniform # choose the model prior 
#tem =  seq(0.4,1,length.out=20)^2 # the sequence of the temperatures

#fit_pimom = S5(X,y,ind_fun=ind_fun,model = model,tuning=tuning,tem=tem,C0=C0)
#fit_pimom$GAM # the searched models by S5
#fit_pimom$OBJ # the corresponding log (unnormalized) posterior probability

#res_pimom = result(fit_pimom)
#str(res_pimom)
#print(res_pimom$hppm) 
#print(res_pimom$hppm.prob)  
#plot(res_pimom$marg.prob,ylim=c(0,1)) 
</code></pre>

<hr>
<h2 id='result_est_LS'>
Posterior inference results from the object of S5
</h2><span id='topic+result_est_LS'></span>

<h3>Description</h3>

<p>Using the object of S5, the Least Square (LS) estimator of the MAP model and Bayesian Model Averaged (BMA) LS estimators of the regression coefficients are provided.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>result_est_LS(res,X,y,verbose = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="result_est_LS_+3A_res">res</code></td>
<td>

<p>an object of the 'S5' function.
</p>
</td></tr>
<tr><td><code id="result_est_LS_+3A_x">X</code></td>
<td>
<p>the covariates.</p>
</td></tr>
<tr><td><code id="result_est_LS_+3A_y">y</code></td>
<td>
<p>the response varaible.</p>
</td></tr>
<tr><td><code id="result_est_LS_+3A_verbose">verbose</code></td>
<td>
<p>logical; default is TRUE.</p>
</td></tr>
</table>


<h3>Value</h3>



<table>
<tr><td><code>intercept.MAP</code></td>
<td>
<p>the least square estimator of the intercept in the MAP model. </p>
</td></tr>
<tr><td><code>beta.MAP</code></td>
<td>
<p>the least square estimator of the regression coefficients in the MAP model.</p>
</td></tr>
<tr><td><code>intercept.BMA</code></td>
<td>
<p>the Baeysian model averaged over the least square estimator of the intercept. </p>
</td></tr>
<tr><td><code>beta.BMA</code></td>
<td>
<p>the Bayesian model averaged over the least square estimator of the regression coefficients.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Shin Minsuk and Ruoxuan Tian
</p>


<h3>References</h3>

<p>Shin, M., Bhattacharya, A., Johnson V. E. (2018) A Scalable Bayesian Variable Selection Using Nonlocal Prior Densities in Ultrahigh-dimensional Settings, Statistica Sinica. 
</p>
<p>Hans, C., Dobra, A., and West, M. (2007). Shotgun stochastic search for large p regression. Journal of the American Statistical Association, 102, 507-516.
</p>
<p>Nikooienejad,A., Wang, W., and Johnson V.E. (2016). Bayesian variable selection for binary outcomes in high dimensional genomic studies using non-local priors. Bioinformatics, 32(9), 1338-45.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>p=5000
n = 100

indx.beta = 1:5
xd0 = rep(0,p);xd0[indx.beta]=1
bt0 = rep(0,p); 
bt0[1:5]=c(1,1.25,1.5,1.75,2)*sample(c(1,-1),5,replace=TRUE)
xd=xd0
bt=bt0
X = matrix(rnorm(n*p),n,p)
y = X%*%bt0 + rnorm(n)*sqrt(1.5)
X = scale(X)
y = y-mean(y)
y = as.vector(y)

### piMoM  
#C0 = 2 # the number of repetitions of S5 algorithms to explore the model space
#tuning = 10 # tuning parameter
#tuning = hyper_par(type="pimom",X,y,thre = p^-0.5)
#print(tuning)
#ind_fun = ind_fun_pimom # choose the prior on the regression coefficients (pimom in this case)
#model = Bernoulli_Uniform # choose the model prior 
#tem =  seq(0.4,1,length.out=20)^2 # the sequence of the temperatures

#fit_pimom = S5(X,y,ind_fun=ind_fun,model = model,tuning=tuning,tem=tem,C0=C0)
#fit_pimom$GAM # the searched models by S5
#fit_pimom$OBJ # the corresponding log (unnormalized) posterior probability

#res_pimom = result(fit_pimom)
#est.LS = result_est_LS(res_pimom,X,y,obj_fun_pimom,verbose=TRUE)
#plot(est.LS$beta.MAP,est.LS$beta.BMA)
#abline(0,1,col="red")
</code></pre>

<hr>
<h2 id='result_est_MAP'>
Posterior inference results from the object of S5
</h2><span id='topic+result_est_MAP'></span>

<h3>Description</h3>

<p>Using the object of S5, the maximum a posteriori (MAP) estimator  and Bayesian Model Averaged (BMA) estimators of the regression coefficients are provided.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>result_est_MAP(res,X,y,obj_fun,verbose = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="result_est_MAP_+3A_res">res</code></td>
<td>

<p>an object of the 'S5' function.
</p>
</td></tr>
<tr><td><code id="result_est_MAP_+3A_x">X</code></td>
<td>
<p>the covariates.</p>
</td></tr>
<tr><td><code id="result_est_MAP_+3A_y">y</code></td>
<td>
<p>the response varaible.</p>
</td></tr>
<tr><td><code id="result_est_MAP_+3A_obj_fun">obj_fun</code></td>
<td>
<p>the negative log (unnormalized) posterior density when a model is given.</p>
</td></tr>
<tr><td><code id="result_est_MAP_+3A_verbose">verbose</code></td>
<td>
<p>logical; default is TRUE.</p>
</td></tr>
</table>


<h3>Value</h3>



<table>
<tr><td><code>intercept.MAP</code></td>
<td>
<p>the MAP estimator of the intercept. </p>
</td></tr>
<tr><td><code>beta.MAP</code></td>
<td>
<p>the MAP estimator of the regression coefficients.</p>
</td></tr>
<tr><td><code>sig.MAP</code></td>
<td>
<p>the MAP estimator of the regression variance.</p>
</td></tr>
<tr><td><code>intercept.BMA</code></td>
<td>
<p>the Baeysian model averaged estimator of the intercept. </p>
</td></tr>
<tr><td><code>beta.BMA</code></td>
<td>
<p>the Bayesian model averaged estimator of the regression coefficients.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Shin Minsuk and Ruoxuan Tian
</p>


<h3>References</h3>

<p>Shin, M., Bhattacharya, A., Johnson V. E. (2018) A Scalable Bayesian Variable Selection Using Nonlocal Prior Densities in Ultrahigh-dimensional Settings, Statistica Sinica. 
</p>
<p>Hans, C., Dobra, A., and West, M. (2007). Shotgun stochastic search for large p regression. Journal of the American Statistical Association, 102, 507-516.
</p>
<p>Nikooienejad,A., Wang, W., and Johnson V.E. (2016). Bayesian variable selection for binary outcomes in high dimensional genomic studies using non-local priors. Bioinformatics, 32(9), 1338-45.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>p=5000
n = 100

indx.beta = 1:5
xd0 = rep(0,p);xd0[indx.beta]=1
bt0 = rep(0,p); 
bt0[1:5]=c(1,1.25,1.5,1.75,2)*sample(c(1,-1),5,replace=TRUE)
xd=xd0
bt=bt0
X = matrix(rnorm(n*p),n,p)
y = X%*%bt0 + rnorm(n)*sqrt(1.5)
X = scale(X)
y = y-mean(y)
y = as.vector(y)

### piMoM  
#C0 = 2 # the number of repetitions of S5 algorithms to explore the model space
#tuning = 10 # tuning parameter
#tuning = hyper_par(type="pimom",X,y,thre = p^-0.5)
#print(tuning)
#ind_fun = ind_fun_pimom # choose the prior on the regression coefficients (pimom in this case)
#model = Bernoulli_Uniform # choose the model prior 
#tem =  seq(0.4,1,length.out=20)^2 # the sequence of the temperatures

#fit_pimom = S5(X,y,ind_fun=ind_fun,model = model,tuning=tuning,tem=tem,C0=C0)
#fit_pimom$GAM # the searched models by S5
#fit_pimom$OBJ # the corresponding log (unnormalized) posterior probability

#res_pimom = result(fit_pimom)
#est.MAP = result_est_MAP(res_pimom,X,y,obj_fun_pimom,verbose=TRUE)
#plot(est.MAP$beta.MAP,est.MAP$beta.BMA)
#abline(0,1,col="red")
</code></pre>

<hr>
<h2 id='S5'>
Simplified shotgun stochastic search algorithm with screening (S5)
</h2><span id='topic+S5'></span>

<h3>Description</h3>

<p>The Simplified Shotgun Stochastic Search with Screening (S5) is proposed by Shin et al (2018), which is a scalable stochastic search algorithm for high-dimensonal Bayesian variable selection.  It is a modified version of the Shotgun Stochasitic Search (SSS, Hans et al., 2007), aimed at rapidly identifying regions of high posterior probability and finding the maximum a posteriori (MAP) model. Also, the S5 provides an approximation of posterior probability of each model (including the marginal inculsion probabilities). For details, see Shin et al. (2018)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>S5(X, y, ind_fun, model, tuning, tem, ITER = 20, S = 20, C0 = 5, verbose = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="S5_+3A_x">X</code></td>
<td>

<p>the covariate matrix (a standardization is recommneded for nonlocal priors).
</p>
</td></tr>
<tr><td><code id="S5_+3A_y">y</code></td>
<td>

<p>a response variable. 
</p>
</td></tr>
<tr><td><code id="S5_+3A_ind_fun">ind_fun</code></td>
<td>

<p>a log-marginal likelihood function of models, which is resulted from a pred-specified priors on the regression coefficients. The default is &quot;piMoM&quot;. See the example below for details.
</p>
</td></tr>
<tr><td><code id="S5_+3A_model">model</code></td>
<td>

<p>a model prior; Uniform or Bernoulli_Uniform. The default is Bernoulli_Uniform
</p>
</td></tr>
<tr><td><code id="S5_+3A_tuning">tuning</code></td>
<td>

<p>a tuning parameter for the objective function (tau for piMoM and peMoM priors; g for the g-prior).
</p>
</td></tr>
<tr><td><code id="S5_+3A_tem">tem</code></td>
<td>

<p>a temperature schedule. The default is seq(0.4,1,length.out=20)^-2.
</p>
</td></tr>
<tr><td><code id="S5_+3A_iter">ITER</code></td>
<td>

<p>the number of iterations in each temperature; default is 20.
</p>
</td></tr>
<tr><td><code id="S5_+3A_s">S</code></td>
<td>

<p>a screening size of variables; default is 20.
</p>
</td></tr>
<tr><td><code id="S5_+3A_c0">C0</code></td>
<td>

<p>a number of repetition of  the S5 algorithm C0 times,default is 2. When the total number of variables is huge and real data sets are considered, using a large number of C0 is recommended, e.g., C0=5.  
</p>
</td></tr>
<tr><td><code id="S5_+3A_verbose">verbose</code></td>
<td>

<p>if TRUE, the function prints the currnet status of the S5 in each temperature; the default is TRUE.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Using the S5 (Shin et al., 2018), you will get all the models searched by S5 algorithm, and their corresponding log (unnormalized) posterior probabilities, and also this function can receive searched model for g-prior,piMoM,and peMoM. 
</p>
<p>After obtaining the object of the S5 function, by using the 'result' function, you can obtain the posterior probabilities of the searched models including the MAP model and  the marginal inclusion probabilities of each variable.
</p>
<p>By using the procedure of Nikooienejad et al. (2016), the 'hyper_par' function chooses the tuning parameter for nonlocal priors (piMoM or peMoM priors).
</p>


<h3>Value</h3>



<table>
<tr><td><code>GAM</code></td>
<td>
<p> the binary vaiables of searched models by S5</p>
</td></tr>
<tr><td><code>OBJ</code></td>
<td>
<p> the corresponding log (unnormalized) posterior probability</p>
</td></tr>
<tr><td><code>tuning</code></td>
<td>
<p>the tuning parameter used in the model selection</p>
</td></tr>

</table>


<h3>Author(s)</h3>

<p>Shin Minsuk and Ruoxuan Tian
</p>


<h3>References</h3>

<p>Shin, M., Bhattacharya, A., Johnson V. E. (2018) A Scalable Bayesian Variable Selection Using Nonlocal Prior Densities in Ultrahigh-dimensional Settings, Statistica Sinica. 
</p>
<p>Hans, C., Dobra, A., and West, M. (2007). Shotgun stochastic search for large p regression. Journal of the American Statistical Association, 102, 507-516.
</p>
<p>Nikooienejad,A., Wang, W., and Johnson V.E. (2016). Bayesian variable selection for binary outcomes in high dimensional genomic studies using non-local priors. Bioinformatics, 32(9), 1338-45.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+result">result</a></code>, <code><a href="#topic+S5_parallel">S5_parallel</a></code>,  <code><a href="#topic+SSS">SSS</a></code>  

</p>


<h3>Examples</h3>

<pre><code class='language-R'>p0 = 5000
n0= 100

indx.beta = 1:5
xd0 = rep(0,p0);xd0[indx.beta]=1
bt0 = rep(0,p0); 
bt0[1:5]=c(1,1.25,1.5,1.75,2)*sample(c(1,-1),5,replace=TRUE)
xd=xd0
bt=bt0
X = matrix(rnorm(n0*p0),n0,p0)
y = crossprod(t(X),bt0) + rnorm(n0)*sqrt(1.5)
X = scale(X)
y = y-mean(y)
y = as.vector(y)

### default setting
#fit_default = S5(X,y)
#res_default = result(fit_default)
#print(res_default$hppm) # the MAP model 
#print(res_default$hppm.prob) # the posterior probability of the hppm 
#plot(res_default$marg.prob,ylim=c(0,1),ylab="marginal inclusion probability") 
# the marginal inclusion probability 

### Nonlocal prior (piMoM prior) by S5
#C0 = 1 # the number of repetitions of S5 algorithms to explore the model space
#tuning = hyper_par(type="pimom",X,y,thre = p^-0.5)  
# tuning parameter selection for nonlocal priors
#print(tuning) 

#ind_fun = ind_fun_pimom # the log-marginal likelihood of models based on piMoM prior
#model = Bernoulli_Uniform 
# the log-marginal likelihood of models based on piMoM prior 
#tem =  seq(0.4,1,length.out=20)^2 
# the temperatures schedule
#fit_pimom = S5(X,y,ind_fun=ind_fun,model=model,tuning=tuning,tem=tem,C0=C0)


#fit_pimom$GAM # the searched models by S5
#fit_pimom$OBJ # the corresponding log (unnormalized) posterior probability

#res_pimom = result(fit_pimom)
#str(res_pimom)
#print(res_pimom$hppm) # the MAP model 
#print(res_pimom$hppm.prob) 
# the posterior probability of the hppm 
#plot(res_pimom$marg.prob,ylim=c(0,1),ylab="marginal inclusion probability") 
# the marginal inclusion probability 


### Get the estimated regression coefficients from Bayesian Model Avaeraging (BMA)
#est.LS = result_est_LS(res_pimom,X,y) # Averged over the Least Square estimators of the models.
#est.MAP = result_est_MAP(res_pimom,X,y,obj_fun_pimom,verbose=TRUE) 
# Averged over the maximum posteriori (MAP) estimators of the models.
</code></pre>

<hr>
<h2 id='S5_additive'>
Simplified shotgun stochastic search algorithm with screening (S5) for additive models
</h2><span id='topic+S5_additive'></span>

<h3>Description</h3>

<p>This is the Simplified Shotgun Stochastic Search with Screening (S5) for high-dimensonal Bayesian variable selection under nonparameteric additive models, which is considered in &quot;Nonlocal Functional Priors for Nonparametric Hypothesis Testing and High-dimensional Model Selection&quot; by Shin and Bhattacharya (2020+). This function utilizes the inverse moment nonlocal functional prior, and see Shin and Bhattacharya (2020+) for details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>S5_additive(X, y, K=5, model, tuning = 0.5*nrow(X), tem, ITER = 20, S = 30, C0 = 5, 
verbose = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="S5_additive_+3A_x">X</code></td>
<td>

<p>the covariate matrix (a standardization is recommneded for nonlocal priors).
</p>
</td></tr>
<tr><td><code id="S5_additive_+3A_y">y</code></td>
<td>

<p>a response variable. 
</p>
</td></tr>
<tr><td><code id="S5_additive_+3A_k">K</code></td>
<td>

<p>the degree of freedom for the B-spline basis
</p>
</td></tr>
<tr><td><code id="S5_additive_+3A_model">model</code></td>
<td>

<p>a model prior; Uniform or Bernoulli_Uniform. The default is Bernoulli_Uniform
</p>
</td></tr>
<tr><td><code id="S5_additive_+3A_tuning">tuning</code></td>
<td>

<p>a tuning parameter for the objective function (tau for the inverse moment prior). The default is 0.5*n.
</p>
</td></tr>
<tr><td><code id="S5_additive_+3A_tem">tem</code></td>
<td>

<p>a temperature schedule. The default is seq(0.4,1,length.out=20)^-2.
</p>
</td></tr>
<tr><td><code id="S5_additive_+3A_iter">ITER</code></td>
<td>

<p>the number of iterations in each temperature; default is 20.
</p>
</td></tr>
<tr><td><code id="S5_additive_+3A_s">S</code></td>
<td>

<p>a screening size of variables; default is 30.
</p>
</td></tr>
<tr><td><code id="S5_additive_+3A_c0">C0</code></td>
<td>

<p>a number of repetition of  the S5 algorithm C0 times,default is 2. When the total number of variables is huge and real data sets are considered, using a large number of C0 is recommended, e.g., C0=5.  
</p>
</td></tr>
<tr><td><code id="S5_additive_+3A_verbose">verbose</code></td>
<td>

<p>if TRUE, the function prints the currnet status of the S5 in each temperature; the default is TRUE.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Using the S5 (Shin et al., 2018), you will get all the models searched by S5 algorithm, and their corresponding log (unnormalized) posterior probabilities, and also this function can receive searched model for g-prior,piMoM,and peMoM. 
</p>
<p>Unlike &quot;S5&quot; function that requires an extra step to get more information of the computation procedure, this function provides full information of the results. 
</p>


<h3>Value</h3>



<table>
<tr><td><code>GAM</code></td>
<td>
<p> the binary vaiables of searched models by S5</p>
</td></tr>
<tr><td><code>OBJ</code></td>
<td>
<p> the corresponding log (unnormalized) posterior probability</p>
</td></tr>
<tr><td><code>phi</code></td>
<td>
<p> the matrix of B-spline basis functions </p>
</td></tr>
<tr><td><code>Knots</code></td>
<td>
<p> the boundaries of knots used in generating the B-spline matrix </p>
</td></tr>
<tr><td><code>K</code></td>
<td>
<p> the degree of freedom of the B-spline basis.  </p>
</td></tr>
<tr><td><code>post</code></td>
<td>
<p>the corresponding (normalized) posterior model probabilities</p>
</td></tr>
<tr><td><code>marg.prob</code></td>
<td>
<p>the marginal inclusion probabilities </p>
</td></tr>
<tr><td><code>ind.MAP</code></td>
<td>
<p>the selected variables from the MAP model</p>
</td></tr>
<tr><td><code>ind.marg</code></td>
<td>
<p>the selected variables whose marginal inclusion probability is larger than 0.5</p>
</td></tr>
<tr><td><code>hppm.prob</code></td>
<td>
<p>the posterior probability of the MAP model</p>
</td></tr>
<tr><td><code>tuning</code></td>
<td>
<p>the tuning parameter used in the model selection</p>
</td></tr>

</table>


<h3>Author(s)</h3>

<p>Shin Minsuk and Ruoxuan Tian
</p>


<h3>References</h3>

<p>Shin, M. and  Bhattacharya, A.(2020) Nonlocal Functional Priors for Nonparametric Hypothesis Testing and High-dimensional  Model Selection. 
</p>
<p>Shin, M., Bhattacharya, A., Johnson V. E. (2018) A Scalable Bayesian Variable Selection Using Nonlocal Prior Densities in Ultrahigh-dimensional Settings, under revision in Statistica Sinica. 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+result">result</a></code>, <code><a href="#topic+S5_parallel">S5_parallel</a></code>,  <code><a href="#topic+SSS">SSS</a></code>  

</p>


<h3>Examples</h3>

<pre><code class='language-R'>p0 = 500
n0 = 200

X = matrix(runif(n0*p0,-2,2),n0,p0)
mu = X[,1]^2 + 2*sin(X[,2]*2) + 2*cos(X[,3]*2) + X[,4]
y = mu + rnorm(n0)
X = scale(X)
y = as.vector(y)

#fit_additive = S5_additive(X,y, tuning = 0.1*ncol(X))
#print(fit_additive$ind.hppm) # the MAP model 
#print(fit_additive$hppm.prob) # the posterior probability of the hppm 
#plot(fit_additive$marg.prob,ylim=c(0,1),ylab="marginal inclusion probability") 
# the marginal inclusion probability 
</code></pre>

<hr>
<h2 id='S5_parallel'>
Parallel version of S5
</h2><span id='topic+S5_parallel'></span>

<h3>Description</h3>

<p>The parallel version of the S5. Multiple S5 chains independently explore the model space to enhance the capacity of searching interesting region of the model space.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>S5_parallel(NC,X,y,ind_fun,model,tuning,tem,ITER=20,S=20,C0=2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="S5_parallel_+3A_nc">NC</code></td>
<td>

<p>a number of cores (the number of parallel S5 chains) to be used.
</p>
</td></tr>
<tr><td><code id="S5_parallel_+3A_x">X</code></td>
<td>

<p>a covariate matrix (a standardization is recommneded for nonlocal priors).
</p>
</td></tr>
<tr><td><code id="S5_parallel_+3A_y">y</code></td>
<td>

<p>a response variable. 
</p>
</td></tr>
<tr><td><code id="S5_parallel_+3A_ind_fun">ind_fun</code></td>
<td>

<p>a log-marginal likelihood function of models, which is resulted from a pred-specified priors on the regression coefficients. The default is piMoM
</p>
</td></tr>
<tr><td><code id="S5_parallel_+3A_model">model</code></td>
<td>

<p>a model prior; Uniform or Bernoulli_Uniform. The default is Bernoulli_Uniform
</p>
</td></tr>
<tr><td><code id="S5_parallel_+3A_tuning">tuning</code></td>
<td>

<p>a tuning parameter for the objective function (tau for piMoM and peMoM priors; g for the g-prior).
</p>
</td></tr>
<tr><td><code id="S5_parallel_+3A_tem">tem</code></td>
<td>

<p>a temperature schedule. The default is seq(0.4,1,length.out=20)^-2.
</p>
</td></tr>
<tr><td><code id="S5_parallel_+3A_iter">ITER</code></td>
<td>

<p>a number of iterations in each temperature; default is 20.
</p>
</td></tr>
<tr><td><code id="S5_parallel_+3A_s">S</code></td>
<td>

<p>a screening size of variables; default is 20.
</p>
</td></tr>
<tr><td><code id="S5_parallel_+3A_c0">C0</code></td>
<td>

<p>a number of repetition of  the S5 algorithm C0 times,default is 2. When the total number of variables is huge and real data sets are considered, using a large number of C0 is recommended, e.g., C0=10.  
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Using the S5 (Shin et al., 2016+), you will get all the models searched by S5 algorithm, and their corresponding log (unnormalized) posterior probabilities, and also this function can receive searched model for g-prior,piMoM,and peMoM. 
</p>
<p>After obtaining the object of the S5 function, by using the 'result' function, you can obtain the posterior probabilities of the searched models including the MAP model and  the marginal inclusion probabilities of each variable. 
</p>
<p>By using the procedure of Nikooienejad et al. (2016), the 'hyper_par' function chooses the tuning parameter for nonlocal priors (piMoM or peMoM priors).
</p>


<h3>Value</h3>



<table>
<tr><td><code>GAM</code></td>
<td>
<p> the binary vaiables of searched models by S5</p>
</td></tr>
<tr><td><code>OBJ</code></td>
<td>
<p> the corresponding log (unnormalized) posterior probability</p>
</td></tr>
<tr><td><code>tuning</code></td>
<td>
<p>the tuning parameter used in the model selection</p>
</td></tr>

</table>


<h3>Author(s)</h3>

<p>Shin Minsuk and Ruoxuan Tian
</p>


<h3>References</h3>

<p>Shin, M., Bhattacharya, A., Johnson V. E. (2016+) A Scalable Bayesian Variable Selection Using Nonlocal Prior Densities in Ultrahigh-dimensional Settings, under revision in Statistica Sinica. 
</p>
<p>Hans, C., Dobra, A., and West, M. (2007). Shotgun stochastic search for large p regression. Journal of the American Statistical Association, 102, 507-516.
</p>
<p>Nikooienejad,A., Wang, W., and Johnson V.E. (2016). Bayesian variable selection for binary outcomes in high dimensional genomic studies using non-local priors. Bioinformatics, 32(9), 1338-45.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+result">result</a></code>, <code><a href="#topic+S5">S5</a></code>  

</p>


<h3>Examples</h3>

<pre><code class='language-R'>p=5000
n = 100

indx.beta = 1:5
xd0 = rep(0,p);xd0[indx.beta]=1
bt0 = rep(0,p); 
bt0[1:5]=c(1,1.25,1.5,1.75,2)*sample(c(1,-1),5,replace=TRUE)
xd=xd0
bt=bt0
X = matrix(rnorm(n*p),n,p)
y = crossprod(t(X),bt0) + rnorm(n)*sqrt(1.5)
X = scale(X)
y = y-mean(y)
y = as.vector(y)

### parallel version of S5 (defalut)
#fit_parallel = S5_parallel(NC=2,X,y)
 

#fit_parallel$GAM # the searched models by S5
#fit_parallel$OBJ # the corresponding log (unnormalized) posterior probability

#res_parallel = result(fit_parallel)
#str(res_parallel)
#print(res_parallel$hppm) # the MAP model 
#print(res_parallel$hppm.prob) # the posterior probability of the hppm 
#plot(res_parallel$marg.prob,ylim=c(0,1),ylab="marginal inclusion probability") 
# the marginal inclusion probability 

### parallel version of S5 (temperature rescheduling)
#NC = 2 # the number of cores for the prallel computing
#C0 = 5 # the number of repetitions of S5 algorithms to explore the model space
#tuning = hyper_par(type="pimom",X,y,thre = p^-0.5)  
# tuning parameter selection for nonlocal priors
#print(tuning) 

#ind_fun = ind_fun_pimom 
#model = Bernoulli_Uniform 
# the log-marginal likelihood of models based on piMoM prior 
#('Uniform' or 'Bernoulli_Uniform').
#tem =  seq(0.4,1,length.out=20)^2 
# the temperatures schedule
#fit_parallel = S5_parallel(NC=2,X,y,ind_fun,model,tuning,tem,C0=C0)
 

#fit_parallel$GAM # the searched models by S5
#fit_parallel$OBJ # the corresponding log (unnormalized) posterior probability

#res_parallel = result(fit_parallel)
#str(res_parallel)
#print(res_parallel$hppm) # the MAP model 
#print(res_parallel$hppm.prob) # the posterior probability of the hppm 
#plot(res_parallel$marg.prob,ylim=c(0,1),ylab="marginal inclusion probability") 
# the marginal inclusion probability 
</code></pre>

<hr>
<h2 id='SSS'>
Shotgun stochastic search algorithm (SSS)
</h2><span id='topic+SSS'></span>

<h3>Description</h3>

<p>The Shotgun Stochastic Search (SSS) was proposed by Hans et al. (2007), which is a stochastic search algorithm for Bayesian variable selection.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SSS(X,y,ind_fun,model,tuning,N=1000,C0=1,verbose=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SSS_+3A_x">X</code></td>
<td>

<p>a covariate matrix (a standardization is recommneded for nonlocal priors).
</p>
</td></tr>
<tr><td><code id="SSS_+3A_y">y</code></td>
<td>

<p>a response variable. 
</p>
</td></tr>
<tr><td><code id="SSS_+3A_ind_fun">ind_fun</code></td>
<td>

<p>a log-marginal likelihood function of models, which is resulted from a pred-specified priors on the regression coefficients. The default is piMoM
</p>
</td></tr>
<tr><td><code id="SSS_+3A_model">model</code></td>
<td>

<p>a model prior; Uniform or Bernoulli_Uniform. The default is Bernoulli_Uniform
</p>
</td></tr>
<tr><td><code id="SSS_+3A_tuning">tuning</code></td>
<td>

<p>a tuning parameter for the objective function (tau for piMoM and peMoM priors; g for the g-prior).
</p>
</td></tr>
<tr><td><code id="SSS_+3A_n">N</code></td>
<td>

<p>a number of iterations of the SSS; default is 1000.
</p>
</td></tr>
<tr><td><code id="SSS_+3A_c0">C0</code></td>
<td>

<p>a number of repetition of  the S5 algorithm C0 times,default is 1. When the total number of variables is huge and real data sets are considered, using a large number of C0 is recommended, e.g., C0=10.  
</p>
</td></tr>
<tr><td><code id="SSS_+3A_verbose">verbose</code></td>
<td>

<p>if TRUE, the function prints the currnet status of the S5 in each temperature; the default is TRUE.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Using the S5 (Shin et al., 2016+), you will get all the models searched by S5 algorithm, and their corresponding log (unnormalized) posterior probabilities, and also this function can receive searched model for g-prior,piMoM,and peMoM. 
</p>
<p>After obtaining the object of the S5 function, by using the 'result' function, you can obtain the posterior probabilities of the searched models including the MAP model and  the marginal inclusion probabilities of each variable.
</p>
<p>By using the procedure of Nikooienejad et al. (2016), the 'hyper_par' function chooses the tuning parameter for nonlocal priors (piMoM or peMoM priors).
</p>


<h3>Value</h3>



<table>
<tr><td><code>GAM</code></td>
<td>
<p> the binary vaiables of searched models by S5</p>
</td></tr>
<tr><td><code>OBJ</code></td>
<td>
<p> the corresponding log (unnormalized) posterior probability</p>
</td></tr>
<tr><td><code>tuning</code></td>
<td>
<p>the tuning parameter used in the model selection</p>
</td></tr>

</table>


<h3>Author(s)</h3>

<p>Shin Minsuk and Ruoxuan Tian
</p>


<h3>References</h3>

<p>Hans, C., Dobra, A., and West, M. (2007). Shotgun stochastic search for large p regression. Journal of the American Statistical Association, 102, 507-516.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+result">result</a></code>, <code><a href="#topic+S5_parallel">S5_parallel</a></code>, <code><a href="#topic+S5">S5</a></code>   

</p>


<h3>Examples</h3>

<pre><code class='language-R'>p=100
n = 200

indx.beta = 1:5
xd0 = rep(0,p);xd0[indx.beta]=1
bt0 = rep(0,p); 
bt0[1:5]=c(1,1.25,1.5,1.75,2)*sample(c(1,-1),5,replace=TRUE)
xd=xd0
bt=bt0
X = matrix(rnorm(n*p),n,p)
y = crossprod(t(X),bt0) + rnorm(n)*sqrt(1.5)
X = scale(X)
y = y-mean(y)
y = as.vector(y)

### default setting
#fit_de_SSS = SSS(X,y)

#res_de_SSS = result(fit_de_SSS)
#print(res_de_SSS$hppm) # the MAP model 
#print(res_de_SSS$hppm.prob) # the posterior probability of the hppm 
#plot(res_de_SSS$marg.prob,ylim=c(0,1),ylab="marginal inclusion probability")
 # the marginal inclusion probability 
</code></pre>

<hr>
<h2 id='Uniform'>
Uniform model prior
</h2><span id='topic+Uniform'></span>

<h3>Description</h3>

<p>A uniform model prior that assigns the same prior mass on each model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Uniform(ind,p)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Uniform_+3A_ind">ind</code></td>
<td>

<p>the index set of variables in a model
</p>
</td></tr>
<tr><td><code id="Uniform_+3A_p">p</code></td>
<td>

<p>the total number of covariates
</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>ind = 1:3 
m = Uniform(ind,p)
print(m)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
