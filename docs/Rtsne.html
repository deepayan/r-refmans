<!DOCTYPE html><html><head><title>Help for package Rtsne</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {Rtsne}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#normalize_input'><p>Normalize input data matrix</p></a></li>
<li><a href='#Rtsne'><p>Barnes-Hut implementation of t-Distributed Stochastic Neighbor Embedding</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>T-Distributed Stochastic Neighbor Embedding using a Barnes-Hut
Implementation</td>
</tr>
<tr>
<td>Version:</td>
<td>0.17</td>
</tr>
<tr>
<td>Description:</td>
<td>An R wrapper around the fast T-distributed Stochastic
    Neighbor Embedding implementation by Van der Maaten  (see <a href="https://github.com/lvdmaaten/bhtsne/">https://github.com/lvdmaaten/bhtsne/</a> for more information on the original implementation).</td>
</tr>
<tr>
<td>License:</td>
<td>file LICENSE</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/jkrijthe/Rtsne">https://github.com/jkrijthe/Rtsne</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp (&ge; 0.11.0), stats</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp</td>
</tr>
<tr>
<td>Suggests:</td>
<td>irlba, testthat</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-12-07 01:33:30 UTC; jkrijthe</td>
</tr>
<tr>
<td>Author:</td>
<td>Jesse Krijthe [aut, cre],
  Laurens van der Maaten [cph] (Author of original C++ code)</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Jesse Krijthe &lt;jkrijthe@gmail.com&gt;</td>
</tr>
<tr>
<td>License_is_FOSS:</td>
<td>yes</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-12-07 11:50:11 UTC</td>
</tr>
</table>
<hr>
<h2 id='normalize_input'>Normalize input data matrix</h2><span id='topic+normalize_input'></span>

<h3>Description</h3>

<p>Mean centers each column of an input data matrix so that it has a mean of zero.
Scales the entire matrix so that the largest absolute of the centered matrix is equal to unity.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>normalize_input(X)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="normalize_input_+3A_x">X</code></td>
<td>
<p>matrix; Input data matrix with rows as observations and columns as variables/dimensions.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Normalization avoids numerical problems when the coordinates (and thus the distances between observations) are very large.
Directly computing distances on this scale may lead to underflow when computing the probabilities in the t-SNE algorithm.
Rescaling the input values mitigates these problems to some extent.
</p>


<h3>Value</h3>

<p>A numeric matrix of the same dimensions as <code>X</code> but centred by column and scaled to have a maximum deviation of 1.
</p>


<h3>Author(s)</h3>

<p>Aaron Lun
</p>


<h3>Examples</h3>

<pre><code class='language-R'>iris_unique &lt;- unique(iris) # Remove duplicates
iris_matrix &lt;- as.matrix(iris_unique[,1:4])
X &lt;- normalize_input(iris_matrix)
colMeans(X)
range(X)
</code></pre>

<hr>
<h2 id='Rtsne'>Barnes-Hut implementation of t-Distributed Stochastic Neighbor Embedding</h2><span id='topic+Rtsne'></span><span id='topic+Rtsne.default'></span><span id='topic+Rtsne.dist'></span><span id='topic+Rtsne.data.frame'></span><span id='topic+Rtsne_neighbors'></span>

<h3>Description</h3>

<p>Wrapper for the C++ implementation of Barnes-Hut t-Distributed Stochastic Neighbor Embedding. t-SNE is a method for constructing a low dimensional embedding of high-dimensional data, distances or similarities. Exact t-SNE can be computed by setting theta=0.0.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Rtsne(X, ...)

## Default S3 method:
Rtsne(
  X,
  dims = 2,
  initial_dims = 50,
  perplexity = 30,
  theta = 0.5,
  check_duplicates = TRUE,
  pca = TRUE,
  partial_pca = FALSE,
  max_iter = 1000,
  verbose = getOption("verbose", FALSE),
  is_distance = FALSE,
  Y_init = NULL,
  pca_center = TRUE,
  pca_scale = FALSE,
  normalize = TRUE,
  stop_lying_iter = ifelse(is.null(Y_init), 250L, 0L),
  mom_switch_iter = ifelse(is.null(Y_init), 250L, 0L),
  momentum = 0.5,
  final_momentum = 0.8,
  eta = 200,
  exaggeration_factor = 12,
  num_threads = 1,
  ...
)

## S3 method for class 'dist'
Rtsne(X, ..., is_distance = TRUE)

## S3 method for class 'data.frame'
Rtsne(X, ...)

Rtsne_neighbors(
  index,
  distance,
  dims = 2,
  perplexity = 30,
  theta = 0.5,
  max_iter = 1000,
  verbose = getOption("verbose", FALSE),
  Y_init = NULL,
  stop_lying_iter = ifelse(is.null(Y_init), 250L, 0L),
  mom_switch_iter = ifelse(is.null(Y_init), 250L, 0L),
  momentum = 0.5,
  final_momentum = 0.8,
  eta = 200,
  exaggeration_factor = 12,
  num_threads = 1,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Rtsne_+3A_x">X</code></td>
<td>
<p>matrix; Data matrix (each row is an observation, each column is a variable)</p>
</td></tr>
<tr><td><code id="Rtsne_+3A_...">...</code></td>
<td>
<p>Other arguments that can be passed to Rtsne</p>
</td></tr>
<tr><td><code id="Rtsne_+3A_dims">dims</code></td>
<td>
<p>integer; Output dimensionality (default: 2)</p>
</td></tr>
<tr><td><code id="Rtsne_+3A_initial_dims">initial_dims</code></td>
<td>
<p>integer; the number of dimensions that should be retained in the initial PCA step (default: 50)</p>
</td></tr>
<tr><td><code id="Rtsne_+3A_perplexity">perplexity</code></td>
<td>
<p>numeric; Perplexity parameter (should not be bigger than 3 * perplexity &lt; nrow(X) - 1, see details for interpretation)</p>
</td></tr>
<tr><td><code id="Rtsne_+3A_theta">theta</code></td>
<td>
<p>numeric; Speed/accuracy trade-off (increase for less accuracy), set to 0.0 for exact TSNE (default: 0.5)</p>
</td></tr>
<tr><td><code id="Rtsne_+3A_check_duplicates">check_duplicates</code></td>
<td>
<p>logical; Checks whether duplicates are present. It is best to make sure there are no duplicates present and set this option to FALSE, especially for large datasets (default: TRUE)</p>
</td></tr>
<tr><td><code id="Rtsne_+3A_pca">pca</code></td>
<td>
<p>logical; Whether an initial PCA step should be performed (default: TRUE)</p>
</td></tr>
<tr><td><code id="Rtsne_+3A_partial_pca">partial_pca</code></td>
<td>
<p>logical; Whether truncated PCA should be used to calculate principal components (requires the irlba package). This is faster for large input matrices (default: FALSE)</p>
</td></tr>
<tr><td><code id="Rtsne_+3A_max_iter">max_iter</code></td>
<td>
<p>integer; Number of iterations (default: 1000)</p>
</td></tr>
<tr><td><code id="Rtsne_+3A_verbose">verbose</code></td>
<td>
<p>logical; Whether progress updates should be printed (default: global &quot;verbose&quot; option, or FALSE if that is not set)</p>
</td></tr>
<tr><td><code id="Rtsne_+3A_is_distance">is_distance</code></td>
<td>
<p>logical; Indicate whether X is a distance matrix (default: FALSE)</p>
</td></tr>
<tr><td><code id="Rtsne_+3A_y_init">Y_init</code></td>
<td>
<p>matrix; Initial locations of the objects. If NULL, random initialization will be used (default: NULL). Note that when using this, the initial stage with exaggerated perplexity values and a larger momentum term will be skipped.</p>
</td></tr>
<tr><td><code id="Rtsne_+3A_pca_center">pca_center</code></td>
<td>
<p>logical; Should data be centered before pca is applied? (default: TRUE)</p>
</td></tr>
<tr><td><code id="Rtsne_+3A_pca_scale">pca_scale</code></td>
<td>
<p>logical; Should data be scaled before pca is applied? (default: FALSE)</p>
</td></tr>
<tr><td><code id="Rtsne_+3A_normalize">normalize</code></td>
<td>
<p>logical; Should data be normalized internally prior to distance calculations with <code><a href="#topic+normalize_input">normalize_input</a></code>? (default: TRUE)</p>
</td></tr>
<tr><td><code id="Rtsne_+3A_stop_lying_iter">stop_lying_iter</code></td>
<td>
<p>integer; Iteration after which the perplexities are no longer exaggerated (default: 250, except when Y_init is used, then 0)</p>
</td></tr>
<tr><td><code id="Rtsne_+3A_mom_switch_iter">mom_switch_iter</code></td>
<td>
<p>integer; Iteration after which the final momentum is used (default: 250, except when Y_init is used, then 0)</p>
</td></tr>
<tr><td><code id="Rtsne_+3A_momentum">momentum</code></td>
<td>
<p>numeric; Momentum used in the first part of the optimization (default: 0.5)</p>
</td></tr>
<tr><td><code id="Rtsne_+3A_final_momentum">final_momentum</code></td>
<td>
<p>numeric; Momentum used in the final part of the optimization (default: 0.8)</p>
</td></tr>
<tr><td><code id="Rtsne_+3A_eta">eta</code></td>
<td>
<p>numeric; Learning rate (default: 200.0)</p>
</td></tr>
<tr><td><code id="Rtsne_+3A_exaggeration_factor">exaggeration_factor</code></td>
<td>
<p>numeric; Exaggeration factor used to multiply the P matrix in the first part of the optimization (default: 12.0)</p>
</td></tr>
<tr><td><code id="Rtsne_+3A_num_threads">num_threads</code></td>
<td>
<p>integer; Number of threads to use when using OpenMP, default is 1. Setting to 0 corresponds to detecting and using all available cores</p>
</td></tr>
<tr><td><code id="Rtsne_+3A_index">index</code></td>
<td>
<p>integer matrix; Each row contains the identity of the nearest neighbors for each observation</p>
</td></tr>
<tr><td><code id="Rtsne_+3A_distance">distance</code></td>
<td>
<p>numeric matrix; Each row contains the distance to the nearest neighbors in <code>index</code> for each observation</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Given a distance matrix <code class="reqn">D</code> between input objects (which by default, is the euclidean distances between two objects), we calculate a similarity score in the original space: </p>
<p style="text-align: center;"><code class="reqn"> p_{j | i} = \frac{\exp(-\|D_{ij}\|^2 / 2 \sigma_i^2)}{\sum_{k \neq i} \exp(-\|D_{ij}\|^2 / 2 \sigma_i^2)} </code>
</p>
<p> which is then symmetrized using: </p>
<p style="text-align: center;"><code class="reqn"> p_{i j}=\frac{p_{j|i} + p_{i|j}}{2n}.</code>
</p>
<p> The <code class="reqn">\sigma</code> for each object is chosen in such a way that the perplexity of <code class="reqn">p_{j|i}</code> has a value that is close to the user defined perplexity. This value effectively controls how many nearest neighbours are taken into account when constructing the embedding in the low-dimensional space.
For the low-dimensional space we use the Cauchy distribution (t-distribution with one degree of freedom) as the distribution of the distances to neighbouring objects:
</p>
<p style="text-align: center;"><code class="reqn"> q_{i j} = \frac{(1+ \| y_i-y_j\|^2)^{-1}}{\sum_{k \neq l} 1+ \| y_k-y_l\|^2)^{-1}}.</code>
</p>
 
<p>By changing the location of the objects y in the embedding to minimize the Kullback-Leibler divergence between these two distributions <code class="reqn"> q_{i j}</code> and <code class="reqn"> p_{i j}</code>, we create a map that focusses on small-scale structure, due to the asymmetry of the KL-divergence. The t-distribution is chosen to avoid the crowding problem: in the original high dimensional space, there are potentially many equidistant objects with moderate distance from a particular object, more than can be accounted for in the low dimensional representation. The t-distribution makes sure that these objects are more spread out in the new representation.
</p>
<p>For larger datasets, a problem with the a simple gradient descent to minimize the Kullback-Leibler divergence is the computational complexity of each gradient step (which is <code class="reqn">O(n^2)</code>). The Barnes-Hut implementation of the algorithm attempts to mitigate this problem using two tricks: (1) approximating small similarities by 0 in the <code class="reqn">p_{ij}</code> distribution, where the non-zero entries are computed by finding 3*perplexity nearest neighbours using an efficient tree search. (2) Using the Barnes-Hut algorithm in the computation of the gradient which approximates large distance similarities using a quadtree. This approximation is controlled by the <code>theta</code> parameter, with smaller values leading to more exact approximations. When <code>theta=0.0</code>, the implementation uses a standard t-SNE implementation. The Barnes-Hut approximation leads to a <code class="reqn">O(n log(n))</code> computational complexity for each iteration.
</p>
<p>During the minimization of the KL-divergence, the implementation uses a trick known as early exaggeration, which multiplies the <code class="reqn">p_{ij}</code>'s by 12 during the first 250 iterations. This leads to tighter clustering and more distance between clusters of objects. This early exaggeration is not used when the user gives an initialization of the objects in the embedding by setting <code>Y_init</code>. During the early exaggeration phase, a momentum term of 0.5 is used while this is changed to 0.8 after the first 250 iterations. All these default parameters can be changed by the user.
</p>
<p>After checking the correctness of the input, the <code>Rtsne</code> function (optionally) does an initial reduction of the feature space using <code><a href="stats.html#topic+prcomp">prcomp</a></code>, before calling the C++ TSNE implementation. Since R's random number generator is used, use <code><a href="base.html#topic+set.seed">set.seed</a></code> before the function call to get reproducible results.
</p>
<p>If <code>X</code> is a data.frame, it is transformed into a matrix using <code><a href="stats.html#topic+model.matrix">model.matrix</a></code>. If <code>X</code> is a <code><a href="stats.html#topic+dist">dist</a></code> object, it is currently first expanded into a full distance matrix.
</p>


<h3>Value</h3>

<p>List with the following elements:
</p>
<table>
<tr><td><code>Y</code></td>
<td>
<p>Matrix containing the new representations for the objects</p>
</td></tr>
<tr><td><code>N</code></td>
<td>
<p>Number of objects</p>
</td></tr>
<tr><td><code>origD</code></td>
<td>
<p>Original Dimensionality before TSNE (only when <code>X</code> is a data matrix)</p>
</td></tr>
<tr><td><code>perplexity</code></td>
<td>
<p>See above</p>
</td></tr>
<tr><td><code>theta</code></td>
<td>
<p>See above</p>
</td></tr>
<tr><td><code>costs</code></td>
<td>
<p>The cost for every object after the final iteration</p>
</td></tr>
<tr><td><code>itercosts</code></td>
<td>
<p>The total costs (KL-divergence) for all objects in every 50th + the last iteration</p>
</td></tr>
<tr><td><code>stop_lying_iter</code></td>
<td>
<p>Iteration after which the perplexities are no longer exaggerated</p>
</td></tr>
<tr><td><code>mom_switch_iter</code></td>
<td>
<p>Iteration after which the final momentum is used</p>
</td></tr>
<tr><td><code>momentum</code></td>
<td>
<p>Momentum used in the first part of the optimization</p>
</td></tr>
<tr><td><code>final_momentum</code></td>
<td>
<p>Momentum used in the final part of the optimization</p>
</td></tr>
<tr><td><code>eta</code></td>
<td>
<p>Learning rate</p>
</td></tr>
<tr><td><code>exaggeration_factor</code></td>
<td>
<p>Exaggeration factor used to multiply the P matrix in the first part of the optimization</p>
</td></tr>
</table>


<h3>Methods (by class)</h3>


<ul>
<li> <p><code>Rtsne(default)</code>: Default Interface
</p>
</li>
<li> <p><code>Rtsne(dist)</code>: tsne on given dist object
</p>
</li>
<li> <p><code>Rtsne(data.frame)</code>: tsne on data.frame
</p>
</li></ul>


<h3>Supplying precomputed distances</h3>

<p>If a distance matrix is already available, this can be directly supplied to <code>Rtsne</code> by setting <code>is_distance=TRUE</code>.
This improves efficiency by avoiding recalculation of distances, but requires some work to get the same results as running default <code>Rtsne</code> on a data matrix.
Specifically, Euclidean distances should be computed from a normalized data matrix - see <code><a href="#topic+normalize_input">normalize_input</a></code> for details.
PCA arguments will also be ignored if <code>is_distance=TRUE</code>.
</p>
<p>NN search results can be directly supplied to <code>Rtsne_neighbors</code> to avoid repeating the (possibly time-consuming) search.
To achieve the same results as <code>Rtsne</code> on the data matrix, the search should be conducted on the normalized data matrix.
The number of nearest neighbors should also be equal to three-fold the <code>perplexity</code>, rounded down to the nearest integer.
Note that pre-supplied NN results cannot be used when <code>theta=0</code> as they are only relevant for the approximate algorithm.
</p>
<p>Any kind of distance metric can be used as input.
In contrast, running <code>Rtsne</code> on a data matrix will always use Euclidean distances.
</p>


<h3>References</h3>

<p>Maaten, L. Van Der, 2014. Accelerating t-SNE using Tree-Based Algorithms. Journal of Machine Learning Research, 15, p.3221-3245.
</p>
<p>van der Maaten, L.J.P. &amp; Hinton, G.E., 2008. Visualizing High-Dimensional Data Using t-SNE. Journal of Machine Learning Research, 9, pp.2579-2605.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>iris_unique &lt;- unique(iris) # Remove duplicates
iris_matrix &lt;- as.matrix(iris_unique[,1:4])

# Set a seed if you want reproducible results
set.seed(42)
tsne_out &lt;- Rtsne(iris_matrix,pca=FALSE,perplexity=30,theta=0.0) # Run TSNE

# Show the objects in the 2D tsne representation
plot(tsne_out$Y,col=iris_unique$Species, asp=1)

# data.frame as input
tsne_out &lt;- Rtsne(iris_unique,pca=FALSE, theta=0.0)

# Using a dist object
set.seed(42)
tsne_out &lt;- Rtsne(dist(normalize_input(iris_matrix)), theta=0.0)
plot(tsne_out$Y,col=iris_unique$Species, asp=1)

set.seed(42)
tsne_out &lt;- Rtsne(as.matrix(dist(normalize_input(iris_matrix))),theta=0.0)
plot(tsne_out$Y,col=iris_unique$Species, asp=1)

# Supplying starting positions (example: continue from earlier embedding)
set.seed(42)
tsne_part1 &lt;- Rtsne(iris_unique[,1:4], theta=0.0, pca=FALSE, max_iter=350)
tsne_part2 &lt;- Rtsne(iris_unique[,1:4], theta=0.0, pca=FALSE, max_iter=650, Y_init=tsne_part1$Y)
plot(tsne_part2$Y,col=iris_unique$Species, asp=1)
## Not run: 
# Fast PCA and multicore

tsne_out &lt;- Rtsne(iris_matrix, theta=0.1, partial_pca = TRUE, initial_dims=3)
tsne_out &lt;- Rtsne(iris_matrix, theta=0.1, num_threads = 2)

## End(Not run)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
