<!DOCTYPE html><html lang="en"><head><title>Help for package wordpiece</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {wordpiece}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#.get_casedness'><p>Determine Casedness of Vocabulary</p></a></li>
<li><a href='#.infer_case_from_vocab'><p>Determine Vocabulary Casedness</p></a></li>
<li><a href='#.new_wordpiece_vocabulary'><p>Constructor for Class wordpiece_vocabulary</p></a></li>
<li><a href='#.process_vocab'><p>Process a Vocabulary for Tokenization</p></a></li>
<li><a href='#.process_wp_vocab'><p>Process a Wordpiece Vocabulary for Tokenization</p></a></li>
<li><a href='#.validate_wordpiece_vocabulary'><p>Validator for Objects of Class wordpiece_vocabulary</p></a></li>
<li><a href='#.wp_tokenize_single_string'><p>Tokenize an Input Word-by-word</p></a></li>
<li><a href='#.wp_tokenize_word'><p>Tokenize a Word</p></a></li>
<li><a href='#load_or_retrieve_vocab'><p>Load a vocabulary file, or retrieve from cache</p></a></li>
<li><a href='#load_vocab'><p>Load a vocabulary file</p></a></li>
<li><a href='#prepare_vocab'><p>Format a Token List as a Vocabulary</p></a></li>
<li><a href='#reexports'><p>Objects exported from other packages</p></a></li>
<li><a href='#set_wordpiece_cache_dir'><p>Set a Cache Directory for wordpiece</p></a></li>
<li><a href='#wordpiece_cache_dir'><p>Retrieve Directory for wordpiece Cache</p></a></li>
<li><a href='#wordpiece_tokenize'><p>Tokenize Sequence with Word Pieces</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>R Implementation of Wordpiece Tokenization</td>
</tr>
<tr>
<td>Version:</td>
<td>2.1.3</td>
</tr>
<tr>
<td>Description:</td>
<td>Apply 'Wordpiece' (&lt;<a href="https://doi.org/10.48550/arXiv.1609.08144">doi:10.48550/arXiv.1609.08144</a>&gt;) tokenization to input text, 
 given an appropriate vocabulary. The 'BERT' (&lt;<a href="https://doi.org/10.48550/arXiv.1810.04805">doi:10.48550/arXiv.1810.04805</a>&gt;) tokenization 
 conventions are used by default.</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/macmillancontentscience/wordpiece">https://github.com/macmillancontentscience/wordpiece</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/macmillancontentscience/wordpiece/issues">https://github.com/macmillancontentscience/wordpiece/issues</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.3.0)</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.apache.org/licenses/LICENSE-2.0">Apache License (&ge; 2)</a></td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.2</td>
</tr>
<tr>
<td>Imports:</td>
<td>dlr (&ge; 1.0.0), fastmatch (&ge; 1.1), memoise (&ge; 2.0.0),
piecemaker (&ge; 1.0.0), rlang, stringi (&ge; 1.0), wordpiece.data
(&ge; 1.0.2)</td>
</tr>
<tr>
<td>Suggests:</td>
<td>covr, knitr, rmarkdown, testthat (&ge; 3.0.0)</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-03-03 14:19:39 UTC; jonathan.bratt</td>
</tr>
<tr>
<td>Author:</td>
<td>Jonathan Bratt <a href="https://orcid.org/0000-0003-2859-0076"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre],
  Jon Harmon <a href="https://orcid.org/0000-0003-4781-4346"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut],
  Bedford Freeman &amp; Worth Pub Grp LLC DBA Macmillan Learning [cph]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Jonathan Bratt &lt;jonathan.bratt@macmillan.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-03-03 15:10:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='.get_casedness'>Determine Casedness of Vocabulary</h2><span id='topic+.get_casedness'></span><span id='topic+.get_casedness.default'></span><span id='topic+.get_casedness.wordpiece_vocabulary'></span><span id='topic+.get_casedness.character'></span>

<h3>Description</h3>

<p>Determine Casedness of Vocabulary
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.get_casedness(v)

## Default S3 method:
.get_casedness(v)

## S3 method for class 'wordpiece_vocabulary'
.get_casedness(v)

## S3 method for class 'character'
.get_casedness(v)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id=".get_casedness_+3A_v">v</code></td>
<td>
<p>An object of class <code>wordpiece_vocabulary</code>, or a character vector.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>TRUE if the vocabulary is case-sensitive, FALSE otherwise.
</p>

<hr>
<h2 id='.infer_case_from_vocab'>Determine Vocabulary Casedness</h2><span id='topic+.infer_case_from_vocab'></span>

<h3>Description</h3>

<p>Determine whether or not a wordpiece vocabulary is case-sensitive.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.infer_case_from_vocab(vocab)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id=".infer_case_from_vocab_+3A_vocab">vocab</code></td>
<td>
<p>The vocabulary as a character vector.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If none of the tokens in the vocabulary start with a capital letter, it will
be assumed to be uncased. Note that tokens like &quot;\[CLS\]&quot; contain uppercase
letters, but don't start with uppercase letters.
</p>


<h3>Value</h3>

<p>TRUE if the vocabulary is cased, FALSE if uncased.
</p>

<hr>
<h2 id='.new_wordpiece_vocabulary'>Constructor for Class wordpiece_vocabulary</h2><span id='topic+.new_wordpiece_vocabulary'></span>

<h3>Description</h3>

<p>Constructor for Class wordpiece_vocabulary
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.new_wordpiece_vocabulary(vocab, is_cased)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id=".new_wordpiece_vocabulary_+3A_vocab">vocab</code></td>
<td>
<p>Character vector of tokens.</p>
</td></tr>
<tr><td><code id=".new_wordpiece_vocabulary_+3A_is_cased">is_cased</code></td>
<td>
<p>Logical; whether the vocabulary is cased.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The vocabulary with <code>is_cased</code> attached as an attribute, and the
class <code>wordpiece_vocabulary</code> applied.
</p>

<hr>
<h2 id='.process_vocab'>Process a Vocabulary for Tokenization</h2><span id='topic+.process_vocab'></span><span id='topic+.process_vocab.default'></span><span id='topic+.process_vocab.wordpiece_vocabulary'></span><span id='topic+.process_vocab.character'></span>

<h3>Description</h3>

<p>Process a Vocabulary for Tokenization
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.process_vocab(v)

## Default S3 method:
.process_vocab(v)

## S3 method for class 'wordpiece_vocabulary'
.process_vocab(v)

## S3 method for class 'character'
.process_vocab(v)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id=".process_vocab_+3A_v">v</code></td>
<td>
<p>An object of class <code>wordpiece_vocabulary</code> or a character vector.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A character vector of tokens for tokenization.
</p>

<hr>
<h2 id='.process_wp_vocab'>Process a Wordpiece Vocabulary for Tokenization</h2><span id='topic+.process_wp_vocab'></span><span id='topic+.process_wp_vocab.default'></span><span id='topic+.process_wp_vocab.wordpiece_vocabulary'></span><span id='topic+.process_wp_vocab.integer'></span><span id='topic+.process_wp_vocab.character'></span>

<h3>Description</h3>

<p>Process a Wordpiece Vocabulary for Tokenization
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.process_wp_vocab(v)

## Default S3 method:
.process_wp_vocab(v)

## S3 method for class 'wordpiece_vocabulary'
.process_wp_vocab(v)

## S3 method for class 'integer'
.process_wp_vocab(v)

## S3 method for class 'character'
.process_wp_vocab(v)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id=".process_wp_vocab_+3A_v">v</code></td>
<td>
<p>An object of class <code>wordpiece_vocabulary</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A character vector of tokens for tokenization.
</p>

<hr>
<h2 id='.validate_wordpiece_vocabulary'>Validator for Objects of Class wordpiece_vocabulary</h2><span id='topic+.validate_wordpiece_vocabulary'></span>

<h3>Description</h3>

<p>Validator for Objects of Class wordpiece_vocabulary
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.validate_wordpiece_vocabulary(vocab)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id=".validate_wordpiece_vocabulary_+3A_vocab">vocab</code></td>
<td>
<p>wordpiece_vocabulary object to validate</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>vocab</code> if the object passes the checks. Otherwise, abort with
message.
</p>

<hr>
<h2 id='.wp_tokenize_single_string'>Tokenize an Input Word-by-word</h2><span id='topic+.wp_tokenize_single_string'></span>

<h3>Description</h3>

<p>Tokenize an Input Word-by-word
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.wp_tokenize_single_string(words, vocab, unk_token, max_chars)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id=".wp_tokenize_single_string_+3A_words">words</code></td>
<td>
<p>Character; a vector of words (generated by space-tokenizing a
single input).</p>
</td></tr>
<tr><td><code id=".wp_tokenize_single_string_+3A_vocab">vocab</code></td>
<td>
<p>Character vector of vocabulary tokens. The tokens are assumed to
be in order of index, with the first index taken as zero to be compatible
with Python implementations.</p>
</td></tr>
<tr><td><code id=".wp_tokenize_single_string_+3A_unk_token">unk_token</code></td>
<td>
<p>Token to represent unknown words.</p>
</td></tr>
<tr><td><code id=".wp_tokenize_single_string_+3A_max_chars">max_chars</code></td>
<td>
<p>Maximum length of word recognized.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A named integer vector of tokenized words.
</p>

<hr>
<h2 id='.wp_tokenize_word'>Tokenize a Word</h2><span id='topic+.wp_tokenize_word'></span>

<h3>Description</h3>

<p>Tokenize a single &quot;word&quot; (no whitespace). The word can technically contain
punctuation, but in BERT's tokenization, punctuation has been split out by
this point.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.wp_tokenize_word(word, vocab, unk_token = "[UNK]", max_chars = 100)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id=".wp_tokenize_word_+3A_word">word</code></td>
<td>
<p>Word to tokenize.</p>
</td></tr>
<tr><td><code id=".wp_tokenize_word_+3A_vocab">vocab</code></td>
<td>
<p>Character vector of vocabulary tokens. The tokens are assumed to
be in order of index, with the first index taken as zero to be compatible
with Python implementations.</p>
</td></tr>
<tr><td><code id=".wp_tokenize_word_+3A_unk_token">unk_token</code></td>
<td>
<p>Token to represent unknown words.</p>
</td></tr>
<tr><td><code id=".wp_tokenize_word_+3A_max_chars">max_chars</code></td>
<td>
<p>Maximum length of word recognized.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Input word as a list of tokens.
</p>

<hr>
<h2 id='load_or_retrieve_vocab'>Load a vocabulary file, or retrieve from cache</h2><span id='topic+load_or_retrieve_vocab'></span>

<h3>Description</h3>

<p>Load a vocabulary file, or retrieve from cache
</p>


<h3>Usage</h3>

<pre><code class='language-R'>load_or_retrieve_vocab(vocab_file)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="load_or_retrieve_vocab_+3A_vocab_file">vocab_file</code></td>
<td>
<p>path to vocabulary file. File is assumed to be a text file,
with one token per line, with the line number corresponding to the index of
that token in the vocabulary.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The vocab as a character vector of tokens. The casedness of the
vocabulary is inferred and attached as the &quot;is_cased&quot; attribute. The
vocabulary indices are taken to be the positions of the tokens,
<em>starting at zero</em> for historical consistency.
</p>
<p>Note that from the perspective of a neural net, the numeric indices <em>are</em>
the tokens, and the mapping from token to index is fixed. If we changed the
indexing (the order of the tokens), it would break any pre-trained models.
</p>

<hr>
<h2 id='load_vocab'>Load a vocabulary file</h2><span id='topic+load_vocab'></span>

<h3>Description</h3>

<p>Load a vocabulary file
</p>


<h3>Usage</h3>

<pre><code class='language-R'>load_vocab(vocab_file)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="load_vocab_+3A_vocab_file">vocab_file</code></td>
<td>
<p>path to vocabulary file. File is assumed to be a text file,
with one token per line, with the line number corresponding to the index of
that token in the vocabulary.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The vocab as a character vector of tokens. The casedness of the
vocabulary is inferred and attached as the &quot;is_cased&quot; attribute. The
vocabulary indices are taken to be the positions of the tokens,
<em>starting at zero</em> for historical consistency.
</p>
<p>Note that from the perspective of a neural net, the numeric indices <em>are</em>
the tokens, and the mapping from token to index is fixed. If we changed the
indexing (the order of the tokens), it would break any pre-trained models.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Get path to sample vocabulary included with package.
vocab_path &lt;- system.file("extdata", "tiny_vocab.txt", package = "wordpiece")
vocab &lt;- load_vocab(vocab_file = vocab_path)
</code></pre>

<hr>
<h2 id='prepare_vocab'>Format a Token List as a Vocabulary</h2><span id='topic+prepare_vocab'></span>

<h3>Description</h3>

<p>We use a special named integer vector with class wordpiece_vocabulary to
provide information about tokens used in <code><a href="#topic+wordpiece_tokenize">wordpiece_tokenize</a></code>.
This function takes a character vector of tokens and puts it into that
format.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prepare_vocab(token_list)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="prepare_vocab_+3A_token_list">token_list</code></td>
<td>
<p>A character vector of tokens.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The vocab as a character vector of tokens. The casedness of the
vocabulary is inferred and attached as the &quot;is_cased&quot; attribute. The
vocabulary indices are taken to be the positions of the tokens,
<em>starting at zero</em> for historical consistency.
</p>
<p>Note that from the perspective of a neural net, the numeric indices <em>are</em>
the tokens, and the mapping from token to index is fixed. If we changed the
indexing (the order of the tokens), it would break any pre-trained models.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>my_vocab &lt;- prepare_vocab(c("some", "example", "tokens"))
class(my_vocab)
attr(my_vocab, "is_cased")
</code></pre>

<hr>
<h2 id='reexports'>Objects exported from other packages</h2><span id='topic+reexports'></span><span id='topic++25+7C+7C+25'></span><span id='topic++25fin+25'></span><span id='topic+wordpiece_vocab'></span>

<h3>Description</h3>

<p>These objects are imported from other packages. Follow the links
below to see their documentation.
</p>

<dl>
<dt>fastmatch</dt><dd><p><code><a href="fastmatch.html#topic+fmatch">%fin%</a></code></p>
</dd>
<dt>rlang</dt><dd><p><code><a href="rlang.html#topic+op-null-default">%||%</a></code></p>
</dd>
<dt>wordpiece.data</dt><dd><p><code><a href="wordpiece.data.html#topic+wordpiece_vocab">wordpiece_vocab</a></code></p>
</dd>
</dl>

<hr>
<h2 id='set_wordpiece_cache_dir'>Set a Cache Directory for wordpiece</h2><span id='topic+set_wordpiece_cache_dir'></span>

<h3>Description</h3>

<p>Use this function to override the cache path used by wordpiece for the
current session. Set the <code>WORDPIECE_CACHE_DIR</code> environment variable
for a more permanent change.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>set_wordpiece_cache_dir(cache_dir = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="set_wordpiece_cache_dir_+3A_cache_dir">cache_dir</code></td>
<td>
<p>Character scalar; a path to a cache directory.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A normalized path to a cache directory. The directory is created if
the user has write access and the directory does not exist.
</p>

<hr>
<h2 id='wordpiece_cache_dir'>Retrieve Directory for wordpiece Cache</h2><span id='topic+wordpiece_cache_dir'></span>

<h3>Description</h3>

<p>The wordpiece cache directory is a platform- and user-specific path where
wordpiece saves caches (such as a downloaded vocabulary). You can override
the default location in a few ways: </p>
 <ul>
<li><p>Option:
<code>wordpiece.dir</code>Use <code><a href="#topic+set_wordpiece_cache_dir">set_wordpiece_cache_dir</a></code> to set a
specific cache directory for this session </p>
</li>
<li><p>Environment:
<code>WORDPIECE_CACHE_DIR</code>Set this environment variable to specify a
wordpiece cache directory for all sessions. </p>
</li>
<li><p>Environment:
<code>R_USER_CACHE_DIR</code>Set this environment variable to specify a cache
directory root for all packages that use the caching system. </p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>wordpiece_cache_dir()
</code></pre>


<h3>Value</h3>

<p>A character vector with the normalized path to the cache.
</p>

<hr>
<h2 id='wordpiece_tokenize'>Tokenize Sequence with Word Pieces</h2><span id='topic+wordpiece_tokenize'></span>

<h3>Description</h3>

<p>Given a sequence of text and a wordpiece vocabulary, tokenizes the text.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>wordpiece_tokenize(
  text,
  vocab = wordpiece_vocab(),
  unk_token = "[UNK]",
  max_chars = 100
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="wordpiece_tokenize_+3A_text">text</code></td>
<td>
<p>Character; text to tokenize.</p>
</td></tr>
<tr><td><code id="wordpiece_tokenize_+3A_vocab">vocab</code></td>
<td>
<p>Character vector of vocabulary tokens. The tokens are assumed to
be in order of index, with the first index taken as zero to be compatible
with Python implementations.</p>
</td></tr>
<tr><td><code id="wordpiece_tokenize_+3A_unk_token">unk_token</code></td>
<td>
<p>Token to represent unknown words.</p>
</td></tr>
<tr><td><code id="wordpiece_tokenize_+3A_max_chars">max_chars</code></td>
<td>
<p>Maximum length of word recognized.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of named integer vectors, giving the tokenization of the input
sequences. The integer values are the token ids, and the names are the
tokens.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tokens &lt;- wordpiece_tokenize(
  text = c(
    "I love tacos!",
    "I also kinda like apples."
  )
)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
