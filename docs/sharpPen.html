<!DOCTYPE html><html><head><title>Help for package sharpPen</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {sharpPen}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#data_sharpening'>
<p>Penalized data sharpening for Local Linear, Quadratic and Cubic Regression</p></a></li>
<li><a href='#derivOperator'><p>Shape Constraint Matrix Construction</p></a></li>
<li><a href='#dpilc'>
<p>Select a Bandwidth for Local Quadratic and Cubic Regression</p></a></li>
<li><a href='#DR_sharpen'>
<p>Shape-Constrained Local Linear Regression via Douglas-Rachford</p></a></li>
<li><a href='#lprOperator'><p>Local Polynomial Estimator Matrix Construction</p></a></li>
<li><a href='#noontemp'><p>Noon Temperatures in Winnipeg, Manitoba</p></a></li>
<li><a href='#numericalDerivative'><p>Numerical Derivative of Smooth Function</p></a></li>
<li><a href='#projection_C'>
<p>Projection operator for rectangle or nonnegative space</p></a></li>
<li><a href='#projection_nb'>
<p>Projection operator for norm balls.</p></a></li>
<li><a href='#relsharp_bigh'><p>Ridge/Enet/LASSO Sharpening via the local polynomial regression with</p>
large bandwidth.</a></li>
<li><a href='#relsharp_bigh_c'><p>Ridge/Enet/LASSO Sharpening via the local polynomial regression with</p>
large bandwidth and then applying the residual sharpening method.</a></li>
<li><a href='#relsharp_linear'><p>Ridge/Enet/LASSO Sharpening via the linear regression.</p></a></li>
<li><a href='#relsharp_linear_c'><p>Ridge/Enet/LASSO Sharpening via the linear regression</p>
and then applying the residual sharpening method.</a></li>
<li><a href='#relsharp_mean'><p>Ridge/Enet/LASSO Sharpening via the Mean</p></a></li>
<li><a href='#relsharp_mean_c'><p>Ridge/Enet/LASSO Sharpening via the Mean and then applying the residual sharpening method.</p></a></li>
<li><a href='#relsharpen'><p>Ridge/Enet/LASSO Sharpening via the penalty matrix.</p></a></li>
<li><a href='#RELsharpening'><p>Ridge/Enet/LASSO Sharpening via the mean/local polynomial regression with</p>
large bandwidth/linear regression.</a></li>
<li><a href='#testfun'><p>Functions for Testing Purposes</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Version:</td>
<td>1.9</td>
</tr>
<tr>
<td>Title:</td>
<td>Penalized Data Sharpening for Local Polynomial Regression</td>
</tr>
<tr>
<td>Author:</td>
<td>W.J. Braun [aut],
  D. Wang [aut, cre],
  X.J. Hu [aut, ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>D. Wang &lt;wdy@student.ubc.ca&gt;</td>
</tr>
<tr>
<td>Depends:</td>
<td>KernSmooth, glmnet, np, Matrix, locpol</td>
</tr>
<tr>
<td>Description:</td>
<td>Functions and data sets for data sharpening.
    Nonparametric regressions are computed subject to smoothness
    and other kinds of penalties. </td>
</tr>
<tr>
<td>License:</td>
<td>Unlimited</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-10-19 01:40:52 UTC; wdy</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-10-19 21:30:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='data_sharpening'>
Penalized data sharpening for Local Linear, Quadratic and Cubic Regression 
</h2><span id='topic+data_sharpening'></span>

<h3>Description</h3>

<p>Penalized data sharpening has been proposed as a way to enforce certain constraints on a local polynomial regression estimator. In addition to a bandwidth, a coefficient of the penalty term is also required. We propose propose systematic approaches for choosing these tuning parameters, in part, by considering the optimization problem from the perspective of ridge regression.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data_sharpening(xx,yy,zz,p,h=NULL,gammaest=NULL,penalty,lambda=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="data_sharpening_+3A_xx">xx</code></td>
<td>

<p>numeric vector of x data.
Missing values are not accepted.
</p>
</td></tr>
<tr><td><code id="data_sharpening_+3A_yy">yy</code></td>
<td>

<p>numeric vector of y data.
This must be same length as <code>x</code>, and
missing values are not accepted.
</p>
</td></tr>
<tr><td><code id="data_sharpening_+3A_zz">zz</code></td>
<td>

<p>numeric vector of gridpoint z data. Missing values are not accepted.
</p>
</td></tr>
<tr><td><code id="data_sharpening_+3A_p">p</code></td>
<td>

<p>degree of local polynomial used.
</p>
</td></tr> 
<tr><td><code id="data_sharpening_+3A_h">h</code></td>
<td>

<p>the kernel bandwidth smoothing parameter. If NULL, this value
will be estimated by function dpill for Local Linear Regression, and
will be estimated by function dpilc for Local Quadratic and Cubic 
Regression.
</p>
</td></tr>
<tr><td><code id="data_sharpening_+3A_gammaest">gammaest</code></td>
<td>

<p>the shape constraint parameter. Cannot be NULL for Periodic shape constraint. Can be NULL for Exponential shape constraint.
</p>
</td></tr>
<tr><td><code id="data_sharpening_+3A_penalty">penalty</code></td>
<td>

<p>the type of shape constraint, can be &quot;Exponential&quot; and &quot;Periodicity&quot;.
</p>
</td></tr>
<tr><td><code id="data_sharpening_+3A_lambda">lambda</code></td>
<td>

<p>a coefficient of the penalty term, default is NULL.
</p>
</td></tr>  
</table>


<h3>Value</h3>

<p>the sharpened response variable.
</p>


<h3>Author(s)</h3>

<p>D.Wang and W.J.Braun</p>


<h3>Examples</h3>

<pre><code class='language-R'>  set.seed(1234567)
  gam&lt;-4
  gamest&lt;-gam
  g &lt;- function(x) 3*sin(x*(gam*pi))+5*cos(x*(gam*pi))+6*x
  sigma&lt;-3
  xx&lt;-seq(0,1,length=100)
  yy&lt;-g(xx)+rnorm(100,sd=sigma)
  zz&lt;-xx
  h1&lt;-dpilc(xx,yy)
  local_fit&lt;-t(lprOperator(h=h1,xx=xx,zz=zz,p=2))%*%yy
  y_sharp&lt;-data_sharpening(xx=xx,yy=yy,zz=zz,p=2,gammaest=gamest,penalty="Periodicity")
  sharp_fit&lt;-t(lprOperator(h=h1,xx=xx,zz=zz,p=2))%*%y_sharp
  plot(c(min(xx),max(xx)),c(min(yy)-0.5,max(yy)+0.5),type="n",,xlab="x",ylab="y")
  legend("bottomright",legend=c("curve_local","curve_sharpen"), col=c(1,3),bty="n",pch=c("-","-"))
  lines(xx,local_fit)
  lines(xx,sharp_fit,col=3, lwd=2)
  points(xx,yy,col= rgb(0.8,0.2,0.2,0.2))
</code></pre>

<hr>
<h2 id='derivOperator'>Shape Constraint Matrix Construction</h2><span id='topic+derivOperator'></span>

<h3>Description</h3>

<p>Construct a shape constraint matrix at 
a corresponding sequence of x data and sequence of gridpoint z.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  derivOperator(penalty,gamma,h, xx,zz,p) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="derivOperator_+3A_penalty">penalty</code></td>
<td>
<p>the type of shape constraint, can be &quot;drv1&quot;, &quot;drv2&quot;, &quot;drv3&quot;, &quot;drv4&quot;, 
&quot;Exponential&quot; and &quot;Periodicity&quot;.</p>
</td></tr>
<tr><td><code id="derivOperator_+3A_gamma">gamma</code></td>
<td>
<p>the shape constraint parameter</p>
</td></tr>
<tr><td><code id="derivOperator_+3A_h">h</code></td>
<td>
<p>the kernel bandwidth smoothing parameter.</p>
</td></tr>
<tr><td><code id="derivOperator_+3A_xx">xx</code></td>
<td>
<p>numeric vector of x data. Missing values are not accepted.</p>
</td></tr>
<tr><td><code id="derivOperator_+3A_zz">zz</code></td>
<td>
<p>numeric vector of gridpoint z data. Missing values are not accepted.</p>
</td></tr>
<tr><td><code id="derivOperator_+3A_p">p</code></td>
<td>
<p>degree of local polynomial used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>shape constraint matrix 
</p>


<h3>Author(s)</h3>

<p>X.J. Hu</p>

<hr>
<h2 id='dpilc'>
Select a Bandwidth for Local Quadratic and Cubic Regression
</h2><span id='topic+dpilc'></span>

<h3>Description</h3>

<p>Use direct plug-in methodology to select the bandwidth
of a local quadratic and local cubic Gaussian kernel regression estimate, 
as an extension of Wand's <code>dpill</code> function. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dpilc(xx, yy, blockmax = 5, divisor = 20, trim = 0.01,
                 proptrun = 0.05, gridsize = 401L, range.x = range(x))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dpilc_+3A_xx">xx</code></td>
<td>

<p>numeric vector of x data.
Missing values are not accepted.
</p>
</td></tr>
<tr><td><code id="dpilc_+3A_yy">yy</code></td>
<td>

<p>numeric vector of y data.
This must be same length as <code>x</code>, and
missing values are not accepted.
</p>
</td></tr>
<tr><td><code id="dpilc_+3A_blockmax">blockmax</code></td>
<td>

<p>the maximum number of blocks of the data for construction
of an initial parametric estimate. 
</p>
</td></tr>
<tr><td><code id="dpilc_+3A_divisor">divisor</code></td>
<td>

<p>the value that the sample size is divided by to determine
a lower limit on the number of blocks of the data for
construction of an initial parametric estimate.
</p>
</td></tr>
<tr><td><code id="dpilc_+3A_trim">trim</code></td>
<td>

<p>the proportion of the sample trimmed from each end in the
<code>x</code> direction before application of the plug-in methodology.
</p>
</td></tr>
<tr><td><code id="dpilc_+3A_proptrun">proptrun</code></td>
<td>

<p>the proportion of the range of <code>x</code> at each end truncated in the
functional estimates.
</p>
</td></tr>
<tr><td><code id="dpilc_+3A_gridsize">gridsize</code></td>
<td>

<p>number of equally-spaced grid points over which the
function is to be estimated.
</p>
</td></tr>
<tr><td><code id="dpilc_+3A_range.x">range.x</code></td>
<td>

<p>vector containing the minimum and maximum values of <code>x</code> at which to
compute the estimate.
For density estimation the default is the minimum and maximum data values
with 5% of the range added to each end.
For regression estimation the default is the minimum and maximum data values.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is a local cubic (also quadratic) extension of
the <code>dpill</code> function of Wand's KernSmooth package.  
The kernel is the standard normal density.
Least squares octic fits over blocks of data are used to 
obtain an initial estimate. As in Wand's implementation
of the Ruppert, Sheather and Wand selector, 
Mallow's <code class="reqn">C_p</code> is used to select
the number of blocks.  An option is available to 
make use of a periodic penalty (with possible trend) 
relating the 4th derivative of the regression function
to a constant (gamma) times the 2nd derivative.  This
avoids the need to calculate the octic fits and reverts
back to the original quartic fits of <code>dpill</code> with
appropriate adjustments to the estimated functionals
needed in the direct-plug-in bandwidth calculation. This 
code is similar to <code>dpilq</code> but uses a 6th degree
polyomial approximation instead of an 8th degree polynomial
approximation.
</p>


<h3>Value</h3>

<p>the selected bandwidth.
</p>


<h3>Warning</h3>

<p>If there are severe irregularities (i.e. outliers, sparse regions)
in the <code>x</code> values then the local polynomial smooths required for the
bandwidth selection algorithm may become degenerate and the function
will crash. Outliers in the <code>y</code> direction may lead to deterioration
of the quality of the selected bandwidth.
</p>


<h3>Author(s)</h3>

<p>D.Wang and W.J.Braun</p>


<h3>References</h3>

<p>Ruppert, D., Sheather, S. J. and Wand, M. P. (1995).
An effective bandwidth selector for local least squares
regression.
<em>Journal of the American Statistical Association</em>,
<b>90</b>, 1257&ndash;1270.
</p>
<p>Wand, M. P. and Jones, M. C. (1995).
<em>Kernel Smoothing.</em>
Chapman and Hall, London.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+ksmooth">ksmooth</a></code>, <code><a href="KernSmooth.html#topic+locpoly">locpoly</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- faithful$eruptions
y &lt;- faithful$waiting
plot(x, y)
h &lt;- dpill(x, y)
fit &lt;- locpoly(x, y, bandwidth = h, degree=1)
lines(fit)
h &lt;- dpilc(x, y)
fit &lt;- locpoly(x, y, bandwidth = h, degree=2)
lines(fit, col=3, lwd=2)
fit &lt;- locpoly(x, y, bandwidth = h, degree=3)
lines(fit, col=2, lwd=2)
</code></pre>

<hr>
<h2 id='DR_sharpen'>
Shape-Constrained Local Linear Regression via Douglas-Rachford 
</h2><span id='topic+DR_sharpen'></span>

<h3>Description</h3>

<p>Local linear regression is applied to bivariate
data.  The response is &lsquo;sharpened&rsquo; or perturbed in a way to render a
curve estimate that satisfies some specified shape constraints. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>DR_sharpen(
x, y, xgrid=NULL, M=200,  h=NULL, mode=NULL, 
    ratio_1=0.14,ratio_2=0.14,ratio_3=0.14,ratio_4=0.14,
    constraint_1=NULL, constraint_2=NULL, constraint_3=NULL,
    constraint_4=NULL, norm="l2", augmentation=FALSE, maxit = 10^5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="DR_sharpen_+3A_x">x</code></td>
<td>
<p>a vector of explanatory variable observations</p>
</td></tr>
<tr><td><code id="DR_sharpen_+3A_y">y</code></td>
<td>
<p>binary vector of responses</p>
</td></tr>
<tr><td><code id="DR_sharpen_+3A_xgrid">xgrid</code></td>
<td>
<p>gridpoints on x-axis where estimates are taken</p>
</td></tr>
<tr><td><code id="DR_sharpen_+3A_m">M</code></td>
<td>
<p>number of equally-spaced gridpoints (if xgrid not specified)</p>
</td></tr>
<tr><td><code id="DR_sharpen_+3A_h">h</code></td>
<td>
<p>bandwidth</p>
</td></tr>
<tr><td><code id="DR_sharpen_+3A_mode">mode</code></td>
<td>
<p>the location of the peak on the x-axis, valid in the unimode case</p>
</td></tr>
<tr><td><code id="DR_sharpen_+3A_ratio_1">ratio_1</code></td>
<td>
<p>control the first derivative shape constraint gap aroud the peak, 
valid in the unimode case</p>
</td></tr>
<tr><td><code id="DR_sharpen_+3A_ratio_2">ratio_2</code></td>
<td>
<p>control the second derivative shape constraint gap aroud the peak, 
valid in the unimode case</p>
</td></tr>
<tr><td><code id="DR_sharpen_+3A_ratio_3">ratio_3</code></td>
<td>
<p>control the third derivative shape constraint gap aroud the peak, 
valid in the unimode case</p>
</td></tr>
<tr><td><code id="DR_sharpen_+3A_ratio_4">ratio_4</code></td>
<td>
<p>control the fourth derivative shape constraint gap aroud the peak,
valid in the unimode case</p>
</td></tr>
<tr><td><code id="DR_sharpen_+3A_constraint_1">constraint_1</code></td>
<td>
<p>a vector of the first derivative shape constraint</p>
</td></tr>
<tr><td><code id="DR_sharpen_+3A_constraint_2">constraint_2</code></td>
<td>
<p>a vector of the second derivative shape constraint</p>
</td></tr>
<tr><td><code id="DR_sharpen_+3A_constraint_3">constraint_3</code></td>
<td>
<p>a vector of the third derivative shape constraint</p>
</td></tr>
<tr><td><code id="DR_sharpen_+3A_constraint_4">constraint_4</code></td>
<td>
<p>a vector of the fourth derivative shape constraint</p>
</td></tr>
<tr><td><code id="DR_sharpen_+3A_norm">norm</code></td>
<td>
<p>the smallest possible distance type: &quot;l2&quot;, &quot;l1&quot; or &quot;linf&quot;. Default is &quot;l2&quot;</p>
</td></tr>
<tr><td><code id="DR_sharpen_+3A_augmentation">augmentation</code></td>
<td>
<p>data augmentation: &quot;TRUE&quot; or &quot;FALSE&quot;, default is &quot;FALSE&quot;</p>
</td></tr>
<tr><td><code id="DR_sharpen_+3A_maxit">maxit</code></td>
<td>
<p>maximum iterarion number, default is 10^5</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Data are perturbed the smallest possible L2 or L1 or Linf distance subject to the 
constraint that the local linear estimate satisfies some specified 
shape constraints.
</p>


<h3>Value</h3>

<table>
<tr><td><code>ysharp</code></td>
<td>
<p>sharpened responses</p>
</td></tr>
<tr><td><code>iteration</code></td>
<td>
<p>number of iterations the function has been 
spend for the convergence</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>D.Wang and W.J.Braun
</p>


<h3>References</h3>

<p>Wang, D. (2022). Penalized and constrained data sharpening methods for kernel regression (Doctoral dissertation, University of British Columbia).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1234567)
gam&lt;-4
g &lt;- function(x) (3*sin(x*(gam*pi))+5*cos(x*(gam*pi))+6*x)*x
n&lt;-100
M&lt;-200
noise &lt;- 1
x&lt;-sort(runif(n,0,1))
y&lt;-g(x)+rnorm(n,sd=noise)
z&lt;- seq(min(x)+1/M, max(x)-1/M, length=M) ############xgrid points
h1&lt;-dpill(x,y)
A&lt;-lprOperator(h=h1,x=x,z=z,p=1)
local_fit&lt;-t(A)
ss_1&lt;-c(sign(numericalDerivative(z,g,k=1)))
DR_sharpen(x=x, y=y, xgrid=z, h=h1, constraint_1=ss_1, norm="linf",maxit =10^3)
</code></pre>

<hr>
<h2 id='lprOperator'>Local Polynomial Estimator Matrix Construction</h2><span id='topic+lprOperator'></span>

<h3>Description</h3>

<p>Construct a matrix based on the local polynomial estimation at 
a corresponding sequence of x data and sequence of gridpoint z.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  lprOperator(h,xx,zz,p)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lprOperator_+3A_h">h</code></td>
<td>
<p>the kernel bandwidth smoothing parameter.</p>
</td></tr>
<tr><td><code id="lprOperator_+3A_xx">xx</code></td>
<td>
<p>numeric vector of x data. Missing values are not accepted.</p>
</td></tr>
<tr><td><code id="lprOperator_+3A_zz">zz</code></td>
<td>
<p>numeric vector of gridpoint z data. Missing values are not accepted.</p>
</td></tr>
<tr><td><code id="lprOperator_+3A_p">p</code></td>
<td>
<p>degree of local polynomial used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>local polynomial estimator matrix 
</p>


<h3>Author(s)</h3>

<p>X.J. Hu</p>

<hr>
<h2 id='noontemp'>Noon Temperatures in Winnipeg, Manitoba</h2><span id='topic+noontemp'></span>

<h3>Description</h3>

<p>Time Series of noon temperature observations from the Winnipeg
International Airport from January 1, 1960 through December 31, 1980.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(noontemp)</code></pre>


<h3>Format</h3>

<p>A single vector.  
</p>

<hr>
<h2 id='numericalDerivative'>Numerical Derivative of Smooth Function</h2><span id='topic+numericalDerivative'></span>

<h3>Description</h3>

<p>Cubic spline interpolation of columns of a matrix for
purpose of computing numerical derivatives at a corresponding
sequence of gridpoints.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>numericalDerivative(x, g, k, delta=.001) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="numericalDerivative_+3A_x">x</code></td>
<td>
<p>numeric vector</p>
</td></tr>
<tr><td><code id="numericalDerivative_+3A_g">g</code></td>
<td>
<p>numeric-valued function of x</p>
</td></tr>
<tr><td><code id="numericalDerivative_+3A_k">k</code></td>
<td>
<p>number of derivatives to be computed</p>
</td></tr>
<tr><td><code id="numericalDerivative_+3A_delta">delta</code></td>
<td>
<p>denominator of Newton quotient approximation</p>
</td></tr>
</table>


<h3>Value</h3>

<p>numeric vector of kth derivative of g(x)
</p>


<h3>Author(s)</h3>

<p>W.J. Braun</p>

<hr>
<h2 id='projection_C'>
Projection operator for rectangle or nonnegative space 
</h2><span id='topic+projection_C'></span>

<h3>Description</h3>

<p>Compute the projection operator for rectangle or nonnegative space. For example,
we construct </p>
<p style="text-align: center;"><code class="reqn">\lambda P_{C}(x/\lambda) = projection_C(\lambda,C,x)</code>
</p>
<p>, 
where <code class="reqn">C</code> can be rectangle or nonnegative space.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>projection_C(
lambda,family=c("rectangle","nonnegative"),
input,bound=c(-1,1))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="projection_C_+3A_lambda">lambda</code></td>
<td>
<p>parameter <code class="reqn">\lambda</code> in the above equation</p>
</td></tr>
<tr><td><code id="projection_C_+3A_family">family</code></td>
<td>
<p>type of <code class="reqn">C</code>, can be rectangle or nonnegative space</p>
</td></tr>
<tr><td><code id="projection_C_+3A_input">input</code></td>
<td>
<p>input x in the above equation</p>
</td></tr>
<tr><td><code id="projection_C_+3A_bound">bound</code></td>
<td>
<p>lower bound and upper bound for rectangle</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Take <code class="reqn">x</code> as input, <code class="reqn">\lambda</code> as parameter.
Calculate <code class="reqn">\lambda P_{C}(x/\lambda)</code> for a given <code class="reqn">C</code> family 
</p>


<h3>Value</h3>

<table>
<tr><td><code>projection</code></td>
<td>
<p><code class="reqn">\lambda P_{C}(x/\lambda)</code></p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>D.Wang and W.J.Braun
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1234567)
family &lt;- "nonnegative"
temp_p1&lt;-runif(10,-1,1)
projection_C(0.5,family=family,temp_p1)
</code></pre>

<hr>
<h2 id='projection_nb'>
Projection operator for norm balls.
</h2><span id='topic+projection_nb'></span>

<h3>Description</h3>

<p>Compute the projection operator for norm balls. For example,
we construct </p>
<p style="text-align: center;"><code class="reqn">\lambda P_{B_{\| \cdot \|_*}[0,r]}(x/\lambda) = projection_nb(\lambda,r,\| \cdot \|_*,x)</code>
</p>
<p>, 
where <code class="reqn">\| \cdot \|_*</code> can be <code class="reqn">l_{1}</code>-norm, <code class="reqn">l_{2}</code>-norm, and 
<code class="reqn">l_{\infty}</code>-norm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>projection_nb(
lambda,radius,family=c("norm2","norm1","norminf"),
input)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="projection_nb_+3A_lambda">lambda</code></td>
<td>
<p>parameter <code class="reqn">\lambda</code> in the above equation</p>
</td></tr>
<tr><td><code id="projection_nb_+3A_radius">radius</code></td>
<td>
<p>parameter <code class="reqn">r</code> in the above equation</p>
</td></tr>
<tr><td><code id="projection_nb_+3A_family">family</code></td>
<td>
<p>select the norm ball type, can be <code class="reqn">l_{1}</code>-norm, <code class="reqn">l_{2}</code>-norm, and
<code class="reqn">l_{\infty}</code>-norm.</p>
</td></tr>
<tr><td><code id="projection_nb_+3A_input">input</code></td>
<td>
<p>input x in the above equation</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Take <code class="reqn">x</code> as input, <code class="reqn">\lambda</code> and <code class="reqn">r</code> as parameters.
Calculate <code class="reqn">\lambda P_{B_{\| \cdot \|_*}[0,r]}(x/\lambda)</code> 
for a given norm ball type.
</p>


<h3>Value</h3>

<table>
<tr><td><code>projection</code></td>
<td>
<p><code class="reqn">\lambda P_{B_{\| \cdot \|_*}[0,r]}(x/\lambda)</code></p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>D.Wang and W.J.Braun
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1234567)
family &lt;- "norm1"
temp_p1&lt;-rep(10,100)
projection_nb(3,1,family=family,temp_p1)
</code></pre>

<hr>
<h2 id='relsharp_bigh'>Ridge/Enet/LASSO Sharpening via the local polynomial regression with 
large bandwidth.</h2><span id='topic+relsharp_bigh'></span>

<h3>Description</h3>

<p>This is a function to shrink responses towards their
estimations of local polynomial regression with large
bandwidth as a form of data sharpening to remove roughness,
prior to use in local polynomial regression.
</p>


<h3>Usage</h3>

<pre><code class='language-R'> relsharp_bigh(x, y, alpha, bigh) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="relsharp_bigh_+3A_x">x</code></td>
<td>
<p>numeric vector of equally spaced x data. Missing values are not accepted.</p>
</td></tr>
<tr><td><code id="relsharp_bigh_+3A_y">y</code></td>
<td>
<p>vector of y data. Missing values are not accepted.</p>
</td></tr>
<tr><td><code id="relsharp_bigh_+3A_alpha">alpha</code></td>
<td>
<p>the elasticnet mixing parameter vector, with alpha in [0,1].</p>
</td></tr>
<tr><td><code id="relsharp_bigh_+3A_bigh">bigh</code></td>
<td>
<p>the kernel bandwidth smoothing parameter.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note that the predictor values are assumed to be equally
spaced.  
</p>


<h3>Value</h3>

<p>numeric matrix of sharpened responses, with each column
corresponding to different values of alpha
</p>


<h3>Author(s)</h3>

<p>D.Wang</p>


<h3>Examples</h3>

<pre><code class='language-R'>x&lt;-seq(0,10,length=100)
g &lt;- function(x) sin(x)
y&lt;-g(x)+rnorm(100)
ys&lt;-relsharp_bigh(x, y,alpha=c(0.2,0.8), dpill(x,y)*4)
y.lp2&lt;-locpoly(x,ys[,1],bandwidth=dpill(x,y),degree=1,gridsize=100)
y.lp8&lt;-locpoly(x,ys[,2],bandwidth=dpill(x,y),degree=1,gridsize=100)
y.lp&lt;-locpoly(x,y,bandwidth=dpill(x,y),degree=1,gridsize=100)
curve(g,x,xlim=c(0,10))
lines(y.lp2,col=2)
lines(y.lp8,col=3)
lines(y.lp,col=5)
norm(as.matrix(g(x) - y.lp2$y),type="2")
norm(as.matrix(g(x) - y.lp8$y),type="2")
norm(as.matrix(g(x) - y.lp$y),type="2")
</code></pre>

<hr>
<h2 id='relsharp_bigh_c'>Ridge/Enet/LASSO Sharpening via the local polynomial regression with 
large bandwidth and then applying the residual sharpening method.</h2><span id='topic+relsharp_bigh_c'></span>

<h3>Description</h3>

<p>This is a function to shrink responses towards their
estimations of local polynomial regression with large
bandwidth and then apply residual sharpening as a form of data sharpening to remove roughness,
prior to use in local polynomial regression.
</p>


<h3>Usage</h3>

<pre><code class='language-R'> relsharp_bigh_c(x, y, alpha, bigh, hband) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="relsharp_bigh_c_+3A_x">x</code></td>
<td>
<p>numeric vector of equally spaced x data. Missing values are not accepted.</p>
</td></tr>
<tr><td><code id="relsharp_bigh_c_+3A_y">y</code></td>
<td>
<p>vector of y data. Missing values are not accepted.</p>
</td></tr>
<tr><td><code id="relsharp_bigh_c_+3A_alpha">alpha</code></td>
<td>
<p>the elasticnet mixing parameter vector, with alpha in [0,1].</p>
</td></tr>
<tr><td><code id="relsharp_bigh_c_+3A_bigh">bigh</code></td>
<td>
<p>the kernel bandwidth smoothing parameter.</p>
</td></tr>
<tr><td><code id="relsharp_bigh_c_+3A_hband">hband</code></td>
<td>
<p>the kernel bandwidth smoothing parameter, which will be used in the residual sharpening method.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note that the predictor values are assumed to be equally
spaced.  
</p>


<h3>Value</h3>

<p>numeric matrix of sharpened responses, with each column
corresponding to different values of alpha
</p>


<h3>Author(s)</h3>

<p>D.Wang</p>


<h3>Examples</h3>

<pre><code class='language-R'>x&lt;-seq(0,10,length=100)
g &lt;- function(x) sin(x)
y&lt;-g(x)+rnorm(100)
ys&lt;-relsharp_bigh_c(x, y,alpha=c(0.2,0.8), dpill(x,y)*4, dpill(x,y))
y.lp2&lt;-locpoly(x,ys[,1],bandwidth=dpill(x,y),degree=1,gridsize=100)
y.lp8&lt;-locpoly(x,ys[,2],bandwidth=dpill(x,y),degree=1,gridsize=100)
y.lp&lt;-locpoly(x,y,bandwidth=dpill(x,y),degree=1,gridsize=100)
curve(g,x,xlim=c(0,10))
lines(y.lp2,col=2)
lines(y.lp8,col=3)
lines(y.lp,col=5)
norm(as.matrix(g(x) - y.lp2$y),type="2")
norm(as.matrix(g(x) - y.lp8$y),type="2")
norm(as.matrix(g(x) - y.lp$y),type="2")
</code></pre>

<hr>
<h2 id='relsharp_linear'>Ridge/Enet/LASSO Sharpening via the linear regression.</h2><span id='topic+relsharp_linear'></span>

<h3>Description</h3>

<p>This is a function to shrink responses towards their
estimations of linear regression as a form of data sharpening to remove roughness,
prior to use in local polynomial regression.
</p>


<h3>Usage</h3>

<pre><code class='language-R'> relsharp_linear(x, y, alpha) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="relsharp_linear_+3A_x">x</code></td>
<td>
<p>numeric vector of equally spaced x data. Missing values are not accepted.</p>
</td></tr>
<tr><td><code id="relsharp_linear_+3A_y">y</code></td>
<td>
<p>vector of y data. Missing values are not accepted.</p>
</td></tr>
<tr><td><code id="relsharp_linear_+3A_alpha">alpha</code></td>
<td>
<p>the elasticnet mixing parameter vector, with alpha in [0,1].</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note that the predictor values are assumed to be equally
spaced.  
</p>


<h3>Value</h3>

<p>numeric matrix of sharpened responses, with each column
corresponding to different values of alpha
</p>


<h3>Author(s)</h3>

<p>D.Wang</p>


<h3>Examples</h3>

<pre><code class='language-R'>x&lt;-seq(0,10,length=100)
g &lt;- function(x) sin(x)
y&lt;-g(x)+rnorm(100)
ys&lt;-relsharp_linear(x, y,alpha=c(0.2,0.8))
y.lp2&lt;-locpoly(x,ys[,1],bandwidth=dpill(x,y),degree=1,gridsize=100)
y.lp8&lt;-locpoly(x,ys[,2],bandwidth=dpill(x,y),degree=1,gridsize=100)
y.lp&lt;-locpoly(x,y,bandwidth=dpill(x,y),degree=1,gridsize=100)
curve(g,x,xlim=c(0,10))
lines(y.lp2,col=2)
lines(y.lp8,col=3)
lines(y.lp,col=5)
norm(as.matrix(g(x) - y.lp2$y),type="2")
norm(as.matrix(g(x) - y.lp8$y),type="2")
norm(as.matrix(g(x) - y.lp$y),type="2")
</code></pre>

<hr>
<h2 id='relsharp_linear_c'>Ridge/Enet/LASSO Sharpening via the linear regression 
and then applying the residual sharpening method.</h2><span id='topic+relsharp_linear_c'></span>

<h3>Description</h3>

<p>This is a function to shrink responses towards their
estimations of linear regression and then apply residual sharpening 
as a form of data sharpening to remove roughness,
prior to use in local polynomial regression.
</p>


<h3>Usage</h3>

<pre><code class='language-R'> relsharp_linear_c(x, y, alpha, hband) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="relsharp_linear_c_+3A_x">x</code></td>
<td>
<p>numeric vector of equally spaced x data. Missing values are not accepted.</p>
</td></tr>
<tr><td><code id="relsharp_linear_c_+3A_y">y</code></td>
<td>
<p>vector of y data. Missing values are not accepted.</p>
</td></tr>
<tr><td><code id="relsharp_linear_c_+3A_alpha">alpha</code></td>
<td>
<p>the elasticnet mixing parameter vector, with alpha in [0,1].</p>
</td></tr>
<tr><td><code id="relsharp_linear_c_+3A_hband">hband</code></td>
<td>
<p>the kernel bandwidth smoothing parameter, which will be used in the residual sharpening method.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note that the predictor values are assumed to be equally
spaced.  
</p>


<h3>Value</h3>

<p>numeric matrix of sharpened responses, with each column
corresponding to different values of alpha
</p>


<h3>Author(s)</h3>

<p>D.Wang</p>


<h3>Examples</h3>

<pre><code class='language-R'>x&lt;-seq(0,10,length=100)
g &lt;- function(x) sin(x)
y&lt;-g(x)+rnorm(100)
ys&lt;-relsharp_linear_c(x, y,alpha=c(0.2,0.8),dpill(x,y))
y.lp2&lt;-locpoly(x,ys[,1],bandwidth=dpill(x,y),degree=1,gridsize=100)
y.lp8&lt;-locpoly(x,ys[,2],bandwidth=dpill(x,y),degree=1,gridsize=100)
y.lp&lt;-locpoly(x,y,bandwidth=dpill(x,y),degree=1,gridsize=100)
curve(g,x,xlim=c(0,10))
lines(y.lp2,col=2)
lines(y.lp8,col=3)
lines(y.lp,col=5)
norm(as.matrix(g(x) - y.lp2$y),type="2")
norm(as.matrix(g(x) - y.lp8$y),type="2")
norm(as.matrix(g(x) - y.lp$y),type="2")
</code></pre>

<hr>
<h2 id='relsharp_mean'>Ridge/Enet/LASSO Sharpening via the Mean</h2><span id='topic+relsharp_mean'></span>

<h3>Description</h3>

<p>This is a function to shrink responses towards their
mean as a form of data sharpening to remove roughness,
prior to use in local polynomial regression.
</p>


<h3>Usage</h3>

<pre><code class='language-R'> relsharp_mean(y, alpha) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="relsharp_mean_+3A_y">y</code></td>
<td>
<p>vector of y data. Missing values are not accepted.</p>
</td></tr>
<tr><td><code id="relsharp_mean_+3A_alpha">alpha</code></td>
<td>
<p>The elasticnet mixing parameter vector, with alpha in [0,1].</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note that the predictor values are assumed to be equally
spaced.  
</p>


<h3>Value</h3>

<p>numeric matrix of sharpened responses, with each column
corresponding to different values of alpha
</p>


<h3>Author(s)</h3>

<p>D.Wang</p>


<h3>Examples</h3>

<pre><code class='language-R'>x&lt;-seq(0,10,length=100)
g &lt;- function(x) sin(x)
y&lt;-g(x)+rnorm(100)
ys&lt;-relsharp_mean(y,alpha=c(0.2,0.8))
y.lp2&lt;-locpoly(x,ys[,1],bandwidth=dpill(x,y),degree=1,gridsize=100)
y.lp8&lt;-locpoly(x,ys[,2],bandwidth=dpill(x,y),degree=1,gridsize=100)
y.lp&lt;-locpoly(x,y,bandwidth=dpill(x,y),degree=1,gridsize=100)
curve(g,x,xlim=c(0,10))
lines(y.lp2,col=2)
lines(y.lp8,col=3)
lines(y.lp,col=5)
norm(as.matrix(g(x) - y.lp2$y),type="2")
norm(as.matrix(g(x) - y.lp8$y),type="2")
norm(as.matrix(g(x) - y.lp$y),type="2")
</code></pre>

<hr>
<h2 id='relsharp_mean_c'>Ridge/Enet/LASSO Sharpening via the Mean and then applying the residual sharpening method.</h2><span id='topic+relsharp_mean_c'></span>

<h3>Description</h3>

<p>This is a function to shrink responses towards their mean and then 
apply residual sharpening as a form of data sharpening to remove roughness,
prior to use in local polynomial regression.
</p>


<h3>Usage</h3>

<pre><code class='language-R'> relsharp_mean_c(x, y, alpha, hband) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="relsharp_mean_c_+3A_x">x</code></td>
<td>
<p>numeric vector of equally spaced x data. Missing values are not accepted.</p>
</td></tr>
<tr><td><code id="relsharp_mean_c_+3A_y">y</code></td>
<td>
<p>vector of y data. Missing values are not accepted.</p>
</td></tr>
<tr><td><code id="relsharp_mean_c_+3A_alpha">alpha</code></td>
<td>
<p>the elasticnet mixing parameter vector, with alpha in [0,1].</p>
</td></tr>
<tr><td><code id="relsharp_mean_c_+3A_hband">hband</code></td>
<td>
<p>the kernel bandwidth smoothing parameter, which will be used in the residual sharpening method.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note that the predictor values are assumed to be equally
spaced.  
</p>


<h3>Value</h3>

<p>numeric matrix of sharpened responses, with each column
corresponding to different values of alpha
</p>


<h3>Author(s)</h3>

<p>D.Wang</p>


<h3>Examples</h3>

<pre><code class='language-R'>x&lt;-seq(0,10,length=100)
g &lt;- function(x) sin(x)
y&lt;-g(x)+rnorm(100)
ys&lt;-relsharp_mean_c(x, y,alpha=c(0.2,0.8), dpill(x,y))
y.lp2&lt;-locpoly(x,ys[,1],bandwidth=dpill(x,y),degree=1,gridsize=100)
y.lp8&lt;-locpoly(x,ys[,2],bandwidth=dpill(x,y),degree=1,gridsize=100)
y.lp&lt;-locpoly(x,y,bandwidth=dpill(x,y),degree=1,gridsize=100)
curve(g,x,xlim=c(0,10))
lines(y.lp2,col=2)
lines(y.lp8,col=3)
lines(y.lp,col=5)
norm(as.matrix(g(x) - y.lp2$y),type="2")
norm(as.matrix(g(x) - y.lp8$y),type="2")
norm(as.matrix(g(x) - y.lp$y),type="2")
</code></pre>

<hr>
<h2 id='relsharpen'>Ridge/Enet/LASSO Sharpening via the penalty matrix.</h2><span id='topic+relsharpen'></span>

<h3>Description</h3>

<p>This is a data sharpening function to remove roughness, 
prior to use in local polynomial regression.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>relsharpen(x, y, h, alpha, p=2, M=51)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="relsharpen_+3A_x">x</code></td>
<td>
<p>numeric vector of equally spaced x data. Missing values are not accepted.</p>
</td></tr>
<tr><td><code id="relsharpen_+3A_y">y</code></td>
<td>
<p>vector of y data. Missing values are not accepted.</p>
</td></tr>
<tr><td><code id="relsharpen_+3A_h">h</code></td>
<td>
<p>the kernel bandwidth smoothing parameter.</p>
</td></tr>
<tr><td><code id="relsharpen_+3A_alpha">alpha</code></td>
<td>
<p>the elasticnet mixing parameter vector, with alpha in [0,1].</p>
</td></tr>
<tr><td><code id="relsharpen_+3A_p">p</code></td>
<td>
<p>the order of the polynomial regression.</p>
</td></tr>
<tr><td><code id="relsharpen_+3A_m">M</code></td>
<td>
<p>the length of the constraint points.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note that the predictor values are assumed to be equally
spaced.  
</p>


<h3>Value</h3>

<p>numeric matrix of sharpened responses, with each column
corresponding to different values of alpha
</p>


<h3>Author(s)</h3>

<p>D.Wang</p>


<h3>Examples</h3>

<pre><code class='language-R'>x&lt;-seq(0,10,length=100)
g &lt;- function(x) sin(x)
y&lt;-g(x)+rnorm(100)
ys&lt;-relsharpen(x, y, dpill(x,y), alpha=c(0.2,0.8), p=2, M=51)
y.lp2&lt;-locpoly(x,ys[,1],bandwidth=dpill(x,y),degree=1,gridsize=100)
y.lp8&lt;-locpoly(x,ys[,2],bandwidth=dpill(x,y),degree=1,gridsize=100)
y.lp&lt;-locpoly(x,y,bandwidth=dpill(x,y),degree=1,gridsize=100)
curve(g,x,xlim=c(0,10))
lines(y.lp2,col=2)
lines(y.lp8,col=3)
lines(y.lp,col=5)
norm(as.matrix(g(x) - y.lp2$y),type="2")
norm(as.matrix(g(x) - y.lp8$y),type="2")
norm(as.matrix(g(x) - y.lp$y),type="2")
</code></pre>

<hr>
<h2 id='RELsharpening'>Ridge/Enet/LASSO Sharpening via the mean/local polynomial regression with 
large bandwidth/linear regression.</h2><span id='topic+RELsharpening'></span>

<h3>Description</h3>

<p>This is a function to shrink responses towards their
mean/estimations of local polynomial regression with large
bandwidth/estimations of linear regression as a form of data sharpening to remove roughness, and reduce the 
bias (when &quot;combine=TRUE&quot;),
prior to use in local polynomial regression.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>RELsharpening(x,y,alpha,type,bigh,hband,combine)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="RELsharpening_+3A_x">x</code></td>
<td>
<p>numeric vector of equally spaced x data. Missing values are not accepted.</p>
</td></tr>
<tr><td><code id="RELsharpening_+3A_y">y</code></td>
<td>
<p>vector of y data. Missing values are not accepted.</p>
</td></tr>
<tr><td><code id="RELsharpening_+3A_alpha">alpha</code></td>
<td>
<p>the elasticnet mixing parameter vector, with alpha in [0,1].</p>
</td></tr>
<tr><td><code id="RELsharpening_+3A_type">type</code></td>
<td>
<p>The type of the base line. In total, we have three types: &quot;mean&quot;, &quot;big_h&quot;, and &quot;linear&quot;.</p>
</td></tr> 
<tr><td><code id="RELsharpening_+3A_bigh">bigh</code></td>
<td>
<p>the kernel bandwidth smoothing parameter.</p>
</td></tr>
<tr><td><code id="RELsharpening_+3A_hband">hband</code></td>
<td>
<p>the kernel bandwidth smoothing parameter, which will be used in the residual sharpening method.</p>
</td></tr>
<tr><td><code id="RELsharpening_+3A_combine">combine</code></td>
<td>
<p>Should the smoother combined with residual method or not, default=FALSE.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note that the predictor values are assumed to be equally
spaced.  
</p>


<h3>Value</h3>

<p>numeric matrix of sharpened responses, with each column
corresponding to different values of alpha
</p>


<h3>Author(s)</h3>

<p>D.Wang</p>


<h3>Examples</h3>

<pre><code class='language-R'>x&lt;-seq(0,10,length=100)
g &lt;- function(x) sin(x)
y&lt;-g(x)+rnorm(100)
ys&lt;-RELsharpening(x, y,alpha=c(0.2,0.8),"big_h", dpill(x,y)*4, dpill(x,y),combine=TRUE)
y.lp2&lt;-locpoly(x,ys[,1],bandwidth=dpill(x,y),degree=1,gridsize=100)
y.lp8&lt;-locpoly(x,ys[,2],bandwidth=dpill(x,y),degree=1,gridsize=100)
y.lp&lt;-locpoly(x,y,bandwidth=dpill(x,y),degree=1,gridsize=100)
curve(g,x,xlim=c(0,10))
lines(y.lp2,col=2)
lines(y.lp8,col=3)
lines(y.lp,col=5)
norm(as.matrix(g(x) - y.lp2$y),type="2")
norm(as.matrix(g(x) - y.lp8$y),type="2")
norm(as.matrix(g(x) - y.lp$y),type="2")
</code></pre>

<hr>
<h2 id='testfun'>Functions for Testing Purposes</h2><span id='topic+testfun'></span>

<h3>Description</h3>

<p>Functions that can be used in simulations to test the
effectiveness of the sharpening procedures.
</p>


<h3>Usage</h3>

<pre><code class='language-R'> testfun(x, k) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="testfun_+3A_x">x</code></td>
<td>
<p>numeric vector</p>
</td></tr>
<tr><td><code id="testfun_+3A_k">k</code></td>
<td>
<p>a numeric constant that controls the height of the peak of
the test function; if missing, a periodic function is
supplied
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>numeric vector of function output
</p>


<h3>Author(s)</h3>

<p>D.Wang</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
