<!DOCTYPE html><html><head><title>Help for package binsegRcpp</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {binsegRcpp}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#binseg'><p>Binary segmentation</p></a></li>
<li><a href='#binseg_interface'><p>binseg interface</p></a></li>
<li><a href='#binseg_normal'><p>Binary segmentation, normal change in mean</p></a></li>
<li><a href='#binseg_normal_cv'><p>Binary segmentation, normal change in mean, cross-validation for model selection</p></a></li>
<li><a href='#case.colors'><p>case colors</p></a></li>
<li><a href='#case.sizes'><p>case sizes</p></a></li>
<li><a href='#check_sizes'><p>check sizes</p></a></li>
<li><a href='#coef.binseg_normal_cv'><p>coef binseg normal cv</p></a></li>
<li><a href='#coef.binsegRcpp'><p>coef binsegRcpp</p></a></li>
<li><a href='#cum_median'><p>cum median</p></a></li>
<li><a href='#cum_median_interface'><p>cum median interface</p></a></li>
<li><a href='#depth_first_interface'><p>depth first interface</p></a></li>
<li><a href='#get_complexity'><p>get complexity</p></a></li>
<li><a href='#get_complexity_best_heuristic_equal_breadth_full'><p>get complexity best heuristic equal breadth full</p></a></li>
<li><a href='#get_complexity_best_heuristic_equal_depth_full'><p>get complexity best heuristic equal depth full</p></a></li>
<li><a href='#get_complexity_best_optimal_cost'><p>get complexity best optimal cost</p></a></li>
<li><a href='#get_complexity_best_optimal_splits'><p>get complexity best optimal splits</p></a></li>
<li><a href='#get_complexity_best_optimal_tree'><p>get complexity best optimal tree</p></a></li>
<li><a href='#get_complexity_empirical'><p>get complexity empirical</p></a></li>
<li><a href='#get_complexity_extreme'><p>get complexity extreme</p></a></li>
<li><a href='#get_complexity_worst'><p>get complexity worst</p></a></li>
<li><a href='#get_distribution_info'><p>get distribution info</p></a></li>
<li><a href='#get_tree_empirical'><p>get tree empirical</p></a></li>
<li><a href='#plot.binseg_normal_cv'><p>plot binseg normal cv</p></a></li>
<li><a href='#plot.binsegRcpp'><p>plot binsegRcpp</p></a></li>
<li><a href='#plot.complexity'><p>plot complexity</p></a></li>
<li><a href='#print.binseg_normal_cv'><p>print binseg normal cv</p></a></li>
<li><a href='#print.binsegRcpp'><p>print binsegRcpp</p></a></li>
<li><a href='#qp.x'><p>qp x</p></a></li>
<li><a href='#random_set_vec'><p>random set vec</p></a></li>
<li><a href='#size_to_splits'><p>size to splits</p></a></li>
<li><a href='#tree_layout'><p>tree layout</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Efficient Implementation of Binary Segmentation</td>
</tr>
<tr>
<td>Version:</td>
<td>2023.8.31</td>
</tr>
<tr>
<td>Author:</td>
<td>Toby Dylan Hocking</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Toby Dylan Hocking &lt;toby.hocking@r-project.org&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Standard template library 
 containers are used to implement an efficient binary segmentation
 algorithm, which is log-linear on average and quadratic in the
 worst case.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/tdhock/binsegRcpp">https://github.com/tdhock/binsegRcpp</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/tdhock/binsegRcpp/issues">https://github.com/tdhock/binsegRcpp/issues</a></td>
</tr>
<tr>
<td>Imports:</td>
<td>data.table, Rcpp</td>
</tr>
<tr>
<td>Suggests:</td>
<td>covr, penaltyLearning, directlabels, ggplot2, testthat,
knitr, markdown, neuroblastoma, changepoint, quadprog</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.2</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-09-06 03:17:47 UTC; tdhock</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-09-06 04:30:11 UTC</td>
</tr>
</table>
<hr>
<h2 id='binseg'>Binary segmentation</h2><span id='topic+binseg'></span><span id='topic+binsegRcpp'></span>

<h3>Description</h3>

<p>Efficient C++ implementation of the classic binary segmentation
algorithm for finding changepoints in a sequence of N data. Output
includes columns which can be used to compute parameters for a
single model in log-linear time, using coef method.</p>


<h3>Usage</h3>

<pre><code class='language-R'>binseg(distribution.str, 
    data.vec, max.segments = NULL, 
    is.validation.vec = rep(FALSE, 
        length(data.vec)), 
    position.vec = seq_along(data.vec), 
    weight.vec = rep(1, 
        length(data.vec)), 
    min.segment.length = NULL, 
    container.str = "multiset")</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="binseg_+3A_distribution.str">distribution.str</code></td>
<td>
<p>String indicating distribution, use <code><a href="#topic+get_distribution_info">get_distribution_info</a></code> to see
possible values.</p>
</td></tr>
<tr><td><code id="binseg_+3A_data.vec">data.vec</code></td>
<td>
<p>Vector of numeric data to segment.</p>
</td></tr>
<tr><td><code id="binseg_+3A_max.segments">max.segments</code></td>
<td>
<p>Maximum number of segments to compute, default=NULL which means to
compute the largest number possible, given <code>is.validation.vec</code> and
<code>min.segment.length</code>. Note that the returned number of segments may
be less than this, if there are min segment length constraints.</p>
</td></tr>
<tr><td><code id="binseg_+3A_is.validation.vec">is.validation.vec</code></td>
<td>
<p>logical vector indicating which data are to be used in validation
set, default=all FALSE (no validation set).</p>
</td></tr>
<tr><td><code id="binseg_+3A_position.vec">position.vec</code></td>
<td>
<p>integer vector of positions at which data are measured,
default=1:length(<code>data.vec</code>).</p>
</td></tr>
<tr><td><code id="binseg_+3A_weight.vec">weight.vec</code></td>
<td>
<p>Numeric vector of non-negative weights for each data point.</p>
</td></tr>
<tr><td><code id="binseg_+3A_min.segment.length">min.segment.length</code></td>
<td>
<p>Positive integer, minimum number of data points per
segment. Default NULL means to use min given <code>distribution.str</code>.</p>
</td></tr>
<tr><td><code id="binseg_+3A_container.str">container.str</code></td>
<td>
<p>C++ container to use for storing breakpoints/cost. Most users
should leave this at the default &quot;multiset&quot; for efficiency but you
could use &quot;list&quot; if you want to study the time complexity of a
slower implementation of binary segmentation.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Each iteration involves first computing and storing the
best split point on one or two segments, then looking up the
segment with the best split so far. The best case time complexity
occurs when splits are equal (N data split into two segments of
size N/2), and the worst case is when splits are unequal (N data
split into one big segment with N-1 data and one small segment
with 1 data point). Looking up the segment with the best split so
far is a constant O(1) time operation using C++ multimap, so O(K)
overall for K iterations/segments. Storage of a new best split
point/cost involves the multimap insert method which is
logarithmic time in the size of the multimap, overall O(K log K)
for equal splits and O(K) for unequal splits. Computing the cost
values, and overall time complexity, depends on the loss. For
normal and poisson distributions the best case O(N log K) time
for equal splits and worst case O(N K) time for unequal
splits. For l1/laplace distributions the best case is O(N log N
log K) time for equal splits and worst case is O(N log N K) time
for unequal splits.</p>


<h3>Value</h3>

<p>list of class binsegRcpp with elements <code>min.segment.length</code>,
<code>distribution.str</code>, param.names, subtrain.borders and splits, which
is a data.table with columns:
</p>
<table>
<tr><td><code>segments</code></td>
<td>
<p>number of segments</p>
</td></tr>
<tr><td><code>loss</code></td>
<td>
<p>subtrain loss</p>
</td></tr>
<tr><td><code>validation.loss</code></td>
<td>
<p>validation loss</p>
</td></tr>
<tr><td><code>end</code></td>
<td>
<p>index of last data point per segment</p>
</td></tr>
<tr><td><code>depth</code></td>
<td>
<p>number of splits to reach segment</p>
</td></tr>
<tr><td><code>before</code></td>
<td>
<p>params before changepoint</p>
</td></tr>
<tr><td><code>after</code></td>
<td>
<p>params after changepoint</p>
</td></tr>
<tr><td><code>before.size</code></td>
<td>
<p>number of data before changepoint</p>
</td></tr>
<tr><td><code>after.size</code></td>
<td>
<p>number of data after changepoint</p>
</td></tr>
<tr><td><code>invalidates.index</code></td>
<td>
<p>index of param invalidated by this split.</p>
</td></tr>
<tr><td><code>invalidates.after</code></td>
<td>
<p>indicates if before/after params invalidated by this split.</p>
</td></tr></table>


<h3>Author(s)</h3>

<p>Toby Dylan Hocking</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data.table::setDTthreads(1)

x &lt;- c(0.1, 0, 1, 1.1, 0.1, 0)
## Compute full path of binary segmentation models from 1 to 6
## segments.
(models &lt;- binsegRcpp::binseg("mean_norm", x))

## Plot loss values using base graphics.
plot(models)

## Same loss values using ggplot2.
if(require("ggplot2")){
  ggplot()+
    geom_point(aes(
      segments, loss),
      data=models$splits)
}

## Compute data table of segments to plot.
(segs.dt &lt;- coef(models, 2:4))

## Plot data, segments, changepoints.
if(require("ggplot2")){
  ggplot()+
    theme_bw()+
    theme(panel.spacing=grid::unit(0, "lines"))+
    facet_grid(segments ~ ., labeller=label_both)+
    geom_vline(aes(
      xintercept=start.pos),
      color="green",
      data=segs.dt[1&lt;start])+
    geom_segment(aes(
      start.pos, mean,
      xend=end.pos, yend=mean),
      data=segs.dt,
      color="green")+
    xlab("Position/index")+
    ylab("Data/mean value")+
    geom_point(aes(
      pos, x),
      data=data.frame(x, pos=seq_along(x)))
}

## Use min.segment.length to constrain segment sizes.
(constrained.models &lt;- binsegRcpp::binseg("mean_norm", x, min.segment.length = 2L))

## Demonstration of model selection using cross-validation in
## simulated data.
seg.mean.vec &lt;- 1:5
data.mean.vec &lt;- rep(seg.mean.vec, each=20)
set.seed(1)
n.data &lt;- length(data.mean.vec)
data.vec &lt;- rnorm(n.data, data.mean.vec, 0.2)
plot(data.vec)

library(data.table)
loss.dt &lt;- data.table(seed=1:10)[, {
  set.seed(seed)
  is.valid &lt;- sample(rep(c(TRUE,FALSE), l=n.data))
  bs.model &lt;- binsegRcpp::binseg("mean_norm", data.vec, is.validation.vec=is.valid)
  bs.model$splits[, data.table(
    segments,
    validation.loss)]
}, by=seed]
loss.stats &lt;- loss.dt[, .(
  mean.valid.loss=mean(validation.loss)
), by=segments]
plot(
  mean.valid.loss ~ segments, loss.stats,
  col=ifelse(
    mean.valid.loss==min(mean.valid.loss),
    "black",
    "red"))

selected.segments &lt;- loss.stats[which.min(mean.valid.loss), segments]
full.model &lt;- binsegRcpp::binseg("mean_norm", data.vec, selected.segments)
(segs.dt &lt;- coef(full.model, selected.segments))
if(require("ggplot2")){
  ggplot()+
    theme_bw()+
    theme(panel.spacing=grid::unit(0, "lines"))+
    geom_vline(aes(
      xintercept=start.pos),
      color="green",
      data=segs.dt[1&lt;start])+
    geom_segment(aes(
      start.pos, mean,
      xend=end.pos, yend=mean),
      data=segs.dt,
      color="green")+
    xlab("Position/index")+
    ylab("Data/mean value")+
    geom_point(aes(
      pos, data.vec),
      data=data.frame(data.vec, pos=seq_along(data.vec)))
}

## Demo of poisson loss, weights.
data.vec &lt;- c(3,4,10,20)
(fit1 &lt;- binsegRcpp::binseg("poisson", data.vec, weight.vec=c(1,1,1,10)))
coef(fit1, 2L)
(fit2 &lt;- binsegRcpp::binseg("poisson", data.vec, weight.vec=c(1,1,10,1)))
coef(fit2, 2L)

</code></pre>

<hr>
<h2 id='binseg_interface'>binseg interface</h2><span id='topic+binseg_interface'></span>

<h3>Description</h3>

<p>Low-level interface to binary segmentation algorithm.</p>


<h3>Usage</h3>

<pre><code class='language-R'>binseg_interface(data_vec, 
    weight_vec, max_segments, 
    min_segment_length, 
    distribution_str, 
    container_str, is_validation_vec, 
    position_vec)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="binseg_interface_+3A_data_vec">data_vec</code></td>
<td>
<p>data_vec </p>
</td></tr>
<tr><td><code id="binseg_interface_+3A_weight_vec">weight_vec</code></td>
<td>
<p>weight_vec </p>
</td></tr>
<tr><td><code id="binseg_interface_+3A_max_segments">max_segments</code></td>
<td>
<p>max_segments </p>
</td></tr>
<tr><td><code id="binseg_interface_+3A_min_segment_length">min_segment_length</code></td>
<td>
<p>min_segment_length </p>
</td></tr>
<tr><td><code id="binseg_interface_+3A_distribution_str">distribution_str</code></td>
<td>
<p>distribution_str </p>
</td></tr>
<tr><td><code id="binseg_interface_+3A_container_str">container_str</code></td>
<td>
<p>container_str </p>
</td></tr>
<tr><td><code id="binseg_interface_+3A_is_validation_vec">is_validation_vec</code></td>
<td>
<p>is_validation_vec </p>
</td></tr>
<tr><td><code id="binseg_interface_+3A_position_vec">position_vec</code></td>
<td>
<p>position_vec </p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Toby Dylan Hocking</p>

<hr>
<h2 id='binseg_normal'>Binary segmentation, normal change in mean</h2><span id='topic+binseg_normal'></span>

<h3>Description</h3>

<p>Calls <code><a href="#topic+binseg">binseg</a></code> to compute a binary segmentation model for change in
mean with constant variance, max normal likelihood = min square
loss.</p>


<h3>Usage</h3>

<pre><code class='language-R'>binseg_normal(data.vec, 
    max.segments = sum(!is.validation.vec), 
    is.validation.vec = rep(FALSE, 
        length(data.vec)), 
    position.vec = seq_along(data.vec))</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="binseg_normal_+3A_data.vec">data.vec</code></td>
<td>
<p>Vector of numeric data to segment.</p>
</td></tr>
<tr><td><code id="binseg_normal_+3A_max.segments">max.segments</code></td>
<td>
<p>Maximum number of segments to compute, default=number of FALSE
entries in <code>is.validation.vec</code>.</p>
</td></tr>
<tr><td><code id="binseg_normal_+3A_is.validation.vec">is.validation.vec</code></td>
<td>
<p>logical vector indicating which data are to be used in validation
set, default=all FALSE (no validation set).</p>
</td></tr>
<tr><td><code id="binseg_normal_+3A_position.vec">position.vec</code></td>
<td>
<p>integer vector of positions at which data are measured,
default=1:length(<code>data.vec</code>).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List output from <code><a href="#topic+binseg">binseg</a></code> which represents a binary segmentation
model.</p>


<h3>Author(s)</h3>

<p>Toby Dylan Hocking</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data.table::setDTthreads(1)

x &lt;- c(0.1, 0, 1, 1.1, 0.1, 0)
## Compute full path of binary segmentation models from 1 to 6
## segments.
(models &lt;- binsegRcpp::binseg_normal(x))

## Plot loss values using base graphics.
plot(models)

## Same loss values using ggplot2.
if(require("ggplot2")){
  ggplot()+
    geom_point(aes(
      segments, loss),
      data=models$splits)
}

## Compute data table of segments to plot.
(segs.dt &lt;- coef(models, 2:4))

## Plot data, segments, changepoints.
if(require("ggplot2")){
  ggplot()+
    theme_bw()+
    theme(panel.spacing=grid::unit(0, "lines"))+
    facet_grid(segments ~ ., labeller=label_both)+
    geom_vline(aes(
      xintercept=start.pos),
      color="green",
      data=segs.dt[1&lt;start])+
    geom_segment(aes(
      start.pos, mean,
      xend=end.pos, yend=mean),
      data=segs.dt,
      color="green")+
    xlab("Position/index")+
    ylab("Data/mean value")+
    geom_point(aes(
      pos, x),
      data=data.frame(x, pos=seq_along(x)))
}

## Demonstration of model selection using cross-validation in
## simulated data.
seg.mean.vec &lt;- 1:5
data.mean.vec &lt;- rep(seg.mean.vec, each=20)
set.seed(1)
n.data &lt;- length(data.mean.vec)
data.vec &lt;- rnorm(n.data, data.mean.vec, 0.2)
plot(data.vec)

library(data.table)
loss.dt &lt;- data.table(seed=1:10)[, {
  set.seed(seed)
  is.valid &lt;- sample(rep(c(TRUE,FALSE), l=n.data))
  bs.model &lt;- binsegRcpp::binseg_normal(data.vec, is.validation.vec=is.valid)
  bs.model$splits[, data.table(
    segments,
    validation.loss)]
}, by=seed]
loss.stats &lt;- loss.dt[, .(
  mean.valid.loss=mean(validation.loss)
), by=segments]
plot(
  mean.valid.loss ~ segments, loss.stats,
  col=ifelse(
    mean.valid.loss==min(mean.valid.loss),
    "black",
    "red"))

selected.segments &lt;- loss.stats[which.min(mean.valid.loss), segments]
full.model &lt;- binsegRcpp::binseg_normal(data.vec, selected.segments)
(segs.dt &lt;- coef(full.model, selected.segments))
if(require("ggplot2")){
  ggplot()+
    theme_bw()+
    theme(panel.spacing=grid::unit(0, "lines"))+
    geom_vline(aes(
      xintercept=start.pos),
      color="green",
      data=segs.dt[1&lt;start])+
    geom_segment(aes(
      start.pos, mean,
      xend=end.pos, yend=mean),
      data=segs.dt,
      color="green")+
    xlab("Position/index")+
    ylab("Data/mean value")+
    geom_point(aes(
      pos, data.vec),
      data=data.frame(data.vec, pos=seq_along(data.vec)))
}

</code></pre>

<hr>
<h2 id='binseg_normal_cv'>Binary segmentation, normal change in mean, cross-validation for model selection</h2><span id='topic+binseg_normal_cv'></span>

<h3>Description</h3>

<p>Efficient implementation of binary segmentation for change in
mean, with automatic model selection via cross-validation.</p>


<h3>Usage</h3>

<pre><code class='language-R'>binseg_normal_cv(data.vec, 
    max.segments = length(data.vec), 
    position.vec = seq_along(data.vec), 
    n.validation.sets = 100L, 
    prop.validation = 0.5)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="binseg_normal_cv_+3A_data.vec">data.vec</code></td>
<td>
<p>Vector of numeric data to segment.</p>
</td></tr>
<tr><td><code id="binseg_normal_cv_+3A_max.segments">max.segments</code></td>
<td>
<p>Maximum number of segments to compute, default=length(<code>data.vec</code>).</p>
</td></tr>
<tr><td><code id="binseg_normal_cv_+3A_position.vec">position.vec</code></td>
<td>
<p>integer vector of positions at which data are measured,
default=1:length(<code>data.vec</code>).</p>
</td></tr>
<tr><td><code id="binseg_normal_cv_+3A_n.validation.sets">n.validation.sets</code></td>
<td>
<p>Number of validation sets.</p>
</td></tr>
<tr><td><code id="binseg_normal_cv_+3A_prop.validation">prop.validation</code></td>
<td>
<p>Proportion of validation set.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Toby Dylan Hocking</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data.table::setDTthreads(1)

seg.mean.vec &lt;- 1:5
data.mean.vec &lt;- rep(seg.mean.vec, each=20)
set.seed(1)
n.data &lt;- length(data.mean.vec)
data.vec &lt;- rnorm(n.data, data.mean.vec, 0.2)
plot(data.vec)
(fit &lt;- binsegRcpp::binseg_normal_cv(data.vec))
seg.dt &lt;- coef(fit)
model.color &lt;- "red"
seg.dt[, segments(start.pos, mean, end.pos, mean, col=model.color)]
seg.dt[start&gt;1, abline(v=start.pos, col=model.color)]

## plot method shows number of times selected.
plot(fit)

if(requireNamespace("neuroblastoma")){
  data(neuroblastoma, package="neuroblastoma", envir=environment())
  library(data.table)
  profiles.dt &lt;- data.table(neuroblastoma$profiles)
  one.chrom &lt;- profiles.dt[profile.id=="4" &amp; chromosome=="2"]
  fit &lt;- one.chrom[, binsegRcpp::binseg_normal_cv(
    logratio, position.vec=position)]
  selected.segs &lt;- coef(fit)
  if(require(ggplot2)){
    ggplot()+
      geom_point(aes(
        position, logratio),
        data=one.chrom)+
      geom_segment(aes(
        start.pos, mean,
        xend=end.pos, yend=mean),
        data=selected.segs,
        color=model.color)+
      geom_vline(aes(
        xintercept=start.pos),
        data=selected.segs[start&gt;1],
        color=model.color)
  }
}

</code></pre>

<hr>
<h2 id='case.colors'>case colors</h2><span id='topic+case.colors'></span>

<h3>Description</h3>

<p>Character vector giving default colors for cases, ordered from
worst to best.</p>


<h3>Usage</h3>

<pre><code class='language-R'>"case.colors"</code></pre>

<hr>
<h2 id='case.sizes'>case sizes</h2><span id='topic+case.sizes'></span>

<h3>Description</h3>

<p>Numeric vector giving default sizes for cases.</p>


<h3>Usage</h3>

<pre><code class='language-R'>"case.sizes"</code></pre>

<hr>
<h2 id='check_sizes'>check sizes</h2><span id='topic+check_sizes'></span>

<h3>Description</h3>

<p>Checks types and values of size inputs.</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_sizes(N.data, min.segment.length, 
    n.segments)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="check_sizes_+3A_n.data">N.data</code></td>
<td>
<p>N.data </p>
</td></tr>
<tr><td><code id="check_sizes_+3A_min.segment.length">min.segment.length</code></td>
<td>
<p>min.segment.length </p>
</td></tr>
<tr><td><code id="check_sizes_+3A_n.segments">n.segments</code></td>
<td>
<p>n.segments </p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Toby Dylan Hocking</p>

<hr>
<h2 id='coef.binseg_normal_cv'>coef binseg normal cv</h2><span id='topic+coef.binseg_normal_cv'></span>

<h3>Description</h3>

<p>Compute a data table of segment start/end/mean values for all
models given by <code>segments</code>.</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'binseg_normal_cv'
coef(object, 
    segments = max(nrow(object$splits)), 
    ...)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="coef.binseg_normal_cv_+3A_object">object</code></td>
<td>
<p>data.table from <code><a href="#topic+binseg_normal_cv">binseg_normal_cv</a></code>.</p>
</td></tr>
<tr><td><code id="coef.binseg_normal_cv_+3A_segments">segments</code></td>
<td>
<p>integer vector, model sizes in number of <code>segments</code>. default=number
of selected <code>segments</code>.</p>
</td></tr>
<tr><td><code id="coef.binseg_normal_cv_+3A_...">...</code></td>
<td>
<p>ignored.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>data.table with one row for each segment.</p>


<h3>Author(s)</h3>

<p>Toby Dylan Hocking</p>

<hr>
<h2 id='coef.binsegRcpp'>coef binsegRcpp</h2><span id='topic+coef.binsegRcpp'></span>

<h3>Description</h3>

<p>Compute a data table of segment start/end/mean values for all
models given by <code>segments</code>.</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'binsegRcpp'
coef(object, 
    segments = 1:min(nrow(object$splits), 
        10), ...)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="coef.binsegRcpp_+3A_object">object</code></td>
<td>
<p>data.table from <code><a href="#topic+binseg">binseg</a></code>.</p>
</td></tr>
<tr><td><code id="coef.binsegRcpp_+3A_segments">segments</code></td>
<td>
<p>integer vector, model sizes in number of <code>segments</code>.</p>
</td></tr>
<tr><td><code id="coef.binsegRcpp_+3A_...">...</code></td>
<td>
<p>ignored.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>data.table with one row for each segment.</p>


<h3>Author(s)</h3>

<p>Toby Dylan Hocking</p>

<hr>
<h2 id='cum_median'>cum median</h2><span id='topic+cum_median'></span>

<h3>Description</h3>

<p>Efficient log-linear cumulative median.</p>


<h3>Usage</h3>

<pre><code class='language-R'>cum_median(data.vec, 
    weight.vec = rep(1, 
        length(data.vec)))</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cum_median_+3A_data.vec">data.vec</code></td>
<td>
<p>Numeric vector of data.</p>
</td></tr>
<tr><td><code id="cum_median_+3A_weight.vec">weight.vec</code></td>
<td>
<p>Numeric vector of weights.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Toby Dylan Hocking</p>

<hr>
<h2 id='cum_median_interface'>cum median interface</h2><span id='topic+cum_median_interface'></span>

<h3>Description</h3>

<p>Efficient log-linear cumulative median.</p>


<h3>Usage</h3>

<pre><code class='language-R'>cum_median_interface(data_vec, 
    weight_vec)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cum_median_interface_+3A_data_vec">data_vec</code></td>
<td>
<p>data_vec </p>
</td></tr>
<tr><td><code id="cum_median_interface_+3A_weight_vec">weight_vec</code></td>
<td>
<p>weight_vec </p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Toby Dylan Hocking</p>

<hr>
<h2 id='depth_first_interface'>depth first interface</h2><span id='topic+depth_first_interface'></span>

<h3>Description</h3>

<p>Use depth first search to compute a data.frame
with one row for each segment, and columns
splits and depth, number/depth of candidate
splits that need to be
computed after splitting that segment.</p>


<h3>Usage</h3>

<pre><code class='language-R'>depth_first_interface(n_data, 
    min_segment_length)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="depth_first_interface_+3A_n_data">n_data</code></td>
<td>
<p>n_data </p>
</td></tr>
<tr><td><code id="depth_first_interface_+3A_min_segment_length">min_segment_length</code></td>
<td>
<p>min_segment_length </p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Toby Dylan Hocking</p>

<hr>
<h2 id='get_complexity'>get complexity</h2><span id='topic+get_complexity'></span>

<h3>Description</h3>

<p>Get empirical and extreme split counts, in order to compare the
empirical and theoretical time complexity of the binary
segmentation algorithm.</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_complexity(models, 
    y.increment = 0.1)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_complexity_+3A_models">models</code></td>
<td>
<p>result of <code><a href="#topic+binseg">binseg</a></code>.</p>
</td></tr>
<tr><td><code id="get_complexity_+3A_y.increment">y.increment</code></td>
<td>
<p>Offset for y column values of totals output table.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List of class &quot;complexity&quot; which has a plot method. Elements
include &quot;iterations&quot; which is a data table with one row per model
size, and column splits with number of splits to check after
computing that model size; &quot;totals&quot; which is a data table with
total number of splits for each case. </p>


<h3>Author(s)</h3>

<p>Toby Dylan Hocking</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Example 1: empirical=worst case.
data.vec &lt;- rep(0:1, l=8)
plot(data.vec)
worst.model &lt;- binsegRcpp::binseg_normal(data.vec)
worst.counts &lt;- binsegRcpp::get_complexity(worst.model)
plot(worst.counts)

## Example 2: empirical=best case for full path.
data.vec &lt;- 1:8
plot(data.vec)
full.model &lt;- binsegRcpp::binseg_normal(data.vec)
full.counts &lt;- binsegRcpp::get_complexity(full.model)
plot(full.counts)

## Example 3: empirical=best case for all partial paths.
data.vec &lt;- c(0,3,6,10,21,22,23,24)
plot(data.vec)
best.model &lt;- binsegRcpp::binseg_normal(data.vec)
best.counts &lt;- binsegRcpp::get_complexity(best.model)
plot(best.counts)

## ggplot comparing examples 1-3.
if(require("ggplot2")){
  library(data.table)
  splits.list &lt;- list()
  for(data.type in names(worst.counts)){
    splits.list[[data.type]] &lt;- rbind(
      data.table(data="worst", worst.counts[[data.type]]),
      data.table(data="best always", best.counts[[data.type]]),
      data.table(data="best full", full.counts[[data.type]]))
  }
  ggplot()+
    facet_grid(data ~ .)+
    geom_line(aes(
      segments, cum.splits, color=case, size=case),
      data=splits.list$iterations[case!="empirical"])+
    geom_point(aes(
      segments, cum.splits, color=case),
      data=splits.list$iterations[case=="empirical"])+
    scale_color_manual(
      values=binsegRcpp::case.colors,
      breaks=names(binsegRcpp::case.colors))+
    scale_size_manual(
      values=binsegRcpp::case.sizes,
      guide="none")
}

## Example 4: empirical case between best/worst.
data.vec &lt;- rep(c(0,1,10,11),8)
plot(data.vec)
m.model &lt;- binsegRcpp::binseg_normal(data.vec)
m.splits &lt;- binsegRcpp::get_complexity(m.model)
plot(m.splits)

## Example 5: worst case for normal change in mean and variance
## model.
mv.model &lt;- binsegRcpp::binseg("meanvar_norm", data.vec)
mv.splits &lt;- binsegRcpp::get_complexity(mv.model)
plot(mv.splits)

## Compare examples 4-5 using ggplot2.
if(require("ggplot2")){
  library(data.table)
  splits.list &lt;- list()
  for(data.type in names(m.splits)){
    splits.list[[data.type]] &lt;- rbind(
      data.table(model="mean and variance", mv.splits[[data.type]]),
      data.table(model="mean only", m.splits[[data.type]]))
  }
  ggplot()+
    facet_grid(model ~ .)+
    geom_line(aes(
      segments, splits, color=case, size=case),
      data=splits.list$iterations[case!="empirical"])+
    geom_point(aes(
      segments, splits, color=case),
      data=splits.list$iterations[case=="empirical"])+
    geom_text(aes(
      x, y,
      label=label,
      color=case),
      hjust=1,
      data=splits.list$totals)+
    scale_color_manual(
      values=binsegRcpp::case.colors,
      guide="none")+
    scale_size_manual(
      values=binsegRcpp::case.sizes,
      guide="none")
}

## Compare cumsums.
if(require("ggplot2")){
  library(data.table)
  splits.list &lt;- list()
  for(data.type in names(m.splits)){
    splits.list[[data.type]] &lt;- rbind(
      data.table(model="mean and variance", mv.splits[[data.type]]),
      data.table(model="mean only", m.splits[[data.type]]))
  }
  ggplot()+
    facet_grid(model ~ .)+
    geom_line(aes(
      segments, cum.splits, color=case, size=case),
      data=splits.list$iterations[case!="empirical"])+
    geom_point(aes(
      segments, cum.splits, color=case),
      data=splits.list$iterations[case=="empirical"])+
    scale_color_manual(
      values=binsegRcpp::case.colors,
      breaks=names(binsegRcpp::case.colors))+
    scale_size_manual(
      values=binsegRcpp::case.sizes,
      guide="none")
}

</code></pre>

<hr>
<h2 id='get_complexity_best_heuristic_equal_breadth_full'>get complexity best heuristic equal breadth full</h2><span id='topic+get_complexity_best_heuristic_equal_breadth_full'></span>

<h3>Description</h3>

<p>Compute a fast approximate best case based on equal size splits.</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_complexity_best_heuristic_equal_breadth_full(N.data, 
    min.segment.length)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_complexity_best_heuristic_equal_breadth_full_+3A_n.data">N.data</code></td>
<td>
<p>N.data </p>
</td></tr>
<tr><td><code id="get_complexity_best_heuristic_equal_breadth_full_+3A_min.segment.length">min.segment.length</code></td>
<td>
<p>min.segment.length </p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Toby Dylan Hocking</p>

<hr>
<h2 id='get_complexity_best_heuristic_equal_depth_full'>get complexity best heuristic equal depth full</h2><span id='topic+get_complexity_best_heuristic_equal_depth_full'></span>

<h3>Description</h3>

<p>Heuristic depth first.</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_complexity_best_heuristic_equal_depth_full(N.data, 
    min.segment.length)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_complexity_best_heuristic_equal_depth_full_+3A_n.data">N.data</code></td>
<td>
<p>N.data </p>
</td></tr>
<tr><td><code id="get_complexity_best_heuristic_equal_depth_full_+3A_min.segment.length">min.segment.length</code></td>
<td>
<p>min.segment.length </p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Toby Dylan Hocking</p>

<hr>
<h2 id='get_complexity_best_optimal_cost'>get complexity best optimal cost</h2><span id='topic+get_complexity_best_optimal_cost'></span>

<h3>Description</h3>

<p>Dynamic programming for computing lower bound on number of split
candidates to compute / best case of binary segmentation. The
dynamic programming recursion is on f(d,s) = best number of splits
for segment of size s which is split d times. Need to optimize
f(d,s) = g(s) + min f(d1,s1) + f(d2,s2) over s1,d1 given that
s1+s2=s, d1+d2+1=d, and g(s) is the number of splits for segment
of size s.</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_complexity_best_optimal_cost(N.data, 
    min.segment.length = 1L, 
    n.segments = NULL)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_complexity_best_optimal_cost_+3A_n.data">N.data</code></td>
<td>
<p>positive integer number of data.</p>
</td></tr>
<tr><td><code id="get_complexity_best_optimal_cost_+3A_min.segment.length">min.segment.length</code></td>
<td>
<p>positive integer min segment length.</p>
</td></tr>
<tr><td><code id="get_complexity_best_optimal_cost_+3A_n.segments">n.segments</code></td>
<td>
<p>positive integer number of segments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>data table with one row for each f(d,s) value computed.</p>


<h3>Author(s)</h3>

<p>Toby Dylan Hocking</p>


<h3>Examples</h3>

<pre><code class='language-R'>
binsegRcpp::get_complexity_best_optimal_cost(
  N.data = 19L, 
  min.segment.length = 3L, 
  n.segments = 4L)

</code></pre>

<hr>
<h2 id='get_complexity_best_optimal_splits'>get complexity best optimal splits</h2><span id='topic+get_complexity_best_optimal_splits'></span>

<h3>Description</h3>

<p>Convert output of <code><a href="#topic+get_complexity_best_optimal_tree">get_complexity_best_optimal_tree</a></code> to counts of
candidate splits that need to be considered at each iteration.</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_complexity_best_optimal_splits(node.dt, 
    min.segment.length)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_complexity_best_optimal_splits_+3A_node.dt">node.dt</code></td>
<td>
<p>node.dt </p>
</td></tr>
<tr><td><code id="get_complexity_best_optimal_splits_+3A_min.segment.length">min.segment.length</code></td>
<td>
<p>min.segment.length </p>
</td></tr>
</table>


<h3>Value</h3>

<p>Data table with one row for each segment.</p>


<h3>Author(s)</h3>

<p>Toby Dylan Hocking</p>

<hr>
<h2 id='get_complexity_best_optimal_tree'>get complexity best optimal tree</h2><span id='topic+get_complexity_best_optimal_tree'></span>

<h3>Description</h3>

<p>decoding.</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_complexity_best_optimal_tree(f.dt)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_complexity_best_optimal_tree_+3A_f.dt">f.dt</code></td>
<td>
<p>f.dt </p>
</td></tr>
</table>


<h3>Value</h3>

<p>Data table with one row for each node in the tree.</p>


<h3>Author(s)</h3>

<p>Toby Dylan Hocking</p>


<h3>Examples</h3>

<pre><code class='language-R'>
N.data &lt;- 19L
min.seg.len &lt;- 3L
max.segments &lt;- 4L
cost.dt &lt;- binsegRcpp::get_complexity_best_optimal_cost(
  N.data, min.seg.len, max.segments)
binsegRcpp::get_complexity_best_optimal_tree(cost.dt)

</code></pre>

<hr>
<h2 id='get_complexity_empirical'>get complexity empirical</h2><span id='topic+get_complexity_empirical'></span>

<h3>Description</h3>

<p>Get empirical split counts. This is a sub-routine of
<code><a href="#topic+get_complexity">get_complexity</a></code>, which should typically be used instead.</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_complexity_empirical(model.dt, 
    min.segment.length = 1L)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_complexity_empirical_+3A_model.dt">model.dt</code></td>
<td>
<p>splits data table from <code><a href="#topic+binseg">binseg</a></code> result list.</p>
</td></tr>
<tr><td><code id="get_complexity_empirical_+3A_min.segment.length">min.segment.length</code></td>
<td>
<p>Minimum segment length, positive integer.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>data.table with one row per model size, and column splits with
number of splits to check after computing that model size.</p>


<h3>Author(s)</h3>

<p>Toby Dylan Hocking</p>

<hr>
<h2 id='get_complexity_extreme'>get complexity extreme</h2><span id='topic+get_complexity_extreme'></span>

<h3>Description</h3>

<p>Compute best and worst case number of splits.</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_complexity_extreme(N.data, 
    min.segment.length = 1L, 
    n.segments = NULL)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_complexity_extreme_+3A_n.data">N.data</code></td>
<td>
<p>number of data to segment, positive integer.</p>
</td></tr>
<tr><td><code id="get_complexity_extreme_+3A_min.segment.length">min.segment.length</code></td>
<td>
<p>minimum segment length, positive integer.</p>
</td></tr>
<tr><td><code id="get_complexity_extreme_+3A_n.segments">n.segments</code></td>
<td>
<p>number of segments, positive integer.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>data.table with one row per model size, and column splits with
number of splits to check after computing that model size. Column
case has values best (equal segment sizes, min splits to check)
and worst (unequal segment sizes, max splits to check).</p>


<h3>Author(s)</h3>

<p>Toby Dylan Hocking</p>

<hr>
<h2 id='get_complexity_worst'>get complexity worst</h2><span id='topic+get_complexity_worst'></span>

<h3>Description</h3>

<p>Get full sequence of splits which results in worst case time
complexity.</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_complexity_worst(N.data, 
    min.segment.length)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_complexity_worst_+3A_n.data">N.data</code></td>
<td>
<p>N.data </p>
</td></tr>
<tr><td><code id="get_complexity_worst_+3A_min.segment.length">min.segment.length</code></td>
<td>
<p>min.segment.length </p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Toby Dylan Hocking</p>

<hr>
<h2 id='get_distribution_info'>get distribution info</h2><span id='topic+get_distribution_info'></span>

<h3>Description</h3>

<p>Compute a data.frame with one row for each distribution
implemented in the C++ code, and columns distribution.str,
parameters, description.</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_distribution_info()</code></pre>


<h3>Author(s)</h3>

<p>Toby Dylan Hocking</p>

<hr>
<h2 id='get_tree_empirical'>get tree empirical</h2><span id='topic+get_tree_empirical'></span>

<h3>Description</h3>

<p>Compute tree for empirical binary segmentation model.</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_tree_empirical(fit)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_tree_empirical_+3A_fit">fit</code></td>
<td>
<p>fit </p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Toby Dylan Hocking</p>

<hr>
<h2 id='plot.binseg_normal_cv'>plot binseg normal cv</h2><span id='topic+plot.binseg_normal_cv'></span>

<h3>Description</h3>

<p>Plot loss values from binary segmentation.</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'binseg_normal_cv'
plot(x, 
    ...)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.binseg_normal_cv_+3A_x">x</code></td>
<td>
<p>data.table from <code><a href="#topic+binseg_normal_cv">binseg_normal_cv</a></code>.</p>
</td></tr>
<tr><td><code id="plot.binseg_normal_cv_+3A_...">...</code></td>
<td>
<p>ignored.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Toby Dylan Hocking</p>

<hr>
<h2 id='plot.binsegRcpp'>plot binsegRcpp</h2><span id='topic+plot.binsegRcpp'></span>

<h3>Description</h3>

<p>Plot loss values from binary segmentation.</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'binsegRcpp'
plot(x, ...)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.binsegRcpp_+3A_x">x</code></td>
<td>
<p>data.table from <code><a href="#topic+binseg">binseg</a></code>.</p>
</td></tr>
<tr><td><code id="plot.binsegRcpp_+3A_...">...</code></td>
<td>
<p>ignored.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Toby Dylan Hocking</p>

<hr>
<h2 id='plot.complexity'>plot complexity</h2><span id='topic+plot.complexity'></span>

<h3>Description</h3>

<p>Plot comparing empirical number of splits to best/worst case.</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'complexity'
plot(x, ...)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.complexity_+3A_x">x</code></td>
<td>
<p>data.table from <code><a href="#topic+get_complexity">get_complexity</a></code>.</p>
</td></tr>
<tr><td><code id="plot.complexity_+3A_...">...</code></td>
<td>
<p>ignored.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Toby Dylan Hocking</p>

<hr>
<h2 id='print.binseg_normal_cv'>print binseg normal cv</h2><span id='topic+print.binseg_normal_cv'></span>

<h3>Description</h3>

<p>Print method for <code><a href="#topic+binseg_normal_cv">binseg_normal_cv</a></code>.</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'binseg_normal_cv'
print(x, 
    ...)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.binseg_normal_cv_+3A_x">x</code></td>
<td>
<p>data.table from <code><a href="#topic+binseg_normal_cv">binseg_normal_cv</a></code>.</p>
</td></tr>
<tr><td><code id="print.binseg_normal_cv_+3A_...">...</code></td>
<td>
<p>ignored.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Toby Dylan Hocking</p>

<hr>
<h2 id='print.binsegRcpp'>print binsegRcpp</h2><span id='topic+print.binsegRcpp'></span>

<h3>Description</h3>

<p>Print method for binsegRcpp.</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'binsegRcpp'
print(x, ...)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.binsegRcpp_+3A_x">x</code></td>
<td>
<p>data.table from <code><a href="#topic+binseg">binseg</a></code>.</p>
</td></tr>
<tr><td><code id="print.binsegRcpp_+3A_...">...</code></td>
<td>
<p>ignored.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Toby Dylan Hocking</p>

<hr>
<h2 id='qp.x'>qp x</h2><span id='topic+qp.x'></span>

<h3>Description</h3>

<p>Solve quadratic program to find x positions.</p>


<h3>Usage</h3>

<pre><code class='language-R'>qp.x(target, y.up, y.lo)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="qp.x_+3A_target">target</code></td>
<td>
<p>target </p>
</td></tr>
<tr><td><code id="qp.x_+3A_y.up">y.up</code></td>
<td>
<p>y.up </p>
</td></tr>
<tr><td><code id="qp.x_+3A_y.lo">y.lo</code></td>
<td>
<p>y.lo </p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Toby Dylan Hocking</p>

<hr>
<h2 id='random_set_vec'>random set vec</h2><span id='topic+random_set_vec'></span>

<h3>Description</h3>

<p>Random set assignment.</p>


<h3>Usage</h3>

<pre><code class='language-R'>random_set_vec(N, props.vec)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="random_set_vec_+3A_n">N</code></td>
<td>
<p>integer, size of output vector.</p>
</td></tr>
<tr><td><code id="random_set_vec_+3A_props.vec">props.vec</code></td>
<td>
<p>numeric vector of set proportions (must sum to one), with set
names.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Random vector of <code>N</code> set names.</p>


<h3>Author(s)</h3>

<p>Toby Dylan Hocking</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(data.table)
library(ggplot2)
library(binsegRcpp)
tvt.props &lt;- c(test=0.19, train=0.67, validation=0.14)
tvt.N &lt;- 1234567L
system.time({
  tvt.vec &lt;- random_set_vec(tvt.N, tvt.props)
})
table(tvt.vec, useNA="ifany")/tvt.N

random_set_vec(6L, c(train=2/3, test=1/3))
random_set_vec(5L, c(train=2/3, test=1/3))
random_set_vec(4L, c(train=2/3, test=1/3))
random_set_vec(3L, c(train=2/3, test=1/3))

test.rev &lt;- function(N, prop.vec, expected.vec){
  result &lt;- list()
  for(fun.name in c("identity", "rev")){
    fun &lt;- get(fun.name)
    ctab &lt;- table(random_set_vec(N, fun(prop.vec)))
    result[[fun.name]] &lt;- ctab
  }
  result$same &lt;- sapply(
    result, function(tab)identical(as.numeric(tab), expected.vec))
  result
}
test.rev(4L, c(test=1/3, train=2/3), c(1, 3))
table(random_set_vec(3L, c(test=0.5, train=0.5)))
table(random_set_vec(3L, c(train=0.5, test=0.5)))
test.rev(3L, c(test=0.4, train=0.6), c(1, 2))
test.rev(3L, c(test=0.49, train=0.51), c(1, 2))
test.rev(3L, c(test=0.6, train=0.4), c(2, 1))
## 2 is optimal after prob=2/3.
test.rev(2L, c(test=0.6, train=0.4), c(1, 1))
test.rev(2L, c(test=0.7, train=0.3), c(2))

## visualize the likelihood as a function of the proportion of
## success.
test.prop &lt;- seq(0, 1, by=0.01)
prob.dt.list &lt;- list()
n.total &lt;- 2
for(n.test in 0:n.total){
  prob.dt.list[[paste(n.test)]] &lt;- data.table(
    n.test,
    test.prop,
    prob=dbinom(n.test, n.total, test.prop))
}
prob.dt &lt;- do.call(rbind, prob.dt.list)
thresh.dt &lt;- data.table(thresh=(1:2)/3)
gg &lt;- ggplot()+
  geom_vline(aes(xintercept=thresh), data=thresh.dt)+
  geom_line(aes(
    test.prop, prob, color=n.test, group=n.test),
    data=prob.dt)
if(requireNamespace("directlabels")){
  directlabels::direct.label(gg, "last.polygons")
}else{
  gg
}

## visualize the binomial likelihood as a function of number of
## successes, for a given probability of success.
n.total &lt;- 43
n.success &lt;- 0:n.total
p.success &lt;- 0.6
lik.dt &lt;- data.table(
  n.success,
  prob=dbinom(n.success, n.total, p.success))
ggplot()+
  geom_point(aes(
    n.success, prob),
    data=lik.dt)+
  geom_vline(xintercept=(n.total+1)*p.success)

## visualize the multinomial likelihood as a function of number of
## successes, for a given probability of success.
n.total &lt;- 43
prob.vec &lt;- c(train=0.6, validation=0.3, test=0.1)
train.dt &lt;- data.table(train=0:n.total)
grid.dt &lt;- train.dt[, data.table(
  validation=0:(n.total-train)), by=train]
grid.dt[, prob := dmultinom(
  c(train, validation, n.total-train-validation),
  n.total,
  prob.vec),
  by=.(train, validation)]

train.bound &lt;- (n.total+1)*prob.vec[["train"]]
validation.bound &lt;- (n.total+1)*prob.vec[["validation"]]
guess.dt &lt;- data.table(
  train=floor(train.bound),
  validation=floor(validation.bound))
max.dt &lt;- grid.dt[which.max(prob)]#same
max.dt[, test := n.total-train-validation]

ggplot()+
  geom_tile(aes(
    train, validation, fill=prob),
    data=grid.dt)+
  scale_fill_gradient(low="white", high="red")+
  theme_bw()+
  geom_vline(
    xintercept=train.bound)+
  geom_hline(
    yintercept=validation.bound)+
  geom_point(aes(
    train, validation),
    shape=1,
    data=guess.dt)+
  coord_equal()

## visualize what happens when we start obs.seq variable above at 1
## or 0. starting at 0 is problematic e.g. 99% train/1% test with
## N=2 observations should return 2 train/0 test (and does when
## obs.seq starts with 1, but does NOT when obs.seq starts with 0).
random_set_vec(2L, c(train=0.99, test=0.01))
obs.dt.list &lt;- list()
cum.dt.list &lt;- list()
for(tvt.N in 2:4){
  obs.dt.list[[paste(tvt.N)]] &lt;- data.table(tvt.N, rbind(
    data.table(start=0, obs=seq(0, tvt.N, l=tvt.N)),
    data.table(start=1, obs=seq(1, tvt.N, l=tvt.N))))
  not.round &lt;- data.table(
    set=c("train", "test"),
    cum.thresh=tvt.N*c((tvt.N-2)/(tvt.N-1), 1))
  cum.dt.list[[paste(tvt.N)]] &lt;- data.table(tvt.N, rbind(
    data.table(round=FALSE, not.round),
    not.round[, .(round=TRUE, set, cum.thresh=round(cum.thresh))]))
}
cum.dt &lt;- do.call(rbind, cum.dt.list)
obs.dt &lt;- do.call(rbind, obs.dt.list)
ggplot()+
  theme_bw()+
  theme(panel.spacing=grid::unit(0, "lines"))+
  facet_grid(tvt.N ~ .)+
  geom_point(aes(
    obs, start),
    data=obs.dt)+
  geom_vline(aes(
    xintercept=cum.thresh, color=round, linetype=round),
    data=cum.dt)

</code></pre>

<hr>
<h2 id='size_to_splits'>size to splits</h2><span id='topic+size_to_splits'></span>

<h3>Description</h3>

<p>Convert segment <code>size</code> to number of splits which must be computed
during the optimization.</p>


<h3>Usage</h3>

<pre><code class='language-R'>size_to_splits(size, 
    min.segment.length)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="size_to_splits_+3A_size">size</code></td>
<td>
<p>Segment <code>size</code>, positive integer.</p>
</td></tr>
<tr><td><code id="size_to_splits_+3A_min.segment.length">min.segment.length</code></td>
<td>
<p>Minimum segment length, positive integer.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Number of splits, integer. </p>


<h3>Author(s)</h3>

<p>Toby Dylan Hocking</p>

<hr>
<h2 id='tree_layout'>tree layout</h2><span id='topic+tree_layout'></span>

<h3>Description</h3>

<p>Compute x,y coordinates for graphing a tree.</p>


<h3>Usage</h3>

<pre><code class='language-R'>tree_layout(node.dt, 
    space = 0.5)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tree_layout_+3A_node.dt">node.dt</code></td>
<td>
<p>node.dt </p>
</td></tr>
<tr><td><code id="tree_layout_+3A_space">space</code></td>
<td>
<p>space </p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Toby Dylan Hocking</p>


<h3>Examples</h3>

<pre><code class='language-R'>
N.data &lt;- 29L
min.seg.len &lt;- 3L
max.segments &lt;- 5L
cost.dt &lt;- binsegRcpp::get_complexity_best_optimal_cost(
  N.data, min.seg.len, max.segments)
set.seed(1)
data.vec &lt;- rnorm(N.data)
fit &lt;- binsegRcpp::binseg_normal(data.vec, max.segments)
tree.list &lt;- list(
  best=binsegRcpp::get_complexity_best_optimal_tree(cost.dt),
  empirical=binsegRcpp::get_tree_empirical(fit))
library(data.table)
tree.dt &lt;- data.table(type=names(tree.list))[, {
  binsegRcpp::tree_layout(tree.list[[type]])
}, by=type]
total.dt &lt;- tree.dt[, .(
  candidate.splits=sum(binsegRcpp::size_to_splits(size, min.seg.len))
), by=type]
join.dt &lt;- total.dt[tree.dt, on="type"]
if(require(ggplot2)){
  ggplot()+
    facet_grid(. ~ type + candidate.splits, labeller=label_both)+
    geom_segment(aes(
      x, depth, 
      xend=parent.x, yend=parent.depth),
      data=join.dt)+
    geom_label(aes(
      x, depth, label=size),
      data=join.dt)+
    scale_y_reverse()
}

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
