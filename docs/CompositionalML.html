<!DOCTYPE html><html><head><title>Help for package CompositionalML</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {CompositionalML}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#CompositionalML-package'>
<p>Machine Learning with Compositional Data</p></a></li>
<li><a href='#alfa-PPR+20with+20compositional+20predictor+20variables'>
<p><code class="reqn">\alpha</code>-PPR with compositional predictor variables</p></a></li>
<li><a href='#alpha-Boruta'>
<p><code class="reqn">\alpha</code>-Boruta variable selection</p></a></li>
<li><a href='#alpha-RF'>
<p><code class="reqn">\alpha</code>-RF</p></a></li>
<li><a href='#alpha-SVM'>
<p><code class="reqn">\alpha</code>-SVM</p></a></li>
<li><a href='#Tuning+20the+20parameters+20of+20the+20alfa-PPR'>
<p>Tuning the parameters of the<code class="reqn">\alpha</code>-PPR</p></a></li>
<li><a href='#Tuning+20the+20parameters+20of+20the+20alpha-RF'>
<p>Tuning the parameters of the <code class="reqn">\alpha</code>-RF</p></a></li>
<li><a href='#Tuning+20the+20parameters+20of+20the+20alpha-SVM'>
<p>Tuning the parameters of the <code class="reqn">\alpha</code>-SVM</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Machine Learning with Compositional Data</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0</td>
</tr>
<tr>
<td>Date:</td>
<td>2024-03-13</td>
</tr>
<tr>
<td>Author:</td>
<td>Michail Tsagris [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Michail Tsagris &lt;mtsagris@uoc.gr&gt;</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>Boruta, Compositional, doParallel, e1071, foreach, graphics,
ranger, Rfast, Rfast2, stats</td>
</tr>
<tr>
<td>Description:</td>
<td>Machine learning algorithms for predictor variables that are compositional data and the response variable is either continuous or categorical. Specifically, the Boruta variable selection algorithm, random forest, support vector machines and projection pursuit regression are included. Relevant papers include: Tsagris M.T., Preston S. and Wood A.T.A. (2011). "A data-based power transformation for compositional data". Fourth International International Workshop on Compositional Data Analysis. &lt;<a href="https://doi.org/10.48550%2FarXiv.1106.1451">doi:10.48550/arXiv.1106.1451</a>&gt; and Alenazi, A. (2023). "A review of compositional data analysis and recent advances". Communications in Statistics&ndash;Theory and Methods, 52(16): 5535&ndash;5567. &lt;<a href="https://doi.org/10.1080%2F03610926.2021.2014890">doi:10.1080/03610926.2021.2014890</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-03-13 12:23:56 UTC; mtsag</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-03-14 12:40:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='CompositionalML-package'>
Machine Learning with Compositional Data
</h2><span id='topic+CompositionalML-package'></span>

<h3>Description</h3>

<p>Description: Machine learning algorithms for predictor variables that are compositional data and the response variable is either continuous or categorical. Specifically, the Boruta variable selection algorithm, random forest, support vector machines and projection pursuit regression are included. Relevant papers include: Tsagris M.T., Preston S. and Wood A.T.A. (2011). &quot;A data-based power transformation for compositional data&quot;. Fourth International International Workshop on Compositional Data Analysis and Alenazi, A. (2023). &quot;A review of compositional data analysis and recent advances&quot;. Communications in Statistics&ndash;Theory and Methods, 52(16): 5535&ndash;5567.
</p>


<h3>Details</h3>


<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> CompositionalML</td>
</tr>
<tr>
 <td style="text-align: left;">
Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
Version: </td><td style="text-align: left;"> 1.0 </td>
</tr>
<tr>
 <td style="text-align: left;">
Date: </td><td style="text-align: left;"> 2024-03-13</td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> GPL-2</td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>



<h3>Maintainers</h3>

<p>Michail Tsagris &lt;mtsagris@uoc.gr&gt;
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Alenazi A. (2023). A review of compositional data analysis and recent advances.
Communications in Statistics&ndash;Theory and Methods, 52(16): 5535&ndash;5567.
</p>
<p>Friedman J. H. and Stuetzle W. (1981). Projection pursuit regression.
Journal of the American Statistical Association, 76, 817-823. doi: 10.2307/2287576.
</p>
<p>Friedman Jerome, Trevor Hastie and Robert Tibshirani (2009).
The elements of statistical learning, 2nd edition. Springer, Berlin.
</p>
<p>Chang Chih-Chung and Lin Chih-Jen: LIBSVM: a library for Support Vector Machines
https://www.csie.ntu.edu.tw/~cjlin/libsvm/
</p>
<p>Kursa M. B. and Rudnicki W. R. (2010). Feature Selection with the
Boruta Package. Journal of Statistical Software, 36(11).
</p>
<p>Tsagris M.T., Preston S. and Wood A.T.A. (2011). A data-based power transformation for compositional data.
Fourth International International Workshop on Compositional Data Analysis.
</p>
<p>Wright M. N. and Ziegler A. (2017). ranger: A fast implementation of random
forests for high dimensional data in C++ and R.
Journal of Statisrical Software 77:1-17. doi:10.18637/jss.v077.i01.
</p>

<hr>
<h2 id='alfa-PPR+20with+20compositional+20predictor+20variables'>
<code class="reqn">\alpha</code>-PPR with compositional predictor variables
</h2><span id='topic+alfa.ppr'></span>

<h3>Description</h3>

<p><code class="reqn">\alpha</code>-PPR with compositional predictor variables.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>alfa.ppr(xnew, y, x, a = seq(-1, 1, by = 0.1), nterms = 1:10)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="alfa-PPR+2B20with+2B20compositional+2B20predictor+2B20variables_+3A_xnew">xnew</code></td>
<td>

<p>A matrix with the new compositional data whose group is to be predicted.
Zeros are allowed, but you must be careful to choose strictly positive vcalues of <code class="reqn">\alpha</code>.
</p>
</td></tr>
<tr><td><code id="alfa-PPR+2B20with+2B20compositional+2B20predictor+2B20variables_+3A_y">y</code></td>
<td>

<p>The response variable, a numerical vector.
</p>
</td></tr>
<tr><td><code id="alfa-PPR+2B20with+2B20compositional+2B20predictor+2B20variables_+3A_x">x</code></td>
<td>

<p>A matrix with the compositional data.
</p>
</td></tr>
<tr><td><code id="alfa-PPR+2B20with+2B20compositional+2B20predictor+2B20variables_+3A_a">a</code></td>
<td>

<p>A vector with a grid of values of the power transformation, it has to be
between -1 and 1. If zero values are present it has to be greater than 0.
If a=0, the isometric log-ratio transformation is applied.
</p>
</td></tr>
<tr><td><code id="alfa-PPR+2B20with+2B20compositional+2B20predictor+2B20variables_+3A_nterms">nterms</code></td>
<td>

<p>The number of terms to include in the model.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is the standard projection pursuit regression (PPR) applied to the
<code class="reqn">\alpha</code>-transformed compositional predictors.
See the built-in function &quot;ppr&quot; for more details.
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>mod</code></td>
<td>

<p>A list with the results of the PPR model for each value of <code class="reqn">\alpha</code> that
includes the PPR output as provided by the function &quot;ppr&quot;, for each value of &quot;nterms&quot;.
</p>
</td></tr>
<tr><td><code>est</code></td>
<td>

<p>A list with the predicted response values of &quot;xnew&quot; for each value of <code class="reqn">\alpha</code>
and number of &quot;nterms&quot;.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Friedman J. H. and Stuetzle W. (1981). Projection pursuit regression.
Journal of the American Statistical Association, 76, 817-823. doi: 10.2307/2287576.
</p>
<p>Tsagris M.T., Preston S. and Wood A.T.A. (2011). A data-based power transformation
for compositional data. In Proceedings of the 4th Compositional Data Analysis
Workshop, Girona, Spain. https://arxiv.org/pdf/1106.1451.pdf
</p>


<h3>See Also</h3>

<p><code><a href="#topic+alfappr.tune">alfappr.tune</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- as.matrix(iris[, 1:3])
x &lt;- x/ rowSums(x)
y &lt;- iris[, 4]
mod &lt;- alfa.ppr(x, y, x, a = c(0, 0.5, 1), nterms = c(2, 3))
mod
</code></pre>

<hr>
<h2 id='alpha-Boruta'>
<code class="reqn">\alpha</code>-Boruta variable selection
</h2><span id='topic+alfa.boruta'></span>

<h3>Description</h3>

<p><code class="reqn">\alpha</code>-Boruta variable selection.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>alfa.boruta(y, x,  a = seq(-1, 1, by = 0.1), runs = 100 )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="alpha-Boruta_+3A_y">y</code></td>
<td>

<p>The response variable, it can either be a factor (for classification) or a
numeric vector (for regression). Depending on the nature of the response
variable, the function will proceed with the necessary task.
</p>
</td></tr>
<tr><td><code id="alpha-Boruta_+3A_x">x</code></td>
<td>

<p>A matrix with the compositional data.
</p>
</td></tr>
<tr><td><code id="alpha-Boruta_+3A_a">a</code></td>
<td>

<p>A vector with a grid of values of the power transformation, it has to be
between -1 and 1. If zero values are present it has to be greater than 0.
If a=0, the isometric log-ratio transformation is applied.
</p>
</td></tr>
<tr><td><code id="alpha-Boruta_+3A_runs">runs</code></td>
<td>

<p>Maximal number of importance source runs. You may increase it to
avoid variables being characterised as &quot;Tentative&quot;.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For each value of <code class="reqn">\alpha</code>, the compositional data are transformed and then
the Boruta variable selection algorithm is applied.
</p>


<h3>Value</h3>

<p>A list with the results of the Boruta variable selection algortihm for each value of <code class="reqn">\alpha</code>
as returned by the function &quot;Boruta&quot; of the package <b>Boruta</b>. The important elements
are these (all the items returned by the function are of course included):
</p>
<table>
<tr><td><code>finalDecision</code></td>
<td>

<p>A factor of three values: &quot;Confirmed&quot;, &quot;Rejected&quot; or &quot;Tentative&quot;&quot;, for each variable,
containing the final result of the variable selection.
</p>
</td></tr>
<tr><td><code>ImpHistory</code></td>
<td>

<p>A data frame of importances of variables gathered in each importance source run.
Beside the importances, it contains maximal, mean and minimal importance of shadow
variables in each run. Rejected attributes get -Inf importance.
Set to NULL if holdHistory was given FALSE.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Kursa M. B. and Rudnicki W. R. (2010). Feature Selection with the
Boruta Package. Journal of Statistical Software, 36(11).
</p>
<p>Tsagris M.T., Preston S. and Wood A.T.A. (2011). A data-based power transformation
for compositional data. In Proceedings of the 4th Compositional Data Analysis
Workshop, Girona, Spain. https://arxiv.org/pdf/1106.1451.pdf
</p>


<h3>See Also</h3>

<p><code><a href="#topic+alfa.rf">alfa.rf</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- as.matrix(iris[, 1:4])
x &lt;- x/ rowSums(x)
y &lt;- iris[, 5]
mod &lt;- alfa.boruta(y, x, a = c(0, 0.5))
mod
</code></pre>

<hr>
<h2 id='alpha-RF'>
<code class="reqn">\alpha</code>-RF
</h2><span id='topic+alfa.rf'></span>

<h3>Description</h3>

<p><code class="reqn">\alpha</code>-RF.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>alfa.rf(xnew, y, x, a = seq(-1, 1, by = 0.1), size = c(1, 2, 3),
depth = c(0, 1), splits = 2:5, R = 500)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="alpha-RF_+3A_xnew">xnew</code></td>
<td>

<p>A matrix with the new compositional data whose group is to be predicted.
Zeros are allowed, but you must be careful to choose strictly positive vcalues of <code class="reqn">\alpha</code>.
</p>
</td></tr>
<tr><td><code id="alpha-RF_+3A_y">y</code></td>
<td>

<p>The response variable, it can either be a factor (for classification) or a
numeric vector (for regression). Depending on the nature of the response
variable, the function will proceed with the necessary task.
</p>
</td></tr>
<tr><td><code id="alpha-RF_+3A_x">x</code></td>
<td>

<p>A matrix with the compositional data.
</p>
</td></tr>
<tr><td><code id="alpha-RF_+3A_a">a</code></td>
<td>

<p>A vector with a grid of values of the power transformation, it has to be
between -1 and 1. If zero values are present it has to be greater than 0.
If a=0, the isometric log-ratio transformation is applied.
</p>
</td></tr>
<tr><td><code id="alpha-RF_+3A_size">size</code></td>
<td>

<p>The minimal node size to split at.
</p>
</td></tr>
<tr><td><code id="alpha-RF_+3A_depth">depth</code></td>
<td>

<p>The maximal tree depth. A value of NULL or 0 corresponds to unlimited depth,
1 to tree stumps (1 split per tree).
</p>
</td></tr>
<tr><td><code id="alpha-RF_+3A_splits">splits</code></td>
<td>

<p>The number of random splits to consider for each candidate splitting variable.
</p>
</td></tr>
<tr><td><code id="alpha-RF_+3A_r">R</code></td>
<td>

<p>The number of trees.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For each value of <code class="reqn">\alpha</code>, the compositional data are transformed and then
the random forest (RF) is applied for one or more combinations of the hyper-parameters.
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>mod</code></td>
<td>

<p>A list with the results of the RF model for each value of <code class="reqn">\alpha</code> that
includes the RF output (a ranger class object) as provided by the function &quot;ranger&quot;
of the package <b>ranger</b>, the configurations used and the predicted values of &quot;xnew&quot;.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Wright M. N. and Ziegler A. (2017). ranger: A fast implementation of random
forests for high dimensional data in C++ and R.
Journal of Statisrical Software 77:1-17. doi:10.18637/jss.v077.i01.
</p>
<p>Tsagris M.T., Preston S. and Wood A.T.A. (2011). A data-based power transformation
for compositional data. In Proceedings of the 4th Compositional Data Analysis
Workshop, Girona, Spain. https://arxiv.org/pdf/1106.1451.pdf
</p>


<h3>See Also</h3>

<p><code><a href="#topic+alfarf.tune">alfarf.tune</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- as.matrix(iris[, 1:4])
x &lt;- x/ rowSums(x)
y &lt;- iris[, 5]
mod &lt;- alfa.rf(x, y, x, a = c(0, 0.5, 1), size = 3, depth = 1, splits = 2:3, R = 500)
mod
</code></pre>

<hr>
<h2 id='alpha-SVM'>
<code class="reqn">\alpha</code>-SVM
</h2><span id='topic+alfa.svm'></span>

<h3>Description</h3>

<p><code class="reqn">\alpha</code>-SVM.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>alfa.svm(xnew, y, x, a = seq(-1, 1, by = 0.1), cost = seq(0.2, 2, by = 0.2), gamma = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="alpha-SVM_+3A_xnew">xnew</code></td>
<td>

<p>A matrix with the new compositional data whose group is to be predicted.
Zeros are allowed, but you must be careful to choose strictly positive vcalues of <code class="reqn">\alpha</code>.
</p>
</td></tr>
<tr><td><code id="alpha-SVM_+3A_y">y</code></td>
<td>

<p>The response variable, it can either be a factor (for classification) or a
numeric vector (for regression). Depending on the nature of the response
variable, the function will proceed with the necessary task.
</p>
</td></tr>
<tr><td><code id="alpha-SVM_+3A_x">x</code></td>
<td>

<p>A matrix with the compositional data.
</p>
</td></tr>
<tr><td><code id="alpha-SVM_+3A_a">a</code></td>
<td>

<p>A vector with a grid of values of the power transformation, it has to be
between -1 and 1. If zero values are present it has to be greater than 0.
If a=0, the isometric log-ratio transformation is applied.
</p>
</td></tr>
<tr><td><code id="alpha-SVM_+3A_cost">cost</code></td>
<td>

<p>A grid of values for the cost of constraints violation. The cost is the
&quot;C&quot;-constant of the regularization term in the Lagrange formulation.
</p>
</td></tr>
<tr><td><code id="alpha-SVM_+3A_gamma">gamma</code></td>
<td>

<p>A grid of values for the <code class="reqn">\gamma</code> parameter of the Gaussian kernel.
If no values are supplied the default grid is used, ten equidistant values
from <code class="reqn">1/D^2</code> to <code class="reqn">\sqrt{D}</code>,
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For each value of <code class="reqn">\alpha</code>, the compositional data are transformed and then
the SVM is applied for one or more combinations of the cost and <code class="reqn">\gamma</code> parameters.
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>mod</code></td>
<td>

<p>A list with the results of the SVM model for each value of <code class="reqn">\alpha</code> that
includes the SVM output (an svm class object) as provided by the function &quot;svm&quot;
of the package <b>e1071</b>, the configurations used (cost and <code class="reqn">\gamma</code> values) and
the predicted values of &quot;xnew&quot;.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Chang Chih-Chung and Lin Chih-Jen: LIBSVM: a library for Support Vector Machines
https://www.csie.ntu.edu.tw/~cjlin/libsvm/
</p>
<p>Tsagris M.T., Preston S. and Wood A.T.A. (2011). A data-based power transformation
for compositional data. In Proceedings of the 4th Compositional Data Analysis
Workshop, Girona, Spain. https://arxiv.org/pdf/1106.1451.pdf
</p>


<h3>See Also</h3>

<p><code><a href="#topic+alfasvm.tune">alfasvm.tune</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- as.matrix(iris[, 1:4])
x &lt;- x/ rowSums(x)
y &lt;- iris[, 5]
mod &lt;- alfa.svm(x, y, x, a = c(0, 0.5, 1), cost = c(0.2, 0.4), gamma = c(0.1, 0.2) )
mod
</code></pre>

<hr>
<h2 id='Tuning+20the+20parameters+20of+20the+20alfa-PPR'>
Tuning the parameters of the<code class="reqn">\alpha</code>-PPR
</h2><span id='topic+alfappr.tune'></span>

<h3>Description</h3>

<p>Tuning the parameters of the<code class="reqn">\alpha</code>-PPR.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>alfappr.tune(y, x, a = seq(-1, 1, by = 0.1), nterms = 1:10, ncores = 1,
folds = NULL, nfolds = 10, seed = NULL, graph = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Tuning+2B20the+2B20parameters+2B20of+2B20the+2B20alfa-PPR_+3A_y">y</code></td>
<td>

<p>The response variable, a numerical vector.
</p>
</td></tr>
<tr><td><code id="Tuning+2B20the+2B20parameters+2B20of+2B20the+2B20alfa-PPR_+3A_x">x</code></td>
<td>

<p>A matrix with the compositional data.
</p>
</td></tr>
<tr><td><code id="Tuning+2B20the+2B20parameters+2B20of+2B20the+2B20alfa-PPR_+3A_a">a</code></td>
<td>

<p>A vector with a grid of values of the power transformation, it has to be
between -1 and 1. If zero values are present it has to be greater than 0.
If a=0, the isometric log-ratio transformation is applied.
</p>
</td></tr>
<tr><td><code id="Tuning+2B20the+2B20parameters+2B20of+2B20the+2B20alfa-PPR_+3A_nterms">nterms</code></td>
<td>

<p>The number of terms to include in the model.
</p>
</td></tr>
<tr><td><code id="Tuning+2B20the+2B20parameters+2B20of+2B20the+2B20alfa-PPR_+3A_ncores">ncores</code></td>
<td>

<p>The number of cores to use. If more than 1, parallel computing will take place.
It is advisable to use it if you have many observations and or many variables,
otherwise it will slow down the process.
</p>
</td></tr>
<tr><td><code id="Tuning+2B20the+2B20parameters+2B20of+2B20the+2B20alfa-PPR_+3A_folds">folds</code></td>
<td>

<p>If you have the list with the folds supply it here. You can also leave it
NULL and it will create folds.
</p>
</td></tr>
<tr><td><code id="Tuning+2B20the+2B20parameters+2B20of+2B20the+2B20alfa-PPR_+3A_nfolds">nfolds</code></td>
<td>

<p>The number of folds in the cross validation.
</p>
</td></tr>
<tr><td><code id="Tuning+2B20the+2B20parameters+2B20of+2B20the+2B20alfa-PPR_+3A_seed">seed</code></td>
<td>

<p>You can specify your own seed number here or leave it NULL.
</p>
</td></tr>
<tr><td><code id="Tuning+2B20the+2B20parameters+2B20of+2B20the+2B20alfa-PPR_+3A_graph">graph</code></td>
<td>

<p>If graph is TRUE (default value) a plot will appear.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>K-fold cross-validation of the <code class="reqn">\alpha</code>-PPR with compositional predictor
variables is performed to select the optimal value of <code class="reqn">\alpha</code> and the numer of
terms in the PPR.
</p>


<h3>Value</h3>

<p>If graph is true, a graph with the estimated performance for each value of <code class="reqn">\alpha</code>.
A list including:
</p>
<table>
<tr><td><code>per</code></td>
<td>

<p>A vector with the estimated performance for each value of <code class="reqn">\alpha</code>.
</p>
</td></tr>
<tr><td><code>performance</code></td>
<td>

<p>A vector with the optimal performance and the optimal number of terms.
</p>
</td></tr>
<tr><td><code>best_a</code></td>
<td>

<p>The value of <code class="reqn">\alpha</code> corresponding to the optimal performance.
</p>
</td></tr>
<tr><td><code>runtime</code></td>
<td>

<p>The time required by the cross-validation procedure.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Friedman J. H. and Stuetzle W. (1981). Projection pursuit regression.
Journal of the American Statistical Association, 76, 817-823. doi: 10.2307/2287576.
</p>
<p>Friedman Jerome, Trevor Hastie and Robert Tibshirani (2009).
The elements of statistical learning, 2nd edition. Springer, Berlin.
</p>
<p>Tsagris M.T., Preston S. and Wood A.T.A. (2011). A data-based power transformation
for compositional data. In Proceedings of the 4th Compositional Data Analysis
Workshop, Girona, Spain. https://arxiv.org/pdf/1106.1451.pdf
</p>


<h3>See Also</h3>

<p><code><a href="#topic+alfa.ppr">alfa.ppr</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- as.matrix(iris[, 1:3])
x &lt;- x/ rowSums(x)
y &lt;- iris[, 4]
mod &lt;- alfappr.tune(y, x, a = c(0, 0.5, 1), nterms = c(2, 3))
mod
</code></pre>

<hr>
<h2 id='Tuning+20the+20parameters+20of+20the+20alpha-RF'>
Tuning the parameters of the <code class="reqn">\alpha</code>-RF
</h2><span id='topic+alfarf.tune'></span>

<h3>Description</h3>

<p>Tuning the parameters of the <code class="reqn">\alpha</code>-RF.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>alfarf.tune(y, x, a = seq(-1, 1, by = 0.1), size = c(1, 2, 3),
depth = c(0, 1), splits = 2:5, R = 500, ncores = 1, folds = NULL,
nfolds = 10, stratified = TRUE, seed = NULL, graph = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Tuning+2B20the+2B20parameters+2B20of+2B20the+2B20alpha-RF_+3A_y">y</code></td>
<td>

<p>The response variable, it can either be a factor (for classification) or a
numeric vector (for regression). Depending on the nature of the response
variable, the function will proceed with the necessary task.
</p>
</td></tr>
<tr><td><code id="Tuning+2B20the+2B20parameters+2B20of+2B20the+2B20alpha-RF_+3A_x">x</code></td>
<td>

<p>A matrix with the compositional data.
</p>
</td></tr>
<tr><td><code id="Tuning+2B20the+2B20parameters+2B20of+2B20the+2B20alpha-RF_+3A_a">a</code></td>
<td>

<p>A vector with a grid of values of the power transformation, it has to be
between -1 and 1. If zero values are present it has to be greater than 0.
If a=0, the isometric log-ratio transformation is applied.
</p>
</td></tr>
<tr><td><code id="Tuning+2B20the+2B20parameters+2B20of+2B20the+2B20alpha-RF_+3A_size">size</code></td>
<td>

<p>The minimal node size to split at.
</p>
</td></tr>
<tr><td><code id="Tuning+2B20the+2B20parameters+2B20of+2B20the+2B20alpha-RF_+3A_depth">depth</code></td>
<td>

<p>The maximal tree depth. A value of NULL or 0 corresponds to unlimited depth,
1 to tree stumps (1 split per tree).
</p>
</td></tr>
<tr><td><code id="Tuning+2B20the+2B20parameters+2B20of+2B20the+2B20alpha-RF_+3A_splits">splits</code></td>
<td>

<p>The number of random splits to consider for each candidate splitting variable.
</p>
</td></tr>
<tr><td><code id="Tuning+2B20the+2B20parameters+2B20of+2B20the+2B20alpha-RF_+3A_r">R</code></td>
<td>

<p>The number of trees.
</p>
</td></tr>
<tr><td><code id="Tuning+2B20the+2B20parameters+2B20of+2B20the+2B20alpha-RF_+3A_ncores">ncores</code></td>
<td>

<p>The number of cores to use. If more than 1, parallel computing will take place.
It is advisable to use it if you have many observations and or many variables,
otherwise it will slow down the process.
</p>
</td></tr>
<tr><td><code id="Tuning+2B20the+2B20parameters+2B20of+2B20the+2B20alpha-RF_+3A_folds">folds</code></td>
<td>

<p>If you have the list with the folds supply it here. You can also leave it
NULL and it will create folds.
</p>
</td></tr>
<tr><td><code id="Tuning+2B20the+2B20parameters+2B20of+2B20the+2B20alpha-RF_+3A_nfolds">nfolds</code></td>
<td>

<p>The number of folds in the cross validation.
</p>
</td></tr>
<tr><td><code id="Tuning+2B20the+2B20parameters+2B20of+2B20the+2B20alpha-RF_+3A_stratified">stratified</code></td>
<td>

<p>Do you want the folds to be created in a stratified way? TRUE or FALSE.
</p>
</td></tr>
<tr><td><code id="Tuning+2B20the+2B20parameters+2B20of+2B20the+2B20alpha-RF_+3A_seed">seed</code></td>
<td>

<p>You can specify your own seed number here or leave it NULL.
</p>
</td></tr>
<tr><td><code id="Tuning+2B20the+2B20parameters+2B20of+2B20the+2B20alpha-RF_+3A_graph">graph</code></td>
<td>

<p>If graph is TRUE (default value) a plot will appear.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>K-fold cross-validation of the <code class="reqn">\alpha</code>-RF with compositional predictor
variables is performed to select the optimal value of <code class="reqn">\alpha</code> and the optimal
configutrations of the random forest (RF).
</p>


<h3>Value</h3>

<p>If graph is true, a graph with the estimated performance for each value of <code class="reqn">\alpha</code>.
A list including:
</p>
<table>
<tr><td><code>per</code></td>
<td>

<p>A vector with the estimated performance for each value of <code class="reqn">\alpha</code>.
</p>
</td></tr>
<tr><td><code>performance</code></td>
<td>

<p>A vector with the optimal performance and the optimal combinations of the
hyper-parameters of the RF.
</p>
</td></tr>
<tr><td><code>best_a</code></td>
<td>

<p>The value of <code class="reqn">\alpha</code> corresponding to the optimal performance.
</p>
</td></tr>
<tr><td><code>runtime</code></td>
<td>

<p>The time required by the cross-validation procedure.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Wright M. N. and Ziegler A. (2017). ranger: A fast implementation of random
forests for high dimensional data in C++ and R.
Journal of Statisrical Software 77:1-17. doi:10.18637/jss.v077.i01.
</p>
<p>Friedman Jerome, Trevor Hastie and Robert Tibshirani (2009).
The elements of statistical learning, 2nd edition. Springer, Berlin.
</p>
<p>Tsagris M.T., Preston S. and Wood A.T.A. (2011). A data-based power transformation
for compositional data. In Proceedings of the 4th Compositional Data Analysis
Workshop, Girona, Spain. https://arxiv.org/pdf/1106.1451.pdf
</p>


<h3>See Also</h3>

<p><code><a href="#topic+alfa.rf">alfa.rf</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- as.matrix(iris[, 1:4])
x &lt;- x/ rowSums(x)
y &lt;- iris[, 5]
mod &lt;- alfa.rf(x, y, x, a = c(0, 0.5, 1), size = 3, depth = 1, splits = 2:3, R = 500)
mod
</code></pre>

<hr>
<h2 id='Tuning+20the+20parameters+20of+20the+20alpha-SVM'>
Tuning the parameters of the <code class="reqn">\alpha</code>-SVM
</h2><span id='topic+alfasvm.tune'></span>

<h3>Description</h3>

<p>Tuning the parameters of the <code class="reqn">\alpha</code>-SVM.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>alfasvm.tune(y, x, a = seq(-1, 1, by = 0.1), cost = seq(0.2, 2, by = 0.2), gamma = NULL,
ncores = 1, folds = NULL, nfolds = 10, stratified = TRUE, seed = NULL, graph = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Tuning+2B20the+2B20parameters+2B20of+2B20the+2B20alpha-SVM_+3A_y">y</code></td>
<td>

<p>The response variable, it can either be a factor (for classification) or a
numeric vector (for regression). Depending on the nature of the response
variable, the function will proceed with the necessary task.
</p>
</td></tr>
<tr><td><code id="Tuning+2B20the+2B20parameters+2B20of+2B20the+2B20alpha-SVM_+3A_x">x</code></td>
<td>

<p>A matrix with the compositional data.
</p>
</td></tr>
<tr><td><code id="Tuning+2B20the+2B20parameters+2B20of+2B20the+2B20alpha-SVM_+3A_a">a</code></td>
<td>

<p>A vector with a grid of values of the power transformation, it has to be
between -1 and 1. If zero values are present it has to be greater than 0.
If a=0, the isometric log-ratio transformation is applied.
</p>
</td></tr>
<tr><td><code id="Tuning+2B20the+2B20parameters+2B20of+2B20the+2B20alpha-SVM_+3A_cost">cost</code></td>
<td>

<p>A grid of values for the cost of constraints violation. The cost is the
&quot;C&quot;-constant of the regularization term in the Lagrange formulation.
</p>
</td></tr>
<tr><td><code id="Tuning+2B20the+2B20parameters+2B20of+2B20the+2B20alpha-SVM_+3A_gamma">gamma</code></td>
<td>

<p>A grid of values for the <code class="reqn">\gamma</code> parameter of the Gaussian kernel.
If no values are supplied the default grid is used, ten equidistant values
from <code class="reqn">1/D^2</code> to <code class="reqn">\sqrt{D}</code>,
</p>
</td></tr>
<tr><td><code id="Tuning+2B20the+2B20parameters+2B20of+2B20the+2B20alpha-SVM_+3A_ncores">ncores</code></td>
<td>

<p>The number of cores to use. If more than 1, parallel computing will take place.
It is advisable to use it if you have many observations and or many variables,
otherwise it will slow down the process.
</p>
</td></tr>
<tr><td><code id="Tuning+2B20the+2B20parameters+2B20of+2B20the+2B20alpha-SVM_+3A_folds">folds</code></td>
<td>

<p>If you have the list with the folds supply it here. You can also leave it
NULL and it will create folds.
</p>
</td></tr>
<tr><td><code id="Tuning+2B20the+2B20parameters+2B20of+2B20the+2B20alpha-SVM_+3A_nfolds">nfolds</code></td>
<td>

<p>The number of folds in the cross validation.
</p>
</td></tr>
<tr><td><code id="Tuning+2B20the+2B20parameters+2B20of+2B20the+2B20alpha-SVM_+3A_stratified">stratified</code></td>
<td>

<p>Do you want the folds to be created in a stratified way? TRUE or FALSE.
</p>
</td></tr>
<tr><td><code id="Tuning+2B20the+2B20parameters+2B20of+2B20the+2B20alpha-SVM_+3A_seed">seed</code></td>
<td>

<p>You can specify your own seed number here or leave it NULL.
</p>
</td></tr>
<tr><td><code id="Tuning+2B20the+2B20parameters+2B20of+2B20the+2B20alpha-SVM_+3A_graph">graph</code></td>
<td>

<p>If graph is TRUE (default value) a plot will appear.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>K-fold cross validation is performed to select the optimal parameters for the SVM
and also estimate the rate of accuracy. For continuous responses the estimated
performance translates to the MSE, while for categorical responses (factors) this
is the accuracy (percentage of crrect classification).
</p>


<h3>Value</h3>

<p>If graph is true, a graph with the estimated performance for each value of <code class="reqn">\alpha</code>.
A list including:
</p>
<table>
<tr><td><code>per</code></td>
<td>

<p>A vector with the estimated performance for each value of <code class="reqn">\alpha</code>.
</p>
</td></tr>
<tr><td><code>performance</code></td>
<td>

<p>A vector with the optimal performance and the optimal combinations of cost and <code class="reqn">\gamma</code> values.
</p>
</td></tr>
<tr><td><code>best_a</code></td>
<td>

<p>The value of <code class="reqn">\alpha</code> corresponding to the optimal performance.
</p>
</td></tr>
<tr><td><code>runtime</code></td>
<td>

<p>The time required by the cross-validation procedure.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Chang Chih-Chung and Lin Chih-Jen: LIBSVM: a library for Support Vector Machines
https://www.csie.ntu.edu.tw/~cjlin/libsvm/
</p>
<p>Friedman Jerome, Trevor Hastie and Robert Tibshirani (2009).
The elements of statistical learning, 2nd edition. Springer, Berlin.
</p>
<p>Tsagris M.T., Preston S. and Wood A.T.A. (2011). A data-based power transformation
for compositional data. In Proceedings of the 4th Compositional Data Analysis
Workshop, Girona, Spain. https://arxiv.org/pdf/1106.1451.pdf
</p>


<h3>See Also</h3>

<p><code><a href="#topic+alfa.svm">alfa.svm</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- as.matrix(iris[, 1:4])
x &lt;- x/ rowSums(x)
y &lt;- iris[, 5]
mod &lt;- alfasvm.tune(y, x, a = c(0, 0.5, 1), cost = c(0.2, 0.4), gamma = c(0.1, 0.2) )
mod
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
