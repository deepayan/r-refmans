<!DOCTYPE html><html><head><title>Help for package SMMA</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {SMMA}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#predict.SMMA'><p>Make Prediction From a SMMA Object</p></a></li>
<li><a href='#print.SMMA'><p>Print Function for objects of Class SMMA</p></a></li>
<li><a href='#RH'><p>The Rotated H-transform of a 3d Array by a Matrix</p></a></li>
<li><a href='#SMMA'><p>Soft Maximin Estimation for Large Scale Array Data with Known Groups</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Soft Maximin Estimation for Large Scale Array-Tensor Models</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.3</td>
</tr>
<tr>
<td>Date:</td>
<td>2020-09-17</td>
</tr>
<tr>
<td>Author:</td>
<td>Adam Lund</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Adam Lund &lt;adam.lund@math.ku.dk&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Efficient design matrix free procedure for solving a soft maximin problem for  large scale array-tensor structured models, see Lund, Mogensen and Hansen (2019) &lt;<a href="https://arxiv.org/abs/1805.02407">arXiv:1805.02407</a>&gt;. Currently Lasso and SCAD penalized estimation is implemented.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp (&ge; 0.12.12)</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.1</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2020-09-17 09:54:32 UTC; adamlund</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2020-09-17 13:00:07 UTC</td>
</tr>
</table>
<hr>
<h2 id='predict.SMMA'>Make Prediction From a SMMA Object</h2><span id='topic+predict.SMMA'></span>

<h3>Description</h3>

<p>Given new covariate data this function computes the linear predictors
based on the estimated model coefficients in an object produced by the function <code>softmaximin</code>. Note that the
data can be supplied in two different formats: i) as a <code class="reqn">n' \times p</code> matrix (<code class="reqn">p</code> is the number of model
coefficients and <code class="reqn">n'</code> is the number of new data points) or ii) as a list of two or three matrices each of
size <code class="reqn">n_i' \times p_i, i = 1, 2, 3</code> (<code class="reqn">n_i'</code> is the number of new marginal data points in the <code class="reqn">i</code>th dimension).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'SMMA'
predict(object, x = NULL, X = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.SMMA_+3A_object">object</code></td>
<td>
<p>An object of class SMMA, produced with <code>softmaximin</code></p>
</td></tr>
<tr><td><code id="predict.SMMA_+3A_x">x</code></td>
<td>
<p>a matrix of size <code class="reqn">n' \times p</code> with <code class="reqn">n'</code> is the number of new data points.</p>
</td></tr>
<tr><td><code id="predict.SMMA_+3A_x">X</code></td>
<td>
<p>a list containing the data matrices each of size <code class="reqn">n'_{i} \times p_i</code>,
where <code class="reqn">n'_{i}</code> is the number of new data points in  the <code class="reqn">i</code>th dimension.</p>
</td></tr>
<tr><td><code id="predict.SMMA_+3A_...">...</code></td>
<td>
<p>ignored</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of length <code>nlambda</code> containing the linear predictors for each  model. If
new covariate data is supplied in one <code class="reqn">n' \times p</code> matrix <code>x</code> each
item  is a vector of length <code class="reqn">n'</code>. If the data is supplied as a list of
matrices each of size <code class="reqn">n'_{i} \times p_i</code>,  each item is an array of size <code class="reqn">n'_1 \times \cdots \times n'_d</code>, with <code class="reqn">d\in \{1,2,3\}</code>.
</p>


<h3>Author(s)</h3>

<p>Adam Lund
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
##size of example
n1 &lt;- 65; n2 &lt;- 26; n3 &lt;- 13; p1 &lt;- 13; p2 &lt;- 5; p3 &lt;- 4

##marginal design matrices (Kronecker components)
X1 &lt;- matrix(rnorm(n1 * p1, 0, 0.5), n1, p1)
X2 &lt;- matrix(rnorm(n2 * p2, 0, 0.5), n2, p2)
X3 &lt;- matrix(rnorm(n3 * p3, 0, 0.5), n3, p3)
X &lt;- list(X1, X2, X3)

component &lt;- rbinom(p1 * p2 * p3, 1, 0.1)
Beta1 &lt;- array(rnorm(p1 * p2 * p3, 0, .1) + component, c(p1 , p2, p3))
Beta2 &lt;- array(rnorm(p1 * p2 * p3, 0, .1) + component, c(p1 , p2, p3))
mu1 &lt;- RH(X3, RH(X2, RH(X1, Beta1)))
mu2 &lt;- RH(X3, RH(X2, RH(X1, Beta2)))
Y1 &lt;- array(rnorm(n1 * n2 * n3, mu1), dim = c(n1, n2, n3))
Y2 &lt;- array(rnorm(n1 * n2 * n3, mu2), dim = c(n1, n2, n3))

Y &lt;- array(NA, c(dim(Y1), 2))
Y[,,, 1] &lt;- Y1; Y[,,, 2] &lt;- Y2;

fit &lt;- softmaximin(X, Y, zeta = 10, penalty = "lasso", alg = "npg")

##new data in matrix form
x &lt;- matrix(rnorm(p1 * p2 * p3), nrow = 1)
predict(fit, x = x)[[15]]

##new data in tensor component form
X1 &lt;- matrix(rnorm(p1), nrow = 1)
X2 &lt;- matrix(rnorm(p2), nrow = 1)
X3 &lt;- matrix(rnorm(p3), nrow = 1)
predict(fit, X = list(X1, X2, X3))[[15]]


</code></pre>

<hr>
<h2 id='print.SMMA'>Print Function for objects of Class SMMA</h2><span id='topic+print.SMMA'></span>

<h3>Description</h3>

<p>This function will print some information about the SMMA object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'SMMA'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.SMMA_+3A_x">x</code></td>
<td>
<p>a SMMA object</p>
</td></tr>
<tr><td><code id="print.SMMA_+3A_...">...</code></td>
<td>
<p>ignored</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Adam Lund
</p>


<h3>Examples</h3>

<pre><code class='language-R'> 

##size of example 
n1 &lt;- 65; n2 &lt;- 26; n3 &lt;- 13; p1 &lt;- 13; p2 &lt;- 5; p3 &lt;- 4

##marginal design matrices (Kronecker components)
X1 &lt;- matrix(rnorm(n1 * p1, 0, 0.5), n1, p1) 
X2 &lt;- matrix(rnorm(n2 * p2, 0, 0.5), n2, p2) 
X3 &lt;- matrix(rnorm(n3 * p3, 0, 0.5), n3, p3) 
X &lt;- list(X1, X2, X3)

component &lt;- rbinom(p1 * p2 * p3, 1, 0.1) 
Beta1 &lt;- array(rnorm(p1 * p2 * p3, 0, .1) + component, c(p1 , p2, p3))
Beta2 &lt;- array(rnorm(p1 * p2 * p3, 0, .1) + component, c(p1 , p2, p3))
mu1 &lt;- RH(X3, RH(X2, RH(X1, Beta1)))
mu2 &lt;- RH(X3, RH(X2, RH(X1, Beta2)))
Y1 &lt;- array(rnorm(n1 * n2 * n3, mu1), dim = c(n1, n2, n3))
Y2 &lt;- array(rnorm(n1 * n2 * n3, mu2), dim = c(n1, n2, n3))

Y &lt;- array(NA, c(dim(Y1), 2))
Y[,,, 1] &lt;- Y1; Y[,,, 2] &lt;- Y2;

fit &lt;- softmaximin(X, Y, zeta = 10, penalty = "lasso", alg = "npg")
fit


</code></pre>

<hr>
<h2 id='RH'>The Rotated H-transform of a 3d Array by a Matrix</h2><span id='topic+RH'></span><span id='topic+glamlasso_RH'></span><span id='topic+Rotate'></span><span id='topic+H'></span>

<h3>Description</h3>

<p>This function is an implementation of the <code class="reqn">\rho</code>-operator found in
<cite>Currie et al 2006</cite>. It forms the basis of the GLAM arithmetic.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>RH(M, A)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="RH_+3A_m">M</code></td>
<td>
<p>a <code class="reqn">n \times p_1</code> matrix.</p>
</td></tr>
<tr><td><code id="RH_+3A_a">A</code></td>
<td>
<p>a 3d array of size <code class="reqn">p_1 \times p_2 \times p_3</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For details see <cite>Currie et al 2006</cite>. Note that this particular implementation
is not used in the  routines underlying the optimization procedure.
</p>


<h3>Value</h3>

<p>A 3d array of size <code class="reqn">p_2 \times p_3 \times n</code>.
</p>


<h3>Author(s)</h3>

<p>Adam Lund
</p>


<h3>References</h3>

<p>Currie, I. D., M. Durban, and P. H. C. Eilers (2006). Generalized linear
array models with applications to multidimensional smoothing.
<em>Journal of the Royal Statistical Society. Series B</em>. 68, 259-280. url = http://dx.doi.org/10.1111/j.1467-9868.2006.00543.x.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
n1 &lt;- 65; n2 &lt;- 26; n3 &lt;- 13; p1 &lt;- 13; p2 &lt;- 5; p3 &lt;- 4

##marginal design matrices (Kronecker components)
X1 &lt;- matrix(rnorm(n1 * p1), n1, p1)
X2 &lt;- matrix(rnorm(n2 * p2), n2, p2)
X3 &lt;- matrix(rnorm(n3 * p3), n3, p3)

Beta &lt;- array(rnorm(p1 * p2 * p3, 0, 1), c(p1 , p2, p3))
max(abs(c(RH(X3, RH(X2, RH(X1, Beta)))) - kronecker(X3, kronecker(X2, X1)) %*% c(Beta)))

</code></pre>

<hr>
<h2 id='SMMA'>Soft Maximin Estimation for Large Scale Array Data with Known Groups</h2><span id='topic+SMMA'></span><span id='topic+softmaximin'></span><span id='topic+pga'></span>

<h3>Description</h3>

<p>Efficient design matrix free procedure for solving a soft maximin problem for
large scale array-tensor structured models, see  <cite>Lund et al., 2020</cite>.
Currently Lasso and SCAD penalized estimation is implemented.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>softmaximin(X,
            Y,
            zeta,
            penalty = c("lasso", "scad"),
            alg = c("npg", "fista"),
            nlambda = 30,
            lambda.min.ratio = 1e-04,
            lambda = NULL,
            penalty.factor = NULL,
            reltol = 1e-05,
            maxiter = 15000,
            steps = 1,
            btmax = 100,
            c = 0.0001,
            tau = 2,
            M = 4,
            nu = 1,
            Lmin = 0,
            log = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SMMA_+3A_x">X</code></td>
<td>
<p>list containing the Kronecker components (1, 2 or 3) of the Kronecker design matrix.
These are  matrices of sizes <code class="reqn">n_i \times p_i</code>.</p>
</td></tr>
<tr><td><code id="SMMA_+3A_y">Y</code></td>
<td>
<p>array of size <code class="reqn">n_1 \times\cdots\times n_d \times G</code> containing the response values.</p>
</td></tr>
<tr><td><code id="SMMA_+3A_zeta">zeta</code></td>
<td>
<p>strictly positive float controlling  the softmaximin approximation accuracy.</p>
</td></tr>
<tr><td><code id="SMMA_+3A_penalty">penalty</code></td>
<td>
<p>string specifying the penalty type. Possible values are <code>"lasso", "scad"</code>.</p>
</td></tr>
<tr><td><code id="SMMA_+3A_alg">alg</code></td>
<td>
<p>string specifying the optimization algorithm. Possible values are <code>"npg", "fista"</code>.</p>
</td></tr>
<tr><td><code id="SMMA_+3A_nlambda">nlambda</code></td>
<td>
<p>positive integer giving the number of <code>lambda</code> values. Used when lambda is not specified.</p>
</td></tr>
<tr><td><code id="SMMA_+3A_lambda.min.ratio">lambda.min.ratio</code></td>
<td>
<p>strictly positive float giving the smallest value for <code>lambda</code>, as a fraction of
<code class="reqn">\lambda_{max}</code>; the (data dependent) smallest value for which all coefficients are zero.
Used when lambda is not specified.</p>
</td></tr>
<tr><td><code id="SMMA_+3A_lambda">lambda</code></td>
<td>
<p>sequence of strictly positive floats used  as penalty parameters.</p>
</td></tr>
<tr><td><code id="SMMA_+3A_penalty.factor">penalty.factor</code></td>
<td>
<p>array of size <code class="reqn">p_1 \times \cdots \times p_d</code> of positive floats. Is multiplied
with each element in <code>lambda</code> to allow differential penalization on the coefficients.</p>
</td></tr>
<tr><td><code id="SMMA_+3A_reltol">reltol</code></td>
<td>
<p>strictly positive float giving the convergence tolerance for the inner loop.</p>
</td></tr>
<tr><td><code id="SMMA_+3A_maxiter">maxiter</code></td>
<td>
<p>positive integer giving the maximum number of  iterations allowed for each <code>lambda</code>
value, when  summing over all outer iterations for said <code>lambda</code>.</p>
</td></tr>
<tr><td><code id="SMMA_+3A_steps">steps</code></td>
<td>
<p>strictly positive integer giving the number of steps used in the multi-step adaptive lasso algorithm for non-convex penalties.
Automatically set to 1 when <code>penalty = "lasso"</code>.</p>
</td></tr>
<tr><td><code id="SMMA_+3A_btmax">btmax</code></td>
<td>
<p>strictly positive integer giving the maximum number of backtracking steps allowed in each iteration. Default is <code>btmax = 100</code>.</p>
</td></tr>
<tr><td><code id="SMMA_+3A_c">c</code></td>
<td>
<p>strictly positive float used in the NPG algorithm. Default is <code>c = 0.0001</code>.</p>
</td></tr>
<tr><td><code id="SMMA_+3A_tau">tau</code></td>
<td>
<p>strictly positive float used to control the stepsize for NPG. Default is <code>tau = 2</code>.</p>
</td></tr>
<tr><td><code id="SMMA_+3A_m">M</code></td>
<td>
<p>positive integer giving the look back for the NPG. Default is <code>M = 4</code>.</p>
</td></tr>
<tr><td><code id="SMMA_+3A_nu">nu</code></td>
<td>
<p>strictly positive float used to control the stepsize. A  value less that 1 will decrease
the stepsize and a value larger than one will increase it. Default is <code>nu = 1</code>.</p>
</td></tr>
<tr><td><code id="SMMA_+3A_lmin">Lmin</code></td>
<td>
<p>non-negative float used by the NPG algorithm to control the stepsize. For the default  <code>Lmin = 0</code>
the maximum step size is the same as for the FISTA algorithm.</p>
</td></tr>
<tr><td><code id="SMMA_+3A_log">log</code></td>
<td>
<p>logical variable indicating whether to use log-loss.  TRUE is default and yields the loss below.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Following <cite>Lund et al., 2020</cite>  this package solves the optimization problem for a linear
model for heterogeneous <code class="reqn">d</code>-dimensional array data (<code class="reqn">d=1,2,3</code>) organized in <code class="reqn">G</code> known groups,
and with identical tensor structured design matrix <code class="reqn">X</code> across all groups.  Specifically <code class="reqn">n = \prod_i^d n_i</code> is the
number of observations in each group, <code class="reqn">Y_g</code>  the  <code class="reqn">n_1\times \cdots \times n_d</code> response array
for group <code class="reqn">g \in \{1,\ldots,G\}</code>, and <code class="reqn">X</code>  a <code class="reqn">n\times p</code> design matrix, with tensor structure
</p>
<p style="text-align: center;"><code class="reqn">X = \bigotimes_{i=1}^d X_i.</code>
</p>

<p>For <code class="reqn">d =1,2,3</code>, <code class="reqn">X_1,\ldots, X_d</code> are the marginal <code class="reqn">n_i\times p_i</code> design matrices (Kronecker components).
Using the GLAM framework  the model equation for group <code class="reqn">g\in \{1,\ldots,G\}</code> is expressed as
</p>
<p style="text-align: center;"><code class="reqn">Y_g = \rho(X_d,\rho(X_{d-1},\ldots,\rho(X_1,B_g))) + E_g,</code>
</p>

<p>where <code class="reqn">\rho</code> is the so called rotated <code class="reqn">H</code>-transfrom (see  <cite>Currie et al., 2006</cite>),
<code class="reqn">B_g</code> for each <code class="reqn">g</code> is a (random) <code class="reqn">p_1\times\cdots\times p_d</code> parameter array
and <code class="reqn">E_g</code>  is  a <code class="reqn">n_1\times \cdots \times n_d</code> error array.
</p>
<p>This package solves the penalized soft maximin problem from <cite>Lund et al., 2020</cite>, given by
</p>
<p style="text-align: center;"><code class="reqn">\min_{\beta}\frac{1}{\zeta}\log\bigg(\sum_{g=1}^G \exp(-\zeta \hat V_g(\beta))\bigg) + \lambda  \Vert\beta\Vert_1, \quad \zeta &gt; 0,\lambda \geq 0</code>
</p>

<p>for the setup described above. Note that
</p>
<p style="text-align: center;"><code class="reqn">\hat V_g(\beta):=\frac{1}{n}(2\beta^\top X^\top vec(Y_g)-\beta^\top X^\top X\beta),</code>
</p>

<p>is the empirical explained variance from <cite>Meinshausen and Buhlmann, 2015</cite>.  See <cite>Lund et al., 2020</cite> for more details and references.
</p>
<p>For <code class="reqn">d=1,2,3</code>, using only the marginal matrices <code class="reqn">X_1,X_2,\ldots</code> (for <code class="reqn">d=1</code> there is only one marginal), the function <code>softmaximin</code>
solves the soft maximin problem for a sequence of penalty parameters <code class="reqn">\lambda_{max}&gt;\ldots &gt;\lambda_{min}&gt;0</code>.
</p>
<p>Two optimization algorithms  are implemented, a non-monotone
proximal gradient (NPG) algorithm and a fast iterative soft thresholding algorithm (FISTA).
We note that this package also solves the problem above with the penalty given by the SCAD
penalty, using the multiple step adaptive lasso procedure to loop over the proximal algorithm.
</p>


<h3>Value</h3>

<p>An object with S3 Class &quot;SMMA&quot;.
</p>
<table>
<tr><td><code>spec</code></td>
<td>
<p>A string indicating the array dimension (1, 2 or 3) and the penalty.</p>
</td></tr>
<tr><td><code>coef</code></td>
<td>
<p>A <code class="reqn">p_1\cdots p_d \times</code> <code>nlambda</code> matrix containing the estimates of
the model coefficients (<code>beta</code>) for each <code>lambda</code>-value.</p>
</td></tr>
<tr><td><code>lambda</code></td>
<td>
<p>A vector containing the sequence of penalty values used in the estimation procedure.</p>
</td></tr>
<tr><td><code>Obj</code></td>
<td>
<p>A matrix containing the objective values for each iteration and each model.</p>
</td></tr>
<tr><td><code>df</code></td>
<td>
<p>The number of nonzero coefficients for each value of <code>lambda</code>.</p>
</td></tr>
<tr><td><code>dimcoef</code></td>
<td>
<p>A vector giving the dimension of the model coefficient array <code class="reqn">\beta</code>.</p>
</td></tr>
<tr><td><code>dimobs</code></td>
<td>
<p>A vector giving the dimension of the observation (response) array <code>Y</code>.</p>
</td></tr>
<tr><td><code>Iter</code></td>
<td>
<p>A list with 4 items:
<code>bt_iter</code>  is total number of backtracking steps performed,
<code>bt_enter</code> is the number of times the backtracking is initiated,
and <code>iter_mat</code> is a vector containing the  number of  iterations for each <code>lambda</code> value
and  <code>iter</code> is total number of iterations i.e. <code>sum(Iter)</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Adam Lund
</p>
<p>Maintainer: Adam Lund, <a href="mailto:adam.lund@math.ku.dk">adam.lund@math.ku.dk</a>
</p>


<h3>References</h3>

<p>Lund, A., S. W. Mogensen and N. R. Hansen (2020). Soft Maximin Estimation for Heterogeneous Array Data.
<em>Preprint</em>.
</p>
<p>Meinshausen, N and P. Buhlmann (2015). Maximin effects in inhomogeneous large-scale data.
<em>The Annals of Statistics</em>. 43, 4, 1801-1830. url = https://doi.org/10.1214/15-AOS1325.
</p>
<p>Currie, I. D., M. Durban, and P. H. C. Eilers (2006). Generalized linear
array models with applications to multidimensional smoothing.
<em>Journal of the Royal Statistical Society. Series B</em>. 68, 259-280. url = http://dx.doi.org/10.1111/j.1467-9868.2006.00543.x.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
##size of example
n1 &lt;- 65; n2 &lt;- 26; n3 &lt;- 13; p1 &lt;- 13; p2 &lt;- 5; p3 &lt;- 4

##marginal design matrices (Kronecker components)
X1 &lt;- matrix(rnorm(n1 * p1), n1, p1)
X2 &lt;- matrix(rnorm(n2 * p2), n2, p2)
X3 &lt;- matrix(rnorm(n3 * p3), n3, p3)
X &lt;- list(X1, X2, X3)

component &lt;- rbinom(p1 * p2 * p3, 1, 0.1)
Beta1 &lt;- array(rnorm(p1 * p2 * p3, 0, 0.1) + component, c(p1 , p2, p3))
mu1 &lt;- RH(X3, RH(X2, RH(X1, Beta1)))
Y1 &lt;- array(rnorm(n1 * n2 * n3), dim = c(n1, n2, n3)) + mu1
Beta2 &lt;- array(rnorm(p1 * p2 * p3, 0, 0.1) + component, c(p1 , p2, p3))
mu2 &lt;- RH(X3, RH(X2, RH(X1, Beta2)))
Y2 &lt;- array(rnorm(n1 * n2 * n3), dim = c(n1, n2, n3)) + mu2
Beta3 &lt;- array(rnorm(p1 * p2 * p3, 0, 0.1) + component, c(p1 , p2, p3))
mu3 &lt;- RH(X3, RH(X2, RH(X1, Beta3)))
Y3 &lt;- array(rnorm(n1 * n2 * n3), dim = c(n1, n2, n3)) + mu3
Beta4 &lt;- array(rnorm(p1 * p2 * p3, 0, 0.1) + component, c(p1 , p2, p3))
mu4 &lt;- RH(X3, RH(X2, RH(X1, Beta4)))
Y4 &lt;- array(rnorm(n1 * n2 * n3), dim = c(n1, n2, n3)) + mu4
Beta5 &lt;- array(rnorm(p1 * p2 * p3, 0, 0.1) + component, c(p1 , p2, p3))
mu5 &lt;- RH(X3, RH(X2, RH(X1, Beta5)))
Y5 &lt;- array(rnorm(n1 * n2 * n3), dim = c(n1, n2, n3)) + mu5

Y &lt;- array(NA, c(dim(Y1), 5))
Y[,,, 1] &lt;- Y1; Y[,,, 2] &lt;- Y2; Y[,,, 3] &lt;- Y3; Y[,,, 4] &lt;- Y4; Y[,,, 5] &lt;- Y5;

fit &lt;- softmaximin(X, Y, zeta = 10, penalty = "lasso", alg = "npg")
Betafit &lt;- fit$coef

modelno &lt;- 15
m &lt;- min(Betafit[ , modelno], c(component))
M &lt;- max(Betafit[ , modelno], c(component))
plot(c(component), type="l", ylim = c(m, M))
lines(Betafit[ , modelno], col = "red")

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
