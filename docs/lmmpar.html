<!DOCTYPE html><html><head><title>Help for package lmmpar</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {lmmpar}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#lmmpar'><p>Parallel Linear Mixed Model</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Parallel Linear Mixed Model</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.0</td>
</tr>
<tr>
<td>Description:</td>
<td>Embarrassingly Parallel Linear Mixed Model calculations spread across local cores which repeat until convergence.</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>LazyData:</td>
<td>TRUE</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.2.2)</td>
</tr>
<tr>
<td>Imports:</td>
<td>MASS, matrixcalc, mnormt, plyr, doParallel, bigmemory</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>6.0.1</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/fulyagokalp/lmmpar">https://github.com/fulyagokalp/lmmpar</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/fulyagokalp/lmmpar/issues">https://github.com/fulyagokalp/lmmpar/issues</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2017-08-02 17:56:25 UTC; barret</td>
</tr>
<tr>
<td>Author:</td>
<td>Fulya Gokalp Yavuz [aut, cre],
  Barret Schloerke [aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Fulya Gokalp Yavuz &lt;fulyagokalp@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2017-08-03 15:17:37 UTC</td>
</tr>
</table>
<hr>
<h2 id='lmmpar'>Parallel Linear Mixed Model</h2><span id='topic+lmmpar'></span>

<h3>Description</h3>

<p>Embarrassingly Parallel Linear Mixed Model calculations spread across local cores which repeat until convergence.  All calculations are currently done locally, but theoretically, the calculations could be extended to multiple machines.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lmmpar(Y, X, Z, subject, beta, R, D, sigma, maxiter = 500, cores = 8,
  verbose = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lmmpar_+3A_y">Y</code></td>
<td>
<p>matrix of responses with observations/subjects on column and repeats for each observation/subject on rows. It is (m x n) dimensional.</p>
</td></tr>
<tr><td><code id="lmmpar_+3A_x">X</code></td>
<td>
<p>observed design matrices for fixed effects. It is (m*n x p) dimensional.</p>
</td></tr>
<tr><td><code id="lmmpar_+3A_z">Z</code></td>
<td>
<p>observed design matrices for random effects. It is (m*n x q) dimensional.</p>
</td></tr>
<tr><td><code id="lmmpar_+3A_subject">subject</code></td>
<td>
<p>vector of positions that belong to each subject.</p>
</td></tr>
<tr><td><code id="lmmpar_+3A_beta">beta</code></td>
<td>
<p>fixed effect estimation vector with length p.</p>
</td></tr>
<tr><td><code id="lmmpar_+3A_r">R</code></td>
<td>
<p>variance-covariance matrix of residuals.</p>
</td></tr>
<tr><td><code id="lmmpar_+3A_d">D</code></td>
<td>
<p>variance-covariance matrix of random effects.</p>
</td></tr>
<tr><td><code id="lmmpar_+3A_sigma">sigma</code></td>
<td>
<p>initial sigma value.</p>
</td></tr>
<tr><td><code id="lmmpar_+3A_maxiter">maxiter</code></td>
<td>
<p>the maximum number of iterations that should be calculated.</p>
</td></tr>
<tr><td><code id="lmmpar_+3A_cores">cores</code></td>
<td>
<p>the number of cores. Why not to use maximum?!</p>
</td></tr>
<tr><td><code id="lmmpar_+3A_verbose">verbose</code></td>
<td>
<p>boolean that defaults to print iteration context</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>
# Set up fake data
n &lt;- 1000  # number of subjects
m &lt;- 4      # number of repeats
N &lt;- n * m  # true size of data
p &lt;- 15     # number of betas
q &lt;- 2      # width of random effects

# Initial parameters
# beta has a 1 for the first value.  all other values are ~N(10, 1)
beta &lt;- rbind(1, matrix(rnorm(p, 10), p, 1))
R &lt;- diag(m)
D &lt;- matrix(c(16, 0, 0, 0.025), nrow = q)
sigma &lt;- 1

# Set up data
subject &lt;- rep(1:n, each = m)
repeats &lt;- rep(1:m, n)

subj_x &lt;- lapply(1:n, function(i) cbind(1, matrix(rnorm(m * p), nrow = m)))
X &lt;- do.call(rbind, subj_x)
Z &lt;- X[, 1:q]
subj_beta &lt;- lapply(1:n, function(i) mnormt::rmnorm(1, rep(0, q), D))
subj_err &lt;- lapply(1:n, function(i) mnormt::rmnorm(1, rep(0, m), sigma * R))

# create a known response
subj_y &lt;- lapply(
  seq_len(n),
  function(i) {
    (subj_x[[i]] %*% beta) +
      (subj_x[[i]][, 1:q] %*% subj_beta[[i]]) +
      subj_err[[i]]
  }
)
Y &lt;- do.call(rbind, subj_y)

# run the algorithm to recover the known betas
ans &lt;- lmmpar(
  Y,
  X,
  Z,
  subject,
  beta = beta,
  R = R,
  D = D,
  cores = 1, # increase for faster computation
  sigma = sigma,
  verbose = TRUE
)
str(ans)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
