<!DOCTYPE html><html><head><title>Help for package pdqr</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {pdqr}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#pdqr-package'><p>pdqr: Work with Custom Distribution Functions</p></a></li>
<li><a href='#as_p'><p>Convert to pdqr-function</p></a></li>
<li><a href='#enpoint'><p>Represent pdqr-function as a set of points</p></a></li>
<li><a href='#form_estimate'><p>Create a pdqr-function for distribution of sample estimate</p></a></li>
<li><a href='#form_mix'><p>Form mixture of distributions</p></a></li>
<li><a href='#form_recenter'><p>Change center and spread of distribution</p></a></li>
<li><a href='#form_regrid'><p>Change grid of pdqr-function</p></a></li>
<li><a href='#form_resupport'><p>Change support of pdqr-function</p></a></li>
<li><a href='#form_retype'><p>Change type of pdqr-function</p></a></li>
<li><a href='#form_smooth'><p>Smooth pdqr-function</p></a></li>
<li><a href='#form_tails'><p>Transform tails of distribution</p></a></li>
<li><a href='#form_trans'><p>Transform pdqr-function</p></a></li>
<li><a href='#meta'><p>Get metadata of pdqr-function</p></a></li>
<li><a href='#methods-group-generic'><p>Pdqr methods for S3 group generic functions</p></a></li>
<li><a href='#methods-plot'><p>Pdqr methods for base plotting functions</p></a></li>
<li><a href='#methods-print'><p>Pdqr methods for print function</p></a></li>
<li><a href='#new_p'><p>Create new pdqr-function</p></a></li>
<li><a href='#pdqr_approx_error'><p>Diagnose pdqr approximation</p></a></li>
<li><a href='#region'><p>Work with regions</p></a></li>
<li><a href='#summ_center'><p>Summarize distribution with center</p></a></li>
<li><a href='#summ_classmetric'><p>Summarize pair of distributions with classification metric</p></a></li>
<li><a href='#summ_distance'><p>Summarize pair of distributions with distance</p></a></li>
<li><a href='#summ_entropy'><p>Summarize distribution with entropy</p></a></li>
<li><a href='#summ_hdr'><p>Summarize distribution with Highest Density Region</p></a></li>
<li><a href='#summ_interval'><p>Summarize distribution with interval</p></a></li>
<li><a href='#summ_moment'><p>Summarize distribution with moment</p></a></li>
<li><a href='#summ_order'><p>Summarize list of pdqr-functions with order</p></a></li>
<li><a href='#summ_prob_true'><p>Summarize boolean distribution with probability</p></a></li>
<li><a href='#summ_pval'><p>Summarize distribution with p-value</p></a></li>
<li><a href='#summ_quantile'><p>Summarize distribution with quantiles</p></a></li>
<li><a href='#summ_roc'><p>Summarize distributions with ROC curve</p></a></li>
<li><a href='#summ_separation'><p>Summarize distributions with separation threshold</p></a></li>
<li><a href='#summ_spread'><p>Summarize distribution with spread</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Work with Custom Distribution Functions</td>
</tr>
<tr>
<td>Version:</td>
<td>0.3.1</td>
</tr>
<tr>
<td>Description:</td>
<td>Create, transform, and summarize custom random
    variables with distribution functions (analogues of 'p*()', 'd*()',
    'q*()', and 'r*()' functions from base R). Two types of distributions
    are supported: "discrete" (random variable has finite number of output
    values) and "continuous" (infinite number of values in the form of
    continuous random variable). Functions for distribution
    transformations and summaries are available. Implemented approaches
    often emphasize approximate and numerical solutions: all distributions
    assume finite support and finite values of density function; some
    methods implemented with simulation techniques.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/echasnovski/pdqr">https://github.com/echasnovski/pdqr</a>,
<a href="https://echasnovski.github.io/pdqr/">https://echasnovski.github.io/pdqr/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/echasnovski/pdqr/issues">https://github.com/echasnovski/pdqr/issues</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.3)</td>
</tr>
<tr>
<td>Imports:</td>
<td>graphics, grDevices, stats</td>
</tr>
<tr>
<td>Suggests:</td>
<td>covr, knitr, pillar, rmarkdown, spelling, testthat, vdiffr</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Language:</td>
<td>en-US</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>Collate:</td>
<td>'as_p.R' 'as_d.R' 'as_q.R' 'as_r.R' 'assertions.R'
'form-compare.R' 'form-other.R' 'form_regrid.R'
'form_resupport.R' 'form_retype.R' 'form_tails.R'
'form_trans.R' 'group-generics.R' 'meta.R' 'print.R' 'new_p.R'
'new_d.R' 'new_q.R' 'new_r.R' 'pdqr-package.R' 'plot.R'
'region.R' 'summ-other.R' 'summ_center.R' 'summ_classmetric.R'
'summ_distance.R' 'summ_entropy.R' 'summ_hdr.R'
'summ_interval.R' 'summ_moment.R' 'summ_order.R' 'summ_pval.R'
'summ_roc.R' 'summ_separation.R' 'summ_spread.R' 'utils-as.R'
'utils-form.R' 'utils-new.R' 'utils-summ.R' 'utils.R' 'x_tbl.R'
'zzz.R'</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-05-12 14:35:42 UTC; evgeni</td>
</tr>
<tr>
<td>Author:</td>
<td>Evgeni Chasnovski <a href="https://orcid.org/0000-0002-1617-4019"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Evgeni Chasnovski &lt;evgeni.chasnovski@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-05-12 14:50:03 UTC</td>
</tr>
</table>
<hr>
<h2 id='pdqr-package'>pdqr: Work with Custom Distribution Functions</h2><span id='topic+pdqr'></span><span id='topic+pdqr-package'></span>

<h3>Description</h3>

<p>Create, transform, and summarize custom random variables with distribution functions (analogues of 'p*()', 'd*()', 'q*()', and 'r*()' functions from base R). Two types of distributions are supported: &quot;discrete&quot; (random variable has finite number of output values) and &quot;continuous&quot; (infinite number of values in the form of continuous random variable). Functions for distribution transformations and summaries are available. Implemented approaches often emphasize approximate and numerical solutions: all distributions assume finite support and finite values of density function; some methods implemented with simulation techniques.
</p>


<h3>Details</h3>

<p>Excerpt of important documentation:
</p>

<ul>
<li><p> README and vignettes provide overview of package functionality.
</p>
</li>
<li><p> Documentation of <a href="#topic+meta_all">meta_*()</a> functions describes implementation
details of pdqr-functions.
</p>

<ul>
<li><p> Documentation of <a href="#topic+methods-print">print()</a> and <a href="#topic+methods-plot">plot()</a>
methods describes how you can interactively explore properties of
pdqr-functions.
</p>
</li></ul>

</li>
<li><p> Documentation of <a href="#topic+new_p">new_*()</a> functions describes the process of
creating pdqr-functions.
</p>
</li>
<li><p> Documentation of <a href="#topic+as_p">as_*()</a> functions describes the process of
updating class of pdqr-functions.
</p>
</li>
<li><p> Documentation of <code style="white-space: pre;">&#8288;form_*()&#8288;</code> functions describes how different
transformation functions work. Important pages are for <code><a href="#topic+form_trans">form_trans()</a></code> and
<a href="#topic+methods-group-generic">Pdqr methods for S3 group generic functions</a>.
</p>
</li>
<li><p> Documentation of <code style="white-space: pre;">&#8288;summ_*()&#8288;</code> functions describes how different summary
functions work. A good place to start is <code><a href="#topic+summ_center">summ_center()</a></code>.
</p>
</li>
<li><p> Documentation of <code><a href="#topic+region">region_*()</a></code> functions describes functionality
to work with regions: data frames defining subset of one dimensional real
line.
</p>
</li></ul>

<p>This package has the following options (should be set by
<a href="base.html#topic+options">options()</a>):
</p>

<ul>
<li> <p><em>&quot;pdqr.approx_discrete_n_grid&quot;</em>. This single number (default to 1000)
determines degree of granularity of how continuous pdqr-function is
approximated with discrete one during some complicated tasks. Approximation
is done by first using <code><a href="#topic+form_regrid">form_regrid()</a></code> with <code>n_grid</code> argument equal to this
option and <code>method = "x"</code>, and then <code><a href="#topic+form_retype">form_retype()</a></code> is used with <code>type = "discrete"</code> and <code>method = "piecelin"</code>. Value of this option should be big
enough for high accuracy and small enough for high computation
speed, for which value 1000 showed to be fairly appropriate.
</p>
</li>
<li> <p><em>&quot;pdqr.assert_args&quot;</em>. This boolean option (default to <code>TRUE</code>) may be used
to turn off sanity checks of function arguments (set it to <code>FALSE</code>), which
will somewhat increase general execution speed. <strong>Use this option at your own
risk in case you are confident that input arguments have correct type and
structure</strong>.
</p>
</li>
<li> <p><em>&quot;pdqr.group_gen.args_new&quot;</em>, <em>&quot;pdqr.group_gen.n_sample&quot;</em>,
<em>&quot;pdqr.group_gen.repair_supp_method&quot;</em>. They may be used to customize behavior
of methods for S3 group generic functions. See <a href="#topic+methods-group-generic">their help page</a> for more information.
</p>
</li></ul>



<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Evgeni Chasnovski <a href="mailto:evgeni.chasnovski@gmail.com">evgeni.chasnovski@gmail.com</a> (<a href="https://orcid.org/0000-0002-1617-4019">ORCID</a>)
</p>


<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://github.com/echasnovski/pdqr">https://github.com/echasnovski/pdqr</a>
</p>
</li>
<li> <p><a href="https://echasnovski.github.io/pdqr/">https://echasnovski.github.io/pdqr/</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/echasnovski/pdqr/issues">https://github.com/echasnovski/pdqr/issues</a>
</p>
</li></ul>


<hr>
<h2 id='as_p'>Convert to pdqr-function</h2><span id='topic+as_p'></span><span id='topic+as_p.default'></span><span id='topic+as_p.pdqr'></span><span id='topic+as_d'></span><span id='topic+as_d.default'></span><span id='topic+as_d.pdqr'></span><span id='topic+as_q'></span><span id='topic+as_q.default'></span><span id='topic+as_q.pdqr'></span><span id='topic+as_r'></span><span id='topic+as_r.default'></span><span id='topic+as_r.pdqr'></span><span id='topic+as-pdqr'></span>

<h3>Description</h3>

<p>Convert some function to be a proper pdqr-function of specific
<a href="#topic+meta_class">class</a>, i.e. a function describing distribution with finite
support and finite values of probability/density.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as_p(f, ...)

## Default S3 method:
as_p(f, support = NULL, ..., n_grid = 10001)

## S3 method for class 'pdqr'
as_p(f, ...)

as_d(f, ...)

## Default S3 method:
as_d(f, support = NULL, ..., n_grid = 10001)

## S3 method for class 'pdqr'
as_d(f, ...)

as_q(f, ...)

## Default S3 method:
as_q(f, support = NULL, ..., n_grid = 10001)

## S3 method for class 'pdqr'
as_q(f, ...)

as_r(f, ...)

## Default S3 method:
as_r(f, support = NULL, ..., n_grid = 10001,
  n_sample = 10000, args_new = list())

## S3 method for class 'pdqr'
as_r(f, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="as_p_+3A_f">f</code></td>
<td>
<p>Appropriate function to be converted (see Details).</p>
</td></tr>
<tr><td><code id="as_p_+3A_...">...</code></td>
<td>
<p>Extra arguments to <code>f</code>.</p>
</td></tr>
<tr><td><code id="as_p_+3A_support">support</code></td>
<td>
<p>Numeric vector with two increasing elements describing desired
support of output. If <code>NULL</code> or any its value is <code>NA</code>, detection is done
using specific algorithms (see Details).</p>
</td></tr>
<tr><td><code id="as_p_+3A_n_grid">n_grid</code></td>
<td>
<p>Number of grid points at which <code>f</code> will be evaluated (see
Details). Bigger values lead to better approximation precision, but worse
memory usage and evaluation speed (direct and in <code style="white-space: pre;">&#8288;summ_*()&#8288;</code> functions).</p>
</td></tr>
<tr><td><code id="as_p_+3A_n_sample">n_sample</code></td>
<td>
<p>Number of points to sample from <code>f</code> inside <code>as_r()</code>.</p>
</td></tr>
<tr><td><code id="as_p_+3A_args_new">args_new</code></td>
<td>
<p>List of extra arguments for <code><a href="#topic+new_d">new_d()</a></code> to control <code><a href="stats.html#topic+density">density()</a></code>
inside <code>as_r()</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>General purpose of <code style="white-space: pre;">&#8288;as_*()&#8288;</code> functions is to create a proper
pdqr-function of desired class from input which doesn't satisfy these
conditions. Here is described sequence of steps which are taken to achieve
that goal.
</p>
<p>If <strong><code>f</code> is already a pdqr-function</strong>, <code style="white-space: pre;">&#8288;as_*()&#8288;</code> functions properly update it
to have specific class. They take input's <a href="#topic+meta_x_tbl">&quot;x_tbl&quot; metadata</a>
and <a href="#topic+meta_type">type</a> to use with corresponding <a href="#topic+new-pdqr">new_*()</a>
function. For example, <code>as_p(f)</code> in case of pdqr-function <code>f</code> is essentially
the same as <code>new_p(x = meta_x_tbl(f), type = meta_type(f))</code>.
</p>
<p>If <strong><code>f</code> is a function describing &quot;honored&quot; distribution</strong>, it is detected
and output is created in predefined way taking into account extra arguments
in <code>...</code>. For more details see &quot;Honored distributions&quot; section.
</p>
<p>If <strong><code>f</code> is some other unknown function</strong>, <code style="white-space: pre;">&#8288;as_*()&#8288;</code> functions use heuristics
for approximating input distribution with a &quot;proper&quot; pdqr-function. Outputs
of <code style="white-space: pre;">&#8288;as_*()&#8288;</code> can be only pdqr-functions of type &quot;continuous&quot; (because of
issues with support detection). It is assumed that <code>f</code> returns values
appropriate for desired output class of <code style="white-space: pre;">&#8288;as_*()&#8288;</code> function and output type
&quot;continuous&quot;. For example, input for <code>as_p()</code> should return values of some
continuous cumulative distribution function (monotonically non-increasing
values from 0 to 1). To manually create function of type &quot;discrete&quot;, supply
data frame input describing it to appropriate <code style="white-space: pre;">&#8288;new_*()&#8288;</code> function.
</p>
<p>General algorithm of how <code style="white-space: pre;">&#8288;as_*()&#8288;</code> functions work for unknown function is as
follows:
</p>

<ul>
<li> <p><strong>Detect support</strong>. See &quot;Support detection&quot; section for more details.
</p>
</li>
<li> <p><strong>Create data frame input for <code style="white-space: pre;">&#8288;new_*()&#8288;</code></strong>. The exact process differs:
</p>

<ul>
<li><p> In <code>as_p()</code> equidistant grid of <code>n_grid</code> points is created inside
detected support. After that, input's values at the grid is taken as
reference points of cumulative distribution function used to
<em>approximate</em> density at that same grid. This method showed to work more
reliably in case density goes to infinity. That grid and density values
are used as &quot;x&quot; and &quot;y&quot; columns of data frame input for <code>new_p()</code>.
</p>
</li>
<li><p> In <code>as_d()</code> &quot;x&quot; column of data frame is the same equidistant grid is
taken as in <code>as_p()</code>. &quot;y&quot; column is taken as input's values at this grid
after possibly imputing infinity values. This imputation is done by
taking maximum from left and right linear extrapolations on mentioned
grid.
</p>
</li>
<li><p> In <code>as_q()</code>, at first inverse of input <code>f</code> function is computed on [0;
1] interval. It is done by approximating it with piecewise-linear
function on [0; 1] equidistant grid with <code>n_grid</code> points, imputing
infinity values (which ensures finite support), and computing inverse of
approximation. This inverse of <code>f</code> is used to create data frame input
with <code>as_p()</code>.
</p>
</li>
<li><p> In <code>as_r()</code> at first d-function with <code>new_d()</code> is created based on the
same sample used for support detection and extra arguments supplied as
list in <code>args_new</code> argument. In other words, density estimation is done
based on sample, generated from input <code>f</code>. After that, its values are
used to create data frame with <code>as_d()</code>.
</p>
</li></ul>

</li>
<li> <p><strong>Use appropriate <code style="white-space: pre;">&#8288;new_*()&#8288;</code> function</strong> with data frame from previous step
and <code>type = "continuous"</code>. This step implies that all tails outside detected
support are trimmed and data frame is normalized to represent proper
piecewise-linear density.
</p>
</li></ul>



<h3>Value</h3>

<p>A pdqr-function of corresponding <a href="#topic+meta_class">class</a>.
</p>


<h3>Honored distributions</h3>

<p>For efficient workflow, some commonly used distributions are recognized as
special (&quot;honored&quot;). Those receive different treatment in <code style="white-space: pre;">&#8288;as_*()&#8288;</code> functions.
</p>
<p>Basically, there is a manually selected list of &quot;honored&quot; distributions with
all their information enough to detect them. Currently that list has all
common univariate distributions <a href="stats.html#topic+Distributions">from 'stats' package</a>,
i.e. all except multinomial and &quot;less common distributions of test
statistics&quot;.
</p>
<p>&quot;Honored&quot; distribution is <strong>recognized only if <code>f</code> is one of <code style="white-space: pre;">&#8288;p*()&#8288;</code>, <code style="white-space: pre;">&#8288;d*()&#8288;</code>,
<code style="white-space: pre;">&#8288;q*()&#8288;</code>, or <code style="white-space: pre;">&#8288;r*()&#8288;</code> function describing honored distribution and is supplied as
variable with original name</strong>. For example, <code>as_d(dunif)</code> will be treated as
&quot;honored&quot; distribution but <code>as_d(function(x) {dunif(x)})</code> will not.
</p>
<p>After it is recognized that input <code>f</code> represents &quot;honored&quot; distribution,
<strong>its support is computed based on predefined rules</strong>. Those take into
account special features of distribution (like infinite support or infinite
density values) and supplied extra arguments in <code>...</code>. Usually output support
&quot;loses&quot; only around <code>1e-6</code> probability on each infinite tail.
</p>
<p>After that, for &quot;discrete&quot; type output <code>new_d()</code> is used for appropriate data
frame input and for &quot;continuous&quot; - <code>as_d()</code> with appropriate <code style="white-space: pre;">&#8288;d*()&#8288;</code> function
and support. D-function is then converted to desired class with <code style="white-space: pre;">&#8288;as_*()&#8288;</code>.
</p>


<h3>Support detection</h3>

<p>In case input is a function without any extra information, <code style="white-space: pre;">&#8288;as_*()&#8288;</code> functions
must know which finite support its output should have. User can supply
desired support directly with <code>support</code> argument, which can also be <code>NULL</code>
(mean automatic detection of both edges) or have <code>NA</code> to detect only those
edges.
</p>
<p>Support is detected in order to preserve as much information as practically
reasonable. Exact methods differ:
</p>

<ul>
<li><p> In <code>as_p()</code> support is detected as values at which input function is equal
to <code>1e-6</code> (left edge detection) and <code>1 - 1e-6</code> (right edge), which means
&quot;losing&quot; <code>1e-6</code> probability on each tail. <strong>Note</strong> that those values are
searched inside [-10^100; 10^100] interval.
</p>
</li>
<li><p> In <code>as_d()</code>, at first an attempt at finding one point of non-zero density
is made by probing 10000 points spread across wide range of real line
(approximately from <code>-1e7</code> to <code>1e7</code>). If input's value at all of them is
zero, error is thrown. After finding such point, cumulative distribution
function is made by integrating input with <a href="stats.html#topic+integrate">integrate()</a>
using found point as reference (without this there will be poor accuracy of
<code>integrate()</code>). Created CDF function is used to find <code>1e-6</code> and <code>1 - 1e-6</code>
quantiles as in <code>as_p()</code>, which serve as detected support.
</p>
</li>
<li><p> In <code>as_q()</code> quantiles for 0 and 1 are probed for being infinite. If they
are, <code>1e-6</code> and <code>1 - 1e-6</code> quantiles are used respectively instead of
infinite values to form detected support.
</p>
</li>
<li><p> In <code>as_r()</code> sample of size <code>n_sample</code> is generated and detected support is
its range stretched by mean difference of sorted points (to account for
possible tails at which points were not generated). <strong>Note</strong> that this means
that original input <code>f</code> &quot;demonstrates its randomness&quot; only once inside
<code>as_r()</code>, with output then used for approximation of &quot;original randomness&quot;.
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+pdqr_approx_error">pdqr_approx_error()</a></code> for computing approximation errors compared to
some reference function (usually input to <code style="white-space: pre;">&#8288;as_*()&#8288;</code> family).
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Convert existing "proper" pdqr-function
set.seed(101)
x &lt;- rnorm(10)
my_d &lt;- new_d(x, "continuous")

my_p &lt;- as_p(my_d)

# Convert "honored" function to be a proper pdqr-function. To use this
# option, supply originally named function.
p_unif &lt;- as_p(punif)
r_beta &lt;- as_r(rbeta, shape1 = 2, shape2 = 2)
d_pois &lt;- as_d(dpois, lambda = 5)

## `pdqr_approx_error()` computes pdqr approximation error
summary(pdqr_approx_error(as_d(dnorm), dnorm))

## This will work as if input is unkonw function because of unsupported
## variable name
my_runif &lt;- function(n) {
  runif(n)
}
r_unif_2 &lt;- as_r(my_runif)
plot(as_d(r_unif_2))

# Convert some other function to be a "proper" pdqr-function
my_d_quadr &lt;- as_d(function(x) {
  0.75 * (1 - x^2)
}, support = c(-1, 1))

# Support detection
unknown &lt;- function(x) {
  dnorm(x, mean = 1)
}
## Completely automatic support detection
as_d(unknown)
## Semi-automatic support detection
as_d(unknown, support = c(-4, NA))
as_d(unknown, support = c(NA, 5))

## If support is very small and very distant from zero, it probably won't
## get detected in `as_d()` (throwing a relevant error)
## Not run: 
as_d(function(x) {
  dnorm(x, mean = 10000, sd = 0.1)
})

## End(Not run)

# Using different level of granularity
as_d(unknown, n_grid = 1001)
</code></pre>

<hr>
<h2 id='enpoint'>Represent pdqr-function as a set of points</h2><span id='topic+enpoint'></span>

<h3>Description</h3>

<p>Function <code>enpoint()</code> suggests a reasonable default ways of converting
pdqr-function into a data frame of numerical values (points) with desirable
number of rows. Representation of pdqr-function as a set of numbers helps to
conduct analysis using approaches outside of 'pdqr' package. For example, one
can visually display pdqr-function with some other plotting functionality.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>enpoint(f, n_points = 1001)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="enpoint_+3A_f">f</code></td>
<td>
<p>A pdqr-function.</p>
</td></tr>
<tr><td><code id="enpoint_+3A_n_points">n_points</code></td>
<td>
<p>Desired number of points in the output. Not used in case of
&quot;discrete&quot; type p-, d-, and q-function <code>f</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Structure of output depends on <a href="#topic+meta_class">class</a> and
<a href="#topic+meta_type">type</a> of input pdqr-function <code>f</code>:
</p>

<ul>
<li> <p><strong>P-functions</strong> are represented with &quot;x&quot; (for &quot;x&quot; values) and &quot;p&quot; (for
cumulative probability at &quot;x&quot; points) columns:
</p>

<ul>
<li><p> For &quot;continuous&quot; type, &quot;x&quot; is taken as an equidistant grid (with
<code>n_points</code> elements) on input's <a href="#topic+meta_support">support</a>.
</p>
</li>
<li><p> For &quot;discrete&quot; type, &quot;x&quot; is taken directly from <a href="#topic+meta_x_tbl">&quot;x_tbl&quot; metadata</a> without using <code>n_points</code> argument.
</p>
</li></ul>

</li>
<li> <p><strong>D-functions</strong> are represented with &quot;x&quot; column and one more (for values of
d-function at &quot;x&quot; points):
</p>

<ul>
<li><p> For &quot;continuous&quot; type, second column is named &quot;y&quot; and is computed as
values of <code>f</code> at elements of &quot;x&quot; column (which is the same grid as in
p-function case).
</p>
</li>
<li><p> For &quot;discrete&quot; it is named &quot;prob&quot;. Both &quot;x&quot; and &quot;prob&quot; columns are
taken from &quot;x_tbl&quot; metadata.
</p>
</li></ul>

</li>
<li> <p><strong>Q-functions</strong> are represented almost as p-functions but in inverse
fashion. Output data frame has &quot;p&quot; (probabilities) and &quot;x&quot; (values of
q-function <code>f</code> at &quot;p&quot; elements) columns.
</p>

<ul>
<li><p> For &quot;continuous&quot; type, &quot;p&quot; is computed as equidistant grid (with
<code>n_points</code> elements) between 0 and 1.
</p>
</li>
<li><p> For &quot;discrete&quot; type, &quot;p&quot; is taken from &quot;cumprob&quot; column of &quot;x_tbl&quot;
metadata.
</p>
</li></ul>

</li>
<li> <p><strong>R-functions</strong> are represented by generating <code>n_points</code> elements from
distribution. Output data frame has columns &quot;n&quot; (consecutive point number,
basically a row number) and &quot;x&quot; (generated elements).
</p>
</li></ul>

<p><strong>Note</strong> that the other way to produce points for p-, d-, and q-functions is
to manually construct them with <code><a href="#topic+form_regrid">form_regrid()</a></code> and <code><a href="#topic+meta_x_tbl">meta_x_tbl()</a></code>. However,
this method may slightly change function values due to possible
renormalization inside <code>form_regrid()</code>.
</p>


<h3>Value</h3>

<p>A data frame with <code>n_points</code> (or less, for &quot;discrete&quot; type p-, d-, or
q-function <code>f</code>) rows and two columns with names depending on <code>f</code>'s class
and type.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pdqr_approx_error">pdqr_approx_error()</a></code> for diagnostics of pdqr-function approximation
accuracy.
</p>
<p><a href="#topic+methods-plot">Pdqr methods for plot()</a> for a direct plotting of
pdqr-functions.
</p>
<p><code><a href="#topic+form_regrid">form_regrid()</a></code> to change underlying grid of pdqr-function.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>d_norm &lt;- as_d(dnorm)
head(enpoint(d_norm))

# Control number of points with `n_points` argument
enpoint(d_norm, n_points = 5)

# Different pdqr classes and types produce different column names in output
colnames(enpoint(new_p(1:2, "discrete")))
colnames(enpoint(new_d(1:2, "discrete")))
colnames(enpoint(new_d(1:2, "continuous")))
colnames(enpoint(new_q(1:2, "continuous")))
colnames(enpoint(new_r(1:2, "continuous")))

# Manual way with different output structure
df &lt;- meta_x_tbl(form_regrid(d_norm, 5))
## Difference in values due to `form_regrid()` renormalization
plot(enpoint(d_norm, 5), type = "l")
lines(df[["x"]], df[["y"]], col = "blue")
</code></pre>

<hr>
<h2 id='form_estimate'>Create a pdqr-function for distribution of sample estimate</h2><span id='topic+form_estimate'></span>

<h3>Description</h3>

<p>Based on pdqr-function, statistic function, and sample size describe the
distribution of sample estimate. This might be useful for statistical
inference.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>form_estimate(f, stat, sample_size, ..., n_sample = 10000,
  args_new = list())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="form_estimate_+3A_f">f</code></td>
<td>
<p>A pdqr-function.</p>
</td></tr>
<tr><td><code id="form_estimate_+3A_stat">stat</code></td>
<td>
<p>Statistic function. Should be able to accept numeric vector of
size <code>sample_size</code> and return single numeric or logical output.</p>
</td></tr>
<tr><td><code id="form_estimate_+3A_sample_size">sample_size</code></td>
<td>
<p>Size of sample for which distribution of sample estimate
is needed.</p>
</td></tr>
<tr><td><code id="form_estimate_+3A_...">...</code></td>
<td>
<p>Other arguments for <code>stat</code>.</p>
</td></tr>
<tr><td><code id="form_estimate_+3A_n_sample">n_sample</code></td>
<td>
<p>Number of elements to generate from distribution of sample
estimate.</p>
</td></tr>
<tr><td><code id="form_estimate_+3A_args_new">args_new</code></td>
<td>
<p>List of extra arguments for <a href="#topic+new-pdqr">new_*()</a> function to
control <code><a href="stats.html#topic+density">density()</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>General idea is to create a sample from target distribution by
generating <code>n_sample</code> samples of size <code>sample_size</code> and compute for each of
them its estimate by calling input <code>stat</code> function. If created sample is
logical, <strong>boolean</strong> pdqr-function (type &quot;discrete&quot; with elements being
exactly 0 and 1) is created with probability of being true estimated as share
of <code>TRUE</code> values (after removing possible <code>NA</code>). If sample is numeric, it is
used as input to <code style="white-space: pre;">&#8288;new_*()&#8288;</code> of appropriate class with <code>type</code> equal to type of
<code>f</code> (if not forced otherwise in <code>args_new</code>).
</p>
<p><strong>Notes</strong>:
</p>

<ul>
<li><p> This function may be very time consuming for large values of <code>n_sample</code> and
<code>sample_size</code>, as total of <code>n_sample*sample_size</code> numbers are generated and
<code>stat</code> function is called <code>n_sample</code> times.
</p>
</li>
<li><p> Output distribution might have a bias compared to true distribution of
sample estimate. One useful technique for bias correction: compute mean value
of estimate using big <code>sample_size</code> (with <code>mean(as_r(f)(sample_size))</code>) and
then recenter distribution to actually have that as a mean.
</p>
</li></ul>



<h3>Value</h3>

<p>A pdqr-function of the same <a href="#topic+meta_class">class</a> and
<a href="#topic+meta_type">type</a> (if not forced otherwise in <code>args_new</code>) as <code>f</code>.
</p>


<h3>See Also</h3>

<p>Other form functions: 
<code><a href="#topic+form_mix">form_mix</a>()</code>,
<code><a href="#topic+form_regrid">form_regrid</a>()</code>,
<code><a href="#topic+form_resupport">form_resupport</a>()</code>,
<code><a href="#topic+form_retype">form_retype</a>()</code>,
<code><a href="#topic+form_smooth">form_smooth</a>()</code>,
<code><a href="#topic+form_tails">form_tails</a>()</code>,
<code><a href="#topic+form_trans">form_trans</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># These examples take some time to run, so be cautious

set.seed(101)

# Type "discrete"
d_dis &lt;- new_d(data.frame(x = 1:4, prob = 1:4 / 10), "discrete")
## Estimate of distribution of mean
form_estimate(d_dis, stat = mean, sample_size = 10)
## To change type of output, supply it in `args_new`
form_estimate(
  d_dis, stat = mean, sample_size = 10,
  args_new = list(type = "continuous")
)

# Type "continuous"
d_unif &lt;- as_d(dunif)
## Supply extra named arguments for `stat` in `...`
plot(form_estimate(d_unif, stat = mean, sample_size = 10, trim = 0.1))
lines(
  form_estimate(d_unif, stat = mean, sample_size = 10, trim = 0.3),
  col = "red"
)
lines(
  form_estimate(d_unif, stat = median, sample_size = 10),
  col = "blue"
)

# Statistic can return single logical value
d_norm &lt;- as_d(dnorm)
all_positive &lt;- function(x) {
  all(x &gt; 0)
}
## Probability of being true should be around 0.5^5
form_estimate(d_norm, stat = all_positive, sample_size = 5)


</code></pre>

<hr>
<h2 id='form_mix'>Form mixture of distributions</h2><span id='topic+form_mix'></span>

<h3>Description</h3>

<p>Based on a list of pdqr-functions and vector of weights form a pdqr-function
for corresponding mixture distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>form_mix(f_list, weights = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="form_mix_+3A_f_list">f_list</code></td>
<td>
<p>List of pdqr-functions. Can have different
<a href="#topic+meta_class">classes</a> and <a href="#topic+meta_type">types</a> (see Details).</p>
</td></tr>
<tr><td><code id="form_mix_+3A_weights">weights</code></td>
<td>
<p>Numeric vector of weights or <code>NULL</code> (default; corresponds to
equal weights). Should be non-negative numbers with positive sum.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><strong>Type of output mixture</strong> is determined by the following algorithm:
</p>

<ul>
<li><p> If <code>f_list</code> consists only from pdqr-functions of &quot;discrete&quot; type, then
output will have &quot;discrete&quot; type.
</p>
</li>
<li><p> If <code>f_list</code> has at least one pdqr-function of type &quot;continuous&quot;, then
output will have &quot;continuous&quot; type. In this case all &quot;discrete&quot;
pdqr-functions in <code>f_list</code> are approximated with corresponding dirac-like
&quot;continuous&quot; functions (with <a href="#topic+form_retype">form_retype(*, method = &quot;dirac&quot;)</a>). <strong>Note</strong> that this approximation has consequences
during computation of comparisons. For example, if original &quot;discrete&quot;
function <code>f</code> is for distribution with one element <code>x</code>, then probability of
<code>f &gt;= x</code> being true is 1. After retyping to dirac-like function, this
probability will be 0.5, because of symmetrical dirac-like approximation.
Using a little nudge to <code>x</code> of <code>1e-7</code> magnitude in the correct direction
(<code>f &gt;= x - 1e-7</code> in this case) will have expected output.
</p>
</li></ul>

<p><strong>Class of output mixture</strong> is determined by the class of the first element
of <code>f_list</code>. To change output class, use one of <code style="white-space: pre;">&#8288;as_*()&#8288;</code> functions to change
class of first element in <code>f_list</code> or class of output.
</p>
<p><strong>Note</strong> that if output &quot;continuous&quot; pdqr-function for mixture distribution
(in theory) should have discontinuous density, it is approximated
continuously: discontinuities are represented as intervals in
<a href="#topic+meta_x_tbl">&quot;x_tbl&quot;</a> with extreme slopes (see Examples).
</p>


<h3>Value</h3>

<p>A pdqr-function for mixture distribution of certain
<a href="#topic+meta_type">type</a> and <a href="#topic+meta_class">class</a> (see Details).
</p>


<h3>See Also</h3>

<p>Other form functions: 
<code><a href="#topic+form_estimate">form_estimate</a>()</code>,
<code><a href="#topic+form_regrid">form_regrid</a>()</code>,
<code><a href="#topic+form_resupport">form_resupport</a>()</code>,
<code><a href="#topic+form_retype">form_retype</a>()</code>,
<code><a href="#topic+form_smooth">form_smooth</a>()</code>,
<code><a href="#topic+form_tails">form_tails</a>()</code>,
<code><a href="#topic+form_trans">form_trans</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># All "discrete"
d_binom &lt;- as_d(dbinom, size = 10, prob = 0.5)
r_pois &lt;- as_r(rpois, lambda = 1)
dis_mix &lt;- form_mix(list(d_binom, r_pois))
plot(dis_mix)

# All "continuous"
p_norm &lt;- as_p(pnorm)
d_unif &lt;- as_d(dunif)

con_mix &lt;- form_mix(list(p_norm, d_unif), weights = c(0.7, 0.3))
## Output is a p-function, as is first element of `f_list`
con_mix
plot(con_mix)

## Use `as_*()` functions to change class
d_con_mix &lt;- as_d(con_mix)

## Theoretical output density should be discontinuous, but here it is
## approximated with continuous function
con_x_tbl &lt;- meta_x_tbl(con_mix)
con_x_tbl[(con_x_tbl$x &gt;= -1e-4) &amp; (con_x_tbl$x &lt;= 1e-4), ]

# Some "discrete", some "continuous"
all_mix &lt;- form_mix(list(d_binom, d_unif))
plot(all_mix)
all_x_tbl &lt;- meta_x_tbl(all_mix)

## What dirac-like approximation looks like
all_x_tbl[(all_x_tbl$x &gt;= 1.5) &amp; (all_x_tbl$x &lt;= 2.5), ]
</code></pre>

<hr>
<h2 id='form_recenter'>Change center and spread of distribution</h2><span id='topic+form_recenter'></span><span id='topic+form_respread'></span>

<h3>Description</h3>

<p>These functions implement linear transformations, output distribution of
which has desired <a href="#topic+summ_center">center</a> and <a href="#topic+summ_spread">spread</a>. These
functions are useful for creating distributions with some input center and
spread value based on present distribution, which is a common task during
hypothesis testing.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>form_recenter(f, to, method = "mean")

form_respread(f, to, method = "sd", center_method = "mean")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="form_recenter_+3A_f">f</code></td>
<td>
<p>A pdqr-function.</p>
</td></tr>
<tr><td><code id="form_recenter_+3A_to">to</code></td>
<td>
<p>A desired value of summary.</p>
</td></tr>
<tr><td><code id="form_recenter_+3A_method">method</code></td>
<td>
<p>Method of computing center for <code>form_recenter()</code> and spread for
<code>form_respread()</code>. Values should be one of possible <code>method</code> values from
<code><a href="#topic+summ_center">summ_center()</a></code> and <code><a href="#topic+summ_spread">summ_spread()</a></code> respectively.</p>
</td></tr>
<tr><td><code id="form_recenter_+3A_center_method">center_method</code></td>
<td>
<p>Method of computing center for <code>form_respread()</code> in
order to preserve it in output.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>form_recenter(f, to, method)</code> is basically a
<code>f - summ_center(f, method) + to</code>: it moves distribution without affecting
its shape so that output distribution has center at <code>to</code>.
</p>
<p><code>form_respread(f, to, method, center_method)</code> is a following linear
transformation: <code>coef * (f - center) + center</code>, where <code>center</code> is
<code>summ_center(f, center_method)</code> and <code>coef</code> is computed so as to guarantee
output distribution to have spread equal to <code>to</code>. In other words, this linear
transformation stretches distribution around its center until the result has
spread equal to <code>to</code> (center remains the same as in input <code>f</code>).
</p>


<h3>Value</h3>

<p>A pdqr-function describing distribution with desired center or
spread.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>my_beta &lt;- as_d(dbeta, shape1 = 1, shape2 = 3)

my_beta2 &lt;- form_recenter(my_beta, to = 2)
summ_center(my_beta2)

my_beta3 &lt;- form_respread(my_beta2, to = 10, method = "range")
summ_spread(my_beta3, method = "range")
## Center remains unchainged
summ_center(my_beta3)
</code></pre>

<hr>
<h2 id='form_regrid'>Change grid of pdqr-function</h2><span id='topic+form_regrid'></span>

<h3>Description</h3>

<p>Modify grid of pdqr-function (rows of <a href="#topic+meta_x_tbl">&quot;x_tbl&quot; metadata</a>) to
increase (upgrid) or decrease (downgrid) granularity using method of choice.
Upgridding might be useful in order to obtain more information during certain
type of transformations. Downgridding might be useful for decreasing amount
of used memory for storing pdqr-function without losing much information.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>form_regrid(f, n_grid, method = "x")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="form_regrid_+3A_f">f</code></td>
<td>
<p>A pdqr-function.</p>
</td></tr>
<tr><td><code id="form_regrid_+3A_n_grid">n_grid</code></td>
<td>
<p>A desired number of grid elements in output.</p>
</td></tr>
<tr><td><code id="form_regrid_+3A_method">method</code></td>
<td>
<p>Regrid method. Should be one of &quot;x&quot; or &quot;q&quot;.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The goal here is to create pdqr-function which is reasonably similar
to <code>f</code> and has <code>n_grid</code> rows in &quot;x_tbl&quot; metadata.
</p>
<p>General algorithm of regridding is as follows:
</p>

<ul>
<li> <p><strong>Compute reference grid</strong>. For method &quot;x&quot; it is a sequence of equidistant
points between edges of <code>f</code>'s <a href="#topic+meta_support">support</a>. For method &quot;q&quot; -
sequence of quantiles for equidistant probabilities from 0 to 1. Lengths of
reference grids for both methods are <code>n_grid</code>.
</p>
</li>
<li> <p><strong>Adjust <code>f</code>'s grid to reference one</strong>. This is done depending on <code>f</code>'s
<a href="#topic+meta_type">type</a> and which kind or regridding is done (upgridding is the
case when <code>n_grid</code> is strictly more than number of rows in &quot;x_tbl&quot; metadata,
downgridding - when it is strictly less):
</p>

<ul>
<li><p> Type &quot;discrete&quot;:
</p>

<ul>
<li><p> UPgridding &quot;discrete&quot; functions is not possible as it is assumed
that input &quot;discrete&quot; functions can't have any &quot;x&quot; values other then
present ones. In this case input is returned, the only case when
output doesn't have desired <code>n_grid</code> rows in &quot;x_tbl&quot; metadata.
</p>
</li>
<li><p> DOWNgridding &quot;discrete&quot; functions is done by computing nearest
match of reference grid to <code>f</code>'s one and collapsing (by summing
probabilities) all &quot;x&quot; values from input to the nearest matched ones.
Here &quot;computing nearest match&quot; means that every element of reference
grid is one-one matched with subset of unique values from <code>f</code>'s &quot;x&quot;
elements. Matching is done in greedy iterative fashion in order to
minimize total distance between reference grid and matched subset.
<strong>Note</strong> that this can result in not optimal (with not minimum total
distance) match and can take a while to compute in some cases.
</p>
</li></ul>

</li>
<li><p> Type &quot;continuous&quot;:
</p>

<ul>
<li><p> UPgridding &quot;continuous&quot; functions is done by adding rows to &quot;x_tbl&quot;
metadata with &quot;x&quot; values equal to those elements of reference grid
which are the furthest away from input &quot;x&quot; grid as a set. Distance
from point to set is meant as minimum of distances between point and
all points of set. Values of &quot;y&quot; and &quot;cumprob&quot; columns are taken as
values of corresponding to <code>f</code> d- and p-functions.
</p>
</li>
<li><p> DOWNgridding &quot;continuous&quot; functions is done by computing nearest
match of reference grid to <code>f</code>'s one (as for &quot;discrete&quot; type) and
removing all unmatched rows from &quot;x_tbl&quot; metadata.
</p>
</li></ul>

</li></ul>

</li></ul>

<p>Special cases of <code>n_grid</code>:
</p>

<ul>
<li><p> If <code>n_grid</code> is the same as number of rows in &quot;x_tbl&quot; metadata, then input
<code>f</code> is returned.
</p>
</li>
<li><p> If <code>n_grid</code> is 1, appropriate <code style="white-space: pre;">&#8288;new_*()&#8288;</code> function is used with single
numeric input equal to distribution's median.
</p>
</li></ul>



<h3>Value</h3>

<p>A pdqr-function with modified grid.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+form_resupport">form_resupport()</a></code> for changing support of pdqr-function.
</p>
<p><code><a href="#topic+form_retype">form_retype()</a></code> for changing type of pdqr-function.
</p>
<p>Other form functions: 
<code><a href="#topic+form_estimate">form_estimate</a>()</code>,
<code><a href="#topic+form_mix">form_mix</a>()</code>,
<code><a href="#topic+form_resupport">form_resupport</a>()</code>,
<code><a href="#topic+form_retype">form_retype</a>()</code>,
<code><a href="#topic+form_smooth">form_smooth</a>()</code>,
<code><a href="#topic+form_tails">form_tails</a>()</code>,
<code><a href="#topic+form_trans">form_trans</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Type "discrete"
d_dis &lt;- new_d(data.frame(x = 1:10, prob = 1:10 / 55), type = "discrete")

# Downgridding
meta_x_tbl(form_regrid(d_dis, n_grid = 4))
meta_x_tbl(form_regrid(d_dis, n_grid = 4, method = "q"))

# Upgridding for "discrete" type isn't possible. Input is returned
identical(d_dis, form_regrid(d_dis, n_grid = 100))

# Type "continuous"
# Downgridding
d_norm &lt;- as_d(dnorm)
plot(d_norm)
lines(form_regrid(d_norm, n_grid = 10), col = "blue")
lines(form_regrid(d_norm, n_grid = 10, method = "q"), col = "green")

# Upgridding
d_con &lt;- new_d(data.frame(x = 1:3, y = rep(0.5, 3)), type = "continuous")
meta_x_tbl(form_regrid(d_con, n_grid = 6))

# Pdqr-function with center at median is returned in case `n_grid` is 1
form_regrid(d_dis, n_grid = 1)
# Dirac-like function is returned
form_regrid(d_con, n_grid = 1)
</code></pre>

<hr>
<h2 id='form_resupport'>Change support of pdqr-function</h2><span id='topic+form_resupport'></span>

<h3>Description</h3>

<p>Modify support of pdqr-function using method of choice.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>form_resupport(f, support, method = "reflect")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="form_resupport_+3A_f">f</code></td>
<td>
<p>A pdqr-function.</p>
</td></tr>
<tr><td><code id="form_resupport_+3A_support">support</code></td>
<td>
<p>Numeric vector with two increasing (or non-decreasing, see
Details) elements describing support of the output. Values can be <code>NA</code>, in
which case corresponding edge(s) are taken from <code>f</code>'s support.</p>
</td></tr>
<tr><td><code id="form_resupport_+3A_method">method</code></td>
<td>
<p>Resupport method. One of &quot;reflect&quot;, &quot;trim&quot;, &quot;winsor&quot;, &quot;linear&quot;.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Method &quot;reflect&quot; takes a density &quot;tails&quot; to the left of <code>support[1]</code>
and to the right of <code>support[2]</code> and reflects them inside <code>support</code>. It means
that values of density inside and outside of supplied <code>support</code> are added
together in &quot;symmetric fashion&quot;:
<code>d(x) = d_f(x) + d_f(l - (x-l)) + d_f(r + (r-x))</code>, where <code>d_f</code> is density of
input, <code>d</code> is density of output, <code>l</code> and <code>r</code> are left and right edges of
input <code>support</code>. This option is useful for repairing support of
<a href="#topic+new-pdqr">new_*()</a>'s output, as by default kernel density estimation in
<code>density()</code> adds tails to the range of input <code>x</code> values. For example, if
there is a need to ensure that distribution has only positive values, one can
do <code>form_resupport(f, c(0, NA), method = "reflect")</code>. <strong>Notes</strong>:
</p>

<ul>
<li><p> For &quot;discrete&quot; pdqr-functions that might result into creating new &quot;x&quot;
values of distribution.
</p>
</li>
<li><p> Reflection over <code>support[1]</code> is done only if it is strictly greater than
<code>f</code>'s left edge of support. Reflection over <code>support[2]</code> - if <code>f</code>'s right
edge is strictly smaller.
</p>
</li></ul>

<p>Method &quot;trim&quot; removes density &quot;tails&quot; outside of <code>support</code>, normalizes the
rest and creates appropriate pdqr-function.
</p>
<p>Method &quot;winsor&quot; makes all density &quot;tails&quot; outside of input <code>support</code>
&quot;squashed&quot; inside it in &quot;dirac-like&quot; fashion. It means that probability from
both tails is moved inside <code>support</code> and becomes concentrated in <code>1e-8</code>
neighborhood of nearest edge. This models a singular dirac distributions at
the edges of <code>support</code>. <strong>Note</strong> that <code>support</code> can represent single point,
in which case output has single element if <code>f</code>'s type is &quot;discrete&quot; or is a
dirac-like distribution in case of &quot;continuous&quot; type.
</p>
<p>Method &quot;linear&quot; transforms <code>f</code>'s support linearly to be input <code>support</code>. For
example, if <code>f</code>'s support is [0; 1] and <code>support</code> is <code>c(-1, 1)</code>, linear
resupport is equivalent to <code>2*f - 1</code>. <strong>Note</strong> that <code>support</code> can represent
single point with the same effect as in &quot;winsor&quot; method.
</p>


<h3>Value</h3>

<p>A pdqr-function with modified support and the same
<a href="#topic+meta_class">class</a> and <a href="#topic+meta_type">type</a> as <code>f</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+form_regrid">form_regrid()</a></code> for changing grid (rows of &quot;x_tbl&quot; metadata) of
pdqr-function.
</p>
<p><code><a href="#topic+form_retype">form_retype()</a></code> for changing type of pdqr-function.
</p>
<p>Other form functions: 
<code><a href="#topic+form_estimate">form_estimate</a>()</code>,
<code><a href="#topic+form_mix">form_mix</a>()</code>,
<code><a href="#topic+form_regrid">form_regrid</a>()</code>,
<code><a href="#topic+form_retype">form_retype</a>()</code>,
<code><a href="#topic+form_smooth">form_smooth</a>()</code>,
<code><a href="#topic+form_tails">form_tails</a>()</code>,
<code><a href="#topic+form_trans">form_trans</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(101)
d_norm &lt;- as_d(dnorm)
d_dis &lt;- new_d(data.frame(x = 1:4, prob = 1:4 / 10), "discrete")

# Method "reflect"
plot(d_norm)
lines(form_resupport(d_norm, c(-2, 1.5), "reflect"), col = "blue")

# For "discrete" functions it might create new values
meta_x_tbl(form_resupport(d_dis, c(NA, 2.25), "reflect"))

# This is often useful to ensure constraints after `new_()`
x &lt;- runif(1e4)
d_x &lt;- new_d(x, "continuous")
plot(d_x)
lines(form_resupport(d_x, c(0, NA), "reflect"), col = "red")
lines(form_resupport(d_x, c(0, 1), "reflect"), col = "blue")

# Method "trim"
plot(d_norm)
lines(form_resupport(d_norm, c(-2, 1.5), "trim"), col = "blue")

# Method "winsor"
plot(d_norm)
lines(form_resupport(d_norm, c(-2, 1.5), "winsor"), col = "blue")

# Method "linear"
plot(d_norm)
lines(form_resupport(d_norm, c(-2, 1.5), "linear"), col = "blue")
</code></pre>

<hr>
<h2 id='form_retype'>Change type of pdqr-function</h2><span id='topic+form_retype'></span>

<h3>Description</h3>

<p>Modify <a href="#topic+meta_type">type</a> of pdqr-function using method of choice.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>form_retype(f, type = NULL, method = "value")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="form_retype_+3A_f">f</code></td>
<td>
<p>A pdqr-function.</p>
</td></tr>
<tr><td><code id="form_retype_+3A_type">type</code></td>
<td>
<p>A desired type of output. Should be one of &quot;discrete&quot; or
&quot;continuous&quot;. If <code>NULL</code> (default), it is chosen as an opposite of <code>f</code>'s
type.</p>
</td></tr>
<tr><td><code id="form_retype_+3A_method">method</code></td>
<td>
<p>Retyping method. Should be one of &quot;value&quot;, &quot;piecelin&quot;, &quot;dirac&quot;.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If type of <code>f</code> is equal to input <code>type</code> then <code>f</code> is returned.
</p>
<p>Method &quot;value&quot; uses renormalized columns of <code>f</code>'s &quot;x_tbl&quot; metadata as values
for output's &quot;x_tbl&quot; metadata. In other words, it preserves ratios between
values of d-function at certain &quot;x&quot; points. Its main advantages are that this
method can work well with any pdqr type and that two consecutive conversions
return the same function. Conversion algorithm is as follows:
</p>

<ul>
<li><p> Retyping from &quot;continuous&quot; to <code>type</code> &quot;discrete&quot; is done by creating
pdqr-function of corresponding class with the following &quot;x_tbl&quot; metadata: &quot;x&quot;
column is the same as in <code>f</code>; &quot;prob&quot; column is equal to <code>f</code>'s &quot;y&quot; column
after renormalization (so that their sum is 1).
</p>
</li>
<li><p> Retyping from &quot;discrete&quot; to <code>type</code> &quot;continuous&quot; is done in the same
fashion: &quot;x&quot; column is the same; &quot;y&quot; column is equal to <code>f</code>'s &quot;prob&quot; column
after renormalization (so that total integral of piecewise-linear density is
equal to 1).
</p>
</li></ul>

<p>Method &quot;piecelin&quot; should be used mostly for converting from &quot;continuous&quot; to
&quot;discrete&quot; type. It uses the fact that 'pdqr' densities are piecewise-linear
(linear in intervals between values of &quot;x&quot; column of <a href="#topic+meta_x_tbl">&quot;x_tbl&quot; metadata</a>) on their <a href="#topic+meta_support">support</a>:
</p>

<ul>
<li><p> Retyping from &quot;continuous&quot; to <code>type</code> &quot;discrete&quot; is done by computing &quot;x&quot;
values as centers of interval masses with probabilities equal to interval
total probabilities.
</p>
</li>
<li><p> Retyping from &quot;discrete&quot; to <code>type</code> &quot;continuous&quot; is made approximately by
trying to compute &quot;x&quot; grid, for which &quot;x&quot; values of input distribution are
going to be centers of mass. Algorithm is approximate and might result into a
big errors in case of small number of &quot;x&quot; values or if they are not
&quot;suitable&quot; for this kind of transformation.
</p>
</li></ul>

<p>Method &quot;dirac&quot; is used mostly for converting from &quot;discrete&quot; to &quot;continuous&quot;
type (for example, in <code>form_mix()</code> in case different types of input
pdqr-functions). It works in the following way:
</p>

<ul>
<li><p> Retyping from &quot;continuous&quot; to <code>type</code> &quot;discrete&quot; works only if &quot;x_tbl&quot;
metadata represents a mixture of dirac-like distributions. In that case it is
transformed to have &quot;x&quot; values from centers of those dirac-like distributions
with corresponding probabilities.
</p>
</li>
<li><p> Retyping from &quot;discrete&quot; to <code>type</code> &quot;continuous&quot; works by transforming each
&quot;x&quot; value from &quot;x_tbl&quot; metadata into dirac-like distribution with total
probability taken from corresponding value of &quot;prob&quot; column. Output
essentially represents a mixture of dirac-like distributions.
</p>
</li></ul>



<h3>Value</h3>

<p>A pdqr-function with type equal to input <code>type</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+form_regrid">form_regrid()</a></code> for changing grid (rows of &quot;x_tbl&quot; metadata) of
pdqr-function.
</p>
<p><code><a href="#topic+form_resupport">form_resupport()</a></code> for changing support of pdqr-function.
</p>
<p>Other form functions: 
<code><a href="#topic+form_estimate">form_estimate</a>()</code>,
<code><a href="#topic+form_mix">form_mix</a>()</code>,
<code><a href="#topic+form_regrid">form_regrid</a>()</code>,
<code><a href="#topic+form_resupport">form_resupport</a>()</code>,
<code><a href="#topic+form_smooth">form_smooth</a>()</code>,
<code><a href="#topic+form_tails">form_tails</a>()</code>,
<code><a href="#topic+form_trans">form_trans</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>my_con &lt;- new_d(data.frame(x = 1:5, y = c(1, 2, 3, 2, 1) / 9), "continuous")
meta_x_tbl(my_con)

# By default, conversion is done to the opposite type
my_dis &lt;- form_retype(my_con)
meta_x_tbl(my_dis)

# Default retyping (with method "value") is accurate when doing consecutive
# retyping
my_con_2 &lt;- form_retype(my_dis, "continuous")
meta_x_tbl(my_con_2)

# Method "dirac"
my_dirac &lt;- form_retype(my_dis, "continuous", method = "dirac")
meta_x_tbl(my_dirac)

# Method "piecelin"
## From "continuous" to "discrete" (preferred direction)
my_dis_piece &lt;- form_retype(my_con, "discrete", method = "piecelin")
meta_x_tbl(my_dis_piece)
## Conversion from "discrete" to "continuous" is very approximate
my_con_piece &lt;- form_retype(my_dis_piece, "continuous", method = "piecelin")
meta_x_tbl(my_con_piece)

plot(my_con, main = 'Approximate nature of method "piecelin"')
lines(my_con_piece, col = "blue")
</code></pre>

<hr>
<h2 id='form_smooth'>Smooth pdqr-function</h2><span id='topic+form_smooth'></span>

<h3>Description</h3>

<p>Smooth pdqr-function using random sampling and corresponding
<a href="#topic+new-pdqr">new_*()</a> function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>form_smooth(f, n_sample = 10000, args_new = list())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="form_smooth_+3A_f">f</code></td>
<td>
<p>A pdqr-function.</p>
</td></tr>
<tr><td><code id="form_smooth_+3A_n_sample">n_sample</code></td>
<td>
<p>Number of elements to sample.</p>
</td></tr>
<tr><td><code id="form_smooth_+3A_args_new">args_new</code></td>
<td>
<p>List of extra arguments for <a href="#topic+new_d">new_*()</a> to control
<code><a href="stats.html#topic+density">density()</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>General idea of smoothing is to preserve &quot;sampling randomness&quot; as
much as reasonably possible while creating more &quot;smooth&quot; probability mass or
density function.
</p>
<p>At first step, sample of size <code>n_sample</code> is generated from distribution
represented by <code>f</code>. Then, based on the sample, &quot;continuous&quot; d-function is
created with <code>new_d()</code> and arguments from <code>args_new</code> list. To account for
<a href="stats.html#topic+density">density()</a>'s default behavior of &quot;stretching range&quot; by
adding small tails, <a href="#topic+meta_support">support</a> of d-function is forced to be
equal to <code>f</code>'s support (this is done with <code><a href="#topic+form_resupport">form_resupport()</a></code> and method
&quot;reflect&quot;). Output represents a &quot;smooth&quot; version of <code>f</code> as d-function.
</p>
<p>Final output is computed by modifying &quot;y&quot; or &quot;prob&quot; column of <code>f</code>'s <a href="#topic+meta_x_tbl">&quot;x_tbl&quot; metadata</a> to be proportional to values of &quot;smooth&quot; output at
corresponding points from &quot;x&quot; column. This way output distribution has
exactly the same &quot;x&quot; grid as <code>f</code> but &quot;more smooth&quot; nature.
</p>


<h3>Value</h3>

<p>A smoothed version of <code>f</code> with the same <a href="#topic+meta_class">class</a> and
<a href="#topic+meta_type">type</a>.
</p>


<h3>See Also</h3>

<p>Other form functions: 
<code><a href="#topic+form_estimate">form_estimate</a>()</code>,
<code><a href="#topic+form_mix">form_mix</a>()</code>,
<code><a href="#topic+form_regrid">form_regrid</a>()</code>,
<code><a href="#topic+form_resupport">form_resupport</a>()</code>,
<code><a href="#topic+form_retype">form_retype</a>()</code>,
<code><a href="#topic+form_tails">form_tails</a>()</code>,
<code><a href="#topic+form_trans">form_trans</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(101)

# Type "discrete"
bad_dis &lt;- new_d(
  data.frame(x = sort(runif(100)), prob = runif(100)),
  type = "discrete"
)
smoothed_dis &lt;- form_smooth(bad_dis)
plot(bad_dis)
lines(smoothed_dis, col = "blue")

# Type "continuous"
bad_con &lt;- new_d(
  data.frame(x = sort(runif(100)), y = runif(100)),
  type = "continuous"
)
smoothed_con &lt;- form_smooth(bad_con)
plot(bad_con)
lines(smoothed_con, col = "blue")
</code></pre>

<hr>
<h2 id='form_tails'>Transform tails of distribution</h2><span id='topic+form_tails'></span>

<h3>Description</h3>

<p>Modify tail(s) of distribution defined by certain cutoff level using method
of choice. This function is useful for doing robust analysis in presence of
possible outliers.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>form_tails(f, level, method = "trim", direction = "both")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="form_tails_+3A_f">f</code></td>
<td>
<p>A pdqr-function.</p>
</td></tr>
<tr><td><code id="form_tails_+3A_level">level</code></td>
<td>
<p>Cutoff level. For direction &quot;both&quot; should be between 0 and 0.5;
for &quot;left&quot; and &quot;right&quot; - between 0 and 1.</p>
</td></tr>
<tr><td><code id="form_tails_+3A_method">method</code></td>
<td>
<p>Modification method. One of &quot;trim&quot; or &quot;winsor&quot;.</p>
</td></tr>
<tr><td><code id="form_tails_+3A_direction">direction</code></td>
<td>
<p>Information about which tail(s) to modify. One of &quot;both&quot;,
&quot;left&quot;, &quot;right&quot;.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Edges for left and right tails are computed as <code>level</code> and <code>1 - level</code> quantiles respectively. The left tail is interval to the left of
left edge, and right tail - to the right of right edge.
</p>
<p>Method &quot;trim&quot; removes tail(s) while normalizing &quot;center part&quot;. Method
&quot;winsor&quot; &quot;squashes&quot; tails inside center of distribution in dirac-like
fashion, i.e. probability of tail(s) is moved inside and becomes concentrated
in <code>1e-8</code> neighborhood of nearest edge.
</p>
<p>Direction &quot;both&quot; affect both tails. Directions &quot;left&quot; and &quot;right&quot; affect only
left and right tail respectively.
</p>


<h3>Value</h3>

<p>A pdqr-function with transformed tail(s).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+form_resupport">form_resupport()</a></code> for changing <a href="#topic+meta_support">support</a> to some
known interval.
</p>
<p><code><a href="#topic+summ_center">summ_center()</a></code> and <code><a href="#topic+summ_spread">summ_spread()</a></code> for computing summaries of distributions.
</p>
<p>Other form functions: 
<code><a href="#topic+form_estimate">form_estimate</a>()</code>,
<code><a href="#topic+form_mix">form_mix</a>()</code>,
<code><a href="#topic+form_regrid">form_regrid</a>()</code>,
<code><a href="#topic+form_resupport">form_resupport</a>()</code>,
<code><a href="#topic+form_retype">form_retype</a>()</code>,
<code><a href="#topic+form_smooth">form_smooth</a>()</code>,
<code><a href="#topic+form_trans">form_trans</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Type "discrete"
my_dis &lt;- new_d(data.frame(x = 1:4, prob = (1:4) / 10), type = "discrete")
meta_x_tbl(form_tails(my_dis, level = 0.1))
meta_x_tbl(
  form_tails(my_dis, level = 0.35, method = "winsor", direction = "left")
)

# Type "continuous"
d_norm &lt;- as_d(dnorm)
plot(d_norm)
lines(form_tails(d_norm, level = 0.1), col = "blue")
lines(
  form_tails(d_norm, level = 0.1, method = "winsor", direction = "right"),
  col = "green"
)

# Use `form_resupport()` and `as_q()` to remove different levels from both
# directions. Here 0.1 level tail from left is removed, and 0.05 level from
# right
new_supp &lt;- as_q(d_norm)(c(0.1, 1 - 0.05))
form_resupport(d_norm, support = new_supp)

# Examples of robust mean
set.seed(101)
x &lt;- rcauchy(1000)
d_x &lt;- new_d(x, "continuous")
summ_mean(d_x)
## Trimmed mean
summ_mean(form_tails(d_x, level = 0.1, method = "trim"))
## Winsorized mean
summ_mean(form_tails(d_x, level = 0.1, method = "winsor"))
</code></pre>

<hr>
<h2 id='form_trans'>Transform pdqr-function</h2><span id='topic+form_trans'></span><span id='topic+form_trans_self'></span>

<h3>Description</h3>

<p>Perform a transformation of pdqr-function(s) (which assumed to be
independent).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>form_trans(f_list, trans, ..., method = "random", n_sample = 10000,
  args_new = list())

form_trans_self(f, trans, ..., method = "random", args_new = list())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="form_trans_+3A_f_list">f_list</code></td>
<td>
<p>A list consisting from pdqr-function(s) and/or single
number(s). Should have at least one pdqr-function (see Details).</p>
</td></tr>
<tr><td><code id="form_trans_+3A_trans">trans</code></td>
<td>
<p>Transformation function. Should take as many (vectorized)
arguments as there are elements in <code>f_list</code> or a single argument for
<code>form_trans_self()</code>. Should return numeric or logical values.</p>
</td></tr>
<tr><td><code id="form_trans_+3A_...">...</code></td>
<td>
<p>Extra arguments to <code>trans</code>.</p>
</td></tr>
<tr><td><code id="form_trans_+3A_method">method</code></td>
<td>
<p>Transformation method. One of &quot;random&quot; or &quot;bruteforce&quot;.</p>
</td></tr>
<tr><td><code id="form_trans_+3A_n_sample">n_sample</code></td>
<td>
<p>Number of elements to sample.</p>
</td></tr>
<tr><td><code id="form_trans_+3A_args_new">args_new</code></td>
<td>
<p>List of extra arguments for <a href="#topic+new_d">new_*()</a> to control
<code><a href="stats.html#topic+density">density()</a></code>.</p>
</td></tr>
<tr><td><code id="form_trans_+3A_f">f</code></td>
<td>
<p>A pdqr-function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>form_trans_self()</code> is a thin wrapper for <code>form_trans()</code> that
accepts a single pdqr-function instead of a list of them.
</p>
<p><a href="#topic+meta_class">Class</a> of output is chosen as class of first pdqr-function in
<code>f_list</code>. <a href="#topic+meta_type">Type</a> of output is chosen to be &quot;discrete&quot; in case
all input pdqr-functions have &quot;discrete&quot; type, and &quot;continuous&quot; otherwise.
</p>
<p>Method &quot;random&quot; performs transformation using random generation of samples:
</p>

<ul>
<li> <p><strong>Generates a sample of size <code>n_sample</code> from every element of <code>f_list</code></strong>
(if element is single number, it is repeated <code>n_sample</code> times).
</p>
</li>
<li> <p><strong>Calls <code>trans</code></strong> with all generated samples (in order aligned with
<code>f_list</code>). <strong>Note</strong> that output should be either numeric or logical and have
<code>n_sample</code> elements (one for each combination of input values in &quot;vectorized&quot;
fashion). So, for example, using <code>sum</code> directly is not possible as it returns
only single number.
</p>
</li>
<li> <p><strong>Creates output pdqr-function</strong>. If output is logical, probability of
being true is estimated as share of <code>TRUE</code> in output, and boolean
pdqr-function is created (type &quot;discrete&quot; with &quot;x&quot; values equal to 0 and 1,
and probabilities of being false and true respectively). If output is
numeric, one of <code style="white-space: pre;">&#8288;new_*()&#8288;</code> (suitable for output class) is called with
arguments from <code>args_new</code> list.
</p>
</li></ul>

<p>Method &quot;bruteforce&quot;:
</p>

<ul>
<li> <p><strong><a href="#topic+form_retype">Retypes</a> input</strong> pdqr-function to &quot;discrete&quot;
type (using &quot;piecelin&quot; method).
</p>
</li>
<li> <p><strong>Computes output for every combination of &quot;x&quot; values</strong> (probability of
which will be a product of corresponding probabilities).
</p>
</li>
<li> <p><strong>Creates pdqr-function of type &quot;discrete&quot;</strong> with suitable <code style="white-space: pre;">&#8288;new_*()&#8288;</code>
function.
</p>
</li>
<li> <p><strong>Possibly retypes to &quot;continuous&quot; type</strong> if output should have it (also
with &quot;piecelin&quot; method).
</p>
</li></ul>

<p><strong>Notes</strong> about &quot;bruteforce&quot; method:
</p>

<ul>
<li><p> Its main advantage is that it is not random.
</p>
</li>
<li><p> It may start to be very memory consuming very quickly.
</p>
</li>
<li><p> It is usually useful when type of output function is &quot;discrete&quot;. In case of
&quot;continuous&quot; type, retyping from &quot;discrete&quot; to &quot;continuous&quot; might introduce
big errors.
</p>
</li>
<li><p> Used &quot;discrete&quot; probabilities shouldn't be very small because they will be
directly multiplied, which might cause numerical accuracy issues.
</p>
</li></ul>



<h3>Value</h3>

<p>A pdqr-function for transformed random variable.
</p>


<h3>See Also</h3>

<p><a href="#topic+methods-group-generic">Pdqr methods for S3 group generic functions</a>
for more accurate implementations of most commonly used functions. Some of
them are direct (without randomness) and some of them use <code>form_trans()</code>
with &quot;random&quot; method.
</p>
<p><code><a href="#topic+form_regrid">form_regrid()</a></code> to increase/decrease granularity of pdqr-functions for method
&quot;bruteforce&quot;.
</p>
<p>Other form functions: 
<code><a href="#topic+form_estimate">form_estimate</a>()</code>,
<code><a href="#topic+form_mix">form_mix</a>()</code>,
<code><a href="#topic+form_regrid">form_regrid</a>()</code>,
<code><a href="#topic+form_resupport">form_resupport</a>()</code>,
<code><a href="#topic+form_retype">form_retype</a>()</code>,
<code><a href="#topic+form_smooth">form_smooth</a>()</code>,
<code><a href="#topic+form_tails">form_tails</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Default "random" transformation
d_norm &lt;- as_d(dnorm)
## More accurate result would give use of `+` directly with: d_norm + d_norm
d_norm_2 &lt;- form_trans(list(d_norm, d_norm), trans = `+`)
plot(d_norm_2)
lines(as_d(dnorm, sd = sqrt(2)), col = "red")

## Input list can have single numbers
form_trans(list(d_norm, 100), trans = `+`)

## Output of `trans` can be logical. Next example is random version of
## `d_norm &gt;= 0`.
form_trans(list(d_norm, 0), trans = `&gt;=`)

# Transformation with "bruteforce" method
power &lt;- function(x, n = 1) {
  x^n
}
p_dis &lt;- new_p(
  data.frame(x = 1:3, prob = c(0.1, 0.2, 0.7)),
  type = "discrete"
)

p_dis_sq &lt;- form_trans_self(
  p_dis, trans = power, n = 2, method = "bruteforce"
)
meta_x_tbl(p_dis_sq)
## Compare with "random" method
p_dis_sq_rand &lt;- form_trans_self(p_dis, trans = power, n = 2)
meta_x_tbl(p_dis_sq_rand)

# `form_trans_self()` is a wrapper for `form_trans()`
form_trans_self(d_norm, trans = function(x) {
  2 * x
})
</code></pre>

<hr>
<h2 id='meta'>Get metadata of pdqr-function</h2><span id='topic+meta'></span><span id='topic+meta_all'></span><span id='topic+meta_class'></span><span id='topic+meta_type'></span><span id='topic+meta_support'></span><span id='topic+meta_x_tbl'></span>

<h3>Description</h3>

<p>Tools for getting metadata of <strong>pdqr-function</strong>: a function which represents
distribution with finite support and finite values of probability/density.
The key metadata which defines underline distribution is <strong>&quot;x_tbl&quot;</strong>. If two
pdqr-functions have the same &quot;x_tbl&quot; metadata, they represent the same
distribution and can be converted to one another with <code style="white-space: pre;">&#8288;as_*()&#8288;</code> family of
functions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>meta_all(f)

meta_class(f)

meta_type(f)

meta_support(f)

meta_x_tbl(f)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="meta_+3A_f">f</code></td>
<td>
<p>A pdqr-function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Internally storage of metadata is implemented as follows:
</p>

<ul>
<li><p> Pdqr class is a first &quot;appropriate&quot; (&quot;p&quot;, &quot;d&quot;, &quot;q&quot;, or &quot;r&quot;) S3 class of
pdqr-function. All &quot;proper&quot; pdqr-functions have full S3 class of the form:
<code>c(cl, "pdqr", "function")</code>, where <code>cl</code> is pdqr class.
</p>
</li>
<li><p> Pdqr type, support, and &quot;x_tbl&quot; are stored into function's
<a href="base.html#topic+environment">environment</a>.
</p>
</li></ul>



<h3>Value</h3>

<p><code>meta_all()</code> returns a list of all metadata. <code>meta_class()</code>,
<code>meta_type()</code>, <code>meta_support</code>, and <code>meta_x_tbl()</code> return corresponding
metadata.
</p>


<h3>Pdqr class</h3>

<p>Pdqr class is returned by <code>meta_class()</code>. This can be one of &quot;p&quot;, &quot;d&quot;, &quot;q&quot;,
&quot;r&quot;. Represents <strong>how pdqr-function describes underlying distribution</strong>:
</p>

<ul>
<li><p> P-function (i.e. of class &quot;p&quot;) returns value of cumulative distribution
function (probability of random variable being not more than certain value)
at points <code>q</code> (its numeric vector input). Internally it is implemented as
direct integration of corresponding (with the same &quot;x_tbl&quot; metadata)
d-function.
</p>
</li>
<li><p> D-function returns value of probability mass or density function (depending
on pdqr type) at points <code>x</code> (its numeric vector input). Internally it is
implemented by directly using &quot;x_tbl&quot; metadata (see section '&quot;x_tbl&quot;
metadata' for more details).
</p>
</li>
<li><p> Q-function returns value of quantile function at points <code>p</code> (its numeric
vector input). Internally it is implemented as inverse of corresponding
p-function (returns the smallest &quot;x&quot; value which has cumulative probability
not less than input).
</p>
</li>
<li><p> R-function generates random sample of size <code>n</code> (its single number input)
from distribution. Internally it is implemented using inverse transform
sampling: certain amount of points from <a href="stats.html#topic+Uniform">standard uniform distribution</a> is generated, and the output is values of
corresponding q-function at generated points.
</p>
</li></ul>

<p>These names are chosen so as to follow <a href="stats.html#topic+Distributions">base R convention</a> of naming distribution functions. All
pdqr-functions take only one argument with the same meaning as the first ones
in base R. It has no other arguments specific to some parameters of
distribution family. To emulate their other common arguments, use the
following transformations (here <code>d_f</code> means a function of class &quot;d&quot;, etc.):
</p>

<ul>
<li><p> For <code>d_f(x, log = TRUE)</code> use <code>log(d_f(x))</code>.
</p>
</li>
<li><p> For <code>p_f(q, lower.tail = FALSE)</code> use <code>1 - p_f(q)</code>.
</p>
</li>
<li><p> For <code>p_f(q, log.p = TRUE)</code> use <code>log(p_f(q))</code>.
</p>
</li>
<li><p> For <code>q_f(p, lower.tail = FALSE)</code> use <code>q_f(1 - p)</code>.
</p>
</li>
<li><p> For <code>q_f(p, log.p = TRUE)</code> use <code>q_f(exp(p))</code>.
</p>
</li></ul>



<h3>Pdqr type</h3>

<p>Pdqr type is returned by <code>meta_type()</code>. This can be one of &quot;discrete&quot; or
&quot;continuous&quot;. Represents <strong>type of underlying distribution</strong>:
</p>

<ul>
<li><p> Type &quot;discrete&quot; is used for distributions with finite number of outcomes.
Functions with &quot;discrete&quot; type has a fixed set of &quot;x&quot; values (&quot;x&quot; column in
&quot;x_tbl&quot; metadata) on which d-function returns possibly non-zero output
(values from &quot;prob&quot; column in &quot;x_tbl&quot; metadata).
</p>
</li>
<li><p> Type &quot;continuous&quot; is used to represent continuous distributions with
piecewise-linear density with finite values and on finite support. Density
goes through points defined by &quot;x&quot; and &quot;y&quot; columns in &quot;x_tbl&quot; metadata.
</p>
</li></ul>



<h3>Pdqr support</h3>

<p>Pdqr support is returned by <code>meta_support()</code>. This is a numeric vector with
two finite values. Represents <strong>support of underlying distribution</strong>: closed
interval, outside of which d-function is equal to zero. <strong>Note</strong> that inside
of support d-function can also be zero, which especially true for &quot;discrete&quot;
functions.
</p>
<p>Technically, pdqr support is range of values from &quot;x&quot; column of &quot;x_tbl&quot;
metadata.
</p>


<h3>&quot;x_tbl&quot; metadata</h3>

<p>Metadata &quot;x_tbl&quot; is returned by <code>meta_x_tbl()</code>. This is a key metadata which
<strong>completely defines distribution</strong>. It is a data frame with three numeric
columns, content of which partially depends on pdqr type.
</p>
<p>Type &quot;discrete&quot; functions have &quot;x_tbl&quot; with columns &quot;x&quot;, &quot;prob&quot;, &quot;cumprob&quot;.
D-functions return a value from &quot;prob&quot; column for input which is very near
(should be equal up to ten digits, defined by <a href="base.html#topic+round">round(*, digits = 10)</a>) to corresponding value of &quot;x&quot; column. Rounding is done to
account for issues with representation of numerical values (see Note section
of <code><a href="base.html#topic++3D+3D">==</a></code>'s help page). For any other input, d-functions return
zero.
</p>
<p>Type &quot;continuous&quot; functions have &quot;x_tbl&quot; with columns &quot;x&quot;, &quot;y&quot;, &quot;cumprob&quot;.
D-functions return a value of piecewise-linear function passing through
points that have &quot;x&quot; and &quot;y&quot; coordinates. For any value outside support (i.e.
strictly less than minimum &quot;x&quot; and strictly more than maximum &quot;x&quot;) output is
zero.
</p>
<p>Column &quot;cumprob&quot; always represents the probability of underlying random
variable being not more than corresponding value in &quot;x&quot; column.
</p>


<h3>Change of metadata</h3>

<p>All metadata of pdqr-functions are not meant to be changed directly. Also
change of pdqr type, support, and &quot;x_tbl&quot; metadata will lead to a complete
change of underlying distribution.
</p>
<p>To change <strong>pdqr class</strong>, for example to convert p-function to d-function,
use <code style="white-space: pre;">&#8288;as_*()&#8288;</code> family of functions: <code><a href="#topic+as_p">as_p()</a></code>, <code><a href="#topic+as_d">as_d()</a></code>, <code><a href="#topic+as_q">as_q()</a></code>, <code><a href="#topic+as_r">as_r()</a></code>.
</p>
<p>To change <strong>pdqr type</strong>, use <code><a href="#topic+form_retype">form_retype()</a></code>. It changes underlying
distribution in the most suitable for user way.
</p>
<p>To change <strong>pdqr support</strong>, use <code><a href="#topic+form_resupport">form_resupport()</a></code> or <code><a href="#topic+form_tails">form_tails()</a></code>.
</p>
<p>Change of <strong>&quot;x_tbl&quot; metadata</strong> is not possible, because basically it means
creating completely new pdqr-function. To do that, supply data frame with
&quot;x_tbl&quot; format suitable for desired &quot;type&quot; to appropriate <code style="white-space: pre;">&#8288;new_*()&#8288;</code> function:
<code><a href="#topic+new_p">new_p()</a></code>, <code><a href="#topic+new_d">new_d()</a></code>, <code><a href="#topic+new_q">new_q()</a></code>, <code><a href="#topic+new_r">new_r()</a></code>. Also, there is a <code><a href="#topic+form_regrid">form_regrid()</a></code>
function which will increase or decrease granularity of pdqr-function.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>d_unif &lt;- as_d(dunif)

str(meta_all(d_unif))

meta_class(d_unif)
meta_type(d_unif)
meta_support(d_unif)
head(meta_x_tbl(d_unif))
</code></pre>

<hr>
<h2 id='methods-group-generic'>Pdqr methods for S3 group generic functions</h2><span id='topic+methods-group-generic'></span><span id='topic+Math.pdqr'></span><span id='topic+Ops.pdqr'></span><span id='topic+Summary.pdqr'></span>

<h3>Description</h3>

<p>There are custom methods implemented for three out of four <a href="base.html#topic+groupGeneric">S3 group generic functions</a>: <code>Math</code>, <code>Ops</code>, <code>Summary</code>. <strong>Note</strong> that many
of them have random nature with an idea of generating samples from input
pdqr-functions, performing certain operation on them (results in one
generated sample from desired random variable), and creating new
pdqr-function with appropriate <a href="#topic+new-pdqr">new_*()</a> function. This is done
with <code><a href="#topic+form_trans">form_trans()</a></code>, so all rules for determining <a href="#topic+meta_class">class</a> and
<a href="#topic+meta_type">type</a> of output is taken from it.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'pdqr'
Math(x, ...)

## S3 method for class 'pdqr'
Ops(e1, e2)

## S3 method for class 'pdqr'
Summary(..., na.rm = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="methods-group-generic_+3A_x">x</code>, <code id="methods-group-generic_+3A_e1">e1</code>, <code id="methods-group-generic_+3A_e2">e2</code></td>
<td>
<p>Objects.</p>
</td></tr>
<tr><td><code id="methods-group-generic_+3A_...">...</code></td>
<td>
<p>Further arguments passed to methods.</p>
</td></tr>
<tr><td><code id="methods-group-generic_+3A_na.rm">na.rm</code></td>
<td>
<p>Logical: should missing values be removed?</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Customization of method behavior may be done using mechanism of
<a href="base.html#topic+options">options()</a>. These are the possible options:
</p>

<ul>
<li> <p><strong><code>pdqr.group_gen.args_new</code></strong>. This will be used as <code>args_new</code> argument for
<code>form_trans()</code> in methods with random nature. Default is <code>list()</code>.
</p>
</li>
<li> <p><strong><code>pdqr.group_gen.n_sample</code></strong>. This will be used as <code>n_sample</code> argument for
<code>form_trans()</code> in methods with random nature. Default is 10000.
</p>
</li>
<li> <p><strong><code>pdqr.group_gen.repair_supp_method</code></strong>. All methods that have random
nature take care of output support by trying to &quot;repair&quot; it, because default
use of <code style="white-space: pre;">&#8288;new_*()&#8288;</code> functions returns a slightly bigger support than range of
input sample (see Examples). Repairing is done with <code><a href="#topic+form_resupport">form_resupport()</a></code> where
target support is computed separately and <code>method</code> argument is controlled by
this option (preferred ones are <code>"reflect"</code>, default, and <code>"trim"</code>). In most
cases output support is computed directly based on special features of
generic function. But for some difficult cases, like <code>gamma()</code>, <code>digamma()</code>,
<code>lgamma()</code>, <code>psigamma()</code>, <code>^</code>, and <code>%%</code> it is a result of simulation (i.e.
slightly random, which slightly increases random nature of those methods).
</p>
</li></ul>



<h3>Value</h3>

<p>All methods return pdqr-function which represents the result of
applying certain function to random variable(s) described with input
pdqr-function(s). <strong>Note</strong> that independence of input random variables is
assumed, i.e. <code>f + f</code> is not the same as <code>2*f</code> (see Examples).
</p>


<h3>Math</h3>

<p>This family of S3 generics represents mathematical functions. Most of the
methods have <strong>random nature</strong>, except <code>abs()</code> and <code>sign()</code> which are
computed directly. Output of <code>sign()</code> has &quot;discrete&quot; type with 3 &quot;x&quot; values:
-1, 0, 1.
</p>
<p><strong>Note</strong> that <code>cumsum()</code>, <code>cumprod()</code>, <code>cummmax()</code>, and <code>cummin()</code> functions
don't make much sense in these implementations: their outputs represent
random variable, sample of which is computed by applying <code style="white-space: pre;">&#8288;cum*()&#8288;</code> function to
a sample, generated from input pdqr-function.
</p>


<h3>Ops</h3>

<p>This family of S3 generics represents common operators. For all functions
(except <code>&amp;</code> and <code>|</code>) input can be a pdqr-function or single number.
</p>
<p>A list of methods with <strong>non-random nature</strong>:
</p>

<ul>
<li> <p><code>!</code>, <code>+</code>, <code>-</code> in case of single input, i.e. <code>!f</code> or <code>-f</code>.
</p>
</li>
<li><p> Functions representing linear transformation, i.e. adding, subtracting,
multiplying, and dividing by a single number. For example, all <code>f + 1</code>,
<code>2 - f</code> (which is actually <code>(-f) + 2</code>), <code>3*f</code> and <code>f/2</code> are linear
transformations, but <code>1 / f</code>, <code>f + g</code> are not.
</p>
</li>
<li><p> Functions for comparing: <code>==</code>, <code>!=</code>, <code>&lt;</code>, <code>&lt;=</code>, <code>&gt;=</code>, <code>&gt;</code>. Their output is
<strong>boolean pdqr-function</strong>: &quot;discrete&quot; type function with elements being
exactly 0 and 1. Probability of 0 represents probability of operator output
being false, and 1 - being true. Probability of being true is computed
directly as <strong>limit of empirical estimation from simulations</strong> (as size of
samples grows to infinity). In other words, output is an exact number which
might be approximated by simulating two big samples of same size from input
<code>e1</code> and <code>e2</code> (one of which can be a single number), and estimating
probability as share of those pairs from samples for which comparison is
true. <strong>Note</strong> that if at least one input has &quot;continuous&quot; type, then:
</p>

<ul>
<li> <p><code>==</code> will always have probability 0 of being true because probability
of generating a certain exact one or two numbers from continuous random
variable is zero.
</p>
</li>
<li> <p><code>!=</code> will always have probability 1 of being true for the same reason
as above.
</p>
</li>
<li><p> Pairs <code>&gt;=</code> and <code>&gt;</code>, <code>&lt;=</code> and <code>&lt;</code> will return the same input because
probability of being equal is always zero.
</p>
</li></ul>

</li>
<li><p> Logical functions <code>&amp;</code> and <code>|</code>. Their input can be only pdqr-functions
(because single number input doesn't make much sense). They are most useful
for applying to boolean pdqr-functions (see description of functions for
comparing), and warning is thrown in case any input is not a boolean
pdqr-function. <code>&amp;</code>'s probability of being true is a product of those
probabilities from input <code>e1</code> and <code>e2</code>. <code>|</code>'s probability of being false is a
product of those probabilities from input <code>e1</code> and <code>e2</code>. <strong>Note</strong> that
probability of being false is a probability of being equal to 0; of being
true - complementary to that.
</p>
</li></ul>

<p>All other methods are <strong>random</strong>. For example, <code>f + f</code>, <code>f^g</code> are random.
</p>


<h3>Summary</h3>

<p>Methods for <code>all()</code> and <code>any()</code> have <strong>non-random nature</strong>. Their input can
be only pdqr-functions, and if any of them is not boolean, a warning is
thrown (because otherwise output doesn't make much sense). They return a
boolean pdqr-function with the following probability of being true:
</p>

<ul>
<li><p> In <code>all()</code> - probability of <em>all</em> input function being true, i.e. product
of probabilities of being true (implemented as complementary to probability
of being equal to 0).
</p>
</li>
<li><p> In <code>any()</code> - probability of <em>any</em> input function being true, i.e.
complementary probability to product of all functions being false
(implemented as probability of being equal to 0).
</p>
</li></ul>

<p>Methods for <code>sum()</code>, <code>prod()</code>, <code>min()</code>, <code>max()</code> have <strong>random nature</strong>. They
are implemented to use vectorized version of certain generic, because
transformation function for <code>form_trans()</code> should be vectorized: for input
samples which all have size n it should also return sample of size n (where
each element is a transformation output for corresponding elements from input
samples). This way <code>min(f, g)</code> can be read as &quot;random variable
representing minimum of <code>f</code> and <code>g</code>&quot;, etc.
</p>
<p><strong>Notes</strong>:
</p>

<ul>
<li> <p><code>range()</code> function doesn't make sense here because it returns 2 numbers per
input and therefore can't be made vectorized. Error is thrown if it is
applied to pdqr-function.
</p>
</li>
<li><p> Although all <code>sum()</code>, <code>prod()</code>, <code>min()</code>, <code>max()</code> accept pdqr-functions or
single numbers, using numbers and &quot;continuous&quot; functions simultaneously is
not a great idea. This is because output will be automatically smoothed (as
<code>form_trans()</code> will use some <code style="white-space: pre;">&#8288;new_*()&#8288;</code> function) which will give a misleading
picture. For a more realistic output:
</p>

<ul>
<li><p> Instead of <code>min(f, num)</code> use
<code>form_resupport(f, c(num, NA), method = "winsor")</code> (see
<code><a href="#topic+form_resupport">form_resupport()</a></code>).
</p>
</li>
<li><p> Instead of <code>max(f, num)</code> use
<code>form_resupport(f, c(NA, num), method = "winsor")</code>.
</p>
</li>
<li><p> Instead of <code>sum(f, num)</code> use <code>f + num</code>.
</p>
</li>
<li><p> Instead of <code>prod(f, num)</code> use <code>f * num</code>.
</p>
</li></ul>

</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+summ_prob_true">summ_prob_true()</a></code> and <code><a href="#topic+summ_prob_false">summ_prob_false()</a></code> for extracting
probability from boolean pdqr-functions.
</p>
<p>Other pdqr methods for generic functions: 
<code><a href="#topic+methods-plot">methods-plot</a></code>,
<code><a href="#topic+methods-print">methods-print</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>d_norm &lt;- as_d(dnorm)
d_unif &lt;- as_d(dunif)
d_dis &lt;- new_d(data.frame(x = 1:4, prob = 1:4 / 10), "discrete")

set.seed(101)

# Math
plot(d_norm, main = "Math methods")
## `abs()` and `sign()` are not random
lines(abs(d_norm), col = "red")
## All others are random
lines(cos(d_norm), col = "green")
lines(cos(d_norm), col = "blue")

## Although here distribution shouldn't change, it changes slightly due to
## random implementation
meta_x_tbl(d_dis)
meta_x_tbl(floor(d_dis))

# Ops
## Single input, linear transformations, and logical are not random
d_dis &gt; 1
!(d_dis &gt; 1)
d_norm &gt;= (2 * d_norm + 1)
## All others are random
plot(d_norm + d_norm)
## This is an exact reference curve
lines(as_d(dnorm, sd = sqrt(2)), col = "red")

plot(d_dis + d_norm)

plot(d_unif^d_unif)

# Summary
## `all()` and `any()` are non-random
all(d_dis &gt; 1, d_dis &gt; 1)
## Others are random
plot(max(d_norm, d_norm, d_norm))

plot(d_norm + d_norm + d_norm)
lines(sum(d_norm, d_norm, d_norm), col = "red")

## Using single numbers is allowed, but gives misleading output in case of
## "continuous" functions. Use other functions instead (see documentation).
plot(min(d_unif, 0.5))
lines(form_resupport(d_unif, c(NA, 0.5), method = "winsor"), col = "blue")

# Use `options()` to control methods
plot(d_unif + d_unif)
op &lt;- options(
  pdqr.group_gen.n_sample = 100,
  pdqr.group_gen.args_new = list(adjust = 0.5)
)
lines(d_unif + d_unif, col = "red")
## `f + f` is different from `2*f` due to independency assumption. Also the
## latter implemented non-randomly.
lines(2 * d_unif, col = "blue")

# Methods for generics attempt to repair support, so they are more reasonable
# to use than direct use of `form_trans()`
d_unif + d_unif
form_trans(list(d_unif, d_unif), `+`)
</code></pre>

<hr>
<h2 id='methods-plot'>Pdqr methods for base plotting functions</h2><span id='topic+methods-plot'></span><span id='topic+plot.p'></span><span id='topic+plot.d'></span><span id='topic+plot.q'></span><span id='topic+plot.r'></span><span id='topic+lines.p'></span><span id='topic+lines.d'></span><span id='topic+lines.q'></span>

<h3>Description</h3>

<p>Pdqr-functions have their own methods for <code><a href="base.html#topic+plot">plot()</a></code> and <code><a href="graphics.html#topic+lines">lines()</a></code> (except
r-functions, see Details).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'p'
plot(x, y = NULL, n_extra_grid = 1001, ...)

## S3 method for class 'd'
plot(x, y = NULL, n_extra_grid = 1001, ...)

## S3 method for class 'q'
plot(x, y = NULL, n_extra_grid = 1001, ...)

## S3 method for class 'r'
plot(x, y = NULL, n_sample = 1000, ...)

## S3 method for class 'p'
lines(x, n_extra_grid = 1001, ...)

## S3 method for class 'd'
lines(x, n_extra_grid = 1001, ...)

## S3 method for class 'q'
lines(x, n_extra_grid = 1001, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="methods-plot_+3A_x">x</code></td>
<td>
<p>Pdqr-function to plot.</p>
</td></tr>
<tr><td><code id="methods-plot_+3A_y">y</code></td>
<td>
<p>Argument for compatibility with <code>plot()</code> signature. Doesn't used.</p>
</td></tr>
<tr><td><code id="methods-plot_+3A_n_extra_grid">n_extra_grid</code></td>
<td>
<p>Number of extra grid points at which to evaluate
pdqr-function (see Details). Supply <code>NULL</code> or <code>0</code> to not use extra grid.</p>
</td></tr>
<tr><td><code id="methods-plot_+3A_...">...</code></td>
<td>
<p>Other arguments for <code>plot()</code> or <a href="graphics.html#topic+hist">hist()</a> (in
case of plotting r-function).</p>
</td></tr>
<tr><td><code id="methods-plot_+3A_n_sample">n_sample</code></td>
<td>
<p>Size of a sample to be generated for plotting histogram in
case of an r-function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Main idea of plotting pdqr-functions is to use plotting mechanisms
for appropriate numerical data.
</p>
<p>Plotting of <a href="#topic+meta_type">type</a> <strong>discrete</strong> functions:
</p>

<ul>
<li><p> P-functions are plotted as step-line with jumps at points of &quot;x&quot; column of
<a href="#topic+meta_x_tbl">&quot;x_tbl&quot; metadata</a>.
</p>
</li>
<li><p> D-functions are plotted with vertical lines at points of &quot;x&quot; column of
&quot;x_tbl&quot; with height equal to values from &quot;prob&quot; column.
</p>
</li>
<li><p> Q-functions are plotted as step-line with jumps at points of &quot;cumprob&quot;
column of &quot;x_tbl&quot;.
</p>
</li>
<li><p> R-functions are plotted by generating sample of size <code>n_sample</code> and calling
<a href="graphics.html#topic+hist">hist()</a> function.
</p>
</li></ul>

<p>Plotting of type <strong>continuous</strong> functions:
</p>

<ul>
<li><p> P-functions are plotted in piecewise-linear fashion at their values on
compound grid: sorted union of &quot;x&quot; column from &quot;x_tbl&quot; metadata and sequence
of length <code>n_extra_grid</code> consisting from equidistant points between edges of
support. Here extra grid is needed to show curvature of lines between &quot;x&quot;
points from &quot;x_tbl&quot; (see Examples).
</p>
</li>
<li><p> D-functions are plotted in the same way as p-functions.
</p>
</li>
<li><p> Q-functions are plotted similarly as p- and d-functions but grid consists
from union of &quot;cumprob&quot; column of &quot;x_tbl&quot; metadata and equidistant grid of
length <code>n_extra_grid</code> from 0 to 1.
</p>
</li>
<li><p> R-functions are plotted the same way as type &quot;discrete&quot; ones: as histogram
of generated sample of size <code>n_sample</code>.
</p>
</li></ul>



<h3>Value</h3>

<p>Output of <a href="base.html#topic+invisible">invisible()</a> without arguments, i.e.
<code>NULL</code> without printing.
</p>


<h3>See Also</h3>

<p>Other pdqr methods for generic functions: 
<code><a href="#topic+methods-group-generic">methods-group-generic</a></code>,
<code><a href="#topic+methods-print">methods-print</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>d_norm_1 &lt;- as_d(dnorm)
d_norm_2 &lt;- as_d(dnorm, mean = 1)

plot(d_norm_1)
lines(d_norm_2, col = "red")

# Usage of `n_extra_grid` is important in case of "continuous" p- and
# q-functions
simple_p &lt;- new_p(data.frame(x = c(0, 1), y = c(0, 1)), "continuous")
plot(simple_p, main = "Case study of n_extra_grid argument")
lines(simple_p, n_extra_grid = 0, col = "red")

# R-functions are plotted with histogram
plot(as_r(d_norm_1))
</code></pre>

<hr>
<h2 id='methods-print'>Pdqr methods for print function</h2><span id='topic+methods-print'></span><span id='topic+print.p'></span><span id='topic+print.d'></span><span id='topic+print.q'></span><span id='topic+print.r'></span>

<h3>Description</h3>

<p>Pdqr-functions have their own methods for <code><a href="base.html#topic+print">print()</a></code> which displays function's
<a href="#topic+meta_all">metadata</a> in readable and concise form.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'p'
print(x, ...)

## S3 method for class 'd'
print(x, ...)

## S3 method for class 'q'
print(x, ...)

## S3 method for class 'r'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="methods-print_+3A_x">x</code></td>
<td>
<p>Pdqr-function to print.</p>
</td></tr>
<tr><td><code id="methods-print_+3A_...">...</code></td>
<td>
<p>Further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Print output of pdqr-function describes the following information:
</p>

<ul>
<li><p> Full name of function <a href="#topic+meta_class">class</a>:
</p>

<ul>
<li><p> P-function is &quot;Cumulative distribution function&quot;.
</p>
</li>
<li><p> D-function is &quot;Probability mass function&quot; for &quot;discrete&quot; type and
&quot;Probability density function&quot; for &quot;continuous&quot;.
</p>
</li>
<li><p> Q-function is &quot;Quantile function&quot;.
</p>
</li>
<li><p> R-function is &quot;Random generation function&quot;.
</p>
</li></ul>

</li>
<li> <p><a href="#topic+meta_type">Type</a> of function in the form &quot;of * type&quot; where &quot;*&quot; is
&quot;discrete&quot; or &quot;continuous&quot; depending on actual type.
</p>
</li>
<li> <p><a href="#topic+meta_support">Support</a> of function.
</p>
</li>
<li><p> Number of elements in distribution for &quot;discrete&quot; type or number of
intervals of piecewise-linear density for &quot;continuous&quot; type.
</p>
</li>
<li><p> If pdqr-function has &quot;discrete&quot; type and exactly two possible values 0 and
1, it is treated as &quot;boolean&quot; pdqr-function and probability of 1 is shown.
This is done to simplify interactive work with output of comparing functions
like <code>&gt;=</code>, etc. (see <a href="#topic+methods-group-generic">description of methods for S3 group generic functions</a>). To extract probabilities from &quot;boolean&quot;
pdqr-function, use <code><a href="#topic+summ_prob_true">summ_prob_true()</a></code> and <code><a href="#topic+summ_prob_false">summ_prob_false()</a></code>.
</p>
</li></ul>

<p>Symbol &quot;~&quot; in <code>print()</code> output indicates that printed value or support is an
approximation to a true one (for readability purpose).
</p>


<h3>See Also</h3>

<p>Other pdqr methods for generic functions: 
<code><a href="#topic+methods-group-generic">methods-group-generic</a></code>,
<code><a href="#topic+methods-plot">methods-plot</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>print(new_d(1:10, "discrete"))

r_unif &lt;- as_r(runif, n_grid = 251)
print(r_unif)

# Printing of boolean pdqr-function
print(r_unif &gt;= 0.3)
</code></pre>

<hr>
<h2 id='new_p'>Create new pdqr-function</h2><span id='topic+new_p'></span><span id='topic+new_d'></span><span id='topic+new_q'></span><span id='topic+new_r'></span><span id='topic+new-pdqr'></span>

<h3>Description</h3>

<p>Functions for creating new pdqr-functions based on numeric sample or data
frame describing distribution. They construct appropriate <a href="#topic+meta_x_tbl">&quot;x_tbl&quot; metadata</a> based on the input and then create pdqr-function (of
corresponding <a href="#topic+meta_class">pdqr class</a>) defined by that &quot;x_tbl&quot;.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>new_p(x, type, ...)

new_d(x, type, ...)

new_q(x, type, ...)

new_r(x, type, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="new_p_+3A_x">x</code></td>
<td>
<p>Numeric vector or data frame with appropriate columns (see &quot;Data
frame input&quot; section).</p>
</td></tr>
<tr><td><code id="new_p_+3A_type">type</code></td>
<td>
<p><a href="#topic+meta_type">Type</a> of pdqr-function. Should be one of &quot;discrete&quot;
or &quot;continuous&quot;.</p>
</td></tr>
<tr><td><code id="new_p_+3A_...">...</code></td>
<td>
<p>Extra arguments for <a href="stats.html#topic+density">density()</a>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Data frame input <code>x</code> is treated as having enough information for
creating (including normalization of &quot;y&quot; column) an &quot;x_tbl&quot; metadata. For
more details see &quot;Data frame input&quot; section.
</p>
<p>Numeric input is transformed into data frame which is then used as &quot;x_tbl&quot;
metadata (for more details see &quot;Numeric input&quot; section):
</p>

<ul>
<li><p> If <code>type</code> is <code>"discrete"</code> then <code>x</code> is viewed as sample from distribution
that can produce only values from <code>x</code>. Input is tabulated and normalized to
form &quot;x_tbl&quot; metadata.
</p>
</li>
<li><p> If <code>type</code> is <code>"continuous"</code> then:
</p>

<ul>
<li><p> If <code>x</code> has 1 element, output distribution represents a <strong>dirac-like</strong>
distribution which is an approximation to singular dirac distribution.
</p>
</li>
<li><p> If <code>x</code> has more than 1 element, output distribution represents a
<strong>density estimation</strong> with <a href="stats.html#topic+density">density()</a> treating <code>x</code> as
sample.
</p>
</li></ul>

</li></ul>



<h3>Value</h3>

<p>A pdqr-function of corresponding <a href="#topic+meta_class">class</a> (&quot;p&quot; for
<code>new_p()</code>, etc.) and <a href="#topic+meta_type">type</a>.
</p>


<h3>Numeric input</h3>

<p>If <code>x</code> is a numeric vector, it is transformed into a data frame which is then
used as <a href="#topic+meta_x_tbl">&quot;x_tbl&quot; metadata</a> to create pdqr-function of
corresponding class.
</p>
<p>First, all <code>NaN</code>, <code>NA</code>, and infinite values are removed with warnings. If
there are no elements left, error is thrown. Then data frame is created in
the way which depends on the <code>type</code> argument.
</p>
<p><strong>For &quot;discrete&quot; type</strong> elements of filtered <code>x</code> are:
</p>

<ul>
<li><p> Rounded to 10th digit to avoid numerical representation issues (see Note
in <code><a href="base.html#topic++3D+3D">==</a></code>'s help page).
</p>
</li>
<li><p> Tabulated (all unique values are counted). Output data frame has three
columns: &quot;x&quot; with unique values, &quot;prob&quot; with normalized (divided by sum)
counts, &quot;cumprob&quot; with cumulative sum of &quot;prob&quot; column.
</p>
</li></ul>

<p><strong>For &quot;continuous&quot; type</strong> output data frame has columns &quot;x&quot;, &quot;y&quot;, &quot;cumprob&quot;.
Choice of algorithm depends on the number of <code>x</code> elements:
</p>

<ul>
<li><p> If <code>x</code> has 1 element, an &quot;x_tbl&quot; metadata describes <strong>dirac-like</strong>
&quot;continuous&quot; pdqr-function. It is implemented as triangular peak with center
at <code>x</code>'s value and width of <code>2e-8</code> (see Examples). This is an approximation
of singular dirac distribution. Data frame has columns &quot;x&quot; with value
<code>c(x-1e-8, x, x+1e-8)</code>, &quot;y&quot; with value <code>c(0, 1e8, 0)</code> normalized to have
total integral of &quot;x&quot;-&quot;y&quot; points of 1, &quot;cumprob&quot; <code>c(0, 0.5, 1)</code>.
</p>
</li>
<li><p> If <code>x</code> has more than 1 element, it serves as input to
<a href="stats.html#topic+density">density(x, ...)</a> for density estimation (here arguments in
<code>...</code> of <code style="white-space: pre;">&#8288;new_*()&#8288;</code> serve as extra arguments to <code>density()</code>). The output's &quot;x&quot;
element is used as &quot;x&quot; column in output data frame. Column &quot;y&quot; is taken as
&quot;y&quot; element of <code>density()</code> output, normalized so that piecewise-linear
function passing through &quot;x&quot;-&quot;y&quot; points has total integral of 1. Column
&quot;cumprob&quot; has cumulative probability of piecewise-linear d-function.
</p>
</li></ul>



<h3>Data frame input</h3>

<p>If <code>x</code> is a data frame, it should have numeric columns appropriate for
<a href="#topic+meta_x_tbl">&quot;x_tbl&quot; metadata</a> of input <code>type</code>: &quot;x&quot;, &quot;prob&quot; for &quot;discrete&quot;
<code>type</code> and &quot;x&quot;, &quot;y&quot; for &quot;continuous&quot; type (&quot;cumprob&quot; column will be computed
inside <code style="white-space: pre;">&#8288;new_*()&#8288;</code>). To become an appropriate &quot;x_tbl&quot; metadata, input data
frame is ordered in increasing order of &quot;x&quot; column and then <strong>imputed</strong> in
the way which depends on the <code>type</code> argument.
</p>
<p><strong>For &quot;discrete&quot; type</strong>:
</p>

<ul>
<li><p> Values in column &quot;x&quot; are rounded to 10th digit to avoid numerical
representation issues (see Note in <code><a href="base.html#topic++3D+3D">==</a></code>'s help page).
</p>
</li>
<li><p> If there are duplicate values in &quot;x&quot; column, they are &quot;squashed&quot; into one
having sum of their probability in &quot;prob&quot; column.
</p>
</li>
<li><p> Column &quot;prob&quot; is normalized by its sum to have total sum of 1.
</p>
</li>
<li><p> Column &quot;cumprob&quot; is computed as cumulative sum of &quot;prob&quot; column.
</p>
</li></ul>

<p><strong>For &quot;continuous&quot; type</strong> column &quot;y&quot; is normalized so that piecewise-linear
function passing through &quot;x&quot;-&quot;y&quot; points has total integral of 1. Column
&quot;cumprob&quot; has cumulative probability of piecewise-linear d-function.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(101)
x &lt;- rnorm(10)

# Type "discrete": `x` values are directly tabulated
my_d_dis &lt;- new_d(x, "discrete")
meta_x_tbl(my_d_dis)
plot(my_d_dis)

# Type "continuous": `x` serves as input to `density()`
my_d_con &lt;- new_d(x, "continuous")
head(meta_x_tbl(my_d_con))
plot(my_d_con)

# Data frame input
## Values in "prob" column will be normalized automatically
my_p_dis &lt;- new_p(data.frame(x = 1:4, prob = 1:4), "discrete")
## As are values in "y" column
my_p_con &lt;- new_p(data.frame(x = 1:3, y = c(0, 10, 0)), "continuous")

# Using bigger bandwidth in `density()`
my_d_con_2 &lt;- new_d(x, "continuous", adjust = 2)
plot(my_d_con, main = "Comparison of density bandwidths")
lines(my_d_con_2, col = "red")

# Dirac-like "continuous" pdqr-function is created if `x` is a single number
meta_x_tbl(new_d(1, "continuous"))
</code></pre>

<hr>
<h2 id='pdqr_approx_error'>Diagnose pdqr approximation</h2><span id='topic+pdqr_approx_error'></span>

<h3>Description</h3>

<p><code>pdqr_approx_error()</code> computes errors that are results of 'pdqr'
approximation, which occurs because of possible tail trimming and assuming
piecewise linearity of density function in case of &quot;continuous&quot; type. For an
easy view summary, use <a href="base.html#topic+summary">summary()</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pdqr_approx_error(f, ref_f, ..., gran = 10, remove_infinity = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pdqr_approx_error_+3A_f">f</code></td>
<td>
<p>A p-, d-, or q-function to diagnose. Usually the output of one of
<code><a href="#topic+as_p">as_p()</a></code>, <code><a href="#topic+as_d">as_d()</a></code>, or <code><a href="#topic+as_q">as_q()</a></code> default methods.</p>
</td></tr>
<tr><td><code id="pdqr_approx_error_+3A_ref_f">ref_f</code></td>
<td>
<p>A &quot;true&quot; distribution function of the same <a href="#topic+meta_class">class</a>
as <code>f</code>. Usually the input to the aforementioned <code style="white-space: pre;">&#8288;as_*()&#8288;</code> function.</p>
</td></tr>
<tr><td><code id="pdqr_approx_error_+3A_...">...</code></td>
<td>
<p>Other arguments to <code>ref_f</code>. If they were supplied to <code style="white-space: pre;">&#8288;as_*()&#8288;</code>
function, then the exact same values must be supplied here.</p>
</td></tr>
<tr><td><code id="pdqr_approx_error_+3A_gran">gran</code></td>
<td>
<p>Degree of grid &quot;granularity&quot; in case of &quot;continuous&quot; type: number
of subintervals to be produced inside every interval of density linearity.
Should be not less than 1 (indicator that original column from
<a href="#topic+meta_x_tbl">&quot;x_tbl&quot;</a> will be used, see details).</p>
</td></tr>
<tr><td><code id="pdqr_approx_error_+3A_remove_infinity">remove_infinity</code></td>
<td>
<p>Whether to remove rows corresponding to infinite
error.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Errors are computed as difference between &quot;true&quot; value (output of
<code>ref_f</code>) and output of pdqr-function <code>f</code>. They are computed at &quot;granulated&quot;
<code>gran</code> times grid (which is an &quot;x&quot; column of &quot;x_tbl&quot; in case <code>f</code> is p- or
d-function and &quot;cumprob&quot; column if q-function). They are usually negative
because of possible tail trimming of reference distribution.
</p>
<p><strong>Notes</strong>:
</p>

<ul>
<li> <p><code>gran</code> argument for &quot;discrete&quot; type is always 1.
</p>
</li>
<li><p> Quantile pdqr approximation of &quot;discrete&quot; distribution with infinite
tale(s) can result into &quot;all one&quot; summary of error. This is expected output
and is because test grid is chosen to be quantiles of pdqr-distribution which
due to renormalization can differ by one from reference ones. For example:
<code>summary(pdqr_approx_error(as_p(ppois, lambda = 10), ppois, lambda = 10))</code>.
</p>
</li></ul>



<h3>Value</h3>

<p>A data frame with the following columns:
</p>

<ul>
<li> <p><strong>grid</strong> <code style="white-space: pre;">&#8288;&lt;dbl&gt;&#8288;</code> : A grid at which errors are computed.
</p>
</li>
<li> <p><strong>error</strong> <code style="white-space: pre;">&#8288;&lt;dbl&gt;&#8288;</code> : Errors which are computed as <code>ref_f(grid, ...) - f(grid)</code>.
</p>
</li>
<li> <p><strong>abserror</strong> <code style="white-space: pre;">&#8288;&lt;dbl&gt;&#8288;</code> : Absolute value of &quot;error&quot; column.
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+enpoint">enpoint()</a></code> for representing pdqr-function as a set of points with
desirable number of rows.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>d_norm &lt;- as_d(dnorm)
error_norm &lt;- pdqr_approx_error(d_norm, dnorm)
summary(error_norm)

# Setting `gran` results into different number of rows in output
error_norm_2 &lt;- pdqr_approx_error(d_norm, dnorm, gran = 1)
nrow(meta_x_tbl(d_norm)) == nrow(error_norm_2)

# By default infinity errors are removed
d_beta &lt;- as_d(dbeta, shape1 = 0.3, shape2 = 0.7)
error_beta_1 &lt;- pdqr_approx_error(d_beta, dbeta, shape1 = 0.3, shape2 = 0.7)
summary(error_beta_1)

# To not remove them, set `remove_infinity` to `FALSE`
error_beta_2 &lt;- pdqr_approx_error(
  d_beta, dbeta, shape1 = 0.3, shape2 = 0.7, remove_infinity = FALSE
)
summary(error_beta_2)
</code></pre>

<hr>
<h2 id='region'>Work with regions</h2><span id='topic+region'></span><span id='topic+region_is_in'></span><span id='topic+region_prob'></span><span id='topic+region_height'></span><span id='topic+region_width'></span><span id='topic+region_distance'></span><span id='topic+region_draw'></span>

<h3>Description</h3>

<p>These functions provide ways of working with a <strong>region</strong>: a data frame with
numeric &quot;left&quot; and &quot;right&quot; columns, each row of which represents a unique
finite interval (open, either type of half-open, or closed). Values of &quot;left&quot;
and &quot;right&quot; columns should create an &quot;ordered&quot; set of intervals:
<code style="white-space: pre;">&#8288;left[1] &lt;= right[1] &lt;= left[2] &lt;= right[2] &lt;= ...&#8288;</code> (intervals with zero
width are accepted). Originally, <code style="white-space: pre;">&#8288;region_*()&#8288;</code> functions were designed to work
with output of <code><a href="#topic+summ_hdr">summ_hdr()</a></code> and <code><a href="#topic+summ_interval">summ_interval()</a></code>, but can be used for any
data frame which satisfies the definition of a region.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>region_is_in(region, x, left_closed = TRUE, right_closed = TRUE)

region_prob(region, f, left_closed = TRUE, right_closed = TRUE)

region_height(region, f, left_closed = TRUE, right_closed = TRUE)

region_width(region)

region_distance(region, region2, method = "Jaccard")

region_draw(region, col = "blue", alpha = 0.2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="region_+3A_region">region</code></td>
<td>
<p>A data frame representing region.</p>
</td></tr>
<tr><td><code id="region_+3A_x">x</code></td>
<td>
<p>Numeric vector to be tested for being inside region.</p>
</td></tr>
<tr><td><code id="region_+3A_left_closed">left_closed</code></td>
<td>
<p>A single logical value representing whether to treat left
ends of intervals as their parts.</p>
</td></tr>
<tr><td><code id="region_+3A_right_closed">right_closed</code></td>
<td>
<p>A single logical value representing whether to treat
right ends of intervals as their parts.</p>
</td></tr>
<tr><td><code id="region_+3A_f">f</code></td>
<td>
<p>A pdqr-function.</p>
</td></tr>
<tr><td><code id="region_+3A_region2">region2</code></td>
<td>
<p>A data frame representing region.</p>
</td></tr>
<tr><td><code id="region_+3A_method">method</code></td>
<td>
<p>Method for computing distance between regions in
<code>region_distance()</code>. Should be one of &quot;Jaccard&quot; or methods of
<code><a href="#topic+summ_distance">summ_distance()</a></code>.</p>
</td></tr>
<tr><td><code id="region_+3A_col">col</code></td>
<td>
<p>Single color of rectangles to be used. Should be appropriate for
<code>col</code> argument of <a href="grDevices.html#topic+col2rgb">col2rgb()</a>.</p>
</td></tr>
<tr><td><code id="region_+3A_alpha">alpha</code></td>
<td>
<p>Single number representing factor modifying the opacity alpha;
typically in [0; 1].</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>region_is_in()</code> tests each value of <code>x</code> for being inside interval.
In other words, if there is a row for which element of <code>x</code> is between &quot;left&quot;
and &quot;right&quot; value (respecting <code>left_closed</code> and <code>right_closed</code> options),
output for that element will be <code>TRUE</code>. <strong>Note</strong> that for zero-width
intervals one of <code>left_closed</code> or <code>right_closed</code> being <code>TRUE</code> is enough to
accept that point as &quot;in region&quot;.
</p>
<p><code>region_prob()</code> computes total probability of region according to
pdqr-function <code>f</code>. If <code>f</code> has &quot;discrete&quot; <a href="#topic+meta_type">type</a>, output is
computed as sum of probabilities for all &quot;x&quot; values from <a href="#topic+meta_x_tbl">&quot;x_tbl&quot; metadata</a> which lie inside a region (respecting <code>left_closed</code>
and <code>right_closed</code> options while using <code>region_is_in()</code>). If <code>f</code> has
&quot;continuous&quot; type, output is computed as integral of density over a region
(<code style="white-space: pre;">&#8288;*_closed&#8288;</code> options having any effect).
</p>
<p><code>region_height()</code> computes &quot;height&quot; of a region (with respect to <code>f</code>):
minimum value of corresponding to <code>f</code> d-function can return based on relevant
points inside a region. If <code>f</code> has &quot;discrete&quot; type, those relevant points are
computed as &quot;x&quot; values from &quot;x_tbl&quot; metadata which lie inside a region (if
there are no such points, output is 0). If <code>f</code> has &quot;continuous&quot; type, the
whole intervals are used as relevant points. The notion of &quot;height&quot; comes
from <code><a href="#topic+summ_hdr">summ_hdr()</a></code> function: if <code>region</code> is <code>summ_hdr(f, level)</code> for some
<code>level</code>, then <code>region_height(region, f)</code> is what is called in <code>summ_hdr()</code>'s
docs as &quot;target height&quot; of HDR. That is, a maximum value of d-function for
which a set consisting from points at which d-function has values not less
than target height and total probability of the set being not less than
<code>level</code>.
</p>
<p><code>region_width()</code> computes total width of a region, i.e. sum of differences
between &quot;right&quot; and &quot;left&quot; columns.
</p>
<p><code>region_distance()</code> computes distance between a pair of regions. As in
<code><a href="#topic+summ_distance">summ_distance()</a></code>, it is a single non-negative number representing how much
two regions differ from one another (bigger values indicate bigger
difference). Argument <code>method</code> represents method of computing distance.
Method &quot;Jaccard&quot; computes Jaccard distance: one minus ratio of intersection
width and union width. Other methods come from <code>summ_distance()</code> and
represent distance between regions as probability distributions:
</p>

<ul>
<li><p> If total width of region is zero (i.e. it consists only from points),
distribution is a uniform discrete one based on points from region.
</p>
</li>
<li><p> If total width is positive, then distribution is a uniform continuous one
based on intervals with positive width.
</p>
</li></ul>

<p><code>region_draw()</code> draws (on current plot) intervals stored in <code>region</code> as
colored rectangles vertically starting from zero and ending in the top of the
plot (technically, at &quot;y&quot; value of <code>2e8</code>).
</p>


<h3>Value</h3>

<p><code>region_is_in()</code> returns a logical vector (with length equal to
length of <code>x</code>) representing whether certain element of <code>x</code> is inside a
region.
</p>
<p><code>region_prob()</code> returns a single number between 0 and 1 representing total
probability of region.
</p>
<p><code>region_height()</code> returns a single number representing a height of a region
with respect to <code>f</code>, i.e. minimum value that corresponding d-function can
return based on relevant points inside a region.
</p>
<p><code>region_width()</code> returns a single number representing total width of a
region.
</p>
<p><code>region_draw()</code> draws colored rectangles filling <code>region</code> intervals.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+summ_hdr">summ_hdr()</a></code> for computing of Highest Density Region.
</p>
<p><code><a href="#topic+summ_interval">summ_interval()</a></code> for computing of single interval summary of distribution.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Type "discrete"
d_binom &lt;- as_d(dbinom, size = 10, prob = 0.7)
hdr_dis &lt;- summ_hdr(d_binom, level = 0.6)
region_is_in(hdr_dis, 0:10)
## This should be not less than 0.6
region_prob(hdr_dis, d_binom)
region_height(hdr_dis, d_binom)
region_width(hdr_dis)

# Type "continuous"
d_norm &lt;- as_d(dnorm)
hdr_con &lt;- summ_hdr(d_norm, level = 0.95)
region_is_in(hdr_con, c(-Inf, -2, 0, 2, Inf))
## This should be approximately equal to 0.95
region_prob(hdr_con, d_norm)
## This should be equal to `d_norm(hdr_con[["left"]][1])`
region_height(hdr_con, d_norm)
region_width(hdr_con)

# Usage of `*_closed` options
region &lt;- data.frame(left = 1, right = 3)
## Closed intervals
region_is_in(region, 1:3)
## Open from left, closed from right
region_is_in(region, 1:3, left_closed = FALSE)
## Closed from left, open from right
region_is_in(region, 1:3, right_closed = FALSE)
## Open intervals
region_is_in(region, 1:3, left_closed = FALSE, right_closed = FALSE)

# Handling of intervals with zero width
region &lt;- data.frame(left = 1, right = 1)
## If at least one of `*_closed` options is `TRUE`, 1 will be considered as
## "in a region"
region_is_in(region, 1)
region_is_in(region, 1, left_closed = FALSE)
region_is_in(region, 1, right_closed = FALSE)
## Only this will return `FALSE`
region_is_in(region, 1, left_closed = FALSE, right_closed = FALSE)

# Distance between regions
region1 &lt;- data.frame(left = c(0, 2), right = c(1, 2))
region2 &lt;- data.frame(left = 0.5, right = 1.5)
region_distance(region1, region2, method = "Jaccard")
region_distance(region1, region2, method = "KS")

# Drawing
d_mix &lt;- form_mix(list(as_d(dnorm), as_d(dnorm, mean = 5)))
plot(d_mix)
region_draw(summ_hdr(d_mix, 0.95))
</code></pre>

<hr>
<h2 id='summ_center'>Summarize distribution with center</h2><span id='topic+summ_center'></span><span id='topic+summ_mean'></span><span id='topic+summ_median'></span><span id='topic+summ_mode'></span><span id='topic+summ_midrange'></span>

<h3>Description</h3>

<p>Functions to compute center of distribution. <code>summ_center()</code> is a wrapper for
respective <code style="white-space: pre;">&#8288;summ_*()&#8288;</code> functions (from this page) with default arguments.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>summ_center(f, method = "mean")

summ_mean(f)

summ_median(f)

summ_mode(f, method = "global")

summ_midrange(f)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summ_center_+3A_f">f</code></td>
<td>
<p>A pdqr-function representing distribution.</p>
</td></tr>
<tr><td><code id="summ_center_+3A_method">method</code></td>
<td>
<p>Method of center computation. For <code>summ_center()</code> is one of
&quot;mean&quot;, &quot;median&quot;, &quot;mode&quot;, &quot;midrange&quot;. For <code>summ_mode()</code> is one of &quot;global&quot;
or &quot;local&quot;.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>summ_mean()</code> computes distribution's mean.
</p>
<p><code>summ_median()</code> computes a smallest <code>x</code> value for which cumulative
probability is not less than 0.5. Essentially, it is a <code>as_q(f)(0.5)</code>. This
also means that for pdqr-functions with type &quot;discrete&quot; it always returns an
entry of &quot;x&quot; column from <code>f</code>'s <a href="#topic+meta_x_tbl">&quot;x_tbl&quot; metadata</a>.
</p>
<p><code style="white-space: pre;">&#8288;summ_mode(*, method = "global")&#8288;</code> computes a smallest <code>x</code> (which is an entry
of &quot;x&quot; column from <code>f</code>'s <code>x_tbl</code>) with the highest probability/density.
<code style="white-space: pre;">&#8288;summ_mode(*, method = "local")&#8288;</code> computes all <code>x</code> values which represent
non-strict <strong>local maxima</strong> of probability mass/density function.
</p>
<p><code>summ_midrange()</code> computes middle point of <code>f</code>'s <a href="#topic+meta_support">support</a>
(average of left and right edges).
</p>


<h3>Value</h3>

<p><code>summ_center()</code>, <code>summ_mean()</code>, <code>summ_median()</code> and <code style="white-space: pre;">&#8288;summ_mode(*, method = "global")&#8288;</code> always return a single number representing a center of
distribution. <code style="white-space: pre;">&#8288;summ_mode(*, method = "local")&#8288;</code> can return a numeric vector
with multiple values representing local maxima.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+summ_spread">summ_spread()</a></code> for computing distribution's spread, <code><a href="#topic+summ_moment">summ_moment()</a></code>
for general moments.
</p>
<p>Other summary functions: 
<code><a href="#topic+summ_classmetric">summ_classmetric</a>()</code>,
<code><a href="#topic+summ_distance">summ_distance</a>()</code>,
<code><a href="#topic+summ_entropy">summ_entropy</a>()</code>,
<code><a href="#topic+summ_hdr">summ_hdr</a>()</code>,
<code><a href="#topic+summ_interval">summ_interval</a>()</code>,
<code><a href="#topic+summ_moment">summ_moment</a>()</code>,
<code><a href="#topic+summ_order">summ_order</a>()</code>,
<code><a href="#topic+summ_prob_true">summ_prob_true</a>()</code>,
<code><a href="#topic+summ_pval">summ_pval</a>()</code>,
<code><a href="#topic+summ_quantile">summ_quantile</a>()</code>,
<code><a href="#topic+summ_roc">summ_roc</a>()</code>,
<code><a href="#topic+summ_separation">summ_separation</a>()</code>,
<code><a href="#topic+summ_spread">summ_spread</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Type "continuous"
d_norm &lt;- as_d(dnorm)
## The same as `summ_center(d_norm, method = "mean")`
summ_mean(d_norm)
summ_median(d_norm)
summ_mode(d_norm)
## As pdqr-functions always have finite support, output here is finite
summ_midrange(d_norm)

# Type "discrete"
d_pois &lt;- as_d(dpois, lambda = 10)
summ_mean(d_pois)
summ_median(d_pois)
## Returns the smallest `x` with highest probability
summ_mode(d_pois)
## Returns all values which are non-strict local maxima
summ_mode(d_pois, method = "local")
## As pdqr-functions always have finite support, output here is finite
summ_midrange(d_pois)

# Details of computing local modes
my_d &lt;- new_d(data.frame(x = 11:15, y = c(0, 1, 0, 2, 0) / 3), "continuous")
## Several values, which are entries of `x_tbl`, are returned as local modes
summ_mode(my_d, method = "local")
</code></pre>

<hr>
<h2 id='summ_classmetric'>Summarize pair of distributions with classification metric</h2><span id='topic+summ_classmetric'></span><span id='topic+summ_classmetric_df'></span>

<h3>Description</h3>

<p>Compute metric of the following one-dimensional binary classification setup:
any <code>x</code> value not more than <code>threshold</code> value is classified as &quot;negative&quot;; if
strictly greater - &quot;positive&quot;. Classification metrics are computed based on
two pdqr-functions: <code>f</code>, which represents the distribution of values which
<em>should be</em> classified as &quot;negative&quot; (&quot;true negative&quot;), and <code>g</code> - the same
for &quot;positive&quot; (&quot;true positive&quot;).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>summ_classmetric(f, g, threshold, method = "F1")

summ_classmetric_df(f, g, threshold, method = "F1")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summ_classmetric_+3A_f">f</code></td>
<td>
<p>A pdqr-function of any <a href="#topic+meta_type">type</a> and
<a href="#topic+meta_class">class</a>. Represents distribution of &quot;true negative&quot; values.</p>
</td></tr>
<tr><td><code id="summ_classmetric_+3A_g">g</code></td>
<td>
<p>A pdqr-function of any type and class. Represents distribution of
&quot;true positive&quot; values.</p>
</td></tr>
<tr><td><code id="summ_classmetric_+3A_threshold">threshold</code></td>
<td>
<p>A numeric vector of classification threshold(s).</p>
</td></tr>
<tr><td><code id="summ_classmetric_+3A_method">method</code></td>
<td>
<p>Method of classification metric (might be a vector for
<code>summ_classmetric_df()</code>). Should be one of &quot;TPR&quot;, &quot;TNR&quot;, &quot;FPR&quot;, &quot;FNR&quot;,
&quot;PPV&quot;, &quot;NPV&quot;, &quot;FDR&quot;, &quot;FOR&quot;, &quot;LR+&quot;, &quot;LR-&quot;, &quot;Acc&quot;, &quot;ER&quot;, &quot;GM&quot;, &quot;F1&quot;, &quot;OP&quot;,
&quot;MCC&quot;, &quot;YI&quot;, &quot;MK&quot;, &quot;Jaccard&quot;, &quot;DOR&quot; (with possible aliases, see Details).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Binary classification setup used here to compute metrics is a
simplified version of the most common one, when there is a finite set of
already classified objects. Usually, there are <code>N</code> objects which are truly
&quot;negative&quot; and <code>P</code> truly &quot;positive&quot; ones. Values <code>N</code> and <code>P</code> can vary, which
often results in class imbalance. However, in current setup both <code>N</code> and
<code>P</code> are equal to 1 (total probability of <code>f</code> and <code>g</code>).
</p>
<p>In common setup, classification of all <code>N + P</code> objects results into the
following values: &quot;TP&quot; (number of truly &quot;positive&quot; values classified as
&quot;positive&quot;), &quot;TN&quot; (number of negatives classified as &quot;negative&quot;), &quot;FP&quot;
(number of negatives falsely classified as &quot;positive&quot;), and &quot;FN&quot; (number of
positives falsely classified as &quot;negative&quot;). In current setup all those
values are equal to respective &quot;rates&quot; (because <code>N</code> and <code>P</code> are both equal to
1).
</p>
<p>Both <code>summ_classmetric()</code> and <code>summ_classmetric_df()</code> allow aliases to some
classification metrics (for readability purposes).
</p>
<p>Following classification metrics are available:
</p>

<ul>
<li><p> Simple metrics:
</p>

<ul>
<li> <p><em>True positive rate</em>, <code>method</code> &quot;TPR&quot; (aliases: &quot;TP&quot;, &quot;sensitivity&quot;,
&quot;recall&quot;): proportion of actual positives correctly classified as such.
Computed as <code>1 - as_p(g)(threshold)</code>.
</p>
</li>
<li> <p><em>True negative rate</em>, <code>method</code> &quot;TNR&quot; (aliases: &quot;TN&quot;, &quot;specificity&quot;):
proportion of actual negatives correctly classified as such. Computed as
<code>as_p(f)(threshold)</code>.
</p>
</li>
<li> <p><em>False positive rate</em>, <code>method</code> &quot;FPR&quot; (aliases: &quot;FP&quot;, &quot;fall-out&quot;):
proportion of actual negatives falsely classified as &quot;positive&quot;. Computed
as <code>1 - as_p(f)(threshold)</code>.
</p>
</li>
<li> <p><em>False negative rate</em>, <code>method</code> &quot;FNR&quot; (aliases: &quot;FN&quot;, &quot;miss_rate&quot;):
proportion of actual positives falsely classified as &quot;negative&quot;. Computed
as <code>as_p(g)(threshold)</code>.
</p>
</li>
<li> <p><em>Positive predictive value</em>, <code>method</code> &quot;PPV&quot; (alias: &quot;precision&quot;):
proportion of output positives that are actually &quot;positive&quot;. Computed as
<code>TP / (TP + FP)</code>.
</p>
</li>
<li> <p><em>Negative predictive value</em>, <code>method</code> &quot;NPV&quot;: proportion of output
negatives that are actually &quot;negative&quot;. Computed as <code>TN / (TN + FN)</code>.
</p>
</li>
<li> <p><em>False discovery rate</em>, <code>method</code> &quot;FDR&quot;: proportion of output positives
that are actually &quot;negative&quot;. Computed as <code>FP / (TP + FP)</code>.
</p>
</li>
<li> <p><em>False omission rate</em>, <code>method</code> &quot;FOR&quot;: proportion of output negatives
that are actually &quot;positive&quot;. Computed as <code>FN / (TN + FN)</code>.
</p>
</li>
<li> <p><em>Positive likelihood</em>, <code>method</code> &quot;LR+&quot;: measures how much the odds of
being &quot;positive&quot; increase when value is classified as &quot;positive&quot;.
Computed as <code>TPR / (1 - TNR)</code>.
</p>
</li>
<li> <p><em>Negative likelihood</em>, <code>method</code> &quot;LR-&quot;: measures how much the odds of
being &quot;positive&quot; decrease when value is classified as &quot;negative&quot;.
Computed as <code>(1 - TPR) / TNR</code>.
</p>
</li></ul>

</li>
<li><p> Combined metrics (for all, except &quot;error rate&quot;, bigger value represents
better classification performance):
</p>

<ul>
<li> <p><em>Accuracy</em>, <code>method</code> &quot;Acc&quot; (alias: &quot;accuracy&quot;): proportion of total
number of input values that were correctly classified. Computed as <code>(TP +   TN) / 2</code> (here 2 is used because of special classification setup,
<code>TP + TN + FP + FN = 2</code>).
</p>
</li>
<li> <p><em>Error rate</em>, <code>method</code> &quot;ER&quot; (alias: &quot;error_rate&quot;): proportion of
total number of input values that were incorrectly classified. Computed
as <code>(FP + FN) / 2</code>.
</p>
</li>
<li> <p><em>Geometric mean</em>, <code>method</code> &quot;GM&quot;: geometric mean of TPR and TNR.
Computed as <code>sqrt(TPR * TNR)</code>.
</p>
</li>
<li> <p><em>F1 score</em>, <code>method</code> &quot;F1&quot;: harmonic mean of PPV and TPR. Computed as
<code>2*TP / (2*TP + FP + FN)</code>.
</p>
</li>
<li> <p><em>Optimized precision</em>, <code>method</code> &quot;OP&quot;: accuracy, penalized for
imbalanced class performance. Computed as <code>Acc - abs(TPR - TNR) / (TPR +   TNR)</code>.
</p>
</li>
<li> <p><em>Matthews correlation coefficient</em>, <code>method</code> &quot;MCC&quot; (alias: &quot;corr&quot;):
correlation between the observed and predicted classifications. Computed
as <code>(TP*TN - FP*FN) / sqrt((TP+FP) * (TN+FN))</code> (here equalities <code>TP+FN =   1</code> and <code>TN+FP = 1</code> are used to simplify formula).
</p>
</li>
<li> <p><em>Youden’s index</em>, <code>method</code> &quot;YI&quot; (aliases: &quot;youden&quot;, &quot;informedness&quot;):
evaluates the discriminative power of the classification setup. Computed
as <code>TPR + TNR - 1</code>.
</p>
</li>
<li> <p><em>Markedness</em>, <code>method</code> &quot;MK&quot; (alias: &quot;markedness&quot;): evaluates the
predictive power of the classification setup. Computed as <code>PPV + NPV -   1</code>.
</p>
</li>
<li> <p><em>Jaccard</em>, <code>method</code> &quot;Jaccard&quot;: accuracy ignoring correct classification
of negatives. Computed as <code>TP / (TP + FP + FN)</code>.
</p>
</li>
<li> <p><em>Diagnostic odds ratio</em>, <code>method</code> &quot;DOR&quot; (alias: &quot;odds_ratio&quot;): ratio
between positive and negative likelihoods. Computed as <code>"LR+" / "LR-"</code>.
</p>
</li></ul>

</li></ul>



<h3>Value</h3>

<p><code>summ_classmetric()</code> returns a numeric vector, of the same length as
<code>threshold</code>, representing classification metrics for different threshold
values.
</p>
<p><code>summ_classmetric_df()</code> returns a data frame with rows corresponding to
<code>threshold</code> values. First column is &quot;threshold&quot; (with <code>threshold</code> values),
and all other represent classification metric for every input method (see
Examples).
</p>


<h3>See Also</h3>

<p><a href="#topic+summ_separation">summ_separation</a> for computing optimal separation threshold (which
is symmetrical with respect to <code>f</code> and <code>g</code>).
</p>
<p>Other summary functions: 
<code><a href="#topic+summ_center">summ_center</a>()</code>,
<code><a href="#topic+summ_distance">summ_distance</a>()</code>,
<code><a href="#topic+summ_entropy">summ_entropy</a>()</code>,
<code><a href="#topic+summ_hdr">summ_hdr</a>()</code>,
<code><a href="#topic+summ_interval">summ_interval</a>()</code>,
<code><a href="#topic+summ_moment">summ_moment</a>()</code>,
<code><a href="#topic+summ_order">summ_order</a>()</code>,
<code><a href="#topic+summ_prob_true">summ_prob_true</a>()</code>,
<code><a href="#topic+summ_pval">summ_pval</a>()</code>,
<code><a href="#topic+summ_quantile">summ_quantile</a>()</code>,
<code><a href="#topic+summ_roc">summ_roc</a>()</code>,
<code><a href="#topic+summ_separation">summ_separation</a>()</code>,
<code><a href="#topic+summ_spread">summ_spread</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>d_unif &lt;- as_d(dunif)
d_norm &lt;- as_d(dnorm)
t_vec &lt;- c(0, 0.5, 0.75, 1.5)

summ_classmetric(d_unif, d_norm, threshold = t_vec, method = "F1")
summ_classmetric(d_unif, d_norm, threshold = t_vec, method = "Acc")

summ_classmetric_df(
  d_unif, d_norm, threshold = t_vec, method = c("F1", "Acc")
)

# Using method aliases
summ_classmetric_df(
  d_unif, d_norm, threshold = t_vec, method = c("TPR", "sensitivity")
)
</code></pre>

<hr>
<h2 id='summ_distance'>Summarize pair of distributions with distance</h2><span id='topic+summ_distance'></span>

<h3>Description</h3>

<p>This function computes distance between two distributions represented by
pdqr-functions. Here &quot;distance&quot; is used in a broad sense: a single
non-negative number representing how much two distributions differ from one
another. Bigger values indicate bigger difference. Zero value means that
input distributions are equivalent based on the method used (except method
&quot;avgdist&quot; which is almost always returns positive value). The notion of
&quot;distance&quot; is useful for doing statistical inference about similarity of two
groups of numbers.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>summ_distance(f, g, method = "KS")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summ_distance_+3A_f">f</code></td>
<td>
<p>A pdqr-function of any <a href="#topic+meta_type">type</a> and
<a href="#topic+meta_class">class</a>.</p>
</td></tr>
<tr><td><code id="summ_distance_+3A_g">g</code></td>
<td>
<p>A pdqr-function of any type and class.</p>
</td></tr>
<tr><td><code id="summ_distance_+3A_method">method</code></td>
<td>
<p>Method for computing distance. Should be one of &quot;KS&quot;, &quot;totvar&quot;,
&quot;compare&quot;, &quot;wass&quot;, &quot;cramer&quot;, &quot;align&quot;, &quot;avgdist&quot;, &quot;entropy&quot;.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Methods can be separated into three categories: probability based,
metric based, and entropy based.
</p>
<p><strong>Probability based</strong> methods return a number between 0 and 1 which is
computed in the way that mostly based on probability:
</p>

<ul>
<li> <p><em>Method &quot;KS&quot;</em> (short for Kolmogorov-Smirnov) computes the supremum of
absolute difference between p-functions corresponding to <code>f</code> and <code>g</code> (<code style="white-space: pre;">&#8288;|F - G|&#8288;</code>). Here &quot;supremum&quot; is meant to describe the fact that if input functions
have different <a href="#topic+meta_type">types</a>, there can be no point at which &quot;KS&quot;
distance is achieved. Instead, there might be a sequence of points from left
to right with <code style="white-space: pre;">&#8288;|F - G|&#8288;</code> values tending to the result (see Examples).
</p>
</li>
<li> <p><em>Method &quot;totvar&quot;</em> (short for &quot;total variation&quot;) computes a biggest absolute
difference of probabilities for any subset of real line. In other words,
there is a set of points for &quot;discrete&quot; type and intervals for &quot;continuous&quot;,
total probability of which under <code>f</code> and <code>g</code> differs the most. <strong>Note</strong> that
if <code>f</code> and <code>g</code> have different types, output is always 1. The set of interest
consists from all &quot;x&quot; values of &quot;discrete&quot; pdqr-function: probability under
&quot;discrete&quot; distribution is 1 and under &quot;continuous&quot; is 0.
</p>
</li>
<li> <p><em>Method &quot;compare&quot;</em> represents a value computed based on probabilities of
one distribution being bigger than the other (see <a href="#topic+methods-group-generic">pdqr methods for &quot;Ops&quot; group generic family</a> for more details on comparing
pdqr-functions). It is computed as
<code>2*max(P(F &gt; G), P(F &lt; G)) + 0.5*P(F = G) - 1</code> (here <code>P(F &gt; G)</code> is basically
<code>summ_prob_true(f &gt; g)</code>). This is maximum of two values (<code>P(F &gt; G) + 0.5*P(F = G)</code> and <code>P(F &lt; G) + 0.5*P(F = G)</code>), normalized to return values from 0
to 1. Other way to look at this measure is that it computes (before
normalization) two <a href="#topic+summ_rocauc">ROC AUC</a> values with method <code>"expected"</code>
for two possible ordering (<code style="white-space: pre;">&#8288;f, g&#8288;</code>, and <code style="white-space: pre;">&#8288;g, f&#8288;</code>) and takes their maximum.
</p>
</li></ul>

<p><strong>Metric based</strong> methods compute &quot;how far&quot; two distributions are apart on the
real line:
</p>

<ul>
<li> <p><em>Method &quot;wass&quot;</em> (short for &quot;Wasserstein&quot;) computes a 1-Wasserstein
distance: &quot;minimum cost of 'moving' one density into another&quot;, or &quot;average
path density point should go while transforming from one into another&quot;. It is
computed as integral of <code style="white-space: pre;">&#8288;|F - G|&#8288;</code> (absolute difference between p-functions).
If any of <code>f</code> and <code>g</code> has &quot;continuous&quot; type, <code><a href="stats.html#topic+integrate">stats::integrate()</a></code> is used, so
relatively small numerical errors can happen.
</p>
</li>
<li> <p><em>Method &quot;cramer&quot;</em> computes Cramer distance: integral of <code>(F - G)^2</code>. This
somewhat relates to &quot;wass&quot; method as <a href="#topic+summ_var">variance</a> relates to <a href="#topic+summ_moment">first central absolute moment</a>. Relatively small numerical errors
can happen.
</p>
</li>
<li> <p><em>Method &quot;align&quot;</em> computes an absolute value of shift <code>d</code> (possibly
negative) that should be added to <code>f</code> to achieve both <code>P(f+d &gt;= g) &gt;= 0.5</code>
and <code>P(f+d &lt;= g) &gt;= 0.5</code> (in other words, align <code>f+d</code> and <code>g</code>) as close as
reasonably possible. Solution is found numerically with <code><a href="stats.html#topic+uniroot">stats::uniroot()</a></code>,
so relatively small numerical errors can happen. Also <strong>note</strong> that this
method is somewhat slow (compared to all others). To increase speed, use less
elements in <a href="#topic+meta_x_tbl">&quot;x_tbl&quot; metadata</a>. For example, with
<code><a href="#topic+form_retype">form_retype()</a></code> or smaller <code>n_grid</code> argument in <a href="#topic+as_p">as_*()</a> functions.
</p>
</li>
<li> <p><em>Method &quot;avgdist&quot;</em> computes average distance between sample values from
inputs. Basically, it is a deterministically computed approximation of
expected value of absolute difference between random variables, or in 'pdqr'
code: <code>summ_mean(abs(f - g))</code> (but computed without randomness). Computation
is done by approximating possibly present continuous pdqr-functions with
discrete ones (see description of <a href="#topic+pdqr-package">&quot;pdqr.approx_discrete_n_grid&quot; option</a> for more information) and then computing output value
directly based on two discrete pdqr-functions. <strong>Note</strong> that this method
almost never returns zero, even for identical inputs (except the case of
discrete pdqr-functions with identical one value).
</p>
</li></ul>

<p><strong>Entropy based</strong> methods compute output based on entropy characteristics:
</p>

<ul>
<li> <p><em>Method &quot;entropy&quot;</em> computes sum of two Kullback-Leibler divergences:
<code>KL(f, g) + KL(g, f)</code>, which are outputs of <code><a href="#topic+summ_entropy2">summ_entropy2()</a></code> with method
&quot;relative&quot;. <strong>Notes</strong>:
</p>

<ul>
<li><p> If <code>f</code> and <code>g</code> don't have the same support, distance can be very high.
</p>
</li>
<li><p> Error is thrown if <code>f</code> and <code>g</code> have different types (the same as in
<code>summ_entropy2()</code>).
</p>
</li></ul>

</li></ul>



<h3>Value</h3>

<p>A single non-negative number representing distance between pair of
distributions. For methods &quot;KS&quot;, &quot;totvar&quot;, and &quot;compare&quot; it is not bigger
than 1. For method &quot;avgdist&quot; it is almost always bigger than 0.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+summ_separation">summ_separation()</a></code> for computation of optimal threshold separating
pair of distributions.
</p>
<p>Other summary functions: 
<code><a href="#topic+summ_center">summ_center</a>()</code>,
<code><a href="#topic+summ_classmetric">summ_classmetric</a>()</code>,
<code><a href="#topic+summ_entropy">summ_entropy</a>()</code>,
<code><a href="#topic+summ_hdr">summ_hdr</a>()</code>,
<code><a href="#topic+summ_interval">summ_interval</a>()</code>,
<code><a href="#topic+summ_moment">summ_moment</a>()</code>,
<code><a href="#topic+summ_order">summ_order</a>()</code>,
<code><a href="#topic+summ_prob_true">summ_prob_true</a>()</code>,
<code><a href="#topic+summ_pval">summ_pval</a>()</code>,
<code><a href="#topic+summ_quantile">summ_quantile</a>()</code>,
<code><a href="#topic+summ_roc">summ_roc</a>()</code>,
<code><a href="#topic+summ_separation">summ_separation</a>()</code>,
<code><a href="#topic+summ_spread">summ_spread</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>d_unif &lt;- as_d(dunif, max = 2)
d_norm &lt;- as_d(dnorm, mean = 1)

vapply(
  c(
    "KS", "totvar", "compare",
    "wass", "cramer", "align", "avgdist",
    "entropy"
  ),
  function(meth) {
    summ_distance(d_unif, d_norm, method = meth)
  },
  numeric(1)
)

# "Supremum" quality of "KS" distance
d_dis &lt;- new_d(2, "discrete")
## Distance is 1, which is a limit of |F - G| at points which tend to 2 from
## left
summ_distance(d_dis, d_unif, method = "KS")
</code></pre>

<hr>
<h2 id='summ_entropy'>Summarize distribution with entropy</h2><span id='topic+summ_entropy'></span><span id='topic+summ_entropy2'></span>

<h3>Description</h3>

<p><code>summ_entropy()</code> computes entropy of single distribution while
<code>summ_entropy2()</code> - for a pair of distributions. For &quot;discrete&quot;
pdqr-functions a classic formula <code>-sum(p * log(p))</code> (in nats) is used. In
&quot;continuous&quot; case a differential entropy is computed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>summ_entropy(f)

summ_entropy2(f, g, method = "relative", clip = exp(-20))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summ_entropy_+3A_f">f</code></td>
<td>
<p>A pdqr-function representing distribution.</p>
</td></tr>
<tr><td><code id="summ_entropy_+3A_g">g</code></td>
<td>
<p>A pdqr-function. Should be the same type as <code>f</code>.</p>
</td></tr>
<tr><td><code id="summ_entropy_+3A_method">method</code></td>
<td>
<p>Entropy method for pair of distributions. One of &quot;relative&quot;
(Kullback–Leibler divergence) or &quot;cross&quot; (for cross-entropy).</p>
</td></tr>
<tr><td><code id="summ_entropy_+3A_clip">clip</code></td>
<td>
<p>Value to be used instead of 0 during <code>log()</code> computation.
<code>-log(clip)</code> represents the maximum value of output entropy.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><strong>Note</strong> that due to <a href="#topic+pdqr_approx_error">pdqr approximation error</a>
there can be a rather big error in entropy estimation in case original
density goes to infinity.
</p>


<h3>Value</h3>

<p>A single number representing entropy. If <code>clip</code> is strictly positive,
then it will be finite.
</p>


<h3>See Also</h3>

<p>Other summary functions: 
<code><a href="#topic+summ_center">summ_center</a>()</code>,
<code><a href="#topic+summ_classmetric">summ_classmetric</a>()</code>,
<code><a href="#topic+summ_distance">summ_distance</a>()</code>,
<code><a href="#topic+summ_hdr">summ_hdr</a>()</code>,
<code><a href="#topic+summ_interval">summ_interval</a>()</code>,
<code><a href="#topic+summ_moment">summ_moment</a>()</code>,
<code><a href="#topic+summ_order">summ_order</a>()</code>,
<code><a href="#topic+summ_prob_true">summ_prob_true</a>()</code>,
<code><a href="#topic+summ_pval">summ_pval</a>()</code>,
<code><a href="#topic+summ_quantile">summ_quantile</a>()</code>,
<code><a href="#topic+summ_roc">summ_roc</a>()</code>,
<code><a href="#topic+summ_separation">summ_separation</a>()</code>,
<code><a href="#topic+summ_spread">summ_spread</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>d_norm &lt;- as_d(dnorm)
d_norm_2 &lt;- as_d(dnorm, mean = 2, sd = 0.5)

summ_entropy(d_norm)
summ_entropy2(d_norm, d_norm_2)
summ_entropy2(d_norm, d_norm_2, method = "cross")

# Increasing `clip` leads to decreasing maximum output value
d_1 &lt;- new_d(1:10, "discrete")
d_2 &lt;- new_d(20:21, "discrete")

## Formally, output isn't clearly defined because functions don't have the
## same support. Direct use of entropy formulas gives infinity output, but
## here maximum value is `-log(clip)`.
summ_entropy2(d_1, d_2, method = "cross")
summ_entropy2(d_1, d_2, method = "cross", clip = exp(-10))
summ_entropy2(d_1, d_2, method = "cross", clip = 0)
</code></pre>

<hr>
<h2 id='summ_hdr'>Summarize distribution with Highest Density Region</h2><span id='topic+summ_hdr'></span>

<h3>Description</h3>

<p><code>summ_hdr()</code> computes a Highest Density Region (HDR) of some pdqr-function
for a supplied <code>level</code>: a union of (closed) intervals total probability of
which is not less than <code>level</code> and probability/density at any point inside it
is bigger than some threshold (which should be maximum one with a property
of HDR having total probability not less than <code>level</code>). This also represents
a set of intervals with the lowest total width among all sets with total
probability not less than a <code>level</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>summ_hdr(f, level = 0.95)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summ_hdr_+3A_f">f</code></td>
<td>
<p>A pdqr-function representing distribution.</p>
</td></tr>
<tr><td><code id="summ_hdr_+3A_level">level</code></td>
<td>
<p>A desired lower bound for a total probability of an output set
of intervals.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>General algorithm of <code>summ_hdr()</code> consists from two steps:
</p>

<ol>
<li> <p><strong>Find &quot;target height&quot;</strong>. That is a value of probability/density which
divides all <a href="#topic+meta_support">support</a> into two sets: the one with
probability/density not less than target height (it is a desired HDR) and the
other - with strictly less. The first set should also have total probability
not less than <code>level</code>.
</p>
</li>
<li> <p><strong>Form a HDR as a set of closed intervals</strong>.
</p>
</li></ol>

<p>If <code>f</code> has &quot;discrete&quot; type, target height is computed by looking at &quot;x&quot;
values of <a href="#topic+meta_x_tbl">&quot;x_tbl&quot; metadata</a> in order of decreasing probability
until their total probability is not less than <code>level</code>. After that, all &quot;x&quot;
values with probability not less than height are considered to form a HDR.
Output is formed as a set of <strong>closed</strong> intervals (i.e. both edges included)
inside of which lie all HDR &quot;x&quot; elements and others - don't.
</p>
<p>If <code>f</code> has &quot;continuous&quot; type, target height is estimated as <code>1-level</code>
quantile of <code>Y = d_f(X)</code> distribution, where <code>d_f</code> is d-function
corresponding to <code>f</code> (<code><a href="#topic+as_d">as_d(f)</a></code> in other words) and <code>X</code> is a random
variable represented by <code>f</code>. Essentially, <code>Y</code> has a distribution of <code>f</code>'s
density values and its <code>1-level</code> quantile is a target height. After that, HDR
is formed as a set of intervals <strong>with positive width</strong> (if <code>level</code> is more
than 0, see Notes) inside which density is not less than target height.
</p>
<p><strong>Notes</strong>:
</p>

<ul>
<li><p> If <code>level</code> is 0, output has one interval of zero width at point of <a href="#topic+summ_mode">global mode</a>.
</p>
</li>
<li><p> If <code>level</code> is 1, output has one interval equal to support.
</p>
</li>
<li><p> Computation of target height in case of &quot;continuous&quot; type is approximate
which in some extreme cases (for example, like <a href="#topic+form_tails">winsorized</a>
distributions) can lead to HDR having total probability very approximate to
and even slightly lower than <code>level</code>.
</p>
</li>
<li><p> If d-function has &quot;plateaus&quot; (consecutive values with equal
probability/density) at computed target height, total probability of HDR can
be considerably bigger than <code>level</code> (see examples). However, this aligns with
HDR definition, as density values should be <strong>not less</strong> than target height
and total probability should be <strong>not less</strong> than <code>level</code>.
</p>
</li></ul>



<h3>Value</h3>

<p>A data frame with one row representing one closed interval of HDR and
the following columns:
</p>

<ul>
<li> <p><strong>left</strong> <code style="white-space: pre;">&#8288;&lt;dbl&gt;&#8288;</code> : Left end of intervals.
</p>
</li>
<li> <p><strong>right</strong> <code style="white-space: pre;">&#8288;&lt;dbl&gt;&#8288;</code> : Right end of intervals.
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+region">region_*()</a></code> family of functions for working with output
HDR.
</p>
<p><code><a href="#topic+summ_interval">summ_interval()</a></code> for computing of single interval summary of distribution.
</p>
<p>Other summary functions: 
<code><a href="#topic+summ_center">summ_center</a>()</code>,
<code><a href="#topic+summ_classmetric">summ_classmetric</a>()</code>,
<code><a href="#topic+summ_distance">summ_distance</a>()</code>,
<code><a href="#topic+summ_entropy">summ_entropy</a>()</code>,
<code><a href="#topic+summ_interval">summ_interval</a>()</code>,
<code><a href="#topic+summ_moment">summ_moment</a>()</code>,
<code><a href="#topic+summ_order">summ_order</a>()</code>,
<code><a href="#topic+summ_prob_true">summ_prob_true</a>()</code>,
<code><a href="#topic+summ_pval">summ_pval</a>()</code>,
<code><a href="#topic+summ_quantile">summ_quantile</a>()</code>,
<code><a href="#topic+summ_roc">summ_roc</a>()</code>,
<code><a href="#topic+summ_separation">summ_separation</a>()</code>,
<code><a href="#topic+summ_spread">summ_spread</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># "discrete" functions
d_dis &lt;- new_d(data.frame(x = 1:4, prob = c(0.4, 0.2, 0.3, 0.1)), "discrete")
summ_hdr(d_dis, 0.3)
summ_hdr(d_dis, 0.5)
summ_hdr(d_dis, 0.9)
## Zero width interval at global mode
summ_hdr(d_dis, 0)

# "continuous" functions
d_norm &lt;- as_d(dnorm)
summ_hdr(d_norm, 0.95)
## Zero width interval at global mode
summ_hdr(d_norm, 0)

# Works well with mixture distributions
d_mix &lt;- form_mix(list(as_d(dnorm), as_d(dnorm, mean = 5)))
summ_hdr(d_mix, 0.95)

# Plateaus
d_unif &lt;- as_d(dunif)
## Returns all support because of density "plateau"
summ_hdr(d_unif, 0.1)

# Draw HDR
plot(d_mix)
region_draw(summ_hdr(d_mix, 0.95))
</code></pre>

<hr>
<h2 id='summ_interval'>Summarize distribution with interval</h2><span id='topic+summ_interval'></span>

<h3>Description</h3>

<p>These functions summarize distribution with one interval based on method of
choice.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>summ_interval(f, level = 0.95, method = "minwidth", n_grid = 10001)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summ_interval_+3A_f">f</code></td>
<td>
<p>A pdqr-function representing distribution.</p>
</td></tr>
<tr><td><code id="summ_interval_+3A_level">level</code></td>
<td>
<p>A number between 0 and 1 representing a coverage degree of
interval. Interpretation depends on <code>method</code> but the bigger is number, the
wider is interval.</p>
</td></tr>
<tr><td><code id="summ_interval_+3A_method">method</code></td>
<td>
<p>Method of interval computation. Should be on of &quot;minwidth&quot;,
&quot;percentile&quot;, &quot;sigma&quot;.</p>
</td></tr>
<tr><td><code id="summ_interval_+3A_n_grid">n_grid</code></td>
<td>
<p>Number of grid elements to be used for &quot;minwidth&quot; method (see
Details).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Method &quot;minwidth&quot; searches for an interval with total probability of
<code>level</code> that has minimum width. This is done with grid search: <code>n_grid</code>
possible intervals with <code>level</code> total probability are computed and the one
with minimum width is returned (if there are several, the one with the
smallest left end). Left ends of computed set of intervals are created as a
grid from <code>0</code> to <code>1-level</code> quantiles with <code>n_grid</code> number of elements. Right
ends are computed so that intervals have <code>level</code> total probability.
</p>
<p>Method &quot;percentile&quot; returns an interval with edges being <code>0.5*(1-level)</code> and
<code>1 - 0.5*(1-level)</code> quantiles. Output has total probability equal to <code>level</code>.
</p>
<p>Method &quot;sigma&quot; computes an interval symmetrically centered at
<a href="#topic+summ_mean">mean</a> of distribution. Left and right edges are distant from
center by the amount of <a href="#topic+summ_sd">standard deviation</a> multiplied by
<code>level</code>'s critical value. Critical value is computed using <a href="stats.html#topic+Normal">normal distribution</a> as <code>qnorm(1 - 0.5*(1-level))</code>, which
corresponds to a way of computing sample confidence interval with known
standard deviation. The final output interval is possibly cut so that not to
be out of <code>f</code>'s <a href="#topic+meta_support">support</a>.
</p>
<p><strong>Note</strong> that supported methods correspond to different ways of computing
<a href="#topic+summ_center">distribution's center</a>. This idea is supported by the fact
that when <code>level</code> is 0, &quot;minwidth&quot; method returns zero width interval at
distribution's <a href="#topic+summ_mode">global mode</a>, &quot;percentile&quot; method -
<a href="#topic+summ_median">median</a>, &quot;sigma&quot; - <a href="#topic+summ_mean">mean</a>.
</p>


<h3>Value</h3>

<p>A <a href="#topic+region">region</a> with one row. That is a data frame with one row
and the following columns:
</p>

<ul>
<li> <p><strong>left</strong> <code style="white-space: pre;">&#8288;&lt;dbl&gt;&#8288;</code> : Left end of interval.
</p>
</li>
<li> <p><strong>right</strong> <code style="white-space: pre;">&#8288;&lt;dbl&gt;&#8288;</code> : Right end of interval.
</p>
</li></ul>

<p>To return a simple numeric vector, call <a href="base.html#topic+unlist">unlist()</a> on
<code>summ_interval()</code>'s output (see Examples).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+summ_hdr">summ_hdr()</a></code> for computing of Highest Density Region, which can
summarize distribution with multiple intervals.
</p>
<p><a href="#topic+region">region_*()</a> family of functions for working with <code>summ_interval()</code>
output.
</p>
<p>Other summary functions: 
<code><a href="#topic+summ_center">summ_center</a>()</code>,
<code><a href="#topic+summ_classmetric">summ_classmetric</a>()</code>,
<code><a href="#topic+summ_distance">summ_distance</a>()</code>,
<code><a href="#topic+summ_entropy">summ_entropy</a>()</code>,
<code><a href="#topic+summ_hdr">summ_hdr</a>()</code>,
<code><a href="#topic+summ_moment">summ_moment</a>()</code>,
<code><a href="#topic+summ_order">summ_order</a>()</code>,
<code><a href="#topic+summ_prob_true">summ_prob_true</a>()</code>,
<code><a href="#topic+summ_pval">summ_pval</a>()</code>,
<code><a href="#topic+summ_quantile">summ_quantile</a>()</code>,
<code><a href="#topic+summ_roc">summ_roc</a>()</code>,
<code><a href="#topic+summ_separation">summ_separation</a>()</code>,
<code><a href="#topic+summ_spread">summ_spread</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Type "discrete"
d_dis &lt;- new_d(data.frame(x = 1:6, prob = c(3:1, 0:2) / 9), "discrete")
summ_interval(d_dis, level = 0.5, method = "minwidth")
summ_interval(d_dis, level = 0.5, method = "percentile")
summ_interval(d_dis, level = 0.5, method = "sigma")

## Visual difference between methods
plot(d_dis)
region_draw(summ_interval(d_dis, 0.5, method = "minwidth"), col = "blue")
region_draw(summ_interval(d_dis, 0.5, method = "percentile"), col = "red")
region_draw(summ_interval(d_dis, 0.5, method = "sigma"), col = "green")

# Type "continuous"
d_con &lt;- form_mix(
  list(as_d(dnorm), as_d(dnorm, mean = 5)),
  weights = c(0.25, 0.75)
)
summ_interval(d_con, level = 0.5, method = "minwidth")
summ_interval(d_con, level = 0.5, method = "percentile")
summ_interval(d_con, level = 0.5, method = "sigma")

## Visual difference between methods
plot(d_con)
region_draw(summ_interval(d_con, 0.5, method = "minwidth"), col = "blue")
region_draw(summ_interval(d_con, 0.5, method = "percentile"), col = "red")
region_draw(summ_interval(d_con, 0.5, method = "sigma"), col = "green")

# Output interval is always inside input's support. Formally, next code
# should return interval from `-Inf` to `Inf`, but output is cut to be inside
# support.
summ_interval(d_con, level = 1, method = "sigma")

# To get vector output, use `unlist()`
unlist(summ_interval(d_con))
</code></pre>

<hr>
<h2 id='summ_moment'>Summarize distribution with moment</h2><span id='topic+summ_moment'></span><span id='topic+summ_skewness'></span><span id='topic+summ_kurtosis'></span>

<h3>Description</h3>

<p><code>summ_moment()</code> computes a moment of distribution. It can be one of eight
kinds determined by the combination of <code>central</code>, <code>standard</code>, and <code>absolute</code>
boolean features. <code>summ_skewness()</code> and <code>summ_kurtosis()</code> are wrappers for
commonly used kinds of moments: third and forth order central standard ones.
<strong>Note</strong> that <code>summ_kurtosis()</code> by default computes excess kurtosis, i.e.
subtracts 3 from computed forth order moment.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>summ_moment(f, order, central = FALSE, standard = FALSE,
  absolute = FALSE)

summ_skewness(f)

summ_kurtosis(f, excess = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summ_moment_+3A_f">f</code></td>
<td>
<p>A pdqr-function representing distribution.</p>
</td></tr>
<tr><td><code id="summ_moment_+3A_order">order</code></td>
<td>
<p>A single number representing order of a moment. Should be
non-negative number (even fractional).</p>
</td></tr>
<tr><td><code id="summ_moment_+3A_central">central</code></td>
<td>
<p>Whether to compute central moment (subtract
<a href="#topic+summ_mean">mean</a> of distribution).</p>
</td></tr>
<tr><td><code id="summ_moment_+3A_standard">standard</code></td>
<td>
<p>Whether to compute standard moment (divide by <a href="#topic+summ_sd">standard deviation</a> of distribution).</p>
</td></tr>
<tr><td><code id="summ_moment_+3A_absolute">absolute</code></td>
<td>
<p>Whether to compute absolute moment (take absolute value of
random variable created after possible effect of <code>central</code> and <code>standard</code>).</p>
</td></tr>
<tr><td><code id="summ_moment_+3A_excess">excess</code></td>
<td>
<p>Whether to compute excess kurtosis (subtract 3 from third order
central standard moment). Default is <code>TRUE</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A single number representing moment. If <code>summ_sd(f)</code> is zero and
<code>standard</code> is <code>TRUE</code>, then it is <code>Inf</code>; otherwise - finite number.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+summ_center">summ_center()</a></code> for computing distribution's center, <code><a href="#topic+summ_spread">summ_spread()</a></code>
for spread.
</p>
<p>Other summary functions: 
<code><a href="#topic+summ_center">summ_center</a>()</code>,
<code><a href="#topic+summ_classmetric">summ_classmetric</a>()</code>,
<code><a href="#topic+summ_distance">summ_distance</a>()</code>,
<code><a href="#topic+summ_entropy">summ_entropy</a>()</code>,
<code><a href="#topic+summ_hdr">summ_hdr</a>()</code>,
<code><a href="#topic+summ_interval">summ_interval</a>()</code>,
<code><a href="#topic+summ_order">summ_order</a>()</code>,
<code><a href="#topic+summ_prob_true">summ_prob_true</a>()</code>,
<code><a href="#topic+summ_pval">summ_pval</a>()</code>,
<code><a href="#topic+summ_quantile">summ_quantile</a>()</code>,
<code><a href="#topic+summ_roc">summ_roc</a>()</code>,
<code><a href="#topic+summ_separation">summ_separation</a>()</code>,
<code><a href="#topic+summ_spread">summ_spread</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>d_beta &lt;- as_d(dbeta, shape1 = 2, shape2 = 1)

# The same as `summ_mean(d_beta)`
summ_moment(d_beta, order = 1)

# The same as `summ_var(d_beta)`
summ_moment(d_beta, order = 2, central = TRUE)

# Return the same number
summ_moment(d_beta, order = 3, central = TRUE, standard = TRUE)
summ_skewness(d_beta)

# Return the same number representing non-excess kurtosis
summ_moment(d_beta, order = 4, central = TRUE, standard = TRUE)
summ_kurtosis(d_beta, excess = FALSE)
</code></pre>

<hr>
<h2 id='summ_order'>Summarize list of pdqr-functions with order</h2><span id='topic+summ_order'></span><span id='topic+summ_sort'></span><span id='topic+summ_rank'></span>

<h3>Description</h3>

<p>Functions for ordering the set of pdqr-functions supplied in a list. This
might be useful for doing comparative statistical inference for several
groups of data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>summ_order(f_list, method = "compare", decreasing = FALSE)

summ_sort(f_list, method = "compare", decreasing = FALSE)

summ_rank(f_list, method = "compare")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summ_order_+3A_f_list">f_list</code></td>
<td>
<p>List of pdqr-functions.</p>
</td></tr>
<tr><td><code id="summ_order_+3A_method">method</code></td>
<td>
<p>Method to be used for ordering. Should be one of &quot;compare&quot;,
&quot;mean&quot;, &quot;median&quot;, &quot;mode&quot;, &quot;midrange&quot;.</p>
</td></tr>
<tr><td><code id="summ_order_+3A_decreasing">decreasing</code></td>
<td>
<p>If <code>TRUE</code> ordering is done decreasingly.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Ties for all methods are handled so as to preserve the original
order.
</p>
<p>Method &quot;compare&quot; is using the following ordering relation: pdqr-function <code>f</code>
is greater than <code>g</code> if and only if <code>P(f &gt;= g) &gt; 0.5</code>, or in code
<code>summ_prob_true(f &gt;= g) &gt; 0.5</code> (see <a href="#topic+methods-group-generic">pdqr methods for &quot;Ops&quot; group generic family</a> for more details on comparing pdqr-functions).
This method orders input based on this relation and <a href="base.html#topic+order">order()</a>
function. <strong>Notes</strong>:
</p>

<ul>
<li><p> This relation doesn't define strictly ordering because it is not
transitive: there can be pdqr-functions <code>f</code>, <code>g</code>, and <code>h</code>, for which <code>f</code> is
greater than <code>g</code>, <code>g</code> is greater than <code>h</code>, and <code>h</code> is greater than <code>f</code> (but
should be otherwise). If not addressed, this might result into dependence of
output on order of the input. It is solved by first preordering <code>f_list</code>
based on method &quot;mean&quot; and then calling <code>order()</code>.
</p>
</li>
<li><p> Because comparing two pdqr-functions can be time consuming, this method
becomes rather slow as number of <code>f_list</code> elements grows.
</p>
</li></ul>

<p>Methods &quot;mean&quot;, &quot;median&quot;, &quot;mode&quot;, and &quot;midrange&quot; are based on
<code><a href="#topic+summ_center">summ_center()</a></code>: ordering of <code>f_list</code> is defined as ordering of corresponding
measures of distribution's center.
</p>


<h3>Value</h3>

<p><code>summ_order()</code> works essentially like <a href="base.html#topic+order">order()</a>. It
returns an integer vector representing a permutation which rearranges
<code>f_list</code> in desired order.
</p>
<p><code>summ_sort()</code> returns a sorted (in desired order) variant of <code>f_list</code>.
</p>
<p><code>summ_rank()</code> returns a numeric vector representing ranks of <code>f_list</code>
elements: 1 for the &quot;smallest&quot;, <code>length(f_list)</code> for the &quot;biggest&quot;.
</p>


<h3>See Also</h3>

<p>Other summary functions: 
<code><a href="#topic+summ_center">summ_center</a>()</code>,
<code><a href="#topic+summ_classmetric">summ_classmetric</a>()</code>,
<code><a href="#topic+summ_distance">summ_distance</a>()</code>,
<code><a href="#topic+summ_entropy">summ_entropy</a>()</code>,
<code><a href="#topic+summ_hdr">summ_hdr</a>()</code>,
<code><a href="#topic+summ_interval">summ_interval</a>()</code>,
<code><a href="#topic+summ_moment">summ_moment</a>()</code>,
<code><a href="#topic+summ_prob_true">summ_prob_true</a>()</code>,
<code><a href="#topic+summ_pval">summ_pval</a>()</code>,
<code><a href="#topic+summ_quantile">summ_quantile</a>()</code>,
<code><a href="#topic+summ_roc">summ_roc</a>()</code>,
<code><a href="#topic+summ_separation">summ_separation</a>()</code>,
<code><a href="#topic+summ_spread">summ_spread</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>d_fun &lt;- as_d(dunif)
f_list &lt;- list(a = d_fun, b = d_fun + 1, c = d_fun - 1)
summ_order(f_list)
summ_sort(f_list)
summ_rank(f_list)

# All methods might give different results on some elaborated pdqr-functions
# Methods "compare" and "mean" are not equivalent
non_mean_list &lt;- list(
  new_d(data.frame(x = c(0.56, 0.815), y = c(1, 1)), "continuous"),
  new_d(data.frame(x = 0:1, y = c(0, 1)), "continuous")
)
summ_order(non_mean_list, method = "compare")
summ_order(non_mean_list, method = "mean")

# Methods powered by `summ_center()` are not equivalent
m &lt;- c(0, 0.2, 0.1)
s &lt;- c(1.1, 1.2, 1.3)
dlnorm_list &lt;- lapply(seq_along(m), function(i) {
  as_d(dlnorm, meanlog = m[i], sdlog = s[i])
})
summ_order(dlnorm_list, method = "mean")
summ_order(dlnorm_list, method = "median")
summ_order(dlnorm_list, method = "mode")

# Method "compare" handles inherited non-transitivity. Here third element is
# "greater" than second (`P(f &gt;= g) &gt; 0.5`), second - than first, and first
# is "greater" than third.
non_trans_list &lt;- list(
  new_d(data.frame(x = c(0.39, 0.44, 0.46), y = c(17, 14, 0)), "continuous"),
  new_d(data.frame(x = c(0.05, 0.3, 0.70), y = c(4, 0, 4)), "continuous"),
  new_d(data.frame(x = c(0.03, 0.40, 0.80), y = c(1, 1, 1)), "continuous")
)
summ_sort(non_trans_list)
## Output doesn't depend on initial order
summ_sort(non_trans_list[c(2, 3, 1)])
</code></pre>

<hr>
<h2 id='summ_prob_true'>Summarize boolean distribution with probability</h2><span id='topic+summ_prob_true'></span><span id='topic+summ_prob_false'></span>

<h3>Description</h3>

<p>Here <code>summ_prob_false()</code> returns a probability of 0 and <code>summ_prob_true()</code> -
complementary probability (one minus <code>summ_prob_false()</code> output). Both of
them check if their input is a <strong>boolean</strong> pdqr-function: type &quot;discrete&quot;
with <code>x</code> in <code>x_tbl</code> identical to <code>c(0, 1)</code>. If it is not, warning is thrown.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>summ_prob_false(f)

summ_prob_true(f)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summ_prob_true_+3A_f">f</code></td>
<td>
<p>A pdqr-function representing distribution.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A single numeric value representing corresponding probability.
</p>


<h3>See Also</h3>

<p>Other summary functions: 
<code><a href="#topic+summ_center">summ_center</a>()</code>,
<code><a href="#topic+summ_classmetric">summ_classmetric</a>()</code>,
<code><a href="#topic+summ_distance">summ_distance</a>()</code>,
<code><a href="#topic+summ_entropy">summ_entropy</a>()</code>,
<code><a href="#topic+summ_hdr">summ_hdr</a>()</code>,
<code><a href="#topic+summ_interval">summ_interval</a>()</code>,
<code><a href="#topic+summ_moment">summ_moment</a>()</code>,
<code><a href="#topic+summ_order">summ_order</a>()</code>,
<code><a href="#topic+summ_pval">summ_pval</a>()</code>,
<code><a href="#topic+summ_quantile">summ_quantile</a>()</code>,
<code><a href="#topic+summ_roc">summ_roc</a>()</code>,
<code><a href="#topic+summ_separation">summ_separation</a>()</code>,
<code><a href="#topic+summ_spread">summ_spread</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>d_unif &lt;- as_d(dunif)
d_norm &lt;- as_d(dnorm)
summ_prob_true(d_unif &gt; d_norm)
summ_prob_false(2 * d_norm &gt; d_unif)

# When input is "continuous" function or doesn't have 0 as distribution
# element, probability of being false is returned as 0.
summ_prob_false(d_unif)
summ_prob_true(new_d(2, "discrete"))
</code></pre>

<hr>
<h2 id='summ_pval'>Summarize distribution with p-value</h2><span id='topic+summ_pval'></span>

<h3>Description</h3>

<p><code>summ_pval()</code> computes p-value(s) based on supplied distribution and observed
value(s). There are several methods of computing p-values (&quot;both&quot;, &quot;right&quot;,
and &quot;left&quot;) as well as several types of multiple comparison adjustments
(using on <code><a href="stats.html#topic+p.adjust">stats::p.adjust()</a></code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>summ_pval(f, obs, method = "both", adjust = "holm")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summ_pval_+3A_f">f</code></td>
<td>
<p>A pdqr-function representing distribution.</p>
</td></tr>
<tr><td><code id="summ_pval_+3A_obs">obs</code></td>
<td>
<p>Numeric vector of observed values to be used as threshold for
p-value. Can have multiple values, in which case output will be adjusted
for multiple comparisons with <a href="stats.html#topic+p.adjust">p.adjust()</a>.</p>
</td></tr>
<tr><td><code id="summ_pval_+3A_method">method</code></td>
<td>
<p>Method representing direction of p-value computation. Should be
one of &quot;both&quot;, &quot;right&quot;, &quot;left&quot;.</p>
</td></tr>
<tr><td><code id="summ_pval_+3A_adjust">adjust</code></td>
<td>
<p>Adjustment method as <code>method</code> argument to <code>p.adjust()</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Method &quot;both&quot; for each element in <code>obs</code> computes two-sided p-value
as <code>min(1, 2 * min(right_p_val, left_p_val))</code>, where <code>right_p_val</code> and
<code>left_p_val</code> are right and left one-sided p-values (ones which are computed
with &quot;right&quot; and &quot;left&quot; methods) of <code>obs</code>'s elements correspondingly.
</p>
<p>Method &quot;right&quot; for each element <code>x</code> of <code>obs</code> computes probability of <code>f &gt;= x</code>
being true (more strictly, of random variable, represented by <code>f</code>, being not
less than <code>x</code>). This corresponds to right one-sided p-value.
</p>
<p>Method &quot;left&quot; for each element <code>x</code> of <code>obs</code> computes probability of <code>f &lt;= x</code>,
which is a left one-sided p-value.
</p>
<p><strong>Note</strong> that by default multiple p-values in output are adjusted with
<code style="white-space: pre;">&#8288;p.adjust(*, method = adjust)&#8288;</code>. To not do any adjustment, use <code>adjust = "none"</code>.
</p>


<h3>Value</h3>

<p>A numeric vector with the same length as <code>obs</code> representing
corresponding p-values after possible adjustment for multiple comparisons.
</p>


<h3>See Also</h3>

<p>Other summary functions: 
<code><a href="#topic+summ_center">summ_center</a>()</code>,
<code><a href="#topic+summ_classmetric">summ_classmetric</a>()</code>,
<code><a href="#topic+summ_distance">summ_distance</a>()</code>,
<code><a href="#topic+summ_entropy">summ_entropy</a>()</code>,
<code><a href="#topic+summ_hdr">summ_hdr</a>()</code>,
<code><a href="#topic+summ_interval">summ_interval</a>()</code>,
<code><a href="#topic+summ_moment">summ_moment</a>()</code>,
<code><a href="#topic+summ_order">summ_order</a>()</code>,
<code><a href="#topic+summ_prob_true">summ_prob_true</a>()</code>,
<code><a href="#topic+summ_quantile">summ_quantile</a>()</code>,
<code><a href="#topic+summ_roc">summ_roc</a>()</code>,
<code><a href="#topic+summ_separation">summ_separation</a>()</code>,
<code><a href="#topic+summ_spread">summ_spread</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Type "discrete"
d_dis &lt;- new_d(data.frame(x = 1:5, prob = c(1, 2, 3, 2, 1) / 9), "discrete")
summ_pval(d_dis, 3, method = "both")
summ_pval(d_dis, 3, method = "right")
summ_pval(d_dis, 3, method = "left")

# Type "continuous"
d_norm &lt;- as_d(dnorm)
summ_pval(d_norm, 2, method = "both")
summ_pval(d_norm, 2, method = "right")
summ_pval(d_norm, 2, method = "left")

# Adjustment is made for multiple observed values
summ_pval(d_norm, seq(0, 2, by = 0.1))
## Use `adjust = "none"` for to not do any adjustment
summ_pval(d_norm, seq(0, 2, by = 0.1), adjust = "none")
</code></pre>

<hr>
<h2 id='summ_quantile'>Summarize distribution with quantiles</h2><span id='topic+summ_quantile'></span>

<h3>Description</h3>

<p>Essentially, this is a more strict wrapper of <code>as_q(f)(probs)</code>. If any value
in <code>probs</code> is outside of segment \[0; 1\], an error is thrown.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>summ_quantile(f, probs)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summ_quantile_+3A_f">f</code></td>
<td>
<p>A pdqr-function representing distribution.</p>
</td></tr>
<tr><td><code id="summ_quantile_+3A_probs">probs</code></td>
<td>
<p>Vector of probabilities for which quantiles should be returned.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numeric vector of the same length as <code>probs</code> representing
corresponding quantiles.
</p>


<h3>See Also</h3>

<p>Other summary functions: 
<code><a href="#topic+summ_center">summ_center</a>()</code>,
<code><a href="#topic+summ_classmetric">summ_classmetric</a>()</code>,
<code><a href="#topic+summ_distance">summ_distance</a>()</code>,
<code><a href="#topic+summ_entropy">summ_entropy</a>()</code>,
<code><a href="#topic+summ_hdr">summ_hdr</a>()</code>,
<code><a href="#topic+summ_interval">summ_interval</a>()</code>,
<code><a href="#topic+summ_moment">summ_moment</a>()</code>,
<code><a href="#topic+summ_order">summ_order</a>()</code>,
<code><a href="#topic+summ_prob_true">summ_prob_true</a>()</code>,
<code><a href="#topic+summ_pval">summ_pval</a>()</code>,
<code><a href="#topic+summ_roc">summ_roc</a>()</code>,
<code><a href="#topic+summ_separation">summ_separation</a>()</code>,
<code><a href="#topic+summ_spread">summ_spread</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>d_norm &lt;- as_d(dnorm)
probs &lt;- c(0.25, 0.5, 0.75)
all.equal(summ_quantile(d_norm, probs), as_q(d_norm)(probs))
</code></pre>

<hr>
<h2 id='summ_roc'>Summarize distributions with ROC curve</h2><span id='topic+summ_roc'></span><span id='topic+summ_rocauc'></span><span id='topic+roc_plot'></span><span id='topic+roc_lines'></span>

<h3>Description</h3>

<p>These functions help you perform a ROC (&quot;Receiver Operating Characteristic&quot;)
analysis for one-dimensional linear classifier: values not more than some
threshold are classified as &quot;negative&quot;, and more than threshold -
as &quot;positive&quot;. Here input pair of pdqr-functions represent &quot;true&quot;
distributions of values with &quot;negative&quot; (<code>f</code>) and &quot;positive&quot; (<code>g</code>) labels.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>summ_roc(f, g, n_grid = 1001)

summ_rocauc(f, g, method = "expected")

roc_plot(roc, ..., add_bisector = TRUE)

roc_lines(roc, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summ_roc_+3A_f">f</code></td>
<td>
<p>A pdqr-function of any <a href="#topic+meta_type">type</a> and
<a href="#topic+meta_class">class</a>. Represents &quot;true&quot; distribution of &quot;negative&quot; values.</p>
</td></tr>
<tr><td><code id="summ_roc_+3A_g">g</code></td>
<td>
<p>A pdqr-function of any type and class. Represents &quot;true&quot;
distribution of &quot;positive&quot; values.</p>
</td></tr>
<tr><td><code id="summ_roc_+3A_n_grid">n_grid</code></td>
<td>
<p>Number of points of ROC curve to be computed.</p>
</td></tr>
<tr><td><code id="summ_roc_+3A_method">method</code></td>
<td>
<p>Method of computing ROC AUC. Should be one of &quot;expected&quot;,
&quot;pessimistic&quot;, &quot;optimistic&quot; (see Details).</p>
</td></tr>
<tr><td><code id="summ_roc_+3A_roc">roc</code></td>
<td>
<p>A data frame representing ROC curve. Typically an output of
<code>summ_roc()</code>.</p>
</td></tr>
<tr><td><code id="summ_roc_+3A_...">...</code></td>
<td>
<p>Other arguments to be passed to <code>plot()</code> or <code>lines()</code>.</p>
</td></tr>
<tr><td><code id="summ_roc_+3A_add_bisector">add_bisector</code></td>
<td>
<p>If <code>TRUE</code> (default), <code>roc_plot()</code> adds bisector line as
reference for &quot;random guess&quot; classifier.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>ROC curve describes how well classifier performs under different
thresholds. For all possible thresholds two classification metrics are
computed which later form x and y coordinates of a curve:
</p>

<ul>
<li> <p><strong>False positive rate (FPR)</strong>: proportion of &quot;negative&quot; distribution which
was (incorrectly) classified as &quot;positive&quot;. This is the same as one minus
&quot;specificity&quot; (proportion of &quot;negative&quot; values classified as &quot;negative&quot;).
</p>
</li>
<li> <p><strong>True positive rate (TPR)</strong>: proportion of &quot;positive&quot; distribution which
was (correctly) classified as &quot;positive&quot;. This is also called &quot;sensitivity&quot;.
</p>
</li></ul>

<p><code>summ_roc()</code> creates a uniform grid of decreasing <code>n_grid</code> values (so that
output points of ROC curve are ordered from left to right) covering range of
all meaningful thresholds. This range is computed as slightly extended range
of <code>f</code> and <code>g</code> supports (extension is needed to achieve extreme values of
&quot;fpr&quot; in presence of &quot;discrete&quot; type). Then FPR and TPR are computed for
every threshold.
</p>
<p><code>summ_rocauc()</code> computes a common general (without any particular threshold
in mind) diagnostic value of classifier, <strong>area under ROC curve</strong> (&quot;ROC AUC&quot;
or &quot;AUROC&quot;). Numerically it is equal to a probability of random variable with
distribution <em><code>g</code> being strictly greater than <code>f</code></em> plus <em>possible correction
for functions being equal</em>, with multiple ways to account for it. Method
&quot;pessimistic&quot; doesn't add correction, &quot;expected&quot; adds half of probability of
<code>f</code> and <code>g</code> being equal (which is default), &quot;optimistic&quot; adds full
probability. <strong>Note</strong> that this means that correction might be done only if
both input pdqr-functions have &quot;discrete&quot; type. See <a href="#topic+methods-group-generic">pdqr methods for &quot;Ops&quot; group generic family</a> for more details on comparing
functions.
</p>
<p><code>roc_plot()</code> and <code>roc_lines()</code> perform plotting (with
<a href="graphics.html#topic+plot.default">plot()</a>) and adding (with <a href="graphics.html#topic+lines">lines()</a>)
ROC curves respectively.
</p>


<h3>Value</h3>

<p><code>summ_roc()</code> returns a data frame with <code>n_grid</code> rows and columns
&quot;threshold&quot; (grid of classification thresholds, ordered decreasingly), &quot;fpr&quot;,
and &quot;tpr&quot; (corresponding false and true positive rates, ordered
non-decreasingly by &quot;fpr&quot;).
</p>
<p><code>summ_rocauc()</code> returns single number representing area under the ROC curve.
</p>
<p><code>roc_plot()</code> and <code>roc_lines()</code> create plotting side effects.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+summ_separation">summ_separation()</a></code> for computing optimal separation threshold.
</p>
<p>Other summary functions: 
<code><a href="#topic+summ_center">summ_center</a>()</code>,
<code><a href="#topic+summ_classmetric">summ_classmetric</a>()</code>,
<code><a href="#topic+summ_distance">summ_distance</a>()</code>,
<code><a href="#topic+summ_entropy">summ_entropy</a>()</code>,
<code><a href="#topic+summ_hdr">summ_hdr</a>()</code>,
<code><a href="#topic+summ_interval">summ_interval</a>()</code>,
<code><a href="#topic+summ_moment">summ_moment</a>()</code>,
<code><a href="#topic+summ_order">summ_order</a>()</code>,
<code><a href="#topic+summ_prob_true">summ_prob_true</a>()</code>,
<code><a href="#topic+summ_pval">summ_pval</a>()</code>,
<code><a href="#topic+summ_quantile">summ_quantile</a>()</code>,
<code><a href="#topic+summ_separation">summ_separation</a>()</code>,
<code><a href="#topic+summ_spread">summ_spread</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>d_norm_1 &lt;- as_d(dnorm)
d_norm_2 &lt;- as_d(dnorm, mean = 1)
roc &lt;- summ_roc(d_norm_1, d_norm_2)
head(roc)

# `summ_rocauc()` is equivalent to probability of `g &gt; f`
summ_rocauc(d_norm_1, d_norm_2)
summ_prob_true(d_norm_2 &gt; d_norm_1)

# Plotting
roc_plot(roc)
roc_lines(summ_roc(d_norm_2, d_norm_1), col = "blue")

# For "discrete" functions `summ_rocauc()` can produce different outputs
d_dis_1 &lt;- new_d(1:2, "discrete")
d_dis_2 &lt;- new_d(2:3, "discrete")
summ_rocauc(d_dis_1, d_dis_2)
summ_rocauc(d_dis_1, d_dis_2, method = "pessimistic")
summ_rocauc(d_dis_1, d_dis_2, method = "optimistic")
## These methods correspond to different ways of plotting ROC curves
roc &lt;- summ_roc(d_dis_1, d_dis_2)
## Default line plot for "expected" method
roc_plot(roc, main = "Different type of plotting ROC curve")
## Method "pessimistic"
roc_lines(roc, type = "s", col = "blue")
## Method "optimistic"
roc_lines(roc, type = "S", col = "green")
</code></pre>

<hr>
<h2 id='summ_separation'>Summarize distributions with separation threshold</h2><span id='topic+summ_separation'></span>

<h3>Description</h3>

<p>Compute for pair of pdqr-functions the optimal threshold that separates
distributions they represent. In other words, <code>summ_separation()</code> solves a
binary classification problem with one-dimensional linear classifier: values
not more than some threshold are classified as one class, and more than
threshold - as another. Order of input functions doesn't matter.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>summ_separation(f, g, method = "KS", n_grid = 10001)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summ_separation_+3A_f">f</code></td>
<td>
<p>A pdqr-function of any <a href="#topic+meta_type">type</a> and
<a href="#topic+meta_class">class</a>. Represents &quot;true&quot; distribution of &quot;negative&quot; values.</p>
</td></tr>
<tr><td><code id="summ_separation_+3A_g">g</code></td>
<td>
<p>A pdqr-function of any type and class. Represents &quot;true&quot;
distribution of &quot;positive&quot; values.</p>
</td></tr>
<tr><td><code id="summ_separation_+3A_method">method</code></td>
<td>
<p>Separation method. Should be one of &quot;KS&quot; (Kolmogorov-Smirnov),
&quot;GM&quot;, &quot;OP&quot;, &quot;F1&quot;, &quot;MCC&quot; (all four are methods for computing classification
metric in <code><a href="#topic+summ_classmetric">summ_classmetric()</a></code>).</p>
</td></tr>
<tr><td><code id="summ_separation_+3A_n_grid">n_grid</code></td>
<td>
<p>Number of grid points to be used during optimization.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>All methods:
</p>

<ul>
<li><p> Return middle point of nearest support edges in case of non-overlapping or
&quot;touching&quot; supports of <code>f</code> and <code>g</code>.
</p>
</li>
<li><p> Return the smallest optimal solution in case of several candidates.
</p>
</li></ul>

<p>Method &quot;KS&quot; computes &quot;x&quot; value at which corresponding p-functions of <code>f</code> and
<code>g</code> achieve supremum of their absolute difference (so input order of <code>f</code> and
<code>g</code> doesn't matter). If input pdqr-functions have the same
<a href="#topic+meta_type">type</a>, then result is a point of maximum absolute difference.
If inputs have different types, then absolute difference of p-functions at
the result point can be not the biggest. In that case output represents a
left limit of points at which target supremum is reached (see Examples).
</p>
<p>Methods &quot;GM&quot;, &quot;OP&quot;, &quot;F1&quot;, &quot;MCC&quot; compute threshold which maximizes
corresponding <a href="#topic+summ_classmetric">classification metric</a> for best suited
classification setup. They evaluate metrics at equidistant grid (with
<code>n_grid</code> elements) for both directions (<code style="white-space: pre;">&#8288;summ_classmetric(f, g, *)&#8288;</code> and
<code style="white-space: pre;">&#8288;summ_classmetric(g, f, *)&#8288;</code>) and return threshold which results into maximum
of both setups. <strong>Note</strong> that other <code>summ_classmetric()</code> methods are either
useless here (always return one of the edges) or are equivalent to ones
already present.
</p>


<h3>Value</h3>

<p>A single number representing optimal separation threshold.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+summ_roc">summ_roc()</a></code> for computing ROC curve related summaries.
</p>
<p><code><a href="#topic+summ_classmetric">summ_classmetric()</a></code> for computing of classification metric for ordered
classification setup.
</p>
<p>Other summary functions: 
<code><a href="#topic+summ_center">summ_center</a>()</code>,
<code><a href="#topic+summ_classmetric">summ_classmetric</a>()</code>,
<code><a href="#topic+summ_distance">summ_distance</a>()</code>,
<code><a href="#topic+summ_entropy">summ_entropy</a>()</code>,
<code><a href="#topic+summ_hdr">summ_hdr</a>()</code>,
<code><a href="#topic+summ_interval">summ_interval</a>()</code>,
<code><a href="#topic+summ_moment">summ_moment</a>()</code>,
<code><a href="#topic+summ_order">summ_order</a>()</code>,
<code><a href="#topic+summ_prob_true">summ_prob_true</a>()</code>,
<code><a href="#topic+summ_pval">summ_pval</a>()</code>,
<code><a href="#topic+summ_quantile">summ_quantile</a>()</code>,
<code><a href="#topic+summ_roc">summ_roc</a>()</code>,
<code><a href="#topic+summ_spread">summ_spread</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>d_norm_1 &lt;- as_d(dnorm)
d_unif &lt;- as_d(dunif)
summ_separation(d_norm_1, d_unif, method = "KS")
summ_separation(d_norm_1, d_unif, method = "OP")

# Mixed types for "KS" method
p_dis &lt;- new_p(1, "discrete")
p_unif &lt;- as_p(punif)
thres &lt;- summ_separation(p_dis, p_unif)
abs(p_dis(thres) - p_unif(thres))
## Actual difference at `thres` is 0. However, supremum (equal to 1) as
## limit value is # reached there.
x_grid &lt;- seq(0, 1, by = 1e-3)
plot(x_grid, abs(p_dis(x_grid) - p_unif(x_grid)), type = "b")

# Handling of non-overlapping supports
summ_separation(new_d(2, "discrete"), new_d(3, "discrete"))

# The smallest "x" value is returned in case of several optimal thresholds
summ_separation(d_norm_1, d_norm_1) == meta_support(d_norm_1)[1]
</code></pre>

<hr>
<h2 id='summ_spread'>Summarize distribution with spread</h2><span id='topic+summ_spread'></span><span id='topic+summ_sd'></span><span id='topic+summ_var'></span><span id='topic+summ_iqr'></span><span id='topic+summ_mad'></span><span id='topic+summ_range'></span>

<h3>Description</h3>

<p>Functions to compute spread (variability, dispersion) of distribution (i.e.
&quot;how wide it is spread&quot;). <code>summ_spread()</code> is a wrapper for respective
<code style="white-space: pre;">&#8288;summ_*()&#8288;</code> functions (from this page) with default arguments.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>summ_spread(f, method = "sd")

summ_sd(f)

summ_var(f)

summ_iqr(f)

summ_mad(f)

summ_range(f)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summ_spread_+3A_f">f</code></td>
<td>
<p>A pdqr-function representing distribution.</p>
</td></tr>
<tr><td><code id="summ_spread_+3A_method">method</code></td>
<td>
<p>Method of spread computation. Should be one of &quot;sd&quot;, &quot;var&quot;,
&quot;iqr&quot;, &quot;mad&quot;, &quot;range&quot;.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>summ_sd()</code> computes distribution's standard deviation.
</p>
<p><code>summ_var()</code> computes distribution's variance.
</p>
<p><code>summ_iqr()</code> computes distribution's interquartile range. Essentially, it is
a <code>as_q(f)(0.75) - as_q(f)(0.25)</code>.
</p>
<p><code>summ_mad()</code> computes distribution's <em>median</em> absolute deviation around the
distribution's <em>median</em>.
</p>
<p><code>summ_range()</code> computes range length (difference between maximum and minimum)
of &quot;x&quot; values within region of positive probability. <strong>Note</strong> that this might
differ from length of <a href="#topic+meta_support">support</a> because the latter might be
affected by tails with zero probability (see Examples).
</p>


<h3>Value</h3>

<p>All functions return a single number representing a spread of
distribution.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+summ_center">summ_center()</a></code> for computing distribution's center, <code><a href="#topic+summ_moment">summ_moment()</a></code>
for general moments.
</p>
<p>Other summary functions: 
<code><a href="#topic+summ_center">summ_center</a>()</code>,
<code><a href="#topic+summ_classmetric">summ_classmetric</a>()</code>,
<code><a href="#topic+summ_distance">summ_distance</a>()</code>,
<code><a href="#topic+summ_entropy">summ_entropy</a>()</code>,
<code><a href="#topic+summ_hdr">summ_hdr</a>()</code>,
<code><a href="#topic+summ_interval">summ_interval</a>()</code>,
<code><a href="#topic+summ_moment">summ_moment</a>()</code>,
<code><a href="#topic+summ_order">summ_order</a>()</code>,
<code><a href="#topic+summ_prob_true">summ_prob_true</a>()</code>,
<code><a href="#topic+summ_pval">summ_pval</a>()</code>,
<code><a href="#topic+summ_quantile">summ_quantile</a>()</code>,
<code><a href="#topic+summ_roc">summ_roc</a>()</code>,
<code><a href="#topic+summ_separation">summ_separation</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Type "continuous"
d_norm &lt;- as_d(dnorm)
## The same as `summ_spread(d_norm, method = "sd")`
summ_sd(d_norm)
summ_var(d_norm)
summ_iqr(d_norm)
summ_mad(d_norm)
summ_range(d_norm)

# Type "discrete"
d_pois &lt;- as_d(dpois, lambda = 10)
summ_sd(d_pois)
summ_var(d_pois)
summ_iqr(d_pois)
summ_mad(d_pois)
summ_range(d_pois)

# Difference of `summ_range(f)` and `diff(meta_support(f))`
zero_tails &lt;- new_d(data.frame(x = 1:5, y = c(0, 0, 1, 0, 0)), "continuous")
## This returns difference between 5 and 1
diff(meta_support(zero_tails))
## This returns difference between 2 and 4 as there are zero-probability
## tails
summ_range(zero_tails)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
