<!DOCTYPE html><html lang="en"><head><title>Help for package mcca</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {mcca}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#mcca-package'>
<p>Diagnostic accuracy methods for classifiers</p></a></li>
<li><a href='#ccp'><p>Calculate CCP Value</p></a></li>
<li><a href='#estp'><p>Inference for Accuracy Improvement Measures based on Bootstrap</p></a></li>
<li><a href='#ests'><p>Inference for Accuracy Measures based on Bootstrap</p></a></li>
<li><a href='#hum'><p>Calculate HUM Value</p></a></li>
<li><a href='#idi'><p>Calculate IDI Value</p></a></li>
<li><a href='#nri'><p>Calculate NRI Value</p></a></li>
<li><a href='#pdi'><p>Calculate PDI Value</p></a></li>
<li><a href='#plot.mcca.hum'><p>Plot 3D ROC surface</p></a></li>
<li><a href='#pm'><p>Calculate Probability Matrix</p></a></li>
<li><a href='#print.mcca.ccp'><p>Print Method for mcca ccp class</p></a></li>
<li><a href='#print.mcca.hum'><p>Print Method for mcca hum class</p></a></li>
<li><a href='#print.mcca.pdi'><p>Print Method for mcca pdi class</p></a></li>
<li><a href='#print.mcca.rsq'><p>Print Method for mcca rsq class</p></a></li>
<li><a href='#rsq'><p>Calculate RSQ Value</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Multi-Category Classification Accuracy</td>
</tr>
<tr>
<td>Version:</td>
<td>0.7.0</td>
</tr>
<tr>
<td>Author:</td>
<td>Ming Gao, Jialiang Li</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Ming Gao &lt;gaoming@umich.edu&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>It contains six common multi-category classification accuracy evaluation measures.
 All of these measures could be found in Li and Ming (2019) &lt;<a href="https://doi.org/10.1002%2Fsim.8103">doi:10.1002/sim.8103</a>&gt;. Specifically,
 Hypervolume Under Manifold (HUM), described in
 Li and Fine (2008) &lt;<a href="https://doi.org/10.1093%2Fbiostatistics%2Fkxm050">doi:10.1093/biostatistics/kxm050</a>&gt;.
 Correct Classification Percentage (CCP), Integrated Discrimination Improvement (IDI), Net Reclassification Improvement (NRI), R-Squared Value (RSQ), described in
 Li, Jiang and Fine (2013) &lt;<a href="https://doi.org/10.1093%2Fbiostatistics%2Fkxs047">doi:10.1093/biostatistics/kxs047</a>&gt;.
 Polytomous Discrimination Index (PDI), described in
 Van Calster et al. (2012) &lt;<a href="https://doi.org/10.1007%2Fs10654-012-9733-3">doi:10.1007/s10654-012-9733-3</a>&gt;.
 Li et al. (2018) &lt;<a href="https://doi.org/10.1177%2F0962280217692830">doi:10.1177/0962280217692830</a>&gt;.
 We described all these above measures and our mcca package in
 Li, Gao and D'Agostino (2019) &lt;<a href="https://doi.org/10.1002%2Fsim.8103">doi:10.1002/sim.8103</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL]</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>Imports:</td>
<td>nnet,rpart,e1071,MASS,stats,pROC,caret,rgl</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/gaoming96/mcca">https://github.com/gaoming96/mcca</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/gaoming96/mcca/issues">https://github.com/gaoming96/mcca/issues</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2019-12-19 22:23:09 UTC; gaoming</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2019-12-20 06:00:08 UTC</td>
</tr>
</table>
<hr>
<h2 id='mcca-package'>
Diagnostic accuracy methods for classifiers
</h2><span id='topic+mcca-package'></span><span id='topic+mcca'></span>

<h3>Description</h3>

<p>Six common multi-category classification accuracy evaluation measures are included i.e., Correct Classification Percentage (CCP), Hypervolume Under Manifold (HUM), Integrated Discrimination Improvement (IDI), Net Reclassification Improvement (NRI), Polytomous Discrimination Index (PDI) and R-squared (RSQ). It allows users to fit many popular classification procedures, such
as multinomial logistic regression, support vector machine, classification tree, and user computed
risk values.
</p>


<h3>Details</h3>


<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> mcca</td>
</tr>
<tr>
 <td style="text-align: left;">
Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
Version: </td><td style="text-align: left;"> 0.6</td>
</tr>
<tr>
 <td style="text-align: left;">
Date: </td><td style="text-align: left;"> 2019-08-05</td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> GPL</td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>



<h3>Functions</h3>


<table>
<tr>
 <td style="text-align: left;">
    <code><a href="#topic+ccp">ccp</a></code> </td><td style="text-align: left;"> Calculate CCP Value </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code><a href="#topic+hum">hum</a></code> </td><td style="text-align: left;"> Calculate HUM Value </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code><a href="#topic+plot.mcca.hum">plot.mcca.hum</a></code> </td><td style="text-align: left;"> Plot 3D ROC curve </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code><a href="#topic+idi">idi</a></code> </td><td style="text-align: left;"> Calculate IDI Value </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code><a href="#topic+nri">nri</a></code> </td><td style="text-align: left;"> Calculate NRI Value </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code><a href="#topic+pdi">pdi</a></code> </td><td style="text-align: left;"> Calculate PDI Value </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code><a href="#topic+rsq">rsq</a></code> </td><td style="text-align: left;"> Calculate RSQ Value </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code><a href="#topic+pm">pm</a></code> </td><td style="text-align: left;"> Calculate Probability Matrix </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code><a href="#topic+ests">ests</a></code> </td><td style="text-align: left;"> Estimated Information for Single Model Evaluation Value </td>
</tr>
<tr>
 <td style="text-align: left;">
    <code><a href="#topic+estp">estp</a></code> </td><td style="text-align: left;"> Estimated Information for Paired Model Evaluation Value </td>
</tr>
<tr>
 <td style="text-align: left;">

  </td>
</tr>

</table>



<h3>Installing and using</h3>

<p>To install this package, make sure you are connected to the internet and issue the following command in the R prompt:
</p>
<pre>
    install.packages("mcca")
  </pre>
<p>To load the package in R:
</p>
<pre>
    library(mcca)
  </pre>


<h3>Citation</h3>

<p>Li J, Gao M, D'Agostino R. Evaluating classification accuracy for modern learning
approaches. Statistics in Medicine. 2019;1-27. https://doi.org/10.1002/sim.8103
</p>


<h3>Author(s)</h3>

<p>Ming Gao, Jialiang Li
</p>
<p>Maintainer: Ming Gao &lt;gaoming@umich.edu&gt;
</p>


<h3>References</h3>

<p>Li, J., Gao, M., D’Agostino, R. (2019). Evaluating Classification Accuracy for Modern Learning Approaches. Statistics in Medicine (Tutorials in Biostatistics). 38(13): 2477-2503.
</p>
<p>Li, Ming G., D'Agostino. (2019). Evaluating Classification Accuracy for Modern Learning Approaches. Statistics in Medicine.
</p>
<p>Li, J. and Fine, J. P. (2008): ROC analysis with multiple tests and multiple classes: methodology and applications in microarray studies. Biostatistics. 9 (3): 566-576.
</p>
<p>Li, J., Chow, Y., Wong, W.K., and Wong, T.Y. (2014). Sorting Multiple Classes in Multi-dimensional ROC Analysis: Parametric and Nonparametric Approaches. Biomarkers. 19(1): 1-8.
</p>
<p>Li, J., Jiang, B. and Fine, J. P. (2013). Multicategory reclassification statistics for assessing Improvements in diagnostic accuracy. Biostatistics. 14(2): 382-394.
</p>
<p>Li, J., Jiang, B., and Fine, J. P. (2013). Letter to Editor: Response. Biostatistics. 14(4): 809-810.
</p>
<p>Van Calster B, Vergouwe Y, Looman CWN, Van Belle
V, Timmerman D and Steyerberg EW. Assessing the
discriminative ability of risk models for more than two outcome
categories. European Journal of Epidemiology 2012; 27: 761 C
770.
</p>
<p>Li, J., Feng, Q., Fine, J.P., Pencina, M.J., Van Calster, B. (2018). Nonparametric estimation and inference for polytomous discrimination index. Statistical Methods in Medical Research. 27(10): 3092—3103.
</p>


<h3>See Also</h3>

<p>CRAN packages <span class="pkg">HUM</span> for HUM.
</p>
<p>CRAN packages <span class="pkg">nnet</span>, <span class="pkg">rpart</span>, <span class="pkg">e1071</span>,  <span class="pkg">MASS</span> employed in this package.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
str(iris)
data &lt;- iris[, 1:4]
label &lt;- iris[, 5]
ccp(y = label, d = data, method = "multinom",maxit = 1000,MaxNWts = 2000,trace=FALSE)
ccp(y = label, d = data, method = "multinom")
ccp(y = label, d = data, method = "svm")
ccp(y = label, d = data, method = "svm",kernel="sigmoid",cost=4,scale=TRUE,coef0=0.5)
ccp(y = label, d = data, method = "tree")
p = as.numeric(label)
ccp(y = label, d = p, method = "label")
hum(y = label, d = data,method = "multinom")
hum(y = label, d = data,method = "svm")
hum(y = label, d = data,method = "svm",kernel="linear",cost=4,scale=TRUE)
hum(y = label, d = data, method = "tree")
ests(y = label, d = data,acc="hum",level=0.95,method = "multinom",trace=FALSE)

## $value
## [1] 0.9972

## $sd
## [1] 0.002051529

## $interval
## [1] 0.9935662 1.0000000
</code></pre>

<hr>
<h2 id='ccp'>Calculate CCP Value</h2><span id='topic+ccp'></span>

<h3>Description</h3>

<p>compute the Correct Classification Percentage (CCP) value of two or three or four categories classifiers with an option to define the specific model or user-defined model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ccp(y, d, method="multinom", ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ccp_+3A_y">y</code></td>
<td>
<p>The multinomial response vector with two, three or four categories.</p>
</td></tr>
<tr><td><code id="ccp_+3A_d">d</code></td>
<td>
<p>The set of candidate markers, including one or more columns. Can be a data frame or a matrix; if the method is &quot;label&quot;, then d should be the label vector.</p>
</td></tr>
<tr><td><code id="ccp_+3A_method">method</code></td>
<td>
<p>Specifies what method is used to construct the classifier based on the marker set in d. Available option includes the following methods:&quot;multinom&quot;: Multinomial Logistic Regression which is the default method, requiring R package nnet;&quot;tree&quot;: Classification Tree method, requiring R package rpart; &quot;svm&quot;: Support Vector Machine (C-classification and radial basis as default), requiring R package e1071; &quot;lda&quot;: Linear Discriminant Analysis, requiring R package lda; &quot;label&quot;: d is a label vector resulted from any external classification algorithm obtained by the user, should be encoded from 1; &quot;prob&quot;: d is a probability matrix resulted from any external classification algorithm obtained by the user.</p>
</td></tr>
<tr><td><code id="ccp_+3A_...">...</code></td>
<td>
<p>Additional arguments in the chosen method's function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function returns the CCP value for predictive markers based on a user-chosen machine learning method. Currently available methods include logistic regression (default), tree, lda, svm and user-computed risk values. This function is general since we can evaluate the accuracy for marker combinations resulted from complicated classification algorithms.
</p>


<h3>Value</h3>

<p>Returns an object of class &quot;mcca.ccp&quot;. The CCP value of the classification using a particular learning method on a set of marker(s).
</p>
<p>An object of class &quot;mcca.ccp&quot; is a list containing at least the following components:
</p>
<table role = "presentation">
<tr><td><code>call</code></td>
<td>
<p>the matched call.</p>
</td></tr>
<tr><td><code>measure</code></td>
<td>
<p>the value of measure.</p>
</td></tr>
<tr><td><code>table</code></td>
<td>
<p>the category-specific value of measure.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Users are advised to change the operating settings of various classifiers since it is well known that machine learning methods require extensive tuning. Currently only some common and intuitive options are set as default and they are by no means the optimal parameterization for a particular data analysis. Users can put machine learning methods' parameters after tuning. A more flexible evaluation is to consider &quot;method=label&quot; in which case the input d should be a label vector.
</p>


<h3>Author(s)</h3>

<p>Ming Gao: gaoming@umich.edu
</p>
<p>Jialiang Li: stalj@nus.edu.sg
</p>


<h3>References</h3>

<p>Li, J., Gao, M., D’Agostino, R. (2019). Evaluating Classification Accuracy for Modern Learning Approaches. Statistics in Medicine (Tutorials in Biostatistics). 38(13): 2477-2503.
</p>
<p>Li, J., Jiang, B. and Fine, J. P. (2013). Multicategory reclassification statistics for assessing Improvements in diagnostic accuracy. Biostatistics. 14(2): 382-394.
</p>
<p>Li, J., Jiang, B., and Fine, J. P. (2013). Letter to Editor: Response. Biostatistics. 14(4): 809-810.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pdi">pdi</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>str(iris)
data &lt;- iris[, 1:4]
label &lt;- iris[, 5]
ccp(y = label, d = data, method = "multinom",maxit = 1000,MaxNWts = 2000,trace=FALSE)

## Call:
## ccp(y = label, d = data, method = "multinom", maxit = 1000,
##     MaxNWts = 2000, trace = FALSE)

## Overall Correct Classification Probability:
##  0.9866667

## Category-specific Correct Classification Probability:
##   CATEGORIES VALUES PREVALENCE
## 1     setosa   1.00  0.3333333
## 2 versicolor   0.98  0.3333333
## 3  virginica   0.98  0.3333333

ccp(y = label, d = data, method = "multinom")
ccp(y = label, d = data, method = "svm")
ccp(y = label, d = data, method = "svm",kernel="sigmoid",cost=4,scale=TRUE,coef0=0.5)
ccp(y = label, d = data, method = "tree")

p = as.numeric(label)
ccp(y = label, d = p, method = "label")


table(mtcars$carb)
for (i in (1:length(mtcars$carb))) {
  if (mtcars$carb[i] == 3 | mtcars$carb[i] == 6 | mtcars$carb[i] == 8) {
    mtcars$carb_new[i] = 9
  }else{
    mtcars$carb_new[i] = mtcars$carb[i]
  }
}
data &lt;- data.matrix(mtcars[, c(1)])
mtcars$carb_new &lt;- factor(mtcars$carb_new)
label &lt;- mtcars$carb_new
str(mtcars)
ccp(y = as.numeric(label), d = data, method = "svm",kernel="radial",cost=1,scale=TRUE)

## Call:
## ccp(y = as.numeric(label), d = data, method = "svm", kernel = "radial", cost = 1, scale = TRUE)

## Overall Correct Classification Probability:
##  0.4375

## Category-specific Correct Classification Probability:
##   CATEGORIES    VALUES PREVALENCE
## 1          1 0.5714286    0.21875
## 2          2 0.2000000    0.31250
## 3          3 0.8000000    0.31250
## 4          4 0.0000000    0.15625

</code></pre>

<hr>
<h2 id='estp'>Inference for Accuracy Improvement Measures based on Bootstrap</h2><span id='topic+estp'></span>

<h3>Description</h3>

<p>compute the bootstrap standard error and confidence interval for the classification accuracy improvement for a pair of nested models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>estp(y, m1, m2, acc="idi", level=0.95, method="multinom", B=250, balance=FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="estp_+3A_y">y</code></td>
<td>
<p>The multinomial response vector with two, three or four categories. It can be factor or integer-valued.</p>
</td></tr>
<tr><td><code id="estp_+3A_m1">m1</code></td>
<td>
<p>The set of marker(s) included in the baseline model, can be a data frame or a matrix; if the method is &quot;prob&quot;, then m1 should be the prediction probablity matrix of the baseline model.</p>
</td></tr>
<tr><td><code id="estp_+3A_m2">m2</code></td>
<td>
<p>The set of additional marker(s) included in the improved model, can be a data frame or a matrix; if the method is &quot;prob&quot;, then m2 should be the prediction probablity matrix of the improved model.</p>
</td></tr>
<tr><td><code id="estp_+3A_acc">acc</code></td>
<td>
<p>Accuracy measure to be evaluated. Allow two choices: &quot;idi&quot;, &quot;nri&quot;.</p>
</td></tr>
<tr><td><code id="estp_+3A_level">level</code></td>
<td>
<p>The confidence level. Default value is 0.95.</p>
</td></tr>
<tr><td><code id="estp_+3A_method">method</code></td>
<td>
<p>Specifies what method is used to construct the classifier based on the marker set in m1 &amp; m2. Available option includes the following methods:&quot;multinom&quot;: Multinomial Logistic Regression which is the default method, requiring R package nnet;&quot;tree&quot;: Classification Tree method, requiring R package rpart;&quot;svm&quot;: Support Vector Machine (C-classification and radial basis as default), requiring R package e1071;&quot;lda&quot;: Linear Discriminant Analysis, requiring R package lda;&quot;prob&quot;: m1 &amp; m2 are risk matrices resulted from any external classification algorithm obtained by the user.</p>
</td></tr>
<tr><td><code id="estp_+3A_b">B</code></td>
<td>
<p>Number of bootstrap resamples.</p>
</td></tr>
<tr><td><code id="estp_+3A_balance">balance</code></td>
<td>
<p>Logical, if TRUE, the class prevalence of the bootstrap sample is forced to be identical to the class prevalence of the original sample. Otherwise the prevalence of the bootstrap sample may be random.</p>
</td></tr>
<tr><td><code id="estp_+3A_...">...</code></td>
<td>
<p>Additional arguments in the chosen method's function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function returns the standard error and confidence interval for a paired model evaluation method. All the other arguments are the same as the function <code><a href="#topic+hum">hum</a></code>.
</p>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>value</code></td>
<td>
<p>The specific value of the classification using a particular learning method on a set of marker(s).</p>
</td></tr>
<tr><td><code>se</code></td>
<td>
<p>The standard error of the value.</p>
</td></tr>
<tr><td><code>interval</code></td>
<td>
<p>The confidence interval of the value.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Users are advised to change the operating settings of various classifiers since it is well known that machine learning methods require extensive tuning. Currently only some common and intuitive options are set as default and they are by no means the optimal parameterization for a particular data analysis. Users can put machine learning methods' parameters after tuning. A more flexible evaluation is to consider &quot;method=prob&quot; in which case the input m1 &amp; m2 should be a matrix of membership probabilities with k columns and each row of m1 &amp; m2 should sum to one.
</p>


<h3>Author(s)</h3>

<p>Ming Gao: gaoming@umich.edu
</p>
<p>Jialiang Li: stalj@nus.edu.sg
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ests">ests</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>table(mtcars$carb)
for (i in (1:length(mtcars$carb))) {
  if (mtcars$carb[i] == 3 | mtcars$carb[i] == 6 | mtcars$carb[i] == 8) {
    mtcars$carb_new[i] = 9
  }else{
    mtcars$carb_new[i] = mtcars$carb[i]
  }
}
data &lt;- data.matrix(mtcars[, c(1,2)])
mtcars$carb_new &lt;- factor(mtcars$carb_new)
label &lt;- mtcars$carb_new
str(mtcars)
estp(y = label, m1 = data[, 1], m2 = data[, 2], acc="idi",method="lda", B=10)

## $value
## [1] 0.1235644

## $se
## [1] 0.07053541

## $interval
## [1] 0.05298885 0.21915088

estp(y = label, m1 = data[, 1], m2 = data[, 2], acc="nri",method="tree",B=5)

## $value
## [1] 0.05

## $se
## [1] 0.09249111

## $interval
## [1] 0.0000000  0.1458333

</code></pre>

<hr>
<h2 id='ests'>Inference for Accuracy Measures based on Bootstrap</h2><span id='topic+ests'></span>

<h3>Description</h3>

<p>compute the bootstrap standard error and confidence interval for the classification accuracy for a single classification model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ests(y, d, acc="hum", level=0.95, method="multinom", B=250, balance=FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ests_+3A_y">y</code></td>
<td>
<p>The multinomial response vector with two, three or four categories. It can be factor or integer-valued.</p>
</td></tr>
<tr><td><code id="ests_+3A_d">d</code></td>
<td>
<p>The set of candidate markers, including one or more columns. Can be a data frame or a matrix; if the method is &quot;prob&quot;, then d should be the probability matrix.</p>
</td></tr>
<tr><td><code id="ests_+3A_acc">acc</code></td>
<td>
<p>Accuracy measure to be evaluated. Allow four choices: &quot;hum&quot;, &quot;pdi&quot;, &quot;ccp&quot; and &quot;rsq&quot;.</p>
</td></tr>
<tr><td><code id="ests_+3A_level">level</code></td>
<td>
<p>The confidence level. Default value is 0.95.</p>
</td></tr>
<tr><td><code id="ests_+3A_method">method</code></td>
<td>
<p>Specifies what method is used to construct the classifier based on the marker set in d. Available option includes the following methods:&quot;multinom&quot;: Multinomial Logistic Regression which is the default method, requiring R package nnet;&quot;tree&quot;: Classification Tree method, requiring R package rpart; &quot;svm&quot;: Support Vector Machine (C-classification and radial basis as default), requiring R package e1071; &quot;lda&quot;: Linear Discriminant Analysis, requiring R package lda; &quot;label&quot;: d is a label vector resulted from any external classification algorithm obtained by the user, should be encoded from 1; &quot;prob&quot;: d is a probability matrix resulted from any external classification algorithm obtained by the user.</p>
</td></tr>
<tr><td><code id="ests_+3A_b">B</code></td>
<td>
<p>Number of bootstrap resamples.</p>
</td></tr>
<tr><td><code id="ests_+3A_balance">balance</code></td>
<td>
<p>Logical, if TRUE, the class prevalence of the bootstrap sample is forced to be identical to the class prevalence of the original sample. Otherwise the prevalence of the bootstrap sample may be random.</p>
</td></tr>
<tr><td><code id="ests_+3A_...">...</code></td>
<td>
<p>Additional arguments in the chosen method's function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function returns the standard error and confidence interval for a single model evaluation method. All the other arguments are the same as the function <code><a href="#topic+hum">hum</a></code>.
</p>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>value</code></td>
<td>
<p>The specific value of the classification using a particular learning method on a set of marker(s).</p>
</td></tr>
<tr><td><code>se</code></td>
<td>
<p>The standard error of the value.</p>
</td></tr>
<tr><td><code>interval</code></td>
<td>
<p>The confidence interval of the value.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Users are advised to change the operating settings of various classifiers since it is well known that machine learning methods require extensive tuning. Currently only some common and intuitive options are set as default and they are by no means the optimal parameterization for a particular data analysis. Users can put machine learning methods' parameters after tuning. A more flexible evaluation is to consider &quot;method=prob&quot; in which case the input d should be a matrix of membership probabilities with k columns and each row of d should sum to one.
</p>


<h3>Author(s)</h3>

<p>Ming Gao: gaoming@umich.edu
</p>
<p>Jialiang Li: stalj@nus.edu.sg
</p>


<h3>See Also</h3>

<p><code><a href="#topic+estp">estp</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>str(iris)
data &lt;- iris[, 1:4]
label &lt;- iris[, 5]
ests(y = label, d = data,acc="hum",level=0.95,method = "multinom",B=10,trace=FALSE)

## $value
## [1] 0.9972

## $se
## [1] 0.002051529

## $interval
## [1] 0.9935662 1.0000000

ests(y = label, d = data,acc="pdi",level=0.85,method = "tree",B=10)

## $value
## [1] 0.9213333

## $se
## [1] 0.02148812

## $interval
## [1] 0.9019608 0.9629630

table(mtcars$carb)
for (i in (1:length(mtcars$carb))) {
  if (mtcars$carb[i] == 3 | mtcars$carb[i] == 6 | mtcars$carb[i] == 8) {
    mtcars$carb_new[i] = 9
  }else{
    mtcars$carb_new[i] = mtcars$carb[i]
  }
}
data &lt;- data.matrix(mtcars[, c(1:2)])
mtcars$carb_new &lt;- factor(mtcars$carb_new)
label &lt;- mtcars$carb_new
str(mtcars)

ests(y = label, d = data,acc="hum",level=0.95,method = "multinom",trace=FALSE,B=5)

## $value
## [1] 0.2822857

## $se
## [1] 0.170327

## $interval
## [1] 0.2662500 0.4494643

</code></pre>

<hr>
<h2 id='hum'>Calculate HUM Value</h2><span id='topic+hum'></span>

<h3>Description</h3>

<p>compute the Hypervolume Under Manifold (HUM) value of two or three or four categories classifiers with an option to define the specific model or user-defined model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hum(y, d, method="multinom", ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="hum_+3A_y">y</code></td>
<td>
<p>The multinomial response vector with two, three or four categories. It can be factor or integer-valued.</p>
</td></tr>
<tr><td><code id="hum_+3A_d">d</code></td>
<td>
<p>The set of candidate markers, including one or more columns. Can be a data frame or a matrix; if the method is &quot;prob&quot;, then d should be the probablity matrix.</p>
</td></tr>
<tr><td><code id="hum_+3A_method">method</code></td>
<td>
<p>Specifies what method is used to construct the classifier based on the marker set in d. Available option includes the following methods:&quot;multinom&quot;: Multinomial Logistic Regression which is the default method, requiring R package nnet;&quot;tree&quot;: Classification Tree method, requiring R package rpart;&quot;svm&quot;: Support Vector Machine (C-classification and radial basis as default), requiring R package e1071;&quot;lda&quot;: Linear Discriminant Analysis, requiring R package lda;&quot;prob&quot;: d is a risk matrix resulted from any external classification algorithm obtained by the user.</p>
</td></tr>
<tr><td><code id="hum_+3A_...">...</code></td>
<td>
<p>Additional arguments in the chosen method's function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function returns the HUM value for predictive markers based on a user-chosen machine learning method. Currently available methods include logistic regression (default), tree, lda, svm and user-computed risk values. For binary outcome, one can use AUC value (HUM reduces to AUC in such case). This function is more general than the package HUM, since we can evaluate the accuracy for marker combinations resulted from complicated classification algorithms.
</p>


<h3>Value</h3>

<p>The HUM value of the classification using a particular learning method on a set of marker(s).
</p>


<h3>Note</h3>

<p>Users are advised to change the operating settings of various classifiers since it is well known that machine learning methods require extensive tuning. Currently only some common and intuitive options are set as default and they are by no means the optimal parameterization for a particular data analysis. Users can put machine learning methods' parameters after tuning. A more flexible evaluation is to consider &quot;method=prob&quot; in which case the input d should be a matrix of membership probabilities with k columns and each row of d should sum to one.
</p>


<h3>Author(s)</h3>

<p>Ming Gao: gaoming@umich.edu
</p>
<p>Jialiang Li: stalj@nus.edu.sg
</p>


<h3>References</h3>

<p>Li, J., Gao, M., D’Agostino, R. (2019). Evaluating Classification Accuracy for Modern Learning Approaches. Statistics in Medicine (Tutorials in Biostatistics). 38(13): 2477-2503.
</p>
<p>Li, J. and Fine, J. P. (2008): ROC analysis with multiple tests and multiple classes: methodology and applications in microarray studies. Biostatistics. 9 (3): 566-576.
</p>
<p>Li, J., Chow, Y., Wong, W.K., and Wong, T.Y. (2014). Sorting Multiple Classes in Multi-dimensional ROC Analysis: Parametric and Nonparametric Approaches. Biomarkers. 19(1): 1-8.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pdi">pdi</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>str(iris)
data &lt;- iris[, 1:4]
label &lt;- iris[, 5]
hum(y = label, d = data,method = "multinom")
## [1] 0.9972
hum(y = label, d = data,method = "svm")
## [1] 0.9964
hum(y = label, d = data,method = "svm",type="C",kernel="linear",cost=4,scale=TRUE)
## [1] 0.9972
hum(y = label, d = data, method = "tree")
## [1] 0.998

data &lt;- data.matrix(iris[, 1:4])
label &lt;- as.numeric(iris[, 5])
# multinomial
require(nnet)
# model
fit &lt;- multinom(label ~ data, maxit = 1000, MaxNWts = 2000)
predict.probs &lt;- predict(fit, type = "probs")
pp&lt;- data.frame(predict.probs)
# extract the probablity assessment vector
head(pp)
hum(y = label, d = pp, method = "prob")
## [1] 0.9972

table(mtcars$carb)
for (i in (1:length(mtcars$carb))) {
  if (mtcars$carb[i] == 3 | mtcars$carb[i] == 6 | mtcars$carb[i] == 8) {
    mtcars$carb_new[i] = 9
  }else{
    mtcars$carb_new[i] = mtcars$carb[i]
  }
}
data &lt;- data.matrix(mtcars[, c(1:10)])
mtcars$carb_new &lt;- factor(mtcars$carb_new)
label &lt;- mtcars$carb_new
str(mtcars)

hum(y = label, d = data, method = "tree",control = rpart::rpart.control(minsplit = 5))
## [1] 1
hum(y = label, d = data, method = "svm",kernel="linear",cost=0.7,scale=TRUE)
## [1] 1
hum(y = label, d = data, method = "svm", kernel ="radial",cost=0.7,scale=TRUE)
## [1] 0.53
</code></pre>

<hr>
<h2 id='idi'>Calculate IDI Value</h2><span id='topic+idi'></span>

<h3>Description</h3>

<p>compute the integrated discrimination improvement (IDI) value of two or three or four categories classifiers with an option to define the specific model or user-defined model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>idi(y, m1, m2, method="multinom", ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="idi_+3A_y">y</code></td>
<td>
<p>The multinomial response vector with two, three or four categories. It can be factor or integer-valued.</p>
</td></tr>
<tr><td><code id="idi_+3A_m1">m1</code></td>
<td>
<p>The set of marker(s) included in the baseline model, can be a data frame or a matrix; if the method is &quot;prob&quot;, then m1 should be the prediction probablity matrix of the baseline model.</p>
</td></tr>
<tr><td><code id="idi_+3A_m2">m2</code></td>
<td>
<p>The set of additional marker(s) included in the improved model, can be a data frame or a matrix; if the method is &quot;prob&quot;, then m2 should be the prediction probablity matrix of the improved model.</p>
</td></tr>
<tr><td><code id="idi_+3A_method">method</code></td>
<td>
<p>Specifies what method is used to construct the classifier based on the marker set in m1 &amp; m2. Available option includes the following methods:&quot;multinom&quot;: Multinomial Logistic Regression which is the default method, requiring R package nnet;&quot;tree&quot;: Classification Tree method, requiring R package rpart;&quot;svm&quot;: Support Vector Machine (C-classification and radial basis as default), requiring R package e1071;&quot;lda&quot;: Linear Discriminant Analysis, requiring R package lda;&quot;prob&quot;: m1 &amp; m2 are risk matrices resulted from any external classification algorithm obtained by the user.</p>
</td></tr>
<tr><td><code id="idi_+3A_...">...</code></td>
<td>
<p>Additional arguments in the chosen method's function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function returns the IDI value for predictive markers based on a user-chosen machine learning method. Currently available methods include logistic regression (default), tree, lda, svm and user-computed risk values. This function is general since we can evaluate the accuracy for marker combinations resulted from complicated classification algorithms.
</p>


<h3>Value</h3>

<p>The IDI value of the classification using a particular learning method on a set of marker(s).
</p>


<h3>Note</h3>

<p>Users are advised to change the operating settings of various classifiers since it is well known that machine learning methods require extensive tuning. Currently only some common and intuitive options are set as default and they are by no means the optimal parameterization for a particular data analysis. Users can put machine learning methods' parameters after tuning. A more flexible evaluation is to consider &quot;method=prob&quot; in which case the input m1 &amp; m2 should be a matrix of membership probabilities with k columns and each row of m1 &amp; m2 should sum to one.
</p>


<h3>Author(s)</h3>

<p>Ming Gao: gaoming@umich.edu
</p>
<p>Jialiang Li: stalj@nus.edu.sg
</p>


<h3>References</h3>

<p>Li, J., Gao, M., D’Agostino, R. (2019). Evaluating Classification Accuracy for Modern Learning Approaches. Statistics in Medicine (Tutorials in Biostatistics). 38(13): 2477-2503.
</p>
<p>Li, J., Jiang, B. and Fine, J. P. (2013). Multicategory reclassification statistics for assessing Improvements in diagnostic accuracy. Biostatistics. 14(2): 382—394.
</p>
<p>Li, J., Jiang, B., and Fine, J. P. (2013). Letter to Editor: Response. Biostatistics. 14(4): 809-810.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+nri">nri</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>table(mtcars$carb)
for (i in (1:length(mtcars$carb))) {
  if (mtcars$carb[i] == 3 | mtcars$carb[i] == 6 | mtcars$carb[i] == 8) {
    mtcars$carb_new[i] = 9
  }else{
    mtcars$carb_new[i] = mtcars$carb[i]
  }
}
data &lt;- data.matrix(mtcars[, c(1,5)])
mtcars$carb_new &lt;- factor(mtcars$carb_new)
label &lt;- mtcars$carb_new
str(mtcars)

idi(y = label, m1 = data[, 1], m2 = data[, 2], "tree")
## [1] 0.09979413
idi(y = label, m1 = data[, 1], m2 = data[, 2], "tree",control=rpart::rpart.control(minsplit=4))
## [1] 0.2216707

</code></pre>

<hr>
<h2 id='nri'>Calculate NRI Value</h2><span id='topic+nri'></span>

<h3>Description</h3>

<p>compute the net reclassification improvement (NRI) value of two or three or four categories classifiers with an option to define the specific model or user-defined model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nri(y, m1, m2, method="multinom", ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="nri_+3A_y">y</code></td>
<td>
<p>The multinomial response vector with two, three or four categories. It can be factor or integer-valued.</p>
</td></tr>
<tr><td><code id="nri_+3A_m1">m1</code></td>
<td>
<p>The set of marker(s) included in the baseline model, can be a data frame or a matrix; if the method is &quot;prob&quot;, then m1 should be the prediction probablity matrix of the baseline model.</p>
</td></tr>
<tr><td><code id="nri_+3A_m2">m2</code></td>
<td>
<p>The set of additional marker(s) included in the improved model, can be a data frame or a matrix; if the method is &quot;prob&quot;, then m2 should be the prediction probablity matrix of the improved model.</p>
</td></tr>
<tr><td><code id="nri_+3A_method">method</code></td>
<td>
<p>Specifies what method is used to construct the classifier based on the marker set in m1 &amp; m2. Available option includes the following methods:&quot;multinom&quot;: Multinomial Logistic Regression which is the default method, requiring R package nnet;&quot;tree&quot;: Classification Tree method, requiring R package rpart;&quot;svm&quot;: Support Vector Machine (C-classification and radial basis as default), requiring R package e1071;&quot;lda&quot;: Linear Discriminant Analysis, requiring R package lda;&quot;label&quot;: m1 &amp; m2 are label vectors resulted from any external classification algorithm obtained by the user;&quot;prob&quot;: m1 &amp; m2 are probability matrices resulted from any external classification algorithm obtained by the user.</p>
</td></tr>
<tr><td><code id="nri_+3A_...">...</code></td>
<td>
<p>Additional arguments in the chosen method's function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function returns the NRI value for predictive markers based on a user-chosen machine learning method. Currently available methods include logistic regression (default), tree, lda, svm and user-computed risk values. This function is general since we can evaluate the accuracy for marker combinations resulted from complicated classification algorithms.
</p>


<h3>Value</h3>

<p>The NRI value of the classification using a particular learning method on a set of marker(s).
</p>


<h3>Note</h3>

<p>Users are advised to change the operating settings of various classifiers since it is well known that machine learning methods require extensive tuning. Currently only some common and intuitive options are set as default and they are by no means the optimal parameterization for a particular data analysis. Users can put machine learning methods' parameters after tuning. A more flexible evaluation is to consider &quot;method=prob&quot; in which case the input m1 &amp; m2 should be a matrix of membership probabilities with k columns and each row of m1 &amp; m2 should sum to one.
</p>


<h3>Author(s)</h3>

<p>Ming Gao: gaoming@umich.edu
</p>
<p>Jialiang Li: stalj@nus.edu.sg
</p>


<h3>References</h3>

<p>Li, J., Gao, M., D’Agostino, R. (2019). Evaluating Classification Accuracy for Modern Learning Approaches. Statistics in Medicine (Tutorials in Biostatistics). 38(13): 2477-2503.
</p>
<p>Li, J., Jiang, B. and Fine, J. P. (2013). Multicategory reclassification statistics for assessing Improvements in diagnostic accuracy. Biostatistics. 14(2): 382—394.
</p>
<p>Li, J., Jiang, B., and Fine, J. P. (2013). Letter to Editor: Response. Biostatistics. 14(4): 809-810.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+idi">idi</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>table(mtcars$carb)
for (i in (1:length(mtcars$carb))) {
  if (mtcars$carb[i] == 3 | mtcars$carb[i] == 6 | mtcars$carb[i] == 8) {
    mtcars$carb_new[i] = 9
  }else{
    mtcars$carb_new[i] = mtcars$carb[i]
  }
}
data &lt;- data.matrix(mtcars[, c(1,5)])
mtcars$carb_new &lt;- factor(mtcars$carb_new)
label &lt;- mtcars$carb_new
str(mtcars)

nri(y = label, m1 = data[, 1], m2 = data[, 2], "lda")
## [1] 0.09375
nri(y = label, m1 = data[, 1], m2 = data[, 2], "tree")
## [1] 0.0625
nri(y = label, m1 = data[, 1], m2 = data[, 2], "tree",control=rpart::rpart.control(minsplit=4))
## [1] 0.1875
</code></pre>

<hr>
<h2 id='pdi'>Calculate PDI Value</h2><span id='topic+pdi'></span>

<h3>Description</h3>

<p>compute the Polytomous Discrimination Index (PDI) value of two or three or four categories classifiers with an option to define the specific model or user-defined model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pdi(y, d, method="multinom", ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="pdi_+3A_y">y</code></td>
<td>
<p>The multinomial response vector with two, three or four categories. It can be factor or integer-valued.</p>
</td></tr>
<tr><td><code id="pdi_+3A_d">d</code></td>
<td>
<p>The set of candidate markers, including one or more columns. Can be a data frame or a matrix; if the method is &quot;prob&quot;, then d should be the probablity matrix.</p>
</td></tr>
<tr><td><code id="pdi_+3A_method">method</code></td>
<td>
<p>Specifies what method is used to construct the classifier based on the marker set in d. Available option includes the following methods:&quot;multinom&quot;: Multinomial Logistic Regression which is the default method, requiring R package nnet;&quot;tree&quot;: Classification Tree method, requiring R package rpart;&quot;svm&quot;: Support Vector Machine (C-classification and radial basis as default), requiring R package e1071;&quot;lda&quot;: Linear Discriminant Analysis, requiring R package lda;&quot;prob&quot;: d is a risk matrix resulted from any external classification algorithm obtained by the user.</p>
</td></tr>
<tr><td><code id="pdi_+3A_...">...</code></td>
<td>
<p>Additional arguments in the chosen method's function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function returns the PDI value for predictive markers based on a user-chosen machine learning method. Currently available methods include logistic regression (default), tree, lda, svm and user-computed risk values. This function is general since we can evaluate the accuracy for marker combinations resulted from complicated classification algorithms.
</p>


<h3>Value</h3>

<p>Returns an object of class &quot;mcca.pdi&quot;. The PDI value of the classification using a particular learning method on a set of marker(s).
</p>
<p>An object of class &quot;mcca.pdi&quot; is a list containing at least the following components:
</p>
<table role = "presentation">
<tr><td><code>call</code></td>
<td>
<p>the matched call.</p>
</td></tr>
<tr><td><code>measure</code></td>
<td>
<p>the value of measure.</p>
</td></tr>
<tr><td><code>table</code></td>
<td>
<p>the category-specific value of measure.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Users are advised to change the operating settings of various classifiers since it is well known that machine learning methods require extensive tuning. Currently only some common and intuitive options are set as default and they are by no means the optimal parameterization for a particular data analysis. Users can put machine learning methods' parameters after tuning. A more flexible evaluation is to consider &quot;method=prob&quot; in which case the input d should be a matrix of membership probabilities with k columns and each row of d should sum to one.
</p>


<h3>Author(s)</h3>

<p>Ming Gao: gaoming@umich.edu
</p>
<p>Jialiang Li: stalj@nus.edu.sg
</p>


<h3>References</h3>

<p>Li, J., Gao, M., D’Agostino, R. (2019). Evaluating Classification Accuracy for Modern Learning Approaches. Statistics in Medicine (Tutorials in Biostatistics). 38(13): 2477-2503.
</p>
<p>Van Calster B, Vergouwe Y, Looman CWN, Van Belle
V, Timmerman D and Steyerberg EW. Assessing the
discriminative ability of risk models for more than two outcome
categories. European Journal of Epidemiology 2012; 27: 761 C
770.
</p>
<p>Li, J., Feng, Q., Fine, J.P., Pencina, M.J., Van Calster, B. (2018). Nonparametric estimation and inference for polytomous discrimination index. Statistical Methods in Medical Research. 27(10): 3092—3103.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+hum">hum</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>str(iris)
data &lt;- iris[, 3]
label &lt;- iris[, 5]
pdi(y = label, d = data,method = "multinom")

## Call:
## pdi(y = label, d = data, method = "multinom")

## Overall Polytomous Discrimination Index:
##  0.9845333

## Category-specific Polytomous Discrimination Index:
##   CATEGORIES VALUES
## 1          1 1.0000
## 2          2 0.9768
## 3          3 0.9768

pdi(y = label, d = data,method = "tree")
pdi(y = label, d = data,method = "tree",control = rpart::rpart.control(minsplit = 200))

data &lt;- data.matrix(iris[, 3])
label &lt;- as.numeric(iris[, 5])
# multinomial
require(nnet)
# model
fit &lt;- multinom(label ~ data, maxit = 1000, MaxNWts = 2000)
predict.probs &lt;- predict(fit, type = "probs")
pp&lt;- data.frame(predict.probs)
# extract the probablity assessment vector
head(pp)
pdi(y = label, d = pp, method = "prob")
</code></pre>

<hr>
<h2 id='plot.mcca.hum'>Plot 3D ROC surface</h2><span id='topic+plot.mcca.hum'></span>

<h3>Description</h3>

<p>plot the 3D ROC surface for a three-category classifier using the 3-dimensional point coordinates, computed by obj which is a mcca.hum class.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'mcca.hum'
plot(x,labs=levels(x$y),coords=1:3,nticks=5,filename='fig.png',cex=0.7, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.mcca.hum_+3A_x">x</code></td>
<td>
<p>An mcca.hum class object, containing probability matrix and labels.</p>
</td></tr>
<tr><td><code id="plot.mcca.hum_+3A_labs">labs</code></td>
<td>
<p>The label names of three coordinates. Default is 'levels(x$y)'.</p>
</td></tr>
<tr><td><code id="plot.mcca.hum_+3A_coords">coords</code></td>
<td>
<p>The coordinates markers. Default is 'c(1,2,3)', which means labs[1] is the x-axis (class 1), labs[2] is the z-axis (class 3) and labs[3] is the y-axis (class 2).</p>
</td></tr>
<tr><td><code id="plot.mcca.hum_+3A_nticks">nticks</code></td>
<td>
<p>Suggested number of ticks.</p>
</td></tr>
<tr><td><code id="plot.mcca.hum_+3A_filename">filename</code></td>
<td>
<p>Filename to save snapshot.</p>
</td></tr>
<tr><td><code id="plot.mcca.hum_+3A_cex">cex</code></td>
<td>
<p>Size for text.</p>
</td></tr>
<tr><td><code id="plot.mcca.hum_+3A_...">...</code></td>
<td>
<p>further arguments to 'plot.default'.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is to plot the 3D ROC surface according to the correct classification probabilities for the three categories, resulted from any statistical or machine learning methods. This function complements the HUM package which can only plot 3D ROC surface for a single diagnostic test or biomarker for three classes.
</p>


<h3>Value</h3>

<p>The function doesn't return any value.
</p>


<h3>Author(s)</h3>

<p>Ming Gao: gaoming@umich.edu
</p>
<p>Jialiang Li: stalj@nus.edu.sg
</p>


<h3>References</h3>

<p>Li, J., and Zhou, X. H. (2009). Nonparametric and semiparametric estimation of the three way receiver operating characteristic surface. Journal of Statistical Planning and Inference. 139: 4133—4142.
</p>
<p>Li, J., Gao, M., D’Agostino, R. (2019). Evaluating Classification Accuracy for Modern Learning Approaches. Statistics in Medicine (Tutorials in Biostatistics). 38(13): 2477-2503.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data &lt;- iris[, 1]
label &lt;- iris[, 5]
a=hum(y = label, d = data,method = "multinom")
#plot(a,filename='fig.png')

</code></pre>

<hr>
<h2 id='pm'>Calculate Probability Matrix</h2><span id='topic+pm'></span>

<h3>Description</h3>

<p>compute the probability matrix of two or three or four categories classifiers with an option to define the specific model or user-defined model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pm(y, d, method="multinom", ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="pm_+3A_y">y</code></td>
<td>
<p>The multinomial response vector with two, three or four categories. It can be factor or integer-valued.</p>
</td></tr>
<tr><td><code id="pm_+3A_d">d</code></td>
<td>
<p>The set of candidate markers, including one or more columns. Can be a data frame or a matrix.</p>
</td></tr>
<tr><td><code id="pm_+3A_method">method</code></td>
<td>
<p>Specifies what method is used to construct the classifier based on the marker set in d. Available option includes the following methods:&quot;multinom&quot;: Multinomial Logistic Regression which is the default method, requiring R package nnet;&quot;tree&quot;: Classification Tree method, requiring R package rpart;&quot;svm&quot;: Support Vector Machine (C-classification and radial basis as default), requiring R package e1071;&quot;lda&quot;: Linear Discriminant Analysis, requiring R package lda.</p>
</td></tr>
<tr><td><code id="pm_+3A_...">...</code></td>
<td>
<p>Additional arguments in the chosen method's function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function returns the probability matrix for predictive markers based on a user-chosen machine learning method. Currently available methods include logistic regression (default), tree, lda, svm and user-computed risk values.
</p>


<h3>Value</h3>

<p>The probability matrix of the classification using a particular learning method on a set of marker(s).
</p>


<h3>Author(s)</h3>

<p>Ming Gao: gaoming@umich.edu
</p>
<p>Jialiang Li: stalj@nus.edu.sg
</p>


<h3>References</h3>

<p>Li, J. and Fine, J. P. (2008): ROC analysis with multiple tests and multiple classes: methodology and applications in microarray studies. Biostatistics. 9 (3): 566-576.
</p>
<p>Li, J., Chow, Y., Wong, W.K., and Wong, T.Y. (2014). Sorting Multiple Classes in Multi-dimensional ROC Analysis: Parametric and Nonparametric Approaches. Biomarkers. 19(1): 1-8.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pdi">pdi</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>str(iris)
data &lt;- iris[, 1:4]
label &lt;- iris[, 5]
pm(y = label, d = data,method = "multinom")

</code></pre>

<hr>
<h2 id='print.mcca.ccp'>Print Method for mcca ccp class</h2><span id='topic+print.mcca.ccp'></span>

<h3>Description</h3>

<p>An S3 method for the print generic. It is designed for a quick look at CCP values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'mcca.ccp'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print.mcca.ccp_+3A_x">x</code></td>
<td>
<p>object of class 'mcca.ccp'.</p>
</td></tr>
<tr><td><code id="print.mcca.ccp_+3A_...">...</code></td>
<td>
<p>further arguments to 'print.default'.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class &quot;mcca.ccp&quot; is a list containing at least the following components:
</p>
<table role = "presentation">
<tr><td><code>call</code></td>
<td>
<p>the matched call.</p>
</td></tr>
<tr><td><code>measure</code></td>
<td>
<p>the value of measure.</p>
</td></tr>
<tr><td><code>table</code></td>
<td>
<p>the category-specific value of measure.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Ming Gao: gaoming@umich.edu
</p>
<p>Jialiang Li: stalj@nus.edu.sg
</p>


<h3>References</h3>

<p>Li, J., Jiang, B. and Fine, J. P. (2013). Multicategory reclassification statistics for assessing Improvements in diagnostic accuracy. Biostatistics. 14(2): 382-394.
</p>
<p>Li, J., Jiang, B., and Fine, J. P. (2013). Letter to Editor: Response. Biostatistics. 14(4): 809-810.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data = iris[, 1:4]
label = iris[, 5]
ccp_object=ccp(y = label, d = data, method = "multinom",maxit = 1000,MaxNWts = 2000,trace=FALSE)
print(ccp_object)

</code></pre>

<hr>
<h2 id='print.mcca.hum'>Print Method for mcca hum class</h2><span id='topic+print.mcca.hum'></span>

<h3>Description</h3>

<p>An S3 method for the print generic. It is designed for a quick look at hum values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'mcca.hum'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print.mcca.hum_+3A_x">x</code></td>
<td>
<p>object of class 'mcca.hum'.</p>
</td></tr>
<tr><td><code id="print.mcca.hum_+3A_...">...</code></td>
<td>
<p>further arguments to 'print.default'.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class &quot;mcca.hum&quot; is a list containing at least the following components:
</p>
<table role = "presentation">
<tr><td><code>call</code></td>
<td>
<p>the matched call.</p>
</td></tr>
<tr><td><code>measure</code></td>
<td>
<p>the value of measure.</p>
</td></tr>
<tr><td><code>table</code></td>
<td>
<p>the category-specific value of measure.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Ming Gao: gaoming@umich.edu
</p>
<p>Jialiang Li: stalj@nus.edu.sg
</p>


<h3>References</h3>

<p>Li, J., Jiang, B. and Fine, J. P. (2013). Multicategory reclassification statistics for assessing Improvements in diagnostic accuracy. Biostatistics. 14(2): 382-394.
</p>
<p>Li, J., Jiang, B., and Fine, J. P. (2013). Letter to Editor: Response. Biostatistics. 14(4): 809-810.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data = iris[, 1:4]
label = iris[, 5]
hum_object=hum(y = label, d = data, method = "multinom",maxit = 1000,MaxNWts = 2000,trace=FALSE)
print(hum_object)

</code></pre>

<hr>
<h2 id='print.mcca.pdi'>Print Method for mcca pdi class</h2><span id='topic+print.mcca.pdi'></span>

<h3>Description</h3>

<p>An S3 method for the print generic. It is designed for a quick look at pdi values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'mcca.pdi'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print.mcca.pdi_+3A_x">x</code></td>
<td>
<p>object of class 'mcca.pdi'.</p>
</td></tr>
<tr><td><code id="print.mcca.pdi_+3A_...">...</code></td>
<td>
<p>further arguments to 'print.default'.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class &quot;mcca.pdi&quot; is a list containing at least the following components:
</p>
<table role = "presentation">
<tr><td><code>call</code></td>
<td>
<p>the matched call.</p>
</td></tr>
<tr><td><code>measure</code></td>
<td>
<p>the value of measure.</p>
</td></tr>
<tr><td><code>table</code></td>
<td>
<p>the category-specific value of measure.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Ming Gao: gaoming@umich.edu
</p>
<p>Jialiang Li: stalj@nus.edu.sg
</p>


<h3>References</h3>

<p>Li, J., Jiang, B. and Fine, J. P. (2013). Multicategory reclassification statistics for assessing Improvements in diagnostic accuracy. Biostatistics. 14(2): 382-394.
</p>
<p>Li, J., Jiang, B., and Fine, J. P. (2013). Letter to Editor: Response. Biostatistics. 14(4): 809-810.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data = iris[, 1:4]
label = iris[, 5]
pdi_object=pdi(y = label, d = data, method = "multinom",maxit = 1000,MaxNWts = 2000,trace=FALSE)
print(pdi_object)

</code></pre>

<hr>
<h2 id='print.mcca.rsq'>Print Method for mcca rsq class</h2><span id='topic+print.mcca.rsq'></span>

<h3>Description</h3>

<p>An S3 method for the print generic. It is designed for a quick look at rsq values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'mcca.rsq'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print.mcca.rsq_+3A_x">x</code></td>
<td>
<p>object of class 'mcca.rsq'.</p>
</td></tr>
<tr><td><code id="print.mcca.rsq_+3A_...">...</code></td>
<td>
<p>further arguments to 'print.default'.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class &quot;mcca.rsq&quot; is a list containing at least the following components:
</p>
<table role = "presentation">
<tr><td><code>call</code></td>
<td>
<p>the matched call.</p>
</td></tr>
<tr><td><code>measure</code></td>
<td>
<p>the value of measure.</p>
</td></tr>
<tr><td><code>table</code></td>
<td>
<p>the category-specific value of measure.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Ming Gao: gaoming@umich.edu
</p>
<p>Jialiang Li: stalj@nus.edu.sg
</p>


<h3>References</h3>

<p>Li, J., Jiang, B. and Fine, J. P. (2013). Multicategory reclassification statistics for assessing Improvements in diagnostic accuracy. Biostatistics. 14(2): 382-394.
</p>
<p>Li, J., Jiang, B., and Fine, J. P. (2013). Letter to Editor: Response. Biostatistics. 14(4): 809-810.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data = iris[, 1:4]
label = iris[, 5]
rsq_object=rsq(y = label, d = data, method = "multinom",maxit = 1000,MaxNWts = 2000,trace=FALSE)
print(rsq_object)

</code></pre>

<hr>
<h2 id='rsq'>Calculate RSQ Value</h2><span id='topic+rsq'></span>

<h3>Description</h3>

<p>compute the R-squared (RSQ) value of two or three or four categories classifiers with an option to define the specific model or user-defined model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rsq(y, d, method="multinom", ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="rsq_+3A_y">y</code></td>
<td>
<p>The multinomial response vector with two, three or four categories. It can be factor or integer-valued.</p>
</td></tr>
<tr><td><code id="rsq_+3A_d">d</code></td>
<td>
<p>The set of candidate markers, including one or more columns. Can be a data frame or a matrix; if the method is &quot;prob&quot;, then d should be the probablity matrix.</p>
</td></tr>
<tr><td><code id="rsq_+3A_method">method</code></td>
<td>
<p>Specifies what method is used to construct the classifier based on the marker set in d. Available option includes the following methods:&quot;multinom&quot;: Multinomial Logistic Regression which is the default method, requiring R package nnet;&quot;tree&quot;: Classification Tree method, requiring R package rpart;&quot;svm&quot;: Support Vector Machine (C-classification and radial basis as default), requiring R package e1071;&quot;lda&quot;: Linear Discriminant Analysis, requiring R package lda;&quot;prob&quot;: d is a risk matrix resulted from any external classification algorithm obtained by the user.</p>
</td></tr>
<tr><td><code id="rsq_+3A_...">...</code></td>
<td>
<p>Additional arguments in the chosen method's function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function returns the RSQ value for predictive markers based on a user-chosen machine learning method. Currently available methods include logistic regression (default), tree, lda, svm and user-computed risk values. This function is general since we can evaluate the accuracy for marker combinations resulted from complicated classification algorithms.
</p>


<h3>Value</h3>

<p>Returns an object of class &quot;mcca.rsq&quot;. The RSQ value of the classification using a particular learning method on a set of marker(s).
</p>
<p>An object of class &quot;mcca.rsq&quot; is a list containing at least the following components:
</p>
<table role = "presentation">
<tr><td><code>call</code></td>
<td>
<p>the matched call.</p>
</td></tr>
<tr><td><code>measure</code></td>
<td>
<p>the value of measure.</p>
</td></tr>
<tr><td><code>table</code></td>
<td>
<p>the category-specific value of measure.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>Users are advised to change the operating settings of various classifiers since it is well known that machine learning methods require extensive tuning. Currently only some common and intuitive options are set as default and they are by no means the optimal parameterization for a particular data analysis. Users can put machine learning methods' parameters after tuning. A more flexible evaluation is to consider &quot;method=prob&quot; in which case the input d should be a matrix of membership probabilities with k columns and each row of d should sum to one.
</p>


<h3>Author(s)</h3>

<p>Ming Gao: gaoming@umich.edu
</p>
<p>Jialiang Li: stalj@nus.edu.sg
</p>


<h3>References</h3>

<p>Li, J., Gao, M., D’Agostino, R. (2019). Evaluating Classification Accuracy for Modern Learning Approaches. Statistics in Medicine (Tutorials in Biostatistics). 38(13): 2477-2503.
</p>
<p>Li, J., Jiang, B. and Fine, J. P. (2013). Multicategory reclassification statistics for assessing Improvements in diagnostic accuracy. Biostatistics. 14(2): 382-394.
</p>
<p>Li, J., Jiang, B., and Fine, J. P. (2013). Letter to Editor: Response. Biostatistics. 14(4): 809-810.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ccp">ccp</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>str(iris)
data &lt;- iris[, 1:4]
label &lt;- iris[, 5]
rsq(y = label, d = data, method="multinom")

## Call:
## rsq(y = label, d = data, method = "multinom")

## Overall R-squared value:
##  0.9637932

## Category-specific R-squared value:
##   CATEGORIES    VALUES
## 1     setosa 0.9999824
## 2 versicolor 0.9456770
## 3  virginica 0.9457203

rsq(y = label, d = data, method = "tree")


data &lt;- data.matrix(iris[, 1:4])
label &lt;- as.numeric(iris[, 5])
# multinomial
require(nnet)
# model
fit &lt;- multinom(label ~ data, maxit = 1000, MaxNWts = 2000)
predict.probs &lt;- predict(fit, type = "probs")
pp&lt;- data.frame(predict.probs)
# extract the probablity assessment vector
head(pp)
rsq(y = label, d = pp, method = "prob")


table(mtcars$carb)
for (i in (1:length(mtcars$carb))) {
  if (mtcars$carb[i] == 3 | mtcars$carb[i] == 6 | mtcars$carb[i] == 8) {
    mtcars$carb_new[i] = 9
  }else{
    mtcars$carb_new[i] = mtcars$carb[i]
  }
}
data &lt;- data.matrix(mtcars[, c(1)])
mtcars$carb_new &lt;- factor(mtcars$carb_new)
label &lt;- mtcars$carb_new
str(mtcars)

rsq(y = label, d = data, method="tree")
rsq(y = label, d = data, method="lda")
rsq(y = label, d = data, method="lda",prior = c(100,1,1,1)/103)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
