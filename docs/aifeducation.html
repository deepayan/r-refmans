<!DOCTYPE html><html lang="en"><head><title>Help for package aifeducation</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {aifeducation}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#.AIFEBaseTransformer'><p>Base <code>R6</code> class for creation and definition of <code>.AIFE*Transformer-like</code> classes</p></a></li>
<li><a href='#.AIFEBertTransformer'><p>Child <code>R6</code> class for creation and training of <code>BERT</code> transformers</p></a></li>
<li><a href='#.AIFEDebertaTransformer'><p>Child <code>R6</code> class for creation and training of <code>DeBERTa-V2</code> transformers</p></a></li>
<li><a href='#.AIFEFunnelTransformer'><p>Child <code>R6</code> class for creation and training of <code>Funnel</code> transformers</p></a></li>
<li><a href='#.AIFELongformerTransformer'><p>Child <code>R6</code> class for creation and training of <code>Longformer</code> transformers</p></a></li>
<li><a href='#.AIFEMpnetTransformer'><p>Child <code>R6</code> class for creation and training of <code>MPNet</code> transformers</p></a></li>
<li><a href='#.AIFERobertaTransformer'><p>Child <code>R6</code> class for creation and training of <code>RoBERTa</code> transformers</p></a></li>
<li><a href='#.AIFETrObj'><p>Transformer objects</p></a></li>
<li><a href='#aife_transformer_maker'><p><code>R6</code> object of the <code>AIFETransformerMaker</code> class</p></a></li>
<li><a href='#AIFEBaseModel'><p>Base class for models using neural nets</p></a></li>
<li><a href='#AIFETransformerMaker'><p><code>R6</code> class for transformer creation</p></a></li>
<li><a href='#AIFETrType'><p>Transformer types</p></a></li>
<li><a href='#auto_n_cores'><p>Number of cores for multiple tasks</p></a></li>
<li><a href='#calc_standard_classification_measures'><p>Calculate standard classification measures</p></a></li>
<li><a href='#check_aif_py_modules'><p>Check if all necessary python modules are available</p></a></li>
<li><a href='#clean_pytorch_log_transformers'><p>Clean pytorch log of transformers</p></a></li>
<li><a href='#cohens_kappa'><p>Calculate Cohen's Kappa</p></a></li>
<li><a href='#create_config_state'><p>Create config for R interfaces</p></a></li>
<li><a href='#create_data_embeddings_description'><p>Generate description for text embeddings</p></a></li>
<li><a href='#create_dir'><p>Create directory if not exists</p></a></li>
<li><a href='#create_synthetic_units_from_matrix'><p>Create synthetic units</p></a></li>
<li><a href='#DataManagerClassifier'><p>Data manager for classification tasks</p></a></li>
<li><a href='#EmbeddedText'><p>Embedded text</p></a></li>
<li><a href='#fleiss_kappa'><p>Calculate Fleiss' Kappa</p></a></li>
<li><a href='#generate_id'><p>Generate ID suffix for objects</p></a></li>
<li><a href='#get_alpha_3_codes'><p>Country Alpha 3 Codes</p></a></li>
<li><a href='#get_coder_metrics'><p>Calculate reliability measures based on content analysis</p></a></li>
<li><a href='#get_file_extension'><p>Get file extension</p></a></li>
<li><a href='#get_n_chunks'><p>Get the number of chunks/sequences for each case</p></a></li>
<li><a href='#get_py_package_versions'><p>Get versions of python components</p></a></li>
<li><a href='#get_synthetic_cases_from_matrix'><p>Create synthetic cases for balancing training data</p></a></li>
<li><a href='#imdb_movie_reviews'><p>Standford Movie Review Dataset</p></a></li>
<li><a href='#install_aifeducation'><p>Install aifeducation on a machine</p></a></li>
<li><a href='#install_py_modules'><p>Installing necessary python modules to an environment</p></a></li>
<li><a href='#is.null_or_na'><p>Check if NULL or NA</p></a></li>
<li><a href='#kendalls_w'><p>Calculate Kendall's coefficient of concordance w</p></a></li>
<li><a href='#kripp_alpha'><p>Calculate Krippendorff's Alpha</p></a></li>
<li><a href='#LargeDataSetBase'><p>Abstract base class for large data sets</p></a></li>
<li><a href='#LargeDataSetForText'><p>Abstract class for large data sets containing raw texts</p></a></li>
<li><a href='#LargeDataSetForTextEmbeddings'><p>Abstract class for large data sets containing text embeddings</p></a></li>
<li><a href='#License_Server'><p>Server function for: graphical user interface for showing the license.</p></a></li>
<li><a href='#load_from_disk'><p>Loading objects created with 'aifeducation'</p></a></li>
<li><a href='#long_load_target_data'><p>Load target data for long running tasks</p></a></li>
<li><a href='#matrix_to_array_c'><p>Reshape matrix to array</p></a></li>
<li><a href='#output_message'><p>Print message</p></a></li>
<li><a href='#print_message'><p>Print message (<code>message()</code>)</p></a></li>
<li><a href='#Reliability_Server'><p>Server function for: graphical user interface for displaying the reliability of classifiers.</p></a></li>
<li><a href='#Reliability_UI'><p>Graphical user interface for displaying the reliability of classifiers.</p></a></li>
<li><a href='#run_py_file'><p>Run python file</p></a></li>
<li><a href='#save_to_disk'><p>Saving objects created with 'aifeducation'</p></a></li>
<li><a href='#set_config_cpu_only'><p>Setting cpu only for 'tensorflow'</p></a></li>
<li><a href='#set_config_gpu_low_memory'><p>Setting gpus' memory usage</p></a></li>
<li><a href='#set_config_os_environ_logger'><p>Sets the level for logging information in tensorflow</p></a></li>
<li><a href='#set_config_tf_logger'><p>Sets the level for logging information in tensorflow</p></a></li>
<li><a href='#set_transformers_logger'><p>Sets the level for logging information of the 'transformers' library</p></a></li>
<li><a href='#start_aifeducation_studio'><p>Aifeducation Studio</p></a></li>
<li><a href='#summarize_tracked_sustainability'><p>Summarizing tracked sustainability data</p></a></li>
<li><a href='#TEClassifierProtoNet'><p>Text embedding classifier with a ProtoNet</p></a></li>
<li><a href='#TEClassifierRegular'><p>Text embedding classifier with a neural net</p></a></li>
<li><a href='#TEFeatureExtractor'><p>Feature extractor for reducing the number for dimensions of text embeddings.</p></a></li>
<li><a href='#TextEmbeddingModel'><p>Text embedding model</p></a></li>
<li><a href='#to_categorical_c'><p>Transforming classes to one-hot encoding</p></a></li>
<li><a href='#vignette_classifier'><p>Vignette classifier</p></a></li>
<li><a href='#vignette_classifier_ProtoNet'><p>Vignette classifier ProtoNet</p></a></li>
<li><a href='#vignette_classifier_sc_pl'><p>Vignette classifier trained with Synthetic Cases and Pseudo Labeling</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Artificial Intelligence for Education</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.2</td>
</tr>
<tr>
<td>Description:</td>
<td>In social and educational settings, the use of Artificial
    Intelligence (AI) is a challenging task. Relevant data is often only
    available in handwritten forms, or the use of data is restricted by
    privacy policies. This often leads to small data sets. Furthermore, in
    the educational and social sciences, data is often unbalanced in terms
    of frequencies. To support educators as well as educational and social
    researchers in using the potentials of AI for their work, this package
    provides a unified interface for neural nets in 'PyTorch' to deal with
    natural language problems. In addition, the package ships with a shiny
    app, providing a graphical user interface.  This allows the usage of
    AI for people without skills in writing python/R scripts.  The tools
    integrate existing mathematical and statistical methods for dealing
    with small data sets via pseudo-labeling (e.g. Cascante-Bonilla et al.
    (2020) &lt;<a href="https://doi.org/10.48550%2FarXiv.2001.06001">doi:10.48550/arXiv.2001.06001</a>&gt;) and imbalanced data via the
    creation of synthetic cases (e.g.  Bunkhumpornpat et al. (2012)
    &lt;<a href="https://doi.org/10.1007%2Fs10489-011-0287-y">doi:10.1007/s10489-011-0287-y</a>&gt;).  Performance evaluation of AI is
    connected to measures from content analysis which educational and
    social researchers are generally more familiar with (e.g. Berding &amp;
    Pargmann (2022) &lt;<a href="https://doi.org/10.30819%2F5581">doi:10.30819/5581</a>&gt;, Gwet (2014)
    &lt;ISBN:978-0-9708062-8-4&gt;, Krippendorff (2019)
    &lt;<a href="https://doi.org/10.4135%2F9781071878781">doi:10.4135/9781071878781</a>&gt;). Estimation of energy consumption and CO2
    emissions during model training is done with the 'python' library
    'codecarbon'.  Finally, all objects created with this package allow to
    share trained AI models with other people.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://fberding.github.io/aifeducation/">https://fberding.github.io/aifeducation/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/cran/aifeducation/issues">https://github.com/cran/aifeducation/issues</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>doParallel, foreach, iotarelr(&ge; 0.1.5), irrCAC, methods, Rcpp
(&ge; 1.0.10), reshape2, reticulate (&ge; 1.34.0), rlang,
smotefamily, stringi, utils</td>
</tr>
<tr>
<td>Suggests:</td>
<td>bslib, DT, fs, future, ggplot2, knitr, promises, readtext,
readxl, rmarkdown, shiny(&ge; 1.9.0), shinyFiles, shinyWidgets,
sortable, testthat (&ge; 3.0.0)</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.2</td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>PyTorch (see vignette "Get started")</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2025-02-05 12:39:46 UTC; WissMit</td>
</tr>
<tr>
<td>Author:</td>
<td>Berding Florian <a href="https://orcid.org/0000-0002-3593-1695"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre],
  Tykhonova Yuliia <a href="https://orcid.org/0009-0006-9015-1006"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut],
  Pargmann Julia <a href="https://orcid.org/0000-0003-3616-0172"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [ctb],
  Leube Anna <a href="https://orcid.org/0009-0001-6949-1608"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [ctb],
  Riebenbauer Elisabeth
    <a href="https://orcid.org/0000-0002-8535-3694"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [ctb],
  Rebmann Karin [ctb],
  Slopinski Andreas [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Berding Florian &lt;florian.berding@uni-hamburg.de&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2025-02-05 13:00:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='.AIFEBaseTransformer'>Base <code>R6</code> class for creation and definition of <code>.AIFE*Transformer-like</code> classes</h2><span id='topic+.AIFEBaseTransformer'></span>

<h3>Description</h3>

<p>This base class is used to create and define <code>.AIFE*Transformer-like</code> classes. It serves as a skeleton
for a future concrete transformer and cannot be used to create an object of itself (an attempt to call <code>new</code>-method
will produce an error).
</p>
<p>See p.1 Base Transformer Class in
<a href="https://fberding.github.io/aifeducation/articles/transformers.html">Transformers for Developers</a> for details.
</p>


<h3>Create</h3>

<p>The <code>create</code>-method is a basic algorithm that is used to create a new transformer, but cannot be
called directly.
</p>


<h3>Train</h3>

<p>The <code>train</code>-method is a basic algorithm that is used to train and tune the transformer but cannot be
called directly.
</p>


<h3>Concrete transformer implementation</h3>

<p>There are already implemented concrete (child) transformers (e.g.
<code>BERT</code>, <code>DeBERTa-V2</code>, etc.), to implement a new one see p.4 Implement A Custom Transformer in
<a href="https://fberding.github.io/aifeducation/articles/transformers.html">Transformers for Developers</a>
</p>


<h3>Public fields</h3>

<div class="r6-fields">

<dl>
<dt><code>params</code></dt><dd><p>A list containing transformer's parameters ('static', 'dynamic' and 'dependent' parameters)
</p>
<p><code>list()</code> containing all the transformer parameters. Can be set with <code>set_model_param()</code>.
</p>


<h4><strong>'Static' parameters</strong></h4>

<p>Regardless of the transformer, the following parameters are always included:
</p>

<ul>
<li> <p><code>ml_framework</code>
</p>
</li>
<li> <p><code>text_dataset</code>
</p>
</li>
<li> <p><code>sustain_track</code>
</p>
</li>
<li> <p><code>sustain_iso_code</code>
</p>
</li>
<li> <p><code>sustain_region</code>
</p>
</li>
<li> <p><code>sustain_interval</code>
</p>
</li>
<li> <p><code>trace</code>
</p>
</li>
<li> <p><code>pytorch_safetensors</code>
</p>
</li>
<li> <p><code>log_dir</code>
</p>
</li>
<li> <p><code>log_write_interval</code>
</p>
</li></ul>




<h4><strong>'Dynamic' parameters</strong></h4>

<p>In the case of <strong>create</strong> it also contains (see <code>create</code>-method for details):
</p>

<ul>
<li> <p><code>model_dir</code>
</p>
</li>
<li> <p><code>vocab_size</code>
</p>
</li>
<li> <p><code>max_position_embeddings</code>
</p>
</li>
<li> <p><code>hidden_size</code>
</p>
</li>
<li> <p><code>hidden_act</code>
</p>
</li>
<li> <p><code>hidden_dropout_prob</code>
</p>
</li>
<li> <p><code>attention_probs_dropout_prob</code>
</p>
</li>
<li> <p><code>intermediate_size</code>
</p>
</li>
<li> <p><code>num_attention_heads</code>
</p>
</li></ul>

<p>In the case of <strong>train</strong> it also contains (see <code>train</code>-method for details):
</p>

<ul>
<li> <p><code>output_dir</code>
</p>
</li>
<li> <p><code>model_dir_path</code>
</p>
</li>
<li> <p><code>p_mask</code>
</p>
</li>
<li> <p><code>whole_word</code>
</p>
</li>
<li> <p><code>val_size</code>
</p>
</li>
<li> <p><code>n_epoch</code>
</p>
</li>
<li> <p><code>batch_size</code>
</p>
</li>
<li> <p><code>chunk_size</code>
</p>
</li>
<li> <p><code>min_seq_len</code>
</p>
</li>
<li> <p><code>full_sequences_only</code>
</p>
</li>
<li> <p><code>learning_rate</code>
</p>
</li>
<li> <p><code>n_workers</code>
</p>
</li>
<li> <p><code>multi_process</code>
</p>
</li>
<li> <p><code>keras_trace</code>
</p>
</li>
<li> <p><code>pytorch_trace</code>
</p>
</li></ul>




<h4><strong>'Dependent' parameters</strong></h4>

<p>Depending on the transformer and the method used class may contain different parameters:
</p>

<ul>
<li> <p><code>vocab_do_lower_case</code>
</p>
</li>
<li> <p><code>num_hidden_layer</code>
</p>
</li>
<li> <p><code>add_prefix_space</code>
</p>
</li>
<li><p> etc.
</p>
</li></ul>

</dd>
<dt><code>temp</code></dt><dd><p>A list containing temporary transformer's parameters
</p>
<p><code>list()</code> containing all the temporary local variables that need to be accessed between the step functions. Can
be set with <code>set_model_temp()</code>.
</p>
<p>For example, it can be a variable <code>tok_new</code> that stores the tokenizer from
<code>steps_for_creation$create_tokenizer_draft</code>. To train the tokenizer, access the variable <code>tok_new</code> in
<code>steps_for_creation$calculate_vocab</code> through the <code>temp</code> list of this class.</p>
</dd>
</dl>

</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-.AIFEBaseTransformer-new"><code>.AIFEBaseTransformer$new()</code></a>
</p>
</li>
<li> <p><a href="#method-.AIFEBaseTransformer-set_title"><code>.AIFEBaseTransformer$set_title()</code></a>
</p>
</li>
<li> <p><a href="#method-.AIFEBaseTransformer-set_model_param"><code>.AIFEBaseTransformer$set_model_param()</code></a>
</p>
</li>
<li> <p><a href="#method-.AIFEBaseTransformer-set_model_temp"><code>.AIFEBaseTransformer$set_model_temp()</code></a>
</p>
</li>
<li> <p><a href="#method-.AIFEBaseTransformer-set_SFC_check_max_pos_emb"><code>.AIFEBaseTransformer$set_SFC_check_max_pos_emb()</code></a>
</p>
</li>
<li> <p><a href="#method-.AIFEBaseTransformer-set_SFC_create_tokenizer_draft"><code>.AIFEBaseTransformer$set_SFC_create_tokenizer_draft()</code></a>
</p>
</li>
<li> <p><a href="#method-.AIFEBaseTransformer-set_SFC_calculate_vocab"><code>.AIFEBaseTransformer$set_SFC_calculate_vocab()</code></a>
</p>
</li>
<li> <p><a href="#method-.AIFEBaseTransformer-set_SFC_save_tokenizer_draft"><code>.AIFEBaseTransformer$set_SFC_save_tokenizer_draft()</code></a>
</p>
</li>
<li> <p><a href="#method-.AIFEBaseTransformer-set_SFC_create_final_tokenizer"><code>.AIFEBaseTransformer$set_SFC_create_final_tokenizer()</code></a>
</p>
</li>
<li> <p><a href="#method-.AIFEBaseTransformer-set_SFC_create_transformer_model"><code>.AIFEBaseTransformer$set_SFC_create_transformer_model()</code></a>
</p>
</li>
<li> <p><a href="#method-.AIFEBaseTransformer-set_required_SFC"><code>.AIFEBaseTransformer$set_required_SFC()</code></a>
</p>
</li>
<li> <p><a href="#method-.AIFEBaseTransformer-set_SFT_load_existing_model"><code>.AIFEBaseTransformer$set_SFT_load_existing_model()</code></a>
</p>
</li>
<li> <p><a href="#method-.AIFEBaseTransformer-set_SFT_cuda_empty_cache"><code>.AIFEBaseTransformer$set_SFT_cuda_empty_cache()</code></a>
</p>
</li>
<li> <p><a href="#method-.AIFEBaseTransformer-set_SFT_create_data_collator"><code>.AIFEBaseTransformer$set_SFT_create_data_collator()</code></a>
</p>
</li>
<li> <p><a href="#method-.AIFEBaseTransformer-create"><code>.AIFEBaseTransformer$create()</code></a>
</p>
</li>
<li> <p><a href="#method-.AIFEBaseTransformer-train"><code>.AIFEBaseTransformer$train()</code></a>
</p>
</li>
<li> <p><a href="#method-.AIFEBaseTransformer-clone"><code>.AIFEBaseTransformer$clone()</code></a>
</p>
</li></ul>


<hr>
<a id="method-.AIFEBaseTransformer-new"></a>



<h4>Method <code>new()</code></h4>

<p>An object of this class cannot be created. Thus, method's call will produce an error.
</p>


<h5>Usage</h5>

<div class="r"><pre>.AIFEBaseTransformer$new()</pre></div>



<h5>Returns</h5>

<p>This method returns an error.
</p>


<hr>
<a id="method-.AIFEBaseTransformer-set_title"></a>



<h4>Method <code>set_title()</code></h4>

<p>Setter for the title. Sets a new value for the <code>title</code> private attribute.
</p>


<h5>Usage</h5>

<div class="r"><pre>.AIFEBaseTransformer$set_title(title)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>title</code></dt><dd><p><code>string</code> A new title.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>This method returns nothing.
</p>


<hr>
<a id="method-.AIFEBaseTransformer-set_model_param"></a>



<h4>Method <code>set_model_param()</code></h4>

<p>Setter for the parameters. Adds a new parameter and its value to the <code>params</code> list.
</p>


<h5>Usage</h5>

<div class="r"><pre>.AIFEBaseTransformer$set_model_param(param_name, param_value)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>param_name</code></dt><dd><p><code>string</code> Parameter's name.</p>
</dd>
<dt><code>param_value</code></dt><dd><p><code>any</code> Parameter's value.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>This method returns nothing.
</p>


<hr>
<a id="method-.AIFEBaseTransformer-set_model_temp"></a>



<h4>Method <code>set_model_temp()</code></h4>

<p>Setter for the temporary model's parameters. Adds a new temporary parameter and its value to the
<code>temp</code> list.
</p>


<h5>Usage</h5>

<div class="r"><pre>.AIFEBaseTransformer$set_model_temp(temp_name, temp_value)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>temp_name</code></dt><dd><p><code>string</code> Parameter's name.</p>
</dd>
<dt><code>temp_value</code></dt><dd><p><code>any</code> Parameter's value.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>This method returns nothing.
</p>


<hr>
<a id="method-.AIFEBaseTransformer-set_SFC_check_max_pos_emb"></a>



<h4>Method <code>set_SFC_check_max_pos_emb()</code></h4>

<p>Setter for the <code>check_max_pos_emb</code> element of the private <code>steps_for_creation</code> list. Sets a new
<code>fun</code> function as the <code>check_max_pos_emb</code> step.
</p>


<h5>Usage</h5>

<div class="r"><pre>.AIFEBaseTransformer$set_SFC_check_max_pos_emb(fun)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>fun</code></dt><dd><p><code style="white-space: pre;">&#8288;function()&#8288;</code> A new function.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>This method returns nothing.
</p>


<hr>
<a id="method-.AIFEBaseTransformer-set_SFC_create_tokenizer_draft"></a>



<h4>Method <code>set_SFC_create_tokenizer_draft()</code></h4>

<p>Setter for the <code>create_tokenizer_draft</code> element of the  private <code>steps_for_creation</code> list. Sets a
new <code>fun</code> function as the <code>create_tokenizer_draft</code> step.
</p>


<h5>Usage</h5>

<div class="r"><pre>.AIFEBaseTransformer$set_SFC_create_tokenizer_draft(fun)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>fun</code></dt><dd><p><code style="white-space: pre;">&#8288;function()&#8288;</code> A new function.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>This method returns nothing.
</p>


<hr>
<a id="method-.AIFEBaseTransformer-set_SFC_calculate_vocab"></a>



<h4>Method <code>set_SFC_calculate_vocab()</code></h4>

<p>Setter for the <code>calculate_vocab</code> element of the private <code>steps_for_creation</code> list. Sets a new <code>fun</code>
function as the <code>calculate_vocab</code> step.
</p>


<h5>Usage</h5>

<div class="r"><pre>.AIFEBaseTransformer$set_SFC_calculate_vocab(fun)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>fun</code></dt><dd><p><code style="white-space: pre;">&#8288;function()&#8288;</code> A new function.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>This method returns nothing.
</p>


<hr>
<a id="method-.AIFEBaseTransformer-set_SFC_save_tokenizer_draft"></a>



<h4>Method <code>set_SFC_save_tokenizer_draft()</code></h4>

<p>Setter for the <code>save_tokenizer_draft</code> element of the private <code>steps_for_creation</code> list. Sets a new
<code>fun</code> function as the <code>save_tokenizer_draft</code> step.
</p>


<h5>Usage</h5>

<div class="r"><pre>.AIFEBaseTransformer$set_SFC_save_tokenizer_draft(fun)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>fun</code></dt><dd><p><code style="white-space: pre;">&#8288;function()&#8288;</code> A new function.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>This method returns nothing.
</p>


<hr>
<a id="method-.AIFEBaseTransformer-set_SFC_create_final_tokenizer"></a>



<h4>Method <code>set_SFC_create_final_tokenizer()</code></h4>

<p>Setter for the <code>create_final_tokenizer</code> element of the private <code>steps_for_creation</code> list. Sets a new
<code>fun</code> function as the <code>create_final_tokenizer</code> step.
</p>


<h5>Usage</h5>

<div class="r"><pre>.AIFEBaseTransformer$set_SFC_create_final_tokenizer(fun)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>fun</code></dt><dd><p><code style="white-space: pre;">&#8288;function()&#8288;</code> A new function.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>This method returns nothing.
</p>


<hr>
<a id="method-.AIFEBaseTransformer-set_SFC_create_transformer_model"></a>



<h4>Method <code>set_SFC_create_transformer_model()</code></h4>

<p>Setter for the <code>create_transformer_model</code> element of the private <code>steps_for_creation</code> list. Sets a
new <code>fun</code> function as the <code>create_transformer_model</code> step.
</p>


<h5>Usage</h5>

<div class="r"><pre>.AIFEBaseTransformer$set_SFC_create_transformer_model(fun)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>fun</code></dt><dd><p><code style="white-space: pre;">&#8288;function()&#8288;</code> A new function.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>This method returns nothing.
</p>


<hr>
<a id="method-.AIFEBaseTransformer-set_required_SFC"></a>



<h4>Method <code>set_required_SFC()</code></h4>

<p>Setter for all required elements of the private <code>steps_for_creation</code> list. Executes setters for all
required creation steps.
</p>


<h5>Usage</h5>

<div class="r"><pre>.AIFEBaseTransformer$set_required_SFC(required_SFC)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>required_SFC</code></dt><dd><p><code>list()</code> A list of all new required steps.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>This method returns nothing.
</p>


<hr>
<a id="method-.AIFEBaseTransformer-set_SFT_load_existing_model"></a>



<h4>Method <code>set_SFT_load_existing_model()</code></h4>

<p>Setter for the <code>load_existing_model</code> element of the private <code>steps_for_training</code> list. Sets a new
<code>fun</code> function as the <code>load_existing_model</code> step.
</p>


<h5>Usage</h5>

<div class="r"><pre>.AIFEBaseTransformer$set_SFT_load_existing_model(fun)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>fun</code></dt><dd><p><code style="white-space: pre;">&#8288;function()&#8288;</code> A new function.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>This method returns nothing.
</p>


<hr>
<a id="method-.AIFEBaseTransformer-set_SFT_cuda_empty_cache"></a>



<h4>Method <code>set_SFT_cuda_empty_cache()</code></h4>

<p>Setter for the <code>cuda_empty_cache</code> element of the private <code>steps_for_training</code> list. Sets a new
<code>fun</code> function as the <code>cuda_empty_cache</code> step.
</p>


<h5>Usage</h5>

<div class="r"><pre>.AIFEBaseTransformer$set_SFT_cuda_empty_cache(fun)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>fun</code></dt><dd><p><code style="white-space: pre;">&#8288;function()&#8288;</code> A new function.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>This method returns nothing.
</p>


<hr>
<a id="method-.AIFEBaseTransformer-set_SFT_create_data_collator"></a>



<h4>Method <code>set_SFT_create_data_collator()</code></h4>

<p>Setter for the <code>create_data_collator</code> element of the private <code>steps_for_training</code> list. Sets a new
<code>fun</code> function as the <code>create_data_collator</code> step. Use this method to make a custom data collator for a
transformer.
</p>


<h5>Usage</h5>

<div class="r"><pre>.AIFEBaseTransformer$set_SFT_create_data_collator(fun)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>fun</code></dt><dd><p><code style="white-space: pre;">&#8288;function()&#8288;</code> A new function.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>This method returns nothing.
</p>


<hr>
<a id="method-.AIFEBaseTransformer-create"></a>



<h4>Method <code>create()</code></h4>

<p>This method creates a transformer configuration based on the child-transformer architecture and a
vocabulary using the python libraries <code>transformers</code> and <code>tokenizers</code>.
</p>
<p>This method <strong>adds</strong> the following parameters to the <code>temp</code> list:
</p>

<ul>
<li> <p><code>log_file</code>
</p>
</li>
<li> <p><code>raw_text_dataset</code>
</p>
</li>
<li> <p><code>pt_safe_save</code>
</p>
</li>
<li> <p><code>value_top</code>
</p>
</li>
<li> <p><code>total_top</code>
</p>
</li>
<li> <p><code>message_top</code>
</p>
</li></ul>

<p>This method <strong>uses</strong> the following parameters from the <code>temp</code> list:
</p>

<ul>
<li> <p><code>log_file</code>
</p>
</li>
<li> <p><code>raw_text_dataset</code>
</p>
</li>
<li> <p><code>tokenizer</code>
</p>
</li></ul>



<h5>Usage</h5>

<div class="r"><pre>.AIFEBaseTransformer$create(
  ml_framework,
  model_dir,
  text_dataset,
  vocab_size,
  max_position_embeddings,
  hidden_size,
  num_attention_heads,
  intermediate_size,
  hidden_act,
  hidden_dropout_prob,
  attention_probs_dropout_prob,
  sustain_track,
  sustain_iso_code,
  sustain_region,
  sustain_interval,
  trace,
  pytorch_safetensors,
  log_dir,
  log_write_interval
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>ml_framework</code></dt><dd><p><code>string</code> Framework to use for training and inference.
</p>

<ul>
<li> <p><code>ml_framework = "tensorflow"</code>: for 'tensorflow'.
</p>
</li>
<li> <p><code>ml_framework = "pytorch"</code>: for 'pytorch'.
</p>
</li></ul>
</dd>
<dt><code>model_dir</code></dt><dd><p><code>string</code> Path to the directory where the model should be saved.</p>
</dd>
<dt><code>text_dataset</code></dt><dd><p>Object of class <a href="#topic+LargeDataSetForText">LargeDataSetForText</a>.</p>
</dd>
<dt><code>vocab_size</code></dt><dd><p><code>int</code> Size of the vocabulary.</p>
</dd>
<dt><code>max_position_embeddings</code></dt><dd><p><code>int</code> Number of maximum position embeddings. This parameter also determines the maximum length of a sequence which
can be processed with the model.</p>
</dd>
<dt><code>hidden_size</code></dt><dd><p><code>int</code> Number of neurons in each layer. This parameter determines the dimensionality of the resulting text
embedding.</p>
</dd>
<dt><code>num_attention_heads</code></dt><dd><p><code>int</code> Number of attention heads.</p>
</dd>
<dt><code>intermediate_size</code></dt><dd><p><code>int</code> Number of neurons in the intermediate layer of the attention mechanism.</p>
</dd>
<dt><code>hidden_act</code></dt><dd><p><code>string</code> Name of the activation function.</p>
</dd>
<dt><code>hidden_dropout_prob</code></dt><dd><p><code>double</code> Ratio of dropout.</p>
</dd>
<dt><code>attention_probs_dropout_prob</code></dt><dd><p><code>double</code> Ratio of dropout for attention probabilities.</p>
</dd>
<dt><code>sustain_track</code></dt><dd><p><code>bool</code> If <code>TRUE</code> energy consumption is tracked during training via the python library codecarbon.</p>
</dd>
<dt><code>sustain_iso_code</code></dt><dd><p><code>string</code> ISO code (Alpha-3-Code) for the country. This variable must be set if sustainability should be tracked. A
list can be found on Wikipedia: <a href="https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes">https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes</a>.</p>
</dd>
<dt><code>sustain_region</code></dt><dd><p><code>string</code> Region within a country. Only available for USA and Canada. See the documentation of codecarbon for more
information <a href="https://mlco2.github.io/codecarbon/parameters.html">https://mlco2.github.io/codecarbon/parameters.html</a>.</p>
</dd>
<dt><code>sustain_interval</code></dt><dd><p><code>integer</code> Interval in seconds for measuring power usage.</p>
</dd>
<dt><code>trace</code></dt><dd><p><code>bool</code> <code>TRUE</code> if information about the progress should be printed to the console.</p>
</dd>
<dt><code>pytorch_safetensors</code></dt><dd><p><code>bool</code> Only relevant for pytorch models.
</p>

<ul>
<li> <p><code>TRUE</code>: a 'pytorch' model is saved in safetensors format.
</p>
</li>
<li> <p><code>FALSE</code> (or 'safetensors' is not available): model is saved in the standard pytorch format (.bin).
</p>
</li></ul>
</dd>
<dt><code>log_dir</code></dt><dd><p>Path to the directory where the log files should be saved.</p>
</dd>
<dt><code>log_write_interval</code></dt><dd><p><code>int</code> Time in seconds determining the interval in which the logger should try to update the log files. Only relevant
if <code>log_dir</code> is not <code>NULL</code>.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>This method does not return an object. Instead, it saves the configuration and vocabulary of the new
model to disk.
</p>


<hr>
<a id="method-.AIFEBaseTransformer-train"></a>



<h4>Method <code>train()</code></h4>

<p>This method can be used to train or fine-tune a transformer based on <code>BERT</code> architecture with the
help of the python libraries <code>transformers</code>, <code>datasets</code>, and <code>tokenizers</code>.
</p>
<p>This method <strong>adds</strong> the following parameters to the <code>temp</code> list:
</p>

<ul>
<li> <p><code>log_file</code>
</p>
</li>
<li> <p><code>loss_file</code>
</p>
</li>
<li> <p><code>from_pt</code>
</p>
</li>
<li> <p><code>from_tf</code>
</p>
</li>
<li> <p><code>load_safe</code>
</p>
</li>
<li> <p><code>raw_text_dataset</code>
</p>
</li>
<li> <p><code>pt_safe_save</code>
</p>
</li>
<li> <p><code>value_top</code>
</p>
</li>
<li> <p><code>total_top</code>
</p>
</li>
<li> <p><code>message_top</code>
</p>
</li></ul>

<p>This method <strong>uses</strong> the following parameters from the <code>temp</code> list:
</p>

<ul>
<li> <p><code>log_file</code>
</p>
</li>
<li> <p><code>raw_text_dataset</code>
</p>
</li>
<li> <p><code>tokenized_dataset</code>
</p>
</li>
<li> <p><code>tokenizer</code>
</p>
</li></ul>



<h5>Usage</h5>

<div class="r"><pre>.AIFEBaseTransformer$train(
  ml_framework,
  output_dir,
  model_dir_path,
  text_dataset,
  p_mask,
  whole_word,
  val_size,
  n_epoch,
  batch_size,
  chunk_size,
  full_sequences_only,
  min_seq_len,
  learning_rate,
  n_workers,
  multi_process,
  sustain_track,
  sustain_iso_code,
  sustain_region,
  sustain_interval,
  trace,
  keras_trace,
  pytorch_trace,
  pytorch_safetensors,
  log_dir,
  log_write_interval
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>ml_framework</code></dt><dd><p><code>string</code> Framework to use for training and inference.
</p>

<ul>
<li> <p><code>ml_framework = "tensorflow"</code>: for 'tensorflow'.
</p>
</li>
<li> <p><code>ml_framework = "pytorch"</code>: for 'pytorch'.
</p>
</li></ul>
</dd>
<dt><code>output_dir</code></dt><dd><p><code>string</code> Path to the directory where the final model should be saved. If the directory does not exist, it will be
created.</p>
</dd>
<dt><code>model_dir_path</code></dt><dd><p><code>string</code> Path to the directory where the original model is stored.</p>
</dd>
<dt><code>text_dataset</code></dt><dd><p>Object of class <a href="#topic+LargeDataSetForText">LargeDataSetForText</a>.</p>
</dd>
<dt><code>p_mask</code></dt><dd><p><code>double</code> Ratio that determines the number of words/tokens used for masking.</p>
</dd>
<dt><code>whole_word</code></dt><dd><p><code>bool</code>
</p>

<ul>
<li> <p><code>TRUE</code>: whole word masking should be applied.
</p>
</li>
<li> <p><code>FALSE</code>: token masking is used.
</p>
</li></ul>
</dd>
<dt><code>val_size</code></dt><dd><p><code>double</code> Ratio that determines the amount of token chunks used for validation.</p>
</dd>
<dt><code>n_epoch</code></dt><dd><p><code>int</code> Number of epochs for training.</p>
</dd>
<dt><code>batch_size</code></dt><dd><p><code>int</code> Size of batches.</p>
</dd>
<dt><code>chunk_size</code></dt><dd><p><code>int</code> Size of every chunk for training.</p>
</dd>
<dt><code>full_sequences_only</code></dt><dd><p><code>bool</code> <code>TRUE</code> for using only chunks with a sequence length equal to <code>chunk_size</code>.</p>
</dd>
<dt><code>min_seq_len</code></dt><dd><p><code>int</code> Only relevant if <code>full_sequences_only = FALSE</code>. Value determines the minimal sequence length included in
training process.</p>
</dd>
<dt><code>learning_rate</code></dt><dd><p><code>double</code> Learning rate for adam optimizer.</p>
</dd>
<dt><code>n_workers</code></dt><dd><p><code>int</code> Number of workers. Only relevant if <code>ml_framework = "tensorflow"</code>.</p>
</dd>
<dt><code>multi_process</code></dt><dd><p><code>bool</code> <code>TRUE</code> if multiple processes should be activated. Only relevant if <code>ml_framework = "tensorflow"</code>.</p>
</dd>
<dt><code>sustain_track</code></dt><dd><p><code>bool</code> If <code>TRUE</code> energy consumption is tracked during training via the python library codecarbon.</p>
</dd>
<dt><code>sustain_iso_code</code></dt><dd><p><code>string</code> ISO code (Alpha-3-Code) for the country. This variable must be set if sustainability should be tracked. A
list can be found on Wikipedia: <a href="https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes">https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes</a>.</p>
</dd>
<dt><code>sustain_region</code></dt><dd><p><code>string</code> Region within a country. Only available for USA and Canada. See the documentation of codecarbon for more
information <a href="https://mlco2.github.io/codecarbon/parameters.html">https://mlco2.github.io/codecarbon/parameters.html</a>.</p>
</dd>
<dt><code>sustain_interval</code></dt><dd><p><code>integer</code> Interval in seconds for measuring power usage.</p>
</dd>
<dt><code>trace</code></dt><dd><p><code>bool</code> <code>TRUE</code> if information about the progress should be printed to the console.</p>
</dd>
<dt><code>keras_trace</code></dt><dd><p><code>int</code>
</p>

<ul>
<li> <p><code>keras_trace = 0</code>: does not print any information about the training process from keras on the console.
</p>
</li>
<li> <p><code>keras_trace = 1</code>: prints a progress bar.
</p>
</li>
<li> <p><code>keras_trace = 2</code>: prints one line of information for every epoch. Only relevant if <code>ml_framework = "tensorflow"</code>.
</p>
</li></ul>
</dd>
<dt><code>pytorch_trace</code></dt><dd><p><code>int</code>
</p>

<ul>
<li> <p><code>pytorch_trace = 0</code>: does not print any information about the training process from pytorch on the console.
</p>
</li>
<li> <p><code>pytorch_trace = 1</code>: prints a progress bar.
</p>
</li></ul>
</dd>
<dt><code>pytorch_safetensors</code></dt><dd><p><code>bool</code> Only relevant for pytorch models.
</p>

<ul>
<li> <p><code>TRUE</code>: a 'pytorch' model is saved in safetensors format.
</p>
</li>
<li> <p><code>FALSE</code> (or 'safetensors' is not available): model is saved in the standard pytorch format (.bin).
</p>
</li></ul>
</dd>
<dt><code>log_dir</code></dt><dd><p>Path to the directory where the log files should be saved.</p>
</dd>
<dt><code>log_write_interval</code></dt><dd><p><code>int</code> Time in seconds determining the interval in which the logger should try to update the log files. Only relevant
if <code>log_dir</code> is not <code>NULL</code>.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>This method does not return an object. Instead, it saves the configuration and vocabulary of the new
model to disk.
</p>


<hr>
<a id="method-.AIFEBaseTransformer-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>.AIFEBaseTransformer$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>References</h3>

<p>Hugging Face transformers documantation:
</p>

<ul>
<li> <p><a href="https://huggingface.co/docs/transformers/model_doc/bert">BERT</a>
</p>
</li>
<li> <p><a href="https://huggingface.co/docs/transformers/model_doc/deberta-v2">DeBERTa</a>
</p>
</li>
<li> <p><a href="https://huggingface.co/docs/transformers/model_doc/funnel">Funnel</a>
</p>
</li>
<li> <p><a href="https://huggingface.co/docs/transformers/model_doc/longformer">Longformer</a>
</p>
</li>
<li> <p><a href="https://huggingface.co/docs/transformers/model_doc/roberta">RoBERTa</a>
</p>
</li>
<li> <p><a href="https://huggingface.co/docs/transformers/model_doc/mpnet">MPNet</a>
</p>
</li></ul>



<h3>See Also</h3>

<p>Other Transformers for developers: 
<code><a href="#topic+.AIFEBertTransformer">.AIFEBertTransformer</a></code>,
<code><a href="#topic+.AIFEDebertaTransformer">.AIFEDebertaTransformer</a></code>,
<code><a href="#topic+.AIFEFunnelTransformer">.AIFEFunnelTransformer</a></code>,
<code><a href="#topic+.AIFELongformerTransformer">.AIFELongformerTransformer</a></code>,
<code><a href="#topic+.AIFEMpnetTransformer">.AIFEMpnetTransformer</a></code>,
<code><a href="#topic+.AIFERobertaTransformer">.AIFERobertaTransformer</a></code>,
<code><a href="#topic+.AIFETrObj">.AIFETrObj</a></code>
</p>

<hr>
<h2 id='.AIFEBertTransformer'>Child <code>R6</code> class for creation and training of <code>BERT</code> transformers</h2><span id='topic+.AIFEBertTransformer'></span>

<h3>Description</h3>

<p>This class has the following methods:
</p>

<ul>
<li> <p><code>create</code>: creates a new transformer based on <code>BERT</code>.
</p>
</li>
<li> <p><code>train</code>: trains and fine-tunes a <code>BERT</code> model.
</p>
</li></ul>



<h3>Create</h3>

<p>New models can be created using the <code>.AIFEBertTransformer$create</code> method.
</p>


<h3>Train</h3>

<p>To train the model, pass the directory of the model to the method <code>.AIFEBertTransformer$train</code>.
</p>
<p>Pre-Trained models that can be fine-tuned using this method are available at <a href="https://huggingface.co/">https://huggingface.co/</a>.
</p>
<p>The model is trained using dynamic masking, as opposed to the original paper, which used static masking.
</p>


<h3>Super class</h3>

<p><code><a href="#topic+.AIFEBaseTransformer">aifeducation::.AIFEBaseTransformer</a></code> -&gt; <code>.AIFEBertTransformer</code>
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-.AIFEBertTransformer-new"><code>.AIFEBertTransformer$new()</code></a>
</p>
</li>
<li> <p><a href="#method-.AIFEBertTransformer-create"><code>.AIFEBertTransformer$create()</code></a>
</p>
</li>
<li> <p><a href="#method-.AIFEBertTransformer-train"><code>.AIFEBertTransformer$train()</code></a>
</p>
</li>
<li> <p><a href="#method-.AIFEBertTransformer-clone"><code>.AIFEBertTransformer$clone()</code></a>
</p>
</li></ul>



<details><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_SFC_calculate_vocab"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_SFC_calculate_vocab'><code>aifeducation::.AIFEBaseTransformer$set_SFC_calculate_vocab()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_SFC_check_max_pos_emb"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_SFC_check_max_pos_emb'><code>aifeducation::.AIFEBaseTransformer$set_SFC_check_max_pos_emb()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_SFC_create_final_tokenizer"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_SFC_create_final_tokenizer'><code>aifeducation::.AIFEBaseTransformer$set_SFC_create_final_tokenizer()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_SFC_create_tokenizer_draft"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_SFC_create_tokenizer_draft'><code>aifeducation::.AIFEBaseTransformer$set_SFC_create_tokenizer_draft()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_SFC_create_transformer_model"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_SFC_create_transformer_model'><code>aifeducation::.AIFEBaseTransformer$set_SFC_create_transformer_model()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_SFC_save_tokenizer_draft"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_SFC_save_tokenizer_draft'><code>aifeducation::.AIFEBaseTransformer$set_SFC_save_tokenizer_draft()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_SFT_create_data_collator"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_SFT_create_data_collator'><code>aifeducation::.AIFEBaseTransformer$set_SFT_create_data_collator()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_SFT_cuda_empty_cache"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_SFT_cuda_empty_cache'><code>aifeducation::.AIFEBaseTransformer$set_SFT_cuda_empty_cache()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_SFT_load_existing_model"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_SFT_load_existing_model'><code>aifeducation::.AIFEBaseTransformer$set_SFT_load_existing_model()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_model_param"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_model_param'><code>aifeducation::.AIFEBaseTransformer$set_model_param()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_model_temp"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_model_temp'><code>aifeducation::.AIFEBaseTransformer$set_model_temp()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_required_SFC"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_required_SFC'><code>aifeducation::.AIFEBaseTransformer$set_required_SFC()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_title"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_title'><code>aifeducation::.AIFEBaseTransformer$set_title()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-.AIFEBertTransformer-new"></a>



<h4>Method <code>new()</code></h4>

<p>Creates a new transformer based on <code>BERT</code> and sets the title.
</p>


<h5>Usage</h5>

<div class="r"><pre>.AIFEBertTransformer$new()</pre></div>



<h5>Returns</h5>

<p>This method returns nothing.
</p>


<hr>
<a id="method-.AIFEBertTransformer-create"></a>



<h4>Method <code>create()</code></h4>

<p>This method creates a transformer configuration based on the <code>BERT</code> base architecture and a
vocabulary based on <code>WordPiece</code> by using the python libraries <code>transformers</code> and <code>tokenizers</code>.
</p>
<p>This method adds the following <em>'dependent' parameters</em> to the base class's inherited <code>params</code> list:
</p>

<ul>
<li> <p><code>vocab_do_lower_case</code>
</p>
</li>
<li> <p><code>num_hidden_layer</code>
</p>
</li></ul>



<h5>Usage</h5>

<div class="r"><pre>.AIFEBertTransformer$create(
  ml_framework = "pytorch",
  model_dir,
  text_dataset,
  vocab_size = 30522,
  vocab_do_lower_case = FALSE,
  max_position_embeddings = 512,
  hidden_size = 768,
  num_hidden_layer = 12,
  num_attention_heads = 12,
  intermediate_size = 3072,
  hidden_act = "gelu",
  hidden_dropout_prob = 0.1,
  attention_probs_dropout_prob = 0.1,
  sustain_track = FALSE,
  sustain_iso_code = NULL,
  sustain_region = NULL,
  sustain_interval = 15,
  trace = TRUE,
  pytorch_safetensors = TRUE,
  log_dir = NULL,
  log_write_interval = 2
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>ml_framework</code></dt><dd><p><code>string</code> Framework to use for training and inference.
</p>

<ul>
<li> <p><code>ml_framework = "tensorflow"</code>: for 'tensorflow'.
</p>
</li>
<li> <p><code>ml_framework = "pytorch"</code>: for 'pytorch'.
</p>
</li></ul>
</dd>
<dt><code>model_dir</code></dt><dd><p><code>string</code> Path to the directory where the model should be saved.</p>
</dd>
<dt><code>text_dataset</code></dt><dd><p>Object of class <a href="#topic+LargeDataSetForText">LargeDataSetForText</a>.</p>
</dd>
<dt><code>vocab_size</code></dt><dd><p><code>int</code> Size of the vocabulary.</p>
</dd>
<dt><code>vocab_do_lower_case</code></dt><dd><p><code>bool</code> <code>TRUE</code> if all words/tokens should be lower case.</p>
</dd>
<dt><code>max_position_embeddings</code></dt><dd><p><code>int</code> Number of maximum position embeddings. This parameter also determines the maximum length of a sequence which
can be processed with the model.</p>
</dd>
<dt><code>hidden_size</code></dt><dd><p><code>int</code> Number of neurons in each layer. This parameter determines the dimensionality of the resulting text
embedding.</p>
</dd>
<dt><code>num_hidden_layer</code></dt><dd><p><code>int</code> Number of hidden layers.</p>
</dd>
<dt><code>num_attention_heads</code></dt><dd><p><code>int</code> Number of attention heads.</p>
</dd>
<dt><code>intermediate_size</code></dt><dd><p><code>int</code> Number of neurons in the intermediate layer of the attention mechanism.</p>
</dd>
<dt><code>hidden_act</code></dt><dd><p><code>string</code> Name of the activation function.</p>
</dd>
<dt><code>hidden_dropout_prob</code></dt><dd><p><code>double</code> Ratio of dropout.</p>
</dd>
<dt><code>attention_probs_dropout_prob</code></dt><dd><p><code>double</code> Ratio of dropout for attention probabilities.</p>
</dd>
<dt><code>sustain_track</code></dt><dd><p><code>bool</code> If <code>TRUE</code> energy consumption is tracked during training via the python library codecarbon.</p>
</dd>
<dt><code>sustain_iso_code</code></dt><dd><p><code>string</code> ISO code (Alpha-3-Code) for the country. This variable must be set if sustainability should be tracked. A
list can be found on Wikipedia: <a href="https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes">https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes</a>.</p>
</dd>
<dt><code>sustain_region</code></dt><dd><p><code>string</code> Region within a country. Only available for USA and Canada. See the documentation of codecarbon for more
information <a href="https://mlco2.github.io/codecarbon/parameters.html">https://mlco2.github.io/codecarbon/parameters.html</a>.</p>
</dd>
<dt><code>sustain_interval</code></dt><dd><p><code>integer</code> Interval in seconds for measuring power usage.</p>
</dd>
<dt><code>trace</code></dt><dd><p><code>bool</code> <code>TRUE</code> if information about the progress should be printed to the console.</p>
</dd>
<dt><code>pytorch_safetensors</code></dt><dd><p><code>bool</code> Only relevant for pytorch models.
</p>

<ul>
<li> <p><code>TRUE</code>: a 'pytorch' model is saved in safetensors format.
</p>
</li>
<li> <p><code>FALSE</code> (or 'safetensors' is not available): model is saved in the standard pytorch format (.bin).
</p>
</li></ul>
</dd>
<dt><code>log_dir</code></dt><dd><p>Path to the directory where the log files should be saved.</p>
</dd>
<dt><code>log_write_interval</code></dt><dd><p><code>int</code> Time in seconds determining the interval in which the logger should try to update the log files. Only relevant
if <code>log_dir</code> is not <code>NULL</code>.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>This method does not return an object. Instead, it saves the configuration and vocabulary of the new
model to disk.
</p>


<hr>
<a id="method-.AIFEBertTransformer-train"></a>



<h4>Method <code>train()</code></h4>

<p>This method can be used to train or fine-tune a transformer based on <code>BERT</code> architecture with the
help of the python libraries <code>transformers</code>, <code>datasets</code>, and <code>tokenizers</code>.
</p>


<h5>Usage</h5>

<div class="r"><pre>.AIFEBertTransformer$train(
  ml_framework = "pytorch",
  output_dir,
  model_dir_path,
  text_dataset,
  p_mask = 0.15,
  whole_word = TRUE,
  val_size = 0.1,
  n_epoch = 1,
  batch_size = 12,
  chunk_size = 250,
  full_sequences_only = FALSE,
  min_seq_len = 50,
  learning_rate = 0.003,
  n_workers = 1,
  multi_process = FALSE,
  sustain_track = FALSE,
  sustain_iso_code = NULL,
  sustain_region = NULL,
  sustain_interval = 15,
  trace = TRUE,
  keras_trace = 1,
  pytorch_trace = 1,
  pytorch_safetensors = TRUE,
  log_dir = NULL,
  log_write_interval = 2
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>ml_framework</code></dt><dd><p><code>string</code> Framework to use for training and inference.
</p>

<ul>
<li> <p><code>ml_framework = "tensorflow"</code>: for 'tensorflow'.
</p>
</li>
<li> <p><code>ml_framework = "pytorch"</code>: for 'pytorch'.
</p>
</li></ul>
</dd>
<dt><code>output_dir</code></dt><dd><p><code>string</code> Path to the directory where the final model should be saved. If the directory does not exist, it will be
created.</p>
</dd>
<dt><code>model_dir_path</code></dt><dd><p><code>string</code> Path to the directory where the original model is stored.</p>
</dd>
<dt><code>text_dataset</code></dt><dd><p>Object of class <a href="#topic+LargeDataSetForText">LargeDataSetForText</a>.</p>
</dd>
<dt><code>p_mask</code></dt><dd><p><code>double</code> Ratio that determines the number of words/tokens used for masking.</p>
</dd>
<dt><code>whole_word</code></dt><dd><p><code>bool</code>
</p>

<ul>
<li> <p><code>TRUE</code>: whole word masking should be applied.
</p>
</li>
<li> <p><code>FALSE</code>: token masking is used.
</p>
</li></ul>
</dd>
<dt><code>val_size</code></dt><dd><p><code>double</code> Ratio that determines the amount of token chunks used for validation.</p>
</dd>
<dt><code>n_epoch</code></dt><dd><p><code>int</code> Number of epochs for training.</p>
</dd>
<dt><code>batch_size</code></dt><dd><p><code>int</code> Size of batches.</p>
</dd>
<dt><code>chunk_size</code></dt><dd><p><code>int</code> Size of every chunk for training.</p>
</dd>
<dt><code>full_sequences_only</code></dt><dd><p><code>bool</code> <code>TRUE</code> for using only chunks with a sequence length equal to <code>chunk_size</code>.</p>
</dd>
<dt><code>min_seq_len</code></dt><dd><p><code>int</code> Only relevant if <code>full_sequences_only = FALSE</code>. Value determines the minimal sequence length included in
training process.</p>
</dd>
<dt><code>learning_rate</code></dt><dd><p><code>double</code> Learning rate for adam optimizer.</p>
</dd>
<dt><code>n_workers</code></dt><dd><p><code>int</code> Number of workers. Only relevant if <code>ml_framework = "tensorflow"</code>.</p>
</dd>
<dt><code>multi_process</code></dt><dd><p><code>bool</code> <code>TRUE</code> if multiple processes should be activated. Only relevant if <code>ml_framework = "tensorflow"</code>.</p>
</dd>
<dt><code>sustain_track</code></dt><dd><p><code>bool</code> If <code>TRUE</code> energy consumption is tracked during training via the python library codecarbon.</p>
</dd>
<dt><code>sustain_iso_code</code></dt><dd><p><code>string</code> ISO code (Alpha-3-Code) for the country. This variable must be set if sustainability should be tracked. A
list can be found on Wikipedia: <a href="https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes">https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes</a>.</p>
</dd>
<dt><code>sustain_region</code></dt><dd><p><code>string</code> Region within a country. Only available for USA and Canada. See the documentation of codecarbon for more
information <a href="https://mlco2.github.io/codecarbon/parameters.html">https://mlco2.github.io/codecarbon/parameters.html</a>.</p>
</dd>
<dt><code>sustain_interval</code></dt><dd><p><code>integer</code> Interval in seconds for measuring power usage.</p>
</dd>
<dt><code>trace</code></dt><dd><p><code>bool</code> <code>TRUE</code> if information about the progress should be printed to the console.</p>
</dd>
<dt><code>keras_trace</code></dt><dd><p><code>int</code>
</p>

<ul>
<li> <p><code>keras_trace = 0</code>: does not print any information about the training process from keras on the console.
</p>
</li>
<li> <p><code>keras_trace = 1</code>: prints a progress bar.
</p>
</li>
<li> <p><code>keras_trace = 2</code>: prints one line of information for every epoch. Only relevant if <code>ml_framework = "tensorflow"</code>.
</p>
</li></ul>
</dd>
<dt><code>pytorch_trace</code></dt><dd><p><code>int</code>
</p>

<ul>
<li> <p><code>pytorch_trace = 0</code>: does not print any information about the training process from pytorch on the console.
</p>
</li>
<li> <p><code>pytorch_trace = 1</code>: prints a progress bar.
</p>
</li></ul>
</dd>
<dt><code>pytorch_safetensors</code></dt><dd><p><code>bool</code> Only relevant for pytorch models.
</p>

<ul>
<li> <p><code>TRUE</code>: a 'pytorch' model is saved in safetensors format.
</p>
</li>
<li> <p><code>FALSE</code> (or 'safetensors' is not available): model is saved in the standard pytorch format (.bin).
</p>
</li></ul>
</dd>
<dt><code>log_dir</code></dt><dd><p>Path to the directory where the log files should be saved.</p>
</dd>
<dt><code>log_write_interval</code></dt><dd><p><code>int</code> Time in seconds determining the interval in which the logger should try to update the log files. Only relevant
if <code>log_dir</code> is not <code>NULL</code>.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>This method does not return an object. Instead the trained or fine-tuned model is saved to disk.
</p>


<hr>
<a id="method-.AIFEBertTransformer-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>.AIFEBertTransformer$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>Note</h3>

<p>This model uses a <code>WordPiece</code> tokenizer like <code>BERT</code> and can be trained with whole word masking. The transformer
library may display a warning, which can be ignored.
</p>


<h3>References</h3>

<p>Devlin, J., Chang, M.‑W., Lee, K., &amp; Toutanova, K. (2019). BERT: Pre-training of Deep Bidirectional
Transformers for Language Understanding. In J. Burstein, C. Doran, &amp; T. Solorio (Eds.), Proceedings of the 2019
Conference of the North (pp. 4171&ndash;4186). Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/N19-1423">doi:10.18653/v1/N19-1423</a>
</p>
<p>Hugging Face documentation
</p>

<ul>
<li> <p><a href="https://huggingface.co/docs/transformers/model_doc/bert">https://huggingface.co/docs/transformers/model_doc/bert</a>
</p>
</li>
<li> <p><a href="https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertForMaskedLM">https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertForMaskedLM</a>
</p>
</li>
<li> <p><a href="https://huggingface.co/docs/transformers/model_doc/bert#transformers.TFBertForMaskedLM">https://huggingface.co/docs/transformers/model_doc/bert#transformers.TFBertForMaskedLM</a>
</p>
</li></ul>



<h3>See Also</h3>

<p>Other Transformers for developers: 
<code><a href="#topic+.AIFEBaseTransformer">.AIFEBaseTransformer</a></code>,
<code><a href="#topic+.AIFEDebertaTransformer">.AIFEDebertaTransformer</a></code>,
<code><a href="#topic+.AIFEFunnelTransformer">.AIFEFunnelTransformer</a></code>,
<code><a href="#topic+.AIFELongformerTransformer">.AIFELongformerTransformer</a></code>,
<code><a href="#topic+.AIFEMpnetTransformer">.AIFEMpnetTransformer</a></code>,
<code><a href="#topic+.AIFERobertaTransformer">.AIFERobertaTransformer</a></code>,
<code><a href="#topic+.AIFETrObj">.AIFETrObj</a></code>
</p>

<hr>
<h2 id='.AIFEDebertaTransformer'>Child <code>R6</code> class for creation and training of <code>DeBERTa-V2</code> transformers</h2><span id='topic+.AIFEDebertaTransformer'></span>

<h3>Description</h3>

<p>This class has the following methods:
</p>

<ul>
<li> <p><code>create</code>: creates a new transformer based on <code>DeBERTa-V2</code>.
</p>
</li>
<li> <p><code>train</code>: trains and fine-tunes a <code>DeBERTa-V2</code> model.
</p>
</li></ul>



<h3>Create</h3>

<p>New models can be created using the <code>.AIFEDebertaTransformer$create</code> method.
</p>


<h3>Train</h3>

<p>To train the model, pass the directory of the model to the method <code>.AIFEDebertaTransformer$train</code>.
</p>
<p>Pre-Trained models which can be fine-tuned with this function are available at <a href="https://huggingface.co/">https://huggingface.co/</a>.
</p>
<p>Training of this model makes use of dynamic masking.
</p>


<h3>Super class</h3>

<p><code><a href="#topic+.AIFEBaseTransformer">aifeducation::.AIFEBaseTransformer</a></code> -&gt; <code>.AIFEDebertaTransformer</code>
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-.AIFEDebertaTransformer-new"><code>.AIFEDebertaTransformer$new()</code></a>
</p>
</li>
<li> <p><a href="#method-.AIFEDebertaTransformer-create"><code>.AIFEDebertaTransformer$create()</code></a>
</p>
</li>
<li> <p><a href="#method-.AIFEDebertaTransformer-train"><code>.AIFEDebertaTransformer$train()</code></a>
</p>
</li>
<li> <p><a href="#method-.AIFEDebertaTransformer-clone"><code>.AIFEDebertaTransformer$clone()</code></a>
</p>
</li></ul>



<details><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_SFC_calculate_vocab"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_SFC_calculate_vocab'><code>aifeducation::.AIFEBaseTransformer$set_SFC_calculate_vocab()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_SFC_check_max_pos_emb"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_SFC_check_max_pos_emb'><code>aifeducation::.AIFEBaseTransformer$set_SFC_check_max_pos_emb()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_SFC_create_final_tokenizer"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_SFC_create_final_tokenizer'><code>aifeducation::.AIFEBaseTransformer$set_SFC_create_final_tokenizer()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_SFC_create_tokenizer_draft"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_SFC_create_tokenizer_draft'><code>aifeducation::.AIFEBaseTransformer$set_SFC_create_tokenizer_draft()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_SFC_create_transformer_model"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_SFC_create_transformer_model'><code>aifeducation::.AIFEBaseTransformer$set_SFC_create_transformer_model()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_SFC_save_tokenizer_draft"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_SFC_save_tokenizer_draft'><code>aifeducation::.AIFEBaseTransformer$set_SFC_save_tokenizer_draft()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_SFT_create_data_collator"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_SFT_create_data_collator'><code>aifeducation::.AIFEBaseTransformer$set_SFT_create_data_collator()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_SFT_cuda_empty_cache"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_SFT_cuda_empty_cache'><code>aifeducation::.AIFEBaseTransformer$set_SFT_cuda_empty_cache()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_SFT_load_existing_model"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_SFT_load_existing_model'><code>aifeducation::.AIFEBaseTransformer$set_SFT_load_existing_model()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_model_param"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_model_param'><code>aifeducation::.AIFEBaseTransformer$set_model_param()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_model_temp"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_model_temp'><code>aifeducation::.AIFEBaseTransformer$set_model_temp()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_required_SFC"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_required_SFC'><code>aifeducation::.AIFEBaseTransformer$set_required_SFC()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_title"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_title'><code>aifeducation::.AIFEBaseTransformer$set_title()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-.AIFEDebertaTransformer-new"></a>



<h4>Method <code>new()</code></h4>

<p>Creates a new transformer based on <code>DeBERTa-V2</code> and sets the title.
</p>


<h5>Usage</h5>

<div class="r"><pre>.AIFEDebertaTransformer$new()</pre></div>



<h5>Returns</h5>

<p>This method returns nothing.
</p>


<hr>
<a id="method-.AIFEDebertaTransformer-create"></a>



<h4>Method <code>create()</code></h4>

<p>This method creates a transformer configuration based on the <code>DeBERTa-V2</code> base architecture and a
vocabulary based on the <code>SentencePiece</code> tokenizer using the python <code>transformers</code> and <code>tokenizers</code> libraries.
</p>
<p>This method adds the following <em>'dependent' parameters</em> to the base class's inherited <code>params</code> list:
</p>

<ul>
<li> <p><code>vocab_do_lower_case</code>
</p>
</li>
<li> <p><code>num_hidden_layer</code>
</p>
</li></ul>



<h5>Usage</h5>

<div class="r"><pre>.AIFEDebertaTransformer$create(
  ml_framework = "pytorch",
  model_dir,
  text_dataset,
  vocab_size = 128100,
  vocab_do_lower_case = FALSE,
  max_position_embeddings = 512,
  hidden_size = 1536,
  num_hidden_layer = 24,
  num_attention_heads = 24,
  intermediate_size = 6144,
  hidden_act = "gelu",
  hidden_dropout_prob = 0.1,
  attention_probs_dropout_prob = 0.1,
  sustain_track = TRUE,
  sustain_iso_code = NULL,
  sustain_region = NULL,
  sustain_interval = 15,
  trace = TRUE,
  pytorch_safetensors = TRUE,
  log_dir = NULL,
  log_write_interval = 2
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>ml_framework</code></dt><dd><p><code>string</code> Framework to use for training and inference.
</p>

<ul>
<li> <p><code>ml_framework = "tensorflow"</code>: for 'tensorflow'.
</p>
</li>
<li> <p><code>ml_framework = "pytorch"</code>: for 'pytorch'.
</p>
</li></ul>
</dd>
<dt><code>model_dir</code></dt><dd><p><code>string</code> Path to the directory where the model should be saved.</p>
</dd>
<dt><code>text_dataset</code></dt><dd><p>Object of class <a href="#topic+LargeDataSetForText">LargeDataSetForText</a>.</p>
</dd>
<dt><code>vocab_size</code></dt><dd><p><code>int</code> Size of the vocabulary.</p>
</dd>
<dt><code>vocab_do_lower_case</code></dt><dd><p><code>bool</code> <code>TRUE</code> if all words/tokens should be lower case.</p>
</dd>
<dt><code>max_position_embeddings</code></dt><dd><p><code>int</code> Number of maximum position embeddings. This parameter also determines the maximum length of a sequence which
can be processed with the model.</p>
</dd>
<dt><code>hidden_size</code></dt><dd><p><code>int</code> Number of neurons in each layer. This parameter determines the dimensionality of the resulting text
embedding.</p>
</dd>
<dt><code>num_hidden_layer</code></dt><dd><p><code>int</code> Number of hidden layers.</p>
</dd>
<dt><code>num_attention_heads</code></dt><dd><p><code>int</code> Number of attention heads.</p>
</dd>
<dt><code>intermediate_size</code></dt><dd><p><code>int</code> Number of neurons in the intermediate layer of the attention mechanism.</p>
</dd>
<dt><code>hidden_act</code></dt><dd><p><code>string</code> Name of the activation function.</p>
</dd>
<dt><code>hidden_dropout_prob</code></dt><dd><p><code>double</code> Ratio of dropout.</p>
</dd>
<dt><code>attention_probs_dropout_prob</code></dt><dd><p><code>double</code> Ratio of dropout for attention probabilities.</p>
</dd>
<dt><code>sustain_track</code></dt><dd><p><code>bool</code> If <code>TRUE</code> energy consumption is tracked during training via the python library codecarbon.</p>
</dd>
<dt><code>sustain_iso_code</code></dt><dd><p><code>string</code> ISO code (Alpha-3-Code) for the country. This variable must be set if sustainability should be tracked. A
list can be found on Wikipedia: <a href="https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes">https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes</a>.</p>
</dd>
<dt><code>sustain_region</code></dt><dd><p><code>string</code> Region within a country. Only available for USA and Canada. See the documentation of codecarbon for more
information <a href="https://mlco2.github.io/codecarbon/parameters.html">https://mlco2.github.io/codecarbon/parameters.html</a>.</p>
</dd>
<dt><code>sustain_interval</code></dt><dd><p><code>integer</code> Interval in seconds for measuring power usage.</p>
</dd>
<dt><code>trace</code></dt><dd><p><code>bool</code> <code>TRUE</code> if information about the progress should be printed to the console.</p>
</dd>
<dt><code>pytorch_safetensors</code></dt><dd><p><code>bool</code> Only relevant for pytorch models.
</p>

<ul>
<li> <p><code>TRUE</code>: a 'pytorch' model is saved in safetensors format.
</p>
</li>
<li> <p><code>FALSE</code> (or 'safetensors' is not available): model is saved in the standard pytorch format (.bin).
</p>
</li></ul>
</dd>
<dt><code>log_dir</code></dt><dd><p>Path to the directory where the log files should be saved.</p>
</dd>
<dt><code>log_write_interval</code></dt><dd><p><code>int</code> Time in seconds determining the interval in which the logger should try to update the log files. Only relevant
if <code>log_dir</code> is not <code>NULL</code>.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>This method does not return an object. Instead, it saves the configuration and vocabulary of the new
model to disk.
</p>


<hr>
<a id="method-.AIFEDebertaTransformer-train"></a>



<h4>Method <code>train()</code></h4>

<p>This method can be used to train or fine-tune a transformer based on <code>DeBERTa-V2</code> architecture with
the help of the python libraries <code>transformers</code>, <code>datasets</code>, and <code>tokenizers</code>.
</p>


<h5>Usage</h5>

<div class="r"><pre>.AIFEDebertaTransformer$train(
  ml_framework = "pytorch",
  output_dir,
  model_dir_path,
  text_dataset,
  p_mask = 0.15,
  whole_word = TRUE,
  val_size = 0.1,
  n_epoch = 1,
  batch_size = 12,
  chunk_size = 250,
  full_sequences_only = FALSE,
  min_seq_len = 50,
  learning_rate = 0.03,
  n_workers = 1,
  multi_process = FALSE,
  sustain_track = TRUE,
  sustain_iso_code = NULL,
  sustain_region = NULL,
  sustain_interval = 15,
  trace = TRUE,
  keras_trace = 1,
  pytorch_trace = 1,
  pytorch_safetensors = TRUE,
  log_dir = NULL,
  log_write_interval = 2
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>ml_framework</code></dt><dd><p><code>string</code> Framework to use for training and inference.
</p>

<ul>
<li> <p><code>ml_framework = "tensorflow"</code>: for 'tensorflow'.
</p>
</li>
<li> <p><code>ml_framework = "pytorch"</code>: for 'pytorch'.
</p>
</li></ul>
</dd>
<dt><code>output_dir</code></dt><dd><p><code>string</code> Path to the directory where the final model should be saved. If the directory does not exist, it will be
created.</p>
</dd>
<dt><code>model_dir_path</code></dt><dd><p><code>string</code> Path to the directory where the original model is stored.</p>
</dd>
<dt><code>text_dataset</code></dt><dd><p>Object of class <a href="#topic+LargeDataSetForText">LargeDataSetForText</a>.</p>
</dd>
<dt><code>p_mask</code></dt><dd><p><code>double</code> Ratio that determines the number of words/tokens used for masking.</p>
</dd>
<dt><code>whole_word</code></dt><dd><p><code>bool</code>
</p>

<ul>
<li> <p><code>TRUE</code>: whole word masking should be applied.
</p>
</li>
<li> <p><code>FALSE</code>: token masking is used.
</p>
</li></ul>
</dd>
<dt><code>val_size</code></dt><dd><p><code>double</code> Ratio that determines the amount of token chunks used for validation.</p>
</dd>
<dt><code>n_epoch</code></dt><dd><p><code>int</code> Number of epochs for training.</p>
</dd>
<dt><code>batch_size</code></dt><dd><p><code>int</code> Size of batches.</p>
</dd>
<dt><code>chunk_size</code></dt><dd><p><code>int</code> Size of every chunk for training.</p>
</dd>
<dt><code>full_sequences_only</code></dt><dd><p><code>bool</code> <code>TRUE</code> for using only chunks with a sequence length equal to <code>chunk_size</code>.</p>
</dd>
<dt><code>min_seq_len</code></dt><dd><p><code>int</code> Only relevant if <code>full_sequences_only = FALSE</code>. Value determines the minimal sequence length included in
training process.</p>
</dd>
<dt><code>learning_rate</code></dt><dd><p><code>double</code> Learning rate for adam optimizer.</p>
</dd>
<dt><code>n_workers</code></dt><dd><p><code>int</code> Number of workers. Only relevant if <code>ml_framework = "tensorflow"</code>.</p>
</dd>
<dt><code>multi_process</code></dt><dd><p><code>bool</code> <code>TRUE</code> if multiple processes should be activated. Only relevant if <code>ml_framework = "tensorflow"</code>.</p>
</dd>
<dt><code>sustain_track</code></dt><dd><p><code>bool</code> If <code>TRUE</code> energy consumption is tracked during training via the python library codecarbon.</p>
</dd>
<dt><code>sustain_iso_code</code></dt><dd><p><code>string</code> ISO code (Alpha-3-Code) for the country. This variable must be set if sustainability should be tracked. A
list can be found on Wikipedia: <a href="https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes">https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes</a>.</p>
</dd>
<dt><code>sustain_region</code></dt><dd><p><code>string</code> Region within a country. Only available for USA and Canada. See the documentation of codecarbon for more
information <a href="https://mlco2.github.io/codecarbon/parameters.html">https://mlco2.github.io/codecarbon/parameters.html</a>.</p>
</dd>
<dt><code>sustain_interval</code></dt><dd><p><code>integer</code> Interval in seconds for measuring power usage.</p>
</dd>
<dt><code>trace</code></dt><dd><p><code>bool</code> <code>TRUE</code> if information about the progress should be printed to the console.</p>
</dd>
<dt><code>keras_trace</code></dt><dd><p><code>int</code>
</p>

<ul>
<li> <p><code>keras_trace = 0</code>: does not print any information about the training process from keras on the console.
</p>
</li>
<li> <p><code>keras_trace = 1</code>: prints a progress bar.
</p>
</li>
<li> <p><code>keras_trace = 2</code>: prints one line of information for every epoch. Only relevant if <code>ml_framework = "tensorflow"</code>.
</p>
</li></ul>
</dd>
<dt><code>pytorch_trace</code></dt><dd><p><code>int</code>
</p>

<ul>
<li> <p><code>pytorch_trace = 0</code>: does not print any information about the training process from pytorch on the console.
</p>
</li>
<li> <p><code>pytorch_trace = 1</code>: prints a progress bar.
</p>
</li></ul>
</dd>
<dt><code>pytorch_safetensors</code></dt><dd><p><code>bool</code> Only relevant for pytorch models.
</p>

<ul>
<li> <p><code>TRUE</code>: a 'pytorch' model is saved in safetensors format.
</p>
</li>
<li> <p><code>FALSE</code> (or 'safetensors' is not available): model is saved in the standard pytorch format (.bin).
</p>
</li></ul>
</dd>
<dt><code>log_dir</code></dt><dd><p>Path to the directory where the log files should be saved.</p>
</dd>
<dt><code>log_write_interval</code></dt><dd><p><code>int</code> Time in seconds determining the interval in which the logger should try to update the log files. Only relevant
if <code>log_dir</code> is not <code>NULL</code>.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>This method does not return an object. Instead the trained or fine-tuned model is saved to disk.
</p>


<hr>
<a id="method-.AIFEDebertaTransformer-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>.AIFEDebertaTransformer$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>Note</h3>

<p>For this model a <code>WordPiece</code> tokenizer is created. The standard implementation of <code>DeBERTa</code> version 2 from
HuggingFace uses a <code>SentencePiece</code> tokenizer. Thus, please use <code>AutoTokenizer</code> from the <code>transformers</code> library to
work with this model.
</p>


<h3>References</h3>

<p>He, P., Liu, X., Gao, J. &amp; Chen, W. (2020). DeBERTa: Decoding-enhanced BERT with Disentangled Attention.
<a href="https://doi.org/10.48550/arXiv.2006.03654">doi:10.48550/arXiv.2006.03654</a>
</p>
<p>Hugging Face documentatio
</p>

<ul>
<li> <p><a href="https://huggingface.co/docs/transformers/model_doc/deberta-v2">https://huggingface.co/docs/transformers/model_doc/deberta-v2</a>
</p>
</li>
<li> <p><a href="https://huggingface.co/docs/transformers/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM">https://huggingface.co/docs/transformers/model_doc/deberta-v2#transformers.DebertaV2ForMaskedLM</a>
</p>
</li>
<li> <p><a href="https://huggingface.co/docs/transformers/model_doc/deberta-v2#transformers.TFDebertaV2ForMaskedLM">https://huggingface.co/docs/transformers/model_doc/deberta-v2#transformers.TFDebertaV2ForMaskedLM</a>
</p>
</li></ul>



<h3>See Also</h3>

<p>Other Transformers for developers: 
<code><a href="#topic+.AIFEBaseTransformer">.AIFEBaseTransformer</a></code>,
<code><a href="#topic+.AIFEBertTransformer">.AIFEBertTransformer</a></code>,
<code><a href="#topic+.AIFEFunnelTransformer">.AIFEFunnelTransformer</a></code>,
<code><a href="#topic+.AIFELongformerTransformer">.AIFELongformerTransformer</a></code>,
<code><a href="#topic+.AIFEMpnetTransformer">.AIFEMpnetTransformer</a></code>,
<code><a href="#topic+.AIFERobertaTransformer">.AIFERobertaTransformer</a></code>,
<code><a href="#topic+.AIFETrObj">.AIFETrObj</a></code>
</p>

<hr>
<h2 id='.AIFEFunnelTransformer'>Child <code>R6</code> class for creation and training of <code>Funnel</code> transformers</h2><span id='topic+.AIFEFunnelTransformer'></span>

<h3>Description</h3>

<p>This class has the following methods:
</p>

<ul>
<li> <p><code>create</code>: creates a new transformer based on <code>Funnel</code>.
</p>
</li>
<li> <p><code>train</code>: trains and fine-tunes a <code>Funnel</code> model.
</p>
</li></ul>



<h3>Create</h3>

<p>New models can be created using the <code>.AIFEFunnelTransformer$create</code> method.
</p>
<p>Model is created with <code>separete_cls = TRUE</code>, <code>truncate_seq = TRUE</code>, and <code>pool_q_only = TRUE</code>.
</p>


<h3>Train</h3>

<p>To train the model, pass the directory of the model to the method <code>.AIFEFunnelTransformer$train</code>.
</p>
<p>Pre-Trained models which can be fine-tuned with this function are available at <a href="https://huggingface.co/">https://huggingface.co/</a>.
</p>
<p>Training of the model makes use of dynamic masking.
</p>


<h3>Super class</h3>

<p><code><a href="#topic+.AIFEBaseTransformer">aifeducation::.AIFEBaseTransformer</a></code> -&gt; <code>.AIFEFunnelTransformer</code>
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-.AIFEFunnelTransformer-new"><code>.AIFEFunnelTransformer$new()</code></a>
</p>
</li>
<li> <p><a href="#method-.AIFEFunnelTransformer-create"><code>.AIFEFunnelTransformer$create()</code></a>
</p>
</li>
<li> <p><a href="#method-.AIFEFunnelTransformer-train"><code>.AIFEFunnelTransformer$train()</code></a>
</p>
</li>
<li> <p><a href="#method-.AIFEFunnelTransformer-clone"><code>.AIFEFunnelTransformer$clone()</code></a>
</p>
</li></ul>



<details><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_SFC_calculate_vocab"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_SFC_calculate_vocab'><code>aifeducation::.AIFEBaseTransformer$set_SFC_calculate_vocab()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_SFC_check_max_pos_emb"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_SFC_check_max_pos_emb'><code>aifeducation::.AIFEBaseTransformer$set_SFC_check_max_pos_emb()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_SFC_create_final_tokenizer"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_SFC_create_final_tokenizer'><code>aifeducation::.AIFEBaseTransformer$set_SFC_create_final_tokenizer()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_SFC_create_tokenizer_draft"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_SFC_create_tokenizer_draft'><code>aifeducation::.AIFEBaseTransformer$set_SFC_create_tokenizer_draft()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_SFC_create_transformer_model"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_SFC_create_transformer_model'><code>aifeducation::.AIFEBaseTransformer$set_SFC_create_transformer_model()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_SFC_save_tokenizer_draft"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_SFC_save_tokenizer_draft'><code>aifeducation::.AIFEBaseTransformer$set_SFC_save_tokenizer_draft()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_SFT_create_data_collator"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_SFT_create_data_collator'><code>aifeducation::.AIFEBaseTransformer$set_SFT_create_data_collator()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_SFT_cuda_empty_cache"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_SFT_cuda_empty_cache'><code>aifeducation::.AIFEBaseTransformer$set_SFT_cuda_empty_cache()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_SFT_load_existing_model"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_SFT_load_existing_model'><code>aifeducation::.AIFEBaseTransformer$set_SFT_load_existing_model()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_model_param"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_model_param'><code>aifeducation::.AIFEBaseTransformer$set_model_param()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_model_temp"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_model_temp'><code>aifeducation::.AIFEBaseTransformer$set_model_temp()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_required_SFC"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_required_SFC'><code>aifeducation::.AIFEBaseTransformer$set_required_SFC()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_title"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_title'><code>aifeducation::.AIFEBaseTransformer$set_title()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-.AIFEFunnelTransformer-new"></a>



<h4>Method <code>new()</code></h4>

<p>Creates a new transformer based on <code>Funnel</code> and sets the title.
</p>


<h5>Usage</h5>

<div class="r"><pre>.AIFEFunnelTransformer$new()</pre></div>



<h5>Returns</h5>

<p>This method returns nothing.
</p>


<hr>
<a id="method-.AIFEFunnelTransformer-create"></a>



<h4>Method <code>create()</code></h4>

<p>This method creates a transformer configuration based on the <code>Funnel</code> transformer base architecture
and a vocabulary based on <code>WordPiece</code> using the python <code>transformers</code> and <code>tokenizers</code> libraries.
</p>
<p>This method adds the following <em>'dependent' parameters</em> to the base class's inherited <code>params</code> list:
</p>

<ul>
<li> <p><code>vocab_do_lower_case</code>
</p>
</li>
<li> <p><code>target_hidden_size</code>
</p>
</li>
<li> <p><code>block_sizes</code>
</p>
</li>
<li> <p><code>num_decoder_layers</code>
</p>
</li>
<li> <p><code>pooling_type</code>
</p>
</li>
<li> <p><code>activation_dropout</code>
</p>
</li></ul>



<h5>Usage</h5>

<div class="r"><pre>.AIFEFunnelTransformer$create(
  ml_framework = "pytorch",
  model_dir,
  text_dataset,
  vocab_size = 30522,
  vocab_do_lower_case = FALSE,
  max_position_embeddings = 512,
  hidden_size = 768,
  target_hidden_size = 64,
  block_sizes = c(4, 4, 4),
  num_attention_heads = 12,
  intermediate_size = 3072,
  num_decoder_layers = 2,
  pooling_type = "mean",
  hidden_act = "gelu",
  hidden_dropout_prob = 0.1,
  attention_probs_dropout_prob = 0.1,
  activation_dropout = 0,
  sustain_track = TRUE,
  sustain_iso_code = NULL,
  sustain_region = NULL,
  sustain_interval = 15,
  trace = TRUE,
  pytorch_safetensors = TRUE,
  log_dir = NULL,
  log_write_interval = 2
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>ml_framework</code></dt><dd><p><code>string</code> Framework to use for training and inference.
</p>

<ul>
<li> <p><code>ml_framework = "tensorflow"</code>: for 'tensorflow'.
</p>
</li>
<li> <p><code>ml_framework = "pytorch"</code>: for 'pytorch'.
</p>
</li></ul>
</dd>
<dt><code>model_dir</code></dt><dd><p><code>string</code> Path to the directory where the model should be saved.</p>
</dd>
<dt><code>text_dataset</code></dt><dd><p>Object of class <a href="#topic+LargeDataSetForText">LargeDataSetForText</a>.</p>
</dd>
<dt><code>vocab_size</code></dt><dd><p><code>int</code> Size of the vocabulary.</p>
</dd>
<dt><code>vocab_do_lower_case</code></dt><dd><p><code>bool</code> <code>TRUE</code> if all words/tokens should be lower case.</p>
</dd>
<dt><code>max_position_embeddings</code></dt><dd><p><code>int</code> Number of maximum position embeddings. This parameter also determines the maximum length of a sequence which
can be processed with the model.</p>
</dd>
<dt><code>hidden_size</code></dt><dd><p><code>int</code> Number of neurons in each layer. This parameter determines the dimensionality of the resulting text
embedding.</p>
</dd>
<dt><code>target_hidden_size</code></dt><dd><p><code>int</code> Number of neurons in the final layer. This parameter determines the dimensionality of the resulting text
embedding.</p>
</dd>
<dt><code>block_sizes</code></dt><dd><p><code>vector</code> of <code>int</code> determining the number and sizes of each block.</p>
</dd>
<dt><code>num_attention_heads</code></dt><dd><p><code>int</code> Number of attention heads.</p>
</dd>
<dt><code>intermediate_size</code></dt><dd><p><code>int</code> Number of neurons in the intermediate layer of the attention mechanism.</p>
</dd>
<dt><code>num_decoder_layers</code></dt><dd><p><code>int</code> Number of decoding layers.</p>
</dd>
<dt><code>pooling_type</code></dt><dd><p><code>string</code> Type of pooling.
</p>

<ul>
<li> <p><code>"mean"</code> for pooling with mean.
</p>
</li>
<li> <p><code>"max"</code> for pooling with maximum values.
</p>
</li></ul>
</dd>
<dt><code>hidden_act</code></dt><dd><p><code>string</code> Name of the activation function.</p>
</dd>
<dt><code>hidden_dropout_prob</code></dt><dd><p><code>double</code> Ratio of dropout.</p>
</dd>
<dt><code>attention_probs_dropout_prob</code></dt><dd><p><code>double</code> Ratio of dropout for attention probabilities.</p>
</dd>
<dt><code>activation_dropout</code></dt><dd><p><code>float</code> Dropout probability between the layers of the feed-forward blocks.</p>
</dd>
<dt><code>sustain_track</code></dt><dd><p><code>bool</code> If <code>TRUE</code> energy consumption is tracked during training via the python library codecarbon.</p>
</dd>
<dt><code>sustain_iso_code</code></dt><dd><p><code>string</code> ISO code (Alpha-3-Code) for the country. This variable must be set if sustainability should be tracked. A
list can be found on Wikipedia: <a href="https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes">https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes</a>.</p>
</dd>
<dt><code>sustain_region</code></dt><dd><p><code>string</code> Region within a country. Only available for USA and Canada. See the documentation of codecarbon for more
information <a href="https://mlco2.github.io/codecarbon/parameters.html">https://mlco2.github.io/codecarbon/parameters.html</a>.</p>
</dd>
<dt><code>sustain_interval</code></dt><dd><p><code>integer</code> Interval in seconds for measuring power usage.</p>
</dd>
<dt><code>trace</code></dt><dd><p><code>bool</code> <code>TRUE</code> if information about the progress should be printed to the console.</p>
</dd>
<dt><code>pytorch_safetensors</code></dt><dd><p><code>bool</code> Only relevant for pytorch models.
</p>

<ul>
<li> <p><code>TRUE</code>: a 'pytorch' model is saved in safetensors format.
</p>
</li>
<li> <p><code>FALSE</code> (or 'safetensors' is not available): model is saved in the standard pytorch format (.bin).
</p>
</li></ul>
</dd>
<dt><code>log_dir</code></dt><dd><p>Path to the directory where the log files should be saved.</p>
</dd>
<dt><code>log_write_interval</code></dt><dd><p><code>int</code> Time in seconds determining the interval in which the logger should try to update the log files. Only relevant
if <code>log_dir</code> is not <code>NULL</code>.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>This method does not return an object. Instead, it saves the configuration and vocabulary of the new
model to disk.
</p>


<hr>
<a id="method-.AIFEFunnelTransformer-train"></a>



<h4>Method <code>train()</code></h4>

<p>This method can be used to train or fine-tune a transformer based on <code>Funnel</code> Transformer
architecture with the help of the python libraries <code>transformers</code>, <code>datasets</code>, and <code>tokenizers</code>.
</p>


<h5>Usage</h5>

<div class="r"><pre>.AIFEFunnelTransformer$train(
  ml_framework = "pytorch",
  output_dir,
  model_dir_path,
  text_dataset,
  p_mask = 0.15,
  whole_word = TRUE,
  val_size = 0.1,
  n_epoch = 1,
  batch_size = 12,
  chunk_size = 250,
  full_sequences_only = FALSE,
  min_seq_len = 50,
  learning_rate = 0.003,
  n_workers = 1,
  multi_process = FALSE,
  sustain_track = TRUE,
  sustain_iso_code = NULL,
  sustain_region = NULL,
  sustain_interval = 15,
  trace = TRUE,
  keras_trace = 1,
  pytorch_trace = 1,
  pytorch_safetensors = TRUE,
  log_dir = NULL,
  log_write_interval = 2
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>ml_framework</code></dt><dd><p><code>string</code> Framework to use for training and inference.
</p>

<ul>
<li> <p><code>ml_framework = "tensorflow"</code>: for 'tensorflow'.
</p>
</li>
<li> <p><code>ml_framework = "pytorch"</code>: for 'pytorch'.
</p>
</li></ul>
</dd>
<dt><code>output_dir</code></dt><dd><p><code>string</code> Path to the directory where the final model should be saved. If the directory does not exist, it will be
created.</p>
</dd>
<dt><code>model_dir_path</code></dt><dd><p><code>string</code> Path to the directory where the original model is stored.</p>
</dd>
<dt><code>text_dataset</code></dt><dd><p>Object of class <a href="#topic+LargeDataSetForText">LargeDataSetForText</a>.</p>
</dd>
<dt><code>p_mask</code></dt><dd><p><code>double</code> Ratio that determines the number of words/tokens used for masking.</p>
</dd>
<dt><code>whole_word</code></dt><dd><p><code>bool</code>
</p>

<ul>
<li> <p><code>TRUE</code>: whole word masking should be applied.
</p>
</li>
<li> <p><code>FALSE</code>: token masking is used.
</p>
</li></ul>
</dd>
<dt><code>val_size</code></dt><dd><p><code>double</code> Ratio that determines the amount of token chunks used for validation.</p>
</dd>
<dt><code>n_epoch</code></dt><dd><p><code>int</code> Number of epochs for training.</p>
</dd>
<dt><code>batch_size</code></dt><dd><p><code>int</code> Size of batches.</p>
</dd>
<dt><code>chunk_size</code></dt><dd><p><code>int</code> Size of every chunk for training.</p>
</dd>
<dt><code>full_sequences_only</code></dt><dd><p><code>bool</code> <code>TRUE</code> for using only chunks with a sequence length equal to <code>chunk_size</code>.</p>
</dd>
<dt><code>min_seq_len</code></dt><dd><p><code>int</code> Only relevant if <code>full_sequences_only = FALSE</code>. Value determines the minimal sequence length included in
training process.</p>
</dd>
<dt><code>learning_rate</code></dt><dd><p><code>double</code> Learning rate for adam optimizer.</p>
</dd>
<dt><code>n_workers</code></dt><dd><p><code>int</code> Number of workers. Only relevant if <code>ml_framework = "tensorflow"</code>.</p>
</dd>
<dt><code>multi_process</code></dt><dd><p><code>bool</code> <code>TRUE</code> if multiple processes should be activated. Only relevant if <code>ml_framework = "tensorflow"</code>.</p>
</dd>
<dt><code>sustain_track</code></dt><dd><p><code>bool</code> If <code>TRUE</code> energy consumption is tracked during training via the python library codecarbon.</p>
</dd>
<dt><code>sustain_iso_code</code></dt><dd><p><code>string</code> ISO code (Alpha-3-Code) for the country. This variable must be set if sustainability should be tracked. A
list can be found on Wikipedia: <a href="https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes">https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes</a>.</p>
</dd>
<dt><code>sustain_region</code></dt><dd><p><code>string</code> Region within a country. Only available for USA and Canada. See the documentation of codecarbon for more
information <a href="https://mlco2.github.io/codecarbon/parameters.html">https://mlco2.github.io/codecarbon/parameters.html</a>.</p>
</dd>
<dt><code>sustain_interval</code></dt><dd><p><code>integer</code> Interval in seconds for measuring power usage.</p>
</dd>
<dt><code>trace</code></dt><dd><p><code>bool</code> <code>TRUE</code> if information about the progress should be printed to the console.</p>
</dd>
<dt><code>keras_trace</code></dt><dd><p><code>int</code>
</p>

<ul>
<li> <p><code>keras_trace = 0</code>: does not print any information about the training process from keras on the console.
</p>
</li>
<li> <p><code>keras_trace = 1</code>: prints a progress bar.
</p>
</li>
<li> <p><code>keras_trace = 2</code>: prints one line of information for every epoch. Only relevant if <code>ml_framework = "tensorflow"</code>.
</p>
</li></ul>
</dd>
<dt><code>pytorch_trace</code></dt><dd><p><code>int</code>
</p>

<ul>
<li> <p><code>pytorch_trace = 0</code>: does not print any information about the training process from pytorch on the console.
</p>
</li>
<li> <p><code>pytorch_trace = 1</code>: prints a progress bar.
</p>
</li></ul>
</dd>
<dt><code>pytorch_safetensors</code></dt><dd><p><code>bool</code> Only relevant for pytorch models.
</p>

<ul>
<li> <p><code>TRUE</code>: a 'pytorch' model is saved in safetensors format.
</p>
</li>
<li> <p><code>FALSE</code> (or 'safetensors' is not available): model is saved in the standard pytorch format (.bin).
</p>
</li></ul>
</dd>
<dt><code>log_dir</code></dt><dd><p>Path to the directory where the log files should be saved.</p>
</dd>
<dt><code>log_write_interval</code></dt><dd><p><code>int</code> Time in seconds determining the interval in which the logger should try to update the log files. Only relevant
if <code>log_dir</code> is not <code>NULL</code>.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>This method does not return an object. Instead the trained or fine-tuned model is saved to disk.
</p>


<hr>
<a id="method-.AIFEFunnelTransformer-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>.AIFEFunnelTransformer$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>Note</h3>

<p>The model uses a configuration with <code>truncate_seq = TRUE</code> to avoid implementation problems with tensorflow.
</p>
<p>This model uses a <code>WordPiece</code> tokenizer like <code>BERT</code> and can be trained with whole word masking. The transformer
library may display a warning, which can be ignored.
</p>


<h3>References</h3>

<p>Dai, Z., Lai, G., Yang, Y. &amp; Le, Q. V. (2020). Funnel-Transformer: Filtering out Sequential Redundancy
for Efficient Language Processing. <a href="https://doi.org/10.48550/arXiv.2006.03236">doi:10.48550/arXiv.2006.03236</a>
</p>
<p>Hugging Face documentation
</p>

<ul>
<li> <p><a href="https://huggingface.co/docs/transformers/model_doc/funnel#funnel-transformer">https://huggingface.co/docs/transformers/model_doc/funnel#funnel-transformer</a>
</p>
</li>
<li> <p><a href="https://huggingface.co/docs/transformers/model_doc/funnel#transformers.FunnelModel">https://huggingface.co/docs/transformers/model_doc/funnel#transformers.FunnelModel</a>
</p>
</li>
<li> <p><a href="https://huggingface.co/docs/transformers/model_doc/funnel#transformers.TFFunnelModel">https://huggingface.co/docs/transformers/model_doc/funnel#transformers.TFFunnelModel</a>
</p>
</li></ul>



<h3>See Also</h3>

<p>Other Transformers for developers: 
<code><a href="#topic+.AIFEBaseTransformer">.AIFEBaseTransformer</a></code>,
<code><a href="#topic+.AIFEBertTransformer">.AIFEBertTransformer</a></code>,
<code><a href="#topic+.AIFEDebertaTransformer">.AIFEDebertaTransformer</a></code>,
<code><a href="#topic+.AIFELongformerTransformer">.AIFELongformerTransformer</a></code>,
<code><a href="#topic+.AIFEMpnetTransformer">.AIFEMpnetTransformer</a></code>,
<code><a href="#topic+.AIFERobertaTransformer">.AIFERobertaTransformer</a></code>,
<code><a href="#topic+.AIFETrObj">.AIFETrObj</a></code>
</p>

<hr>
<h2 id='.AIFELongformerTransformer'>Child <code>R6</code> class for creation and training of <code>Longformer</code> transformers</h2><span id='topic+.AIFELongformerTransformer'></span>

<h3>Description</h3>

<p>This class has the following methods:
</p>

<ul>
<li> <p><code>create</code>: creates a new transformer based on <code>Longformer</code>.
</p>
</li>
<li> <p><code>train</code>: trains and fine-tunes a <code>Longformer</code> model.
</p>
</li></ul>



<h3>Create</h3>

<p>New models can be created using the <code>.AIFELongformerTransformer$create</code> method.
</p>


<h3>Train</h3>

<p>To train the model, pass the directory of the model to the method <code>.AIFELongformerTransformer$train</code>.
</p>
<p>Pre-Trained models which can be fine-tuned with this function are available at <a href="https://huggingface.co/">https://huggingface.co/</a>.
</p>
<p>Training of this model makes use of dynamic masking.
</p>


<h3>Super class</h3>

<p><code><a href="#topic+.AIFEBaseTransformer">aifeducation::.AIFEBaseTransformer</a></code> -&gt; <code>.AIFELongformerTransformer</code>
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-.AIFELongformerTransformer-new"><code>.AIFELongformerTransformer$new()</code></a>
</p>
</li>
<li> <p><a href="#method-.AIFELongformerTransformer-create"><code>.AIFELongformerTransformer$create()</code></a>
</p>
</li>
<li> <p><a href="#method-.AIFELongformerTransformer-train"><code>.AIFELongformerTransformer$train()</code></a>
</p>
</li>
<li> <p><a href="#method-.AIFELongformerTransformer-clone"><code>.AIFELongformerTransformer$clone()</code></a>
</p>
</li></ul>



<details><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_SFC_calculate_vocab"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_SFC_calculate_vocab'><code>aifeducation::.AIFEBaseTransformer$set_SFC_calculate_vocab()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_SFC_check_max_pos_emb"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_SFC_check_max_pos_emb'><code>aifeducation::.AIFEBaseTransformer$set_SFC_check_max_pos_emb()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_SFC_create_final_tokenizer"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_SFC_create_final_tokenizer'><code>aifeducation::.AIFEBaseTransformer$set_SFC_create_final_tokenizer()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_SFC_create_tokenizer_draft"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_SFC_create_tokenizer_draft'><code>aifeducation::.AIFEBaseTransformer$set_SFC_create_tokenizer_draft()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_SFC_create_transformer_model"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_SFC_create_transformer_model'><code>aifeducation::.AIFEBaseTransformer$set_SFC_create_transformer_model()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_SFC_save_tokenizer_draft"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_SFC_save_tokenizer_draft'><code>aifeducation::.AIFEBaseTransformer$set_SFC_save_tokenizer_draft()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_SFT_create_data_collator"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_SFT_create_data_collator'><code>aifeducation::.AIFEBaseTransformer$set_SFT_create_data_collator()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_SFT_cuda_empty_cache"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_SFT_cuda_empty_cache'><code>aifeducation::.AIFEBaseTransformer$set_SFT_cuda_empty_cache()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_SFT_load_existing_model"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_SFT_load_existing_model'><code>aifeducation::.AIFEBaseTransformer$set_SFT_load_existing_model()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_model_param"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_model_param'><code>aifeducation::.AIFEBaseTransformer$set_model_param()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_model_temp"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_model_temp'><code>aifeducation::.AIFEBaseTransformer$set_model_temp()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_required_SFC"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_required_SFC'><code>aifeducation::.AIFEBaseTransformer$set_required_SFC()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_title"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_title'><code>aifeducation::.AIFEBaseTransformer$set_title()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-.AIFELongformerTransformer-new"></a>



<h4>Method <code>new()</code></h4>

<p>Creates a new transformer based on <code>Longformer</code> and sets the
title.
</p>


<h5>Usage</h5>

<div class="r"><pre>.AIFELongformerTransformer$new()</pre></div>



<h5>Returns</h5>

<p>This method returns nothing
</p>


<hr>
<a id="method-.AIFELongformerTransformer-create"></a>



<h4>Method <code>create()</code></h4>

<p>This method creates a transformer configuration based on the <code>Longformer</code> base architecture and a
vocabulary based on <code style="white-space: pre;">&#8288;Byte-Pair Encoding&#8288;</code> (BPE) tokenizer using the python <code>transformers</code> and <code>tokenizers</code>
libraries.
</p>
<p>This method adds the following <em>'dependent' parameters</em> to the base class's inherited <code>params</code> list:
</p>

<ul>
<li> <p><code>add_prefix_space</code>
</p>
</li>
<li> <p><code>trim_offsets</code>
</p>
</li>
<li> <p><code>num_hidden_layer</code>
</p>
</li>
<li> <p><code>attention_window</code>
</p>
</li></ul>



<h5>Usage</h5>

<div class="r"><pre>.AIFELongformerTransformer$create(
  ml_framework = "pytorch",
  model_dir,
  text_dataset,
  vocab_size = 30522,
  add_prefix_space = FALSE,
  trim_offsets = TRUE,
  max_position_embeddings = 512,
  hidden_size = 768,
  num_hidden_layer = 12,
  num_attention_heads = 12,
  intermediate_size = 3072,
  hidden_act = "gelu",
  hidden_dropout_prob = 0.1,
  attention_probs_dropout_prob = 0.1,
  attention_window = 512,
  sustain_track = TRUE,
  sustain_iso_code = NULL,
  sustain_region = NULL,
  sustain_interval = 15,
  trace = TRUE,
  pytorch_safetensors = TRUE,
  log_dir = NULL,
  log_write_interval = 2
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>ml_framework</code></dt><dd><p><code>string</code> Framework to use for training and inference.
</p>

<ul>
<li> <p><code>ml_framework = "tensorflow"</code>: for 'tensorflow'.
</p>
</li>
<li> <p><code>ml_framework = "pytorch"</code>: for 'pytorch'.
</p>
</li></ul>
</dd>
<dt><code>model_dir</code></dt><dd><p><code>string</code> Path to the directory where the model should be saved.</p>
</dd>
<dt><code>text_dataset</code></dt><dd><p>Object of class <a href="#topic+LargeDataSetForText">LargeDataSetForText</a>.</p>
</dd>
<dt><code>vocab_size</code></dt><dd><p><code>int</code> Size of the vocabulary.</p>
</dd>
<dt><code>add_prefix_space</code></dt><dd><p><code>bool</code> <code>TRUE</code> if an additional space should be inserted to the leading words.</p>
</dd>
<dt><code>trim_offsets</code></dt><dd><p><code>bool</code> <code>TRUE</code> trims the whitespaces from the produced offsets.</p>
</dd>
<dt><code>max_position_embeddings</code></dt><dd><p><code>int</code> Number of maximum position embeddings. This parameter also determines the maximum length of a sequence which
can be processed with the model.</p>
</dd>
<dt><code>hidden_size</code></dt><dd><p><code>int</code> Number of neurons in each layer. This parameter determines the dimensionality of the resulting text
embedding.</p>
</dd>
<dt><code>num_hidden_layer</code></dt><dd><p><code>int</code> Number of hidden layers.</p>
</dd>
<dt><code>num_attention_heads</code></dt><dd><p><code>int</code> Number of attention heads.</p>
</dd>
<dt><code>intermediate_size</code></dt><dd><p><code>int</code> Number of neurons in the intermediate layer of the attention mechanism.</p>
</dd>
<dt><code>hidden_act</code></dt><dd><p><code>string</code> Name of the activation function.</p>
</dd>
<dt><code>hidden_dropout_prob</code></dt><dd><p><code>double</code> Ratio of dropout.</p>
</dd>
<dt><code>attention_probs_dropout_prob</code></dt><dd><p><code>double</code> Ratio of dropout for attention probabilities.</p>
</dd>
<dt><code>attention_window</code></dt><dd><p><code>int</code> Size of the window around each token for attention mechanism in every layer.</p>
</dd>
<dt><code>sustain_track</code></dt><dd><p><code>bool</code> If <code>TRUE</code> energy consumption is tracked during training via the python library codecarbon.</p>
</dd>
<dt><code>sustain_iso_code</code></dt><dd><p><code>string</code> ISO code (Alpha-3-Code) for the country. This variable must be set if sustainability should be tracked. A
list can be found on Wikipedia: <a href="https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes">https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes</a>.</p>
</dd>
<dt><code>sustain_region</code></dt><dd><p><code>string</code> Region within a country. Only available for USA and Canada. See the documentation of codecarbon for more
information <a href="https://mlco2.github.io/codecarbon/parameters.html">https://mlco2.github.io/codecarbon/parameters.html</a>.</p>
</dd>
<dt><code>sustain_interval</code></dt><dd><p><code>integer</code> Interval in seconds for measuring power usage.</p>
</dd>
<dt><code>trace</code></dt><dd><p><code>bool</code> <code>TRUE</code> if information about the progress should be printed to the console.</p>
</dd>
<dt><code>pytorch_safetensors</code></dt><dd><p><code>bool</code> Only relevant for pytorch models.
</p>

<ul>
<li> <p><code>TRUE</code>: a 'pytorch' model is saved in safetensors format.
</p>
</li>
<li> <p><code>FALSE</code> (or 'safetensors' is not available): model is saved in the standard pytorch format (.bin).
</p>
</li></ul>
</dd>
<dt><code>log_dir</code></dt><dd><p>Path to the directory where the log files should be saved.</p>
</dd>
<dt><code>log_write_interval</code></dt><dd><p><code>int</code> Time in seconds determining the interval in which the logger should try to update the log files. Only relevant
if <code>log_dir</code> is not <code>NULL</code>.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>This method does not return an object. Instead, it saves the configuration and vocabulary of the new
model to disk.
</p>


<hr>
<a id="method-.AIFELongformerTransformer-train"></a>



<h4>Method <code>train()</code></h4>

<p>This method can be used to train or fine-tune a transformer based on <code>Longformer</code> Transformer
architecture with the help of the python libraries <code>transformers</code>, <code>datasets</code>, and <code>tokenizers</code>.
</p>


<h5>Usage</h5>

<div class="r"><pre>.AIFELongformerTransformer$train(
  ml_framework = "pytorch",
  output_dir,
  model_dir_path,
  text_dataset,
  p_mask = 0.15,
  val_size = 0.1,
  n_epoch = 1,
  batch_size = 12,
  chunk_size = 250,
  full_sequences_only = FALSE,
  min_seq_len = 50,
  learning_rate = 0.03,
  n_workers = 1,
  multi_process = FALSE,
  sustain_track = TRUE,
  sustain_iso_code = NULL,
  sustain_region = NULL,
  sustain_interval = 15,
  trace = TRUE,
  keras_trace = 1,
  pytorch_trace = 1,
  pytorch_safetensors = TRUE,
  log_dir = NULL,
  log_write_interval = 2
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>ml_framework</code></dt><dd><p><code>string</code> Framework to use for training and inference.
</p>

<ul>
<li> <p><code>ml_framework = "tensorflow"</code>: for 'tensorflow'.
</p>
</li>
<li> <p><code>ml_framework = "pytorch"</code>: for 'pytorch'.
</p>
</li></ul>
</dd>
<dt><code>output_dir</code></dt><dd><p><code>string</code> Path to the directory where the final model should be saved. If the directory does not exist, it will be
created.</p>
</dd>
<dt><code>model_dir_path</code></dt><dd><p><code>string</code> Path to the directory where the original model is stored.</p>
</dd>
<dt><code>text_dataset</code></dt><dd><p>Object of class <a href="#topic+LargeDataSetForText">LargeDataSetForText</a>.</p>
</dd>
<dt><code>p_mask</code></dt><dd><p><code>double</code> Ratio that determines the number of words/tokens used for masking.</p>
</dd>
<dt><code>val_size</code></dt><dd><p><code>double</code> Ratio that determines the amount of token chunks used for validation.</p>
</dd>
<dt><code>n_epoch</code></dt><dd><p><code>int</code> Number of epochs for training.</p>
</dd>
<dt><code>batch_size</code></dt><dd><p><code>int</code> Size of batches.</p>
</dd>
<dt><code>chunk_size</code></dt><dd><p><code>int</code> Size of every chunk for training.</p>
</dd>
<dt><code>full_sequences_only</code></dt><dd><p><code>bool</code> <code>TRUE</code> for using only chunks with a sequence length equal to <code>chunk_size</code>.</p>
</dd>
<dt><code>min_seq_len</code></dt><dd><p><code>int</code> Only relevant if <code>full_sequences_only = FALSE</code>. Value determines the minimal sequence length included in
training process.</p>
</dd>
<dt><code>learning_rate</code></dt><dd><p><code>double</code> Learning rate for adam optimizer.</p>
</dd>
<dt><code>n_workers</code></dt><dd><p><code>int</code> Number of workers. Only relevant if <code>ml_framework = "tensorflow"</code>.</p>
</dd>
<dt><code>multi_process</code></dt><dd><p><code>bool</code> <code>TRUE</code> if multiple processes should be activated. Only relevant if <code>ml_framework = "tensorflow"</code>.</p>
</dd>
<dt><code>sustain_track</code></dt><dd><p><code>bool</code> If <code>TRUE</code> energy consumption is tracked during training via the python library codecarbon.</p>
</dd>
<dt><code>sustain_iso_code</code></dt><dd><p><code>string</code> ISO code (Alpha-3-Code) for the country. This variable must be set if sustainability should be tracked. A
list can be found on Wikipedia: <a href="https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes">https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes</a>.</p>
</dd>
<dt><code>sustain_region</code></dt><dd><p><code>string</code> Region within a country. Only available for USA and Canada. See the documentation of codecarbon for more
information <a href="https://mlco2.github.io/codecarbon/parameters.html">https://mlco2.github.io/codecarbon/parameters.html</a>.</p>
</dd>
<dt><code>sustain_interval</code></dt><dd><p><code>integer</code> Interval in seconds for measuring power usage.</p>
</dd>
<dt><code>trace</code></dt><dd><p><code>bool</code> <code>TRUE</code> if information about the progress should be printed to the console.</p>
</dd>
<dt><code>keras_trace</code></dt><dd><p><code>int</code>
</p>

<ul>
<li> <p><code>keras_trace = 0</code>: does not print any information about the training process from keras on the console.
</p>
</li>
<li> <p><code>keras_trace = 1</code>: prints a progress bar.
</p>
</li>
<li> <p><code>keras_trace = 2</code>: prints one line of information for every epoch. Only relevant if <code>ml_framework = "tensorflow"</code>.
</p>
</li></ul>
</dd>
<dt><code>pytorch_trace</code></dt><dd><p><code>int</code>
</p>

<ul>
<li> <p><code>pytorch_trace = 0</code>: does not print any information about the training process from pytorch on the console.
</p>
</li>
<li> <p><code>pytorch_trace = 1</code>: prints a progress bar.
</p>
</li></ul>
</dd>
<dt><code>pytorch_safetensors</code></dt><dd><p><code>bool</code> Only relevant for pytorch models.
</p>

<ul>
<li> <p><code>TRUE</code>: a 'pytorch' model is saved in safetensors format.
</p>
</li>
<li> <p><code>FALSE</code> (or 'safetensors' is not available): model is saved in the standard pytorch format (.bin).
</p>
</li></ul>
</dd>
<dt><code>log_dir</code></dt><dd><p>Path to the directory where the log files should be saved.</p>
</dd>
<dt><code>log_write_interval</code></dt><dd><p><code>int</code> Time in seconds determining the interval in which the logger should try to update the log files. Only relevant
if <code>log_dir</code> is not <code>NULL</code>.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>This method does not return an object. Instead the trained or fine-tuned model is saved to disk.
</p>


<hr>
<a id="method-.AIFELongformerTransformer-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>.AIFELongformerTransformer$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>References</h3>

<p>Beltagy, I., Peters, M. E., &amp; Cohan, A. (2020). Longformer: The Long-Document Transformer.
<a href="https://doi.org/10.48550/arXiv.2004.05150">doi:10.48550/arXiv.2004.05150</a>
</p>
<p>Hugging Face Documentation
</p>

<ul>
<li> <p><a href="https://huggingface.co/docs/transformers/model_doc/longformer">https://huggingface.co/docs/transformers/model_doc/longformer</a>
</p>
</li>
<li> <p><a href="https://huggingface.co/docs/transformers/model_doc/longformer#transformers.LongformerModel">https://huggingface.co/docs/transformers/model_doc/longformer#transformers.LongformerModel</a>
</p>
</li>
<li> <p><a href="https://huggingface.co/docs/transformers/model_doc/longformer#transformers.TFLongformerModel">https://huggingface.co/docs/transformers/model_doc/longformer#transformers.TFLongformerModel</a>
</p>
</li></ul>



<h3>See Also</h3>

<p>Other Transformers for developers: 
<code><a href="#topic+.AIFEBaseTransformer">.AIFEBaseTransformer</a></code>,
<code><a href="#topic+.AIFEBertTransformer">.AIFEBertTransformer</a></code>,
<code><a href="#topic+.AIFEDebertaTransformer">.AIFEDebertaTransformer</a></code>,
<code><a href="#topic+.AIFEFunnelTransformer">.AIFEFunnelTransformer</a></code>,
<code><a href="#topic+.AIFEMpnetTransformer">.AIFEMpnetTransformer</a></code>,
<code><a href="#topic+.AIFERobertaTransformer">.AIFERobertaTransformer</a></code>,
<code><a href="#topic+.AIFETrObj">.AIFETrObj</a></code>
</p>

<hr>
<h2 id='.AIFEMpnetTransformer'>Child <code>R6</code> class for creation and training of <code>MPNet</code> transformers</h2><span id='topic+.AIFEMpnetTransformer'></span>

<h3>Description</h3>

<p>This class has the following methods:
</p>

<ul>
<li> <p><code>create</code>: creates a new transformer based on <code>MPNet</code>.
</p>
</li>
<li> <p><code>train</code>: trains and fine-tunes a <code>MPNet</code> model.
</p>
</li></ul>



<h3>Create</h3>

<p>New models can be created using the <code>.AIFEMpnetTransformer$create</code> method.
</p>


<h3>Train</h3>

<p>To train the model, pass the directory of the model to the method <code>.AIFEMpnetTransformer$train</code>.
</p>


<h3>Super class</h3>

<p><code><a href="#topic+.AIFEBaseTransformer">aifeducation::.AIFEBaseTransformer</a></code> -&gt; <code>.AIFEMpnetTransformer</code>
</p>


<h3>Public fields</h3>

<div class="r6-fields">

<dl>
<dt><code>special_tokens_list</code></dt><dd><p><code>list</code> List for special tokens with the following elements:
</p>

<ul>
<li> <p><code>cls</code> - CLS token representation (<code style="white-space: pre;">&#8288;&lt;s&gt;&#8288;</code>)
</p>
</li>
<li> <p><code>pad</code> - pad token representation (<code style="white-space: pre;">&#8288;&lt;pad&gt;&#8288;</code>)
</p>
</li>
<li> <p><code>sep</code> - sep token representation (<code style="white-space: pre;">&#8288;&lt;/s&gt;&#8288;</code>)
</p>
</li>
<li> <p><code>unk</code> - unk token representation (<code style="white-space: pre;">&#8288;&lt;unk&gt;&#8288;</code>)
</p>
</li>
<li> <p><code>mask</code> - mask token representation (<code style="white-space: pre;">&#8288;&lt;mask&gt;&#8288;</code>)
</p>
</li></ul>
</dd>
</dl>

</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-.AIFEMpnetTransformer-new"><code>.AIFEMpnetTransformer$new()</code></a>
</p>
</li>
<li> <p><a href="#method-.AIFEMpnetTransformer-create"><code>.AIFEMpnetTransformer$create()</code></a>
</p>
</li>
<li> <p><a href="#method-.AIFEMpnetTransformer-train"><code>.AIFEMpnetTransformer$train()</code></a>
</p>
</li>
<li> <p><a href="#method-.AIFEMpnetTransformer-clone"><code>.AIFEMpnetTransformer$clone()</code></a>
</p>
</li></ul>



<details><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_SFC_calculate_vocab"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_SFC_calculate_vocab'><code>aifeducation::.AIFEBaseTransformer$set_SFC_calculate_vocab()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_SFC_check_max_pos_emb"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_SFC_check_max_pos_emb'><code>aifeducation::.AIFEBaseTransformer$set_SFC_check_max_pos_emb()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_SFC_create_final_tokenizer"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_SFC_create_final_tokenizer'><code>aifeducation::.AIFEBaseTransformer$set_SFC_create_final_tokenizer()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_SFC_create_tokenizer_draft"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_SFC_create_tokenizer_draft'><code>aifeducation::.AIFEBaseTransformer$set_SFC_create_tokenizer_draft()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_SFC_create_transformer_model"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_SFC_create_transformer_model'><code>aifeducation::.AIFEBaseTransformer$set_SFC_create_transformer_model()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_SFC_save_tokenizer_draft"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_SFC_save_tokenizer_draft'><code>aifeducation::.AIFEBaseTransformer$set_SFC_save_tokenizer_draft()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_SFT_create_data_collator"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_SFT_create_data_collator'><code>aifeducation::.AIFEBaseTransformer$set_SFT_create_data_collator()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_SFT_cuda_empty_cache"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_SFT_cuda_empty_cache'><code>aifeducation::.AIFEBaseTransformer$set_SFT_cuda_empty_cache()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_SFT_load_existing_model"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_SFT_load_existing_model'><code>aifeducation::.AIFEBaseTransformer$set_SFT_load_existing_model()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_model_param"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_model_param'><code>aifeducation::.AIFEBaseTransformer$set_model_param()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_model_temp"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_model_temp'><code>aifeducation::.AIFEBaseTransformer$set_model_temp()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_required_SFC"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_required_SFC'><code>aifeducation::.AIFEBaseTransformer$set_required_SFC()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_title"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_title'><code>aifeducation::.AIFEBaseTransformer$set_title()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-.AIFEMpnetTransformer-new"></a>



<h4>Method <code>new()</code></h4>

<p>Creates a new transformer based on <code>MPNet</code> and sets the title.
</p>


<h5>Usage</h5>

<div class="r"><pre>.AIFEMpnetTransformer$new()</pre></div>



<h5>Returns</h5>

<p>This method returns nothing.
</p>


<hr>
<a id="method-.AIFEMpnetTransformer-create"></a>



<h4>Method <code>create()</code></h4>

<p>This method creates a transformer configuration based on the <code>MPNet</code> base architecture.
</p>
<p>This method adds the following <em>'dependent' parameters</em> to the base class's inherited <code>params</code> list:
</p>

<ul>
<li> <p><code>vocab_do_lower_case</code>
</p>
</li>
<li> <p><code>num_hidden_layer</code>
</p>
</li></ul>



<h5>Usage</h5>

<div class="r"><pre>.AIFEMpnetTransformer$create(
  ml_framework = "pytorch",
  model_dir,
  text_dataset,
  vocab_size = 30522,
  vocab_do_lower_case = FALSE,
  max_position_embeddings = 512,
  hidden_size = 768,
  num_hidden_layer = 12,
  num_attention_heads = 12,
  intermediate_size = 3072,
  hidden_act = "gelu",
  hidden_dropout_prob = 0.1,
  attention_probs_dropout_prob = 0.1,
  sustain_track = FALSE,
  sustain_iso_code = NULL,
  sustain_region = NULL,
  sustain_interval = 15,
  trace = TRUE,
  pytorch_safetensors = TRUE,
  log_dir = NULL,
  log_write_interval = 2
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>ml_framework</code></dt><dd><p><code>string</code> Framework to use for training and inference.
</p>

<ul>
<li> <p><code>ml_framework = "tensorflow"</code>: for 'tensorflow'.
</p>
</li>
<li> <p><code>ml_framework = "pytorch"</code>: for 'pytorch'.
</p>
</li></ul>
</dd>
<dt><code>model_dir</code></dt><dd><p><code>string</code> Path to the directory where the model should be saved.</p>
</dd>
<dt><code>text_dataset</code></dt><dd><p>Object of class <a href="#topic+LargeDataSetForText">LargeDataSetForText</a>.</p>
</dd>
<dt><code>vocab_size</code></dt><dd><p><code>int</code> Size of the vocabulary.</p>
</dd>
<dt><code>vocab_do_lower_case</code></dt><dd><p><code>bool</code> <code>TRUE</code> if all words/tokens should be lower case.</p>
</dd>
<dt><code>max_position_embeddings</code></dt><dd><p><code>int</code> Number of maximum position embeddings. This parameter also determines the maximum length of a sequence which
can be processed with the model.</p>
</dd>
<dt><code>hidden_size</code></dt><dd><p><code>int</code> Number of neurons in each layer. This parameter determines the dimensionality of the resulting text
embedding.</p>
</dd>
<dt><code>num_hidden_layer</code></dt><dd><p><code>int</code> Number of hidden layers.</p>
</dd>
<dt><code>num_attention_heads</code></dt><dd><p><code>int</code> Number of attention heads.</p>
</dd>
<dt><code>intermediate_size</code></dt><dd><p><code>int</code> Number of neurons in the intermediate layer of the attention mechanism.</p>
</dd>
<dt><code>hidden_act</code></dt><dd><p><code>string</code> Name of the activation function.</p>
</dd>
<dt><code>hidden_dropout_prob</code></dt><dd><p><code>double</code> Ratio of dropout.</p>
</dd>
<dt><code>attention_probs_dropout_prob</code></dt><dd><p><code>double</code> Ratio of dropout for attention probabilities.</p>
</dd>
<dt><code>sustain_track</code></dt><dd><p><code>bool</code> If <code>TRUE</code> energy consumption is tracked during training via the python library codecarbon.</p>
</dd>
<dt><code>sustain_iso_code</code></dt><dd><p><code>string</code> ISO code (Alpha-3-Code) for the country. This variable must be set if sustainability should be tracked. A
list can be found on Wikipedia: <a href="https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes">https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes</a>.</p>
</dd>
<dt><code>sustain_region</code></dt><dd><p><code>string</code> Region within a country. Only available for USA and Canada. See the documentation of codecarbon for more
information <a href="https://mlco2.github.io/codecarbon/parameters.html">https://mlco2.github.io/codecarbon/parameters.html</a>.</p>
</dd>
<dt><code>sustain_interval</code></dt><dd><p><code>integer</code> Interval in seconds for measuring power usage.</p>
</dd>
<dt><code>trace</code></dt><dd><p><code>bool</code> <code>TRUE</code> if information about the progress should be printed to the console.</p>
</dd>
<dt><code>pytorch_safetensors</code></dt><dd><p><code>bool</code> Only relevant for pytorch models.
</p>

<ul>
<li> <p><code>TRUE</code>: a 'pytorch' model is saved in safetensors format.
</p>
</li>
<li> <p><code>FALSE</code> (or 'safetensors' is not available): model is saved in the standard pytorch format (.bin).
</p>
</li></ul>
</dd>
<dt><code>log_dir</code></dt><dd><p>Path to the directory where the log files should be saved.</p>
</dd>
<dt><code>log_write_interval</code></dt><dd><p><code>int</code> Time in seconds determining the interval in which the logger should try to update the log files. Only relevant
if <code>log_dir</code> is not <code>NULL</code>.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>This method does not return an object. Instead, it saves the configuration and vocabulary of the new
model to disk.
</p>


<hr>
<a id="method-.AIFEMpnetTransformer-train"></a>



<h4>Method <code>train()</code></h4>

<p>This method can be used to train or fine-tune a transformer based on <code>MPNet</code> architecture with the
help of the python libraries <code>transformers</code>, <code>datasets</code>, and <code>tokenizers</code>.
</p>
<p>This method adds the following <em>'dependent' parameter</em> to the base class's inherited <code>params</code> list:
</p>

<ul>
<li> <p><code>p_perm</code>
</p>
</li></ul>



<h5>Usage</h5>

<div class="r"><pre>.AIFEMpnetTransformer$train(
  ml_framework = "pytorch",
  output_dir,
  model_dir_path,
  text_dataset,
  p_mask = 0.15,
  p_perm = 0.15,
  whole_word = TRUE,
  val_size = 0.1,
  n_epoch = 1,
  batch_size = 12,
  chunk_size = 250,
  full_sequences_only = FALSE,
  min_seq_len = 50,
  learning_rate = 0.003,
  n_workers = 1,
  multi_process = FALSE,
  sustain_track = FALSE,
  sustain_iso_code = NULL,
  sustain_region = NULL,
  sustain_interval = 15,
  trace = TRUE,
  keras_trace = 1,
  pytorch_trace = 1,
  pytorch_safetensors = TRUE,
  log_dir = NULL,
  log_write_interval = 2
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>ml_framework</code></dt><dd><p><code>string</code> Framework to use for training and inference.
</p>

<ul>
<li> <p><code>ml_framework = "tensorflow"</code>: for 'tensorflow'.
</p>
</li>
<li> <p><code>ml_framework = "pytorch"</code>: for 'pytorch'.
</p>
</li></ul>
</dd>
<dt><code>output_dir</code></dt><dd><p><code>string</code> Path to the directory where the final model should be saved. If the directory does not exist, it will be
created.</p>
</dd>
<dt><code>model_dir_path</code></dt><dd><p><code>string</code> Path to the directory where the original model is stored.</p>
</dd>
<dt><code>text_dataset</code></dt><dd><p>Object of class <a href="#topic+LargeDataSetForText">LargeDataSetForText</a>.</p>
</dd>
<dt><code>p_mask</code></dt><dd><p><code>double</code> Ratio that determines the number of words/tokens used for masking.</p>
</dd>
<dt><code>p_perm</code></dt><dd><p><code>double</code> Ratio that determines the number of words/tokens used for permutation.</p>
</dd>
<dt><code>whole_word</code></dt><dd><p><code>bool</code>
</p>

<ul>
<li> <p><code>TRUE</code>: whole word masking should be applied.
</p>
</li>
<li> <p><code>FALSE</code>: token masking is used.
</p>
</li></ul>
</dd>
<dt><code>val_size</code></dt><dd><p><code>double</code> Ratio that determines the amount of token chunks used for validation.</p>
</dd>
<dt><code>n_epoch</code></dt><dd><p><code>int</code> Number of epochs for training.</p>
</dd>
<dt><code>batch_size</code></dt><dd><p><code>int</code> Size of batches.</p>
</dd>
<dt><code>chunk_size</code></dt><dd><p><code>int</code> Size of every chunk for training.</p>
</dd>
<dt><code>full_sequences_only</code></dt><dd><p><code>bool</code> <code>TRUE</code> for using only chunks with a sequence length equal to <code>chunk_size</code>.</p>
</dd>
<dt><code>min_seq_len</code></dt><dd><p><code>int</code> Only relevant if <code>full_sequences_only = FALSE</code>. Value determines the minimal sequence length included in
training process.</p>
</dd>
<dt><code>learning_rate</code></dt><dd><p><code>double</code> Learning rate for adam optimizer.</p>
</dd>
<dt><code>n_workers</code></dt><dd><p><code>int</code> Number of workers. Only relevant if <code>ml_framework = "tensorflow"</code>.</p>
</dd>
<dt><code>multi_process</code></dt><dd><p><code>bool</code> <code>TRUE</code> if multiple processes should be activated. Only relevant if <code>ml_framework = "tensorflow"</code>.</p>
</dd>
<dt><code>sustain_track</code></dt><dd><p><code>bool</code> If <code>TRUE</code> energy consumption is tracked during training via the python library codecarbon.</p>
</dd>
<dt><code>sustain_iso_code</code></dt><dd><p><code>string</code> ISO code (Alpha-3-Code) for the country. This variable must be set if sustainability should be tracked. A
list can be found on Wikipedia: <a href="https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes">https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes</a>.</p>
</dd>
<dt><code>sustain_region</code></dt><dd><p><code>string</code> Region within a country. Only available for USA and Canada. See the documentation of codecarbon for more
information <a href="https://mlco2.github.io/codecarbon/parameters.html">https://mlco2.github.io/codecarbon/parameters.html</a>.</p>
</dd>
<dt><code>sustain_interval</code></dt><dd><p><code>integer</code> Interval in seconds for measuring power usage.</p>
</dd>
<dt><code>trace</code></dt><dd><p><code>bool</code> <code>TRUE</code> if information about the progress should be printed to the console.</p>
</dd>
<dt><code>keras_trace</code></dt><dd><p><code>int</code>
</p>

<ul>
<li> <p><code>keras_trace = 0</code>: does not print any information about the training process from keras on the console.
</p>
</li>
<li> <p><code>keras_trace = 1</code>: prints a progress bar.
</p>
</li>
<li> <p><code>keras_trace = 2</code>: prints one line of information for every epoch. Only relevant if <code>ml_framework = "tensorflow"</code>.
</p>
</li></ul>
</dd>
<dt><code>pytorch_trace</code></dt><dd><p><code>int</code>
</p>

<ul>
<li> <p><code>pytorch_trace = 0</code>: does not print any information about the training process from pytorch on the console.
</p>
</li>
<li> <p><code>pytorch_trace = 1</code>: prints a progress bar.
</p>
</li></ul>
</dd>
<dt><code>pytorch_safetensors</code></dt><dd><p><code>bool</code> Only relevant for pytorch models.
</p>

<ul>
<li> <p><code>TRUE</code>: a 'pytorch' model is saved in safetensors format.
</p>
</li>
<li> <p><code>FALSE</code> (or 'safetensors' is not available): model is saved in the standard pytorch format (.bin).
</p>
</li></ul>
</dd>
<dt><code>log_dir</code></dt><dd><p>Path to the directory where the log files should be saved.</p>
</dd>
<dt><code>log_write_interval</code></dt><dd><p><code>int</code> Time in seconds determining the interval in which the logger should try to update the log files. Only relevant
if <code>log_dir</code> is not <code>NULL</code>.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>This method does not return an object. Instead the trained or fine-tuned model is saved to disk.
</p>


<hr>
<a id="method-.AIFEMpnetTransformer-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>.AIFEMpnetTransformer$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>Note</h3>

<p>Using this class with <code>tensorflow</code> is not supported. Supported framework is <code>pytorch</code>.
</p>


<h3>References</h3>

<p>Song,K., Tan, X., Qin, T., Lu, J. &amp; Liu, T.-Y. (2020). MPNet: Masked and Permuted Pre-training for
Language Understanding. <a href="https://doi.org/10.48550/arXiv.2004.09297">doi:10.48550/arXiv.2004.09297</a>
</p>
<p>Hugging Face documentation
</p>

<ul>
<li> <p><a href="https://huggingface.co/docs/transformers/model_doc/mpnet">https://huggingface.co/docs/transformers/model_doc/mpnet</a>
</p>
</li>
<li> <p><a href="https://huggingface.co/docs/transformers/model_doc/mpnet#transformers.MPNetForMaskedLM">https://huggingface.co/docs/transformers/model_doc/mpnet#transformers.MPNetForMaskedLM</a>
</p>
</li>
<li> <p><a href="https://huggingface.co/docs/transformers/model_doc/mpnet#transformers.TFMPNetForMaskedLM">https://huggingface.co/docs/transformers/model_doc/mpnet#transformers.TFMPNetForMaskedLM</a>
</p>
</li></ul>



<h3>See Also</h3>

<p>Other Transformers for developers: 
<code><a href="#topic+.AIFEBaseTransformer">.AIFEBaseTransformer</a></code>,
<code><a href="#topic+.AIFEBertTransformer">.AIFEBertTransformer</a></code>,
<code><a href="#topic+.AIFEDebertaTransformer">.AIFEDebertaTransformer</a></code>,
<code><a href="#topic+.AIFEFunnelTransformer">.AIFEFunnelTransformer</a></code>,
<code><a href="#topic+.AIFELongformerTransformer">.AIFELongformerTransformer</a></code>,
<code><a href="#topic+.AIFERobertaTransformer">.AIFERobertaTransformer</a></code>,
<code><a href="#topic+.AIFETrObj">.AIFETrObj</a></code>
</p>

<hr>
<h2 id='.AIFERobertaTransformer'>Child <code>R6</code> class for creation and training of <code>RoBERTa</code> transformers</h2><span id='topic+.AIFERobertaTransformer'></span>

<h3>Description</h3>

<p>This class has the following methods:
</p>

<ul>
<li> <p><code>create</code>: creates a new transformer based on <code>RoBERTa</code>.
</p>
</li>
<li> <p><code>train</code>: trains and fine-tunes a <code>RoBERTa</code> model.
</p>
</li></ul>



<h3>Create</h3>

<p>New models can be created using the <code>.AIFERobertaTransformer$create</code> method.
</p>


<h3>Train</h3>

<p>To train the model, pass the directory of the model to the method <code>.AIFERobertaTransformer$train</code>.
</p>
<p>Pre-Trained models which can be fine-tuned with this function are available at <a href="https://huggingface.co/">https://huggingface.co/</a>.
</p>
<p>Training of this model makes use of dynamic masking.
</p>


<h3>Super class</h3>

<p><code><a href="#topic+.AIFEBaseTransformer">aifeducation::.AIFEBaseTransformer</a></code> -&gt; <code>.AIFERobertaTransformer</code>
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-.AIFERobertaTransformer-new"><code>.AIFERobertaTransformer$new()</code></a>
</p>
</li>
<li> <p><a href="#method-.AIFERobertaTransformer-create"><code>.AIFERobertaTransformer$create()</code></a>
</p>
</li>
<li> <p><a href="#method-.AIFERobertaTransformer-train"><code>.AIFERobertaTransformer$train()</code></a>
</p>
</li>
<li> <p><a href="#method-.AIFERobertaTransformer-clone"><code>.AIFERobertaTransformer$clone()</code></a>
</p>
</li></ul>



<details><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_SFC_calculate_vocab"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_SFC_calculate_vocab'><code>aifeducation::.AIFEBaseTransformer$set_SFC_calculate_vocab()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_SFC_check_max_pos_emb"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_SFC_check_max_pos_emb'><code>aifeducation::.AIFEBaseTransformer$set_SFC_check_max_pos_emb()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_SFC_create_final_tokenizer"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_SFC_create_final_tokenizer'><code>aifeducation::.AIFEBaseTransformer$set_SFC_create_final_tokenizer()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_SFC_create_tokenizer_draft"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_SFC_create_tokenizer_draft'><code>aifeducation::.AIFEBaseTransformer$set_SFC_create_tokenizer_draft()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_SFC_create_transformer_model"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_SFC_create_transformer_model'><code>aifeducation::.AIFEBaseTransformer$set_SFC_create_transformer_model()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_SFC_save_tokenizer_draft"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_SFC_save_tokenizer_draft'><code>aifeducation::.AIFEBaseTransformer$set_SFC_save_tokenizer_draft()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_SFT_create_data_collator"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_SFT_create_data_collator'><code>aifeducation::.AIFEBaseTransformer$set_SFT_create_data_collator()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_SFT_cuda_empty_cache"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_SFT_cuda_empty_cache'><code>aifeducation::.AIFEBaseTransformer$set_SFT_cuda_empty_cache()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_SFT_load_existing_model"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_SFT_load_existing_model'><code>aifeducation::.AIFEBaseTransformer$set_SFT_load_existing_model()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_model_param"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_model_param'><code>aifeducation::.AIFEBaseTransformer$set_model_param()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_model_temp"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_model_temp'><code>aifeducation::.AIFEBaseTransformer$set_model_temp()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_required_SFC"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_required_SFC'><code>aifeducation::.AIFEBaseTransformer$set_required_SFC()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic=".AIFEBaseTransformer" data-id="set_title"><a href='../../aifeducation/html/.AIFEBaseTransformer.html#method-.AIFEBaseTransformer-set_title'><code>aifeducation::.AIFEBaseTransformer$set_title()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-.AIFERobertaTransformer-new"></a>



<h4>Method <code>new()</code></h4>

<p>Creates a new transformer based on <code>RoBERTa</code> and sets the title.
</p>


<h5>Usage</h5>

<div class="r"><pre>.AIFERobertaTransformer$new()</pre></div>



<h5>Returns</h5>

<p>This method returns nothing.
</p>


<hr>
<a id="method-.AIFERobertaTransformer-create"></a>



<h4>Method <code>create()</code></h4>

<p>This method creates a transformer configuration based on the <code>RoBERTa</code> base architecture and a
vocabulary based on <code style="white-space: pre;">&#8288;Byte-Pair Encoding&#8288;</code> (BPE) tokenizer using the python <code>transformers</code> and <code>tokenizers</code>
libraries.
</p>
<p>This method adds the following <em>'dependent' parameters</em> to the base class' inherited <code>params</code> list:
</p>

<ul>
<li> <p><code>add_prefix_space</code>
</p>
</li>
<li> <p><code>trim_offsets</code>
</p>
</li>
<li> <p><code>num_hidden_layer</code>
</p>
</li></ul>



<h5>Usage</h5>

<div class="r"><pre>.AIFERobertaTransformer$create(
  ml_framework = "pytorch",
  model_dir,
  text_dataset,
  vocab_size = 30522,
  add_prefix_space = FALSE,
  trim_offsets = TRUE,
  max_position_embeddings = 512,
  hidden_size = 768,
  num_hidden_layer = 12,
  num_attention_heads = 12,
  intermediate_size = 3072,
  hidden_act = "gelu",
  hidden_dropout_prob = 0.1,
  attention_probs_dropout_prob = 0.1,
  sustain_track = TRUE,
  sustain_iso_code = NULL,
  sustain_region = NULL,
  sustain_interval = 15,
  trace = TRUE,
  pytorch_safetensors = TRUE,
  log_dir = NULL,
  log_write_interval = 2
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>ml_framework</code></dt><dd><p><code>string</code> Framework to use for training and inference.
</p>

<ul>
<li> <p><code>ml_framework = "tensorflow"</code>: for 'tensorflow'.
</p>
</li>
<li> <p><code>ml_framework = "pytorch"</code>: for 'pytorch'.
</p>
</li></ul>
</dd>
<dt><code>model_dir</code></dt><dd><p><code>string</code> Path to the directory where the model should be saved.</p>
</dd>
<dt><code>text_dataset</code></dt><dd><p>Object of class <a href="#topic+LargeDataSetForText">LargeDataSetForText</a>.</p>
</dd>
<dt><code>vocab_size</code></dt><dd><p><code>int</code> Size of the vocabulary.</p>
</dd>
<dt><code>add_prefix_space</code></dt><dd><p><code>bool</code> <code>TRUE</code> if an additional space should be inserted to the leading words.</p>
</dd>
<dt><code>trim_offsets</code></dt><dd><p><code>bool</code> <code>TRUE</code> trims the whitespaces from the produced offsets.</p>
</dd>
<dt><code>max_position_embeddings</code></dt><dd><p><code>int</code> Number of maximum position embeddings. This parameter also determines the maximum length of a sequence which
can be processed with the model.</p>
</dd>
<dt><code>hidden_size</code></dt><dd><p><code>int</code> Number of neurons in each layer. This parameter determines the dimensionality of the resulting text
embedding.</p>
</dd>
<dt><code>num_hidden_layer</code></dt><dd><p><code>int</code> Number of hidden layers.</p>
</dd>
<dt><code>num_attention_heads</code></dt><dd><p><code>int</code> Number of attention heads.</p>
</dd>
<dt><code>intermediate_size</code></dt><dd><p><code>int</code> Number of neurons in the intermediate layer of the attention mechanism.</p>
</dd>
<dt><code>hidden_act</code></dt><dd><p><code>string</code> Name of the activation function.</p>
</dd>
<dt><code>hidden_dropout_prob</code></dt><dd><p><code>double</code> Ratio of dropout.</p>
</dd>
<dt><code>attention_probs_dropout_prob</code></dt><dd><p><code>double</code> Ratio of dropout for attention probabilities.</p>
</dd>
<dt><code>sustain_track</code></dt><dd><p><code>bool</code> If <code>TRUE</code> energy consumption is tracked during training via the python library codecarbon.</p>
</dd>
<dt><code>sustain_iso_code</code></dt><dd><p><code>string</code> ISO code (Alpha-3-Code) for the country. This variable must be set if sustainability should be tracked. A
list can be found on Wikipedia: <a href="https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes">https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes</a>.</p>
</dd>
<dt><code>sustain_region</code></dt><dd><p><code>string</code> Region within a country. Only available for USA and Canada. See the documentation of codecarbon for more
information <a href="https://mlco2.github.io/codecarbon/parameters.html">https://mlco2.github.io/codecarbon/parameters.html</a>.</p>
</dd>
<dt><code>sustain_interval</code></dt><dd><p><code>integer</code> Interval in seconds for measuring power usage.</p>
</dd>
<dt><code>trace</code></dt><dd><p><code>bool</code> <code>TRUE</code> if information about the progress should be printed to the console.</p>
</dd>
<dt><code>pytorch_safetensors</code></dt><dd><p><code>bool</code> Only relevant for pytorch models.
</p>

<ul>
<li> <p><code>TRUE</code>: a 'pytorch' model is saved in safetensors format.
</p>
</li>
<li> <p><code>FALSE</code> (or 'safetensors' is not available): model is saved in the standard pytorch format (.bin).
</p>
</li></ul>
</dd>
<dt><code>log_dir</code></dt><dd><p>Path to the directory where the log files should be saved.</p>
</dd>
<dt><code>log_write_interval</code></dt><dd><p><code>int</code> Time in seconds determining the interval in which the logger should try to update the log files. Only relevant
if <code>log_dir</code> is not <code>NULL</code>.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>This method does not return an object. Instead, it saves the configuration and vocabulary of the new
model to disk.
</p>


<hr>
<a id="method-.AIFERobertaTransformer-train"></a>



<h4>Method <code>train()</code></h4>

<p>This method can be used to train or fine-tune a transformer based on <code>RoBERTa</code> Transformer
architecture with the help of the python libraries <code>transformers</code>, <code>datasets</code>, and <code>tokenizers</code>.
</p>


<h5>Usage</h5>

<div class="r"><pre>.AIFERobertaTransformer$train(
  ml_framework = "pytorch",
  output_dir,
  model_dir_path,
  text_dataset,
  p_mask = 0.15,
  val_size = 0.1,
  n_epoch = 1,
  batch_size = 12,
  chunk_size = 250,
  full_sequences_only = FALSE,
  min_seq_len = 50,
  learning_rate = 0.03,
  n_workers = 1,
  multi_process = FALSE,
  sustain_track = TRUE,
  sustain_iso_code = NULL,
  sustain_region = NULL,
  sustain_interval = 15,
  trace = TRUE,
  keras_trace = 1,
  pytorch_trace = 1,
  pytorch_safetensors = TRUE,
  log_dir = NULL,
  log_write_interval = 2
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>ml_framework</code></dt><dd><p><code>string</code> Framework to use for training and inference.
</p>

<ul>
<li> <p><code>ml_framework = "tensorflow"</code>: for 'tensorflow'.
</p>
</li>
<li> <p><code>ml_framework = "pytorch"</code>: for 'pytorch'.
</p>
</li></ul>
</dd>
<dt><code>output_dir</code></dt><dd><p><code>string</code> Path to the directory where the final model should be saved. If the directory does not exist, it will be
created.</p>
</dd>
<dt><code>model_dir_path</code></dt><dd><p><code>string</code> Path to the directory where the original model is stored.</p>
</dd>
<dt><code>text_dataset</code></dt><dd><p>Object of class <a href="#topic+LargeDataSetForText">LargeDataSetForText</a>.</p>
</dd>
<dt><code>p_mask</code></dt><dd><p><code>double</code> Ratio that determines the number of words/tokens used for masking.</p>
</dd>
<dt><code>val_size</code></dt><dd><p><code>double</code> Ratio that determines the amount of token chunks used for validation.</p>
</dd>
<dt><code>n_epoch</code></dt><dd><p><code>int</code> Number of epochs for training.</p>
</dd>
<dt><code>batch_size</code></dt><dd><p><code>int</code> Size of batches.</p>
</dd>
<dt><code>chunk_size</code></dt><dd><p><code>int</code> Size of every chunk for training.</p>
</dd>
<dt><code>full_sequences_only</code></dt><dd><p><code>bool</code> <code>TRUE</code> for using only chunks with a sequence length equal to <code>chunk_size</code>.</p>
</dd>
<dt><code>min_seq_len</code></dt><dd><p><code>int</code> Only relevant if <code>full_sequences_only = FALSE</code>. Value determines the minimal sequence length included in
training process.</p>
</dd>
<dt><code>learning_rate</code></dt><dd><p><code>double</code> Learning rate for adam optimizer.</p>
</dd>
<dt><code>n_workers</code></dt><dd><p><code>int</code> Number of workers. Only relevant if <code>ml_framework = "tensorflow"</code>.</p>
</dd>
<dt><code>multi_process</code></dt><dd><p><code>bool</code> <code>TRUE</code> if multiple processes should be activated. Only relevant if <code>ml_framework = "tensorflow"</code>.</p>
</dd>
<dt><code>sustain_track</code></dt><dd><p><code>bool</code> If <code>TRUE</code> energy consumption is tracked during training via the python library codecarbon.</p>
</dd>
<dt><code>sustain_iso_code</code></dt><dd><p><code>string</code> ISO code (Alpha-3-Code) for the country. This variable must be set if sustainability should be tracked. A
list can be found on Wikipedia: <a href="https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes">https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes</a>.</p>
</dd>
<dt><code>sustain_region</code></dt><dd><p><code>string</code> Region within a country. Only available for USA and Canada. See the documentation of codecarbon for more
information <a href="https://mlco2.github.io/codecarbon/parameters.html">https://mlco2.github.io/codecarbon/parameters.html</a>.</p>
</dd>
<dt><code>sustain_interval</code></dt><dd><p><code>integer</code> Interval in seconds for measuring power usage.</p>
</dd>
<dt><code>trace</code></dt><dd><p><code>bool</code> <code>TRUE</code> if information about the progress should be printed to the console.</p>
</dd>
<dt><code>keras_trace</code></dt><dd><p><code>int</code>
</p>

<ul>
<li> <p><code>keras_trace = 0</code>: does not print any information about the training process from keras on the console.
</p>
</li>
<li> <p><code>keras_trace = 1</code>: prints a progress bar.
</p>
</li>
<li> <p><code>keras_trace = 2</code>: prints one line of information for every epoch. Only relevant if <code>ml_framework = "tensorflow"</code>.
</p>
</li></ul>
</dd>
<dt><code>pytorch_trace</code></dt><dd><p><code>int</code>
</p>

<ul>
<li> <p><code>pytorch_trace = 0</code>: does not print any information about the training process from pytorch on the console.
</p>
</li>
<li> <p><code>pytorch_trace = 1</code>: prints a progress bar.
</p>
</li></ul>
</dd>
<dt><code>pytorch_safetensors</code></dt><dd><p><code>bool</code> Only relevant for pytorch models.
</p>

<ul>
<li> <p><code>TRUE</code>: a 'pytorch' model is saved in safetensors format.
</p>
</li>
<li> <p><code>FALSE</code> (or 'safetensors' is not available): model is saved in the standard pytorch format (.bin).
</p>
</li></ul>
</dd>
<dt><code>log_dir</code></dt><dd><p>Path to the directory where the log files should be saved.</p>
</dd>
<dt><code>log_write_interval</code></dt><dd><p><code>int</code> Time in seconds determining the interval in which the logger should try to update the log files. Only relevant
if <code>log_dir</code> is not <code>NULL</code>.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>This method does not return an object. Instead the trained or fine-tuned model is saved to disk.
</p>


<hr>
<a id="method-.AIFERobertaTransformer-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>.AIFERobertaTransformer$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>References</h3>

<p>Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., &amp;
Stoyanov, V. (2019). RoBERTa: A Robustly Optimized BERT Pretraining Approach. <a href="https://doi.org/10.48550/arXiv.1907.11692">doi:10.48550/arXiv.1907.11692</a>
</p>
<p>Hugging Face Documentation
</p>

<ul>
<li> <p><a href="https://huggingface.co/docs/transformers/model_doc/roberta">https://huggingface.co/docs/transformers/model_doc/roberta</a>
</p>
</li>
<li> <p><a href="https://huggingface.co/docs/transformers/model_doc/roberta#transformers.RobertaModel">https://huggingface.co/docs/transformers/model_doc/roberta#transformers.RobertaModel</a>
</p>
</li>
<li> <p><a href="https://huggingface.co/docs/transformers/model_doc/roberta#transformers.TFRobertaModel">https://huggingface.co/docs/transformers/model_doc/roberta#transformers.TFRobertaModel</a>
</p>
</li></ul>



<h3>See Also</h3>

<p>Other Transformers for developers: 
<code><a href="#topic+.AIFEBaseTransformer">.AIFEBaseTransformer</a></code>,
<code><a href="#topic+.AIFEBertTransformer">.AIFEBertTransformer</a></code>,
<code><a href="#topic+.AIFEDebertaTransformer">.AIFEDebertaTransformer</a></code>,
<code><a href="#topic+.AIFEFunnelTransformer">.AIFEFunnelTransformer</a></code>,
<code><a href="#topic+.AIFELongformerTransformer">.AIFELongformerTransformer</a></code>,
<code><a href="#topic+.AIFEMpnetTransformer">.AIFEMpnetTransformer</a></code>,
<code><a href="#topic+.AIFETrObj">.AIFETrObj</a></code>
</p>

<hr>
<h2 id='.AIFETrObj'>Transformer objects</h2><span id='topic+.AIFETrObj'></span>

<h3>Description</h3>

<p>This list contains transformer objects. Elements of the list are used in the public <code>make</code> of the
<a href="#topic+AIFETransformerMaker">AIFETransformerMaker</a> <code>R6</code> class. This list is not designed to be used directly.
</p>
<p>It has the following elements: bert, roberta, deberta_v2, funnel, longformer, mpnet
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.AIFETrObj
</code></pre>


<h3>Format</h3>

<p>An object of class <code>list</code> of length 6.
</p>


<h3>See Also</h3>

<p>Other Transformers for developers: 
<code><a href="#topic+.AIFEBaseTransformer">.AIFEBaseTransformer</a></code>,
<code><a href="#topic+.AIFEBertTransformer">.AIFEBertTransformer</a></code>,
<code><a href="#topic+.AIFEDebertaTransformer">.AIFEDebertaTransformer</a></code>,
<code><a href="#topic+.AIFEFunnelTransformer">.AIFEFunnelTransformer</a></code>,
<code><a href="#topic+.AIFELongformerTransformer">.AIFELongformerTransformer</a></code>,
<code><a href="#topic+.AIFEMpnetTransformer">.AIFEMpnetTransformer</a></code>,
<code><a href="#topic+.AIFERobertaTransformer">.AIFERobertaTransformer</a></code>
</p>

<hr>
<h2 id='aife_transformer_maker'><code>R6</code> object of the <code>AIFETransformerMaker</code> class</h2><span id='topic+aife_transformer_maker'></span>

<h3>Description</h3>

<p>Object for creating the transformers with different types. See <a href="#topic+AIFETransformerMaker">AIFETransformerMaker</a> class for
details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>aife_transformer_maker
</code></pre>


<h3>Format</h3>

<p>An object of class <code>AIFETransformerMaker</code> (inherits from <code>R6</code>) of length 3.
</p>


<h3>See Also</h3>

<p>Other Transformer: 
<code><a href="#topic+AIFETrType">AIFETrType</a></code>,
<code><a href="#topic+AIFETransformerMaker">AIFETransformerMaker</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Use 'make' method of the 'aifeducation::aife_transformer_maker' object
# Pass string with the type of transformers
# Allowed types are "bert", "deberta_v2", "funnel", etc. See aifeducation::AIFETrType list
my_bert &lt;- aife_transformer_maker$make("bert")

# Or use elements of the 'aifeducation::AIFETrType' list
my_longformer &lt;- aife_transformer_maker$make(AIFETrType$longformer)

# Run 'create' or 'train' methods of the transformer in order to create a
# new transformer or train the newly created one, respectively
# my_bert$create(...)
# my_bert$train(...)

# my_longformer$create(...)
# my_longformer$train(...)

</code></pre>

<hr>
<h2 id='AIFEBaseModel'>Base class for models using neural nets</h2><span id='topic+AIFEBaseModel'></span>

<h3>Description</h3>

<p>Abstract class for all models that do not rely on the python library 'transformers'.
</p>


<h3>Value</h3>

<p>Objects of this containing fields and methods used in several other classes in 'ai for education'. This class
is <strong>not</strong> designed for a direct application and should only be used by developers.
</p>


<h3>Public fields</h3>

<div class="r6-fields">

<dl>
<dt><code>model</code></dt><dd><p>('tensorflow_model' or 'pytorch_model')<br />
Field for storing the 'tensorflow' or 'pytorch' model after loading.</p>
</dd>
<dt><code>model_config</code></dt><dd><p>('list()')<br />
List for storing information about the configuration of the model.</p>
</dd>
<dt><code>last_training</code></dt><dd><p>('list()')<br />
List for storing the history, the configuration, and the results of the last
training. This information will be overwritten if a new training is started.
</p>

<ul>
<li> <p><code>last_training$start_time</code>: Time point when training started.
</p>
</li>
<li> <p><code>last_training$learning_time</code>: Duration of the training process.
</p>
</li>
<li> <p><code>last_training$finish_time</code>: Time when the last training finished.
</p>
</li>
<li> <p><code>last_training$history</code>: History of the last training.
</p>
</li>
<li> <p><code>last_training$data</code>: Object of class <code>table</code> storing the initial frequencies of the passed data.
</p>
</li>
<li> <p><code>last_training$config</code>: List storing the configuration used for the last training.
</p>
</li></ul>
</dd>
</dl>

</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-AIFEBaseModel-get_model_info"><code>AIFEBaseModel$get_model_info()</code></a>
</p>
</li>
<li> <p><a href="#method-AIFEBaseModel-get_text_embedding_model"><code>AIFEBaseModel$get_text_embedding_model()</code></a>
</p>
</li>
<li> <p><a href="#method-AIFEBaseModel-set_publication_info"><code>AIFEBaseModel$set_publication_info()</code></a>
</p>
</li>
<li> <p><a href="#method-AIFEBaseModel-get_publication_info"><code>AIFEBaseModel$get_publication_info()</code></a>
</p>
</li>
<li> <p><a href="#method-AIFEBaseModel-set_model_license"><code>AIFEBaseModel$set_model_license()</code></a>
</p>
</li>
<li> <p><a href="#method-AIFEBaseModel-get_model_license"><code>AIFEBaseModel$get_model_license()</code></a>
</p>
</li>
<li> <p><a href="#method-AIFEBaseModel-set_documentation_license"><code>AIFEBaseModel$set_documentation_license()</code></a>
</p>
</li>
<li> <p><a href="#method-AIFEBaseModel-get_documentation_license"><code>AIFEBaseModel$get_documentation_license()</code></a>
</p>
</li>
<li> <p><a href="#method-AIFEBaseModel-set_model_description"><code>AIFEBaseModel$set_model_description()</code></a>
</p>
</li>
<li> <p><a href="#method-AIFEBaseModel-get_model_description"><code>AIFEBaseModel$get_model_description()</code></a>
</p>
</li>
<li> <p><a href="#method-AIFEBaseModel-save"><code>AIFEBaseModel$save()</code></a>
</p>
</li>
<li> <p><a href="#method-AIFEBaseModel-load"><code>AIFEBaseModel$load()</code></a>
</p>
</li>
<li> <p><a href="#method-AIFEBaseModel-get_package_versions"><code>AIFEBaseModel$get_package_versions()</code></a>
</p>
</li>
<li> <p><a href="#method-AIFEBaseModel-get_sustainability_data"><code>AIFEBaseModel$get_sustainability_data()</code></a>
</p>
</li>
<li> <p><a href="#method-AIFEBaseModel-get_ml_framework"><code>AIFEBaseModel$get_ml_framework()</code></a>
</p>
</li>
<li> <p><a href="#method-AIFEBaseModel-get_text_embedding_model_name"><code>AIFEBaseModel$get_text_embedding_model_name()</code></a>
</p>
</li>
<li> <p><a href="#method-AIFEBaseModel-check_embedding_model"><code>AIFEBaseModel$check_embedding_model()</code></a>
</p>
</li>
<li> <p><a href="#method-AIFEBaseModel-count_parameter"><code>AIFEBaseModel$count_parameter()</code></a>
</p>
</li>
<li> <p><a href="#method-AIFEBaseModel-is_configured"><code>AIFEBaseModel$is_configured()</code></a>
</p>
</li>
<li> <p><a href="#method-AIFEBaseModel-get_private"><code>AIFEBaseModel$get_private()</code></a>
</p>
</li>
<li> <p><a href="#method-AIFEBaseModel-get_all_fields"><code>AIFEBaseModel$get_all_fields()</code></a>
</p>
</li>
<li> <p><a href="#method-AIFEBaseModel-clone"><code>AIFEBaseModel$clone()</code></a>
</p>
</li></ul>


<hr>
<a id="method-AIFEBaseModel-get_model_info"></a>



<h4>Method <code>get_model_info()</code></h4>

<p>Method for requesting the model information.
</p>


<h5>Usage</h5>

<div class="r"><pre>AIFEBaseModel$get_model_info()</pre></div>



<h5>Returns</h5>

<p><code>list</code> of all relevant model information.
</p>


<hr>
<a id="method-AIFEBaseModel-get_text_embedding_model"></a>



<h4>Method <code>get_text_embedding_model()</code></h4>

<p>Method for requesting the text embedding model information.
</p>


<h5>Usage</h5>

<div class="r"><pre>AIFEBaseModel$get_text_embedding_model()</pre></div>



<h5>Returns</h5>

<p><code>list</code> of all relevant model information on the text embedding model underlying the model.
</p>


<hr>
<a id="method-AIFEBaseModel-set_publication_info"></a>



<h4>Method <code>set_publication_info()</code></h4>

<p>Method for setting publication information of the model.
</p>


<h5>Usage</h5>

<div class="r"><pre>AIFEBaseModel$set_publication_info(authors, citation, url = NULL)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>authors</code></dt><dd><p>List of authors.</p>
</dd>
<dt><code>citation</code></dt><dd><p>Free text citation.</p>
</dd>
<dt><code>url</code></dt><dd><p>URL of a corresponding homepage.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>Function does not return a value. It is used for setting the private members for publication information.
</p>


<hr>
<a id="method-AIFEBaseModel-get_publication_info"></a>



<h4>Method <code>get_publication_info()</code></h4>

<p>Method for requesting the bibliographic information of the model.
</p>


<h5>Usage</h5>

<div class="r"><pre>AIFEBaseModel$get_publication_info()</pre></div>



<h5>Returns</h5>

<p><code>list</code> with all saved bibliographic information.
</p>


<hr>
<a id="method-AIFEBaseModel-set_model_license"></a>



<h4>Method <code>set_model_license()</code></h4>

<p>Method for setting the license of the model.
</p>


<h5>Usage</h5>

<div class="r"><pre>AIFEBaseModel$set_model_license(license = "CC BY")</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>license</code></dt><dd><p><code>string</code> containing the abbreviation of the license or the license text.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>Function does not return a value. It is used for setting the private member for the software license of
the model.
</p>


<hr>
<a id="method-AIFEBaseModel-get_model_license"></a>



<h4>Method <code>get_model_license()</code></h4>

<p>Method for getting the license of the model.
</p>


<h5>Usage</h5>

<div class="r"><pre>AIFEBaseModel$get_model_license()</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>license</code></dt><dd><p><code>string</code> containing the abbreviation of the license or the license text.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p><code>string</code> representing the license for the model.
</p>


<hr>
<a id="method-AIFEBaseModel-set_documentation_license"></a>



<h4>Method <code>set_documentation_license()</code></h4>

<p>Method for setting the license of the model's documentation.
</p>


<h5>Usage</h5>

<div class="r"><pre>AIFEBaseModel$set_documentation_license(license = "CC BY")</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>license</code></dt><dd><p><code>string</code> containing the abbreviation of the license or the license text.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>Function does not return a value. It is used for setting the private member for the documentation license
of the model.
</p>


<hr>
<a id="method-AIFEBaseModel-get_documentation_license"></a>



<h4>Method <code>get_documentation_license()</code></h4>

<p>Method for getting the license of the model's documentation.
</p>


<h5>Usage</h5>

<div class="r"><pre>AIFEBaseModel$get_documentation_license()</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>license</code></dt><dd><p><code>string</code> containing the abbreviation of the license or the license text.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>Returns the license as a <code>string</code>.
</p>


<hr>
<a id="method-AIFEBaseModel-set_model_description"></a>



<h4>Method <code>set_model_description()</code></h4>

<p>Method for setting a description of the model.
</p>


<h5>Usage</h5>

<div class="r"><pre>AIFEBaseModel$set_model_description(
  eng = NULL,
  native = NULL,
  abstract_eng = NULL,
  abstract_native = NULL,
  keywords_eng = NULL,
  keywords_native = NULL
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>eng</code></dt><dd><p><code>string</code> A text describing the training, its theoretical and empirical background, and output in
English.</p>
</dd>
<dt><code>native</code></dt><dd><p><code>string</code> A text describing the training , its theoretical and empirical background, and output in
the native language of the model.</p>
</dd>
<dt><code>abstract_eng</code></dt><dd><p><code>string</code> A text providing a summary of the description in English.</p>
</dd>
<dt><code>abstract_native</code></dt><dd><p><code>string</code> A text providing a summary of the description in the native language of the
model.</p>
</dd>
<dt><code>keywords_eng</code></dt><dd><p><code>vector</code> of keyword in English.</p>
</dd>
<dt><code>keywords_native</code></dt><dd><p><code>vector</code> of keyword in the native language of the model.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>Function does not return a value. It is used for setting the private members for the description of the
model.
</p>


<hr>
<a id="method-AIFEBaseModel-get_model_description"></a>



<h4>Method <code>get_model_description()</code></h4>

<p>Method for requesting the model description.
</p>


<h5>Usage</h5>

<div class="r"><pre>AIFEBaseModel$get_model_description()</pre></div>



<h5>Returns</h5>

<p><code>list</code> with the description of the classifier in English and the native language.
</p>


<hr>
<a id="method-AIFEBaseModel-save"></a>



<h4>Method <code>save()</code></h4>

<p>Method for saving a model.
</p>


<h5>Usage</h5>

<div class="r"><pre>AIFEBaseModel$save(dir_path, folder_name)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>dir_path</code></dt><dd><p><code>string</code> Path of the directory where the model should be saved.</p>
</dd>
<dt><code>folder_name</code></dt><dd><p><code>string</code> Name of the folder that should be created within the directory.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>Function does not return a value. It saves the model to disk.
</p>


<hr>
<a id="method-AIFEBaseModel-load"></a>



<h4>Method <code>load()</code></h4>

<p>Method for importing a model.
</p>


<h5>Usage</h5>

<div class="r"><pre>AIFEBaseModel$load(dir_path)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>dir_path</code></dt><dd><p><code>string</code> Path of the directory where the model is saved.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>Function does not return a value. It is used to load the weights of a model.
</p>


<hr>
<a id="method-AIFEBaseModel-get_package_versions"></a>



<h4>Method <code>get_package_versions()</code></h4>

<p>Method for requesting a summary of the R and python packages' versions used for creating the model.
</p>


<h5>Usage</h5>

<div class="r"><pre>AIFEBaseModel$get_package_versions()</pre></div>



<h5>Returns</h5>

<p>Returns a <code>list</code> containing the versions of the relevant R and python packages.
</p>


<hr>
<a id="method-AIFEBaseModel-get_sustainability_data"></a>



<h4>Method <code>get_sustainability_data()</code></h4>

<p>Method for requesting a summary of tracked energy consumption during training and an estimate of the
resulting CO2 equivalents in kg.
</p>


<h5>Usage</h5>

<div class="r"><pre>AIFEBaseModel$get_sustainability_data()</pre></div>



<h5>Returns</h5>

<p>Returns a <code>list</code> containing the tracked energy consumption, CO2 equivalents in kg, information on the
tracker used, and technical information on the training infrastructure.
</p>


<hr>
<a id="method-AIFEBaseModel-get_ml_framework"></a>



<h4>Method <code>get_ml_framework()</code></h4>

<p>Method for requesting the machine learning framework used for the model.
</p>


<h5>Usage</h5>

<div class="r"><pre>AIFEBaseModel$get_ml_framework()</pre></div>



<h5>Returns</h5>

<p>Returns a <code>string</code> describing the machine learning framework used for the classifier.
</p>


<hr>
<a id="method-AIFEBaseModel-get_text_embedding_model_name"></a>



<h4>Method <code>get_text_embedding_model_name()</code></h4>

<p>Method for requesting the name (unique id) of the underlying text embedding model.
</p>


<h5>Usage</h5>

<div class="r"><pre>AIFEBaseModel$get_text_embedding_model_name()</pre></div>



<h5>Returns</h5>

<p>Returns a <code>string</code> describing name of the text embedding model.
</p>


<hr>
<a id="method-AIFEBaseModel-check_embedding_model"></a>



<h4>Method <code>check_embedding_model()</code></h4>

<p>Method for checking if the provided text embeddings are created with the same <a href="#topic+TextEmbeddingModel">TextEmbeddingModel</a>
as the model.
</p>


<h5>Usage</h5>

<div class="r"><pre>AIFEBaseModel$check_embedding_model(text_embeddings)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>text_embeddings</code></dt><dd><p>Object of class <a href="#topic+EmbeddedText">EmbeddedText</a> or <a href="#topic+LargeDataSetForTextEmbeddings">LargeDataSetForTextEmbeddings</a>.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p><code>TRUE</code> if the underlying <a href="#topic+TextEmbeddingModel">TextEmbeddingModel</a> are the same. <code>FALSE</code> if the models differ.
</p>


<hr>
<a id="method-AIFEBaseModel-count_parameter"></a>



<h4>Method <code>count_parameter()</code></h4>

<p>Method for counting the trainable parameters of a model.
</p>


<h5>Usage</h5>

<div class="r"><pre>AIFEBaseModel$count_parameter()</pre></div>



<h5>Returns</h5>

<p>Returns the number of trainable parameters of the model.
</p>


<hr>
<a id="method-AIFEBaseModel-is_configured"></a>



<h4>Method <code>is_configured()</code></h4>

<p>Method for checking if the model was successfully configured. An object can only be used if this
value is <code>TRUE</code>.
</p>


<h5>Usage</h5>

<div class="r"><pre>AIFEBaseModel$is_configured()</pre></div>



<h5>Returns</h5>

<p><code>bool</code> <code>TRUE</code> if the model is fully configured. <code>FALSE</code> if not.
</p>


<hr>
<a id="method-AIFEBaseModel-get_private"></a>



<h4>Method <code>get_private()</code></h4>

<p>Method for requesting all private fields and methods. Used for loading and updating an object.
</p>


<h5>Usage</h5>

<div class="r"><pre>AIFEBaseModel$get_private()</pre></div>



<h5>Returns</h5>

<p>Returns a <code>list</code> with all private fields and methods.
</p>


<hr>
<a id="method-AIFEBaseModel-get_all_fields"></a>



<h4>Method <code>get_all_fields()</code></h4>

<p>Return all fields.
</p>


<h5>Usage</h5>

<div class="r"><pre>AIFEBaseModel$get_all_fields()</pre></div>



<h5>Returns</h5>

<p>Method returns a <code>list</code> containing all public and private fields
of the object.
</p>


<hr>
<a id="method-AIFEBaseModel-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>AIFEBaseModel$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>



<hr>
<h2 id='AIFETransformerMaker'><code>R6</code> class for transformer creation</h2><span id='topic+AIFETransformerMaker'></span>

<h3>Description</h3>

<p>This class was developed to make the creation of transformers easier for users. Pass the transformer's
type to the <code>make</code> method and get desired transformer. Now run the <code>create</code> or/and <code>train</code> methods of the new
transformer.
</p>
<p>The already created <a href="#topic+aife_transformer_maker">aife_transformer_maker</a> object of this class can be used.
</p>
<p>See p.3 Transformer Maker in
<a href="https://fberding.github.io/aifeducation/articles/transformers.html">Transformers for Developers</a> for details.
</p>
<p>See <a href="#topic+.AIFEBaseTransformer">.AIFEBaseTransformer</a> class for details.
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-AIFETransformerMaker-make"><code>AIFETransformerMaker$make()</code></a>
</p>
</li>
<li> <p><a href="#method-AIFETransformerMaker-clone"><code>AIFETransformerMaker$clone()</code></a>
</p>
</li></ul>


<hr>
<a id="method-AIFETransformerMaker-make"></a>



<h4>Method <code>make()</code></h4>

<p>Creates a new transformer with the passed type.
</p>


<h5>Usage</h5>

<div class="r"><pre>AIFETransformerMaker$make(type)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>type</code></dt><dd><p><code>string</code> A type of the new transformer. Allowed types are bert, roberta, deberta_v2, funnel, longformer, mpnet. See
<a href="#topic+AIFETrType">AIFETrType</a> list.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>If success - a new transformer, otherwise - an error (passed type is invalid).
</p>


<hr>
<a id="method-AIFETransformerMaker-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>AIFETransformerMaker$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>See Also</h3>

<p>Other Transformer: 
<code><a href="#topic+AIFETrType">AIFETrType</a></code>,
<code><a href="#topic+aife_transformer_maker">aife_transformer_maker</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Create transformer maker
tr_maker &lt;- AIFETransformerMaker$new()

# Use 'make' method of the 'tr_maker' object
# Pass string with the type of transformers
# Allowed types are "bert", "deberta_v2", "funnel", etc. See aifeducation::AIFETrType list
my_bert &lt;- tr_maker$make("bert")

# Or use elements of the 'aifeducation::AIFETrType' list
my_longformer &lt;- tr_maker$make(AIFETrType$longformer)

# Run 'create' or 'train' methods of the transformer in order to create a
# new transformer or train the newly created one, respectively
# my_bert$create(...)
# my_bert$train(...)

# my_longformer$create(...)
# my_longformer$train(...)

</code></pre>

<hr>
<h2 id='AIFETrType'>Transformer types</h2><span id='topic+AIFETrType'></span>

<h3>Description</h3>

<p>This list contains transformer types. Elements of the list can be used in the public <code>make</code> of the
<a href="#topic+AIFETransformerMaker">AIFETransformerMaker</a> <code>R6</code> class as input parameter <code>type</code>.
</p>
<p>It has the following elements:
</p>

<ul>
<li> <p><code>bert</code> = 'bert'
</p>
</li>
<li> <p><code>roberta</code> = 'roberta'
</p>
</li>
<li> <p><code>deberta_v2</code> = 'deberta_v2'
</p>
</li>
<li> <p><code>funnel</code> = 'funnel'
</p>
</li>
<li> <p><code>longformer</code> = 'longformer'
</p>
</li>
<li> <p><code>mpnet</code> = 'mpnet'
</p>
<p>Elements can be used like <code>AIFETrType$bert</code>, <code>AIFETrType$deberta_v2</code>, <code>AIFETrType$funnel</code>, etc.
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>AIFETrType
</code></pre>


<h3>Format</h3>

<p>An object of class <code>list</code> of length 6.
</p>


<h3>See Also</h3>

<p>Other Transformer: 
<code><a href="#topic+AIFETransformerMaker">AIFETransformerMaker</a></code>,
<code><a href="#topic+aife_transformer_maker">aife_transformer_maker</a></code>
</p>

<hr>
<h2 id='auto_n_cores'>Number of cores for multiple tasks</h2><span id='topic+auto_n_cores'></span>

<h3>Description</h3>

<p>Function for getting the number of cores that should be used
for parallel processing of tasks. The number of cores is set to 75 % of the
available cores. If the environment variable <code>CI</code> is set to <code>"true"</code> or if the
process is running on cran <code>2</code> is returned.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>auto_n_cores()
</code></pre>


<h3>Value</h3>

<p>Returns <code>int</code> as the number of cores.
</p>


<h3>See Also</h3>

<p>Other Utils: 
<code><a href="#topic+clean_pytorch_log_transformers">clean_pytorch_log_transformers</a>()</code>,
<code><a href="#topic+create_config_state">create_config_state</a>()</code>,
<code><a href="#topic+create_dir">create_dir</a>()</code>,
<code><a href="#topic+generate_id">generate_id</a>()</code>,
<code><a href="#topic+get_file_extension">get_file_extension</a>()</code>,
<code><a href="#topic+get_py_package_versions">get_py_package_versions</a>()</code>,
<code><a href="#topic+is.null_or_na">is.null_or_na</a>()</code>,
<code><a href="#topic+output_message">output_message</a>()</code>,
<code><a href="#topic+print_message">print_message</a>()</code>,
<code><a href="#topic+run_py_file">run_py_file</a>()</code>
</p>

<hr>
<h2 id='calc_standard_classification_measures'>Calculate standard classification measures</h2><span id='topic+calc_standard_classification_measures'></span>

<h3>Description</h3>

<p>Function for calculating recall, precision, and f1.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calc_standard_classification_measures(true_values, predicted_values)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="calc_standard_classification_measures_+3A_true_values">true_values</code></td>
<td>
<p><code>factor</code> containing the true labels/categories.</p>
</td></tr>
<tr><td><code id="calc_standard_classification_measures_+3A_predicted_values">predicted_values</code></td>
<td>
<p><code>factor</code> containing the predicted labels/categories.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a matrix which contains the cases categories in the rows and the measures (precision, recall, f1) in
the columns.
</p>


<h3>See Also</h3>

<p>Other classifier_utils: 
<code><a href="#topic+get_coder_metrics">get_coder_metrics</a>()</code>
</p>

<hr>
<h2 id='check_aif_py_modules'>Check if all necessary python modules are available</h2><span id='topic+check_aif_py_modules'></span>

<h3>Description</h3>

<p>This function checks if all  python modules necessary for the package aifeducation to work are
available.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_aif_py_modules(trace = TRUE, check = "pytorch")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="check_aif_py_modules_+3A_trace">trace</code></td>
<td>
<p><code>bool</code> <code>TRUE</code> if a list with all modules and their availability should be printed to the console.</p>
</td></tr>
<tr><td><code id="check_aif_py_modules_+3A_check">check</code></td>
<td>
<p><code>string</code> determining the machine learning framework to check for.
</p>

<ul>
<li> <p><code>check = "pytorch"</code>: for 'pytorch'.
</p>
</li>
<li> <p><code>check = "tensorflow"</code>: for 'tensorflow'.
</p>
</li>
<li> <p><code>check = "all"</code>: for both frameworks.
</p>
</li></ul>
</td></tr>
</table>


<h3>Value</h3>

<p>The function prints a table with all relevant packages and shows which modules are available or unavailable.
</p>
<p>If all relevant modules are available, the functions returns <code>TRUE</code>. In all other cases it returns <code>FALSE</code>
</p>


<h3>See Also</h3>

<p>Other Installation and Configuration: 
<code><a href="#topic+install_aifeducation">install_aifeducation</a>()</code>,
<code><a href="#topic+install_py_modules">install_py_modules</a>()</code>,
<code><a href="#topic+set_transformers_logger">set_transformers_logger</a>()</code>
</p>

<hr>
<h2 id='clean_pytorch_log_transformers'>Clean pytorch log of transformers</h2><span id='topic+clean_pytorch_log_transformers'></span>

<h3>Description</h3>

<p>Function for preparing and cleaning the log created by an object of class Trainer from the python
library 'transformer's.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>clean_pytorch_log_transformers(log)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="clean_pytorch_log_transformers_+3A_log">log</code></td>
<td>
<p><code>data.frame</code> containing the log.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a <code>data.frame</code> containing epochs, loss, and val_loss.
</p>


<h3>See Also</h3>

<p>Other Utils: 
<code><a href="#topic+auto_n_cores">auto_n_cores</a>()</code>,
<code><a href="#topic+create_config_state">create_config_state</a>()</code>,
<code><a href="#topic+create_dir">create_dir</a>()</code>,
<code><a href="#topic+generate_id">generate_id</a>()</code>,
<code><a href="#topic+get_file_extension">get_file_extension</a>()</code>,
<code><a href="#topic+get_py_package_versions">get_py_package_versions</a>()</code>,
<code><a href="#topic+is.null_or_na">is.null_or_na</a>()</code>,
<code><a href="#topic+output_message">output_message</a>()</code>,
<code><a href="#topic+print_message">print_message</a>()</code>,
<code><a href="#topic+run_py_file">run_py_file</a>()</code>
</p>

<hr>
<h2 id='cohens_kappa'>Calculate Cohen's Kappa</h2><span id='topic+cohens_kappa'></span>

<h3>Description</h3>

<p>This function calculates different version of Cohen's Kappa.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cohens_kappa(rater_one, rater_two)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cohens_kappa_+3A_rater_one">rater_one</code></td>
<td>
<p><code>factor</code> rating of the first coder.</p>
</td></tr>
<tr><td><code id="cohens_kappa_+3A_rater_two">rater_two</code></td>
<td>
<p><code>factor</code> ratings of the second coder.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a <code>list</code> containing the results for Cohen' Kappa if no weights
are applied (<code>kappa_unweighted</code>), if weights are applied and the weights increase
linear (<code>kappa_linear</code>), and if weights are applied and the weights increase quadratic
(<code>kappa_squared</code>).
</p>


<h3>References</h3>

<p>Cohen, J (1968). Weighted kappa: Nominal scale agreement
with provision for scaled disagreement or partial credit.
Psychological Bulletin, 70(4), 213–220. <a href="doi:10.1037/h0026256">doi:10.1037/h0026256</a>
</p>
<p>Cohen, J (1960). A Coefficient of Agreement for Nominal Scales.
Educational and Psychological Measurement, 20(1), 37–46. <a href="doi:10.1177/001316446002000104">doi:10.1177/001316446002000104</a>
</p>


<h3>See Also</h3>

<p>Other performance measures: 
<code><a href="#topic+fleiss_kappa">fleiss_kappa</a>()</code>,
<code><a href="#topic+kendalls_w">kendalls_w</a>()</code>,
<code><a href="#topic+kripp_alpha">kripp_alpha</a>()</code>
</p>

<hr>
<h2 id='create_config_state'>Create config for R interfaces</h2><span id='topic+create_config_state'></span>

<h3>Description</h3>

<p>Function creates a config that can be saved to disk. It is used during loading
an object from disk in order to set the correct configuration.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_config_state(object)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="create_config_state_+3A_object">object</code></td>
<td>
<p>Object of class <code>"TEClassifierRegular"</code>, <code>"TEClassifierProtoNet"</code>,
<code>"TEFeatureExtractor"</code>, <code>"TextEmbeddingModel"</code>, <code>"LargeDataSetForTextEmbeddings"</code>,
<code>"LargeDataSetForText"</code>, <code>"EmbeddedText"</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a <code>list</code> that contains the class of the object, the public, and
private fields.
</p>


<h3>See Also</h3>

<p>Other Utils: 
<code><a href="#topic+auto_n_cores">auto_n_cores</a>()</code>,
<code><a href="#topic+clean_pytorch_log_transformers">clean_pytorch_log_transformers</a>()</code>,
<code><a href="#topic+create_dir">create_dir</a>()</code>,
<code><a href="#topic+generate_id">generate_id</a>()</code>,
<code><a href="#topic+get_file_extension">get_file_extension</a>()</code>,
<code><a href="#topic+get_py_package_versions">get_py_package_versions</a>()</code>,
<code><a href="#topic+is.null_or_na">is.null_or_na</a>()</code>,
<code><a href="#topic+output_message">output_message</a>()</code>,
<code><a href="#topic+print_message">print_message</a>()</code>,
<code><a href="#topic+run_py_file">run_py_file</a>()</code>
</p>

<hr>
<h2 id='create_data_embeddings_description'>Generate description for text embeddings</h2><span id='topic+create_data_embeddings_description'></span>

<h3>Description</h3>

<p>Function generates a description for the underling <a href="#topic+TextEmbeddingModel">TextEmbeddingModel</a> of
give text embeddings.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_data_embeddings_description(embeddings)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="create_data_embeddings_description_+3A_embeddings">embeddings</code></td>
<td>
<p>Object of class <a href="#topic+LargeDataSetForTextEmbeddings">LargeDataSetForTextEmbeddings</a> or <a href="#topic+EmbeddedText">EmbeddedText</a>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a <code>shiny::tagList</code> containing the html elements for the user interface.
</p>


<h3>See Also</h3>

<p>Other studio_utils: 
<code><a href="#topic+long_load_target_data">long_load_target_data</a>()</code>
</p>

<hr>
<h2 id='create_dir'>Create directory if not exists</h2><span id='topic+create_dir'></span>

<h3>Description</h3>

<p>Check whether the passed <code>dir_path</code> directory exists. If not, creates a new directory and prints a <code>msg</code>
message if <code>trace</code> is <code>TRUE</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_dir(dir_path, trace, msg = "Creating Directory", msg_fun = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="create_dir_+3A_dir_path">dir_path</code></td>
<td>
<p><code>string</code> A new directory path that should be created.</p>
</td></tr>
<tr><td><code id="create_dir_+3A_trace">trace</code></td>
<td>
<p><code>bool</code> Whether a <code>msg</code> message should be printed.</p>
</td></tr>
<tr><td><code id="create_dir_+3A_msg">msg</code></td>
<td>
<p><code>string</code> A message that should be printed if <code>trace</code> is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="create_dir_+3A_msg_fun">msg_fun</code></td>
<td>
<p><code>func</code> Function used for printing the message.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>TRUE</code> or <code>FALSE</code> depending on whether the shiny app is active.
</p>


<h3>See Also</h3>

<p>Other Utils: 
<code><a href="#topic+auto_n_cores">auto_n_cores</a>()</code>,
<code><a href="#topic+clean_pytorch_log_transformers">clean_pytorch_log_transformers</a>()</code>,
<code><a href="#topic+create_config_state">create_config_state</a>()</code>,
<code><a href="#topic+generate_id">generate_id</a>()</code>,
<code><a href="#topic+get_file_extension">get_file_extension</a>()</code>,
<code><a href="#topic+get_py_package_versions">get_py_package_versions</a>()</code>,
<code><a href="#topic+is.null_or_na">is.null_or_na</a>()</code>,
<code><a href="#topic+output_message">output_message</a>()</code>,
<code><a href="#topic+print_message">print_message</a>()</code>,
<code><a href="#topic+run_py_file">run_py_file</a>()</code>
</p>

<hr>
<h2 id='create_synthetic_units_from_matrix'>Create synthetic units</h2><span id='topic+create_synthetic_units_from_matrix'></span>

<h3>Description</h3>

<p>Function for creating synthetic cases in order to balance the data for training with
<a href="#topic+TEClassifierRegular">TEClassifierRegular</a> or <a href="#topic+TEClassifierProtoNet">TEClassifierProtoNet</a>]. This is an auxiliary function for use with <a href="#topic+get_synthetic_cases_from_matrix">get_synthetic_cases_from_matrix</a> to allow
parallel computations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_synthetic_units_from_matrix(
  matrix_form,
  target,
  required_cases,
  k,
  method,
  cat,
  k_s,
  max_k
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="create_synthetic_units_from_matrix_+3A_matrix_form">matrix_form</code></td>
<td>
<p>Named <code>matrix</code> containing the text embeddings in matrix form. In most cases this object is taken
from <a href="#topic+EmbeddedText">EmbeddedText</a>$embeddings.</p>
</td></tr>
<tr><td><code id="create_synthetic_units_from_matrix_+3A_target">target</code></td>
<td>
<p>Named <code>factor</code> containing the labels/categories of the corresponding cases.</p>
</td></tr>
<tr><td><code id="create_synthetic_units_from_matrix_+3A_required_cases">required_cases</code></td>
<td>
<p><code>int</code> Number of cases necessary to fill the gab between the frequency of the class under
investigation and the major class.</p>
</td></tr>
<tr><td><code id="create_synthetic_units_from_matrix_+3A_k">k</code></td>
<td>
<p><code>int</code> The number of nearest neighbors during sampling process.</p>
</td></tr>
<tr><td><code id="create_synthetic_units_from_matrix_+3A_method">method</code></td>
<td>
<p><code>vector</code> containing strings of the requested methods for generating new cases. Currently
&quot;smote&quot;,&quot;dbsmote&quot;, and &quot;adas&quot; from the package smotefamily are available.</p>
</td></tr>
<tr><td><code id="create_synthetic_units_from_matrix_+3A_cat">cat</code></td>
<td>
<p><code>string</code> The category for which new cases should be created.</p>
</td></tr>
<tr><td><code id="create_synthetic_units_from_matrix_+3A_k_s">k_s</code></td>
<td>
<p><code>int</code> Number of ks in the complete generation process.</p>
</td></tr>
<tr><td><code id="create_synthetic_units_from_matrix_+3A_max_k">max_k</code></td>
<td>
<p><code>int</code> The maximum number of nearest neighbors during sampling process.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a <code>list</code> which contains the text embeddings of the new synthetic cases as a named <code>data.frame</code> and
their labels as a named <code>factor</code>.
</p>


<h3>See Also</h3>

<p>Other data_management_utils: 
<code><a href="#topic+get_n_chunks">get_n_chunks</a>()</code>,
<code><a href="#topic+get_synthetic_cases_from_matrix">get_synthetic_cases_from_matrix</a>()</code>
</p>

<hr>
<h2 id='DataManagerClassifier'>Data manager for classification tasks</h2><span id='topic+DataManagerClassifier'></span>

<h3>Description</h3>

<p>Abstract class for managing the data and samples during training a classifier. DataManagerClassifier is
used with <a href="#topic+TEClassifierRegular">TEClassifierRegular</a> and <a href="#topic+TEClassifierProtoNet">TEClassifierProtoNet</a>.
</p>


<h3>Value</h3>

<p>Objects of this class are used for ensuring the correct data management for training different types of
classifiers. Objects of this class are also used for data augmentation by creating synthetic cases with different
techniques.
</p>


<h3>Public fields</h3>

<div class="r6-fields">

<dl>
<dt><code>config</code></dt><dd><p>('list')<br />
Field for storing configuration of the <a href="#topic+DataManagerClassifier">DataManagerClassifier</a>.</p>
</dd>
<dt><code>state</code></dt><dd><p>('list')<br />
Field for storing the current state of the <a href="#topic+DataManagerClassifier">DataManagerClassifier</a>.</p>
</dd>
<dt><code>datasets</code></dt><dd><p>('list')<br />
Field for storing the data sets used during training. All elements of the list are data sets of class
<code>datasets.arrow_dataset.Dataset</code>. The following data sets are available:
</p>

<ul>
<li><p> data_labeled: all cases which have a label.
</p>
</li>
<li><p> data_unlabeled: all cases which have no label.
</p>
</li>
<li><p> data_labeled_synthetic: all synthetic cases with their corresponding labels.
</p>
</li>
<li><p> data_labeled_pseudo: subset of data_unlabeled if pseudo labels were estimated by a classifier.
</p>
</li></ul>
</dd>
<dt><code>name_idx</code></dt><dd><p>('named vector')<br />
Field for storing the pairs of indexes and names of every case. The pairs for labeled and unlabeled data are
separated.</p>
</dd>
<dt><code>samples</code></dt><dd><p>('list')<br />
Field for storing the assignment of every cases to a train, validation or test data set depending on the
concrete fold. Only the indexes and not the names are stored. In addition, the list contains the assignment for
the final training which excludes a test data set. If the <a href="#topic+DataManagerClassifier">DataManagerClassifier</a> uses <code>i</code> folds the sample for
the final training can be requested with <code>i+1</code>.</p>
</dd>
</dl>

</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-DataManagerClassifier-new"><code>DataManagerClassifier$new()</code></a>
</p>
</li>
<li> <p><a href="#method-DataManagerClassifier-get_config"><code>DataManagerClassifier$get_config()</code></a>
</p>
</li>
<li> <p><a href="#method-DataManagerClassifier-get_labeled_data"><code>DataManagerClassifier$get_labeled_data()</code></a>
</p>
</li>
<li> <p><a href="#method-DataManagerClassifier-get_unlabeled_data"><code>DataManagerClassifier$get_unlabeled_data()</code></a>
</p>
</li>
<li> <p><a href="#method-DataManagerClassifier-get_samples"><code>DataManagerClassifier$get_samples()</code></a>
</p>
</li>
<li> <p><a href="#method-DataManagerClassifier-set_state"><code>DataManagerClassifier$set_state()</code></a>
</p>
</li>
<li> <p><a href="#method-DataManagerClassifier-get_n_folds"><code>DataManagerClassifier$get_n_folds()</code></a>
</p>
</li>
<li> <p><a href="#method-DataManagerClassifier-get_n_classes"><code>DataManagerClassifier$get_n_classes()</code></a>
</p>
</li>
<li> <p><a href="#method-DataManagerClassifier-get_statistics"><code>DataManagerClassifier$get_statistics()</code></a>
</p>
</li>
<li> <p><a href="#method-DataManagerClassifier-get_dataset"><code>DataManagerClassifier$get_dataset()</code></a>
</p>
</li>
<li> <p><a href="#method-DataManagerClassifier-get_val_dataset"><code>DataManagerClassifier$get_val_dataset()</code></a>
</p>
</li>
<li> <p><a href="#method-DataManagerClassifier-get_test_dataset"><code>DataManagerClassifier$get_test_dataset()</code></a>
</p>
</li>
<li> <p><a href="#method-DataManagerClassifier-create_synthetic"><code>DataManagerClassifier$create_synthetic()</code></a>
</p>
</li>
<li> <p><a href="#method-DataManagerClassifier-add_replace_pseudo_data"><code>DataManagerClassifier$add_replace_pseudo_data()</code></a>
</p>
</li>
<li> <p><a href="#method-DataManagerClassifier-clone"><code>DataManagerClassifier$clone()</code></a>
</p>
</li></ul>


<hr>
<a id="method-DataManagerClassifier-new"></a>



<h4>Method <code>new()</code></h4>

<p>Creating a new instance of this class.
</p>


<h5>Usage</h5>

<div class="r"><pre>DataManagerClassifier$new(
  data_embeddings,
  data_targets,
  folds = 5,
  val_size = 0.25,
  class_levels,
  one_hot_encoding = TRUE,
  add_matrix_map = TRUE,
  sc_methods = "dbsmote",
  sc_min_k = 1,
  sc_max_k = 10,
  trace = TRUE,
  n_cores = auto_n_cores()
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>data_embeddings</code></dt><dd><p>Object of class <a href="#topic+EmbeddedText">EmbeddedText</a> or <a href="#topic+LargeDataSetForTextEmbeddings">LargeDataSetForTextEmbeddings</a> from which the
<a href="#topic+DataManagerClassifier">DataManagerClassifier</a> should be created.</p>
</dd>
<dt><code>data_targets</code></dt><dd><p><code>factor</code> containing the labels for cases stored in <code>data_embeddings</code>. Factor must be named
and has to use the same names used in <code>data_embeddings</code>. Missing values are supported and should be supplied
(e.g., for pseudo labeling).</p>
</dd>
<dt><code>folds</code></dt><dd><p><code>int</code> determining the number of cross-fold samples. Value must be at least 2.</p>
</dd>
<dt><code>val_size</code></dt><dd><p><code>double</code> between 0 and 1, indicating the proportion of cases of each class which should be used
for the validation sample. The remaining cases are part of the training data.</p>
</dd>
<dt><code>class_levels</code></dt><dd><p><code>vector</code> containing the possible levels of the labels.</p>
</dd>
<dt><code>one_hot_encoding</code></dt><dd><p><code>bool</code> If <code>TRUE</code> all labels are converted to one hot encoding.</p>
</dd>
<dt><code>add_matrix_map</code></dt><dd><p><code>bool</code> If <code>TRUE</code> all embeddings are transformed into a two dimensional matrix. The number
of rows equals the number of cases. The number of columns equals <code>times*features</code>.</p>
</dd>
<dt><code>sc_methods</code></dt><dd><p><code>string</code> determining the technique used for creating synthetic cases.</p>
</dd>
<dt><code>sc_min_k</code></dt><dd><p><code>int</code> determining the minimal number of neighbors during the creating of synthetic cases.</p>
</dd>
<dt><code>sc_max_k</code></dt><dd><p><code>int</code> determining the minimal number of neighbors during the creating of synthetic cases.</p>
</dd>
<dt><code>trace</code></dt><dd><p><code>bool</code> If <code>TRUE</code> information on the process are printed to the console.</p>
</dd>
<dt><code>n_cores</code></dt><dd><p><code>int</code> Number of cores which should be used during the calculation of synthetic cases.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>Method returns an initialized object of class <a href="#topic+DataManagerClassifier">DataManagerClassifier</a>.
</p>


<hr>
<a id="method-DataManagerClassifier-get_config"></a>



<h4>Method <code>get_config()</code></h4>

<p>Method for requesting the configuration of the <a href="#topic+DataManagerClassifier">DataManagerClassifier</a>.
</p>


<h5>Usage</h5>

<div class="r"><pre>DataManagerClassifier$get_config()</pre></div>



<h5>Returns</h5>

<p>Returns a <code>list</code> storing the configuration of the <a href="#topic+DataManagerClassifier">DataManagerClassifier</a>.
</p>


<hr>
<a id="method-DataManagerClassifier-get_labeled_data"></a>



<h4>Method <code>get_labeled_data()</code></h4>

<p>Method for requesting the complete labeled data set.
</p>


<h5>Usage</h5>

<div class="r"><pre>DataManagerClassifier$get_labeled_data()</pre></div>



<h5>Returns</h5>

<p>Returns an object of class <code>datasets.arrow_dataset.Dataset</code> containing all cases with labels.
</p>


<hr>
<a id="method-DataManagerClassifier-get_unlabeled_data"></a>



<h4>Method <code>get_unlabeled_data()</code></h4>

<p>Method for requesting the complete unlabeled data set.
</p>


<h5>Usage</h5>

<div class="r"><pre>DataManagerClassifier$get_unlabeled_data()</pre></div>



<h5>Returns</h5>

<p>Returns an object of class <code>datasets.arrow_dataset.Dataset</code> containing all cases without labels.
</p>


<hr>
<a id="method-DataManagerClassifier-get_samples"></a>



<h4>Method <code>get_samples()</code></h4>

<p>Method for requesting the assignments to train, validation, and test data sets for every fold and
the final training.
</p>


<h5>Usage</h5>

<div class="r"><pre>DataManagerClassifier$get_samples()</pre></div>



<h5>Returns</h5>

<p>Returns a <code>list</code> storing the assignments to a train, validation, and test data set for every fold. In the
case of the sample for the final training the test data set is always empty (<code>NULL</code>).
</p>


<hr>
<a id="method-DataManagerClassifier-set_state"></a>



<h4>Method <code>set_state()</code></h4>

<p>Method for setting the current state of the <a href="#topic+DataManagerClassifier">DataManagerClassifier</a>.
</p>


<h5>Usage</h5>

<div class="r"><pre>DataManagerClassifier$set_state(iteration, step = NULL)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>iteration</code></dt><dd><p><code>int</code> determining the current iteration of the training. That is iteration determines the fold
to use for training, validation, and testing. If <em>i</em> is the number of fold <em>i+1</em> request the sample for the
final training. For requesting the sample for the final training iteration can take a string <code>"final"</code>.</p>
</dd>
<dt><code>step</code></dt><dd><p><code>int</code> determining the step for estimating and using pseudo labels during training. Only relevant if
training is requested with pseudo labels.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>Method does not return anything. It is used for setting the internal state of the DataManager.
</p>


<hr>
<a id="method-DataManagerClassifier-get_n_folds"></a>



<h4>Method <code>get_n_folds()</code></h4>

<p>Method for requesting the number of folds the <a href="#topic+DataManagerClassifier">DataManagerClassifier</a> can use with the current data.
</p>


<h5>Usage</h5>

<div class="r"><pre>DataManagerClassifier$get_n_folds()</pre></div>



<h5>Returns</h5>

<p>Returns the number of folds the <a href="#topic+DataManagerClassifier">DataManagerClassifier</a> uses.
</p>


<hr>
<a id="method-DataManagerClassifier-get_n_classes"></a>



<h4>Method <code>get_n_classes()</code></h4>

<p>Method for requesting the number of classes.
</p>


<h5>Usage</h5>

<div class="r"><pre>DataManagerClassifier$get_n_classes()</pre></div>



<h5>Returns</h5>

<p>Returns the number classes.
</p>


<hr>
<a id="method-DataManagerClassifier-get_statistics"></a>



<h4>Method <code>get_statistics()</code></h4>

<p>Method for requesting descriptive sample statistics.
</p>


<h5>Usage</h5>

<div class="r"><pre>DataManagerClassifier$get_statistics()</pre></div>



<h5>Returns</h5>

<p>Returns a table describing the absolute frequencies of the labeled and unlabeled data. The rows contain
the length of the sequences while the columns contain the labels.
</p>


<hr>
<a id="method-DataManagerClassifier-get_dataset"></a>



<h4>Method <code>get_dataset()</code></h4>

<p>Method for requesting a data set for training depending in the current state of the
DataManagerClassifier.
</p>


<h5>Usage</h5>

<div class="r"><pre>DataManagerClassifier$get_dataset(
  inc_labeled = TRUE,
  inc_unlabeled = FALSE,
  inc_synthetic = FALSE,
  inc_pseudo_data = FALSE
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>inc_labeled</code></dt><dd><p><code>bool</code> If <code>TRUE</code> the data set includes all cases which have labels.</p>
</dd>
<dt><code>inc_unlabeled</code></dt><dd><p><code>bool</code> If <code>TRUE</code> the data set includes all cases which have no labels.</p>
</dd>
<dt><code>inc_synthetic</code></dt><dd><p><code>bool</code> If <code>TRUE</code> the data set includes all synthetic cases with their corresponding labels.</p>
</dd>
<dt><code>inc_pseudo_data</code></dt><dd><p><code>bool</code> If <code>TRUE</code> the data set includes all cases which have pseudo labels.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>Returns an object of class <code>datasets.arrow_dataset.Dataset</code> containing the requested kind of data along
with all requested transformations for training. Please note that this method returns a data sets that is
designed for training only. The corresponding validation data set is requested with <code>get_val_dataset</code> and the
corresponding test data set with <code>get_test_dataset</code>.
</p>


<hr>
<a id="method-DataManagerClassifier-get_val_dataset"></a>



<h4>Method <code>get_val_dataset()</code></h4>

<p>Method for requesting a data set for validation depending in the current state of the
<a href="#topic+DataManagerClassifier">DataManagerClassifier</a>.
</p>


<h5>Usage</h5>

<div class="r"><pre>DataManagerClassifier$get_val_dataset()</pre></div>



<h5>Returns</h5>

<p>Returns an object of class <code>datasets.arrow_dataset.Dataset</code> containing the requested kind of data along
with all requested transformations for validation. The corresponding data set for training can be requested
with <code>get_dataset</code> and the corresponding data set for testing with <code>get_test_dataset</code>.
</p>


<hr>
<a id="method-DataManagerClassifier-get_test_dataset"></a>



<h4>Method <code>get_test_dataset()</code></h4>

<p>Method for requesting a data set for testing depending in the current state of the
DataManagerClassifier.
</p>


<h5>Usage</h5>

<div class="r"><pre>DataManagerClassifier$get_test_dataset()</pre></div>



<h5>Returns</h5>

<p>Returns an object of class <code>datasets.arrow_dataset.Dataset</code> containing the requested kind of data along
with all requested transformations for validation. The corresponding data set for training can be requested
with <code>get_dataset</code> and the corresponding data set for validation with <code>get_val_dataset</code>.
</p>


<hr>
<a id="method-DataManagerClassifier-create_synthetic"></a>



<h4>Method <code>create_synthetic()</code></h4>

<p>Method for generating synthetic data used during training. The process uses all labeled data
belonging to the current state of the DataManagerClassifier.
</p>


<h5>Usage</h5>

<div class="r"><pre>DataManagerClassifier$create_synthetic(trace = TRUE, inc_pseudo_data = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>trace</code></dt><dd><p><code>bool</code> If <code>TRUE</code> information on the process are printed to the console.</p>
</dd>
<dt><code>inc_pseudo_data</code></dt><dd><p><code>bool</code> If <code>TRUE</code> data with pseudo labels are used in addition to the labeled data for
generating synthetic cases.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>This method does nothing return. It generates a new data set for synthetic cases which are stored as an
object of class <code>datasets.arrow_dataset.Dataset</code> in the field <code>datasets$data_labeled_synthetic</code>. Please note
that a call of this method will override an existing data set in the corresponding field.
</p>


<hr>
<a id="method-DataManagerClassifier-add_replace_pseudo_data"></a>



<h4>Method <code>add_replace_pseudo_data()</code></h4>

<p>Method for adding data with pseudo labels generated by a classifier
</p>


<h5>Usage</h5>

<div class="r"><pre>DataManagerClassifier$add_replace_pseudo_data(inputs, labels)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>inputs</code></dt><dd><p><code>array</code> or <code>matrix</code> representing the input data.</p>
</dd>
<dt><code>labels</code></dt><dd><p><code>factor</code> containing the corresponding pseudo labels.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>This method does nothing return. It generates a new data set for synthetic cases which are stored as an
object of class <code>datasets.arrow_dataset.Dataset</code> in the field <code>datasets$data_labeled_pseudo</code>. Please note that
a call of this method will override an existing data set in the corresponding field.
</p>


<hr>
<a id="method-DataManagerClassifier-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>DataManagerClassifier$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>See Also</h3>

<p>Other Data Management: 
<code><a href="#topic+EmbeddedText">EmbeddedText</a></code>,
<code><a href="#topic+LargeDataSetForText">LargeDataSetForText</a></code>,
<code><a href="#topic+LargeDataSetForTextEmbeddings">LargeDataSetForTextEmbeddings</a></code>
</p>

<hr>
<h2 id='EmbeddedText'>Embedded text</h2><span id='topic+EmbeddedText'></span>

<h3>Description</h3>

<p>Object of class <code>R6</code> which stores the text embeddings generated by an object of class
<a href="#topic+TextEmbeddingModel">TextEmbeddingModel</a>. The text embeddings are stored within memory/RAM. In the case of a high number of documents
the data may not fit into memory/RAM. Thus, please use this object only for a small sample of texts. In general, it
is recommended to use an object of class <a href="#topic+LargeDataSetForTextEmbeddings">LargeDataSetForTextEmbeddings</a> which can deal with any number of texts.
</p>


<h3>Value</h3>

<p>Returns an object of class <a href="#topic+EmbeddedText">EmbeddedText</a>. These objects are used for storing and managing the text
embeddings created with objects of class <a href="#topic+TextEmbeddingModel">TextEmbeddingModel</a>. Objects of class <a href="#topic+EmbeddedText">EmbeddedText</a> serve as input for
objects of class <a href="#topic+TEClassifierRegular">TEClassifierRegular</a>, <a href="#topic+TEClassifierProtoNet">TEClassifierProtoNet</a>, and <a href="#topic+TEFeatureExtractor">TEFeatureExtractor</a>. The main aim of this
class is to provide a structured link between embedding models and classifiers. Since objects of this class save
information on the text embedding model that created the text embedding it ensures that only embedding generated
with same embedding model are combined. Furthermore, the stored information allows objects to check if embeddings
of the correct text embedding model are used for training and predicting.
</p>


<h3>Public fields</h3>

<div class="r6-fields">

<dl>
<dt><code>embeddings</code></dt><dd><p>('data.frame()')<br />
data.frame containing the text embeddings for all chunks. Documents are in the rows. Embedding dimensions are
in the columns.</p>
</dd>
</dl>

</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-EmbeddedText-configure"><code>EmbeddedText$configure()</code></a>
</p>
</li>
<li> <p><a href="#method-EmbeddedText-save"><code>EmbeddedText$save()</code></a>
</p>
</li>
<li> <p><a href="#method-EmbeddedText-is_configured"><code>EmbeddedText$is_configured()</code></a>
</p>
</li>
<li> <p><a href="#method-EmbeddedText-load_from_disk"><code>EmbeddedText$load_from_disk()</code></a>
</p>
</li>
<li> <p><a href="#method-EmbeddedText-get_model_info"><code>EmbeddedText$get_model_info()</code></a>
</p>
</li>
<li> <p><a href="#method-EmbeddedText-get_model_label"><code>EmbeddedText$get_model_label()</code></a>
</p>
</li>
<li> <p><a href="#method-EmbeddedText-get_times"><code>EmbeddedText$get_times()</code></a>
</p>
</li>
<li> <p><a href="#method-EmbeddedText-get_features"><code>EmbeddedText$get_features()</code></a>
</p>
</li>
<li> <p><a href="#method-EmbeddedText-get_original_features"><code>EmbeddedText$get_original_features()</code></a>
</p>
</li>
<li> <p><a href="#method-EmbeddedText-is_compressed"><code>EmbeddedText$is_compressed()</code></a>
</p>
</li>
<li> <p><a href="#method-EmbeddedText-add_feature_extractor_info"><code>EmbeddedText$add_feature_extractor_info()</code></a>
</p>
</li>
<li> <p><a href="#method-EmbeddedText-get_feature_extractor_info"><code>EmbeddedText$get_feature_extractor_info()</code></a>
</p>
</li>
<li> <p><a href="#method-EmbeddedText-convert_to_LargeDataSetForTextEmbeddings"><code>EmbeddedText$convert_to_LargeDataSetForTextEmbeddings()</code></a>
</p>
</li>
<li> <p><a href="#method-EmbeddedText-n_rows"><code>EmbeddedText$n_rows()</code></a>
</p>
</li>
<li> <p><a href="#method-EmbeddedText-get_all_fields"><code>EmbeddedText$get_all_fields()</code></a>
</p>
</li>
<li> <p><a href="#method-EmbeddedText-clone"><code>EmbeddedText$clone()</code></a>
</p>
</li></ul>


<hr>
<a id="method-EmbeddedText-configure"></a>



<h4>Method <code>configure()</code></h4>

<p>Creates a new object representing text embeddings.
</p>


<h5>Usage</h5>

<div class="r"><pre>EmbeddedText$configure(
  model_name = NA,
  model_label = NA,
  model_date = NA,
  model_method = NA,
  model_version = NA,
  model_language = NA,
  param_seq_length = NA,
  param_chunks = NULL,
  param_features = NULL,
  param_overlap = NULL,
  param_emb_layer_min = NULL,
  param_emb_layer_max = NULL,
  param_emb_pool_type = NULL,
  param_aggregation = NULL,
  embeddings
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>model_name</code></dt><dd><p><code>string</code> Name of the model that generates this embedding.</p>
</dd>
<dt><code>model_label</code></dt><dd><p><code>string</code> Label of the model that generates this embedding.</p>
</dd>
<dt><code>model_date</code></dt><dd><p><code>string</code> Date when the embedding generating model was created.</p>
</dd>
<dt><code>model_method</code></dt><dd><p><code>string</code> Method of the underlying embedding model.</p>
</dd>
<dt><code>model_version</code></dt><dd><p><code>string</code> Version of the model that generated this embedding.</p>
</dd>
<dt><code>model_language</code></dt><dd><p><code>string</code> Language of the model that generated this embedding.</p>
</dd>
<dt><code>param_seq_length</code></dt><dd><p><code>int</code> Maximum number of tokens that processes the generating model for a chunk.</p>
</dd>
<dt><code>param_chunks</code></dt><dd><p><code>int</code> Maximum number of chunks which are supported by the generating model.</p>
</dd>
<dt><code>param_features</code></dt><dd><p><code>int</code> Number of dimensions of the text embeddings.</p>
</dd>
<dt><code>param_overlap</code></dt><dd><p><code>int</code> Number of tokens that were added at the beginning of the sequence for the next chunk
by this model.    #'</p>
</dd>
<dt><code>param_emb_layer_min</code></dt><dd><p><code>int</code> or <code>string</code> determining the first layer to be included in the creation of
embeddings.</p>
</dd>
<dt><code>param_emb_layer_max</code></dt><dd><p><code>int</code> or <code>string</code> determining the last layer to be included in the creation of
embeddings.</p>
</dd>
<dt><code>param_emb_pool_type</code></dt><dd><p><code>string</code> determining the method for pooling the token embeddings within each layer.</p>
</dd>
<dt><code>param_aggregation</code></dt><dd><p><code>string</code> Aggregation method of the hidden states. Deprecated. Only included for backward
compatibility.</p>
</dd>
<dt><code>embeddings</code></dt><dd><p><code>data.frame</code> containing the text embeddings.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>Returns an object of class <a href="#topic+EmbeddedText">EmbeddedText</a> which stores the text embeddings produced by an objects of
class <a href="#topic+TextEmbeddingModel">TextEmbeddingModel</a>.
</p>


<hr>
<a id="method-EmbeddedText-save"></a>



<h4>Method <code>save()</code></h4>

<p>Saves a data set to disk.
</p>


<h5>Usage</h5>

<div class="r"><pre>EmbeddedText$save(dir_path, folder_name, create_dir = TRUE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>dir_path</code></dt><dd><p>Path where to store the data set.</p>
</dd>
<dt><code>folder_name</code></dt><dd><p><code>string</code> Name of the folder for storing the data set.</p>
</dd>
<dt><code>create_dir</code></dt><dd><p><code>bool</code> If <code>True</code> the directory will be created if it does not exist.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>Method does not return anything. It write the data set to disk.
</p>


<hr>
<a id="method-EmbeddedText-is_configured"></a>



<h4>Method <code>is_configured()</code></h4>

<p>Method for checking if the model was successfully configured. An object can only be used if this
value is <code>TRUE</code>.
</p>


<h5>Usage</h5>

<div class="r"><pre>EmbeddedText$is_configured()</pre></div>



<h5>Returns</h5>

<p><code>bool</code> <code>TRUE</code> if the model is fully configured. <code>FALSE</code> if not.
</p>


<hr>
<a id="method-EmbeddedText-load_from_disk"></a>



<h4>Method <code>load_from_disk()</code></h4>

<p>loads an object of class <a href="#topic+EmbeddedText">EmbeddedText</a> from disk and updates the object to the current version of
the package.
</p>


<h5>Usage</h5>

<div class="r"><pre>EmbeddedText$load_from_disk(dir_path)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>dir_path</code></dt><dd><p>Path where the data set set is stored.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>Method does not return anything. It loads an object from disk.
</p>


<hr>
<a id="method-EmbeddedText-get_model_info"></a>



<h4>Method <code>get_model_info()</code></h4>

<p>Method for retrieving information about the model that generated this embedding.
</p>


<h5>Usage</h5>

<div class="r"><pre>EmbeddedText$get_model_info()</pre></div>



<h5>Returns</h5>

<p><code>list</code> contains all saved information about the underlying text embedding model.
</p>


<hr>
<a id="method-EmbeddedText-get_model_label"></a>



<h4>Method <code>get_model_label()</code></h4>

<p>Method for retrieving the label of the model that generated this embedding.
</p>


<h5>Usage</h5>

<div class="r"><pre>EmbeddedText$get_model_label()</pre></div>



<h5>Returns</h5>

<p><code>string</code> Label of the corresponding text embedding model
</p>


<hr>
<a id="method-EmbeddedText-get_times"></a>



<h4>Method <code>get_times()</code></h4>

<p>Number of chunks/times of the text embeddings.
</p>


<h5>Usage</h5>

<div class="r"><pre>EmbeddedText$get_times()</pre></div>



<h5>Returns</h5>

<p>Returns an <code>int</code> describing the number of chunks/times of the text embeddings.
</p>


<hr>
<a id="method-EmbeddedText-get_features"></a>



<h4>Method <code>get_features()</code></h4>

<p>Number of actual features/dimensions of the text embeddings.In the case a
<a href="#topic+TEFeatureExtractor">feature extractor</a> was used the number of features is smaller as the original number of
features. To receive the original number of features (the number of features before applying a
<a href="#topic+TEFeatureExtractor">feature extractor</a>) you can use the method <code>get_original_features</code> of this class.
</p>


<h5>Usage</h5>

<div class="r"><pre>EmbeddedText$get_features()</pre></div>



<h5>Returns</h5>

<p>Returns an <code>int</code> describing the number of features/dimensions of the text embeddings.
</p>


<hr>
<a id="method-EmbeddedText-get_original_features"></a>



<h4>Method <code>get_original_features()</code></h4>

<p>Number of original features/dimensions of the text embeddings.
</p>


<h5>Usage</h5>

<div class="r"><pre>EmbeddedText$get_original_features()</pre></div>



<h5>Returns</h5>

<p>Returns an <code>int</code> describing the number of features/dimensions if no
<a href="#topic+TEFeatureExtractor">feature extractor</a>) is used or before a <a href="#topic+TEFeatureExtractor">feature extractor</a>) is
applied.
</p>


<hr>
<a id="method-EmbeddedText-is_compressed"></a>



<h4>Method <code>is_compressed()</code></h4>

<p>Checks if the text embedding were reduced by a <a href="#topic+TEFeatureExtractor">feature extractor</a>.
</p>


<h5>Usage</h5>

<div class="r"><pre>EmbeddedText$is_compressed()</pre></div>



<h5>Returns</h5>

<p>Returns <code>TRUE</code> if the number of dimensions was reduced by a <a href="#topic+TEFeatureExtractor">feature extractor</a>. If
not return <code>FALSE</code>.
</p>


<hr>
<a id="method-EmbeddedText-add_feature_extractor_info"></a>



<h4>Method <code>add_feature_extractor_info()</code></h4>

<p>Method setting information on the <a href="#topic+TEFeatureExtractor">feature extractor</a> that was used to reduce
the number of dimensions of the text embeddings. This information should only be used if a
<a href="#topic+TEFeatureExtractor">feature extractor</a> was applied.
</p>


<h5>Usage</h5>

<div class="r"><pre>EmbeddedText$add_feature_extractor_info(
  model_name,
  model_label = NA,
  features = NA,
  method = NA,
  noise_factor = NA,
  optimizer = NA
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>model_name</code></dt><dd><p><code>string</code> Name of the underlying <a href="#topic+TextEmbeddingModel">TextEmbeddingModel</a>.</p>
</dd>
<dt><code>model_label</code></dt><dd><p><code>string</code> Label of the underlying <a href="#topic+TextEmbeddingModel">TextEmbeddingModel</a>.</p>
</dd>
<dt><code>features</code></dt><dd><p><code>int</code> Number of dimension (features) for the <strong>compressed</strong> text embeddings.</p>
</dd>
<dt><code>method</code></dt><dd><p><code>string</code> Method that the <a href="#topic+TEFeatureExtractor">TEFeatureExtractor</a> applies for genereating the compressed text
embeddings.</p>
</dd>
<dt><code>noise_factor</code></dt><dd><p><code>double</code> Noise factor of the <a href="#topic+TEFeatureExtractor">TEFeatureExtractor</a>.</p>
</dd>
<dt><code>optimizer</code></dt><dd><p><code>string</code> Optimizer used during training the <a href="#topic+TEFeatureExtractor">TEFeatureExtractor</a>.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>Method does nothing return. It sets information on a <a href="#topic+TEFeatureExtractor">feature extractor</a>.
</p>


<hr>
<a id="method-EmbeddedText-get_feature_extractor_info"></a>



<h4>Method <code>get_feature_extractor_info()</code></h4>

<p>Method for receiving information on the <a href="#topic+TEFeatureExtractor">feature extractor</a> that was used to
reduce the number of dimensions of the text embeddings.
</p>


<h5>Usage</h5>

<div class="r"><pre>EmbeddedText$get_feature_extractor_info()</pre></div>



<h5>Returns</h5>

<p>Returns a <code>list</code> with information on the <a href="#topic+TEFeatureExtractor">feature extractor</a>. If no
<a href="#topic+TEFeatureExtractor">feature extractor</a> was used it returns <code>NULL</code>.
</p>


<hr>
<a id="method-EmbeddedText-convert_to_LargeDataSetForTextEmbeddings"></a>



<h4>Method <code>convert_to_LargeDataSetForTextEmbeddings()</code></h4>

<p>Method for converting this object to an object of class <a href="#topic+LargeDataSetForTextEmbeddings">LargeDataSetForTextEmbeddings</a>.
</p>


<h5>Usage</h5>

<div class="r"><pre>EmbeddedText$convert_to_LargeDataSetForTextEmbeddings()</pre></div>



<h5>Returns</h5>

<p>Returns an object of class <a href="#topic+LargeDataSetForTextEmbeddings">LargeDataSetForTextEmbeddings</a> which uses memory mapping allowing to work
with large data sets.
</p>


<hr>
<a id="method-EmbeddedText-n_rows"></a>



<h4>Method <code>n_rows()</code></h4>

<p>Number of rows.
</p>


<h5>Usage</h5>

<div class="r"><pre>EmbeddedText$n_rows()</pre></div>



<h5>Returns</h5>

<p>Returns the number of rows of the text embeddings which represent the number of cases.
</p>


<hr>
<a id="method-EmbeddedText-get_all_fields"></a>



<h4>Method <code>get_all_fields()</code></h4>

<p>Return all fields.
</p>


<h5>Usage</h5>

<div class="r"><pre>EmbeddedText$get_all_fields()</pre></div>



<h5>Returns</h5>

<p>Method returns a <code>list</code> containing all public and private fields
of the object.
</p>


<hr>
<a id="method-EmbeddedText-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>EmbeddedText$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>See Also</h3>

<p>Other Data Management: 
<code><a href="#topic+DataManagerClassifier">DataManagerClassifier</a></code>,
<code><a href="#topic+LargeDataSetForText">LargeDataSetForText</a></code>,
<code><a href="#topic+LargeDataSetForTextEmbeddings">LargeDataSetForTextEmbeddings</a></code>
</p>

<hr>
<h2 id='fleiss_kappa'>Calculate Fleiss' Kappa</h2><span id='topic+fleiss_kappa'></span>

<h3>Description</h3>

<p>This function calculates Fleiss' Kappa.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fleiss_kappa(rater_one, rater_two, additional_raters = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="fleiss_kappa_+3A_rater_one">rater_one</code></td>
<td>
<p><code>factor</code> rating of the first coder.</p>
</td></tr>
<tr><td><code id="fleiss_kappa_+3A_rater_two">rater_two</code></td>
<td>
<p><code>factor</code> ratings of the second coder.</p>
</td></tr>
<tr><td><code id="fleiss_kappa_+3A_additional_raters">additional_raters</code></td>
<td>
<p><code>list</code> Additional raters with same requirements as <code>rater_one</code> and <code>rater_two</code>. If
there are no additional raters set to <code>NULL</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Retuns the value for Fleiss' Kappa.
</p>


<h3>References</h3>

<p>Fleiss, J. L. (1971). Measuring nominal scale agreement among
many raters. Psychological Bulletin, 76(5), 378–382. <a href="doi:10.1037/h0031619">doi:10.1037/h0031619</a>
</p>


<h3>See Also</h3>

<p>Other performance measures: 
<code><a href="#topic+cohens_kappa">cohens_kappa</a>()</code>,
<code><a href="#topic+kendalls_w">kendalls_w</a>()</code>,
<code><a href="#topic+kripp_alpha">kripp_alpha</a>()</code>
</p>

<hr>
<h2 id='generate_id'>Generate ID suffix for objects</h2><span id='topic+generate_id'></span>

<h3>Description</h3>

<p>Function for generating an ID suffix for objects of class <a href="#topic+TextEmbeddingModel">TextEmbeddingModel</a>,
<a href="#topic+TEClassifierRegular">TEClassifierRegular</a>, and <a href="#topic+TEClassifierProtoNet">TEClassifierProtoNet</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>generate_id(length = 16)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="generate_id_+3A_length">length</code></td>
<td>
<p><code>int</code> determining the length of the id suffix.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a <code>string</code> of the requested length.
</p>


<h3>See Also</h3>

<p>Other Utils: 
<code><a href="#topic+auto_n_cores">auto_n_cores</a>()</code>,
<code><a href="#topic+clean_pytorch_log_transformers">clean_pytorch_log_transformers</a>()</code>,
<code><a href="#topic+create_config_state">create_config_state</a>()</code>,
<code><a href="#topic+create_dir">create_dir</a>()</code>,
<code><a href="#topic+get_file_extension">get_file_extension</a>()</code>,
<code><a href="#topic+get_py_package_versions">get_py_package_versions</a>()</code>,
<code><a href="#topic+is.null_or_na">is.null_or_na</a>()</code>,
<code><a href="#topic+output_message">output_message</a>()</code>,
<code><a href="#topic+print_message">print_message</a>()</code>,
<code><a href="#topic+run_py_file">run_py_file</a>()</code>
</p>

<hr>
<h2 id='get_alpha_3_codes'>Country Alpha 3 Codes</h2><span id='topic+get_alpha_3_codes'></span>

<h3>Description</h3>

<p>Function for requesting a <code>vector</code> containing the alpha-3 codes for most countries.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_alpha_3_codes()
</code></pre>


<h3>Value</h3>

<p>Returns a <code>vector</code> containing the alpha-3 codes for most countries.
</p>


<h3>See Also</h3>

<p>Other Auxiliary Functions: 
<code><a href="#topic+matrix_to_array_c">matrix_to_array_c</a>()</code>,
<code><a href="#topic+summarize_tracked_sustainability">summarize_tracked_sustainability</a>()</code>,
<code><a href="#topic+to_categorical_c">to_categorical_c</a>()</code>
</p>

<hr>
<h2 id='get_coder_metrics'>Calculate reliability measures based on content analysis</h2><span id='topic+get_coder_metrics'></span>

<h3>Description</h3>

<p>This function calculates different reliability measures which are based on the empirical research method
of content analysis.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_coder_metrics(
  true_values = NULL,
  predicted_values = NULL,
  return_names_only = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_coder_metrics_+3A_true_values">true_values</code></td>
<td>
<p><code>factor</code> containing the true labels/categories.</p>
</td></tr>
<tr><td><code id="get_coder_metrics_+3A_predicted_values">predicted_values</code></td>
<td>
<p><code>factor</code> containing the predicted labels/categories.</p>
</td></tr>
<tr><td><code id="get_coder_metrics_+3A_return_names_only">return_names_only</code></td>
<td>
<p><code>bool</code> If <code>TRUE</code> returns only the names of the resulting vector. Use <code>FALSE</code> to request
computation of the values.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>If <code>return_names_only = FALSE</code> returns a <code>vector</code> with the following reliability measures:
</p>

<ul>
<li> <p><strong>iota_index</strong>: Iota Index from the Iota Reliability Concept Version 2.
</p>
</li>
<li> <p><strong>min_iota2</strong>: Minimal Iota from Iota Reliability Concept Version 2.
</p>
</li>
<li> <p><strong>avg_iota2</strong>: Average Iota from Iota Reliability Concept Version 2.
</p>
</li>
<li> <p><strong>max_iota2</strong>: Maximum Iota from Iota Reliability Concept Version 2.
</p>
</li>
<li> <p><strong>min_alpha</strong>: Minmal Alpha Reliability from Iota Reliability Concept Version 2.
</p>
</li>
<li> <p><strong>avg_alpha</strong>: Average Alpha Reliability from Iota Reliability Concept Version 2.
</p>
</li>
<li> <p><strong>max_alpha</strong>: Maximum Alpha Reliability from Iota Reliability Concept Version 2.
</p>
</li>
<li> <p><strong>static_iota_index</strong>: Static Iota Index from Iota Reliability Concept Version 2.
</p>
</li>
<li> <p><strong>dynamic_iota_index</strong>: Dynamic Iota Index Iota Reliability Concept Version 2.
</p>
</li>
<li> <p><strong>kalpha_nominal</strong>: Krippendorff's Alpha for nominal variables.
</p>
</li>
<li> <p><strong>kalpha_ordinal</strong>: Krippendorff's Alpha for ordinal variables.
</p>
</li>
<li> <p><strong>kendall</strong>: Kendall's coefficient of concordance W with correction for ties.
</p>
</li>
<li> <p><strong>c_kappa_unweighted</strong>: Cohen's Kappa unweighted.
</p>
</li>
<li> <p><strong>c_kappa_linear</strong>: Weighted Cohen's Kappa with linear increasing weights.
</p>
</li>
<li> <p><strong>c_kappa_squared</strong>: Weighted Cohen's Kappa with quadratic increasing weights.
</p>
</li>
<li> <p><strong>kappa_fleiss</strong>: Fleiss' Kappa for multiple raters without exact estimation.
</p>
</li>
<li> <p><strong>percentage_agreement</strong>: Percentage Agreement.
</p>
</li>
<li> <p><strong>balanced_accuracy</strong>: Average accuracy within each class.
</p>
</li>
<li> <p><strong>gwet_ac</strong>: Gwet's AC1/AC2 agreement coefficient.
</p>
</li></ul>

<p>If <code>return_names_only = TRUE</code> returns only the names of the vector elements.
</p>


<h3>See Also</h3>

<p>Other classifier_utils: 
<code><a href="#topic+calc_standard_classification_measures">calc_standard_classification_measures</a>()</code>
</p>

<hr>
<h2 id='get_file_extension'>Get file extension</h2><span id='topic+get_file_extension'></span>

<h3>Description</h3>

<p>Function for requesting the file extension
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_file_extension(file_path)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_file_extension_+3A_file_path">file_path</code></td>
<td>
<p><code>string</code> Path to a file.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns the extension of a file as a string.
</p>


<h3>See Also</h3>

<p>Other Utils: 
<code><a href="#topic+auto_n_cores">auto_n_cores</a>()</code>,
<code><a href="#topic+clean_pytorch_log_transformers">clean_pytorch_log_transformers</a>()</code>,
<code><a href="#topic+create_config_state">create_config_state</a>()</code>,
<code><a href="#topic+create_dir">create_dir</a>()</code>,
<code><a href="#topic+generate_id">generate_id</a>()</code>,
<code><a href="#topic+get_py_package_versions">get_py_package_versions</a>()</code>,
<code><a href="#topic+is.null_or_na">is.null_or_na</a>()</code>,
<code><a href="#topic+output_message">output_message</a>()</code>,
<code><a href="#topic+print_message">print_message</a>()</code>,
<code><a href="#topic+run_py_file">run_py_file</a>()</code>
</p>

<hr>
<h2 id='get_n_chunks'>Get the number of chunks/sequences for each case</h2><span id='topic+get_n_chunks'></span>

<h3>Description</h3>

<p>Function for calculating the number of chunks/sequences for every case.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_n_chunks(text_embeddings, features, times)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_n_chunks_+3A_text_embeddings">text_embeddings</code></td>
<td>
<p><code>data.frame</code> or <code>array</code> containing the text embeddings.</p>
</td></tr>
<tr><td><code id="get_n_chunks_+3A_features">features</code></td>
<td>
<p><code>int</code> Number of features within each sequence.</p>
</td></tr>
<tr><td><code id="get_n_chunks_+3A_times">times</code></td>
<td>
<p><code>int</code> Number of sequences.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Named<code>vector</code> of integers representing the number of chunks/sequences for every case.
</p>


<h3>See Also</h3>

<p>Other data_management_utils: 
<code><a href="#topic+create_synthetic_units_from_matrix">create_synthetic_units_from_matrix</a>()</code>,
<code><a href="#topic+get_synthetic_cases_from_matrix">get_synthetic_cases_from_matrix</a>()</code>
</p>

<hr>
<h2 id='get_py_package_versions'>Get versions of python components</h2><span id='topic+get_py_package_versions'></span>

<h3>Description</h3>

<p>Function for requesting a summary of the versions of all
critical python components.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_py_package_versions()
</code></pre>


<h3>Value</h3>

<p>Returns a list that contains the version number of python and
the versions of critical python packages. If a package is not available
version is set to <code>NA</code>.
</p>


<h3>See Also</h3>

<p>Other Utils: 
<code><a href="#topic+auto_n_cores">auto_n_cores</a>()</code>,
<code><a href="#topic+clean_pytorch_log_transformers">clean_pytorch_log_transformers</a>()</code>,
<code><a href="#topic+create_config_state">create_config_state</a>()</code>,
<code><a href="#topic+create_dir">create_dir</a>()</code>,
<code><a href="#topic+generate_id">generate_id</a>()</code>,
<code><a href="#topic+get_file_extension">get_file_extension</a>()</code>,
<code><a href="#topic+is.null_or_na">is.null_or_na</a>()</code>,
<code><a href="#topic+output_message">output_message</a>()</code>,
<code><a href="#topic+print_message">print_message</a>()</code>,
<code><a href="#topic+run_py_file">run_py_file</a>()</code>
</p>

<hr>
<h2 id='get_synthetic_cases_from_matrix'>Create synthetic cases for balancing training data</h2><span id='topic+get_synthetic_cases_from_matrix'></span>

<h3>Description</h3>

<p>This function creates synthetic cases for balancing the training with an object of the class
<a href="#topic+TEClassifierRegular">TEClassifierRegular</a> or <a href="#topic+TEClassifierProtoNet">TEClassifierProtoNet</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_synthetic_cases_from_matrix(
  matrix_form,
  times,
  features,
  target,
  sequence_length,
  method = c("smote"),
  min_k = 1,
  max_k = 6
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_synthetic_cases_from_matrix_+3A_matrix_form">matrix_form</code></td>
<td>
<p>Named <code>matrix</code> containing the text embeddings in a matrix form.</p>
</td></tr>
<tr><td><code id="get_synthetic_cases_from_matrix_+3A_times">times</code></td>
<td>
<p><code>int</code> for the number of sequences/times.</p>
</td></tr>
<tr><td><code id="get_synthetic_cases_from_matrix_+3A_features">features</code></td>
<td>
<p><code>int</code> for the number of features within each sequence.</p>
</td></tr>
<tr><td><code id="get_synthetic_cases_from_matrix_+3A_target">target</code></td>
<td>
<p>Named <code>factor</code> containing the labels of the corresponding embeddings.</p>
</td></tr>
<tr><td><code id="get_synthetic_cases_from_matrix_+3A_sequence_length">sequence_length</code></td>
<td>
<p><code>int</code> Length of the text embedding sequences.</p>
</td></tr>
<tr><td><code id="get_synthetic_cases_from_matrix_+3A_method">method</code></td>
<td>
<p><code>vector</code> containing strings of the requested methods for generating new cases. Currently &quot;smote&quot;,
&quot;dbsmote&quot;, and &quot;adas&quot; from the package smotefamily are available.</p>
</td></tr>
<tr><td><code id="get_synthetic_cases_from_matrix_+3A_min_k">min_k</code></td>
<td>
<p><code>int</code> The minimal number of nearest neighbors during sampling process.</p>
</td></tr>
<tr><td><code id="get_synthetic_cases_from_matrix_+3A_max_k">max_k</code></td>
<td>
<p><code>int</code> The maximum number of nearest neighbors during sampling process.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>list</code> with the following components:
</p>

<ul>
<li> <p><code>syntetic_embeddings</code>: Named <code>data.frame</code> containing the text embeddings of the synthetic cases.
</p>
</li>
<li> <p><code>syntetic_targets</code>: Named <code>factor</code> containing the labels of the corresponding synthetic cases.
</p>
</li>
<li> <p><code>n_syntetic_units</code>: <code>table</code> showing the number of synthetic cases for every label/category.
</p>
</li></ul>



<h3>See Also</h3>

<p>Other data_management_utils: 
<code><a href="#topic+create_synthetic_units_from_matrix">create_synthetic_units_from_matrix</a>()</code>,
<code><a href="#topic+get_n_chunks">get_n_chunks</a>()</code>
</p>

<hr>
<h2 id='imdb_movie_reviews'>Standford Movie Review Dataset</h2><span id='topic+imdb_movie_reviews'></span>

<h3>Description</h3>

<p>A <a href="base.html#topic+data.frame">data.frame</a> consisting of a subset of 100 negative and 200 positive movie reviews from the
dataset provided by Maas et al. (2011). The <a href="base.html#topic+data.frame">data.frame</a> consists of three columns. The first column 'text'
stores the movie review. The second stores the labels (0 = negative, 1 = positive). The last column stores the id.
The purpose of the data is for illustration in vignettes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>imdb_movie_reviews
</code></pre>


<h3>Format</h3>

<p>data.frame
</p>


<h3>References</h3>

<p>Maas, A. L., Daly, R. E., Pham, P. T., Huang, D., Ng, A. Y., &amp; Potts, C. (2011). Learning Word Vectors
for Sentiment Analysis. In D. Lin, Y. Matsumoto, &amp; R. Mihalcea (Eds.), Proceedings of the 49th Annual Meeting of
the Association for Computational Linguistics: Human Language Technologies (pp. 142–150). Association for
Computational Linguistics. https://aclanthology.org/P11-1015
</p>

<hr>
<h2 id='install_aifeducation'>Install aifeducation on a machine</h2><span id='topic+install_aifeducation'></span>

<h3>Description</h3>

<p>Function for installing 'aifeducation' on a machine. This functions assumes that not 'python' and no
'miniconda' is installed. Only'pytorch' is installed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>install_aifeducation(install_aifeducation_studio = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="install_aifeducation_+3A_install_aifeducation_studio">install_aifeducation_studio</code></td>
<td>
<p><code>bool</code> If <code>TRUE</code> all necessary R packages are installed for using AI for Education
Studio.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Function does nothing return. It installs python, optional R packages, and necessary 'python' packages on a
machine.
</p>


<h3>See Also</h3>

<p>Other Installation and Configuration: 
<code><a href="#topic+check_aif_py_modules">check_aif_py_modules</a>()</code>,
<code><a href="#topic+install_py_modules">install_py_modules</a>()</code>,
<code><a href="#topic+set_transformers_logger">set_transformers_logger</a>()</code>
</p>

<hr>
<h2 id='install_py_modules'>Installing necessary python modules to an environment</h2><span id='topic+install_py_modules'></span>

<h3>Description</h3>

<p>Function for installing the necessary python modules.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>install_py_modules(
  envname = "aifeducation",
  install = "pytorch",
  transformer_version = "&lt;=4.46",
  tokenizers_version = "&lt;=0.20.4",
  pandas_version = "&lt;=2.2.3",
  datasets_version = "&lt;=3.1.0",
  codecarbon_version = "&lt;=2.8.2",
  safetensors_version = "&lt;=0.4.5",
  torcheval_version = "&lt;=0.0.7",
  accelerate_version = "&lt;=1.1.1",
  pytorch_cuda_version = "12.1",
  python_version = "3.9",
  remove_first = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="install_py_modules_+3A_envname">envname</code></td>
<td>
<p><code>string</code> Name of the environment where the packages should be installed.</p>
</td></tr>
<tr><td><code id="install_py_modules_+3A_install">install</code></td>
<td>
<p><code>character</code> determining which machine learning frameworks should be installed.
</p>

<ul>
<li> <p><code>install = "all"</code>: for 'pytorch' and 'tensorflow'.
</p>
</li>
<li> <p><code>install = "pytorch"</code>: for 'pytorch'.
</p>
</li>
<li> <p><code>install = "tensorflow"</code>: for 'tensorflow'.
</p>
</li></ul>
</td></tr>
<tr><td><code id="install_py_modules_+3A_transformer_version">transformer_version</code></td>
<td>
<p><code>string</code> determining the desired version of the python library 'transformers'.</p>
</td></tr>
<tr><td><code id="install_py_modules_+3A_tokenizers_version">tokenizers_version</code></td>
<td>
<p><code>string</code> determining the desired version of the python library 'tokenizers'.</p>
</td></tr>
<tr><td><code id="install_py_modules_+3A_pandas_version">pandas_version</code></td>
<td>
<p><code>string</code> determining the desired version of the python library 'pandas'.</p>
</td></tr>
<tr><td><code id="install_py_modules_+3A_datasets_version">datasets_version</code></td>
<td>
<p><code>string</code> determining the desired version of the python library 'datasets'.</p>
</td></tr>
<tr><td><code id="install_py_modules_+3A_codecarbon_version">codecarbon_version</code></td>
<td>
<p><code>string</code> determining the desired version of the python library 'codecarbon'.</p>
</td></tr>
<tr><td><code id="install_py_modules_+3A_safetensors_version">safetensors_version</code></td>
<td>
<p><code>string</code> determining the desired version of the python library 'safetensors'.</p>
</td></tr>
<tr><td><code id="install_py_modules_+3A_torcheval_version">torcheval_version</code></td>
<td>
<p><code>string</code> determining the desired version of the python library 'torcheval'.</p>
</td></tr>
<tr><td><code id="install_py_modules_+3A_accelerate_version">accelerate_version</code></td>
<td>
<p><code>string</code> determining the desired version of the python library 'accelerate'.</p>
</td></tr>
<tr><td><code id="install_py_modules_+3A_pytorch_cuda_version">pytorch_cuda_version</code></td>
<td>
<p><code>string</code> determining the desired version of 'cuda' for ' PyTorch'.</p>
</td></tr>
<tr><td><code id="install_py_modules_+3A_python_version">python_version</code></td>
<td>
<p><code>string</code> Python version to use.</p>
</td></tr>
<tr><td><code id="install_py_modules_+3A_remove_first">remove_first</code></td>
<td>
<p><code>bool</code> If <code>TRUE</code> removes the environment completely before recreating the environment and
installing the packages. If <code>FALSE</code> the packages are installed in the existing environment without any prior
changes.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns no values or objects. Function is used for installing the necessary python libraries in a conda
environment.
</p>


<h3>See Also</h3>

<p>Other Installation and Configuration: 
<code><a href="#topic+check_aif_py_modules">check_aif_py_modules</a>()</code>,
<code><a href="#topic+install_aifeducation">install_aifeducation</a>()</code>,
<code><a href="#topic+set_transformers_logger">set_transformers_logger</a>()</code>
</p>

<hr>
<h2 id='is.null_or_na'>Check if NULL or NA</h2><span id='topic+is.null_or_na'></span>

<h3>Description</h3>

<p>Function for checking if an object is <code>NULL</code> or .
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.null_or_na(object)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="is.null_or_na_+3A_object">object</code></td>
<td>
<p>An object to test.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns <code>FALSE</code> if the object is not <code>NULL</code> and not <code>NA</code>. Returns <code>TRUE</code> in all other cases.
</p>


<h3>See Also</h3>

<p>Other Utils: 
<code><a href="#topic+auto_n_cores">auto_n_cores</a>()</code>,
<code><a href="#topic+clean_pytorch_log_transformers">clean_pytorch_log_transformers</a>()</code>,
<code><a href="#topic+create_config_state">create_config_state</a>()</code>,
<code><a href="#topic+create_dir">create_dir</a>()</code>,
<code><a href="#topic+generate_id">generate_id</a>()</code>,
<code><a href="#topic+get_file_extension">get_file_extension</a>()</code>,
<code><a href="#topic+get_py_package_versions">get_py_package_versions</a>()</code>,
<code><a href="#topic+output_message">output_message</a>()</code>,
<code><a href="#topic+print_message">print_message</a>()</code>,
<code><a href="#topic+run_py_file">run_py_file</a>()</code>
</p>

<hr>
<h2 id='kendalls_w'>Calculate Kendall's coefficient of concordance w</h2><span id='topic+kendalls_w'></span>

<h3>Description</h3>

<p>This function calculates Kendall's coefficient of concordance w with and without correction.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kendalls_w(rater_one, rater_two, additional_raters = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="kendalls_w_+3A_rater_one">rater_one</code></td>
<td>
<p><code>factor</code> rating of the first coder.</p>
</td></tr>
<tr><td><code id="kendalls_w_+3A_rater_two">rater_two</code></td>
<td>
<p><code>factor</code> ratings of the second coder.</p>
</td></tr>
<tr><td><code id="kendalls_w_+3A_additional_raters">additional_raters</code></td>
<td>
<p><code>list</code> Additional raters with same requirements as <code>rater_one</code> and <code>rater_two</code>. If
there are no additional raters set to <code>NULL</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a <code>list</code> containing the results for Kendall's coefficient of concordance w
with and without correction.
</p>


<h3>See Also</h3>

<p>Other performance measures: 
<code><a href="#topic+cohens_kappa">cohens_kappa</a>()</code>,
<code><a href="#topic+fleiss_kappa">fleiss_kappa</a>()</code>,
<code><a href="#topic+kripp_alpha">kripp_alpha</a>()</code>
</p>

<hr>
<h2 id='kripp_alpha'>Calculate Krippendorff's Alpha</h2><span id='topic+kripp_alpha'></span>

<h3>Description</h3>

<p>This function calculates different Krippendorff's Alpha for nominal and ordinal variables.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kripp_alpha(rater_one, rater_two, additional_raters = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="kripp_alpha_+3A_rater_one">rater_one</code></td>
<td>
<p><code>factor</code> rating of the first coder.</p>
</td></tr>
<tr><td><code id="kripp_alpha_+3A_rater_two">rater_two</code></td>
<td>
<p><code>factor</code> ratings of the second coder.</p>
</td></tr>
<tr><td><code id="kripp_alpha_+3A_additional_raters">additional_raters</code></td>
<td>
<p><code>list</code> Additional raters with same requirements as <code>rater_one</code> and <code>rater_two</code>. If
there are no additional raters set to <code>NULL</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a <code>list</code> containing the results for Krippendorff's Alpha for
nominal and ordinal data.
</p>


<h3>References</h3>

<p>Krippendorff, K. (2019). Content Analysis: An Introduction to
Its Methodology (4th Ed.). SAGE
</p>


<h3>See Also</h3>

<p>Other performance measures: 
<code><a href="#topic+cohens_kappa">cohens_kappa</a>()</code>,
<code><a href="#topic+fleiss_kappa">fleiss_kappa</a>()</code>,
<code><a href="#topic+kendalls_w">kendalls_w</a>()</code>
</p>

<hr>
<h2 id='LargeDataSetBase'>Abstract base class for large data sets</h2><span id='topic+LargeDataSetBase'></span>

<h3>Description</h3>

<p>This object contains public and private methods which may be useful for every large data sets. Objects
of this class are not intended to be used directly. <a href="#topic+LargeDataSetForTextEmbeddings">LargeDataSetForTextEmbeddings</a> or <a href="#topic+LargeDataSetForText">LargeDataSetForText</a>.
</p>


<h3>Value</h3>

<p>Returns a new object of this class.
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-LargeDataSetBase-n_cols"><code>LargeDataSetBase$n_cols()</code></a>
</p>
</li>
<li> <p><a href="#method-LargeDataSetBase-n_rows"><code>LargeDataSetBase$n_rows()</code></a>
</p>
</li>
<li> <p><a href="#method-LargeDataSetBase-get_colnames"><code>LargeDataSetBase$get_colnames()</code></a>
</p>
</li>
<li> <p><a href="#method-LargeDataSetBase-get_dataset"><code>LargeDataSetBase$get_dataset()</code></a>
</p>
</li>
<li> <p><a href="#method-LargeDataSetBase-reduce_to_unique_ids"><code>LargeDataSetBase$reduce_to_unique_ids()</code></a>
</p>
</li>
<li> <p><a href="#method-LargeDataSetBase-select"><code>LargeDataSetBase$select()</code></a>
</p>
</li>
<li> <p><a href="#method-LargeDataSetBase-get_ids"><code>LargeDataSetBase$get_ids()</code></a>
</p>
</li>
<li> <p><a href="#method-LargeDataSetBase-save"><code>LargeDataSetBase$save()</code></a>
</p>
</li>
<li> <p><a href="#method-LargeDataSetBase-load_from_disk"><code>LargeDataSetBase$load_from_disk()</code></a>
</p>
</li>
<li> <p><a href="#method-LargeDataSetBase-load"><code>LargeDataSetBase$load()</code></a>
</p>
</li>
<li> <p><a href="#method-LargeDataSetBase-get_all_fields"><code>LargeDataSetBase$get_all_fields()</code></a>
</p>
</li>
<li> <p><a href="#method-LargeDataSetBase-clone"><code>LargeDataSetBase$clone()</code></a>
</p>
</li></ul>


<hr>
<a id="method-LargeDataSetBase-n_cols"></a>



<h4>Method <code>n_cols()</code></h4>

<p>Number of columns in the data set.
</p>


<h5>Usage</h5>

<div class="r"><pre>LargeDataSetBase$n_cols()</pre></div>



<h5>Returns</h5>

<p><code>int</code> describing the number of columns in the data set.
</p>


<hr>
<a id="method-LargeDataSetBase-n_rows"></a>



<h4>Method <code>n_rows()</code></h4>

<p>Number of rows in the data set.
</p>


<h5>Usage</h5>

<div class="r"><pre>LargeDataSetBase$n_rows()</pre></div>



<h5>Returns</h5>

<p><code>int</code> describing the number of rows in the data set.
</p>


<hr>
<a id="method-LargeDataSetBase-get_colnames"></a>



<h4>Method <code>get_colnames()</code></h4>

<p>Get names of the columns in the data set.
</p>


<h5>Usage</h5>

<div class="r"><pre>LargeDataSetBase$get_colnames()</pre></div>



<h5>Returns</h5>

<p><code>vector</code> containing the names of the columns as <code>string</code>s.
</p>


<hr>
<a id="method-LargeDataSetBase-get_dataset"></a>



<h4>Method <code>get_dataset()</code></h4>

<p>Get data set.
</p>


<h5>Usage</h5>

<div class="r"><pre>LargeDataSetBase$get_dataset()</pre></div>



<h5>Returns</h5>

<p>Returns the data set of this object as an object of class <code>datasets.arrow_dataset.Dataset</code>.
</p>


<hr>
<a id="method-LargeDataSetBase-reduce_to_unique_ids"></a>



<h4>Method <code>reduce_to_unique_ids()</code></h4>

<p>Reduces the data set to a data set containing only unique ids. In the case an id exists multiple
times in the data set the first case remains in the data set. The other cases are dropped.
</p>
<p><strong>Attention</strong> Calling this method will change the data set in place.
</p>


<h5>Usage</h5>

<div class="r"><pre>LargeDataSetBase$reduce_to_unique_ids()</pre></div>



<h5>Returns</h5>

<p>Method does not return anything. It changes the data set of this object in place.
</p>


<hr>
<a id="method-LargeDataSetBase-select"></a>



<h4>Method <code>select()</code></h4>

<p>Returns a data set which contains only the cases belonging to the specific indices.
</p>


<h5>Usage</h5>

<div class="r"><pre>LargeDataSetBase$select(indicies)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>indicies</code></dt><dd><p><code>vector</code> of <code>int</code> for selecting rows in the data set. <strong>Attention</strong> The indices are zero-based.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>Returns a data set of class <code>datasets.arrow_dataset.Dataset</code> with the selected rows.
</p>


<hr>
<a id="method-LargeDataSetBase-get_ids"></a>



<h4>Method <code>get_ids()</code></h4>

<p>Get ids
</p>


<h5>Usage</h5>

<div class="r"><pre>LargeDataSetBase$get_ids()</pre></div>



<h5>Returns</h5>

<p>Returns a <code>vector</code> containing the ids of every row as <code>string</code>s.
</p>


<hr>
<a id="method-LargeDataSetBase-save"></a>



<h4>Method <code>save()</code></h4>

<p>Saves a data set to disk.
</p>


<h5>Usage</h5>

<div class="r"><pre>LargeDataSetBase$save(dir_path, folder_name, create_dir = TRUE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>dir_path</code></dt><dd><p>Path where to store the data set.</p>
</dd>
<dt><code>folder_name</code></dt><dd><p><code>string</code> Name of the folder for storing the data set.</p>
</dd>
<dt><code>create_dir</code></dt><dd><p><code>bool</code> If <code>True</code> the directory will be created if it does not exist.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>Method does not return anything. It write the data set to disk.
</p>


<hr>
<a id="method-LargeDataSetBase-load_from_disk"></a>



<h4>Method <code>load_from_disk()</code></h4>

<p>loads an object of class <a href="#topic+LargeDataSetBase">LargeDataSetBase</a> from disk 'and updates the object to the current version
of the package.
</p>


<h5>Usage</h5>

<div class="r"><pre>LargeDataSetBase$load_from_disk(dir_path)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>dir_path</code></dt><dd><p>Path where the data set set is stored.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>Method does not return anything. It loads an object from disk.
</p>


<hr>
<a id="method-LargeDataSetBase-load"></a>



<h4>Method <code>load()</code></h4>

<p>Loads a data set from disk.
</p>


<h5>Usage</h5>

<div class="r"><pre>LargeDataSetBase$load(dir_path)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>dir_path</code></dt><dd><p>Path where the data set is stored.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>Method does not return anything. It loads a data set from disk.
</p>


<hr>
<a id="method-LargeDataSetBase-get_all_fields"></a>



<h4>Method <code>get_all_fields()</code></h4>

<p>Return all fields.
</p>


<h5>Usage</h5>

<div class="r"><pre>LargeDataSetBase$get_all_fields()</pre></div>



<h5>Returns</h5>

<p>Method returns a <code>list</code> containing all public and private fields of the object.
</p>


<hr>
<a id="method-LargeDataSetBase-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>LargeDataSetBase$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>



<hr>
<h2 id='LargeDataSetForText'>Abstract class for large data sets containing raw texts</h2><span id='topic+LargeDataSetForText'></span>

<h3>Description</h3>

<p>This object stores raw texts. The data of this objects is not stored in memory directly. By using memory
mapping these objects allow to work with data sets which do not fit into memory/RAM.
</p>


<h3>Value</h3>

<p>Returns a new object of this class.
</p>


<h3>Super class</h3>

<p><code><a href="#topic+LargeDataSetBase">aifeducation::LargeDataSetBase</a></code> -&gt; <code>LargeDataSetForText</code>
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-LargeDataSetForText-new"><code>LargeDataSetForText$new()</code></a>
</p>
</li>
<li> <p><a href="#method-LargeDataSetForText-add_from_files_txt"><code>LargeDataSetForText$add_from_files_txt()</code></a>
</p>
</li>
<li> <p><a href="#method-LargeDataSetForText-add_from_files_pdf"><code>LargeDataSetForText$add_from_files_pdf()</code></a>
</p>
</li>
<li> <p><a href="#method-LargeDataSetForText-add_from_files_xlsx"><code>LargeDataSetForText$add_from_files_xlsx()</code></a>
</p>
</li>
<li> <p><a href="#method-LargeDataSetForText-add_from_data.frame"><code>LargeDataSetForText$add_from_data.frame()</code></a>
</p>
</li>
<li> <p><a href="#method-LargeDataSetForText-get_private"><code>LargeDataSetForText$get_private()</code></a>
</p>
</li>
<li> <p><a href="#method-LargeDataSetForText-clone"><code>LargeDataSetForText$clone()</code></a>
</p>
</li></ul>



<details><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="LargeDataSetBase" data-id="get_all_fields"><a href='../../aifeducation/html/LargeDataSetBase.html#method-LargeDataSetBase-get_all_fields'><code>aifeducation::LargeDataSetBase$get_all_fields()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="LargeDataSetBase" data-id="get_colnames"><a href='../../aifeducation/html/LargeDataSetBase.html#method-LargeDataSetBase-get_colnames'><code>aifeducation::LargeDataSetBase$get_colnames()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="LargeDataSetBase" data-id="get_dataset"><a href='../../aifeducation/html/LargeDataSetBase.html#method-LargeDataSetBase-get_dataset'><code>aifeducation::LargeDataSetBase$get_dataset()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="LargeDataSetBase" data-id="get_ids"><a href='../../aifeducation/html/LargeDataSetBase.html#method-LargeDataSetBase-get_ids'><code>aifeducation::LargeDataSetBase$get_ids()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="LargeDataSetBase" data-id="load"><a href='../../aifeducation/html/LargeDataSetBase.html#method-LargeDataSetBase-load'><code>aifeducation::LargeDataSetBase$load()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="LargeDataSetBase" data-id="load_from_disk"><a href='../../aifeducation/html/LargeDataSetBase.html#method-LargeDataSetBase-load_from_disk'><code>aifeducation::LargeDataSetBase$load_from_disk()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="LargeDataSetBase" data-id="n_cols"><a href='../../aifeducation/html/LargeDataSetBase.html#method-LargeDataSetBase-n_cols'><code>aifeducation::LargeDataSetBase$n_cols()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="LargeDataSetBase" data-id="n_rows"><a href='../../aifeducation/html/LargeDataSetBase.html#method-LargeDataSetBase-n_rows'><code>aifeducation::LargeDataSetBase$n_rows()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="LargeDataSetBase" data-id="reduce_to_unique_ids"><a href='../../aifeducation/html/LargeDataSetBase.html#method-LargeDataSetBase-reduce_to_unique_ids'><code>aifeducation::LargeDataSetBase$reduce_to_unique_ids()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="LargeDataSetBase" data-id="save"><a href='../../aifeducation/html/LargeDataSetBase.html#method-LargeDataSetBase-save'><code>aifeducation::LargeDataSetBase$save()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="LargeDataSetBase" data-id="select"><a href='../../aifeducation/html/LargeDataSetBase.html#method-LargeDataSetBase-select'><code>aifeducation::LargeDataSetBase$select()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-LargeDataSetForText-new"></a>



<h4>Method <code>new()</code></h4>

<p>Method for creation of <a href="#topic+LargeDataSetForText">LargeDataSetForText</a> instance. It can be initialized with <code>init_data</code>
parameter if passed (Uses <code>add_from_data.frame()</code> method if <code>init_data</code> is <code>data.frame</code>).
</p>


<h5>Usage</h5>

<div class="r"><pre>LargeDataSetForText$new(init_data = NULL)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>init_data</code></dt><dd><p>Initial <code>data.frame</code> for dataset.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>A new instance of this class initialized with <code>init_data</code> if passed.
</p>


<hr>
<a id="method-LargeDataSetForText-add_from_files_txt"></a>



<h4>Method <code>add_from_files_txt()</code></h4>

<p>Method for adding raw texts saved within .txt files to the data set. Please note the the directory
should contain one folder for each .txt file. In order to create an informative data set every folder can
contain the following additional files:
</p>

<ul>
<li><p> bib_entry.txt: containing a text version of the bibliographic information of the raw text.
</p>
</li>
<li><p> license.txt: containing a statement about the license to use the raw text such as &quot;CC BY&quot;.
</p>
</li>
<li><p> url_license.txt: containing the url/link to the license in the internet.
</p>
</li>
<li><p> text_license.txt: containing the license in raw text.
</p>
</li>
<li><p> url_source.txt: containing the url/link to the source in the internet.
</p>
<p>The id of every .txt file is the file name without file extension. Please be aware to provide unique file
names. Id and raw texts are mandatory, bibliographic and license information are optional.
</p>
</li></ul>



<h5>Usage</h5>

<div class="r"><pre>LargeDataSetForText$add_from_files_txt(
  dir_path,
  batch_size = 500,
  log_file = NULL,
  log_write_interval = 2,
  log_top_value = 0,
  log_top_total = 1,
  log_top_message = NA,
  trace = TRUE
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>dir_path</code></dt><dd><p>Path to the directory where the files are stored.</p>
</dd>
<dt><code>batch_size</code></dt><dd><p><code>int</code> determining the number of files to process at once.</p>
</dd>
<dt><code>log_file</code></dt><dd><p><code>string</code> Path to the file where the log should be saved. If no logging is desired set this
argument to <code>NULL</code>.</p>
</dd>
<dt><code>log_write_interval</code></dt><dd><p><code>int</code> Time in seconds determining the interval in which the logger should try to update
the log files. Only relevant if <code>log_file</code> is not <code>NULL</code>.</p>
</dd>
<dt><code>log_top_value</code></dt><dd><p><code>int</code> indicating the current iteration of the process.</p>
</dd>
<dt><code>log_top_total</code></dt><dd><p><code>int</code> determining the maximal number of iterations.</p>
</dd>
<dt><code>log_top_message</code></dt><dd><p><code>string</code> providing additional information of the process.</p>
</dd>
<dt><code>trace</code></dt><dd><p><code>bool</code> If <code>TRUE</code> information on the progress is printed to the console.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>The method does not return anything. It adds new raw texts to the data set.
</p>


<hr>
<a id="method-LargeDataSetForText-add_from_files_pdf"></a>



<h4>Method <code>add_from_files_pdf()</code></h4>

<p>Method for adding raw texts saved within .pdf files to the data set. Please note the the directory
should contain one folder for each .pdf file. In order to create an informative data set every folder can
contain the following additional files:
</p>

<ul>
<li><p> bib_entry.txt: containing a text version of the bibliographic information
of the raw text.
</p>
</li>
<li><p> license.txt: containing a statement about the license to use the raw text
such as &quot;CC BY&quot;.
</p>
</li>
<li><p> url_license.txt: containing the url/link to the license in the internet.
</p>
</li>
<li><p> text_license.txt: containing the license in raw text.
</p>
</li>
<li><p> url_source.txt: containing the url/link to the source in the internet.
</p>
<p>The id of every .pdf file is the file name without file extension. Please be aware to provide unique file
names. Id and raw texts are mandatory, bibliographic and license information are optional.
</p>
</li></ul>



<h5>Usage</h5>

<div class="r"><pre>LargeDataSetForText$add_from_files_pdf(
  dir_path,
  batch_size = 500,
  log_file = NULL,
  log_write_interval = 2,
  log_top_value = 0,
  log_top_total = 1,
  log_top_message = NA,
  trace = TRUE
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>dir_path</code></dt><dd><p>Path to the directory where the files are stored.</p>
</dd>
<dt><code>batch_size</code></dt><dd><p><code>int</code> determining the number of files to process at once.</p>
</dd>
<dt><code>log_file</code></dt><dd><p><code>string</code> Path to the file where the log should be saved. If no logging is desired set this
argument to <code>NULL</code>.</p>
</dd>
<dt><code>log_write_interval</code></dt><dd><p><code>int</code> Time in seconds determining the interval in which the logger should try to update
the log files. Only relevant if <code>log_file</code> is not <code>NULL</code>.</p>
</dd>
<dt><code>log_top_value</code></dt><dd><p><code>int</code> indicating the current iteration of the process.</p>
</dd>
<dt><code>log_top_total</code></dt><dd><p><code>int</code> determining the maximal number of iterations.</p>
</dd>
<dt><code>log_top_message</code></dt><dd><p><code>string</code> providing additional information of the process.</p>
</dd>
<dt><code>trace</code></dt><dd><p><code>bool</code> If <code>TRUE</code> information on the progress is printed to the console.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>The method does not return anything. It adds new raw texts to the data set.
</p>


<hr>
<a id="method-LargeDataSetForText-add_from_files_xlsx"></a>



<h4>Method <code>add_from_files_xlsx()</code></h4>

<p>Method for adding raw texts saved within .xlsx files to the data set. The method assumes that the
texts are saved in the rows and that the columns store the id and the raw texts in the columns. In addition, a
column for the bibliography information and the license can be added. The column names for these rows must be
specified with the following arguments. They must be the same for all .xlsx files in the chosen directory. Id
and raw texts are mandatory, bibliographic, license, license's url, license's text, and source's url are
optional. Additional columns are dropped.
</p>


<h5>Usage</h5>

<div class="r"><pre>LargeDataSetForText$add_from_files_xlsx(
  dir_path,
  trace = TRUE,
  id_column = "id",
  text_column = "text",
  bib_entry_column = "bib_entry",
  license_column = "license",
  url_license_column = "url_license",
  text_license_column = "text_license",
  url_source_column = "url_source",
  log_file = NULL,
  log_write_interval = 2,
  log_top_value = 0,
  log_top_total = 1,
  log_top_message = NA
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>dir_path</code></dt><dd><p>Path to the directory where the files are stored.</p>
</dd>
<dt><code>trace</code></dt><dd><p><code>bool</code> If <code>TRUE</code> prints information on the progress to the console.</p>
</dd>
<dt><code>id_column</code></dt><dd><p><code>string</code> Name of the column storing the ids for the texts.</p>
</dd>
<dt><code>text_column</code></dt><dd><p><code>string</code> Name of the column storing the raw text.</p>
</dd>
<dt><code>bib_entry_column</code></dt><dd><p><code>string</code> Name of the column storing the bibliographic information of the texts.</p>
</dd>
<dt><code>license_column</code></dt><dd><p><code>string</code> Name of the column storing information about the licenses.</p>
</dd>
<dt><code>url_license_column</code></dt><dd><p><code>string</code> Name of the column storing information about the url to the license in the
internet.</p>
</dd>
<dt><code>text_license_column</code></dt><dd><p><code>string</code> Name of the column storing the license as text.</p>
</dd>
<dt><code>url_source_column</code></dt><dd><p><code>string</code> Name of the column storing information about about the url to the source in the
internet.</p>
</dd>
<dt><code>log_file</code></dt><dd><p><code>string</code> Path to the file where the log should be saved. If no logging is desired set this
argument to <code>NULL</code>.</p>
</dd>
<dt><code>log_write_interval</code></dt><dd><p><code>int</code> Time in seconds determining the interval in which the logger should try to update
the log files. Only relevant if <code>log_file</code> is not <code>NULL</code>.</p>
</dd>
<dt><code>log_top_value</code></dt><dd><p><code>int</code> indicating the current iteration of the process.</p>
</dd>
<dt><code>log_top_total</code></dt><dd><p><code>int</code> determining the maximal number of iterations.</p>
</dd>
<dt><code>log_top_message</code></dt><dd><p><code>string</code> providing additional information of the process.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>The method does not return anything. It adds new raw texts to the data set.
</p>


<hr>
<a id="method-LargeDataSetForText-add_from_data.frame"></a>



<h4>Method <code>add_from_data.frame()</code></h4>

<p>Method for adding raw texts from a <code>data.frame</code>
</p>


<h5>Usage</h5>

<div class="r"><pre>LargeDataSetForText$add_from_data.frame(data_frame)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>data_frame</code></dt><dd><p>Object of class <code>data.frame</code> with at least the following columns &quot;id&quot;,&quot;text&quot;,&quot;bib_entry&quot;,
&quot;license&quot;, &quot;url_license&quot;, &quot;text_license&quot;, and &quot;url_source&quot;. If &quot;id&quot; and7or &quot;text&quot; is missing an error occurs.
If the other columns are not present in the <code>data.frame</code> they are added with empty values(<code>NA</code>).
Additional columns are dropped.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>The method does not return anything. It adds new raw texts to the data set.
</p>


<hr>
<a id="method-LargeDataSetForText-get_private"></a>



<h4>Method <code>get_private()</code></h4>

<p>Method for requesting all private fields and methods. Used for loading and updating an object.
</p>


<h5>Usage</h5>

<div class="r"><pre>LargeDataSetForText$get_private()</pre></div>



<h5>Returns</h5>

<p>Returns a <code>list</code> with all private fields and methods.
</p>


<hr>
<a id="method-LargeDataSetForText-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>LargeDataSetForText$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>See Also</h3>

<p>Other Data Management: 
<code><a href="#topic+DataManagerClassifier">DataManagerClassifier</a></code>,
<code><a href="#topic+EmbeddedText">EmbeddedText</a></code>,
<code><a href="#topic+LargeDataSetForTextEmbeddings">LargeDataSetForTextEmbeddings</a></code>
</p>

<hr>
<h2 id='LargeDataSetForTextEmbeddings'>Abstract class for large data sets containing text embeddings</h2><span id='topic+LargeDataSetForTextEmbeddings'></span>

<h3>Description</h3>

<p>This object stores text embeddings which are usually produced by an object of class
<a href="#topic+TextEmbeddingModel">TextEmbeddingModel</a>. The data of this objects is not stored in memory directly. By using memory mapping these
objects allow to work with data sets which do not fit into memory/RAM.
</p>
<p><a href="#topic+LargeDataSetForTextEmbeddings">LargeDataSetForTextEmbeddings</a> are used for storing and managing the text embeddings created with objects of class
<a href="#topic+TextEmbeddingModel">TextEmbeddingModel</a>. Objects of class <a href="#topic+LargeDataSetForTextEmbeddings">LargeDataSetForTextEmbeddings</a> serve as input for objects of class
<a href="#topic+TEClassifierRegular">TEClassifierRegular</a>, <a href="#topic+TEClassifierProtoNet">TEClassifierProtoNet</a>, and <a href="#topic+TEFeatureExtractor">TEFeatureExtractor</a>. The main aim of this class is to provide a
structured link between embedding models and classifiers. Since objects of this class save information on the text
embedding model that created the text embedding it ensures that only embedding generated with same embedding model
are combined. Furthermore, the stored information allows objects to check if embeddings of the correct text
embedding model are used for training and predicting.
</p>


<h3>Value</h3>

<p>Returns a new object of this class.
</p>


<h3>Super class</h3>

<p><code><a href="#topic+LargeDataSetBase">aifeducation::LargeDataSetBase</a></code> -&gt; <code>LargeDataSetForTextEmbeddings</code>
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-LargeDataSetForTextEmbeddings-configure"><code>LargeDataSetForTextEmbeddings$configure()</code></a>
</p>
</li>
<li> <p><a href="#method-LargeDataSetForTextEmbeddings-is_configured"><code>LargeDataSetForTextEmbeddings$is_configured()</code></a>
</p>
</li>
<li> <p><a href="#method-LargeDataSetForTextEmbeddings-get_text_embedding_model_name"><code>LargeDataSetForTextEmbeddings$get_text_embedding_model_name()</code></a>
</p>
</li>
<li> <p><a href="#method-LargeDataSetForTextEmbeddings-get_model_info"><code>LargeDataSetForTextEmbeddings$get_model_info()</code></a>
</p>
</li>
<li> <p><a href="#method-LargeDataSetForTextEmbeddings-load_from_disk"><code>LargeDataSetForTextEmbeddings$load_from_disk()</code></a>
</p>
</li>
<li> <p><a href="#method-LargeDataSetForTextEmbeddings-get_model_label"><code>LargeDataSetForTextEmbeddings$get_model_label()</code></a>
</p>
</li>
<li> <p><a href="#method-LargeDataSetForTextEmbeddings-add_feature_extractor_info"><code>LargeDataSetForTextEmbeddings$add_feature_extractor_info()</code></a>
</p>
</li>
<li> <p><a href="#method-LargeDataSetForTextEmbeddings-get_feature_extractor_info"><code>LargeDataSetForTextEmbeddings$get_feature_extractor_info()</code></a>
</p>
</li>
<li> <p><a href="#method-LargeDataSetForTextEmbeddings-is_compressed"><code>LargeDataSetForTextEmbeddings$is_compressed()</code></a>
</p>
</li>
<li> <p><a href="#method-LargeDataSetForTextEmbeddings-get_times"><code>LargeDataSetForTextEmbeddings$get_times()</code></a>
</p>
</li>
<li> <p><a href="#method-LargeDataSetForTextEmbeddings-get_features"><code>LargeDataSetForTextEmbeddings$get_features()</code></a>
</p>
</li>
<li> <p><a href="#method-LargeDataSetForTextEmbeddings-get_original_features"><code>LargeDataSetForTextEmbeddings$get_original_features()</code></a>
</p>
</li>
<li> <p><a href="#method-LargeDataSetForTextEmbeddings-add_embeddings_from_array"><code>LargeDataSetForTextEmbeddings$add_embeddings_from_array()</code></a>
</p>
</li>
<li> <p><a href="#method-LargeDataSetForTextEmbeddings-add_embeddings_from_EmbeddedText"><code>LargeDataSetForTextEmbeddings$add_embeddings_from_EmbeddedText()</code></a>
</p>
</li>
<li> <p><a href="#method-LargeDataSetForTextEmbeddings-add_embeddings_from_LargeDataSetForTextEmbeddings"><code>LargeDataSetForTextEmbeddings$add_embeddings_from_LargeDataSetForTextEmbeddings()</code></a>
</p>
</li>
<li> <p><a href="#method-LargeDataSetForTextEmbeddings-convert_to_EmbeddedText"><code>LargeDataSetForTextEmbeddings$convert_to_EmbeddedText()</code></a>
</p>
</li>
<li> <p><a href="#method-LargeDataSetForTextEmbeddings-clone"><code>LargeDataSetForTextEmbeddings$clone()</code></a>
</p>
</li></ul>



<details><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="LargeDataSetBase" data-id="get_all_fields"><a href='../../aifeducation/html/LargeDataSetBase.html#method-LargeDataSetBase-get_all_fields'><code>aifeducation::LargeDataSetBase$get_all_fields()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="LargeDataSetBase" data-id="get_colnames"><a href='../../aifeducation/html/LargeDataSetBase.html#method-LargeDataSetBase-get_colnames'><code>aifeducation::LargeDataSetBase$get_colnames()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="LargeDataSetBase" data-id="get_dataset"><a href='../../aifeducation/html/LargeDataSetBase.html#method-LargeDataSetBase-get_dataset'><code>aifeducation::LargeDataSetBase$get_dataset()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="LargeDataSetBase" data-id="get_ids"><a href='../../aifeducation/html/LargeDataSetBase.html#method-LargeDataSetBase-get_ids'><code>aifeducation::LargeDataSetBase$get_ids()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="LargeDataSetBase" data-id="load"><a href='../../aifeducation/html/LargeDataSetBase.html#method-LargeDataSetBase-load'><code>aifeducation::LargeDataSetBase$load()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="LargeDataSetBase" data-id="n_cols"><a href='../../aifeducation/html/LargeDataSetBase.html#method-LargeDataSetBase-n_cols'><code>aifeducation::LargeDataSetBase$n_cols()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="LargeDataSetBase" data-id="n_rows"><a href='../../aifeducation/html/LargeDataSetBase.html#method-LargeDataSetBase-n_rows'><code>aifeducation::LargeDataSetBase$n_rows()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="LargeDataSetBase" data-id="reduce_to_unique_ids"><a href='../../aifeducation/html/LargeDataSetBase.html#method-LargeDataSetBase-reduce_to_unique_ids'><code>aifeducation::LargeDataSetBase$reduce_to_unique_ids()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="LargeDataSetBase" data-id="save"><a href='../../aifeducation/html/LargeDataSetBase.html#method-LargeDataSetBase-save'><code>aifeducation::LargeDataSetBase$save()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="LargeDataSetBase" data-id="select"><a href='../../aifeducation/html/LargeDataSetBase.html#method-LargeDataSetBase-select'><code>aifeducation::LargeDataSetBase$select()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-LargeDataSetForTextEmbeddings-configure"></a>



<h4>Method <code>configure()</code></h4>

<p>Creates a new object representing text embeddings.
</p>


<h5>Usage</h5>

<div class="r"><pre>LargeDataSetForTextEmbeddings$configure(
  model_name = NA,
  model_label = NA,
  model_date = NA,
  model_method = NA,
  model_version = NA,
  model_language = NA,
  param_seq_length = NA,
  param_chunks = NULL,
  param_features = NULL,
  param_overlap = NULL,
  param_emb_layer_min = NULL,
  param_emb_layer_max = NULL,
  param_emb_pool_type = NULL,
  param_aggregation = NULL
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>model_name</code></dt><dd><p><code>string</code> Name of the model that generates this embedding.</p>
</dd>
<dt><code>model_label</code></dt><dd><p><code>string</code> Label of the model that generates this embedding.</p>
</dd>
<dt><code>model_date</code></dt><dd><p><code>string</code> Date when the embedding generating model was created.</p>
</dd>
<dt><code>model_method</code></dt><dd><p><code>string</code> Method of the underlying embedding model.</p>
</dd>
<dt><code>model_version</code></dt><dd><p><code>string</code> Version of the model that generated this embedding.</p>
</dd>
<dt><code>model_language</code></dt><dd><p><code>string</code> Language of the model that generated this embedding.</p>
</dd>
<dt><code>param_seq_length</code></dt><dd><p><code>int</code> Maximum number of tokens that processes the generating model for a chunk.</p>
</dd>
<dt><code>param_chunks</code></dt><dd><p><code>int</code> Maximum number of chunks which are supported by the generating model.</p>
</dd>
<dt><code>param_features</code></dt><dd><p><code>int</code> Number of dimensions of the text embeddings.</p>
</dd>
<dt><code>param_overlap</code></dt><dd><p><code>int</code> Number of tokens that were added at the beginning of the sequence for the next chunk
by this model.</p>
</dd>
<dt><code>param_emb_layer_min</code></dt><dd><p><code>int</code> or <code>string</code> determining the first layer to be included in the creation of
embeddings.</p>
</dd>
<dt><code>param_emb_layer_max</code></dt><dd><p><code>int</code> or <code>string</code> determining the last layer to be included in the creation of
embeddings.</p>
</dd>
<dt><code>param_emb_pool_type</code></dt><dd><p><code>string</code> determining the method for pooling the token embeddings within each layer.</p>
</dd>
<dt><code>param_aggregation</code></dt><dd><p><code>string</code> Aggregation method of the hidden states. Deprecated. Only included for backward
compatibility.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>The method returns a new object of this class.
</p>


<hr>
<a id="method-LargeDataSetForTextEmbeddings-is_configured"></a>



<h4>Method <code>is_configured()</code></h4>

<p>Method for checking if the model was successfully configured. An object can only be used if this
value is <code>TRUE</code>.
</p>


<h5>Usage</h5>

<div class="r"><pre>LargeDataSetForTextEmbeddings$is_configured()</pre></div>



<h5>Returns</h5>

<p><code>bool</code> <code>TRUE</code> if the model is fully configured. <code>FALSE</code> if not.
</p>


<hr>
<a id="method-LargeDataSetForTextEmbeddings-get_text_embedding_model_name"></a>



<h4>Method <code>get_text_embedding_model_name()</code></h4>

<p>Method for requesting the name (unique id) of the underlying text embedding model.
</p>


<h5>Usage</h5>

<div class="r"><pre>LargeDataSetForTextEmbeddings$get_text_embedding_model_name()</pre></div>



<h5>Returns</h5>

<p>Returns a <code>string</code> describing name of the text embedding model.
</p>


<hr>
<a id="method-LargeDataSetForTextEmbeddings-get_model_info"></a>



<h4>Method <code>get_model_info()</code></h4>

<p>Method for retrieving information about the model that generated this embedding.
</p>


<h5>Usage</h5>

<div class="r"><pre>LargeDataSetForTextEmbeddings$get_model_info()</pre></div>



<h5>Returns</h5>

<p><code>list</code> containing all saved information about the underlying text embedding model.
</p>


<hr>
<a id="method-LargeDataSetForTextEmbeddings-load_from_disk"></a>



<h4>Method <code>load_from_disk()</code></h4>

<p>loads an object of class <a href="#topic+LargeDataSetForTextEmbeddings">LargeDataSetForTextEmbeddings</a> from disk and updates the object to the
current version of the package.
</p>


<h5>Usage</h5>

<div class="r"><pre>LargeDataSetForTextEmbeddings$load_from_disk(dir_path)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>dir_path</code></dt><dd><p>Path where the data set set is stored.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>Method does not return anything. It loads an object from disk.
</p>


<hr>
<a id="method-LargeDataSetForTextEmbeddings-get_model_label"></a>



<h4>Method <code>get_model_label()</code></h4>

<p>Method for retrieving the label of the model that generated this embedding.
</p>


<h5>Usage</h5>

<div class="r"><pre>LargeDataSetForTextEmbeddings$get_model_label()</pre></div>



<h5>Returns</h5>

<p><code>string</code> Label of the corresponding text embedding model
</p>


<hr>
<a id="method-LargeDataSetForTextEmbeddings-add_feature_extractor_info"></a>



<h4>Method <code>add_feature_extractor_info()</code></h4>

<p>Method setting information on the <a href="#topic+TEFeatureExtractor">TEFeatureExtractor</a> that was used to reduce the number of
dimensions of the text embeddings. This information should only be used if a <a href="#topic+TEFeatureExtractor">TEFeatureExtractor</a> was applied.
</p>


<h5>Usage</h5>

<div class="r"><pre>LargeDataSetForTextEmbeddings$add_feature_extractor_info(
  model_name,
  model_label = NA,
  features = NA,
  method = NA,
  noise_factor = NA,
  optimizer = NA
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>model_name</code></dt><dd><p><code>string</code> Name of the underlying <a href="#topic+TextEmbeddingModel">TextEmbeddingModel</a>.</p>
</dd>
<dt><code>model_label</code></dt><dd><p><code>string</code> Label of the underlying <a href="#topic+TextEmbeddingModel">TextEmbeddingModel</a>.</p>
</dd>
<dt><code>features</code></dt><dd><p><code>int</code> Number of dimension (features) for the <strong>compressed</strong> text embeddings.</p>
</dd>
<dt><code>method</code></dt><dd><p><code>string</code> Method that the <a href="#topic+TEFeatureExtractor">TEFeatureExtractor</a> applies for genereating the compressed text
embeddings.</p>
</dd>
<dt><code>noise_factor</code></dt><dd><p><code>double</code> Noise factor of the <a href="#topic+TEFeatureExtractor">TEFeatureExtractor</a>.</p>
</dd>
<dt><code>optimizer</code></dt><dd><p><code>string</code> Optimizer used during training the <a href="#topic+TEFeatureExtractor">TEFeatureExtractor</a>.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>Method does nothing return. It sets information on a <a href="#topic+TEFeatureExtractor">TEFeatureExtractor</a>.
</p>


<hr>
<a id="method-LargeDataSetForTextEmbeddings-get_feature_extractor_info"></a>



<h4>Method <code>get_feature_extractor_info()</code></h4>

<p>Method for receiving information on the <a href="#topic+TEFeatureExtractor">TEFeatureExtractor</a> that was used to reduce the number of
dimensions of the text embeddings.
</p>


<h5>Usage</h5>

<div class="r"><pre>LargeDataSetForTextEmbeddings$get_feature_extractor_info()</pre></div>



<h5>Returns</h5>

<p>Returns a <code>list</code> with information on the <a href="#topic+TEFeatureExtractor">TEFeatureExtractor</a>. If no <a href="#topic+TEFeatureExtractor">TEFeatureExtractor</a> was used it
returns <code>NULL</code>.
</p>


<hr>
<a id="method-LargeDataSetForTextEmbeddings-is_compressed"></a>



<h4>Method <code>is_compressed()</code></h4>

<p>Checks if the text embedding were reduced by a <a href="#topic+TEFeatureExtractor">TEFeatureExtractor</a>.
</p>


<h5>Usage</h5>

<div class="r"><pre>LargeDataSetForTextEmbeddings$is_compressed()</pre></div>



<h5>Returns</h5>

<p>Returns <code>TRUE</code> if the number of dimensions was reduced by a <a href="#topic+TEFeatureExtractor">TEFeatureExtractor</a>. If not return <code>FALSE</code>.
</p>


<hr>
<a id="method-LargeDataSetForTextEmbeddings-get_times"></a>



<h4>Method <code>get_times()</code></h4>

<p>Number of chunks/times of the text embeddings.
</p>


<h5>Usage</h5>

<div class="r"><pre>LargeDataSetForTextEmbeddings$get_times()</pre></div>



<h5>Returns</h5>

<p>Returns an <code>int</code> describing the number of chunks/times of the text embeddings.
</p>


<hr>
<a id="method-LargeDataSetForTextEmbeddings-get_features"></a>



<h4>Method <code>get_features()</code></h4>

<p>Number of actual features/dimensions of the text embeddings.In the case a <a href="#topic+TEFeatureExtractor">TEFeatureExtractor</a> was
used the number of features is smaller as the original number of features. To receive the original number of
features (the number of features before applying a <a href="#topic+TEFeatureExtractor">TEFeatureExtractor</a>) you can use the method
<code>get_original_features</code> of this class.
</p>


<h5>Usage</h5>

<div class="r"><pre>LargeDataSetForTextEmbeddings$get_features()</pre></div>



<h5>Returns</h5>

<p>Returns an <code>int</code> describing the number of features/dimensions of the text embeddings.
</p>


<hr>
<a id="method-LargeDataSetForTextEmbeddings-get_original_features"></a>



<h4>Method <code>get_original_features()</code></h4>

<p>Number of original features/dimensions of the text embeddings.
</p>


<h5>Usage</h5>

<div class="r"><pre>LargeDataSetForTextEmbeddings$get_original_features()</pre></div>



<h5>Returns</h5>

<p>Returns an <code>int</code> describing the number of features/dimensions if no <a href="#topic+TEFeatureExtractor">TEFeatureExtractor</a>) is used or
before a <a href="#topic+TEFeatureExtractor">TEFeatureExtractor</a>) is applied.
</p>


<hr>
<a id="method-LargeDataSetForTextEmbeddings-add_embeddings_from_array"></a>



<h4>Method <code>add_embeddings_from_array()</code></h4>

<p>Method for adding new data to the data set from an <code>array</code>. Please note that the method does not
check if cases already exist in the data set. To reduce the data set to unique cases call the method
<code>reduce_to_unique_ids</code>.
</p>


<h5>Usage</h5>

<div class="r"><pre>LargeDataSetForTextEmbeddings$add_embeddings_from_array(embedding_array)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>embedding_array</code></dt><dd><p><code>array</code> containing the text embeddings.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>The method does not return anything. It adds new data to the data set.
</p>


<hr>
<a id="method-LargeDataSetForTextEmbeddings-add_embeddings_from_EmbeddedText"></a>



<h4>Method <code>add_embeddings_from_EmbeddedText()</code></h4>

<p>Method for adding new data to the data set from an <a href="#topic+EmbeddedText">EmbeddedText</a>. Please note that the method does
not check if cases already exist in the data set. To reduce the data set to unique cases call the method
<code>reduce_to_unique_ids</code>.
</p>


<h5>Usage</h5>

<div class="r"><pre>LargeDataSetForTextEmbeddings$add_embeddings_from_EmbeddedText(EmbeddedText)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>EmbeddedText</code></dt><dd><p>Object of class <a href="#topic+EmbeddedText">EmbeddedText</a>.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>The method does not return anything. It adds new data to the data set.
</p>


<hr>
<a id="method-LargeDataSetForTextEmbeddings-add_embeddings_from_LargeDataSetForTextEmbeddings"></a>



<h4>Method <code>add_embeddings_from_LargeDataSetForTextEmbeddings()</code></h4>

<p>Method for adding new data to the data set from an <a href="#topic+LargeDataSetForTextEmbeddings">LargeDataSetForTextEmbeddings</a>. Please note that
the method does not check if cases already exist in the data set. To reduce the data set to unique cases call
the method <code>reduce_to_unique_ids</code>.
</p>


<h5>Usage</h5>

<div class="r"><pre>LargeDataSetForTextEmbeddings$add_embeddings_from_LargeDataSetForTextEmbeddings(
  dataset
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>dataset</code></dt><dd><p>Object of class <a href="#topic+LargeDataSetForTextEmbeddings">LargeDataSetForTextEmbeddings</a>.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>The method does not return anything. It adds new data to the data set.
</p>


<hr>
<a id="method-LargeDataSetForTextEmbeddings-convert_to_EmbeddedText"></a>



<h4>Method <code>convert_to_EmbeddedText()</code></h4>

<p>Method for converting this object to an object of class <a href="#topic+EmbeddedText">EmbeddedText</a>.
</p>
<p><strong>Attention</strong> This object uses memory mapping to allow the usage of data sets
that do not fit into memory. By calling this method the data set will be loaded and stored into memory/RAM.
This may lead to an out-of-memory error.
</p>


<h5>Usage</h5>

<div class="r"><pre>LargeDataSetForTextEmbeddings$convert_to_EmbeddedText()</pre></div>



<h5>Returns</h5>

<p>LargeDataSetForTextEmbeddings an object of class <a href="#topic+EmbeddedText">EmbeddedText</a> which is stored in the memory/RAM.
</p>


<hr>
<a id="method-LargeDataSetForTextEmbeddings-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>LargeDataSetForTextEmbeddings$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>See Also</h3>

<p>Other Data Management: 
<code><a href="#topic+DataManagerClassifier">DataManagerClassifier</a></code>,
<code><a href="#topic+EmbeddedText">EmbeddedText</a></code>,
<code><a href="#topic+LargeDataSetForText">LargeDataSetForText</a></code>
</p>

<hr>
<h2 id='License_Server'>Server function for: graphical user interface for showing the license.</h2><span id='topic+License_Server'></span>

<h3>Description</h3>

<p>Functions generates the functionality of a page on the server.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>License_Server(id)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="License_Server_+3A_id">id</code></td>
<td>
<p><code>string</code> determining the id for the namespace.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>This function does nothing return. It is used to create the functionality of a page for a shiny app.
</p>

<hr>
<h2 id='load_from_disk'>Loading objects created with 'aifeducation'</h2><span id='topic+load_from_disk'></span>

<h3>Description</h3>

<p>Function for loading objects created with 'aifeducation'.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>load_from_disk(dir_path)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="load_from_disk_+3A_dir_path">dir_path</code></td>
<td>
<p><code>string</code> Path to the directory where the model is stored.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns an object of class <a href="#topic+TEClassifierRegular">TEClassifierRegular</a>, <a href="#topic+TEClassifierProtoNet">TEClassifierProtoNet</a>,  <a href="#topic+TEFeatureExtractor">TEFeatureExtractor</a>,
<a href="#topic+TextEmbeddingModel">TextEmbeddingModel</a>, <a href="#topic+LargeDataSetForTextEmbeddings">LargeDataSetForTextEmbeddings</a>, <a href="#topic+LargeDataSetForText">LargeDataSetForText</a> or <a href="#topic+EmbeddedText">EmbeddedText</a>.
</p>


<h3>See Also</h3>

<p>Other Saving and Loading: 
<code><a href="#topic+save_to_disk">save_to_disk</a>()</code>
</p>

<hr>
<h2 id='long_load_target_data'>Load target data for long running tasks</h2><span id='topic+long_load_target_data'></span>

<h3>Description</h3>

<p>Function loads the target data for a long running task.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>long_load_target_data(file_path, selectet_column)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="long_load_target_data_+3A_file_path">file_path</code></td>
<td>
<p><code>string</code> Path to the file storing the target data.</p>
</td></tr>
<tr><td><code id="long_load_target_data_+3A_selectet_column">selectet_column</code></td>
<td>
<p><code>string</code> Name of the column containing the target data.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function assumes that the target data is stored as a columns with the cases in the rows and the
categories in the columns. The ids of the cases must be stored in a column called &quot;id&quot;.
</p>


<h3>Value</h3>

<p>Returns a named factor containing the target data.
</p>


<h3>See Also</h3>

<p>Other studio_utils: 
<code><a href="#topic+create_data_embeddings_description">create_data_embeddings_description</a>()</code>
</p>

<hr>
<h2 id='matrix_to_array_c'>Reshape matrix to array</h2><span id='topic+matrix_to_array_c'></span>

<h3>Description</h3>

<p>Function written in C++ for reshaping a matrix containing sequential data into
an array for use with keras.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>matrix_to_array_c(matrix, times, features)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="matrix_to_array_c_+3A_matrix">matrix</code></td>
<td>
<p><code>matrix</code> containing the sequential data.</p>
</td></tr>
<tr><td><code id="matrix_to_array_c_+3A_times">times</code></td>
<td>
<p><code>uword</code> Number of sequences.</p>
</td></tr>
<tr><td><code id="matrix_to_array_c_+3A_features">features</code></td>
<td>
<p><code>uword</code> Number of features within each sequence.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns an array. The first dimension corresponds to the cases,
the second to the times, and the third to the features.
</p>


<h3>See Also</h3>

<p>Other Auxiliary Functions: 
<code><a href="#topic+get_alpha_3_codes">get_alpha_3_codes</a>()</code>,
<code><a href="#topic+summarize_tracked_sustainability">summarize_tracked_sustainability</a>()</code>,
<code><a href="#topic+to_categorical_c">to_categorical_c</a>()</code>
</p>

<hr>
<h2 id='output_message'>Print message</h2><span id='topic+output_message'></span>

<h3>Description</h3>

<p>Prints a message <code>msg</code> if <code>trace</code> parameter is <code>TRUE</code> with current date with <code>message()</code> or <code>cat()</code>
function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>output_message(msg, trace, msg_fun)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="output_message_+3A_msg">msg</code></td>
<td>
<p><code>string</code> Message that should be printed.</p>
</td></tr>
<tr><td><code id="output_message_+3A_trace">trace</code></td>
<td>
<p><code>bool</code> Silent printing (<code>FALSE</code>) or not (<code>TRUE</code>).</p>
</td></tr>
<tr><td><code id="output_message_+3A_msg_fun">msg_fun</code></td>
<td>
<p><code>bool</code> value that determines what function should be used. <code>TRUE</code> for <code>message()</code>, <code>FALSE</code> for
<code>cat()</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>This function returns nothing.
</p>


<h3>See Also</h3>

<p>Other Utils: 
<code><a href="#topic+auto_n_cores">auto_n_cores</a>()</code>,
<code><a href="#topic+clean_pytorch_log_transformers">clean_pytorch_log_transformers</a>()</code>,
<code><a href="#topic+create_config_state">create_config_state</a>()</code>,
<code><a href="#topic+create_dir">create_dir</a>()</code>,
<code><a href="#topic+generate_id">generate_id</a>()</code>,
<code><a href="#topic+get_file_extension">get_file_extension</a>()</code>,
<code><a href="#topic+get_py_package_versions">get_py_package_versions</a>()</code>,
<code><a href="#topic+is.null_or_na">is.null_or_na</a>()</code>,
<code><a href="#topic+print_message">print_message</a>()</code>,
<code><a href="#topic+run_py_file">run_py_file</a>()</code>
</p>

<hr>
<h2 id='print_message'>Print message (<code>message()</code>)</h2><span id='topic+print_message'></span>

<h3>Description</h3>

<p>Prints a message <code>msg</code> if <code>trace</code> parameter is <code>TRUE</code> with current date with <code>message()</code> function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>print_message(msg, trace)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print_message_+3A_msg">msg</code></td>
<td>
<p><code>string</code> Message that should be printed.</p>
</td></tr>
<tr><td><code id="print_message_+3A_trace">trace</code></td>
<td>
<p><code>bool</code> Silent printing (<code>FALSE</code>) or not (<code>TRUE</code>).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>This function returns nothing.
</p>


<h3>See Also</h3>

<p>Other Utils: 
<code><a href="#topic+auto_n_cores">auto_n_cores</a>()</code>,
<code><a href="#topic+clean_pytorch_log_transformers">clean_pytorch_log_transformers</a>()</code>,
<code><a href="#topic+create_config_state">create_config_state</a>()</code>,
<code><a href="#topic+create_dir">create_dir</a>()</code>,
<code><a href="#topic+generate_id">generate_id</a>()</code>,
<code><a href="#topic+get_file_extension">get_file_extension</a>()</code>,
<code><a href="#topic+get_py_package_versions">get_py_package_versions</a>()</code>,
<code><a href="#topic+is.null_or_na">is.null_or_na</a>()</code>,
<code><a href="#topic+output_message">output_message</a>()</code>,
<code><a href="#topic+run_py_file">run_py_file</a>()</code>
</p>

<hr>
<h2 id='Reliability_Server'>Server function for: graphical user interface for displaying the reliability of classifiers.</h2><span id='topic+Reliability_Server'></span>

<h3>Description</h3>

<p>Functions generates the functionality of a page on the server.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Reliability_Server(id, model)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Reliability_Server_+3A_id">id</code></td>
<td>
<p><code>string</code> determining the id for the namespace.</p>
</td></tr>
<tr><td><code id="Reliability_Server_+3A_model">model</code></td>
<td>
<p>Model used for inference.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>This function does nothing return. It is used to create the functionality of a page for a shiny app.
</p>


<h3>See Also</h3>

<p>Other studio_gui_page_classifier_reliability: 
<code><a href="#topic+Reliability_UI">Reliability_UI</a>()</code>
</p>

<hr>
<h2 id='Reliability_UI'>Graphical user interface for displaying the reliability of classifiers.</h2><span id='topic+Reliability_UI'></span>

<h3>Description</h3>

<p>Functions generates the tab within a page for displaying infomration on the reliability of classifiers.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Reliability_UI(id)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Reliability_UI_+3A_id">id</code></td>
<td>
<p><code>string</code> determining the id for the namespace.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>This function does nothing return. It is used to build a page for a shiny app.
</p>


<h3>See Also</h3>

<p>Other studio_gui_page_classifier_reliability: 
<code><a href="#topic+Reliability_Server">Reliability_Server</a>()</code>
</p>

<hr>
<h2 id='run_py_file'>Run python file</h2><span id='topic+run_py_file'></span>

<h3>Description</h3>

<p>Used to run python files with <code>reticulate::py_run_file()</code> from folder <code>python</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>run_py_file(py_file_name)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="run_py_file_+3A_py_file_name">py_file_name</code></td>
<td>
<p><code>string</code> Name of a python file to run. The file must be in the <code>python</code> folder of <code>aifeducation</code>
package.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>This function returns nothing.
</p>


<h3>See Also</h3>

<p>Other Utils: 
<code><a href="#topic+auto_n_cores">auto_n_cores</a>()</code>,
<code><a href="#topic+clean_pytorch_log_transformers">clean_pytorch_log_transformers</a>()</code>,
<code><a href="#topic+create_config_state">create_config_state</a>()</code>,
<code><a href="#topic+create_dir">create_dir</a>()</code>,
<code><a href="#topic+generate_id">generate_id</a>()</code>,
<code><a href="#topic+get_file_extension">get_file_extension</a>()</code>,
<code><a href="#topic+get_py_package_versions">get_py_package_versions</a>()</code>,
<code><a href="#topic+is.null_or_na">is.null_or_na</a>()</code>,
<code><a href="#topic+output_message">output_message</a>()</code>,
<code><a href="#topic+print_message">print_message</a>()</code>
</p>

<hr>
<h2 id='save_to_disk'>Saving objects created with 'aifeducation'</h2><span id='topic+save_to_disk'></span>

<h3>Description</h3>

<p>Function for saving objects created with 'aifeducation'.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>save_to_disk(object, dir_path, folder_name)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="save_to_disk_+3A_object">object</code></td>
<td>
<p>Object of class <a href="#topic+TEClassifierRegular">TEClassifierRegular</a>, <a href="#topic+TEClassifierProtoNet">TEClassifierProtoNet</a>,  <a href="#topic+TEFeatureExtractor">TEFeatureExtractor</a>,
<a href="#topic+TextEmbeddingModel">TextEmbeddingModel</a>, <a href="#topic+LargeDataSetForTextEmbeddings">LargeDataSetForTextEmbeddings</a>, <a href="#topic+LargeDataSetForText">LargeDataSetForText</a> or <a href="#topic+EmbeddedText">EmbeddedText</a> which should be
saved.</p>
</td></tr>
<tr><td><code id="save_to_disk_+3A_dir_path">dir_path</code></td>
<td>
<p><code>string</code> Path to the directory where the should model is stored.</p>
</td></tr>
<tr><td><code id="save_to_disk_+3A_folder_name">folder_name</code></td>
<td>
<p><code>string</code> Name of the folder where the files should be stored.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Function does not return a value. It saves the model to disk.
</p>
<p>No return value, called for side effects.
</p>


<h3>See Also</h3>

<p>Other Saving and Loading: 
<code><a href="#topic+load_from_disk">load_from_disk</a>()</code>
</p>

<hr>
<h2 id='set_config_cpu_only'>Setting cpu only for 'tensorflow'</h2><span id='topic+set_config_cpu_only'></span>

<h3>Description</h3>

<p>This functions configurates 'tensorflow' to use only cpus.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>set_config_cpu_only()
</code></pre>


<h3>Value</h3>

<p>This function does not return anything. It is used for its side effects.
</p>


<h3>Note</h3>

<p>os$environ$setdefault(&quot;CUDA_VISIBLE_DEVICES&quot;,&quot;-1&quot;)
</p>


<h3>See Also</h3>

<p>Other Installation and Configuration Tensorflow: 
<code><a href="#topic+set_config_gpu_low_memory">set_config_gpu_low_memory</a>()</code>,
<code><a href="#topic+set_config_os_environ_logger">set_config_os_environ_logger</a>()</code>,
<code><a href="#topic+set_config_tf_logger">set_config_tf_logger</a>()</code>
</p>

<hr>
<h2 id='set_config_gpu_low_memory'>Setting gpus' memory usage</h2><span id='topic+set_config_gpu_low_memory'></span>

<h3>Description</h3>

<p>This function changes the memory usage of the gpus to allow computations on machines with small memory.
With this function, some computations of large models may be possible but the speed of computation decreases.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>set_config_gpu_low_memory()
</code></pre>


<h3>Value</h3>

<p>This function does not return anything. It is used for its side effects.
</p>


<h3>Note</h3>

<p>This function sets TF_GPU_ALLOCATOR to <code>"cuda_malloc_async"</code> and sets memory growth to <code>TRUE</code>.
</p>


<h3>See Also</h3>

<p>Other Installation and Configuration Tensorflow: 
<code><a href="#topic+set_config_cpu_only">set_config_cpu_only</a>()</code>,
<code><a href="#topic+set_config_os_environ_logger">set_config_os_environ_logger</a>()</code>,
<code><a href="#topic+set_config_tf_logger">set_config_tf_logger</a>()</code>
</p>

<hr>
<h2 id='set_config_os_environ_logger'>Sets the level for logging information in tensorflow</h2><span id='topic+set_config_os_environ_logger'></span>

<h3>Description</h3>

<p>This function changes the level for logging information with 'tensorflow' via the os environment. This
function must be called before importing 'tensorflow'.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>set_config_os_environ_logger(level = "ERROR")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="set_config_os_environ_logger_+3A_level">level</code></td>
<td>
<p><code>string</code> Minimal level that should be printed to console. Four levels are available: INFO, WARNING,
ERROR and NONE.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>This function does not return anything. It is used for its side effects.
</p>


<h3>See Also</h3>

<p>Other Installation and Configuration Tensorflow: 
<code><a href="#topic+set_config_cpu_only">set_config_cpu_only</a>()</code>,
<code><a href="#topic+set_config_gpu_low_memory">set_config_gpu_low_memory</a>()</code>,
<code><a href="#topic+set_config_tf_logger">set_config_tf_logger</a>()</code>
</p>

<hr>
<h2 id='set_config_tf_logger'>Sets the level for logging information in tensorflow</h2><span id='topic+set_config_tf_logger'></span>

<h3>Description</h3>

<p>This function changes the level for logging information with 'tensorflow'.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>set_config_tf_logger(level = "ERROR")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="set_config_tf_logger_+3A_level">level</code></td>
<td>
<p><code>string</code> Minimal level that should be printed to console. Five levels are available: FATAL, ERROR, WARN,
INFO, and DEBUG.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>This function does not return anything. It is used for its side effects.
</p>


<h3>See Also</h3>

<p>Other Installation and Configuration Tensorflow: 
<code><a href="#topic+set_config_cpu_only">set_config_cpu_only</a>()</code>,
<code><a href="#topic+set_config_gpu_low_memory">set_config_gpu_low_memory</a>()</code>,
<code><a href="#topic+set_config_os_environ_logger">set_config_os_environ_logger</a>()</code>
</p>

<hr>
<h2 id='set_transformers_logger'>Sets the level for logging information of the 'transformers' library</h2><span id='topic+set_transformers_logger'></span>

<h3>Description</h3>

<p>This function changes the level for logging information of the 'transformers' library. It influences the
output printed to console for creating and training transformer models as well as <a href="#topic+TextEmbeddingModel">TextEmbeddingModel</a>s.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>set_transformers_logger(level = "ERROR")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="set_transformers_logger_+3A_level">level</code></td>
<td>
<p><code>string</code> Minimal level that should be printed to console. Four levels are available: INFO, WARNING,
ERROR and DEBUG</p>
</td></tr>
</table>


<h3>Value</h3>

<p>This function does not return anything. It is used for its side effects.
</p>


<h3>See Also</h3>

<p>Other Installation and Configuration: 
<code><a href="#topic+check_aif_py_modules">check_aif_py_modules</a>()</code>,
<code><a href="#topic+install_aifeducation">install_aifeducation</a>()</code>,
<code><a href="#topic+install_py_modules">install_py_modules</a>()</code>
</p>

<hr>
<h2 id='start_aifeducation_studio'>Aifeducation Studio</h2><span id='topic+start_aifeducation_studio'></span>

<h3>Description</h3>

<p>Functions starts a shiny app that represents Aifeducation Studio.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>start_aifeducation_studio()
</code></pre>


<h3>Value</h3>

<p>This function does nothing return. It is used to start a shiny app.
</p>

<hr>
<h2 id='summarize_tracked_sustainability'>Summarizing tracked sustainability data</h2><span id='topic+summarize_tracked_sustainability'></span>

<h3>Description</h3>

<p>Function for summarizing the tracked sustainability data with a tracker of the python library
'codecarbon'.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>summarize_tracked_sustainability(sustainability_tracker)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="summarize_tracked_sustainability_+3A_sustainability_tracker">sustainability_tracker</code></td>
<td>
<p>Object of class <code>codecarbon.emissions_tracker.OfflineEmissionsTracker</code> of the python
library codecarbon.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a <code>list</code> which contains the tracked sustainability data.
</p>


<h3>See Also</h3>

<p>Other Auxiliary Functions: 
<code><a href="#topic+get_alpha_3_codes">get_alpha_3_codes</a>()</code>,
<code><a href="#topic+matrix_to_array_c">matrix_to_array_c</a>()</code>,
<code><a href="#topic+to_categorical_c">to_categorical_c</a>()</code>
</p>

<hr>
<h2 id='TEClassifierProtoNet'>Text embedding classifier with a ProtoNet</h2><span id='topic+TEClassifierProtoNet'></span>

<h3>Description</h3>

<p>Abstract class for neural nets with 'keras'/'tensorflow' and 'pytorch'.
</p>
<p>This object represents in implementation of a prototypical network for few-shot learning as described by Snell,
Swersky, and Zemel (2017). The network uses a multi way contrastive loss described by Zhang et al. (2019). The
network learns to scale the metric as described by Oreshkin, Rodriguez, and Lacoste (2018)
</p>


<h3>Value</h3>

<p>Objects of this class are used for assigning texts to classes/categories. For the creation and training of a
classifier an object of class <a href="#topic+EmbeddedText">EmbeddedText</a> or <a href="#topic+LargeDataSetForTextEmbeddings">LargeDataSetForTextEmbeddings</a> and a <code>factor</code> are necessary. The
object of class <a href="#topic+EmbeddedText">EmbeddedText</a> or <a href="#topic+LargeDataSetForTextEmbeddings">LargeDataSetForTextEmbeddings</a> contains the numerical text representations (text
embeddings) of the raw texts generated by an object of class <a href="#topic+TextEmbeddingModel">TextEmbeddingModel</a>. The <code>factor</code> contains the
classes/categories for every text. Missing values (unlabeled cases) are supported. For predictions an object of
class <a href="#topic+EmbeddedText">EmbeddedText</a> or <a href="#topic+LargeDataSetForTextEmbeddings">LargeDataSetForTextEmbeddings</a> has to be used which was created with the same
<a href="#topic+TextEmbeddingModel">TextEmbeddingModel</a> as for training.
</p>


<h3>Super classes</h3>

<p><code><a href="#topic+AIFEBaseModel">aifeducation::AIFEBaseModel</a></code> -&gt; <code><a href="#topic+TEClassifierRegular">aifeducation::TEClassifierRegular</a></code> -&gt; <code>TEClassifierProtoNet</code>
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-TEClassifierProtoNet-configure"><code>TEClassifierProtoNet$configure()</code></a>
</p>
</li>
<li> <p><a href="#method-TEClassifierProtoNet-train"><code>TEClassifierProtoNet$train()</code></a>
</p>
</li>
<li> <p><a href="#method-TEClassifierProtoNet-embed"><code>TEClassifierProtoNet$embed()</code></a>
</p>
</li>
<li> <p><a href="#method-TEClassifierProtoNet-plot_embeddings"><code>TEClassifierProtoNet$plot_embeddings()</code></a>
</p>
</li>
<li> <p><a href="#method-TEClassifierProtoNet-clone"><code>TEClassifierProtoNet$clone()</code></a>
</p>
</li></ul>



<details><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="AIFEBaseModel" data-id="count_parameter"><a href='../../aifeducation/html/AIFEBaseModel.html#method-AIFEBaseModel-count_parameter'><code>aifeducation::AIFEBaseModel$count_parameter()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="AIFEBaseModel" data-id="get_all_fields"><a href='../../aifeducation/html/AIFEBaseModel.html#method-AIFEBaseModel-get_all_fields'><code>aifeducation::AIFEBaseModel$get_all_fields()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="AIFEBaseModel" data-id="get_documentation_license"><a href='../../aifeducation/html/AIFEBaseModel.html#method-AIFEBaseModel-get_documentation_license'><code>aifeducation::AIFEBaseModel$get_documentation_license()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="AIFEBaseModel" data-id="get_ml_framework"><a href='../../aifeducation/html/AIFEBaseModel.html#method-AIFEBaseModel-get_ml_framework'><code>aifeducation::AIFEBaseModel$get_ml_framework()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="AIFEBaseModel" data-id="get_model_description"><a href='../../aifeducation/html/AIFEBaseModel.html#method-AIFEBaseModel-get_model_description'><code>aifeducation::AIFEBaseModel$get_model_description()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="AIFEBaseModel" data-id="get_model_info"><a href='../../aifeducation/html/AIFEBaseModel.html#method-AIFEBaseModel-get_model_info'><code>aifeducation::AIFEBaseModel$get_model_info()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="AIFEBaseModel" data-id="get_model_license"><a href='../../aifeducation/html/AIFEBaseModel.html#method-AIFEBaseModel-get_model_license'><code>aifeducation::AIFEBaseModel$get_model_license()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="AIFEBaseModel" data-id="get_package_versions"><a href='../../aifeducation/html/AIFEBaseModel.html#method-AIFEBaseModel-get_package_versions'><code>aifeducation::AIFEBaseModel$get_package_versions()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="AIFEBaseModel" data-id="get_private"><a href='../../aifeducation/html/AIFEBaseModel.html#method-AIFEBaseModel-get_private'><code>aifeducation::AIFEBaseModel$get_private()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="AIFEBaseModel" data-id="get_publication_info"><a href='../../aifeducation/html/AIFEBaseModel.html#method-AIFEBaseModel-get_publication_info'><code>aifeducation::AIFEBaseModel$get_publication_info()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="AIFEBaseModel" data-id="get_sustainability_data"><a href='../../aifeducation/html/AIFEBaseModel.html#method-AIFEBaseModel-get_sustainability_data'><code>aifeducation::AIFEBaseModel$get_sustainability_data()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="AIFEBaseModel" data-id="get_text_embedding_model"><a href='../../aifeducation/html/AIFEBaseModel.html#method-AIFEBaseModel-get_text_embedding_model'><code>aifeducation::AIFEBaseModel$get_text_embedding_model()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="AIFEBaseModel" data-id="get_text_embedding_model_name"><a href='../../aifeducation/html/AIFEBaseModel.html#method-AIFEBaseModel-get_text_embedding_model_name'><code>aifeducation::AIFEBaseModel$get_text_embedding_model_name()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="AIFEBaseModel" data-id="is_configured"><a href='../../aifeducation/html/AIFEBaseModel.html#method-AIFEBaseModel-is_configured'><code>aifeducation::AIFEBaseModel$is_configured()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="AIFEBaseModel" data-id="load"><a href='../../aifeducation/html/AIFEBaseModel.html#method-AIFEBaseModel-load'><code>aifeducation::AIFEBaseModel$load()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="AIFEBaseModel" data-id="set_documentation_license"><a href='../../aifeducation/html/AIFEBaseModel.html#method-AIFEBaseModel-set_documentation_license'><code>aifeducation::AIFEBaseModel$set_documentation_license()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="AIFEBaseModel" data-id="set_model_description"><a href='../../aifeducation/html/AIFEBaseModel.html#method-AIFEBaseModel-set_model_description'><code>aifeducation::AIFEBaseModel$set_model_description()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="AIFEBaseModel" data-id="set_model_license"><a href='../../aifeducation/html/AIFEBaseModel.html#method-AIFEBaseModel-set_model_license'><code>aifeducation::AIFEBaseModel$set_model_license()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="AIFEBaseModel" data-id="set_publication_info"><a href='../../aifeducation/html/AIFEBaseModel.html#method-AIFEBaseModel-set_publication_info'><code>aifeducation::AIFEBaseModel$set_publication_info()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="TEClassifierRegular" data-id="check_embedding_model"><a href='../../aifeducation/html/TEClassifierRegular.html#method-TEClassifierRegular-check_embedding_model'><code>aifeducation::TEClassifierRegular$check_embedding_model()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="TEClassifierRegular" data-id="check_feature_extractor_object_type"><a href='../../aifeducation/html/TEClassifierRegular.html#method-TEClassifierRegular-check_feature_extractor_object_type'><code>aifeducation::TEClassifierRegular$check_feature_extractor_object_type()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="TEClassifierRegular" data-id="load_from_disk"><a href='../../aifeducation/html/TEClassifierRegular.html#method-TEClassifierRegular-load_from_disk'><code>aifeducation::TEClassifierRegular$load_from_disk()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="TEClassifierRegular" data-id="predict"><a href='../../aifeducation/html/TEClassifierRegular.html#method-TEClassifierRegular-predict'><code>aifeducation::TEClassifierRegular$predict()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="TEClassifierRegular" data-id="requires_compression"><a href='../../aifeducation/html/TEClassifierRegular.html#method-TEClassifierRegular-requires_compression'><code>aifeducation::TEClassifierRegular$requires_compression()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="TEClassifierRegular" data-id="save"><a href='../../aifeducation/html/TEClassifierRegular.html#method-TEClassifierRegular-save'><code>aifeducation::TEClassifierRegular$save()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-TEClassifierProtoNet-configure"></a>



<h4>Method <code>configure()</code></h4>

<p>Creating a new instance of this class.
</p>


<h5>Usage</h5>

<div class="r"><pre>TEClassifierProtoNet$configure(
  ml_framework = "pytorch",
  name = NULL,
  label = NULL,
  text_embeddings = NULL,
  feature_extractor = NULL,
  target_levels = NULL,
  dense_size = 4,
  dense_layers = 0,
  rec_size = 4,
  rec_layers = 2,
  rec_type = "gru",
  rec_bidirectional = FALSE,
  embedding_dim = 2,
  self_attention_heads = 0,
  intermediate_size = NULL,
  attention_type = "fourier",
  add_pos_embedding = TRUE,
  rec_dropout = 0.1,
  repeat_encoder = 1,
  dense_dropout = 0.4,
  recurrent_dropout = 0.4,
  encoder_dropout = 0.1,
  optimizer = "adam"
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>ml_framework</code></dt><dd><p><code>string</code> Currently only pytorch is supported (<code>ml_framework="pytorch"</code>).</p>
</dd>
<dt><code>name</code></dt><dd><p><code>string</code> Name of the new classifier. Please refer to common name conventions. Free text can be used
with parameter <code>label</code>.</p>
</dd>
<dt><code>label</code></dt><dd><p><code>string</code> Label for the new classifier. Here you can use free text.</p>
</dd>
<dt><code>text_embeddings</code></dt><dd><p>An object of class <a href="#topic+TextEmbeddingModel">TextEmbeddingModel</a> or <a href="#topic+LargeDataSetForTextEmbeddings">LargeDataSetForTextEmbeddings</a>.</p>
</dd>
<dt><code>feature_extractor</code></dt><dd><p>Object of class <a href="#topic+TEFeatureExtractor">TEFeatureExtractor</a> which should be used in order to reduce the number
of dimensions of the text embeddings. If no feature extractor should be applied set <code>NULL</code>.</p>
</dd>
<dt><code>target_levels</code></dt><dd><p><code>vector</code> containing the levels (categories or classes) within the target data. Please not
that order matters. For ordinal data please ensure that the levels are sorted correctly with later levels
indicating a higher category/class. For nominal data the order does not matter.</p>
</dd>
<dt><code>dense_size</code></dt><dd><p><code>int</code> Number of neurons for each dense layer.</p>
</dd>
<dt><code>dense_layers</code></dt><dd><p><code>int</code> Number of dense layers.</p>
</dd>
<dt><code>rec_size</code></dt><dd><p><code>int</code> Number of neurons for each recurrent layer.</p>
</dd>
<dt><code>rec_layers</code></dt><dd><p><code>int</code> Number of recurrent layers.</p>
</dd>
<dt><code>rec_type</code></dt><dd><p><code>string</code> Type of the recurrent layers.<code>rec_type="gru"</code> for Gated Recurrent Unit and
<code>rec_type="lstm"</code> for Long Short-Term Memory.</p>
</dd>
<dt><code>rec_bidirectional</code></dt><dd><p><code>bool</code> If <code>TRUE</code> a bidirectional version of the recurrent layers is used.</p>
</dd>
<dt><code>embedding_dim</code></dt><dd><p><code>int</code> determining the number of dimensions for the text embedding.</p>
</dd>
<dt><code>self_attention_heads</code></dt><dd><p><code>int</code> determining the number of attention heads for a self-attention layer. Only
relevant if <code>attention_type="multihead"</code>.</p>
</dd>
<dt><code>intermediate_size</code></dt><dd><p><code>int</code> determining the size of the projection layer within a each transformer encoder.</p>
</dd>
<dt><code>attention_type</code></dt><dd><p><code>string</code> Choose the relevant attention type. Possible values are <code>"fourier"</code> and
<code>"multihead"</code>. Please note that you may see different values for a case for different input orders if you choose <code>fourier</code> on linux.</p>
</dd>
<dt><code>add_pos_embedding</code></dt><dd><p><code>bool</code> <code>TRUE</code> if positional embedding should be used.</p>
</dd>
<dt><code>rec_dropout</code></dt><dd><p><code>double</code> ranging between 0 and lower 1, determining the dropout between bidirectional
recurrent layers.</p>
</dd>
<dt><code>repeat_encoder</code></dt><dd><p><code>int</code> determining how many times the encoder should be added to the network.</p>
</dd>
<dt><code>dense_dropout</code></dt><dd><p><code>double</code> ranging between 0 and lower 1, determining the dropout between dense layers.</p>
</dd>
<dt><code>recurrent_dropout</code></dt><dd><p><code>double</code> ranging between 0 and lower 1, determining the recurrent dropout for each
recurrent layer. Only relevant for keras models.</p>
</dd>
<dt><code>encoder_dropout</code></dt><dd><p><code>double</code> ranging between 0 and lower 1, determining the dropout for the dense projection
within the encoder layers.</p>
</dd>
<dt><code>optimizer</code></dt><dd><p><code>string</code> <code>"adam"</code> or <code>"rmsprop"</code> .</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>Returns an object of class <a href="#topic+TEClassifierProtoNet">TEClassifierProtoNet</a> which is ready for training.
</p>


<hr>
<a id="method-TEClassifierProtoNet-train"></a>



<h4>Method <code>train()</code></h4>

<p>Method for training a neural net.
</p>
<p>Training includes a routine for early stopping. In the case that loss&lt;0.0001
and Accuracy=1.00 and Average Iota=1.00 training stops. The history uses the values
of the last trained epoch for the remaining epochs.
</p>
<p>After training the model with the best values for Average Iota, Accuracy, and Loss
on the validation data set is used as the final model.
</p>


<h5>Usage</h5>

<div class="r"><pre>TEClassifierProtoNet$train(
  data_embeddings,
  data_targets,
  data_folds = 5,
  data_val_size = 0.25,
  use_sc = TRUE,
  sc_method = "dbsmote",
  sc_min_k = 1,
  sc_max_k = 10,
  use_pl = TRUE,
  pl_max_steps = 3,
  pl_max = 1,
  pl_anchor = 1,
  pl_min = 0,
  sustain_track = TRUE,
  sustain_iso_code = NULL,
  sustain_region = NULL,
  sustain_interval = 15,
  epochs = 40,
  batch_size = 35,
  Ns = 5,
  Nq = 3,
  loss_alpha = 0.5,
  loss_margin = 0.5,
  sampling_separate = FALSE,
  sampling_shuffle = TRUE,
  dir_checkpoint,
  trace = TRUE,
  ml_trace = 1,
  log_dir = NULL,
  log_write_interval = 10,
  n_cores = auto_n_cores()
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>data_embeddings</code></dt><dd><p>Object of class <a href="#topic+EmbeddedText">EmbeddedText</a> or <a href="#topic+LargeDataSetForTextEmbeddings">LargeDataSetForTextEmbeddings</a>.</p>
</dd>
<dt><code>data_targets</code></dt><dd><p><code>factor</code> containing the labels for cases stored in <code>data_embeddings</code>. Factor must be named
and has to use the same names used in <code>data_embeddings</code>.</p>
</dd>
<dt><code>data_folds</code></dt><dd><p><code>int</code> determining the number of cross-fold samples.</p>
</dd>
<dt><code>data_val_size</code></dt><dd><p><code>double</code> between 0 and 1, indicating the proportion of cases of each class which should be
used for the validation sample during the estimation of the model. The remaining cases are part of the training
data.</p>
</dd>
<dt><code>use_sc</code></dt><dd><p><code>bool</code> <code>TRUE</code> if the estimation should integrate synthetic cases. <code>FALSE</code> if not.</p>
</dd>
<dt><code>sc_method</code></dt><dd><p><code>vector</code> containing the method for generating synthetic cases. Possible are <code>sc_method="adas"</code>,
<code>sc_method="smote"</code>, and <code>sc_method="dbsmote"</code>.</p>
</dd>
<dt><code>sc_min_k</code></dt><dd><p><code>int</code> determining the minimal number of k which is used for creating synthetic units.</p>
</dd>
<dt><code>sc_max_k</code></dt><dd><p><code>int</code> determining the maximal number of k which is used for creating synthetic units.</p>
</dd>
<dt><code>use_pl</code></dt><dd><p><code>bool</code> <code>TRUE</code> if the estimation should integrate pseudo-labeling. <code>FALSE</code> if not.</p>
</dd>
<dt><code>pl_max_steps</code></dt><dd><p><code>int</code> determining the maximum number of steps during pseudo-labeling.</p>
</dd>
<dt><code>pl_max</code></dt><dd><p><code>double</code> between 0 and 1, setting the maximal level of confidence for considering a case for
pseudo-labeling.</p>
</dd>
<dt><code>pl_anchor</code></dt><dd><p><code>double</code> between 0 and 1 indicating the reference point for sorting the new cases of every
label. See notes for more details.</p>
</dd>
<dt><code>pl_min</code></dt><dd><p><code>double</code> between 0 and 1, setting the minimal level of confidence for considering a case for
pseudo-labeling.</p>
</dd>
<dt><code>sustain_track</code></dt><dd><p><code>bool</code> If <code>TRUE</code> energy consumption is tracked during training via the python library
'codecarbon'.</p>
</dd>
<dt><code>sustain_iso_code</code></dt><dd><p><code>string</code> ISO code (Alpha-3-Code) for the country. This variable must be set if
sustainability should be tracked. A list can be found on Wikipedia:
<a href="https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes">https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes</a>.</p>
</dd>
<dt><code>sustain_region</code></dt><dd><p>Region within a country. Only available for USA and Canada See the documentation of
codecarbon for more information. <a href="https://mlco2.github.io/codecarbon/parameters.html">https://mlco2.github.io/codecarbon/parameters.html</a></p>
</dd>
<dt><code>sustain_interval</code></dt><dd><p><code>int</code> Interval in seconds for measuring power usage.</p>
</dd>
<dt><code>epochs</code></dt><dd><p><code>int</code> Number of training epochs.</p>
</dd>
<dt><code>batch_size</code></dt><dd><p><code>int</code> Size of the batches for training.</p>
</dd>
<dt><code>Ns</code></dt><dd><p><code>int</code> Number of cases for every class in the sample.</p>
</dd>
<dt><code>Nq</code></dt><dd><p><code>int</code> Number of cases for every class in the query.</p>
</dd>
<dt><code>loss_alpha</code></dt><dd><p><code>double</code> Value between 0 and 1 indicating how strong the loss should focus on pulling cases to
its corresponding prototypes or pushing cases away from other prototypes. The higher the value the more the
loss concentrates on pulling cases to its corresponding prototypes.</p>
</dd>
<dt><code>loss_margin</code></dt><dd><p><code>double</code> Value greater 0 indicating the minimal distance of every case from prototypes of
other classes</p>
</dd>
<dt><code>sampling_separate</code></dt><dd><p><code>bool</code> If <code>TRUE</code> the cases for every class are divided into a data set for sample and for query.
These are never mixed. If <code>TRUE</code> sample and query cases are drawn from the same data pool. That is, a case can be
part of sample in one epoch and in another epoch it can be part of query. It is ensured that a case is never part of
sample and query at the same time. In addition, it is ensured that every cases exists only once during
a training step.</p>
</dd>
<dt><code>sampling_shuffle</code></dt><dd><p><code>bool</code> If <code>TRUE</code> cases a randomly drawn from the data during every step. If <code>FALSE</code>
the cases are not shuffled.</p>
</dd>
<dt><code>dir_checkpoint</code></dt><dd><p><code>string</code> Path to the directory where the checkpoint during training should be saved. If the
directory does not exist, it is created.</p>
</dd>
<dt><code>trace</code></dt><dd><p><code>bool</code> <code>TRUE</code>, if information about the estimation phase should be printed to the console.</p>
</dd>
<dt><code>ml_trace</code></dt><dd><p><code>int</code> <code>ml_trace=0</code> does not print any information about the training process from pytorch on the
console.</p>
</dd>
<dt><code>log_dir</code></dt><dd><p><code>string</code> Path to the directory where the log files should be saved. If no logging is desired set
this argument to <code>NULL</code>.</p>
</dd>
<dt><code>log_write_interval</code></dt><dd><p><code>int</code> Time in seconds determining the interval in which the logger should try to update
the log files. Only relevant if <code>log_dir</code> is not <code>NULL</code>.</p>
</dd>
<dt><code>n_cores</code></dt><dd><p><code>int</code> Number of cores which should be used during the calculation of synthetic cases. Only relevant if
<code>use_sc=TRUE</code>.</p>
</dd>
<dt><code>balance_class_weights</code></dt><dd><p><code>bool</code> If <code>TRUE</code> class weights are generated based on the frequencies of the
training data with the method Inverse Class Frequency'. If <code>FALSE</code> each class has the weight 1.</p>
</dd>
<dt><code>balance_sequence_length</code></dt><dd><p><code>bool</code> If <code>TRUE</code> sample weights are generated for the length of sequences based on
the frequencies of the training data with the method Inverse Class Frequency'. If <code>FALSE</code> each sequences length
has the weight 1.</p>
</dd>
</dl>

</div>



<h5>Details</h5>


<ul>
<li> <p><code>sc_max_k</code>: All values from sc_min_k up to sc_max_k are successively used. If
the number of <code>sc_max_k</code> is too high, the value is reduced to a number that allows the calculating of synthetic
units.
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;pl_anchor:&#8288;</code> With the help of this value, the new cases are sorted. For
this aim, the distance from the anchor is calculated and all cases are arranged into an ascending order.
</p>
</li></ul>




<h5>Returns</h5>

<p>Function does not return a value. It changes the object into a trained classifier.
</p>


<hr>
<a id="method-TEClassifierProtoNet-embed"></a>



<h4>Method <code>embed()</code></h4>

<p>Method for embedding documents. Please do not confuse this type of embeddings with the embeddings of
texts created by an object of class <a href="#topic+TextEmbeddingModel">TextEmbeddingModel</a>. These embeddings embed documents according to their
similarity to specific classes.
</p>


<h5>Usage</h5>

<div class="r"><pre>TEClassifierProtoNet$embed(embeddings_q = NULL, batch_size = 32)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>embeddings_q</code></dt><dd><p>Object of class <a href="#topic+EmbeddedText">EmbeddedText</a> or <a href="#topic+LargeDataSetForTextEmbeddings">LargeDataSetForTextEmbeddings</a> containing the text
embeddings for all cases which should be embedded into the classification space.</p>
</dd>
<dt><code>batch_size</code></dt><dd><p><code>int</code> batch size.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>Returns a <code>list</code> containing the following elements
</p>

<ul>
<li> <p><code>embeddings_q</code>: embeddings for the cases (query sample).
</p>
</li>
<li> <p><code>embeddings_prototypes</code>: embeddings of the prototypes which were learned during training. They represents the
center for the different classes.
</p>
</li></ul>



<hr>
<a id="method-TEClassifierProtoNet-plot_embeddings"></a>



<h4>Method <code>plot_embeddings()</code></h4>

<p>Method for creating a plot to visualize embeddings and their corresponding centers (prototypes).
</p>


<h5>Usage</h5>

<div class="r"><pre>TEClassifierProtoNet$plot_embeddings(
  embeddings_q,
  classes_q = NULL,
  batch_size = 12,
  alpha = 0.5,
  size_points = 3,
  size_points_prototypes = 8,
  inc_unlabeled = TRUE
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>embeddings_q</code></dt><dd><p>Object of class <a href="#topic+EmbeddedText">EmbeddedText</a> or <a href="#topic+LargeDataSetForTextEmbeddings">LargeDataSetForTextEmbeddings</a> containing the text
embeddings for all cases which should be embedded into the classification space.</p>
</dd>
<dt><code>classes_q</code></dt><dd><p>Named <code>factor</code> containg the true classes for every case. Please note that the names must match
the names/ids in <code>embeddings_q</code>.</p>
</dd>
<dt><code>batch_size</code></dt><dd><p><code>int</code> batch size.</p>
</dd>
<dt><code>alpha</code></dt><dd><p><code>float</code> Value indicating how transparent the points should be (important
if many points overlap). Does not apply to points representing prototypes.</p>
</dd>
<dt><code>size_points</code></dt><dd><p><code>int</code> Size of the points excluding the points for prototypes.</p>
</dd>
<dt><code>size_points_prototypes</code></dt><dd><p><code>int</code> Size of points representing prototypes.</p>
</dd>
<dt><code>inc_unlabeled</code></dt><dd><p><code>bool</code> If <code>TRUE</code> plot includes unlabeled cases as data points.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>Returns a plot of class <code>ggplot</code>visualizing embeddings.
</p>


<hr>
<a id="method-TEClassifierProtoNet-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>TEClassifierProtoNet$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>References</h3>

<p>Oreshkin, B. N., Rodriguez, P. &amp; Lacoste, A. (2018). TADAM: Task dependent adaptive metric for improved
few-shot learning. https://doi.org/10.48550/arXiv.1805.10123
</p>
<p>Snell, J., Swersky, K. &amp; Zemel, R. S. (2017). Prototypical Networks for Few-shot Learning.
https://doi.org/10.48550/arXiv.1703.05175
</p>
<p>Zhang, X., Nie, J., Zong, L., Yu, H. &amp; Liang, W. (2019). One Shot Learning with Margin. In Q. Yang, Z.-H.
Zhou, Z. Gong, M.-L. Zhang &amp; S.-J. Huang (Eds.), Lecture Notes in Computer Science. Advances in Knowledge Discovery
and Data Mining (Vol. 11440, pp. 305–317). Springer International Publishing.
https://doi.org/10.1007/978-3-030-16145-3_24
</p>


<h3>See Also</h3>

<p>Other Classification: 
<code><a href="#topic+TEClassifierRegular">TEClassifierRegular</a></code>
</p>

<hr>
<h2 id='TEClassifierRegular'>Text embedding classifier with a neural net</h2><span id='topic+TEClassifierRegular'></span>

<h3>Description</h3>

<p>Abstract class for neural nets with 'keras'/'tensorflow' and ' pytorch'.
</p>


<h3>Value</h3>

<p>Objects of this class are used for assigning texts to classes/categories. For the creation and training of a
classifier an object of class <a href="#topic+EmbeddedText">EmbeddedText</a> or <a href="#topic+LargeDataSetForTextEmbeddings">LargeDataSetForTextEmbeddings</a> on the one hand and a <a href="base.html#topic+factor">factor</a> on
the other hand are necessary.
</p>
<p>The object of class <a href="#topic+EmbeddedText">EmbeddedText</a> or <a href="#topic+LargeDataSetForTextEmbeddings">LargeDataSetForTextEmbeddings</a>  contains the numerical text representations
(text embeddings) of the raw texts generated by an object of class <a href="#topic+TextEmbeddingModel">TextEmbeddingModel</a>. For supporting large data
sets it is recommended to use <a href="#topic+LargeDataSetForTextEmbeddings">LargeDataSetForTextEmbeddings</a> instead of <a href="#topic+EmbeddedText">EmbeddedText</a>.
</p>
<p>The <code>factor</code> contains the classes/categories for every text. Missing values (unlabeled cases) are supported and can
be used for pseudo labeling.
</p>
<p>For predictions an object of class <a href="#topic+EmbeddedText">EmbeddedText</a> or <a href="#topic+LargeDataSetForTextEmbeddings">LargeDataSetForTextEmbeddings</a> has to be used which was
created with the same <a href="#topic+TextEmbeddingModel">TextEmbeddingModel</a> as for training.
</p>


<h3>Super class</h3>

<p><code><a href="#topic+AIFEBaseModel">aifeducation::AIFEBaseModel</a></code> -&gt; <code>TEClassifierRegular</code>
</p>


<h3>Public fields</h3>

<div class="r6-fields">

<dl>
<dt><code>feature_extractor</code></dt><dd><p>('list()')<br />
List for storing information and objects about the feature_extractor.</p>
</dd>
<dt><code>reliability</code></dt><dd><p>('list()')<br />
</p>
<p>List for storing central reliability measures of the last training.
</p>

<ul>
<li> <p><code>reliability$test_metric</code>: Array containing the reliability measures for the test data for
every fold and step (in case of pseudo-labeling).
</p>
</li>
<li> <p><code>reliability$test_metric_mean</code>: Array containing the reliability measures for the test data.
The values represent the mean values for every fold.
</p>
</li>
<li> <p><code>reliability$raw_iota_objects</code>: List containing all iota_object generated with the package <code>iotarelr</code>
for every fold at the end of the last training for the test data.
</p>
</li>
<li> <p><code>reliability$raw_iota_objects$iota_objects_end</code>: List of objects with class <code>iotarelr_iota2</code> containing the
estimated iota reliability of the second generation for the final model for every fold for the test data.
</p>
</li>
<li> <p><code>reliability$raw_iota_objects$iota_objects_end_free</code>: List of objects with class <code>iotarelr_iota2</code> containing
the estimated iota reliability of the second generation for the final model for every fold for the test data.
Please note that the model is estimated without forcing the Assignment Error Matrix to be in line with the
assumption of weak superiority.
</p>
</li>
<li> <p><code>reliability$iota_object_end</code>: Object of class <code>iotarelr_iota2</code> as a mean of the individual objects
for every fold for the test data.
</p>
</li>
<li> <p><code>reliability$iota_object_end_free</code>: Object of class <code>iotarelr_iota2</code> as a mean of the individual objects
for every fold. Please note that the model is estimated without forcing the Assignment Error Matrix to be in
line with the assumption of weak superiority.
</p>
</li>
<li> <p><code>reliability$standard_measures_end</code>: Object of class <code>list</code> containing the final measures for precision,
recall, and f1 for every fold.
</p>
</li>
<li> <p><code>reliability$standard_measures_mean</code>: <code>matrix</code> containing the mean measures for precision, recall, and f1.
</p>
</li></ul>
</dd>
</dl>

</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-TEClassifierRegular-configure"><code>TEClassifierRegular$configure()</code></a>
</p>
</li>
<li> <p><a href="#method-TEClassifierRegular-train"><code>TEClassifierRegular$train()</code></a>
</p>
</li>
<li> <p><a href="#method-TEClassifierRegular-predict"><code>TEClassifierRegular$predict()</code></a>
</p>
</li>
<li> <p><a href="#method-TEClassifierRegular-check_embedding_model"><code>TEClassifierRegular$check_embedding_model()</code></a>
</p>
</li>
<li> <p><a href="#method-TEClassifierRegular-check_feature_extractor_object_type"><code>TEClassifierRegular$check_feature_extractor_object_type()</code></a>
</p>
</li>
<li> <p><a href="#method-TEClassifierRegular-requires_compression"><code>TEClassifierRegular$requires_compression()</code></a>
</p>
</li>
<li> <p><a href="#method-TEClassifierRegular-save"><code>TEClassifierRegular$save()</code></a>
</p>
</li>
<li> <p><a href="#method-TEClassifierRegular-load_from_disk"><code>TEClassifierRegular$load_from_disk()</code></a>
</p>
</li>
<li> <p><a href="#method-TEClassifierRegular-clone"><code>TEClassifierRegular$clone()</code></a>
</p>
</li></ul>



<details><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="AIFEBaseModel" data-id="count_parameter"><a href='../../aifeducation/html/AIFEBaseModel.html#method-AIFEBaseModel-count_parameter'><code>aifeducation::AIFEBaseModel$count_parameter()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="AIFEBaseModel" data-id="get_all_fields"><a href='../../aifeducation/html/AIFEBaseModel.html#method-AIFEBaseModel-get_all_fields'><code>aifeducation::AIFEBaseModel$get_all_fields()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="AIFEBaseModel" data-id="get_documentation_license"><a href='../../aifeducation/html/AIFEBaseModel.html#method-AIFEBaseModel-get_documentation_license'><code>aifeducation::AIFEBaseModel$get_documentation_license()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="AIFEBaseModel" data-id="get_ml_framework"><a href='../../aifeducation/html/AIFEBaseModel.html#method-AIFEBaseModel-get_ml_framework'><code>aifeducation::AIFEBaseModel$get_ml_framework()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="AIFEBaseModel" data-id="get_model_description"><a href='../../aifeducation/html/AIFEBaseModel.html#method-AIFEBaseModel-get_model_description'><code>aifeducation::AIFEBaseModel$get_model_description()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="AIFEBaseModel" data-id="get_model_info"><a href='../../aifeducation/html/AIFEBaseModel.html#method-AIFEBaseModel-get_model_info'><code>aifeducation::AIFEBaseModel$get_model_info()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="AIFEBaseModel" data-id="get_model_license"><a href='../../aifeducation/html/AIFEBaseModel.html#method-AIFEBaseModel-get_model_license'><code>aifeducation::AIFEBaseModel$get_model_license()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="AIFEBaseModel" data-id="get_package_versions"><a href='../../aifeducation/html/AIFEBaseModel.html#method-AIFEBaseModel-get_package_versions'><code>aifeducation::AIFEBaseModel$get_package_versions()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="AIFEBaseModel" data-id="get_private"><a href='../../aifeducation/html/AIFEBaseModel.html#method-AIFEBaseModel-get_private'><code>aifeducation::AIFEBaseModel$get_private()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="AIFEBaseModel" data-id="get_publication_info"><a href='../../aifeducation/html/AIFEBaseModel.html#method-AIFEBaseModel-get_publication_info'><code>aifeducation::AIFEBaseModel$get_publication_info()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="AIFEBaseModel" data-id="get_sustainability_data"><a href='../../aifeducation/html/AIFEBaseModel.html#method-AIFEBaseModel-get_sustainability_data'><code>aifeducation::AIFEBaseModel$get_sustainability_data()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="AIFEBaseModel" data-id="get_text_embedding_model"><a href='../../aifeducation/html/AIFEBaseModel.html#method-AIFEBaseModel-get_text_embedding_model'><code>aifeducation::AIFEBaseModel$get_text_embedding_model()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="AIFEBaseModel" data-id="get_text_embedding_model_name"><a href='../../aifeducation/html/AIFEBaseModel.html#method-AIFEBaseModel-get_text_embedding_model_name'><code>aifeducation::AIFEBaseModel$get_text_embedding_model_name()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="AIFEBaseModel" data-id="is_configured"><a href='../../aifeducation/html/AIFEBaseModel.html#method-AIFEBaseModel-is_configured'><code>aifeducation::AIFEBaseModel$is_configured()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="AIFEBaseModel" data-id="load"><a href='../../aifeducation/html/AIFEBaseModel.html#method-AIFEBaseModel-load'><code>aifeducation::AIFEBaseModel$load()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="AIFEBaseModel" data-id="set_documentation_license"><a href='../../aifeducation/html/AIFEBaseModel.html#method-AIFEBaseModel-set_documentation_license'><code>aifeducation::AIFEBaseModel$set_documentation_license()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="AIFEBaseModel" data-id="set_model_description"><a href='../../aifeducation/html/AIFEBaseModel.html#method-AIFEBaseModel-set_model_description'><code>aifeducation::AIFEBaseModel$set_model_description()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="AIFEBaseModel" data-id="set_model_license"><a href='../../aifeducation/html/AIFEBaseModel.html#method-AIFEBaseModel-set_model_license'><code>aifeducation::AIFEBaseModel$set_model_license()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="AIFEBaseModel" data-id="set_publication_info"><a href='../../aifeducation/html/AIFEBaseModel.html#method-AIFEBaseModel-set_publication_info'><code>aifeducation::AIFEBaseModel$set_publication_info()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-TEClassifierRegular-configure"></a>



<h4>Method <code>configure()</code></h4>

<p>Creating a new instance of this class.
</p>


<h5>Usage</h5>

<div class="r"><pre>TEClassifierRegular$configure(
  ml_framework = "pytorch",
  name = NULL,
  label = NULL,
  text_embeddings = NULL,
  feature_extractor = NULL,
  target_levels = NULL,
  dense_size = 4,
  dense_layers = 0,
  rec_size = 4,
  rec_layers = 2,
  rec_type = "gru",
  rec_bidirectional = FALSE,
  self_attention_heads = 0,
  intermediate_size = NULL,
  attention_type = "fourier",
  add_pos_embedding = TRUE,
  rec_dropout = 0.1,
  repeat_encoder = 1,
  dense_dropout = 0.4,
  recurrent_dropout = 0.4,
  encoder_dropout = 0.1,
  optimizer = "adam"
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>ml_framework</code></dt><dd><p><code>string</code> Framework to use for training and inference. <code>ml_framework="tensorflow"</code> for
'tensorflow' and <code>ml_framework="pytorch"</code> for 'pytorch'</p>
</dd>
<dt><code>name</code></dt><dd><p><code>string</code> Name of the new classifier. Please refer to common name conventions. Free text can be used
with parameter <code>label</code>.</p>
</dd>
<dt><code>label</code></dt><dd><p><code>string</code> Label for the new classifier. Here you can use free text.</p>
</dd>
<dt><code>text_embeddings</code></dt><dd><p>An object of class <a href="#topic+EmbeddedText">EmbeddedText</a> or <a href="#topic+LargeDataSetForTextEmbeddings">LargeDataSetForTextEmbeddings</a>.</p>
</dd>
<dt><code>feature_extractor</code></dt><dd><p>Object of class <a href="#topic+TEFeatureExtractor">TEFeatureExtractor</a> which should be used in order to reduce the number
of dimensions of the text embeddings. If no feature extractor should be applied set <code>NULL</code>.</p>
</dd>
<dt><code>target_levels</code></dt><dd><p><code>vector</code> containing the levels (categories or classes) within the target data. Please not
that order matters. For ordinal data please ensure that the levels are sorted correctly with later levels
indicating a higher category/class. For nominal data the order does not matter.</p>
</dd>
<dt><code>dense_size</code></dt><dd><p><code>int</code> Number of neurons for each dense layer.</p>
</dd>
<dt><code>dense_layers</code></dt><dd><p><code>int</code> Number of dense layers.</p>
</dd>
<dt><code>rec_size</code></dt><dd><p><code>int</code> Number of neurons for each recurrent layer.</p>
</dd>
<dt><code>rec_layers</code></dt><dd><p><code>int</code> Number of recurrent layers.</p>
</dd>
<dt><code>rec_type</code></dt><dd><p><code>string</code> Type of the recurrent layers. <code>rec_type="gru"</code> for Gated Recurrent Unit and
<code>rec_type="lstm"</code> for Long Short-Term Memory.</p>
</dd>
<dt><code>rec_bidirectional</code></dt><dd><p><code>bool</code> If <code>TRUE</code> a bidirectional version of the recurrent layers is used.</p>
</dd>
<dt><code>self_attention_heads</code></dt><dd><p><code>int</code> determining the number of attention heads for a self-attention layer. Only
relevant if <code>attention_type="multihead"</code></p>
</dd>
<dt><code>intermediate_size</code></dt><dd><p><code>int</code> determining the size of the projection layer within a each transformer encoder.</p>
</dd>
<dt><code>attention_type</code></dt><dd><p><code>string</code> Choose the relevant attention type. Possible values are <code>fourier</code> and <code>multihead</code>. Please note
that you may see different values for a case for different input orders if you choose <code>fourier</code> on linux.</p>
</dd>
<dt><code>add_pos_embedding</code></dt><dd><p><code>bool</code> <code>TRUE</code> if positional embedding should be used.</p>
</dd>
<dt><code>rec_dropout</code></dt><dd><p><code>int</code> ranging between 0 and lower 1, determining the dropout between bidirectional recurrent
layers.</p>
</dd>
<dt><code>repeat_encoder</code></dt><dd><p><code>int</code> determining how many times the encoder should be added to the network.</p>
</dd>
<dt><code>dense_dropout</code></dt><dd><p><code>int</code> ranging between 0 and lower 1, determining the dropout between dense layers.</p>
</dd>
<dt><code>recurrent_dropout</code></dt><dd><p><code>int</code> ranging between 0 and lower 1, determining the recurrent dropout for each
recurrent layer. Only relevant for keras models.</p>
</dd>
<dt><code>encoder_dropout</code></dt><dd><p><code>int</code> ranging between 0 and lower 1, determining the dropout for the dense projection
within the encoder layers.</p>
</dd>
<dt><code>optimizer</code></dt><dd><p><code>string</code> <code>"adam"</code> or <code>"rmsprop"</code> .</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>Returns an object of class <a href="#topic+TEClassifierRegular">TEClassifierRegular</a> which is ready for training.
</p>


<hr>
<a id="method-TEClassifierRegular-train"></a>



<h4>Method <code>train()</code></h4>

<p>Method for training a neural net.
</p>
<p>Training includes a routine for early stopping. In the case that loss&lt;0.0001
and Accuracy=1.00 and Average Iota=1.00 training stops. The history uses the values
of the last trained epoch for the remaining epochs.
</p>
<p>After training the model with the best values for Average Iota, Accuracy, and Loss
on the validation data set is used as the final model.
</p>


<h5>Usage</h5>

<div class="r"><pre>TEClassifierRegular$train(
  data_embeddings,
  data_targets,
  data_folds = 5,
  data_val_size = 0.25,
  balance_class_weights = TRUE,
  balance_sequence_length = TRUE,
  use_sc = TRUE,
  sc_method = "dbsmote",
  sc_min_k = 1,
  sc_max_k = 10,
  use_pl = TRUE,
  pl_max_steps = 3,
  pl_max = 1,
  pl_anchor = 1,
  pl_min = 0,
  sustain_track = TRUE,
  sustain_iso_code = NULL,
  sustain_region = NULL,
  sustain_interval = 15,
  epochs = 40,
  batch_size = 32,
  dir_checkpoint,
  trace = TRUE,
  ml_trace = 1,
  log_dir = NULL,
  log_write_interval = 10,
  n_cores = auto_n_cores()
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>data_embeddings</code></dt><dd><p>Object of class <a href="#topic+EmbeddedText">EmbeddedText</a> or <a href="#topic+LargeDataSetForTextEmbeddings">LargeDataSetForTextEmbeddings</a>.</p>
</dd>
<dt><code>data_targets</code></dt><dd><p><code>factor</code> containing the labels for cases stored in <code>data_embeddings</code>. Factor must be named
and has to use the same names used in <code>data_embeddings</code>.</p>
</dd>
<dt><code>data_folds</code></dt><dd><p><code>int</code> determining the number of cross-fold samples.</p>
</dd>
<dt><code>data_val_size</code></dt><dd><p><code>double</code> between 0 and 1, indicating the proportion of cases of each class which should be
used for the validation sample during the estimation of the model. The remaining cases are part of the training
data.</p>
</dd>
<dt><code>balance_class_weights</code></dt><dd><p><code>bool</code> If <code>TRUE</code> class weights are generated based on the frequencies of the
training data with the method Inverse Class Frequency'. If <code>FALSE</code> each class has the weight 1.</p>
</dd>
<dt><code>balance_sequence_length</code></dt><dd><p><code>bool</code> If <code>TRUE</code> sample weights are generated for the length of sequences based on
the frequencies of the training data with the method Inverse Class Frequency'. If <code>FALSE</code> each sequences length
has the weight 1.</p>
</dd>
<dt><code>use_sc</code></dt><dd><p><code>bool</code> <code>TRUE</code> if the estimation should integrate synthetic cases. <code>FALSE</code> if not.</p>
</dd>
<dt><code>sc_method</code></dt><dd><p><code>vector</code> containing the method for generating synthetic cases. Possible are <code>sc_method="adas"</code>,
<code>sc_method="smote"</code>, and <code>sc_method="dbsmote"</code>.</p>
</dd>
<dt><code>sc_min_k</code></dt><dd><p><code>int</code> determining the minimal number of k which is used for creating synthetic units.</p>
</dd>
<dt><code>sc_max_k</code></dt><dd><p><code>int</code> determining the maximal number of k which is used for creating synthetic units.</p>
</dd>
<dt><code>use_pl</code></dt><dd><p><code>bool</code> <code>TRUE</code> if the estimation should integrate pseudo-labeling. <code>FALSE</code> if not.</p>
</dd>
<dt><code>pl_max_steps</code></dt><dd><p><code>int</code> determining the maximum number of steps during pseudo-labeling.</p>
</dd>
<dt><code>pl_max</code></dt><dd><p><code>double</code> between 0 and 1, setting the maximal level of confidence for considering a case for
pseudo-labeling.</p>
</dd>
<dt><code>pl_anchor</code></dt><dd><p><code>double</code> between 0 and 1 indicating the reference point for sorting the new cases of every
label. See notes for more details.</p>
</dd>
<dt><code>pl_min</code></dt><dd><p><code>double</code> between 0 and 1, setting the minimal level of confidence for considering a case for
pseudo-labeling.</p>
</dd>
<dt><code>sustain_track</code></dt><dd><p><code>bool</code> If <code>TRUE</code> energy consumption is tracked during training via the python library
'codecarbon'.</p>
</dd>
<dt><code>sustain_iso_code</code></dt><dd><p><code>string</code> ISO code (Alpha-3-Code) for the country. This variable must be set if
sustainability should be tracked. A list can be found on Wikipedia:
<a href="https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes">https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes</a>.</p>
</dd>
<dt><code>sustain_region</code></dt><dd><p>Region within a country. Only available for USA and Canada See the documentation of
codecarbon for more information. <a href="https://mlco2.github.io/codecarbon/parameters.html">https://mlco2.github.io/codecarbon/parameters.html</a></p>
</dd>
<dt><code>sustain_interval</code></dt><dd><p><code>int</code> Interval in seconds for measuring power usage.</p>
</dd>
<dt><code>epochs</code></dt><dd><p><code>int</code> Number of training epochs.</p>
</dd>
<dt><code>batch_size</code></dt><dd><p><code>int</code> Size of the batches for training.</p>
</dd>
<dt><code>dir_checkpoint</code></dt><dd><p><code>string</code> Path to the directory where the checkpoint during training should be saved. If the
directory does not exist, it is created.</p>
</dd>
<dt><code>trace</code></dt><dd><p><code>bool</code> <code>TRUE</code>, if information about the estimation phase should be printed to the console.</p>
</dd>
<dt><code>ml_trace</code></dt><dd><p><code>int</code> <code>ml_trace=0</code> does not print any information about the training process from pytorch on the
console.</p>
</dd>
<dt><code>log_dir</code></dt><dd><p><code>string</code> Path to the directory where the log files should be saved. If no logging is desired set
this argument to <code>NULL</code>.</p>
</dd>
<dt><code>log_write_interval</code></dt><dd><p><code>int</code> Time in seconds determining the interval in which the logger should try to update
the log files. Only relevant if <code>log_dir</code> is not <code>NULL</code>.</p>
</dd>
<dt><code>n_cores</code></dt><dd><p><code>int</code> Number of cores which should be used during the calculation of synthetic cases. Only relevant if
<code>use_sc=TRUE</code>.</p>
</dd>
</dl>

</div>



<h5>Details</h5>


<ul>
<li> <p><code>sc_max_k</code>: All values from sc_min_k up to sc_max_k are successively used. If
the number of sc_max_k is too high, the value is reduced to a number that allows the calculating of synthetic
units.
</p>
</li>
<li> <p><code>pl_anchor</code>: With the help of this value, the new cases are sorted. For
this aim, the distance from the anchor is calculated and all cases are arranged into an ascending order.
</p>
</li></ul>




<h5>Returns</h5>

<p>Function does not return a value. It changes the object into a trained classifier.
</p>


<hr>
<a id="method-TEClassifierRegular-predict"></a>



<h4>Method <code>predict()</code></h4>

<p>Method for predicting new data with a trained neural net.
</p>


<h5>Usage</h5>

<div class="r"><pre>TEClassifierRegular$predict(newdata, batch_size = 32, ml_trace = 1)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>newdata</code></dt><dd><p>Object of class <a href="#topic+TextEmbeddingModel">TextEmbeddingModel</a> or <a href="#topic+LargeDataSetForTextEmbeddings">LargeDataSetForTextEmbeddings</a> for which predictions
should be made. In addition, this method allows to use objects of class <code>array</code> and
<code>datasets.arrow_dataset.Dataset</code>. However, these should be used only by developers.</p>
</dd>
<dt><code>batch_size</code></dt><dd><p><code>int</code> Size of batches.</p>
</dd>
<dt><code>ml_trace</code></dt><dd><p><code>int</code> <code>ml_trace=0</code> does not print any information on the process from the machine learning
framework.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>Returns a <code>data.frame</code> containing the predictions and the probabilities of the different labels for each
case.
</p>


<hr>
<a id="method-TEClassifierRegular-check_embedding_model"></a>



<h4>Method <code>check_embedding_model()</code></h4>

<p>Method for checking if the provided text embeddings are created with the same <a href="#topic+TextEmbeddingModel">TextEmbeddingModel</a>
as the classifier.
</p>


<h5>Usage</h5>

<div class="r"><pre>TEClassifierRegular$check_embedding_model(
  text_embeddings,
  require_compressed = FALSE
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>text_embeddings</code></dt><dd><p>Object of class <a href="#topic+EmbeddedText">EmbeddedText</a> or <a href="#topic+LargeDataSetForTextEmbeddings">LargeDataSetForTextEmbeddings</a>.</p>
</dd>
<dt><code>require_compressed</code></dt><dd><p><code>TRUE</code> if a compressed version of the embeddings are necessary. Compressed embeddings
are created by an object of class <a href="#topic+TEFeatureExtractor">TEFeatureExtractor</a>.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p><code>TRUE</code> if the underlying <a href="#topic+TextEmbeddingModel">TextEmbeddingModel</a> is the same. <code>FALSE</code> if the models differ.
</p>


<hr>
<a id="method-TEClassifierRegular-check_feature_extractor_object_type"></a>



<h4>Method <code>check_feature_extractor_object_type()</code></h4>

<p>Method for checking an object of class <a href="#topic+TEFeatureExtractor">TEFeatureExtractor</a>.
</p>


<h5>Usage</h5>

<div class="r"><pre>TEClassifierRegular$check_feature_extractor_object_type(feature_extractor)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>feature_extractor</code></dt><dd><p>Object of class <a href="#topic+TEFeatureExtractor">TEFeatureExtractor</a></p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>This method does nothing returns. It raises an error if
</p>

<ul>
<li><p> the object is <code>NULL</code>
</p>
</li>
<li><p> the object does not rely on the same machine learning framework as the classifier
</p>
</li>
<li><p> the object is not trained.
</p>
</li></ul>



<hr>
<a id="method-TEClassifierRegular-requires_compression"></a>



<h4>Method <code>requires_compression()</code></h4>

<p>Method for checking if provided text embeddings must be compressed via a <a href="#topic+TEFeatureExtractor">TEFeatureExtractor</a> before
processing.
</p>


<h5>Usage</h5>

<div class="r"><pre>TEClassifierRegular$requires_compression(text_embeddings)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>text_embeddings</code></dt><dd><p>Object of class <a href="#topic+EmbeddedText">EmbeddedText</a>, <a href="#topic+LargeDataSetForTextEmbeddings">LargeDataSetForTextEmbeddings</a>, <code>array</code> or
<code>datasets.arrow_dataset.Dataset</code>.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>Return <code>TRUE</code> if a compression is necessary and <code>FALSE</code> if not.
</p>


<hr>
<a id="method-TEClassifierRegular-save"></a>



<h4>Method <code>save()</code></h4>

<p>Method for saving a model.
</p>


<h5>Usage</h5>

<div class="r"><pre>TEClassifierRegular$save(dir_path, folder_name)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>dir_path</code></dt><dd><p><code>string</code> Path of the directory where the model should be saved.</p>
</dd>
<dt><code>folder_name</code></dt><dd><p><code>string</code> Name of the folder that should be created within the directory.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>Function does not return a value. It saves the model to disk.
</p>


<hr>
<a id="method-TEClassifierRegular-load_from_disk"></a>



<h4>Method <code>load_from_disk()</code></h4>

<p>loads an object from disk and updates the object to the current version of the package.
</p>


<h5>Usage</h5>

<div class="r"><pre>TEClassifierRegular$load_from_disk(dir_path)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>dir_path</code></dt><dd><p>Path where the object set is stored.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>Method does not return anything. It loads an object from disk.
</p>


<hr>
<a id="method-TEClassifierRegular-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>TEClassifierRegular$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>See Also</h3>

<p>Other Classification: 
<code><a href="#topic+TEClassifierProtoNet">TEClassifierProtoNet</a></code>
</p>

<hr>
<h2 id='TEFeatureExtractor'>Feature extractor for reducing the number for dimensions of text embeddings.</h2><span id='topic+TEFeatureExtractor'></span>

<h3>Description</h3>

<p>Abstract class for auto encoders with 'pytorch'.
</p>


<h3>Value</h3>

<p>Objects of this class are used for reducing the number of dimensions of text embeddings created by an object
of class <a href="#topic+TextEmbeddingModel">TextEmbeddingModel</a>.
</p>
<p>For training an object of class <a href="#topic+EmbeddedText">EmbeddedText</a> or <a href="#topic+LargeDataSetForTextEmbeddings">LargeDataSetForTextEmbeddings</a> generated by an object of class
<a href="#topic+TextEmbeddingModel">TextEmbeddingModel</a> is necessary. Passing raw texts is not supported.
</p>
<p>For prediction an ob object class <a href="#topic+EmbeddedText">EmbeddedText</a> or <a href="#topic+LargeDataSetForTextEmbeddings">LargeDataSetForTextEmbeddings</a> is necessary that was generated
with the same <a href="#topic+TextEmbeddingModel">TextEmbeddingModel</a> as during training. Prediction outputs a new object of class <a href="#topic+EmbeddedText">EmbeddedText</a> or
<a href="#topic+LargeDataSetForTextEmbeddings">LargeDataSetForTextEmbeddings</a> which contains a text embedding with a lower number of dimensions.
</p>
<p>All models use tied weights for the encoder and decoder layers (except <code>method="lstm"</code>) and apply the estimation of
orthogonal weights. In addition, training tries to train the model to achieve uncorrelated features.
</p>
<p>Objects of class <a href="#topic+TEFeatureExtractor">TEFeatureExtractor</a> are designed to be used with classifiers such as <a href="#topic+TEClassifierRegular">TEClassifierRegular</a> and
<a href="#topic+TEClassifierProtoNet">TEClassifierProtoNet</a>.
</p>


<h3>Super class</h3>

<p><code><a href="#topic+AIFEBaseModel">aifeducation::AIFEBaseModel</a></code> -&gt; <code>TEFeatureExtractor</code>
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-TEFeatureExtractor-configure"><code>TEFeatureExtractor$configure()</code></a>
</p>
</li>
<li> <p><a href="#method-TEFeatureExtractor-train"><code>TEFeatureExtractor$train()</code></a>
</p>
</li>
<li> <p><a href="#method-TEFeatureExtractor-load_from_disk"><code>TEFeatureExtractor$load_from_disk()</code></a>
</p>
</li>
<li> <p><a href="#method-TEFeatureExtractor-extract_features"><code>TEFeatureExtractor$extract_features()</code></a>
</p>
</li>
<li> <p><a href="#method-TEFeatureExtractor-extract_features_large"><code>TEFeatureExtractor$extract_features_large()</code></a>
</p>
</li>
<li> <p><a href="#method-TEFeatureExtractor-is_trained"><code>TEFeatureExtractor$is_trained()</code></a>
</p>
</li>
<li> <p><a href="#method-TEFeatureExtractor-clone"><code>TEFeatureExtractor$clone()</code></a>
</p>
</li></ul>



<details><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="AIFEBaseModel" data-id="check_embedding_model"><a href='../../aifeducation/html/AIFEBaseModel.html#method-AIFEBaseModel-check_embedding_model'><code>aifeducation::AIFEBaseModel$check_embedding_model()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="AIFEBaseModel" data-id="count_parameter"><a href='../../aifeducation/html/AIFEBaseModel.html#method-AIFEBaseModel-count_parameter'><code>aifeducation::AIFEBaseModel$count_parameter()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="AIFEBaseModel" data-id="get_all_fields"><a href='../../aifeducation/html/AIFEBaseModel.html#method-AIFEBaseModel-get_all_fields'><code>aifeducation::AIFEBaseModel$get_all_fields()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="AIFEBaseModel" data-id="get_documentation_license"><a href='../../aifeducation/html/AIFEBaseModel.html#method-AIFEBaseModel-get_documentation_license'><code>aifeducation::AIFEBaseModel$get_documentation_license()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="AIFEBaseModel" data-id="get_ml_framework"><a href='../../aifeducation/html/AIFEBaseModel.html#method-AIFEBaseModel-get_ml_framework'><code>aifeducation::AIFEBaseModel$get_ml_framework()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="AIFEBaseModel" data-id="get_model_description"><a href='../../aifeducation/html/AIFEBaseModel.html#method-AIFEBaseModel-get_model_description'><code>aifeducation::AIFEBaseModel$get_model_description()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="AIFEBaseModel" data-id="get_model_info"><a href='../../aifeducation/html/AIFEBaseModel.html#method-AIFEBaseModel-get_model_info'><code>aifeducation::AIFEBaseModel$get_model_info()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="AIFEBaseModel" data-id="get_model_license"><a href='../../aifeducation/html/AIFEBaseModel.html#method-AIFEBaseModel-get_model_license'><code>aifeducation::AIFEBaseModel$get_model_license()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="AIFEBaseModel" data-id="get_package_versions"><a href='../../aifeducation/html/AIFEBaseModel.html#method-AIFEBaseModel-get_package_versions'><code>aifeducation::AIFEBaseModel$get_package_versions()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="AIFEBaseModel" data-id="get_private"><a href='../../aifeducation/html/AIFEBaseModel.html#method-AIFEBaseModel-get_private'><code>aifeducation::AIFEBaseModel$get_private()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="AIFEBaseModel" data-id="get_publication_info"><a href='../../aifeducation/html/AIFEBaseModel.html#method-AIFEBaseModel-get_publication_info'><code>aifeducation::AIFEBaseModel$get_publication_info()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="AIFEBaseModel" data-id="get_sustainability_data"><a href='../../aifeducation/html/AIFEBaseModel.html#method-AIFEBaseModel-get_sustainability_data'><code>aifeducation::AIFEBaseModel$get_sustainability_data()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="AIFEBaseModel" data-id="get_text_embedding_model"><a href='../../aifeducation/html/AIFEBaseModel.html#method-AIFEBaseModel-get_text_embedding_model'><code>aifeducation::AIFEBaseModel$get_text_embedding_model()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="AIFEBaseModel" data-id="get_text_embedding_model_name"><a href='../../aifeducation/html/AIFEBaseModel.html#method-AIFEBaseModel-get_text_embedding_model_name'><code>aifeducation::AIFEBaseModel$get_text_embedding_model_name()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="AIFEBaseModel" data-id="is_configured"><a href='../../aifeducation/html/AIFEBaseModel.html#method-AIFEBaseModel-is_configured'><code>aifeducation::AIFEBaseModel$is_configured()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="AIFEBaseModel" data-id="load"><a href='../../aifeducation/html/AIFEBaseModel.html#method-AIFEBaseModel-load'><code>aifeducation::AIFEBaseModel$load()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="AIFEBaseModel" data-id="save"><a href='../../aifeducation/html/AIFEBaseModel.html#method-AIFEBaseModel-save'><code>aifeducation::AIFEBaseModel$save()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="AIFEBaseModel" data-id="set_documentation_license"><a href='../../aifeducation/html/AIFEBaseModel.html#method-AIFEBaseModel-set_documentation_license'><code>aifeducation::AIFEBaseModel$set_documentation_license()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="AIFEBaseModel" data-id="set_model_description"><a href='../../aifeducation/html/AIFEBaseModel.html#method-AIFEBaseModel-set_model_description'><code>aifeducation::AIFEBaseModel$set_model_description()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="AIFEBaseModel" data-id="set_model_license"><a href='../../aifeducation/html/AIFEBaseModel.html#method-AIFEBaseModel-set_model_license'><code>aifeducation::AIFEBaseModel$set_model_license()</code></a></span></li>
<li><span class="pkg-link" data-pkg="aifeducation" data-topic="AIFEBaseModel" data-id="set_publication_info"><a href='../../aifeducation/html/AIFEBaseModel.html#method-AIFEBaseModel-set_publication_info'><code>aifeducation::AIFEBaseModel$set_publication_info()</code></a></span></li>
</ul>
</details>

<hr>
<a id="method-TEFeatureExtractor-configure"></a>



<h4>Method <code>configure()</code></h4>

<p>Creating a new instance of this class.
</p>


<h5>Usage</h5>

<div class="r"><pre>TEFeatureExtractor$configure(
  ml_framework = "pytorch",
  name = NULL,
  label = NULL,
  text_embeddings = NULL,
  features = 128,
  method = "lstm",
  noise_factor = 0.2,
  optimizer = "adam"
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>ml_framework</code></dt><dd><p><code>string</code> Framework to use for training and inference. Currently only <code>ml_framework="pytorch"</code>
is supported.</p>
</dd>
<dt><code>name</code></dt><dd><p><code>string</code> Name of the new classifier. Please refer to common name conventions. Free text can be used
with parameter <code>label</code>.</p>
</dd>
<dt><code>label</code></dt><dd><p><code>string</code> Label for the new classifier. Here you can use free text.</p>
</dd>
<dt><code>text_embeddings</code></dt><dd><p>An object of class <a href="#topic+EmbeddedText">EmbeddedText</a> or <a href="#topic+LargeDataSetForTextEmbeddings">LargeDataSetForTextEmbeddings</a>.</p>
</dd>
<dt><code>features</code></dt><dd><p><code>int</code> determining the number of dimensions to which the dimension of the text embedding should be
reduced.</p>
</dd>
<dt><code>method</code></dt><dd><p><code>string</code> Method to use for the feature extraction. <code>"lstm"</code> for an extractor based on LSTM-layers or
<code>"dense"</code> for dense layers.</p>
</dd>
<dt><code>noise_factor</code></dt><dd><p><code>double</code> between 0 and a value lower 1 indicating how much noise should be added for the
training of the feature extractor.</p>
</dd>
<dt><code>optimizer</code></dt><dd><p><code>string</code> <code>"adam"</code> or <code>"rmsprop"</code> .</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>Returns an object of class <a href="#topic+TEFeatureExtractor">TEFeatureExtractor</a> which is ready for training.
</p>


<hr>
<a id="method-TEFeatureExtractor-train"></a>



<h4>Method <code>train()</code></h4>

<p>Method for training a neural net.
</p>


<h5>Usage</h5>

<div class="r"><pre>TEFeatureExtractor$train(
  data_embeddings,
  data_val_size = 0.25,
  sustain_track = TRUE,
  sustain_iso_code = NULL,
  sustain_region = NULL,
  sustain_interval = 15,
  epochs = 40,
  batch_size = 32,
  dir_checkpoint,
  trace = TRUE,
  ml_trace = 1,
  log_dir = NULL,
  log_write_interval = 10
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>data_embeddings</code></dt><dd><p>Object of class <a href="#topic+EmbeddedText">EmbeddedText</a> or <a href="#topic+LargeDataSetForTextEmbeddings">LargeDataSetForTextEmbeddings</a>.</p>
</dd>
<dt><code>data_val_size</code></dt><dd><p><code>double</code> between 0 and 1, indicating the proportion of cases which should be used for the
validation sample.</p>
</dd>
<dt><code>sustain_track</code></dt><dd><p><code>bool</code> If <code>TRUE</code> energy consumption is tracked during training via the python library
'codecarbon'.</p>
</dd>
<dt><code>sustain_iso_code</code></dt><dd><p><code>string</code> ISO code (Alpha-3-Code) for the country. This variable must be set if
sustainability should be tracked. A list can be found on Wikipedia:
<a href="https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes">https://en.wikipedia.org/wiki/List_of_ISO_3166_country_codes</a>.</p>
</dd>
<dt><code>sustain_region</code></dt><dd><p>Region within a country. Only available for USA and Canada See the documentation of
'codecarbon' for more information. <a href="https://mlco2.github.io/codecarbon/parameters.html">https://mlco2.github.io/codecarbon/parameters.html</a></p>
</dd>
<dt><code>sustain_interval</code></dt><dd><p><code>int</code> Interval in seconds for measuring power usage.</p>
</dd>
<dt><code>epochs</code></dt><dd><p><code>int</code> Number of training epochs.</p>
</dd>
<dt><code>batch_size</code></dt><dd><p><code>int</code> Size of batches.</p>
</dd>
<dt><code>dir_checkpoint</code></dt><dd><p><code>string</code> Path to the directory where the checkpoint during training should be saved. If the
directory does not exist, it is created.</p>
</dd>
<dt><code>trace</code></dt><dd><p><code>bool</code> <code>TRUE</code>, if information about the estimation phase should be printed to the console.</p>
</dd>
<dt><code>ml_trace</code></dt><dd><p><code>int</code> <code>ml_trace=0</code> does not print any information about the training process from pytorch on
the console. <code>ml_trace=1</code> prints a progress bar.</p>
</dd>
<dt><code>log_dir</code></dt><dd><p><code>string</code> Path to the directory where the log files should be saved. If no logging is desired set
this argument to <code>NULL</code>.</p>
</dd>
<dt><code>log_write_interval</code></dt><dd><p><code>int</code> Time in seconds determining the interval in which the logger should try to update
the log files. Only relevant if <code>log_dir</code> is not <code>NULL</code>.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>Function does not return a value. It changes the object into a trained classifier.
</p>


<hr>
<a id="method-TEFeatureExtractor-load_from_disk"></a>



<h4>Method <code>load_from_disk()</code></h4>

<p>loads an object from disk and updates the object to the current version of the package.
</p>


<h5>Usage</h5>

<div class="r"><pre>TEFeatureExtractor$load_from_disk(dir_path)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>dir_path</code></dt><dd><p>Path where the object set is stored.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>Method does not return anything. It loads an object from disk.
</p>


<hr>
<a id="method-TEFeatureExtractor-extract_features"></a>



<h4>Method <code>extract_features()</code></h4>

<p>Method for extracting features. Applying this method reduces the number of dimensions of the text
embeddings. Please note that this method should only be used if a small number of cases should be compressed
since the data is loaded completely into memory. For a high number of cases please use the method
<code>extract_features_large</code>.
</p>


<h5>Usage</h5>

<div class="r"><pre>TEFeatureExtractor$extract_features(data_embeddings, batch_size)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>data_embeddings</code></dt><dd><p>Object of class <a href="#topic+EmbeddedText">EmbeddedText</a>,<a href="#topic+LargeDataSetForTextEmbeddings">LargeDataSetForTextEmbeddings</a>,
<code>datasets.arrow_dataset.Dataset</code> or <code>array</code> containing the text embeddings which should be reduced in their
dimensions.</p>
</dd>
<dt><code>batch_size</code></dt><dd><p><code>int</code> batch size.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>Returns an object of class <a href="#topic+EmbeddedText">EmbeddedText</a> containing the compressed embeddings.
</p>


<hr>
<a id="method-TEFeatureExtractor-extract_features_large"></a>



<h4>Method <code>extract_features_large()</code></h4>

<p>Method for extracting features from a large number of cases. Applying this method reduces the number
of dimensions of the text embeddings.
</p>


<h5>Usage</h5>

<div class="r"><pre>TEFeatureExtractor$extract_features_large(
  data_embeddings,
  batch_size,
  trace = FALSE
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>data_embeddings</code></dt><dd><p>Object of class <a href="#topic+EmbeddedText">EmbeddedText</a> or <a href="#topic+LargeDataSetForTextEmbeddings">LargeDataSetForTextEmbeddings</a> containing the text
embeddings which should be reduced in their dimensions.</p>
</dd>
<dt><code>batch_size</code></dt><dd><p><code>int</code> batch size.</p>
</dd>
<dt><code>trace</code></dt><dd><p><code>bool</code> If <code>TRUE</code> information about the progress is printed to the console.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>Returns an object of class <a href="#topic+LargeDataSetForTextEmbeddings">LargeDataSetForTextEmbeddings</a> containing the compressed embeddings.
</p>


<hr>
<a id="method-TEFeatureExtractor-is_trained"></a>



<h4>Method <code>is_trained()</code></h4>

<p>Check if the <a href="#topic+TEFeatureExtractor">TEFeatureExtractor</a> is trained.
</p>


<h5>Usage</h5>

<div class="r"><pre>TEFeatureExtractor$is_trained()</pre></div>



<h5>Returns</h5>

<p>Returns <code>TRUE</code> if the object is trained and <code>FALSE</code> if not.
</p>


<hr>
<a id="method-TEFeatureExtractor-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>TEFeatureExtractor$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>See Also</h3>

<p>Other Text Embedding: 
<code><a href="#topic+TextEmbeddingModel">TextEmbeddingModel</a></code>
</p>

<hr>
<h2 id='TextEmbeddingModel'>Text embedding model</h2><span id='topic+TextEmbeddingModel'></span>

<h3>Description</h3>

<p>This <code>R6</code> class stores a text embedding model which can be used to tokenize, encode, decode, and embed
raw texts. The object provides a unique interface for different text processing methods.
</p>


<h3>Value</h3>

<p>Objects of class <a href="#topic+TextEmbeddingModel">TextEmbeddingModel</a> transform raw texts into numerical representations which can be used
for downstream tasks. For this aim objects of this class allow to tokenize raw texts, to encode tokens to sequences
of integers, and to decode sequences of integers back to tokens.
</p>


<h3>Public fields</h3>

<div class="r6-fields">

<dl>
<dt><code>last_training</code></dt><dd><p>('list()')<br />
List for storing the history and the results of the last training. This
information will be overwritten if a new training is started.</p>
</dd>
<dt><code>tokenizer_statistics</code></dt><dd><p>('matrix()')<br />
Matrix containing the tokenizer statistics for the creation of the tokenizer
and all training runs according to Kaya &amp; Tantuğ (2024).
</p>
<p>Kaya, Y. B., &amp; Tantuğ, A. C. (2024). Effect of tokenization granularity for Turkish
large language models. Intelligent Systems with Applications, 21, 200335.
https://doi.org/10.1016/j.iswa.2024.200335</p>
</dd>
</dl>

</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-TextEmbeddingModel-configure"><code>TextEmbeddingModel$configure()</code></a>
</p>
</li>
<li> <p><a href="#method-TextEmbeddingModel-load_from_disk"><code>TextEmbeddingModel$load_from_disk()</code></a>
</p>
</li>
<li> <p><a href="#method-TextEmbeddingModel-load"><code>TextEmbeddingModel$load()</code></a>
</p>
</li>
<li> <p><a href="#method-TextEmbeddingModel-save"><code>TextEmbeddingModel$save()</code></a>
</p>
</li>
<li> <p><a href="#method-TextEmbeddingModel-encode"><code>TextEmbeddingModel$encode()</code></a>
</p>
</li>
<li> <p><a href="#method-TextEmbeddingModel-decode"><code>TextEmbeddingModel$decode()</code></a>
</p>
</li>
<li> <p><a href="#method-TextEmbeddingModel-get_special_tokens"><code>TextEmbeddingModel$get_special_tokens()</code></a>
</p>
</li>
<li> <p><a href="#method-TextEmbeddingModel-embed"><code>TextEmbeddingModel$embed()</code></a>
</p>
</li>
<li> <p><a href="#method-TextEmbeddingModel-embed_large"><code>TextEmbeddingModel$embed_large()</code></a>
</p>
</li>
<li> <p><a href="#method-TextEmbeddingModel-fill_mask"><code>TextEmbeddingModel$fill_mask()</code></a>
</p>
</li>
<li> <p><a href="#method-TextEmbeddingModel-set_publication_info"><code>TextEmbeddingModel$set_publication_info()</code></a>
</p>
</li>
<li> <p><a href="#method-TextEmbeddingModel-get_publication_info"><code>TextEmbeddingModel$get_publication_info()</code></a>
</p>
</li>
<li> <p><a href="#method-TextEmbeddingModel-set_model_license"><code>TextEmbeddingModel$set_model_license()</code></a>
</p>
</li>
<li> <p><a href="#method-TextEmbeddingModel-get_model_license"><code>TextEmbeddingModel$get_model_license()</code></a>
</p>
</li>
<li> <p><a href="#method-TextEmbeddingModel-set_documentation_license"><code>TextEmbeddingModel$set_documentation_license()</code></a>
</p>
</li>
<li> <p><a href="#method-TextEmbeddingModel-get_documentation_license"><code>TextEmbeddingModel$get_documentation_license()</code></a>
</p>
</li>
<li> <p><a href="#method-TextEmbeddingModel-set_model_description"><code>TextEmbeddingModel$set_model_description()</code></a>
</p>
</li>
<li> <p><a href="#method-TextEmbeddingModel-get_model_description"><code>TextEmbeddingModel$get_model_description()</code></a>
</p>
</li>
<li> <p><a href="#method-TextEmbeddingModel-get_model_info"><code>TextEmbeddingModel$get_model_info()</code></a>
</p>
</li>
<li> <p><a href="#method-TextEmbeddingModel-get_package_versions"><code>TextEmbeddingModel$get_package_versions()</code></a>
</p>
</li>
<li> <p><a href="#method-TextEmbeddingModel-get_basic_components"><code>TextEmbeddingModel$get_basic_components()</code></a>
</p>
</li>
<li> <p><a href="#method-TextEmbeddingModel-get_transformer_components"><code>TextEmbeddingModel$get_transformer_components()</code></a>
</p>
</li>
<li> <p><a href="#method-TextEmbeddingModel-get_sustainability_data"><code>TextEmbeddingModel$get_sustainability_data()</code></a>
</p>
</li>
<li> <p><a href="#method-TextEmbeddingModel-get_ml_framework"><code>TextEmbeddingModel$get_ml_framework()</code></a>
</p>
</li>
<li> <p><a href="#method-TextEmbeddingModel-count_parameter"><code>TextEmbeddingModel$count_parameter()</code></a>
</p>
</li>
<li> <p><a href="#method-TextEmbeddingModel-is_configured"><code>TextEmbeddingModel$is_configured()</code></a>
</p>
</li>
<li> <p><a href="#method-TextEmbeddingModel-get_private"><code>TextEmbeddingModel$get_private()</code></a>
</p>
</li>
<li> <p><a href="#method-TextEmbeddingModel-get_all_fields"><code>TextEmbeddingModel$get_all_fields()</code></a>
</p>
</li>
<li> <p><a href="#method-TextEmbeddingModel-clone"><code>TextEmbeddingModel$clone()</code></a>
</p>
</li></ul>


<hr>
<a id="method-TextEmbeddingModel-configure"></a>



<h4>Method <code>configure()</code></h4>

<p>Method for creating a new text embedding model
</p>


<h5>Usage</h5>

<div class="r"><pre>TextEmbeddingModel$configure(
  model_name = NULL,
  model_label = NULL,
  model_language = NULL,
  method = NULL,
  ml_framework = "pytorch",
  max_length = 0,
  chunks = 2,
  overlap = 0,
  emb_layer_min = "middle",
  emb_layer_max = "2_3_layer",
  emb_pool_type = "average",
  model_dir = NULL,
  trace = FALSE
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>model_name</code></dt><dd><p><code>string</code> containing the name of the new model.</p>
</dd>
<dt><code>model_label</code></dt><dd><p><code>string</code> containing the label/title of the new model.</p>
</dd>
<dt><code>model_language</code></dt><dd><p><code>string</code> containing the language which the model
represents (e.g., English).</p>
</dd>
<dt><code>method</code></dt><dd><p><code>string</code> determining the kind of embedding model. Currently
the following models are supported:
<code>method="bert"</code> for Bidirectional Encoder Representations from Transformers (BERT),
<code>method="roberta"</code> for A Robustly Optimized BERT Pretraining Approach (RoBERTa),
<code>method="longformer"</code> for Long-Document Transformer,
<code>method="funnel"</code> for Funnel-Transformer,
<code>method="deberta_v2"</code> for Decoding-enhanced BERT with Disentangled Attention (DeBERTa V2),
<code style="white-space: pre;">&#8288;method="glove"`` for GlobalVector Clusters, and &#8288;</code>method=&quot;lda&quot;' for topic modeling. See
details for more information.</p>
</dd>
<dt><code>ml_framework</code></dt><dd><p><code>string</code> Framework to use for the model.
<code>ml_framework="tensorflow"</code> for 'tensorflow' and <code>ml_framework="pytorch"</code>
for 'pytorch'. Only relevant for transformer models. To request bag-of-words model
set <code>ml_framework=NULL</code>.</p>
</dd>
<dt><code>max_length</code></dt><dd><p><code>int</code> determining the maximum length of token
sequences used in transformer models. Not relevant for the other methods.</p>
</dd>
<dt><code>chunks</code></dt><dd><p><code>int</code> Maximum number of chunks. Must be at least 2.</p>
</dd>
<dt><code>overlap</code></dt><dd><p><code>int</code> determining the number of tokens which should be added
at the beginning of the next chunk. Only relevant for transformer models.</p>
</dd>
<dt><code>emb_layer_min</code></dt><dd><p><code>int</code> or <code>string</code> determining the first layer to be included
in the creation of embeddings. An integer correspondents to the layer number. The first
layer has the number 1. Instead of an integer the following strings are possible:
<code>"start"</code> for the first layer, <code>"middle"</code> for the middle layer,
<code>"2_3_layer"</code> for the layer two-third layer, and <code>"last"</code> for the last layer.</p>
</dd>
<dt><code>emb_layer_max</code></dt><dd><p><code>int</code> or <code>string</code> determining the last layer to be included
in the creation of embeddings. An integer correspondents to the layer number. The first
layer has the number 1. Instead of an integer the following strings are possible:
<code>"start"</code> for the first layer, <code>"middle"</code> for the middle layer,
<code>"2_3_layer"</code> for the layer two-third layer, and <code>"last"</code> for the last layer.</p>
</dd>
<dt><code>emb_pool_type</code></dt><dd><p><code>string</code> determining the method for pooling the token embeddings
within each layer. If <code>"cls"</code> only the embedding of the CLS token is used. If
<code>"average"</code> the token embedding of all tokens are averaged (excluding padding tokens).
<code style="white-space: pre;">&#8288;"cls&#8288;</code> is not supported for <code>method="funnel"</code>.</p>
</dd>
<dt><code>model_dir</code></dt><dd><p><code>string</code> path to the directory where the
BERT model is stored.</p>
</dd>
<dt><code>trace</code></dt><dd><p><code>bool</code> <code>TRUE</code> prints information about the progress.
<code>FALSE</code> does not.</p>
</dd>
</dl>

</div>



<h5>Details</h5>

<p>In the case of any transformer (e.g.<code>method="bert"</code>,
<code>method="roberta"</code>, and <code>method="longformer"</code>),
a pretrained transformer model must be supplied via <code>model_dir</code>.
</p>



<h5>Returns</h5>

<p>Returns an object of class <a href="#topic+TextEmbeddingModel">TextEmbeddingModel</a>.
</p>


<hr>
<a id="method-TextEmbeddingModel-load_from_disk"></a>



<h4>Method <code>load_from_disk()</code></h4>

<p>loads an object from disk
and updates the object to the current version of the package.
</p>


<h5>Usage</h5>

<div class="r"><pre>TextEmbeddingModel$load_from_disk(dir_path)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>dir_path</code></dt><dd><p>Path where the object set is stored.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>Method does not return anything. It loads an object from disk.
</p>


<hr>
<a id="method-TextEmbeddingModel-load"></a>



<h4>Method <code>load()</code></h4>

<p>Method for loading a transformers model into R.
</p>


<h5>Usage</h5>

<div class="r"><pre>TextEmbeddingModel$load(dir_path)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>dir_path</code></dt><dd><p><code>string</code> containing the path to the relevant
model directory.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>Function does not return a value. It is used for loading a saved
transformer model into the R interface.
</p>


<hr>
<a id="method-TextEmbeddingModel-save"></a>



<h4>Method <code>save()</code></h4>

<p>Method for saving a transformer model on disk.Relevant
only for transformer models.
</p>


<h5>Usage</h5>

<div class="r"><pre>TextEmbeddingModel$save(dir_path, folder_name)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>dir_path</code></dt><dd><p><code>string</code> containing the path to the relevant
model directory.</p>
</dd>
<dt><code>folder_name</code></dt><dd><p><code>string</code> Name for the folder created within the directory.
This folder contains all model files.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>Function does not return a value. It is used for saving a transformer model
to disk.
</p>


<hr>
<a id="method-TextEmbeddingModel-encode"></a>



<h4>Method <code>encode()</code></h4>

<p>Method for encoding words of raw texts into integers.
</p>


<h5>Usage</h5>

<div class="r"><pre>TextEmbeddingModel$encode(
  raw_text,
  token_encodings_only = FALSE,
  to_int = TRUE,
  trace = FALSE
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>raw_text</code></dt><dd><p><code>vector</code>containing the raw texts.</p>
</dd>
<dt><code>token_encodings_only</code></dt><dd><p><code>bool</code> If <code>TRUE</code>, only the token
encodings are returned. If <code>FALSE</code>, the complete encoding is returned
which is important for some transformer models.</p>
</dd>
<dt><code>to_int</code></dt><dd><p><code>bool</code> If <code>TRUE</code> the integer ids of the tokens are
returned. If <code>FALSE</code> the tokens are returned. Argument only applies
for transformer models and if <code>token_encodings_only=TRUE</code>.</p>
</dd>
<dt><code>trace</code></dt><dd><p><code>bool</code> If <code>TRUE</code>, information of the progress
is printed. <code>FALSE</code> if not requested.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p><code>list</code> containing the integer or token sequences of the raw texts with
special tokens.
</p>


<hr>
<a id="method-TextEmbeddingModel-decode"></a>



<h4>Method <code>decode()</code></h4>

<p>Method for decoding a sequence of integers into tokens
</p>


<h5>Usage</h5>

<div class="r"><pre>TextEmbeddingModel$decode(int_seqence, to_token = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>int_seqence</code></dt><dd><p><code>list</code> containing the integer sequences which
should be transformed to tokens or plain text.</p>
</dd>
<dt><code>to_token</code></dt><dd><p><code>bool</code> If <code>FALSE</code> plain text is returned.
If <code>TRUE</code> a sequence of tokens is returned. Argument only relevant
if the model is based on a transformer.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p><code>list</code> of token sequences
</p>


<hr>
<a id="method-TextEmbeddingModel-get_special_tokens"></a>



<h4>Method <code>get_special_tokens()</code></h4>

<p>Method for receiving the special tokens of the model
</p>


<h5>Usage</h5>

<div class="r"><pre>TextEmbeddingModel$get_special_tokens()</pre></div>



<h5>Returns</h5>

<p>Returns a <code>matrix</code> containing the special tokens in the rows
and their type, token, and id in the columns.
</p>


<hr>
<a id="method-TextEmbeddingModel-embed"></a>



<h4>Method <code>embed()</code></h4>

<p>Method for creating text embeddings from raw texts.
This method should only be used if a small number of texts should be transformed
into text embeddings. For a large number of texts please use the method <code>embed_large</code>.
In the case of using a GPU and running out of memory while using 'tensorflow'  reduce the
batch size or restart R and switch to use cpu only via <code>set_config_cpu_only</code>. In general,
not relevant for 'pytorch'.
</p>


<h5>Usage</h5>

<div class="r"><pre>TextEmbeddingModel$embed(
  raw_text = NULL,
  doc_id = NULL,
  batch_size = 8,
  trace = FALSE,
  return_large_dataset = FALSE
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>raw_text</code></dt><dd><p><code>vector</code> containing the raw texts.</p>
</dd>
<dt><code>doc_id</code></dt><dd><p><code>vector</code> containing the corresponding IDs for every text.</p>
</dd>
<dt><code>batch_size</code></dt><dd><p><code>int</code> determining the maximal size of every batch.</p>
</dd>
<dt><code>trace</code></dt><dd><p><code>bool</code> <code>TRUE</code>, if information about the progression
should be printed on console.</p>
</dd>
<dt><code>return_large_dataset</code></dt><dd><p>'bool' If <code>TRUE</code> the retuned object is of class
<a href="#topic+LargeDataSetForTextEmbeddings">LargeDataSetForTextEmbeddings</a>. If <code>FALSE</code> it is of class <a href="#topic+EmbeddedText">EmbeddedText</a></p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>Method returns an object of class <a href="#topic+EmbeddedText">EmbeddedText</a> or <a href="#topic+LargeDataSetForTextEmbeddings">LargeDataSetForTextEmbeddings</a>. This object
contains the embeddings as a <a href="base.html#topic+data.frame">data.frame</a> and information about the
model creating the embeddings.
</p>


<hr>
<a id="method-TextEmbeddingModel-embed_large"></a>



<h4>Method <code>embed_large()</code></h4>

<p>Method for creating text embeddings from raw texts.
</p>


<h5>Usage</h5>

<div class="r"><pre>TextEmbeddingModel$embed_large(
  large_datas_set,
  batch_size = 32,
  trace = FALSE,
  log_file = NULL,
  log_write_interval = 2
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>large_datas_set</code></dt><dd><p>Object of class <a href="#topic+LargeDataSetForText">LargeDataSetForText</a> containing the
raw texts.</p>
</dd>
<dt><code>batch_size</code></dt><dd><p><code>int</code> determining the maximal size of every batch.</p>
</dd>
<dt><code>trace</code></dt><dd><p><code>bool</code> <code>TRUE</code>, if information about the progression
should be printed on console.</p>
</dd>
<dt><code>log_file</code></dt><dd><p><code>string</code> Path to the file where the log should be saved.
If no logging is desired set this argument to <code>NULL</code>.</p>
</dd>
<dt><code>log_write_interval</code></dt><dd><p><code>int</code> Time in seconds determining the interval in which
the logger should try to update the log files. Only relevant if <code>log_file</code> is not <code>NULL</code>.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>Method returns an object of class <a href="#topic+LargeDataSetForTextEmbeddings">LargeDataSetForTextEmbeddings</a>.
</p>


<hr>
<a id="method-TextEmbeddingModel-fill_mask"></a>



<h4>Method <code>fill_mask()</code></h4>

<p>Method for calculating tokens behind mask tokens.
</p>


<h5>Usage</h5>

<div class="r"><pre>TextEmbeddingModel$fill_mask(text, n_solutions = 5)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>text</code></dt><dd><p><code>string</code> Text containing mask tokens.</p>
</dd>
<dt><code>n_solutions</code></dt><dd><p><code>int</code> Number estimated tokens for every mask.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>Returns a <code>list</code> containing a <code>data.frame</code> for every
mask. The <code>data.frame</code> contains the solutions in the rows and reports
the score, token id, and token string in the columns.
</p>


<hr>
<a id="method-TextEmbeddingModel-set_publication_info"></a>



<h4>Method <code>set_publication_info()</code></h4>

<p>Method for setting the bibliographic information of the model.
</p>


<h5>Usage</h5>

<div class="r"><pre>TextEmbeddingModel$set_publication_info(type, authors, citation, url = NULL)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>type</code></dt><dd><p><code>string</code> Type of information which should be changed/added.
<code>developer</code>, and <code>modifier</code> are possible.</p>
</dd>
<dt><code>authors</code></dt><dd><p>List of people.</p>
</dd>
<dt><code>citation</code></dt><dd><p><code>string</code> Citation in free text.</p>
</dd>
<dt><code>url</code></dt><dd><p><code>string</code> Corresponding URL if applicable.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>Function does not return a value. It is used to set the private
members for publication information of the model.
</p>


<hr>
<a id="method-TextEmbeddingModel-get_publication_info"></a>



<h4>Method <code>get_publication_info()</code></h4>

<p>Method for getting the bibliographic information of the model.
</p>


<h5>Usage</h5>

<div class="r"><pre>TextEmbeddingModel$get_publication_info()</pre></div>



<h5>Returns</h5>

<p><code>list</code> of bibliographic information.
</p>


<hr>
<a id="method-TextEmbeddingModel-set_model_license"></a>



<h4>Method <code>set_model_license()</code></h4>

<p>Method for setting the license of the model
</p>


<h5>Usage</h5>

<div class="r"><pre>TextEmbeddingModel$set_model_license(license = "CC BY")</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>license</code></dt><dd><p><code>string</code> containing the abbreviation of the license or
the license text.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>Function does not return a value. It is used for setting the private
member for the software license of the model.
</p>


<hr>
<a id="method-TextEmbeddingModel-get_model_license"></a>



<h4>Method <code>get_model_license()</code></h4>

<p>Method for requesting the license of the model
</p>


<h5>Usage</h5>

<div class="r"><pre>TextEmbeddingModel$get_model_license()</pre></div>



<h5>Returns</h5>

<p><code>string</code> License of the model
</p>


<hr>
<a id="method-TextEmbeddingModel-set_documentation_license"></a>



<h4>Method <code>set_documentation_license()</code></h4>

<p>Method for setting the license of models' documentation.
</p>


<h5>Usage</h5>

<div class="r"><pre>TextEmbeddingModel$set_documentation_license(license = "CC BY")</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>license</code></dt><dd><p><code>string</code> containing the abbreviation of the license or
the license text.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>Function does not return a value. It is used to set the private member for the
documentation license of the model.
</p>


<hr>
<a id="method-TextEmbeddingModel-get_documentation_license"></a>



<h4>Method <code>get_documentation_license()</code></h4>

<p>Method for getting the license of the models' documentation.
</p>


<h5>Usage</h5>

<div class="r"><pre>TextEmbeddingModel$get_documentation_license()</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>license</code></dt><dd><p><code>string</code> containing the abbreviation of the license or
the license text.</p>
</dd>
</dl>

</div>


<hr>
<a id="method-TextEmbeddingModel-set_model_description"></a>



<h4>Method <code>set_model_description()</code></h4>

<p>Method for setting a description of the model
</p>


<h5>Usage</h5>

<div class="r"><pre>TextEmbeddingModel$set_model_description(
  eng = NULL,
  native = NULL,
  abstract_eng = NULL,
  abstract_native = NULL,
  keywords_eng = NULL,
  keywords_native = NULL
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>eng</code></dt><dd><p><code>string</code> A text describing the training of the classifier,
its theoretical and empirical background, and the different output labels
in English.</p>
</dd>
<dt><code>native</code></dt><dd><p><code>string</code> A text describing the training of the classifier,
its theoretical and empirical background, and the different output labels
in the native language of the model.</p>
</dd>
<dt><code>abstract_eng</code></dt><dd><p><code>string</code> A text providing a summary of the description
in English.</p>
</dd>
<dt><code>abstract_native</code></dt><dd><p><code>string</code> A text providing a summary of the description
in the native language of the classifier.</p>
</dd>
<dt><code>keywords_eng</code></dt><dd><p><code>vector</code>of keywords in English.</p>
</dd>
<dt><code>keywords_native</code></dt><dd><p><code>vector</code>of keywords in the native language of the classifier.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>Function does not return a value. It is used to set the private members for the
description of the model.
</p>


<hr>
<a id="method-TextEmbeddingModel-get_model_description"></a>



<h4>Method <code>get_model_description()</code></h4>

<p>Method for requesting the model description.
</p>


<h5>Usage</h5>

<div class="r"><pre>TextEmbeddingModel$get_model_description()</pre></div>



<h5>Returns</h5>

<p><code>list</code> with the description of the model in English
and the native language.
</p>


<hr>
<a id="method-TextEmbeddingModel-get_model_info"></a>



<h4>Method <code>get_model_info()</code></h4>

<p>Method for requesting the model information
</p>


<h5>Usage</h5>

<div class="r"><pre>TextEmbeddingModel$get_model_info()</pre></div>



<h5>Returns</h5>

<p><code>list</code> of all relevant model information
</p>


<hr>
<a id="method-TextEmbeddingModel-get_package_versions"></a>



<h4>Method <code>get_package_versions()</code></h4>

<p>Method for requesting a summary of the R and python packages'
versions used for creating the model.
</p>


<h5>Usage</h5>

<div class="r"><pre>TextEmbeddingModel$get_package_versions()</pre></div>



<h5>Returns</h5>

<p>Returns a <code>list</code> containing the versions of the relevant
R and python packages.
</p>


<hr>
<a id="method-TextEmbeddingModel-get_basic_components"></a>



<h4>Method <code>get_basic_components()</code></h4>

<p>Method for requesting the part of interface's configuration that is
necessary for all models.
</p>


<h5>Usage</h5>

<div class="r"><pre>TextEmbeddingModel$get_basic_components()</pre></div>



<h5>Returns</h5>

<p>Returns a <code>list</code>.
</p>


<hr>
<a id="method-TextEmbeddingModel-get_transformer_components"></a>



<h4>Method <code>get_transformer_components()</code></h4>

<p>Method for requesting the part of interface's configuration that is
necessary for transformer models.
</p>


<h5>Usage</h5>

<div class="r"><pre>TextEmbeddingModel$get_transformer_components()</pre></div>



<h5>Returns</h5>

<p>Returns a <code>list</code>.
</p>


<hr>
<a id="method-TextEmbeddingModel-get_sustainability_data"></a>



<h4>Method <code>get_sustainability_data()</code></h4>

<p>Method for requesting a log of tracked energy consumption
during training and an estimate of the resulting CO2 equivalents in kg.
</p>


<h5>Usage</h5>

<div class="r"><pre>TextEmbeddingModel$get_sustainability_data()</pre></div>



<h5>Returns</h5>

<p>Returns a <code>matrix</code> containing the tracked energy consumption,
CO2 equivalents in kg, information on the tracker used, and technical
information on the training infrastructure for every training run.
</p>


<hr>
<a id="method-TextEmbeddingModel-get_ml_framework"></a>



<h4>Method <code>get_ml_framework()</code></h4>

<p>Method for requesting the machine learning framework used
for the classifier.
</p>


<h5>Usage</h5>

<div class="r"><pre>TextEmbeddingModel$get_ml_framework()</pre></div>



<h5>Returns</h5>

<p>Returns a <code>string</code> describing the machine learning framework used
for the classifier.
</p>


<hr>
<a id="method-TextEmbeddingModel-count_parameter"></a>



<h4>Method <code>count_parameter()</code></h4>

<p>Method for counting the trainable parameters of a model.
</p>


<h5>Usage</h5>

<div class="r"><pre>TextEmbeddingModel$count_parameter(with_head = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>with_head</code></dt><dd><p><code>bool</code> If <code>TRUE</code> the number of parameters is returned including
the language modeling head of the model. If <code>FALSE</code> only the number of parameters of
the core model is returned.</p>
</dd>
</dl>

</div>



<h5>Returns</h5>

<p>Returns the number of trainable parameters of the model.
</p>


<hr>
<a id="method-TextEmbeddingModel-is_configured"></a>



<h4>Method <code>is_configured()</code></h4>

<p>Method for checking if the model was successfully configured.
An object can only be used if this value is <code>TRUE</code>.
</p>


<h5>Usage</h5>

<div class="r"><pre>TextEmbeddingModel$is_configured()</pre></div>



<h5>Returns</h5>

<p><code>bool</code> <code>TRUE</code> if the model is fully configured. <code>FALSE</code> if not.
</p>


<hr>
<a id="method-TextEmbeddingModel-get_private"></a>



<h4>Method <code>get_private()</code></h4>

<p>Method for requesting all private fields and methods. Used
for loading and updating an object.
</p>


<h5>Usage</h5>

<div class="r"><pre>TextEmbeddingModel$get_private()</pre></div>



<h5>Returns</h5>

<p>Returns a <code>list</code> with all private fields and methods.
</p>


<hr>
<a id="method-TextEmbeddingModel-get_all_fields"></a>



<h4>Method <code>get_all_fields()</code></h4>

<p>Return all fields.
</p>


<h5>Usage</h5>

<div class="r"><pre>TextEmbeddingModel$get_all_fields()</pre></div>



<h5>Returns</h5>

<p>Method returns a <code>list</code> containing all public and private fields
of the object.
</p>


<hr>
<a id="method-TextEmbeddingModel-clone"></a>



<h4>Method <code>clone()</code></h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>TextEmbeddingModel$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt><dd><p>Whether to make a deep clone.</p>
</dd>
</dl>

</div>




<h3>See Also</h3>

<p>Other Text Embedding: 
<code><a href="#topic+TEFeatureExtractor">TEFeatureExtractor</a></code>
</p>

<hr>
<h2 id='to_categorical_c'>Transforming classes to one-hot encoding</h2><span id='topic+to_categorical_c'></span>

<h3>Description</h3>

<p>Function written in C++ transforming a vector of classes (int) into
a binary class matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>to_categorical_c(class_vector, n_classes)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="to_categorical_c_+3A_class_vector">class_vector</code></td>
<td>
<p><code>vector</code> containing integers for every class. The
integers must range from 0 to n_classes-1.</p>
</td></tr>
<tr><td><code id="to_categorical_c_+3A_n_classes">n_classes</code></td>
<td>
<p><code>int</code> Total number of classes.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a <code>matrix</code> containing the binary representation for
every class.
</p>


<h3>See Also</h3>

<p>Other Auxiliary Functions: 
<code><a href="#topic+get_alpha_3_codes">get_alpha_3_codes</a>()</code>,
<code><a href="#topic+matrix_to_array_c">matrix_to_array_c</a>()</code>,
<code><a href="#topic+summarize_tracked_sustainability">summarize_tracked_sustainability</a>()</code>
</p>

<hr>
<h2 id='vignette_classifier'>Vignette classifier</h2><span id='topic+vignette_classifier'></span>

<h3>Description</h3>

<p>An object of class <a href="#topic+TEClassifierRegular">TEClassifierRegular</a> trained with the a subset of the Standford Movie Review
Dataset. The purpose of classifier is for illustration in vignettes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vignette_classifier
</code></pre>


<h3>Format</h3>

<p>R6
</p>

<hr>
<h2 id='vignette_classifier_ProtoNet'>Vignette classifier ProtoNet</h2><span id='topic+vignette_classifier_ProtoNet'></span>

<h3>Description</h3>

<p>An object of class <a href="#topic+TEClassifierProtoNet">TEClassifierProtoNet</a> trained with the a subset of the Standford Movie Review
Dataset. The purpose of classifier is for illustration in vignettes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vignette_classifier_ProtoNet
</code></pre>


<h3>Format</h3>

<p>R6
</p>

<hr>
<h2 id='vignette_classifier_sc_pl'>Vignette classifier trained with Synthetic Cases and Pseudo Labeling</h2><span id='topic+vignette_classifier_sc_pl'></span>

<h3>Description</h3>

<p>An object of class <a href="#topic+TEClassifierProtoNet">TEClassifierProtoNet</a> trained with the a subset of the Standford Movie Review
Dataset. The purpose of classifier is for illustration in vignettes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vignette_classifier_sc_pl
</code></pre>


<h3>Format</h3>

<p>R6
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
