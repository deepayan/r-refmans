<!DOCTYPE html><html><head><title>Help for package performance</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {performance}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#performance-package'><p>performance: An R Package for Assessment, Comparison and Testing of</p>
Statistical Models</a></li>
<li><a href='#binned_residuals'><p>Binned residuals for binomial logistic regression</p></a></li>
<li><a href='#check_autocorrelation'><p>Check model for independence of residuals.</p></a></li>
<li><a href='#check_clusterstructure'><p>Check suitability of data for clustering</p></a></li>
<li><a href='#check_collinearity'><p>Check for multicollinearity of model terms</p></a></li>
<li><a href='#check_convergence'><p>Convergence test for mixed effects models</p></a></li>
<li><a href='#check_distribution'><p>Classify the distribution of a model-family using machine learning</p></a></li>
<li><a href='#check_factorstructure'><p>Check suitability of data for Factor Analysis (FA) with Bartlett's Test of Sphericity and KMO</p></a></li>
<li><a href='#check_heterogeneity_bias'><p>Check model predictor for heterogeneity bias</p></a></li>
<li><a href='#check_heteroscedasticity'><p>Check model for (non-)constant error variance</p></a></li>
<li><a href='#check_homogeneity'><p>Check model for homogeneity of variances</p></a></li>
<li><a href='#check_itemscale'><p>Describe Properties of Item Scales</p></a></li>
<li><a href='#check_model'><p>Visual check of model assumptions</p></a></li>
<li><a href='#check_multimodal'><p>Check if a distribution is unimodal or multimodal</p></a></li>
<li><a href='#check_normality'><p>Check model for (non-)normality of residuals.</p></a></li>
<li><a href='#check_outliers'><p>Outliers detection (check for influential observations)</p></a></li>
<li><a href='#check_overdispersion'><p>Check overdispersion (and underdispersion) of GL(M)M's</p></a></li>
<li><a href='#check_predictions'><p>Posterior predictive checks</p></a></li>
<li><a href='#check_residuals'><p>Check uniformity of simulated residuals</p></a></li>
<li><a href='#check_singularity'><p>Check mixed models for boundary fits</p></a></li>
<li><a href='#check_sphericity'><p>Check model for violation of sphericity</p></a></li>
<li><a href='#check_symmetry'><p>Check distribution symmetry</p></a></li>
<li><a href='#check_zeroinflation'><p>Check for zero-inflation in count models</p></a></li>
<li><a href='#classify_distribution'><p>Classify the distribution of a model-family using machine learning</p></a></li>
<li><a href='#compare_performance'><p>Compare performance of different models</p></a></li>
<li><a href='#cronbachs_alpha'><p>Cronbach's Alpha for Items or Scales</p></a></li>
<li><a href='#display.performance_model'><p>Print tables in different output formats</p></a></li>
<li><a href='#icc'><p>Intraclass Correlation Coefficient (ICC)</p></a></li>
<li><a href='#item_difficulty'><p>Difficulty of Questionnaire Items</p></a></li>
<li><a href='#item_discrimination'><p>Discrimination of Questionnaire Items</p></a></li>
<li><a href='#item_intercor'><p>Mean Inter-Item-Correlation</p></a></li>
<li><a href='#item_reliability'><p>Reliability Test for Items or Scales</p></a></li>
<li><a href='#item_split_half'><p>Split-Half Reliability</p></a></li>
<li><a href='#looic'><p>LOO-related Indices for Bayesian regressions.</p></a></li>
<li><a href='#model_performance'><p>Model Performance</p></a></li>
<li><a href='#model_performance.ivreg'><p>Performance of instrumental variable regression models</p></a></li>
<li><a href='#model_performance.kmeans'><p>Model summary for k-means clustering</p></a></li>
<li><a href='#model_performance.lavaan'><p>Performance of lavaan SEM / CFA Models</p></a></li>
<li><a href='#model_performance.lm'><p>Performance of Regression Models</p></a></li>
<li><a href='#model_performance.merMod'><p>Performance of Mixed Models</p></a></li>
<li><a href='#model_performance.rma'><p>Performance of Meta-Analysis Models</p></a></li>
<li><a href='#model_performance.stanreg'><p>Performance of Bayesian Models</p></a></li>
<li><a href='#performance_accuracy'><p>Accuracy of predictions from model fit</p></a></li>
<li><a href='#performance_aicc'><p>Compute the AIC or second-order AIC</p></a></li>
<li><a href='#performance_cv'><p>Cross-validated model performance</p></a></li>
<li><a href='#performance_hosmer'><p>Hosmer-Lemeshow goodness-of-fit test</p></a></li>
<li><a href='#performance_logloss'><p>Log Loss</p></a></li>
<li><a href='#performance_mae'><p>Mean Absolute Error of Models</p></a></li>
<li><a href='#performance_mse'><p>Mean Square Error of Linear Models</p></a></li>
<li><a href='#performance_pcp'><p>Percentage of Correct Predictions</p></a></li>
<li><a href='#performance_rmse'><p>Root Mean Squared Error</p></a></li>
<li><a href='#performance_roc'><p>Simple ROC curve</p></a></li>
<li><a href='#performance_rse'><p>Residual Standard Error for Linear Models</p></a></li>
<li><a href='#performance_score'><p>Proper Scoring Rules</p></a></li>
<li><a href='#r2'><p>Compute the model's R2</p></a></li>
<li><a href='#r2_bayes'><p>Bayesian R2</p></a></li>
<li><a href='#r2_coxsnell'><p>Cox &amp; Snell's R2</p></a></li>
<li><a href='#r2_efron'><p>Efron's R2</p></a></li>
<li><a href='#r2_kullback'><p>Kullback-Leibler R2</p></a></li>
<li><a href='#r2_loo'><p>LOO-adjusted R2</p></a></li>
<li><a href='#r2_mcfadden'><p>McFadden's R2</p></a></li>
<li><a href='#r2_mckelvey'><p>McKelvey &amp; Zavoinas R2</p></a></li>
<li><a href='#r2_nagelkerke'><p>Nagelkerke's R2</p></a></li>
<li><a href='#r2_nakagawa'><p>Nakagawa's R2 for mixed models</p></a></li>
<li><a href='#r2_somers'><p>Somers' Dxy rank correlation for binary outcomes</p></a></li>
<li><a href='#r2_tjur'><p>Tjur's R2 - coefficient of determination (D)</p></a></li>
<li><a href='#r2_xu'><p>Xu' R2 (Omega-squared)</p></a></li>
<li><a href='#r2_zeroinflated'><p>R2 for models with zero-inflation</p></a></li>
<li><a href='#reexports'><p>Objects exported from other packages</p></a></li>
<li><a href='#simulate_residuals'><p>Simulate randomized quantile residuals from a model</p></a></li>
<li><a href='#test_bf'><p>Test if models are different</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Assessment of Regression Models Performance</td>
</tr>
<tr>
<td>Version:</td>
<td>0.11.0</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Daniel Lüdecke &lt;d.luedecke@uke.de&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Utilities for computing measures to assess model quality,
    which are not directly provided by R's 'base' or 'stats' packages.
    These include e.g. measures like r-squared, intraclass correlation
    coefficient (Nakagawa, Johnson &amp; Schielzeth (2017)
    &lt;<a href="https://doi.org/10.1098%2Frsif.2017.0213">doi:10.1098/rsif.2017.0213</a>&gt;), root mean squared error or functions to
    check models for overdispersion, singularity or zero-inflation and
    more. Functions apply to a large variety of regression models,
    including generalized linear models, mixed effects models and Bayesian
    models. References: Lüdecke et al. (2021) &lt;<a href="https://doi.org/10.21105%2Fjoss.03139">doi:10.21105/joss.03139</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://easystats.github.io/performance/">https://easystats.github.io/performance/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/easystats/performance/issues">https://github.com/easystats/performance/issues</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.6)</td>
</tr>
<tr>
<td>Imports:</td>
<td>bayestestR (&ge; 0.13.2), insight (&ge; 0.19.9), datawizard (&ge;
0.9.1), stats, utils</td>
</tr>
<tr>
<td>Suggests:</td>
<td>AER, afex, BayesFactor, bayesplot, betareg, bigutilsr,
blavaan, boot, brms, car, carData, CompQuadForm, correlation,
cplm, dbscan, DHARMa, estimatr, fixest, flextable, forecast,
ftExtra, gamm4, ggplot2, glmmTMB, graphics, Hmisc, httr, ICS,
ICSOutlier, ISLR, ivreg, lavaan, lme4, lmtest, loo, MASS,
Matrix, mclogit, mclust, metadat, metafor, mgcv, mlogit,
multimode, nestedLogit, nlme, nonnest2, ordinal, parallel,
parameters (&ge; 0.21.4), patchwork, pscl, psych, quantreg,
qqplotr (&ge; 0.0.6), randomForest, rempsyc, rmarkdown, rstanarm,
rstantools, sandwich, see (&ge; 0.8.2), survey, survival,
testthat (&ge; 3.2.1), tweedie, VGAM, withr (&ge; 3.0.0)</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Language:</td>
<td>en-US</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.1</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>Config/testthat/parallel:</td>
<td>true</td>
</tr>
<tr>
<td>Config/Needs/website:</td>
<td>rstudio/bslib, r-lib/pkgdown,
easystats/easystatstemplate</td>
</tr>
<tr>
<td>Config/rcmdcheck/ignore-inconsequential-notes:</td>
<td>true</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-03-22 21:30:42 UTC; DL</td>
</tr>
<tr>
<td>Author:</td>
<td>Daniel Lüdecke <a href="https://orcid.org/0000-0002-8895-3206"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre] (@strengejacke),
  Dominique Makowski
    <a href="https://orcid.org/0000-0001-5375-9967"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut, ctb]
    (@Dom_Makowski),
  Mattan S. Ben-Shachar
    <a href="https://orcid.org/0000-0002-4287-4801"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut, ctb]
    (@mattansb),
  Indrajeet Patil <a href="https://orcid.org/0000-0003-1995-6531"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, ctb] (@patilindrajeets),
  Philip Waggoner <a href="https://orcid.org/0000-0002-7825-7573"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, ctb],
  Brenton M. Wiernik
    <a href="https://orcid.org/0000-0001-9560-6336"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut, ctb]
    (@bmwiernik),
  Rémi Thériault <a href="https://orcid.org/0000-0003-4315-6788"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, ctb] (@rempsyc),
  Vincent Arel-Bundock
    <a href="https://orcid.org/0000-0003-2042-7063"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [ctb],
  Martin Jullum [rev],
  gjo11 [rev],
  Etienne Bacher <a href="https://orcid.org/0000-0002-9271-5075"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [ctb]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-03-22 22:20:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='performance-package'>performance: An R Package for Assessment, Comparison and Testing of
Statistical Models</h2><span id='topic+performance-package'></span>

<h3>Description</h3>

<p>A crucial aspect when building regression models is to evaluate the
quality of modelfit. It is important to investigate how well models fit
to the data and which fit indices to report. Functions to create
diagnostic plots or to compute fit measures do exist, however, mostly
spread over different packages. There is no unique and consistent
approach to assess the model quality for different kind of models.
</p>
<p>The primary goal of the <strong>performance</strong> package is to fill this gap and
to provide utilities for computing <strong>indices of model quality</strong> and
<strong>goodness of fit</strong>. These include measures like r-squared (R2), root
mean squared error (RMSE) or intraclass correlation coefficient (ICC),
but also functions to check (mixed) models for overdispersion,
zero-inflation, convergence or singularity.
</p>
<p>References: Lüdecke et al. (2021) <a href="https://doi.org/10.21105/joss.03139">doi:10.21105/joss.03139</a>
</p>


<h3>Details</h3>

<p><code>performance-package</code>
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Daniel Lüdecke <a href="mailto:d.luedecke@uke.de">d.luedecke@uke.de</a> (<a href="https://orcid.org/0000-0002-8895-3206">ORCID</a>) (@strengejacke)
</p>
<p>Authors:
</p>

<ul>
<li><p> Dominique Makowski <a href="mailto:dom.makowski@gmail.com">dom.makowski@gmail.com</a> (<a href="https://orcid.org/0000-0001-5375-9967">ORCID</a>) (@Dom_Makowski) [contributor]
</p>
</li>
<li><p> Mattan S. Ben-Shachar <a href="mailto:matanshm@post.bgu.ac.il">matanshm@post.bgu.ac.il</a> (<a href="https://orcid.org/0000-0002-4287-4801">ORCID</a>) (@mattansb) [contributor]
</p>
</li>
<li><p> Indrajeet Patil <a href="mailto:patilindrajeet.science@gmail.com">patilindrajeet.science@gmail.com</a> (<a href="https://orcid.org/0000-0003-1995-6531">ORCID</a>) (@patilindrajeets) [contributor]
</p>
</li>
<li><p> Philip Waggoner <a href="mailto:philip.waggoner@gmail.com">philip.waggoner@gmail.com</a> (<a href="https://orcid.org/0000-0002-7825-7573">ORCID</a>) [contributor]
</p>
</li>
<li><p> Brenton M. Wiernik <a href="mailto:brenton@wiernik.org">brenton@wiernik.org</a> (<a href="https://orcid.org/0000-0001-9560-6336">ORCID</a>) (@bmwiernik) [contributor]
</p>
</li>
<li><p> Rémi Thériault <a href="mailto:remi.theriault@mail.mcgill.ca">remi.theriault@mail.mcgill.ca</a> (<a href="https://orcid.org/0000-0003-4315-6788">ORCID</a>) (@rempsyc) [contributor]
</p>
</li></ul>

<p>Other contributors:
</p>

<ul>
<li><p> Vincent Arel-Bundock <a href="mailto:vincent.arel-bundock@umontreal.ca">vincent.arel-bundock@umontreal.ca</a> (<a href="https://orcid.org/0000-0003-2042-7063">ORCID</a>) [contributor]
</p>
</li>
<li><p> Martin Jullum [reviewer]
</p>
</li>
<li><p> gjo11 [reviewer]
</p>
</li>
<li><p> Etienne Bacher <a href="mailto:etienne.bacher@protonmail.com">etienne.bacher@protonmail.com</a> (<a href="https://orcid.org/0000-0002-9271-5075">ORCID</a>) [contributor]
</p>
</li></ul>



<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://easystats.github.io/performance/">https://easystats.github.io/performance/</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/easystats/performance/issues">https://github.com/easystats/performance/issues</a>
</p>
</li></ul>


<hr>
<h2 id='binned_residuals'>Binned residuals for binomial logistic regression</h2><span id='topic+binned_residuals'></span>

<h3>Description</h3>

<p>Check model quality of binomial logistic regression models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>binned_residuals(
  model,
  term = NULL,
  n_bins = NULL,
  show_dots = NULL,
  ci = 0.95,
  ci_type = c("exact", "gaussian", "boot"),
  residuals = c("deviance", "pearson", "response"),
  iterations = 1000,
  verbose = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="binned_residuals_+3A_model">model</code></td>
<td>
<p>A <code>glm</code>-object with <em>binomial</em>-family.</p>
</td></tr>
<tr><td><code id="binned_residuals_+3A_term">term</code></td>
<td>
<p>Name of independent variable from <code>x</code>. If not <code>NULL</code>,
average residuals for the categories of <code>term</code> are plotted; else,
average residuals for the estimated probabilities of the response are
plotted.</p>
</td></tr>
<tr><td><code id="binned_residuals_+3A_n_bins">n_bins</code></td>
<td>
<p>Numeric, the number of bins to divide the data. If
<code>n_bins = NULL</code>, the square root of the number of observations is
taken.</p>
</td></tr>
<tr><td><code id="binned_residuals_+3A_show_dots">show_dots</code></td>
<td>
<p>Logical, if <code>TRUE</code>, will show data points in the plot. Set
to <code>FALSE</code> for models with many observations, if generating the plot is too
time-consuming. By default, <code>show_dots = NULL</code>. In this case <code>binned_residuals()</code>
tries to guess whether performance will be poor due to a very large model
and thus automatically shows or hides dots.</p>
</td></tr>
<tr><td><code id="binned_residuals_+3A_ci">ci</code></td>
<td>
<p>Numeric, the confidence level for the error bounds.</p>
</td></tr>
<tr><td><code id="binned_residuals_+3A_ci_type">ci_type</code></td>
<td>
<p>Character, the type of error bounds to calculate. Can be
<code>"exact"</code> (default), <code>"gaussian"</code> or <code>"boot"</code>. <code>"exact"</code> calculates the
error bounds based on the exact binomial distribution, using <code><a href="stats.html#topic+binom.test">binom.test()</a></code>.
<code>"gaussian"</code> uses the Gaussian approximation, while <code>"boot"</code> uses a simple
bootstrap method, where confidence intervals are calculated based on the
quantiles of the bootstrap distribution.</p>
</td></tr>
<tr><td><code id="binned_residuals_+3A_residuals">residuals</code></td>
<td>
<p>Character, the type of residuals to calculate. Can be
<code>"deviance"</code> (default), <code>"pearson"</code> or <code>"response"</code>. It is recommended to
use <code>"response"</code> only for those models where other residuals are not
available.</p>
</td></tr>
<tr><td><code id="binned_residuals_+3A_iterations">iterations</code></td>
<td>
<p>Integer, the number of iterations to use for the
bootstrap method. Only used if <code>ci_type = "boot"</code>.</p>
</td></tr>
<tr><td><code id="binned_residuals_+3A_verbose">verbose</code></td>
<td>
<p>Toggle warnings and messages.</p>
</td></tr>
<tr><td><code id="binned_residuals_+3A_...">...</code></td>
<td>
<p>Currently not used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Binned residual plots are achieved by &quot;dividing the data into
categories (bins) based on their fitted values, and then plotting
the average residual versus the average fitted value for each bin.&quot;
<em>(Gelman, Hill 2007: 97)</em>. If the model were true, one would
expect about 95% of the residuals to fall inside the error bounds.
</p>
<p>If <code>term</code> is not <code>NULL</code>, one can compare the residuals in
relation to a specific model predictor. This may be helpful to check if a
term would fit better when transformed, e.g. a rising and falling pattern
of residuals along the x-axis is a signal to consider taking the logarithm
of the predictor (cf. Gelman and Hill 2007, pp. 97-98).
</p>


<h3>Value</h3>

<p>A data frame representing the data that is mapped in the accompanying
plot. In case all residuals are inside the error bounds, points are black.
If some of the residuals are outside the error bounds (indicated by the
grey-shaded area), blue points indicate residuals that are OK, while red
points indicate model under- or over-fitting for the relevant range of
estimated probabilities.
</p>


<h3>Note</h3>

<p><code>binned_residuals()</code> returns a data frame, however, the <code>print()</code>
method only returns a short summary of the result. The data frame itself
is used for plotting. The <code>plot()</code> method, in turn, creates a ggplot-object.
</p>


<h3>References</h3>

<p>Gelman, A., and Hill, J. (2007). Data analysis using regression and
multilevel/hierarchical models. Cambridge; New York: Cambridge University
Press.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>model &lt;- glm(vs ~ wt + mpg, data = mtcars, family = "binomial")
result &lt;- binned_residuals(model)
result

# look at the data frame
as.data.frame(result)


# plot
if (require("see")) {
  plot(result, show_dots = TRUE)
}


</code></pre>

<hr>
<h2 id='check_autocorrelation'>Check model for independence of residuals.</h2><span id='topic+check_autocorrelation'></span><span id='topic+check_autocorrelation.default'></span>

<h3>Description</h3>

<p>Check model for independence of residuals, i.e. for autocorrelation
of error terms.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_autocorrelation(x, ...)

## Default S3 method:
check_autocorrelation(x, nsim = 1000, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="check_autocorrelation_+3A_x">x</code></td>
<td>
<p>A model object.</p>
</td></tr>
<tr><td><code id="check_autocorrelation_+3A_...">...</code></td>
<td>
<p>Currently not used.</p>
</td></tr>
<tr><td><code id="check_autocorrelation_+3A_nsim">nsim</code></td>
<td>
<p>Number of simulations for the Durbin-Watson-Test.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Performs a Durbin-Watson-Test to check for autocorrelated residuals.
In case of autocorrelation, robust standard errors return more accurate
results for the estimates, or maybe a mixed model with error term for the
cluster groups should be used.
</p>


<h3>Value</h3>

<p>Invisibly returns the p-value of the test statistics. A p-value &lt; 0.05
indicates autocorrelated residuals.
</p>


<h3>See Also</h3>

<p>Other functions to check model assumptions and and assess model quality: 
<code><a href="#topic+check_collinearity">check_collinearity</a>()</code>,
<code><a href="#topic+check_convergence">check_convergence</a>()</code>,
<code><a href="#topic+check_heteroscedasticity">check_heteroscedasticity</a>()</code>,
<code><a href="#topic+check_homogeneity">check_homogeneity</a>()</code>,
<code><a href="#topic+check_model">check_model</a>()</code>,
<code><a href="#topic+check_outliers">check_outliers</a>()</code>,
<code><a href="#topic+check_overdispersion">check_overdispersion</a>()</code>,
<code><a href="#topic+check_predictions">check_predictions</a>()</code>,
<code><a href="#topic+check_singularity">check_singularity</a>()</code>,
<code><a href="#topic+check_zeroinflation">check_zeroinflation</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>m &lt;- lm(mpg ~ wt + cyl + gear + disp, data = mtcars)
check_autocorrelation(m)
</code></pre>

<hr>
<h2 id='check_clusterstructure'>Check suitability of data for clustering</h2><span id='topic+check_clusterstructure'></span>

<h3>Description</h3>

<p>This checks whether the data is appropriate for clustering using the Hopkins'
H statistic of given data. If the value of Hopkins statistic is close to 0
(below 0.5), then we can reject the null hypothesis and conclude that the
dataset is significantly clusterable. A value for H lower than 0.25 indicates
a clustering tendency at the <code style="white-space: pre;">&#8288;90%&#8288;</code> confidence level. The visual assessment of
cluster tendency (VAT) approach (Bezdek and Hathaway, 2002) consists in
investigating the heatmap of the ordered dissimilarity matrix. Following
this, one can potentially detect the clustering tendency by counting the
number of square shaped blocks along the diagonal.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_clusterstructure(x, standardize = TRUE, distance = "euclidean", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="check_clusterstructure_+3A_x">x</code></td>
<td>
<p>A data frame.</p>
</td></tr>
<tr><td><code id="check_clusterstructure_+3A_standardize">standardize</code></td>
<td>
<p>Standardize the dataframe before clustering (default).</p>
</td></tr>
<tr><td><code id="check_clusterstructure_+3A_distance">distance</code></td>
<td>
<p>Distance method used. Other methods than &quot;euclidean&quot;
(default) are exploratory in the context of clustering tendency. See
<code><a href="stats.html#topic+dist">stats::dist()</a></code> for list of available methods.</p>
</td></tr>
<tr><td><code id="check_clusterstructure_+3A_...">...</code></td>
<td>
<p>Arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The H statistic (numeric)
</p>


<h3>References</h3>


<ul>
<li><p> Lawson, R. G., &amp; Jurs, P. C. (1990). New index for clustering
tendency and its application to chemical problems. Journal of chemical
information and computer sciences, 30(1), 36-41.
</p>
</li>
<li><p> Bezdek, J. C., &amp; Hathaway, R. J. (2002, May). VAT: A tool for visual
assessment of (cluster) tendency. In Proceedings of the 2002 International
Joint Conference on Neural Networks. IJCNN02 (3), 2225-2230. IEEE.
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+check_kmo">check_kmo()</a></code>, <code><a href="#topic+check_sphericity_bartlett">check_sphericity_bartlett()</a></code> and
<code><a href="#topic+check_factorstructure">check_factorstructure()</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(performance)
check_clusterstructure(iris[, 1:4])
plot(check_clusterstructure(iris[, 1:4]))

</code></pre>

<hr>
<h2 id='check_collinearity'>Check for multicollinearity of model terms</h2><span id='topic+check_collinearity'></span><span id='topic+multicollinearity'></span><span id='topic+check_collinearity.default'></span><span id='topic+check_collinearity.glmmTMB'></span><span id='topic+check_concurvity'></span>

<h3>Description</h3>

<p><code>check_collinearity()</code> checks regression models for
multicollinearity by calculating the variance inflation factor (VIF).
<code>multicollinearity()</code> is an alias for <code>check_collinearity()</code>.
<code>check_concurvity()</code> is a wrapper around <code>mgcv::concurvity()</code>, and can be
considered as a collinearity check for smooth terms in GAMs. Confidence
intervals for VIF and tolerance are based on Marcoulides et al.
(2019, Appendix B).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_collinearity(x, ...)

multicollinearity(x, ...)

## Default S3 method:
check_collinearity(x, ci = 0.95, verbose = TRUE, ...)

## S3 method for class 'glmmTMB'
check_collinearity(
  x,
  component = c("all", "conditional", "count", "zi", "zero_inflated"),
  ci = 0.95,
  verbose = TRUE,
  ...
)

check_concurvity(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="check_collinearity_+3A_x">x</code></td>
<td>
<p>A model object (that should at least respond to <code>vcov()</code>,
and if possible, also to <code>model.matrix()</code> - however, it also should
work without <code>model.matrix()</code>).</p>
</td></tr>
<tr><td><code id="check_collinearity_+3A_...">...</code></td>
<td>
<p>Currently not used.</p>
</td></tr>
<tr><td><code id="check_collinearity_+3A_ci">ci</code></td>
<td>
<p>Confidence Interval (CI) level for VIF and tolerance values.</p>
</td></tr>
<tr><td><code id="check_collinearity_+3A_verbose">verbose</code></td>
<td>
<p>Toggle off warnings or messages.</p>
</td></tr>
<tr><td><code id="check_collinearity_+3A_component">component</code></td>
<td>
<p>For models with zero-inflation component, multicollinearity
can be checked for the conditional model (count component,
<code>component = "conditional"</code> or <code>component = "count"</code>),
zero-inflation component (<code>component = "zero_inflated"</code> or
<code>component = "zi"</code>) or both components (<code>component = "all"</code>).
Following model-classes are currently supported: <code>hurdle</code>,
<code>zeroinfl</code>, <code>zerocount</code>, <code>MixMod</code> and <code>glmmTMB</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame with information about name of the model term, the
variance inflation factor and associated confidence intervals, the factor
by which the standard error is increased due to possible correlation
with other terms, and tolerance values (including confidence intervals),
where <code>tolerance = 1/vif</code>.
</p>


<h3>Multicollinearity</h3>

<p>Multicollinearity should not be confused with a raw strong correlation
between predictors. What matters is the association between one or more
predictor variables, <em>conditional on the other variables in the
model</em>. In a nutshell, multicollinearity means that once you know the
effect of one predictor, the value of knowing the other predictor is rather
low. Thus, one of the predictors doesn't help much in terms of better
understanding the model or predicting the outcome. As a consequence, if
multicollinearity is a problem, the model seems to suggest that the
predictors in question don't seems to be reliably associated with the
outcome (low estimates, high standard errors), although these predictors
actually are strongly associated with the outcome, i.e. indeed might have
strong effect (<em>McElreath 2020, chapter 6.1</em>).
</p>
<p>Multicollinearity might arise when a third, unobserved variable has a causal
effect on each of the two predictors that are associated with the outcome.
In such cases, the actual relationship that matters would be the association
between the unobserved variable and the outcome.
</p>
<p>Remember: &quot;Pairwise correlations are not the problem. It is the conditional
associations - not correlations - that matter.&quot; (<em>McElreath 2020, p. 169</em>)
</p>


<h3>Interpretation of the Variance Inflation Factor</h3>

<p>The variance inflation factor is a measure to analyze the magnitude of
multicollinearity of model terms. A VIF less than 5 indicates a low
correlation of that predictor with other predictors. A value between 5 and
10 indicates a moderate correlation, while VIF values larger than 10 are a
sign for high, not tolerable correlation of model predictors (<em>James et al.
2013</em>). The <em>Increased SE</em> column in the output indicates how much larger
the standard error is due to the association with other predictors
conditional on the remaining variables in the model. Note that these
thresholds, although commonly used, are also criticized for being too high.
<em>Zuur et al. (2010)</em> suggest using lower values, e.g. a VIF of 3 or larger
may already no longer be considered as &quot;low&quot;.
</p>


<h3>Multicollinearity and Interaction Terms</h3>

<p>If interaction terms are included in a model, high VIF values are expected.
This portion of multicollinearity among the component terms of an
interaction is also called &quot;inessential ill-conditioning&quot;, which leads to
inflated VIF values that are typically seen for models with interaction
terms <em>(Francoeur 2013)</em>.
</p>


<h3>Concurvity for Smooth Terms in Generalized Additive Models</h3>

<p><code>check_concurvity()</code> is a wrapper around <code>mgcv::concurvity()</code>, and can be
considered as a collinearity check for smooth terms in GAMs.&quot;Concurvity
occurs when some smooth term in a model could be approximated by one or more
of the other smooth terms in the model.&quot; (see <code>?mgcv::concurvity</code>).
<code>check_concurvity()</code> returns a column named <em>VIF</em>, which is the &quot;worst&quot;
measure. While <code>mgcv::concurvity()</code> range between 0 and 1, the <em>VIF</em> value
is <code>1 / (1 - worst)</code>, to make interpretation comparable to classical VIF
values, i.e. <code>1</code> indicates no problems, while higher values indicate
increasing lack of identifiability. The <em>VIF proportion</em> column equals the
&quot;estimate&quot; column from <code>mgcv::concurvity()</code>, ranging from 0 (no problem) to
1 (total lack of identifiability).
</p>


<h3>Note</h3>

<p>The code to compute the confidence intervals for the VIF and tolerance
values was adapted from the Appendix B from the Marcoulides et al. paper.
Thus, credits go to these authors the original algorithm. There is also
a <a href="https://easystats.github.io/see/articles/performance.html"><code>plot()</code>-method</a>
implemented in the <a href="https://easystats.github.io/see/"><span class="pkg">see</span>-package</a>.
</p>


<h3>References</h3>


<ul>
<li><p> Francoeur, R. B. (2013). Could Sequential Residual Centering Resolve
Low Sensitivity in Moderated Regression? Simulations and Cancer Symptom
Clusters. Open Journal of Statistics, 03(06), 24-44.
</p>
</li>
<li><p> James, G., Witten, D., Hastie, T., and Tibshirani, R. (eds.). (2013).
An introduction to statistical learning: with applications in R. New York:
Springer.
</p>
</li>
<li><p> Marcoulides, K. M., and Raykov, T. (2019). Evaluation of Variance
Inflation Factors in Regression Models Using Latent Variable Modeling
Methods. Educational and Psychological Measurement, 79(5), 874–882.
</p>
</li>
<li><p> McElreath, R. (2020). Statistical rethinking: A Bayesian course with
examples in R and Stan. 2nd edition. Chapman and Hall/CRC.
</p>
</li>
<li><p> Vanhove, J. (2019). Collinearity isn't a disease that needs curing.
<a href="https://janhove.github.io/posts/2019-09-11-collinearity/">webpage</a>
</p>
</li>
<li><p> Zuur AF, Ieno EN, Elphick CS. A protocol for data exploration to avoid
common statistical problems: Data exploration. Methods in Ecology and
Evolution (2010) 1:3–14.
</p>
</li></ul>



<h3>See Also</h3>

<p>Other functions to check model assumptions and and assess model quality: 
<code><a href="#topic+check_autocorrelation">check_autocorrelation</a>()</code>,
<code><a href="#topic+check_convergence">check_convergence</a>()</code>,
<code><a href="#topic+check_heteroscedasticity">check_heteroscedasticity</a>()</code>,
<code><a href="#topic+check_homogeneity">check_homogeneity</a>()</code>,
<code><a href="#topic+check_model">check_model</a>()</code>,
<code><a href="#topic+check_outliers">check_outliers</a>()</code>,
<code><a href="#topic+check_overdispersion">check_overdispersion</a>()</code>,
<code><a href="#topic+check_predictions">check_predictions</a>()</code>,
<code><a href="#topic+check_singularity">check_singularity</a>()</code>,
<code><a href="#topic+check_zeroinflation">check_zeroinflation</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>m &lt;- lm(mpg ~ wt + cyl + gear + disp, data = mtcars)
check_collinearity(m)


# plot results
x &lt;- check_collinearity(m)
plot(x)

</code></pre>

<hr>
<h2 id='check_convergence'>Convergence test for mixed effects models</h2><span id='topic+check_convergence'></span>

<h3>Description</h3>

<p><code>check_convergence()</code> provides an alternative convergence
test for <code>merMod</code>-objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_convergence(x, tolerance = 0.001, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="check_convergence_+3A_x">x</code></td>
<td>
<p>A <code>merMod</code> or <code>glmmTMB</code>-object.</p>
</td></tr>
<tr><td><code id="check_convergence_+3A_tolerance">tolerance</code></td>
<td>
<p>Indicates up to which value the convergence result is
accepted. The smaller <code>tolerance</code> is, the stricter the test will be.</p>
</td></tr>
<tr><td><code id="check_convergence_+3A_...">...</code></td>
<td>
<p>Currently not used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>TRUE</code> if convergence is fine and <code>FALSE</code> if convergence
is suspicious. Additionally, the convergence value is returned as attribute.
</p>


<h3>Convergence and log-likelihood</h3>

<p>Convergence problems typically arise when the model hasn't converged
to a solution where the log-likelihood has a true maximum. This may result
in unreliable and overly complex (or non-estimable) estimates and standard
errors.
</p>


<h3>Inspect model convergence</h3>

<p><strong>lme4</strong> performs a convergence-check (see <code>?lme4::convergence</code>),
however, as as discussed <a href="https://github.com/lme4/lme4/issues/120">here</a>
and suggested by one of the lme4-authors in
<a href="https://github.com/lme4/lme4/issues/120#issuecomment-39920269">this comment</a>,
this check can be too strict. <code>check_convergence()</code> thus provides an
alternative convergence test for <code>merMod</code>-objects.
</p>


<h3>Resolving convergence issues</h3>

<p>Convergence issues are not easy to diagnose. The help page on
<code>?lme4::convergence</code> provides most of the current advice about
how to resolve convergence issues. Another clue might be large parameter
values, e.g. estimates (on the scale of the linear predictor) larger than
10 in (non-identity link) generalized linear model <em>might</em> indicate
<a href="https://stats.oarc.ucla.edu/other/mult-pkg/faq/general/faqwhat-is-complete-or-quasi-complete-separation-in-logisticprobit-regression-and-how-do-we-deal-with-them/">complete separation</a>.
Complete separation can be addressed by regularization, e.g. penalized
regression or Bayesian regression with appropriate priors on the fixed effects.
</p>


<h3>Convergence versus Singularity</h3>

<p>Note the different meaning between singularity and convergence: singularity
indicates an issue with the &quot;true&quot; best estimate, i.e. whether the maximum
likelihood estimation for the variance-covariance matrix of the random effects
is positive definite or only semi-definite. Convergence is a question of
whether we can assume that the numerical optimization has worked correctly
or not.
</p>


<h3>See Also</h3>

<p>Other functions to check model assumptions and and assess model quality: 
<code><a href="#topic+check_autocorrelation">check_autocorrelation</a>()</code>,
<code><a href="#topic+check_collinearity">check_collinearity</a>()</code>,
<code><a href="#topic+check_heteroscedasticity">check_heteroscedasticity</a>()</code>,
<code><a href="#topic+check_homogeneity">check_homogeneity</a>()</code>,
<code><a href="#topic+check_model">check_model</a>()</code>,
<code><a href="#topic+check_outliers">check_outliers</a>()</code>,
<code><a href="#topic+check_overdispersion">check_overdispersion</a>()</code>,
<code><a href="#topic+check_predictions">check_predictions</a>()</code>,
<code><a href="#topic+check_singularity">check_singularity</a>()</code>,
<code><a href="#topic+check_zeroinflation">check_zeroinflation</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(cbpp, package = "lme4")
set.seed(1)
cbpp$x &lt;- rnorm(nrow(cbpp))
cbpp$x2 &lt;- runif(nrow(cbpp))

model &lt;- lme4::glmer(
  cbind(incidence, size - incidence) ~ period + x + x2 + (1 + x | herd),
  data = cbpp,
  family = binomial()
)

check_convergence(model)


model &lt;- suppressWarnings(glmmTMB::glmmTMB(
  Sepal.Length ~ poly(Petal.Width, 4) * poly(Petal.Length, 4) +
    (1 + poly(Petal.Width, 4) | Species),
  data = iris
))
check_convergence(model)


</code></pre>

<hr>
<h2 id='check_distribution'>Classify the distribution of a model-family using machine learning</h2><span id='topic+check_distribution'></span>

<h3>Description</h3>

<p>Choosing the right distributional family for regression models is essential
to get more accurate estimates and standard errors. This function may help to
check a models' distributional family and see if the model-family probably
should be reconsidered. Since it is difficult to exactly predict the correct
model family, consider this function as somewhat experimental.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_distribution(model)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="check_distribution_+3A_model">model</code></td>
<td>
<p>Typically, a model (that should response to <code>residuals()</code>).
May also be a numeric vector.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function uses an internal random forest model to classify the
distribution from a model-family. Currently, following distributions are
trained (i.e. results of <code>check_distribution()</code> may be one of the
following): <code>"bernoulli"</code>, <code>"beta"</code>, <code>"beta-binomial"</code>,
<code>"binomial"</code>, <code>"chi"</code>, <code>"exponential"</code>, <code>"F"</code>,
<code>"gamma"</code>, <code>"lognormal"</code>, <code>"normal"</code>, <code>"negative binomial"</code>, <code>"negative binomial (zero-inflated)"</code>, <code>"pareto"</code>,
<code>"poisson"</code>, <code>"poisson (zero-inflated)"</code>, <code>"uniform"</code> and
<code>"weibull"</code>.
<br /> <br />
Note the similarity between certain distributions according to shape, skewness,
etc. Thus, the predicted distribution may not be perfectly representing the
distributional family of the underlying fitted model, or the response value.
<br /> <br />
There is a <code>plot()</code> method, which shows the probabilities of all predicted
distributions, however, only if the probability is greater than zero.
</p>


<h3>Note</h3>

<p>This function is somewhat experimental and might be improved in future
releases. The final decision on the model-family should also be based on
theoretical aspects and other information about the data and the model.
<br /> <br />
There is also a
<a href="https://easystats.github.io/see/articles/performance.html"><code>plot()</code>-method</a>
implemented in the
<a href="https://easystats.github.io/see/"><span class="pkg">see</span>-package</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(sleepstudy, package = "lme4")
model &lt;&lt;- lme4::lmer(Reaction ~ Days + (Days | Subject), sleepstudy)
check_distribution(model)


plot(check_distribution(model))

</code></pre>

<hr>
<h2 id='check_factorstructure'>Check suitability of data for Factor Analysis (FA) with Bartlett's Test of Sphericity and KMO</h2><span id='topic+check_factorstructure'></span><span id='topic+check_kmo'></span><span id='topic+check_sphericity_bartlett'></span>

<h3>Description</h3>

<p>This checks whether the data is appropriate for Factor Analysis (FA) by
running the Bartlett's Test of Sphericity and the Kaiser, Meyer, Olkin (KMO)
Measure of Sampling Adequacy (MSA). See <strong>details</strong> below for more information
about the interpretation and meaning of each test.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_factorstructure(x, n = NULL, ...)

check_kmo(x, n = NULL, ...)

check_sphericity_bartlett(x, n = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="check_factorstructure_+3A_x">x</code></td>
<td>
<p>A dataframe or a correlation matrix. If the latter is passed, <code>n</code>
must be provided.</p>
</td></tr>
<tr><td><code id="check_factorstructure_+3A_n">n</code></td>
<td>
<p>If a correlation matrix was passed, the number of observations must
be specified.</p>
</td></tr>
<tr><td><code id="check_factorstructure_+3A_...">...</code></td>
<td>
<p>Arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>



<h4>Bartlett's Test of Sphericity</h4>

<p>Bartlett's (1951) test of sphericity tests whether a matrix (of correlations)
is significantly different from an identity matrix (filled with 0). It tests
whether the correlation coefficients are all 0. The test computes the
probability that the correlation matrix has significant correlations among at
least some of the variables in a dataset, a prerequisite for factor analysis
to work.
</p>
<p>While it is often suggested to check whether Bartlett’s test of sphericity is
significant before starting with factor analysis, one needs to remember that
the test is testing a pretty extreme scenario (that all correlations are non-significant).
As the sample size increases, this test tends to be always significant, which
makes it not particularly useful or informative in well-powered studies.
</p>



<h4>Kaiser, Meyer, Olkin (KMO)</h4>

<p><em>(Measure of Sampling Adequacy (MSA) for Factor Analysis.)</em>
</p>
<p>Kaiser (1970) introduced a Measure of Sampling Adequacy (MSA), later modified
by Kaiser and Rice (1974). The Kaiser-Meyer-Olkin (KMO) statistic, which can
vary from 0 to 1, indicates the degree to which each variable in a set is
predicted without error by the other variables.
</p>
<p>A value of 0 indicates that the sum of partial correlations is large relative
to the sum correlations, indicating factor analysis is likely to be
inappropriate. A KMO value close to 1 indicates that the sum of partial
correlations is not large relative to the sum of correlations and so factor
analysis should yield distinct and reliable factors. It means that patterns
of correlations are relatively compact, and so factor analysis should yield
distinct and reliable factors. Values smaller than 0.5 suggest that you should
either collect more data or rethink which variables to include.
</p>
<p>Kaiser (1974) suggested that KMO &gt; .9 were marvelous, in the .80s,
meritorious, in the .70s, middling, in the .60s, mediocre, in the .50s,
miserable, and less than .5, unacceptable. Hair et al. (2006) suggest
accepting a value &gt; 0.5. Values between 0.5 and 0.7 are mediocre, and values
between 0.7 and 0.8 are good.
</p>
<p>Variables with individual KMO values below 0.5 could be considered for
exclusion them from the analysis (note that you would need to re-compute the
KMO indices as they are dependent on the whole dataset).
</p>



<h3>Value</h3>

<p>A list of lists of indices related to sphericity and KMO.
</p>


<h3>References</h3>

<p>This function is a wrapper around the <code>KMO</code> and the <code>cortest.bartlett()</code>
functions in the <strong>psych</strong> package (Revelle, 2016).
</p>

<ul>
<li><p> Revelle, W. (2016). How To: Use the psych package for Factor Analysis
and data reduction.
</p>
</li>
<li><p> Bartlett, M. S. (1951). The effect of standardization on a Chi-square
approximation in factor analysis. Biometrika, 38(3/4), 337-344.
</p>
</li>
<li><p> Kaiser, H. F. (1970). A second generation little jiffy.
Psychometrika, 35(4), 401-415.
</p>
</li>
<li><p> Kaiser, H. F., &amp; Rice, J. (1974). Little jiffy, mark IV. Educational
and psychological measurement, 34(1), 111-117.
</p>
</li>
<li><p> Kaiser, H. F. (1974). An index of factorial simplicity.
Psychometrika, 39(1), 31-36.
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+check_clusterstructure">check_clusterstructure()</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(performance)

check_factorstructure(mtcars)

# One can also pass a correlation matrix
r &lt;- cor(mtcars)
check_factorstructure(r, n = nrow(mtcars))

</code></pre>

<hr>
<h2 id='check_heterogeneity_bias'>Check model predictor for heterogeneity bias</h2><span id='topic+check_heterogeneity_bias'></span>

<h3>Description</h3>

<p><code>check_heterogeneity_bias()</code> checks if model predictors or variables may
cause a heterogeneity bias, i.e. if variables have a within- and/or
between-effect (<em>Bell and Jones, 2015</em>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_heterogeneity_bias(x, select = NULL, group = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="check_heterogeneity_bias_+3A_x">x</code></td>
<td>
<p>A data frame or a mixed model object.</p>
</td></tr>
<tr><td><code id="check_heterogeneity_bias_+3A_select">select</code></td>
<td>
<p>Character vector (or formula) with names of variables to select
that should be checked. If <code>x</code> is a mixed model object, this argument
will be ignored.</p>
</td></tr>
<tr><td><code id="check_heterogeneity_bias_+3A_group">group</code></td>
<td>
<p>Character vector (or formula) with the name of the variable that
indicates the group- or cluster-ID. If <code>x</code> is a model object, this
argument will be ignored.</p>
</td></tr>
</table>


<h3>References</h3>


<ul>
<li><p> Bell A, Jones K. 2015. Explaining Fixed Effects: Random Effects
Modeling of Time-Series Cross-Sectional and Panel Data. Political Science
Research and Methods, 3(1), 133–153.
</p>
</li></ul>



<h3>See Also</h3>

<p>For further details, read the vignette
<a href="https://easystats.github.io/parameters/articles/demean.html">https://easystats.github.io/parameters/articles/demean.html</a> and also
see documentation for <code><a href="datawizard.html#topic+demean">datawizard::demean()</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(iris)
iris$ID &lt;- sample(1:4, nrow(iris), replace = TRUE) # fake-ID
check_heterogeneity_bias(iris, select = c("Sepal.Length", "Petal.Length"), group = "ID")
</code></pre>

<hr>
<h2 id='check_heteroscedasticity'>Check model for (non-)constant error variance</h2><span id='topic+check_heteroscedasticity'></span><span id='topic+check_heteroskedasticity'></span>

<h3>Description</h3>

<p>Significance testing for linear regression models assumes that
the model errors (or residuals) have constant variance. If this assumption
is violated the p-values from the model are no longer reliable.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_heteroscedasticity(x, ...)

check_heteroskedasticity(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="check_heteroscedasticity_+3A_x">x</code></td>
<td>
<p>A model object.</p>
</td></tr>
<tr><td><code id="check_heteroscedasticity_+3A_...">...</code></td>
<td>
<p>Currently not used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This test of the hypothesis of (non-)constant error is also called
<em>Breusch-Pagan test</em> (<cite>1979</cite>).
</p>


<h3>Value</h3>

<p>The p-value of the test statistics. A p-value &lt; 0.05 indicates a
non-constant variance (heteroskedasticity).
</p>


<h3>Note</h3>

<p>There is also a <a href="https://easystats.github.io/see/articles/performance.html"><code>plot()</code>-method</a>
implemented in the <a href="https://easystats.github.io/see/"><span class="pkg">see</span>-package</a>.
</p>


<h3>References</h3>

<p>Breusch, T. S., and Pagan, A. R. (1979) A simple test for heteroscedasticity
and random coefficient variation. Econometrica 47, 1287-1294.
</p>


<h3>See Also</h3>

<p>Other functions to check model assumptions and and assess model quality: 
<code><a href="#topic+check_autocorrelation">check_autocorrelation</a>()</code>,
<code><a href="#topic+check_collinearity">check_collinearity</a>()</code>,
<code><a href="#topic+check_convergence">check_convergence</a>()</code>,
<code><a href="#topic+check_homogeneity">check_homogeneity</a>()</code>,
<code><a href="#topic+check_model">check_model</a>()</code>,
<code><a href="#topic+check_outliers">check_outliers</a>()</code>,
<code><a href="#topic+check_overdispersion">check_overdispersion</a>()</code>,
<code><a href="#topic+check_predictions">check_predictions</a>()</code>,
<code><a href="#topic+check_singularity">check_singularity</a>()</code>,
<code><a href="#topic+check_zeroinflation">check_zeroinflation</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>m &lt;&lt;- lm(mpg ~ wt + cyl + gear + disp, data = mtcars)
check_heteroscedasticity(m)

# plot results
if (require("see")) {
  x &lt;- check_heteroscedasticity(m)
  plot(x)
}
</code></pre>

<hr>
<h2 id='check_homogeneity'>Check model for homogeneity of variances</h2><span id='topic+check_homogeneity'></span><span id='topic+check_homogeneity.afex_aov'></span>

<h3>Description</h3>

<p>Check model for homogeneity of variances between groups described
by independent variables in a model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_homogeneity(x, method = c("bartlett", "fligner", "levene", "auto"), ...)

## S3 method for class 'afex_aov'
check_homogeneity(x, method = "levene", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="check_homogeneity_+3A_x">x</code></td>
<td>
<p>A linear model or an ANOVA object.</p>
</td></tr>
<tr><td><code id="check_homogeneity_+3A_method">method</code></td>
<td>
<p>Name of the method (underlying test) that should be performed
to check the homogeneity of variances. May either be <code>"levene"</code> for
Levene's Test for Homogeneity of Variance, <code>"bartlett"</code> for the
Bartlett test (assuming normal distributed samples or groups),
<code>"fligner"</code> for the Fligner-Killeen test (rank-based, non-parametric
test), or <code>"auto"</code>. In the latter case, Bartlett test is used if the
model response is normal distributed, else Fligner-Killeen test is used.</p>
</td></tr>
<tr><td><code id="check_homogeneity_+3A_...">...</code></td>
<td>
<p>Arguments passed down to <code>car::leveneTest()</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Invisibly returns the p-value of the test statistics. A p-value &lt;
0.05 indicates a significant difference in the variance between the groups.
</p>


<h3>Note</h3>

<p>There is also a <a href="https://easystats.github.io/see/articles/performance.html"><code>plot()</code>-method</a>
implemented in the <a href="https://easystats.github.io/see/"><span class="pkg">see</span>-package</a>.
</p>


<h3>See Also</h3>

<p>Other functions to check model assumptions and and assess model quality: 
<code><a href="#topic+check_autocorrelation">check_autocorrelation</a>()</code>,
<code><a href="#topic+check_collinearity">check_collinearity</a>()</code>,
<code><a href="#topic+check_convergence">check_convergence</a>()</code>,
<code><a href="#topic+check_heteroscedasticity">check_heteroscedasticity</a>()</code>,
<code><a href="#topic+check_model">check_model</a>()</code>,
<code><a href="#topic+check_outliers">check_outliers</a>()</code>,
<code><a href="#topic+check_overdispersion">check_overdispersion</a>()</code>,
<code><a href="#topic+check_predictions">check_predictions</a>()</code>,
<code><a href="#topic+check_singularity">check_singularity</a>()</code>,
<code><a href="#topic+check_zeroinflation">check_zeroinflation</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>model &lt;&lt;- lm(len ~ supp + dose, data = ToothGrowth)
check_homogeneity(model)

# plot results
if (require("see")) {
  result &lt;- check_homogeneity(model)
  plot(result)
}
</code></pre>

<hr>
<h2 id='check_itemscale'>Describe Properties of Item Scales</h2><span id='topic+check_itemscale'></span>

<h3>Description</h3>

<p>Compute various measures of internal consistencies
applied to (sub)scales, which items were extracted using
<code>parameters::principal_components()</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_itemscale(x, factor_index = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="check_itemscale_+3A_x">x</code></td>
<td>
<p>An object of class <code>parameters_pca</code>, as returned by
<code><a href="parameters.html#topic+principal_components">parameters::principal_components()</a></code>, or a data frame.</p>
</td></tr>
<tr><td><code id="check_itemscale_+3A_factor_index">factor_index</code></td>
<td>
<p>If <code>x</code> is a data frame, <code>factor_index</code> must be specified.
It must be a numeric vector of same length as number of columns in <code>x</code>, where
each element is the index of the factor to which the respective column in <code>x</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>check_itemscale()</code> calculates various measures of internal
consistencies, such as Cronbach's alpha, item difficulty or discrimination
etc. on subscales which were built from several items. Subscales are
retrieved from the results of <code><a href="parameters.html#topic+principal_components">parameters::principal_components()</a></code>, i.e.
based on how many components were extracted from the PCA,
<code>check_itemscale()</code> retrieves those variables that belong to a component
and calculates the above mentioned measures.
</p>


<h3>Value</h3>

<p>A list of data frames, with related measures of internal
consistencies of each subscale.
</p>


<h3>Note</h3>


<ul>
<li> <p><em>Item difficulty</em> should range between 0.2 and 0.8. Ideal value
is <code>p+(1-p)/2</code> (which mostly is between 0.5 and 0.8). See
<code><a href="#topic+item_difficulty">item_difficulty()</a></code> for details.
</p>
</li>
<li><p> For <em>item discrimination</em>, acceptable values are 0.20 or higher;
the closer to 1.00 the better. See <code><a href="#topic+item_reliability">item_reliability()</a></code> for more
details.
</p>
</li>
<li><p> In case the total <em>Cronbach's alpha</em> value is below the acceptable
cut-off of 0.7 (mostly if an index has few items), the
<em>mean inter-item-correlation</em> is an alternative measure to indicate
acceptability. Satisfactory range lies between 0.2 and 0.4. See also
<code><a href="#topic+item_intercor">item_intercor()</a></code>.
</p>
</li></ul>



<h3>References</h3>


<ul>
<li><p> Briggs SR, Cheek JM (1986) The role of factor analysis in the development
and evaluation of personality scales. Journal of Personality, 54(1),
106-148. doi: 10.1111/j.1467-6494.1986.tb00391.x
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>
# data generation from '?prcomp', slightly modified
C &lt;- chol(S &lt;- toeplitz(0.9^(0:15)))
set.seed(17)
X &lt;- matrix(rnorm(1600), 100, 16)
Z &lt;- X %*% C

pca &lt;- parameters::principal_components(
  as.data.frame(Z),
  rotation = "varimax",
  n = 3
)
pca
check_itemscale(pca)

# as data frame
check_itemscale(
  as.data.frame(Z),
  factor_index = parameters::closest_component(pca)
)

</code></pre>

<hr>
<h2 id='check_model'>Visual check of model assumptions</h2><span id='topic+check_model'></span><span id='topic+check_model.default'></span>

<h3>Description</h3>

<p>Visual check of various model assumptions (normality of residuals, normality
of random effects, linear relationship, homogeneity of variance,
multicollinearity).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_model(x, ...)

## Default S3 method:
check_model(
  x,
  dot_size = 2,
  line_size = 0.8,
  panel = TRUE,
  check = "all",
  alpha = 0.2,
  dot_alpha = 0.8,
  colors = c("#3aaf85", "#1b6ca8", "#cd201f"),
  theme = "see::theme_lucid",
  detrend = TRUE,
  show_dots = NULL,
  bandwidth = "nrd",
  type = "density",
  residual_type = NULL,
  verbose = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="check_model_+3A_x">x</code></td>
<td>
<p>A model object.</p>
</td></tr>
<tr><td><code id="check_model_+3A_...">...</code></td>
<td>
<p>Arguments passed down to the individual check functions, especially
to <code>check_predictions()</code> and <code>binned_residuals()</code>.</p>
</td></tr>
<tr><td><code id="check_model_+3A_dot_size">dot_size</code>, <code id="check_model_+3A_line_size">line_size</code></td>
<td>
<p>Size of line and dot-geoms.</p>
</td></tr>
<tr><td><code id="check_model_+3A_panel">panel</code></td>
<td>
<p>Logical, if <code>TRUE</code>, plots are arranged as panels; else,
single plots for each diagnostic are returned.</p>
</td></tr>
<tr><td><code id="check_model_+3A_check">check</code></td>
<td>
<p>Character vector, indicating which checks for should be performed
and plotted. May be one or more of <code>"all"</code>, <code>"vif"</code>, <code>"qq"</code>, <code>"normality"</code>,
<code>"linearity"</code>, <code>"ncv"</code>, <code>"homogeneity"</code>, <code>"outliers"</code>, <code>"reqq"</code>, <code>"pp_check"</code>,
<code>"binned_residuals"</code> or <code>"overdispersion"</code>. Note that not all check apply
to all type of models (see 'Details'). <code>"reqq"</code> is a QQ-plot for random
effects and only available for mixed models. <code>"ncv"</code> is an alias for
<code>"linearity"</code>, and checks for non-constant variance, i.e. for
heteroscedasticity, as well as the linear relationship. By default, all
possible checks are performed and plotted.</p>
</td></tr>
<tr><td><code id="check_model_+3A_alpha">alpha</code>, <code id="check_model_+3A_dot_alpha">dot_alpha</code></td>
<td>
<p>The alpha level of the confidence bands and dot-geoms.
Scalar from 0 to 1.</p>
</td></tr>
<tr><td><code id="check_model_+3A_colors">colors</code></td>
<td>
<p>Character vector with color codes (hex-format). Must be of
length 3. First color is usually used for reference lines, second color
for dots, and third color for outliers or extreme values.</p>
</td></tr>
<tr><td><code id="check_model_+3A_theme">theme</code></td>
<td>
<p>String, indicating the name of the plot-theme. Must be in the
format <code>"package::theme_name"</code> (e.g. <code>"ggplot2::theme_minimal"</code>).</p>
</td></tr>
<tr><td><code id="check_model_+3A_detrend">detrend</code></td>
<td>
<p>Logical. Should Q-Q/P-P plots be detrended? Defaults to
<code>TRUE</code> for linear models or when <code>residual_type = "normal"</code>. Defaults to
<code>FALSE</code> for QQ plots based on simulated residuals (i.e. when
<code>residual_type = "simulated"</code>).</p>
</td></tr>
<tr><td><code id="check_model_+3A_show_dots">show_dots</code></td>
<td>
<p>Logical, if <code>TRUE</code>, will show data points in the plot. Set
to <code>FALSE</code> for models with many observations, if generating the plot is too
time-consuming. By default, <code>show_dots = NULL</code>. In this case <code>check_model()</code>
tries to guess whether performance will be poor due to a very large model
and thus automatically shows or hides dots.</p>
</td></tr>
<tr><td><code id="check_model_+3A_bandwidth">bandwidth</code></td>
<td>
<p>A character string indicating the smoothing bandwidth to
be used. Unlike <code>stats::density()</code>, which used <code>"nrd0"</code> as default, the
default used here is <code>"nrd"</code> (which seems to give more plausible results
for non-Gaussian models). When problems with plotting occur, try to change
to a different value.</p>
</td></tr>
<tr><td><code id="check_model_+3A_type">type</code></td>
<td>
<p>Plot type for the posterior predictive checks plot. Can be <code>"density"</code>,
<code>"discrete_dots"</code>, <code>"discrete_interval"</code> or <code>"discrete_both"</code> (the <code style="white-space: pre;">&#8288;discrete_*&#8288;</code>
options are appropriate for models with discrete - binary, integer or ordinal
etc. - outcomes).</p>
</td></tr>
<tr><td><code id="check_model_+3A_residual_type">residual_type</code></td>
<td>
<p>Character, indicating the type of residuals to be used.
For non-Gaussian models, the default is <code>"simulated"</code>, which uses simulated
residuals. These are based on <code><a href="#topic+simulate_residuals">simulate_residuals()</a></code> and thus uses the
<strong>DHARMa</strong> package to return randomized quantile residuals. For Gaussian
models, the default is <code>"normal"</code>, which uses the default residuals from
the model. Setting <code>residual_type = "normal"</code> for non-Gaussian models will
use a half-normal Q-Q plot of the absolute value of the standardized deviance
residuals.</p>
</td></tr>
<tr><td><code id="check_model_+3A_verbose">verbose</code></td>
<td>
<p>If <code>FALSE</code> (default), suppress most warning messages.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For Bayesian models from packages <strong>rstanarm</strong> or <strong>brms</strong>,
models will be &quot;converted&quot; to their frequentist counterpart, using
<a href="https://easystats.github.io/bayestestR/reference/convert_bayesian_as_frequentist.html"><code>bayestestR::bayesian_as_frequentist</code></a>.
A more advanced model-check for Bayesian models will be implemented at a
later stage.
</p>
<p>See also the related <a href="https://easystats.github.io/performance/articles/check_model.html">vignette</a>.
</p>


<h3>Value</h3>

<p>The data frame that is used for plotting.
</p>


<h3>Posterior Predictive Checks</h3>

<p>Posterior predictive checks can be used to look for systematic discrepancies
between real and simulated data. It helps to see whether the type of model
(distributional family) fits well to the data. See <code><a href="#topic+check_predictions">check_predictions()</a></code>
for further details.
</p>


<h3>Linearity Assumption</h3>

<p>The plot <strong>Linearity</strong> checks the assumption of linear relationship.
However, the spread of dots also indicate possible heteroscedasticity (i.e.
non-constant variance, hence, the alias <code>"ncv"</code> for this plot), thus it shows
if residuals have non-linear patterns. This plot helps to see whether
predictors may have a non-linear relationship with the outcome, in which case
the reference line may roughly indicate that relationship. A straight and
horizontal line indicates that the model specification seems to be ok. But
for instance, if the line would be U-shaped, some of the predictors probably
should better be modeled as quadratic term. See <code><a href="#topic+check_heteroscedasticity">check_heteroscedasticity()</a></code>
for further details.
</p>
<p><strong>Some caution is needed</strong> when interpreting these plots. Although these
plots are helpful to check model assumptions, they do not necessarily indicate
so-called &quot;lack of fit&quot;, e.g. missed non-linear relationships or interactions.
Thus, it is always recommended to also look at
<a href="https://strengejacke.github.io/ggeffects/articles/introduction_partial_residuals.html">effect plots, including partial residuals</a>.
</p>


<h3>Homogeneity of Variance</h3>

<p>This plot checks the assumption of equal variance (homoscedasticity). The
desired pattern would be that dots spread equally above and below a straight,
horizontal line and show no apparent deviation.
</p>


<h3>Influential Observations</h3>

<p>This plot is used to identify influential observations. If any points in this
plot fall outside of Cook’s distance (the dashed lines) then it is considered
an influential observation. See <code><a href="#topic+check_outliers">check_outliers()</a></code> for further details.
</p>


<h3>Multicollinearity</h3>

<p>This plot checks for potential collinearity among predictors. In a nutshell,
multicollinearity means that once you know the effect of one predictor, the
value of knowing the other predictor is rather low. Multicollinearity might
arise when a third, unobserved variable has a causal effect on each of the
two predictors that are associated with the outcome. In such cases, the actual
relationship that matters would be the association between the unobserved
variable and the outcome. See <code><a href="#topic+check_collinearity">check_collinearity()</a></code> for further details.
</p>


<h3>Normality of Residuals</h3>

<p>This plot is used to determine if the residuals of the regression model are
normally distributed. Usually, dots should fall along the line. If there is
some deviation (mostly at the tails), this indicates that the model doesn't
predict the outcome well for that range that shows larger deviations from
the line. For generalized linear models and when <code>residual_type = "normal"</code>,
a half-normal Q-Q plot of the absolute value of the standardized deviance
residuals is shown, however, the interpretation of the plot remains the same.
See <code><a href="#topic+check_normality">check_normality()</a></code> for further details. Usually, for generalized linear
(mixed) models, a test for uniformity of residuals based on simulated residuals
is conducted (see next section).
</p>


<h3>Uniformity of Residuals</h3>

<p>Fore non-Gaussian models, when <code>residual_type = "simulated"</code> (the default
for generalized linear (mixed) models), residuals are not expected to be
normally distributed. In this case, the created Q-Q plot checks the uniformity
of residuals. The interpretation of the plot is the same as for the normal
Q-Q plot. See <code><a href="#topic+simulate_residuals">simulate_residuals()</a></code> and <code><a href="#topic+check_residuals">check_residuals()</a></code> for further
details.
</p>


<h3>Overdispersion</h3>

<p>For count models, an <em>overdispersion plot</em> is shown. Overdispersion occurs
when the observed variance is higher than the variance of a theoretical model.
For Poisson models, variance increases with the mean and, therefore, variance
usually (roughly) equals the mean value. If the variance is much higher,
the data are &quot;overdispersed&quot;. See <code><a href="#topic+check_overdispersion">check_overdispersion()</a></code> for further
details.
</p>


<h3>Binned Residuals</h3>

<p>For models from binomial families, a <em>binned residuals plot</em> is shown.
Binned residual plots are achieved by cutting the the data into bins and then
plotting the average residual versus the average fitted value for each bin.
If the model were true, one would expect about 95% of the residuals to fall
inside the error bounds. See <code><a href="#topic+binned_residuals">binned_residuals()</a></code> for further details.
</p>


<h3>Residuals for (Generalized) Linear Models</h3>

<p>Plots that check the homogeneity of variance use standardized Pearson's
residuals for generalized linear models, and standardized residuals for
linear models. The plots for the normality of residuals (with overlayed
normal curve) and for the linearity assumption use the default residuals
for <code>lm</code> and <code>glm</code> (which are deviance residuals for <code>glm</code>). The Q-Q plots
use simulated residuals (see <code><a href="#topic+simulate_residuals">simulate_residuals()</a></code>) for non-Gaussian
models and standardized residuals for linear models.
</p>


<h3>Troubleshooting</h3>

<p>For models with many observations, or for more complex models in general,
generating the plot might become very slow. One reason might be that the
underlying graphic engine becomes slow for plotting many data points. In
such cases, setting the argument <code>show_dots = FALSE</code> might help. Furthermore,
look at the <code>check</code> argument and see if some of the model checks could be
skipped, which also increases performance.
</p>


<h3>Note</h3>

<p>This function just prepares the data for plotting. To create the plots,
<strong>see</strong> needs to be installed. Furthermore, this function suppresses
all possible warnings. In case you observe suspicious plots, please refer
to the dedicated functions (like <code>check_collinearity()</code>,
<code>check_normality()</code> etc.) to get informative messages and warnings.
</p>


<h3>See Also</h3>

<p>Other functions to check model assumptions and and assess model quality: 
<code><a href="#topic+check_autocorrelation">check_autocorrelation</a>()</code>,
<code><a href="#topic+check_collinearity">check_collinearity</a>()</code>,
<code><a href="#topic+check_convergence">check_convergence</a>()</code>,
<code><a href="#topic+check_heteroscedasticity">check_heteroscedasticity</a>()</code>,
<code><a href="#topic+check_homogeneity">check_homogeneity</a>()</code>,
<code><a href="#topic+check_outliers">check_outliers</a>()</code>,
<code><a href="#topic+check_overdispersion">check_overdispersion</a>()</code>,
<code><a href="#topic+check_predictions">check_predictions</a>()</code>,
<code><a href="#topic+check_singularity">check_singularity</a>()</code>,
<code><a href="#topic+check_zeroinflation">check_zeroinflation</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

m &lt;- lm(mpg ~ wt + cyl + gear + disp, data = mtcars)
check_model(m)

data(sleepstudy, package = "lme4")
m &lt;- lme4::lmer(Reaction ~ Days + (Days | Subject), sleepstudy)
check_model(m, panel = FALSE)


</code></pre>

<hr>
<h2 id='check_multimodal'>Check if a distribution is unimodal or multimodal</h2><span id='topic+check_multimodal'></span>

<h3>Description</h3>

<p>For univariate distributions (one-dimensional vectors), this functions
performs a Ameijeiras-Alonso et al. (2018) excess mass test. For multivariate
distributions (data frames), it uses mixture modelling. However, it seems that
it always returns a significant result (suggesting that the distribution is
multimodal). A better method might be needed here.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_multimodal(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="check_multimodal_+3A_x">x</code></td>
<td>
<p>A numeric vector or a data frame.</p>
</td></tr>
<tr><td><code id="check_multimodal_+3A_...">...</code></td>
<td>
<p>Arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>References</h3>


<ul>
<li><p> Ameijeiras-Alonso, J., Crujeiras, R. M., and Rodríguez-Casal, A. (2019).
Mode testing, critical bandwidth and excess mass. Test, 28(3), 900-919.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>

# Univariate
x &lt;- rnorm(1000)
check_multimodal(x)

x &lt;- c(rnorm(1000), rnorm(1000, 2))
check_multimodal(x)

# Multivariate
m &lt;- data.frame(
  x = rnorm(200),
  y = rbeta(200, 2, 1)
)
plot(m$x, m$y)
check_multimodal(m)

m &lt;- data.frame(
  x = c(rnorm(100), rnorm(100, 4)),
  y = c(rbeta(100, 2, 1), rbeta(100, 1, 4))
)
plot(m$x, m$y)
check_multimodal(m)


</code></pre>

<hr>
<h2 id='check_normality'>Check model for (non-)normality of residuals.</h2><span id='topic+check_normality'></span><span id='topic+check_normality.merMod'></span>

<h3>Description</h3>

<p>Check model for (non-)normality of residuals.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_normality(x, ...)

## S3 method for class 'merMod'
check_normality(x, effects = c("fixed", "random"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="check_normality_+3A_x">x</code></td>
<td>
<p>A model object.</p>
</td></tr>
<tr><td><code id="check_normality_+3A_...">...</code></td>
<td>
<p>Currently not used.</p>
</td></tr>
<tr><td><code id="check_normality_+3A_effects">effects</code></td>
<td>
<p>Should normality for residuals (<code>"fixed"</code>) or random
effects (<code>"random"</code>) be tested? Only applies to mixed-effects models.
May be abbreviated.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>check_normality()</code> calls <code>stats::shapiro.test</code> and checks the
standardized residuals (or studentized residuals for mixed models) for
normal distribution. Note that this formal test almost always yields
significant results for the distribution of residuals and visual inspection
(e.g. Q-Q plots) are preferable. For generalized linear models, no formal
statistical test is carried out. Rather, there's only a <code>plot()</code> method for
GLMs. This plot shows a half-normal Q-Q plot of the absolute value of the
standardized deviance residuals is shown (in line with changes in
<code>plot.lm()</code> for R 4.3+).
</p>


<h3>Value</h3>

<p>The p-value of the test statistics. A p-value &lt; 0.05 indicates a
significant deviation from normal distribution.
</p>


<h3>Note</h3>

<p>For mixed-effects models, studentized residuals, and <em>not</em>
standardized residuals, are used for the test. There is also a
<a href="https://easystats.github.io/see/articles/performance.html"><code>plot()</code>-method</a>
implemented in the <a href="https://easystats.github.io/see/"><strong>see</strong>-package</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
m &lt;&lt;- lm(mpg ~ wt + cyl + gear + disp, data = mtcars)
check_normality(m)

# plot results
x &lt;- check_normality(m)
plot(x)


# QQ-plot
plot(check_normality(m), type = "qq")

# PP-plot
plot(check_normality(m), type = "pp")


</code></pre>

<hr>
<h2 id='check_outliers'>Outliers detection (check for influential observations)</h2><span id='topic+check_outliers'></span><span id='topic+check_outliers.default'></span><span id='topic+check_outliers.numeric'></span><span id='topic+check_outliers.data.frame'></span><span id='topic+check_outliers.performance_simres'></span>

<h3>Description</h3>

<p>Checks for and locates influential observations (i.e.,
&quot;outliers&quot;) via several distance and/or clustering methods. If several
methods are selected, the returned &quot;Outlier&quot; vector will be a composite
outlier score, made of the average of the binary (0 or 1) results of each
method. It represents the probability of each observation of being
classified as an outlier by at least one method. The decision rule used by
default is to classify as outliers observations which composite outlier
score is superior or equal to 0.5 (i.e., that were classified as outliers
by at least half of the methods). See the <strong>Details</strong> section below
for a description of the methods.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_outliers(x, ...)

## Default S3 method:
check_outliers(
  x,
  method = c("cook", "pareto"),
  threshold = NULL,
  ID = NULL,
  verbose = TRUE,
  ...
)

## S3 method for class 'numeric'
check_outliers(x, method = "zscore_robust", threshold = NULL, ...)

## S3 method for class 'data.frame'
check_outliers(x, method = "mahalanobis", threshold = NULL, ID = NULL, ...)

## S3 method for class 'performance_simres'
check_outliers(
  x,
  type = "default",
  iterations = 100,
  alternative = "two.sided",
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="check_outliers_+3A_x">x</code></td>
<td>
<p>A model, a data.frame, a <code>performance_simres</code> <code><a href="#topic+simulate_residuals">simulate_residuals()</a></code>
or a <code>DHARMa</code> object.</p>
</td></tr>
<tr><td><code id="check_outliers_+3A_...">...</code></td>
<td>
<p>When <code>method = "ics"</code>, further arguments in <code>...</code> are passed
down to <code><a href="ICSOutlier.html#topic+ics.outlier">ICSOutlier::ics.outlier()</a></code>. When <code>method = "mahalanobis"</code>,
they are  passed down to <code><a href="stats.html#topic+mahalanobis">stats::mahalanobis()</a></code>. <code>percentage_central</code> can
be specified when <code>method = "mcd"</code>. For objects of class <code>performance_simres</code>
or <code>DHARMa</code>, further arguments are passed down to <code>DHARMa::testOutliers()</code>.</p>
</td></tr>
<tr><td><code id="check_outliers_+3A_method">method</code></td>
<td>
<p>The outlier detection method(s). Can be <code>"all"</code> or some of
<code>"cook"</code>, <code>"pareto"</code>, <code>"zscore"</code>, <code>"zscore_robust"</code>, <code>"iqr"</code>, <code>"ci"</code>, <code>"eti"</code>,
<code>"hdi"</code>, <code>"bci"</code>, <code>"mahalanobis"</code>, <code>"mahalanobis_robust"</code>, <code>"mcd"</code>, <code>"ics"</code>,
<code>"optics"</code> or <code>"lof"</code>.</p>
</td></tr>
<tr><td><code id="check_outliers_+3A_threshold">threshold</code></td>
<td>
<p>A list containing the threshold values for each method (e.g.
<code>list('mahalanobis' = 7, 'cook' = 1)</code>), above which an observation is
considered as outlier. If <code>NULL</code>, default values will be used (see
'Details'). If a numeric value is given, it will be used as the threshold
for any of the method run.</p>
</td></tr>
<tr><td><code id="check_outliers_+3A_id">ID</code></td>
<td>
<p>Optional, to report an ID column along with the row number.</p>
</td></tr>
<tr><td><code id="check_outliers_+3A_verbose">verbose</code></td>
<td>
<p>Toggle warnings.</p>
</td></tr>
<tr><td><code id="check_outliers_+3A_type">type</code></td>
<td>
<p>Type of method to test for outliers. Can be one of <code>"default"</code>,
<code>"binomial"</code> or <code>"bootstrap"</code>. Only applies when <code>x</code> is an object returned
by <code>simulate_residuals()</code> or of class <code>DHARMa</code>. See 'Details' in
<code>?DHARMa::testOutliers</code> for a detailed description of the types.</p>
</td></tr>
<tr><td><code id="check_outliers_+3A_iterations">iterations</code></td>
<td>
<p>Number of simulations to run.</p>
</td></tr>
<tr><td><code id="check_outliers_+3A_alternative">alternative</code></td>
<td>
<p>A character string specifying the alternative hypothesis.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Outliers can be defined as particularly influential observations.
Most methods rely on the computation of some distance metric, and the
observations greater than a certain threshold are considered outliers.
Importantly, outliers detection methods are meant to provide information to
consider for the researcher, rather than to be an automatized procedure
which mindless application is a substitute for thinking.
</p>
<p>An <strong>example sentence</strong> for reporting the usage of the composite method
could be:
</p>
<p><em>&quot;Based on a composite outlier score (see the 'check_outliers' function
in the 'performance' R package; Lüdecke et al., 2021) obtained via the joint
application of multiple outliers detection algorithms (Z-scores, Iglewicz,
1993; Interquartile range (IQR); Mahalanobis distance, Cabana, 2019; Robust
Mahalanobis distance, Gnanadesikan and Kettenring, 1972; Minimum Covariance
Determinant, Leys et al., 2018; Invariant Coordinate Selection, Archimbaud et
al., 2018; OPTICS, Ankerst et al., 1999; Isolation Forest, Liu et al. 2008;
and Local Outlier Factor, Breunig et al., 2000), we excluded n participants
that were classified as outliers by at least half of the methods used.&quot;</em>
</p>


<h3>Value</h3>

<p>A logical vector of the detected outliers with a nice printing
method: a check (message) on whether outliers were detected or not. The
information on the distance measure and whether or not an observation is
considered as outlier can be recovered with the <a href="base.html#topic+as.data.frame">as.data.frame</a>
function. Note that the function will (silently) return a vector of <code>FALSE</code>
for non-supported data types such as character strings.
</p>


<h3>Model-specific methods</h3>


<ul>
<li> <p><strong>Cook's Distance</strong>:
Among outlier detection methods, Cook's distance and leverage are less
common than the basic Mahalanobis distance, but still used. Cook's distance
estimates the variations in regression coefficients after removing each
observation, one by one (Cook, 1977). Since Cook's distance is in the metric
of an F distribution with p and n-p degrees of freedom, the median point of
the quantile distribution can be used as a cut-off (Bollen, 1985). A common
approximation or heuristic is to use 4 divided by the numbers of
observations, which usually corresponds to a lower threshold (i.e., more
outliers are detected). This only works for frequentist models. For Bayesian
models, see <code>pareto</code>.
</p>
</li>
<li> <p><strong>Pareto</strong>:
The reliability and approximate convergence of Bayesian models can be
assessed using the estimates for the shape parameter k of the generalized
Pareto distribution. If the estimated tail shape parameter k exceeds 0.5, the
user should be warned, although in practice the authors of the <strong>loo</strong>
package observed good performance for values of k up to 0.7 (the default
threshold used by <code>performance</code>).
</p>
</li></ul>



<h3>Univariate methods</h3>


<ul>
<li> <p><strong>Z-scores</strong> <code style="white-space: pre;">&#8288;("zscore", "zscore_robust")&#8288;</code>:
The Z-score, or standard score, is a way of describing a data point as
deviance from a central value, in terms of standard deviations from the mean
(<code>"zscore"</code>) or, as it is here the case (<code>"zscore_robust"</code>) by
default (Iglewicz, 1993), in terms of Median Absolute Deviation (MAD) from
the median (which are robust measures of dispersion and centrality). The
default threshold to classify outliers is 1.959 (<code>threshold = list("zscore" = 1.959)</code>),
corresponding to the 2.5% (<code>qnorm(0.975)</code>) most extreme observations
(assuming the data is normally distributed). Importantly, the Z-score
method is univariate: it is computed column by column. If a dataframe is
passed, the Z-score is calculated for each variable separately, and the
maximum (absolute) Z-score is kept for each observations. Thus, all
observations that are extreme on at least one variable might be detected
as outliers. Thus, this method is not suited for high dimensional data
(with many columns), returning too liberal results (detecting many outliers).
</p>
</li>
<li> <p><strong>IQR</strong> <code>("iqr")</code>:
Using the IQR (interquartile range) is a robust method developed by John
Tukey, which often appears in box-and-whisker plots (e.g., in
<a href="ggplot2.html#topic+geom_boxplot">ggplot2::geom_boxplot</a>). The interquartile range is the range between the first
and the third quartiles. Tukey considered as outliers any data point that
fell outside of either 1.5 times (the default threshold is 1.7) the IQR below
the first or above the third quartile. Similar to the Z-score method, this is
a univariate method for outliers detection, returning outliers detected for
at least one column, and might thus not be suited to high dimensional data.
The distance score for the IQR is the absolute deviation from the median of
the upper and lower IQR thresholds. Then, this value is divided by the IQR
threshold, to “standardize” it and facilitate interpretation.
</p>
</li>
<li> <p><strong>CI</strong> <code style="white-space: pre;">&#8288;("ci", "eti", "hdi", "bci")&#8288;</code>:
Another univariate method is to compute, for each variable, some sort of
&quot;confidence&quot; interval and consider as outliers values lying beyond the edges
of that interval. By default, <code>"ci"</code> computes the Equal-Tailed Interval
(<code>"eti"</code>), but other types of intervals are available, such as Highest
Density Interval (<code>"hdi"</code>) or the Bias Corrected and Accelerated
Interval (<code>"bci"</code>). The default threshold is <code>0.95</code>, considering
as outliers all observations that are outside the 95% CI on any of the
variable. See <code><a href="bayestestR.html#topic+ci">bayestestR::ci()</a></code> for more details
about the intervals. The distance score for the CI methods is the absolute
deviation from the median of the upper and lower CI thresholds. Then, this
value is divided by the difference between the upper and lower CI bounds
divided by two, to “standardize” it and facilitate interpretation.
</p>
</li></ul>



<h3>Multivariate methods</h3>


<ul>
<li> <p><strong>Mahalanobis Distance</strong>:
Mahalanobis distance (Mahalanobis, 1930) is often used for multivariate
outliers detection as this distance takes into account the shape of the
observations. The default <code>threshold</code> is often arbitrarily set to some
deviation (in terms of SD or MAD) from the mean (or median) of the
Mahalanobis distance. However, as the Mahalanobis distance can be
approximated by a Chi squared distribution (Rousseeuw and Van Zomeren, 1990),
we can use the alpha quantile of the chi-square distribution with k degrees
of freedom (k being the number of columns). By default, the alpha threshold
is set to 0.025 (corresponding to the 2.5\
Cabana, 2019). This criterion is a natural extension of the median plus or
minus a coefficient times the MAD method (Leys et al., 2013).
</p>
</li>
<li> <p><strong>Robust Mahalanobis Distance</strong>:
A robust version of Mahalanobis distance using an Orthogonalized
Gnanadesikan-Kettenring pairwise estimator (Gnanadesikan and Kettenring,
1972). Requires the <strong>bigutilsr</strong> package. See the <code><a href="bigutilsr.html#topic+covrob_ogk">bigutilsr::dist_ogk()</a></code>
function.
</p>
</li>
<li> <p><strong>Minimum Covariance Determinant (MCD)</strong>:
Another robust version of Mahalanobis. Leys et al. (2018) argue that
Mahalanobis Distance is not a robust way to determine outliers, as it uses
the means and covariances of all the data - including the outliers - to
determine individual difference scores. Minimum Covariance Determinant
calculates the mean and covariance matrix based on the most central subset of
the data (by default, 66\
is deemed to be a more robust method of identifying and removing outliers
than regular Mahalanobis distance.
This method has a <code>percentage_central</code> argument that allows specifying
the breakdown point (0.75, the default, is recommended by Leys et al. 2018,
but a commonly used alternative is 0.50).
</p>
</li>
<li> <p><strong>Invariant Coordinate Selection (ICS)</strong>:
The outlier are detected using ICS, which by default uses an alpha threshold
of 0.025 (corresponding to the 2.5\
value for outliers classification. Refer to the help-file of
<code><a href="ICSOutlier.html#topic+ics.outlier">ICSOutlier::ics.outlier()</a></code> to get more details about this procedure.
Note that <code>method = "ics"</code> requires both <strong>ICS</strong> and <strong>ICSOutlier</strong>
to be installed, and that it takes some time to compute the results. You
can speed up computation time using parallel computing. Set the number of
cores to use with <code>options(mc.cores = 4)</code> (for example).
</p>
</li>
<li> <p><strong>OPTICS</strong>:
The Ordering Points To Identify the Clustering Structure (OPTICS) algorithm
(Ankerst et al., 1999) is using similar concepts to DBSCAN (an unsupervised
clustering technique that can be used for outliers detection). The threshold
argument is passed as <code>minPts</code>, which corresponds to the minimum size
of a cluster. By default, this size is set at 2 times the number of columns
(Sander et al., 1998). Compared to the other techniques, that will always
detect several outliers (as these are usually defined as a percentage of
extreme values), this algorithm functions in a different manner and won't
always detect outliers. Note that <code>method = "optics"</code> requires the
<strong>dbscan</strong> package to be installed, and that it takes some time to compute
the results.
</p>
</li>
<li> <p><strong>Local Outlier Factor</strong>:
Based on a K nearest neighbors algorithm, LOF compares the local density of
a point to the local densities of its neighbors instead of computing a
distance from the center (Breunig et al., 2000). Points that have a
substantially lower density than their neighbors are considered outliers. A
LOF score of approximately 1 indicates that density around the point is
comparable to its neighbors. Scores significantly larger than 1 indicate
outliers. The default threshold of 0.025 will classify as outliers the
observations located at <code style="white-space: pre;">&#8288;qnorm(1-0.025) * SD)&#8288;</code> of the log-transformed
LOF distance. Requires the <strong>dbscan</strong> package.
</p>
</li></ul>



<h3>Methods for simulated residuals</h3>

<p>The approach for detecting outliers based on simulated residuals differs
from the traditional methods and may not be detecting outliers as expected.
Literally, this approach compares observed to simulated values. However, we
do not know the deviation of the observed data to the model expectation, and
thus, the term &quot;outlier&quot; should be taken with a grain of salt. It refers to
&quot;simulation outliers&quot;. Basically, the comparison tests whether on observed
data point is outside the simulated range. It is strongly recommended to read
the related documentations in the <strong>DHARMa</strong> package, e.g. <code>?DHARMa::testOutliers</code>.
</p>


<h3>Threshold specification</h3>

<p>Default thresholds are currently specified as follows:
</p>
<div class="sourceCode"><pre>list(
  zscore = stats::qnorm(p = 1 - 0.001 / 2),
  zscore_robust = stats::qnorm(p = 1 - 0.001 / 2),
  iqr = 1.7,
  ci = 1 - 0.001,
  eti = 1 - 0.001,
  hdi = 1 - 0.001,
  bci = 1 - 0.001,
  cook = stats::qf(0.5, ncol(x), nrow(x) - ncol(x)),
  pareto = 0.7,
  mahalanobis = stats::qchisq(p = 1 - 0.001, df = ncol(x)),
  mahalanobis_robust = stats::qchisq(p = 1 - 0.001, df = ncol(x)),
  mcd = stats::qchisq(p = 1 - 0.001, df = ncol(x)),
  ics = 0.001,
  optics = 2 * ncol(x),
  lof = 0.001
)
</pre></div>


<h3>Meta-analysis models</h3>

<p>For meta-analysis models (e.g. objects of class <code>rma</code> from the <em>metafor</em>
package or <code>metagen</code> from package <em>meta</em>), studies are defined as outliers
when their confidence interval lies outside the confidence interval of the
pooled effect.
</p>


<h3>Note</h3>

<p>There is also a
<a href="https://easystats.github.io/see/articles/performance.html"><code>plot()</code>-method</a>
implemented in the
<a href="https://easystats.github.io/see/"><span class="pkg">see</span>-package</a>. <strong>Please
note</strong> that the range of the distance-values along the y-axis is re-scaled
to range from 0 to 1.
</p>


<h3>References</h3>


<ul>
<li><p> Archimbaud, A., Nordhausen, K., and Ruiz-Gazen, A. (2018). ICS for
multivariate outlier detection with application to quality control.
Computational Statistics and Data Analysis, 128, 184-199.
<a href="https://doi.org/10.1016/j.csda.2018.06.011">doi:10.1016/j.csda.2018.06.011</a>
</p>
</li>
<li><p> Gnanadesikan, R., and Kettenring, J. R. (1972). Robust estimates, residuals,
and outlier detection with multiresponse data. Biometrics, 81-124.
</p>
</li>
<li><p> Bollen, K. A., and Jackman, R. W. (1985). Regression diagnostics: An
expository treatment of outliers and influential cases. Sociological Methods
and Research, 13(4), 510-542.
</p>
</li>
<li><p> Cabana, E., Lillo, R. E., and Laniado, H. (2019). Multivariate outlier
detection based on a robust Mahalanobis distance with shrinkage estimators.
arXiv preprint arXiv:1904.02596.
</p>
</li>
<li><p> Cook, R. D. (1977). Detection of influential observation in linear
regression. Technometrics, 19(1), 15-18.
</p>
</li>
<li><p> Iglewicz, B., and Hoaglin, D. C. (1993). How to detect and handle outliers
(Vol. 16). Asq Press.
</p>
</li>
<li><p> Leys, C., Klein, O., Dominicy, Y., and Ley, C. (2018). Detecting
multivariate outliers: Use a robust variant of Mahalanobis distance. Journal
of Experimental Social Psychology, 74, 150-156.
</p>
</li>
<li><p> Liu, F. T., Ting, K. M., and Zhou, Z. H. (2008, December). Isolation forest.
In 2008 Eighth IEEE International Conference on Data Mining (pp. 413-422).
IEEE.
</p>
</li>
<li><p> Lüdecke, D., Ben-Shachar, M. S., Patil, I., Waggoner, P., and Makowski, D.
(2021). performance: An R package for assessment, comparison and testing of
statistical models. Journal of Open Source Software, 6(60), 3139.
<a href="https://doi.org/10.21105/joss.03139">doi:10.21105/joss.03139</a>
</p>
</li>
<li><p> Thériault, R., Ben-Shachar, M. S., Patil, I., Lüdecke, D., Wiernik, B. M.,
and Makowski, D. (2023). Check your outliers! An introduction to identifying
statistical outliers in R with easystats. <a href="https://doi.org/10.31234/osf.io/bu6nt">doi:10.31234/osf.io/bu6nt</a>
</p>
</li>
<li><p> Rousseeuw, P. J., and Van Zomeren, B. C. (1990). Unmasking multivariate
outliers and leverage points. Journal of the American Statistical
association, 85(411), 633-639.
</p>
</li></ul>



<h3>See Also</h3>

<p>Other functions to check model assumptions and and assess model quality: 
<code><a href="#topic+check_autocorrelation">check_autocorrelation</a>()</code>,
<code><a href="#topic+check_collinearity">check_collinearity</a>()</code>,
<code><a href="#topic+check_convergence">check_convergence</a>()</code>,
<code><a href="#topic+check_heteroscedasticity">check_heteroscedasticity</a>()</code>,
<code><a href="#topic+check_homogeneity">check_homogeneity</a>()</code>,
<code><a href="#topic+check_model">check_model</a>()</code>,
<code><a href="#topic+check_overdispersion">check_overdispersion</a>()</code>,
<code><a href="#topic+check_predictions">check_predictions</a>()</code>,
<code><a href="#topic+check_singularity">check_singularity</a>()</code>,
<code><a href="#topic+check_zeroinflation">check_zeroinflation</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data &lt;- mtcars # Size nrow(data) = 32

# For single variables ------------------------------------------------------
outliers_list &lt;- check_outliers(data$mpg) # Find outliers
outliers_list # Show the row index of the outliers
as.numeric(outliers_list) # The object is a binary vector...
filtered_data &lt;- data[!outliers_list, ] # And can be used to filter a dataframe
nrow(filtered_data) # New size, 28 (4 outliers removed)

# Find all observations beyond +/- 2 SD
check_outliers(data$mpg, method = "zscore", threshold = 2)

# For dataframes ------------------------------------------------------
check_outliers(data) # It works the same way on dataframes

# You can also use multiple methods at once
outliers_list &lt;- check_outliers(data, method = c(
  "mahalanobis",
  "iqr",
  "zscore"
))
outliers_list

# Using `as.data.frame()`, we can access more details!
outliers_info &lt;- as.data.frame(outliers_list)
head(outliers_info)
outliers_info$Outlier # Including the probability of being an outlier

# And we can be more stringent in our outliers removal process
filtered_data &lt;- data[outliers_info$Outlier &lt; 0.1, ]

# We can run the function stratified by groups using `{datawizard}` package:
group_iris &lt;- datawizard::data_group(iris, "Species")
check_outliers(group_iris)



# You can also run all the methods
check_outliers(data, method = "all", verbose = FALSE)

# For statistical models ---------------------------------------------
# select only mpg and disp (continuous)
mt1 &lt;- mtcars[, c(1, 3, 4)]
# create some fake outliers and attach outliers to main df
mt2 &lt;- rbind(mt1, data.frame(
  mpg = c(37, 40), disp = c(300, 400),
  hp = c(110, 120)
))
# fit model with outliers
model &lt;- lm(disp ~ mpg + hp, data = mt2)

outliers_list &lt;- check_outliers(model)
plot(outliers_list)

insight::get_data(model)[outliers_list, ] # Show outliers data


</code></pre>

<hr>
<h2 id='check_overdispersion'>Check overdispersion (and underdispersion) of GL(M)M's</h2><span id='topic+check_overdispersion'></span><span id='topic+check_overdispersion.performance_simres'></span>

<h3>Description</h3>

<p><code>check_overdispersion()</code> checks generalized linear (mixed)
models for overdispersion (and underdispersion).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_overdispersion(x, ...)

## S3 method for class 'performance_simres'
check_overdispersion(x, alternative = c("two.sided", "less", "greater"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="check_overdispersion_+3A_x">x</code></td>
<td>
<p>Fitted model of class <code>merMod</code>, <code>glmmTMB</code>, <code>glm</code>, or <code>glm.nb</code>
(package <strong>MASS</strong>), or an object returned by <code>simulate_residuals()</code>.</p>
</td></tr>
<tr><td><code id="check_overdispersion_+3A_...">...</code></td>
<td>
<p>Arguments passed down to <code><a href="#topic+simulate_residuals">simulate_residuals()</a></code>. This only applies
for models with zero-inflation component, or for models of class <code>glmmTMB</code>
from <code>nbinom1</code> or <code>nbinom2</code> family.</p>
</td></tr>
<tr><td><code id="check_overdispersion_+3A_alternative">alternative</code></td>
<td>
<p>A character string specifying the alternative hypothesis.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Overdispersion occurs when the observed variance is higher than the
variance of a theoretical model. For Poisson models, variance increases
with the mean and, therefore, variance usually (roughly) equals the mean
value. If the variance is much higher, the data are &quot;overdispersed&quot;. A less
common case is underdispersion, where the variance is much lower than the
mean.
</p>


<h3>Value</h3>

<p>A list with results from the overdispersion test, like chi-squared
statistics, p-value or dispersion ratio.
</p>


<h3>Interpretation of the Dispersion Ratio</h3>

<p>If the dispersion ratio is close to one, a Poisson model fits well to the
data. Dispersion ratios larger than one indicate overdispersion, thus a
negative binomial model or similar might fit better to the data. Dispersion
ratios much smaller than one indicate underdispersion. A p-value &lt; .05
indicates either overdispersion or underdispersion (the first being more common).
</p>


<h3>Overdispersion in Poisson Models</h3>

<p>For Poisson models, the overdispersion test is based on the code from
<em>Gelman and Hill (2007), page 115</em>.
</p>


<h3>Overdispersion in Negative Binomial or Zero-Inflated Models</h3>

<p>For negative binomial (mixed) models or models with zero-inflation component,
the overdispersion test is based simulated residuals (see <code><a href="#topic+simulate_residuals">simulate_residuals()</a></code>).
</p>


<h3>Overdispersion in Mixed Models</h3>

<p>For <code>merMod</code>- and <code>glmmTMB</code>-objects, <code>check_overdispersion()</code>
is based on the code in the
<a href="http://bbolker.github.io/mixedmodels-misc/glmmFAQ.html">GLMM FAQ</a>,
section <em>How can I deal with overdispersion in GLMMs?</em>. Note that this
function only returns an <em>approximate</em> estimate of an overdispersion
parameter. Using this approach would be inaccurate for zero-inflated or
negative binomial mixed models (fitted with <code>glmmTMB</code>), thus, in such cases,
the overdispersion test is based on <code><a href="#topic+simulate_residuals">simulate_residuals()</a></code> (which is identical
to <code>check_overdispersion(simulate_residuals(model))</code>).
</p>


<h3>How to fix Overdispersion</h3>

<p>Overdispersion can be fixed by either modeling the dispersion parameter, or
by choosing a different distributional family (like Quasi-Poisson, or
negative binomial, see <em>Gelman and Hill (2007), pages 115-116</em>).
</p>


<h3>Tests based on simulated residuals</h3>

<p>For certain models, resp. model from certain families, tests are based on
simulated residuals (see <code><a href="#topic+simulate_residuals">simulate_residuals()</a></code>). These are usually more
accurate for testing such models than the traditionally used Pearson residuals.
However, when simulating from more complex models, such as mixed models or
models with zero-inflation, there are several important considerations.
Arguments specified in <code>...</code> are passed to <code><a href="#topic+simulate_residuals">simulate_residuals()</a></code>, which
relies on <code><a href="DHARMa.html#topic+simulateResiduals">DHARMa::simulateResiduals()</a></code> (and therefore, arguments in <code>...</code>
are passed further down to <em>DHARMa</em>). The defaults in DHARMa are set on the
most conservative option that works for all models. However, in many cases,
the help advises to use different settings in particular situations or for
particular models. It is recommended to read the 'Details' in
<code>?DHARMa::simulateResiduals</code> closely to understand the implications of the
simulation process and which arguments should be modified to get the most
accurate results.
</p>


<h3>References</h3>


<ul>
<li><p> Bolker B et al. (2017):
<a href="http://bbolker.github.io/mixedmodels-misc/glmmFAQ.html">GLMM FAQ.</a>
</p>
</li>
<li><p> Gelman, A., and Hill, J. (2007). Data analysis using regression and
multilevel/hierarchical models. Cambridge; New York: Cambridge University
Press.
</p>
</li></ul>



<h3>See Also</h3>

<p>Other functions to check model assumptions and and assess model quality: 
<code><a href="#topic+check_autocorrelation">check_autocorrelation</a>()</code>,
<code><a href="#topic+check_collinearity">check_collinearity</a>()</code>,
<code><a href="#topic+check_convergence">check_convergence</a>()</code>,
<code><a href="#topic+check_heteroscedasticity">check_heteroscedasticity</a>()</code>,
<code><a href="#topic+check_homogeneity">check_homogeneity</a>()</code>,
<code><a href="#topic+check_model">check_model</a>()</code>,
<code><a href="#topic+check_outliers">check_outliers</a>()</code>,
<code><a href="#topic+check_predictions">check_predictions</a>()</code>,
<code><a href="#topic+check_singularity">check_singularity</a>()</code>,
<code><a href="#topic+check_zeroinflation">check_zeroinflation</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(Salamanders, package = "glmmTMB")
m &lt;- glm(count ~ spp + mined, family = poisson, data = Salamanders)
check_overdispersion(m)

</code></pre>

<hr>
<h2 id='check_predictions'>Posterior predictive checks</h2><span id='topic+check_predictions'></span><span id='topic+check_predictions.default'></span><span id='topic+posterior_predictive_check'></span><span id='topic+check_posterior_predictions'></span>

<h3>Description</h3>

<p>Posterior predictive checks mean &quot;simulating replicated data
under the fitted model and then comparing these to the observed data&quot;
(<em>Gelman and Hill, 2007, p. 158</em>). Posterior predictive checks
can be used to &quot;look for systematic discrepancies between real and
simulated data&quot; (<em>Gelman et al. 2014, p. 169</em>).
</p>
<p><strong>performance</strong> provides posterior predictive check methods for a variety
of frequentist models (e.g., <code>lm</code>, <code>merMod</code>, <code>glmmTMB</code>, ...). For Bayesian
models, the model is passed to <code><a href="bayesplot.html#topic+pp_check">bayesplot::pp_check()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_predictions(object, ...)

## Default S3 method:
check_predictions(
  object,
  iterations = 50,
  check_range = FALSE,
  re_formula = NULL,
  bandwidth = "nrd",
  type = "density",
  verbose = TRUE,
  ...
)

posterior_predictive_check(object, ...)

check_posterior_predictions(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="check_predictions_+3A_object">object</code></td>
<td>
<p>A statistical model.</p>
</td></tr>
<tr><td><code id="check_predictions_+3A_...">...</code></td>
<td>
<p>Passed down to <code>simulate()</code>.</p>
</td></tr>
<tr><td><code id="check_predictions_+3A_iterations">iterations</code></td>
<td>
<p>The number of draws to simulate/bootstrap.</p>
</td></tr>
<tr><td><code id="check_predictions_+3A_check_range">check_range</code></td>
<td>
<p>Logical, if <code>TRUE</code>, includes a plot with the minimum
value of the original response against the minimum values of the replicated
responses, and the same for the maximum value. This plot helps judging whether
the variation in the original data is captured by the model or not
(<em>Gelman et al. 2020, pp.163</em>). The minimum and maximum values of <code>y</code> should
be inside the range of the related minimum and maximum values of <code>yrep</code>.</p>
</td></tr>
<tr><td><code id="check_predictions_+3A_re_formula">re_formula</code></td>
<td>
<p>Formula containing group-level effects (random effects) to
be considered in the simulated data. If <code>NULL</code> (default), condition
on all random effects. If <code>NA</code> or <code>~0</code>, condition on no random
effects. See <code>simulate()</code> in <strong>lme4</strong>.</p>
</td></tr>
<tr><td><code id="check_predictions_+3A_bandwidth">bandwidth</code></td>
<td>
<p>A character string indicating the smoothing bandwidth to
be used. Unlike <code>stats::density()</code>, which used <code>"nrd0"</code> as default, the
default used here is <code>"nrd"</code> (which seems to give more plausible results
for non-Gaussian models). When problems with plotting occur, try to change
to a different value.</p>
</td></tr>
<tr><td><code id="check_predictions_+3A_type">type</code></td>
<td>
<p>Plot type for the posterior predictive checks plot. Can be <code>"density"</code>,
<code>"discrete_dots"</code>, <code>"discrete_interval"</code> or <code>"discrete_both"</code> (the <code style="white-space: pre;">&#8288;discrete_*&#8288;</code>
options are appropriate for models with discrete - binary, integer or ordinal
etc. - outcomes).</p>
</td></tr>
<tr><td><code id="check_predictions_+3A_verbose">verbose</code></td>
<td>
<p>Toggle warnings.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>An example how posterior predictive checks can also be used for model
comparison is Figure 6 from <em>Gabry et al. 2019, Figure 6</em>.
</p>
<p><br /> <img src="../help/figures/pp_check.png" width="90%" alt="Posterior Predictive Check" /> <br />
The model shown in the right panel (b) can simulate new data that are more
similar to the observed outcome than the model in the left panel (a). Thus,
model (b) is likely to be preferred over model (a).
</p>


<h3>Value</h3>

<p>A data frame of simulated responses and the original response vector.
</p>


<h3>Note</h3>

<p>Every model object that has a <code>simulate()</code>-method should work with
<code>check_predictions()</code>. On R 3.6.0 and higher, if <strong>bayesplot</strong> (or a
package that imports <strong>bayesplot</strong> such as <strong>rstanarm</strong> or <strong>brms</strong>)
is loaded, <code>pp_check()</code> is also available as an alias for <code>check_predictions()</code>.
</p>


<h3>References</h3>


<ul>
<li><p> Gabry, J., Simpson, D., Vehtari, A., Betancourt, M., and Gelman, A. (2019).
Visualization in Bayesian workflow. Journal of the Royal Statistical Society:
Series A (Statistics in Society), 182(2), 389–402. https://doi.org/10.1111/rssa.12378
</p>
</li>
<li><p> Gelman, A., and Hill, J. (2007). Data analysis using regression and
multilevel/hierarchical models. Cambridge; New York: Cambridge University Press.
</p>
</li>
<li><p> Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., and
Rubin, D. B. (2014). Bayesian data analysis. (Third edition). CRC Press.
</p>
</li>
<li><p> Gelman, A., Hill, J., and Vehtari, A. (2020). Regression and Other Stories.
Cambridge University Press.
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+simulate_residuals">simulate_residuals()</a></code> and <code><a href="#topic+check_residuals">check_residuals()</a></code>.
</p>
<p>Other functions to check model assumptions and and assess model quality: 
<code><a href="#topic+check_autocorrelation">check_autocorrelation</a>()</code>,
<code><a href="#topic+check_collinearity">check_collinearity</a>()</code>,
<code><a href="#topic+check_convergence">check_convergence</a>()</code>,
<code><a href="#topic+check_heteroscedasticity">check_heteroscedasticity</a>()</code>,
<code><a href="#topic+check_homogeneity">check_homogeneity</a>()</code>,
<code><a href="#topic+check_model">check_model</a>()</code>,
<code><a href="#topic+check_outliers">check_outliers</a>()</code>,
<code><a href="#topic+check_overdispersion">check_overdispersion</a>()</code>,
<code><a href="#topic+check_singularity">check_singularity</a>()</code>,
<code><a href="#topic+check_zeroinflation">check_zeroinflation</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# linear model
model &lt;- lm(mpg ~ disp, data = mtcars)
check_predictions(model)

# discrete/integer outcome
set.seed(99)
d &lt;- iris
d$skewed &lt;- rpois(150, 1)
model &lt;- glm(
  skewed ~ Species + Petal.Length + Petal.Width,
  family = poisson(),
  data = d
)
check_predictions(model, type = "discrete_both")

</code></pre>

<hr>
<h2 id='check_residuals'>Check uniformity of simulated residuals</h2><span id='topic+check_residuals'></span><span id='topic+check_residuals.default'></span>

<h3>Description</h3>

<p><code>check_residuals()</code> checks generalized linear (mixed) models for uniformity
of randomized quantile residuals, which can be used to identify typical model
misspecification problems, such as over/underdispersion, zero-inflation, and
residual spatial and temporal autocorrelation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_residuals(x, ...)

## Default S3 method:
check_residuals(x, alternative = c("two.sided", "less", "greater"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="check_residuals_+3A_x">x</code></td>
<td>
<p>An object returned by <code><a href="#topic+simulate_residuals">simulate_residuals()</a></code> or
<code><a href="DHARMa.html#topic+simulateResiduals">DHARMa::simulateResiduals()</a></code>.</p>
</td></tr>
<tr><td><code id="check_residuals_+3A_...">...</code></td>
<td>
<p>Passed down to <code><a href="stats.html#topic+ks.test">stats::ks.test()</a></code>.</p>
</td></tr>
<tr><td><code id="check_residuals_+3A_alternative">alternative</code></td>
<td>
<p>A character string specifying the alternative hypothesis.
See <code><a href="stats.html#topic+ks.test">stats::ks.test()</a></code> for details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Uniformity of residuals is checked using a Kolmogorov-Smirnov test.
There is a <code>plot()</code> method to visualize the distribution of the residuals.
The test for uniformity basically tests to which extent the observed values
deviate from the model expectations (i.e. simulated values). In this sense,
the <code>check_residuals()</code> function has similar goals like <code><a href="#topic+check_predictions">check_predictions()</a></code>.
</p>


<h3>Value</h3>

<p>The p-value of the test statistics.
</p>


<h3>Tests based on simulated residuals</h3>

<p>For certain models, resp. model from certain families, tests like
<code><a href="#topic+check_zeroinflation">check_zeroinflation()</a></code> or <code><a href="#topic+check_overdispersion">check_overdispersion()</a></code> are based on
simulated residuals. These are usually more accurate for such tests than
the traditionally used Pearson residuals. However, when simulating from more
complex models, such as mixed models or models with zero-inflation, there are
several important considerations. <code>simulate_residuals()</code> relies on
<code><a href="DHARMa.html#topic+simulateResiduals">DHARMa::simulateResiduals()</a></code>, and additional arguments specified in <code>...</code>
are passed further down to that function. The defaults in DHARMa are set on
the most conservative option that works for all models. However, in many
cases, the help advises to use different settings in particular situations
or for particular models. It is recommended to read the 'Details' in
<code>?DHARMa::simulateResiduals</code> closely to understand the implications of the
simulation process and which arguments should be modified to get the most
accurate results.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+simulate_residuals">simulate_residuals()</a></code>, <code><a href="#topic+check_zeroinflation">check_zeroinflation()</a></code>,
<code><a href="#topic+check_overdispersion">check_overdispersion()</a></code> and <code><a href="#topic+check_predictions">check_predictions()</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
dat &lt;- DHARMa::createData(sampleSize = 100, overdispersion = 0.5, family = poisson())
m &lt;- glm(observedResponse ~ Environment1, family = poisson(), data = dat)
res &lt;- simulate_residuals(m)
check_residuals(res)

</code></pre>

<hr>
<h2 id='check_singularity'>Check mixed models for boundary fits</h2><span id='topic+check_singularity'></span>

<h3>Description</h3>

<p>Check mixed models for boundary fits.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_singularity(x, tolerance = 1e-05, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="check_singularity_+3A_x">x</code></td>
<td>
<p>A mixed model.</p>
</td></tr>
<tr><td><code id="check_singularity_+3A_tolerance">tolerance</code></td>
<td>
<p>Indicates up to which value the convergence result is
accepted. The larger <code>tolerance</code> is, the stricter the test
will be.</p>
</td></tr>
<tr><td><code id="check_singularity_+3A_...">...</code></td>
<td>
<p>Currently not used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If a model is &quot;singular&quot;, this means that some dimensions of the
variance-covariance matrix have been estimated as exactly zero. This
often occurs for mixed models with complex random effects structures.
</p>
<p>&quot;While singular models are statistically well defined (it is theoretically
sensible for the true maximum likelihood estimate to correspond to a singular
fit), there are real concerns that (1) singular fits correspond to overfitted
models that may have poor power; (2) chances of numerical problems and
mis-convergence are higher for singular models (e.g. it may be computationally
difficult to compute profile confidence intervals for such models); (3)
standard inferential procedures such as Wald statistics and likelihood ratio
tests may be inappropriate.&quot; (<em>lme4 Reference Manual</em>)
</p>
<p>There is no gold-standard about how to deal with singularity and which
random-effects specification to choose. Beside using fully Bayesian methods
(with informative priors), proposals in a frequentist framework are:
</p>

<ul>
<li><p> avoid fitting overly complex models, such that the variance-covariance
matrices can be estimated precisely enough (<em>Matuschek et al. 2017</em>)
</p>
</li>
<li><p> use some form of model selection to choose a model that balances
predictive accuracy and overfitting/type I error (<em>Bates et al. 2015</em>,
<em>Matuschek et al. 2017</em>)
</p>
</li>
<li><p> &quot;keep it maximal&quot;, i.e. fit the most complex model consistent with the
experimental design, removing only terms required to allow a non-singular
fit (<em>Barr et al. 2013</em>)
</p>
</li></ul>

<p>Note the different meaning between singularity and convergence: singularity
indicates an issue with the &quot;true&quot; best estimate, i.e. whether the maximum
likelihood estimation for the variance-covariance matrix of the random
effects is positive definite or only semi-definite. Convergence is a
question of whether we can assume that the numerical optimization has
worked correctly or not.
</p>


<h3>Value</h3>

<p><code>TRUE</code> if the model fit is singular.
</p>


<h3>References</h3>


<ul>
<li><p> Bates D, Kliegl R, Vasishth S, Baayen H. Parsimonious Mixed Models.
arXiv:1506.04967, June 2015.
</p>
</li>
<li><p> Barr DJ, Levy R, Scheepers C, Tily HJ. Random effects structure for
confirmatory hypothesis testing: Keep it maximal. Journal of Memory and
Language, 68(3):255-278, April 2013.
</p>
</li>
<li><p> Matuschek H, Kliegl R, Vasishth S, Baayen H, Bates D. Balancing type
I error and power in linear mixed models. Journal of Memory and Language,
94:305-315, 2017.
</p>
</li>
<li><p> lme4 Reference Manual, <a href="https://cran.r-project.org/package=lme4">https://cran.r-project.org/package=lme4</a>
</p>
</li></ul>



<h3>See Also</h3>

<p>Other functions to check model assumptions and and assess model quality: 
<code><a href="#topic+check_autocorrelation">check_autocorrelation</a>()</code>,
<code><a href="#topic+check_collinearity">check_collinearity</a>()</code>,
<code><a href="#topic+check_convergence">check_convergence</a>()</code>,
<code><a href="#topic+check_heteroscedasticity">check_heteroscedasticity</a>()</code>,
<code><a href="#topic+check_homogeneity">check_homogeneity</a>()</code>,
<code><a href="#topic+check_model">check_model</a>()</code>,
<code><a href="#topic+check_outliers">check_outliers</a>()</code>,
<code><a href="#topic+check_overdispersion">check_overdispersion</a>()</code>,
<code><a href="#topic+check_predictions">check_predictions</a>()</code>,
<code><a href="#topic+check_zeroinflation">check_zeroinflation</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(lme4)
data(sleepstudy)
set.seed(123)
sleepstudy$mygrp &lt;- sample(1:5, size = 180, replace = TRUE)
sleepstudy$mysubgrp &lt;- NA
for (i in 1:5) {
  filter_group &lt;- sleepstudy$mygrp == i
  sleepstudy$mysubgrp[filter_group] &lt;-
    sample(1:30, size = sum(filter_group), replace = TRUE)
}

model &lt;- lmer(
  Reaction ~ Days + (1 | mygrp / mysubgrp) + (1 | Subject),
  data = sleepstudy
)

check_singularity(model)

</code></pre>

<hr>
<h2 id='check_sphericity'>Check model for violation of sphericity</h2><span id='topic+check_sphericity'></span>

<h3>Description</h3>

<p>Check model for violation of sphericity. For <a href="#topic+check_factorstructure">Bartlett's Test of Sphericity</a>
(used for correlation matrices and factor analyses), see <a href="#topic+check_sphericity_bartlett">check_sphericity_bartlett</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_sphericity(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="check_sphericity_+3A_x">x</code></td>
<td>
<p>A model object.</p>
</td></tr>
<tr><td><code id="check_sphericity_+3A_...">...</code></td>
<td>
<p>Arguments passed to <code>car::Anova</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Invisibly returns the p-values of the test statistics. A p-value &lt;
0.05 indicates a violation of sphericity.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(Soils, package = "carData")
soils.mod &lt;- lm(
  cbind(pH, N, Dens, P, Ca, Mg, K, Na, Conduc) ~ Block + Contour * Depth,
  data = Soils
)

check_sphericity(Manova(soils.mod))

</code></pre>

<hr>
<h2 id='check_symmetry'>Check distribution symmetry</h2><span id='topic+check_symmetry'></span>

<h3>Description</h3>

<p>Uses Hotelling and Solomons test of symmetry by testing if the standardized
nonparametric skew (<code class="reqn">\frac{(Mean - Median)}{SD}</code>) is different than 0.
<br /><br />
This is an underlying assumption of Wilcoxon signed-rank test.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_symmetry(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="check_symmetry_+3A_x">x</code></td>
<td>
<p>Model or numeric vector</p>
</td></tr>
<tr><td><code id="check_symmetry_+3A_...">...</code></td>
<td>
<p>Not used.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>V &lt;- suppressWarnings(wilcox.test(mtcars$mpg))
check_symmetry(V)

</code></pre>

<hr>
<h2 id='check_zeroinflation'>Check for zero-inflation in count models</h2><span id='topic+check_zeroinflation'></span><span id='topic+check_zeroinflation.default'></span><span id='topic+check_zeroinflation.performance_simres'></span>

<h3>Description</h3>

<p><code>check_zeroinflation()</code> checks whether count models are
over- or underfitting zeros in the outcome.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>check_zeroinflation(x, ...)

## Default S3 method:
check_zeroinflation(x, tolerance = 0.05, ...)

## S3 method for class 'performance_simres'
check_zeroinflation(
  x,
  tolerance = 0.1,
  alternative = c("two.sided", "less", "greater"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="check_zeroinflation_+3A_x">x</code></td>
<td>
<p>Fitted model of class <code>merMod</code>, <code>glmmTMB</code>, <code>glm</code>, or <code>glm.nb</code>
(package <strong>MASS</strong>).</p>
</td></tr>
<tr><td><code id="check_zeroinflation_+3A_...">...</code></td>
<td>
<p>Arguments passed down to <code><a href="#topic+simulate_residuals">simulate_residuals()</a></code>. This only applies
for models with zero-inflation component, or for models of class <code>glmmTMB</code>
from <code>nbinom1</code> or <code>nbinom2</code> family.</p>
</td></tr>
<tr><td><code id="check_zeroinflation_+3A_tolerance">tolerance</code></td>
<td>
<p>The tolerance for the ratio of observed and predicted
zeros to considered as over- or underfitting zeros. A ratio
between 1 +/- <code>tolerance</code> is considered as OK, while a ratio
beyond or below this threshold would indicate over- or underfitting.</p>
</td></tr>
<tr><td><code id="check_zeroinflation_+3A_alternative">alternative</code></td>
<td>
<p>A character string specifying the alternative hypothesis.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If the amount of observed zeros is larger than the amount of
predicted zeros, the model is underfitting zeros, which indicates a
zero-inflation in the data. In such cases, it is recommended to use
negative binomial or zero-inflated models.
</p>
<p>In case of negative binomial models, models with zero-inflation component,
or hurdle models, the results from <code>check_zeroinflation()</code> are based on
<code><a href="#topic+simulate_residuals">simulate_residuals()</a></code>, i.e. <code>check_zeroinflation(simulate_residuals(model))</code>
is internally called if necessary.
</p>


<h3>Value</h3>

<p>A list with information about the amount of predicted and observed
zeros in the outcome, as well as the ratio between these two values.
</p>


<h3>Tests based on simulated residuals</h3>

<p>For certain models, resp. model from certain families, tests are based on
simulated residuals (see <code><a href="#topic+simulate_residuals">simulate_residuals()</a></code>). These are usually more
accurate for testing such models than the traditionally used Pearson residuals.
However, when simulating from more complex models, such as mixed models or
models with zero-inflation, there are several important considerations.
Arguments specified in <code>...</code> are passed to <code><a href="#topic+simulate_residuals">simulate_residuals()</a></code>, which
relies on <code><a href="DHARMa.html#topic+simulateResiduals">DHARMa::simulateResiduals()</a></code> (and therefore, arguments in <code>...</code>
are passed further down to <em>DHARMa</em>). The defaults in DHARMa are set on the
most conservative option that works for all models. However, in many cases,
the help advises to use different settings in particular situations or for
particular models. It is recommended to read the 'Details' in
<code>?DHARMa::simulateResiduals</code> closely to understand the implications of the
simulation process and which arguments should be modified to get the most
accurate results.
</p>


<h3>See Also</h3>

<p>Other functions to check model assumptions and and assess model quality: 
<code><a href="#topic+check_autocorrelation">check_autocorrelation</a>()</code>,
<code><a href="#topic+check_collinearity">check_collinearity</a>()</code>,
<code><a href="#topic+check_convergence">check_convergence</a>()</code>,
<code><a href="#topic+check_heteroscedasticity">check_heteroscedasticity</a>()</code>,
<code><a href="#topic+check_homogeneity">check_homogeneity</a>()</code>,
<code><a href="#topic+check_model">check_model</a>()</code>,
<code><a href="#topic+check_outliers">check_outliers</a>()</code>,
<code><a href="#topic+check_overdispersion">check_overdispersion</a>()</code>,
<code><a href="#topic+check_predictions">check_predictions</a>()</code>,
<code><a href="#topic+check_singularity">check_singularity</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(Salamanders, package = "glmmTMB")
m &lt;- glm(count ~ spp + mined, family = poisson, data = Salamanders)
check_zeroinflation(m)

# for models with zero-inflation component, it's better to carry out
# the check for zero-inflation using simulated residuals
m &lt;- glmmTMB::glmmTMB(
  count ~ spp + mined,
  ziformula = ~ mined + spp,
  family = poisson,
  data = Salamanders
)
res &lt;- simulate_residuals(m)
check_zeroinflation(res)

</code></pre>

<hr>
<h2 id='classify_distribution'>Classify the distribution of a model-family using machine learning</h2><span id='topic+classify_distribution'></span>

<h3>Description</h3>

<p>Classify the distribution of a model-family using machine learning
</p>


<h3>Details</h3>

<p>The trained model to classify distributions, which is used by the
<code>check_distribution()</code> function.
</p>

<hr>
<h2 id='compare_performance'>Compare performance of different models</h2><span id='topic+compare_performance'></span>

<h3>Description</h3>

<p><code>compare_performance()</code> computes indices of model
performance for different models at once and hence allows comparison of
indices across models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>compare_performance(
  ...,
  metrics = "all",
  rank = FALSE,
  estimator = "ML",
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="compare_performance_+3A_...">...</code></td>
<td>
<p>Multiple model objects (also of different classes).</p>
</td></tr>
<tr><td><code id="compare_performance_+3A_metrics">metrics</code></td>
<td>
<p>Can be <code>"all"</code>, <code>"common"</code> or a character vector of
metrics to be computed. See related
<code><a href="#topic+model_performance">documentation()</a></code> of object's class for
details.</p>
</td></tr>
<tr><td><code id="compare_performance_+3A_rank">rank</code></td>
<td>
<p>Logical, if <code>TRUE</code>, models are ranked according to 'best'
overall model performance. See 'Details'.</p>
</td></tr>
<tr><td><code id="compare_performance_+3A_estimator">estimator</code></td>
<td>
<p>Only for linear models. Corresponds to the different
estimators for the standard deviation of the errors. If <code>estimator = "ML"</code>
(default, except for <code>performance_aic()</code> when the model object is of class
<code>lmerMod</code>), the scaling is done by <code>n</code> (the biased ML estimator), which is
then equivalent to using <code>AIC(logLik())</code>. Setting it to <code>"REML"</code> will give
the same results as <code>AIC(logLik(..., REML = TRUE))</code>.</p>
</td></tr>
<tr><td><code id="compare_performance_+3A_verbose">verbose</code></td>
<td>
<p>Toggle warnings.</p>
</td></tr>
</table>


<h3>Details</h3>



<h4>Model Weights</h4>

<p>When information criteria (IC) are requested in <code>metrics</code> (i.e., any of <code>"all"</code>,
<code>"common"</code>, <code>"AIC"</code>, <code>"AICc"</code>, <code>"BIC"</code>, <code>"WAIC"</code>, or <code>"LOOIC"</code>), model
weights based on these criteria are also computed. For all IC except LOOIC,
weights are computed as <code>w = exp(-0.5 * delta_ic) / sum(exp(-0.5 * delta_ic))</code>,
where <code>delta_ic</code> is the difference between the model's IC value and the
smallest IC value in the model set (Burnham and Anderson, 2002).
For LOOIC, weights are computed as &quot;stacking weights&quot; using
<code><a href="loo.html#topic+loo_model_weights">loo::stacking_weights()</a></code>.
</p>



<h4>Ranking Models</h4>

<p>When <code>rank = TRUE</code>, a new column <code>Performance_Score</code> is returned.
This score ranges from 0\
performance. Note that all score value do not necessarily sum up to 100\
Rather, calculation is based on normalizing all indices (i.e. rescaling
them to a range from 0 to 1), and taking the mean value of all indices for
each model. This is a rather quick heuristic, but might be helpful as
exploratory index.
<br /> <br />
In particular when models are of different types (e.g. mixed models,
classical linear models, logistic regression, ...), not all indices will be
computed for each model. In case where an index can't be calculated for a
specific model type, this model gets an <code>NA</code> value. All indices that
have any <code>NA</code>s are excluded from calculating the performance score.
<br /> <br />
There is a <code>plot()</code>-method for <code>compare_performance()</code>,
which creates a &quot;spiderweb&quot; plot, where the different indices are
normalized and larger values indicate better model performance.
Hence, points closer to the center indicate worse fit indices
(see <a href="https://easystats.github.io/see/articles/performance.html">online-documentation</a>
for more details).
</p>



<h4>REML versus ML estimator</h4>

<p>By default, <code>estimator = "ML"</code>, which means that values from information
criteria (AIC, AICc, BIC) for specific model classes (like models from <em>lme4</em>)
are based on the ML-estimator, while the default behaviour of <code>AIC()</code> for
such classes is setting <code>REML = TRUE</code>. This default is intentional, because
comparing information criteria based on REML fits is usually not valid
(it might be useful, though, if all models share the same fixed effects -
however, this is usually not the case for nested models, which is a
prerequisite for the LRT). Set <code>estimator = "REML"</code> explicitly return the
same (AIC/...) values as from the defaults in <code>AIC.merMod()</code>.
</p>



<h3>Value</h3>

<p>A data frame with one row per model and one column per &quot;index&quot; (see
<code>metrics</code>).
</p>


<h3>Note</h3>

<p>There is also a <a href="https://easystats.github.io/see/articles/performance.html"><code>plot()</code>-method</a> implemented in the <a href="https://easystats.github.io/see/"><span class="pkg">see</span>-package</a>.
</p>


<h3>References</h3>

<p>Burnham, K. P., and Anderson, D. R. (2002).
<em>Model selection and multimodel inference: A practical information-theoretic approach</em> (2nd ed.).
Springer-Verlag. <a href="https://doi.org/10.1007/b97636">doi:10.1007/b97636</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(iris)
lm1 &lt;- lm(Sepal.Length ~ Species, data = iris)
lm2 &lt;- lm(Sepal.Length ~ Species + Petal.Length, data = iris)
lm3 &lt;- lm(Sepal.Length ~ Species * Petal.Length, data = iris)
compare_performance(lm1, lm2, lm3)
compare_performance(lm1, lm2, lm3, rank = TRUE)

m1 &lt;- lm(mpg ~ wt + cyl, data = mtcars)
m2 &lt;- glm(vs ~ wt + mpg, data = mtcars, family = "binomial")
m3 &lt;- lme4::lmer(Petal.Length ~ Sepal.Length + (1 | Species), data = iris)
compare_performance(m1, m2, m3)

</code></pre>

<hr>
<h2 id='cronbachs_alpha'>Cronbach's Alpha for Items or Scales</h2><span id='topic+cronbachs_alpha'></span>

<h3>Description</h3>

<p>Compute various measures of internal consistencies
for tests or item-scales of questionnaires.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cronbachs_alpha(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cronbachs_alpha_+3A_x">x</code></td>
<td>
<p>A matrix or a data frame.</p>
</td></tr>
<tr><td><code id="cronbachs_alpha_+3A_...">...</code></td>
<td>
<p>Currently not used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Cronbach's Alpha value for <code>x</code>. A value closer to 1
indicates greater internal consistency, where usually following
rule of thumb is applied to interpret the results:
&alpha; &lt; 0.5 is unacceptable,
0.5 &lt; &alpha; &lt; 0.6 is poor,
0.6 &lt; &alpha; &lt; 0.7 is questionable,
0.7 &lt; &alpha; &lt; 0.8 is acceptable,
and everything &gt; 0.8 is good or excellent.
</p>


<h3>Value</h3>

<p>The Cronbach's Alpha value for <code>x</code>.
</p>


<h3>References</h3>

<p>Bland, J. M., and Altman, D. G. Statistics notes: Cronbach's
alpha. BMJ 1997;314:572. 10.1136/bmj.314.7080.572
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(mtcars)
x &lt;- mtcars[, c("cyl", "gear", "carb", "hp")]
cronbachs_alpha(x)
</code></pre>

<hr>
<h2 id='display.performance_model'>Print tables in different output formats</h2><span id='topic+display.performance_model'></span><span id='topic+print_md.performance_model'></span><span id='topic+print_md.compare_performance'></span>

<h3>Description</h3>

<p>Prints tables (i.e. data frame) in different output formats.
<code>print_md()</code> is a alias for <code>display(format = "markdown")</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'performance_model'
display(object, format = "markdown", digits = 2, caption = NULL, ...)

## S3 method for class 'performance_model'
print_md(
  x,
  digits = 2,
  caption = "Indices of model performance",
  layout = "horizontal",
  ...
)

## S3 method for class 'compare_performance'
print_md(
  x,
  digits = 2,
  caption = "Comparison of Model Performance Indices",
  layout = "horizontal",
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="display.performance_model_+3A_object">object</code>, <code id="display.performance_model_+3A_x">x</code></td>
<td>
<p>An object returned by <code><a href="#topic+model_performance">model_performance()</a></code> or
or <code><a href="#topic+compare_performance">compare_performance()</a></code>.
or its summary.</p>
</td></tr>
<tr><td><code id="display.performance_model_+3A_format">format</code></td>
<td>
<p>String, indicating the output format. Currently, only
<code>"markdown"</code> is supported.</p>
</td></tr>
<tr><td><code id="display.performance_model_+3A_digits">digits</code></td>
<td>
<p>Number of decimal places.</p>
</td></tr>
<tr><td><code id="display.performance_model_+3A_caption">caption</code></td>
<td>
<p>Table caption as string. If <code>NULL</code>, no table caption is printed.</p>
</td></tr>
<tr><td><code id="display.performance_model_+3A_...">...</code></td>
<td>
<p>Currently not used.</p>
</td></tr>
<tr><td><code id="display.performance_model_+3A_layout">layout</code></td>
<td>
<p>Table layout (can be either <code>"horizontal"</code> or <code>"vertical"</code>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>display()</code> is useful when the table-output from functions,
which is usually printed as formatted text-table to console, should
be formatted for pretty table-rendering in markdown documents, or if
knitted from rmarkdown to PDF or Word files. See
<a href="https://easystats.github.io/parameters/articles/model_parameters_formatting.html">vignette</a>
for examples.
</p>


<h3>Value</h3>

<p>A character vector. If <code>format = "markdown"</code>, the return value
will be a character vector in markdown-table format.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>model &lt;- lm(mpg ~ wt + cyl, data = mtcars)
mp &lt;- model_performance(model)
display(mp)
</code></pre>

<hr>
<h2 id='icc'>Intraclass Correlation Coefficient (ICC)</h2><span id='topic+icc'></span><span id='topic+variance_decomposition'></span>

<h3>Description</h3>

<p>This function calculates the intraclass-correlation coefficient (ICC) -
sometimes also called <em>variance partition coefficient</em> (VPC) or
<em>repeatability</em> - for mixed effects models. The ICC can be calculated for all
models supported by <code>insight::get_variance()</code>. For models fitted with the
<strong>brms</strong>-package, <code>icc()</code> might fail due to the large variety of
models and families supported by the <strong>brms</strong>-package. In such cases, an
alternative to the ICC is the <code>variance_decomposition()</code>, which is based
on the posterior predictive distribution (see 'Details').
</p>


<h3>Usage</h3>

<pre><code class='language-R'>icc(
  model,
  by_group = FALSE,
  tolerance = 1e-05,
  ci = NULL,
  iterations = 100,
  ci_method = NULL,
  verbose = TRUE,
  ...
)

variance_decomposition(model, re_formula = NULL, robust = TRUE, ci = 0.95, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="icc_+3A_model">model</code></td>
<td>
<p>A (Bayesian) mixed effects model.</p>
</td></tr>
<tr><td><code id="icc_+3A_by_group">by_group</code></td>
<td>
<p>Logical, if <code>TRUE</code>, <code>icc()</code> returns the variance
components for each random-effects level (if there are multiple levels).
See 'Details'.</p>
</td></tr>
<tr><td><code id="icc_+3A_tolerance">tolerance</code></td>
<td>
<p>Tolerance for singularity check of random effects, to decide
whether to compute random effect variances or not. Indicates up to which
value the convergence result is accepted. The larger tolerance is, the
stricter the test will be. See <code><a href="#topic+check_singularity">performance::check_singularity()</a></code>.</p>
</td></tr>
<tr><td><code id="icc_+3A_ci">ci</code></td>
<td>
<p>Confidence resp. credible interval level. For <code>icc()</code> and <code>r2()</code>,
confidence intervals are based on bootstrapped samples from the ICC resp.
R2 value. See <code>iterations</code>.</p>
</td></tr>
<tr><td><code id="icc_+3A_iterations">iterations</code></td>
<td>
<p>Number of bootstrap-replicates when computing confidence
intervals for the ICC or R2.</p>
</td></tr>
<tr><td><code id="icc_+3A_ci_method">ci_method</code></td>
<td>
<p>Character string, indicating the bootstrap-method. Should
be <code>NULL</code> (default), in which case <code>lme4::bootMer()</code> is used for
bootstrapped confidence intervals. However, if bootstrapped intervals cannot
be calculated this was, try <code>ci_method = "boot"</code>, which falls back to
<code>boot::boot()</code>. This may successfully return bootstrapped confidence intervals,
but bootstrapped samples may not be appropriate for the multilevel structure
of the model. There is also an option <code>ci_method = "analytical"</code>, which tries
to calculate analytical confidence assuming a chi-squared distribution.
However, these intervals are rather inaccurate and often too narrow. It is
recommended to calculate bootstrapped confidence intervals for mixed models.</p>
</td></tr>
<tr><td><code id="icc_+3A_verbose">verbose</code></td>
<td>
<p>Toggle warnings and messages.</p>
</td></tr>
<tr><td><code id="icc_+3A_...">...</code></td>
<td>
<p>Arguments passed down to <code>brms::posterior_predict()</code>.</p>
</td></tr>
<tr><td><code id="icc_+3A_re_formula">re_formula</code></td>
<td>
<p>Formula containing group-level effects to be considered in
the prediction. If <code>NULL</code> (default), include all group-level effects.
Else, for instance for nested models, name a specific group-level effect
to calculate the variance decomposition for this group-level. See 'Details'
and <code>?brms::posterior_predict</code>.</p>
</td></tr>
<tr><td><code id="icc_+3A_robust">robust</code></td>
<td>
<p>Logical, if <code>TRUE</code>, the median instead of mean is used to
calculate the central tendency of the variances.</p>
</td></tr>
</table>


<h3>Details</h3>



<h4>Interpretation</h4>

<p>The ICC can be interpreted as &quot;the proportion of the variance explained by
the grouping structure in the population&quot;. The grouping structure entails
that measurements are organized into groups (e.g., test scores in a school
can be grouped by classroom if there are multiple classrooms and each
classroom was administered the same test) and ICC indexes how strongly
measurements in the same group resemble each other. This index goes from 0,
if the grouping conveys no information, to 1, if all observations in a group
are identical (<em>Gelman and Hill, 2007, p. 258</em>). In other word, the ICC -
sometimes conceptualized as the measurement repeatability - &quot;can also be
interpreted as the expected correlation between two randomly drawn units
that are in the same group&quot; <em>(Hox 2010: 15)</em>, although this definition might
not apply to mixed models with more complex random effects structures. The
ICC can help determine whether a mixed model is even necessary: an ICC of
zero (or very close to zero) means the observations within clusters are no
more similar than observations from different clusters, and setting it as a
random factor might not be necessary.
</p>



<h4>Difference with R2</h4>

<p>The coefficient of determination R2 (that can be computed with <code><a href="#topic+r2">r2()</a></code>)
quantifies the proportion of variance explained by a statistical model, but
its definition in mixed model is complex (hence, different methods to compute
a proxy exist). ICC is related to R2 because they are both ratios of
variance components. More precisely, R2 is the proportion of the explained
variance (of the full model), while the ICC is the proportion of explained
variance that can be attributed to the random effects. In simple cases, the
ICC corresponds to the difference between the <em>conditional R2</em> and the
<em>marginal R2</em> (see <code><a href="#topic+r2_nakagawa">r2_nakagawa()</a></code>).
</p>



<h4>Calculation</h4>

<p>The ICC is calculated by dividing the random effect variance,
&sigma;<sup>2</sup><sub>i</sub>, by
the total variance, i.e. the sum of the random effect variance and the
residual variance, &sigma;<sup>2</sup><sub>&epsilon;</sub>.
</p>



<h4>Adjusted and unadjusted ICC</h4>

<p><code>icc()</code> calculates an adjusted and an unadjusted ICC, which both take all
sources of uncertainty (i.e. of <em>all random effects</em>) into account. While
the <em>adjusted ICC</em> only relates to the random effects, the <em>unadjusted ICC</em>
also takes the fixed effects variances into account, more precisely, the
fixed effects variance is added to the denominator of the formula to
calculate the ICC (see <em>Nakagawa et al. 2017</em>). Typically, the <em>adjusted</em>
ICC is of interest when the analysis of random effects is of interest.
<code>icc()</code> returns a meaningful ICC also for more complex random effects
structures, like models with random slopes or nested design (more than two
levels) and is applicable for models with other distributions than Gaussian.
For more details on the computation of the variances, see
<code>?insight::get_variance</code>.
</p>



<h4>ICC for unconditional and conditional models</h4>

<p>Usually, the ICC is calculated for the null model (&quot;unconditional model&quot;).
However, according to <em>Raudenbush and Bryk (2002)</em> or
<em>Rabe-Hesketh and Skrondal (2012)</em> it is also feasible to compute the
ICC for full models with covariates (&quot;conditional models&quot;) and compare how
much, e.g., a level-2 variable explains the portion of variation in the
grouping structure (random intercept).
</p>



<h4>ICC for specific group-levels</h4>

<p>The proportion of variance for specific levels related to the overall model
can be computed by setting <code>by_group = TRUE</code>. The reported ICC is
the variance for each (random effect) group compared to the total
variance of the model. For mixed models with a simple random intercept,
this is identical to the classical (adjusted) ICC.
</p>



<h4>Variance decomposition for brms-models</h4>

<p>If <code>model</code> is of class <code>brmsfit</code>, <code>icc()</code> might fail due to the large
variety of models and families supported by the <strong>brms</strong> package. In such
cases, <code>variance_decomposition()</code> is an alternative ICC measure. The function
calculates a variance decomposition based on the posterior predictive
distribution. In this case, first, the draws from the posterior predictive
distribution <em>not conditioned</em> on group-level terms
(<code>posterior_predict(..., re_formula = NA)</code>) are calculated as well as draws
from this distribution <em>conditioned</em> on <em>all random effects</em> (by default,
unless specified else in <code>re_formula</code>) are taken. Then, second, the variances
for each of these draws are calculated. The &quot;ICC&quot; is then the ratio between
these two variances. This is the recommended way to analyse
random-effect-variances for non-Gaussian models. It is then possible to
compare variances across models, also by specifying different group-level
terms via the <code>re_formula</code>-argument.
</p>
<p>Sometimes, when the variance of the posterior predictive distribution is
very large, the variance ratio in the output makes no sense, e.g. because
it is negative. In such cases, it might help to use <code>robust = TRUE</code>.
</p>



<h3>Value</h3>

<p>A list with two values, the adjusted ICC and the unadjusted ICC. For
<code>variance_decomposition()</code>, a list with two values, the decomposed
ICC as well as the credible intervals for this ICC.
</p>


<h3>References</h3>


<ul>
<li><p> Hox, J. J. (2010). Multilevel analysis: techniques and applications
(2nd ed). New York: Routledge.
</p>
</li>
<li><p> Nakagawa, S., Johnson, P. C. D., and Schielzeth, H. (2017). The
coefficient of determination R2 and intra-class correlation coefficient
from generalized linear mixed-effects models revisited and expanded.
Journal of The Royal Society Interface, 14(134), 20170213.
</p>
</li>
<li><p> Rabe-Hesketh, S., and Skrondal, A. (2012). Multilevel and longitudinal
modeling using Stata (3rd ed). College Station, Tex: Stata Press
Publication.
</p>
</li>
<li><p> Raudenbush, S. W., and Bryk, A. S. (2002). Hierarchical linear models:
applications and data analysis methods (2nd ed). Thousand Oaks: Sage
Publications.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>
model &lt;- lme4::lmer(Sepal.Length ~ Petal.Length + (1 | Species), data = iris)
icc(model)

# ICC for specific group-levels
data(sleepstudy, package = "lme4")
set.seed(12345)
sleepstudy$grp &lt;- sample(1:5, size = 180, replace = TRUE)
sleepstudy$subgrp &lt;- NA
for (i in 1:5) {
  filter_group &lt;- sleepstudy$grp == i
  sleepstudy$subgrp[filter_group] &lt;-
    sample(1:30, size = sum(filter_group), replace = TRUE)
}
model &lt;- lme4::lmer(
  Reaction ~ Days + (1 | grp / subgrp) + (1 | Subject),
  data = sleepstudy
)
icc(model, by_group = TRUE)

</code></pre>

<hr>
<h2 id='item_difficulty'>Difficulty of Questionnaire Items</h2><span id='topic+item_difficulty'></span>

<h3>Description</h3>

<p>Compute various measures of internal consistencies
for tests or item-scales of questionnaires.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>item_difficulty(x, maximum_value = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="item_difficulty_+3A_x">x</code></td>
<td>
<p>Depending on the function, <code>x</code> may be a <code>matrix</code> as
returned by the <code>cor()</code>-function, or a data frame
with items (e.g. from a test or questionnaire).</p>
</td></tr>
<tr><td><code id="item_difficulty_+3A_maximum_value">maximum_value</code></td>
<td>
<p>Numeric value, indicating the maximum value of an item.
If <code>NULL</code> (default), the maximum is taken from the maximum value of all
columns in <code>x</code> (assuming that the maximum value at least appears once in
the data). If <code>NA</code>, each item's maximum value is taken as maximum. If the
required maximum value is not present in the data, specify the theoreritcal
maximum using <code>maximum_value</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><em>Item difficutly</em> of an item is defined as the quotient of the sum
actually achieved for this item of all and the maximum achievable score.
This function calculates the item difficulty, which should range between
0.2 and 0.8. Lower values are a signal for more difficult items, while
higher values close to one are a sign for easier items. The ideal value
for item difficulty is <code>p + (1 - p) / 2</code>, where <code>p = 1 / max(x)</code>. In most
cases, the ideal item difficulty lies between 0.5 and 0.8.
</p>


<h3>Value</h3>

<p>A data frame with three columns: The name(s) of the item(s), the item
difficulties for each item, and the ideal item difficulty.
</p>


<h3>References</h3>


<ul>
<li><p> Bortz, J., and Döring, N. (2006). Quantitative Methoden der Datenerhebung.
In J. Bortz and N. Döring, Forschungsmethoden und Evaluation. Springer:
Berlin, Heidelberg: 137–293
</p>
</li>
<li><p> Kelava A, Moosbrugger H (2020). Deskriptivstatistische Itemanalyse und
Testwertbestimmung. In: Moosbrugger H,  Kelava A, editors. Testtheorie und
Fragebogenkonstruktion. Berlin, Heidelberg: Springer, 143–158
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>data(mtcars)
x &lt;- mtcars[, c("cyl", "gear", "carb", "hp")]
item_difficulty(x)
</code></pre>

<hr>
<h2 id='item_discrimination'>Discrimination of Questionnaire Items</h2><span id='topic+item_discrimination'></span>

<h3>Description</h3>

<p>Compute various measures of internal consistencies
for tests or item-scales of questionnaires.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>item_discrimination(x, standardize = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="item_discrimination_+3A_x">x</code></td>
<td>
<p>A matrix or a data frame.</p>
</td></tr>
<tr><td><code id="item_discrimination_+3A_standardize">standardize</code></td>
<td>
<p>Logical, if <code>TRUE</code>, the data frame's vectors will be
standardized. Recommended when the variables have different measures /
scales.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function calculates the item discriminations (corrected item-total
correlations for each item of <code>x</code> with the remaining items) for each item
of a scale. The absolute value of the item discrimination indices should be
above 0.2. An index between 0.2 and 0.4 is considered as &quot;fair&quot;, while a
satisfactory index ranges from 0.4 to 0.7. Items with low discrimination
indices are often ambiguously worded and should be examined. Items with
negative indices should be examined to determine why a negative value was
obtained (e.g. reversed answer categories regarding positive and negative
poles).
</p>


<h3>Value</h3>

<p>A data frame with the item discrimination (<em>corrected item-total
correlations</em>) for each item of the scale.
</p>


<h3>References</h3>


<ul>
<li><p> Kelava A, Moosbrugger H (2020). Deskriptivstatistische Itemanalyse und
Testwertbestimmung. In: Moosbrugger H,  Kelava A, editors. Testtheorie und
Fragebogenkonstruktion. Berlin, Heidelberg: Springer, 143–158
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>data(mtcars)
x &lt;- mtcars[, c("cyl", "gear", "carb", "hp")]
item_discrimination(x)
</code></pre>

<hr>
<h2 id='item_intercor'>Mean Inter-Item-Correlation</h2><span id='topic+item_intercor'></span>

<h3>Description</h3>

<p>Compute various measures of internal consistencies
for tests or item-scales of questionnaires.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>item_intercor(x, method = c("pearson", "spearman", "kendall"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="item_intercor_+3A_x">x</code></td>
<td>
<p>A matrix as returned by the <code>cor()</code>-function,
or a data frame with items (e.g. from a test or questionnaire).</p>
</td></tr>
<tr><td><code id="item_intercor_+3A_method">method</code></td>
<td>
<p>Correlation computation method. May be one of
<code>"pearson"</code> (default), <code>"spearman"</code> or <code>"kendall"</code>.
You may use initial letter only.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function calculates a mean inter-item-correlation, i.e. a
correlation matrix of <code>x</code> will be computed (unless <code>x</code> is already a matrix
as returned by the <code>cor()</code> function) and the mean of the sum of all items'
correlation values is returned. Requires either a data frame or a computed
<code>cor()</code> object.
</p>
<p>&quot;Ideally, the average inter-item correlation for a set of items should be
between 0.20 and 0.40, suggesting that while the items are reasonably
homogeneous, they do contain sufficiently unique variance so as to not be
isomorphic with each other. When values are lower than 0.20, then the items
may not be representative of the same content domain. If values are higher
than 0.40, the items may be only capturing a small bandwidth of the
construct.&quot; <em>(Piedmont 2014)</em>
</p>


<h3>Value</h3>

<p>The mean inter-item-correlation value for <code>x</code>.
</p>


<h3>References</h3>

<p>Piedmont RL. 2014. Inter-item Correlations. In: Michalos AC (eds)
Encyclopedia of Quality of Life and Well-Being Research. Dordrecht: Springer,
3303-3304. <a href="https://doi.org/10.1007/978-94-007-0753-5_1493">doi:10.1007/978-94-007-0753-5_1493</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(mtcars)
x &lt;- mtcars[, c("cyl", "gear", "carb", "hp")]
item_intercor(x)
</code></pre>

<hr>
<h2 id='item_reliability'>Reliability Test for Items or Scales</h2><span id='topic+item_reliability'></span>

<h3>Description</h3>

<p>Compute various measures of internal consistencies
for tests or item-scales of questionnaires.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>item_reliability(x, standardize = FALSE, digits = 3)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="item_reliability_+3A_x">x</code></td>
<td>
<p>A matrix or a data frame.</p>
</td></tr>
<tr><td><code id="item_reliability_+3A_standardize">standardize</code></td>
<td>
<p>Logical, if <code>TRUE</code>, the data frame's vectors will be
standardized. Recommended when the variables have different measures /
scales.</p>
</td></tr>
<tr><td><code id="item_reliability_+3A_digits">digits</code></td>
<td>
<p>Amount of digits for returned values.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function calculates the item discriminations (corrected item-total
correlations for each item of <code>x</code> with the remaining items) and the
Cronbach's alpha for each item, if it was deleted from the scale. The
absolute value of the item discrimination indices should be above 0.2. An
index between 0.2 and 0.4 is considered as &quot;fair&quot;, while an index above 0.4
(or below -0.4) is &quot;good&quot;. The range of satisfactory values is from 0.4 to
0.7. Items with low discrimination indices are often ambiguously worded and
should be examined. Items with negative indices should be examined to
determine why a negative value was obtained (e.g. reversed answer categories
regarding positive and negative poles).
</p>


<h3>Value</h3>

<p>A data frame with the corrected item-total correlations (<em>item
discrimination</em>, column <code>item_discrimination</code>) and Cronbach's Alpha
(if item deleted, column <code>alpha_if_deleted</code>) for each item
of the scale, or <code>NULL</code> if data frame had too less columns.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(mtcars)
x &lt;- mtcars[, c("cyl", "gear", "carb", "hp")]
item_reliability(x)
</code></pre>

<hr>
<h2 id='item_split_half'>Split-Half Reliability</h2><span id='topic+item_split_half'></span>

<h3>Description</h3>

<p>Compute various measures of internal consistencies
for tests or item-scales of questionnaires.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>item_split_half(x, digits = 3)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="item_split_half_+3A_x">x</code></td>
<td>
<p>A matrix or a data frame.</p>
</td></tr>
<tr><td><code id="item_split_half_+3A_digits">digits</code></td>
<td>
<p>Amount of digits for returned values.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function calculates the split-half reliability for items in
<code>x</code>, including the Spearman-Brown adjustment. Splitting is done by
selecting odd versus even columns in <code>x</code>. A value closer to 1
indicates greater internal consistency.
</p>


<h3>Value</h3>

<p>A list with two elements: the split-half reliability <code>splithalf</code>
and the Spearman-Brown corrected split-half reliability
<code>spearmanbrown</code>.
</p>


<h3>References</h3>


<ul>
<li><p> Spearman C. 1910. Correlation calculated from faulty data. British
Journal of Psychology (3): 271-295. <a href="https://doi.org/10.1111/j.2044-8295.1910.tb00206.x">doi:10.1111/j.2044-8295.1910.tb00206.x</a>
</p>
</li>
<li><p> Brown W. 1910. Some experimental results in the correlation of mental
abilities. British Journal of Psychology (3): 296-322. <a href="https://doi.org/10.1111/j.2044-8295.1910.tb00207.x">doi:10.1111/j.2044-8295.1910.tb00207.x</a>
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>data(mtcars)
x &lt;- mtcars[, c("cyl", "gear", "carb", "hp")]
item_split_half(x)
</code></pre>

<hr>
<h2 id='looic'>LOO-related Indices for Bayesian regressions.</h2><span id='topic+looic'></span>

<h3>Description</h3>

<p>Compute LOOIC (leave-one-out cross-validation (LOO) information
criterion) and ELPD (expected log predictive density) for Bayesian
regressions. For LOOIC and ELPD, smaller and larger values are respectively
indicative of a better fit.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>looic(model, verbose = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="looic_+3A_model">model</code></td>
<td>
<p>A Bayesian regression model.</p>
</td></tr>
<tr><td><code id="looic_+3A_verbose">verbose</code></td>
<td>
<p>Toggle off warnings.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with four elements, the ELPD, LOOIC and their standard errors.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

model &lt;- suppressWarnings(rstanarm::stan_glm(
  mpg ~ wt + cyl,
  data = mtcars,
  chains = 1,
  iter = 500,
  refresh = 0
))
looic(model)


</code></pre>

<hr>
<h2 id='model_performance'>Model Performance</h2><span id='topic+model_performance'></span><span id='topic+performance'></span>

<h3>Description</h3>

<p>See the documentation for your object's class:
</p>

<ul>
<li> <p><a href="#topic+model_performance.lm">Frequentist Regressions</a>
</p>
</li>
<li> <p><a href="#topic+model_performance.ivreg">Instrumental Variables Regressions</a>
</p>
</li>
<li> <p><a href="#topic+model_performance.merMod">Mixed models</a>
</p>
</li>
<li> <p><a href="#topic+model_performance.stanreg">Bayesian models</a>
</p>
</li>
<li> <p><a href="#topic+model_performance.lavaan">CFA / SEM lavaan models</a>
</p>
</li>
<li> <p><a href="#topic+model_performance.rma">Meta-analysis models</a>
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>model_performance(model, ...)

performance(model, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="model_performance_+3A_model">model</code></td>
<td>
<p>Statistical model.</p>
</td></tr>
<tr><td><code id="model_performance_+3A_...">...</code></td>
<td>
<p>Arguments passed to or from other methods, resp. for
<code>compare_performance()</code>, one or multiple model objects (also of
different classes).</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>model_performance()</code> correctly detects transformed response and
returns the &quot;corrected&quot; AIC and BIC value on the original scale. To get back
to the original scale, the likelihood of the model is multiplied by the
Jacobian/derivative of the transformation.
</p>


<h3>Value</h3>

<p>A data frame (with one row) and one column per &quot;index&quot; (see <code>metrics</code>).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+compare_performance">compare_performance()</a></code> to compare performance of many different models.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>model &lt;- lm(mpg ~ wt + cyl, data = mtcars)
model_performance(model)

model &lt;- glm(vs ~ wt + mpg, data = mtcars, family = "binomial")
model_performance(model)
</code></pre>

<hr>
<h2 id='model_performance.ivreg'>Performance of instrumental variable regression models</h2><span id='topic+model_performance.ivreg'></span>

<h3>Description</h3>

<p>Performance of instrumental variable regression models
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ivreg'
model_performance(model, metrics = "all", verbose = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="model_performance.ivreg_+3A_model">model</code></td>
<td>
<p>A model.</p>
</td></tr>
<tr><td><code id="model_performance.ivreg_+3A_metrics">metrics</code></td>
<td>
<p>Can be <code>"all"</code>, <code>"common"</code> or a character vector of
metrics to be computed (some of <code>c("AIC", "AICc", "BIC", "R2", "RMSE", "SIGMA", "Sargan", "Wu_Hausman", "weak_instruments")</code>). <code>"common"</code> will
compute AIC, BIC, R2 and RMSE.</p>
</td></tr>
<tr><td><code id="model_performance.ivreg_+3A_verbose">verbose</code></td>
<td>
<p>Toggle off warnings.</p>
</td></tr>
<tr><td><code id="model_performance.ivreg_+3A_...">...</code></td>
<td>
<p>Arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>model_performance()</code> correctly detects transformed response and
returns the &quot;corrected&quot; AIC and BIC value on the original scale. To get back
to the original scale, the likelihood of the model is multiplied by the
Jacobian/derivative of the transformation.
</p>

<hr>
<h2 id='model_performance.kmeans'>Model summary for k-means clustering</h2><span id='topic+model_performance.kmeans'></span>

<h3>Description</h3>

<p>Model summary for k-means clustering
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'kmeans'
model_performance(model, verbose = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="model_performance.kmeans_+3A_model">model</code></td>
<td>
<p>Object of type <code>kmeans</code>.</p>
</td></tr>
<tr><td><code id="model_performance.kmeans_+3A_verbose">verbose</code></td>
<td>
<p>Toggle off warnings.</p>
</td></tr>
<tr><td><code id="model_performance.kmeans_+3A_...">...</code></td>
<td>
<p>Arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'># a 2-dimensional example
x &lt;- rbind(
  matrix(rnorm(100, sd = 0.3), ncol = 2),
  matrix(rnorm(100, mean = 1, sd = 0.3), ncol = 2)
)
colnames(x) &lt;- c("x", "y")
model &lt;- kmeans(x, 2)
model_performance(model)
</code></pre>

<hr>
<h2 id='model_performance.lavaan'>Performance of lavaan SEM / CFA Models</h2><span id='topic+model_performance.lavaan'></span>

<h3>Description</h3>

<p>Compute indices of model performance for SEM or CFA models from the
<strong>lavaan</strong> package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'lavaan'
model_performance(model, metrics = "all", verbose = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="model_performance.lavaan_+3A_model">model</code></td>
<td>
<p>A <strong>lavaan</strong> model.</p>
</td></tr>
<tr><td><code id="model_performance.lavaan_+3A_metrics">metrics</code></td>
<td>
<p>Can be <code>"all"</code> or a character vector of metrics to be
computed (some of <code>"Chi2"</code>, <code>"Chi2_df"</code>, <code>"p_Chi2"</code>, <code>"Baseline"</code>,
<code>"Baseline_df"</code>, <code>"p_Baseline"</code>, <code>"GFI"</code>, <code>"AGFI"</code>, <code>"NFI"</code>, <code>"NNFI"</code>,
<code>"CFI"</code>, <code>"RMSEA"</code>, <code>"RMSEA_CI_low"</code>, <code>"RMSEA_CI_high"</code>, <code>"p_RMSEA"</code>,
<code>"RMR"</code>, <code>"SRMR"</code>, <code>"RFI"</code>, <code>"PNFI"</code>, <code>"IFI"</code>, <code>"RNI"</code>, <code>"Loglikelihood"</code>,
<code>"AIC"</code>, <code>"BIC"</code>, and <code>"BIC_adjusted"</code>.</p>
</td></tr>
<tr><td><code id="model_performance.lavaan_+3A_verbose">verbose</code></td>
<td>
<p>Toggle off warnings.</p>
</td></tr>
<tr><td><code id="model_performance.lavaan_+3A_...">...</code></td>
<td>
<p>Arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>



<h4>Indices of fit</h4>


<ul>
<li> <p><strong>Chisq</strong>: The model Chi-squared assesses overall fit and the
discrepancy between the sample and fitted covariance matrices. Its p-value
should be &gt; .05 (i.e., the hypothesis of a perfect fit cannot be
rejected). However, it is quite sensitive to sample size.
</p>
</li>
<li> <p><strong>GFI/AGFI</strong>: The (Adjusted) Goodness of Fit is the proportion
of variance accounted for by the estimated population covariance.
Analogous to R2. The GFI and the AGFI should be &gt; .95 and &gt; .90,
respectively.
</p>
</li>
<li> <p><strong>NFI/NNFI/TLI</strong>: The (Non) Normed Fit Index. An NFI of 0.95,
indicates the model of interest improves the fit by 95\
null model. The NNFI (also called the Tucker Lewis index; TLI) is
preferable for smaller samples. They should be &gt; .90 (Byrne, 1994) or &gt;
.95 (Schumacker and Lomax, 2004).
</p>
</li>
<li> <p><strong>CFI</strong>: The Comparative Fit Index is a revised form of NFI.
Not very sensitive to sample size (Fan, Thompson, and Wang, 1999). Compares
the fit of a target model to the fit of an independent, or null, model. It
should be &gt; .90.
</p>
</li>
<li> <p><strong>RMSEA</strong>: The Root Mean Square Error of Approximation is a
parsimony-adjusted index. Values closer to 0 represent a good fit. It
should be &lt; .08 or &lt; .05. The p-value printed with it tests the hypothesis
that RMSEA is less than or equal to .05 (a cutoff sometimes used for good
fit), and thus should be not significant.
</p>
</li>
<li> <p><strong>RMR/SRMR</strong>: the (Standardized) Root Mean Square Residual
represents the square-root of the difference between the residuals of the
sample covariance matrix and the hypothesized model. As the RMR can be
sometimes hard to interpret, better to use SRMR. Should be &lt; .08.
</p>
</li>
<li> <p><strong>RFI</strong>: the Relative Fit Index, also known as RHO1, is not
guaranteed to vary from 0 to 1. However, RFI close to 1 indicates a good
fit.
</p>
</li>
<li> <p><strong>IFI</strong>: the Incremental Fit Index (IFI) adjusts the Normed Fit
Index (NFI) for sample size and degrees of freedom (Bollen's, 1989). Over
0.90 is a good fit, but the index can exceed 1.
</p>
</li>
<li> <p><strong>PNFI</strong>: the Parsimony-Adjusted Measures Index. There is no
commonly agreed-upon cutoff value for an acceptable model for this index.
Should be &gt; 0.50. </p>
</li></ul>


<p>See the documentation for <code>?lavaan::fitmeasures</code>.
</p>


<h4>What to report</h4>

<p>Kline (2015) suggests that at a minimum the following indices should be
reported: The model <strong>chi-square</strong>, the <strong>RMSEA</strong>, the <strong>CFI</strong>
and the <strong>SRMR</strong>.
</p>



<h3>Value</h3>

<p>A data frame (with one row) and one column per &quot;index&quot; (see
<code>metrics</code>).
</p>


<h3>References</h3>


<ul>
<li><p> Byrne, B. M. (1994). Structural equation modeling with EQS and
EQS/Windows. Thousand Oaks, CA: Sage Publications.
</p>
</li>
<li><p> Tucker, L. R., and Lewis, C. (1973). The reliability coefficient for
maximum likelihood factor analysis. Psychometrika, 38, 1-10.
</p>
</li>
<li><p> Schumacker, R. E., and Lomax, R. G. (2004). A beginner's guide to
structural equation modeling, Second edition. Mahwah, NJ: Lawrence Erlbaum
Associates.
</p>
</li>
<li><p> Fan, X., B. Thompson, and L. Wang (1999). Effects of sample size,
estimation method, and model specification on structural equation modeling
fit indexes. Structural Equation Modeling, 6, 56-83.
</p>
</li>
<li><p> Kline, R. B. (2015). Principles and practice of structural equation
modeling. Guilford publications.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>
# Confirmatory Factor Analysis (CFA) ---------
data(HolzingerSwineford1939, package = "lavaan")
structure &lt;- " visual  =~ x1 + x2 + x3
               textual =~ x4 + x5 + x6
               speed   =~ x7 + x8 + x9 "
model &lt;- lavaan::cfa(structure, data = HolzingerSwineford1939)
model_performance(model)

</code></pre>

<hr>
<h2 id='model_performance.lm'>Performance of Regression Models</h2><span id='topic+model_performance.lm'></span>

<h3>Description</h3>

<p>Compute indices of model performance for regression models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'lm'
model_performance(model, metrics = "all", verbose = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="model_performance.lm_+3A_model">model</code></td>
<td>
<p>A model.</p>
</td></tr>
<tr><td><code id="model_performance.lm_+3A_metrics">metrics</code></td>
<td>
<p>Can be <code>"all"</code>, <code>"common"</code> or a character vector of
metrics to be computed (one or more of <code>"AIC"</code>, <code>"AICc"</code>, <code>"BIC"</code>, <code>"R2"</code>,
<code>"R2_adj"</code>, <code>"RMSE"</code>, <code>"SIGMA"</code>, <code>"LOGLOSS"</code>, <code>"PCP"</code>, <code>"SCORE"</code>).
<code>"common"</code> will compute AIC, BIC, R2 and RMSE.</p>
</td></tr>
<tr><td><code id="model_performance.lm_+3A_verbose">verbose</code></td>
<td>
<p>Toggle off warnings.</p>
</td></tr>
<tr><td><code id="model_performance.lm_+3A_...">...</code></td>
<td>
<p>Arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Depending on <code>model</code>, following indices are computed:
</p>

<ul>
<li> <p><strong>AIC</strong>: Akaike's Information Criterion, see <code>?stats::AIC</code>
</p>
</li>
<li> <p><strong>AICc</strong>: Second-order (or small sample) AIC with a correction for small sample sizes
</p>
</li>
<li> <p><strong>BIC</strong>: Bayesian Information Criterion, see <code>?stats::BIC</code>
</p>
</li>
<li> <p><strong>R2</strong>: r-squared value, see <code><a href="#topic+r2">r2()</a></code>
</p>
</li>
<li> <p><strong>R2_adj</strong>: adjusted r-squared, see <code><a href="#topic+r2">r2()</a></code>
</p>
</li>
<li> <p><strong>RMSE</strong>: root mean squared error, see <code><a href="#topic+performance_rmse">performance_rmse()</a></code>
</p>
</li>
<li> <p><strong>SIGMA</strong>: residual standard deviation, see <code><a href="insight.html#topic+get_sigma">insight::get_sigma()</a></code>
</p>
</li>
<li> <p><strong>LOGLOSS</strong>: Log-loss, see <code><a href="#topic+performance_logloss">performance_logloss()</a></code>
</p>
</li>
<li> <p><strong>SCORE_LOG</strong>: score of logarithmic proper scoring rule, see <code><a href="#topic+performance_score">performance_score()</a></code>
</p>
</li>
<li> <p><strong>SCORE_SPHERICAL</strong>: score of spherical proper scoring rule, see <code><a href="#topic+performance_score">performance_score()</a></code>
</p>
</li>
<li> <p><strong>PCP</strong>: percentage of correct predictions, see <code><a href="#topic+performance_pcp">performance_pcp()</a></code>
</p>
</li></ul>

<p><code>model_performance()</code> correctly detects transformed response and
returns the &quot;corrected&quot; AIC and BIC value on the original scale. To get back
to the original scale, the likelihood of the model is multiplied by the
Jacobian/derivative of the transformation.
</p>


<h3>Value</h3>

<p>A data frame (with one row) and one column per &quot;index&quot; (see <code>metrics</code>).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>model &lt;- lm(mpg ~ wt + cyl, data = mtcars)
model_performance(model)

model &lt;- glm(vs ~ wt + mpg, data = mtcars, family = "binomial")
model_performance(model)
</code></pre>

<hr>
<h2 id='model_performance.merMod'>Performance of Mixed Models</h2><span id='topic+model_performance.merMod'></span>

<h3>Description</h3>

<p>Compute indices of model performance for mixed models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'merMod'
model_performance(
  model,
  metrics = "all",
  estimator = "REML",
  verbose = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="model_performance.merMod_+3A_model">model</code></td>
<td>
<p>A mixed effects model.</p>
</td></tr>
<tr><td><code id="model_performance.merMod_+3A_metrics">metrics</code></td>
<td>
<p>Can be <code>"all"</code>, <code>"common"</code> or a character vector of
metrics to be computed (some of <code>c("AIC", "AICc", "BIC", "R2", "ICC", "RMSE", "SIGMA", "LOGLOSS", "SCORE")</code>). <code>"common"</code> will compute AIC,
BIC, R2, ICC and RMSE.</p>
</td></tr>
<tr><td><code id="model_performance.merMod_+3A_estimator">estimator</code></td>
<td>
<p>Only for linear models. Corresponds to the different
estimators for the standard deviation of the errors. If <code>estimator = "ML"</code>
(default, except for <code>performance_aic()</code> when the model object is of class
<code>lmerMod</code>), the scaling is done by <code>n</code> (the biased ML estimator), which is
then equivalent to using <code>AIC(logLik())</code>. Setting it to <code>"REML"</code> will give
the same results as <code>AIC(logLik(..., REML = TRUE))</code>.</p>
</td></tr>
<tr><td><code id="model_performance.merMod_+3A_verbose">verbose</code></td>
<td>
<p>Toggle warnings and messages.</p>
</td></tr>
<tr><td><code id="model_performance.merMod_+3A_...">...</code></td>
<td>
<p>Arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>



<h4>Intraclass Correlation Coefficient (ICC)</h4>

<p>This method returns the <em>adjusted ICC</em> only, as this is typically of
interest when judging the variance attributed to the random effects part of
the model (see also <code><a href="#topic+icc">icc()</a></code>).
</p>



<h4>REML versus ML estimator</h4>

<p>The default behaviour of <code>model_performance()</code> when computing AIC or BIC of
linear mixed model from package <strong>lme4</strong> is the same as for <code>AIC()</code> or
<code>BIC()</code> (i.e. <code>estimator = "REML"</code>). However, for model comparison using
<code>compare_performance()</code> sets <code>estimator = "ML"</code> by default, because
<em>comparing</em> information criteria based on REML fits is usually not valid
(unless all models have the same fixed effects). Thus, make sure to set
the correct estimator-value when looking at fit-indices or comparing model
fits.
</p>



<h4>Other performance indices</h4>

<p>Furthermore, see 'Details' in <code><a href="#topic+model_performance.lm">model_performance.lm()</a></code> for more details
on returned indices.
</p>



<h3>Value</h3>

<p>A data frame (with one row) and one column per &quot;index&quot; (see
<code>metrics</code>).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
model &lt;- lme4::lmer(Petal.Length ~ Sepal.Length + (1 | Species), data = iris)
model_performance(model)

</code></pre>

<hr>
<h2 id='model_performance.rma'>Performance of Meta-Analysis Models</h2><span id='topic+model_performance.rma'></span>

<h3>Description</h3>

<p>Compute indices of model performance for meta-analysis model from the
<strong>metafor</strong> package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'rma'
model_performance(
  model,
  metrics = "all",
  estimator = "ML",
  verbose = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="model_performance.rma_+3A_model">model</code></td>
<td>
<p>A <code>rma</code> object as returned by <code>metafor::rma()</code>.</p>
</td></tr>
<tr><td><code id="model_performance.rma_+3A_metrics">metrics</code></td>
<td>
<p>Can be <code>"all"</code> or a character vector of metrics to be
computed (some of <code>c("AIC", "BIC", "I2", "H2", "TAU2", "R2", "CochransQ", "QE", "Omnibus", "QM")</code>).</p>
</td></tr>
<tr><td><code id="model_performance.rma_+3A_estimator">estimator</code></td>
<td>
<p>Only for linear models. Corresponds to the different
estimators for the standard deviation of the errors. If <code>estimator = "ML"</code>
(default, except for <code>performance_aic()</code> when the model object is of class
<code>lmerMod</code>), the scaling is done by <code>n</code> (the biased ML estimator), which is
then equivalent to using <code>AIC(logLik())</code>. Setting it to <code>"REML"</code> will give
the same results as <code>AIC(logLik(..., REML = TRUE))</code>.</p>
</td></tr>
<tr><td><code id="model_performance.rma_+3A_verbose">verbose</code></td>
<td>
<p>Toggle off warnings.</p>
</td></tr>
<tr><td><code id="model_performance.rma_+3A_...">...</code></td>
<td>
<p>Arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>



<h4>Indices of fit</h4>


<ul>
<li> <p><strong>AIC</strong> Akaike's Information Criterion, see <code>?stats::AIC</code>
</p>
</li>
<li> <p><strong>BIC</strong> Bayesian Information Criterion, see <code>?stats::BIC</code>
</p>
</li>
<li> <p><strong>I2</strong>: For a random effects model, <code>I2</code> estimates (in
percent) how much of the total variability in the effect size estimates
can be attributed to heterogeneity among the true effects. For a
mixed-effects model, <code>I2</code> estimates how much of the unaccounted
variability can be attributed to residual heterogeneity.
</p>
</li>
<li> <p><strong>H2</strong>: For a random-effects model, <code>H2</code> estimates the
ratio of the total amount of variability in the effect size estimates to
the amount of sampling variability. For a mixed-effects model, <code>H2</code>
estimates the ratio of the unaccounted variability in the effect size
estimates to the amount of sampling variability.
</p>
</li>
<li> <p><strong>TAU2</strong>: The amount of (residual) heterogeneity in the random
or mixed effects model.
</p>
</li>
<li> <p><strong>CochransQ (QE)</strong>: Test for (residual) Heterogeneity. Without
moderators in the model, this is simply Cochran's Q-test.
</p>
</li>
<li> <p><strong>Omnibus (QM)</strong>: Omnibus test of parameters.
</p>
</li>
<li> <p><strong>R2</strong>: Pseudo-R2-statistic, which indicates the amount of
heterogeneity accounted for by the moderators included in a fixed-effects
model.
</p>
</li></ul>

<p>See the documentation for <code>?metafor::fitstats</code>.
</p>



<h3>Value</h3>

<p>A data frame (with one row) and one column per &quot;index&quot; (see
<code>metrics</code>).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(dat.bcg, package = "metadat")
dat &lt;- metafor::escalc(
  measure = "RR",
  ai = tpos,
  bi = tneg,
  ci = cpos,
  di = cneg,
  data = dat.bcg
)
model &lt;- metafor::rma(yi, vi, data = dat, method = "REML")
model_performance(model)

</code></pre>

<hr>
<h2 id='model_performance.stanreg'>Performance of Bayesian Models</h2><span id='topic+model_performance.stanreg'></span><span id='topic+model_performance.BFBayesFactor'></span>

<h3>Description</h3>

<p>Compute indices of model performance for (general) linear models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'stanreg'
model_performance(model, metrics = "all", verbose = TRUE, ...)

## S3 method for class 'BFBayesFactor'
model_performance(
  model,
  metrics = "all",
  verbose = TRUE,
  average = FALSE,
  prior_odds = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="model_performance.stanreg_+3A_model">model</code></td>
<td>
<p>Object of class <code>stanreg</code> or <code>brmsfit</code>.</p>
</td></tr>
<tr><td><code id="model_performance.stanreg_+3A_metrics">metrics</code></td>
<td>
<p>Can be <code>"all"</code>, <code>"common"</code> or a character vector of
metrics to be computed (some of <code>c("LOOIC", "WAIC", "R2", "R2_adj", "RMSE", "SIGMA", "LOGLOSS", "SCORE")</code>). <code>"common"</code> will compute LOOIC,
WAIC, R2 and RMSE.</p>
</td></tr>
<tr><td><code id="model_performance.stanreg_+3A_verbose">verbose</code></td>
<td>
<p>Toggle off warnings.</p>
</td></tr>
<tr><td><code id="model_performance.stanreg_+3A_...">...</code></td>
<td>
<p>Arguments passed to or from other methods.</p>
</td></tr>
<tr><td><code id="model_performance.stanreg_+3A_average">average</code></td>
<td>
<p>Compute model-averaged index? See <code><a href="bayestestR.html#topic+weighted_posteriors">bayestestR::weighted_posteriors()</a></code>.</p>
</td></tr>
<tr><td><code id="model_performance.stanreg_+3A_prior_odds">prior_odds</code></td>
<td>
<p>Optional vector of prior odds for the models compared to
the first model (or the denominator, for <code>BFBayesFactor</code> objects). For
<code>data.frame</code>s, this will be used as the basis of weighting.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Depending on <code>model</code>, the following indices are computed:
</p>

<ul>
<li> <p><strong>ELPD</strong>: expected log predictive density. Larger ELPD values
mean better fit. See <code><a href="#topic+looic">looic()</a></code>.
</p>
</li>
<li> <p><strong>LOOIC</strong>: leave-one-out cross-validation (LOO) information
criterion. Lower LOOIC values mean better fit. See <code><a href="#topic+looic">looic()</a></code>.
</p>
</li>
<li> <p><strong>WAIC</strong>: widely applicable information criterion. Lower WAIC
values mean better fit. See <code>?loo::waic</code>.
</p>
</li>
<li> <p><strong>R2</strong>: r-squared value, see <code><a href="#topic+r2_bayes">r2_bayes()</a></code>.
</p>
</li>
<li> <p><strong>R2_adjusted</strong>: LOO-adjusted r-squared, see <code><a href="#topic+r2_loo">r2_loo()</a></code>.
</p>
</li>
<li> <p><strong>RMSE</strong>: root mean squared error, see <code><a href="#topic+performance_rmse">performance_rmse()</a></code>.
</p>
</li>
<li> <p><strong>SIGMA</strong>: residual standard deviation, see <code><a href="insight.html#topic+get_sigma">insight::get_sigma()</a></code>.
</p>
</li>
<li> <p><strong>LOGLOSS</strong>: Log-loss, see <code><a href="#topic+performance_logloss">performance_logloss()</a></code>.
</p>
</li>
<li> <p><strong>SCORE_LOG</strong>: score of logarithmic proper scoring rule, see <code><a href="#topic+performance_score">performance_score()</a></code>.
</p>
</li>
<li> <p><strong>SCORE_SPHERICAL</strong>: score of spherical proper scoring rule, see <code><a href="#topic+performance_score">performance_score()</a></code>.
</p>
</li>
<li> <p><strong>PCP</strong>: percentage of correct predictions, see <code><a href="#topic+performance_pcp">performance_pcp()</a></code>.
</p>
</li></ul>



<h3>Value</h3>

<p>A data frame (with one row) and one column per &quot;index&quot; (see
<code>metrics</code>).
</p>


<h3>References</h3>

<p>Gelman, A., Goodrich, B., Gabry, J., and Vehtari, A. (2018).
R-squared for Bayesian regression models. The American Statistician, The
American Statistician, 1-6.
</p>


<h3>See Also</h3>

<p><a href="#topic+r2_bayes">r2_bayes</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

model &lt;- suppressWarnings(rstanarm::stan_glm(
  mpg ~ wt + cyl,
  data = mtcars,
  chains = 1,
  iter = 500,
  refresh = 0
))
model_performance(model)

model &lt;- suppressWarnings(rstanarm::stan_glmer(
  mpg ~ wt + cyl + (1 | gear),
  data = mtcars,
  chains = 1,
  iter = 500,
  refresh = 0
))
model_performance(model)


</code></pre>

<hr>
<h2 id='performance_accuracy'>Accuracy of predictions from model fit</h2><span id='topic+performance_accuracy'></span>

<h3>Description</h3>

<p>This function calculates the predictive accuracy of linear
or logistic regression models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>performance_accuracy(
  model,
  method = c("cv", "boot"),
  k = 5,
  n = 1000,
  ci = 0.95,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="performance_accuracy_+3A_model">model</code></td>
<td>
<p>A linear or logistic regression model. A mixed-effects model is
also accepted.</p>
</td></tr>
<tr><td><code id="performance_accuracy_+3A_method">method</code></td>
<td>
<p>Character string, indicating whether cross-validation
(<code>method = "cv"</code>) or bootstrapping (<code>method = "boot"</code>) is used to
compute the accuracy values.</p>
</td></tr>
<tr><td><code id="performance_accuracy_+3A_k">k</code></td>
<td>
<p>The number of folds for the k-fold cross-validation.</p>
</td></tr>
<tr><td><code id="performance_accuracy_+3A_n">n</code></td>
<td>
<p>Number of bootstrap-samples.</p>
</td></tr>
<tr><td><code id="performance_accuracy_+3A_ci">ci</code></td>
<td>
<p>The level of the confidence interval.</p>
</td></tr>
<tr><td><code id="performance_accuracy_+3A_verbose">verbose</code></td>
<td>
<p>Toggle warnings.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For linear models, the accuracy is the correlation coefficient
between the actual and the predicted value of the outcome. For
logistic regression models, the accuracy corresponds to the
AUC-value, calculated with the <code>bayestestR::auc()</code>-function.
<br /> <br />
The accuracy is the mean value of multiple correlation resp.
AUC-values, which are either computed with cross-validation
or non-parametric bootstrapping (see argument <code>method</code>).
The standard error is the standard deviation of the computed
correlation resp. AUC-values.
</p>


<h3>Value</h3>

<p>A list with three values: The <code>Accuracy</code> of the model
predictions, i.e. the proportion of accurately predicted values from the
model, its standard error, <code>SE</code>, and the <code>Method</code> used to compute
the accuracy.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>model &lt;- lm(mpg ~ wt + cyl, data = mtcars)
performance_accuracy(model)

model &lt;- glm(vs ~ wt + mpg, data = mtcars, family = "binomial")
performance_accuracy(model)
</code></pre>

<hr>
<h2 id='performance_aicc'>Compute the AIC or second-order AIC</h2><span id='topic+performance_aicc'></span><span id='topic+performance_aic'></span><span id='topic+performance_aic.default'></span><span id='topic+performance_aic.lmerMod'></span>

<h3>Description</h3>

<p>Compute the AIC or the second-order Akaike's information criterion (AICc).
<code>performance_aic()</code> is a small wrapper that returns the AIC, however, for
models with a transformed response variable, <code>performance_aic()</code> returns the
corrected AIC value (see 'Examples'). It is a generic function that also
works for some models that don't have a AIC method (like Tweedie models).
<code>performance_aicc()</code> returns the second-order (or &quot;small sample&quot;) AIC that
incorporates a correction for small sample sizes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>performance_aicc(x, ...)

performance_aic(x, ...)

## Default S3 method:
performance_aic(x, estimator = "ML", verbose = TRUE, ...)

## S3 method for class 'lmerMod'
performance_aic(x, estimator = "REML", verbose = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="performance_aicc_+3A_x">x</code></td>
<td>
<p>A model object.</p>
</td></tr>
<tr><td><code id="performance_aicc_+3A_...">...</code></td>
<td>
<p>Currently not used.</p>
</td></tr>
<tr><td><code id="performance_aicc_+3A_estimator">estimator</code></td>
<td>
<p>Only for linear models. Corresponds to the different
estimators for the standard deviation of the errors. If <code>estimator = "ML"</code>
(default, except for <code>performance_aic()</code> when the model object is of class
<code>lmerMod</code>), the scaling is done by <code>n</code> (the biased ML estimator), which is
then equivalent to using <code>AIC(logLik())</code>. Setting it to <code>"REML"</code> will give
the same results as <code>AIC(logLik(..., REML = TRUE))</code>.</p>
</td></tr>
<tr><td><code id="performance_aicc_+3A_verbose">verbose</code></td>
<td>
<p>Toggle warnings.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>performance_aic()</code> correctly detects transformed response and,
unlike <code>stats::AIC()</code>, returns the &quot;corrected&quot; AIC value on the original
scale. To get back to the original scale, the likelihood of the model is
multiplied by the Jacobian/derivative of the transformation.
</p>


<h3>Value</h3>

<p>Numeric, the AIC or AICc value.
</p>


<h3>References</h3>


<ul>
<li><p> Akaike, H. (1973) Information theory as an extension of the maximum
likelihood principle. In: Second International Symposium on Information
Theory, pp. 267-281. Petrov, B.N., Csaki, F., Eds, Akademiai Kiado, Budapest.
</p>
</li>
<li><p> Hurvich, C. M., Tsai, C.-L. (1991) Bias of the corrected AIC criterion
for underfitted regression and time series models. Biometrika 78, 499–509.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>m &lt;- lm(mpg ~ wt + cyl + gear + disp, data = mtcars)
AIC(m)
performance_aicc(m)

# correct AIC for models with transformed response variable
data("mtcars")
mtcars$mpg &lt;- floor(mtcars$mpg)
model &lt;- lm(log(mpg) ~ factor(cyl), mtcars)

# wrong AIC, not corrected for log-transformation
AIC(model)

# performance_aic() correctly detects transformed response and
# returns corrected AIC
performance_aic(model)
</code></pre>

<hr>
<h2 id='performance_cv'>Cross-validated model performance</h2><span id='topic+performance_cv'></span>

<h3>Description</h3>

<p>This function cross-validates regression models in a
user-supplied new sample or by using holdout (train-test), k-fold, or
leave-one-out cross-validation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>performance_cv(
  model,
  data = NULL,
  method = c("holdout", "k_fold", "loo"),
  metrics = "all",
  prop = 0.3,
  k = 5,
  stack = TRUE,
  verbose = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="performance_cv_+3A_model">model</code></td>
<td>
<p>A regression model.</p>
</td></tr>
<tr><td><code id="performance_cv_+3A_data">data</code></td>
<td>
<p>Optional. A data frame containing the same variables as <code>model</code>
that will be used as the cross-validation sample.</p>
</td></tr>
<tr><td><code id="performance_cv_+3A_method">method</code></td>
<td>
<p>Character string, indicating the cross-validation method to use:
whether holdout (<code>"holdout"</code>, aka train-test), k-fold (<code>"k_fold"</code>), or
leave-one-out (<code>"loo"</code>). If <code>data</code> is supplied, this argument is ignored.</p>
</td></tr>
<tr><td><code id="performance_cv_+3A_metrics">metrics</code></td>
<td>
<p>Can be <code>"all"</code>, <code>"common"</code> or a character vector of metrics to be
computed (some of <code>c("ELPD", "Deviance", "MSE", "RMSE", "R2")</code>). &quot;common&quot; will
compute R2 and RMSE.</p>
</td></tr>
<tr><td><code id="performance_cv_+3A_prop">prop</code></td>
<td>
<p>If <code>method = "holdout"</code>, what proportion of the sample to hold
out as the test sample?</p>
</td></tr>
<tr><td><code id="performance_cv_+3A_k">k</code></td>
<td>
<p>If <code>method = "k_fold"</code>, the number of folds to use.</p>
</td></tr>
<tr><td><code id="performance_cv_+3A_stack">stack</code></td>
<td>
<p>Logical. If <code>method = "k_fold"</code>, should performance be computed
by stacking residuals from each holdout fold and calculating each metric on
the stacked data (<code>TRUE</code>, default) or should performance be computed by
calculating metrics within each holdout fold and averaging performance
across each fold (<code>FALSE</code>)?</p>
</td></tr>
<tr><td><code id="performance_cv_+3A_verbose">verbose</code></td>
<td>
<p>Toggle warnings.</p>
</td></tr>
<tr><td><code id="performance_cv_+3A_...">...</code></td>
<td>
<p>Not used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame with columns for each metric requested, as well as <code>k</code>
if <code>method = "holdout"</code> and the <code>Method</code> used for cross-validation. If
<code>method = "holdout"</code> and <code>stack = TRUE</code>, the standard error (standard
deviation across holdout folds) for each metric is also included.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>model &lt;- lm(mpg ~ wt + cyl, data = mtcars)
performance_cv(model)

</code></pre>

<hr>
<h2 id='performance_hosmer'>Hosmer-Lemeshow goodness-of-fit test</h2><span id='topic+performance_hosmer'></span>

<h3>Description</h3>

<p>Check model quality of logistic regression models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>performance_hosmer(model, n_bins = 10)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="performance_hosmer_+3A_model">model</code></td>
<td>
<p>A <code>glm</code>-object with binomial-family.</p>
</td></tr>
<tr><td><code id="performance_hosmer_+3A_n_bins">n_bins</code></td>
<td>
<p>Numeric, the number of bins to divide the data.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A well-fitting model shows <em>no</em> significant difference between
the model and the observed data, i.e. the reported p-value should be
greater than 0.05.
</p>


<h3>Value</h3>

<p>An object of class <code>hoslem_test</code> with following values:
<code>chisq</code>, the Hosmer-Lemeshow chi-squared statistic; <code>df</code>, degrees
of freedom and <code>p.value</code> the p-value for the goodness-of-fit test.
</p>


<h3>References</h3>

<p>Hosmer, D. W., and Lemeshow, S. (2000). Applied Logistic Regression. Hoboken,
NJ, USA: John Wiley and Sons, Inc. <a href="https://doi.org/10.1002/0471722146">doi:10.1002/0471722146</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>model &lt;- glm(vs ~ wt + mpg, data = mtcars, family = "binomial")
performance_hosmer(model)
</code></pre>

<hr>
<h2 id='performance_logloss'>Log Loss</h2><span id='topic+performance_logloss'></span>

<h3>Description</h3>

<p>Compute the log loss for models with binary outcome.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>performance_logloss(model, verbose = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="performance_logloss_+3A_model">model</code></td>
<td>
<p>Model with binary outcome.</p>
</td></tr>
<tr><td><code id="performance_logloss_+3A_verbose">verbose</code></td>
<td>
<p>Toggle off warnings.</p>
</td></tr>
<tr><td><code id="performance_logloss_+3A_...">...</code></td>
<td>
<p>Currently not used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Logistic regression models predict the probability of an outcome of being a
&quot;success&quot; or &quot;failure&quot; (or 1 and 0 etc.). <code>performance_logloss()</code> evaluates
how good or bad the predicted probabilities are. High values indicate bad
predictions, while low values indicate good predictions. The lower the
log-loss, the better the model predicts the outcome.
</p>


<h3>Value</h3>

<p>Numeric, the log loss of <code>model</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+performance_score">performance_score()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(mtcars)
m &lt;- glm(formula = vs ~ hp + wt, family = binomial, data = mtcars)
performance_logloss(m)
</code></pre>

<hr>
<h2 id='performance_mae'>Mean Absolute Error of Models</h2><span id='topic+performance_mae'></span><span id='topic+mae'></span>

<h3>Description</h3>

<p>Compute mean absolute error of models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>performance_mae(model, ...)

mae(model, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="performance_mae_+3A_model">model</code></td>
<td>
<p>A model.</p>
</td></tr>
<tr><td><code id="performance_mae_+3A_...">...</code></td>
<td>
<p>Arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Numeric, the mean absolute error of <code>model</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(mtcars)
m &lt;- lm(mpg ~ hp + gear, data = mtcars)
performance_mae(m)
</code></pre>

<hr>
<h2 id='performance_mse'>Mean Square Error of Linear Models</h2><span id='topic+performance_mse'></span><span id='topic+mse'></span>

<h3>Description</h3>

<p>Compute mean square error of linear models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>performance_mse(model, ...)

mse(model, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="performance_mse_+3A_model">model</code></td>
<td>
<p>A model.</p>
</td></tr>
<tr><td><code id="performance_mse_+3A_...">...</code></td>
<td>
<p>Arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The mean square error is the mean of the sum of squared residuals, i.e. it
measures the average of the squares of the errors. Less technically speaking,
the mean square error can be considered as the variance of the residuals,
i.e. the variation in the outcome the model doesn't explain. Lower values
(closer to zero) indicate better fit.
</p>


<h3>Value</h3>

<p>Numeric, the mean square error of <code>model</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(mtcars)
m &lt;- lm(mpg ~ hp + gear, data = mtcars)
performance_mse(m)
</code></pre>

<hr>
<h2 id='performance_pcp'>Percentage of Correct Predictions</h2><span id='topic+performance_pcp'></span>

<h3>Description</h3>

<p>Percentage of correct predictions (PCP) for models
with binary outcome.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>performance_pcp(model, ci = 0.95, method = "Herron", verbose = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="performance_pcp_+3A_model">model</code></td>
<td>
<p>Model with binary outcome.</p>
</td></tr>
<tr><td><code id="performance_pcp_+3A_ci">ci</code></td>
<td>
<p>The level of the confidence interval.</p>
</td></tr>
<tr><td><code id="performance_pcp_+3A_method">method</code></td>
<td>
<p>Name of the method to calculate the PCP (see 'Details').
Default is <code>"Herron"</code>. May be abbreviated.</p>
</td></tr>
<tr><td><code id="performance_pcp_+3A_verbose">verbose</code></td>
<td>
<p>Toggle off warnings.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>method = "Gelman-Hill"</code> (or <code>"gelman_hill"</code>) computes the
PCP based on the proposal from <em>Gelman and Hill 2017, 99</em>, which is
defined as the proportion of cases for which the deterministic prediction
is wrong, i.e. the proportion where the predicted probability is above 0.5,
although y=0 (and vice versa) (see also <em>Herron 1999, 90</em>).
</p>
<p><code>method = "Herron"</code> (or <code>"herron"</code>) computes a modified version
of the PCP (<em>Herron 1999, 90-92</em>), which is the sum of predicted
probabilities, where y=1, plus the sum of 1 - predicted probabilities,
where y=0, divided by the number of observations. This approach is said to
be more accurate.
</p>
<p>The PCP ranges from 0 to 1, where values closer to 1 mean that the model
predicts the outcome better than models with an PCP closer to 0. In general,
the PCP should be above 0.5 (i.e. 50\
Furthermore, the PCP of the full model should be considerably above
the null model's PCP.
</p>
<p>The likelihood-ratio test indicates whether the model has a significantly
better fit than the null-model (in such cases, p &lt; 0.05).
</p>


<h3>Value</h3>

<p>A list with several elements: the percentage of correct predictions
of the full and the null model, their confidence intervals, as well as the
chi-squared and p-value from the Likelihood-Ratio-Test between the full and
null model.
</p>


<h3>References</h3>


<ul>
<li><p> Herron, M. (1999). Postestimation Uncertainty in Limited Dependent
Variable Models. Political Analysis, 8, 83–98.
</p>
</li>
<li><p> Gelman, A., and Hill, J. (2007). Data analysis using regression and
multilevel/hierarchical models. Cambridge; New York: Cambridge University
Press, 99.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>data(mtcars)
m &lt;- glm(formula = vs ~ hp + wt, family = binomial, data = mtcars)
performance_pcp(m)
performance_pcp(m, method = "Gelman-Hill")
</code></pre>

<hr>
<h2 id='performance_rmse'>Root Mean Squared Error</h2><span id='topic+performance_rmse'></span><span id='topic+rmse'></span>

<h3>Description</h3>

<p>Compute root mean squared error for (mixed effects) models,
including Bayesian regression models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>performance_rmse(model, normalized = FALSE, verbose = TRUE)

rmse(model, normalized = FALSE, verbose = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="performance_rmse_+3A_model">model</code></td>
<td>
<p>A model.</p>
</td></tr>
<tr><td><code id="performance_rmse_+3A_normalized">normalized</code></td>
<td>
<p>Logical, use <code>TRUE</code> if normalized rmse should be returned.</p>
</td></tr>
<tr><td><code id="performance_rmse_+3A_verbose">verbose</code></td>
<td>
<p>Toggle off warnings.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The RMSE is the square root of the variance of the residuals and indicates
the absolute fit of the model to the data (difference between observed data
to model's predicted values). It can be interpreted as the standard
deviation of the unexplained variance, and is in the same units as the
response variable. Lower values indicate better model fit.
</p>
<p>The normalized RMSE is the proportion of the RMSE related to the
range of the response variable. Hence, lower values indicate
less residual variance.
</p>


<h3>Value</h3>

<p>Numeric, the root mean squared error.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(Orthodont, package = "nlme")
m &lt;- nlme::lme(distance ~ age, data = Orthodont)

# RMSE
performance_rmse(m, normalized = FALSE)

# normalized RMSE
performance_rmse(m, normalized = TRUE)

</code></pre>

<hr>
<h2 id='performance_roc'>Simple ROC curve</h2><span id='topic+performance_roc'></span>

<h3>Description</h3>

<p>This function calculates a simple ROC curves of x/y coordinates
based on response and predictions of a binomial model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>performance_roc(x, ..., predictions, new_data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="performance_roc_+3A_x">x</code></td>
<td>
<p>A numeric vector, representing the outcome (0/1), or a model with
binomial outcome.</p>
</td></tr>
<tr><td><code id="performance_roc_+3A_...">...</code></td>
<td>
<p>One or more models with binomial outcome. In this case,
<code>new_data</code> is ignored.</p>
</td></tr>
<tr><td><code id="performance_roc_+3A_predictions">predictions</code></td>
<td>
<p>If <code>x</code> is numeric, a numeric vector of same length
as <code>x</code>, representing the actual predicted values.</p>
</td></tr>
<tr><td><code id="performance_roc_+3A_new_data">new_data</code></td>
<td>
<p>If <code>x</code> is a model, a data frame that is passed to
<code>predict()</code> as <code>newdata</code>-argument. If <code>NULL</code>, the ROC for
the full model is calculated.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame with three columns, the x/y-coordinate pairs for the ROC
curve (<code>Sensitivity</code> and <code>Specificity</code>), and a column with the
model name.
</p>


<h3>Note</h3>

<p>There is also a <a href="https://easystats.github.io/see/articles/performance.html"><code>plot()</code>-method</a>
implemented in the <a href="https://easystats.github.io/see/"><span class="pkg">see</span>-package</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(bayestestR)
data(iris)

set.seed(123)
iris$y &lt;- rbinom(nrow(iris), size = 1, .3)
folds &lt;- sample(nrow(iris), size = nrow(iris) / 8, replace = FALSE)
test_data &lt;- iris[folds, ]
train_data &lt;- iris[-folds, ]

model &lt;- glm(y ~ Sepal.Length + Sepal.Width, data = train_data, family = "binomial")
as.data.frame(performance_roc(model, new_data = test_data))

roc &lt;- performance_roc(model, new_data = test_data)
area_under_curve(roc$Specificity, roc$Sensitivity)

if (interactive()) {
  m1 &lt;- glm(y ~ Sepal.Length + Sepal.Width, data = iris, family = "binomial")
  m2 &lt;- glm(y ~ Sepal.Length + Petal.Width, data = iris, family = "binomial")
  m3 &lt;- glm(y ~ Sepal.Length + Species, data = iris, family = "binomial")
  performance_roc(m1, m2, m3)

  # if you have `see` package installed, you can also plot comparison of
  # ROC curves for different models
  if (require("see")) plot(performance_roc(m1, m2, m3))
}
</code></pre>

<hr>
<h2 id='performance_rse'>Residual Standard Error for Linear Models</h2><span id='topic+performance_rse'></span>

<h3>Description</h3>

<p>Compute residual standard error of linear models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>performance_rse(model)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="performance_rse_+3A_model">model</code></td>
<td>
<p>A model.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The residual standard error is the square root of the residual
sum of squares divided by the residual degrees of freedom.
</p>


<h3>Value</h3>

<p>Numeric, the residual standard error of <code>model</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(mtcars)
m &lt;- lm(mpg ~ hp + gear, data = mtcars)
performance_rse(m)
</code></pre>

<hr>
<h2 id='performance_score'>Proper Scoring Rules</h2><span id='topic+performance_score'></span>

<h3>Description</h3>

<p>Calculates the logarithmic, quadratic/Brier and spherical score
from a model with binary or count outcome.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>performance_score(model, verbose = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="performance_score_+3A_model">model</code></td>
<td>
<p>Model with binary or count outcome.</p>
</td></tr>
<tr><td><code id="performance_score_+3A_verbose">verbose</code></td>
<td>
<p>Toggle off warnings.</p>
</td></tr>
<tr><td><code id="performance_score_+3A_...">...</code></td>
<td>
<p>Arguments from other functions, usually only used internally.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Proper scoring rules can be used to evaluate the quality of model
predictions and model fit. <code>performance_score()</code> calculates the logarithmic,
quadratic/Brier and spherical scoring rules. The spherical rule takes values
in the interval <code style="white-space: pre;">&#8288;[0, 1]&#8288;</code>, with values closer to 1 indicating a more
accurate model, and the logarithmic rule in the interval <code style="white-space: pre;">&#8288;[-Inf, 0]&#8288;</code>,
with values closer to 0 indicating a more accurate model.
</p>
<p>For <code>stan_lmer()</code> and <code>stan_glmer()</code> models, the predicted values
are based on <code>posterior_predict()</code>, instead of <code>predict()</code>. Thus,
results may differ more than expected from their non-Bayesian counterparts
in <strong>lme4</strong>.
</p>


<h3>Value</h3>

<p>A list with three elements, the logarithmic, quadratic/Brier and spherical score.
</p>


<h3>Note</h3>

<p>Code is partially based on
<a href="https://drizopoulos.github.io/GLMMadaptive/reference/scoring_rules.html">GLMMadaptive::scoring_rules()</a>.
</p>


<h3>References</h3>

<p>Carvalho, A. (2016). An overview of applications of proper scoring rules.
Decision Analysis 13, 223–242. <a href="https://doi.org/10.1287/deca.2016.0337">doi:10.1287/deca.2016.0337</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+performance_logloss">performance_logloss()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Dobson (1990) Page 93: Randomized Controlled Trial :
counts &lt;- c(18, 17, 15, 20, 10, 20, 25, 13, 12)
outcome &lt;- gl(3, 1, 9)
treatment &lt;- gl(3, 3)
model &lt;- glm(counts ~ outcome + treatment, family = poisson())

performance_score(model)

data(Salamanders, package = "glmmTMB")
model &lt;- glmmTMB::glmmTMB(
  count ~ spp + mined + (1 | site),
  zi = ~ spp + mined,
  family = nbinom2(),
  data = Salamanders
)

performance_score(model)


</code></pre>

<hr>
<h2 id='r2'>Compute the model's R2</h2><span id='topic+r2'></span><span id='topic+r2.default'></span><span id='topic+r2.merMod'></span>

<h3>Description</h3>

<p>Calculate the R2, also known as the coefficient of
determination, value for different model objects. Depending on the model,
R2, pseudo-R2, or marginal / adjusted R2 values are returned.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>r2(model, ...)

## Default S3 method:
r2(model, ci = NULL, verbose = TRUE, ...)

## S3 method for class 'merMod'
r2(model, ci = NULL, tolerance = 1e-05, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="r2_+3A_model">model</code></td>
<td>
<p>A statistical model.</p>
</td></tr>
<tr><td><code id="r2_+3A_...">...</code></td>
<td>
<p>Arguments passed down to the related r2-methods.</p>
</td></tr>
<tr><td><code id="r2_+3A_ci">ci</code></td>
<td>
<p>Confidence interval level, as scalar. If <code>NULL</code> (default), no
confidence intervals for R2 are calculated.</p>
</td></tr>
<tr><td><code id="r2_+3A_verbose">verbose</code></td>
<td>
<p>Logical. Should details about R2 and CI methods be given
(<code>TRUE</code>) or not (<code>FALSE</code>)?</p>
</td></tr>
<tr><td><code id="r2_+3A_tolerance">tolerance</code></td>
<td>
<p>Tolerance for singularity check of random effects, to decide
whether to compute random effect variances for the conditional r-squared
or not. Indicates up to which value the convergence result is accepted. When
<code>r2_nakagawa()</code> returns a warning, stating that random effect variances
can't be computed (and thus, the conditional r-squared is <code>NA</code>),
decrease the tolerance-level. See also <code><a href="#topic+check_singularity">check_singularity()</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a list containing values related to the most appropriate R2
for the given model (or <code>NULL</code> if no R2 could be extracted). See the
list below:
</p>

<ul>
<li><p> Logistic models: <a href="#topic+r2_tjur">Tjur's R2</a>
</p>
</li>
<li><p> General linear models: <a href="#topic+r2_nagelkerke">Nagelkerke's R2</a>
</p>
</li>
<li><p> Multinomial Logit: <a href="#topic+r2_mcfadden">McFadden's R2</a>
</p>
</li>
<li><p> Models with zero-inflation: <a href="#topic+r2_zeroinflated">R2 for zero-inflated models</a>
</p>
</li>
<li><p> Mixed models: <a href="#topic+r2_nakagawa">Nakagawa's R2</a>
</p>
</li>
<li><p> Bayesian models: <a href="#topic+r2_bayes">R2 bayes</a>
</p>
</li></ul>



<h3>Note</h3>

<p>If there is no <code>r2()</code>-method defined for the given model class,
<code>r2()</code> tries to return a &quot;generic&quot; r-quared value, calculated as following:
<code style="white-space: pre;">&#8288;1-sum((y-y_hat)^2)/sum((y-y_bar)^2))&#8288;</code>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+r2_bayes">r2_bayes()</a></code>, <code><a href="#topic+r2_coxsnell">r2_coxsnell()</a></code>, <code><a href="#topic+r2_kullback">r2_kullback()</a></code>,
<code><a href="#topic+r2_loo">r2_loo()</a></code>, <code><a href="#topic+r2_mcfadden">r2_mcfadden()</a></code>, <code><a href="#topic+r2_nagelkerke">r2_nagelkerke()</a></code>,
<code><a href="#topic+r2_nakagawa">r2_nakagawa()</a></code>, <code><a href="#topic+r2_tjur">r2_tjur()</a></code>, <code><a href="#topic+r2_xu">r2_xu()</a></code> and
<code><a href="#topic+r2_zeroinflated">r2_zeroinflated()</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Pseudo r-quared for GLM
model &lt;- glm(vs ~ wt + mpg, data = mtcars, family = "binomial")
r2(model)

# r-squared including confidence intervals
model &lt;- lm(mpg ~ wt + hp, data = mtcars)
r2(model, ci = 0.95)

model &lt;- lme4::lmer(Sepal.Length ~ Petal.Length + (1 | Species), data = iris)
r2(model)

</code></pre>

<hr>
<h2 id='r2_bayes'>Bayesian R2</h2><span id='topic+r2_bayes'></span><span id='topic+r2_posterior'></span><span id='topic+r2_posterior.brmsfit'></span><span id='topic+r2_posterior.stanreg'></span><span id='topic+r2_posterior.BFBayesFactor'></span>

<h3>Description</h3>

<p>Compute R2 for Bayesian models. For mixed models (including a
random part), it additionally computes the R2 related to the fixed effects
only (marginal R2). While <code>r2_bayes()</code> returns a single R2 value,
<code>r2_posterior()</code> returns a posterior sample of Bayesian R2 values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>r2_bayes(model, robust = TRUE, ci = 0.95, verbose = TRUE, ...)

r2_posterior(model, ...)

## S3 method for class 'brmsfit'
r2_posterior(model, verbose = TRUE, ...)

## S3 method for class 'stanreg'
r2_posterior(model, verbose = TRUE, ...)

## S3 method for class 'BFBayesFactor'
r2_posterior(model, average = FALSE, prior_odds = NULL, verbose = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="r2_bayes_+3A_model">model</code></td>
<td>
<p>A Bayesian regression model (from <strong>brms</strong>,
<strong>rstanarm</strong>, <strong>BayesFactor</strong>, etc).</p>
</td></tr>
<tr><td><code id="r2_bayes_+3A_robust">robust</code></td>
<td>
<p>Logical, if <code>TRUE</code>, the median instead of mean is used to
calculate the central tendency of the variances.</p>
</td></tr>
<tr><td><code id="r2_bayes_+3A_ci">ci</code></td>
<td>
<p>Value or vector of probability of the CI (between 0 and 1) to be
estimated.</p>
</td></tr>
<tr><td><code id="r2_bayes_+3A_verbose">verbose</code></td>
<td>
<p>Toggle off warnings.</p>
</td></tr>
<tr><td><code id="r2_bayes_+3A_...">...</code></td>
<td>
<p>Arguments passed to <code>r2_posterior()</code>.</p>
</td></tr>
<tr><td><code id="r2_bayes_+3A_average">average</code></td>
<td>
<p>Compute model-averaged index? See <code><a href="bayestestR.html#topic+weighted_posteriors">bayestestR::weighted_posteriors()</a></code>.</p>
</td></tr>
<tr><td><code id="r2_bayes_+3A_prior_odds">prior_odds</code></td>
<td>
<p>Optional vector of prior odds for the models compared to
the first model (or the denominator, for <code>BFBayesFactor</code> objects). For
<code>data.frame</code>s, this will be used as the basis of weighting.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>r2_bayes()</code> returns an &quot;unadjusted&quot; R2 value. See
<code><a href="#topic+r2_loo">r2_loo()</a></code> to calculate a LOO-adjusted R2, which comes
conceptually closer to an adjusted R2 measure.
</p>
<p>For mixed models, the conditional and marginal R2 are returned. The marginal
R2 considers only the variance of the fixed effects, while the conditional
R2 takes both the fixed and random effects into account.
</p>
<p><code>r2_posterior()</code> is the actual workhorse for <code>r2_bayes()</code> and
returns a posterior sample of Bayesian R2 values.
</p>


<h3>Value</h3>

<p>A list with the Bayesian R2 value. For mixed models, a list with the
Bayesian R2 value and the marginal Bayesian R2 value. The standard errors
and credible intervals for the R2 values are saved as attributes.
</p>


<h3>References</h3>

<p>Gelman, A., Goodrich, B., Gabry, J., and Vehtari, A. (2018).
R-squared for Bayesian regression models. The American Statistician, 1–6.
<a href="https://doi.org/10.1080/00031305.2018.1549100">doi:10.1080/00031305.2018.1549100</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(performance)

model &lt;- suppressWarnings(rstanarm::stan_glm(
  mpg ~ wt + cyl,
  data = mtcars,
  chains = 1,
  iter = 500,
  refresh = 0,
  show_messages = FALSE
))
r2_bayes(model)

model &lt;- suppressWarnings(rstanarm::stan_lmer(
  Petal.Length ~ Petal.Width + (1 | Species),
  data = iris,
  chains = 1,
  iter = 500,
  refresh = 0
))
r2_bayes(model)



model &lt;- suppressWarnings(brms::brm(
  mpg ~ wt + cyl,
  data = mtcars,
  silent = 2,
  refresh = 0
))
r2_bayes(model)

model &lt;- suppressWarnings(brms::brm(
  Petal.Length ~ Petal.Width + (1 | Species),
  data = iris,
  silent = 2,
  refresh = 0
))
r2_bayes(model)


</code></pre>

<hr>
<h2 id='r2_coxsnell'>Cox &amp; Snell's R2</h2><span id='topic+r2_coxsnell'></span>

<h3>Description</h3>

<p>Calculates the pseudo-R2 value based on the proposal from <em>Cox &amp; Snell (1989)</em>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>r2_coxsnell(model, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="r2_coxsnell_+3A_model">model</code></td>
<td>
<p>Model with binary outcome.</p>
</td></tr>
<tr><td><code id="r2_coxsnell_+3A_...">...</code></td>
<td>
<p>Currently not used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This index was proposed by <em>Cox and Snell (1989, pp. 208-9)</em> and,
apparently independently, by <em>Magee (1990)</em>; but had been suggested
earlier for binary response models by <em>Maddala (1983)</em>. However, this
index achieves a maximum of less than 1 for discrete models (i.e. models
whose likelihood is a product of probabilities) which have a maximum of 1,
instead of densities, which can become infinite <em>(Nagelkerke, 1991)</em>.
</p>


<h3>Value</h3>

<p>A named vector with the R2 value.
</p>


<h3>References</h3>


<ul>
<li><p> Cox, D. R., Snell, E. J. (1989). Analysis of binary data (Vol. 32).
Monographs on Statistics and Applied Probability.
</p>
</li>
<li><p> Magee, L. (1990). R 2 measures based on Wald and likelihood ratio
joint significance tests. The American Statistician, 44(3), 250-253.
</p>
</li>
<li><p> Maddala, G. S. (1986). Limited-dependent and qualitative variables in
econometrics (No. 3). Cambridge university press.
</p>
</li>
<li><p> Nagelkerke, N. J. (1991). A note on a general definition of the
coefficient of determination. Biometrika, 78(3), 691-692.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>model &lt;- glm(vs ~ wt + mpg, data = mtcars, family = "binomial")
r2_coxsnell(model)

</code></pre>

<hr>
<h2 id='r2_efron'>Efron's R2</h2><span id='topic+r2_efron'></span>

<h3>Description</h3>

<p>Calculates Efron's pseudo R2.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>r2_efron(model)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="r2_efron_+3A_model">model</code></td>
<td>
<p>Generalized linear model.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Efron's R2 is calculated by taking the sum of the squared model residuals,
divided by the total variability in the dependent variable. This R2 equals
the squared correlation between the predicted values and actual values,
however, note that model residuals from generalized linear models are not
generally comparable to those of OLS.
</p>


<h3>Value</h3>

<p>The R2 value.
</p>


<h3>References</h3>

<p>Efron, B. (1978). Regression and ANOVA with zero-one data: Measures of
residual variation. Journal of the American Statistical Association, 73,
113-121.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Dobson (1990) Page 93: Randomized Controlled Trial:
counts &lt;- c(18, 17, 15, 20, 10, 20, 25, 13, 12) #
outcome &lt;- gl(3, 1, 9)
treatment &lt;- gl(3, 3)
model &lt;- glm(counts ~ outcome + treatment, family = poisson())

r2_efron(model)
</code></pre>

<hr>
<h2 id='r2_kullback'>Kullback-Leibler R2</h2><span id='topic+r2_kullback'></span><span id='topic+r2_kullback.glm'></span>

<h3>Description</h3>

<p>Calculates the Kullback-Leibler-divergence-based
R2 for generalized linear models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>r2_kullback(model, ...)

## S3 method for class 'glm'
r2_kullback(model, adjust = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="r2_kullback_+3A_model">model</code></td>
<td>
<p>A generalized linear model.</p>
</td></tr>
<tr><td><code id="r2_kullback_+3A_...">...</code></td>
<td>
<p>Additional arguments. Currently not used.</p>
</td></tr>
<tr><td><code id="r2_kullback_+3A_adjust">adjust</code></td>
<td>
<p>Logical, if <code>TRUE</code> (the default), the adjusted R2 value is
returned.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A named vector with the R2 value.
</p>


<h3>References</h3>

<p>Cameron, A. C. and Windmeijer, A. G. (1997) An R-squared measure of goodness
of fit for some common nonlinear regression models. Journal of Econometrics,
77: 329-342.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>model &lt;- glm(vs ~ wt + mpg, data = mtcars, family = "binomial")
r2_kullback(model)
</code></pre>

<hr>
<h2 id='r2_loo'>LOO-adjusted R2</h2><span id='topic+r2_loo'></span><span id='topic+r2_loo_posterior'></span><span id='topic+r2_loo_posterior.brmsfit'></span><span id='topic+r2_loo_posterior.stanreg'></span>

<h3>Description</h3>

<p>Compute LOO-adjusted R2.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>r2_loo(model, robust = TRUE, ci = 0.95, verbose = TRUE, ...)

r2_loo_posterior(model, ...)

## S3 method for class 'brmsfit'
r2_loo_posterior(model, verbose = TRUE, ...)

## S3 method for class 'stanreg'
r2_loo_posterior(model, verbose = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="r2_loo_+3A_model">model</code></td>
<td>
<p>A Bayesian regression model (from <strong>brms</strong>,
<strong>rstanarm</strong>, <strong>BayesFactor</strong>, etc).</p>
</td></tr>
<tr><td><code id="r2_loo_+3A_robust">robust</code></td>
<td>
<p>Logical, if <code>TRUE</code>, the median instead of mean is used to
calculate the central tendency of the variances.</p>
</td></tr>
<tr><td><code id="r2_loo_+3A_ci">ci</code></td>
<td>
<p>Value or vector of probability of the CI (between 0 and 1) to be
estimated.</p>
</td></tr>
<tr><td><code id="r2_loo_+3A_verbose">verbose</code></td>
<td>
<p>Toggle off warnings.</p>
</td></tr>
<tr><td><code id="r2_loo_+3A_...">...</code></td>
<td>
<p>Arguments passed to <code>r2_posterior()</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>r2_loo()</code> returns an &quot;adjusted&quot; R2 value computed using a
leave-one-out-adjusted posterior distribution. This is conceptually similar
to an adjusted/unbiased R2 estimate in classical regression modeling. See
<code><a href="#topic+r2_bayes">r2_bayes()</a></code> for an &quot;unadjusted&quot; R2.
</p>
<p>Mixed models are not currently fully supported.
</p>
<p><code>r2_loo_posterior()</code> is the actual workhorse for <code>r2_loo()</code> and
returns a posterior sample of LOO-adjusted Bayesian R2 values.
</p>


<h3>Value</h3>

<p>A list with the Bayesian R2 value. For mixed models, a list with the
Bayesian R2 value and the marginal Bayesian R2 value. The standard errors
and credible intervals for the R2 values are saved as attributes.
</p>
<p>A list with the LOO-adjusted R2 value. The standard errors
and credible intervals for the R2 values are saved as attributes.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
model &lt;- suppressWarnings(rstanarm::stan_glm(
  mpg ~ wt + cyl,
  data = mtcars,
  chains = 1,
  iter = 500,
  refresh = 0,
  show_messages = FALSE
))
r2_loo(model)

</code></pre>

<hr>
<h2 id='r2_mcfadden'>McFadden's R2</h2><span id='topic+r2_mcfadden'></span>

<h3>Description</h3>

<p>Calculates McFadden's pseudo R2.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>r2_mcfadden(model, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="r2_mcfadden_+3A_model">model</code></td>
<td>
<p>Generalized linear or multinomial logit (<code>mlogit</code>) model.</p>
</td></tr>
<tr><td><code id="r2_mcfadden_+3A_...">...</code></td>
<td>
<p>Currently not used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>For most models, a list with McFadden's R2 and adjusted McFadden's
R2 value. For some models, only McFadden's R2 is available.
</p>


<h3>References</h3>


<ul>
<li><p> McFadden, D. (1987). Regression-based specification tests for the
multinomial logit model. Journal of econometrics, 34(1-2), 63-82.
</p>
</li>
<li><p> McFadden, D. (1973). Conditional logit analysis of qualitative choice
behavior.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>if (require("mlogit")) {
  data("Fishing", package = "mlogit")
  Fish &lt;- mlogit.data(Fishing, varying = c(2:9), shape = "wide", choice = "mode")

  model &lt;- mlogit(mode ~ price + catch, data = Fish)
  r2_mcfadden(model)
}
</code></pre>

<hr>
<h2 id='r2_mckelvey'>McKelvey &amp; Zavoinas R2</h2><span id='topic+r2_mckelvey'></span>

<h3>Description</h3>

<p>Calculates McKelvey and Zavoinas pseudo R2.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>r2_mckelvey(model)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="r2_mckelvey_+3A_model">model</code></td>
<td>
<p>Generalized linear model.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>McKelvey and Zavoinas R2 is based on the explained variance,
where the variance of the predicted response is divided by the sum
of the variance of the predicted response and residual variance.
For binomial models, the residual variance is either <code>pi^2/3</code>
for logit-link and 1 for probit-link. For poisson-models, the
residual variance is based on log-normal approximation, similar to
the <em>distribution-specific variance</em> as described in
<code>?insight::get_variance</code>.
</p>


<h3>Value</h3>

<p>The R2 value.
</p>


<h3>References</h3>

<p>McKelvey, R., Zavoina, W. (1975), &quot;A Statistical Model for the Analysis of
Ordinal Level Dependent Variables&quot;, Journal of Mathematical Sociology 4, S.
103–120.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Dobson (1990) Page 93: Randomized Controlled Trial:
counts &lt;- c(18, 17, 15, 20, 10, 20, 25, 13, 12) #
outcome &lt;- gl(3, 1, 9)
treatment &lt;- gl(3, 3)
model &lt;- glm(counts ~ outcome + treatment, family = poisson())

r2_mckelvey(model)
</code></pre>

<hr>
<h2 id='r2_nagelkerke'>Nagelkerke's R2</h2><span id='topic+r2_nagelkerke'></span>

<h3>Description</h3>

<p>Calculate Nagelkerke's pseudo-R2.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>r2_nagelkerke(model, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="r2_nagelkerke_+3A_model">model</code></td>
<td>
<p>A generalized linear model, including cumulative links resp.
multinomial models.</p>
</td></tr>
<tr><td><code id="r2_nagelkerke_+3A_...">...</code></td>
<td>
<p>Currently not used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A named vector with the R2 value.
</p>


<h3>References</h3>

<p>Nagelkerke, N. J. (1991). A note on a general definition of the coefficient
of determination. Biometrika, 78(3), 691-692.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>model &lt;- glm(vs ~ wt + mpg, data = mtcars, family = "binomial")
r2_nagelkerke(model)
</code></pre>

<hr>
<h2 id='r2_nakagawa'>Nakagawa's R2 for mixed models</h2><span id='topic+r2_nakagawa'></span>

<h3>Description</h3>

<p>Compute the <em>marginal</em> and <em>conditional</em> r-squared value for
mixed effects models with complex random effects structures.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>r2_nakagawa(
  model,
  by_group = FALSE,
  tolerance = 1e-05,
  ci = NULL,
  iterations = 100,
  ci_method = NULL,
  verbose = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="r2_nakagawa_+3A_model">model</code></td>
<td>
<p>A mixed effects model.</p>
</td></tr>
<tr><td><code id="r2_nakagawa_+3A_by_group">by_group</code></td>
<td>
<p>Logical, if <code>TRUE</code>, returns the explained variance
at different levels (if there are multiple levels). This is essentially
similar to the variance reduction approach by <em>Hox (2010), pp. 69-78</em>.</p>
</td></tr>
<tr><td><code id="r2_nakagawa_+3A_tolerance">tolerance</code></td>
<td>
<p>Tolerance for singularity check of random effects, to decide
whether to compute random effect variances for the conditional r-squared
or not. Indicates up to which value the convergence result is accepted. When
<code>r2_nakagawa()</code> returns a warning, stating that random effect variances
can't be computed (and thus, the conditional r-squared is <code>NA</code>),
decrease the tolerance-level. See also <code><a href="#topic+check_singularity">check_singularity()</a></code>.</p>
</td></tr>
<tr><td><code id="r2_nakagawa_+3A_ci">ci</code></td>
<td>
<p>Confidence resp. credible interval level. For <code>icc()</code> and <code>r2()</code>,
confidence intervals are based on bootstrapped samples from the ICC resp.
R2 value. See <code>iterations</code>.</p>
</td></tr>
<tr><td><code id="r2_nakagawa_+3A_iterations">iterations</code></td>
<td>
<p>Number of bootstrap-replicates when computing confidence
intervals for the ICC or R2.</p>
</td></tr>
<tr><td><code id="r2_nakagawa_+3A_ci_method">ci_method</code></td>
<td>
<p>Character string, indicating the bootstrap-method. Should
be <code>NULL</code> (default), in which case <code>lme4::bootMer()</code> is used for
bootstrapped confidence intervals. However, if bootstrapped intervals cannot
be calculated this was, try <code>ci_method = "boot"</code>, which falls back to
<code>boot::boot()</code>. This may successfully return bootstrapped confidence intervals,
but bootstrapped samples may not be appropriate for the multilevel structure
of the model. There is also an option <code>ci_method = "analytical"</code>, which tries
to calculate analytical confidence assuming a chi-squared distribution.
However, these intervals are rather inaccurate and often too narrow. It is
recommended to calculate bootstrapped confidence intervals for mixed models.</p>
</td></tr>
<tr><td><code id="r2_nakagawa_+3A_verbose">verbose</code></td>
<td>
<p>Toggle warnings and messages.</p>
</td></tr>
<tr><td><code id="r2_nakagawa_+3A_...">...</code></td>
<td>
<p>Arguments passed down to <code>brms::posterior_predict()</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Marginal and conditional r-squared values for mixed models are calculated
based on <em>Nakagawa et al. (2017)</em>. For more details on the computation of
the variances, see <code>?insight::get_variance</code>. The random effect variances are
actually the mean random effect variances, thus the r-squared value is also
appropriate for mixed models with random slopes or nested random effects
(see <em>Johnson, 2014</em>).
</p>

<ul>
<li> <p><strong>Conditional R2</strong>: takes both the fixed and random effects into account.
</p>
</li>
<li> <p><strong>Marginal R2</strong>: considers only the variance of the fixed effects.
</p>
</li></ul>

<p>The contribution of random effects can be deduced by subtracting the
marginal R2 from the conditional R2 or by computing the <code><a href="#topic+icc">icc()</a></code>.
</p>


<h3>Value</h3>

<p>A list with the conditional and marginal R2 values.
</p>


<h3>References</h3>


<ul>
<li><p> Hox, J. J. (2010). Multilevel analysis: techniques and applications
(2nd ed). New York: Routledge.
</p>
</li>
<li><p> Johnson, P. C. D. (2014). Extension of Nakagawa and Schielzeth’s R2 GLMM
to random slopes models. Methods in Ecology and Evolution, 5(9), 944–946.
<a href="https://doi.org/10.1111/2041-210X.12225">doi:10.1111/2041-210X.12225</a>
</p>
</li>
<li><p> Nakagawa, S., and Schielzeth, H. (2013). A general and simple method for
obtaining R2 from generalized linear mixed-effects models. Methods in
Ecology and Evolution, 4(2), 133–142. <a href="https://doi.org/10.1111/j.2041-210x.2012.00261.x">doi:10.1111/j.2041-210x.2012.00261.x</a>
</p>
</li>
<li><p> Nakagawa, S., Johnson, P. C. D., and Schielzeth, H. (2017). The
coefficient of determination R2 and intra-class correlation coefficient from
generalized linear mixed-effects models revisited and expanded. Journal of
The Royal Society Interface, 14(134), 20170213.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>
model &lt;- lme4::lmer(Sepal.Length ~ Petal.Length + (1 | Species), data = iris)
r2_nakagawa(model)
r2_nakagawa(model, by_group = TRUE)

</code></pre>

<hr>
<h2 id='r2_somers'>Somers' Dxy rank correlation for binary outcomes</h2><span id='topic+r2_somers'></span>

<h3>Description</h3>

<p>Calculates the Somers' Dxy rank correlation for logistic regression models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>r2_somers(model)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="r2_somers_+3A_model">model</code></td>
<td>
<p>A logistic regression model.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A named vector with the R2 value.
</p>


<h3>References</h3>

<p>Somers, R. H. (1962). A new asymmetric measure of association for
ordinal variables. American Sociological Review. 27 (6).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
if (require("correlation") &amp;&amp; require("Hmisc")) {
  model &lt;- glm(vs ~ wt + mpg, data = mtcars, family = "binomial")
  r2_somers(model)
}


</code></pre>

<hr>
<h2 id='r2_tjur'>Tjur's R2 - coefficient of determination (D)</h2><span id='topic+r2_tjur'></span>

<h3>Description</h3>

<p>This method calculates the Coefficient of Discrimination <code>D</code>
(also known as Tjur's R2; <cite>Tjur, 2009</cite>) for generalized linear (mixed) models
for binary outcomes. It is an alternative to other pseudo-R2 values like
Nagelkerke's R2 or Cox-Snell R2. The Coefficient of Discrimination <code>D</code>
can be read like any other (pseudo-)R2 value.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>r2_tjur(model, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="r2_tjur_+3A_model">model</code></td>
<td>
<p>Binomial Model.</p>
</td></tr>
<tr><td><code id="r2_tjur_+3A_...">...</code></td>
<td>
<p>Arguments from other functions, usually only used internally.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A named vector with the R2 value.
</p>


<h3>References</h3>

<p>Tjur, T. (2009). Coefficients of determination in logistic regression
models - A new proposal: The coefficient of discrimination. The American
Statistician, 63(4), 366-372.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>model &lt;- glm(vs ~ wt + mpg, data = mtcars, family = "binomial")
r2_tjur(model)

</code></pre>

<hr>
<h2 id='r2_xu'>Xu' R2 (Omega-squared)</h2><span id='topic+r2_xu'></span>

<h3>Description</h3>

<p>Calculates Xu' Omega-squared value, a simple R2 equivalent for
linear mixed models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>r2_xu(model)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="r2_xu_+3A_model">model</code></td>
<td>
<p>A linear (mixed) model.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>r2_xu()</code> is a crude measure for the explained variance from
linear (mixed) effects models, which is originally denoted as
&Omega;<sup>2</sup>.
</p>


<h3>Value</h3>

<p>The R2 value.
</p>


<h3>References</h3>

<p>Xu, R. (2003). Measuring explained variation in linear mixed effects models.
Statistics in Medicine, 22(22), 3527–3541. <a href="https://doi.org/10.1002/sim.1572">doi:10.1002/sim.1572</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>model &lt;- lm(Sepal.Length ~ Petal.Length + Species, data = iris)
r2_xu(model)
</code></pre>

<hr>
<h2 id='r2_zeroinflated'>R2 for models with zero-inflation</h2><span id='topic+r2_zeroinflated'></span>

<h3>Description</h3>

<p>Calculates R2 for models with zero-inflation component, including mixed
effects models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>r2_zeroinflated(model, method = c("default", "correlation"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="r2_zeroinflated_+3A_model">model</code></td>
<td>
<p>A model.</p>
</td></tr>
<tr><td><code id="r2_zeroinflated_+3A_method">method</code></td>
<td>
<p>Indicates the method to calculate R2. See 'Details'. May be
abbreviated.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The default-method calculates an R2 value based on the residual variance
divided by the total variance. For <code>method = "correlation"</code>, R2 is a
correlation-based measure, which is rather crude. It simply computes the
squared correlation between the model's actual and predicted response.
</p>


<h3>Value</h3>

<p>For the default-method, a list with the R2 and adjusted R2 values.
For <code>method = "correlation"</code>, a named numeric vector with the
correlation-based R2 value.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
if (require("pscl")) {
  data(bioChemists)
  model &lt;- zeroinfl(
    art ~ fem + mar + kid5 + ment | kid5 + phd,
    data = bioChemists
  )

  r2_zeroinflated(model)
}

</code></pre>

<hr>
<h2 id='reexports'>Objects exported from other packages</h2><span id='topic+reexports'></span><span id='topic+display'></span><span id='topic+print_md'></span><span id='topic+print_html'></span>

<h3>Description</h3>

<p>These objects are imported from other packages. Follow the links
below to see their documentation.
</p>

<dl>
<dt>insight</dt><dd><p><code><a href="insight.html#topic+display">display</a></code>, <code><a href="insight.html#topic+display">print_html</a></code>, <code><a href="insight.html#topic+display">print_md</a></code></p>
</dd>
</dl>

<hr>
<h2 id='simulate_residuals'>Simulate randomized quantile residuals from a model</h2><span id='topic+simulate_residuals'></span><span id='topic+residuals.performance_simres'></span>

<h3>Description</h3>

<p>Returns simulated residuals from a model. This is useful for
checking the uniformity of residuals, in particular for non-Gaussian models,
where the residuals are not expected to be normally distributed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>simulate_residuals(x, iterations = 250, ...)

## S3 method for class 'performance_simres'
residuals(object, quantile_function = NULL, outlier_values = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="simulate_residuals_+3A_x">x</code></td>
<td>
<p>A model object.</p>
</td></tr>
<tr><td><code id="simulate_residuals_+3A_iterations">iterations</code></td>
<td>
<p>Number of simulations to run.</p>
</td></tr>
<tr><td><code id="simulate_residuals_+3A_...">...</code></td>
<td>
<p>Arguments passed on to <code><a href="DHARMa.html#topic+simulateResiduals">DHARMa::simulateResiduals()</a></code>.</p>
</td></tr>
<tr><td><code id="simulate_residuals_+3A_object">object</code></td>
<td>
<p>A <code>performance_simres</code> object, as returned by <code>simulate_residuals()</code>.</p>
</td></tr>
<tr><td><code id="simulate_residuals_+3A_quantile_function">quantile_function</code></td>
<td>
<p>A function to apply to the residuals. If <code>NULL</code>, the
residuals are returned as is. If not <code>NULL</code>, the residuals are passed to this
function. This is useful for returning normally distributed residuals, for
example: <code>residuals(x, quantile_function = qnorm)</code>.</p>
</td></tr>
<tr><td><code id="simulate_residuals_+3A_outlier_values">outlier_values</code></td>
<td>
<p>A vector of length 2, specifying the values to replace
<code>-Inf</code> and <code>Inf</code> with, respectively.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is a small wrapper around <code><a href="DHARMa.html#topic+simulateResiduals">DHARMa::simulateResiduals()</a></code>.
It basically only sets <code>plot = FALSE</code> and adds an additional class attribute
(<code>"performance_sim_res"</code>), which allows using the DHARMa object in own plotting
functions from the <strong>see</strong> package. See also <code>vignette("DHARMa")</code>. There is a
<code>plot()</code> method to visualize the distribution of the residuals.
</p>


<h3>Value</h3>

<p>Simulated residuals, which can be further processed with
<code><a href="#topic+check_residuals">check_residuals()</a></code>. The returned object is of class <code>DHARMa</code> and
<code>performance_simres</code>.
</p>


<h3>Tests based on simulated residuals</h3>

<p>For certain models, resp. model from certain families, tests like
<code><a href="#topic+check_zeroinflation">check_zeroinflation()</a></code> or <code><a href="#topic+check_overdispersion">check_overdispersion()</a></code> are based on
simulated residuals. These are usually more accurate for such tests than
the traditionally used Pearson residuals. However, when simulating from more
complex models, such as mixed models or models with zero-inflation, there are
several important considerations. <code>simulate_residuals()</code> relies on
<code><a href="DHARMa.html#topic+simulateResiduals">DHARMa::simulateResiduals()</a></code>, and additional arguments specified in <code>...</code>
are passed further down to that function. The defaults in DHARMa are set on
the most conservative option that works for all models. However, in many
cases, the help advises to use different settings in particular situations
or for particular models. It is recommended to read the 'Details' in
<code>?DHARMa::simulateResiduals</code> closely to understand the implications of the
simulation process and which arguments should be modified to get the most
accurate results.
</p>


<h3>References</h3>


<ul>
<li><p> Hartig, F., &amp; Lohse, L. (2022). DHARMa: Residual Diagnostics for Hierarchical
(Multi-Level / Mixed) Regression Models (Version 0.4.5). Retrieved from
https://CRAN.R-project.org/package=DHARMa
</p>
</li>
<li><p> Dunn, P. K., &amp; Smyth, G. K. (1996). Randomized Quantile Residuals. Journal
of Computational and Graphical Statistics, 5(3), 236. <a href="https://doi.org/10.2307/1390802">doi:10.2307/1390802</a>
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+check_residuals">check_residuals()</a></code>, <code><a href="#topic+check_zeroinflation">check_zeroinflation()</a></code>,
<code><a href="#topic+check_overdispersion">check_overdispersion()</a></code> and <code><a href="#topic+check_predictions">check_predictions()</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
m &lt;- lm(mpg ~ wt + cyl + gear + disp, data = mtcars)
simulate_residuals(m)

# extract residuals
head(residuals(simulate_residuals(m)))

</code></pre>

<hr>
<h2 id='test_bf'>Test if models are different</h2><span id='topic+test_bf'></span><span id='topic+test_bf.default'></span><span id='topic+test_likelihoodratio'></span><span id='topic+test_lrt'></span><span id='topic+test_performance'></span><span id='topic+test_vuong'></span><span id='topic+test_wald'></span>

<h3>Description</h3>

<p>Testing whether models are &quot;different&quot; in terms of accuracy or explanatory
power is a delicate and often complex procedure, with many limitations and
prerequisites. Moreover, many tests exist, each coming with its own
interpretation, and set of strengths and weaknesses.
</p>
<p>The <code>test_performance()</code> function runs the most relevant and appropriate
tests based on the type of input (for instance, whether the models are
<em>nested</em> or not). However, it still requires the user to understand what the
tests are and what they do in order to prevent their misinterpretation. See
the <em>Details</em> section for more information regarding the different tests
and their interpretation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>test_bf(...)

## Default S3 method:
test_bf(..., reference = 1, text_length = NULL)

test_likelihoodratio(..., estimator = "ML", verbose = TRUE)

test_lrt(..., estimator = "ML", verbose = TRUE)

test_performance(..., reference = 1, verbose = TRUE)

test_vuong(..., verbose = TRUE)

test_wald(..., verbose = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="test_bf_+3A_...">...</code></td>
<td>
<p>Multiple model objects.</p>
</td></tr>
<tr><td><code id="test_bf_+3A_reference">reference</code></td>
<td>
<p>This only applies when models are non-nested, and determines
which model should be taken as a reference, against which all the other
models are tested.</p>
</td></tr>
<tr><td><code id="test_bf_+3A_text_length">text_length</code></td>
<td>
<p>Numeric, length (number of chars) of output lines.
<code>test_bf()</code> describes models by their formulas, which can lead to
overly long lines in the output. <code>text_length</code> fixes the length of
lines to a specified limit.</p>
</td></tr>
<tr><td><code id="test_bf_+3A_estimator">estimator</code></td>
<td>
<p>Applied when comparing regression models using
<code>test_likelihoodratio()</code>. Corresponds to the different estimators for
the standard deviation of the errors. Defaults to <code>"OLS"</code> for linear models,
<code>"ML"</code> for all other models (including mixed models), or <code>"REML"</code> for
linear mixed models when these have the same fixed effects. See 'Details'.</p>
</td></tr>
<tr><td><code id="test_bf_+3A_verbose">verbose</code></td>
<td>
<p>Toggle warning and messages.</p>
</td></tr>
</table>


<h3>Details</h3>



<h4>Nested vs. Non-nested Models</h4>

<p>Model's &quot;nesting&quot; is an important concept of models comparison. Indeed, many
tests only make sense when the models are <em>&quot;nested&quot;,</em> i.e., when their
predictors are nested. This means that all the <em>fixed effects</em> predictors of
a model are contained within the <em>fixed effects</em> predictors of a larger model
(sometimes referred to as the encompassing model). For instance,
<code>model1 (y ~ x1 + x2)</code> is &quot;nested&quot; within <code>model2 (y ~ x1 + x2 + x3)</code>. Usually,
people have a list of nested models, for instance <code>m1 (y ~ 1)</code>, <code>m2 (y ~ x1)</code>,
<code>m3 (y ~ x1 + x2)</code>, <code>m4 (y ~ x1 + x2 + x3)</code>, and it is conventional
that they are &quot;ordered&quot; from the smallest to largest, but it is up to the
user to reverse the order from largest to smallest. The test then shows
whether a more parsimonious model, or whether adding a predictor, results in
a significant difference in the model's performance. In this case, models are
usually compared <em>sequentially</em>: m2 is tested against m1, m3 against m2,
m4 against m3, etc.
</p>
<p>Two models are considered as <em>&quot;non-nested&quot;</em> if their predictors are
different. For instance, <code>model1 (y ~ x1 + x2)</code> and <code>model2 (y ~ x3 + x4)</code>.
In the case of non-nested models, all models are usually compared
against the same <em>reference</em> model (by default, the first of the list).
</p>
<p>Nesting is detected via the <code>insight::is_nested_models()</code> function.
Note that, apart from the nesting, in order for the tests to be valid,
other requirements have often to be the fulfilled. For instance, outcome
variables (the response) must be the same. You cannot meaningfully test
whether apples are significantly different from oranges!
</p>



<h4>Estimator of the standard deviation</h4>

<p>The estimator is relevant when comparing regression models using
<code>test_likelihoodratio()</code>. If <code>estimator = "OLS"</code>, then it uses the same
method as <code>anova(..., test = "LRT")</code> implemented in base R, i.e., scaling
by n-k (the unbiased OLS estimator) and using this estimator under the
alternative hypothesis. If <code>estimator = "ML"</code>, which is for instance used
by <code>lrtest(...)</code> in package <strong>lmtest</strong>, the scaling is done by n (the
biased ML estimator) and the estimator under the null hypothesis. In
moderately large samples, the differences should be negligible, but it
is possible that OLS would perform slightly better in small samples with
Gaussian errors. For <code>estimator = "REML"</code>, the LRT is based on the REML-fit
log-likelihoods of the models. Note that not all types of estimators are
available for all model classes.
</p>



<h4>REML versus ML estimator</h4>

<p>When <code>estimator = "ML"</code>, which is the default for linear mixed models (unless
they share the same fixed effects), values from information criteria (AIC,
AICc) are based on the ML-estimator, while the default behaviour of <code>AIC()</code>
may be different (in particular for linear mixed models from <strong>lme4</strong>, which
sets <code>REML = TRUE</code>). This default in <code>test_likelihoodratio()</code> intentional,
because comparing information criteria based on REML fits requires the same
fixed effects for all models, which is often not the case. Thus, while
<code>anova.merMod()</code> automatically refits all models to REML when performing a
LRT, <code>test_likelihoodratio()</code> checks if a comparison based on REML fits is
indeed valid, and if so, uses REML as default (else, ML is the default).
Set the <code>estimator</code> argument explicitely to override the default behaviour.
</p>



<h4>Tests Description</h4>


<ul>
<li> <p><strong>Bayes factor for Model Comparison</strong> - <code>test_bf()</code>: If all
models were fit from the same data, the returned <code>BF</code> shows the Bayes
Factor (see <code>bayestestR::bayesfactor_models()</code>) for each model against
the reference model (which depends on whether the models are nested or
not). Check out
<a href="https://easystats.github.io/bayestestR/articles/bayes_factors.html#bayesfactor_models">this vignette</a>
for more details.
</p>
</li>
<li> <p><strong>Wald's F-Test</strong> - <code>test_wald()</code>: The Wald test is a rough
approximation of the Likelihood Ratio Test. However, it is more applicable
than the LRT: you can often run a Wald test in situations where no other
test can be run. Importantly, this test only makes statistical sense if the
models are nested.
</p>
<p>Note: this test is also available in base R
through the <code><a href="stats.html#topic+anova">anova()</a></code> function. It returns an <code>F-value</code> column
as a statistic and its associated p-value.
</p>
</li>
<li> <p><strong>Likelihood Ratio Test (LRT)</strong> - <code>test_likelihoodratio()</code>:
The LRT tests which model is a better (more likely) explanation of the
data. Likelihood-Ratio-Test (LRT) gives usually somewhat close results (if
not equivalent) to the Wald test and, similarly, only makes sense for
nested models. However, maximum likelihood tests make stronger assumptions
than method of moments tests like the F-test, and in turn are more
efficient. Agresti (1990) suggests that you should use the LRT instead of
the Wald test for small sample sizes (under or about 30) or if the
parameters are large.
</p>
<p>Note: for regression models, this is similar to
<code>anova(..., test="LRT")</code> (on models) or <code>lmtest::lrtest(...)</code>, depending
on the <code>estimator</code> argument. For <strong>lavaan</strong> models (SEM, CFA), the function
calls <code>lavaan::lavTestLRT()</code>.
</p>
<p>For models with transformed response variables (like <code>log(x)</code> or <code>sqrt(x)</code>),
<code>logLik()</code> returns a wrong log-likelihood. However, <code>test_likelihoodratio()</code>
calls <code>insight::get_loglikelihood()</code> with <code>check_response=TRUE</code>, which
returns a corrected log-likelihood value for models with transformed
response variables. Furthermore, since the LRT only accepts nested
models (i.e. models that differ in their fixed effects), the computed
log-likelihood is always based on the ML estimator, not on the REML fits.
</p>
</li>
<li> <p><strong>Vuong's Test</strong> - <code>test_vuong()</code>: Vuong's (1989) test can
be used both for nested and non-nested models, and actually consists of two
tests.
</p>

<ul>
<li><p> The <strong>Test of Distinguishability</strong> (the <code>Omega2</code> column and
its associated p-value) indicates whether or not the models can possibly be
distinguished on the basis of the observed data. If its p-value is
significant, it means the models are distinguishable.
</p>
</li>
<li><p> The <strong>Robust Likelihood Test</strong> (the <code>LR</code> column and its
associated p-value) indicates whether each model fits better than the
reference model. If the models are nested, then the test works as a robust
LRT. The code for this function is adapted from the <strong>nonnest2</strong>
package, and all credit go to their authors.
</p>
</li></ul>

</li></ul>




<h3>Value</h3>

<p>A data frame containing the relevant indices.
</p>


<h3>References</h3>


<ul>
<li><p> Vuong, Q. H. (1989). Likelihood ratio tests for model selection and
non-nested hypotheses. Econometrica, 57, 307-333.
</p>
</li>
<li><p> Merkle, E. C., You, D., &amp; Preacher, K. (2016). Testing non-nested
structural equation models. Psychological Methods, 21, 151-163.
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+compare_performance">compare_performance()</a></code> to compare the performance indices of
many different models.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Nested Models
# -------------
m1 &lt;- lm(Sepal.Length ~ Petal.Width, data = iris)
m2 &lt;- lm(Sepal.Length ~ Petal.Width + Species, data = iris)
m3 &lt;- lm(Sepal.Length ~ Petal.Width * Species, data = iris)

test_performance(m1, m2, m3)

test_bf(m1, m2, m3)
test_wald(m1, m2, m3) # Equivalent to anova(m1, m2, m3)

# Equivalent to lmtest::lrtest(m1, m2, m3)
test_likelihoodratio(m1, m2, m3, estimator = "ML")

# Equivalent to anova(m1, m2, m3, test='LRT')
test_likelihoodratio(m1, m2, m3, estimator = "OLS")

if (require("CompQuadForm")) {
  test_vuong(m1, m2, m3) # nonnest2::vuongtest(m1, m2, nested=TRUE)

  # Non-nested Models
  # -----------------
  m1 &lt;- lm(Sepal.Length ~ Petal.Width, data = iris)
  m2 &lt;- lm(Sepal.Length ~ Petal.Length, data = iris)
  m3 &lt;- lm(Sepal.Length ~ Species, data = iris)

  test_performance(m1, m2, m3)
  test_bf(m1, m2, m3)
  test_vuong(m1, m2, m3) # nonnest2::vuongtest(m1, m2)
}

# Tweak the output
# ----------------
test_performance(m1, m2, m3, include_formula = TRUE)


# SEM / CFA (lavaan objects)
# --------------------------
# Lavaan Models
if (require("lavaan")) {
  structure &lt;- " visual  =~ x1 + x2 + x3
                 textual =~ x4 + x5 + x6
                 speed   =~ x7 + x8 + x9

                  visual ~~ textual + speed "
  m1 &lt;- lavaan::cfa(structure, data = HolzingerSwineford1939)

  structure &lt;- " visual  =~ x1 + x2 + x3
                 textual =~ x4 + x5 + x6
                 speed   =~ x7 + x8 + x9

                  visual ~~ 0 * textual + speed "
  m2 &lt;- lavaan::cfa(structure, data = HolzingerSwineford1939)

  structure &lt;- " visual  =~ x1 + x2 + x3
                 textual =~ x4 + x5 + x6
                 speed   =~ x7 + x8 + x9

                  visual ~~ 0 * textual + 0 * speed "
  m3 &lt;- lavaan::cfa(structure, data = HolzingerSwineford1939)

  test_likelihoodratio(m1, m2, m3)

  # Different Model Types
  # ---------------------
  if (require("lme4") &amp;&amp; require("mgcv")) {
    m1 &lt;- lm(Sepal.Length ~ Petal.Length + Species, data = iris)
    m2 &lt;- lmer(Sepal.Length ~ Petal.Length + (1 | Species), data = iris)
    m3 &lt;- gam(Sepal.Length ~ s(Petal.Length, by = Species) + Species, data = iris)

    test_performance(m1, m2, m3)
  }
}

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
