<!DOCTYPE html><html><head><title>Help for package tokenizers</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {tokenizers}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#basic-tokenizers'><p>Basic tokenizers</p></a></li>
<li><a href='#chunk_text'><p>Chunk text into smaller segments</p></a></li>
<li><a href='#count_words'><p>Count words, sentences, characters</p></a></li>
<li><a href='#mobydick'><p>The text of Moby Dick</p></a></li>
<li><a href='#ngram-tokenizers'><p>N-gram tokenizers</p></a></li>
<li><a href='#tokenize_character_shingles'><p>Character shingle tokenizers</p></a></li>
<li><a href='#tokenize_ptb'><p>Penn Treebank Tokenizer</p></a></li>
<li><a href='#tokenize_word_stems'><p>Word stem tokenizer</p></a></li>
<li><a href='#tokenizers'><p>Tokenizers</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Fast, Consistent Tokenization of Natural Language Text</td>
</tr>
<tr>
<td>Version:</td>
<td>0.3.0</td>
</tr>
<tr>
<td>Date:</td>
<td>2022-12-19</td>
</tr>
<tr>
<td>Description:</td>
<td>Convert natural language text into tokens. Includes tokenizers for
    shingled n-grams, skip n-grams, words, word stems, sentences, paragraphs,
    characters, shingled characters, lines, Penn Treebank, regular
    expressions, as well as functions for counting characters, words, and sentences,
    and a function for splitting longer texts into separate documents, each with
    the same number of words.  The tokenizers have a consistent interface, and
    the package is built on the 'stringi' and 'Rcpp' packages for  fast
    yet correct tokenization in 'UTF-8'. </td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>LazyData:</td>
<td>yes</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://docs.ropensci.org/tokenizers/">https://docs.ropensci.org/tokenizers/</a>,
<a href="https://github.com/ropensci/tokenizers">https://github.com/ropensci/tokenizers</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/ropensci/tokenizers/issues">https://github.com/ropensci/tokenizers/issues</a></td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.1</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.1.3)</td>
</tr>
<tr>
<td>Imports:</td>
<td>stringi (&ge; 1.0.1), Rcpp (&ge; 0.12.3), SnowballC (&ge; 0.5.1)</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Suggests:</td>
<td>covr, knitr, rmarkdown, stopwords (&ge; 0.9.0), testthat</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-12-20 21:28:10 UTC; lmullen</td>
</tr>
<tr>
<td>Author:</td>
<td>Lincoln Mullen <a href="https://orcid.org/0000-0001-5103-6917"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre],
  Os Keyes <a href="https://orcid.org/0000-0001-5196-609X"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [ctb],
  Dmitriy Selivanov [ctb],
  Jeffrey Arnold <a href="https://orcid.org/0000-0001-9953-3904"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [ctb],
  Kenneth Benoit <a href="https://orcid.org/0000-0002-0797-564X"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Lincoln Mullen &lt;lincoln@lincolnmullen.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-12-22 08:50:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='basic-tokenizers'>Basic tokenizers</h2><span id='topic+basic-tokenizers'></span><span id='topic+tokenize_characters'></span><span id='topic+tokenize_words'></span><span id='topic+tokenize_sentences'></span><span id='topic+tokenize_lines'></span><span id='topic+tokenize_paragraphs'></span><span id='topic+tokenize_regex'></span>

<h3>Description</h3>

<p>These functions perform basic tokenization into words, sentences, paragraphs,
lines, and characters. The functions can be piped into one another to create
at most two levels of tokenization. For instance, one might split a text into
paragraphs and then word tokens, or into sentences and then word tokens.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tokenize_characters(
  x,
  lowercase = TRUE,
  strip_non_alphanum = TRUE,
  simplify = FALSE
)

tokenize_words(
  x,
  lowercase = TRUE,
  stopwords = NULL,
  strip_punct = TRUE,
  strip_numeric = FALSE,
  simplify = FALSE
)

tokenize_sentences(x, lowercase = FALSE, strip_punct = FALSE, simplify = FALSE)

tokenize_lines(x, simplify = FALSE)

tokenize_paragraphs(x, paragraph_break = "\n\n", simplify = FALSE)

tokenize_regex(x, pattern = "\\s+", simplify = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="basic-tokenizers_+3A_x">x</code></td>
<td>
<p>A character vector or a list of character vectors to be tokenized.
If <code>x</code> is a character vector, it can be of any length, and each element
will be tokenized separately. If <code>x</code> is a list of character vectors,
where each element of the list should have a length of 1.</p>
</td></tr>
<tr><td><code id="basic-tokenizers_+3A_lowercase">lowercase</code></td>
<td>
<p>Should the tokens be made lower case? The default value
varies by tokenizer; it is only <code>TRUE</code> by default for the tokenizers
that you are likely to use last.</p>
</td></tr>
<tr><td><code id="basic-tokenizers_+3A_strip_non_alphanum">strip_non_alphanum</code></td>
<td>
<p>Should punctuation and white space be stripped?</p>
</td></tr>
<tr><td><code id="basic-tokenizers_+3A_simplify">simplify</code></td>
<td>
<p><code>FALSE</code> by default so that a consistent value is
returned regardless of length of input. If <code>TRUE</code>, then an input with
a single element will return a character vector of tokens instead of a
list.</p>
</td></tr>
<tr><td><code id="basic-tokenizers_+3A_stopwords">stopwords</code></td>
<td>
<p>A character vector of stop words to be excluded.</p>
</td></tr>
<tr><td><code id="basic-tokenizers_+3A_strip_punct">strip_punct</code></td>
<td>
<p>Should punctuation be stripped?</p>
</td></tr>
<tr><td><code id="basic-tokenizers_+3A_strip_numeric">strip_numeric</code></td>
<td>
<p>Should numbers be stripped?</p>
</td></tr>
<tr><td><code id="basic-tokenizers_+3A_paragraph_break">paragraph_break</code></td>
<td>
<p>A string identifying the boundary between two
paragraphs.</p>
</td></tr>
<tr><td><code id="basic-tokenizers_+3A_pattern">pattern</code></td>
<td>
<p>A regular expression that defines the split.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of character vectors containing the tokens, with one element
in the list for each element that was passed as input. If <code>simplify =
  TRUE</code> and only a single element was passed as input, then the output is a
character vector of tokens.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>song &lt;-  paste0("How many roads must a man walk down\n",
                "Before you call him a man?\n",
                "How many seas must a white dove sail\n",
                "Before she sleeps in the sand?\n",
                "\n",
                "How many times must the cannonballs fly\n",
                "Before they're forever banned?\n",
                "The answer, my friend, is blowin' in the wind.\n",
                "The answer is blowin' in the wind.\n")

tokenize_words(song)
tokenize_words(song, strip_punct = FALSE)
tokenize_sentences(song)
tokenize_paragraphs(song)
tokenize_lines(song)
tokenize_characters(song)
</code></pre>

<hr>
<h2 id='chunk_text'>Chunk text into smaller segments</h2><span id='topic+chunk_text'></span>

<h3>Description</h3>

<p>Given a text or vector/list of texts, break the texts into smaller segments
each with the same number of words. This allows you to treat a very long
document, such as a novel, as a set of smaller documents.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>chunk_text(x, chunk_size = 100, doc_id = names(x), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="chunk_text_+3A_x">x</code></td>
<td>
<p>A character vector or a list of character vectors to be tokenized
into n-grams. If <code>x</code> is a character vector, it can be of any length,
and each element will be chunked separately. If <code>x</code> is a list of
character vectors, each element of the list should have a length of 1.</p>
</td></tr>
<tr><td><code id="chunk_text_+3A_chunk_size">chunk_size</code></td>
<td>
<p>The number of words in each chunk.</p>
</td></tr>
<tr><td><code id="chunk_text_+3A_doc_id">doc_id</code></td>
<td>
<p>The document IDs as a character vector. This will be taken from
the names of the <code>x</code> vector if available. <code>NULL</code> is acceptable.</p>
</td></tr>
<tr><td><code id="chunk_text_+3A_...">...</code></td>
<td>
<p>Arguments passed on to <code><a href="#topic+tokenize_words">tokenize_words</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Chunking the text passes it through <code><a href="#topic+tokenize_words">tokenize_words</a></code>,
which will strip punctuation and lowercase the text unless you provide
arguments to pass along to that function.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
chunked &lt;- chunk_text(mobydick, chunk_size = 100)
length(chunked)
chunked[1:3]

## End(Not run)
</code></pre>

<hr>
<h2 id='count_words'>Count words, sentences, characters</h2><span id='topic+count_words'></span><span id='topic+count_characters'></span><span id='topic+count_sentences'></span>

<h3>Description</h3>

<p>Count words, sentences, and characters in input texts. These functions use
the <code>stringi</code> package, so they handle the counting of Unicode strings
(e.g., characters with diacritical marks) in a way that makes sense to people
counting characters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>count_words(x)

count_characters(x)

count_sentences(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="count_words_+3A_x">x</code></td>
<td>
<p>A character vector or a list of character vectors. If <code>x</code> is a
character vector, it can be of any length, and each element will be
tokenized separately. If <code>x</code> is a list of character vectors, each
element of the list should have a length of 1.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An integer vector containing the counted elements. If the input
vector or list has names, they will be preserved.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>count_words(mobydick)
count_sentences(mobydick)
count_characters(mobydick)
</code></pre>

<hr>
<h2 id='mobydick'>The text of Moby Dick</h2><span id='topic+mobydick'></span>

<h3>Description</h3>

<p>The text of Moby Dick, by Herman Melville, taken from Project Gutenberg.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mobydick
</code></pre>


<h3>Format</h3>

<p>A named character vector with length 1.
</p>


<h3>Source</h3>

<p><a href="http://www.gutenberg.org/">http://www.gutenberg.org/</a>
</p>

<hr>
<h2 id='ngram-tokenizers'>N-gram tokenizers</h2><span id='topic+ngram-tokenizers'></span><span id='topic+tokenize_ngrams'></span><span id='topic+tokenize_skip_ngrams'></span>

<h3>Description</h3>

<p>These functions tokenize their inputs into different kinds of n-grams. The
input can be a character vector of any length, or a list of character vectors
where each character vector in the list has a length of 1. See details for an
explanation of what each function does.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tokenize_ngrams(
  x,
  lowercase = TRUE,
  n = 3L,
  n_min = n,
  stopwords = character(),
  ngram_delim = " ",
  simplify = FALSE
)

tokenize_skip_ngrams(
  x,
  lowercase = TRUE,
  n_min = 1,
  n = 3,
  k = 1,
  stopwords = character(),
  simplify = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ngram-tokenizers_+3A_x">x</code></td>
<td>
<p>A character vector or a list of character vectors to be tokenized
into n-grams. If <code>x</code> is a character vector, it can be of any length,
and each element will be tokenized separately. If <code>x</code> is a list of
character vectors, each element of the list should have a length of 1.</p>
</td></tr>
<tr><td><code id="ngram-tokenizers_+3A_lowercase">lowercase</code></td>
<td>
<p>Should the tokens be made lower case?</p>
</td></tr>
<tr><td><code id="ngram-tokenizers_+3A_n">n</code></td>
<td>
<p>The number of words in the n-gram. This must be an integer greater
than or equal to 1.</p>
</td></tr>
<tr><td><code id="ngram-tokenizers_+3A_n_min">n_min</code></td>
<td>
<p>The minimum number of words in the n-gram. This must be an
integer greater than or equal to 1, and less than or equal to <code>n</code>.</p>
</td></tr>
<tr><td><code id="ngram-tokenizers_+3A_stopwords">stopwords</code></td>
<td>
<p>A character vector of stop words to be excluded from the
n-grams.</p>
</td></tr>
<tr><td><code id="ngram-tokenizers_+3A_ngram_delim">ngram_delim</code></td>
<td>
<p>The separator between words in an n-gram.</p>
</td></tr>
<tr><td><code id="ngram-tokenizers_+3A_simplify">simplify</code></td>
<td>
<p><code>FALSE</code> by default so that a consistent value is
returned regardless of length of input. If <code>TRUE</code>, then an input with
a single element will return a character vector of tokens instead of a
list.</p>
</td></tr>
<tr><td><code id="ngram-tokenizers_+3A_k">k</code></td>
<td>
<p>For the skip n-gram tokenizer, the maximum skip distance between
words. The function will compute all skip n-grams between <code>0</code> and
<code>k</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

 <dl>
<dt><code>tokenize_ngrams</code>:</dt><dd><p> Basic shingled n-grams. A
contiguous subsequence of <code>n</code> words. This will compute shingled n-grams
for every value of between <code>n_min</code> (which must be at least 1) and
<code>n</code>. </p>
</dd> <dt><code>tokenize_skip_ngrams</code>:</dt><dd><p>Skip n-grams. A subsequence
of <code>n</code> words which are at most a gap of <code>k</code> words between them. The
skip n-grams will be calculated for all values from <code>0</code> to <code>k</code>. </p>
</dd> </dl>

<p>These functions will strip all punctuation and normalize all whitespace to a
single space character.
</p>


<h3>Value</h3>

<p>A list of character vectors containing the tokens, with one element
in the list for each element that was passed as input. If <code>simplify =
  TRUE</code> and only a single element was passed as input, then the output is a
character vector of tokens.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>song &lt;-  paste0("How many roads must a man walk down\n",
                "Before you call him a man?\n",
                "How many seas must a white dove sail\n",
                "Before she sleeps in the sand?\n",
                "\n",
                "How many times must the cannonballs fly\n",
                "Before they're forever banned?\n",
                "The answer, my friend, is blowin' in the wind.\n",
                "The answer is blowin' in the wind.\n")

tokenize_ngrams(song, n = 4)
tokenize_ngrams(song, n = 4, n_min = 1)
tokenize_skip_ngrams(song, n = 4, k = 2)
</code></pre>

<hr>
<h2 id='tokenize_character_shingles'>Character shingle tokenizers</h2><span id='topic+tokenize_character_shingles'></span>

<h3>Description</h3>

<p>The character shingle tokenizer functions like an n-gram tokenizer, except
the units that are shingled are characters instead of words. Options to the
function let you determine whether non-alphanumeric characters like
punctuation should be retained or discarded.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tokenize_character_shingles(
  x,
  n = 3L,
  n_min = n,
  lowercase = TRUE,
  strip_non_alphanum = TRUE,
  simplify = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tokenize_character_shingles_+3A_x">x</code></td>
<td>
<p>A character vector or a list of character vectors to be tokenized
into character shingles. If <code>x</code> is a character vector, it can be of
any length, and each element will be tokenized separately. If <code>x</code> is a
list of character vectors, each element of the list should have a length of
1.</p>
</td></tr>
<tr><td><code id="tokenize_character_shingles_+3A_n">n</code></td>
<td>
<p>The number of characters in each shingle. This must be an integer
greater than or equal to 1.</p>
</td></tr>
<tr><td><code id="tokenize_character_shingles_+3A_n_min">n_min</code></td>
<td>
<p>This must be an integer greater than or equal to 1, and less
than or equal to <code>n</code>.</p>
</td></tr>
<tr><td><code id="tokenize_character_shingles_+3A_lowercase">lowercase</code></td>
<td>
<p>Should the characters be made lower case?</p>
</td></tr>
<tr><td><code id="tokenize_character_shingles_+3A_strip_non_alphanum">strip_non_alphanum</code></td>
<td>
<p>Should punctuation and white space be stripped?</p>
</td></tr>
<tr><td><code id="tokenize_character_shingles_+3A_simplify">simplify</code></td>
<td>
<p><code>FALSE</code> by default so that a consistent value is
returned regardless of length of input. If <code>TRUE</code>, then an input with
a single element will return a character vector of tokens instead of a
list.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of character vectors containing the tokens, with one element
in the list for each element that was passed as input. If <code>simplify =
  TRUE</code> and only a single element was passed as input, then the output is a
character vector of tokens.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- c("Now is the hour of our discontent")
tokenize_character_shingles(x)
tokenize_character_shingles(x, n = 5)
tokenize_character_shingles(x, n = 5, strip_non_alphanum = FALSE)
tokenize_character_shingles(x, n = 5, n_min = 3, strip_non_alphanum = FALSE)

</code></pre>

<hr>
<h2 id='tokenize_ptb'>Penn Treebank Tokenizer</h2><span id='topic+tokenize_ptb'></span>

<h3>Description</h3>

<p>This function implements the Penn Treebank word tokenizer.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tokenize_ptb(x, lowercase = FALSE, simplify = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tokenize_ptb_+3A_x">x</code></td>
<td>
<p>A character vector or a list of character vectors to be tokenized
into n-grams. If <code>x</code> is a character vector, it can be of any length,
and each element will be tokenized separately. If <code>x</code> is a list of
character vectors, each element of the list should have a length of 1.</p>
</td></tr>
<tr><td><code id="tokenize_ptb_+3A_lowercase">lowercase</code></td>
<td>
<p>Should the tokens be made lower case?</p>
</td></tr>
<tr><td><code id="tokenize_ptb_+3A_simplify">simplify</code></td>
<td>
<p><code>FALSE</code> by default so that a consistent value is
returned regardless of length of input. If <code>TRUE</code>, then an input with
a single element will return a character vector of tokens instead of a
list.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This tokenizer uses regular expressions to tokenize text similar to
the tokenization used in the Penn Treebank. It assumes that text has
already been split into sentences. The tokenizer does the following:
</p>
 <ul>
<li><p>splits common English contractions, e.g. <code style="white-space: pre;">&#8288;don't&#8288;</code> is
tokenized into <code style="white-space: pre;">&#8288;do n't&#8288;</code> and <code style="white-space: pre;">&#8288;they'll&#8288;</code> is tokenized into -&gt;
<code style="white-space: pre;">&#8288;they 'll&#8288;</code>, </p>
</li>
<li><p>handles punctuation characters as separate tokens,
</p>
</li>
<li><p>splits commas and single quotes off from words, when they are
followed by whitespace, </p>
</li>
<li><p>splits off periods that occur at the end of
the sentence. </p>
</li></ul>

<p>This function is a port of the Python NLTK version of the Penn
Treebank Tokenizer.
</p>


<h3>Value</h3>

<p>A list of character vectors containing the tokens, with one element
in the list for each element that was passed as input. If <code>simplify =
  TRUE</code> and only a single element was passed as input, then the output is a
character vector of tokens.
</p>


<h3>References</h3>

<p><a href="https://www.nltk.org/_modules/nltk/tokenize/treebank.html#TreebankWordTokenizer">NLTK
TreebankWordTokenizer</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>song &lt;- list(paste0("How many roads must a man walk down\n",
                    "Before you call him a man?"),
             paste0("How many seas must a white dove sail\n",
                    "Before she sleeps in the sand?\n"),
             paste0("How many times must the cannonballs fly\n",
                    "Before they're forever banned?\n"),
             "The answer, my friend, is blowin' in the wind.",
             "The answer is blowin' in the wind.")
tokenize_ptb(song)
tokenize_ptb(c("Good muffins cost $3.88\nin New York. Please buy me\ntwo of them.",
  "They'll save and invest more.",
  "Hi, I can't say hello."))
</code></pre>

<hr>
<h2 id='tokenize_word_stems'>Word stem tokenizer</h2><span id='topic+tokenize_word_stems'></span>

<h3>Description</h3>

<p>This function turns its input into a character vector of word stems. This is
just a wrapper around the <code><a href="SnowballC.html#topic+wordStem">wordStem</a></code> function from the
SnowballC package which does the heavy lifting, but this function provides a
consistent interface with the rest of the tokenizers in this package. The
input can be a character vector of any length, or a list of character vectors
where each character vector in the list has a length of 1.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tokenize_word_stems(
  x,
  language = "english",
  stopwords = NULL,
  simplify = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tokenize_word_stems_+3A_x">x</code></td>
<td>
<p>A character vector or a list of character vectors to be tokenized.
If <code>x</code> is a character vector, it can be of any length, and each
element will be tokenized separately. If <code>x</code> is a list of character
vectors, where each element of the list should have a length of 1.</p>
</td></tr>
<tr><td><code id="tokenize_word_stems_+3A_language">language</code></td>
<td>
<p>The language to use for word stemming. This must be one of
the languages available in the SnowballC package. A list is provided by
<code><a href="SnowballC.html#topic+getStemLanguages">getStemLanguages</a></code>.</p>
</td></tr>
<tr><td><code id="tokenize_word_stems_+3A_stopwords">stopwords</code></td>
<td>
<p>A character vector of stop words to be excluded</p>
</td></tr>
<tr><td><code id="tokenize_word_stems_+3A_simplify">simplify</code></td>
<td>
<p><code>FALSE</code> by default so that a consistent value is
returned regardless of length of input. If <code>TRUE</code>, then an input with
a single element will return a character vector of tokens instead of a
list.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function will strip all white space and punctuation and make
all word stems lowercase.
</p>


<h3>Value</h3>

<p>A list of character vectors containing the tokens, with one element
in the list for each element that was passed as input. If <code>simplify =
  TRUE</code> and only a single element was passed as input, then the output is a
character vector of tokens.
</p>


<h3>See Also</h3>

<p><code><a href="SnowballC.html#topic+wordStem">wordStem</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>song &lt;-  paste0("How many roads must a man walk down\n",
                "Before you call him a man?\n",
                "How many seas must a white dove sail\n",
                "Before she sleeps in the sand?\n",
                "\n",
                "How many times must the cannonballs fly\n",
                "Before they're forever banned?\n",
                "The answer, my friend, is blowin' in the wind.\n",
                "The answer is blowin' in the wind.\n")

tokenize_word_stems(song)
</code></pre>

<hr>
<h2 id='tokenizers'>Tokenizers</h2><span id='topic+tokenizers'></span>

<h3>Description</h3>

<p>A collection of functions with a consistent interface to convert natural
language text into tokens.
</p>


<h3>Details</h3>

<p>The tokenizers in this package have a consistent interface. They all take
either a character vector of any length, or a list where each element is a
character vector of length one. The idea is that each element comprises a
text. Then each function returns a list with the same length as the input
vector, where each element in the list are the tokens generated by the
function. If the input character vector or list is named, then the names are
preserved.
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
