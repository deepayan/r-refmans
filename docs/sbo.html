<!DOCTYPE html><html><head><title>Help for package sbo</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {sbo}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#as_sbo_dictionary'><p>Coerce to dictionary</p></a></li>
<li><a href='#babble'><p>Babble!</p></a></li>
<li><a href='#eval_sbo_predictor'><p>Evaluate Stupid Back-off next-word predictions</p></a></li>
<li><a href='#kgram_freqs'><p>k-gram frequency tables</p></a></li>
<li><a href='#plot.word_coverage'><p>Plot method for word_coverage objects</p></a></li>
<li><a href='#predict.sbo_kgram_freqs'><p>Predict method for k-gram frequency tables</p></a></li>
<li><a href='#predict.sbo_predictor'><p>Predict method for Stupid Back-off text predictor</p></a></li>
<li><a href='#preprocess'><p>Preprocess text corpus</p></a></li>
<li><a href='#prune'><p>Prune k-gram objects</p></a></li>
<li><a href='#sbo_dictionary'><p>Dictionaries</p></a></li>
<li><a href='#sbo_predictions'><p>Stupid Back-off text predictions</p></a></li>
<li><a href='#sbo-package'><p>sbo: Text Prediction via Stupid Back-Off N-Gram Models</p></a></li>
<li><a href='#tokenize_sentences'><p>Sentence tokenizer</p></a></li>
<li><a href='#twitter_dict'><p>Top 1000 dictionary from Twitter training set</p></a></li>
<li><a href='#twitter_freqs'><p>k-gram frequencies from Twitter training set</p></a></li>
<li><a href='#twitter_predtable'><p>Next-word prediction tables from 3-gram model trained on Twitter training</p>
set</a></li>
<li><a href='#twitter_test'><p>Twitter test set</p></a></li>
<li><a href='#twitter_train'><p>Twitter training set</p></a></li>
<li><a href='#word_coverage'><p>Word coverage fraction</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Text Prediction via Stupid Back-Off N-Gram Models</td>
</tr>
<tr>
<td>Version:</td>
<td>0.5.0</td>
</tr>
<tr>
<td>Author:</td>
<td>Valerio Gherardi</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Valerio Gherardi &lt;vgherard@sissa.it&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Utilities for training and evaluating text predictors based on Stupid Back-Off N-gram models (Brants et al., 2007, <a href="https://www.aclweb.org/anthology/D07-1090/">https://www.aclweb.org/anthology/D07-1090/</a>).</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.1.9000</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, testthat</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp, rlang, tidyr, dplyr, utils, stats, graphics</td>
</tr>
<tr>
<td>Suggests:</td>
<td>ggplot2, knitr, rmarkdown, cli, testthat, covr</td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>C++11</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://vgherard.github.io/sbo/">https://vgherard.github.io/sbo/</a>, <a href="https://github.com/vgherard/sbo">https://github.com/vgherard/sbo</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/vgherard/sbo/issues">https://github.com/vgherard/sbo/issues</a></td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2020-12-05 18:45:16 UTC; vale</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2020-12-05 19:50:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='as_sbo_dictionary'>Coerce to dictionary</h2><span id='topic+as_sbo_dictionary'></span><span id='topic+as_sbo_dictionary.character'></span>

<h3>Description</h3>

<p>Coerce objects to <code>sbo_dictionary</code> class.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as_sbo_dictionary(x, ...)

## S3 method for class 'character'
as_sbo_dictionary(x, .preprocess = identity, EOS = "", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="as_sbo_dictionary_+3A_x">x</code></td>
<td>
<p>object to be coerced.</p>
</td></tr>
<tr><td><code id="as_sbo_dictionary_+3A_...">...</code></td>
<td>
<p>further arguments passed to or from other methods.</p>
</td></tr>
<tr><td><code id="as_sbo_dictionary_+3A_.preprocess">.preprocess</code></td>
<td>
<p>a function for corpus preprocessing.</p>
</td></tr>
<tr><td><code id="as_sbo_dictionary_+3A_eos">EOS</code></td>
<td>
<p>a length one character vector listing all (single character)
end-of-sentence tokens.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is an S3 generic for coercing existing objects to
<code>sbo_dictionary</code> class objects. Currently, only a method for character
vectors is implemented, and this will be described below.
</p>
<p><em><strong>Character vector input</strong></em>:
Calling <code>as_sbo_dictionary(x)</code> simply decorates the character
vector <code>x</code> with the class <code>sbo_dictionary</code> attribute,
and with customizable <code>.preprocess</code> and <code>EOS</code> attributes.
</p>


<h3>Value</h3>

<p>A <code>sbo_dictionary</code> object.
</p>


<h3>Author(s)</h3>

<p>Valerio Gherardi
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dict &lt;- as_sbo_dictionary(c("a","b","c"), .preprocess = tolower, EOS = ".")
</code></pre>

<hr>
<h2 id='babble'>Babble!</h2><span id='topic+babble'></span>

<h3>Description</h3>

<p>Generate random text based on Stupid Back-off language model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>babble(model, input = NA, n_max = 100L, L = attr(model, "L"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="babble_+3A_model">model</code></td>
<td>
<p>a <code>sbo_predictor</code> object.</p>
</td></tr>
<tr><td><code id="babble_+3A_input">input</code></td>
<td>
<p>a length one character vector. Starting point for babbling!
If <code>NA</code>, as by default, a random word is sampled from the model's
dictionary.</p>
</td></tr>
<tr><td><code id="babble_+3A_n_max">n_max</code></td>
<td>
<p>a length one integer. Maximum number of words to generate.</p>
</td></tr>
<tr><td><code id="babble_+3A_l">L</code></td>
<td>
<p>a length one integer. Number of next-word suggestions from
which to sample (see details).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function generates random text from a Stupid Back-off language
model.
<code>babble</code> randomly samples one of the top L next word
predictions. Text generation stops when an End-Of-Sentence token is
encountered, or when the number of generated words exceeds n_max.
</p>


<h3>Value</h3>

<p>A character vector of length one.
</p>


<h3>Author(s)</h3>

<p>Valerio Gherardi
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Babble!
p &lt;- sbo_predictor(twitter_predtable)
set.seed(840) # Set seed for reproducibility
babble(p)
</code></pre>

<hr>
<h2 id='eval_sbo_predictor'>Evaluate Stupid Back-off next-word predictions</h2><span id='topic+eval_sbo_predictor'></span>

<h3>Description</h3>

<p>Evaluate next-word predictions based on Stupid Back-off N-gram
model on a test corpus.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>eval_sbo_predictor(model, test, L = attr(model, "L"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="eval_sbo_predictor_+3A_model">model</code></td>
<td>
<p>a <code>sbo_predictor</code> object.</p>
</td></tr>
<tr><td><code id="eval_sbo_predictor_+3A_test">test</code></td>
<td>
<p>a character vector. Perform a single prediction on each entry of
this vector (see details).</p>
</td></tr>
<tr><td><code id="eval_sbo_predictor_+3A_l">L</code></td>
<td>
<p>Maximum number of predictions for each input sentence
(maximum allowed is <code>attr(model, "L")</code>)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function allows to obtain information on the quality of
Stupid Back-off model predictions, such as next-word prediction accuracy,
or the word-rank distribution of correct prediction, by direct test against
a test set corpus. For a reasonable estimate of prediction accuracy, the
different entries of the <code>test</code> vector should be uncorrelated
documents (e.g. separate tweets, as in the <code><a href="#topic+twitter_test">twitter_test</a></code>
example dataset).
</p>
<p>More in detail, <code>eval_sbo_predictor</code> performs the following operations:
</p>

<ol>
<li><p> Sample a single sentence from each entry of the character vector
<code>test</code>.
</p>
</li>
<li><p> Sample a single $N$-gram from each sentence obtained in the previous step.
</p>
</li>
<li><p> Predict next words from the $(N-1)$-gram prefix.
</p>
</li>
<li><p> Return all predictions, together with the true word completions.
</p>
</li></ol>



<h3>Value</h3>

<p>A tibble, containing the input $(N-1)$-grams, the true completions,
the predicted completions and a column indicating whether one of the
predictions were correct or not.
</p>


<h3>Author(s)</h3>

<p>Valerio Gherardi
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Evaluating next-word predictions from a Stupid Back-off N-gram model
if (suppressMessages(require(dplyr) &amp;&amp; require(ggplot2))) {
        p &lt;- sbo_predictor(twitter_predtable)
        set.seed(840) # Set seed for reproducibility
        test &lt;- sample(twitter_test, 500)
        eval &lt;- eval_sbo_predictor(p, test)
        
        ## Compute three-word accuracies
        eval %&gt;% summarise(accuracy = sum(correct)/n()) # Overall accuracy
        eval %&gt;% # Accuracy for in-sentence predictions
                filter(true != "&lt;EOS&gt;") %&gt;%
                summarise(accuracy = sum(correct) / n())
        
        ## Make histogram of word-rank distribution for correct predictions
        dict &lt;- attr(twitter_predtable, "dict")
        eval %&gt;%
                filter(correct, true != "&lt;EOS&gt;") %&gt;%
                transmute(rank = match(true, table = dict)) %&gt;%
                ggplot(aes(x = rank)) + geom_histogram(binwidth = 30)
}

</code></pre>

<hr>
<h2 id='kgram_freqs'>k-gram frequency tables</h2><span id='topic+kgram_freqs'></span><span id='topic+sbo_kgram_freqs'></span><span id='topic+kgram_freqs_fast'></span><span id='topic+sbo_kgram_freqs_fast'></span>

<h3>Description</h3>

<p>Get k-gram frequency tables from a training corpus.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kgram_freqs(corpus, N, dict, .preprocess = identity, EOS = "")

sbo_kgram_freqs(corpus, N, dict, .preprocess = identity, EOS = "")

kgram_freqs_fast(corpus, N, dict, erase = "", lower_case = FALSE, EOS = "")

sbo_kgram_freqs_fast(corpus, N, dict, erase = "", lower_case = FALSE, EOS = "")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kgram_freqs_+3A_corpus">corpus</code></td>
<td>
<p>a character vector. The training corpus from which to extract
k-gram frequencies.</p>
</td></tr>
<tr><td><code id="kgram_freqs_+3A_n">N</code></td>
<td>
<p>a length one integer. The maximum order of k-grams
for which frequencies are to be extracted.</p>
</td></tr>
<tr><td><code id="kgram_freqs_+3A_dict">dict</code></td>
<td>
<p>either a <code>sbo_dictionary</code> object, a character vector,
or a formula (see details). The language model dictionary.</p>
</td></tr>
<tr><td><code id="kgram_freqs_+3A_.preprocess">.preprocess</code></td>
<td>
<p>a function to apply before k-gram
tokenization.</p>
</td></tr>
<tr><td><code id="kgram_freqs_+3A_eos">EOS</code></td>
<td>
<p>a length one character vector listing all (single character)
end-of-sentence tokens.</p>
</td></tr>
<tr><td><code id="kgram_freqs_+3A_erase">erase</code></td>
<td>
<p>a length one character vector. Regular expression matching
parts  of text to be erased from input. The default removes anything not
alphanumeric, white space, apostrophes or punctuation characters
(i.e. &quot;.?!:;&quot;).</p>
</td></tr>
<tr><td><code id="kgram_freqs_+3A_lower_case">lower_case</code></td>
<td>
<p>a length one logical vector. If TRUE, puts everything to
lower case.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>These functions extract all k-gram frequency tables from a text
corpus up to a specified k-gram order N. These are
the building blocks to train any N-gram model. The functions
<code>sbo_kgram_freqs()</code> and <code>sbo_kgram_freqs_fast()</code> are aliases for
<code>kgram_freqs()</code> and <code>kgram_freqs_fast()</code>, respectively.
</p>
<p>The optimized version <code>kgram_freqs_fast(erase = x, lower_case = y)</code>
is equivalent to
<code>kgram_freqs(.preprocess = preprocess(erase = x, lower_case = y))</code>,
but more efficient (both from the speed and memory point of view).
</p>
<p>Both <code>kgram_freqs()</code> and <code>kgram_freqs_fast()</code> employ a fixed
(user specified) dictionary: any out-of-vocabulary word gets effectively
replaced by an &quot;unknown word&quot; token. This is specified through the argument
<code>dict</code>, which accepts three types of arguments: a <code>sbo_dictionary</code>
object, a character vector (containing the words of the dictionary) or a
formula. In this last case, valid formulas can be either <code>max_size ~ V</code>
or <code>target ~ f</code>, where <code>V</code> and <code>f</code> represent a dictionary size
and a corpus word coverage fraction (of <code>corpus</code>), respectively. This
usage of the <code>dict</code> argument allows to build the model dictionary
'on the fly'.
</p>
<p>The return value is a &quot;<code>sbo_kgram_freqs</code>&quot; object, i.e. a list of N tibbles,
storing frequency counts for each k-gram observed in the training corpus, for
k = 1, 2, ..., N. In these tables, words are represented by
integer numbers corresponding to their position in the
reference dictionary. The special codes <code>0</code>,
<code>length(dictionary)+1</code> and <code>length(dictionary)+2</code>
correspond to the &quot;Begin-Of-Sentence&quot;, &quot;End-Of-Sentence&quot;
and &quot;Unknown word&quot; tokens, respectively.
</p>
<p>Furthermore, the returned objected has the following attributes:
</p>

<ul>
<li> <p><code>N</code>: The highest order of N-grams.
</p>
</li>
<li> <p><code>dict</code>: The reference dictionary, sorted by word frequency.
</p>
</li>
<li> <p><code>.preprocess</code>: The function used for text preprocessing.
</p>
</li>
<li> <p><code>EOS</code>: A length one character vector listing all (single character)
end-of-sentence tokens employed in k-gram tokenization.
</p>
</li></ul>

<p>The <code>.preprocess</code> argument of <code>kgram_freqs</code> allows the user to
apply a custom transformation to the training corpus, before kgram
tokenization takes place.
</p>
<p>The algorithm for k-gram tokenization considers anything separated by
(any number of) white spaces (i.e. &quot; &quot;) as a single word. Sentences are split
according to end-of-sentence (single character) tokens, as specified
by the <code>EOS</code> argument. Additionally text belonging to different entries of
the preprocessed input vector which are understood to belong to different
sentences.
</p>
<p><em>Nota Bene</em>: It is useful to keep in mind that the function
passed through the  <code>.preprocess</code> argument also captures its enclosing
environment, which is by default the environment in which the former
was defined.
If, for instance, <code>.preprocess</code> was defined in the global environment,
and the latter binds heavy objects, the resulting <code>sbo_kgram_freqs</code> will
contain bindings to the same objects. If <code>sbo_kgram_freqs</code> is stored out of
memory and recalled in another R session, these objects will also be reloaded
in memory.
For this reason, for non interactive use, it is advisable to avoid using
preprocessing functions defined in the global environment
(for instance, <code>base::identity</code> is preferred to <code>function(x) x</code>).
</p>


<h3>Value</h3>

<p>A <code>sbo_kgram_freqs</code> object, containing the k-gram
frequency tables for k = 1, 2, ..., N.
</p>


<h3>Author(s)</h3>

<p>Valerio Gherardi
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Obtain k-gram frequency table from corpus
## Get k-gram frequencies, for k &lt;= N = 3.
## The dictionary is built on the fly, using the most frequent 1000 words.
freqs &lt;- kgram_freqs(corpus = twitter_train, N = 3, dict = max_size ~ 1000,
                     .preprocess = preprocess, EOS = ".?!:;")
freqs
## Using a predefined dictionary
freqs &lt;- kgram_freqs_fast(twitter_train, N = 3, dict = twitter_dict,
                          erase = "[^.?!:;'\\w\\s]", lower_case = TRUE,
                          EOS = ".?!:;")
freqs
## 2-grams, no preprocessing, use a dictionary covering 50% of corpus
freqs &lt;- kgram_freqs(corpus = twitter_train, N = 2, dict = target ~ 0.5,
                     EOS = ".?!:;")
freqs


# Obtain k-gram frequency table from corpus
freqs &lt;- kgram_freqs_fast(twitter_train, N = 3, dict = twitter_dict)
## Print result
freqs

</code></pre>

<hr>
<h2 id='plot.word_coverage'>Plot method for word_coverage objects</h2><span id='topic+plot.word_coverage'></span>

<h3>Description</h3>

<p>Plot cumulative corpus coverage fraction of a dictionary.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'word_coverage'
plot(
  x,
  include_EOS = FALSE,
  show_limit = TRUE,
  type = "l",
  xlim = c(0, length(x)),
  ylim = c(0, 1),
  xticks = seq(from = 0, to = length(x), by = length(x)/5),
  yticks = seq(from = 0, to = 1, by = 0.25),
  xlab = "Rank",
  ylab = "Covered fraction",
  title = "Cumulative corpus coverage fraction of dictionary",
  subtitle = "_default_",
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.word_coverage_+3A_x">x</code></td>
<td>
<p>a <code>word_coverage</code> object.</p>
</td></tr>
<tr><td><code id="plot.word_coverage_+3A_include_eos">include_EOS</code></td>
<td>
<p>length one logical. Should End-Of-Sentence tokens be
considered in the computation of coverage fraction?</p>
</td></tr>
<tr><td><code id="plot.word_coverage_+3A_show_limit">show_limit</code></td>
<td>
<p>length one logical. If <code>TRUE</code>, plots an horizontal
line corresponding to the total coverage fraction.</p>
</td></tr>
<tr><td><code id="plot.word_coverage_+3A_type">type</code></td>
<td>
<p>what type of plot should be drawn, as detailed in <code>?plot</code>.</p>
</td></tr>
<tr><td><code id="plot.word_coverage_+3A_xlim">xlim</code></td>
<td>
<p>length two numeric. Extremes of the x-range.</p>
</td></tr>
<tr><td><code id="plot.word_coverage_+3A_ylim">ylim</code></td>
<td>
<p>length two numeric. Extremes of the y-range.</p>
</td></tr>
<tr><td><code id="plot.word_coverage_+3A_xticks">xticks</code></td>
<td>
<p>numeric vector. position of the x-axis ticks.</p>
</td></tr>
<tr><td><code id="plot.word_coverage_+3A_yticks">yticks</code></td>
<td>
<p>numeric vector. position of the y-axis ticks.</p>
</td></tr>
<tr><td><code id="plot.word_coverage_+3A_xlab">xlab</code></td>
<td>
<p>length one character. The x-axis label.</p>
</td></tr>
<tr><td><code id="plot.word_coverage_+3A_ylab">ylab</code></td>
<td>
<p>length one character. The y-axis label.</p>
</td></tr>
<tr><td><code id="plot.word_coverage_+3A_title">title</code></td>
<td>
<p>length one character. Plot title.</p>
</td></tr>
<tr><td><code id="plot.word_coverage_+3A_subtitle">subtitle</code></td>
<td>
<p>length one character. Plot subtitle; if &quot;<em>default</em>&quot;, prints
dictionary length and total covered fraction.</p>
</td></tr>
<tr><td><code id="plot.word_coverage_+3A_...">...</code></td>
<td>
<p>further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function generates nice plots of cumulative corpus coverage
fractions. The <code>x</code> coordinate in the resulting plot is the word rank in the
underlying dictionary; the <code>y</code> coordinate at
<code>x</code> is the cumulative coverage fraction for <code>rank &lt;= x</code>.
</p>


<h3>Author(s)</h3>

<p>Valerio Gherardi
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
c &lt;- word_coverage(twitter_dict, twitter_test)
plot(c)

</code></pre>

<hr>
<h2 id='predict.sbo_kgram_freqs'>Predict method for k-gram frequency tables</h2><span id='topic+predict.sbo_kgram_freqs'></span>

<h3>Description</h3>

<p>Predictive text based on Stupid Back-off N-gram model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'sbo_kgram_freqs'
predict(object, input, lambda = 0.4, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.sbo_kgram_freqs_+3A_object">object</code></td>
<td>
<p>a <code>sbo_kgram_freqs</code> object.</p>
</td></tr>
<tr><td><code id="predict.sbo_kgram_freqs_+3A_input">input</code></td>
<td>
<p>a length one character vector, containing the input for
next-word prediction.</p>
</td></tr>
<tr><td><code id="predict.sbo_kgram_freqs_+3A_lambda">lambda</code></td>
<td>
<p>a numeric vector of length one. The back-off penalization
in Stupid Back-off algorithm.</p>
</td></tr>
<tr><td><code id="predict.sbo_kgram_freqs_+3A_...">...</code></td>
<td>
<p>further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A tibble containing the next-word probabilities for all words
in the dictionary.
</p>


<h3>Author(s)</h3>

<p>Valerio Gherardi
</p>


<h3>Examples</h3>

<pre><code class='language-R'>predict(twitter_freqs, "i love")
</code></pre>

<hr>
<h2 id='predict.sbo_predictor'>Predict method for Stupid Back-off text predictor</h2><span id='topic+predict.sbo_predictor'></span>

<h3>Description</h3>

<p>Predictive text based on Stupid Back-off N-gram model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'sbo_predictor'
predict(object, input, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.sbo_predictor_+3A_object">object</code></td>
<td>
<p>a <code>sbo_predictor</code> object.</p>
</td></tr>
<tr><td><code id="predict.sbo_predictor_+3A_input">input</code></td>
<td>
<p>a character vector, containing the input for next-word prediction.</p>
</td></tr>
<tr><td><code id="predict.sbo_predictor_+3A_...">...</code></td>
<td>
<p>further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This method returns the top <code>L</code> next-word predictions from a
text predictor trained with Stupid Back-Off.
</p>
<p>Trying to predict from a <code>sbo_predtable</code> results into an error. Instead,
one should load a <code>sbo_predictor</code> object and use this one to predict(),
as shown in the example below.
</p>


<h3>Value</h3>

<p>A character vector if <code>length(input) == 1</code>, otherwise a
character matrix.
</p>


<h3>Author(s)</h3>

<p>Valerio Gherardi
</p>


<h3>Examples</h3>

<pre><code class='language-R'>p &lt;- sbo_predictor(twitter_predtable)
x &lt;- predict(p, "i love")
x
x &lt;- predict(p, "you love")
x
#N.B. the top predictions here are x[1], followed by x[2] and x[3].
predict(p, c("i love", "you love")) # Behaviour with length()&gt;1 input.
</code></pre>

<hr>
<h2 id='preprocess'>Preprocess text corpus</h2><span id='topic+preprocess'></span>

<h3>Description</h3>

<p>A simple text preprocessing utility.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>preprocess(input, erase = "[^.?!:;'\\w\\s]", lower_case = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="preprocess_+3A_input">input</code></td>
<td>
<p>a character vector.</p>
</td></tr>
<tr><td><code id="preprocess_+3A_erase">erase</code></td>
<td>
<p>a length one character vector. Regular expression matching parts of
text to be erased from input. The default removes anything not alphanumeric,
white space, apostrophes or punctuation characters (i.e. &quot;.?!:;&quot;).</p>
</td></tr>
<tr><td><code id="preprocess_+3A_lower_case">lower_case</code></td>
<td>
<p>a length one logical vector. If TRUE, puts everything to lower
case.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a character vector containing the processed output.
</p>


<h3>Author(s)</h3>

<p>Valerio Gherardi
</p>


<h3>Examples</h3>

<pre><code class='language-R'>preprocess("Hi @ there! I'm using `sbo`.")
</code></pre>

<hr>
<h2 id='prune'>Prune k-gram objects</h2><span id='topic+prune'></span><span id='topic+prune.sbo_kgram_freqs'></span><span id='topic+prune.sbo_predtable'></span>

<h3>Description</h3>

<p>Prune <code>M</code>-gram frequency tables or Stupid Back-Off prediction tables for
an <code>M</code>-gram model to a smaller order <code>N</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prune(object, N, ...)

## S3 method for class 'sbo_kgram_freqs'
prune(object, N, ...)

## S3 method for class 'sbo_predtable'
prune(object, N, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="prune_+3A_object">object</code></td>
<td>
<p>A <code><a href="#topic+kgram_freqs">kgram_freqs</a></code> or a
<code><a href="#topic+sbo_predtable">sbo_predtable</a></code> class object.</p>
</td></tr>
<tr><td><code id="prune_+3A_n">N</code></td>
<td>
<p>a length one positive integer. N-gram order of the new object.</p>
</td></tr>
<tr><td><code id="prune_+3A_...">...</code></td>
<td>
<p>further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This generic function provides a helper to prune M-gram frequency
tables or M-gram models, represented by <code>sbo_kgram_freqs</code> and
<code>sbo_predtable</code> objects respectively, to objects of a smaller N-gram
order, N &lt; M. For k-gram frequency objects, frequency tables for
k &gt; N are simply dropped. For <code>sbo_predtable</code>'s, the predictions coming
from the nested N-gram model are instead retained. In both cases, all other
other attributes besides k-gram order (such as the corpus preprocessing
function, or the <code>lambda</code> penalty in Stupid Back-Off training) are left
unchanged.
</p>


<h3>Value</h3>

<p>an object of the same class of the input <code>object</code>.
</p>


<h3>Author(s)</h3>

<p>Valerio Gherardi
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Drop k-gram frequencies for k &gt; 2 
freqs &lt;- twitter_freqs
summary(freqs)
freqs &lt;- prune(freqs, N = 2)
summary(freqs)
# Extract a 2-gram model from a larger 3-gram model 
pt &lt;- twitter_predtable
summary(pt)
pt &lt;- prune(pt, N = 2)
summary(pt)
</code></pre>

<hr>
<h2 id='sbo_dictionary'>Dictionaries</h2><span id='topic+sbo_dictionary'></span><span id='topic+dictionary'></span>

<h3>Description</h3>

<p>Build dictionary from training corpus.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sbo_dictionary(
  corpus,
  max_size = Inf,
  target = 1,
  .preprocess = identity,
  EOS = ""
)

dictionary(
  corpus,
  max_size = Inf,
  target = 1,
  .preprocess = identity,
  EOS = ""
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sbo_dictionary_+3A_corpus">corpus</code></td>
<td>
<p>a character vector. The training corpus from which to extract
the dictionary.</p>
</td></tr>
<tr><td><code id="sbo_dictionary_+3A_max_size">max_size</code></td>
<td>
<p>a length one numeric. If less than <code>Inf</code>, only the most
frequent <code>max_size</code> words are retained in the dictionary.</p>
</td></tr>
<tr><td><code id="sbo_dictionary_+3A_target">target</code></td>
<td>
<p>a length one numeric between <code>0</code> and <code>1</code>.
If less than one, retains
only as many words as needed to cover a fraction <code>target</code> of the
training corpus.</p>
</td></tr>
<tr><td><code id="sbo_dictionary_+3A_.preprocess">.preprocess</code></td>
<td>
<p>a function for corpus preprocessing. Takes a character
vector as input and returns a character vector.</p>
</td></tr>
<tr><td><code id="sbo_dictionary_+3A_eos">EOS</code></td>
<td>
<p>a length one character vector listing all (single character)
end-of-sentence tokens.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function <code>dictionary()</code> is an alias for
<code>sbo_dictionary()</code>.
</p>
<p>This function builds a dictionary using the most frequent words in a
training corpus. Two pruning criterions can be applied:
</p>

<ol>
<li><p> Dictionary size, as implemented by the <code>max_size</code> argument.
</p>
</li>
<li><p> Target coverage fraction, as implemented by the <code>target</code> argument.
</p>
</li></ol>

<p>If both these criterions imply non-trivial cuts, the most restrictive
critierion applies.
</p>
<p>The <code>.preprocess</code> argument allows the user to apply a custom
transformation to the training corpus, before word tokenization. The
<code>EOS</code> argument allows to specify a set of characters to be identified
as End-Of-Sentence tokens (and thus not part of words).
</p>
<p>The returned object is a <code>sbo_dictionary</code> object, which is a
character vector containing words sorted by decreasing corpus frequency.
Furthermore, the object stores as attributes the original values of
<code>.preprocess</code> and <code>EOS</code> (i.e. the function used in corpus
preprocessing and the End-Of-Sentence characters for sentence tokenization).
</p>


<h3>Value</h3>

<p>A <code>sbo_dictionary</code> object.
</p>


<h3>Author(s)</h3>

<p>Valerio Gherardi
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Extract dictionary from `twitter_train` corpus (all words)
dict &lt;- sbo_dictionary(twitter_train)
# Extract dictionary from `twitter_train` corpus (top 1000 words)
dict &lt;- sbo_dictionary(twitter_train, max_size = 1000)
# Extract dictionary from `twitter_train` corpus (coverage target = 50%)
dict &lt;- sbo_dictionary(twitter_train, target = 0.5)

</code></pre>

<hr>
<h2 id='sbo_predictions'>Stupid Back-off text predictions</h2><span id='topic+sbo_predictions'></span><span id='topic+sbo_predictor'></span><span id='topic+predictor'></span><span id='topic+sbo_predictor.character'></span><span id='topic+sbo_predictor.sbo_kgram_freqs'></span><span id='topic+sbo_predictor.sbo_predtable'></span><span id='topic+sbo_predtable'></span><span id='topic+predtable'></span><span id='topic+sbo_predtable.character'></span><span id='topic+sbo_predtable.sbo_kgram_freqs'></span>

<h3>Description</h3>

<p>Train a text predictor via Stupid Back-off
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sbo_predictor(object, ...)

predictor(object, ...)

## S3 method for class 'character'
sbo_predictor(
  object,
  N,
  dict,
  .preprocess = identity,
  EOS = "",
  lambda = 0.4,
  L = 3L,
  filtered = "&lt;UNK&gt;",
  ...
)

## S3 method for class 'sbo_kgram_freqs'
sbo_predictor(object, lambda = 0.4, L = 3L, filtered = "&lt;UNK&gt;", ...)

## S3 method for class 'sbo_predtable'
sbo_predictor(object, ...)

sbo_predtable(object, lambda = 0.4, L = 3L, filtered = "&lt;UNK&gt;", ...)

predtable(object, lambda = 0.4, L = 3L, filtered = "&lt;UNK&gt;", ...)

## S3 method for class 'character'
sbo_predtable(
  object,
  lambda = 0.4,
  L = 3L,
  filtered = "&lt;UNK&gt;",
  N,
  dict,
  .preprocess = identity,
  EOS = "",
  ...
)

## S3 method for class 'sbo_kgram_freqs'
sbo_predtable(object, lambda = 0.4, L = 3L, filtered = "&lt;UNK&gt;", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sbo_predictions_+3A_object">object</code></td>
<td>
<p>either a character vector or an object inheriting from classes
<code>sbo_kgram_freqs</code> or <code>sbo_predtable</code>. Defines the method to use for
training.</p>
</td></tr>
<tr><td><code id="sbo_predictions_+3A_...">...</code></td>
<td>
<p>further arguments passed to or from other methods.</p>
</td></tr>
<tr><td><code id="sbo_predictions_+3A_n">N</code></td>
<td>
<p>a length one integer. Order 'N' of the N-gram model.</p>
</td></tr>
<tr><td><code id="sbo_predictions_+3A_dict">dict</code></td>
<td>
<p>a <code>sbo_dictionary</code>, a character vector or a formula. For
more details see <code><a href="#topic+kgram_freqs">kgram_freqs</a></code>.</p>
</td></tr>
<tr><td><code id="sbo_predictions_+3A_.preprocess">.preprocess</code></td>
<td>
<p>a function for corpus preprocessing. For
more details see <code><a href="#topic+kgram_freqs">kgram_freqs</a></code>.</p>
</td></tr>
<tr><td><code id="sbo_predictions_+3A_eos">EOS</code></td>
<td>
<p>a length one character vector. String listing End-Of-Sentence
characters. For more details see <code><a href="#topic+kgram_freqs">kgram_freqs</a></code>.</p>
</td></tr>
<tr><td><code id="sbo_predictions_+3A_lambda">lambda</code></td>
<td>
<p>a length one numeric. Penalization in the
Stupid Back-off algorithm.</p>
</td></tr>
<tr><td><code id="sbo_predictions_+3A_l">L</code></td>
<td>
<p>a length one integer. Maximum number of next-word predictions
for a given input (top scoring predictions are retained).</p>
</td></tr>
<tr><td><code id="sbo_predictions_+3A_filtered">filtered</code></td>
<td>
<p>a character vector. Words to exclude from next-word
predictions. The strings '&lt;UNK&gt;' and '&lt;EOS&gt;' are reserved keywords
referring to the Unknown-Word and End-Of-Sentence tokens, respectively.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>These functions are generics used to train a text predictor
with Stupid Back-Off. The functions <code>predictor()</code> and
<code>predtable()</code> are aliases for <code>sbo_predictor()</code> and
<code>sbo_predtable()</code>, respectively.
</p>
<p>The <code>sbo_predictor</code> data structure carries
all information
required for prediction in a compact and efficient (upon retrieval) way,
by directly storing the top <code>L</code> next-word predictions for each
k-gram prefix observed in the training corpus.
</p>
<p>The <code>sbo_predictor</code> objects are for interactive use. If the training
process is computationally heavy, one can store a &quot;raw&quot; version of the
text predictor in a <code>sbo_predtable</code> class object, which can be safely
saved out of memory (with e.g. <code>save()</code>).
The resulting object can be restored
in another R session, and the corresponding <code>sbo_predictor</code> object
can be loaded rapidly using again the generic constructor
<code>sbo_predictor()</code> (see example below).
</p>
<p>The returned objects are a <code>sbo_predictor</code> and a <code>sbo_predtable</code>
objects.
The latter contains Stupid Back-Off prediction tables, storing next-word
prediction for each k-gram prefix observed in the text, whereas the former
is an external pointer to an equivalent (but processed) C++ structure.
</p>
<p>Both objects have the following attributes:
</p>

<ul>
<li> <p><code>N</code>: The order of the underlying N-gram model, &quot;<code>N</code>&quot;.
</p>
</li>
<li> <p><code>dict</code>: The model dictionary.
</p>
</li>
<li> <p><code>lambda</code>: The penalization used in the Stupid Back-Off algorithm.
</p>
</li>
<li> <p><code>L</code>: The maximum number of next-word predictions for a given text
input.
</p>
</li>
<li> <p><code>.preprocess</code>: The function used for text preprocessing.
</p>
</li>
<li> <p><code>EOS</code>: A length one character vector listing all (single character)
end-of-sentence tokens.
</p>
</li></ul>



<h3>Value</h3>

<p>A <code>sbo_predictor</code> object for <code>sbo_predictor()</code>, a
<code>sbo_predtable</code> object for <code>sbo_predtable()</code>.
</p>


<h3>Author(s)</h3>

<p>Valerio Gherardi
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.sbo_predictor">predict.sbo_predictor</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Train a text predictor directly from corpus
p &lt;- sbo_predictor(twitter_train, N = 3, dict = max_size ~ 1000,
                   .preprocess = preprocess, EOS = ".?!:;")


# Train a text predictor from previously computed 'kgram_freqs' object
p &lt;- sbo_predictor(twitter_freqs)


# Load a text predictor from a Stupid Back-Off prediction table
p &lt;- sbo_predictor(twitter_predtable)


# Predict from Stupid Back-Off text predictor
p &lt;- sbo_predictor(twitter_predtable)
predict(p, "i love")


# Build Stupid Back-Off prediction tables directly from corpus
t &lt;- sbo_predtable(twitter_train, N = 3, dict = max_size ~ 1000, 
                   .preprocess = preprocess, EOS = ".?!:;")


# Build Stupid Back-Off prediction tables from kgram_freqs object
t &lt;- sbo_predtable(twitter_freqs)

## Not run: 
# Save and reload a 'sbo_predtable' object with base::save()
save(t)
load("t.rda")

## End(Not run)
</code></pre>

<hr>
<h2 id='sbo-package'>sbo: Text Prediction via Stupid Back-Off N-Gram Models</h2><span id='topic+sbo'></span><span id='topic+sbo-package'></span>

<h3>Description</h3>

<p>Utilities for building and evaluating text prediction functions based on Stupid Back-Off N-gram models.
</p>


<h3>Author(s)</h3>

<p>Valerio Gherardi
</p>


<h3>References</h3>

<p>The Stupid Back-Off smoothing method for N-gram models was introduced by
Brants et al., <a href="https://www.aclweb.org/anthology/D07-1090/">https://www.aclweb.org/anthology/D07-1090/</a> (2007).
</p>


<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://vgherard.github.io/sbo/">https://vgherard.github.io/sbo/</a>
</p>
</li>
<li> <p><a href="https://github.com/vgherard/sbo">https://github.com/vgherard/sbo</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/vgherard/sbo/issues">https://github.com/vgherard/sbo/issues</a>
</p>
</li></ul>


<hr>
<h2 id='tokenize_sentences'>Sentence tokenizer</h2><span id='topic+tokenize_sentences'></span>

<h3>Description</h3>

<p>Get sentence tokens from text
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tokenize_sentences(input, EOS = ".?!:;")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tokenize_sentences_+3A_input">input</code></td>
<td>
<p>a character vector.</p>
</td></tr>
<tr><td><code id="tokenize_sentences_+3A_eos">EOS</code></td>
<td>
<p>a length one character vector listing all (single character)
end-of-sentence tokens.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a character vector, each entry of which corresponds to a single
sentence.
</p>


<h3>Author(s)</h3>

<p>Valerio Gherardi
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tokenize_sentences("Hi there! I'm using `sbo`.")
</code></pre>

<hr>
<h2 id='twitter_dict'>Top 1000 dictionary from Twitter training set</h2><span id='topic+twitter_dict'></span>

<h3>Description</h3>

<p>Top 1000 dictionary from Twitter training set
</p>


<h3>Usage</h3>

<pre><code class='language-R'>twitter_dict
</code></pre>


<h3>Format</h3>

<p>A character vector. Contains the 1000 most frequent words from
the example training set <code><a href="#topic+twitter_train">twitter_train</a></code>, sorted by word
rank.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+twitter_train">twitter_train</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>head(twitter_dict, 10)
</code></pre>

<hr>
<h2 id='twitter_freqs'>k-gram frequencies from Twitter training set</h2><span id='topic+twitter_freqs'></span>

<h3>Description</h3>

<p>k-gram frequencies from Twitter training set
</p>


<h3>Usage</h3>

<pre><code class='language-R'>twitter_freqs
</code></pre>


<h3>Format</h3>

<p>A <code>sbo_kgram_freqs</code> object. Contains k-gram frequencies from
the example training set <code><a href="#topic+twitter_train">twitter_train</a></code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+twitter_train">twitter_train</a></code>
</p>

<hr>
<h2 id='twitter_predtable'>Next-word prediction tables from 3-gram model trained on Twitter training
set</h2><span id='topic+twitter_predtable'></span>

<h3>Description</h3>

<p>Next-word prediction tables from 3-gram model trained on Twitter training
set
</p>


<h3>Usage</h3>

<pre><code class='language-R'>twitter_predtable
</code></pre>


<h3>Format</h3>

<p>A <code>sbo_predtable</code> object. Contains prediction tables of a 3-gram
Stupid Back-off model trained on the example training set
<code><a href="#topic+twitter_train">twitter_train</a></code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+twitter_train">twitter_train</a></code>
</p>

<hr>
<h2 id='twitter_test'>Twitter test set</h2><span id='topic+twitter_test'></span>

<h3>Description</h3>

<p>Twitter test set
</p>


<h3>Usage</h3>

<pre><code class='language-R'>twitter_test
</code></pre>


<h3>Format</h3>

<p>A collection of 10'000 Twitter posts in English.
</p>


<h3>Source</h3>

<p><a href="https://www.kaggle.com/crmercado/tweets-blogs-news-swiftkey-dataset-4million">https://www.kaggle.com/crmercado/tweets-blogs-news-swiftkey-dataset-4million</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+twitter_train">twitter_train</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>head(twitter_test)
</code></pre>

<hr>
<h2 id='twitter_train'>Twitter training set</h2><span id='topic+twitter_train'></span>

<h3>Description</h3>

<p>Twitter training set
</p>


<h3>Usage</h3>

<pre><code class='language-R'>twitter_train
</code></pre>


<h3>Format</h3>

<p>A collection of 50'000 Twitter posts in English.
</p>


<h3>Source</h3>

<p><a href="https://www.kaggle.com/crmercado/tweets-blogs-news-swiftkey-dataset-4million">https://www.kaggle.com/crmercado/tweets-blogs-news-swiftkey-dataset-4million</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+twitter_test">twitter_test</a></code>, <code><a href="#topic+twitter_dict">twitter_dict</a></code>,
<code><a href="#topic+twitter_freqs">twitter_freqs</a></code>, <code><a href="#topic+twitter_predtable">twitter_predtable</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>head(twitter_train)
</code></pre>

<hr>
<h2 id='word_coverage'>Word coverage fraction</h2><span id='topic+word_coverage'></span><span id='topic+word_coverage.sbo_dictionary'></span><span id='topic+word_coverage.character'></span><span id='topic+word_coverage.sbo_kgram_freqs'></span><span id='topic+word_coverage.sbo_predictions'></span>

<h3>Description</h3>

<p>Compute total and cumulative corpus coverage fraction of a dictionary.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>word_coverage(object, corpus, ...)

## S3 method for class 'sbo_dictionary'
word_coverage(object, corpus, ...)

## S3 method for class 'character'
word_coverage(object, corpus, .preprocess = identity, EOS = "", ...)

## S3 method for class 'sbo_kgram_freqs'
word_coverage(object, corpus, ...)

## S3 method for class 'sbo_predictions'
word_coverage(object, corpus, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="word_coverage_+3A_object">object</code></td>
<td>
<p>either a character vector, or an object inheriting from one of
the classes <code>sbo_dictionary</code>, <code>sbo_kgram_freqs</code>,
<code>sbo_predtable</code> or <code>sbo_predictor</code>.
The object storing the dictionary for which corpus coverage is to be
computed.</p>
</td></tr>
<tr><td><code id="word_coverage_+3A_corpus">corpus</code></td>
<td>
<p>a character vector.</p>
</td></tr>
<tr><td><code id="word_coverage_+3A_...">...</code></td>
<td>
<p>further arguments passed to or from other methods.</p>
</td></tr>
<tr><td><code id="word_coverage_+3A_.preprocess">.preprocess</code></td>
<td>
<p>preprocessing function for training corpus. See
<code><a href="#topic+kgram_freqs">kgram_freqs</a></code> and <code><a href="#topic+sbo_dictionary">sbo_dictionary</a></code> for
further details.</p>
</td></tr>
<tr><td><code id="word_coverage_+3A_eos">EOS</code></td>
<td>
<p>a length one character vector. String containing End-Of-Sentence
characters, see  <code><a href="#topic+kgram_freqs">kgram_freqs</a></code> and
<code><a href="#topic+sbo_dictionary">sbo_dictionary</a></code> for further details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function computes the corpus coverage fraction of a dictionary,
that is the fraction of words appearing in corpus which are contained in the
original dictionary.
</p>
<p>This function is a generic, accepting as <code>object</code> argument any object
storing a dictionary, along with a preprocessing function and a list
of End-Of-Sentence characters. This includes all <code>sbo</code> main classes:
<code>sbo_dictionary</code>, <code>sbo_kgram_freqs</code>, <code>sbo_predtable</code> and
<code>sbo_predictor</code>. When <code>object</code> is a character vector, the preprocessing
function and the End-Of-Sentence characters must be specified explicitly.
</p>
<p>The coverage fraction is computed cumulatively, and the dependence of
coverage with respect to maximal rank can be explored through <code>plot()</code>
(see examples below)
</p>


<h3>Value</h3>

<p>a <code>word_coverage</code> object.
</p>


<h3>Author(s)</h3>

<p>Valerio Gherardi
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.sbo_predictor">predict.sbo_predictor</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
c &lt;- word_coverage(twitter_dict, twitter_train)
print(c)
summary(c)
# Plot coverage fraction, including the End-Of-Sentence in word counts.
plot(c, include_EOS = TRUE)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
