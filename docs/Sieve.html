<!DOCTYPE html><html><head><title>Help for package Sieve</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {Sieve}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#Sieve-package'>
<p>Nonparametric Estimation by the Method of Sieves</p></a></li>
<li><a href='#clean_up_result'><p>Clean up the fitted model</p></a></li>
<li><a href='#create_index_matrix'><p>Create the index matrix for multivariate regression</p></a></li>
<li><a href='#GenSamples'><p>Generate some simulation/testing samples with nonlinear truth.</p></a></li>
<li><a href='#sieve_predict'><p>Predict the outcome of interest for new samples</p></a></li>
<li><a href='#sieve_preprocess'><p>Preprocess the original data for sieve estimation.</p></a></li>
<li><a href='#sieve_solver'><p>Calculate the coefficients for the basis functions</p></a></li>
<li><a href='#sieve.sgd.predict'><p>Sieve-SGD makes prediction with new predictors.</p></a></li>
<li><a href='#sieve.sgd.preprocess'><p>Preprocess the original data for sieve-SGD estimation.</p></a></li>
<li><a href='#sieve.sgd.solver'><p>Fit sieve-SGD estimators, using progressive validation for hyperparameter tuning.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Nonparametric Estimation by the Method of Sieves</td>
</tr>
<tr>
<td>Version:</td>
<td>2.1</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-10-19</td>
</tr>
<tr>
<td>Author:</td>
<td>Tianyu Zhang</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Tianyu Zhang &lt;tianyuz3@andrew.cmu.edu&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Performs multivariate nonparametric regression/classification by the method of sieves (using orthogonal basis). The method is suitable for moderate high-dimensional features (dimension &lt; 100). The l1-penalized sieve estimator, a nonparametric generalization of Lasso, is adaptive to the feature dimension with provable theoretical guarantees. We also include a nonparametric stochastic gradient descent estimator, Sieve-SGD, for online or large scale batch problems. Details of the methods can be found in: &lt;<a href="https://doi.org/10.48550/arXiv.2206.02994">doi:10.48550/arXiv.2206.02994</a>&gt; &lt;<a href="https://doi.org/10.48550/arXiv.2104.00846">doi:10.48550/arXiv.2104.00846</a>&gt;&lt;<a href="https://doi.org/10.48550/arXiv.2310.12140">doi:10.48550/arXiv.2310.12140</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp, combinat, glmnet, methods, MASS</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-10-19 13:48:46 UTC; tianyuzhang</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-10-19 14:10:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='Sieve-package'>
Nonparametric Estimation by the Method of Sieves
</h2><span id='topic+Sieve-package'></span><span id='topic+Sieve'></span>

<h3>Description</h3>

<p>Performs multivariate nonparametric regression/classification by the method of sieves (using orthogonal basis). The method is suitable for moderate high-dimensional features (dimension &lt; 100). The l1-penalized sieve estimator, a nonparametric generalization of Lasso, is adaptive to the feature dimension with provable theoretical guarantees. We also include a nonparametric stochastic gradient descent estimator, Sieve-SGD, for online or large scale batch problems. Details of the methods can be found in: &lt;arXiv:2206.02994&gt; &lt;arXiv:2104.00846&gt;&lt;arXiv:2310.12140&gt;.
</p>


<h3>Details</h3>

<p>The DESCRIPTION file:
</p>

<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> Sieve</td>
</tr>
<tr>
 <td style="text-align: left;">
Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
Title: </td><td style="text-align: left;"> Nonparametric Estimation by the Method of Sieves</td>
</tr>
<tr>
 <td style="text-align: left;">
Version: </td><td style="text-align: left;"> 2.1</td>
</tr>
<tr>
 <td style="text-align: left;">
Date: </td><td style="text-align: left;"> 2023-10-19</td>
</tr>
<tr>
 <td style="text-align: left;">
Author: </td><td style="text-align: left;"> Tianyu Zhang</td>
</tr>
<tr>
 <td style="text-align: left;">
Maintainer: </td><td style="text-align: left;"> Tianyu Zhang &lt;tianyuz3@andrew.cmu.edu&gt;</td>
</tr>
<tr>
 <td style="text-align: left;">
Description: </td><td style="text-align: left;"> Performs multivariate nonparametric regression/classification by the method of sieves (using orthogonal basis). The method is suitable for moderate high-dimensional features (dimension &lt; 100). The l1-penalized sieve estimator, a nonparametric generalization of Lasso, is adaptive to the feature dimension with provable theoretical guarantees. We also include a nonparametric stochastic gradient descent estimator, Sieve-SGD, for online or large scale batch problems. Details of the methods can be found in: &lt;arXiv:2206.02994&gt; &lt;arXiv:2104.00846&gt;&lt;arXiv:2310.12140&gt;.</td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> GPL-2</td>
</tr>
<tr>
 <td style="text-align: left;">
Imports: </td><td style="text-align: left;"> Rcpp,
combinat,
glmnet,
methods,
MASS</td>
</tr>
<tr>
 <td style="text-align: left;">
LinkingTo: </td><td style="text-align: left;"> Rcpp, RcppArmadillo</td>
</tr>
<tr>
 <td style="text-align: left;">
RoxygenNote: </td><td style="text-align: left;"> 7.2.3</td>
</tr>
<tr>
 <td style="text-align: left;">
Encoding: </td><td style="text-align: left;"> UTF-8</td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>


<p>Index of help topics:
</p>
<pre>
GenSamples              Generate some simulation/testing samples with
                        nonlinear truth.
Sieve-package           Nonparametric Estimation by the Method of
                        Sieves
clean_up_result         Clean up the fitted model
create_index_matrix     Create the index matrix for multivariate
                        regression
sieve.sgd.predict       Sieve-SGD makes prediction with new predictors.
sieve.sgd.preprocess    Preprocess the original data for sieve-SGD
                        estimation.
sieve.sgd.solver        Fit sieve-SGD estimators, using progressive
                        validation for hyperparameter tuning.
sieve_predict           Predict the outcome of interest for new samples
sieve_preprocess        Preprocess the original data for sieve
                        estimation.
sieve_solver            Calculate the coefficients for the basis
                        functions
</pre>
<p>~~ An overview of how to use the ~~
~~ package, including the most ~~
~~ important functions ~~
</p>


<h3>Author(s)</h3>

<p>Tianyu Zhang
</p>
<p>Maintainer: Tianyu Zhang &lt;tianyuz3@andrew.cmu.edu&gt;
</p>


<h3>References</h3>

<p>Tianyu Zhang and Noah Simon (2022) &lt;arXiv:2206.02994&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
xdim &lt;- 5
basisN &lt;- 1000
type &lt;- 'cosine'

#non-linear additive truth. Half of the features are truly associated with the outcome
TrainData &lt;- GenSamples(s.size = 300, xdim = xdim, 
            frho = 'additive', frho.para = xdim/2)

#noise-free testing samples
TestData &lt;- GenSamples(s.size = 1e3, xdim = xdim, noise.para = 0, 
            frho = 'additive', frho.para = xdim/2)

sieve.model &lt;- sieve_preprocess(X = TrainData[,2:(xdim+1)], 
            basisN = basisN, type = type, interaction_order = 2)

sieve.model &lt;- sieve_solver(sieve.model, TrainData$Y, l1 = TRUE)

sieve_model_prediction &lt;- sieve_predict(testX = TestData[,2:(xdim+1)], 
                testY = TestData$Y, sieve.model)

</code></pre>

<hr>
<h2 id='clean_up_result'>Clean up the fitted model</h2><span id='topic+clean_up_result'></span>

<h3>Description</h3>

<p>Clean up the fitted model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>clean_up_result(sieve.model)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="clean_up_result_+3A_sieve.model">sieve.model</code></td>
<td>
<p>a sieve sgd model.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a processed sieve.model, adding function names and extract the best model
</p>

<hr>
<h2 id='create_index_matrix'>Create the index matrix for multivariate regression</h2><span id='topic+create_index_matrix'></span>

<h3>Description</h3>

<p>Create the index matrix for multivariate regression
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_index_matrix(xdim, basisN = NULL, maxj = NULL, interaction_order = 5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="create_index_matrix_+3A_xdim">xdim</code></td>
<td>
<p>a number. It specifies the predictors' dimension.</p>
</td></tr>
<tr><td><code id="create_index_matrix_+3A_basisn">basisN</code></td>
<td>
<p>a number. The number of basis function to use.</p>
</td></tr>
<tr><td><code id="create_index_matrix_+3A_maxj">maxj</code></td>
<td>
<p>a number. We use this to specify the largest row product in the index list.</p>
</td></tr>
<tr><td><code id="create_index_matrix_+3A_interaction_order">interaction_order</code></td>
<td>
<p>a number The maximum order of interaction. 1 means additive model, 2 means including pairwise interaction terms, etc.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a matrix. The first column is the product of the indices, the rest columns are the index vectors for constructing multivariate basis functions.
</p>

<hr>
<h2 id='GenSamples'>Generate some simulation/testing samples with nonlinear truth.</h2><span id='topic+GenSamples'></span>

<h3>Description</h3>

<p>This function is used in several examples in the package.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>GenSamples(
  s.size,
  xdim = 1,
  x.dis = "uniform",
  x.para = NULL,
  frho = "linear",
  frho.para = 100,
  y.type = "continuous",
  noise.dis = "normal",
  noise.para = 0.5
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="GenSamples_+3A_s.size">s.size</code></td>
<td>
<p>a number. Sample size.</p>
</td></tr>
<tr><td><code id="GenSamples_+3A_xdim">xdim</code></td>
<td>
<p>a number. Dimension of the feature vectors X.</p>
</td></tr>
<tr><td><code id="GenSamples_+3A_x.dis">x.dis</code></td>
<td>
<p>a string. It specifies the distribution of feature X. The default is uniform distribution over <code>xdim</code>-dimensional unit cube.</p>
</td></tr>
<tr><td><code id="GenSamples_+3A_x.para">x.para</code></td>
<td>
<p>extra parameter to specify the feature distribution.</p>
</td></tr>
<tr><td><code id="GenSamples_+3A_frho">frho</code></td>
<td>
<p>a string. It specifies the true regression/log odds functions used to generate the data set. The default is a linear function.</p>
</td></tr>
<tr><td><code id="GenSamples_+3A_frho.para">frho.para</code></td>
<td>
<p>extra parameter to specify the true underlying regression/log odds function.</p>
</td></tr>
<tr><td><code id="GenSamples_+3A_y.type">y.type</code></td>
<td>
<p>a string. Default is <code>y.type = 'continuous'</code>, meaning the outcome is numerical and the problem is regression. Set it to <code>y.type = 'binary'</code> for binary outcome.</p>
</td></tr>
<tr><td><code id="GenSamples_+3A_noise.dis">noise.dis</code></td>
<td>
<p>a string. For the distribution of the noise variable (under regression probelm settings). Default is Gaussian distribution.</p>
</td></tr>
<tr><td><code id="GenSamples_+3A_noise.para">noise.para</code></td>
<td>
<p>a number. It specifies the magnitude of the noise in regression settings.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code>data.frame</code>. The variable <code>Y</code> is the outcome (either continuous or binary). Each of the rest of the variables corresponds to one dimension of the feature vector.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>xdim &lt;- 1 #1 dimensional feature
#generate 1000 training samples
TrainData &lt;- GenSamples(s.size = 1000, xdim = xdim)
#generate some noise-free testing samples
TestData &lt;- GenSamples(s.size = 1000, xdim = xdim, noise.para = 0)
</code></pre>

<hr>
<h2 id='sieve_predict'>Predict the outcome of interest for new samples</h2><span id='topic+sieve_predict'></span>

<h3>Description</h3>

<p>Use the fitted sieve regression model from sieve_solver. It also returns the testing mean-squared errors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sieve_predict(model, testX, testY = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sieve_predict_+3A_model">model</code></td>
<td>
<p>a list. Use the fitted model from sieve_solver.</p>
</td></tr>
<tr><td><code id="sieve_predict_+3A_testx">testX</code></td>
<td>
<p>a data frame. Dimension equals to test sample size x feature diemnsion. Should be of a similar format as the training feature provided to sieve_preprocess.</p>
</td></tr>
<tr><td><code id="sieve_predict_+3A_testy">testY</code></td>
<td>
<p>a vector. The outcome of testing samples (if known). Default is NULL. For regression problems, the algorithm also returns the testing mean-squared errors.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list. 
</p>
<table>
<tr><td><code>predictY</code></td>
<td>
<p>a matrix. Dimension is test sample size (# of rows) x number of penalty hyperparameter lambda (# of columns). 
For regression problem, that is, when family = &quot;gaussian&quot;, each entry is the estimated conditional mean (or predictor of outcome Y). For classification problems (family = &quot;binomial&quot;), each entry is the predicted probability of having Y = 1 (which class is defined as &quot;class 1&quot; depends on the training data labeling). </p>
</td></tr>
<tr><td><code>MSE</code></td>
<td>
<p>For regression problem, when testY is provided, the algorithm also calculates the mean-sqaured errors using testing data. Each entry of <code>MSE</code> correponds to one value of penalization hyperparameter <code>lambda</code></p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>xdim &lt;- 1 #1 dimensional feature
#generate 1000 training samples
TrainData &lt;- GenSamples(s.size = 1000, xdim = xdim)
#use 50 cosine basis functions
type &lt;- 'cosine'
basisN &lt;- 50 
sieve.model &lt;- sieve_preprocess(X = TrainData[,2:(xdim+1)], 
                                basisN = basisN, type = type)
sieve.fit&lt;- sieve_solver(model = sieve.model, Y = TrainData$Y)
#generate 1000 testing samples
TestData &lt;- GenSamples(s.size = 1000, xdim = xdim)
sieve.prediction &lt;- sieve_predict(model = sieve.fit, 
                                  testX = TestData[,2:(xdim+1)], 
                                  testY = TestData$Y)
###if the outcome is binary, 
###need to solve a nonparametric logistic regression problem
xdim &lt;- 1
TrainData &lt;- GenSamples(s.size = 1e3, xdim = xdim, y.type = 'binary', frho = 'nonlinear_binary')
sieve.model &lt;- sieve_preprocess(X = TrainData[,2:(xdim+1)], 
                                basisN = basisN, type = type)
sieve.fit&lt;- sieve_solver(model = sieve.model, Y = TrainData$Y,
                         family = 'binomial')
                         
###the predicted value is conditional probability (of taking class 1).
TrainData &lt;- GenSamples(s.size = 1e3, xdim = xdim, y.type = 'binary', frho = 'nonlinear_binary')
sieve.prediction &lt;- sieve_predict(model = sieve.fit, 
                                  testX = TestData[,2:(xdim+1)])
</code></pre>

<hr>
<h2 id='sieve_preprocess'>Preprocess the original data for sieve estimation.</h2><span id='topic+sieve_preprocess'></span>

<h3>Description</h3>

<p>Generate the design matrix for the downstream lasso-type penalized model fitting.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sieve_preprocess(
  X,
  basisN = NULL,
  maxj = NULL,
  type = "cosine",
  interaction_order = 3,
  index_matrix = NULL,
  norm_feature = TRUE,
  norm_para = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sieve_preprocess_+3A_x">X</code></td>
<td>
<p>a data frame containing original features. The (i,j)-th element is the j-th dimension of the i-th sample's feature vector. 
So the number of rows equals to the sample size and the number of columns equals to the feature dimension.</p>
</td></tr>
<tr><td><code id="sieve_preprocess_+3A_basisn">basisN</code></td>
<td>
<p>number of sieve basis function. It is in general larger than the dimension of the original feature. 
Default is 50*dimension of original feature. A larger value has a smaller approximation error but it is harder to estimate.
The computational time/memory requirement should scale linearly to <code>basisN</code>.</p>
</td></tr>
<tr><td><code id="sieve_preprocess_+3A_maxj">maxj</code></td>
<td>
<p>a number. the maximum index product of the basis function. A larger value means more basisN. 
If basisN is already specified, do not need to provide value for this argument.</p>
</td></tr>
<tr><td><code id="sieve_preprocess_+3A_type">type</code></td>
<td>
<p>a string. It specifies what kind of basis functions are used. The default is (aperiodic) cosine basis functions, which is suitable for most purpose.</p>
</td></tr>
<tr><td><code id="sieve_preprocess_+3A_interaction_order">interaction_order</code></td>
<td>
<p>a number. It also controls the model complexity. 1 means fitting an additive model, 2 means fitting a model allows, 3 means interaction terms between 3 dimensions of the feature, etc. The default is 3. 
For large sample size, lower dimension problems, try a larger value (but need to be smaller than the dimension of original features); for smaller sample size and higher dimensional problems, try set it to a smaller value (1 or 2).</p>
</td></tr>
<tr><td><code id="sieve_preprocess_+3A_index_matrix">index_matrix</code></td>
<td>
<p>a matrix. provide a pre-generated index matrix. The default is NULL, meaning sieve_preprocess will generate one for the user.</p>
</td></tr>
<tr><td><code id="sieve_preprocess_+3A_norm_feature">norm_feature</code></td>
<td>
<p>a logical variable. Default is TRUE. It means sieve_preprocess will rescale the each dimension of features to 0 and 1. Only set to FALSE when user already manually rescale them between 0 and 1.</p>
</td></tr>
<tr><td><code id="sieve_preprocess_+3A_norm_para">norm_para</code></td>
<td>
<p>a matrix. It specifies how the features are normalized. For training data, use the default value NULL.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing the necessary information for next step model fitting. Typically, the list is used as the main input of Sieve::sieve_solver.
</p>
<table>
<tr><td><code>Phi</code></td>
<td>
<p>a matrix. This is the design matrix directly used by the next step model fitting. The (i,j)-th element of this matrix is the evaluation of i-th sample's feature at the j-th basis function. The dimension of this matrix is sample size x basisN.</p>
</td></tr> 
<tr><td><code>X</code></td>
<td>
<p>a matrix. This is the rescaled original feature/predictor matrix.</p>
</td></tr> 
<tr><td><code>type</code></td>
<td>
<p>a string. The type of basis funtion.</p>
</td></tr>
<tr><td><code>index_matrix</code></td>
<td>
<p>a matrix. It specifies what are the product basis functions used when constructing the design matrix Phi. It has a dimension basisN x dimension of original features. There are at most interaction_order many non-1 elements in each row.</p>
</td></tr>
<tr><td><code>basisN</code></td>
<td>
<p>a number. Number of sieve basis functions.</p>
</td></tr>
<tr><td><code>norm_para</code></td>
<td>
<p>a matrix. It records how each dimension of the feature/predictor is rescaled, which is useful when rescaling the testing sample's predictors.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>xdim &lt;- 1 #1 dimensional feature
#generate 1000 training samples
TrainData &lt;- GenSamples(s.size = 1000, xdim = xdim)
#use 50 cosine basis functions
type &lt;- 'cosine'
basisN &lt;- 50 
sieve.model &lt;- sieve_preprocess(X = TrainData[,2:(xdim+1)], 
                                basisN = basisN, type = type)
#sieve.model$Phi #Phi is the design matrix

xdim &lt;- 5 #1 dimensional feature
#generate 1000 training samples
#only the first two dimensions are truly associated with the outcome
TrainData &lt;- GenSamples(s.size = 1000, xdim = xdim, 
                              frho = 'additive', frho.para = 2)
                              
#use 1000 basis functions
#each of them is a product of univariate cosine functions.
type &lt;- 'cosine'
basisN &lt;- 1000 
sieve.model &lt;- sieve_preprocess(X = TrainData[,2:(xdim+1)], 
                                basisN = basisN, type = type)
#sieve.model$Phi #Phi is the design matrix

#fit a nonaprametric additive model by setting interaction_order = 1
sieve.model &lt;- sieve_preprocess(X = TrainData[,2:(xdim+1)], 
                                basisN = basisN, type = type, 
                                interaction_order = 1)
#sieve.model$index_matrix #for each row, there is at most one entry &gt;= 2. 
#this means there are no basis functions varying in more than 2-dimensions 
#that is, we are fitting additive models without interaction between features.
</code></pre>

<hr>
<h2 id='sieve_solver'>Calculate the coefficients for the basis functions</h2><span id='topic+sieve_solver'></span>

<h3>Description</h3>

<p>This is the main function that performs sieve estimation. It calculate the coefficients by solving a penalized lasso type problem.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sieve_solver(
  model,
  Y,
  l1 = TRUE,
  family = "gaussian",
  lambda = NULL,
  nlambda = 100
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sieve_solver_+3A_model">model</code></td>
<td>
<p>a list. Typically, it is the output of Sieve::sieve_preprocess.</p>
</td></tr>
<tr><td><code id="sieve_solver_+3A_y">Y</code></td>
<td>
<p>a vector. The outcome variable. The length of Y equals to the training sample size, which should also match the row number of X in model.</p>
</td></tr>
<tr><td><code id="sieve_solver_+3A_l1">l1</code></td>
<td>
<p>a logical variable. TRUE means calculating the coefficients by sovling a l1-penalized empirical risk minimization problem. FALSE means solving a least-square problem. Default is TRUE.</p>
</td></tr>
<tr><td><code id="sieve_solver_+3A_family">family</code></td>
<td>
<p>a string. 'gaussian', mean-squared-error regression problem.</p>
</td></tr>
<tr><td><code id="sieve_solver_+3A_lambda">lambda</code></td>
<td>
<p>same as the lambda of glmnet::glmnet.</p>
</td></tr>
<tr><td><code id="sieve_solver_+3A_nlambda">nlambda</code></td>
<td>
<p>a number. Number of penalization hyperparameter used when solving the lasso-type problem. Default is 100.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list. In addition to the preprocessing information, it also has the fitted value.
</p>
<table>
<tr><td><code>Phi</code></td>
<td>
<p>a matrix. This is the design matrix directly used by the next step model fitting. The (i,j)-th element of this matrix is the evaluation of i-th sample's feature at the j-th basis function. The dimension of this matrix is sample size x basisN.</p>
</td></tr>
<tr><td><code>X</code></td>
<td>
<p>a matrix. This is the rescaled original feature/predictor matrix. </p>
</td></tr>
<tr><td><code>beta_hat</code></td>
<td>
<p>a matrix. Dimension is basisN x nlambda. The j-th column corresponds to the fitted regression coeffcients using the j-th hyperparameter in lambda.</p>
</td></tr>
<tr><td><code>type</code></td>
<td>
<p>a string. The type of basis funtion.</p>
</td></tr>
<tr><td><code>index_matrix</code></td>
<td>
<p>a matrix. It specifies what are the product basis functions used when constructing the design matrix Phi. It has a dimension basisN x dimension of original features. There are at most interaction_order many non-1 elements in each row.</p>
</td></tr>
<tr><td><code>basisN</code></td>
<td>
<p>a number. Number of sieve basis functions.</p>
</td></tr>
<tr><td><code>norm_para</code></td>
<td>
<p>a matrix. It records how each dimension of the feature/predictor is rescaled, which is useful when rescaling the testing sample's predictors.</p>
</td></tr>
<tr><td><code>lambda</code></td>
<td>
<p>a vector. It records the penalization hyperparameter used when solving the lasso problems. Default has a length of 100, meaning the algorithm tried 100 different penalization hyperparameters.</p>
</td></tr>
<tr><td><code>family</code></td>
<td>
<p>a string. 'gaussian', continuous numerical outcome, regression probelm; 'binomial', binary outcome, classification problem.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>xdim &lt;- 1 #1 dimensional feature
#generate 1000 training samples
TrainData &lt;- GenSamples(s.size = 1000, xdim = xdim)
#use 50 cosine basis functions
type &lt;- 'cosine'
basisN &lt;- 50 
sieve.model &lt;- sieve_preprocess(X = TrainData[,2:(xdim+1)], 
                                basisN = basisN, type = type)
sieve.fit&lt;- sieve_solver(model = sieve.model, Y = TrainData$Y)

###if the outcome is binary, 
###need to solve a nonparametric logistic regression problem
xdim &lt;- 1
TrainData &lt;- GenSamples(s.size = 1e3, xdim = xdim, y.type = 'binary', frho = 'nonlinear_binary')
sieve.model &lt;- sieve_preprocess(X = TrainData[,2:(xdim+1)], 
                                basisN = basisN, type = type)
sieve.fit&lt;- sieve_solver(model = sieve.model, Y = TrainData$Y,
                         family = 'binomial')
</code></pre>

<hr>
<h2 id='sieve.sgd.predict'>Sieve-SGD makes prediction with new predictors.</h2><span id='topic+sieve.sgd.predict'></span>

<h3>Description</h3>

<p>Sieve-SGD makes prediction with new predictors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sieve.sgd.predict(sieve.model, X)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sieve.sgd.predict_+3A_sieve.model">sieve.model</code></td>
<td>
<p>a list initiated using sieve.sgd.preprocess and sieve.sgd.solver. Check the documentation of sieve.sgd.preprocess for more information.</p>
</td></tr>
<tr><td><code id="sieve.sgd.predict_+3A_x">X</code></td>
<td>
<p>a data frame containing prediction features/ independent variables.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>sieve.sgd.predict will update the given sieve.model input list.
</p>
<table>
<tr><td><code>inf.list</code></td>
<td>
<p>In each entry of the list inf.list, the array prdy is the predicted outcome under the given hyperparameter combination.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>frho.para &lt;- xdim &lt;- 1 ##predictor dimension
frho &lt;- 'additive' ###truth is a sum of absolute functions 
type &lt;- 'cosine' ###use cosine functions as the basis functions
#generate training data
TrainData &lt;- GenSamples(s.size = 1e3, xdim = xdim, 
                                frho.para = frho.para, 
                                frho = frho, noise.para = 0.1)
#preprocess the model
sieve.model &lt;- sieve.sgd.preprocess(X = TrainData[,2:(xdim+1)], 
                                    type = type,
                                    s = c(1,2),
                                    r0 = c(0.5, 2, 4),
                                    J = c(1, 4, 8))

##train the model
sieve.model &lt;- sieve.sgd.solver(sieve.model = sieve.model, 
                                X = TrainData[,2:(xdim+1)], 
                                Y  = TrainData[,1])
##generate new data
NewData &lt;- GenSamples(s.size = 5e2, xdim = xdim, 
                      frho.para = frho.para, 
                      frho = frho, noise.para = 0.1)
sieve.model &lt;- sieve.sgd.predict(sieve.model, X = NewData[, 2:(xdim+1)])
plot(NewData[, 2:(xdim+1)], sieve.model$best_model$prdy)
</code></pre>

<hr>
<h2 id='sieve.sgd.preprocess'>Preprocess the original data for sieve-SGD estimation.</h2><span id='topic+sieve.sgd.preprocess'></span>

<h3>Description</h3>

<p>Preprocess the original data for sieve-SGD estimation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sieve.sgd.preprocess(
  X,
  s = c(2),
  r0 = c(2),
  J = c(1),
  type = c("cosine"),
  interaction_order = c(3),
  omega = c(0.51),
  norm_feature = TRUE,
  norm_para = NULL,
  lower_q = 0.005,
  upper_q = 0.995
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sieve.sgd.preprocess_+3A_x">X</code></td>
<td>
<p>a data frame containing prediction features/ independent variables. The (i,j)-th element is the j-th dimension of the i-th sample's feature vector. 
So the number of rows equals to the sample size and the number of columns equals to the feature/covariate dimension. If the complete data set is large, this can be a representative subset of it (ideally have more than 1000 samples).</p>
</td></tr>
<tr><td><code id="sieve.sgd.preprocess_+3A_s">s</code></td>
<td>
<p>numerical array. Smoothness parameter, a smaller s corresponds to a more flexible model. Default is 2. The elements of this array should take values greater than 0.5. The larger s is, the smoother we are assuming the truth to be.</p>
</td></tr>
<tr><td><code id="sieve.sgd.preprocess_+3A_r0">r0</code></td>
<td>
<p>numerical array. Initial learning rate/step size, don't set it too large. The step size at each iteration will be r0*(sample size)^(-1/(2s+1)), which is slowly decaying.</p>
</td></tr>
<tr><td><code id="sieve.sgd.preprocess_+3A_j">J</code></td>
<td>
<p>numerical array. Initial number of basis functions, a larger J corresponds to a more flexible estimator The number of basis functions at each iteration will be J*(sample size)^(1/(2s+1)), which is slowly increasing. We recommend use J that is at least the dimension of predictor, i.e. the column number of the X matrix.</p>
</td></tr>
<tr><td><code id="sieve.sgd.preprocess_+3A_type">type</code></td>
<td>
<p>a string. It specifies what kind of basis functions are used. The default is (aperiodic) cosine basis functions ('cosine'), which is enough for generic usage.</p>
</td></tr>
<tr><td><code id="sieve.sgd.preprocess_+3A_interaction_order">interaction_order</code></td>
<td>
<p>a number. It also controls the model complexity. 1 means fitting an additive model, 2 means fitting a model allows, 3 means interaction terms between 3 dimensions of the feature, etc. The default is 3. 
For large sample size, lower dimension problems, try a larger value (but need to be smaller than the dimension of original features); for smaller sample size and higher dimensional problems, try set it to a smaller value (1 or 2).</p>
</td></tr>
<tr><td><code id="sieve.sgd.preprocess_+3A_omega">omega</code></td>
<td>
<p>the rate of dimension-reduction parameter. Default is 0.51, usually do not need to change.</p>
</td></tr>
<tr><td><code id="sieve.sgd.preprocess_+3A_norm_feature">norm_feature</code></td>
<td>
<p>a logical variable. Default is TRUE. It means sieve_preprocess will rescale the each dimension of features to 0 and 1. Only set to FALSE when user already manually rescale them between 0 and 1.</p>
</td></tr>
<tr><td><code id="sieve.sgd.preprocess_+3A_norm_para">norm_para</code></td>
<td>
<p>a matrix. It specifies how the features are normalized. For training data, use the default value NULL.</p>
</td></tr>
<tr><td><code id="sieve.sgd.preprocess_+3A_lower_q">lower_q</code></td>
<td>
<p>lower quantile used in normalization. Default is 0.01 (1% quantile).</p>
</td></tr>
<tr><td><code id="sieve.sgd.preprocess_+3A_upper_q">upper_q</code></td>
<td>
<p>upper quantile used in normalization. Default is 0.99 (99% quantile).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing the necessary information for next step model fitting. Typically, the list is used as the main input of sieve.sgd.solver.
</p>
<table>
<tr><td><code>s.size.sofar</code></td>
<td>
<p>a number. Number of samples has been processed so far.</p>
</td></tr>
<tr><td><code>type</code></td>
<td>
<p>a string. The type of basis funtion.</p>
</td></tr>
<tr><td><code>hyper.para.list</code></td>
<td>
<p>a list of hyperparameters.</p>
</td></tr>
<tr><td><code>index.matrix</code></td>
<td>
<p>a matrix. Identifies the multivariate basis functions used in fitting.</p>
</td></tr>
<tr><td><code>index.row.prod</code></td>
<td>
<p>the index product for each basis function. It is used in calculating basis function - specific learning rates.</p>
</td></tr>
<tr><td><code>inf.list</code></td>
<td>
<p>a list storing the fitted results. It has a length of &quot;number of unique combinations of the hyperparameters&quot;. The component of inf.list is itself a list, it has a hyper.para.index domain to specify its corresponding hyperparameters (need to be used together with hyper.para.list). Its rolling.cv domain is the progressive validation statistics for hyperparameter tuning; beta.f is the regression coefficients for the first length(beta.f) basis functions, the rest of the basis have 0 coefficients.</p>
</td></tr>
<tr><td><code>norm_para</code></td>
<td>
<p>a matrix. It records how each dimension of the feature/predictor is rescaled, which is useful when rescaling the testing sample's predictors.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>xdim &lt;- 1 #1 dimensional feature
#generate 1000 training samples
TrainData &lt;- GenSamples(s.size = 1000, xdim = xdim)
sieve.model &lt;- sieve.sgd.preprocess(X = TrainData[,2:(xdim+1)])
</code></pre>

<hr>
<h2 id='sieve.sgd.solver'>Fit sieve-SGD estimators, using progressive validation for hyperparameter tuning.</h2><span id='topic+sieve.sgd.solver'></span>

<h3>Description</h3>

<p>Fit sieve-SGD estimators, using progressive validation for hyperparameter tuning.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sieve.sgd.solver(sieve.model, X, Y, cv_weight_rate = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sieve.sgd.solver_+3A_sieve.model">sieve.model</code></td>
<td>
<p>a list initiated using sieve.sgd.preprocess. Check the documentation of sieve.sgd.preprocess for more information.</p>
</td></tr>
<tr><td><code id="sieve.sgd.solver_+3A_x">X</code></td>
<td>
<p>a data frame containing prediction features/ independent variables.</p>
</td></tr>
<tr><td><code id="sieve.sgd.solver_+3A_y">Y</code></td>
<td>
<p>training outcome.</p>
</td></tr>
<tr><td><code id="sieve.sgd.solver_+3A_cv_weight_rate">cv_weight_rate</code></td>
<td>
<p>this governs the divergence rate of rolling validation statistics. Default is set to be 1 and in general does not need to be changed.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list. It contains the fitted regression coefficients and progressive validation statistics for each hyperparameter combination.
</p>
<table>
<tr><td><code>s.size.sofar</code></td>
<td>
<p>a number. Number of samples has been processed so far.</p>
</td></tr>
<tr><td><code>type</code></td>
<td>
<p>a string. The type of basis funtion.</p>
</td></tr>
<tr><td><code>hyper.para.list</code></td>
<td>
<p>a list of hyperparameters.</p>
</td></tr>
<tr><td><code>index.matrix</code></td>
<td>
<p>a matrix. Identifies the multivariate basis functions used in fitting.</p>
</td></tr>
<tr><td><code>index.row.prod</code></td>
<td>
<p>the index product for each basis function. It is used in calculating basis function - specific learning rates.</p>
</td></tr>
<tr><td><code>inf.list</code></td>
<td>
<p>a list storing the fitted results. It has a length of &quot;number of unique combinations of the hyperparameters&quot;. The component of inf.list is itself a list, it has a hyper.para.index domain to specify its corresponding hyperparameters (need to be used together with hyper.para.list). Its rolling.cv domain is the progressive validation statistics for hyperparameter tuning; beta.f is the regression coefficients for the first length(beta.f) basis functions, the rest of the basis have 0 coefficients.</p>
</td></tr>
<tr><td><code>norm_para</code></td>
<td>
<p>a matrix. It records how each dimension of the feature/predictor is rescaled, which is useful when rescaling the testing sample's predictors.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>frho.para &lt;- xdim &lt;- 1 ##predictor dimension
frho &lt;- 'additive' ###truth is a sum of absolute functions 
type &lt;- 'cosine' ###use cosine functions as the basis functions
#generate training data
TrainData &lt;- GenSamples(s.size = 1e3, xdim = xdim, 
                                frho.para = frho.para, 
                                frho = frho, noise.para = 0.1)
#preprocess the model
sieve.model &lt;- sieve.sgd.preprocess(X = TrainData[,2:(xdim+1)], 
                                    type = type,
                                    s = c(1,2),
                                    r0 = c(0.5, 2, 4),
                                    J = c(1, 4, 8))

##train the model
sieve.model &lt;- sieve.sgd.solver(sieve.model = sieve.model, 
                                X = TrainData[,2:(xdim+1)], 
                                Y  = TrainData[,1])

##sieve-SGD can do multiple passes over the data, just like other SGD methods.
##usually a second pass can still improve the prediction accuracy
##watch out overfitting when performing multiple passes!
sieve.model &lt;- sieve.sgd.solver(sieve.model = sieve.model, 
                              X = TrainData[,2:(xdim+1)], 
                              Y  = TrainData[,1])
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
