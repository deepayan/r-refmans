<!DOCTYPE html><html><head><title>Help for package ipred</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {ipred}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#bagging'><p>Bagging Classification, Regression and Survival Trees</p></a></li>
<li><a href='#bootest'><p>Bootstrap Error Rate Estimators</p></a></li>
<li><a href='#control.errorest'><p> Control Error Rate Estimators</p></a></li>
<li><a href='#cv'><p>Cross-validated Error Rate Estimators.</p></a></li>
<li><a href='#DLBCL'><p> Diffuse Large B-Cell Lymphoma</p></a></li>
<li><a href='#dystrophy'><p>Detection of muscular dystrophy carriers.</p></a></li>
<li><a href='#errorest'><p> Estimators of Prediction Error</p></a></li>
<li><a href='#GlaucomaMVF'><p> Glaucoma Database</p></a></li>
<li><a href='#inbagg'><p>Indirect Bagging</p></a></li>
<li><a href='#inclass'><p>Indirect Classification</p></a></li>
<li><a href='#ipred-internal'><p>Internal ipred functions</p></a></li>
<li><a href='#ipredknn'><p> k-Nearest Neighbour Classification</p></a></li>
<li><a href='#kfoldcv'><p> Subsamples for k-fold Cross-Validation</p></a></li>
<li><a href='#mypredict.lm'><p>Predictions Based on Linear Models</p></a></li>
<li><a href='#predict.classbagg'><p> Predictions from Bagging Trees</p></a></li>
<li><a href='#predict.inbagg'><p>Predictions from an Inbagg Object</p></a></li>
<li><a href='#predict.inclass'><p>Predictions from an Inclass Object</p></a></li>
<li><a href='#predict.ipredknn'><p> Predictions from k-Nearest Neighbors</p></a></li>
<li><a href='#predict.slda'><p> Predictions from Stabilised Linear Discriminant Analysis</p></a></li>
<li><a href='#print.classbagg'><p>Print Method for Bagging Trees</p></a></li>
<li><a href='#print.cvclass'><p>Print Method for Error Rate Estimators</p></a></li>
<li><a href='#print.inbagg'><p>Print Method for Inbagg Object</p></a></li>
<li><a href='#print.inclass'><p>Print Method for Inclass Object</p></a></li>
<li><a href='#prune.classbagg'><p> Pruning for Bagging</p></a></li>
<li><a href='#rsurv'><p> Simulate Survival Data</p></a></li>
<li><a href='#sbrier'><p> Model Fit for Survival Data</p></a></li>
<li><a href='#slda'><p> Stabilised Linear Discriminant Analysis</p></a></li>
<li><a href='#Smoking'><p>Smoking Styles</p></a></li>
<li><a href='#summary.classbagg'><p>Summarising Bagging</p></a></li>
<li><a href='#summary.inbagg'><p>Summarising Inbagg</p></a></li>
<li><a href='#summary.inclass'><p>Summarising Inclass</p></a></li>
<li><a href='#varset'><p>Simulation Model</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Improved Predictors</td>
</tr>
<tr>
<td>Version:</td>
<td>0.9-14</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-03-09</td>
</tr>
<tr>
<td>Description:</td>
<td>Improved predictive models by indirect classification and
  bagging for classification, regression and survival problems 
  as well as resampling based estimators of prediction error. </td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.10)</td>
</tr>
<tr>
<td>Imports:</td>
<td>rpart (&ge; 3.1-8), MASS, survival, nnet, class, prodlim</td>
</tr>
<tr>
<td>Suggests:</td>
<td>mvtnorm, mlbench, TH.data, randomForest, party</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-03-09 13:48:29 UTC; hothorn</td>
</tr>
<tr>
<td>Author:</td>
<td>Andrea Peters [aut],
  Torsten Hothorn [aut, cre],
  Brian D. Ripley [ctb],
  Terry Therneau [ctb],
  Beth Atkinson [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Torsten Hothorn &lt;Torsten.Hothorn@R-project.org&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-03-09 14:50:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='bagging'>Bagging Classification, Regression and Survival Trees </h2><span id='topic+bagging'></span><span id='topic+ipredbagg'></span><span id='topic+ipredbagg.factor'></span><span id='topic+ipredbagg.integer'></span><span id='topic+ipredbagg.numeric'></span><span id='topic+ipredbagg.Surv'></span><span id='topic+ipredbagg.default'></span><span id='topic+bagging.data.frame'></span><span id='topic+bagging.default'></span>

<h3>Description</h3>

<p>Bagging for classification, regression and survival trees.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'factor'
ipredbagg(y, X=NULL, nbagg=25, control=
                 rpart.control(minsplit=2, cp=0, xval=0), 
                 comb=NULL, coob=FALSE, ns=length(y), keepX = TRUE, ...)
## S3 method for class 'numeric'
ipredbagg(y, X=NULL, nbagg=25, control=rpart.control(xval=0), 
                  comb=NULL, coob=FALSE, ns=length(y), keepX = TRUE, ...)
## S3 method for class 'Surv'
ipredbagg(y, X=NULL, nbagg=25, control=rpart.control(xval=0), 
               comb=NULL, coob=FALSE, ns=dim(y)[1], keepX = TRUE, ...)
## S3 method for class 'data.frame'
bagging(formula, data, subset, na.action=na.rpart, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bagging_+3A_y">y</code></td>
<td>
<p>the response variable: either a factor vector of class labels
(bagging classification trees), a vector of numerical values 
(bagging regression trees) or an object of class 
<code><a href="survival.html#topic+Surv">Surv</a></code> (bagging survival trees).</p>
</td></tr>
<tr><td><code id="bagging_+3A_x">X</code></td>
<td>
<p>a data frame of predictor variables.</p>
</td></tr>
<tr><td><code id="bagging_+3A_nbagg">nbagg</code></td>
<td>
<p>an integer giving the number of bootstrap replications. </p>
</td></tr>
<tr><td><code id="bagging_+3A_coob">coob</code></td>
<td>
<p>a logical indicating whether an out-of-bag estimate of the
error rate (misclassification error, root mean squared error
or Brier score) should be computed. 
See <code><a href="#topic+predict.classbagg">predict.classbagg</a></code> for
details.</p>
</td></tr>
<tr><td><code id="bagging_+3A_control">control</code></td>
<td>
<p>options that control details of the <code>rpart</code>
algorithm, see <code><a href="rpart.html#topic+rpart.control">rpart.control</a></code>. It is
wise to set <code>xval = 0</code> in order to save computing 
time. Note that the 
default values depend on the class of <code>y</code>.</p>
</td></tr>
<tr><td><code id="bagging_+3A_comb">comb</code></td>
<td>
<p>a list of additional models for model combination, see below
for some examples. Note that argument <code>method</code> for double-bagging is no longer there, 
<code>comb</code> is much more flexible.</p>
</td></tr>
<tr><td><code id="bagging_+3A_ns">ns</code></td>
<td>
<p>number of sample to draw from the learning sample. By default,
the usual bootstrap n out of n with replacement is performed. 
If <code>ns</code> is smaller than <code>length(y)</code>, subagging
(Buehlmann and Yu, 2002), i.e. sampling <code>ns</code> out of
<code>length(y)</code> without replacement, is performed.</p>
</td></tr>
<tr><td><code id="bagging_+3A_keepx">keepX</code></td>
<td>
<p>a logical indicating whether the data frame of predictors
should be returned. Note that the computation of the 
out-of-bag estimator requires  <code>keepX=TRUE</code>.</p>
</td></tr>
<tr><td><code id="bagging_+3A_formula">formula</code></td>
<td>
<p>a formula of the form <code>lhs ~ rhs</code> where <code>lhs</code> 
is the response variable and <code>rhs</code> a set of
predictors.</p>
</td></tr>
<tr><td><code id="bagging_+3A_data">data</code></td>
<td>
<p>optional data frame containing the variables in the
model formula.</p>
</td></tr> 
<tr><td><code id="bagging_+3A_subset">subset</code></td>
<td>
<p>optional vector specifying a subset of observations
to be used.</p>
</td></tr>
<tr><td><code id="bagging_+3A_na.action">na.action</code></td>
<td>
<p>function which indicates what should happen when
the data contain <code>NA</code>s.  Defaults to
<code><a href="rpart.html#topic+na.rpart">na.rpart</a></code>.</p>
</td></tr>
<tr><td><code id="bagging_+3A_...">...</code></td>
<td>
<p>additional parameters passed to <code>ipredbagg</code> or 
<code><a href="rpart.html#topic+rpart">rpart</a></code>, respectively.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The random forest implementations <code><a href="randomForest.html#topic+randomForest">randomForest</a></code>
and <code><a href="party.html#topic+cforest">cforest</a></code> are more flexible and reliable for computing
bootstrap-aggregated trees than this function and should be used instead.
</p>
<p>Bagging for classification and regression trees were suggested by
Breiman (1996a, 1998) in order to stabilise trees. 
</p>
<p>The trees in this function are computed using the implementation in the 
<code><a href="rpart.html#topic+rpart">rpart</a></code> package. The generic function <code>ipredbagg</code>
implements methods for different responses. If <code>y</code> is a factor,
classification trees are constructed. For numerical vectors
<code>y</code>, regression trees are aggregated and if <code>y</code> is a survival 
object, bagging survival trees (Hothorn et al, 2003) is performed. 
The function <code>bagging</code> offers a formula based interface to
<code>ipredbagg</code>.
</p>
<p><code>nbagg</code> bootstrap samples are drawn and a tree is constructed 
for each of them. There is no general rule when to stop the tree 
growing. The size of the
trees can be controlled by <code>control</code> argument 
or <code><a href="#topic+prune.classbagg">prune.classbagg</a></code>. By
default, classification trees are as large as possible whereas regression
trees and survival trees are build with the standard options of
<code><a href="rpart.html#topic+rpart.control">rpart.control</a></code>. If <code>nbagg=1</code>, one single tree is
computed for the whole learning sample without bootstrapping.
</p>
<p>If <code>coob</code> is TRUE, the out-of-bag sample (Breiman,
1996b) is used to estimate the prediction error 
corresponding to <code>class(y)</code>. Alternatively, the out-of-bag sample can
be used for model combination, an out-of-bag error rate estimator is not 
available in this case. Double-bagging (Hothorn and Lausen,
2003) computes a LDA on the out-of-bag sample and uses the discriminant
variables as additional predictors for the classification trees. <code>comb</code>
is an optional list of lists with two elements <code>model</code> and <code>predict</code>. 
<code>model</code> is a function with arguments <code>formula</code> and <code>data</code>. 
<code>predict</code> is a function with arguments <code>object, newdata</code> only. If
the estimation of the covariance matrix in <code><a href="MASS.html#topic+lda">lda</a></code> fails due to a
limited out-of-bag sample size, one can use <code><a href="#topic+slda">slda</a></code> instead.
See the example section for an example of double-bagging. The methodology is
not limited to a combination with LDA: bundling (Hothorn and Lausen, 2002b) 
can be used with arbitrary classifiers.
</p>
<p>NOTE: Up to ipred version 0.9-0, bagging was performed using a modified version 
of the original rpart function. Due to interface changes in rpart 3.1-55, the
bagging function had to be rewritten. Results of previous version are not 
exactly reproducible.
</p>


<h3>Value</h3>

<p>The class of the object returned depends on <code>class(y)</code>:
<code>classbagg, regbagg</code> and <code>survbagg</code>. Each is a list with elements
</p>
<table>
<tr><td><code>y</code></td>
<td>
<p>the vector of responses.</p>
</td></tr>
<tr><td><code>X</code></td>
<td>
<p>the data frame of predictors.</p>
</td></tr>
<tr><td><code>mtrees</code></td>
<td>
<p>multiple trees: a list of length <code>nbagg</code> containing the
trees (and possibly additional objects) for each bootstrap sample.</p>
</td></tr>
<tr><td><code>OOB</code></td>
<td>
<p>logical whether the out-of-bag estimate should be computed.</p>
</td></tr>
<tr><td><code>err</code></td>
<td>
<p>if <code>OOB=TRUE</code>, the out-of-bag estimate of
misclassification or root mean squared error or the Brier score for censored
data.</p>
</td></tr>
<tr><td><code>comb</code></td>
<td>
<p>logical whether a combination of models was requested.</p>
</td></tr>
</table>
<p>For each class methods for the generics <code><a href="rpart.html#topic+prune.rpart">prune.rpart</a></code>, 
<code><a href="#topic+print">print</a></code>, <code><a href="base.html#topic+summary">summary</a></code> and <code><a href="stats.html#topic+predict">predict</a></code> are
available for inspection of the results and prediction, for example:
<code><a href="#topic+print.classbagg">print.classbagg</a></code>, <code><a href="#topic+summary.classbagg">summary.classbagg</a></code>, 
<code><a href="#topic+predict.classbagg">predict.classbagg</a></code>  and <code><a href="#topic+prune.classbagg">prune.classbagg</a></code> for
classification problems.
</p>


<h3>References</h3>

 
<p>Leo Breiman (1996a), Bagging Predictors. <em>Machine Learning</em>
<b>24</b>(2), 123&ndash;140.
</p>
<p>Leo Breiman (1996b), Out-Of-Bag Estimation. <em>Technical Report</em>
<a href="https://www.stat.berkeley.edu/~breiman/OOBestimation.pdf">https://www.stat.berkeley.edu/~breiman/OOBestimation.pdf</a>.
</p>
<p>Leo Breiman (1998), Arcing Classifiers. <em>The Annals of Statistics</em>
<b>26</b>(3), 801&ndash;824.
</p>
<p>Peter Buehlmann and Bin Yu (2002), Analyzing Bagging. <em>The Annals of
Statistics</em> <b>30</b>(4), 927&ndash;961.
</p>
<p>Torsten Hothorn and Berthold Lausen (2003), Double-Bagging: Combining
classifiers by bootstrap aggregation. <em>Pattern Recognition</em>,
<b>36</b>(6), 1303&ndash;1309. 
</p>
<p>Torsten Hothorn and Berthold Lausen (2005), Bundling Classifiers by Bagging
Trees. <em>Computational Statistics &amp; Data Analysis</em>, 49, 1068&ndash;1078.
</p>
<p>Torsten Hothorn, Berthold Lausen, Axel Benner and Martin
Radespiel-Troeger (2004), Bagging Survival Trees.
<em>Statistics in Medicine</em>, <b>23</b>(1), 77&ndash;91.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library("MASS")
library("survival")

# Classification: Breast Cancer data

data("BreastCancer", package = "mlbench")

# Test set error bagging (nbagg = 50): 3.7% (Breiman, 1998, Table 5)

mod &lt;- bagging(Class ~ Cl.thickness + Cell.size
                + Cell.shape + Marg.adhesion   
                + Epith.c.size + Bare.nuclei   
                + Bl.cromatin + Normal.nucleoli
                + Mitoses, data=BreastCancer, coob=TRUE)
print(mod)

# Test set error bagging (nbagg=50): 7.9% (Breiman, 1996a, Table 2)
data("Ionosphere", package = "mlbench")
Ionosphere$V2 &lt;- NULL # constant within groups

bagging(Class ~ ., data=Ionosphere, coob=TRUE)

# Double-Bagging: combine LDA and classification trees

# predict returns the linear discriminant values, i.e. linear combinations
# of the original predictors

comb.lda &lt;- list(list(model=lda, predict=function(obj, newdata)
                                 predict(obj, newdata)$x))

# Note: out-of-bag estimator is not available in this situation, use
# errorest

mod &lt;- bagging(Class ~ ., data=Ionosphere, comb=comb.lda) 

predict(mod, Ionosphere[1:10,])

# Regression:

data("BostonHousing", package = "mlbench")

# Test set error (nbagg=25, trees pruned): 3.41 (Breiman, 1996a, Table 8)

mod &lt;- bagging(medv ~ ., data=BostonHousing, coob=TRUE)
print(mod)

library("mlbench")
learn &lt;- as.data.frame(mlbench.friedman1(200))

# Test set error (nbagg=25, trees pruned): 2.47 (Breiman, 1996a, Table 8)

mod &lt;- bagging(y ~ ., data=learn, coob=TRUE)
print(mod)

# Survival data

# Brier score for censored data estimated by 
# 10 times 10-fold cross-validation: 0.2 (Hothorn et al,
# 2002)

data("DLBCL", package = "ipred")
mod &lt;- bagging(Surv(time,cens) ~ MGEc.1 + MGEc.2 + MGEc.3 + MGEc.4 + MGEc.5 +
                                 MGEc.6 + MGEc.7 + MGEc.8 + MGEc.9 +
                                 MGEc.10 + IPI, data=DLBCL, coob=TRUE)

print(mod)


</code></pre>

<hr>
<h2 id='bootest'>Bootstrap Error Rate Estimators</h2><span id='topic+bootest'></span><span id='topic+bootest.default'></span><span id='topic+bootest.factor'></span><span id='topic+bootest.numeric'></span><span id='topic+bootest.integer'></span><span id='topic+bootest.Surv'></span>

<h3>Description</h3>

<p>Those functions are low-level functions used by <code><a href="#topic+errorest">errorest</a></code> and
are normally not called by users.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'factor'
bootest(y, formula, data, model, predict, nboot=25,
bc632plus=FALSE, list.tindx = NULL, predictions = FALSE, 
both.boot = FALSE, ...)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bootest_+3A_y">y</code></td>
<td>
<p>the response variable, either of class <code>factor</code>
(classification), <code>numeric</code> (regression) or <code>Surv</code> (survival).</p>
</td></tr>
<tr><td><code id="bootest_+3A_formula">formula</code></td>
<td>
<p>a formula object.</p>
</td></tr>
<tr><td><code id="bootest_+3A_data">data</code></td>
<td>
<p>data frame of predictors and response described in
<code>formula</code>.</p>
</td></tr>   
<tr><td><code id="bootest_+3A_model">model</code></td>
<td>
<p>a function implementing the predictive model to be
evaluated. The function <code>model</code> can either return an
object representing a fitted model or a function with
argument <code>newdata</code> which returns predicted values. In
this case, the <code>predict</code> argument to <code>errorest</code> is
ignored.</p>
</td></tr>
<tr><td><code id="bootest_+3A_predict">predict</code></td>
<td>
<p>a function with arguments <code>object</code> and <code>newdata</code>
only which predicts the status of the observations in <code>newdata</code> based
on the fitted model in <code>object</code>.</p>
</td></tr>
<tr><td><code id="bootest_+3A_nboot">nboot</code></td>
<td>
<p>number of bootstrap replications to be used.</p>
</td></tr>
<tr><td><code id="bootest_+3A_bc632plus">bc632plus</code></td>
<td>
<p>logical. Should the bias corrected version of misclassification
error be computed?</p>
</td></tr>
<tr><td><code id="bootest_+3A_predictions">predictions</code></td>
<td>
<p>logical, return a matrix of predictions. The ith column
contains predictions of the ith out-of-bootstrap sample and 'NA's
corresponding to the ith bootstrap sample.</p>
</td></tr>
<tr><td><code id="bootest_+3A_list.tindx">list.tindx</code></td>
<td>
<p>list of numeric vectors, indicating which
observations are included in each bootstrap sample.</p>
</td></tr>
<tr><td><code id="bootest_+3A_both.boot">both.boot</code></td>
<td>
<p>logical, return both (bootstrap and 632plus) estimations or
only one of them.</p>
</td></tr>
<tr><td><code id="bootest_+3A_...">...</code></td>
<td>
<p>additional arguments to <code>model</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>See <code><a href="#topic+errorest">errorest</a></code>.
</p>

<hr>
<h2 id='control.errorest'> Control Error Rate Estimators </h2><span id='topic+control.errorest'></span>

<h3>Description</h3>

<p>Some parameters that control the behaviour of <code><a href="#topic+errorest">errorest</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>control.errorest(k = 10, nboot = 25, strat = FALSE, random = TRUE, 
                 predictions = FALSE, getmodels=FALSE, list.tindx = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="control.errorest_+3A_k">k</code></td>
<td>
<p>integer, specify $k$ for $k$-fold cross-validation.</p>
</td></tr>
<tr><td><code id="control.errorest_+3A_nboot">nboot</code></td>
<td>
<p>integer, number of bootstrap replications.</p>
</td></tr>
<tr><td><code id="control.errorest_+3A_strat">strat</code></td>
<td>
<p>logical, if <code>TRUE</code>, cross-validation is performed 
using stratified sampling (for classification problems).</p>
</td></tr>
<tr><td><code id="control.errorest_+3A_random">random</code></td>
<td>
<p>logical, if <code>TRUE</code>, cross-validation is performed using
a random ordering of the data.</p>
</td></tr>
<tr><td><code id="control.errorest_+3A_predictions">predictions</code></td>
<td>
<p>logical, indicates whether the prediction
for each observation should be returned or not
(classification and regression only). For a bootstrap
based estimator a matrix of size 'number of observations' times nboot is
returned with predicted values of the ith out-of-bootstrap sample in column
i and 'NA's for those observations not included in the ith out-of-bootstrap 
sample.</p>
</td></tr>
<tr><td><code id="control.errorest_+3A_getmodels">getmodels</code></td>
<td>
<p>logical, indicates a list of all models should be
returned. For cross-validation only.</p>
</td></tr>
<tr><td><code id="control.errorest_+3A_list.tindx">list.tindx</code></td>
<td>
<p>list of numeric vectors, indicating which
observations are included in each bootstrap or cross-validation sample, respectively.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with the same components as arguments. 
</p>

<hr>
<h2 id='cv'>Cross-validated Error Rate Estimators.</h2><span id='topic+cv'></span><span id='topic+cv.default'></span><span id='topic+cv.factor'></span><span id='topic+cv.numeric'></span><span id='topic+cv.integer'></span><span id='topic+cv.Surv'></span>

<h3>Description</h3>

<p>Those functions are low-level functions used by <code><a href="#topic+errorest">errorest</a></code> and
are normally not called by users.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'factor'
cv(y, formula, data, model, predict, k=10, random=TRUE, 
            strat=FALSE,
            predictions=NULL, getmodels=NULL, list.tindx = NULL, ...) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cv_+3A_y">y</code></td>
<td>
<p>response variable, either of class <code>factor</code>
(classification), <code>numeric</code> (regression) or <code>Surv</code> (survival).</p>
</td></tr>
<tr><td><code id="cv_+3A_formula">formula</code></td>
<td>
<p>a formula object.</p>
</td></tr>
<tr><td><code id="cv_+3A_data">data</code></td>
<td>
<p>data frame of predictors and response described in <code>formula</code>.</p>
</td></tr>
<tr><td><code id="cv_+3A_model">model</code></td>
<td>
<p>a function implementing the predictive model to be
evaluated. The function <code>model</code> can either return an
object representing a fitted model or a function with
argument <code>newdata</code> which returns predicted values. In
this case, the <code>predict</code> argument to <code>errorest</code> is
ignored.</p>
</td></tr>
<tr><td><code id="cv_+3A_predict">predict</code></td>
<td>
<p>a function with arguments <code>object</code> and <code>newdata</code>
only which predicts the status of the observations in <code>newdata</code> based
on the fitted model in <code>object</code>.</p>
</td></tr>
<tr><td><code id="cv_+3A_k">k</code></td>
<td>
<p>k-fold cross-validation.</p>
</td></tr>
<tr><td><code id="cv_+3A_random">random</code></td>
<td>
<p>logical, indicates whether a random order or the given
order of the data should be used for sample splitting or not, defaults to
<code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="cv_+3A_strat">strat</code></td>
<td>
<p>logical, stratified sampling or not, defaults to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="cv_+3A_predictions">predictions</code></td>
<td>
<p>logical, return the prediction of each observation.</p>
</td></tr>
<tr><td><code id="cv_+3A_getmodels">getmodels</code></td>
<td>
<p>logical, return a list of models for each fold.</p>
</td></tr>
<tr><td><code id="cv_+3A_list.tindx">list.tindx</code></td>
<td>
<p>list of numeric vectors, indicating which
observations are included in each cross-validation sample.</p>
</td></tr>
<tr><td><code id="cv_+3A_...">...</code></td>
<td>
<p>additional arguments to <code>model</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>See <code><a href="#topic+errorest">errorest</a></code>.
</p>

<hr>
<h2 id='DLBCL'> Diffuse Large B-Cell Lymphoma </h2><span id='topic+DLBCL'></span>

<h3>Description</h3>

<p>A data frame with gene expression data from diffuse large B-cell
lymphoma (DLBCL) patients.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("DLBCL")</code></pre>


<h3>Format</h3>

<p>This data frame contains the following columns:
</p>

<dl>
<dt>DLCL.Sample</dt><dd><p>DLBCL identifier.</p>
</dd>
<dt>Gene.Expression</dt><dd><p>Gene expression group.</p>
</dd>
<dt>time</dt><dd><p>survival time in month.</p>
</dd>
<dt>cens</dt><dd><p>censoring: 0 censored, 1 dead.</p>
</dd>
<dt>IPI</dt><dd><p>International prognostic index.</p>
</dd>
<dt>MGEc.1</dt><dd><p>mean gene expression in cluster 1.</p>
</dd>
<dt>MGEc.2</dt><dd><p>mean gene expression in cluster 2.</p>
</dd>
<dt>MGEc.3</dt><dd><p>mean gene expression in cluster 3.</p>
</dd>
<dt>MGEc.4</dt><dd><p>mean gene expression in cluster 4.</p>
</dd>
<dt>MGEc.5</dt><dd><p>mean gene expression in cluster 5.</p>
</dd>
<dt>MGEc.6</dt><dd><p>mean gene expression in cluster 6.</p>
</dd>
<dt>MGEc.7</dt><dd><p>mean gene expression in cluster 7.</p>
</dd>
<dt>MGEc.8</dt><dd><p>mean gene expression in cluster 8.</p>
</dd>
<dt>MGEc.9</dt><dd><p>mean gene expression in cluster 9.</p>
</dd>
<dt>MGEc.10</dt><dd><p>mean gene expression in cluster 10.</p>
</dd>
</dl>



<h3>Source</h3>

<p>Except of <code>MGE</code>, the data is published at
<a href="http://llmpp.nih.gov/lymphoma/data.shtml">http://llmpp.nih.gov/lymphoma/data.shtml</a>. <code>MGEc.*</code> is the mean of
the gene expression in each of ten clusters derived by agglomerative average
linkage hierarchical cluster analysis (Hothorn et al., 2002).
</p>


<h3>References</h3>

<p>Ash A. Alizadeh et. al (2000), Distinct types of diffuse large
B-cell lymphoma identified by gene
expression profiling. <em>Nature</em>, <b>403</b>, 504&ndash;509.
</p>
<p>Torsten Hothorn, Berthold Lausen, Axel Benner and Martin
Radespiel-Troeger (2004), Bagging Survival Trees. 
<em>Statistics in Medicine</em>, <b>23</b>, 77&ndash;91.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
suppressWarnings(RNGversion("3.5.3"))
set.seed(290875)

data("DLBCL", package="ipred")
library("survival")
survfit(Surv(time, cens) ~ 1, data=DLBCL)

</code></pre>

<hr>
<h2 id='dystrophy'>Detection of muscular dystrophy carriers.</h2><span id='topic+dystrophy'></span>

<h3>Description</h3>

<p>The <code>dystrophy</code> data frame has 209 rows and 10 columns.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(dystrophy)</code></pre>


<h3>Format</h3>

<p>This data frame contains the following columns:
</p>

<dl>
<dt>OBS</dt><dd><p>numeric. Observation number.</p>
</dd>
<dt>HospID</dt><dd><p>numeric. Hospital ID number.</p>
</dd>
<dt>AGE</dt><dd><p>numeric, age in years.</p>
</dd>
<dt>M</dt><dd><p>numeric. Month of examination.</p>
</dd>
<dt>Y</dt><dd><p>numeric. Year of examination.</p>
</dd>
<dt>CK</dt><dd><p>numeric. Serum marker creatine kinase.</p>
</dd>
<dt>H</dt><dd><p>numeric. Serum marker hemopexin.</p>
</dd>
<dt>PK</dt><dd><p>numeric. Serum marker pyruvate kinase.</p>
</dd>
<dt>LD</dt><dd><p>numeric. Serum marker lactate dehydroginase.</p>
</dd>
<dt>Class</dt><dd><p>factor with levels, <code>carrier</code> and <code>normal</code>.</p>
</dd>
</dl>



<h3>Details</h3>

<p>Duchenne Muscular Dystrophy (DMD) is a genetically transmitted disease,
passed from a mother to her children. Affected female offspring usually suffer
no apparent symptoms, male offspring with the disease die at young age.
Although female carriers have no physical symptoms they tend to exhibit
elevated levels of certain serum enzymes or proteins.
<br />
The dystrophy dataset contains 209 observations of 75 female DMD carriers and
134 female DMD non-carrier. It includes 6 variables describing age of the
female and the serum parameters serum marker creatine kinase (CK), serum marker
hemopexin (H), serum marker pyruvate kinase (PK) and serum marker lactate
dehydroginase (LD). The serum markers CK and H may be measured rather
inexpensive from frozen serum, PK and LD requires fresh serum.
</p>


<h3>Source</h3>

<p>D.Andrews and A. Herzberg (1985), Data. Berlin: Springer-Verlag.
</p>


<h3>References</h3>

<p>Robert Tibshirani and Geoffry Hinton (1998), Coaching variables for regression and classification. Statistics and Computing 8, 25-33.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

data("dystrophy")
library("rpart")
errorest(Class~CK+H~AGE+PK+LD, data = dystrophy, model = inbagg, 
pFUN = list(list(model = lm, predict = mypredict.lm), list(model = rpart)), 
ns = 0.75, estimator = "cv")

## End(Not run)
</code></pre>

<hr>
<h2 id='errorest'> Estimators of Prediction Error </h2><span id='topic+errorest'></span><span id='topic+errorest.data.frame'></span><span id='topic+errorest.default'></span>

<h3>Description</h3>

<p>Resampling based estimates of prediction error: misclassification error, 
root mean squared error or Brier score for survival data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'data.frame'
errorest(formula, data, subset, na.action=na.omit, 
         model=NULL, predict=NULL,
         estimator=c("cv", "boot", "632plus"), 
         est.para=control.errorest(), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="errorest_+3A_formula">formula</code></td>
<td>
<p>a formula of the form <code>lhs ~ rhs</code>. 
Either describing the model of explanatory and 
response variables in the usual way (see <code><a href="stats.html#topic+lm">lm</a></code>) 
or the model between explanatory and intermediate variables
in the framework of indirect classification, 
see <code><a href="#topic+inclass">inclass</a></code>.</p>
</td></tr>
<tr><td><code id="errorest_+3A_data">data</code></td>
<td>
<p>a data frame containing the variables in the model formula 
and additionally the class membership variable 
if <code>model = inclass</code>. <code>data</code> is required for
indirect classification, otherwise <code>formula</code> is evaluated
in the calling environment.</p>
</td></tr>
<tr><td><code id="errorest_+3A_subset">subset</code></td>
<td>
<p>optional vector, specifying a subset of observations to 
be used.</p>
</td></tr>
<tr><td><code id="errorest_+3A_na.action">na.action</code></td>
<td>
<p>function which indicates what should happen when the data
contains <code>NA</code>'s, defaults to <code><a href="stats.html#topic+na.omit">na.omit</a></code>.</p>
</td></tr> 
<tr><td><code id="errorest_+3A_model">model</code></td>
<td>
<p>function. Modelling technique whose error rate is to be 
estimated. The function <code>model</code> can either return an 
object representing a fitted model or a function with
argument <code>newdata</code> which returns predicted values. In
this case, the <code>predict</code> argument to <code>errorest</code> is
ignored.</p>
</td></tr>
<tr><td><code id="errorest_+3A_predict">predict</code></td>
<td>
<p>function. Prediction method to be used. The vector of 
predicted values must have the same length as the the 
number of to-be-predicted observations. Predictions 
corresponding to missing data must be replaced by <code>NA</code>.
Additionally, <code>predict</code> has to return predicted values 
comparable to the responses (that is: factors for 
classification problems). See the example on how to make
this sure for any predictor.</p>
</td></tr>
<tr><td><code id="errorest_+3A_estimator">estimator</code></td>
<td>
<p>estimator of the misclassification error: 
<code>cv</code> cross-validation, <code>boot</code> bootstrap or 
<code>632plus</code> bias corrected bootstrap (classification
only). </p>
</td></tr>
<tr><td><code id="errorest_+3A_est.para">est.para</code></td>
<td>
<p>a list of additional parameters that control the
calculation of the estimator, see 
<code><a href="#topic+control.errorest">control.errorest</a></code> for details.</p>
</td></tr>
<tr><td><code id="errorest_+3A_...">...</code></td>
<td>
<p>additional parameters to <code>model</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The prediction error for classification and regression models as well as
predictive models for censored data using cross-validation or the 
bootstrap can be computed by <code>errorest</code>. For classification problems,
the estimated misclassification error is returned. The root mean squared
error is computed for regression problems and the Brier score for censored
data (Graf et al., 1999) is reported if the response is censored. 
</p>
<p>Any model can be specified as long as it is a function with arguments
<code>model(formula, data, subset, na.action, ...)</code>. If 
a method <code>predict.model(object, newdata, ...)</code> is available,
<code>predict</code> does not need to be specified. However, <code>predict</code> 
has to return predicted values in the same order and of the same length
corresponding to the response. See the examples below. 
</p>
<p>$k$-fold cross-validation and the usual bootstrap estimator with
<code>est.para$nboot</code> bootstrap replications can be computed for
all kind of problems. The bias corrected .632+ bootstrap
by Efron and Tibshirani (1997) is available for classification problems
only. Use <code><a href="#topic+control.errorest">control.errorest</a></code> to specify additional arguments.
</p>
<p><code>errorest</code> is a formula based interface to the generic functions 
<code><a href="#topic+cv">cv</a></code> or <code><a href="#topic+bootest">bootest</a></code> which implement methods for
classification, regression and survival problems.
</p>


<h3>Value</h3>

<p>The class of the object returned depends on the class of the response
variable and the estimator used. In each case, it is a list with an element
<code>error</code> and additional information. <code>print</code> methods are available
for the inspection of the results.
</p>


<h3>References</h3>

<p>Brian D. Ripley (1996), <em>Pattern Recognition and Neural Networks</em>.
Cambridge: Cambridge University Press.
</p>
<p>Bradley Efron and Robert Tibshirani (1997),
Improvements on Cross-Validation: The .632+ Bootstrap Estimator.
<em>Journal of the American Statistical Association</em> <b>92</b>(438),
548&ndash;560.
</p>
<p>Erika Graf, Claudia Schmoor, Willi Sauerbrei and Martin Schumacher (1999), 
Assessment and comparison of prognostic classification schemes for 
survival data. <em>Statistics in Medicine</em> <b>18</b>(17-18), 2529&ndash;2545.
</p>
<p>Rosa A. Schiavo and David J. Hand (2000), Ten More Years of Error Rate
Research. <em>International Statistical Review</em> <b>68</b>(3), 296-310. 
</p>
<p>David J. Hand, Hua Gui Li, Niall M. Adams (2001),
Supervised Classification with Structured Class Definitions.
<em>Computational Statistics &amp; Data Analysis</em> <b>36</b>,
209&ndash;225.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Classification

data("iris")
library("MASS")

# force predict to return class labels only
mypredict.lda &lt;- function(object, newdata)
  predict(object, newdata = newdata)$class

# 10-fold cv of LDA for Iris data
errorest(Species ~ ., data=iris, model=lda, 
         estimator = "cv", predict= mypredict.lda)

data("PimaIndiansDiabetes", package = "mlbench")
## Not run: 
# 632+ bootstrap of LDA for Diabetes data
errorest(diabetes ~ ., data=PimaIndiansDiabetes, model=lda,
         estimator = "632plus", predict= mypredict.lda)

## End(Not run)

#cv of a fixed partition of the data
list.tindx &lt;- list(1:100, 101:200, 201:300, 301:400, 401:500,
        501:600, 601:700, 701:768)

errorest(diabetes ~ ., data=PimaIndiansDiabetes, model=lda,
          estimator = "cv", predict = mypredict.lda,
          est.para = control.errorest(list.tindx = list.tindx))

## Not run: 
#both bootstrap estimations based on fixed partitions

list.tindx &lt;- vector(mode = "list", length = 25)
for(i in 1:25) {
  list.tindx[[i]] &lt;- sample(1:768, 768, TRUE)
}

errorest(diabetes ~ ., data=PimaIndiansDiabetes, model=lda,
          estimator = c("boot", "632plus"), predict= mypredict.lda,
          est.para = control.errorest(list.tindx = list.tindx))


## End(Not run)
data("Glass", package = "mlbench")

# LDA has cross-validated misclassification error of
# 38% (Ripley, 1996, page 98)

# Pruned trees about 32% (Ripley, 1996, page 230)

# use stratified sampling here, i.e. preserve the class proportions
errorest(Type ~ ., data=Glass, model=lda, 
         predict=mypredict.lda, est.para=control.errorest(strat=TRUE))

# force predict to return class labels
mypredict.rpart &lt;- function(object, newdata)
  predict(object, newdata = newdata,type="class")

library("rpart")
pruneit &lt;- function(formula, ...)
  prune(rpart(formula, ...), cp =0.01)

errorest(Type ~ ., data=Glass, model=pruneit,
         predict=mypredict.rpart, est.para=control.errorest(strat=TRUE))

# compute sensitivity and specifity for stabilised LDA

data("GlaucomaM", package = "TH.data")

error &lt;- errorest(Class ~ ., data=GlaucomaM, model=slda,
  predict=mypredict.lda, est.para=control.errorest(predictions=TRUE))

# sensitivity 

mean(error$predictions[GlaucomaM$Class == "glaucoma"] == "glaucoma")

# specifity

mean(error$predictions[GlaucomaM$Class == "normal"] == "normal")

# Indirect Classification: Smoking data

data(Smoking)
# Set three groups of variables:
# 1) explanatory variables are: TarY, NicY, COY, Sex, Age
# 2) intermediate variables are: TVPS, BPNL, COHB
# 3) response (resp) is defined by:

resp &lt;- function(data){
  data &lt;- data[, c("TVPS", "BPNL", "COHB")]
  res &lt;- t(t(data) &gt; c(4438, 232.5, 58))
  res &lt;- as.factor(ifelse(apply(res, 1, sum) &gt; 2, 1, 0))
  res
}

response &lt;- resp(Smoking[ ,c("TVPS", "BPNL", "COHB")])
smoking &lt;- cbind(Smoking, response)

formula &lt;- response~TVPS+BPNL+COHB~TarY+NicY+COY+Sex+Age

# Estimation per leave-one-out estimate for the misclassification is 
# 36.36% (Hand et al., 2001), using indirect classification with 
# linear models
## Not run: 
errorest(formula, data = smoking, model = inclass,estimator = "cv", 
         pFUN = list(list(model=lm, predict = mypredict.lm)), cFUN = resp,  
         est.para=control.errorest(k=nrow(smoking)))

## End(Not run)

# Regression

data("BostonHousing", package = "mlbench")

# 10-fold cv of lm for Boston Housing data
errorest(medv ~ ., data=BostonHousing, model=lm,
         est.para=control.errorest(random=FALSE))

# the same, with "model" returning a function for prediction
# instead of an object of class "lm"

mylm &lt;- function(formula, data) {
  mod &lt;- lm(formula, data)
  function(newdata) predict(mod, newdata)
}

errorest(medv ~ ., data=BostonHousing, model=mylm,
est.para=control.errorest(random=FALSE))


# Survival data

data("GBSG2", package = "TH.data")
library("survival")

# prediction is fitted Kaplan-Meier
predict.survfit &lt;- function(object, newdata) object

# 5-fold cv of Kaplan-Meier for GBSG2 study
errorest(Surv(time, cens) ~ 1, data=GBSG2, model=survfit,
         predict=predict.survfit, est.para=control.errorest(k=5))


</code></pre>

<hr>
<h2 id='GlaucomaMVF'> Glaucoma Database </h2><span id='topic+GlaucomaMVF'></span>

<h3>Description</h3>

<p>The <code>GlaucomaMVF</code> data has 170 observations in two classes. 
66 predictors are derived from a confocal laser scanning image of the 
optic nerve head, from a visual field test, a fundus photography and a 
measurement of the intra occular pressure. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("GlaucomaMVF")</code></pre>


<h3>Format</h3>

<p>This data frame contains the following predictors describing the
morphology of the optic nerve head, the visual field, the intra 
occular pressure and a membership variable:
</p>

<dl>
<dt>ag</dt><dd><p>area global.</p>
</dd>
<dt>at</dt><dd><p>area temporal.</p>
</dd>
<dt>as</dt><dd><p>area superior.</p>
</dd>
<dt>an</dt><dd><p>area nasal.</p>
</dd>
<dt>ai</dt><dd><p>area inferior.</p>
</dd>
<dt>eag</dt><dd><p>effective area global.</p>
</dd>
<dt>eat</dt><dd><p>effective area temporal.</p>
</dd>
<dt>eas</dt><dd><p>effective area superior.</p>
</dd>
<dt>ean</dt><dd><p>effective area nasal.</p>
</dd>
<dt>eai</dt><dd><p>effective area inferior.</p>
</dd>
<dt>abrg</dt><dd><p>area below reference global.</p>
</dd>
<dt>abrt</dt><dd><p>area below reference temporal.</p>
</dd>
<dt>abrs</dt><dd><p>area below reference superior.</p>
</dd>
<dt>abrn</dt><dd><p>area below reference nasal.</p>
</dd>
<dt>abri</dt><dd><p>area below reference inferior.</p>
</dd>
<dt>hic</dt><dd><p>height in contour.</p>
</dd>
<dt>mhcg</dt><dd><p>mean height contour global.</p>
</dd>
<dt>mhct</dt><dd><p>mean height contour temporal.</p>
</dd>
<dt>mhcs</dt><dd><p>mean height contour superior.</p>
</dd>
<dt>mhcn</dt><dd><p>mean height contour nasal.</p>
</dd>
<dt>mhci</dt><dd><p>mean height contour inferior.</p>
</dd>
<dt>phcg</dt><dd><p>peak height contour.</p>
</dd>
<dt>phct</dt><dd><p>peak height contour temporal.</p>
</dd>
<dt>phcs</dt><dd><p>peak height contour superior.</p>
</dd>
<dt>phcn</dt><dd><p>peak height contour nasal.</p>
</dd>
<dt>phci</dt><dd><p>peak height contour inferior.</p>
</dd>
<dt>hvc</dt><dd><p>height variation contour.</p>
</dd>
<dt>vbsg</dt><dd><p>volume below surface global.</p>
</dd>
<dt>vbst</dt><dd><p>volume below surface temporal.</p>
</dd>
<dt>vbss</dt><dd><p>volume below surface superior.</p>
</dd>
<dt>vbsn</dt><dd><p>volume below surface nasal.</p>
</dd>
<dt>vbsi</dt><dd><p>volume below surface inferior.</p>
</dd>
<dt>vasg</dt><dd><p>volume above surface global.</p>
</dd>
<dt>vast</dt><dd><p>volume above surface temporal.</p>
</dd>
<dt>vass</dt><dd><p>volume above surface superior.</p>
</dd>
<dt>vasn</dt><dd><p>volume above surface nasal.</p>
</dd>
<dt>vasi</dt><dd><p>volume above surface inferior.</p>
</dd>
<dt>vbrg</dt><dd><p>volume below reference global.</p>
</dd>
<dt>vbrt</dt><dd><p>volume below reference temporal.</p>
</dd>
<dt>vbrs</dt><dd><p>volume below reference superior.</p>
</dd>
<dt>vbrn</dt><dd><p>volume below reference nasal.</p>
</dd>
<dt>vbri</dt><dd><p>volume below reference inferior.</p>
</dd>
<dt>varg</dt><dd><p>volume above reference global.</p>
</dd>
<dt>vart</dt><dd><p>volume above reference temporal.</p>
</dd>
<dt>vars</dt><dd><p>volume above reference superior.</p>
</dd>
<dt>varn</dt><dd><p>volume above reference nasal.</p>
</dd>
<dt>vari</dt><dd><p>volume above reference inferior.</p>
</dd>
<dt>mdg</dt><dd><p>mean depth global.</p>
</dd>
<dt>mdt</dt><dd><p>mean depth temporal.</p>
</dd>
<dt>mds</dt><dd><p>mean depth superior.</p>
</dd>
<dt>mdn</dt><dd><p>mean depth nasal.</p>
</dd>
<dt>mdi</dt><dd><p>mean depth inferior.</p>
</dd>
<dt>tmg</dt><dd><p>third moment global.</p>
</dd>
<dt>tmt</dt><dd><p>third moment temporal.</p>
</dd>
<dt>tms</dt><dd><p>third moment superior.</p>
</dd>
<dt>tmn</dt><dd><p>third moment nasal.</p>
</dd>
<dt>tmi</dt><dd><p>third moment inferior.</p>
</dd>
<dt>mr</dt><dd><p>mean radius.</p>
</dd>
<dt>rnf</dt><dd><p>retinal nerve fiber thickness.</p>
</dd>
<dt>mdic</dt><dd><p>mean depth in contour.</p>
</dd>
<dt>emd</dt><dd><p>effective mean depth.</p>
</dd>
<dt>mv</dt><dd><p>mean variability.</p>
</dd>
<dt>tension</dt><dd><p>intra occular pressure.</p>
</dd>
<dt>clv</dt><dd><p>corrected loss variance, variability of the visual field.</p>
</dd>
<dt>cs</dt><dd><p>contrast sensitivity of the visual field.</p>
</dd>
<dt>lora</dt><dd><p>loss of rim area, measured by fundus photography.</p>
</dd>
<dt>Class</dt><dd><p>a factor with levels <code>glaucoma</code> and <code>normal</code>.</p>
</dd>
</dl>



<h3>Details</h3>

<p>Confocal laser images of the eye background are taken with the 
Heidelberg Retina Tomograph and variables 1-62 are derived. 
Most of these variables describe either the area or volume in 
certain parts of the papilla and are measured in 
four sectors (temporal, superior, nasal and inferior) as well 
as for the whole papilla (global). The global measurement is, 
roughly, the sum of the measurements taken in the four sector.
</p>
<p>The perimeter &lsquo;Octopus&rsquo; measures the visual field variables <code>clv</code> 
and <code>cs</code>, stereo optic disks photographs were taken with a 
telecentric fundus camera and <code>lora</code> is derived.
</p>
<p>Observations of both groups are matched by age and sex, 
to prevent for possible confounding. 
</p>


<h3>Note</h3>

<p><code>GLaucomMVF</code> overlaps in some parts with <code><a href="TH.data.html#topic+GlaucomaM">GlaucomaM</a></code>.
</p>


<h3>Source</h3>

<p>Andrea Peters, Berthold Lausen, Georg Michelson and Olaf Gefeller (2003),
Diagnosis of glaucoma by indirect classifiers.
<em>Methods of Information in Medicine</em> <b>1</b>, 99-103.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

data("GlaucomaMVF", package = "ipred")
library("rpart")

response &lt;- function (data) {
  attach(data)
  res &lt;- ifelse((!is.na(clv) &amp; !is.na(lora) &amp; clv &gt;= 5.1 &amp; lora &gt;= 
        49.23372) | (!is.na(clv) &amp; !is.na(lora) &amp; !is.na(cs) &amp; 
        clv &lt; 5.1 &amp; lora &gt;= 58.55409 &amp; cs &lt; 1.405) | (is.na(clv) &amp; 
        !is.na(lora) &amp; !is.na(cs) &amp; lora &gt;= 58.55409 &amp; cs &lt; 1.405) | 
        (!is.na(clv) &amp; is.na(lora) &amp; cs &lt; 1.405), 0, 1)
  detach(data)
  factor (res, labels = c("glaucoma", "normal"))
}

errorest(Class~clv+lora+cs~., data = GlaucomaMVF, model=inclass, 
       estimator="cv", pFUN = list(list(model = rpart)), cFUN = response)

## End(Not run)
</code></pre>

<hr>
<h2 id='inbagg'>Indirect Bagging</h2><span id='topic+inbagg'></span><span id='topic+inbagg.default'></span><span id='topic+inbagg.data.frame'></span>

<h3>Description</h3>

<p>Function to perform the indirect bagging and subagging.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'data.frame'
inbagg(formula, data, pFUN=NULL, 
  cFUN=list(model = NULL, predict = NULL, training.set = NULL), 
  nbagg = 25, ns = 0.5, replace = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="inbagg_+3A_formula">formula</code></td>
<td>
<p>formula. A <code>formula</code> specified as <code>y~w1+w2+w3~x1+x2+x3</code> describes how to model the intermediate variables <code>w1, w2, w3</code> and the response variable <code>y</code>, if no other formula is specified by the elements of <code>pFUN</code> or in <code>cFUN</code></p>
</td></tr>
<tr><td><code id="inbagg_+3A_data">data</code></td>
<td>
<p>data frame of explanatory, intermediate and response variables.</p>
</td></tr>
<tr><td><code id="inbagg_+3A_pfun">pFUN</code></td>
<td>
<p>list of lists, which describe models for the intermediate variables, details are given below.</p>
</td></tr>
<tr><td><code id="inbagg_+3A_cfun">cFUN</code></td>
<td>
<p>either a fixed function with argument <code>newdata</code> and returning the class membership by default, or a list specifying a classifying model, similar to one element of <code>pFUN</code>. Details are given below.</p>
</td></tr>
<tr><td><code id="inbagg_+3A_nbagg">nbagg</code></td>
<td>
<p>number of bootstrap samples.</p>
</td></tr>
<tr><td><code id="inbagg_+3A_ns">ns</code></td>
<td>
<p>proportion of sample to be drawn from the learning
sample. By default, subagging with 50% is performed, i.e. draw
0.5*n out of n without replacement.</p>
</td></tr>
<tr><td><code id="inbagg_+3A_replace">replace</code></td>
<td>
<p>logical. Draw with or without replacement.</p>
</td></tr>
<tr><td><code id="inbagg_+3A_...">...</code></td>
<td>
<p>additional arguments (e.g. <code>subset</code>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A given data set is subdivided into three types of variables: explanatory, intermediate and response variables.<br />
</p>
<p>Here, each specified intermediate variable is modelled separately
following <code>pFUN</code>, a list of lists with elements specifying an
arbitrary number of models for the intermediate variables and an
optional element <code>training.set = c("oob", "bag", "all")</code>. The
element <code>training.set</code> determines whether, predictive models for
the intermediate are calculated based on the out-of-bag sample
(<code>"oob"</code>), the default, on the bag sample (<code>"bag"</code>) or on all
available observations (<code>"all"</code>). The elements of <code>pFUN</code>,
specifying the models for the intermediate variables are lists as
described in <code><a href="#topic+inclass">inclass</a></code>.
Note that, if no formula is given in these elements, the functional
relationship of <code>formula</code> is used.<br />
</p>
<p>The response variable is modelled following <code>cFUN</code>.
This can either be a fixed classifying function as described in Peters
et al. (2003) or a list,
which specifies the  modelling technique to be applied. The list
contains the arguments <code>model</code> (which model to be fitted),
<code>predict</code> (optional, how to predict), <code>formula</code> (optional, of
type <code>y~w1+w2+w3+x1+x2</code> determines the variables the classifying
function is based on) and the optional argument <code>training.set =
  c("fitted.bag", "original", "fitted.subset")</code>
specifying whether the classifying function is trained on the predicted
observations of the bag sample (<code>"fitted.bag"</code>),
on the original observations (<code>"original"</code>) or on the
predicted observations not included in a defined subset
(<code>"fitted.subset"</code>). Per default the formula specified in
<code>formula</code> determines the variables, the classifying function is
based on.<br />
</p>
<p>Note that the default of <code>cFUN = list(model = NULL, training.set = "fitted.bag")</code>
uses the function <code><a href="rpart.html#topic+rpart">rpart</a></code> and
the predict function <code>predict(object, newdata, type = "class")</code>.
</p>


<h3>Value</h3>

<p>An object of class <code>"inbagg"</code>, that is a list with elements
</p>
<table>
<tr><td><code>mtrees</code></td>
<td>
<p>a list of length <code>nbagg</code>, describing the prediction
models corresponding
to each bootstrap sample. Each element of <code>mtrees</code>
is a list with elements <code>bindx</code> (observations of bag sample),
<code>btree</code> (classifying function of bag sample) and <code>bfct</code> (predictive models for intermediates of bag sample).</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>vector of response values.</p>
</td></tr>
<tr><td><code>W</code></td>
<td>
<p>data frame of intermediate variables.</p>
</td></tr>
<tr><td><code>X</code></td>
<td>
<p>data frame of explanatory variables.</p>
</td></tr>
</table>


<h3>References</h3>

<p>David J. Hand, Hua Gui Li, Niall M. Adams (2001),
Supervised classification with structured class definitions.
<em>Computational Statistics &amp; Data Analysis</em> <b>36</b>,
209&ndash;225.
</p>
<p>Andrea Peters, Berthold Lausen, Georg Michelson and Olaf Gefeller (2003),
Diagnosis of glaucoma by indirect classifiers.
<em>Methods of Information in Medicine</em> <b>1</b>, 99-103.
</p>


<h3>See Also</h3>

<p><code><a href="rpart.html#topic+rpart">rpart</a></code>, <code><a href="#topic+bagging">bagging</a></code>,
<code><a href="stats.html#topic+lm">lm</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>
library("MASS")
library("rpart")
y &lt;- as.factor(sample(1:2, 100, replace = TRUE))
W &lt;- mvrnorm(n = 200, mu = rep(0, 3), Sigma = diag(3))
X &lt;- mvrnorm(n = 200, mu = rep(2, 3), Sigma = diag(3))
colnames(W) &lt;- c("w1", "w2", "w3") 
colnames(X) &lt;- c("x1", "x2", "x3") 
DATA &lt;- data.frame(y, W, X)


pFUN &lt;- list(list(formula = w1~x1+x2, model = lm, predict = mypredict.lm),
list(model = rpart))

inbagg(y~w1+w2+w3~x1+x2+x3, data = DATA, pFUN = pFUN)
</code></pre>

<hr>
<h2 id='inclass'>Indirect Classification</h2><span id='topic+inclass'></span><span id='topic+inclass.default'></span><span id='topic+inclass.data.frame'></span>

<h3>Description</h3>

<p>A framework for the indirect classification approach.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'data.frame'
inclass(formula, data, pFUN = NULL, cFUN = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="inclass_+3A_formula">formula</code></td>
<td>
<p>formula. A <code>formula</code> specified as
<code>y~w1+w2+w3~x1+x2+x3</code> models each intermediate variable
<code>w1, w2, w3</code> by <code>wi~x1+x2+x3</code> and the response by
<code>y~w1+w2+w3</code> if no other formulas are given in <code>pFUN</code> or <code>cFUN</code>.</p>
</td></tr>
<tr><td><code id="inclass_+3A_data">data</code></td>
<td>
<p>data frame of explanatory, intermediate and response variables.</p>
</td></tr>
<tr><td><code id="inclass_+3A_pfun">pFUN</code></td>
<td>
<p>list of lists, which describe models for the intermediate variables, see below for details.</p>
</td></tr>
<tr><td><code id="inclass_+3A_cfun">cFUN</code></td>
<td>
<p>either a function or a list which describes the model for the
response variable. The function has the argument <code>newdata</code> only.</p>
</td></tr>
<tr><td><code id="inclass_+3A_...">...</code></td>
<td>
<p>additional arguments, passed to model fitting of the
response variable.</p>
</td></tr> 
</table>


<h3>Details</h3>

<p>A given data set is subdivided into three types of variables: those to be
used predicting the class (explanatory variables) those to be used defining
the class (intermediate variables) and the class membership variable itself
(response variable). Intermediate variables are modelled based on the
explanatory variables, the class membership variable is defined on the
intermediate variables.<br />
</p>
<p>Each specified intermediate variable is modelled separately 
following <code>pFUN</code> and a formula specified by <code>formula</code>.
<code>pFUN</code> is a list of lists, the maximum length of
<code>pFUN</code> is the number of intermediate variables. Each element of
<code>pFUN</code> is a list with elements:<br />
<code>model</code> -  a function with arguments <code>formula</code> and
<code>data</code>; <br />
<code>predict</code> - an optional function with arguments <code>object, newdata</code> only, 
if <code>predict</code> is not specified, the predict method of <code>model</code>
is used; <br />
<code>formula</code> - specifies the formula for the corresponding
<code>model</code> (optional),
the formula described in <code>y~w1+w2+w3~x1+x2+x3</code> is used if no other is
specified.
<br />
</p>
<p>The response is classified following <code>cFUN</code>, which is either a fixed
function or a list as described below. The determined function <code>cFUN</code> assigns the intermediate (and
explanatory) variables to a certain class membership, the list
<code>cFUN</code> has the elements <code>formula, model, predict</code> and
<code>training.set</code>. The elements <code>formula, model, predict</code> are
structured as described by <code>pFUN</code>, the described model is
trained on the original (intermediate variables) if <code>training.set="original"</code>
or if <code>training.set = NULL</code>, on the fitted values if
<code>training.set = "fitted"</code> or on observations not included in a
specified subset if <code>training.set = "subset"</code>.
<br /> 
</p>
<p>A list of prediction models corresponding to each 
intermediate variable, a predictive function for the response, a list of
specifications for the intermediate and for the response are returned. <br />
For a detailed description on indirect
classification see Hand et al. (2001).
</p>


<h3>Value</h3>

<p>An object of class <code>inclass</code>, consisting of a list of 
</p>
<table>
<tr><td><code>model.intermediate</code></td>
<td>
<p>list of fitted models for each intermediate
variable.</p>
</td></tr>
<tr><td><code>model.response</code></td>
<td>
<p>predictive model for the response variable.</p>
</td></tr>
<tr><td><code>para.intermediate</code></td>
<td>
<p>list, where each element is again a list and specifies
the model for each intermediate variable.</p>
</td></tr>
<tr><td><code>para.response</code></td>
<td>
<p>a list which specifies the model for response variable.</p>
</td></tr>
</table>


<h3>References</h3>

<p>David J. Hand, Hua Gui Li, Niall M. Adams (2001),
Supervised classification with structured class definitions.
<em>Computational Statistics &amp; Data Analysis</em> <b>36</b>,
209&ndash;225.
</p>
<p>Andrea Peters, Berthold Lausen, Georg Michelson and Olaf Gefeller (2003),
Diagnosis of glaucoma by indirect classifiers.
<em>Methods of Information in Medicine</em> <b>1</b>, 99-103.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+bagging">bagging</a></code>, <code><a href="#topic+inclass">inclass</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>data("Smoking", package = "ipred")
# Set three groups of variables:
# 1) explanatory variables are: TarY, NicY, COY, Sex, Age
# 2) intermediate variables are: TVPS, BPNL, COHB
# 3) response (resp) is defined by:

classify &lt;- function(data){
  data &lt;- data[,c("TVPS", "BPNL", "COHB")]
  res &lt;- t(t(data) &gt; c(4438, 232.5, 58))
  res &lt;- as.factor(ifelse(apply(res, 1, sum) &gt; 2, 1, 0))
  res
}

response &lt;- classify(Smoking[ ,c("TVPS", "BPNL", "COHB")])
smoking &lt;- data.frame(Smoking, response)

formula &lt;- response~TVPS+BPNL+COHB~TarY+NicY+COY+Sex+Age

inclass(formula, data = smoking, pFUN = list(list(model = lm, predict =
mypredict.lm)), cFUN = classify)

</code></pre>

<hr>
<h2 id='ipred-internal'>Internal ipred functions</h2><span id='topic+getsurv'></span>

<h3>Description</h3>

<p>Internal ipred functions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getsurv(obj, times)
</code></pre>


<h3>Details</h3>

<p>This functions are not to be called by the user.
</p>

<hr>
<h2 id='ipredknn'> k-Nearest Neighbour Classification </h2><span id='topic+ipredknn'></span>

<h3>Description</h3>

<p>$k$-nearest neighbour classification with an interface compatible to 
<code><a href="#topic+bagging">bagging</a></code> and <code><a href="#topic+errorest">errorest</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ipredknn(formula, data, subset, na.action, k=5, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ipredknn_+3A_formula">formula</code></td>
<td>
<p>a formula of the form <code>lhs ~ rhs</code> where <code>lhs</code> 
is the response variable and <code>rhs</code> a set of
predictors.</p>
</td></tr>
<tr><td><code id="ipredknn_+3A_data">data</code></td>
<td>
<p>optional data frame containing the variables in the
model formula.</p>
</td></tr> 
<tr><td><code id="ipredknn_+3A_subset">subset</code></td>
<td>
<p>optional vector specifying a subset of observations
to be used.</p>
</td></tr>
<tr><td><code id="ipredknn_+3A_na.action">na.action</code></td>
<td>
<p>function which indicates what should happen when
the data contain <code>NA</code>s.</p>
</td></tr>
<tr><td><code id="ipredknn_+3A_k">k</code></td>
<td>
<p>number of neighbours considered, defaults to 5.</p>
</td></tr>
<tr><td><code id="ipredknn_+3A_...">...</code></td>
<td>
<p>additional parameters.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a wrapper to <code><a href="class.html#topic+knn">knn</a></code> in order to be able to 
use k-NN in <code><a href="#topic+bagging">bagging</a></code> and <code><a href="#topic+errorest">errorest</a></code>. 
</p>


<h3>Value</h3>

<p>An object of class <code>ipredknn</code>. See <code><a href="#topic+predict.ipredknn">predict.ipredknn</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library("mlbench")
learn &lt;- as.data.frame(mlbench.twonorm(300))

mypredict.knn &lt;- function(object, newdata) 
                   predict.ipredknn(object, newdata, type="class")

errorest(classes ~., data=learn, model=ipredknn, 
         predict=mypredict.knn)


</code></pre>

<hr>
<h2 id='kfoldcv'> Subsamples for k-fold Cross-Validation </h2><span id='topic+kfoldcv'></span>

<h3>Description</h3>

<p>Computes feasible sample sizes for the k groups in k-fold cv if N/k is not
an integer.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kfoldcv(k, N, nlevel=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kfoldcv_+3A_k">k</code></td>
<td>
<p> number of groups. </p>
</td></tr>
<tr><td><code id="kfoldcv_+3A_n">N</code></td>
<td>
<p> total sample size. </p>
</td></tr>
<tr><td><code id="kfoldcv_+3A_nlevel">nlevel</code></td>
<td>
<p> a vector of sample sizes for stratified sampling.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If N/k is not an integer, k-fold cv is not unique. Determine meaningful
sample sizes.
</p>


<h3>Value</h3>

<p>A vector of length <code>k</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# 10-fold CV with N = 91

kfoldcv(10, 91)	



</code></pre>

<hr>
<h2 id='mypredict.lm'>Predictions Based on Linear Models</h2><span id='topic+mypredict.lm'></span>

<h3>Description</h3>

<p>Function to predict a vector of full length (number of observations), where predictions according to missing
explanatory values are replaced by <code>NA</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mypredict.lm(object, newdata)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mypredict.lm_+3A_object">object</code></td>
<td>
<p>an object of class <code>lm</code>.</p>
</td></tr>
<tr><td><code id="mypredict.lm_+3A_newdata">newdata</code></td>
<td>
<p>matrix or data frame to be predicted according to <code>object</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Vector of predicted values.
</p>


<h3>Note</h3>

<p><code>predict.lm</code> delivers a vector of reduced length, i.e. rows where
explanatory variables are missing are omitted. The full length of the
predicted observation vector is necessary in the indirect classification
approach (<code><a href="#topic+predict.inclass">predict.inclass</a></code>).</p>

<hr>
<h2 id='predict.classbagg'> Predictions from Bagging Trees </h2><span id='topic+predict.classbagg'></span><span id='topic+predict.regbagg'></span><span id='topic+predict.survbagg'></span>

<h3>Description</h3>

<p>Predict the outcome of a new observation based on multiple trees.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'classbagg'
predict(object, newdata=NULL, type=c("class", "prob"),
                            aggregation=c("majority", "average", "weighted"), ...)
## S3 method for class 'regbagg'
predict(object, newdata=NULL, aggregation=c("average",
                "weighted"), ...)
## S3 method for class 'survbagg'
predict(object, newdata=NULL,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.classbagg_+3A_object">object</code></td>
<td>
<p>object of classes <code>classbagg</code>, <code>regbagg</code> or
<code>survbagg</code>.</p>
</td></tr>
<tr><td><code id="predict.classbagg_+3A_newdata">newdata</code></td>
<td>
<p>a data frame of new observations. </p>
</td></tr>
<tr><td><code id="predict.classbagg_+3A_type">type</code></td>
<td>
<p>character string denoting the type of predicted value
returned for classification trees. Either <code>class</code> 
(predicted classes are returned) or <code>prob</code> 
(estimated class probabilities are returned).</p>
</td></tr>
<tr><td><code id="predict.classbagg_+3A_aggregation">aggregation</code></td>
<td>
<p>character string specifying how to aggregate, see below.</p>
</td></tr>
<tr><td><code id="predict.classbagg_+3A_...">...</code></td>
<td>
<p>additional arguments, currently not passed to any function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>There are (at least) three different ways to aggregate the predictions of
bagging classification trees. Most famous is class majority voting
(<code>aggregation="majority"</code>) where the most frequent class is returned. The
second way is choosing the class with maximal averaged class probability
(<code>aggregation="average"</code>). The third method is based on the &quot;aggregated learning
sample&quot;, introduced by Hothorn et al. (2003) for survival trees.
The prediction of a new observation is the majority class, mean or
Kaplan-Meier curve of all observations from the learning sample 
identified by the <code>nbagg</code> leaves containing the new observation.  
For regression trees, only averaged or weighted predictions are possible. 
</p>
<p>By default, the out-of-bag estimate is computed if <code>newdata</code> is NOT
specified. Therefore, the predictions of <code>predict(object)</code> are &quot;honest&quot;
in some way (this is not possible for combined models via <code>comb</code> in
<code><a href="#topic+bagging">bagging</a></code>). 
If you like to compute the predictions for the learning sample
itself, use <code>newdata</code> to specify your data. 
</p>


<h3>Value</h3>

<p>The predicted class or estimated class probabilities are returned for
classification trees. The predicted endpoint is returned in regression
problems and the predicted Kaplan-Meier curve is returned for survival
trees. 
</p>


<h3>References</h3>

 
<p>Leo Breiman (1996), Bagging Predictors. <em>Machine Learning</em>
<b>24</b>(2), 123&ndash;140.
</p>
<p>Torsten Hothorn, Berthold Lausen, Axel Benner and Martin
Radespiel-Troeger (2004), Bagging Survival Trees.
<em>Statistics in Medicine</em>, <b>23</b>(1), 77&ndash;91.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data("Ionosphere", package = "mlbench")
Ionosphere$V2 &lt;- NULL # constant within groups

# nbagg = 10 for performance reasons here
mod &lt;- bagging(Class ~ ., data=Ionosphere)

# out-of-bag estimate

mean(predict(mod) != Ionosphere$Class)

# predictions for the first 10 observations

predict(mod, newdata=Ionosphere[1:10,])

predict(mod, newdata=Ionosphere[1:10,], type="prob")

</code></pre>

<hr>
<h2 id='predict.inbagg'>Predictions from an Inbagg Object</h2><span id='topic+predict.inbagg'></span>

<h3>Description</h3>

<p>Predicts the class membership of new observations through indirect
bagging.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'inbagg'
predict(object, newdata, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.inbagg_+3A_object">object</code></td>
<td>
<p>object of class <code>inbagg</code>, see <code><a href="#topic+inbagg">inbagg</a></code>.</p>
</td></tr>
<tr><td><code id="predict.inbagg_+3A_newdata">newdata</code></td>
<td>
<p>data frame to be classified.</p>
</td></tr>
<tr><td><code id="predict.inbagg_+3A_...">...</code></td>
<td>
<p>additional argumends corresponding to the predictive models.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Predictions of class memberships are calculated. i.e. values of the
intermediate variables are predicted following <code>pFUN</code> and classified following <code>cFUN</code>,
see <code><a href="#topic+inbagg">inbagg</a></code>.
</p>


<h3>Value</h3>

<p>The vector of predicted classes is returned.
</p>


<h3>References</h3>

<p>David J. Hand, Hua Gui Li, Niall M. Adams (2001),
Supervised classification with structured class definitions.
<em>Computational Statistics &amp; Data Analysis</em> <b>36</b>, 
209&ndash;225.
</p>
<p>Andrea Peters, Berthold Lausen, Georg Michelson and Olaf Gefeller (2003),
Diagnosis of glaucoma by indirect classifiers.
<em>Methods of Information in Medicine</em> <b>1</b>, 99-103.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+inbagg">inbagg</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>
library("MASS")
library("rpart")
y &lt;- as.factor(sample(1:2, 100, replace = TRUE))
W &lt;- mvrnorm(n = 200, mu = rep(0, 3), Sigma = diag(3)) 
X &lt;- mvrnorm(n = 200, mu = rep(2, 3), Sigma = diag(3))
colnames(W) &lt;- c("w1", "w2", "w3")
colnames(X) &lt;- c("x1", "x2", "x3")
DATA &lt;- data.frame(y, W, X)

pFUN &lt;- list(list(formula = w1~x1+x2, model = lm),
list(model = rpart))

RES &lt;- inbagg(y~w1+w2+w3~x1+x2+x3, data = DATA, pFUN = pFUN)
predict(RES, newdata = X)
</code></pre>

<hr>
<h2 id='predict.inclass'>Predictions from an Inclass Object</h2><span id='topic+predict.inclass'></span>

<h3>Description</h3>

<p>Predicts the class membership of new observations through indirect
classification.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'inclass'
predict(object, newdata, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.inclass_+3A_object">object</code></td>
<td>
<p> object of class <code>inclass</code>, see <code><a href="#topic+inclass">inclass</a></code>.</p>
</td></tr>
<tr><td><code id="predict.inclass_+3A_newdata">newdata</code></td>
<td>
<p>data frame to be classified.</p>
</td></tr>
<tr><td><code id="predict.inclass_+3A_...">...</code></td>
<td>
<p>additional arguments corresponding to the predictive models 
specified in <code><a href="#topic+inclass">inclass</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Predictions of class memberships are calculated. i.e. values of the
intermediate variables are predicted and classified following <code>cFUN</code>,
see <code><a href="#topic+inclass">inclass</a></code>.
</p>


<h3>Value</h3>

<p>The vector of predicted classes is returned.
</p>


<h3>References</h3>

<p>David J. Hand, Hua Gui Li, Niall M. Adams (2001),
Supervised classification with structured class definitions.
<em>Computational Statistics &amp; Data Analysis</em> <b>36</b>,
209&ndash;225.
</p>
<p>Andrea Peters, Berthold Lausen, Georg Michelson and Olaf Gefeller (2003),
Diagnosis of glaucoma by indirect classifiers.
<em>Methods of Information in Medicine</em> <b>1</b>, 99-103.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+inclass">inclass</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Simulation model, classification rule following Hand et al. (2001)

theta90 &lt;- varset(N = 1000, sigma = 0.1, theta = 90, threshold = 0)

dataset &lt;- as.data.frame(cbind(theta90$explanatory, theta90$intermediate))
names(dataset) &lt;- c(colnames(theta90$explanatory),
colnames(theta90$intermediate))

classify &lt;- function(Y, threshold = 0) {
  Y &lt;- Y[,c("y1", "y2")]
  z &lt;- (Y &gt; threshold)
  resp &lt;- as.factor(ifelse((z[,1] + z[,2]) &gt; 1, 1, 0))
  return(resp)
}

formula &lt;- response~y1+y2~x1+x2

fit &lt;- inclass(formula, data = dataset, pFUN = list(list(model = lm)), 
 cFUN = classify)

predict(object = fit, newdata = dataset)


data("Smoking", package = "ipred")

# explanatory variables are: TarY, NicY, COY, Sex, Age
# intermediate variables are: TVPS, BPNL, COHB
# reponse is defined by:

classify &lt;- function(data){
  data &lt;- data[,c("TVPS", "BPNL", "COHB")]
  res &lt;- t(t(data) &gt; c(4438, 232.5, 58))
  res &lt;- as.factor(ifelse(apply(res, 1, sum) &gt; 2, 1, 0))
  res
}

response &lt;- classify(Smoking[ ,c("TVPS", "BPNL", "COHB")])
smoking &lt;- cbind(Smoking, response)

formula &lt;- response~TVPS+BPNL+COHB~TarY+NicY+COY+Sex+Age

fit &lt;- inclass(formula, data = smoking, 
  pFUN = list(list(model = lm)), cFUN = classify)


predict(object = fit, newdata = smoking)

## End(Not run)

data("GlaucomaMVF", package = "ipred")
library("rpart")
glaucoma &lt;- GlaucomaMVF[,(names(GlaucomaMVF) != "tension")]
# explanatory variables are derived by laser scanning image and intra occular pressure
# intermediate variables are: clv, cs, lora
# response is defined by

classify &lt;- function (data) {
  attach(data) 
  res &lt;- ifelse((!is.na(clv) &amp; !is.na(lora) &amp; clv &gt;= 5.1 &amp; lora &gt;= 
        49.23372) | (!is.na(clv) &amp; !is.na(lora) &amp; !is.na(cs) &amp; 
        clv &lt; 5.1 &amp; lora &gt;= 58.55409 &amp; cs &lt; 1.405) | (is.na(clv) &amp; 
        !is.na(lora) &amp; !is.na(cs) &amp; lora &gt;= 58.55409 &amp; cs &lt; 1.405) | 
        (!is.na(clv) &amp; is.na(lora) &amp; cs &lt; 1.405), 0, 1)
  detach(data)
  factor (res, labels = c("glaucoma", "normal"))
}

fit &lt;- inclass(Class~clv+lora+cs~., data = glaucoma, 
             pFUN = list(list(model = rpart)), cFUN = classify)

data("GlaucomaM", package = "TH.data")
predict(object = fit, newdata = GlaucomaM)

</code></pre>

<hr>
<h2 id='predict.ipredknn'> Predictions from k-Nearest Neighbors </h2><span id='topic+predict.ipredknn'></span>

<h3>Description</h3>

<p>Predict the class of a new observation based on k-NN. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ipredknn'
predict(object, newdata, type=c("prob", "class"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.ipredknn_+3A_object">object</code></td>
<td>
<p>object of class <code>ipredknn</code>.</p>
</td></tr>
<tr><td><code id="predict.ipredknn_+3A_newdata">newdata</code></td>
<td>
<p>a data frame of new observations. </p>
</td></tr>
<tr><td><code id="predict.ipredknn_+3A_type">type</code></td>
<td>
<p>return either the predicted class or the 
the proportion of the votes for the winning
class.</p>
</td></tr>
<tr><td><code id="predict.ipredknn_+3A_...">...</code></td>
<td>
<p>additional arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is a method for the generic function <code><a href="stats.html#topic+predict">predict</a></code>
for class <code>ipredknn</code>. For the details see <code><a href="class.html#topic+knn">knn</a></code>.
</p>


<h3>Value</h3>

<p>Either the predicted class or the
the proportion of the votes for the winning class.
</p>

<hr>
<h2 id='predict.slda'> Predictions from Stabilised Linear Discriminant Analysis </h2><span id='topic+predict.slda'></span>

<h3>Description</h3>

<p>Predict the class of a new observation based on stabilised LDA.  
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'slda'
predict(object, newdata, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.slda_+3A_object">object</code></td>
<td>
<p>object of class <code>slda</code>.</p>
</td></tr>
<tr><td><code id="predict.slda_+3A_newdata">newdata</code></td>
<td>
<p>a data frame of new observations. </p>
</td></tr>
<tr><td><code id="predict.slda_+3A_...">...</code></td>
<td>
<p>additional arguments passed to
<code><a href="MASS.html#topic+predict.lda">predict.lda</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is a method for the generic function <code><a href="stats.html#topic+predict">predict</a></code>
for class <code>slda</code>. For the details see <code><a href="MASS.html#topic+predict.lda">predict.lda</a></code>.
</p>


<h3>Value</h3>

<p>A list with components
</p>
<table>
<tr><td><code>class</code></td>
<td>
<p>the predicted class (a factor).</p>
</td></tr>
<tr><td><code>posterior</code></td>
<td>
<p>posterior probabilities for the classes.</p>
</td></tr>
<tr><td><code>x</code></td>
<td>
<p>the scores of test cases.</p>
</td></tr>
</table>

<hr>
<h2 id='print.classbagg'>Print Method for Bagging Trees</h2><span id='topic+print'></span><span id='topic+print.classbagg'></span><span id='topic+print.regbagg'></span><span id='topic+print.survbagg'></span>

<h3>Description</h3>

<p>Print objects returned by <code><a href="#topic+bagging">bagging</a></code> in nice layout.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'classbagg'
print(x, digits, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.classbagg_+3A_x">x</code></td>
<td>
<p>object returned by <code><a href="#topic+bagging">bagging</a></code>.</p>
</td></tr>
<tr><td><code id="print.classbagg_+3A_digits">digits</code></td>
<td>
<p>how many digits should be printed.</p>
</td></tr>
<tr><td><code id="print.classbagg_+3A_...">...</code></td>
<td>
<p>further arguments to be passed to or from methods.</p>
</td></tr>  
</table>


<h3>Value</h3>

<p>none
</p>

<hr>
<h2 id='print.cvclass'>Print Method for Error Rate Estimators</h2><span id='topic+print.cvclass'></span><span id='topic+print.cvreg'></span><span id='topic+print.cvsurv'></span><span id='topic+print.bootestclass'></span><span id='topic+print.bootestreg'></span><span id='topic+print.bootestsurv'></span>

<h3>Description</h3>

<p>Print objects returned by <code><a href="#topic+errorest">errorest</a></code> in nice layout.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'cvclass'
print(x, digits=4, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.cvclass_+3A_x">x</code></td>
<td>
<p>an object returned by <code><a href="#topic+errorest">errorest</a></code>.</p>
</td></tr>
<tr><td><code id="print.cvclass_+3A_digits">digits</code></td>
<td>
<p>how many digits should be printed.</p>
</td></tr>
<tr><td><code id="print.cvclass_+3A_...">...</code></td>
<td>
<p>further arguments to be passed to or from methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>none
</p>

<hr>
<h2 id='print.inbagg'>Print Method for Inbagg Object</h2><span id='topic+print.inbagg'></span>

<h3>Description</h3>

<p>Print object of class <code>inbagg</code> in nice layout.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'inbagg'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.inbagg_+3A_x">x</code></td>
<td>
<p>object of class <code>inbagg</code>.</p>
</td></tr>
<tr><td><code id="print.inbagg_+3A_...">...</code></td>
<td>
<p>additional arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>An object of class <code>inbagg</code> is printed. Information about number and names of the intermediate variables,
and the number of drawn bootstrap samples is given.
</p>

<hr>
<h2 id='print.inclass'>Print Method for Inclass Object</h2><span id='topic+print.inclass'></span>

<h3>Description</h3>

<p>Print object of class <code>inclass</code> in nice layout.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'inclass'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.inclass_+3A_x">x</code></td>
<td>
<p>object of class <code>inclass</code>.</p>
</td></tr>
<tr><td><code id="print.inclass_+3A_...">...</code></td>
<td>
<p>additional arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>An object of class <code>inclass</code> is printed. Information about number and names of the intermediate variables, the used modelling technique and the number of
drawn bootstrap samples is given.
</p>

<hr>
<h2 id='prune.classbagg'> Pruning for Bagging  </h2><span id='topic+prune.classbagg'></span><span id='topic+prune.regbagg'></span><span id='topic+prune.survbagg'></span>

<h3>Description</h3>

<p>Prune each of the trees returned by <code><a href="#topic+bagging">bagging</a></code>. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'classbagg'
prune(tree, cp=0.01,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="prune.classbagg_+3A_tree">tree</code></td>
<td>
<p> an object returned by <code><a href="#topic+bagging">bagging</a></code> 
(calling this <code>tree</code> is needed by the generic function 
<code>prune</code> in package <code>rpart</code>).</p>
</td></tr>
<tr><td><code id="prune.classbagg_+3A_cp">cp</code></td>
<td>
<p>complexity parameter, see <code><a href="rpart.html#topic+prune.rpart">prune.rpart</a></code>.</p>
</td></tr>
<tr><td><code id="prune.classbagg_+3A_...">...</code></td>
<td>
<p>additional arguments to <code><a href="rpart.html#topic+prune.rpart">prune.rpart</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>By default, <code><a href="#topic+bagging">bagging</a></code> grows classification 
trees of maximal size. One may want to prune each tree, however, 
it is not clear whether or not this may decrease prediction error. 
</p>


<h3>Value</h3>

<p>An object of the same class as <code>tree</code> with the trees pruned. 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data("Glass", package = "mlbench")
library("rpart")

mod &lt;- bagging(Type ~ ., data=Glass, nbagg=10, coob=TRUE)
pmod &lt;- prune(mod)
print(pmod)


</code></pre>

<hr>
<h2 id='rsurv'> Simulate Survival Data </h2><span id='topic+rsurv'></span>

<h3>Description</h3>

<p>Simulation Setup for Survival Data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rsurv(N, model=c("A", "B", "C", "D", "tree"), gamma=NULL, fact=1, pnon=10,
      gethaz=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rsurv_+3A_n">N</code></td>
<td>
<p> number of observations. </p>
</td></tr>
<tr><td><code id="rsurv_+3A_model">model</code></td>
<td>
<p> type of model. </p>
</td></tr>
<tr><td><code id="rsurv_+3A_gamma">gamma</code></td>
<td>
<p>simulate censoring time as runif(N, 0, gamma). Defaults to
<code>NULL</code> (no censoring).</p>
</td></tr>
<tr><td><code id="rsurv_+3A_fact">fact</code></td>
<td>
<p>scale parameter for <code>model=tree</code>.</p>
</td></tr>
<tr><td><code id="rsurv_+3A_pnon">pnon</code></td>
<td>
<p>number of additional non-informative variables for the tree
model.</p>
</td></tr>
<tr><td><code id="rsurv_+3A_gethaz">gethaz</code></td>
<td>
<p>logical, indicating wheather the hazard rate for each 
observation should be returned.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Simulation setup similar to configurations used in LeBlanc and Crowley
(1992) or Keles and Segal (2002) as well as a tree model used in Hothorn et
al. (2004). See Hothorn et al. (2004) for the details. 
</p>


<h3>Value</h3>

<p>A data  frame with elements <code>time</code>, <code>cens</code>, <code>X1</code> ...
<code>X5</code>. If <code>pnon</code> &gt; 0, additional noninformative covariables are
added. If <code>gethaz=TRUE</code>, the <code>hazard</code> attribute returns the hazard
rates.
</p>


<h3>References</h3>

 
<p>M. LeBlanc and J. Crowley (1992), Relative Risk Trees for 
Censored Survival Data. <em>Biometrics</em> <b>48</b>, 411&ndash;425.
</p>
<p>S. Keles and M. R. Segal (2002), Residual-based tree-structured 
survival analysis. <em>Statistics in Medicine</em>, <b>21</b>, 313&ndash;326.
</p>
<p>Torsten Hothorn, Berthold Lausen, Axel Benner and Martin
Radespiel-Troeger (2004), Bagging Survival Trees.
<em>Statistics in Medicine</em>, <b>23</b>(1), 77&ndash;91.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library("survival")
# 3*X1 + X2
simdat &lt;- rsurv(500, model="C")
coxph(Surv(time, cens) ~ ., data=simdat)

</code></pre>

<hr>
<h2 id='sbrier'> Model Fit for Survival Data </h2><span id='topic+sbrier'></span>

<h3>Description</h3>

<p>Model fit for survival data: the integrated Brier score for censored
observations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sbrier(obj, pred, btime= range(obj[,1]))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sbrier_+3A_obj">obj</code></td>
<td>
<p>an object of class <code>Surv</code>.</p>
</td></tr>
<tr><td><code id="sbrier_+3A_pred">pred</code></td>
<td>
<p>predicted values. Either a probability or a list of
<code>survfit</code> objects. </p>
</td></tr>
<tr><td><code id="sbrier_+3A_btime">btime</code></td>
<td>
<p>numeric vector of times, the integrated Brier score is
computed if this is of <code>length &gt; 1</code>. 
The Brier score at <code>btime</code>
is returned otherwise.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>There is no obvious criterion of model fit for censored data. The Brier
score for censoring as well as it's integrated version were suggested by
Graf et al (1999).
</p>
<p>The integrated Brier score is always computed over a subset of the
interval given by the range of the time slot of the survival object <code>obj</code>.
</p>


<h3>Value</h3>

<p>The (integrated) Brier score with attribute <code>time</code> is returned. 
</p>


<h3>References</h3>

 
<p>Erika Graf, Claudia Schmoor, Willi Sauerbrei and Martin Schumacher (1999),
Assessment and comparison of prognostic classification schemes for
survival data. <em>Statistics in Medicine</em> <b>18</b>(17-18), 2529&ndash;2545.
</p>


<h3>See Also</h3>

<p>More measures for the validation of predicted surival probabilities 
are implemented in package <code>pec</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library("survival")
data("DLBCL", package = "ipred")
smod &lt;- Surv(DLBCL$time, DLBCL$cens)

KM &lt;- survfit(smod ~ 1)
# integrated Brier score up to max(DLBCL$time)
sbrier(smod, KM)

# integrated Brier score up to time=50
sbrier(smod, KM, btime=c(0, 50))

# Brier score for time=50
sbrier(smod, KM, btime=50)

# a "real" model: one single survival tree with Intern. Prognostic Index
# and mean gene expression in the first cluster as predictors
mod &lt;- bagging(Surv(time, cens) ~ MGEc.1 + IPI, data=DLBCL, nbagg=1)

# this is a list of survfit objects (==KM-curves), one for each observation
# in DLBCL
pred &lt;- predict(mod, newdata=DLBCL)

# integrated Brier score up to max(time)
sbrier(smod, pred)

# Brier score at time=50
sbrier(smod, pred, btime=50)
# artificial examples and illustrations

cleans &lt;- function(x) { attr(x, "time") &lt;- NULL; names(x) &lt;- NULL; x }

n &lt;- 100
time &lt;- rpois(n, 20)
cens &lt;- rep(1, n)

# checks, Graf et al. page 2536, no censoring at all!
# no information: \pi(t) = 0.5 

a &lt;- sbrier(Surv(time, cens), rep(0.5, n), time[50])
stopifnot(all.equal(cleans(a),0.25))

# some information: \pi(t) = S(t)

n &lt;- 100
time &lt;- 1:100
mod &lt;- survfit(Surv(time, cens) ~ 1)
a &lt;- sbrier(Surv(time, cens), rep(list(mod), n))
mymin &lt;- mod$surv * (1 - mod$surv)
cleans(a)
sum(mymin)/diff(range(time))

# independent of ordering
rand &lt;- sample(1:100)
b &lt;- sbrier(Surv(time, cens)[rand], rep(list(mod), n)[rand])
stopifnot(all.equal(cleans(a), cleans(b)))



# 2 groups at different risk

time &lt;- c(1:10, 21:30)
strata &lt;- c(rep(1, 10), rep(2, 10))
cens &lt;- rep(1, length(time))

# no information about the groups

a &lt;- sbrier(Surv(time, cens), survfit(Surv(time, cens) ~ 1))
b &lt;- sbrier(Surv(time, cens), rep(list(survfit(Surv(time, cens) ~1)), 20))
stopifnot(all.equal(a, b))

# risk groups known

mod &lt;- survfit(Surv(time, cens) ~ strata)
b &lt;- sbrier(Surv(time, cens), c(rep(list(mod[1]), 10), rep(list(mod[2]), 10)))
stopifnot(a &gt; b)

### GBSG2 data
data("GBSG2", package = "TH.data")

thsum &lt;- function(x) {
  ret &lt;- c(median(x), quantile(x, 0.25), quantile(x,0.75))
  names(ret)[1] &lt;- "Median"
  ret
}

t(apply(GBSG2[,c("age", "tsize", "pnodes",
                 "progrec", "estrec")], 2, thsum))

table(GBSG2$menostat)
table(GBSG2$tgrade)
table(GBSG2$horTh)

# pooled Kaplan-Meier

mod &lt;- survfit(Surv(time, cens) ~ 1, data=GBSG2)
# integrated Brier score
sbrier(Surv(GBSG2$time, GBSG2$cens), mod)
# Brier score at 5 years
sbrier(Surv(GBSG2$time, GBSG2$cens), mod, btime=1825)

# Nottingham prognostic index

GBSG2 &lt;- GBSG2[order(GBSG2$time),]

NPI &lt;- 0.2*GBSG2$tsize/10 + 1 + as.integer(GBSG2$tgrade)
NPI[NPI &lt; 3.4] &lt;- 1
NPI[NPI &gt;= 3.4 &amp; NPI &lt;=5.4] &lt;- 2
NPI[NPI &gt; 5.4] &lt;- 3

mod &lt;- survfit(Surv(time, cens) ~ NPI, data=GBSG2)
plot(mod)

pred &lt;- c()
survs &lt;- c()
for (i in sort(unique(NPI)))
    survs &lt;- c(survs, getsurv(mod[i], 1825))

for (i in 1:nrow(GBSG2))
   pred &lt;- c(pred, survs[NPI[i]])

# Brier score of NPI at t=5 years
sbrier(Surv(GBSG2$time, GBSG2$cens), pred, btime=1825)


</code></pre>

<hr>
<h2 id='slda'> Stabilised Linear Discriminant Analysis </h2><span id='topic+slda'></span><span id='topic+slda.default'></span><span id='topic+slda.formula'></span><span id='topic+slda.factor'></span>

<h3>Description</h3>

<p>Linear discriminant analysis based on left-spherically 
distributed linear scores.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'formula'
slda(formula, data, subset, na.action=na.rpart, ...)
## S3 method for class 'factor'
slda(y, X, q=NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="slda_+3A_y">y</code></td>
<td>
<p>the response variable: a factor vector of class labels.</p>
</td></tr>
<tr><td><code id="slda_+3A_x">X</code></td>
<td>
<p>a data frame of predictor variables.</p>
</td></tr>
<tr><td><code id="slda_+3A_q">q</code></td>
<td>
<p>the number of positive eigenvalues the scores are derived from,
see below.</p>
</td></tr>
<tr><td><code id="slda_+3A_formula">formula</code></td>
<td>
<p>a formula of the form <code>lhs ~ rhs</code> where <code>lhs</code> 
is the response variable and <code>rhs</code> a set of
predictors.</p>
</td></tr>
<tr><td><code id="slda_+3A_data">data</code></td>
<td>
<p>optional data frame containing the variables in the
model formula.</p>
</td></tr> 
<tr><td><code id="slda_+3A_subset">subset</code></td>
<td>
<p>optional vector specifying a subset of observations
to be used.</p>
</td></tr>
<tr><td><code id="slda_+3A_na.action">na.action</code></td>
<td>
<p>function which indicates what should happen when
the data contain <code>NA</code>s.  Defaults to
<code><a href="rpart.html#topic+na.rpart">na.rpart</a></code>.</p>
</td></tr>
<tr><td><code id="slda_+3A_...">...</code></td>
<td>
<p>additional parameters passed to <code><a href="MASS.html#topic+lda">lda</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function implements the LDA for <code class="reqn">q</code>-dimensional linear scores of
the original <code class="reqn">p</code> predictors derived from the <code class="reqn">PC_q</code> rule by Laeuter
et al. (1998). Based on the product sum matrix 
</p>
<p style="text-align: center;"><code class="reqn">W = (X - \bar{X})^\top(X - \bar{X})</code>
</p>

<p>the eigenvalue problem <code class="reqn">WD = diag(W)DL</code> is solved. The first <code class="reqn">q</code>
columns <code class="reqn">D_q</code> of <code class="reqn">D</code> are used as a weight matrix for the 
original <code class="reqn">p</code> predictors: <code class="reqn">XD_q</code>. By default, <code class="reqn">q</code> is the number
of eigenvalues greater one. The <code class="reqn">q</code>-dimensional linear scores are
left-spherically distributed and are used as predictors for a classical 
LDA. 
</p>
<p>This form of reduction of the dimensionality was 
developed for discriminant analysis problems by Laeuter (1992) and was used
for multivariate tests by Laeuter et al. (1998), Kropf (2000) gives an
overview. For details on left-spherically distributions see Fang and 
Zhang (1990).  
</p>


<h3>Value</h3>

<p>An object of class <code>slda</code>, a list with components
</p>
<table>
<tr><td><code>scores</code></td>
<td>
<p>the weight matrix.</p>
</td></tr>
<tr><td><code>mylda</code></td>
<td>
<p>an object of class <code>lda</code>.</p>
</td></tr>
</table>


<h3>References</h3>

 
<p>Fang Kai-Tai and Zhang Yao-Ting (1990), <em>Generalized Multivariate
Analysis</em>, Springer, Berlin.
</p>
<p>Siegfried Kropf (2000), <em>Hochdimensionale multivariate Verfahren in der
medizinischen Statistik</em>, Shaker Verlag, Aachen (in german).
</p>
<p>Juergen Laeuter (1992), <em>Stabile multivariate Verfahren</em>,
Akademie Verlag, Berlin (in german).
</p>
<p>Juergen Laeuter, Ekkehard Glimm and Siegfried Kropf (1998), Multivariate
Tests Based on Left-Spherically Distributed Linear Scores. <em>The Annals
of Statistics</em>, <b>26</b>(5) 1972&ndash;1988. 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+predict.slda">predict.slda</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library("mlbench")
library("MASS")
learn &lt;- as.data.frame(mlbench.twonorm(100))
test &lt;- as.data.frame(mlbench.twonorm(1000))

mlda &lt;- lda(classes ~ ., data=learn)
mslda &lt;- slda(classes ~ ., data=learn)

print(mean(predict(mlda, newdata=test)$class != test$classes))
print(mean(predict(mslda, newdata=test)$class != test$classes))

</code></pre>

<hr>
<h2 id='Smoking'>Smoking Styles</h2><span id='topic+Smoking'></span>

<h3>Description</h3>

<p>The <code>Smoking</code> data frame has 55 rows and 9 columns.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("Smoking")</code></pre>


<h3>Format</h3>

<p>This data frame contains the following columns:
</p>

<dl>
<dt>NR</dt><dd><p>numeric, patient number.</p>
</dd>
<dt>Sex</dt><dd><p>factor, sex of patient.</p>
</dd>
<dt>Age</dt><dd><p>factor, age group of patient, grouping consisting of those in their twenties, those in their thirties and so on.</p>
</dd>
<dt>TarY</dt><dd><p>numeric, tar yields of the cigarettes.</p>
</dd>
<dt>NicY</dt><dd><p>numeric, nicotine yields of the cigarettes.</p>
</dd>
<dt>COY</dt><dd><p>numeric, carbon monoxide (CO) yield of the cigarettes.</p>
</dd>
<dt>TVPS</dt><dd><p>numeric, total volume puffed smoke.</p>
</dd>
<dt>BPNL</dt><dd><p>numeric, blood plasma nicotine level.</p>
</dd>
<dt>COHB</dt><dd><p>numeric, carboxyhaemoglobin level, i.e. amount of CO absorbed by the blood stream.</p>
</dd>
</dl>



<h3>Details</h3>

<p>The data describes different smoking habits of probands.
</p>


<h3>Source</h3>

<p>Hand and Taylor (1987), Study F <em>Smoking Styles</em>.
</p>


<h3>References</h3>

<p>D.J. Hand and C.C. Taylor (1987), 
<em>Multivariate analysis of variance and repeated measures.</em> London: Chapman &amp;
Hall, pp. 167&ndash;181.
</p>

<hr>
<h2 id='summary.classbagg'>Summarising Bagging</h2><span id='topic+summary.classbagg'></span><span id='topic+summary.regbagg'></span><span id='topic+summary.survbagg'></span><span id='topic+print.summary.bagging'></span>

<h3>Description</h3>

<p><code>summary</code> method for objects returned by <code><a href="#topic+bagging">bagging</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'classbagg'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.classbagg_+3A_object">object</code></td>
<td>
<p>object returned by <code><a href="#topic+bagging">bagging</a></code>.</p>
</td></tr>
<tr><td><code id="summary.classbagg_+3A_...">...</code></td>
<td>
<p>further arguments to be passed to or from methods.</p>
</td></tr>  
</table>


<h3>Details</h3>

<p>A representation of all trees in the object is printed.
</p>


<h3>Value</h3>

<p>none
</p>

<hr>
<h2 id='summary.inbagg'>Summarising Inbagg</h2><span id='topic+summary.inbagg'></span><span id='topic+print.summary.inbagg'></span>

<h3>Description</h3>

<p>Summary of inbagg is returned.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'inbagg'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.inbagg_+3A_object">object</code></td>
<td>
<p>an object of class <code>inbagg</code>.</p>
</td></tr>
<tr><td><code id="summary.inbagg_+3A_...">...</code></td>
<td>
<p>additional arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A representation of an indirect bagging model 
(the intermediates variables, the number of bootstrap samples, the trees) is printed.
</p>


<h3>Value</h3>

<p>none
</p>


<h3>See Also</h3>

<p><code><a href="#topic+print.summary.inbagg">print.summary.inbagg</a></code></p>

<hr>
<h2 id='summary.inclass'>Summarising Inclass</h2><span id='topic+summary.inclass'></span><span id='topic+print.summary.inclass'></span>

<h3>Description</h3>

<p>Summary of inclass is returned.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'inclass'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.inclass_+3A_object">object</code></td>
<td>
<p>an object of class <code>inclass</code>.</p>
</td></tr>
<tr><td><code id="summary.inclass_+3A_...">...</code></td>
<td>
<p>additional arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A representation of an indirect classification model 
(the intermediates variables, which modelling technique is used and the
prediction model) is printed.
</p>


<h3>Value</h3>

<p>none
</p>


<h3>See Also</h3>

<p><code><a href="#topic+print.summary.inclass">print.summary.inclass</a></code></p>

<hr>
<h2 id='varset'>Simulation Model</h2><span id='topic+varset'></span>

<h3>Description</h3>

<p>Three sets of variables are calculated: explanatory, intermediate and response variables. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>varset(N, sigma=0.1, theta=90, threshold=0, u=1:3)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="varset_+3A_n">N</code></td>
<td>
<p>number of simulated observations.</p>
</td></tr>
<tr><td><code id="varset_+3A_sigma">sigma</code></td>
<td>
<p>standard deviation of the error term.</p>
</td></tr>
<tr><td><code id="varset_+3A_theta">theta</code></td>
<td>
<p>angle between two u vectors.</p>
</td></tr>
<tr><td><code id="varset_+3A_threshold">threshold</code></td>
<td>
<p>cutpoint for classifying to 0 or 1.</p>
</td></tr>
<tr><td><code id="varset_+3A_u">u</code></td>
<td>
<p>starting values.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For each observation values of two explanatory variables <code class="reqn">x = (x_1, x_2)^{\top}</code> and of two responses <code class="reqn">y = (y_1, y_2)^{\top}</code> are simulated, following the formula:
</p>
<p style="text-align: center;"><code class="reqn">
y = U*x+e = ({u_1^{\top} \atop u_2^{\top}})*x+e
</code>
</p>

<p>where x is the evaluation of as standard normal random variable and e is generated by a normal variable with standard deviation <code>sigma</code>. U is a 2*2 Matrix, where 
</p>
<p style="text-align: center;"><code class="reqn">
u_1 = ({u_{1, 1} \atop u_{1, 2}}),
u_2 = ({u_{2, 1} \atop u_{2, 2}}),
||u_1|| = ||u_2|| = 1,
</code>
</p>

<p>i.e. a matrix of two normalised vectors.
</p>


<h3>Value</h3>

<p>A list containing the following arguments
</p>
<table>
<tr><td><code>explanatory</code></td>
<td>
<p>N*2 matrix of 2 explanatory variables.</p>
</td></tr>
<tr><td><code>intermediate</code></td>
<td>
<p>N*2 matrix of 2 intermediate variables.</p>
</td></tr>
<tr><td><code>response</code></td>
<td>
<p>response vectors with values 0 or 1.</p>
</td></tr>
</table>


<h3>References</h3>

<p>David J. Hand, Hua Gui Li, Niall M. Adams (2001),
Supervised classification with structured class definitions.
<em>Computational Statistics &amp; Data Analysis</em> <b>36</b>,
209&ndash;225.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
theta90 &lt;- varset(N = 1000, sigma = 0.1, theta = 90, threshold = 0)
theta0 &lt;- varset(N = 1000, sigma = 0.1, theta = 0, threshold = 0)
par(mfrow = c(1, 2))
plot(theta0$intermediate)
plot(theta90$intermediate)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
