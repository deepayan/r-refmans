<!DOCTYPE html><html><head><title>Help for package qlcData</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {qlcData}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#qlcData-package'>
<p>Processing data for quantitative language comparison (QLC)</p></a></li>
<li><a href='#.expandValues'>
<p>Internal helper</p></a></li>
<li><a href='#asPhylo'>
<p>Convert Glottolog trees to phylo format</p></a></li>
<li><a href='#getTree'>
<p>Extract parts out of the full Glottolog 2016 tree</p></a></li>
<li><a href='#glottolog'>
<p>Glottolog data from <a href="https://glottolog.org">https://glottolog.org</a></p></a></li>
<li><a href='#join_align'>
<p>Join various multialignments into one combined dataframe</p></a></li>
<li><a href='#pass_align'>
<p>Transfer alignment from one string to another</p></a></li>
<li><a href='#read_align'>
<p>Reading different versions of linguistic multialignments.</p></a></li>
<li><a href='#recode'>
<p>Recoding nominal data</p></a></li>
<li><a href='#tokenize'>
<p>Tokenization and transliteration of character strings based on an orthography profile</p></a></li>
<li><a href='#write.profile'>
<p>Writing (and reading) of an orthography profile skeleton</p></a></li>
<li><a href='#write.recoding'>
<p>Reading and writing of recoding files.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Processing Data for Quantitative Language Comparison</td>
</tr>
<tr>
<td>Description:</td>
<td>Functionality to read, recode, and transcode data as used in 
  quantitative language comparison, specifically to deal with multilingual 
  orthographic variation (Moran &amp; Cysouw (2018) &lt;<a href="https://doi.org/10.5281%2Fzenodo.1296780">doi:10.5281/zenodo.1296780</a>&gt;) 
  and with the recoding of nominal data.</td>
</tr>
<tr>
<td>Version:</td>
<td>0.3</td>
</tr>
<tr>
<td>Date:</td>
<td>2024-06-07</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>stringi (&ge; 0.2-5), yaml (&ge; 2.1.11), shiny, docopt,
data.tree, phytools, ape</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown, DiagrammeR</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-06-08 08:52:02 UTC; cysouw</td>
</tr>
<tr>
<td>Author:</td>
<td>Michael Cysouw <a href="https://orcid.org/0000-0003-3168-4946"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Michael Cysouw &lt;cysouw@mac.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-06-10 17:10:01 UTC</td>
</tr>
</table>
<hr>
<h2 id='qlcData-package'>
Processing data for quantitative language comparison (QLC)
</h2><span id='topic+qlcData-package'></span><span id='topic+qlcData'></span>

<h3>Description</h3>

<p>The package offers various functions to read, transcode and process data. There are many different function to read in data. Also a general framework to recode nominal data is included. Further, there is a general approach to describe orthographic systems through so-called Orthography Profiles. It offers functions to write such profiles based on some actual written text, and to test and correct profiles given concrete data. The main end-use is to produce tokenized texts in so-called tailored grapheme clusters.
</p>


<h3>Details</h3>


<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> qlcData</td>
</tr>
<tr>
 <td style="text-align: left;">
Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
Version: </td><td style="text-align: left;"> 0.3</td>
</tr>
<tr>
 <td style="text-align: left;">
Date: </td><td style="text-align: left;"> 2024-06-07</td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> GPL-3</td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>

<p>Various functions to read specific data formats of QLC are documented in <code><a href="#topic+read_align">read_align</a></code>, <code><a href="#topic+read.profile">read.profile</a></code>, <code><a href="#topic+read.recoding">read.recoding</a></code>.
</p>
<p>The <code><a href="#topic+recode">recode</a></code> function allows for an easy and transparent way to specify a recoding of an existing nominal dataset. The specification of the recoding-decisions is preferably saved in an easily accessible YAML-file. There are utility function <code><a href="#topic+write.profile">write.profile</a></code> for writing and reading such files included.
</p>
<p>For processing of strings using orthography profiles, the central function is <code><a href="#topic+tokenize">tokenize</a></code>. A basic sceleton for an orthography profile can be produced with <code><a href="#topic+write.profile">write.profile</a></code>
</p>


<h3>Author(s)</h3>

<p>Michael Cysouw &lt;cysouw@mac.com&gt;
</p>

<hr>
<h2 id='.expandValues'>
Internal helper
</h2><span id='topic+.expandValues'></span>

<h3>Description</h3>

<p>produce combinations of nominal variables
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.expandValues(attributes, data, all)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".expandValues_+3A_attributes">attributes</code></td>
<td>

<p>a list of attributes to be recoded. Vectors (as elements of the list) are possible to specify combinations of attributes to be recoded as a single complex attribute.
</p>
</td></tr>
<tr><td><code id=".expandValues_+3A_data">data</code></td>
<td>

<p>a data frame with nominal data, attributes as columns, observations as rows.
</p>
</td></tr>
<tr><td><code id=".expandValues_+3A_all">all</code></td>
<td>

<p>Logical: should all values be produced or only those with existing data?
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Just a helper.
</p>


<h3>Value</h3>

<p><code>expandValues</code> is an internal help function to show the various value-combinations when combining attributes.
</p>


<h3>Author(s)</h3>

<p>Michael Cysouw
</p>

<hr>
<h2 id='asPhylo'>
Convert Glottolog trees to phylo format
</h2><span id='topic+asPhylo'></span>

<h3>Description</h3>

<p>Convenience version of conversion to phylo format of the <code>ape</code> package, used throughout packages for phylogenetic methods. This conversion offers various options to tweak the branch lenghts of Glottolog trees (which do not have any inherent branch lengths). Currently very slow for large trees!
</p>


<h3>Usage</h3>

<pre><code class='language-R'>asPhylo(tree, height = 100, fixed.branches = NULL, long.root = NULL, 
  multi.allow = FALSE, quick = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="asPhylo_+3A_tree">tree</code></td>
<td>

<p>(selection of) Glottolog data, preferably extracted with <code><a href="#topic+getTree">getTree</a></code>. If the selection is not logically a tree, then it will lead to errors.
</p>
</td></tr>
<tr><td><code id="asPhylo_+3A_height">height</code></td>
<td>

<p>Height of the tree. By default all nodes in the tree will be simply equally spaced on this height in an ultrametric fashion, i.e. all leaves will be at height zero.
</p>
</td></tr>
<tr><td><code id="asPhylo_+3A_fixed.branches">fixed.branches</code></td>
<td>

<p>Alternatively to <code>height</code>, specify a fixed length for all branches. Note that this happens before all singleton nodes will be removed, so branches with singleton nodes will become as long as the amount of singleton nodes on it.
</p>
</td></tr>
<tr><td><code id="asPhylo_+3A_long.root">long.root</code></td>
<td>

<p>The Glottolog version included in this packages includes a 'World' root and six area nodes below this root. These nodes are not strictly speaking genealogical nodes, though they are often practical for phylogenitic inference. Specify the length of these nodes here.
</p>
</td></tr>
<tr><td><code id="asPhylo_+3A_multi.allow">multi.allow</code></td>
<td>

<p>Allow multichotomies in tree 
</p>
</td></tr>
<tr><td><code id="asPhylo_+3A_quick">quick</code></td>
<td>

<p>Conversion when quick approach is used  
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code><a href="ape.html#topic+phylo">phylo</a></code> format is a widely used format for phylogenetic inference. However, many methods depend crucially on branch lengths. As linguistic trees mostly do not have branch lenghts, this conversion offers a few options to tweak branch lengths.
</p>
<p>Note that phylo trees do not allow singleton nodes, so they are removed internally (by <code><a href="ape.html#topic+collapse.singles">collapse.singles</a></code>). The tree is also forced to be binary (by <code><a href="ape.html#topic+multi2di">multi2di</a></code>), which is also expected in many phylogenetic analysis.
</p>
<p>The order of length assignment is: reduce (in getTree) &gt;&gt; height &gt;&gt; fixed length &gt;&gt; long root &gt;&gt; collapse singles, which can be used to lead to different results. Also the option <code>reduce</code> in <code><a href="#topic+getTree">getTree</a></code> can be used to influence the branch lenghts (see Examples below).
</p>
<p>The collapsing of singleton internal nodes after branch length leads to an interesting effect that branch lengths are somehow proportional to the number of internal diversity of the tree, which might make sense as a proxy to branch lengths. Something like <code>reduce = F, fixed.branches = 1, long.root = 10</code> seem promising.
</p>


<h3>Value</h3>

<p>An object of class <code>phylo</code>.
</p>


<h3>Note</h3>

<p>Depends internally on <code><a href="data.tree.html#topic+ToNewick">ToNewick</a></code> which is currently very slow for large trees.
</p>


<h3>Author(s)</h3>

<p>Michael Cysouw &lt;cysouw@mac.com
</p>


<h3>Examples</h3>

<pre><code class='language-R'># many different effects can be achieved by combining options

isoCodes &lt;- c("deu", "eng", "swe", "swh", "xho", "fin")
treeWithInternal &lt;- getTree(isoCodes, reduce = FALSE)
treeNoInternal &lt;- getTree(isoCodes, reduce = TRUE)


# to understand the influence of the option 'reduce' also check
library(data.tree)
plot(FromDataFrameNetwork(treeWithInternal))
plot(FromDataFrameNetwork(treeNoInternal))


library(ape)
oldpar &lt;- par("mfcol")
par(mfcol = c(2,3))

phylo &lt;- asPhylo(treeWithInternal, height = 20)
plot(phylo, main = "reduce = FALSE\nheight = 20", cex = 1)
edgelabels(round(phylo$edge.length), cex = 1.5)

phylo &lt;- asPhylo(treeNoInternal, height = 20)
plot(phylo, main = "reduce = TRUE\nheight = 20", cex = 1)
edgelabels(round(phylo$edge.length), cex = 1.5)

phylo &lt;- asPhylo(treeWithInternal, fixed.branches = 1)
plot(phylo, main = "reduce = FALSE\nfixed.branches = 1", cex = 1)
edgelabels(round(phylo$edge.length), cex = 1.5)

phylo &lt;- asPhylo(treeNoInternal, fixed.branches = 1)
plot(phylo, main = "reduce = TRUE\nfixed.branches = 1", cex = 1)
edgelabels(round(phylo$edge.length), cex = 1.5)

phylo &lt;- asPhylo(treeWithInternal, fixed.branches = 1, long.root = 10)
plot(phylo, main = "reduce = FALSE\nfixed.branches = 1, long.root = 10", cex = 1)
edgelabels(round(phylo$edge.length), cex = 1.5)

phylo &lt;- asPhylo(treeNoInternal, fixed.branches = 1, long.root = 10)
plot(phylo, main = "reduce = TRUE\nfixed.branches = 1, long.root = 10", cex = 1)
edgelabels(round(phylo$edge.length), cex = 1.5)

par(mfcol = oldpar)

</code></pre>

<hr>
<h2 id='getTree'>
Extract parts out of the full Glottolog 2016 tree
</h2><span id='topic+getTree'></span>

<h3>Description</h3>

<p>getTree is a convenience function to extract parts from the Glottolog 2016 data as provided here in <code><a href="#topic+glottolog">glottolog</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getTree(up = NULL, kind = "iso", down = NULL, reduce = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="getTree_+3A_up">up</code></td>
<td>

<p>a vector of names from which to extract the tree upwards. Can be names from Glottolog, ISO 639-3 codes, WALS codes or glottocodes. Default settings expect ISO 639-3 codes.
</p>
</td></tr>
<tr><td><code id="getTree_+3A_kind">kind</code></td>
<td>

<p>what kind of names are specified in <code>up</code>? Choose one of <code>iso</code>, <code>wals</code>, <code>glottocode</code> or <code>name</code> for full names from Glottolog.
</p>
</td></tr>
<tr><td><code id="getTree_+3A_down">down</code></td>
<td>

<p>a vector of family names from Glottolog from which to extract the tree downwards. For consistency, a node &quot;World&quot; as added on top, linking separate families. Specifying families that are part of other families in <code>down</code> will lead to a warning, but any overlap will be gracefully removed.
</p>
</td></tr>
<tr><td><code id="getTree_+3A_reduce">reduce</code></td>
<td>

<p>remove all nodes in the tree that do not branch.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Specifying both <code>up</code> and <code>down</code> will extract only the intersection of the two. So any name in <code>up</code> that lies outside any family in <code>down</code> will be discarded.
</p>


<h3>Value</h3>

<p>Returns a data.frame with the relevant lines from the full glottolog data.
</p>


<h3>Note</h3>

<p>This function is hard-coded to only use the data as available in <code><a href="#topic+glottolog">glottolog</a></code>.
</p>


<h3>Author(s)</h3>

<p>Michael Cysouw &lt;cysouw@mac.com
</p>


<h3>See Also</h3>

<p><code><a href="#topic+asPhylo">asPhylo</a></code> for tweaking branch lengths in phylo conversion.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># use getTree() to select genealogical parts of the data
data(glottolog)

( aalawa &lt;- getTree(up = "aala1237", kind = "glottocode") )
( kandas &lt;- getTree(down = "Kandas-Duke of York") )
( treeFull &lt;- getTree(up = c("deu", "eng", "ind", "cha"), kind = "iso") )
( treeReduced &lt;- getTree(up = c("deu", "eng", "ind", "cha"), kind = "iso", reduce = TRUE) )


# use FromDataFrameNetwork() to visualize the tree
# and export it into various other tree formats in R

library(data.tree)
treeF &lt;- FromDataFrameNetwork(treeFull)
treeR &lt;- FromDataFrameNetwork(treeReduced)

plot(treeF)
plot(treeR)

# turn into phylo format from library 'ape'
t &lt;- as.phylo.Node(treeR)
plot(t)

# turn into dendrogram
t &lt;- as.dendrogram(treeF)
plot(t, center = TRUE)

</code></pre>

<hr>
<h2 id='glottolog'>
Glottolog data from <a href="https://glottolog.org">https://glottolog.org</a>
</h2><span id='topic+glottolog'></span>

<h3>Description</h3>

<p>Data from Glottolog 2016 with added WALS codes and speaker-community size. Various minor corrections and additions were performed in the preparation of the data (see Details). All stocks (i.e. largest reconstructable units) are linked to macroareas, and they are linked to a single root node calles 'World'.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("glottolog")</code></pre>


<h3>Format</h3>

<p>A data frame with 22007 observations on the following 10 variables.
</p>

<dl>
<dt><code>name</code></dt><dd><p>a character vector with the name of the entity.</p>
</dd>
<dt><code>father</code></dt><dd><p>a character vector with the name of the direct parent entity.</p>
</dd>
<dt><code>stock</code></dt><dd><p>a factor with the highest reconstructable unit. This column is added just for convenience, it does not add any new information.</p>
</dd>
<dt><code>glottocode</code></dt><dd><p>a character vector with the glottocode. The same identifier is added as rownames of the data.</p>
</dd>
<dt><code>iso</code></dt><dd><p>a character vector with ISO 639-3 language codes</p>
</dd>
<dt><code>wals</code></dt><dd><p>a character vector with WALS language codes</p>
</dd>
<dt><code>type</code></dt><dd><p>a factor with levels <code>dialect</code>, <code>family</code> and <code>language</code></p>
</dd>
<dt><code>longitude</code></dt><dd><p>a numeric vector with geographic coordinates as available in the Glottolog</p>
</dd>
<dt><code>latitude</code></dt><dd><p>a numeric vector with geographic coordinates as available in the Glottolog</p>
</dd>
<dt><code>population</code></dt><dd><p>a numeric vector with speaker community size from an old Ethnologue version (13th Edition), licensed to the MPI-EVA in Leipzig.</p>
</dd>
</dl>



<h3>Details</h3>

<p>For Glottolog data: the names were uniquified by adding a glottocode when a name occurs more than once (typically in some cases of a language and a family having the same name). Entries classified as 'bookkeeping', 'unattested' 'artificial language', 'sign language', 'speech register' and 'unclassifiable' were removed. Links to WALS codes were added: note that about 20 links are missing, and for the non-unique links one link was chosen by data availability. Some macro codes from ISO 639-3 were added.
</p>
<p>A level 'area' was added to the tree, separating all languages in six areas: Eurasia, Africa, Southeast Asia, Sahul, North America and South America. This is reminiscent of the proposal from Dryer (1992), though Austronesian is grouped with Southeast Asia here, because that makes more sense genealogically. Still, these nodes are surely not monophyletic! Mixed languages are not assigned to an area.
</p>
<p>Please note that the data provided here is not identical to the online version of Glottolog, as the online version is constantly being updated! This is Glottolog 2016. Updates might be made available when they are provided for download from the website.
</p>
<p>The format of the glottolog data might seem a bit convoluted, but by using <code><a href="#topic+getTree">getTree</a></code> it is actually really easy to extract genealogical parts of the glottolog data and by using <code><a href="data.tree.html#topic+FromDataFrameNetwork">FromDataFrameNetwork</a></code> this can be nicely plotted and turned into various tree format as used in R.
</p>


<h3>Source</h3>

<p>Glottolog 2016 data from <a href="https://glottolog.org">https://glottolog.org</a>. WALS 2013 data from <a href="https://glottolog.org">https://glottolog.org</a>. Information on macrolanguages from <a href="https://iso639-3.sil.org/code_tables/macrolanguage_mappings/data">https://iso639-3.sil.org/code_tables/macrolanguage_mappings/data</a>. All data downloaded in March 2017. Population numbers are from the 13th edition of the Ethnologue, licenced to the MPI-EVA in Leipzig.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># use getTree() to select genealogical parts of the data
data(glottolog)

( aalawa &lt;- getTree(up = "aala1237", kind = "glottocode") )
( kandas &lt;- getTree(down = "Kandas-Duke of York") )
( treeFull &lt;- getTree(up = c("deu", "eng", "ind", "cha"), kind = "iso") )
( treeReduced &lt;- getTree(up = c("deu", "eng", "ind", "cha"), kind = "iso", reduce = TRUE) )

# check out areas
( areas &lt;- glottolog[glottolog$type == "area", "name"] )
# stocks in Southeast Asia
glottolog[glottolog$father == areas[1], "name"]


# use FromDataFrameNetwork() to visualize the tree
# and export it into various other tree formats in R

library(data.tree)
treeF &lt;- FromDataFrameNetwork(treeFull)
treeR &lt;- FromDataFrameNetwork(treeReduced)

plot(treeF)
plot(treeR)

# turn into phylo format from library 'ape'
t &lt;- as.phylo.Node(treeR)
plot(t)

# turn into dendrogram
t &lt;- as.dendrogram(treeF)
plot(t, center = TRUE)

</code></pre>

<hr>
<h2 id='join_align'>
Join various multialignments into one combined dataframe
</h2><span id='topic+join_align'></span><span id='topic+join.align'></span>

<h3>Description</h3>

<p>Multialignments are mostly stored in separate files per cognateset. This function can be used to bind together a list of multialignments read by <a href="#topic+read_align">read_align</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>join_align(alignments)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="join_align_+3A_alignments">alignments</code></td>
<td>

<p>A list of objects as returned from <a href="#topic+read_align">read_align</a>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The alignments have to be reordered for this to work properly. Also, duplicate data (i.e. multiple words from the same language) will be removed. Simply the first occurrence is retained. This is not ideal, but it is currently the best and easiest solution.
</p>


<h3>Value</h3>

<p>The result will be a dataframe with <code>doculects</code> as rows and <code>alignments</code> as columns.
</p>


<h3>Author(s)</h3>

<p>Michael Cysouw &lt;cysouw@mac.com&gt;
</p>

<hr>
<h2 id='pass_align'>
Transfer alignment from one string to another
</h2><span id='topic+pass_align'></span>

<h3>Description</h3>

<p>In the alignment of linguistic strings, it is often better to perform the alignment on a simplified string. This function allows to pass back the alignment from the simplified string to the original
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pass_align(originals, alignment, sep = " ", in.gap = "-", out.gap = "-")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pass_align_+3A_originals">originals</code></td>
<td>

<p>Vector of strings in the original form, with separators
</p>
</td></tr>
<tr><td><code id="pass_align_+3A_alignment">alignment</code></td>
<td>

<p>Vector of simplified strings after alignment, with separators and gaps. The number of non-gap parts should match the number of parts of the originals
</p>
</td></tr>
<tr><td><code id="pass_align_+3A_sep">sep</code></td>
<td>

<p>Symbol used as separator between parts of the strings
</p>
</td></tr>
<tr><td><code id="pass_align_+3A_in.gap">in.gap</code></td>
<td>

<p>Symbol used as gap indicator in the alignments
</p>
</td></tr>
<tr><td><code id="pass_align_+3A_out.gap">out.gap</code></td>
<td>

<p>Symbol used as gap indicator in the output. This is useful when the gap symbol from the alignments occurs as character in the originals .
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Given some strings, a sound (or graphemic) alignment inserts gaps into the strings in such a way as to align the columns between different strings. We assume here an original string that is separated by <code>sep</code> into parts (segments, sounds, tailored grapheme clusters). After simplification (e.g. through <code><a href="#topic+tokenize">tokenize</a></code>) and alignment (currently using non-R software) a string is retuned with extra <code>gap</code>s inserted. The number of non-gap parts should match the original string.
</p>


<h3>Value</h3>

<p>Vector of original strings with the gaps inserted from the aligned strings.
</p>


<h3>Note</h3>

<p>There is a bash-executable distributed with this package (based on the <code>docopt</code> package) that let you use this function directly in a bash-terminal. The easiest way to use this executable is to softlink the executable to some directory in your bash PATH, for example <code>/usr/local/bin</code> or simply <code>~/bin</code>. To softlink the function <code>tokenize</code> to this directory, use something like the following in your bash terminal:
</p>
<p><code>ln -is `Rscript -e 'cat(system.file("exec/pass_align", package="qlcData"))'` ~/bin</code>
</p>
<p>From within R your can also use the following (again, optionally changing the linked-to directory from <code>~/bin</code> to anything more suitable on your system):
</p>
<p><code>file.symlink(system.file("exec/pass_align", package="qlcData"), "~/bin")</code>
</p>


<h3>Author(s)</h3>

<p>Michael Cysouw &lt;cysouw@mac.com&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'># make some strings with separators
l &lt;- list(letters[1:3], letters[4:7], letters[10:15])
originals &lt;- sapply(l, paste, collapse = " ")
cbind(originals)

# make some alignment
# note that this alignment is non-sensical!
alignment &lt;- c("X - - - X - X", "X X - - - X X", "X X X - X X X")
cbind(alignment)

# match originals to the alignment
transferred &lt;- pass_align(originals, alignment)
cbind(transferred)

# ========

# a slighly more interesting example
# using the bare-bones pairwise alignment from adist()
originals &lt;- c("cute kitten class","utter tentacles")
cbind(originals)

# adist returns strings of pairwise Levenshtein operations
# "I" signals insertion
(levenshtein &lt;- attr(adist(originals, counts = TRUE), "trafos"))

# pass alignments to original strings, show the insertions as "-" gaps
alignment &lt;- c(levenshtein[1,2], levenshtein[2,1])
transferred &lt;- pass_align(originals, alignment, 
    sep = "", in.gap = "I", out.gap = "-")
cbind(transferred)

</code></pre>

<hr>
<h2 id='read_align'>
Reading different versions of linguistic multialignments.
</h2><span id='topic+read_align'></span>

<h3>Description</h3>

<p>Multialignments of strings are a central step for historical linguistics (quite similar to multialignments in bioinformatics). There is no consensus (yet) about the file-structure for multialignments in linguistics. Currently, this functions offers to read various flavours of multialignment, trying to harmonize the internal R-structure.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>read_align(file, flavor)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="read_align_+3A_file">file</code></td>
<td>

<p>Multialignment to be read
</p>
</td></tr>
<tr><td><code id="read_align_+3A_flavor">flavor</code></td>
<td>

<p>Currently two flavours are implemented <code>"PAD"</code> and <code>"BDPA"</code>
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The flavor <code>"PAD"</code> refers to the Phonetische Atlas Deutschlands, which provides multialignments for german dialects. The flavor <code>"BDPA"</code> refers to the Benchmark Database for Phonetic Alignments.
</p>


<h3>Value</h3>

<p>Multialignment-files often contain various different kinds of information. An attempt is made to turn the data into a list with the following elements:
</p>
<table>
<tr><td><code>meta</code></td>
<td>
<p>: Metadata</p>
</td></tr>
<tr><td><code>align</code></td>
<td>
<p>: The actual alignments as a dataframe. When IDs are present in the original file, they are used as rownames. Some attempt is made to add useful column names.</p>
</td></tr>
<tr><td><code>doculects</code></td>
<td>
<p>: The rows of the alignment normally are some kind of doculects (&quot;languages&quot;, &quot;dialects&quot;). However, because these doculects might occur more than once (when two different, but cognate words from a languages are included) these names are not used as rownames of <code>$align</code>, but presented separately here.</p>
</td></tr>
<tr><td><code>annotations</code></td>
<td>
<p>: The columns of a multialignment can have annotations, e.g. metathesis or orthographic standard. These annotations are saved here as a dataframe with the same number of columns as the <code>$align</code> dataframe. The name of the annotation is put in the rownames.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michael Cysouw &lt;cysouw@mac.com&gt;
</p>


<h3>References</h3>

<p>BDPA is available at <a href="https://alignments.lingpy.org">https://alignments.lingpy.org</a>. PAD is available at <a href="https://github.com/cysouw/PAD/">https://github.com/cysouw/PAD/</a>
</p>

<hr>
<h2 id='recode'>
Recoding nominal data
</h2><span id='topic+recode'></span>

<h3>Description</h3>

<p>Nominal data (&lsquo;categorical data&rsquo;) are data that consist of attributes, and each attribute consists of various discrete values (&lsquo;types&rsquo;). The different values that are distinguished in comparative linguistics are mostly open to debate, and different scholars like to make different decisions as to the definition of values. The <code>recode</code> function allows for an easy and transparent way to specify a recoding of an existing dataset.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>recode(recoding, data = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="recode_+3A_recoding">recoding</code></td>
<td>

<p>a <code>recoding</code> data structure, specifying the decisions of the recoding. It can also be a path to a file containing the specifications in YAML format. See Details.
</p>
</td></tr>
<tr><td><code id="recode_+3A_data">data</code></td>
<td>

<p>a data frame with nominal data, attributes as columns, observations as rows. If nothing is provided, an attempt is made to read the data with <code><a href="utils.html#topic+read.csv">read.csv</a></code> from the relative path provided in <code>originalData</code> in the metadata of the recoding.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Recoding nominal data is normally considered too complex to be performed purely within R. It is possible to do it completely within R, but it is proposed here to use an external YAML document to specify the decisions that are taken in the recoding. The typical process of recoding will be to use <a href="#topic+write.recoding">write.recoding</a> to prepare a skeleton that allows for quick and easy YAML-specification of a recoding. Or a YAML-recoding is written manually using various shortcuts (see below), and <a href="#topic+read.recoding">read.recoding</a> is used to turn it into a full-fledged recoding that can also be used to document the decisions made. The function <code>recode</code> then combines the original data with the recoding, and produces a recoded dataframe.
</p>
<p>The <code>recoding data structure</code> in the YAML document basically consists of a list of recodings, each of which describes a new attribute, based on one or more attributes from the original data. Each new attribute is described by:
</p>

<ul>
<li> <p><em>attribute</em>: the new attribute name.
</p>
</li>
<li> <p><em>values</em>: a character vector with the new value names.
</p>
</li>
<li> <p><em>link</em>: a numeric vector with length of the original number of values. Each entry specifies the number of the new value. Zero can be used for any values that should be ignored in the new attribute.
</p>
</li>
<li> <p><em>recodingOf</em>: the name(s) of the original attribute that forms the basis of the recoding. If there are multiple attributes listed, then the new attribute will be a combination of the original attributes.
</p>
</li>
<li> <p><em>OriginalValues</em>: a character vector with the value names from the original attribute. These are only added to the template to make it easier to specify the recoding. In the actual recoding the listing in this file will be ignored. It is important to keep the ordering as specified, otherwise the linking will be wrong. The ordering of the values follows the result of <code>levels</code>, which is determined by the current locale.
</p>
</li></ul>

<p>There is a vignette available with detailed information about the process of recoding, check <code>recoding nominal data</code>.
</p>


<h3>Value</h3>

<p><code>recode</code> returns a data frame with the recoded attributes
</p>


<h3>Author(s)</h3>

<p>Michael Cysouw &lt;cysouw@mac.com&gt;
</p>


<h3>References</h3>

<p>Cysouw, Michael, Jeffrey Craig Good, Mihai Albu and Hans-Jörg Bibiko. 2005. Can GOLD &quot;cope&quot; with WALS? Retrofitting an ontology onto the World Atlas of Language Structures. <em>Proceedings of E-MELD Workshop 2005</em>, <a href="https://emeld.org/workshop/2005/papers/good-paper.pdf">https://emeld.org/workshop/2005/papers/good-paper.pdf</a>
</p>


<h3>See Also</h3>

<p>The World Atlas of Language Structure (WALS) contains typical data that most people would very much like to recode before using for further analysis. See Cysouw et al. 2005 for a discussion of various issues surrounding the WALS data.
</p>

<hr>
<h2 id='tokenize'>
Tokenization and transliteration of character strings based on an orthography profile
</h2><span id='topic+tokenize'></span>

<h3>Description</h3>

<p>To process strings it is often very useful to tokenise them into graphemes (i.e. functional units of the orthography), and possibly replace those graphemes by other symbols to harmonize the orthographic representation of different orthographic representations (&lsquo;transcription/transliteration&rsquo;). As a quick and easy way to specify, save, and document the decisions taken for the tokenization, we propose using an orthography profile.
</p>
<p>This function is the main function to produce, test and apply orthography profiles.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tokenize(strings, 
  profile = NULL, transliterate = NULL,
  method = "global", ordering = c("size", "context", "reverse"),
  sep = " ", sep.replace = NULL, missing = "\u2047", normalize = "NFC",
  regex = FALSE, silent = FALSE,
  file.out = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tokenize_+3A_strings">strings</code></td>
<td>

<p>Vector of strings to the tokenized. It is also possibly to pass a filename, which will then simply be read as <code>scan(strings, sep = "\n", what = "character")</code>.
</p>
</td></tr>
<tr><td><code id="tokenize_+3A_profile">profile</code></td>
<td>

<p>Orthography profile specifying the graphemes for the tokenization, and possibly any replacements of the available graphemes. Can be a reference to a file or an R object. If NULL then the orthography profile will be created on the fly using the defaults of <code>write.profile</code>.
</p>
</td></tr>
<tr><td><code id="tokenize_+3A_transliterate">transliterate</code></td>
<td>

<p>Default <code>NULL</code>, meaning no transliteration is to be performed. Alternatively, specify the name of the column in the orthography profile that should be used for replacement.
</p>
</td></tr>
<tr><td><code id="tokenize_+3A_method">method</code></td>
<td>

<p>Method to be used for parsing the strings into graphemes. Currently two options are implemented: <code>global</code> and <code>linear</code>. See Details for further explanation.
</p>
</td></tr>
<tr><td><code id="tokenize_+3A_ordering">ordering</code></td>
<td>

<p>Method for ordering. Currently three different methods are implemented, which can be combined (see Details below): <code>size</code>, <code>context</code>, <code>reverse</code> and <code>frequency</code>. Use <code>NULL</code> to prevent ordering and use the top to bottom order as specified in the orthography profile.
</p>
</td></tr>
<tr><td><code id="tokenize_+3A_sep">sep</code></td>
<td>

<p>Separator to be inserted between graphemes. Defaults to space. This function assumes that the separator specified here does not occur in the data. If it does, unexpected things might happen. Consider removing the chosen seperator from your strings first, e.g. by using <code><a href="base.html#topic+gsub">gsub</a></code> or use the option <code>sep.replace</code>.
</p>
</td></tr>
<tr><td><code id="tokenize_+3A_sep.replace">sep.replace</code></td>
<td>

<p>Sometimes, the chosen separator (see above) occurs in the strings to be parsed. This is technically not a problem, but the result might show unexpected sequences. When <code>sep.replace</code> is specified, this marking is inserted in the string at those places where the <code>sep</code> marker occurs. Typical usage in linguistics would be <code>sep = " ", sep.replace = "#"</code> adding spaces between graphemes and replacing spaces in the input string by hashes in the output string.
</p>
</td></tr>
<tr><td><code id="tokenize_+3A_missing">missing</code></td>
<td>

<p>Character to be inserted at transliteration when no transliteration is specified. Defaults to DOUBLE QUESTION MARK at U+2047. Change this when this character appears in the input string.
</p>
</td></tr>
<tr><td><code id="tokenize_+3A_normalize">normalize</code></td>
<td>

<p>Which normalization to use before tokenization, defaults to &quot;NFC&quot;. Other option is &quot;NFD&quot;. Any other input will result in no normalisation being performed.
</p>
</td></tr>
<tr><td><code id="tokenize_+3A_regex">regex</code></td>
<td>

<p>Logical: when <code>regex = FALSE</code> internally the matching of graphemes is done exact, i.e. without using regular expressions. When <code>regex = TRUE</code> ICU-style regular expression (see <code><a href="stringi.html#topic+stringi-search-regex">stringi-search-regex</a></code>) are used for all content in the profile (including the Grapheme-column!), so any reserved characters have to be escaped in the orthography profile. Specifically, add a slash &quot;\&quot; before any occurrence of the characters <code> [](){}|+*.-!?ˆ$\ </code> in your profile (except of course when these characters are used in their regular expression meaning).
</p>
<p>Note that this parameter also influences whether contexts should be considered in the tokenization (internally, contextual searching uses regular expressions). By default, when <code>regex = FALSE</code>, context is ignored. If <code>regex = TRUE</code> then the function checks whether there are columns called <code>Left</code> (for the left context) and <code>Right</code> (for the right context), and optionally a column called <code>Class</code> (for the specification of grapheme-classes) in the orthography profile. These are hard-coded column-names, so please adapt your orthography profile accordingly. The columns <code>Left</code> and <code>Right</code> allow for regular expression to specify context.
</p>
</td></tr>
<tr><td><code id="tokenize_+3A_silent">silent</code></td>
<td>

<p>Logical: by default missing characters in the strings are reported with a warning. use <code>silent = TRUE</code> to supress these warnings.
</p>
</td></tr>
<tr><td><code id="tokenize_+3A_file.out">file.out</code></td>
<td>

<p>Filename for results to be written. No suffix should be specified, as various different files with different suffixes are produced (see Details below). When <code>file.out</code> is specified, then the data is written to disk AND the R dataframe is returned invisibly.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Given a set of graphemes, there are at least two different methods to tokenize strings. The first is called <code>global</code> here: this approach takes the first grapheme, matches this grapheme globally at all places in the string, and then turns to the next string. The other approach is called <code>linear</code> here: this approach walks through the string from left to right. At the first character it looks through all graphemes whether there is any match, and then walks further to the end of the match and starts again. In some special cases these two methods can lead to different results (see Examples).
</p>
<p>The ordering or the lines in the ortography profile is of crucial importance, and different orderings will lead to radically different results. To simply use the top to bottom ordering as specified in the profile, use <code>order = NULL</code>. Currently, there are four different ordering strategies implemented: <code>size</code>, <code>context</code>, <code>reverse</code> and <code>frequency</code>. By specifying more than one in a vector, these orderings are used to break ties, e.g. the default specification <code>c("size", "context", "reverse")</code> will first order by size, and for those with the same size, it will order by whether any context is specifed (with context coming first). For lines that are still tied (i.e. the have the same size and all either have or have no context) the order will be reversed in comparison to the order as attested in the profile. Reversing order can be useful, because hand-written profiles tend to put general rules before specific rules, which mostly should be applied in reverse order.
</p>

<ul>
<li> <p><code>size</code>: order the lines in the profile by the size of the grapheme, largest first. Size is measured by number of Unicode characters after normalization as specified in the option <code>normalize</code>. For example, <code>é</code> has a size of 1 with <code>normalize = "NFC"</code>, but a size of 2 with <code>normalize = "NFD"</code>.
</p>
</li>
<li> <p><code>context:</code> order the lines by whether they have any context specified, lines with context coming first. Note that this only works when the option <code>regex = TRUE</code> is also chosen.
</p>
</li>
<li> <p><code>reverse:</code> order the lines from bottom to top.
</p>
</li>
<li> <p><code>frequency</code>: order the lines by the frequency with which they match in the specified strings before tokenization, least frequent coming first. This frequency of course depends crucially on the available strings, so it will lead to different orderings when applied to different data. Also note that this frequency is (necessarily) measured before graphemes are identified, so these ordering frequencies are not the same as the final frequencies shown in the outpur. Frequency of course also strongly differs on whether context is used for the matching through <code>regex = TRUE</code>.
</p>
</li></ul>



<h3>Value</h3>

<p>Without specificatino of <code>file.out</code>, the function <code>tokenize</code> will return a list of four:
</p>
<table>
<tr><td><code>strings</code></td>
<td>
<p>a dataframe with the original and the tokenized/transliterated strings</p>
</td></tr>
<tr><td><code>profile</code></td>
<td>
<p>a dataframe with the graphemes with added frequencies. The dataframe is ordered according to the order that resulted from the specifications in <code>ordering</code>.</p>
</td></tr>
<tr><td><code>errors</code></td>
<td>
<p>a dataframe with all original strings that contain unmatched parts.</p>
</td></tr>
<tr><td><code>missing</code></td>
<td>
<p>a dataframe with the graphemes that are missing from the original orthography profilr, as indicated in the errors. Note that the report of missing characters does currently not lead to correct results for transliterated strings.</p>
</td></tr>
</table>
<p>When <code>file.out</code> is specified, these four tables will be written to three different tab-separated files (with header lines): <code>file_strings.tsv</code> for the strings, <code>file_profile.tsv</code> for the orthrography profile, <code>file_errors.tsv</code> for the strings that have unidentifyable parts, and <code>file_missing.tsv</code> for the graphemes that seem to be missing. When there is nothing missing, then no file for the missing strings is produced.
</p>


<h3>Note</h3>

<p>When <code>regex = TRUE</code>, regular expressions are acceptable in the columns &lsquo;Grapheme', &rsquo;Left' and 'Right'. Backreferences in the transliteration column are not possible (yet). When regular expressions are allowed, all literal uses of special regex-characters have to be escaped! Any literal occurrence of the following characters has then to be preceded by a backslash <code> \ </code>.
</p>

<ul>
<li><p> - (U+002D, HYPHEN-MINUS)
</p>
</li>
<li><p> ! (U+0021, EXCLAMATION MARK)
</p>
</li>
<li><p> ? (U+003F, QUESTION MARK)
</p>
</li>
<li><p> . (U+002E, FULL STOP)
</p>
</li>
<li><p> ( (U+0028, LEFT PARENTHESIS)
</p>
</li>
<li><p> ) (U+0029, RIGHT PARENTHESIS)
</p>
</li>
<li><p> \[ (U+005B, LEFT SQUARE BRACKET)
</p>
</li>
<li><p> \] (U+005D, RIGHT SQUARE BRACKET)
</p>
</li>
<li><p> { (U+007B, LEFT CURLY BRACKET)
</p>
</li>
<li><p> } (U+007D, RIGHT CURLY BRACKET)
</p>
</li>
<li><p> | (007C, VERTICAL LINE)
</p>
</li>
<li><p> * (U+002A, ASTERISK)
</p>
</li>
<li><p> \ (U+005C, REVERSE SOLIDUS)
</p>
</li>
<li><p> ˆ (U+005E, CIRCUMFLEX ACCENT)
</p>
</li>
<li><p> + (U+002B, PLUS SIGN)
</p>
</li>
<li><p> $ (U+0024, DOLLAR SIGN)
</p>
</li></ul>

<p>Note that overlapping matching does not (yet) work with regular expressions. That means that for example &quot;aa&quot; is only found once in &quot;aaa&quot;. In some special cases this might lead to problems that might have to be explicitly specified in the profile, e.g. a grapheme &quot;aa&quot; with a left context &quot;a&quot;. See examples below. This problem arises because overlap is only available in literal searches <code><a href="stringi.html#topic+stri_opts_fixed">stri_opts_fixed</a></code>, but the current function uses regex-searching, which does not catch overlap <code><a href="stringi.html#topic+stri_opts_regex">stri_opts_regex</a></code>.
</p>


<h3>Note</h3>

<p>There is a bash-executable distributed with this package (based on the <code>docopt</code> package) that let you use this function directly in a bash-terminal. The easiest way to use this executable is to softlink the executable to some directory in your bash PATH, for example <code>/usr/local/bin</code> or simply <code>~/bin</code>. To softlink the function <code>tokenize</code> to this directory, use something like the following in your bash terminal:
</p>
<p><code>ln -is `Rscript -e 'cat(system.file("exec/tokenize", package="qlcData"))'` ~/bin</code>
</p>
<p>From within R your can also use the following (again, optionally changing the linked-to directory from <code>~/bin</code> to anything more suitable on your system):
</p>
<p><code>file.symlink(system.file("exec/tokenize", package="qlcData"), "~/bin")</code>
</p>


<h3>Author(s)</h3>

<p>Michael Cysouw &lt;cysouw@mac.com&gt;
</p>


<h3>References</h3>

<p>Moran &amp; Cysouw (forthcoming)
</p>


<h3>See Also</h3>

<p>See also <code><a href="#topic+write.profile">write.profile</a></code> for preparing a skeleton orthography profile.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># simple example with interesting warning and error reporting
# the string might look like "AABB" but it isn't...
(string &lt;- "\u0041\u0410\u0042\u0412")
tokenize(string,c("A","B"))

# make an ad-hoc orthography profile
profile &lt;- cbind(
    Grapheme = c("a","ä","n","ng","ch","sch"), 
    Trans = c("a","e","n","N","x","sh"))
# tokenization
tokenize(c("nana", "änngschä", "ach"), profile)
# with replacements and a warning
tokenize(c("Naná", "änngschä", "ach"), profile, transliterate = "Trans")

# different results of ordering
tokenize("aaa", c("a","aa"), order = NULL)
tokenize("aaa", c("a","aa"), order = "size")

# regexmatching does not catch overlap, which can lead to wrong results
# the second example results in a warning instead of just parsing "ab bb"
# this should occur only rarely in natural language
tokenize("abbb", profile = c("ab","bb"), order = NULL)
tokenize("abbb", profile = c("ab","bb"), order = NULL, regex = TRUE)

# different parsing methods can lead to different results
# note that in natural language this is VERY unlikely to happen
tokenize("abc", c("bc","ab","a","c"), order = NULL, method = "global")$strings
tokenize("abc", c("bc","ab","a","c"), order = NULL, method = "linear")$strings
</code></pre>

<hr>
<h2 id='write.profile'>
Writing (and reading) of an orthography profile skeleton
</h2><span id='topic+write.profile'></span><span id='topic+read.profile'></span>

<h3>Description</h3>

<p>To process strings, it is often very useful to tokenise them into graphemes (i.e. functional units of the orthography), and possibly replace those graphemes by other symbols to harmonize the orthographic representation of different orthographic representations (&lsquo;transcription&rsquo;). As a quick and easy way to specify, save, and document the decisions taken for the tokenization, we propose using an orthography profile. 
</p>
<p>Provided here is a function to prepare a skeleton for an orthography profile. This function takes some strings and lists detailed information on the Unicode characters in the strings.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>write.profile(strings, 
    normalize = NULL, info = TRUE, editing = FALSE, sep = NULL, 
    file.out = NULL, collation.locale = NULL)

read.profile(profile)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="write.profile_+3A_strings">strings</code></td>
<td>

<p>A vector of strings on which to base the orthography profile. It is also possibly to pass a filename, which will then simply be read as <code>scan(strings, sep = "\n", what = "character")</code>.
</p>
</td></tr>
<tr><td><code id="write.profile_+3A_normalize">normalize</code></td>
<td>

<p>Should any unicode normalization be applied before making a profile? By default, no normalization is applied, giving direct feedback on the actual encoding as observed in the strings. Other options are <code>NFC</code> and <code>NFD</code>. In combination with <code>sep</code> these options can lead to different insights into the structure of your strings (see examples below).
</p>
</td></tr>
<tr><td><code id="write.profile_+3A_info">info</code></td>
<td>

<p>Add columns with Unicode information on the graphemes: Unicode code points, Unicode names, and frequency of occurrence in the input strings.
</p>
</td></tr>
<tr><td><code id="write.profile_+3A_editing">editing</code></td>
<td>

<p>Add empty columns for further editing of the orthography profile: left context, right context, class, and translitation. See <code><a href="#topic+tokenize">tokenize</a></code> for detailed information on their usage.
</p>
</td></tr>
<tr><td><code id="write.profile_+3A_sep">sep</code></td>
<td>

<p>separator to separate the strings. When NULL (by default), then unicode character definitions are used to split (as provided by UCI, ported to R by <code>stringi::stri_split_boundaries</code>. When <code>sep</code> is specified, strings are split by this separator. Often useful is <code>sep = ""</code> to split by unicode codepoints (see examples below).
</p>
</td></tr>
<tr><td><code id="write.profile_+3A_file.out">file.out</code></td>
<td>

<p>Filename for writing the profile to disk. When <code>NULL</code> the profile is returned as an R dataframe consisting of strings. When <code>file.out</code> is specified (as a path to a file), then the profile is written to disk and the R dataframe is returned invisibly.
</p>
</td></tr>
<tr><td><code id="write.profile_+3A_collation.locale">collation.locale</code></td>
<td>

<p>Specify to ordering to be used in writing the profile. By default it uses the ordering as specified in the current locale (check <code>Sys.getlocale("LC_COLLATE")</code>).
</p>
</td></tr>
<tr><td><code id="write.profile_+3A_profile">profile</code></td>
<td>

<p>An orthography profile to be read. Has to be a tab-delimited file with a header. There should be at least a column called &quot;Grapheme&quot;.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>String are devided into default grapheme clusters as defined by the Unicode specification. Underlying code is due to the UCI as ported to R in the <code>stringi</code> package.
</p>


<h3>Value</h3>

<p>A dataframe with strings representing a skeleton of an orthography profile.
</p>


<h3>Note</h3>

<p>There is a bash-executable distributed with this package (based on the <code>docopt</code> package) that let you use this function directly in a bash-terminal. The easiest way to use this executable is to softlink the executable to some directory in your bash PATH, for example <code>/usr/local/bin</code> or simply <code>~/bin</code>. To softlink the function <code>tokenize</code> to this directory, use something like the following in your bash terminal:
</p>
<p><code>ln -is `Rscript -e 'cat(system.file("exec/writeprofile", package="qlcData"))'` ~/bin</code>
</p>
<p>From within R your can also use the following (again, optionally changing the linked-to directory from <code>~/bin</code> to anything more suitable on your system):
</p>
<p><code>file.symlink(system.file("exec/writeprofile", package="qlcData"), "~/bin")</code>
</p>


<h3>Author(s)</h3>

<p>Michael Cysouw &lt;cysouw@mac.com&gt;
</p>


<h3>References</h3>

<p>Moran &amp; Cysouw (2018) &quot;The Unicode cookbook for linguists&quot;. Language Science Press. &lt;doi:10.5281/zenodo.1296780&gt;.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+tokenize">tokenize</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># produce statistics, showing two different kinds of "A"s in Unicode.
# look at the output of "example" in the console to get the point!
(example &lt;- "\u0041\u0391\u0410")
write.profile(example)

# note the differences. Again, look at the example in the console!
(example &lt;- "\u00d9\u00da\u00db\u0055\u0300\u0055\u0301\u0055\u0302")
# default settings
write.profile(example)
# split according to unicode codepoints
write.profile(example, sep = "")
# after NFC normalization unicode codepoints have changed
write.profile(example, normalize = "NFC", sep = "")
# NFD normalization gives yet another structure of the codepoints
write.profile(example, normalize = "NFD", sep = "")
# note that NFC and NFD normalization are identical under unicode character definitions!
write.profile(example, normalize = "NFD")
write.profile(example, normalize = "NFC")
</code></pre>

<hr>
<h2 id='write.recoding'>
Reading and writing of recoding files.
</h2><span id='topic+write.recoding'></span><span id='topic+read.recoding'></span>

<h3>Description</h3>

<p>Nominal data (&lsquo;categorical data&rsquo;) are data that consist of attributes, and each attribute consists of various discrete values (&lsquo;types&rsquo;). The different values that are distinguished in comparative linguistics are mostly open to debate, and different scholars like to make different decisions as to the definition of values. The <code><a href="#topic+recode">recode</a></code> function allows for an easy and transparent way to specify a recoding of an existing dataset. The current functions help with the preparations and usage of recoding specifications.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>write.recoding(data, attributes = NULL, all.options = FALSE, file = NULL)
read.recoding(recoding, file = NULL, data = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="write.recoding_+3A_data">data</code></td>
<td>

<p>a data frame with nominal data, attributes as columns, observations as rows. Optionally a single column can be supplied. In that case the argument <code>attributes</code> can be left unspecified.
</p>
</td></tr>
<tr><td><code id="write.recoding_+3A_recoding">recoding</code></td>
<td>

<p>a <code>recoding</code> data structure, specifying the decisions of the recoding. It can also be a path to a file containing the specifications in YAML format. See Details.
</p>
</td></tr>
<tr><td><code id="write.recoding_+3A_attributes">attributes</code></td>
<td>

<p>a list of attributes to be recoded. Vectors (as elements of the list) are possible to specify combinations of attributes to be recoded as a single complex attribute. When <code>NULL</code>, then all attributes are included individually.
</p>
</td></tr>
<tr><td><code id="write.recoding_+3A_all.options">all.options</code></td>
<td>

<p>For combinations of attributes: should all theoretical interactions of the attributes be considered (TRUE) or should only the actually existing combinations in the data be presented (FALSE, by default) in the recoding?
</p>
</td></tr>
<tr><td><code id="write.recoding_+3A_file">file</code></td>
<td>

<p>file in which the recoding should be written. The recoding template is by default written to a file in YAML format. When <code>file=NULL</code>, the template is not converted to YAML, but returned inside R as a nested list.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Recoding nominal data is normally considered too complex to be performed purely within R. It is possible to do it completely within R, but it is proposed here to use an external YAML document to specify the decisions that are taken in the recoding. The typical process of recoding will be to use <code>write.recoding.template</code> to prepare a skeleton that allows for quick and easy YAML-specification of a recoding. Or a YAML-recoding is written manually using various shortcuts (see below), and <code>read.recoding</code> is used to turn it into a full-fledged recoding that can also be used to document the decisions made. The function <code>recode</code> then combines the original data with the recoding, and produces a recoded dataframe.
</p>
<p>The <code>recoding data structure</code> in the YAML document basically consists of a list of recodings, each of which describes a new attribute, based on one or more attributes from the original data. Each new attribute is described by:
</p>

<ul>
<li> <p><em>attribute</em>: the new attribute name.
</p>
</li>
<li> <p><em>values</em>: a character vector with the new value names, optionally each value name has a linkage abbreviation (the name of the name)
</p>
</li>
<li> <p><em>link</em>: a vector with length of the original number of values. Each entry specifies a link to the new value. Zero can be used for any values that should be ignored in the new attribute. Either a pure numeric vector, using the numbering of the new values, or a vector of names of the new value names. It is also possible to use the new values here without specifying the values in the preceding item.
</p>
</li>
<li> <p><em>recodingOf</em>: the name(s) of the original attribute that forms the basis of the recoding. If there are multiple attributes listed, then the new attribute will be a combination of the original attributes.
</p>
</li>
<li> <p><em>OriginalFrequencies</em>: a character vector with the value names from the original attribute and their frequency of occurrence. These are only added to the template to make it easier to specify the recoding. In the actual recoding the listing in this file will be ignored. It is important to keep the ordering as specified, otherwise the linking will be wrong. The ordering of the values follows the result of <code>levels</code>, which is determined by the current locale.
</p>
</li></ul>

<p>For writing recodings by hand, there are various shortcuts allowed:
</p>

<ul>
<li><p> the names <code>attributes</code>, <code>values</code>, etc. can be abbreviated. The first letter should be sufficient.
</p>
</li>
<li><p> the <code>recodingOf</code> can be the full name of the attribute in the original data, or simply a number of the column in the data frame.
</p>
</li>
<li><p> the specification of <code>attribute</code> and <code>values</code> can be left out, although the result will be uninformative names like &lsquo;Att1&rsquo; and &lsquo;Val1&rsquo;.
</p>
</li>
<li><p> it is also possible to add an item <code>doNotRecode</code> with a vector of original attribute names (or column numbers). These original attributes will then be included unchanged in the recoded data table.
</p>
</li></ul>

<p>A minimal recoding consist thus of a specification of <code>recodingOf</code> and <code>link</code>. Without <code>link</code> nothing will be recoded. Omitting <code>recodingOf</code> will lead to an error.
</p>
<p>There is a vignette available with detailed information about the process of recoding, check <code>recoding nominal data</code>.
</p>


<h3>Value</h3>

<p><code>write.recoding.template</code> by default (when <code>yaml=TRUE</code>) writes a YAML structure to the specified file. When <code>yaml=FALSE</code> the same structure is returned inside R as a nested list.
</p>
<p><code>read.recoding</code> either reads a recoding from file, or a list structure within R, and cleans up all the shortcuts used. The output is by default a list structure to be used in <code>recode</code>, though it is also possible to write the result to a YAML-file (when <code>file</code> is specified). When <code>data</code> is specified, the output will be embelished with all the original names from the original data, which makes for an even better documentation of the recoding.
</p>


<h3>Author(s)</h3>

<p>Michael Cysouw &lt;cysouw@mac.com&gt;
</p>


<h3>References</h3>

<p>Cysouw, Michael, Jeffrey Craig Good, Mihai Albu and Hans-Jörg Bibiko. 2005. Can GOLD &quot;cope&quot; with WALS? Retrofitting an ontology onto the World Atlas of Language Structures. <em>Proceedings of E-MELD Workshop 2005</em>, <a href="https://web.archive.org/web/20221007002846/https://emeld.org/workshop/2005/papers/good-paper.pdf">https://web.archive.org/web/20221007002846/https://emeld.org/workshop/2005/papers/good-paper.pdf</a>
</p>


<h3>See Also</h3>

<p>The World Atlas of Language Structure (WALS) contains typical data that most people would very much like to recode before using for further analysis. See Cysouw et al. 2005 for a discussion of various issues surrounding the WALS data.
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
