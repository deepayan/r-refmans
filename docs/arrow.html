<!DOCTYPE html><html><head><title>Help for package arrow</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {arrow}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#arrow-package'><p>arrow: Integration to 'Apache' 'Arrow'</p></a></li>
<li><a href='#acero'><p>Functions available in Arrow dplyr queries</p></a></li>
<li><a href='#add_filename'><p>Add the data filename as a column</p></a></li>
<li><a href='#Array'><p>Array Classes</p></a></li>
<li><a href='#ArrayData'><p>ArrayData class</p></a></li>
<li><a href='#arrow_array'><p>Create an Arrow Array</p></a></li>
<li><a href='#arrow_info'><p>Report information on the package's capabilities</p></a></li>
<li><a href='#arrow_table'><p>Create an Arrow Table</p></a></li>
<li><a href='#as_arrow_array'><p>Convert an object to an Arrow Array</p></a></li>
<li><a href='#as_arrow_table'><p>Convert an object to an Arrow Table</p></a></li>
<li><a href='#as_chunked_array'><p>Convert an object to an Arrow ChunkedArray</p></a></li>
<li><a href='#as_data_type'><p>Convert an object to an Arrow DataType</p></a></li>
<li><a href='#as_record_batch'><p>Convert an object to an Arrow RecordBatch</p></a></li>
<li><a href='#as_record_batch_reader'><p>Convert an object to an Arrow RecordBatchReader</p></a></li>
<li><a href='#as_schema'><p>Convert an object to an Arrow Schema</p></a></li>
<li><a href='#buffer'><p>Create a Buffer</p></a></li>
<li><a href='#Buffer'><p>Buffer class</p></a></li>
<li><a href='#call_function'><p>Call an Arrow compute function</p></a></li>
<li><a href='#cast'><p>Change the type of an array or column</p></a></li>
<li><a href='#cast_options'><p>Cast options</p></a></li>
<li><a href='#chunked_array'><p>Create a Chunked Array</p></a></li>
<li><a href='#ChunkedArray'><p>ChunkedArray class</p></a></li>
<li><a href='#Codec'><p>Compression Codec class</p></a></li>
<li><a href='#codec_is_available'><p>Check whether a compression codec is available</p></a></li>
<li><a href='#compression'><p>Compressed stream classes</p></a></li>
<li><a href='#concat_arrays'><p>Concatenate zero or more Arrays</p></a></li>
<li><a href='#concat_tables'><p>Concatenate one or more Tables</p></a></li>
<li><a href='#contains_regex'><p>Does this string contain regex metacharacters?</p></a></li>
<li><a href='#copy_files'><p>Copy files between FileSystems</p></a></li>
<li><a href='#cpu_count'><p>Manage the global CPU thread pool in libarrow</p></a></li>
<li><a href='#create_package_with_all_dependencies'><p>Create a source bundle that includes all thirdparty dependencies</p></a></li>
<li><a href='#csv_convert_options'><p>CSV Convert Options</p></a></li>
<li><a href='#csv_parse_options'><p>CSV Parsing Options</p></a></li>
<li><a href='#csv_read_options'><p>CSV Reading Options</p></a></li>
<li><a href='#csv_write_options'><p>CSV Writing Options</p></a></li>
<li><a href='#CsvFileFormat'><p>CSV dataset file format</p></a></li>
<li><a href='#CsvReadOptions'><p>File reader options</p></a></li>
<li><a href='#CsvTableReader'><p>Arrow CSV and JSON table reader classes</p></a></li>
<li><a href='#data-type'><p>Create Arrow data types</p></a></li>
<li><a href='#Dataset'><p>Multi-file datasets</p></a></li>
<li><a href='#dataset_factory'><p>Create a DatasetFactory</p></a></li>
<li><a href='#DataType'><p>DataType class</p></a></li>
<li><a href='#default_memory_pool'><p>Arrow's default MemoryPool</p></a></li>
<li><a href='#dictionary'><p>Create a dictionary type</p></a></li>
<li><a href='#DictionaryType'><p>class DictionaryType</p></a></li>
<li><a href='#enums'><p>Arrow enums</p></a></li>
<li><a href='#Expression'><p>Arrow expressions</p></a></li>
<li><a href='#ExtensionArray'><p>ExtensionArray class</p></a></li>
<li><a href='#ExtensionType'><p>ExtensionType class</p></a></li>
<li><a href='#FeatherReader'><p>FeatherReader class</p></a></li>
<li><a href='#field'><p>Create a Field</p></a></li>
<li><a href='#Field'><p>Field class</p></a></li>
<li><a href='#FileFormat'><p>Dataset file formats</p></a></li>
<li><a href='#FileInfo'><p>FileSystem entry info</p></a></li>
<li><a href='#FileSelector'><p>file selector</p></a></li>
<li><a href='#FileSystem'><p>FileSystem classes</p></a></li>
<li><a href='#FileWriteOptions'><p>Format-specific write options</p></a></li>
<li><a href='#FixedWidthType'><p>FixedWidthType class</p></a></li>
<li><a href='#flight_connect'><p>Connect to a Flight server</p></a></li>
<li><a href='#flight_disconnect'><p>Explicitly close a Flight client</p></a></li>
<li><a href='#flight_get'><p>Get data from a Flight server</p></a></li>
<li><a href='#flight_put'><p>Send data to a Flight server</p></a></li>
<li><a href='#FragmentScanOptions'><p>Format-specific scan options</p></a></li>
<li><a href='#get_stringr_pattern_options'><p>Get <code>stringr</code> pattern options</p></a></li>
<li><a href='#gs_bucket'><p>Connect to a Google Cloud Storage (GCS) bucket</p></a></li>
<li><a href='#hive_partition'><p>Construct Hive partitioning</p></a></li>
<li><a href='#infer_schema'><p>Extract a schema from an object</p></a></li>
<li><a href='#infer_type'><p>Infer the arrow Array type from an R object</p></a></li>
<li><a href='#InputStream'><p>InputStream classes</p></a></li>
<li><a href='#install_arrow'><p>Install or upgrade the Arrow library</p></a></li>
<li><a href='#install_pyarrow'><p>Install pyarrow for use with reticulate</p></a></li>
<li><a href='#io_thread_count'><p>Manage the global I/O thread pool in libarrow</p></a></li>
<li><a href='#JsonFileFormat'><p>JSON dataset file format</p></a></li>
<li><a href='#list_compute_functions'><p>List available Arrow C++ compute functions</p></a></li>
<li><a href='#list_flights'><p>See available resources on a Flight server</p></a></li>
<li><a href='#load_flight_server'><p>Load a Python Flight server</p></a></li>
<li><a href='#make_readable_file'><p>Handle a range of possible input sources</p></a></li>
<li><a href='#map_batches'><p>Apply a function to a stream of RecordBatches</p></a></li>
<li><a href='#match_arrow'><p>Value matching for Arrow objects</p></a></li>
<li><a href='#MemoryPool'><p>MemoryPool class</p></a></li>
<li><a href='#Message'><p>Message class</p></a></li>
<li><a href='#MessageReader'><p>MessageReader class</p></a></li>
<li><a href='#mmap_create'><p>Create a new read/write memory mapped file of a given size</p></a></li>
<li><a href='#mmap_open'><p>Open a memory mapped file</p></a></li>
<li><a href='#new_extension_type'><p>Extension types</p></a></li>
<li><a href='#open_dataset'><p>Open a multi-file dataset</p></a></li>
<li><a href='#open_delim_dataset'><p>Open a multi-file dataset of CSV or other delimiter-separated format</p></a></li>
<li><a href='#OutputStream'><p>OutputStream classes</p></a></li>
<li><a href='#ParquetArrowReaderProperties'><p>ParquetArrowReaderProperties class</p></a></li>
<li><a href='#ParquetFileReader'><p>ParquetFileReader class</p></a></li>
<li><a href='#ParquetFileWriter'><p>ParquetFileWriter class</p></a></li>
<li><a href='#ParquetReaderProperties'><p>ParquetReaderProperties class</p></a></li>
<li><a href='#ParquetWriterProperties'><p>ParquetWriterProperties class</p></a></li>
<li><a href='#Partitioning'><p>Define Partitioning for a Dataset</p></a></li>
<li><a href='#read_delim_arrow'><p>Read a CSV or other delimited file with Arrow</p></a></li>
<li><a href='#read_feather'><p>Read a Feather file (an Arrow IPC file)</p></a></li>
<li><a href='#read_ipc_stream'><p>Read Arrow IPC stream format</p></a></li>
<li><a href='#read_json_arrow'><p>Read a JSON file</p></a></li>
<li><a href='#read_message'><p>Read a Message from a stream</p></a></li>
<li><a href='#read_parquet'><p>Read a Parquet file</p></a></li>
<li><a href='#read_schema'><p>Read a Schema from a stream</p></a></li>
<li><a href='#record_batch'><p>Create a RecordBatch</p></a></li>
<li><a href='#RecordBatch'><p>RecordBatch class</p></a></li>
<li><a href='#RecordBatchReader'><p>RecordBatchReader classes</p></a></li>
<li><a href='#RecordBatchWriter'><p>RecordBatchWriter classes</p></a></li>
<li><a href='#recycle_scalars'><p>Recycle scalar values in a list of arrays</p></a></li>
<li><a href='#reexports'><p>Objects exported from other packages</p></a></li>
<li><a href='#register_binding'><p>Register compute bindings</p></a></li>
<li><a href='#register_scalar_function'><p>Register user-defined functions</p></a></li>
<li><a href='#repeat_value_as_array'><p>Take an object of length 1 and repeat it.</p></a></li>
<li><a href='#s3_bucket'><p>Connect to an AWS S3 bucket</p></a></li>
<li><a href='#scalar'><p>Create an Arrow Scalar</p></a></li>
<li><a href='#Scalar'><p>Arrow scalars</p></a></li>
<li><a href='#Scanner'><p>Scan the contents of a dataset</p></a></li>
<li><a href='#schema'><p>Create a schema or extract one from an object.</p></a></li>
<li><a href='#Schema'><p>Schema class</p></a></li>
<li><a href='#show_exec_plan'><p>Show the details of an Arrow Execution Plan</p></a></li>
<li><a href='#Table'><p>Table class</p></a></li>
<li><a href='#to_arrow'><p>Create an Arrow object from a DuckDB connection</p></a></li>
<li><a href='#to_duckdb'><p>Create a (virtual) DuckDB table from an Arrow object</p></a></li>
<li><a href='#unify_schemas'><p>Combine and harmonize schemas</p></a></li>
<li><a href='#value_counts'><p><code>table</code> for Arrow objects</p></a></li>
<li><a href='#vctrs_extension_array'><p>Extension type for generic typed vectors</p></a></li>
<li><a href='#write_csv_arrow'><p>Write CSV file to disk</p></a></li>
<li><a href='#write_dataset'><p>Write a dataset</p></a></li>
<li><a href='#write_delim_dataset'><p>Write a dataset into partitioned flat files.</p></a></li>
<li><a href='#write_feather'><p>Write a Feather file (an Arrow IPC file)</p></a></li>
<li><a href='#write_ipc_stream'><p>Write Arrow IPC stream format</p></a></li>
<li><a href='#write_parquet'><p>Write Parquet file to disk</p></a></li>
<li><a href='#write_to_raw'><p>Write Arrow data to a raw vector</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Integration to 'Apache' 'Arrow'</td>
</tr>
<tr>
<td>Version:</td>
<td>15.0.1</td>
</tr>
<tr>
<td>Description:</td>
<td>'Apache' 'Arrow' <a href="https://arrow.apache.org/">https://arrow.apache.org/</a> is a cross-language
    development platform for in-memory data. It specifies a standardized
    language-independent columnar memory format for flat and hierarchical data,
    organized for efficient analytic operations on modern hardware. This
    package provides an interface to the 'Arrow C++' library.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.4)</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.apache.org/licenses/LICENSE-2.0">Apache License (&ge; 2.0)</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/apache/arrow/">https://github.com/apache/arrow/</a>, <a href="https://arrow.apache.org/docs/r/">https://arrow.apache.org/docs/r/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/apache/arrow/issues">https://github.com/apache/arrow/issues</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Language:</td>
<td>en-US</td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>C++17; for AWS S3 support on Linux, libcurl and
openssl (optional); cmake &gt;= 3.16 (build-time only, and only
for full source build)</td>
</tr>
<tr>
<td>Biarch:</td>
<td>true</td>
</tr>
<tr>
<td>Imports:</td>
<td>assertthat, bit64 (&ge; 0.9-7), glue, methods, purrr, R6, rlang
(&ge; 1.0.0), stats, tidyselect (&ge; 1.0.0), utils, vctrs</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>Suggests:</td>
<td>blob, curl, cli, DBI, dbplyr, decor, distro, dplyr, duckdb
(&ge; 0.2.8), hms, jsonlite, knitr, lubridate, pillar, pkgload,
reticulate, rmarkdown, stringi, stringr, sys, testthat (&ge;
3.1.0), tibble, tzdb, withr</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>cpp11 (&ge; 0.4.2)</td>
</tr>
<tr>
<td>Collate:</td>
<td>'arrowExports.R' 'enums.R' 'arrow-object.R' 'type.R'
'array-data.R' 'arrow-datum.R' 'array.R' 'arrow-info.R'
'arrow-package.R' 'arrow-tabular.R' 'buffer.R'
'chunked-array.R' 'io.R' 'compression.R' 'scalar.R' 'compute.R'
'config.R' 'csv.R' 'dataset.R' 'dataset-factory.R'
'dataset-format.R' 'dataset-partition.R' 'dataset-scan.R'
'dataset-write.R' 'dictionary.R' 'dplyr-across.R'
'dplyr-arrange.R' 'dplyr-by.R' 'dplyr-collect.R'
'dplyr-count.R' 'dplyr-datetime-helpers.R' 'dplyr-distinct.R'
'dplyr-eval.R' 'dplyr-filter.R' 'dplyr-funcs-augmented.R'
'dplyr-funcs-conditional.R' 'dplyr-funcs-datetime.R'
'dplyr-funcs-doc.R' 'dplyr-funcs-math.R' 'dplyr-funcs-simple.R'
'dplyr-funcs-string.R' 'dplyr-funcs-type.R' 'expression.R'
'dplyr-funcs.R' 'dplyr-glimpse.R' 'dplyr-group-by.R'
'dplyr-join.R' 'dplyr-mutate.R' 'dplyr-select.R'
'dplyr-slice.R' 'dplyr-summarize.R' 'dplyr-union.R'
'record-batch.R' 'table.R' 'dplyr.R' 'duckdb.R' 'extension.R'
'feather.R' 'field.R' 'filesystem.R' 'flight.R'
'install-arrow.R' 'ipc-stream.R' 'json.R' 'memory-pool.R'
'message.R' 'metadata.R' 'parquet.R' 'python.R'
'query-engine.R' 'record-batch-reader.R'
'record-batch-writer.R' 'reexports-bit64.R'
'reexports-tidyselect.R' 'schema.R' 'udf.R' 'util.R'</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-03-08 03:22:58 UTC; nic</td>
</tr>
<tr>
<td>Author:</td>
<td>Neal Richardson [aut],
  Ian Cook [aut],
  Nic Crane [aut, cre],
  Dewey Dunnington <a href="https://orcid.org/0000-0002-9415-4582"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut],
  Romain François <a href="https://orcid.org/0000-0002-2444-4226"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut],
  Jonathan Keane [aut],
  Dragoș Moldovan-Grünfeld [aut],
  Jeroen Ooms [aut],
  Jacob Wujciak-Jens [aut],
  Javier Luraschi [ctb],
  Karl Dunkle Werner
    <a href="https://orcid.org/0000-0003-0523-7309"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [ctb],
  Jeffrey Wong [ctb],
  Apache Arrow [aut, cph]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Nic Crane &lt;thisisnic@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-03-12 12:30:10 UTC</td>
</tr>
</table>
<hr>
<h2 id='arrow-package'>arrow: Integration to 'Apache' 'Arrow'</h2><span id='topic+arrow'></span><span id='topic+arrow-package'></span>

<h3>Description</h3>

<p>'Apache' 'Arrow' <a href="https://arrow.apache.org/">https://arrow.apache.org/</a> is a cross-language development platform for in-memory data. It specifies a standardized language-independent columnar memory format for flat and hierarchical data, organized for efficient analytic operations on modern hardware. This package provides an interface to the 'Arrow C++' library.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Nic Crane <a href="mailto:thisisnic@gmail.com">thisisnic@gmail.com</a>
</p>
<p>Authors:
</p>

<ul>
<li><p> Neal Richardson <a href="mailto:neal.p.richardson@gmail.com">neal.p.richardson@gmail.com</a>
</p>
</li>
<li><p> Ian Cook <a href="mailto:ianmcook@gmail.com">ianmcook@gmail.com</a>
</p>
</li>
<li><p> Dewey Dunnington <a href="mailto:dewey@fishandwhistle.net">dewey@fishandwhistle.net</a> (<a href="https://orcid.org/0000-0002-9415-4582">ORCID</a>)
</p>
</li>
<li><p> Romain François (<a href="https://orcid.org/0000-0002-2444-4226">ORCID</a>)
</p>
</li>
<li><p> Jonathan Keane <a href="mailto:jkeane@gmail.com">jkeane@gmail.com</a>
</p>
</li>
<li><p> Dragoș Moldovan-Grünfeld <a href="mailto:dragos.mold@gmail.com">dragos.mold@gmail.com</a>
</p>
</li>
<li><p> Jeroen Ooms <a href="mailto:jeroen@berkeley.edu">jeroen@berkeley.edu</a>
</p>
</li>
<li><p> Jacob Wujciak-Jens <a href="mailto:jacob@wujciak.de">jacob@wujciak.de</a>
</p>
</li>
<li><p> Apache Arrow <a href="mailto:dev@arrow.apache.org">dev@arrow.apache.org</a> [copyright holder]
</p>
</li></ul>

<p>Other contributors:
</p>

<ul>
<li><p> Javier Luraschi <a href="mailto:javier@rstudio.com">javier@rstudio.com</a> [contributor]
</p>
</li>
<li><p> Karl Dunkle Werner <a href="mailto:karldw@users.noreply.github.com">karldw@users.noreply.github.com</a> (<a href="https://orcid.org/0000-0003-0523-7309">ORCID</a>) [contributor]
</p>
</li>
<li><p> Jeffrey Wong <a href="mailto:jeffreyw@netflix.com">jeffreyw@netflix.com</a> [contributor]
</p>
</li></ul>



<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://github.com/apache/arrow/">https://github.com/apache/arrow/</a>
</p>
</li>
<li> <p><a href="https://arrow.apache.org/docs/r/">https://arrow.apache.org/docs/r/</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/apache/arrow/issues">https://github.com/apache/arrow/issues</a>
</p>
</li></ul>


<hr>
<h2 id='acero'>Functions available in Arrow dplyr queries</h2><span id='topic+acero'></span><span id='topic+arrow-functions'></span><span id='topic+arrow-verbs'></span><span id='topic+arrow-dplyr'></span>

<h3>Description</h3>

<p>The <code>arrow</code> package contains methods for 37 <code>dplyr</code> table functions, many of
which are &quot;verbs&quot; that do transformations to one or more tables.
The package also has mappings of 212 R functions to the corresponding
functions in the Arrow compute library. These allow you to write code inside
of <code>dplyr</code> methods that call R functions, including many in packages like
<code>stringr</code> and <code>lubridate</code>, and they will get translated to Arrow and run
on the Arrow query engine (Acero). This document lists all of the mapped
functions.
</p>


<h3><code>dplyr</code> verbs</h3>

<p>Most verb functions return an <code>arrow_dplyr_query</code> object, similar in spirit
to a <code>dbplyr::tbl_lazy</code>. This means that the verbs do not eagerly evaluate
the query on the data. To run the query, call either <code>compute()</code>,
which returns an <code>arrow</code> <a href="#topic+Table">Table</a>, or <code>collect()</code>, which pulls the resulting
Table into an R <code>tibble</code>.
</p>

<ul>
<li> <p><code><a href="dplyr.html#topic+filter-joins">anti_join()</a></code>: the <code>copy</code> and <code>na_matches</code> arguments are ignored
</p>
</li>
<li> <p><code><a href="dplyr.html#topic+arrange">arrange()</a></code>
</p>
</li>
<li> <p><code><a href="dplyr.html#topic+compute">collapse()</a></code>
</p>
</li>
<li> <p><code><a href="dplyr.html#topic+compute">collect()</a></code>
</p>
</li>
<li> <p><code><a href="dplyr.html#topic+compute">compute()</a></code>
</p>
</li>
<li> <p><code><a href="dplyr.html#topic+count">count()</a></code>
</p>
</li>
<li> <p><code><a href="dplyr.html#topic+distinct">distinct()</a></code>: <code>.keep_all = TRUE</code> not supported
</p>
</li>
<li> <p><code><a href="dplyr.html#topic+explain">explain()</a></code>
</p>
</li>
<li> <p><code><a href="dplyr.html#topic+filter">filter()</a></code>
</p>
</li>
<li> <p><code><a href="dplyr.html#topic+mutate-joins">full_join()</a></code>: the <code>copy</code> and <code>na_matches</code> arguments are ignored
</p>
</li>
<li> <p><code><a href="dplyr.html#topic+glimpse">glimpse()</a></code>
</p>
</li>
<li> <p><code><a href="dplyr.html#topic+group_by">group_by()</a></code>
</p>
</li>
<li> <p><code><a href="dplyr.html#topic+group_by_drop_default">group_by_drop_default()</a></code>
</p>
</li>
<li> <p><code><a href="dplyr.html#topic+group_data">group_vars()</a></code>
</p>
</li>
<li> <p><code><a href="dplyr.html#topic+group_data">groups()</a></code>
</p>
</li>
<li> <p><code><a href="dplyr.html#topic+mutate-joins">inner_join()</a></code>: the <code>copy</code> and <code>na_matches</code> arguments are ignored
</p>
</li>
<li> <p><code><a href="dplyr.html#topic+mutate-joins">left_join()</a></code>: the <code>copy</code> and <code>na_matches</code> arguments are ignored
</p>
</li>
<li> <p><code><a href="dplyr.html#topic+mutate">mutate()</a></code>: window functions (e.g. things that require aggregation within groups) not currently supported
</p>
</li>
<li> <p><code><a href="dplyr.html#topic+pull">pull()</a></code>: the <code>name</code> argument is not supported; returns an R vector by default but this behavior is deprecated and will return an Arrow <a href="#topic+ChunkedArray">ChunkedArray</a> in a future release. Provide <code>as_vector = TRUE/FALSE</code> to control this behavior, or set <code>options(arrow.pull_as_vector)</code> globally.
</p>
</li>
<li> <p><code><a href="dplyr.html#topic+relocate">relocate()</a></code>
</p>
</li>
<li> <p><code><a href="dplyr.html#topic+rename">rename()</a></code>
</p>
</li>
<li> <p><code><a href="dplyr.html#topic+rename">rename_with()</a></code>
</p>
</li>
<li> <p><code><a href="dplyr.html#topic+mutate-joins">right_join()</a></code>: the <code>copy</code> and <code>na_matches</code> arguments are ignored
</p>
</li>
<li> <p><code><a href="dplyr.html#topic+select">select()</a></code>
</p>
</li>
<li> <p><code><a href="dplyr.html#topic+filter-joins">semi_join()</a></code>: the <code>copy</code> and <code>na_matches</code> arguments are ignored
</p>
</li>
<li> <p><code><a href="dplyr.html#topic+explain">show_query()</a></code>
</p>
</li>
<li> <p><code><a href="dplyr.html#topic+slice">slice_head()</a></code>: slicing within groups not supported; Arrow datasets do not have row order, so head is non-deterministic; <code>prop</code> only supported on queries where <code>nrow()</code> is knowable without evaluating
</p>
</li>
<li> <p><code><a href="dplyr.html#topic+slice">slice_max()</a></code>: slicing within groups not supported; <code>with_ties = TRUE</code> (dplyr default) is not supported; <code>prop</code> only supported on queries where <code>nrow()</code> is knowable without evaluating
</p>
</li>
<li> <p><code><a href="dplyr.html#topic+slice">slice_min()</a></code>: slicing within groups not supported; <code>with_ties = TRUE</code> (dplyr default) is not supported; <code>prop</code> only supported on queries where <code>nrow()</code> is knowable without evaluating
</p>
</li>
<li> <p><code><a href="dplyr.html#topic+slice">slice_sample()</a></code>: slicing within groups not supported; <code>replace = TRUE</code> and the <code>weight_by</code> argument not supported; <code>n</code> only supported on queries where <code>nrow()</code> is knowable without evaluating
</p>
</li>
<li> <p><code><a href="dplyr.html#topic+slice">slice_tail()</a></code>: slicing within groups not supported; Arrow datasets do not have row order, so tail is non-deterministic; <code>prop</code> only supported on queries where <code>nrow()</code> is knowable without evaluating
</p>
</li>
<li> <p><code><a href="dplyr.html#topic+summarise">summarise()</a></code>: window functions not currently supported; arguments <code>.drop = FALSE</code> and '.groups = &quot;rowwise&quot; not supported
</p>
</li>
<li> <p><code><a href="dplyr.html#topic+count">tally()</a></code>
</p>
</li>
<li> <p><code><a href="dplyr.html#topic+transmute">transmute()</a></code>
</p>
</li>
<li> <p><code><a href="dplyr.html#topic+group_by">ungroup()</a></code>
</p>
</li>
<li> <p><code><a href="dplyr.html#topic+setops">union()</a></code>
</p>
</li>
<li> <p><code><a href="dplyr.html#topic+setops">union_all()</a></code>
</p>
</li></ul>



<h3>Function mappings</h3>

<p>In the list below, any differences in behavior or support between Acero and
the R function are listed. If no notes follow the function name, then you
can assume that the function works in Acero just as it does in R.
</p>
<p>Functions can be called either as <code>pkg::fun()</code> or just <code>fun()</code>, i.e. both
<code>str_sub()</code> and <code>stringr::str_sub()</code> work.
</p>
<p>In addition to these functions, you can call any of Arrow's 262 compute
functions directly. Arrow has many functions that don't map to an existing R
function. In other cases where there is an R function mapping, you can still
call the Arrow function directly if you don't want the adaptations that the R
mapping has that make Acero behave like R. These functions are listed in the
<a href="https://arrow.apache.org/docs/cpp/compute.html">C++ documentation</a>, and
in the function registry in R, they are named with an <code>arrow_</code> prefix, such
as <code>arrow_ascii_is_decimal</code>.
</p>


<h4>arrow</h4>


<ul>
<li> <p><code><a href="#topic+add_filename">add_filename()</a></code>
</p>
</li>
<li> <p><code><a href="#topic+cast">cast()</a></code>
</p>
</li></ul>




<h4>base</h4>


<ul>
<li> <p><code><a href="base.html#topic++21">!</a></code>
</p>
</li>
<li> <p><code><a href="base.html#topic++21+3D">!=</a></code>
</p>
</li>
<li> <p><code><a href="base.html#topic++25+25">%%</a></code>
</p>
</li>
<li> <p><code><a href="base.html#topic++25+2F+25">%/%</a></code>
</p>
</li>
<li> <p><code><a href="base.html#topic++25in+25">%in%</a></code>
</p>
</li>
<li> <p><code><a href="base.html#topic++26">&amp;</a></code>
</p>
</li>
<li> <p><code><a href="base.html#topic++2A">*</a></code>
</p>
</li>
<li> <p><code><a href="base.html#topic++2B">+</a></code>
</p>
</li>
<li> <p><code><a href="base.html#topic+-">-</a></code>
</p>
</li>
<li> <p><code><a href="base.html#topic++2F">/</a></code>
</p>
</li>
<li> <p><code><a href="base.html#topic++3C">&lt;</a></code>
</p>
</li>
<li> <p><code><a href="base.html#topic++3C+3D">&lt;=</a></code>
</p>
</li>
<li> <p><code><a href="base.html#topic++3D+3D">==</a></code>
</p>
</li>
<li> <p><code><a href="base.html#topic++3E">&gt;</a></code>
</p>
</li>
<li> <p><code><a href="base.html#topic++3E+3D">&gt;=</a></code>
</p>
</li>
<li> <p><code><a href="base.html#topic+ISOdatetime">ISOdate()</a></code>
</p>
</li>
<li> <p><code><a href="base.html#topic+ISOdatetime">ISOdatetime()</a></code>
</p>
</li>
<li> <p><code><a href="base.html#topic++5E">^</a></code>
</p>
</li>
<li> <p><code><a href="base.html#topic+MathFun">abs()</a></code>
</p>
</li>
<li> <p><code><a href="base.html#topic+Trig">acos()</a></code>
</p>
</li>
<li> <p><code><a href="base.html#topic+all">all()</a></code>
</p>
</li>
<li> <p><code><a href="base.html#topic+any">any()</a></code>
</p>
</li>
<li> <p><code><a href="base.html#topic+as.Date">as.Date()</a></code>: Multiple <code>tryFormats</code> not supported in Arrow.
Consider using the lubridate specialised parsing functions <code>ymd()</code>, <code>ymd()</code>, etc.
</p>
</li>
<li> <p><code><a href="base.html#topic+character">as.character()</a></code>
</p>
</li>
<li> <p><code><a href="base.html#topic+difftime">as.difftime()</a></code>: only supports <code>units = "secs"</code> (the default)
</p>
</li>
<li> <p><code><a href="base.html#topic+double">as.double()</a></code>
</p>
</li>
<li> <p><code><a href="base.html#topic+integer">as.integer()</a></code>
</p>
</li>
<li> <p><code><a href="base.html#topic+logical">as.logical()</a></code>
</p>
</li>
<li> <p><code><a href="base.html#topic+numeric">as.numeric()</a></code>
</p>
</li>
<li> <p><code><a href="base.html#topic+Trig">asin()</a></code>
</p>
</li>
<li> <p><code><a href="base.html#topic+Round">ceiling()</a></code>
</p>
</li>
<li> <p><code><a href="base.html#topic+Trig">cos()</a></code>
</p>
</li>
<li> <p><code><a href="base.html#topic+data.frame">data.frame()</a></code>: <code>row.names</code> and <code>check.rows</code> arguments not supported;
<code>stringsAsFactors</code> must be <code>FALSE</code>
</p>
</li>
<li> <p><code><a href="base.html#topic+difftime">difftime()</a></code>: only supports <code>units = "secs"</code> (the default);
<code>tz</code> argument not supported
</p>
</li>
<li> <p><code><a href="base.html#topic+startsWith">endsWith()</a></code>
</p>
</li>
<li> <p><code><a href="base.html#topic+Log">exp()</a></code>
</p>
</li>
<li> <p><code><a href="base.html#topic+Round">floor()</a></code>
</p>
</li>
<li> <p><code><a href="base.html#topic+format">format()</a></code>
</p>
</li>
<li> <p><code><a href="base.html#topic+grep">grepl()</a></code>
</p>
</li>
<li> <p><code><a href="base.html#topic+grep">gsub()</a></code>
</p>
</li>
<li> <p><code><a href="base.html#topic+ifelse">ifelse()</a></code>
</p>
</li>
<li> <p><code><a href="base.html#topic+character">is.character()</a></code>
</p>
</li>
<li> <p><code><a href="base.html#topic+double">is.double()</a></code>
</p>
</li>
<li> <p><code><a href="base.html#topic+factor">is.factor()</a></code>
</p>
</li>
<li> <p><code><a href="base.html#topic+is.finite">is.finite()</a></code>
</p>
</li>
<li> <p><code><a href="base.html#topic+is.finite">is.infinite()</a></code>
</p>
</li>
<li> <p><code><a href="base.html#topic+integer">is.integer()</a></code>
</p>
</li>
<li> <p><code><a href="base.html#topic+list">is.list()</a></code>
</p>
</li>
<li> <p><code><a href="base.html#topic+logical">is.logical()</a></code>
</p>
</li>
<li> <p><code><a href="base.html#topic+NA">is.na()</a></code>
</p>
</li>
<li> <p><code><a href="base.html#topic+is.finite">is.nan()</a></code>
</p>
</li>
<li> <p><code><a href="base.html#topic+numeric">is.numeric()</a></code>
</p>
</li>
<li> <p><code><a href="base.html#topic+Log">log()</a></code>
</p>
</li>
<li> <p><code><a href="base.html#topic+Log">log10()</a></code>
</p>
</li>
<li> <p><code><a href="base.html#topic+Log">log1p()</a></code>
</p>
</li>
<li> <p><code><a href="base.html#topic+Log">log2()</a></code>
</p>
</li>
<li> <p><code><a href="base.html#topic+Log">logb()</a></code>
</p>
</li>
<li> <p><code><a href="base.html#topic+Extremes">max()</a></code>
</p>
</li>
<li> <p><code><a href="base.html#topic+mean">mean()</a></code>
</p>
</li>
<li> <p><code><a href="base.html#topic+Extremes">min()</a></code>
</p>
</li>
<li> <p><code><a href="base.html#topic+nchar">nchar()</a></code>: <code>allowNA = TRUE</code> and <code>keepNA = TRUE</code> not supported
</p>
</li>
<li> <p><code><a href="base.html#topic+paste">paste()</a></code>: the <code>collapse</code> argument is not yet supported
</p>
</li>
<li> <p><code><a href="base.html#topic+paste">paste0()</a></code>: the <code>collapse</code> argument is not yet supported
</p>
</li>
<li> <p><code><a href="base.html#topic+Extremes">pmax()</a></code>
</p>
</li>
<li> <p><code><a href="base.html#topic+Extremes">pmin()</a></code>
</p>
</li>
<li> <p><code><a href="base.html#topic+prod">prod()</a></code>
</p>
</li>
<li> <p><code><a href="base.html#topic+Round">round()</a></code>
</p>
</li>
<li> <p><code><a href="base.html#topic+sign">sign()</a></code>
</p>
</li>
<li> <p><code><a href="base.html#topic+Trig">sin()</a></code>
</p>
</li>
<li> <p><code><a href="base.html#topic+MathFun">sqrt()</a></code>
</p>
</li>
<li> <p><code><a href="base.html#topic+startsWith">startsWith()</a></code>
</p>
</li>
<li> <p><code><a href="base.html#topic+strptime">strftime()</a></code>
</p>
</li>
<li> <p><code><a href="base.html#topic+strptime">strptime()</a></code>: accepts a <code>unit</code> argument not present in the <code>base</code> function.
Valid values are &quot;s&quot;, &quot;ms&quot; (default), &quot;us&quot;, &quot;ns&quot;.
</p>
</li>
<li> <p><code><a href="base.html#topic+strrep">strrep()</a></code>
</p>
</li>
<li> <p><code><a href="base.html#topic+strsplit">strsplit()</a></code>
</p>
</li>
<li> <p><code><a href="base.html#topic+grep">sub()</a></code>
</p>
</li>
<li> <p><code><a href="base.html#topic+substr">substr()</a></code>: <code>start</code> and <code>stop</code> must be length 1
</p>
</li>
<li> <p><code><a href="base.html#topic+substr">substring()</a></code>
</p>
</li>
<li> <p><code><a href="base.html#topic+sum">sum()</a></code>
</p>
</li>
<li> <p><code><a href="base.html#topic+Trig">tan()</a></code>
</p>
</li>
<li> <p><code><a href="base.html#topic+chartr">tolower()</a></code>
</p>
</li>
<li> <p><code><a href="base.html#topic+chartr">toupper()</a></code>
</p>
</li>
<li> <p><code><a href="base.html#topic+Round">trunc()</a></code>
</p>
</li>
<li> <p><code><a href="base.html#topic++7C">|</a></code>
</p>
</li></ul>




<h4>bit64</h4>


<ul>
<li> <p><code><a href="bit64.html#topic+as.integer64.character">as.integer64()</a></code>
</p>
</li>
<li> <p><code><a href="bit64.html#topic+bit64-package">is.integer64()</a></code>
</p>
</li></ul>




<h4>dplyr</h4>


<ul>
<li> <p><code><a href="dplyr.html#topic+across">across()</a></code>
</p>
</li>
<li> <p><code><a href="dplyr.html#topic+between">between()</a></code>
</p>
</li>
<li> <p><code><a href="dplyr.html#topic+case_when">case_when()</a></code>: <code>.ptype</code> and <code>.size</code> arguments not supported
</p>
</li>
<li> <p><code><a href="dplyr.html#topic+coalesce">coalesce()</a></code>
</p>
</li>
<li> <p><code><a href="dplyr.html#topic+desc">desc()</a></code>
</p>
</li>
<li> <p><code><a href="dplyr.html#topic+across">if_all()</a></code>
</p>
</li>
<li> <p><code><a href="dplyr.html#topic+across">if_any()</a></code>
</p>
</li>
<li> <p><code><a href="dplyr.html#topic+if_else">if_else()</a></code>
</p>
</li>
<li> <p><code><a href="dplyr.html#topic+context">n()</a></code>
</p>
</li>
<li> <p><code><a href="dplyr.html#topic+n_distinct">n_distinct()</a></code>
</p>
</li></ul>




<h4>lubridate</h4>


<ul>
<li> <p><code><a href="lubridate.html#topic+am">am()</a></code>
</p>
</li>
<li> <p><code><a href="lubridate.html#topic+as_date">as_date()</a></code>
</p>
</li>
<li> <p><code><a href="lubridate.html#topic+as_date">as_datetime()</a></code>
</p>
</li>
<li> <p><code><a href="lubridate.html#topic+round_date">ceiling_date()</a></code>
</p>
</li>
<li> <p><code><a href="lubridate.html#topic+date">date()</a></code>
</p>
</li>
<li> <p><code><a href="lubridate.html#topic+date_decimal">date_decimal()</a></code>
</p>
</li>
<li> <p><code><a href="lubridate.html#topic+day">day()</a></code>
</p>
</li>
<li> <p><code><a href="lubridate.html#topic+duration">ddays()</a></code>
</p>
</li>
<li> <p><code><a href="lubridate.html#topic+decimal_date">decimal_date()</a></code>
</p>
</li>
<li> <p><code><a href="lubridate.html#topic+duration">dhours()</a></code>
</p>
</li>
<li> <p><code><a href="lubridate.html#topic+duration">dmicroseconds()</a></code>
</p>
</li>
<li> <p><code><a href="lubridate.html#topic+duration">dmilliseconds()</a></code>
</p>
</li>
<li> <p><code><a href="lubridate.html#topic+duration">dminutes()</a></code>
</p>
</li>
<li> <p><code><a href="lubridate.html#topic+duration">dmonths()</a></code>
</p>
</li>
<li> <p><code><a href="lubridate.html#topic+ymd">dmy()</a></code>: <code>locale</code> argument not supported
</p>
</li>
<li> <p><code><a href="lubridate.html#topic+ymd_hms">dmy_h()</a></code>: <code>locale</code> argument not supported
</p>
</li>
<li> <p><code><a href="lubridate.html#topic+ymd_hms">dmy_hm()</a></code>: <code>locale</code> argument not supported
</p>
</li>
<li> <p><code><a href="lubridate.html#topic+ymd_hms">dmy_hms()</a></code>: <code>locale</code> argument not supported
</p>
</li>
<li> <p><code><a href="lubridate.html#topic+duration">dnanoseconds()</a></code>
</p>
</li>
<li> <p><code><a href="lubridate.html#topic+duration">dpicoseconds()</a></code>: not supported
</p>
</li>
<li> <p><code><a href="lubridate.html#topic+duration">dseconds()</a></code>
</p>
</li>
<li> <p><code><a href="lubridate.html#topic+dst">dst()</a></code>
</p>
</li>
<li> <p><code><a href="lubridate.html#topic+duration">dweeks()</a></code>
</p>
</li>
<li> <p><code><a href="lubridate.html#topic+duration">dyears()</a></code>
</p>
</li>
<li> <p><code><a href="lubridate.html#topic+ymd">dym()</a></code>: <code>locale</code> argument not supported
</p>
</li>
<li> <p><code><a href="lubridate.html#topic+week">epiweek()</a></code>
</p>
</li>
<li> <p><code><a href="lubridate.html#topic+year">epiyear()</a></code>
</p>
</li>
<li> <p><code><a href="lubridate.html#topic+parse_date_time">fast_strptime()</a></code>: non-default values of <code>lt</code> and <code>cutoff_2000</code> not supported
</p>
</li>
<li> <p><code><a href="lubridate.html#topic+round_date">floor_date()</a></code>
</p>
</li>
<li> <p><code><a href="lubridate.html#topic+force_tz">force_tz()</a></code>: Timezone conversion from non-UTC timezone not supported;
<code>roll_dst</code> values of 'error' and 'boundary' are supported for nonexistent times,
<code>roll_dst</code> values of 'error', 'pre', and 'post' are supported for ambiguous times.
</p>
</li>
<li> <p><code><a href="lubridate.html#topic+format_ISO8601">format_ISO8601()</a></code>
</p>
</li>
<li> <p><code><a href="lubridate.html#topic+hour">hour()</a></code>
</p>
</li>
<li> <p><code><a href="lubridate.html#topic+date_utils">is.Date()</a></code>
</p>
</li>
<li> <p><code><a href="lubridate.html#topic+posix_utils">is.POSIXct()</a></code>
</p>
</li>
<li> <p><code><a href="lubridate.html#topic+is.instant">is.instant()</a></code>
</p>
</li>
<li> <p><code><a href="lubridate.html#topic+is.instant">is.timepoint()</a></code>
</p>
</li>
<li> <p><code><a href="lubridate.html#topic+week">isoweek()</a></code>
</p>
</li>
<li> <p><code><a href="lubridate.html#topic+year">isoyear()</a></code>
</p>
</li>
<li> <p><code><a href="lubridate.html#topic+leap_year">leap_year()</a></code>
</p>
</li>
<li> <p><code><a href="lubridate.html#topic+make_datetime">make_date()</a></code>
</p>
</li>
<li> <p><code><a href="lubridate.html#topic+make_datetime">make_datetime()</a></code>: only supports UTC (default) timezone
</p>
</li>
<li> <p><code><a href="lubridate.html#topic+make_difftime">make_difftime()</a></code>: only supports <code>units = "secs"</code> (the default);
providing both <code>num</code> and <code>...</code> is not supported
</p>
</li>
<li> <p><code><a href="lubridate.html#topic+day">mday()</a></code>
</p>
</li>
<li> <p><code><a href="lubridate.html#topic+ymd">mdy()</a></code>: <code>locale</code> argument not supported
</p>
</li>
<li> <p><code><a href="lubridate.html#topic+ymd_hms">mdy_h()</a></code>: <code>locale</code> argument not supported
</p>
</li>
<li> <p><code><a href="lubridate.html#topic+ymd_hms">mdy_hm()</a></code>: <code>locale</code> argument not supported
</p>
</li>
<li> <p><code><a href="lubridate.html#topic+ymd_hms">mdy_hms()</a></code>: <code>locale</code> argument not supported
</p>
</li>
<li> <p><code><a href="lubridate.html#topic+minute">minute()</a></code>
</p>
</li>
<li> <p><code><a href="lubridate.html#topic+month">month()</a></code>
</p>
</li>
<li> <p><code><a href="lubridate.html#topic+ymd">my()</a></code>: <code>locale</code> argument not supported
</p>
</li>
<li> <p><code><a href="lubridate.html#topic+ymd">myd()</a></code>: <code>locale</code> argument not supported
</p>
</li>
<li> <p><code><a href="lubridate.html#topic+parse_date_time">parse_date_time()</a></code>: <code>quiet = FALSE</code> is not supported
Available formats are H, I, j, M, S, U, w, W, y, Y, R, T.
On Linux and OS X additionally a, A, b, B, Om, p, r are available.
</p>
</li>
<li> <p><code><a href="lubridate.html#topic+am">pm()</a></code>
</p>
</li>
<li> <p><code><a href="lubridate.html#topic+day">qday()</a></code>
</p>
</li>
<li> <p><code><a href="lubridate.html#topic+quarter">quarter()</a></code>
</p>
</li>
<li> <p><code><a href="lubridate.html#topic+round_date">round_date()</a></code>
</p>
</li>
<li> <p><code><a href="lubridate.html#topic+second">second()</a></code>
</p>
</li>
<li> <p><code><a href="lubridate.html#topic+quarter">semester()</a></code>
</p>
</li>
<li> <p><code><a href="lubridate.html#topic+tz">tz()</a></code>
</p>
</li>
<li> <p><code><a href="lubridate.html#topic+day">wday()</a></code>
</p>
</li>
<li> <p><code><a href="lubridate.html#topic+week">week()</a></code>
</p>
</li>
<li> <p><code><a href="lubridate.html#topic+with_tz">with_tz()</a></code>
</p>
</li>
<li> <p><code><a href="lubridate.html#topic+day">yday()</a></code>
</p>
</li>
<li> <p><code><a href="lubridate.html#topic+ymd">ydm()</a></code>: <code>locale</code> argument not supported
</p>
</li>
<li> <p><code><a href="lubridate.html#topic+ymd_hms">ydm_h()</a></code>: <code>locale</code> argument not supported
</p>
</li>
<li> <p><code><a href="lubridate.html#topic+ymd_hms">ydm_hm()</a></code>: <code>locale</code> argument not supported
</p>
</li>
<li> <p><code><a href="lubridate.html#topic+ymd_hms">ydm_hms()</a></code>: <code>locale</code> argument not supported
</p>
</li>
<li> <p><code><a href="lubridate.html#topic+year">year()</a></code>
</p>
</li>
<li> <p><code><a href="lubridate.html#topic+ymd">ym()</a></code>: <code>locale</code> argument not supported
</p>
</li>
<li> <p><code><a href="lubridate.html#topic+ymd">ymd()</a></code>: <code>locale</code> argument not supported
</p>
</li>
<li> <p><code><a href="lubridate.html#topic+ymd_hms">ymd_h()</a></code>: <code>locale</code> argument not supported
</p>
</li>
<li> <p><code><a href="lubridate.html#topic+ymd_hms">ymd_hm()</a></code>: <code>locale</code> argument not supported
</p>
</li>
<li> <p><code><a href="lubridate.html#topic+ymd_hms">ymd_hms()</a></code>: <code>locale</code> argument not supported
</p>
</li>
<li> <p><code><a href="lubridate.html#topic+ymd">yq()</a></code>: <code>locale</code> argument not supported
</p>
</li></ul>




<h4>methods</h4>


<ul>
<li> <p><code><a href="methods.html#topic+is">is()</a></code>
</p>
</li></ul>




<h4>rlang</h4>


<ul>
<li> <p><code><a href="rlang.html#topic+type-predicates">is_character()</a></code>
</p>
</li>
<li> <p><code><a href="rlang.html#topic+type-predicates">is_double()</a></code>
</p>
</li>
<li> <p><code><a href="rlang.html#topic+type-predicates">is_integer()</a></code>
</p>
</li>
<li> <p><code><a href="rlang.html#topic+type-predicates">is_list()</a></code>
</p>
</li>
<li> <p><code><a href="rlang.html#topic+type-predicates">is_logical()</a></code>
</p>
</li></ul>




<h4>stats</h4>


<ul>
<li> <p><code><a href="stats.html#topic+median">median()</a></code>: approximate median (t-digest) is computed
</p>
</li>
<li> <p><code><a href="stats.html#topic+quantile">quantile()</a></code>: <code>probs</code> must be length 1;
approximate quantile (t-digest) is computed
</p>
</li>
<li> <p><code><a href="stats.html#topic+sd">sd()</a></code>
</p>
</li>
<li> <p><code><a href="stats.html#topic+cor">var()</a></code>
</p>
</li></ul>




<h4>stringi</h4>


<ul>
<li> <p><code><a href="stringi.html#topic+stri_reverse">stri_reverse()</a></code>
</p>
</li></ul>




<h4>stringr</h4>

<p>Pattern modifiers <code>coll()</code> and <code>boundary()</code> are not supported in any functions.
</p>

<ul>
<li> <p><code><a href="stringr.html#topic+str_c">str_c()</a></code>: the <code>collapse</code> argument is not yet supported
</p>
</li>
<li> <p><code><a href="stringr.html#topic+str_count">str_count()</a></code>: <code>pattern</code> must be a length 1 character vector
</p>
</li>
<li> <p><code><a href="stringr.html#topic+str_detect">str_detect()</a></code>
</p>
</li>
<li> <p><code><a href="stringr.html#topic+str_dup">str_dup()</a></code>
</p>
</li>
<li> <p><code><a href="stringr.html#topic+str_starts">str_ends()</a></code>
</p>
</li>
<li> <p><code><a href="stringr.html#topic+str_length">str_length()</a></code>
</p>
</li>
<li> <p><code><a href="stringr.html#topic+str_like">str_like()</a></code>
</p>
</li>
<li> <p><code><a href="stringr.html#topic+str_pad">str_pad()</a></code>
</p>
</li>
<li> <p><code><a href="stringr.html#topic+str_remove">str_remove()</a></code>
</p>
</li>
<li> <p><code><a href="stringr.html#topic+str_remove">str_remove_all()</a></code>
</p>
</li>
<li> <p><code><a href="stringr.html#topic+str_replace">str_replace()</a></code>
</p>
</li>
<li> <p><code><a href="stringr.html#topic+str_replace">str_replace_all()</a></code>
</p>
</li>
<li> <p><code><a href="stringr.html#topic+str_split">str_split()</a></code>: Case-insensitive string splitting and splitting into 0 parts not supported
</p>
</li>
<li> <p><code><a href="stringr.html#topic+str_starts">str_starts()</a></code>
</p>
</li>
<li> <p><code><a href="stringr.html#topic+str_sub">str_sub()</a></code>: <code>start</code> and <code>end</code> must be length 1
</p>
</li>
<li> <p><code><a href="stringr.html#topic+case">str_to_lower()</a></code>
</p>
</li>
<li> <p><code><a href="stringr.html#topic+case">str_to_title()</a></code>
</p>
</li>
<li> <p><code><a href="stringr.html#topic+case">str_to_upper()</a></code>
</p>
</li>
<li> <p><code><a href="stringr.html#topic+str_trim">str_trim()</a></code>
</p>
</li></ul>




<h4>tibble</h4>


<ul>
<li> <p><code><a href="tibble.html#topic+tibble">tibble()</a></code>
</p>
</li></ul>




<h4>tidyselect</h4>


<ul>
<li> <p><code><a href="tidyselect.html#topic+all_of">all_of()</a></code>
</p>
</li>
<li> <p><code><a href="tidyselect.html#topic+starts_with">contains()</a></code>
</p>
</li>
<li> <p><code><a href="tidyselect.html#topic+starts_with">ends_with()</a></code>
</p>
</li>
<li> <p><code><a href="tidyselect.html#topic+everything">everything()</a></code>
</p>
</li>
<li> <p><code><a href="tidyselect.html#topic+everything">last_col()</a></code>
</p>
</li>
<li> <p><code><a href="tidyselect.html#topic+starts_with">matches()</a></code>
</p>
</li>
<li> <p><code><a href="tidyselect.html#topic+starts_with">num_range()</a></code>
</p>
</li>
<li> <p><code><a href="tidyselect.html#topic+one_of">one_of()</a></code>
</p>
</li>
<li> <p><code><a href="tidyselect.html#topic+starts_with">starts_with()</a></code>
</p>
</li></ul>



<hr>
<h2 id='add_filename'>Add the data filename as a column</h2><span id='topic+add_filename'></span>

<h3>Description</h3>

<p>This function only exists inside <code>arrow</code> <code>dplyr</code> queries, and it only is
valid when querying on a <code>FileSystemDataset</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>add_filename()
</code></pre>


<h3>Details</h3>

<p>To use filenames generated by this function in subsequent pipeline steps, you
must either call <code><a href="dplyr.html#topic+compute">compute()</a></code> or
<code><a href="dplyr.html#topic+collect">collect()</a></code> first. See Examples.
</p>


<h3>Value</h3>

<p>A <code>FieldRef</code> <code><a href="#topic+Expression">Expression</a></code> that refers to the filename
augmented column.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
open_dataset("nyc-taxi") %&gt;% mutate(
  file =
    add_filename()
)

# To use a verb like mutate() with add_filename() we need to first call
# compute()
open_dataset("nyc-taxi") %&gt;%
  mutate(file = add_filename()) %&gt;%
  compute() %&gt;%
  mutate(filename_length = nchar(file))

## End(Not run)

</code></pre>

<hr>
<h2 id='Array'>Array Classes</h2><span id='topic+Array'></span><span id='topic+DictionaryArray'></span><span id='topic+StructArray'></span><span id='topic+ListArray'></span><span id='topic+LargeListArray'></span><span id='topic+FixedSizeListArray'></span><span id='topic+MapArray'></span>

<h3>Description</h3>

<p>An <code>Array</code> is an immutable data array with some logical type
and some length. Most logical types are contained in the base
<code>Array</code> class; there are also subclasses for <code>DictionaryArray</code>, <code>ListArray</code>,
and <code>StructArray</code>.
</p>


<h3>Factory</h3>

<p>The <code>Array$create()</code> factory method instantiates an <code>Array</code> and
takes the following arguments:
</p>

<ul>
<li> <p><code>x</code>: an R vector, list, or <code>data.frame</code>
</p>
</li>
<li> <p><code>type</code>: an optional <a href="#topic+data-type">data type</a> for <code>x</code>. If omitted, the type
will be inferred from the data.
</p>
</li></ul>

<p><code>Array$create()</code> will return the appropriate subclass of <code>Array</code>, such as
<code>DictionaryArray</code> when given an R factor.
</p>
<p>To compose a <code>DictionaryArray</code> directly, call <code>DictionaryArray$create()</code>,
which takes two arguments:
</p>

<ul>
<li> <p><code>x</code>: an R vector or <code>Array</code> of integers for the dictionary indices
</p>
</li>
<li> <p><code>dict</code>: an R vector or <code>Array</code> of dictionary values (like R factor levels
but not limited to strings only)
</p>
</li></ul>



<h3>Usage</h3>

<div class="sourceCode"><pre>a &lt;- Array$create(x)
length(a)

print(a)
a == a
</pre></div>


<h3>Methods</h3>


<ul>
<li> <p><code style="white-space: pre;">&#8288;$IsNull(i)&#8288;</code>: Return true if value at index is null. Does not boundscheck
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$IsValid(i)&#8288;</code>: Return true if value at index is valid. Does not boundscheck
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$length()&#8288;</code>: Size in the number of elements this array contains
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$nbytes()&#8288;</code>: Total number of bytes consumed by the elements of the array
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$offset&#8288;</code>: A relative position into another array's data, to enable zero-copy slicing
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$null_count&#8288;</code>: The number of null entries in the array
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$type&#8288;</code>: logical type of data
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$type_id()&#8288;</code>: type id
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$Equals(other)&#8288;</code> : is this array equal to <code>other</code>
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$ApproxEquals(other)&#8288;</code> :
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$Diff(other)&#8288;</code> : return a string expressing the difference between two arrays
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$data()&#8288;</code>: return the underlying <a href="#topic+ArrayData">ArrayData</a>
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$as_vector()&#8288;</code>: convert to an R vector
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$ToString()&#8288;</code>: string representation of the array
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$Slice(offset, length = NULL)&#8288;</code>: Construct a zero-copy slice of the array
with the indicated offset and length. If length is <code>NULL</code>, the slice goes
until the end of the array.
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$Take(i)&#8288;</code>: return an <code>Array</code> with values at positions given by integers
(R vector or Array Array) <code>i</code>.
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$Filter(i, keep_na = TRUE)&#8288;</code>: return an <code>Array</code> with values at positions where logical
vector (or Arrow boolean Array) <code>i</code> is <code>TRUE</code>.
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$SortIndices(descending = FALSE)&#8288;</code>: return an <code>Array</code> of integer positions that can be
used to rearrange the <code>Array</code> in ascending or descending order
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$RangeEquals(other, start_idx, end_idx, other_start_idx)&#8288;</code> :
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$cast(target_type, safe = TRUE, options = cast_options(safe))&#8288;</code>: Alter the
data in the array to change its type.
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$View(type)&#8288;</code>: Construct a zero-copy view of this array with the given type.
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$Validate()&#8288;</code> : Perform any validation checks to determine obvious inconsistencies
within the array's internal data. This can be an expensive check, potentially <code>O(length)</code>
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>my_array &lt;- Array$create(1:10)
my_array$type
my_array$cast(int8())

# Check if value is null; zero-indexed
na_array &lt;- Array$create(c(1:5, NA))
na_array$IsNull(0)
na_array$IsNull(5)
na_array$IsValid(5)
na_array$null_count

# zero-copy slicing; the offset of the new Array will be the same as the index passed to $Slice
new_array &lt;- na_array$Slice(5)
new_array$offset

# Compare 2 arrays
na_array2 &lt;- na_array
na_array2 == na_array # element-wise comparison
na_array2$Equals(na_array) # overall comparison
</code></pre>

<hr>
<h2 id='ArrayData'>ArrayData class</h2><span id='topic+ArrayData'></span>

<h3>Description</h3>

<p>The <code>ArrayData</code> class allows you to get and inspect the data
inside an <code>arrow::Array</code>.
</p>


<h3>Usage</h3>

<div class="sourceCode"><pre>data &lt;- Array$create(x)$data()

data$type
data$length
data$null_count
data$offset
data$buffers
</pre></div>


<h3>Methods</h3>

<p>...
</p>

<hr>
<h2 id='arrow_array'>Create an Arrow Array</h2><span id='topic+arrow_array'></span>

<h3>Description</h3>

<p>Create an Arrow Array
</p>


<h3>Usage</h3>

<pre><code class='language-R'>arrow_array(x, type = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="arrow_array_+3A_x">x</code></td>
<td>
<p>An R object representable as an Arrow array, e.g. a vector, list, or <code>data.frame</code>.</p>
</td></tr>
<tr><td><code id="arrow_array_+3A_type">type</code></td>
<td>
<p>An optional <a href="#topic+data-type">data type</a> for <code>x</code>. If omitted, the type will be inferred from the data.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>my_array &lt;- arrow_array(1:10)

# Compare 2 arrays
na_array &lt;- arrow_array(c(1:5, NA))
na_array2 &lt;- na_array
na_array2 == na_array # element-wise comparison
</code></pre>

<hr>
<h2 id='arrow_info'>Report information on the package's capabilities</h2><span id='topic+arrow_info'></span><span id='topic+arrow_available'></span><span id='topic+arrow_with_acero'></span><span id='topic+arrow_with_dataset'></span><span id='topic+arrow_with_substrait'></span><span id='topic+arrow_with_parquet'></span><span id='topic+arrow_with_s3'></span><span id='topic+arrow_with_gcs'></span><span id='topic+arrow_with_json'></span>

<h3>Description</h3>

<p>This function summarizes a number of build-time configurations and run-time
settings for the Arrow package. It may be useful for diagnostics.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>arrow_info()

arrow_available()

arrow_with_acero()

arrow_with_dataset()

arrow_with_substrait()

arrow_with_parquet()

arrow_with_s3()

arrow_with_gcs()

arrow_with_json()
</code></pre>


<h3>Value</h3>

<p><code>arrow_info()</code> returns a list including version information, boolean
&quot;capabilities&quot;, and statistics from Arrow's memory allocator, and also
Arrow's run-time information. The <code style="white-space: pre;">&#8288;_available()&#8288;</code> functions return a logical
value whether or not the C++ library was built with support for them.
</p>


<h3>See Also</h3>

<p>If any capabilities are <code>FALSE</code>, see the
<a href="https://arrow.apache.org/docs/r/articles/install.html">install guide</a>
for guidance on reinstalling the package.
</p>

<hr>
<h2 id='arrow_table'>Create an Arrow Table</h2><span id='topic+arrow_table'></span>

<h3>Description</h3>

<p>Create an Arrow Table
</p>


<h3>Usage</h3>

<pre><code class='language-R'>arrow_table(..., schema = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="arrow_table_+3A_...">...</code></td>
<td>
<p>A <code>data.frame</code> or a named set of Arrays or vectors. If given a
mixture of data.frames and named vectors, the inputs will be autospliced together
(see examples). Alternatively, you can provide a single Arrow IPC
<code>InputStream</code>, <code>Message</code>, <code>Buffer</code>, or R <code>raw</code> object containing a <code>Buffer</code>.</p>
</td></tr>
<tr><td><code id="arrow_table_+3A_schema">schema</code></td>
<td>
<p>a <a href="#topic+Schema">Schema</a>, or <code>NULL</code> (the default) to infer the schema from
the data in <code>...</code>. When providing an Arrow IPC buffer, <code>schema</code> is required.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><a href="#topic+Table">Table</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tbl &lt;- arrow_table(name = rownames(mtcars), mtcars)
dim(tbl)
dim(head(tbl))
names(tbl)
tbl$mpg
tbl[["cyl"]]
as.data.frame(tbl[4:8, c("gear", "hp", "wt")])
</code></pre>

<hr>
<h2 id='as_arrow_array'>Convert an object to an Arrow Array</h2><span id='topic+as_arrow_array'></span><span id='topic+as_arrow_array.Array'></span><span id='topic+as_arrow_array.Scalar'></span><span id='topic+as_arrow_array.ChunkedArray'></span>

<h3>Description</h3>

<p>The <code>as_arrow_array()</code> function is identical to <code>Array$create()</code> except
that it is an S3 generic, which allows methods to be defined in other
packages to convert objects to <a href="#topic+Array">Array</a>. <code>Array$create()</code> is slightly faster
because it tries to convert in C++ before falling back on
<code>as_arrow_array()</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as_arrow_array(x, ..., type = NULL)

## S3 method for class 'Array'
as_arrow_array(x, ..., type = NULL)

## S3 method for class 'Scalar'
as_arrow_array(x, ..., type = NULL)

## S3 method for class 'ChunkedArray'
as_arrow_array(x, ..., type = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="as_arrow_array_+3A_x">x</code></td>
<td>
<p>An object to convert to an Arrow Array</p>
</td></tr>
<tr><td><code id="as_arrow_array_+3A_...">...</code></td>
<td>
<p>Passed to S3 methods</p>
</td></tr>
<tr><td><code id="as_arrow_array_+3A_type">type</code></td>
<td>
<p>A <a href="#topic+data-type">type</a> for the final Array. A value of <code>NULL</code>
will default to the type guessed by <code><a href="#topic+infer_type">infer_type()</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An <a href="#topic+Array">Array</a> with type <code>type</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>as_arrow_array(1:5)

</code></pre>

<hr>
<h2 id='as_arrow_table'>Convert an object to an Arrow Table</h2><span id='topic+as_arrow_table'></span><span id='topic+as_arrow_table.default'></span><span id='topic+as_arrow_table.Table'></span><span id='topic+as_arrow_table.RecordBatch'></span><span id='topic+as_arrow_table.data.frame'></span><span id='topic+as_arrow_table.RecordBatchReader'></span><span id='topic+as_arrow_table.Dataset'></span><span id='topic+as_arrow_table.arrow_dplyr_query'></span><span id='topic+as_arrow_table.Schema'></span>

<h3>Description</h3>

<p>Whereas <code><a href="#topic+arrow_table">arrow_table()</a></code> constructs a table from one or more columns,
<code>as_arrow_table()</code> converts a single object to an Arrow <a href="#topic+Table">Table</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as_arrow_table(x, ..., schema = NULL)

## Default S3 method:
as_arrow_table(x, ...)

## S3 method for class 'Table'
as_arrow_table(x, ..., schema = NULL)

## S3 method for class 'RecordBatch'
as_arrow_table(x, ..., schema = NULL)

## S3 method for class 'data.frame'
as_arrow_table(x, ..., schema = NULL)

## S3 method for class 'RecordBatchReader'
as_arrow_table(x, ...)

## S3 method for class 'Dataset'
as_arrow_table(x, ...)

## S3 method for class 'arrow_dplyr_query'
as_arrow_table(x, ...)

## S3 method for class 'Schema'
as_arrow_table(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="as_arrow_table_+3A_x">x</code></td>
<td>
<p>An object to convert to an Arrow Table</p>
</td></tr>
<tr><td><code id="as_arrow_table_+3A_...">...</code></td>
<td>
<p>Passed to S3 methods</p>
</td></tr>
<tr><td><code id="as_arrow_table_+3A_schema">schema</code></td>
<td>
<p>a <a href="#topic+Schema">Schema</a>, or <code>NULL</code> (the default) to infer the schema from
the data in <code>...</code>. When providing an Arrow IPC buffer, <code>schema</code> is required.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <a href="#topic+Table">Table</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># use as_arrow_table() for a single object
as_arrow_table(data.frame(col1 = 1, col2 = "two"))

# use arrow_table() to create from columns
arrow_table(col1 = 1, col2 = "two")

</code></pre>

<hr>
<h2 id='as_chunked_array'>Convert an object to an Arrow ChunkedArray</h2><span id='topic+as_chunked_array'></span><span id='topic+as_chunked_array.ChunkedArray'></span><span id='topic+as_chunked_array.Array'></span>

<h3>Description</h3>

<p>Whereas <code><a href="#topic+chunked_array">chunked_array()</a></code> constructs a <a href="#topic+ChunkedArray">ChunkedArray</a> from zero or more
<a href="#topic+Array">Array</a>s or R vectors, <code>as_chunked_array()</code> converts a single object to a
<a href="#topic+ChunkedArray">ChunkedArray</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as_chunked_array(x, ..., type = NULL)

## S3 method for class 'ChunkedArray'
as_chunked_array(x, ..., type = NULL)

## S3 method for class 'Array'
as_chunked_array(x, ..., type = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="as_chunked_array_+3A_x">x</code></td>
<td>
<p>An object to convert to an Arrow Chunked Array</p>
</td></tr>
<tr><td><code id="as_chunked_array_+3A_...">...</code></td>
<td>
<p>Passed to S3 methods</p>
</td></tr>
<tr><td><code id="as_chunked_array_+3A_type">type</code></td>
<td>
<p>A <a href="#topic+data-type">type</a> for the final Array. A value of <code>NULL</code>
will default to the type guessed by <code><a href="#topic+infer_type">infer_type()</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <a href="#topic+ChunkedArray">ChunkedArray</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>as_chunked_array(1:5)

</code></pre>

<hr>
<h2 id='as_data_type'>Convert an object to an Arrow DataType</h2><span id='topic+as_data_type'></span><span id='topic+as_data_type.DataType'></span><span id='topic+as_data_type.Field'></span><span id='topic+as_data_type.Schema'></span>

<h3>Description</h3>

<p>Convert an object to an Arrow DataType
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as_data_type(x, ...)

## S3 method for class 'DataType'
as_data_type(x, ...)

## S3 method for class 'Field'
as_data_type(x, ...)

## S3 method for class 'Schema'
as_data_type(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="as_data_type_+3A_x">x</code></td>
<td>
<p>An object to convert to an Arrow <a href="#topic+data-type">DataType</a></p>
</td></tr>
<tr><td><code id="as_data_type_+3A_...">...</code></td>
<td>
<p>Passed to S3 methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <a href="#topic+data-type">DataType</a> object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>as_data_type(int32())

</code></pre>

<hr>
<h2 id='as_record_batch'>Convert an object to an Arrow RecordBatch</h2><span id='topic+as_record_batch'></span><span id='topic+as_record_batch.RecordBatch'></span><span id='topic+as_record_batch.Table'></span><span id='topic+as_record_batch.arrow_dplyr_query'></span><span id='topic+as_record_batch.data.frame'></span>

<h3>Description</h3>

<p>Whereas <code><a href="#topic+record_batch">record_batch()</a></code> constructs a <a href="#topic+RecordBatch">RecordBatch</a> from one or more columns,
<code>as_record_batch()</code> converts a single object to an Arrow <a href="#topic+RecordBatch">RecordBatch</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as_record_batch(x, ..., schema = NULL)

## S3 method for class 'RecordBatch'
as_record_batch(x, ..., schema = NULL)

## S3 method for class 'Table'
as_record_batch(x, ..., schema = NULL)

## S3 method for class 'arrow_dplyr_query'
as_record_batch(x, ...)

## S3 method for class 'data.frame'
as_record_batch(x, ..., schema = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="as_record_batch_+3A_x">x</code></td>
<td>
<p>An object to convert to an Arrow RecordBatch</p>
</td></tr>
<tr><td><code id="as_record_batch_+3A_...">...</code></td>
<td>
<p>Passed to S3 methods</p>
</td></tr>
<tr><td><code id="as_record_batch_+3A_schema">schema</code></td>
<td>
<p>a <a href="#topic+Schema">Schema</a>, or <code>NULL</code> (the default) to infer the schema from
the data in <code>...</code>. When providing an Arrow IPC buffer, <code>schema</code> is required.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <a href="#topic+RecordBatch">RecordBatch</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># use as_record_batch() for a single object
as_record_batch(data.frame(col1 = 1, col2 = "two"))

# use record_batch() to create from columns
record_batch(col1 = 1, col2 = "two")

</code></pre>

<hr>
<h2 id='as_record_batch_reader'>Convert an object to an Arrow RecordBatchReader</h2><span id='topic+as_record_batch_reader'></span><span id='topic+as_record_batch_reader.RecordBatchReader'></span><span id='topic+as_record_batch_reader.Table'></span><span id='topic+as_record_batch_reader.RecordBatch'></span><span id='topic+as_record_batch_reader.data.frame'></span><span id='topic+as_record_batch_reader.Dataset'></span><span id='topic+as_record_batch_reader.function'></span><span id='topic+as_record_batch_reader.arrow_dplyr_query'></span><span id='topic+as_record_batch_reader.Scanner'></span>

<h3>Description</h3>

<p>Convert an object to an Arrow RecordBatchReader
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as_record_batch_reader(x, ...)

## S3 method for class 'RecordBatchReader'
as_record_batch_reader(x, ...)

## S3 method for class 'Table'
as_record_batch_reader(x, ...)

## S3 method for class 'RecordBatch'
as_record_batch_reader(x, ...)

## S3 method for class 'data.frame'
as_record_batch_reader(x, ...)

## S3 method for class 'Dataset'
as_record_batch_reader(x, ...)

## S3 method for class ''function''
as_record_batch_reader(x, ..., schema)

## S3 method for class 'arrow_dplyr_query'
as_record_batch_reader(x, ...)

## S3 method for class 'Scanner'
as_record_batch_reader(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="as_record_batch_reader_+3A_x">x</code></td>
<td>
<p>An object to convert to a <a href="#topic+RecordBatchReader">RecordBatchReader</a></p>
</td></tr>
<tr><td><code id="as_record_batch_reader_+3A_...">...</code></td>
<td>
<p>Passed to S3 methods</p>
</td></tr>
<tr><td><code id="as_record_batch_reader_+3A_schema">schema</code></td>
<td>
<p>The <code><a href="#topic+schema">schema()</a></code> that must match the schema returned by each
call to <code>x</code> when <code>x</code> is a function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <a href="#topic+RecordBatchReader">RecordBatchReader</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
reader &lt;- as_record_batch_reader(data.frame(col1 = 1, col2 = "two"))
reader$read_next_batch()

</code></pre>

<hr>
<h2 id='as_schema'>Convert an object to an Arrow Schema</h2><span id='topic+as_schema'></span><span id='topic+as_schema.Schema'></span><span id='topic+as_schema.StructType'></span>

<h3>Description</h3>

<p>Convert an object to an Arrow Schema
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as_schema(x, ...)

## S3 method for class 'Schema'
as_schema(x, ...)

## S3 method for class 'StructType'
as_schema(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="as_schema_+3A_x">x</code></td>
<td>
<p>An object to convert to a <code><a href="#topic+schema">schema()</a></code></p>
</td></tr>
<tr><td><code id="as_schema_+3A_...">...</code></td>
<td>
<p>Passed to S3 methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <a href="#topic+Schema">Schema</a> object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>as_schema(schema(col1 = int32()))

</code></pre>

<hr>
<h2 id='buffer'>Create a Buffer</h2><span id='topic+buffer'></span>

<h3>Description</h3>

<p>Create a Buffer
</p>


<h3>Usage</h3>

<pre><code class='language-R'>buffer(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="buffer_+3A_x">x</code></td>
<td>
<p>R object. Only raw, numeric and integer vectors are currently supported</p>
</td></tr>
</table>


<h3>Value</h3>

<p>an instance of <code>Buffer</code> that borrows memory from <code>x</code>
</p>


<h3>See Also</h3>

<p><a href="#topic+Buffer">Buffer</a>
</p>

<hr>
<h2 id='Buffer'>Buffer class</h2><span id='topic+Buffer'></span>

<h3>Description</h3>

<p>A Buffer is an object containing a pointer to a piece of
contiguous memory with a particular size.
</p>


<h3>Factory</h3>

<p><code>buffer()</code> lets you create an <code>arrow::Buffer</code> from an R object
</p>


<h3>Methods</h3>


<ul>
<li> <p><code style="white-space: pre;">&#8288;$is_mutable&#8288;</code> : is this buffer mutable?
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$ZeroPadding()&#8288;</code> : zero bytes in padding, i.e. bytes between size and capacity
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$size&#8288;</code> : size in memory, in bytes
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$capacity&#8288;</code>: possible capacity, in bytes
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>my_buffer &lt;- buffer(c(1, 2, 3, 4))
my_buffer$is_mutable
my_buffer$ZeroPadding()
my_buffer$size
my_buffer$capacity
</code></pre>

<hr>
<h2 id='call_function'>Call an Arrow compute function</h2><span id='topic+call_function'></span>

<h3>Description</h3>

<p>This function provides a lower-level API for calling Arrow functions by their
string function name. You won't use it directly for most applications.
Many Arrow compute functions are mapped to R methods,
and in a <code>dplyr</code> evaluation context, <a href="#topic+list_compute_functions">all Arrow functions</a>
are callable with an <code>arrow_</code> prefix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>call_function(
  function_name,
  ...,
  args = list(...),
  options = empty_named_list()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="call_function_+3A_function_name">function_name</code></td>
<td>
<p>string Arrow compute function name</p>
</td></tr>
<tr><td><code id="call_function_+3A_...">...</code></td>
<td>
<p>Function arguments, which may include <code>Array</code>, <code>ChunkedArray</code>, <code>Scalar</code>,
<code>RecordBatch</code>, or <code>Table</code>.</p>
</td></tr>
<tr><td><code id="call_function_+3A_args">args</code></td>
<td>
<p>list arguments as an alternative to specifying in <code>...</code></p>
</td></tr>
<tr><td><code id="call_function_+3A_options">options</code></td>
<td>
<p>named list of C++ function options.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When passing indices in <code>...</code>, <code>args</code>, or <code>options</code>, express them as
0-based integers (consistent with C++).
</p>


<h3>Value</h3>

<p>An <code>Array</code>, <code>ChunkedArray</code>, <code>Scalar</code>, <code>RecordBatch</code>, or <code>Table</code>, whatever the compute function results in.
</p>


<h3>See Also</h3>

<p><a href="https://arrow.apache.org/docs/cpp/compute.html">Arrow C++ documentation</a> for
the functions and their respective options.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>a &lt;- Array$create(c(1L, 2L, 3L, NA, 5L))
s &lt;- Scalar$create(4L)
call_function("coalesce", a, s)

a &lt;- Array$create(rnorm(10000))
call_function("quantile", a, options = list(q = seq(0, 1, 0.25)))
</code></pre>

<hr>
<h2 id='cast'>Change the type of an array or column</h2><span id='topic+cast'></span>

<h3>Description</h3>

<p>This is a wrapper around the <code style="white-space: pre;">&#8288;$cast()&#8288;</code> method that many Arrow objects have.
It is more convenient to call inside <code>dplyr</code> pipelines than the method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cast(x, to, safe = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cast_+3A_x">x</code></td>
<td>
<p>an <code>Array</code>, <code>Table</code>, <code>Expression</code>, or similar Arrow data object.</p>
</td></tr>
<tr><td><code id="cast_+3A_to">to</code></td>
<td>
<p><a href="#topic+DataType">DataType</a> to cast to; for <a href="#topic+Table">Table</a> and <a href="#topic+RecordBatch">RecordBatch</a>,
it should be a <a href="#topic+Schema">Schema</a>.</p>
</td></tr>
<tr><td><code id="cast_+3A_safe">safe</code></td>
<td>
<p>logical: only allow the type conversion if no data is lost
(truncation, overflow, etc.). Default is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="cast_+3A_...">...</code></td>
<td>
<p>specific <code>CastOptions</code> to set</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An <a href="#topic+Expression">Expression</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+data-type">data-type</a></code> for a list of <a href="#topic+DataType">DataType</a> to be used with <code>to</code>.
</p>
<p><a href="https://arrow.apache.org/docs/cpp/api/compute.html?highlight=castoptions#arrow%3A%3Acompute%3A%3ACastOptions">Arrow C++ CastOptions documentation</a> # nolint
for the list of supported CastOptions.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
mtcars %&gt;%
  arrow_table() %&gt;%
  mutate(cyl = cast(cyl, string()))

## End(Not run)
</code></pre>

<hr>
<h2 id='cast_options'>Cast options</h2><span id='topic+cast_options'></span>

<h3>Description</h3>

<p>Cast options
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cast_options(safe = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cast_options_+3A_safe">safe</code></td>
<td>
<p>logical: enforce safe conversion? Default <code>TRUE</code></p>
</td></tr>
<tr><td><code id="cast_options_+3A_...">...</code></td>
<td>
<p>additional cast options, such as <code>allow_int_overflow</code>,
<code>allow_time_truncate</code>, and <code>allow_float_truncate</code>, which are set to <code>!safe</code>
by default</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list
</p>

<hr>
<h2 id='chunked_array'>Create a Chunked Array</h2><span id='topic+chunked_array'></span>

<h3>Description</h3>

<p>Create a Chunked Array
</p>


<h3>Usage</h3>

<pre><code class='language-R'>chunked_array(..., type = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="chunked_array_+3A_...">...</code></td>
<td>
<p>R objects to coerce into a ChunkedArray. They must be of the same type.</p>
</td></tr>
<tr><td><code id="chunked_array_+3A_type">type</code></td>
<td>
<p>An optional <a href="#topic+data-type">data type</a>. If omitted, the type will be inferred from the data.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><a href="#topic+ChunkedArray">ChunkedArray</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Pass items into chunked_array as separate objects to create chunks
class_scores &lt;- chunked_array(c(87, 88, 89), c(94, 93, 92), c(71, 72, 73))

# If you pass a list into chunked_array, you get a list of length 1
list_scores &lt;- chunked_array(list(c(9.9, 9.6, 9.5), c(8.2, 8.3, 8.4), c(10.0, 9.9, 9.8)))

# When constructing a ChunkedArray, the first chunk is used to infer type.
infer_type(chunked_array(c(1, 2, 3), c(5L, 6L, 7L)))

# Concatenating chunked arrays returns a new chunked array containing all chunks
a &lt;- chunked_array(c(1, 2), 3)
b &lt;- chunked_array(c(4, 5), 6)
c(a, b)
</code></pre>

<hr>
<h2 id='ChunkedArray'>ChunkedArray class</h2><span id='topic+ChunkedArray'></span>

<h3>Description</h3>

<p>A <code>ChunkedArray</code> is a data structure managing a list of
primitive Arrow <a href="#topic+Array">Arrays</a> logically as one large array. Chunked arrays
may be grouped together in a <a href="#topic+Table">Table</a>.
</p>


<h3>Factory</h3>

<p>The <code>ChunkedArray$create()</code> factory method instantiates the object from
various Arrays or R vectors. <code>chunked_array()</code> is an alias for it.
</p>


<h3>Methods</h3>


<ul>
<li> <p><code style="white-space: pre;">&#8288;$length()&#8288;</code>: Size in the number of elements this array contains
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$chunk(i)&#8288;</code>: Extract an <code>Array</code> chunk by integer position
</p>
</li>
<li><p> '$nbytes() : Total number of bytes consumed by the elements of the array
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$as_vector()&#8288;</code>: convert to an R vector
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$Slice(offset, length = NULL)&#8288;</code>: Construct a zero-copy slice of the array
with the indicated offset and length. If length is <code>NULL</code>, the slice goes
until the end of the array.
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$Take(i)&#8288;</code>: return a <code>ChunkedArray</code> with values at positions given by
integers <code>i</code>. If <code>i</code> is an Arrow <code>Array</code> or <code>ChunkedArray</code>, it will be
coerced to an R vector before taking.
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$Filter(i, keep_na = TRUE)&#8288;</code>: return a <code>ChunkedArray</code> with values at positions where
logical vector or Arrow boolean-type <code style="white-space: pre;">&#8288;(Chunked)Array&#8288;</code> <code>i</code> is <code>TRUE</code>.
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$SortIndices(descending = FALSE)&#8288;</code>: return an <code>Array</code> of integer positions that can be
used to rearrange the <code>ChunkedArray</code> in ascending or descending order
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$cast(target_type, safe = TRUE, options = cast_options(safe))&#8288;</code>: Alter the
data in the array to change its type.
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$null_count&#8288;</code>: The number of null entries in the array
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$chunks&#8288;</code>: return a list of <code>Array</code>s
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$num_chunks&#8288;</code>: integer number of chunks in the <code>ChunkedArray</code>
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$type&#8288;</code>: logical type of data
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$View(type)&#8288;</code>: Construct a zero-copy view of this <code>ChunkedArray</code> with the
given type.
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$Validate()&#8288;</code>: Perform any validation checks to determine obvious inconsistencies
within the array's internal data. This can be an expensive check, potentially <code>O(length)</code>
</p>
</li></ul>



<h3>See Also</h3>

<p><a href="#topic+Array">Array</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Pass items into chunked_array as separate objects to create chunks
class_scores &lt;- chunked_array(c(87, 88, 89), c(94, 93, 92), c(71, 72, 73))
class_scores$num_chunks

# When taking a Slice from a chunked_array, chunks are preserved
class_scores$Slice(2, length = 5)

# You can combine Take and SortIndices to return a ChunkedArray with 1 chunk
# containing all values, ordered.
class_scores$Take(class_scores$SortIndices(descending = TRUE))

# If you pass a list into chunked_array, you get a list of length 1
list_scores &lt;- chunked_array(list(c(9.9, 9.6, 9.5), c(8.2, 8.3, 8.4), c(10.0, 9.9, 9.8)))
list_scores$num_chunks

# When constructing a ChunkedArray, the first chunk is used to infer type.
doubles &lt;- chunked_array(c(1, 2, 3), c(5L, 6L, 7L))
doubles$type

# Concatenating chunked arrays returns a new chunked array containing all chunks
a &lt;- chunked_array(c(1, 2), 3)
b &lt;- chunked_array(c(4, 5), 6)
c(a, b)
</code></pre>

<hr>
<h2 id='Codec'>Compression Codec class</h2><span id='topic+Codec'></span>

<h3>Description</h3>

<p>Codecs allow you to create <a href="#topic+compression">compressed input and output streams</a>.
</p>


<h3>Factory</h3>

<p>The <code>Codec$create()</code> factory method takes the following arguments:
</p>

<ul>
<li> <p><code>type</code>: string name of the compression method. Possible values are
&quot;uncompressed&quot;, &quot;snappy&quot;, &quot;gzip&quot;, &quot;brotli&quot;, &quot;zstd&quot;, &quot;lz4&quot;, &quot;lzo&quot;, or
&quot;bz2&quot;. <code>type</code> may be upper- or lower-cased. Not all methods may be
available; support depends on build-time flags for the C++ library.
See <code><a href="#topic+codec_is_available">codec_is_available()</a></code>. Most builds support at least &quot;snappy&quot; and
&quot;gzip&quot;. All support &quot;uncompressed&quot;.
</p>
</li>
<li> <p><code>compression_level</code>: compression level, the default value (<code>NA</code>) uses the
default compression level for the selected compression <code>type</code>.
</p>
</li></ul>


<hr>
<h2 id='codec_is_available'>Check whether a compression codec is available</h2><span id='topic+codec_is_available'></span>

<h3>Description</h3>

<p>Support for compression libraries depends on the build-time settings of
the Arrow C++ library. This function lets you know which are available for
use.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>codec_is_available(type)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="codec_is_available_+3A_type">type</code></td>
<td>
<p>A string, one of &quot;uncompressed&quot;, &quot;snappy&quot;, &quot;gzip&quot;, &quot;brotli&quot;,
&quot;zstd&quot;, &quot;lz4&quot;, &quot;lzo&quot;, or &quot;bz2&quot;, case-insensitive.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Logical: is <code>type</code> available?
</p>


<h3>Examples</h3>

<pre><code class='language-R'>codec_is_available("gzip")
</code></pre>

<hr>
<h2 id='compression'>Compressed stream classes</h2><span id='topic+compression'></span><span id='topic+CompressedOutputStream'></span><span id='topic+CompressedInputStream'></span>

<h3>Description</h3>

<p><code>CompressedInputStream</code> and <code>CompressedOutputStream</code>
allow you to apply a compression <a href="#topic+Codec">Codec</a> to an
input or output stream.
</p>


<h3>Factory</h3>

<p>The <code>CompressedInputStream$create()</code> and <code>CompressedOutputStream$create()</code>
factory methods instantiate the object and take the following arguments:
</p>

<ul>
<li> <p><code>stream</code> An <a href="#topic+InputStream">InputStream</a> or <a href="#topic+OutputStream">OutputStream</a>, respectively
</p>
</li>
<li> <p><code>codec</code> A <code>Codec</code>, either a <a href="#topic+Codec">Codec</a> instance or a string
</p>
</li>
<li> <p><code>compression_level</code> compression level for when the <code>codec</code> argument is given as a string
</p>
</li></ul>



<h3>Methods</h3>

<p>Methods are inherited from <a href="#topic+InputStream">InputStream</a> and <a href="#topic+OutputStream">OutputStream</a>, respectively
</p>

<hr>
<h2 id='concat_arrays'>Concatenate zero or more Arrays</h2><span id='topic+concat_arrays'></span><span id='topic+c.Array'></span>

<h3>Description</h3>

<p>Concatenates zero or more <a href="#topic+Array">Array</a> objects into a single
array. This operation will make a copy of its input; if you need
the behavior of a single Array but don't need a
single object, use <a href="#topic+ChunkedArray">ChunkedArray</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>concat_arrays(..., type = NULL)

## S3 method for class 'Array'
c(...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="concat_arrays_+3A_...">...</code></td>
<td>
<p>zero or more <a href="#topic+Array">Array</a> objects to concatenate</p>
</td></tr>
<tr><td><code id="concat_arrays_+3A_type">type</code></td>
<td>
<p>An optional <code>type</code> describing the desired
type for the final Array.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A single <a href="#topic+Array">Array</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>concat_arrays(Array$create(1:3), Array$create(4:5))
</code></pre>

<hr>
<h2 id='concat_tables'>Concatenate one or more Tables</h2><span id='topic+concat_tables'></span>

<h3>Description</h3>

<p>Concatenate one or more <a href="#topic+Table">Table</a> objects into a single table. This operation
does not copy array data, but instead creates new chunked arrays for each
column that point at existing array data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>concat_tables(..., unify_schemas = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="concat_tables_+3A_...">...</code></td>
<td>
<p>A <a href="#topic+Table">Table</a></p>
</td></tr>
<tr><td><code id="concat_tables_+3A_unify_schemas">unify_schemas</code></td>
<td>
<p>If TRUE, the schemas of the tables will be first unified
with fields of the same name being merged, then each table will be promoted
to the unified schema before being concatenated. Otherwise, all tables should
have the same schema.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>tbl &lt;- arrow_table(name = rownames(mtcars), mtcars)
prius &lt;- arrow_table(name = "Prius", mpg = 58, cyl = 4, disp = 1.8)
combined &lt;- concat_tables(tbl, prius)
tail(combined)$to_data_frame()
</code></pre>

<hr>
<h2 id='contains_regex'>Does this string contain regex metacharacters?</h2><span id='topic+contains_regex'></span>

<h3>Description</h3>

<p>Does this string contain regex metacharacters?
</p>


<h3>Usage</h3>

<pre><code class='language-R'>contains_regex(string)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="contains_regex_+3A_string">string</code></td>
<td>
<p>String to be tested</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Logical: does <code>string</code> contain regex metacharacters?
</p>

<hr>
<h2 id='copy_files'>Copy files between FileSystems</h2><span id='topic+copy_files'></span>

<h3>Description</h3>

<p>Copy files between FileSystems
</p>


<h3>Usage</h3>

<pre><code class='language-R'>copy_files(from, to, chunk_size = 1024L * 1024L)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="copy_files_+3A_from">from</code></td>
<td>
<p>A string path to a local directory or file, a URI, or a
<code>SubTreeFileSystem</code>. Files will be copied recursively from this path.</p>
</td></tr>
<tr><td><code id="copy_files_+3A_to">to</code></td>
<td>
<p>A string path to a local directory or file, a URI, or a
<code>SubTreeFileSystem</code>. Directories will be created as necessary</p>
</td></tr>
<tr><td><code id="copy_files_+3A_chunk_size">chunk_size</code></td>
<td>
<p>The maximum size of block to read before flushing
to the destination file. A larger chunk_size will use more memory while
copying but may help accommodate high latency FileSystems.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Nothing: called for side effects in the file system
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Copy an S3 bucket's files to a local directory:
copy_files("s3://your-bucket-name", "local-directory")
# Using a FileSystem object
copy_files(s3_bucket("your-bucket-name"), "local-directory")
# Or go the other way, from local to S3
copy_files("local-directory", s3_bucket("your-bucket-name"))

</code></pre>

<hr>
<h2 id='cpu_count'>Manage the global CPU thread pool in libarrow</h2><span id='topic+cpu_count'></span><span id='topic+set_cpu_count'></span>

<h3>Description</h3>

<p>Manage the global CPU thread pool in libarrow
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cpu_count()

set_cpu_count(num_threads)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cpu_count_+3A_num_threads">num_threads</code></td>
<td>
<p>integer: New number of threads for thread pool</p>
</td></tr>
</table>

<hr>
<h2 id='create_package_with_all_dependencies'>Create a source bundle that includes all thirdparty dependencies</h2><span id='topic+create_package_with_all_dependencies'></span>

<h3>Description</h3>

<p>Create a source bundle that includes all thirdparty dependencies
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_package_with_all_dependencies(dest_file = NULL, source_file = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="create_package_with_all_dependencies_+3A_dest_file">dest_file</code></td>
<td>
<p>File path for the new tar.gz package. Defaults to
<code>arrow_V.V.V_with_deps.tar.gz</code> in the current directory (<code>V.V.V</code> is the version)</p>
</td></tr>
<tr><td><code id="create_package_with_all_dependencies_+3A_source_file">source_file</code></td>
<td>
<p>File path for the input tar.gz package. Defaults to
downloading the package from CRAN (or whatever you have set as the first in
<code>getOption("repos")</code>)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The full path to <code>dest_file</code>, invisibly
</p>
<p>This function is used for setting up an offline build. If it's possible to
download at build time, don't use this function. Instead, let <code>cmake</code>
download the required dependencies for you.
These downloaded dependencies are only used in the build if
<code>ARROW_DEPENDENCY_SOURCE</code> is unset, <code>BUNDLED</code>, or <code>AUTO</code>.
https://arrow.apache.org/docs/developers/cpp/building.html#offline-builds
</p>
<p>If you're using binary packages you shouldn't need to use this function. You
should download the appropriate binary from your package repository, transfer
that to the offline computer, and install that. Any OS can create the source
bundle, but it cannot be installed on Windows. (Instead, use a standard
Windows binary package.)
</p>
<p>Note if you're using RStudio Package Manager on Linux: If you still want to
make a source bundle with this function, make sure to set the first repo in
<code>options("repos")</code> to be a mirror that contains source packages (that is:
something other than the RSPM binary mirror URLs).
</p>


<h4>Steps for an offline install with optional dependencies:</h4>



<h5>Using a computer with internet access, pre-download the dependencies:</h5>


<ul>
<li><p> Install the <code>arrow</code> package <em>or</em> run
<code>source("https://raw.githubusercontent.com/apache/arrow/main/r/R/install-arrow.R")</code>
</p>
</li>
<li><p> Run <code>create_package_with_all_dependencies("my_arrow_pkg.tar.gz")</code>
</p>
</li>
<li><p> Copy the newly created <code>my_arrow_pkg.tar.gz</code> to the computer without internet access
</p>
</li></ul>




<h5>On the computer without internet access, install the prepared package:</h5>


<ul>
<li><p> Install the <code>arrow</code> package from the copied file
</p>

<ul>
<li> <p><code>install.packages("my_arrow_pkg.tar.gz", dependencies = c("Depends", "Imports", "LinkingTo"))</code>
</p>
</li>
<li><p> This installation will build from source, so <code>cmake</code> must be available
</p>
</li></ul>

</li>
<li><p> Run <code><a href="#topic+arrow_info">arrow_info()</a></code> to check installed capabilities
</p>
</li></ul>





<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
new_pkg &lt;- create_package_with_all_dependencies()
# Note: this works when run in the same R session, but it's meant to be
# copied to a different computer.
install.packages(new_pkg, dependencies = c("Depends", "Imports", "LinkingTo"))

## End(Not run)
</code></pre>

<hr>
<h2 id='csv_convert_options'>CSV Convert Options</h2><span id='topic+csv_convert_options'></span>

<h3>Description</h3>

<p>CSV Convert Options
</p>


<h3>Usage</h3>

<pre><code class='language-R'>csv_convert_options(
  check_utf8 = TRUE,
  null_values = c("", "NA"),
  true_values = c("T", "true", "TRUE"),
  false_values = c("F", "false", "FALSE"),
  strings_can_be_null = FALSE,
  col_types = NULL,
  auto_dict_encode = FALSE,
  auto_dict_max_cardinality = 50L,
  include_columns = character(),
  include_missing_columns = FALSE,
  timestamp_parsers = NULL,
  decimal_point = "."
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="csv_convert_options_+3A_check_utf8">check_utf8</code></td>
<td>
<p>Logical: check UTF8 validity of string columns?</p>
</td></tr>
<tr><td><code id="csv_convert_options_+3A_null_values">null_values</code></td>
<td>
<p>Character vector of recognized spellings for null values.
Analogous to the <code>na.strings</code> argument to
<code><a href="utils.html#topic+read.table">read.csv()</a></code> or <code>na</code> in <code><a href="readr.html#topic+read_delim">readr::read_csv()</a></code>.</p>
</td></tr>
<tr><td><code id="csv_convert_options_+3A_true_values">true_values</code></td>
<td>
<p>Character vector of recognized spellings for <code>TRUE</code> values</p>
</td></tr>
<tr><td><code id="csv_convert_options_+3A_false_values">false_values</code></td>
<td>
<p>Character vector of recognized spellings for <code>FALSE</code> values</p>
</td></tr>
<tr><td><code id="csv_convert_options_+3A_strings_can_be_null">strings_can_be_null</code></td>
<td>
<p>Logical: can string / binary columns have
null values? Similar to the <code>quoted_na</code> argument to <code><a href="readr.html#topic+read_delim">readr::read_csv()</a></code></p>
</td></tr>
<tr><td><code id="csv_convert_options_+3A_col_types">col_types</code></td>
<td>
<p>A <code>Schema</code> or <code>NULL</code> to infer types</p>
</td></tr>
<tr><td><code id="csv_convert_options_+3A_auto_dict_encode">auto_dict_encode</code></td>
<td>
<p>Logical: Whether to try to automatically
dictionary-encode string / binary data (think <code>stringsAsFactors</code>).
This setting is ignored for non-inferred columns (those in <code>col_types</code>).</p>
</td></tr>
<tr><td><code id="csv_convert_options_+3A_auto_dict_max_cardinality">auto_dict_max_cardinality</code></td>
<td>
<p>If <code>auto_dict_encode</code>, string/binary columns
are dictionary-encoded up to this number of unique values (default 50),
after which it switches to regular encoding.</p>
</td></tr>
<tr><td><code id="csv_convert_options_+3A_include_columns">include_columns</code></td>
<td>
<p>If non-empty, indicates the names of columns from the
CSV file that should be actually read and converted (in the vector's order).</p>
</td></tr>
<tr><td><code id="csv_convert_options_+3A_include_missing_columns">include_missing_columns</code></td>
<td>
<p>Logical: if <code>include_columns</code> is provided, should
columns named in it but not found in the data be included as a column of
type <code>null()</code>? The default (<code>FALSE</code>) means that the reader will instead
raise an error.</p>
</td></tr>
<tr><td><code id="csv_convert_options_+3A_timestamp_parsers">timestamp_parsers</code></td>
<td>
<p>User-defined timestamp parsers. If more than one
parser is specified, the CSV conversion logic will try parsing values
starting from the beginning of this vector. Possible values are
(a) <code>NULL</code>, the default, which uses the ISO-8601 parser;
(b) a character vector of <a href="base.html#topic+strptime">strptime</a> parse strings; or
(c) a list of <a href="#topic+TimestampParser">TimestampParser</a> objects.</p>
</td></tr>
<tr><td><code id="csv_convert_options_+3A_decimal_point">decimal_point</code></td>
<td>
<p>Character to use for decimal point in floating point numbers.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>
tf &lt;- tempfile()
on.exit(unlink(tf))
writeLines("x\n1\nNULL\n2\nNA", tf)
read_csv_arrow(tf, convert_options = csv_convert_options(null_values = c("", "NA", "NULL")))
open_csv_dataset(tf, convert_options = csv_convert_options(null_values = c("", "NA", "NULL")))

</code></pre>

<hr>
<h2 id='csv_parse_options'>CSV Parsing Options</h2><span id='topic+csv_parse_options'></span>

<h3>Description</h3>

<p>CSV Parsing Options
</p>


<h3>Usage</h3>

<pre><code class='language-R'>csv_parse_options(
  delimiter = ",",
  quoting = TRUE,
  quote_char = "\"",
  double_quote = TRUE,
  escaping = FALSE,
  escape_char = "\\",
  newlines_in_values = FALSE,
  ignore_empty_lines = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="csv_parse_options_+3A_delimiter">delimiter</code></td>
<td>
<p>Field delimiting character</p>
</td></tr>
<tr><td><code id="csv_parse_options_+3A_quoting">quoting</code></td>
<td>
<p>Logical: are strings quoted?</p>
</td></tr>
<tr><td><code id="csv_parse_options_+3A_quote_char">quote_char</code></td>
<td>
<p>Quoting character, if <code>quoting</code> is <code>TRUE</code></p>
</td></tr>
<tr><td><code id="csv_parse_options_+3A_double_quote">double_quote</code></td>
<td>
<p>Logical: are quotes inside values double-quoted?</p>
</td></tr>
<tr><td><code id="csv_parse_options_+3A_escaping">escaping</code></td>
<td>
<p>Logical: whether escaping is used</p>
</td></tr>
<tr><td><code id="csv_parse_options_+3A_escape_char">escape_char</code></td>
<td>
<p>Escaping character, if <code>escaping</code> is <code>TRUE</code></p>
</td></tr>
<tr><td><code id="csv_parse_options_+3A_newlines_in_values">newlines_in_values</code></td>
<td>
<p>Logical: are values allowed to contain CR (<code>0x0d</code>)
and LF (<code>0x0a</code>) characters?</p>
</td></tr>
<tr><td><code id="csv_parse_options_+3A_ignore_empty_lines">ignore_empty_lines</code></td>
<td>
<p>Logical: should empty lines be ignored (default) or
generate a row of missing values (if <code>FALSE</code>)?</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>
tf &lt;- tempfile()
on.exit(unlink(tf))
writeLines("x\n1\n\n2", tf)
read_csv_arrow(tf, parse_options = csv_parse_options(ignore_empty_lines = FALSE))
open_csv_dataset(tf, parse_options = csv_parse_options(ignore_empty_lines = FALSE))

</code></pre>

<hr>
<h2 id='csv_read_options'>CSV Reading Options</h2><span id='topic+csv_read_options'></span>

<h3>Description</h3>

<p>CSV Reading Options
</p>


<h3>Usage</h3>

<pre><code class='language-R'>csv_read_options(
  use_threads = option_use_threads(),
  block_size = 1048576L,
  skip_rows = 0L,
  column_names = character(0),
  autogenerate_column_names = FALSE,
  encoding = "UTF-8",
  skip_rows_after_names = 0L
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="csv_read_options_+3A_use_threads">use_threads</code></td>
<td>
<p>Whether to use the global CPU thread pool</p>
</td></tr>
<tr><td><code id="csv_read_options_+3A_block_size">block_size</code></td>
<td>
<p>Block size we request from the IO layer; also determines
the size of chunks when use_threads is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="csv_read_options_+3A_skip_rows">skip_rows</code></td>
<td>
<p>Number of lines to skip before reading data (default 0).</p>
</td></tr>
<tr><td><code id="csv_read_options_+3A_column_names">column_names</code></td>
<td>
<p>Character vector to supply column names. If length-0
(the default), the first non-skipped row will be parsed to generate column
names, unless <code>autogenerate_column_names</code> is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="csv_read_options_+3A_autogenerate_column_names">autogenerate_column_names</code></td>
<td>
<p>Logical: generate column names instead of
using the first non-skipped row (the default)? If <code>TRUE</code>, column names will
be &quot;f0&quot;, &quot;f1&quot;, ..., &quot;fN&quot;.</p>
</td></tr>
<tr><td><code id="csv_read_options_+3A_encoding">encoding</code></td>
<td>
<p>The file encoding. (default <code>"UTF-8"</code>)</p>
</td></tr>
<tr><td><code id="csv_read_options_+3A_skip_rows_after_names">skip_rows_after_names</code></td>
<td>
<p>Number of lines to skip after the column names (default 0).
This number can be larger than the number of rows in one block, and empty rows are counted.
The order of application is as follows:
- <code>skip_rows</code> is applied (if non-zero);
- column names are read (unless <code>column_names</code> is set);
- <code>skip_rows_after_names</code> is applied (if non-zero).</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>
tf &lt;- tempfile()
on.exit(unlink(tf))
writeLines("my file has a non-data header\nx\n1\n2", tf)
read_csv_arrow(tf, read_options = csv_read_options(skip_rows = 1))
open_csv_dataset(tf, read_options = csv_read_options(skip_rows = 1))

</code></pre>

<hr>
<h2 id='csv_write_options'>CSV Writing Options</h2><span id='topic+csv_write_options'></span>

<h3>Description</h3>

<p>CSV Writing Options
</p>


<h3>Usage</h3>

<pre><code class='language-R'>csv_write_options(
  include_header = TRUE,
  batch_size = 1024L,
  null_string = "",
  delimiter = ",",
  eol = "\n",
  quoting_style = c("Needed", "AllValid", "None")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="csv_write_options_+3A_include_header">include_header</code></td>
<td>
<p>Whether to write an initial header line with column names</p>
</td></tr>
<tr><td><code id="csv_write_options_+3A_batch_size">batch_size</code></td>
<td>
<p>Maximum number of rows processed at a time.</p>
</td></tr>
<tr><td><code id="csv_write_options_+3A_null_string">null_string</code></td>
<td>
<p>The string to be written for null values. Must not contain quotation marks.</p>
</td></tr>
<tr><td><code id="csv_write_options_+3A_delimiter">delimiter</code></td>
<td>
<p>Field delimiter</p>
</td></tr>
<tr><td><code id="csv_write_options_+3A_eol">eol</code></td>
<td>
<p>The end of line character to use for ending rows</p>
</td></tr>
<tr><td><code id="csv_write_options_+3A_quoting_style">quoting_style</code></td>
<td>
<p>How to handle quotes. &quot;Needed&quot; (Only enclose values in quotes which need them, because their CSV
rendering can contain quotes itself (e.g. strings or binary values)), &quot;AllValid&quot; (Enclose all valid values in
quotes), or &quot;None&quot; (Do not enclose any values in quotes).</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>tf &lt;- tempfile()
on.exit(unlink(tf))
write_csv_arrow(airquality, tf, write_options = csv_write_options(null_string = "-99"))
</code></pre>

<hr>
<h2 id='CsvFileFormat'>CSV dataset file format</h2><span id='topic+CsvFileFormat'></span>

<h3>Description</h3>

<p>A <code>CSVFileFormat</code> is a <a href="#topic+FileFormat">FileFormat</a> subclass which holds information about how to
read and parse the files included in a CSV <code>Dataset</code>.
</p>


<h3>Value</h3>

<p>A <code>CsvFileFormat</code> object
</p>


<h3>Factory</h3>

<p><code>CSVFileFormat$create()</code> can take options in the form of lists passed through as <code>parse_options</code>,
<code>read_options</code>, or <code>convert_options</code> parameters.  Alternatively, readr-style options can be passed
through individually.  While it is possible to pass in <code>CSVReadOptions</code>, <code>CSVConvertOptions</code>, and <code>CSVParseOptions</code>
objects, this is not recommended as options set in these objects are not validated for compatibility.
</p>


<h3>See Also</h3>

<p><a href="#topic+FileFormat">FileFormat</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Set up directory for examples
tf &lt;- tempfile()
dir.create(tf)
on.exit(unlink(tf))
df &lt;- data.frame(x = c("1", "2", "NULL"))
write.table(df, file.path(tf, "file1.txt"), sep = ",", row.names = FALSE)

# Create CsvFileFormat object with Arrow-style null_values option
format &lt;- CsvFileFormat$create(convert_options = list(null_values = c("", "NA", "NULL")))
open_dataset(tf, format = format)

# Use readr-style options
format &lt;- CsvFileFormat$create(na = c("", "NA", "NULL"))
open_dataset(tf, format = format)

</code></pre>

<hr>
<h2 id='CsvReadOptions'>File reader options</h2><span id='topic+CsvReadOptions'></span><span id='topic+CsvWriteOptions'></span><span id='topic+CsvParseOptions'></span><span id='topic+TimestampParser'></span><span id='topic+CsvConvertOptions'></span><span id='topic+JsonReadOptions'></span><span id='topic+JsonParseOptions'></span>

<h3>Description</h3>

<p><code>CsvReadOptions</code>, <code>CsvParseOptions</code>, <code>CsvConvertOptions</code>,
<code>JsonReadOptions</code>, <code>JsonParseOptions</code>, and <code>TimestampParser</code> are containers for various
file reading options. See their usage in <code><a href="#topic+read_csv_arrow">read_csv_arrow()</a></code> and
<code><a href="#topic+read_json_arrow">read_json_arrow()</a></code>, respectively.
</p>


<h3>Factory</h3>

<p>The <code>CsvReadOptions$create()</code> and <code>JsonReadOptions$create()</code> factory methods
take the following arguments:
</p>

<ul>
<li> <p><code>use_threads</code> Whether to use the global CPU thread pool
</p>
</li>
<li> <p><code>block_size</code> Block size we request from the IO layer; also determines
the size of chunks when use_threads is <code>TRUE</code>. NB: if <code>FALSE</code>, JSON input
must end with an empty line.
</p>
</li></ul>

<p><code>CsvReadOptions$create()</code> further accepts these additional arguments:
</p>

<ul>
<li> <p><code>skip_rows</code> Number of lines to skip before reading data (default 0).
</p>
</li>
<li> <p><code>column_names</code> Character vector to supply column names. If length-0
(the default), the first non-skipped row will be parsed to generate column
names, unless <code>autogenerate_column_names</code> is <code>TRUE</code>.
</p>
</li>
<li> <p><code>autogenerate_column_names</code> Logical: generate column names instead of
using the first non-skipped row (the default)? If <code>TRUE</code>, column names will
be &quot;f0&quot;, &quot;f1&quot;, ..., &quot;fN&quot;.
</p>
</li>
<li> <p><code>encoding</code> The file encoding. (default <code>"UTF-8"</code>)
</p>
</li>
<li> <p><code>skip_rows_after_names</code> Number of lines to skip after the column names (default 0).
This number can be larger than the number of rows in one block, and empty rows are counted.
The order of application is as follows:
</p>

<ul>
<li> <p><code>skip_rows</code> is applied (if non-zero);
</p>
</li>
<li><p> column names are read (unless <code>column_names</code> is set);
</p>
</li>
<li> <p><code>skip_rows_after_names</code> is applied (if non-zero).
</p>
</li></ul>

</li></ul>

<p><code>CsvParseOptions$create()</code> takes the following arguments:
</p>

<ul>
<li> <p><code>delimiter</code> Field delimiting character (default <code>","</code>)
</p>
</li>
<li> <p><code>quoting</code> Logical: are strings quoted? (default <code>TRUE</code>)
</p>
</li>
<li> <p><code>quote_char</code> Quoting character, if <code>quoting</code> is <code>TRUE</code> (default <code>'"'</code>)
</p>
</li>
<li> <p><code>double_quote</code> Logical: are quotes inside values double-quoted? (default <code>TRUE</code>)
</p>
</li>
<li> <p><code>escaping</code> Logical: whether escaping is used (default <code>FALSE</code>)
</p>
</li>
<li> <p><code>escape_char</code> Escaping character, if <code>escaping</code> is <code>TRUE</code> (default <code>"\\"</code>)
</p>
</li>
<li> <p><code>newlines_in_values</code> Logical: are values allowed to contain CR (<code>0x0d</code>)
and LF (<code>0x0a</code>) characters? (default <code>FALSE</code>)
</p>
</li>
<li> <p><code>ignore_empty_lines</code> Logical: should empty lines be ignored (default) or
generate a row of missing values (if <code>FALSE</code>)?
</p>
</li></ul>

<p><code>JsonParseOptions$create()</code> accepts only the <code>newlines_in_values</code> argument.
</p>
<p><code>CsvConvertOptions$create()</code> takes the following arguments:
</p>

<ul>
<li> <p><code>check_utf8</code> Logical: check UTF8 validity of string columns? (default <code>TRUE</code>)
</p>
</li>
<li> <p><code>null_values</code> character vector of recognized spellings for null values.
Analogous to the <code>na.strings</code> argument to
<code><a href="utils.html#topic+read.table">read.csv()</a></code> or <code>na</code> in <code><a href="readr.html#topic+read_delim">readr::read_csv()</a></code>.
</p>
</li>
<li> <p><code>strings_can_be_null</code> Logical: can string / binary columns have
null values? Similar to the <code>quoted_na</code> argument to <code><a href="readr.html#topic+read_delim">readr::read_csv()</a></code>.
(default <code>FALSE</code>)
</p>
</li>
<li> <p><code>true_values</code> character vector of recognized spellings for <code>TRUE</code> values
</p>
</li>
<li> <p><code>false_values</code> character vector of recognized spellings for <code>FALSE</code> values
</p>
</li>
<li> <p><code>col_types</code> A <code>Schema</code> or <code>NULL</code> to infer types
</p>
</li>
<li> <p><code>auto_dict_encode</code> Logical: Whether to try to automatically
dictionary-encode string / binary data (think <code>stringsAsFactors</code>). Default <code>FALSE</code>.
This setting is ignored for non-inferred columns (those in <code>col_types</code>).
</p>
</li>
<li> <p><code>auto_dict_max_cardinality</code> If <code>auto_dict_encode</code>, string/binary columns
are dictionary-encoded up to this number of unique values (default 50),
after which it switches to regular encoding.
</p>
</li>
<li> <p><code>include_columns</code> If non-empty, indicates the names of columns from the
CSV file that should be actually read and converted (in the vector's order).
</p>
</li>
<li> <p><code>include_missing_columns</code> Logical: if <code>include_columns</code> is provided, should
columns named in it but not found in the data be included as a column of
type <code>null()</code>? The default (<code>FALSE</code>) means that the reader will instead
raise an error.
</p>
</li>
<li> <p><code>timestamp_parsers</code> User-defined timestamp parsers. If more than one
parser is specified, the CSV conversion logic will try parsing values
starting from the beginning of this vector. Possible values are
(a) <code>NULL</code>, the default, which uses the ISO-8601 parser;
(b) a character vector of <a href="base.html#topic+strptime">strptime</a> parse strings; or
(c) a list of <a href="#topic+TimestampParser">TimestampParser</a> objects.
</p>
</li>
<li> <p><code>decimal_point</code> Character to use for decimal point in floating point numbers. Default: &quot;.&quot;
</p>
</li></ul>

<p><code>TimestampParser$create()</code> takes an optional <code>format</code> string argument.
See <code><a href="base.html#topic+strptime">strptime()</a></code> for example syntax.
The default is to use an ISO-8601 format parser.
</p>
<p>The <code>CsvWriteOptions$create()</code> factory method takes the following arguments:
</p>

<ul>
<li> <p><code>include_header</code> Whether to write an initial header line with column names
</p>
</li>
<li> <p><code>batch_size</code> Maximum number of rows processed at a time. Default is 1024.
</p>
</li>
<li> <p><code>null_string</code> The string to be written for null values. Must not contain
quotation marks. Default is an empty string (<code>""</code>).
</p>
</li>
<li> <p><code>eol</code> The end of line character to use for ending rows.
</p>
</li>
<li> <p><code>delimiter</code> Field delimiter
</p>
</li>
<li> <p><code>quoting_style</code> Quoting style: &quot;Needed&quot; (Only enclose values in quotes which need them, because their CSV
rendering can contain quotes itself (e.g. strings or binary values)), &quot;AllValid&quot; (Enclose all valid values in
quotes), or &quot;None&quot; (Do not enclose any values in quotes).
</p>
</li></ul>



<h3>Active bindings</h3>


<ul>
<li> <p><code>column_names</code>: from <code>CsvReadOptions</code>
</p>
</li></ul>


<hr>
<h2 id='CsvTableReader'>Arrow CSV and JSON table reader classes</h2><span id='topic+CsvTableReader'></span><span id='topic+JsonTableReader'></span>

<h3>Description</h3>

<p><code>CsvTableReader</code> and <code>JsonTableReader</code> wrap the Arrow C++ CSV
and JSON table readers. See their usage in <code><a href="#topic+read_csv_arrow">read_csv_arrow()</a></code> and
<code><a href="#topic+read_json_arrow">read_json_arrow()</a></code>, respectively.
</p>


<h3>Factory</h3>

<p>The <code>CsvTableReader$create()</code> and <code>JsonTableReader$create()</code> factory methods
take the following arguments:
</p>

<ul>
<li> <p><code>file</code> An Arrow <a href="#topic+InputStream">InputStream</a>
</p>
</li>
<li> <p><code>convert_options</code> (CSV only), <code>parse_options</code>, <code>read_options</code>: see
<a href="#topic+CsvReadOptions">CsvReadOptions</a>
</p>
</li>
<li> <p><code>...</code> additional parameters.
</p>
</li></ul>



<h3>Methods</h3>


<ul>
<li> <p><code style="white-space: pre;">&#8288;$Read()&#8288;</code>: returns an Arrow Table.
</p>
</li></ul>


<hr>
<h2 id='data-type'>Create Arrow data types</h2><span id='topic+data-type'></span><span id='topic+int8'></span><span id='topic+int16'></span><span id='topic+int32'></span><span id='topic+int64'></span><span id='topic+uint8'></span><span id='topic+uint16'></span><span id='topic+uint32'></span><span id='topic+uint64'></span><span id='topic+float16'></span><span id='topic+halffloat'></span><span id='topic+float32'></span><span id='topic+float'></span><span id='topic+float64'></span><span id='topic+boolean'></span><span id='topic+bool'></span><span id='topic+utf8'></span><span id='topic+large_utf8'></span><span id='topic+binary'></span><span id='topic+large_binary'></span><span id='topic+fixed_size_binary'></span><span id='topic+string'></span><span id='topic+date32'></span><span id='topic+date64'></span><span id='topic+time32'></span><span id='topic+time64'></span><span id='topic+duration'></span><span id='topic+null'></span><span id='topic+timestamp'></span><span id='topic+decimal'></span><span id='topic+decimal128'></span><span id='topic+decimal256'></span><span id='topic+struct'></span><span id='topic+list_of'></span><span id='topic+large_list_of'></span><span id='topic+FixedSizeListType'></span><span id='topic+fixed_size_list_of'></span><span id='topic+MapType'></span><span id='topic+map_of'></span>

<h3>Description</h3>

<p>These functions create type objects corresponding to Arrow types. Use them
when defining a <code><a href="#topic+schema">schema()</a></code> or as inputs to other types, like <code>struct</code>. Most
of these functions don't take arguments, but a few do.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>int8()

int16()

int32()

int64()

uint8()

uint16()

uint32()

uint64()

float16()

halffloat()

float32()

float()

float64()

boolean()

bool()

utf8()

large_utf8()

binary()

large_binary()

fixed_size_binary(byte_width)

string()

date32()

date64()

time32(unit = c("ms", "s"))

time64(unit = c("ns", "us"))

duration(unit = c("s", "ms", "us", "ns"))

null()

timestamp(unit = c("s", "ms", "us", "ns"), timezone = "")

decimal(precision, scale)

decimal128(precision, scale)

decimal256(precision, scale)

struct(...)

list_of(type)

large_list_of(type)

fixed_size_list_of(type, list_size)

map_of(key_type, item_type, .keys_sorted = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="data-type_+3A_byte_width">byte_width</code></td>
<td>
<p>byte width for <code>FixedSizeBinary</code> type.</p>
</td></tr>
<tr><td><code id="data-type_+3A_unit">unit</code></td>
<td>
<p>For time/timestamp types, the time unit. <code>time32()</code> can take
either &quot;s&quot; or &quot;ms&quot;, while <code>time64()</code> can be &quot;us&quot; or &quot;ns&quot;. <code>timestamp()</code> can
take any of those four values.</p>
</td></tr>
<tr><td><code id="data-type_+3A_timezone">timezone</code></td>
<td>
<p>For <code>timestamp()</code>, an optional time zone string.</p>
</td></tr>
<tr><td><code id="data-type_+3A_precision">precision</code></td>
<td>
<p>For <code>decimal()</code>, <code>decimal128()</code>, and <code>decimal256()</code> the
number of significant digits the arrow <code>decimal</code> type can represent. The
maximum precision for <code>decimal128()</code> is 38 significant digits, while for
<code>decimal256()</code> it is 76 digits. <code>decimal()</code> will use it to choose which
type of decimal to return.</p>
</td></tr>
<tr><td><code id="data-type_+3A_scale">scale</code></td>
<td>
<p>For <code>decimal()</code>, <code>decimal128()</code>, and <code>decimal256()</code> the number
of digits after the decimal point. It can be negative.</p>
</td></tr>
<tr><td><code id="data-type_+3A_...">...</code></td>
<td>
<p>For <code>struct()</code>, a named list of types to define the struct columns</p>
</td></tr>
<tr><td><code id="data-type_+3A_type">type</code></td>
<td>
<p>For <code>list_of()</code>, a data type to make a list-of-type</p>
</td></tr>
<tr><td><code id="data-type_+3A_list_size">list_size</code></td>
<td>
<p>list size for <code>FixedSizeList</code> type.</p>
</td></tr>
<tr><td><code id="data-type_+3A_key_type">key_type</code>, <code id="data-type_+3A_item_type">item_type</code></td>
<td>
<p>For <code>MapType</code>, the key and item types.</p>
</td></tr>
<tr><td><code id="data-type_+3A_.keys_sorted">.keys_sorted</code></td>
<td>
<p>Use <code>TRUE</code> to assert that keys of a <code>MapType</code> are
sorted.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A few functions have aliases:
</p>

<ul>
<li> <p><code>utf8()</code> and <code>string()</code>
</p>
</li>
<li> <p><code>float16()</code> and <code>halffloat()</code>
</p>
</li>
<li> <p><code>float32()</code> and <code>float()</code>
</p>
</li>
<li> <p><code>bool()</code> and <code>boolean()</code>
</p>
</li>
<li><p> When called inside an <code>arrow</code> function, such as <code>schema()</code> or <code>cast()</code>,
<code>double()</code> also is supported as a way of creating a <code>float64()</code>
</p>
</li></ul>

<p><code>date32()</code> creates a datetime type with a &quot;day&quot; unit, like the R <code>Date</code>
class. <code>date64()</code> has a &quot;ms&quot; unit.
</p>
<p><code>uint32</code> (32 bit unsigned integer), <code>uint64</code> (64 bit unsigned integer), and
<code>int64</code> (64-bit signed integer) types may contain values that exceed the
range of R's <code>integer</code> type (32-bit signed integer). When these arrow objects
are translated to R objects, <code>uint32</code> and <code>uint64</code> are converted to <code>double</code>
(&quot;numeric&quot;) and <code>int64</code> is converted to <code>bit64::integer64</code>. For <code>int64</code>
types, this conversion can be disabled (so that <code>int64</code> always yields a
<code>bit64::integer64</code> object) by setting <code>options(arrow.int64_downcast = FALSE)</code>.
</p>
<p><code>decimal128()</code> creates a <code>Decimal128Type</code>. Arrow decimals are fixed-point
decimal numbers encoded as a scalar integer. The <code>precision</code> is the number of
significant digits that the decimal type can represent; the <code>scale</code> is the
number of digits after the decimal point. For example, the number 1234.567
has a precision of 7 and a scale of 3. Note that <code>scale</code> can be negative.
</p>
<p>As an example, <code>decimal128(7, 3)</code> can exactly represent the numbers 1234.567 and
-1234.567 (encoded internally as the 128-bit integers 1234567 and -1234567,
respectively), but neither 12345.67 nor 123.4567.
</p>
<p><code>decimal128(5, -3)</code> can exactly represent the number 12345000 (encoded
internally as the 128-bit integer 12345), but neither 123450000 nor 1234500.
The <code>scale</code> can be thought of as an argument that controls rounding. When
negative, <code>scale</code> causes the number to be expressed using scientific notation
and power of 10.
</p>
<p><code>decimal256()</code> creates a <code>Decimal256Type</code>, which allows for higher maximum
precision. For most use cases, the maximum precision offered by <code>Decimal128Type</code>
is sufficient, and it will result in a more compact and more efficient encoding.
</p>
<p><code>decimal()</code> creates either a <code>Decimal128Type</code> or a <code>Decimal256Type</code>
depending on the value for <code>precision</code>. If <code>precision</code> is greater than 38 a
<code>Decimal256Type</code> is returned, otherwise a <code>Decimal128Type</code>.
</p>
<p>Use <code>decimal128()</code> or <code>decimal256()</code> as the names are more informative than
<code>decimal()</code>.
</p>


<h3>Value</h3>

<p>An Arrow type object inheriting from <a href="#topic+DataType">DataType</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dictionary">dictionary()</a></code> for creating a dictionary (factor-like) type.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
bool()
struct(a = int32(), b = double())
timestamp("ms", timezone = "CEST")
time64("ns")

# Use the cast method to change the type of data contained in Arrow objects.
# Please check the documentation of each data object class for details.
my_scalar &lt;- Scalar$create(0L, type = int64()) # int64
my_scalar$cast(timestamp("ns")) # timestamp[ns]

my_array &lt;- Array$create(0L, type = int64()) # int64
my_array$cast(timestamp("s", timezone = "UTC")) # timestamp[s, tz=UTC]

my_chunked_array &lt;- chunked_array(0L, 1L) # int32
my_chunked_array$cast(date32()) # date32[day]

# You can also use `cast()` in an Arrow dplyr query.
if (requireNamespace("dplyr", quietly = TRUE)) {
  library(dplyr, warn.conflicts = FALSE)
  arrow_table(mtcars) %&gt;%
    transmute(
      col1 = cast(cyl, string()),
      col2 = cast(cyl, int8())
    ) %&gt;%
    compute()
}

</code></pre>

<hr>
<h2 id='Dataset'>Multi-file datasets</h2><span id='topic+Dataset'></span><span id='topic+FileSystemDataset'></span><span id='topic+UnionDataset'></span><span id='topic+InMemoryDataset'></span><span id='topic+DatasetFactory'></span><span id='topic+FileSystemDatasetFactory'></span>

<h3>Description</h3>

<p>Arrow Datasets allow you to query against data that has been split across
multiple files. This sharding of data may indicate partitioning, which
can accelerate queries that only touch some partitions (files).
</p>
<p>A <code>Dataset</code> contains one or more <code>Fragments</code>, such as files, of potentially
differing type and partitioning.
</p>
<p>For <code>Dataset$create()</code>, see <code><a href="#topic+open_dataset">open_dataset()</a></code>, which is an alias for it.
</p>
<p><code>DatasetFactory</code> is used to provide finer control over the creation of <code>Dataset</code>s.
</p>


<h3>Factory</h3>

<p><code>DatasetFactory</code> is used to create a <code>Dataset</code>, inspect the <a href="#topic+Schema">Schema</a> of the
fragments contained in it, and declare a partitioning.
<code>FileSystemDatasetFactory</code> is a subclass of <code>DatasetFactory</code> for
discovering files in the local file system, the only currently supported
file system.
</p>
<p>For the <code>DatasetFactory$create()</code> factory method, see <code><a href="#topic+dataset_factory">dataset_factory()</a></code>, an
alias for it. A <code>DatasetFactory</code> has:
</p>

<ul>
<li> <p><code style="white-space: pre;">&#8288;$Inspect(unify_schemas)&#8288;</code>: If <code>unify_schemas</code> is <code>TRUE</code>, all fragments
will be scanned and a unified <a href="#topic+Schema">Schema</a> will be created from them; if <code>FALSE</code>
(default), only the first fragment will be inspected for its schema. Use this
fast path when you know and trust that all fragments have an identical schema.
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$Finish(schema, unify_schemas)&#8288;</code>: Returns a <code>Dataset</code>. If <code>schema</code> is provided,
it will be used for the <code>Dataset</code>; if omitted, a <code>Schema</code> will be created from
inspecting the fragments (files) in the dataset, following <code>unify_schemas</code>
as described above.
</p>
</li></ul>

<p><code>FileSystemDatasetFactory$create()</code> is a lower-level factory method and
takes the following arguments:
</p>

<ul>
<li> <p><code>filesystem</code>: A <a href="#topic+FileSystem">FileSystem</a>
</p>
</li>
<li> <p><code>selector</code>: Either a <a href="#topic+FileSelector">FileSelector</a> or <code>NULL</code>
</p>
</li>
<li> <p><code>paths</code>: Either a character vector of file paths or <code>NULL</code>
</p>
</li>
<li> <p><code>format</code>: A <a href="#topic+FileFormat">FileFormat</a>
</p>
</li>
<li> <p><code>partitioning</code>: Either <code>Partitioning</code>, <code>PartitioningFactory</code>, or <code>NULL</code>
</p>
</li></ul>



<h3>Methods</h3>

<p>A <code>Dataset</code> has the following methods:
</p>

<ul>
<li> <p><code style="white-space: pre;">&#8288;$NewScan()&#8288;</code>: Returns a <a href="#topic+ScannerBuilder">ScannerBuilder</a> for building a query
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$WithSchema()&#8288;</code>: Returns a new Dataset with the specified schema.
This method currently supports only adding, removing, or reordering
fields in the schema: you cannot alter or cast the field types.
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$schema&#8288;</code>: Active binding that returns the <a href="#topic+Schema">Schema</a> of the Dataset; you
may also replace the dataset's schema by using <code>ds$schema &lt;- new_schema</code>.
</p>
</li></ul>

<p><code>FileSystemDataset</code> has the following methods:
</p>

<ul>
<li> <p><code style="white-space: pre;">&#8288;$files&#8288;</code>: Active binding, returns the files of the <code>FileSystemDataset</code>
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$format&#8288;</code>: Active binding, returns the <a href="#topic+FileFormat">FileFormat</a> of the <code>FileSystemDataset</code>
</p>
</li></ul>

<p><code>UnionDataset</code> has the following methods:
</p>

<ul>
<li> <p><code style="white-space: pre;">&#8288;$children&#8288;</code>: Active binding, returns all child <code>Dataset</code>s.
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+open_dataset">open_dataset()</a></code> for a simple interface to creating a <code>Dataset</code>
</p>

<hr>
<h2 id='dataset_factory'>Create a DatasetFactory</h2><span id='topic+dataset_factory'></span>

<h3>Description</h3>

<p>A <a href="#topic+Dataset">Dataset</a> can constructed using one or more <a href="#topic+DatasetFactory">DatasetFactory</a>s.
This function helps you construct a <code>DatasetFactory</code> that you can pass to
<code><a href="#topic+open_dataset">open_dataset()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dataset_factory(
  x,
  filesystem = NULL,
  format = c("parquet", "arrow", "ipc", "feather", "csv", "tsv", "text", "json"),
  partitioning = NULL,
  hive_style = NA,
  factory_options = list(),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dataset_factory_+3A_x">x</code></td>
<td>
<p>A string path to a directory containing data files, a vector of one
one or more string paths to data files, or a list of <code>DatasetFactory</code> objects
whose datasets should be combined. If this argument is specified it will be
used to construct a <code>UnionDatasetFactory</code> and other arguments will be
ignored.</p>
</td></tr>
<tr><td><code id="dataset_factory_+3A_filesystem">filesystem</code></td>
<td>
<p>A <a href="#topic+FileSystem">FileSystem</a> object; if omitted, the <code>FileSystem</code> will
be detected from <code>x</code></p>
</td></tr>
<tr><td><code id="dataset_factory_+3A_format">format</code></td>
<td>
<p>A <a href="#topic+FileFormat">FileFormat</a> object, or a string identifier of the format of
the files in <code>x</code>. Currently supported values:
</p>

<ul>
<li><p> &quot;parquet&quot;
</p>
</li>
<li><p> &quot;ipc&quot;/&quot;arrow&quot;/&quot;feather&quot;, all aliases for each other; for Feather, note that
only version 2 files are supported
</p>
</li>
<li><p> &quot;csv&quot;/&quot;text&quot;, aliases for the same thing (because comma is the default
delimiter for text files
</p>
</li>
<li><p> &quot;tsv&quot;, equivalent to passing <code style="white-space: pre;">&#8288;format = "text", delimiter = "\t"&#8288;</code>
</p>
</li></ul>

<p>Default is &quot;parquet&quot;, unless a <code>delimiter</code> is also specified, in which case
it is assumed to be &quot;text&quot;.</p>
</td></tr>
<tr><td><code id="dataset_factory_+3A_partitioning">partitioning</code></td>
<td>
<p>One of
</p>

<ul>
<li><p> A <code>Schema</code>, in which case the file paths relative to <code>sources</code> will be
parsed, and path segments will be matched with the schema fields. For
example, <code>schema(year = int16(), month = int8())</code> would create partitions
for file paths like &quot;2019/01/file.parquet&quot;, &quot;2019/02/file.parquet&quot;, etc.
</p>
</li>
<li><p> A character vector that defines the field names corresponding to those
path segments (that is, you're providing the names that would correspond
to a <code>Schema</code> but the types will be autodetected)
</p>
</li>
<li><p> A <code>HivePartitioning</code> or <code>HivePartitioningFactory</code>, as returned
by <code><a href="#topic+hive_partition">hive_partition()</a></code> which parses explicit or autodetected fields from
Hive-style path segments
</p>
</li>
<li> <p><code>NULL</code> for no partitioning
</p>
</li></ul>
</td></tr>
<tr><td><code id="dataset_factory_+3A_hive_style">hive_style</code></td>
<td>
<p>Logical: if <code>partitioning</code> is a character vector or a
<code>Schema</code>, should it be interpreted as specifying Hive-style partitioning?
Default is <code>NA</code>, which means to inspect the file paths for Hive-style
partitioning and behave accordingly.</p>
</td></tr>
<tr><td><code id="dataset_factory_+3A_factory_options">factory_options</code></td>
<td>
<p>list of optional FileSystemFactoryOptions:
</p>

<ul>
<li> <p><code>partition_base_dir</code>: string path segment prefix to ignore when
discovering partition information with DirectoryPartitioning. Not
meaningful (ignored with a warning) for HivePartitioning, nor is it
valid when providing a vector of file paths.
</p>
</li>
<li> <p><code>exclude_invalid_files</code>: logical: should files that are not valid data
files be excluded? Default is <code>FALSE</code> because checking all files up
front incurs I/O and thus will be slower, especially on remote
filesystems. If false and there are invalid files, there will be an
error at scan time. This is the only FileSystemFactoryOption that is
valid for both when providing a directory path in which to discover
files and when providing a vector of file paths.
</p>
</li>
<li> <p><code>selector_ignore_prefixes</code>: character vector of file prefixes to ignore
when discovering files in a directory. If invalid files can be excluded
by a common filename prefix this way, you can avoid the I/O cost of
<code>exclude_invalid_files</code>. Not valid when providing a vector of file paths
(but if you're providing the file list, you can filter invalid files
yourself).
</p>
</li></ul>
</td></tr>
<tr><td><code id="dataset_factory_+3A_...">...</code></td>
<td>
<p>Additional format-specific options, passed to
<code><a href="#topic+FileFormat">FileFormat$create()</a></code>. For CSV options, note that you can specify them either
with the Arrow C++ library naming (&quot;delimiter&quot;, &quot;quoting&quot;, etc.) or the
<code>readr</code>-style naming used in <code><a href="#topic+read_csv_arrow">read_csv_arrow()</a></code> (&quot;delim&quot;, &quot;quote&quot;, etc.).
Not all <code>readr</code> options are currently supported; please file an issue if you
encounter one that <code>arrow</code> should support.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If you would only have a single <code>DatasetFactory</code> (for example, you have a
single directory containing Parquet files), you can call <code>open_dataset()</code>
directly. Use <code>dataset_factory()</code> when you
want to combine different directories, file systems, or file formats.
</p>


<h3>Value</h3>

<p>A <code>DatasetFactory</code> object. Pass this to <code><a href="#topic+open_dataset">open_dataset()</a></code>,
in a list potentially with other <code>DatasetFactory</code> objects, to create
a <code>Dataset</code>.
</p>

<hr>
<h2 id='DataType'>DataType class</h2><span id='topic+DataType'></span>

<h3>Description</h3>

<p>DataType class
</p>


<h3>R6 Methods</h3>


<ul>
<li> <p><code style="white-space: pre;">&#8288;$ToString()&#8288;</code>: String representation of the DataType
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$Equals(other)&#8288;</code>: Is the DataType equal to <code>other</code>
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$fields()&#8288;</code>: The children fields associated with this type
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$code(namespace)&#8288;</code>: Produces an R call of the data type. Use <code>namespace=TRUE</code> to call with <code style="white-space: pre;">&#8288;arrow::&#8288;</code>.
</p>
</li></ul>

<p>There are also some active bindings:
</p>

<ul>
<li> <p><code style="white-space: pre;">&#8288;$id&#8288;</code>: integer Arrow type id.
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$name&#8288;</code>: string Arrow type name.
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$num_fields&#8288;</code>: number of child fields.
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+infer_type">infer_type()</a></code>
</p>
<p><code><a href="#topic+data-type">data-type</a></code>
</p>

<hr>
<h2 id='default_memory_pool'>Arrow's default <a href="#topic+MemoryPool">MemoryPool</a></h2><span id='topic+default_memory_pool'></span>

<h3>Description</h3>

<p>Arrow's default <a href="#topic+MemoryPool">MemoryPool</a>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>default_memory_pool()
</code></pre>


<h3>Value</h3>

<p>the default <a href="#topic+MemoryPool">MemoryPool</a>
</p>

<hr>
<h2 id='dictionary'>Create a dictionary type</h2><span id='topic+dictionary'></span>

<h3>Description</h3>

<p>Create a dictionary type
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dictionary(index_type = int32(), value_type = utf8(), ordered = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dictionary_+3A_index_type">index_type</code></td>
<td>
<p>A DataType for the indices (default <code><a href="#topic+int32">int32()</a></code>)</p>
</td></tr>
<tr><td><code id="dictionary_+3A_value_type">value_type</code></td>
<td>
<p>A DataType for the values (default <code><a href="#topic+utf8">utf8()</a></code>)</p>
</td></tr>
<tr><td><code id="dictionary_+3A_ordered">ordered</code></td>
<td>
<p>Is this an ordered dictionary (default <code>FALSE</code>)?</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <a href="#topic+DictionaryType">DictionaryType</a>
</p>


<h3>See Also</h3>

<p><a href="#topic+data-type">Other Arrow data types</a>
</p>

<hr>
<h2 id='DictionaryType'>class DictionaryType</h2><span id='topic+DictionaryType'></span>

<h3>Description</h3>

<p>class DictionaryType
</p>


<h3>Methods</h3>

<p>TODO
</p>

<hr>
<h2 id='enums'>Arrow enums</h2><span id='topic+enums'></span><span id='topic+TimeUnit'></span><span id='topic+DateUnit'></span><span id='topic+Type'></span><span id='topic+StatusCode'></span><span id='topic+FileMode'></span><span id='topic+MessageType'></span><span id='topic+CompressionType'></span><span id='topic+FileType'></span><span id='topic+ParquetVersionType'></span><span id='topic+MetadataVersion'></span><span id='topic+QuantileInterpolation'></span><span id='topic+NullEncodingBehavior'></span><span id='topic+NullHandlingBehavior'></span><span id='topic+RoundMode'></span><span id='topic+JoinType'></span>

<h3>Description</h3>

<p>Arrow enums
</p>


<h3>Usage</h3>

<pre><code class='language-R'>TimeUnit

DateUnit

Type

StatusCode

FileMode

MessageType

CompressionType

FileType

ParquetVersionType

MetadataVersion

QuantileInterpolation

NullEncodingBehavior

NullHandlingBehavior

RoundMode

JoinType
</code></pre>


<h3>Format</h3>

<p>An object of class <code>TimeUnit::type</code> (inherits from <code>arrow-enum</code>) of length 4.
</p>
<p>An object of class <code>DateUnit</code> (inherits from <code>arrow-enum</code>) of length 2.
</p>
<p>An object of class <code>Type::type</code> (inherits from <code>arrow-enum</code>) of length 39.
</p>
<p>An object of class <code>StatusCode</code> (inherits from <code>arrow-enum</code>) of length 13.
</p>
<p>An object of class <code>FileMode</code> (inherits from <code>arrow-enum</code>) of length 3.
</p>
<p>An object of class <code>MessageType</code> (inherits from <code>arrow-enum</code>) of length 5.
</p>
<p>An object of class <code>Compression::type</code> (inherits from <code>arrow-enum</code>) of length 9.
</p>
<p>An object of class <code>FileType</code> (inherits from <code>arrow-enum</code>) of length 4.
</p>
<p>An object of class <code>ParquetVersionType</code> (inherits from <code>arrow-enum</code>) of length 4.
</p>
<p>An object of class <code>MetadataVersion</code> (inherits from <code>arrow-enum</code>) of length 5.
</p>
<p>An object of class <code>QuantileInterpolation</code> (inherits from <code>arrow-enum</code>) of length 5.
</p>
<p>An object of class <code>NullEncodingBehavior</code> (inherits from <code>arrow-enum</code>) of length 2.
</p>
<p>An object of class <code>NullHandlingBehavior</code> (inherits from <code>arrow-enum</code>) of length 3.
</p>
<p>An object of class <code>RoundMode</code> (inherits from <code>arrow-enum</code>) of length 10.
</p>
<p>An object of class <code>JoinType</code> (inherits from <code>arrow-enum</code>) of length 8.
</p>

<hr>
<h2 id='Expression'>Arrow expressions</h2><span id='topic+Expression'></span>

<h3>Description</h3>

<p><code>Expression</code>s are used to define filter logic for passing to a <a href="#topic+Dataset">Dataset</a>
<a href="#topic+Scanner">Scanner</a>.
</p>
<p><code>Expression$scalar(x)</code> constructs an <code>Expression</code> which always evaluates to
the provided scalar (length-1) R value.
</p>
<p><code>Expression$field_ref(name)</code> is used to construct an <code>Expression</code> which
evaluates to the named column in the <code>Dataset</code> against which it is evaluated.
</p>
<p><code>Expression$create(function_name, ..., options)</code> builds a function-call
<code>Expression</code> containing one or more <code>Expression</code>s. Anything in <code>...</code> that
is not already an expression will be wrapped in <code>Expression$scalar()</code>.
</p>
<p><code>Expression$op(FUN, ...)</code> is for logical and arithmetic operators. Scalar
inputs in <code>...</code> will be attempted to be cast to the common type of the
<code>Expression</code>s in the call so that the types of the columns in the <code>Dataset</code>
are preserved and not unnecessarily upcast, which may be expensive.
</p>

<hr>
<h2 id='ExtensionArray'>ExtensionArray class</h2><span id='topic+ExtensionArray'></span>

<h3>Description</h3>

<p>ExtensionArray class
</p>


<h3>Methods</h3>

<p>The <code>ExtensionArray</code> class inherits from <code>Array</code>, but also provides
access to the underlying storage of the extension.
</p>

<ul>
<li> <p><code style="white-space: pre;">&#8288;$storage()&#8288;</code>: Returns the underlying <a href="#topic+Array">Array</a> used to store
values.
</p>
</li></ul>

<p>The <code>ExtensionArray</code> is not intended to be subclassed for extension
types.
</p>

<hr>
<h2 id='ExtensionType'>ExtensionType class</h2><span id='topic+ExtensionType'></span>

<h3>Description</h3>

<p>ExtensionType class
</p>


<h3>Methods</h3>

<p>The <code>ExtensionType</code> class inherits from <code>DataType</code>, but also defines
extra methods specific to extension types:
</p>

<ul>
<li> <p><code style="white-space: pre;">&#8288;$storage_type()&#8288;</code>: Returns the underlying <a href="#topic+DataType">DataType</a> used to store
values.
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$storage_id()&#8288;</code>: Returns the <a href="#topic+Type">Type</a> identifier corresponding to the
<code style="white-space: pre;">&#8288;$storage_type()&#8288;</code>.
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$extension_name()&#8288;</code>: Returns the extension name.
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$extension_metadata()&#8288;</code>: Returns the serialized version of the extension
metadata as a <code><a href="base.html#topic+raw">raw()</a></code> vector.
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$extension_metadata_utf8()&#8288;</code>: Returns the serialized version of the
extension metadata as a UTF-8 encoded string.
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$WrapArray(array)&#8288;</code>: Wraps a storage <a href="#topic+Array">Array</a> into an <a href="#topic+ExtensionArray">ExtensionArray</a>
with this extension type.
</p>
</li></ul>

<p>In addition, subclasses may override the following methods to customize
the behaviour of extension classes.
</p>

<ul>
<li> <p><code style="white-space: pre;">&#8288;$deserialize_instance()&#8288;</code>: This method is called when a new <a href="#topic+ExtensionType">ExtensionType</a>
is initialized and is responsible for parsing and validating
the serialized extension_metadata (a <code><a href="base.html#topic+raw">raw()</a></code> vector)
such that its contents can be inspected by fields and/or methods
of the R6 ExtensionType subclass. Implementations must also check the
<code>storage_type</code> to make sure it is compatible with the extension type.
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$as_vector(extension_array)&#8288;</code>: Convert an <a href="#topic+Array">Array</a> or <a href="#topic+ChunkedArray">ChunkedArray</a> to an R
vector. This method is called by <code><a href="base.html#topic+as.vector">as.vector()</a></code> on <a href="#topic+ExtensionArray">ExtensionArray</a>
objects, when a <a href="#topic+RecordBatch">RecordBatch</a> containing an <a href="#topic+ExtensionArray">ExtensionArray</a> is
converted to a <code><a href="base.html#topic+data.frame">data.frame()</a></code>, or when a <a href="#topic+ChunkedArray">ChunkedArray</a> (e.g., a column
in a <a href="#topic+Table">Table</a>) is converted to an R vector. The default method returns the
converted storage array.
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$ToString()&#8288;</code> Return a string representation that will be printed
to the console when this type or an Array of this type is printed.
</p>
</li></ul>


<hr>
<h2 id='FeatherReader'>FeatherReader class</h2><span id='topic+FeatherReader'></span>

<h3>Description</h3>

<p>This class enables you to interact with Feather files. Create
one to connect to a file or other InputStream, and call <code>Read()</code> on it to
make an <code>arrow::Table</code>. See its usage in <code><a href="#topic+read_feather">read_feather()</a></code>.
</p>


<h3>Factory</h3>

<p>The <code>FeatherReader$create()</code> factory method instantiates the object and
takes the following argument:
</p>

<ul>
<li> <p><code>file</code> an Arrow file connection object inheriting from <code>RandomAccessFile</code>.
</p>
</li></ul>



<h3>Methods</h3>


<ul>
<li> <p><code style="white-space: pre;">&#8288;$Read(columns)&#8288;</code>: Returns a <code>Table</code> of the selected columns, a vector of
integer indices
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$column_names&#8288;</code>: Active binding, returns the column names in the Feather file
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$schema&#8288;</code>: Active binding, returns the schema of the Feather file
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$version&#8288;</code>: Active binding, returns <code>1</code> or <code>2</code>, according to the Feather
file version
</p>
</li></ul>


<hr>
<h2 id='field'>Create a Field</h2><span id='topic+field'></span>

<h3>Description</h3>

<p>Create a Field
</p>


<h3>Usage</h3>

<pre><code class='language-R'>field(name, type, metadata, nullable = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="field_+3A_name">name</code></td>
<td>
<p>field name</p>
</td></tr>
<tr><td><code id="field_+3A_type">type</code></td>
<td>
<p>logical type, instance of <a href="#topic+DataType">DataType</a></p>
</td></tr>
<tr><td><code id="field_+3A_metadata">metadata</code></td>
<td>
<p>currently ignored</p>
</td></tr>
<tr><td><code id="field_+3A_nullable">nullable</code></td>
<td>
<p>TRUE if field is nullable</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><a href="#topic+Field">Field</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>field("x", int32())
</code></pre>

<hr>
<h2 id='Field'>Field class</h2><span id='topic+Field'></span>

<h3>Description</h3>

<p><code>field()</code> lets you create an <code>arrow::Field</code> that maps a
<a href="#topic+data-type">DataType</a> to a column name. Fields are contained in
<a href="#topic+Schema">Schemas</a>.
</p>


<h3>Methods</h3>


<ul>
<li> <p><code>f$ToString()</code>: convert to a string
</p>
</li>
<li> <p><code>f$Equals(other)</code>: test for equality. More naturally called as <code>f == other</code>
</p>
</li></ul>


<hr>
<h2 id='FileFormat'>Dataset file formats</h2><span id='topic+FileFormat'></span><span id='topic+ParquetFileFormat'></span><span id='topic+IpcFileFormat'></span>

<h3>Description</h3>

<p>A <code>FileFormat</code> holds information about how to read and parse the files
included in a <code>Dataset</code>. There are subclasses corresponding to the supported
file formats (<code>ParquetFileFormat</code> and <code>IpcFileFormat</code>).
</p>


<h3>Factory</h3>

<p><code>FileFormat$create()</code> takes the following arguments:
</p>

<ul>
<li> <p><code>format</code>: A string identifier of the file format. Currently supported values:
</p>

<ul>
<li><p> &quot;parquet&quot;
</p>
</li>
<li><p> &quot;ipc&quot;/&quot;arrow&quot;/&quot;feather&quot;, all aliases for each other; for Feather, note that
only version 2 files are supported
</p>
</li>
<li><p> &quot;csv&quot;/&quot;text&quot;, aliases for the same thing (because comma is the default
delimiter for text files
</p>
</li>
<li><p> &quot;tsv&quot;, equivalent to passing <code style="white-space: pre;">&#8288;format = "text", delimiter = "\t"&#8288;</code>
</p>
</li></ul>

</li>
<li> <p><code>...</code>: Additional format-specific options
</p>
<p><code>format = "parquet"</code>:
</p>

<ul>
<li> <p><code>dict_columns</code>: Names of columns which should be read as dictionaries.
</p>
</li>
<li><p> Any Parquet options from <a href="#topic+FragmentScanOptions">FragmentScanOptions</a>.
</p>
</li></ul>

<p><code>format = "text"</code>: see <a href="#topic+CsvParseOptions">CsvParseOptions</a>. Note that you can specify them either
with the Arrow C++ library naming (&quot;delimiter&quot;, &quot;quoting&quot;, etc.) or the
<code>readr</code>-style naming used in <code><a href="#topic+read_csv_arrow">read_csv_arrow()</a></code> (&quot;delim&quot;, &quot;quote&quot;, etc.).
Not all <code>readr</code> options are currently supported; please file an issue if
you encounter one that <code>arrow</code> should support. Also, the following options are
supported. From <a href="#topic+CsvReadOptions">CsvReadOptions</a>:
</p>

<ul>
<li> <p><code>skip_rows</code>
</p>
</li>
<li> <p><code>column_names</code>. Note that if a <a href="#topic+Schema">Schema</a> is specified, <code>column_names</code> must match those specified in the schema.
</p>
</li>
<li> <p><code>autogenerate_column_names</code>
From <a href="#topic+CsvFragmentScanOptions">CsvFragmentScanOptions</a> (these values can be overridden at scan time):
</p>
</li>
<li> <p><code>convert_options</code>: a <a href="#topic+CsvConvertOptions">CsvConvertOptions</a>
</p>
</li>
<li> <p><code>block_size</code>
</p>
</li></ul>

</li></ul>

<p>It returns the appropriate subclass of <code>FileFormat</code> (e.g. <code>ParquetFileFormat</code>)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Semi-colon delimited files
# Set up directory for examples
tf &lt;- tempfile()
dir.create(tf)
on.exit(unlink(tf))
write.table(mtcars, file.path(tf, "file1.txt"), sep = ";", row.names = FALSE)

# Create FileFormat object
format &lt;- FileFormat$create(format = "text", delimiter = ";")

open_dataset(tf, format = format)

</code></pre>

<hr>
<h2 id='FileInfo'>FileSystem entry info</h2><span id='topic+FileInfo'></span>

<h3>Description</h3>

<p>FileSystem entry info
</p>


<h3>Methods</h3>


<ul>
<li> <p><code>base_name()</code> : The file base name (component after the last directory
separator).
</p>
</li>
<li> <p><code>extension()</code> : The file extension
</p>
</li></ul>



<h3>Active bindings</h3>


<ul>
<li> <p><code style="white-space: pre;">&#8288;$type&#8288;</code>: The file type
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$path&#8288;</code>: The full file path in the filesystem
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$size&#8288;</code>: The size in bytes, if available.  Only regular files are
guaranteed to have a size.
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$mtime&#8288;</code>: The time of last modification, if available.
</p>
</li></ul>


<hr>
<h2 id='FileSelector'>file selector</h2><span id='topic+FileSelector'></span>

<h3>Description</h3>

<p>file selector
</p>


<h3>Factory</h3>

<p>The <code style="white-space: pre;">&#8288;$create()&#8288;</code> factory method instantiates a <code>FileSelector</code> given the 3 fields
described below.
</p>


<h3>Fields</h3>


<ul>
<li> <p><code>base_dir</code>: The directory in which to select files. If the path exists but
doesn't point to a directory, this should be an error.
</p>
</li>
<li> <p><code>allow_not_found</code>: The behavior if <code>base_dir</code> doesn't exist in the
filesystem. If <code>FALSE</code>, an error is returned.  If <code>TRUE</code>, an empty
selection is returned
</p>
</li>
<li> <p><code>recursive</code>: Whether to recurse into subdirectories.
</p>
</li></ul>


<hr>
<h2 id='FileSystem'>FileSystem classes</h2><span id='topic+FileSystem'></span><span id='topic+LocalFileSystem'></span><span id='topic+S3FileSystem'></span><span id='topic+GcsFileSystem'></span><span id='topic+SubTreeFileSystem'></span>

<h3>Description</h3>

<p><code>FileSystem</code> is an abstract file system API,
<code>LocalFileSystem</code> is an implementation accessing files
on the local machine. <code>SubTreeFileSystem</code> is an implementation that delegates
to another implementation after prepending a fixed base path
</p>


<h3>Factory</h3>

<p><code>LocalFileSystem$create()</code> returns the object and takes no arguments.
</p>
<p><code>SubTreeFileSystem$create()</code> takes the following arguments:
</p>

<ul>
<li> <p><code>base_path</code>, a string path
</p>
</li>
<li> <p><code>base_fs</code>, a <code>FileSystem</code> object
</p>
</li></ul>

<p><code>S3FileSystem$create()</code> optionally takes arguments:
</p>

<ul>
<li> <p><code>anonymous</code>: logical, default <code>FALSE</code>. If true, will not attempt to look up
credentials using standard AWS configuration methods.
</p>
</li>
<li> <p><code>access_key</code>, <code>secret_key</code>: authentication credentials. If one is provided,
the other must be as well. If both are provided, they will override any
AWS configuration set at the environment level.
</p>
</li>
<li> <p><code>session_token</code>: optional string for authentication along with
<code>access_key</code> and <code>secret_key</code>
</p>
</li>
<li> <p><code>role_arn</code>: string AWS ARN of an AccessRole. If provided instead of <code>access_key</code> and
<code>secret_key</code>, temporary credentials will be fetched by assuming this role.
</p>
</li>
<li> <p><code>session_name</code>: optional string identifier for the assumed role session.
</p>
</li>
<li> <p><code>external_id</code>: optional unique string identifier that might be required
when you assume a role in another account.
</p>
</li>
<li> <p><code>load_frequency</code>: integer, frequency (in seconds) with which temporary
credentials from an assumed role session will be refreshed. Default is
900 (i.e. 15 minutes)
</p>
</li>
<li> <p><code>region</code>: AWS region to connect to. If omitted, the AWS library will
provide a sensible default based on client configuration, falling back
to &quot;us-east-1&quot; if no other alternatives are found.
</p>
</li>
<li> <p><code>endpoint_override</code>: If non-empty, override region with a connect string
such as &quot;localhost:9000&quot;. This is useful for connecting to file systems
that emulate S3.
</p>
</li>
<li> <p><code>scheme</code>: S3 connection transport (default &quot;https&quot;)
</p>
</li>
<li> <p><code>proxy_options</code>: optional string, URI of a proxy to use when connecting
to S3
</p>
</li>
<li> <p><code>background_writes</code>: logical, whether <code>OutputStream</code> writes will be issued
in the background, without blocking (default <code>TRUE</code>)
</p>
</li>
<li> <p><code>allow_bucket_creation</code>: logical, if TRUE, the filesystem will create
buckets if <code style="white-space: pre;">&#8288;$CreateDir()&#8288;</code> is called on the bucket level (default <code>FALSE</code>).
</p>
</li>
<li> <p><code>allow_bucket_deletion</code>: logical, if TRUE, the filesystem will delete
buckets if<code style="white-space: pre;">&#8288;$DeleteDir()&#8288;</code> is called on the bucket level (default <code>FALSE</code>).
</p>
</li>
<li> <p><code>request_timeout</code>: Socket read time on Windows and macOS in seconds. If
negative, the AWS SDK default (typically 3 seconds).
</p>
</li>
<li> <p><code>connect_timeout</code>: Socket connection timeout in seconds. If negative, AWS
SDK default is used (typically 1 second).
</p>
</li></ul>

<p><code>GcsFileSystem$create()</code> optionally takes arguments:
</p>

<ul>
<li> <p><code>anonymous</code>: logical, default <code>FALSE</code>. If true, will not attempt to look up
credentials using standard GCS configuration methods.
</p>
</li>
<li> <p><code>access_token</code>: optional string for authentication. Should be provided along
with <code>expiration</code>
</p>
</li>
<li> <p><code>expiration</code>: <code>POSIXct</code>. optional datetime representing point at which
<code>access_token</code> will expire.
</p>
</li>
<li> <p><code>json_credentials</code>: optional string for authentication. Either a string
containing JSON credentials or a path to their location on the filesystem.
If a path to credentials is given, the file should be UTF-8 encoded.
</p>
</li>
<li> <p><code>endpoint_override</code>: if non-empty, will connect to provided host name / port,
such as &quot;localhost:9001&quot;, instead of default GCS ones. This is primarily useful
for testing purposes.
</p>
</li>
<li> <p><code>scheme</code>: connection transport (default &quot;https&quot;)
</p>
</li>
<li> <p><code>default_bucket_location</code>: the default location (or &quot;region&quot;) to create new
buckets in.
</p>
</li>
<li> <p><code>retry_limit_seconds</code>: the maximum amount of time to spend retrying if
the filesystem encounters errors. Default is 15 seconds.
</p>
</li>
<li> <p><code>default_metadata</code>: default metadata to write in new objects.
</p>
</li>
<li> <p><code>project_id</code>: the project to use for creating buckets.
</p>
</li></ul>



<h3>Methods</h3>


<ul>
<li> <p><code>path(x)</code>: Create a <code>SubTreeFileSystem</code> from the current <code>FileSystem</code>
rooted at the specified path <code>x</code>.
</p>
</li>
<li> <p><code>cd(x)</code>: Create a <code>SubTreeFileSystem</code> from the current <code>FileSystem</code>
rooted at the specified path <code>x</code>.
</p>
</li>
<li> <p><code>ls(path, ...)</code>: List files or objects at the given path or from the root
of the <code>FileSystem</code> if <code>path</code> is not provided. Additional arguments passed
to <code>FileSelector$create</code>, see <a href="#topic+FileSelector">FileSelector</a>.
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$GetFileInfo(x)&#8288;</code>: <code>x</code> may be a <a href="#topic+FileSelector">FileSelector</a> or a character
vector of paths. Returns a list of <a href="#topic+FileInfo">FileInfo</a>
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$CreateDir(path, recursive = TRUE)&#8288;</code>: Create a directory and subdirectories.
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$DeleteDir(path)&#8288;</code>: Delete a directory and its contents, recursively.
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$DeleteDirContents(path)&#8288;</code>: Delete a directory's contents, recursively.
Like <code style="white-space: pre;">&#8288;$DeleteDir()&#8288;</code>,
but doesn't delete the directory itself. Passing an empty path (<code>""</code>) will
wipe the entire filesystem tree.
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$DeleteFile(path)&#8288;</code> : Delete a file.
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$DeleteFiles(paths)&#8288;</code> : Delete many files. The default implementation
issues individual delete operations in sequence.
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$Move(src, dest)&#8288;</code>: Move / rename a file or directory. If the destination
exists:
if it is a non-empty directory, an error is returned
otherwise, if it has the same type as the source, it is replaced
otherwise, behavior is unspecified (implementation-dependent).
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$CopyFile(src, dest)&#8288;</code>: Copy a file. If the destination exists and is a
directory, an error is returned. Otherwise, it is replaced.
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$OpenInputStream(path)&#8288;</code>: Open an <a href="#topic+InputStream">input stream</a> for
sequential reading.
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$OpenInputFile(path)&#8288;</code>: Open an <a href="#topic+RandomAccessFile">input file</a> for random
access reading.
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$OpenOutputStream(path)&#8288;</code>: Open an <a href="#topic+OutputStream">output stream</a> for
sequential writing.
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$OpenAppendStream(path)&#8288;</code>: Open an <a href="#topic+OutputStream">output stream</a> for
appending.
</p>
</li></ul>



<h3>Active bindings</h3>


<ul>
<li> <p><code style="white-space: pre;">&#8288;$type_name&#8288;</code>: string filesystem type name, such as &quot;local&quot;, &quot;s3&quot;, etc.
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$region&#8288;</code>: string AWS region, for <code>S3FileSystem</code> and <code>SubTreeFileSystem</code>
containing a <code>S3FileSystem</code>
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$base_fs&#8288;</code>: for <code>SubTreeFileSystem</code>, the <code>FileSystem</code> it contains
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$base_path&#8288;</code>: for <code>SubTreeFileSystem</code>, the path in <code style="white-space: pre;">&#8288;$base_fs&#8288;</code> which is considered
root in this <code>SubTreeFileSystem</code>.
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$options&#8288;</code>: for <code>GcsFileSystem</code>, the options used to create the
<code>GcsFileSystem</code> instance as a <code>list</code>
</p>
</li></ul>



<h3>Notes</h3>

<p>On S3FileSystem, <code style="white-space: pre;">&#8288;$CreateDir()&#8288;</code> on a top-level directory creates a new bucket.
When S3FileSystem creates new buckets (assuming allow_bucket_creation is TRUE),
it does not pass any non-default settings. In AWS S3, the bucket and all
objects will be not publicly visible, and will have no bucket policies
and no resource tags. To have more control over how buckets are created,
use a different API to create them.
</p>
<p>On S3FileSystem, output is only produced for fatal errors or when printing
return values. For troubleshooting, the log level can be set using the
environment variable <code>ARROW_S3_LOG_LEVEL</code> (e.g.,
<code>Sys.setenv("ARROW_S3_LOG_LEVEL"="DEBUG")</code>). The log level must be set prior
to running any code that interacts with S3. Possible values include 'FATAL'
(the default), 'ERROR', 'WARN', 'INFO', 'DEBUG' (recommended), 'TRACE', and
'OFF'.
</p>

<hr>
<h2 id='FileWriteOptions'>Format-specific write options</h2><span id='topic+FileWriteOptions'></span>

<h3>Description</h3>

<p>A <code>FileWriteOptions</code> holds write options specific to a <code>FileFormat</code>.
</p>

<hr>
<h2 id='FixedWidthType'>FixedWidthType class</h2><span id='topic+FixedWidthType'></span>

<h3>Description</h3>

<p>FixedWidthType class
</p>


<h3>Methods</h3>

<p>TODO
</p>

<hr>
<h2 id='flight_connect'>Connect to a Flight server</h2><span id='topic+flight_connect'></span>

<h3>Description</h3>

<p>Connect to a Flight server
</p>


<h3>Usage</h3>

<pre><code class='language-R'>flight_connect(host = "localhost", port, scheme = "grpc+tcp")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="flight_connect_+3A_host">host</code></td>
<td>
<p>string hostname to connect to</p>
</td></tr>
<tr><td><code id="flight_connect_+3A_port">port</code></td>
<td>
<p>integer port to connect on</p>
</td></tr>
<tr><td><code id="flight_connect_+3A_scheme">scheme</code></td>
<td>
<p>URL scheme, default is &quot;grpc+tcp&quot;</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>pyarrow.flight.FlightClient</code>.
</p>

<hr>
<h2 id='flight_disconnect'>Explicitly close a Flight client</h2><span id='topic+flight_disconnect'></span>

<h3>Description</h3>

<p>Explicitly close a Flight client
</p>


<h3>Usage</h3>

<pre><code class='language-R'>flight_disconnect(client)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="flight_disconnect_+3A_client">client</code></td>
<td>
<p>The client to disconnect</p>
</td></tr>
</table>

<hr>
<h2 id='flight_get'>Get data from a Flight server</h2><span id='topic+flight_get'></span>

<h3>Description</h3>

<p>Get data from a Flight server
</p>


<h3>Usage</h3>

<pre><code class='language-R'>flight_get(client, path)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="flight_get_+3A_client">client</code></td>
<td>
<p><code>pyarrow.flight.FlightClient</code>, as returned by <code><a href="#topic+flight_connect">flight_connect()</a></code></p>
</td></tr>
<tr><td><code id="flight_get_+3A_path">path</code></td>
<td>
<p>string identifier under which data is stored</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <a href="#topic+Table">Table</a>
</p>

<hr>
<h2 id='flight_put'>Send data to a Flight server</h2><span id='topic+flight_put'></span>

<h3>Description</h3>

<p>Send data to a Flight server
</p>


<h3>Usage</h3>

<pre><code class='language-R'>flight_put(client, data, path, overwrite = TRUE, max_chunksize = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="flight_put_+3A_client">client</code></td>
<td>
<p><code>pyarrow.flight.FlightClient</code>, as returned by <code><a href="#topic+flight_connect">flight_connect()</a></code></p>
</td></tr>
<tr><td><code id="flight_put_+3A_data">data</code></td>
<td>
<p><code>data.frame</code>, <a href="#topic+RecordBatch">RecordBatch</a>, or <a href="#topic+Table">Table</a> to upload</p>
</td></tr>
<tr><td><code id="flight_put_+3A_path">path</code></td>
<td>
<p>string identifier to store the data under</p>
</td></tr>
<tr><td><code id="flight_put_+3A_overwrite">overwrite</code></td>
<td>
<p>logical: if <code>path</code> exists on <code>client</code> already, should we
replace it with the contents of <code>data</code>? Default is <code>TRUE</code>; if <code>FALSE</code> and
<code>path</code> exists, the function will error.</p>
</td></tr>
<tr><td><code id="flight_put_+3A_max_chunksize">max_chunksize</code></td>
<td>
<p>integer: Maximum number of rows for RecordBatch chunks
when a <code>data.frame</code> is sent.
Individual chunks may be smaller depending on the chunk layout of individual columns.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>client</code>, invisibly.
</p>

<hr>
<h2 id='FragmentScanOptions'>Format-specific scan options</h2><span id='topic+FragmentScanOptions'></span><span id='topic+CsvFragmentScanOptions'></span><span id='topic+ParquetFragmentScanOptions'></span><span id='topic+JsonFragmentScanOptions'></span>

<h3>Description</h3>

<p>A <code>FragmentScanOptions</code> holds options specific to a <code>FileFormat</code> and a scan
operation.
</p>


<h3>Factory</h3>

<p><code>FragmentScanOptions$create()</code> takes the following arguments:
</p>

<ul>
<li> <p><code>format</code>: A string identifier of the file format. Currently supported values:
</p>

<ul>
<li><p> &quot;parquet&quot;
</p>
</li>
<li><p> &quot;csv&quot;/&quot;text&quot;, aliases for the same format.
</p>
</li></ul>

</li>
<li> <p><code>...</code>: Additional format-specific options
</p>
<p><code>format = "parquet"</code>:
</p>

<ul>
<li> <p><code>use_buffered_stream</code>: Read files through buffered input streams rather than
loading entire row groups at once. This may be enabled
to reduce memory overhead. Disabled by default.
</p>
</li>
<li> <p><code>buffer_size</code>: Size of buffered stream, if enabled. Default is 8KB.
</p>
</li>
<li> <p><code>pre_buffer</code>: Pre-buffer the raw Parquet data. This can improve performance
on high-latency filesystems. Disabled by default.
</p>
</li>
<li> <p><code>thrift_string_size_limit</code>: Maximum string size allocated for decoding thrift
strings. May need to be increased in order to read
files with especially large headers. Default value
100000000.
</p>
</li>
<li> <p><code>thrift_container_size_limit</code>: Maximum size of thrift containers.  May need to be
increased in order to read files with especially large
headers. Default value 1000000.
<code>format = "text"</code>: see <a href="#topic+CsvConvertOptions">CsvConvertOptions</a>. Note that options can only be
specified with the Arrow C++ library naming. Also, &quot;block_size&quot; from
<a href="#topic+CsvReadOptions">CsvReadOptions</a> may be given.
</p>
</li></ul>

</li></ul>

<p>It returns the appropriate subclass of <code>FragmentScanOptions</code>
(e.g. <code>CsvFragmentScanOptions</code>).
</p>

<hr>
<h2 id='get_stringr_pattern_options'>Get <code>stringr</code> pattern options</h2><span id='topic+get_stringr_pattern_options'></span>

<h3>Description</h3>

<p>This function assigns definitions for the <code>stringr</code> pattern modifier
functions (<code>fixed()</code>, <code>regex()</code>, etc.) inside itself, and uses them to
evaluate the quoted expression <code>pattern</code>, returning a list that is used
to control pattern matching behavior in internal <code>arrow</code> functions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_stringr_pattern_options(pattern)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_stringr_pattern_options_+3A_pattern">pattern</code></td>
<td>
<p>Unevaluated expression containing a call to a <code>stringr</code>
pattern modifier function</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List containing elements <code>pattern</code>, <code>fixed</code>, and <code>ignore_case</code>
</p>

<hr>
<h2 id='gs_bucket'>Connect to a Google Cloud Storage (GCS) bucket</h2><span id='topic+gs_bucket'></span>

<h3>Description</h3>

<p><code>gs_bucket()</code> is a convenience function to create an <code>GcsFileSystem</code> object
that holds onto its relative path
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gs_bucket(bucket, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gs_bucket_+3A_bucket">bucket</code></td>
<td>
<p>string GCS bucket name or path</p>
</td></tr>
<tr><td><code id="gs_bucket_+3A_...">...</code></td>
<td>
<p>Additional connection options, passed to <code>GcsFileSystem$create()</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>SubTreeFileSystem</code> containing an <code>GcsFileSystem</code> and the bucket's
relative path. Note that this function's success does not guarantee that you
are authorized to access the bucket's contents.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
bucket &lt;- gs_bucket("voltrondata-labs-datasets")

</code></pre>

<hr>
<h2 id='hive_partition'>Construct Hive partitioning</h2><span id='topic+hive_partition'></span>

<h3>Description</h3>

<p>Hive partitioning embeds field names and values in path segments, such as
&quot;/year=2019/month=2/data.parquet&quot;.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hive_partition(..., null_fallback = NULL, segment_encoding = "uri")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hive_partition_+3A_...">...</code></td>
<td>
<p>named list of <a href="#topic+data-type">data types</a>, passed to <code><a href="#topic+schema">schema()</a></code></p>
</td></tr>
<tr><td><code id="hive_partition_+3A_null_fallback">null_fallback</code></td>
<td>
<p>character to be used in place of missing values (<code>NA</code> or <code>NULL</code>)
in partition columns. Default is <code>"__HIVE_DEFAULT_PARTITION__"</code>,
which is what Hive uses.</p>
</td></tr>
<tr><td><code id="hive_partition_+3A_segment_encoding">segment_encoding</code></td>
<td>
<p>Decode partition segments after splitting paths.
Default is <code>"uri"</code> (URI-decode segments). May also be <code>"none"</code> (leave as-is).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Because fields are named in the path segments, order of fields passed to
<code>hive_partition()</code> does not matter.
</p>


<h3>Value</h3>

<p>A <a href="#topic+Partitioning">HivePartitioning</a>, or a <code>HivePartitioningFactory</code> if
calling <code>hive_partition()</code> with no arguments.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
hive_partition(year = int16(), month = int8())

</code></pre>

<hr>
<h2 id='infer_schema'>Extract a schema from an object</h2><span id='topic+infer_schema'></span>

<h3>Description</h3>

<p>Extract a schema from an object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>infer_schema(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="infer_schema_+3A_x">x</code></td>
<td>
<p>An object which has a schema, e.g. a <code>Dataset</code></p>
</td></tr>
</table>

<hr>
<h2 id='infer_type'>Infer the arrow Array type from an R object</h2><span id='topic+infer_type'></span><span id='topic+type'></span>

<h3>Description</h3>

<p><code><a href="#topic+type">type()</a></code> is deprecated in favor of <code><a href="#topic+infer_type">infer_type()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>infer_type(x, ...)

type(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="infer_type_+3A_x">x</code></td>
<td>
<p>an R object (usually a vector) to be converted to an <a href="#topic+Array">Array</a> or
<a href="#topic+ChunkedArray">ChunkedArray</a>.</p>
</td></tr>
<tr><td><code id="infer_type_+3A_...">...</code></td>
<td>
<p>Passed to S3 methods</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An arrow <a href="#topic+data-type">data type</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>infer_type(1:10)
infer_type(1L:10L)
infer_type(c(1, 1.5, 2))
infer_type(c("A", "B", "C"))
infer_type(mtcars)
infer_type(Sys.Date())
infer_type(as.POSIXlt(Sys.Date()))
infer_type(vctrs::new_vctr(1:5, class = "my_custom_vctr_class"))
</code></pre>

<hr>
<h2 id='InputStream'>InputStream classes</h2><span id='topic+InputStream'></span><span id='topic+RandomAccessFile'></span><span id='topic+MemoryMappedFile'></span><span id='topic+ReadableFile'></span><span id='topic+BufferReader'></span>

<h3>Description</h3>

<p><code>RandomAccessFile</code> inherits from <code>InputStream</code> and is a base
class for: <code>ReadableFile</code> for reading from a file; <code>MemoryMappedFile</code> for
the same but with memory mapping; and <code>BufferReader</code> for reading from a
buffer. Use these with the various table readers.
</p>


<h3>Factory</h3>

<p>The <code style="white-space: pre;">&#8288;$create()&#8288;</code> factory methods instantiate the <code>InputStream</code> object and
take the following arguments, depending on the subclass:
</p>

<ul>
<li> <p><code>path</code> For <code>ReadableFile</code>, a character file name
</p>
</li>
<li> <p><code>x</code> For <code>BufferReader</code>, a <a href="#topic+Buffer">Buffer</a> or an object that can be
made into a buffer via <code>buffer()</code>.
</p>
</li></ul>

<p>To instantiate a <code>MemoryMappedFile</code>, call <code><a href="#topic+mmap_open">mmap_open()</a></code>.
</p>


<h3>Methods</h3>


<ul>
<li> <p><code style="white-space: pre;">&#8288;$GetSize()&#8288;</code>:
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$supports_zero_copy()&#8288;</code>: Logical
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$seek(position)&#8288;</code>: go to that position in the stream
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$tell()&#8288;</code>: return the position in the stream
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$close()&#8288;</code>: close the stream
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$Read(nbytes)&#8288;</code>: read data from the stream, either a specified <code>nbytes</code> or
all, if <code>nbytes</code> is not provided
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$ReadAt(position, nbytes)&#8288;</code>: similar to <code style="white-space: pre;">&#8288;$seek(position)$Read(nbytes)&#8288;</code>
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$Resize(size)&#8288;</code>: for a <code>MemoryMappedFile</code> that is writeable
</p>
</li></ul>


<hr>
<h2 id='install_arrow'>Install or upgrade the Arrow library</h2><span id='topic+install_arrow'></span>

<h3>Description</h3>

<p>Use this function to install the latest release of <code>arrow</code>, to switch to or
from a nightly development version, or on Linux to try reinstalling with
all necessary C++ dependencies.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>install_arrow(
  nightly = FALSE,
  binary = Sys.getenv("LIBARROW_BINARY", TRUE),
  use_system = Sys.getenv("ARROW_USE_PKG_CONFIG", FALSE),
  minimal = Sys.getenv("LIBARROW_MINIMAL", FALSE),
  verbose = Sys.getenv("ARROW_R_DEV", FALSE),
  repos = getOption("repos"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="install_arrow_+3A_nightly">nightly</code></td>
<td>
<p>logical: Should we install a development version of the
package, or should we install from CRAN (the default).</p>
</td></tr>
<tr><td><code id="install_arrow_+3A_binary">binary</code></td>
<td>
<p>On Linux, value to set for the environment variable
<code>LIBARROW_BINARY</code>, which governs how C++ binaries are used, if at all.
The default value, <code>TRUE</code>, tells the installation script to detect the
Linux distribution and version and find an appropriate C++ library. <code>FALSE</code>
would tell the script not to retrieve a binary and instead build Arrow C++
from source. Other valid values are strings corresponding to a Linux
distribution-version, to override the value that would be detected. See the
<a href="https://arrow.apache.org/docs/r/articles/install.html">install guide</a>
for further details.</p>
</td></tr>
<tr><td><code id="install_arrow_+3A_use_system">use_system</code></td>
<td>
<p>logical: Should we use <code>pkg-config</code> to look for Arrow
system packages? Default is <code>FALSE</code>. If <code>TRUE</code>, source installation may be
faster, but there is a risk of version mismatch. This sets the
<code>ARROW_USE_PKG_CONFIG</code> environment variable.</p>
</td></tr>
<tr><td><code id="install_arrow_+3A_minimal">minimal</code></td>
<td>
<p>logical: If building from source, should we build without
optional dependencies (compression libraries, for example)? Default is
<code>FALSE</code>. This sets the <code>LIBARROW_MINIMAL</code> environment variable.</p>
</td></tr>
<tr><td><code id="install_arrow_+3A_verbose">verbose</code></td>
<td>
<p>logical: Print more debugging output when installing? Default
is <code>FALSE</code>. This sets the <code>ARROW_R_DEV</code> environment variable.</p>
</td></tr>
<tr><td><code id="install_arrow_+3A_repos">repos</code></td>
<td>
<p>character vector of base URLs of the repositories to install
from (passed to <code>install.packages()</code>)</p>
</td></tr>
<tr><td><code id="install_arrow_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <code>install.packages()</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note that, unlike packages like <code>tensorflow</code>, <code>blogdown</code>, and others that
require external dependencies, you do not need to run <code>install_arrow()</code>
after a successful <code>arrow</code> installation.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+arrow_info">arrow_info()</a></code> to see if the package was configured with
necessary C++ dependencies.
<a href="https://arrow.apache.org/docs/r/articles/install.html">install guide</a>
for more ways to tune installation on Linux.
</p>

<hr>
<h2 id='install_pyarrow'>Install pyarrow for use with reticulate</h2><span id='topic+install_pyarrow'></span>

<h3>Description</h3>

<p><code>pyarrow</code> is the Python package for Apache Arrow. This function helps with
installing it for use with <code>reticulate</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>install_pyarrow(envname = NULL, nightly = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="install_pyarrow_+3A_envname">envname</code></td>
<td>
<p>The name or full path of the Python environment to install
into. This can be a virtualenv or conda environment created by <code>reticulate</code>.
See <code>reticulate::py_install()</code>.</p>
</td></tr>
<tr><td><code id="install_pyarrow_+3A_nightly">nightly</code></td>
<td>
<p>logical: Should we install a development version of the
package? Default is to use the official release version.</p>
</td></tr>
<tr><td><code id="install_pyarrow_+3A_...">...</code></td>
<td>
<p>additional arguments passed to <code>reticulate::py_install()</code>.</p>
</td></tr>
</table>

<hr>
<h2 id='io_thread_count'>Manage the global I/O thread pool in libarrow</h2><span id='topic+io_thread_count'></span><span id='topic+set_io_thread_count'></span>

<h3>Description</h3>

<p>Manage the global I/O thread pool in libarrow
</p>


<h3>Usage</h3>

<pre><code class='language-R'>io_thread_count()

set_io_thread_count(num_threads)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="io_thread_count_+3A_num_threads">num_threads</code></td>
<td>
<p>integer: New number of threads for thread pool. At least
two threads are recommended to support all operations in the arrow
package.</p>
</td></tr>
</table>

<hr>
<h2 id='JsonFileFormat'>JSON dataset file format</h2><span id='topic+JsonFileFormat'></span>

<h3>Description</h3>

<p>A <code>JsonFileFormat</code> is a <a href="#topic+FileFormat">FileFormat</a> subclass which holds information about how to
read and parse the files included in a JSON <code>Dataset</code>.
</p>


<h3>Value</h3>

<p>A <code>JsonFileFormat</code> object
</p>


<h3>Factory</h3>

<p><code>JsonFileFormat$create()</code> can take options in the form of lists passed through as <code>parse_options</code>,
or <code>read_options</code> parameters.
</p>
<p>Available <code>read_options</code> parameters:
</p>

<ul>
<li> <p><code>use_threads</code>: Whether to use the global CPU thread pool. Default <code>TRUE</code>. If <code>FALSE</code>, JSON input must end with an
empty line.
</p>
</li>
<li> <p><code>block_size</code>: Block size we request from the IO layer; also determines size of chunks when <code>use_threads</code>
is <code>TRUE</code>.
</p>
</li></ul>

<p>Available <code>parse_options</code> parameters:
</p>

<ul>
<li> <p><code>newlines_in_values</code>:Logical: are values allowed to contain CR (<code>0x0d</code> or <code style="white-space: pre;">&#8288;\r&#8288;</code>) and LF (<code>0x0a</code> or <code style="white-space: pre;">&#8288;\n&#8288;</code>)
characters? (default <code>FALSE</code>)
</p>
</li></ul>



<h3>See Also</h3>

<p><a href="#topic+FileFormat">FileFormat</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

</code></pre>

<hr>
<h2 id='list_compute_functions'>List available Arrow C++ compute functions</h2><span id='topic+list_compute_functions'></span>

<h3>Description</h3>

<p>This function lists the names of all available Arrow C++ library compute functions.
These can be called by passing to <code><a href="#topic+call_function">call_function()</a></code>, or they can be
called by name with an <code>arrow_</code> prefix inside a <code>dplyr</code> verb.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>list_compute_functions(pattern = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="list_compute_functions_+3A_pattern">pattern</code></td>
<td>
<p>Optional regular expression to filter the function list</p>
</td></tr>
<tr><td><code id="list_compute_functions_+3A_...">...</code></td>
<td>
<p>Additional parameters passed to <code>grep()</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>The resulting list describes the capabilities of your <code>arrow</code> build.
Some functions, such as string and regular expression functions,
require optional build-time C++ dependencies. If your <code>arrow</code> package
was not compiled with those features enabled, those functions will
not appear in this list.
</p>
<p>Some functions take options that need to be passed when calling them
(in a list called <code>options</code>). These options require custom handling
in C++; many functions already have that handling set up but not all do.
If you encounter one that needs special handling for options, please
report an issue.
</p>
<p>Note that this list does <em>not</em> enumerate all of the R bindings for these functions.
The package includes Arrow methods for many base R functions that can
be called directly on Arrow objects, as well as some tidyverse-flavored versions
available inside <code>dplyr</code> verbs.
</p>


<h3>Value</h3>

<p>A character vector of available Arrow C++ function names
</p>


<h3>Examples</h3>

<pre><code class='language-R'>available_funcs &lt;- list_compute_functions()
utf8_funcs &lt;- list_compute_functions(pattern = "^UTF8", ignore.case = TRUE)
</code></pre>

<hr>
<h2 id='list_flights'>See available resources on a Flight server</h2><span id='topic+list_flights'></span><span id='topic+flight_path_exists'></span>

<h3>Description</h3>

<p>See available resources on a Flight server
</p>


<h3>Usage</h3>

<pre><code class='language-R'>list_flights(client)

flight_path_exists(client, path)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="list_flights_+3A_client">client</code></td>
<td>
<p><code>pyarrow.flight.FlightClient</code>, as returned by <code><a href="#topic+flight_connect">flight_connect()</a></code></p>
</td></tr>
<tr><td><code id="list_flights_+3A_path">path</code></td>
<td>
<p>string identifier under which data is stored</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>list_flights()</code> returns a character vector of paths.
<code>flight_path_exists()</code> returns a logical value, the equivalent of <code>path %in% list_flights()</code>
</p>

<hr>
<h2 id='load_flight_server'>Load a Python Flight server</h2><span id='topic+load_flight_server'></span>

<h3>Description</h3>

<p>Load a Python Flight server
</p>


<h3>Usage</h3>

<pre><code class='language-R'>load_flight_server(name, path = system.file(package = "arrow"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="load_flight_server_+3A_name">name</code></td>
<td>
<p>string Python module name</p>
</td></tr>
<tr><td><code id="load_flight_server_+3A_path">path</code></td>
<td>
<p>file system path where the Python module is found. Default is
to look in the <code style="white-space: pre;">&#8288;inst/&#8288;</code> directory for included modules.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>
load_flight_server("demo_flight_server")

</code></pre>

<hr>
<h2 id='make_readable_file'>Handle a range of possible input sources</h2><span id='topic+make_readable_file'></span>

<h3>Description</h3>

<p>Handle a range of possible input sources
</p>


<h3>Usage</h3>

<pre><code class='language-R'>make_readable_file(file, mmap = TRUE, random_access = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="make_readable_file_+3A_file">file</code></td>
<td>
<p>A character file name, <code>raw</code> vector, or an Arrow input stream</p>
</td></tr>
<tr><td><code id="make_readable_file_+3A_mmap">mmap</code></td>
<td>
<p>Logical: whether to memory-map the file (default <code>TRUE</code>)</p>
</td></tr>
<tr><td><code id="make_readable_file_+3A_random_access">random_access</code></td>
<td>
<p>Logical: whether the result must be a RandomAccessFile</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An <code>InputStream</code> or a subclass of one.
</p>

<hr>
<h2 id='map_batches'>Apply a function to a stream of RecordBatches</h2><span id='topic+map_batches'></span>

<h3>Description</h3>

<p>As an alternative to calling <code>collect()</code> on a <code>Dataset</code> query, you can
use this function to access the stream of <code>RecordBatch</code>es in the <code>Dataset</code>.
This lets you do more complex operations in R that operate on chunks of data
without having to hold the entire Dataset in memory at once. You can include
<code>map_batches()</code> in a dplyr pipeline and do additional dplyr methods on the
stream of data in Arrow after it.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>map_batches(X, FUN, ..., .schema = NULL, .lazy = TRUE, .data.frame = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="map_batches_+3A_x">X</code></td>
<td>
<p>A <code>Dataset</code> or <code>arrow_dplyr_query</code> object, as returned by the
<code>dplyr</code> methods on <code>Dataset</code>.</p>
</td></tr>
<tr><td><code id="map_batches_+3A_fun">FUN</code></td>
<td>
<p>A function or <code>purrr</code>-style lambda expression to apply to each
batch. It must return a RecordBatch or something coercible to one via
&lsquo;as_record_batch()&rsquo;.</p>
</td></tr>
<tr><td><code id="map_batches_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <code>FUN</code></p>
</td></tr>
<tr><td><code id="map_batches_+3A_.schema">.schema</code></td>
<td>
<p>An optional <code><a href="#topic+schema">schema()</a></code>. If NULL, the schema will be inferred
from the first batch.</p>
</td></tr>
<tr><td><code id="map_batches_+3A_.lazy">.lazy</code></td>
<td>
<p>Use <code>TRUE</code> to evaluate <code>FUN</code> lazily as batches are read from
the result; use <code>FALSE</code> to evaluate <code>FUN</code> on all batches before returning
the reader.</p>
</td></tr>
<tr><td><code id="map_batches_+3A_.data.frame">.data.frame</code></td>
<td>
<p>Deprecated argument, ignored</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is experimental and not recommended for production use. It is also
single-threaded and runs in R not C++, so it won't be as fast as core
Arrow methods.
</p>


<h3>Value</h3>

<p>An <code>arrow_dplyr_query</code>.
</p>

<hr>
<h2 id='match_arrow'>Value matching for Arrow objects</h2><span id='topic+match_arrow'></span><span id='topic+is_in'></span>

<h3>Description</h3>

<p><code>base::match()</code> and <code style="white-space: pre;">&#8288;base::%in%&#8288;</code> are not generics, so we can't just define Arrow methods for
them. These functions expose the analogous functions in the Arrow C++ library.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>match_arrow(x, table, ...)

is_in(x, table, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="match_arrow_+3A_x">x</code></td>
<td>
<p><code>Scalar</code>, <code>Array</code> or <code>ChunkedArray</code></p>
</td></tr>
<tr><td><code id="match_arrow_+3A_table">table</code></td>
<td>
<p><code>Scalar</code>, Array<code style="white-space: pre;">&#8288;, &#8288;</code>ChunkedArray', or R vector lookup table.</p>
</td></tr>
<tr><td><code id="match_arrow_+3A_...">...</code></td>
<td>
<p>additional arguments, ignored</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>match_arrow()</code> returns an <code>int32</code>-type Arrow object of the same length
and type as <code>x</code> with the (0-based) indexes into <code>table</code>. <code>is_in()</code> returns a
<code>boolean</code>-type Arrow object of the same length and type as <code>x</code> with values indicating
per element of <code>x</code> it it is present in <code>table</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># note that the returned value is 0-indexed
cars_tbl &lt;- arrow_table(name = rownames(mtcars), mtcars)
match_arrow(Scalar$create("Mazda RX4 Wag"), cars_tbl$name)

is_in(Array$create("Mazda RX4 Wag"), cars_tbl$name)

# Although there are multiple matches, you are returned the index of the first
# match, as with the base R equivalent
match(4, mtcars$cyl) # 1-indexed
match_arrow(Scalar$create(4), cars_tbl$cyl) # 0-indexed

# If `x` contains multiple values, you are returned the indices of the first
# match for each value.
match(c(4, 6, 8), mtcars$cyl)
match_arrow(Array$create(c(4, 6, 8)), cars_tbl$cyl)

# Return type matches type of `x`
is_in(c(4, 6, 8), mtcars$cyl) # returns vector
is_in(Scalar$create(4), mtcars$cyl) # returns Scalar
is_in(Array$create(c(4, 6, 8)), cars_tbl$cyl) # returns Array
is_in(ChunkedArray$create(c(4, 6), 8), cars_tbl$cyl) # returns ChunkedArray
</code></pre>

<hr>
<h2 id='MemoryPool'>MemoryPool class</h2><span id='topic+MemoryPool'></span>

<h3>Description</h3>

<p>MemoryPool class
</p>


<h3>Methods</h3>


<ul>
<li> <p><code>backend_name</code>: one of &quot;jemalloc&quot;, &quot;mimalloc&quot;, or &quot;system&quot;. Alternative
memory allocators are optionally enabled at build time. Windows builds
generally have <code>mimalloc</code>, and most others have both <code>jemalloc</code> (used by
default) and <code>mimalloc</code>. To change memory allocators at runtime, set the
environment variable <code>ARROW_DEFAULT_MEMORY_POOL</code> to one of those strings
prior to loading the <code>arrow</code> library.
</p>
</li>
<li> <p><code>bytes_allocated</code>
</p>
</li>
<li> <p><code>max_memory</code>
</p>
</li></ul>


<hr>
<h2 id='Message'>Message class</h2><span id='topic+Message'></span>

<h3>Description</h3>

<p>Message class
</p>


<h3>Methods</h3>

<p>TODO
</p>

<hr>
<h2 id='MessageReader'>MessageReader class</h2><span id='topic+MessageReader'></span>

<h3>Description</h3>

<p>MessageReader class
</p>


<h3>Methods</h3>

<p>TODO
</p>

<hr>
<h2 id='mmap_create'>Create a new read/write memory mapped file of a given size</h2><span id='topic+mmap_create'></span>

<h3>Description</h3>

<p>Create a new read/write memory mapped file of a given size
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mmap_create(path, size)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mmap_create_+3A_path">path</code></td>
<td>
<p>file path</p>
</td></tr>
<tr><td><code id="mmap_create_+3A_size">size</code></td>
<td>
<p>size in bytes</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <a href="#topic+MemoryMappedFile">arrow::io::MemoryMappedFile</a>
</p>

<hr>
<h2 id='mmap_open'>Open a memory mapped file</h2><span id='topic+mmap_open'></span>

<h3>Description</h3>

<p>Open a memory mapped file
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mmap_open(path, mode = c("read", "write", "readwrite"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mmap_open_+3A_path">path</code></td>
<td>
<p>file path</p>
</td></tr>
<tr><td><code id="mmap_open_+3A_mode">mode</code></td>
<td>
<p>file mode (read/write/readwrite)</p>
</td></tr>
</table>

<hr>
<h2 id='new_extension_type'>Extension types</h2><span id='topic+new_extension_type'></span><span id='topic+new_extension_array'></span><span id='topic+register_extension_type'></span><span id='topic+reregister_extension_type'></span><span id='topic+unregister_extension_type'></span>

<h3>Description</h3>

<p>Extension arrays are wrappers around regular Arrow <a href="#topic+Array">Array</a> objects
that provide some customized behaviour and/or storage. A common use-case
for extension types is to define a customized conversion between an
an Arrow <a href="#topic+Array">Array</a> and an R object when the default conversion is slow
or loses metadata important to the interpretation of values in the array.
For most types, the built-in
<a href="#topic+vctrs_extension_type">vctrs extension type</a> is probably sufficient.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>new_extension_type(
  storage_type,
  extension_name,
  extension_metadata = raw(),
  type_class = ExtensionType
)

new_extension_array(storage_array, extension_type)

register_extension_type(extension_type)

reregister_extension_type(extension_type)

unregister_extension_type(extension_name)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="new_extension_type_+3A_storage_type">storage_type</code></td>
<td>
<p>The <a href="#topic+data-type">data type</a> of the underlying storage
array.</p>
</td></tr>
<tr><td><code id="new_extension_type_+3A_extension_name">extension_name</code></td>
<td>
<p>The extension name. This should be namespaced using
&quot;dot&quot; syntax (i.e., &quot;some_package.some_type&quot;). The namespace &quot;arrow&quot;
is reserved for extension types defined by the Apache Arrow libraries.</p>
</td></tr>
<tr><td><code id="new_extension_type_+3A_extension_metadata">extension_metadata</code></td>
<td>
<p>A <code><a href="base.html#topic+raw">raw()</a></code> or <code><a href="base.html#topic+character">character()</a></code> vector containing the
serialized version of the type. Character vectors must be length 1 and
are converted to UTF-8 before converting to <code><a href="base.html#topic+raw">raw()</a></code>.</p>
</td></tr>
<tr><td><code id="new_extension_type_+3A_type_class">type_class</code></td>
<td>
<p>An <a href="R6.html#topic+R6Class">R6::R6Class</a> whose <code style="white-space: pre;">&#8288;$new()&#8288;</code> class method will be
used to construct a new instance of the type.</p>
</td></tr>
<tr><td><code id="new_extension_type_+3A_storage_array">storage_array</code></td>
<td>
<p>An <a href="#topic+Array">Array</a> object of the underlying storage.</p>
</td></tr>
<tr><td><code id="new_extension_type_+3A_extension_type">extension_type</code></td>
<td>
<p>An <a href="#topic+ExtensionType">ExtensionType</a> instance.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>These functions create, register, and unregister <a href="#topic+ExtensionType">ExtensionType</a>
and <a href="#topic+ExtensionArray">ExtensionArray</a> objects. To use an extension type you will have to:
</p>

<ul>
<li><p> Define an <a href="R6.html#topic+R6Class">R6::R6Class</a> that inherits from <a href="#topic+ExtensionType">ExtensionType</a> and reimplement
one or more methods (e.g., <code>deserialize_instance()</code>).
</p>
</li>
<li><p> Make a type constructor function (e.g., <code>my_extension_type()</code>) that calls
<code><a href="#topic+new_extension_type">new_extension_type()</a></code> to create an R6 instance that can be used as a
<a href="#topic+data-type">data type</a> elsewhere in the package.
</p>
</li>
<li><p> Make an array constructor function (e.g., <code>my_extension_array()</code>) that
calls <code><a href="#topic+new_extension_array">new_extension_array()</a></code> to create an <a href="#topic+Array">Array</a> instance of your
extension type.
</p>
</li>
<li><p> Register a dummy instance of your extension type created using
you constructor function using <code><a href="#topic+register_extension_type">register_extension_type()</a></code>.
</p>
</li></ul>

<p>If defining an extension type in an R package, you will probably want to
use <code><a href="#topic+reregister_extension_type">reregister_extension_type()</a></code> in that package's <code><a href="base.html#topic+.onLoad">.onLoad()</a></code> hook
since your package will probably get reloaded in the same R session
during its development and <code><a href="#topic+register_extension_type">register_extension_type()</a></code> will error if
called twice for the same <code>extension_name</code>. For an example of an
extension type that uses most of these features, see
<code><a href="#topic+vctrs_extension_type">vctrs_extension_type()</a></code>.
</p>


<h3>Value</h3>


<ul>
<li> <p><code>new_extension_type()</code> returns an <a href="#topic+ExtensionType">ExtensionType</a> instance according
to the <code>type_class</code> specified.
</p>
</li>
<li> <p><code>new_extension_array()</code> returns an <a href="#topic+ExtensionArray">ExtensionArray</a> whose <code style="white-space: pre;">&#8288;$type&#8288;</code>
corresponds to <code>extension_type</code>.
</p>
</li>
<li> <p><code>register_extension_type()</code>, <code>unregister_extension_type()</code>
and <code>reregister_extension_type()</code> return <code>NULL</code>, invisibly.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'># Create the R6 type whose methods control how Array objects are
# converted to R objects, how equality between types is computed,
# and how types are printed.
QuantizedType &lt;- R6::R6Class(
  "QuantizedType",
  inherit = ExtensionType,
  public = list(
    # methods to access the custom metadata fields
    center = function() private$.center,
    scale = function() private$.scale,

    # called when an Array of this type is converted to an R vector
    as_vector = function(extension_array) {
      if (inherits(extension_array, "ExtensionArray")) {
        unquantized_arrow &lt;-
          (extension_array$storage()$cast(float64()) / private$.scale) +
          private$.center

        as.vector(unquantized_arrow)
      } else {
        super$as_vector(extension_array)
      }
    },

    # populate the custom metadata fields from the serialized metadata
    deserialize_instance = function() {
      vals &lt;- as.numeric(strsplit(self$extension_metadata_utf8(), ";")[[1]])
      private$.center &lt;- vals[1]
      private$.scale &lt;- vals[2]
    }
  ),
  private = list(
    .center = NULL,
    .scale = NULL
  )
)

# Create a helper type constructor that calls new_extension_type()
quantized &lt;- function(center = 0, scale = 1, storage_type = int32()) {
  new_extension_type(
    storage_type = storage_type,
    extension_name = "arrow.example.quantized",
    extension_metadata = paste(center, scale, sep = ";"),
    type_class = QuantizedType
  )
}

# Create a helper array constructor that calls new_extension_array()
quantized_array &lt;- function(x, center = 0, scale = 1,
                            storage_type = int32()) {
  type &lt;- quantized(center, scale, storage_type)
  new_extension_array(
    Array$create((x - center) * scale, type = storage_type),
    type
  )
}

# Register the extension type so that Arrow knows what to do when
# it encounters this extension type
reregister_extension_type(quantized())

# Create Array objects and use them!
(vals &lt;- runif(5, min = 19, max = 21))

(array &lt;- quantized_array(
  vals,
  center = 20,
  scale = 2^15 - 1,
  storage_type = int16()
)
)

array$type$center()
array$type$scale()

as.vector(array)
</code></pre>

<hr>
<h2 id='open_dataset'>Open a multi-file dataset</h2><span id='topic+open_dataset'></span>

<h3>Description</h3>

<p>Arrow Datasets allow you to query against data that has been split across
multiple files. This sharding of data may indicate partitioning, which
can accelerate queries that only touch some partitions (files). Call
<code>open_dataset()</code> to point to a directory of data files and return a
<code>Dataset</code>, then use <code>dplyr</code> methods to query it.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>open_dataset(
  sources,
  schema = NULL,
  partitioning = hive_partition(),
  hive_style = NA,
  unify_schemas = NULL,
  format = c("parquet", "arrow", "ipc", "feather", "csv", "tsv", "text", "json"),
  factory_options = list(),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="open_dataset_+3A_sources">sources</code></td>
<td>
<p>One of:
</p>

<ul>
<li><p> a string path or URI to a directory containing data files
</p>
</li>
<li><p> a <a href="#topic+FileSystem">FileSystem</a> that references a directory containing data files
(such as what is returned by <code><a href="#topic+s3_bucket">s3_bucket()</a></code>)
</p>
</li>
<li><p> a string path or URI to a single file
</p>
</li>
<li><p> a character vector of paths or URIs to individual data files
</p>
</li>
<li><p> a list of <code>Dataset</code> objects as created by this function
</p>
</li>
<li><p> a list of <code>DatasetFactory</code> objects as created by <code><a href="#topic+dataset_factory">dataset_factory()</a></code>.
</p>
</li></ul>

<p>When <code>sources</code> is a vector of file URIs, they must all use the same protocol
and point to files located in the same file system and having the same
format.</p>
</td></tr>
<tr><td><code id="open_dataset_+3A_schema">schema</code></td>
<td>
<p><a href="#topic+Schema">Schema</a> for the <code>Dataset</code>. If <code>NULL</code> (the default), the schema
will be inferred from the data sources.</p>
</td></tr>
<tr><td><code id="open_dataset_+3A_partitioning">partitioning</code></td>
<td>
<p>When <code>sources</code> is a directory path/URI, one of:
</p>

<ul>
<li><p> a <code>Schema</code>, in which case the file paths relative to <code>sources</code> will be
parsed, and path segments will be matched with the schema fields.
</p>
</li>
<li><p> a character vector that defines the field names corresponding to those
path segments (that is, you're providing the names that would correspond
to a <code>Schema</code> but the types will be autodetected)
</p>
</li>
<li><p> a <code>Partitioning</code> or <code>PartitioningFactory</code>, such as returned
by <code><a href="#topic+hive_partition">hive_partition()</a></code>
</p>
</li>
<li> <p><code>NULL</code> for no partitioning
</p>
</li></ul>

<p>The default is to autodetect Hive-style partitions unless
<code>hive_style = FALSE</code>. See the &quot;Partitioning&quot; section for details.
When <code>sources</code> is not a directory path/URI, <code>partitioning</code> is ignored.</p>
</td></tr>
<tr><td><code id="open_dataset_+3A_hive_style">hive_style</code></td>
<td>
<p>Logical: should <code>partitioning</code> be interpreted as
Hive-style? Default is <code>NA</code>, which means to inspect the file paths for
Hive-style partitioning and behave accordingly.</p>
</td></tr>
<tr><td><code id="open_dataset_+3A_unify_schemas">unify_schemas</code></td>
<td>
<p>logical: should all data fragments (files, <code>Dataset</code>s)
be scanned in order to create a unified schema from them? If <code>FALSE</code>, only
the first fragment will be inspected for its schema. Use this fast path
when you know and trust that all fragments have an identical schema.
The default is <code>FALSE</code> when creating a dataset from a directory path/URI or
vector of file paths/URIs (because there may be many files and scanning may
be slow) but <code>TRUE</code> when <code>sources</code> is a list of <code>Dataset</code>s (because there
should be few <code>Dataset</code>s in the list and their <code>Schema</code>s are already in
memory).</p>
</td></tr>
<tr><td><code id="open_dataset_+3A_format">format</code></td>
<td>
<p>A <a href="#topic+FileFormat">FileFormat</a> object, or a string identifier of the format of
the files in <code>x</code>. This argument is ignored when <code>sources</code> is a list of <code>Dataset</code> objects.
Currently supported values:
</p>

<ul>
<li><p> &quot;parquet&quot;
</p>
</li>
<li><p> &quot;ipc&quot;/&quot;arrow&quot;/&quot;feather&quot;, all aliases for each other; for Feather, note that
only version 2 files are supported
</p>
</li>
<li><p> &quot;csv&quot;/&quot;text&quot;, aliases for the same thing (because comma is the default
delimiter for text files
</p>
</li>
<li><p> &quot;tsv&quot;, equivalent to passing <code style="white-space: pre;">&#8288;format = "text", delimiter = "\t"&#8288;</code>
</p>
</li>
<li><p> &quot;json&quot;, for JSON format datasets Note: only newline-delimited JSON (aka ND-JSON) datasets
are currently supported
Default is &quot;parquet&quot;, unless a <code>delimiter</code> is also specified, in which case
it is assumed to be &quot;text&quot;.
</p>
</li></ul>
</td></tr>
<tr><td><code id="open_dataset_+3A_factory_options">factory_options</code></td>
<td>
<p>list of optional FileSystemFactoryOptions:
</p>

<ul>
<li> <p><code>partition_base_dir</code>: string path segment prefix to ignore when
discovering partition information with DirectoryPartitioning. Not
meaningful (ignored with a warning) for HivePartitioning, nor is it
valid when providing a vector of file paths.
</p>
</li>
<li> <p><code>exclude_invalid_files</code>: logical: should files that are not valid data
files be excluded? Default is <code>FALSE</code> because checking all files up
front incurs I/O and thus will be slower, especially on remote
filesystems. If false and there are invalid files, there will be an
error at scan time. This is the only FileSystemFactoryOption that is
valid for both when providing a directory path in which to discover
files and when providing a vector of file paths.
</p>
</li>
<li> <p><code>selector_ignore_prefixes</code>: character vector of file prefixes to ignore
when discovering files in a directory. If invalid files can be excluded
by a common filename prefix this way, you can avoid the I/O cost of
<code>exclude_invalid_files</code>. Not valid when providing a vector of file paths
(but if you're providing the file list, you can filter invalid files
yourself).
</p>
</li></ul>
</td></tr>
<tr><td><code id="open_dataset_+3A_...">...</code></td>
<td>
<p>additional arguments passed to <code>dataset_factory()</code> when <code>sources</code>
is a directory path/URI or vector of file paths/URIs, otherwise ignored.
These may include <code>format</code> to indicate the file format, or other
format-specific options (see <code><a href="#topic+read_csv_arrow">read_csv_arrow()</a></code>, <code><a href="#topic+read_parquet">read_parquet()</a></code> and <code><a href="#topic+read_feather">read_feather()</a></code> on how to specify these).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <a href="#topic+Dataset">Dataset</a> R6 object. Use <code>dplyr</code> methods on it to query the data,
or call <code><a href="#topic+Scanner">$NewScan()</a></code> to construct a query directly.
</p>


<h3>Partitioning</h3>

<p>Data is often split into multiple files and nested in subdirectories based on the value of one or more
columns in the data. It may be a column that is commonly referenced in
queries, or it may be time-based, for some examples. Data that is divided
this way is &quot;partitioned,&quot; and the values for those partitioning columns are
encoded into the file path segments.
These path segments are effectively virtual columns in the dataset, and
because their values are known prior to reading the files themselves, we can
greatly speed up filtered queries by skipping some files entirely.
</p>
<p>Arrow supports reading partition information from file paths in two forms:
</p>

<ul>
<li><p> &quot;Hive-style&quot;, deriving from the Apache Hive project and common to some
database systems. Partitions are encoded as &quot;key=value&quot; in path segments,
such as <code>"year=2019/month=1/file.parquet"</code>. While they may be awkward as
file names, they have the advantage of being self-describing.
</p>
</li>
<li><p> &quot;Directory&quot; partitioning, which is Hive without the key names, like
<code>"2019/01/file.parquet"</code>. In order to use these, we need know at least
what names to give the virtual columns that come from the path segments.
</p>
</li></ul>

<p>The default behavior in <code>open_dataset()</code> is to inspect the file paths
contained in the provided directory, and if they look like Hive-style, parse
them as Hive. If your dataset has Hive-style partitioning in the file paths,
you do not need to provide anything in the <code>partitioning</code> argument to
<code>open_dataset()</code> to use them. If you do provide a character vector of
partition column names, they will be ignored if they match what is detected,
and if they don't match, you'll get an error. (If you want to rename
partition columns, do that using <code>select()</code> or <code>rename()</code> after opening the
dataset.). If you provide a <code>Schema</code> and the names match what is detected,
it will use the types defined by the Schema. In the example file path above,
you could provide a Schema to specify that &quot;month&quot; should be <code>int8()</code>
instead of the <code>int32()</code> it will be parsed as by default.
</p>
<p>If your file paths do not appear to be Hive-style, or if you pass
<code>hive_style = FALSE</code>, the <code>partitioning</code> argument will be used to create
Directory partitioning. A character vector of names is required to create
partitions; you may instead provide a <code>Schema</code> to map those names to desired
column types, as described above. If neither are provided, no partitioning
information will be taken from the file paths.
</p>


<h3>See Also</h3>

<p><a href="https://arrow.apache.org/docs/r/articles/dataset.html">
datasets article</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Set up directory for examples
tf &lt;- tempfile()
dir.create(tf)
on.exit(unlink(tf))

write_dataset(mtcars, tf, partitioning = "cyl")

# You can specify a directory containing the files for your dataset and
# open_dataset will scan all files in your directory.
open_dataset(tf)

# You can also supply a vector of paths
open_dataset(c(file.path(tf, "cyl=4/part-0.parquet"), file.path(tf, "cyl=8/part-0.parquet")))

## You must specify the file format if using a format other than parquet.
tf2 &lt;- tempfile()
dir.create(tf2)
on.exit(unlink(tf2))
write_dataset(mtcars, tf2, format = "ipc")
# This line will results in errors when you try to work with the data
## Not run: 
open_dataset(tf2)

## End(Not run)
# This line will work
open_dataset(tf2, format = "ipc")

## You can specify file partitioning to include it as a field in your dataset
# Create a temporary directory and write example dataset
tf3 &lt;- tempfile()
dir.create(tf3)
on.exit(unlink(tf3))
write_dataset(airquality, tf3, partitioning = c("Month", "Day"), hive_style = FALSE)

# View files - you can see the partitioning means that files have been written
# to folders based on Month/Day values
tf3_files &lt;- list.files(tf3, recursive = TRUE)

# With no partitioning specified, dataset contains all files but doesn't include
# directory names as field names
open_dataset(tf3)

# Now that partitioning has been specified, your dataset contains columns for Month and Day
open_dataset(tf3, partitioning = c("Month", "Day"))

# If you want to specify the data types for your fields, you can pass in a Schema
open_dataset(tf3, partitioning = schema(Month = int8(), Day = int8()))

</code></pre>

<hr>
<h2 id='open_delim_dataset'>Open a multi-file dataset of CSV or other delimiter-separated format</h2><span id='topic+open_delim_dataset'></span><span id='topic+open_csv_dataset'></span><span id='topic+open_tsv_dataset'></span>

<h3>Description</h3>

<p>A wrapper around <a href="#topic+open_dataset">open_dataset</a> which explicitly includes parameters mirroring <code><a href="#topic+read_csv_arrow">read_csv_arrow()</a></code>,
<code><a href="#topic+read_delim_arrow">read_delim_arrow()</a></code>, and <code><a href="#topic+read_tsv_arrow">read_tsv_arrow()</a></code> to allow for easy switching between functions
for opening single files and functions for opening datasets.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>open_delim_dataset(
  sources,
  schema = NULL,
  partitioning = hive_partition(),
  hive_style = NA,
  unify_schemas = NULL,
  factory_options = list(),
  delim = ",",
  quote = "\"",
  escape_double = TRUE,
  escape_backslash = FALSE,
  col_names = TRUE,
  col_types = NULL,
  na = c("", "NA"),
  skip_empty_rows = TRUE,
  skip = 0L,
  convert_options = NULL,
  read_options = NULL,
  timestamp_parsers = NULL,
  quoted_na = TRUE,
  parse_options = NULL
)

open_csv_dataset(
  sources,
  schema = NULL,
  partitioning = hive_partition(),
  hive_style = NA,
  unify_schemas = NULL,
  factory_options = list(),
  quote = "\"",
  escape_double = TRUE,
  escape_backslash = FALSE,
  col_names = TRUE,
  col_types = NULL,
  na = c("", "NA"),
  skip_empty_rows = TRUE,
  skip = 0L,
  convert_options = NULL,
  read_options = NULL,
  timestamp_parsers = NULL,
  quoted_na = TRUE,
  parse_options = NULL
)

open_tsv_dataset(
  sources,
  schema = NULL,
  partitioning = hive_partition(),
  hive_style = NA,
  unify_schemas = NULL,
  factory_options = list(),
  quote = "\"",
  escape_double = TRUE,
  escape_backslash = FALSE,
  col_names = TRUE,
  col_types = NULL,
  na = c("", "NA"),
  skip_empty_rows = TRUE,
  skip = 0L,
  convert_options = NULL,
  read_options = NULL,
  timestamp_parsers = NULL,
  quoted_na = TRUE,
  parse_options = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="open_delim_dataset_+3A_sources">sources</code></td>
<td>
<p>One of:
</p>

<ul>
<li><p> a string path or URI to a directory containing data files
</p>
</li>
<li><p> a <a href="#topic+FileSystem">FileSystem</a> that references a directory containing data files
(such as what is returned by <code><a href="#topic+s3_bucket">s3_bucket()</a></code>)
</p>
</li>
<li><p> a string path or URI to a single file
</p>
</li>
<li><p> a character vector of paths or URIs to individual data files
</p>
</li>
<li><p> a list of <code>Dataset</code> objects as created by this function
</p>
</li>
<li><p> a list of <code>DatasetFactory</code> objects as created by <code><a href="#topic+dataset_factory">dataset_factory()</a></code>.
</p>
</li></ul>

<p>When <code>sources</code> is a vector of file URIs, they must all use the same protocol
and point to files located in the same file system and having the same
format.</p>
</td></tr>
<tr><td><code id="open_delim_dataset_+3A_schema">schema</code></td>
<td>
<p><a href="#topic+Schema">Schema</a> for the <code>Dataset</code>. If <code>NULL</code> (the default), the schema
will be inferred from the data sources.</p>
</td></tr>
<tr><td><code id="open_delim_dataset_+3A_partitioning">partitioning</code></td>
<td>
<p>When <code>sources</code> is a directory path/URI, one of:
</p>

<ul>
<li><p> a <code>Schema</code>, in which case the file paths relative to <code>sources</code> will be
parsed, and path segments will be matched with the schema fields.
</p>
</li>
<li><p> a character vector that defines the field names corresponding to those
path segments (that is, you're providing the names that would correspond
to a <code>Schema</code> but the types will be autodetected)
</p>
</li>
<li><p> a <code>Partitioning</code> or <code>PartitioningFactory</code>, such as returned
by <code><a href="#topic+hive_partition">hive_partition()</a></code>
</p>
</li>
<li> <p><code>NULL</code> for no partitioning
</p>
</li></ul>

<p>The default is to autodetect Hive-style partitions unless
<code>hive_style = FALSE</code>. See the &quot;Partitioning&quot; section for details.
When <code>sources</code> is not a directory path/URI, <code>partitioning</code> is ignored.</p>
</td></tr>
<tr><td><code id="open_delim_dataset_+3A_hive_style">hive_style</code></td>
<td>
<p>Logical: should <code>partitioning</code> be interpreted as
Hive-style? Default is <code>NA</code>, which means to inspect the file paths for
Hive-style partitioning and behave accordingly.</p>
</td></tr>
<tr><td><code id="open_delim_dataset_+3A_unify_schemas">unify_schemas</code></td>
<td>
<p>logical: should all data fragments (files, <code>Dataset</code>s)
be scanned in order to create a unified schema from them? If <code>FALSE</code>, only
the first fragment will be inspected for its schema. Use this fast path
when you know and trust that all fragments have an identical schema.
The default is <code>FALSE</code> when creating a dataset from a directory path/URI or
vector of file paths/URIs (because there may be many files and scanning may
be slow) but <code>TRUE</code> when <code>sources</code> is a list of <code>Dataset</code>s (because there
should be few <code>Dataset</code>s in the list and their <code>Schema</code>s are already in
memory).</p>
</td></tr>
<tr><td><code id="open_delim_dataset_+3A_factory_options">factory_options</code></td>
<td>
<p>list of optional FileSystemFactoryOptions:
</p>

<ul>
<li> <p><code>partition_base_dir</code>: string path segment prefix to ignore when
discovering partition information with DirectoryPartitioning. Not
meaningful (ignored with a warning) for HivePartitioning, nor is it
valid when providing a vector of file paths.
</p>
</li>
<li> <p><code>exclude_invalid_files</code>: logical: should files that are not valid data
files be excluded? Default is <code>FALSE</code> because checking all files up
front incurs I/O and thus will be slower, especially on remote
filesystems. If false and there are invalid files, there will be an
error at scan time. This is the only FileSystemFactoryOption that is
valid for both when providing a directory path in which to discover
files and when providing a vector of file paths.
</p>
</li>
<li> <p><code>selector_ignore_prefixes</code>: character vector of file prefixes to ignore
when discovering files in a directory. If invalid files can be excluded
by a common filename prefix this way, you can avoid the I/O cost of
<code>exclude_invalid_files</code>. Not valid when providing a vector of file paths
(but if you're providing the file list, you can filter invalid files
yourself).
</p>
</li></ul>
</td></tr>
<tr><td><code id="open_delim_dataset_+3A_delim">delim</code></td>
<td>
<p>Single character used to separate fields within a record.</p>
</td></tr>
<tr><td><code id="open_delim_dataset_+3A_quote">quote</code></td>
<td>
<p>Single character used to quote strings.</p>
</td></tr>
<tr><td><code id="open_delim_dataset_+3A_escape_double">escape_double</code></td>
<td>
<p>Does the file escape quotes by doubling them?
i.e. If this option is <code>TRUE</code>, the value <code style="white-space: pre;">&#8288;""""&#8288;</code> represents
a single quote, <code style="white-space: pre;">&#8288;\"&#8288;</code>.</p>
</td></tr>
<tr><td><code id="open_delim_dataset_+3A_escape_backslash">escape_backslash</code></td>
<td>
<p>Does the file use backslashes to escape special
characters? This is more general than <code>escape_double</code> as backslashes
can be used to escape the delimiter character, the quote character, or
to add special characters like <code style="white-space: pre;">&#8288;\\n&#8288;</code>.</p>
</td></tr>
<tr><td><code id="open_delim_dataset_+3A_col_names">col_names</code></td>
<td>
<p>If <code>TRUE</code>, the first row of the input will be used as the
column names and will not be included in the data frame. If <code>FALSE</code>, column
names will be generated by Arrow, starting with &quot;f0&quot;, &quot;f1&quot;, ..., &quot;fN&quot;.
Alternatively, you can specify a character vector of column names.</p>
</td></tr>
<tr><td><code id="open_delim_dataset_+3A_col_types">col_types</code></td>
<td>
<p>A compact string representation of the column types,
an Arrow <a href="#topic+Schema">Schema</a>, or <code>NULL</code> (the default) to infer types from the data.</p>
</td></tr>
<tr><td><code id="open_delim_dataset_+3A_na">na</code></td>
<td>
<p>A character vector of strings to interpret as missing values.</p>
</td></tr>
<tr><td><code id="open_delim_dataset_+3A_skip_empty_rows">skip_empty_rows</code></td>
<td>
<p>Should blank rows be ignored altogether? If
<code>TRUE</code>, blank rows will not be represented at all. If <code>FALSE</code>, they will be
filled with missings.</p>
</td></tr>
<tr><td><code id="open_delim_dataset_+3A_skip">skip</code></td>
<td>
<p>Number of lines to skip before reading data.</p>
</td></tr>
<tr><td><code id="open_delim_dataset_+3A_convert_options">convert_options</code></td>
<td>
<p>see <a href="#topic+csv_convert_options">CSV conversion options</a></p>
</td></tr>
<tr><td><code id="open_delim_dataset_+3A_read_options">read_options</code></td>
<td>
<p>see <a href="#topic+csv_read_options">CSV reading options</a></p>
</td></tr>
<tr><td><code id="open_delim_dataset_+3A_timestamp_parsers">timestamp_parsers</code></td>
<td>
<p>User-defined timestamp parsers. If more than one
parser is specified, the CSV conversion logic will try parsing values
starting from the beginning of this vector. Possible values are:
</p>

<ul>
<li> <p><code>NULL</code>: the default, which uses the ISO-8601 parser
</p>
</li>
<li><p> a character vector of <a href="base.html#topic+strptime">strptime</a> parse strings
</p>
</li>
<li><p> a list of <a href="#topic+TimestampParser">TimestampParser</a> objects
</p>
</li></ul>
</td></tr>
<tr><td><code id="open_delim_dataset_+3A_quoted_na">quoted_na</code></td>
<td>
<p>Should missing values inside quotes be treated as missing
values (the default) or strings. (Note that this is different from the
the Arrow C++ default for the corresponding convert option,
<code>strings_can_be_null</code>.)</p>
</td></tr>
<tr><td><code id="open_delim_dataset_+3A_parse_options">parse_options</code></td>
<td>
<p>see <a href="#topic+csv_parse_options">CSV parsing options</a>.
If given, this overrides any
parsing options provided in other arguments (e.g. <code>delim</code>, <code>quote</code>, etc.).</p>
</td></tr>
</table>


<h3>Options currently supported by <code><a href="#topic+read_delim_arrow">read_delim_arrow()</a></code> which are not supported here</h3>


<ul>
<li> <p><code>file</code> (instead, please specify files in <code>sources</code>)
</p>
</li>
<li> <p><code>col_select</code> (instead, subset columns after dataset creation)
</p>
</li>
<li> <p><code>as_data_frame</code> (instead, convert to data frame after dataset creation)
</p>
</li>
<li> <p><code>parse_options</code>
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+open_dataset">open_dataset()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Set up directory for examples
tf &lt;- tempfile()
dir.create(tf)
df &lt;- data.frame(x = c("1", "2", "NULL"))

file_path &lt;- file.path(tf, "file1.txt")
write.table(df, file_path, sep = ",", row.names = FALSE)

read_csv_arrow(file_path, na = c("", "NA", "NULL"), col_names = "y", skip = 1)
open_csv_dataset(file_path, na = c("", "NA", "NULL"), col_names = "y", skip = 1)

unlink(tf)

</code></pre>

<hr>
<h2 id='OutputStream'>OutputStream classes</h2><span id='topic+OutputStream'></span><span id='topic+FileOutputStream'></span><span id='topic+BufferOutputStream'></span>

<h3>Description</h3>

<p><code>FileOutputStream</code> is for writing to a file;
<code>BufferOutputStream</code> writes to a buffer;
You can create one and pass it to any of the table writers, for example.
</p>


<h3>Factory</h3>

<p>The <code style="white-space: pre;">&#8288;$create()&#8288;</code> factory methods instantiate the <code>OutputStream</code> object and
take the following arguments, depending on the subclass:
</p>

<ul>
<li> <p><code>path</code> For <code>FileOutputStream</code>, a character file name
</p>
</li>
<li> <p><code>initial_capacity</code> For <code>BufferOutputStream</code>, the size in bytes of the
buffer.
</p>
</li></ul>



<h3>Methods</h3>


<ul>
<li> <p><code style="white-space: pre;">&#8288;$tell()&#8288;</code>: return the position in the stream
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$close()&#8288;</code>: close the stream
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$write(x)&#8288;</code>: send <code>x</code> to the stream
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$capacity()&#8288;</code>: for <code>BufferOutputStream</code>
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$finish()&#8288;</code>: for <code>BufferOutputStream</code>
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$GetExtentBytesWritten()&#8288;</code>: for <code>MockOutputStream</code>, report how many bytes
were sent.
</p>
</li></ul>


<hr>
<h2 id='ParquetArrowReaderProperties'>ParquetArrowReaderProperties class</h2><span id='topic+ParquetArrowReaderProperties'></span>

<h3>Description</h3>

<p>This class holds settings to control how a Parquet file is read
by <a href="#topic+ParquetFileReader">ParquetFileReader</a>.
</p>


<h3>Factory</h3>

<p>The <code>ParquetArrowReaderProperties$create()</code> factory method instantiates the object
and takes the following arguments:
</p>

<ul>
<li> <p><code>use_threads</code> Logical: whether to use multithreading (default <code>TRUE</code>)
</p>
</li></ul>



<h3>Methods</h3>


<ul>
<li> <p><code style="white-space: pre;">&#8288;$read_dictionary(column_index)&#8288;</code>
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$set_read_dictionary(column_index, read_dict)&#8288;</code>
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$use_threads(use_threads)&#8288;</code>
</p>
</li></ul>


<hr>
<h2 id='ParquetFileReader'>ParquetFileReader class</h2><span id='topic+ParquetFileReader'></span>

<h3>Description</h3>

<p>This class enables you to interact with Parquet files.
</p>


<h3>Factory</h3>

<p>The <code>ParquetFileReader$create()</code> factory method instantiates the object and
takes the following arguments:
</p>

<ul>
<li> <p><code>file</code> A character file name, raw vector, or Arrow file connection object
(e.g. <code>RandomAccessFile</code>).
</p>
</li>
<li> <p><code>props</code> Optional <a href="#topic+ParquetArrowReaderProperties">ParquetArrowReaderProperties</a>
</p>
</li>
<li> <p><code>mmap</code> Logical: whether to memory-map the file (default <code>TRUE</code>)
</p>
</li>
<li> <p><code>reader_props</code> Optional <a href="#topic+ParquetReaderProperties">ParquetReaderProperties</a>
</p>
</li>
<li> <p><code>...</code> Additional arguments, currently ignored
</p>
</li></ul>



<h3>Methods</h3>


<ul>
<li> <p><code style="white-space: pre;">&#8288;$ReadTable(column_indices)&#8288;</code>: get an <code>arrow::Table</code> from the file. The optional
<code style="white-space: pre;">&#8288;column_indices=&#8288;</code> argument is a 0-based integer vector indicating which columns to retain.
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$ReadRowGroup(i, column_indices)&#8288;</code>: get an <code>arrow::Table</code> by reading the <code>i</code>th row group (0-based).
The optional <code style="white-space: pre;">&#8288;column_indices=&#8288;</code> argument is a 0-based integer vector indicating which columns to retain.
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$ReadRowGroups(row_groups, column_indices)&#8288;</code>: get an <code>arrow::Table</code> by reading several row
groups (0-based integers).
The optional <code style="white-space: pre;">&#8288;column_indices=&#8288;</code> argument is a 0-based integer vector indicating which columns to retain.
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$GetSchema()&#8288;</code>: get the <code>arrow::Schema</code> of the data in the file
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$ReadColumn(i)&#8288;</code>: read the <code>i</code>th column (0-based) as a <a href="#topic+ChunkedArray">ChunkedArray</a>.
</p>
</li></ul>



<h3>Active bindings</h3>


<ul>
<li> <p><code style="white-space: pre;">&#8288;$num_rows&#8288;</code>: number of rows.
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$num_columns&#8288;</code>: number of columns.
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$num_row_groups&#8288;</code>: number of row groups.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>
f &lt;- system.file("v0.7.1.parquet", package = "arrow")
pq &lt;- ParquetFileReader$create(f)
pq$GetSchema()
if (codec_is_available("snappy")) {
  # This file has compressed data columns
  tab &lt;- pq$ReadTable()
  tab$schema
}

</code></pre>

<hr>
<h2 id='ParquetFileWriter'>ParquetFileWriter class</h2><span id='topic+ParquetFileWriter'></span>

<h3>Description</h3>

<p>This class enables you to interact with Parquet files.
</p>


<h3>Factory</h3>

<p>The <code>ParquetFileWriter$create()</code> factory method instantiates the object and
takes the following arguments:
</p>

<ul>
<li> <p><code>schema</code> A <a href="#topic+Schema">Schema</a>
</p>
</li>
<li> <p><code>sink</code> An <a href="#topic+OutputStream">arrow::io::OutputStream</a>
</p>
</li>
<li> <p><code>properties</code> An instance of <a href="#topic+ParquetWriterProperties">ParquetWriterProperties</a>
</p>
</li>
<li> <p><code>arrow_properties</code> An instance of <code>ParquetArrowWriterProperties</code>
</p>
</li></ul>



<h3>Methods</h3>


<ul>
<li> <p><code>WriteTable</code> Write a <a href="#topic+Table">Table</a> to <code>sink</code>
</p>
</li>
<li> <p><code>Close</code> Close the writer. Note: does not close the <code>sink</code>.
<a href="#topic+OutputStream">arrow::io::OutputStream</a> has its own <code>close()</code> method.
</p>
</li></ul>


<hr>
<h2 id='ParquetReaderProperties'>ParquetReaderProperties class</h2><span id='topic+ParquetReaderProperties'></span>

<h3>Description</h3>

<p>This class holds settings to control how a Parquet file is read
by <a href="#topic+ParquetFileReader">ParquetFileReader</a>.
</p>


<h3>Factory</h3>

<p>The <code>ParquetReaderProperties$create()</code> factory method instantiates the object
and takes no arguments.
</p>


<h3>Methods</h3>


<ul>
<li> <p><code style="white-space: pre;">&#8288;$thrift_string_size_limit()&#8288;</code>
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$set_thrift_string_size_limit()&#8288;</code>
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$thrift_container_size_limit()&#8288;</code>
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$set_thrift_container_size_limit()&#8288;</code>
</p>
</li></ul>


<hr>
<h2 id='ParquetWriterProperties'>ParquetWriterProperties class</h2><span id='topic+ParquetWriterProperties'></span>

<h3>Description</h3>

<p>This class holds settings to control how a Parquet file is read
by <a href="#topic+ParquetFileWriter">ParquetFileWriter</a>.
</p>


<h3>Details</h3>

<p>The parameters <code>compression</code>, <code>compression_level</code>, <code>use_dictionary</code>
and write_statistics' support various patterns:
</p>

<ul>
<li><p> The default <code>NULL</code> leaves the parameter unspecified, and the C++ library
uses an appropriate default for each column (defaults listed above)
</p>
</li>
<li><p> A single, unnamed, value (e.g. a single string for <code>compression</code>) applies to all columns
</p>
</li>
<li><p> An unnamed vector, of the same size as the number of columns, to specify a
value for each column, in positional order
</p>
</li>
<li><p> A named vector, to specify the value for the named columns, the default
value for the setting is used when not supplied
</p>
</li></ul>

<p>Unlike the high-level <a href="#topic+write_parquet">write_parquet</a>, <code>ParquetWriterProperties</code> arguments
use the C++ defaults. Currently this means &quot;uncompressed&quot; rather than
&quot;snappy&quot; for the <code>compression</code> argument.
</p>


<h3>Factory</h3>

<p>The <code>ParquetWriterProperties$create()</code> factory method instantiates the object
and takes the following arguments:
</p>

<ul>
<li> <p><code>table</code>: table to write (required)
</p>
</li>
<li> <p><code>version</code>: Parquet version, &quot;1.0&quot; or &quot;2.0&quot;. Default &quot;1.0&quot;
</p>
</li>
<li> <p><code>compression</code>: Compression type, algorithm <code>"uncompressed"</code>
</p>
</li>
<li> <p><code>compression_level</code>: Compression level; meaning depends on compression algorithm
</p>
</li>
<li> <p><code>use_dictionary</code>: Specify if we should use dictionary encoding. Default <code>TRUE</code>
</p>
</li>
<li> <p><code>write_statistics</code>: Specify if we should write statistics. Default <code>TRUE</code>
</p>
</li>
<li> <p><code>data_page_size</code>: Set a target threshold for the approximate encoded
size of data pages within a column chunk (in bytes). Default 1 MiB.
</p>
</li></ul>



<h3>See Also</h3>

<p><a href="#topic+write_parquet">write_parquet</a>
</p>
<p><a href="#topic+Schema">Schema</a> for information about schemas and metadata handling.
</p>

<hr>
<h2 id='Partitioning'>Define Partitioning for a Dataset</h2><span id='topic+Partitioning'></span><span id='topic+DirectoryPartitioning'></span><span id='topic+HivePartitioning'></span><span id='topic+DirectoryPartitioningFactory'></span><span id='topic+HivePartitioningFactory'></span>

<h3>Description</h3>

<p>Pass a <code>Partitioning</code> object to a <a href="#topic+FileSystemDatasetFactory">FileSystemDatasetFactory</a>'s <code style="white-space: pre;">&#8288;$create()&#8288;</code>
method to indicate how the file's paths should be interpreted to define
partitioning.
</p>
<p><code>DirectoryPartitioning</code> describes how to interpret raw path segments, in
order. For example, <code>schema(year = int16(), month = int8())</code> would define
partitions for file paths like &quot;2019/01/file.parquet&quot;,
&quot;2019/02/file.parquet&quot;, etc. In this scheme <code>NULL</code> values will be skipped. In
the previous example: when writing a dataset if the month was <code>NA</code> (or
<code>NULL</code>), the files would be placed in &quot;2019/file.parquet&quot;. When reading, the
rows in &quot;2019/file.parquet&quot; would return an <code>NA</code> for the month column. An
error will be raised if an outer directory is <code>NULL</code> and an inner directory
is not.
</p>
<p><code>HivePartitioning</code> is for Hive-style partitioning, which embeds field
names and values in path segments, such as
&quot;/year=2019/month=2/data.parquet&quot;. Because fields are named in the path
segments, order does not matter. This partitioning scheme allows <code>NULL</code>
values. They will be replaced by a configurable <code>null_fallback</code> which
defaults to the string <code>"__HIVE_DEFAULT_PARTITION__"</code> when writing. When
reading, the <code>null_fallback</code> string will be replaced with <code>NA</code>s as
appropriate.
</p>
<p><code>PartitioningFactory</code> subclasses instruct the <code>DatasetFactory</code> to detect
partition features from the file paths.
</p>


<h3>Factory</h3>

<p>Both <code>DirectoryPartitioning$create()</code> and <code>HivePartitioning$create()</code>
methods take a <a href="#topic+Schema">Schema</a> as a single input argument. The helper
function <code><a href="#topic+hive_partition">hive_partition(...)</a></code> is shorthand for
<code>HivePartitioning$create(schema(...))</code>.
</p>
<p>With <code>DirectoryPartitioningFactory$create()</code>, you can provide just the
names of the path segments (in our example, <code>c("year", "month")</code>), and
the <code>DatasetFactory</code> will infer the data types for those partition variables.
<code>HivePartitioningFactory$create()</code> takes no arguments: both variable names
and their types can be inferred from the file paths. <code>hive_partition()</code> with
no arguments returns a <code>HivePartitioningFactory</code>.
</p>

<hr>
<h2 id='read_delim_arrow'>Read a CSV or other delimited file with Arrow</h2><span id='topic+read_delim_arrow'></span><span id='topic+read_csv_arrow'></span><span id='topic+read_csv2_arrow'></span><span id='topic+read_tsv_arrow'></span>

<h3>Description</h3>

<p>These functions uses the Arrow C++ CSV reader to read into a <code>tibble</code>.
Arrow C++ options have been mapped to argument names that follow those of
<code>readr::read_delim()</code>, and <code>col_select</code> was inspired by <code>vroom::vroom()</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>read_delim_arrow(
  file,
  delim = ",",
  quote = "\"",
  escape_double = TRUE,
  escape_backslash = FALSE,
  schema = NULL,
  col_names = TRUE,
  col_types = NULL,
  col_select = NULL,
  na = c("", "NA"),
  quoted_na = TRUE,
  skip_empty_rows = TRUE,
  skip = 0L,
  parse_options = NULL,
  convert_options = NULL,
  read_options = NULL,
  as_data_frame = TRUE,
  timestamp_parsers = NULL,
  decimal_point = "."
)

read_csv_arrow(
  file,
  quote = "\"",
  escape_double = TRUE,
  escape_backslash = FALSE,
  schema = NULL,
  col_names = TRUE,
  col_types = NULL,
  col_select = NULL,
  na = c("", "NA"),
  quoted_na = TRUE,
  skip_empty_rows = TRUE,
  skip = 0L,
  parse_options = NULL,
  convert_options = NULL,
  read_options = NULL,
  as_data_frame = TRUE,
  timestamp_parsers = NULL
)

read_csv2_arrow(
  file,
  quote = "\"",
  escape_double = TRUE,
  escape_backslash = FALSE,
  schema = NULL,
  col_names = TRUE,
  col_types = NULL,
  col_select = NULL,
  na = c("", "NA"),
  quoted_na = TRUE,
  skip_empty_rows = TRUE,
  skip = 0L,
  parse_options = NULL,
  convert_options = NULL,
  read_options = NULL,
  as_data_frame = TRUE,
  timestamp_parsers = NULL
)

read_tsv_arrow(
  file,
  quote = "\"",
  escape_double = TRUE,
  escape_backslash = FALSE,
  schema = NULL,
  col_names = TRUE,
  col_types = NULL,
  col_select = NULL,
  na = c("", "NA"),
  quoted_na = TRUE,
  skip_empty_rows = TRUE,
  skip = 0L,
  parse_options = NULL,
  convert_options = NULL,
  read_options = NULL,
  as_data_frame = TRUE,
  timestamp_parsers = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="read_delim_arrow_+3A_file">file</code></td>
<td>
<p>A character file name or URI, literal data (either a single string or a <a href="base.html#topic+raw">raw</a> vector),
an Arrow input stream, or a <code>FileSystem</code> with path (<code>SubTreeFileSystem</code>).
</p>
<p>If a file name, a memory-mapped Arrow <a href="#topic+InputStream">InputStream</a> will be opened and
closed when finished; compression will be detected from the file extension
and handled automatically. If an input stream is provided, it will be left
open.
</p>
<p>To be recognised as literal data, the input must be wrapped with <code>I()</code>.</p>
</td></tr>
<tr><td><code id="read_delim_arrow_+3A_delim">delim</code></td>
<td>
<p>Single character used to separate fields within a record.</p>
</td></tr>
<tr><td><code id="read_delim_arrow_+3A_quote">quote</code></td>
<td>
<p>Single character used to quote strings.</p>
</td></tr>
<tr><td><code id="read_delim_arrow_+3A_escape_double">escape_double</code></td>
<td>
<p>Does the file escape quotes by doubling them?
i.e. If this option is <code>TRUE</code>, the value <code style="white-space: pre;">&#8288;""""&#8288;</code> represents
a single quote, <code style="white-space: pre;">&#8288;\"&#8288;</code>.</p>
</td></tr>
<tr><td><code id="read_delim_arrow_+3A_escape_backslash">escape_backslash</code></td>
<td>
<p>Does the file use backslashes to escape special
characters? This is more general than <code>escape_double</code> as backslashes
can be used to escape the delimiter character, the quote character, or
to add special characters like <code style="white-space: pre;">&#8288;\\n&#8288;</code>.</p>
</td></tr>
<tr><td><code id="read_delim_arrow_+3A_schema">schema</code></td>
<td>
<p><a href="#topic+Schema">Schema</a> that describes the table. If provided, it will be
used to satisfy both <code>col_names</code> and <code>col_types</code>.</p>
</td></tr>
<tr><td><code id="read_delim_arrow_+3A_col_names">col_names</code></td>
<td>
<p>If <code>TRUE</code>, the first row of the input will be used as the
column names and will not be included in the data frame. If <code>FALSE</code>, column
names will be generated by Arrow, starting with &quot;f0&quot;, &quot;f1&quot;, ..., &quot;fN&quot;.
Alternatively, you can specify a character vector of column names.</p>
</td></tr>
<tr><td><code id="read_delim_arrow_+3A_col_types">col_types</code></td>
<td>
<p>A compact string representation of the column types,
an Arrow <a href="#topic+Schema">Schema</a>, or <code>NULL</code> (the default) to infer types from the data.</p>
</td></tr>
<tr><td><code id="read_delim_arrow_+3A_col_select">col_select</code></td>
<td>
<p>A character vector of column names to keep, as in the
&quot;select&quot; argument to <code>data.table::fread()</code>, or a
<a href="tidyselect.html#topic+eval_select">tidy selection specification</a>
of columns, as used in <code>dplyr::select()</code>.</p>
</td></tr>
<tr><td><code id="read_delim_arrow_+3A_na">na</code></td>
<td>
<p>A character vector of strings to interpret as missing values.</p>
</td></tr>
<tr><td><code id="read_delim_arrow_+3A_quoted_na">quoted_na</code></td>
<td>
<p>Should missing values inside quotes be treated as missing
values (the default) or strings. (Note that this is different from the
the Arrow C++ default for the corresponding convert option,
<code>strings_can_be_null</code>.)</p>
</td></tr>
<tr><td><code id="read_delim_arrow_+3A_skip_empty_rows">skip_empty_rows</code></td>
<td>
<p>Should blank rows be ignored altogether? If
<code>TRUE</code>, blank rows will not be represented at all. If <code>FALSE</code>, they will be
filled with missings.</p>
</td></tr>
<tr><td><code id="read_delim_arrow_+3A_skip">skip</code></td>
<td>
<p>Number of lines to skip before reading data.</p>
</td></tr>
<tr><td><code id="read_delim_arrow_+3A_parse_options">parse_options</code></td>
<td>
<p>see <a href="#topic+csv_parse_options">CSV parsing options</a>.
If given, this overrides any
parsing options provided in other arguments (e.g. <code>delim</code>, <code>quote</code>, etc.).</p>
</td></tr>
<tr><td><code id="read_delim_arrow_+3A_convert_options">convert_options</code></td>
<td>
<p>see <a href="#topic+csv_convert_options">CSV conversion options</a></p>
</td></tr>
<tr><td><code id="read_delim_arrow_+3A_read_options">read_options</code></td>
<td>
<p>see <a href="#topic+csv_read_options">CSV reading options</a></p>
</td></tr>
<tr><td><code id="read_delim_arrow_+3A_as_data_frame">as_data_frame</code></td>
<td>
<p>Should the function return a <code>tibble</code> (default) or
an Arrow <a href="#topic+Table">Table</a>?</p>
</td></tr>
<tr><td><code id="read_delim_arrow_+3A_timestamp_parsers">timestamp_parsers</code></td>
<td>
<p>User-defined timestamp parsers. If more than one
parser is specified, the CSV conversion logic will try parsing values
starting from the beginning of this vector. Possible values are:
</p>

<ul>
<li> <p><code>NULL</code>: the default, which uses the ISO-8601 parser
</p>
</li>
<li><p> a character vector of <a href="base.html#topic+strptime">strptime</a> parse strings
</p>
</li>
<li><p> a list of <a href="#topic+TimestampParser">TimestampParser</a> objects
</p>
</li></ul>
</td></tr>
<tr><td><code id="read_delim_arrow_+3A_decimal_point">decimal_point</code></td>
<td>
<p>Character to use for decimal point in floating point numbers.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>read_csv_arrow()</code> and <code>read_tsv_arrow()</code> are wrappers around
<code>read_delim_arrow()</code> that specify a delimiter. <code>read_csv2_arrow()</code> uses <code style="white-space: pre;">&#8288;;&#8288;</code> for
the delimiter and <code style="white-space: pre;">&#8288;,&#8288;</code> for the decimal point.
</p>
<p>Note that not all <code>readr</code> options are currently implemented here. Please file
an issue if you encounter one that <code>arrow</code> should support.
</p>
<p>If you need to control Arrow-specific reader parameters that don't have an
equivalent in <code>readr::read_csv()</code>, you can either provide them in the
<code>parse_options</code>, <code>convert_options</code>, or <code>read_options</code> arguments, or you can
use <a href="#topic+CsvTableReader">CsvTableReader</a> directly for lower-level access.
</p>


<h3>Value</h3>

<p>A <code>tibble</code>, or a Table if <code>as_data_frame = FALSE</code>.
</p>


<h3>Specifying column types and names</h3>

<p>By default, the CSV reader will infer the column names and data types from the file, but there
are a few ways you can specify them directly.
</p>
<p>One way is to provide an Arrow <a href="#topic+Schema">Schema</a> in the <code>schema</code> argument,
which is an ordered map of column name to type.
When provided, it satisfies both the <code>col_names</code> and <code>col_types</code> arguments.
This is good if you know all of this information up front.
</p>
<p>You can also pass a <code>Schema</code> to the <code>col_types</code> argument. If you do this,
column names will still be inferred from the file unless you also specify
<code>col_names</code>. In either case, the column names in the <code>Schema</code> must match the
data's column names, whether they are explicitly provided or inferred. That
said, this <code>Schema</code> does not have to reference all columns: those omitted
will have their types inferred.
</p>
<p>Alternatively, you can declare column types by providing the compact string representation
that <code>readr</code> uses to the <code>col_types</code> argument. This means you provide a
single string, one character per column, where the characters map to Arrow
types analogously to the <code>readr</code> type mapping:
</p>

<ul>
<li><p> &quot;c&quot;: <code><a href="#topic+utf8">utf8()</a></code>
</p>
</li>
<li><p> &quot;i&quot;: <code><a href="#topic+int32">int32()</a></code>
</p>
</li>
<li><p> &quot;n&quot;: <code><a href="#topic+float64">float64()</a></code>
</p>
</li>
<li><p> &quot;d&quot;: <code><a href="#topic+float64">float64()</a></code>
</p>
</li>
<li><p> &quot;l&quot;: <code><a href="#topic+bool">bool()</a></code>
</p>
</li>
<li><p> &quot;f&quot;: <code><a href="#topic+dictionary">dictionary()</a></code>
</p>
</li>
<li><p> &quot;D&quot;: <code><a href="#topic+date32">date32()</a></code>
</p>
</li>
<li><p> &quot;T&quot;: <code><a href="#topic+timestamp">timestamp(unit = &quot;ns&quot;)</a></code>
</p>
</li>
<li><p> &quot;t&quot;: <code><a href="#topic+time32">time32()</a></code> (The <code>unit</code> arg is set to the default value <code>"ms"</code>)
</p>
</li>
<li><p> &quot;_&quot;: <code><a href="#topic+null">null()</a></code>
</p>
</li>
<li><p> &quot;-&quot;: <code><a href="#topic+null">null()</a></code>
</p>
</li>
<li><p> &quot;?&quot;: infer the type from the data
</p>
</li></ul>

<p>If you use the compact string representation for <code>col_types</code>, you must also
specify <code>col_names</code>.
</p>
<p>Regardless of how types are specified, all columns with a <code>null()</code> type will
be dropped.
</p>
<p>Note that if you are specifying column names, whether by <code>schema</code> or
<code>col_names</code>, and the CSV file has a header row that would otherwise be used
to identify column names, you'll need to add <code>skip = 1</code> to skip that row.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tf &lt;- tempfile()
on.exit(unlink(tf))
write.csv(mtcars, file = tf)
df &lt;- read_csv_arrow(tf)
dim(df)
# Can select columns
df &lt;- read_csv_arrow(tf, col_select = starts_with("d"))

# Specifying column types and names
write.csv(data.frame(x = c(1, 3), y = c(2, 4)), file = tf, row.names = FALSE)
read_csv_arrow(tf, schema = schema(x = int32(), y = utf8()), skip = 1)
read_csv_arrow(tf, col_types = schema(y = utf8()))
read_csv_arrow(tf, col_types = "ic", col_names = c("x", "y"), skip = 1)

# Note that if a timestamp column contains time zones,
# the string "T" `col_types` specification won't work.
# To parse timestamps with time zones, provide a [Schema] to `col_types`
# and specify the time zone in the type object:
tf &lt;- tempfile()
write.csv(data.frame(x = "1970-01-01T12:00:00+12:00"), file = tf, row.names = FALSE)
read_csv_arrow(
  tf,
  col_types = schema(x = timestamp(unit = "us", timezone = "UTC"))
)

# Read directly from strings with `I()`
read_csv_arrow(I("x,y\n1,2\n3,4"))
read_delim_arrow(I(c("x y", "1 2", "3 4")), delim = " ")
</code></pre>

<hr>
<h2 id='read_feather'>Read a Feather file (an Arrow IPC file)</h2><span id='topic+read_feather'></span><span id='topic+read_ipc_file'></span>

<h3>Description</h3>

<p>Feather provides binary columnar serialization for data frames.
It is designed to make reading and writing data frames efficient,
and to make sharing data across data analysis languages easy.
<code><a href="#topic+read_feather">read_feather()</a></code> can read both the Feather Version 1 (V1), a legacy version available starting in 2016,
and the Version 2 (V2), which is the Apache Arrow IPC file format.
<code><a href="#topic+read_ipc_file">read_ipc_file()</a></code> is an alias of <code><a href="#topic+read_feather">read_feather()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>read_feather(file, col_select = NULL, as_data_frame = TRUE, mmap = TRUE)

read_ipc_file(file, col_select = NULL, as_data_frame = TRUE, mmap = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="read_feather_+3A_file">file</code></td>
<td>
<p>A character file name or URI, <code>raw</code> vector, an Arrow input stream,
or a <code>FileSystem</code> with path (<code>SubTreeFileSystem</code>).
If a file name or URI, an Arrow <a href="#topic+InputStream">InputStream</a> will be opened and
closed when finished. If an input stream is provided, it will be left
open.</p>
</td></tr>
<tr><td><code id="read_feather_+3A_col_select">col_select</code></td>
<td>
<p>A character vector of column names to keep, as in the
&quot;select&quot; argument to <code>data.table::fread()</code>, or a
<a href="tidyselect.html#topic+eval_select">tidy selection specification</a>
of columns, as used in <code>dplyr::select()</code>.</p>
</td></tr>
<tr><td><code id="read_feather_+3A_as_data_frame">as_data_frame</code></td>
<td>
<p>Should the function return a <code>tibble</code> (default) or
an Arrow <a href="#topic+Table">Table</a>?</p>
</td></tr>
<tr><td><code id="read_feather_+3A_mmap">mmap</code></td>
<td>
<p>Logical: whether to memory-map the file (default <code>TRUE</code>)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>tibble</code> if <code>as_data_frame</code> is <code>TRUE</code> (the default), or an
Arrow <a href="#topic+Table">Table</a> otherwise
</p>


<h3>See Also</h3>

<p><a href="#topic+FeatherReader">FeatherReader</a> and <a href="#topic+RecordBatchReader">RecordBatchReader</a> for lower-level access to reading Arrow IPC data.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># We recommend the ".arrow" extension for Arrow IPC files (Feather V2).
tf &lt;- tempfile(fileext = ".arrow")
on.exit(unlink(tf))
write_feather(mtcars, tf)
df &lt;- read_feather(tf)
dim(df)
# Can select columns
df &lt;- read_feather(tf, col_select = starts_with("d"))
</code></pre>

<hr>
<h2 id='read_ipc_stream'>Read Arrow IPC stream format</h2><span id='topic+read_ipc_stream'></span>

<h3>Description</h3>

<p>Apache Arrow defines two formats for <a href="https://arrow.apache.org/docs/format/Columnar.html#serialization-and-interprocess-communication-ipc">serializing data for interprocess communication (IPC)</a>:
a &quot;stream&quot; format and a &quot;file&quot; format, known as Feather. <code>read_ipc_stream()</code>
and <code><a href="#topic+read_feather">read_feather()</a></code> read those formats, respectively.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>read_ipc_stream(file, as_data_frame = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="read_ipc_stream_+3A_file">file</code></td>
<td>
<p>A character file name or URI, <code>raw</code> vector, an Arrow input stream,
or a <code>FileSystem</code> with path (<code>SubTreeFileSystem</code>).
If a file name or URI, an Arrow <a href="#topic+InputStream">InputStream</a> will be opened and
closed when finished. If an input stream is provided, it will be left
open.</p>
</td></tr>
<tr><td><code id="read_ipc_stream_+3A_as_data_frame">as_data_frame</code></td>
<td>
<p>Should the function return a <code>tibble</code> (default) or
an Arrow <a href="#topic+Table">Table</a>?</p>
</td></tr>
<tr><td><code id="read_ipc_stream_+3A_...">...</code></td>
<td>
<p>extra parameters passed to <code>read_feather()</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>tibble</code> if <code>as_data_frame</code> is <code>TRUE</code> (the default), or an
Arrow <a href="#topic+Table">Table</a> otherwise
</p>


<h3>See Also</h3>

<p><code><a href="#topic+write_feather">write_feather()</a></code> for writing IPC files. <a href="#topic+RecordBatchReader">RecordBatchReader</a> for a
lower-level interface.
</p>

<hr>
<h2 id='read_json_arrow'>Read a JSON file</h2><span id='topic+read_json_arrow'></span>

<h3>Description</h3>

<p>Wrapper around <a href="#topic+JsonTableReader">JsonTableReader</a> to read a newline-delimited JSON (ndjson) file into a
data frame or Arrow Table.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>read_json_arrow(
  file,
  col_select = NULL,
  as_data_frame = TRUE,
  schema = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="read_json_arrow_+3A_file">file</code></td>
<td>
<p>A character file name or URI, literal data (either a single string or a <a href="base.html#topic+raw">raw</a> vector),
an Arrow input stream, or a <code>FileSystem</code> with path (<code>SubTreeFileSystem</code>).
</p>
<p>If a file name, a memory-mapped Arrow <a href="#topic+InputStream">InputStream</a> will be opened and
closed when finished; compression will be detected from the file extension
and handled automatically. If an input stream is provided, it will be left
open.
</p>
<p>To be recognised as literal data, the input must be wrapped with <code>I()</code>.</p>
</td></tr>
<tr><td><code id="read_json_arrow_+3A_col_select">col_select</code></td>
<td>
<p>A character vector of column names to keep, as in the
&quot;select&quot; argument to <code>data.table::fread()</code>, or a
<a href="tidyselect.html#topic+eval_select">tidy selection specification</a>
of columns, as used in <code>dplyr::select()</code>.</p>
</td></tr>
<tr><td><code id="read_json_arrow_+3A_as_data_frame">as_data_frame</code></td>
<td>
<p>Should the function return a <code>tibble</code> (default) or
an Arrow <a href="#topic+Table">Table</a>?</p>
</td></tr>
<tr><td><code id="read_json_arrow_+3A_schema">schema</code></td>
<td>
<p><a href="#topic+Schema">Schema</a> that describes the table.</p>
</td></tr>
<tr><td><code id="read_json_arrow_+3A_...">...</code></td>
<td>
<p>Additional options passed to <code>JsonTableReader$create()</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>If passed a path, will detect and handle compression from the file extension
(e.g. <code>.json.gz</code>).
</p>
<p>If <code>schema</code> is not provided, Arrow data types are inferred from the data:
</p>

<ul>
<li><p> JSON null values convert to the <code><a href="#topic+null">null()</a></code> type, but can fall back to any other type.
</p>
</li>
<li><p> JSON booleans convert to <code><a href="#topic+boolean">boolean()</a></code>.
</p>
</li>
<li><p> JSON numbers convert to <code><a href="#topic+int64">int64()</a></code>, falling back to <code><a href="#topic+float64">float64()</a></code> if a non-integer is encountered.
</p>
</li>
<li><p> JSON strings of the kind &quot;YYYY-MM-DD&quot; and &quot;YYYY-MM-DD hh:mm:ss&quot; convert to <code><a href="#topic+timestamp">timestamp(unit = &quot;s&quot;)</a></code>,
falling back to <code><a href="#topic+utf8">utf8()</a></code> if a conversion error occurs.
</p>
</li>
<li><p> JSON arrays convert to a <code><a href="#topic+list_of">list_of()</a></code> type, and inference proceeds recursively on the JSON arrays' values.
</p>
</li>
<li><p> Nested JSON objects convert to a <code><a href="#topic+struct">struct()</a></code> type, and inference proceeds recursively on the JSON objects' values.
</p>
</li></ul>

<p>When <code>as_data_frame = TRUE</code>, Arrow types are further converted to R types.
</p>


<h3>Value</h3>

<p>A <code>tibble</code>, or a Table if <code>as_data_frame = FALSE</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
tf &lt;- tempfile()
on.exit(unlink(tf))
writeLines('
    { "hello": 3.5, "world": false, "yo": "thing" }
    { "hello": 3.25, "world": null }
    { "hello": 0.0, "world": true, "yo": null }
  ', tf, useBytes = TRUE)

read_json_arrow(tf)

# Read directly from strings with `I()`
read_json_arrow(I(c('{"x": 1, "y": 2}', '{"x": 3, "y": 4}')))

</code></pre>

<hr>
<h2 id='read_message'>Read a Message from a stream</h2><span id='topic+read_message'></span>

<h3>Description</h3>

<p>Read a Message from a stream
</p>


<h3>Usage</h3>

<pre><code class='language-R'>read_message(stream)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="read_message_+3A_stream">stream</code></td>
<td>
<p>an InputStream</p>
</td></tr>
</table>

<hr>
<h2 id='read_parquet'>Read a Parquet file</h2><span id='topic+read_parquet'></span>

<h3>Description</h3>

<p>'<a href="https://parquet.apache.org/">Parquet</a>' is a columnar storage file format.
This function enables you to read Parquet files into R.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>read_parquet(
  file,
  col_select = NULL,
  as_data_frame = TRUE,
  props = ParquetArrowReaderProperties$create(),
  mmap = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="read_parquet_+3A_file">file</code></td>
<td>
<p>A character file name or URI, <code>raw</code> vector, an Arrow input stream,
or a <code>FileSystem</code> with path (<code>SubTreeFileSystem</code>).
If a file name or URI, an Arrow <a href="#topic+InputStream">InputStream</a> will be opened and
closed when finished. If an input stream is provided, it will be left
open.</p>
</td></tr>
<tr><td><code id="read_parquet_+3A_col_select">col_select</code></td>
<td>
<p>A character vector of column names to keep, as in the
&quot;select&quot; argument to <code>data.table::fread()</code>, or a
<a href="tidyselect.html#topic+eval_select">tidy selection specification</a>
of columns, as used in <code>dplyr::select()</code>.</p>
</td></tr>
<tr><td><code id="read_parquet_+3A_as_data_frame">as_data_frame</code></td>
<td>
<p>Should the function return a <code>tibble</code> (default) or
an Arrow <a href="#topic+Table">Table</a>?</p>
</td></tr>
<tr><td><code id="read_parquet_+3A_props">props</code></td>
<td>
<p><a href="#topic+ParquetArrowReaderProperties">ParquetArrowReaderProperties</a></p>
</td></tr>
<tr><td><code id="read_parquet_+3A_mmap">mmap</code></td>
<td>
<p>Use TRUE to use memory mapping where possible</p>
</td></tr>
<tr><td><code id="read_parquet_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <code>ParquetFileReader$create()</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>tibble</code> if <code>as_data_frame</code> is <code>TRUE</code> (the default), or an
Arrow <a href="#topic+Table">Table</a> otherwise.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
tf &lt;- tempfile()
on.exit(unlink(tf))
write_parquet(mtcars, tf)
df &lt;- read_parquet(tf, col_select = starts_with("d"))
head(df)

</code></pre>

<hr>
<h2 id='read_schema'>Read a Schema from a stream</h2><span id='topic+read_schema'></span>

<h3>Description</h3>

<p>Read a Schema from a stream
</p>


<h3>Usage</h3>

<pre><code class='language-R'>read_schema(stream, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="read_schema_+3A_stream">stream</code></td>
<td>
<p>a <code>Message</code>, <code>InputStream</code>, or <code>Buffer</code></p>
</td></tr>
<tr><td><code id="read_schema_+3A_...">...</code></td>
<td>
<p>currently ignored</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <a href="#topic+Schema">Schema</a>
</p>

<hr>
<h2 id='record_batch'>Create a RecordBatch</h2><span id='topic+record_batch'></span>

<h3>Description</h3>

<p>Create a RecordBatch
</p>


<h3>Usage</h3>

<pre><code class='language-R'>record_batch(..., schema = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="record_batch_+3A_...">...</code></td>
<td>
<p>A <code>data.frame</code> or a named set of Arrays or vectors. If given a
mixture of data.frames and vectors, the inputs will be autospliced together
(see examples). Alternatively, you can provide a single Arrow IPC
<code>InputStream</code>, <code>Message</code>, <code>Buffer</code>, or R <code>raw</code> object containing a <code>Buffer</code>.</p>
</td></tr>
<tr><td><code id="record_batch_+3A_schema">schema</code></td>
<td>
<p>a <a href="#topic+Schema">Schema</a>, or <code>NULL</code> (the default) to infer the schema from
the data in <code>...</code>. When providing an Arrow IPC buffer, <code>schema</code> is required.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>batch &lt;- record_batch(name = rownames(mtcars), mtcars)
dim(batch)
dim(head(batch))
names(batch)
batch$mpg
batch[["cyl"]]
as.data.frame(batch[4:8, c("gear", "hp", "wt")])
</code></pre>

<hr>
<h2 id='RecordBatch'>RecordBatch class</h2><span id='topic+RecordBatch'></span>

<h3>Description</h3>

<p>A record batch is a collection of equal-length arrays matching
a particular <a href="#topic+Schema">Schema</a>. It is a table-like data structure that is semantically
a sequence of <a href="#topic+Field">fields</a>, each a contiguous Arrow <a href="#topic+Array">Array</a>.
</p>


<h3>S3 Methods and Usage</h3>

<p>Record batches are data-frame-like, and many methods you expect to work on
a <code>data.frame</code> are implemented for <code>RecordBatch</code>. This includes <code>[</code>, <code>[[</code>,
<code>$</code>, <code>names</code>, <code>dim</code>, <code>nrow</code>, <code>ncol</code>, <code>head</code>, and <code>tail</code>. You can also pull
the data from an Arrow record batch into R with <code>as.data.frame()</code>. See the
examples.
</p>
<p>A caveat about the <code>$</code> method: because <code>RecordBatch</code> is an <code>R6</code> object,
<code>$</code> is also used to access the object's methods (see below). Methods take
precedence over the table's columns. So, <code>batch$Slice</code> would return the
&quot;Slice&quot; method function even if there were a column in the table called
&quot;Slice&quot;.
</p>


<h3>R6 Methods</h3>

<p>In addition to the more R-friendly S3 methods, a <code>RecordBatch</code> object has
the following R6 methods that map onto the underlying C++ methods:
</p>

<ul>
<li> <p><code style="white-space: pre;">&#8288;$Equals(other)&#8288;</code>: Returns <code>TRUE</code> if the <code>other</code> record batch is equal
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$column(i)&#8288;</code>: Extract an <code>Array</code> by integer position from the batch
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$column_name(i)&#8288;</code>: Get a column's name by integer position
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$names()&#8288;</code>: Get all column names (called by <code>names(batch)</code>)
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$nbytes()&#8288;</code>: Total number of bytes consumed by the elements of the record batch
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$RenameColumns(value)&#8288;</code>: Set all column names (called by <code>names(batch) &lt;- value</code>)
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$GetColumnByName(name)&#8288;</code>: Extract an <code>Array</code> by string name
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$RemoveColumn(i)&#8288;</code>: Drops a column from the batch by integer position
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$SelectColumns(indices)&#8288;</code>: Return a new record batch with a selection of columns, expressed as 0-based integers.
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$Slice(offset, length = NULL)&#8288;</code>: Create a zero-copy view starting at the
indicated integer offset and going for the given length, or to the end
of the table if <code>NULL</code>, the default.
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$Take(i)&#8288;</code>: return an <code>RecordBatch</code> with rows at positions given by
integers (R vector or Array Array) <code>i</code>.
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$Filter(i, keep_na = TRUE)&#8288;</code>: return an <code>RecordBatch</code> with rows at positions where logical
vector (or Arrow boolean Array) <code>i</code> is <code>TRUE</code>.
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$SortIndices(names, descending = FALSE)&#8288;</code>: return an <code>Array</code> of integer row
positions that can be used to rearrange the <code>RecordBatch</code> in ascending or
descending order by the first named column, breaking ties with further named
columns. <code>descending</code> can be a logical vector of length one or of the same
length as <code>names</code>.
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$serialize()&#8288;</code>: Returns a raw vector suitable for interprocess communication
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$cast(target_schema, safe = TRUE, options = cast_options(safe))&#8288;</code>: Alter
the schema of the record batch.
</p>
</li></ul>

<p>There are also some active bindings
</p>

<ul>
<li> <p><code style="white-space: pre;">&#8288;$num_columns&#8288;</code>
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$num_rows&#8288;</code>
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$schema&#8288;</code>
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$metadata&#8288;</code>: Returns the key-value metadata of the <code>Schema</code> as a named list.
Modify or replace by assigning in (<code>batch$metadata &lt;- new_metadata</code>).
All list elements are coerced to string. See <code>schema()</code> for more information.
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$columns&#8288;</code>: Returns a list of <code>Array</code>s
</p>
</li></ul>


<hr>
<h2 id='RecordBatchReader'>RecordBatchReader classes</h2><span id='topic+RecordBatchReader'></span><span id='topic+RecordBatchStreamReader'></span><span id='topic+RecordBatchFileReader'></span>

<h3>Description</h3>

<p>Apache Arrow defines two formats for <a href="https://arrow.apache.org/docs/format/Columnar.html#serialization-and-interprocess-communication-ipc">serializing data for interprocess communication (IPC)</a>:
a &quot;stream&quot; format and a &quot;file&quot; format, known as Feather.
<code>RecordBatchStreamReader</code> and <code>RecordBatchFileReader</code> are
interfaces for accessing record batches from input sources in those formats,
respectively.
</p>
<p>For guidance on how to use these classes, see the examples section.
</p>


<h3>Factory</h3>

<p>The <code>RecordBatchFileReader$create()</code> and <code>RecordBatchStreamReader$create()</code>
factory methods instantiate the object and
take a single argument, named according to the class:
</p>

<ul>
<li> <p><code>file</code> A character file name, raw vector, or Arrow file connection object
(e.g. <a href="#topic+RandomAccessFile">RandomAccessFile</a>).
</p>
</li>
<li> <p><code>stream</code> A raw vector, <a href="#topic+Buffer">Buffer</a>, or <a href="#topic+InputStream">InputStream</a>.
</p>
</li></ul>



<h3>Methods</h3>


<ul>
<li> <p><code style="white-space: pre;">&#8288;$read_next_batch()&#8288;</code>: Returns a <code>RecordBatch</code>, iterating through the
Reader. If there are no further batches in the Reader, it returns <code>NULL</code>.
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$schema&#8288;</code>: Returns a <a href="#topic+Schema">Schema</a> (active binding)
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$batches()&#8288;</code>: Returns a list of <code>RecordBatch</code>es
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$read_table()&#8288;</code>: Collects the reader's <code>RecordBatch</code>es into a <a href="#topic+Table">Table</a>
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$get_batch(i)&#8288;</code>: For <code>RecordBatchFileReader</code>, return a particular batch
by an integer index.
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$num_record_batches()&#8288;</code>: For <code>RecordBatchFileReader</code>, see how many batches
are in the file.
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+read_ipc_stream">read_ipc_stream()</a></code> and <code><a href="#topic+read_feather">read_feather()</a></code> provide a much simpler interface
for reading data from these formats and are sufficient for many use cases.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tf &lt;- tempfile()
on.exit(unlink(tf))

batch &lt;- record_batch(chickwts)

# This opens a connection to the file in Arrow
file_obj &lt;- FileOutputStream$create(tf)
# Pass that to a RecordBatchWriter to write data conforming to a schema
writer &lt;- RecordBatchFileWriter$create(file_obj, batch$schema)
writer$write(batch)
# You may write additional batches to the stream, provided that they have
# the same schema.
# Call "close" on the writer to indicate end-of-file/stream
writer$close()
# Then, close the connection--closing the IPC message does not close the file
file_obj$close()

# Now, we have a file we can read from. Same pattern: open file connection,
# then pass it to a RecordBatchReader
read_file_obj &lt;- ReadableFile$create(tf)
reader &lt;- RecordBatchFileReader$create(read_file_obj)
# RecordBatchFileReader knows how many batches it has (StreamReader does not)
reader$num_record_batches
# We could consume the Reader by calling $read_next_batch() until all are,
# consumed, or we can call $read_table() to pull them all into a Table
tab &lt;- reader$read_table()
# Call as.data.frame to turn that Table into an R data.frame
df &lt;- as.data.frame(tab)
# This should be the same data we sent
all.equal(df, chickwts, check.attributes = FALSE)
# Unlike the Writers, we don't have to close RecordBatchReaders,
# but we do still need to close the file connection
read_file_obj$close()
</code></pre>

<hr>
<h2 id='RecordBatchWriter'>RecordBatchWriter classes</h2><span id='topic+RecordBatchWriter'></span><span id='topic+RecordBatchStreamWriter'></span><span id='topic+RecordBatchFileWriter'></span>

<h3>Description</h3>

<p>Apache Arrow defines two formats for <a href="https://arrow.apache.org/docs/format/Columnar.html#serialization-and-interprocess-communication-ipc">serializing data for interprocess communication (IPC)</a>:
a &quot;stream&quot; format and a &quot;file&quot; format, known as Feather.
<code>RecordBatchStreamWriter</code> and <code>RecordBatchFileWriter</code> are
interfaces for writing record batches to those formats, respectively.
</p>
<p>For guidance on how to use these classes, see the examples section.
</p>


<h3>Factory</h3>

<p>The <code>RecordBatchFileWriter$create()</code> and <code>RecordBatchStreamWriter$create()</code>
factory methods instantiate the object and take the following arguments:
</p>

<ul>
<li> <p><code>sink</code> An <code>OutputStream</code>
</p>
</li>
<li> <p><code>schema</code> A <a href="#topic+Schema">Schema</a> for the data to be written
</p>
</li>
<li> <p><code>use_legacy_format</code> logical: write data formatted so that Arrow libraries
versions 0.14 and lower can read it. Default is <code>FALSE</code>. You can also
enable this by setting the environment variable <code>ARROW_PRE_0_15_IPC_FORMAT=1</code>.
</p>
</li>
<li> <p><code>metadata_version</code>: A string like &quot;V5&quot; or the equivalent integer indicating
the Arrow IPC MetadataVersion. Default (NULL) will use the latest version,
unless the environment variable <code>ARROW_PRE_1_0_METADATA_VERSION=1</code>, in
which case it will be V4.
</p>
</li></ul>



<h3>Methods</h3>


<ul>
<li> <p><code style="white-space: pre;">&#8288;$write(x)&#8288;</code>: Write a <a href="#topic+RecordBatch">RecordBatch</a>, <a href="#topic+Table">Table</a>, or <code>data.frame</code>, dispatching
to the methods below appropriately
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$write_batch(batch)&#8288;</code>: Write a <code>RecordBatch</code> to stream
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$write_table(table)&#8288;</code>: Write a <code>Table</code> to stream
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$close()&#8288;</code>: close stream. Note that this indicates end-of-file or
end-of-stream&ndash;it does not close the connection to the <code>sink</code>. That needs
to be closed separately.
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+write_ipc_stream">write_ipc_stream()</a></code> and <code><a href="#topic+write_feather">write_feather()</a></code> provide a much simpler
interface for writing data to these formats and are sufficient for many use
cases. <code><a href="#topic+write_to_raw">write_to_raw()</a></code> is a version that serializes data to a buffer.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tf &lt;- tempfile()
on.exit(unlink(tf))

batch &lt;- record_batch(chickwts)

# This opens a connection to the file in Arrow
file_obj &lt;- FileOutputStream$create(tf)
# Pass that to a RecordBatchWriter to write data conforming to a schema
writer &lt;- RecordBatchFileWriter$create(file_obj, batch$schema)
writer$write(batch)
# You may write additional batches to the stream, provided that they have
# the same schema.
# Call "close" on the writer to indicate end-of-file/stream
writer$close()
# Then, close the connection--closing the IPC message does not close the file
file_obj$close()

# Now, we have a file we can read from. Same pattern: open file connection,
# then pass it to a RecordBatchReader
read_file_obj &lt;- ReadableFile$create(tf)
reader &lt;- RecordBatchFileReader$create(read_file_obj)
# RecordBatchFileReader knows how many batches it has (StreamReader does not)
reader$num_record_batches
# We could consume the Reader by calling $read_next_batch() until all are,
# consumed, or we can call $read_table() to pull them all into a Table
tab &lt;- reader$read_table()
# Call as.data.frame to turn that Table into an R data.frame
df &lt;- as.data.frame(tab)
# This should be the same data we sent
all.equal(df, chickwts, check.attributes = FALSE)
# Unlike the Writers, we don't have to close RecordBatchReaders,
# but we do still need to close the file connection
read_file_obj$close()
</code></pre>

<hr>
<h2 id='recycle_scalars'>Recycle scalar values in a list of arrays</h2><span id='topic+recycle_scalars'></span>

<h3>Description</h3>

<p>Recycle scalar values in a list of arrays
</p>


<h3>Usage</h3>

<pre><code class='language-R'>recycle_scalars(arrays)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="recycle_scalars_+3A_arrays">arrays</code></td>
<td>
<p>List of arrays</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List of arrays with any vector/Scalar/Array/ChunkedArray values of length 1 recycled
</p>

<hr>
<h2 id='reexports'>Objects exported from other packages</h2><span id='topic+reexports'></span><span id='topic+print.integer64'></span><span id='topic+str.integer64'></span><span id='topic+contains'></span><span id='topic+select_helpers'></span><span id='topic+ends_with'></span><span id='topic+everything'></span><span id='topic+matches'></span><span id='topic+num_range'></span><span id='topic+one_of'></span><span id='topic+starts_with'></span><span id='topic+last_col'></span><span id='topic+all_of'></span>

<h3>Description</h3>

<p>These objects are imported from other packages. Follow the links
below to see their documentation.
</p>

<dl>
<dt>bit64</dt><dd><p><code><a href="bit64.html#topic+bit64-package">print.integer64</a></code>, <code><a href="bit64.html#topic+bit64-package">str.integer64</a></code></p>
</dd>
<dt>tidyselect</dt><dd><p><code><a href="tidyselect.html#topic+all_of">all_of</a></code>, <code><a href="tidyselect.html#topic+starts_with">contains</a></code>, <code><a href="tidyselect.html#topic+starts_with">ends_with</a></code>, <code><a href="tidyselect.html#topic+everything">everything</a></code>, <code><a href="tidyselect.html#topic+everything">last_col</a></code>, <code><a href="tidyselect.html#topic+starts_with">matches</a></code>, <code><a href="tidyselect.html#topic+starts_with">num_range</a></code>, <code><a href="tidyselect.html#topic+one_of">one_of</a></code>, <code><a href="tidyselect.html#topic+starts_with">starts_with</a></code></p>
</dd>
</dl>

<hr>
<h2 id='register_binding'>Register compute bindings</h2><span id='topic+register_binding'></span><span id='topic+register_binding_agg'></span>

<h3>Description</h3>

<p>The <code>register_binding()</code> and <code>register_binding_agg()</code> functions
are used to populate a list of functions that operate on (and return)
Expressions. These are the basis for the <code>.data</code> mask inside dplyr methods.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>register_binding(
  fun_name,
  fun,
  registry = nse_funcs,
  update_cache = FALSE,
  notes = character(0)
)

register_binding_agg(
  fun_name,
  agg_fun,
  registry = agg_funcs,
  notes = character(0)
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="register_binding_+3A_fun_name">fun_name</code></td>
<td>
<p>A string containing a function name in the form <code>"function"</code> or
<code>"package::function"</code>. The package name is currently not used but
may be used in the future to allow these types of function calls.</p>
</td></tr>
<tr><td><code id="register_binding_+3A_fun">fun</code></td>
<td>
<p>A function or <code>NULL</code> to un-register a previous function.
This function must accept <code>Expression</code> objects as arguments and return
<code>Expression</code> objects instead of regular R objects.</p>
</td></tr>
<tr><td><code id="register_binding_+3A_registry">registry</code></td>
<td>
<p>An environment in which the functions should be
assigned.</p>
</td></tr>
<tr><td><code id="register_binding_+3A_update_cache">update_cache</code></td>
<td>
<p>Update .cache$functions at the time of registration.
the default is FALSE because the majority of usage is to register
bindings at package load, after which we create the cache once. The
reason why .cache$functions is needed in addition to nse_funcs for
non-aggregate functions could be revisited...it is currently used
as the data mask in mutate, filter, and aggregate (but not
summarise) because the data mask has to be a list.</p>
</td></tr>
<tr><td><code id="register_binding_+3A_notes">notes</code></td>
<td>
<p>string for the docs: note any limitations or differences in
behavior between the Arrow version and the R function.</p>
</td></tr>
<tr><td><code id="register_binding_+3A_agg_fun">agg_fun</code></td>
<td>
<p>An aggregate function or <code>NULL</code> to un-register a previous
aggregate function. This function must accept <code>Expression</code> objects as
arguments and return a <code>list()</code> with components:
</p>

<ul>
<li> <p><code>fun</code>: string function name
</p>
</li>
<li> <p><code>data</code>: list of 0 or more <code>Expression</code>s
</p>
</li>
<li> <p><code>options</code>: list of function options, as passed to call_function
</p>
</li></ul>
</td></tr>
</table>


<h3>Value</h3>

<p>The previously registered binding or <code>NULL</code> if no previously
registered function existed.
</p>


<h3>Writing bindings</h3>


<ul>
<li> <p><code>Expression$create()</code> will wrap any non-Expression inputs as Scalar
Expressions. If you want to try to coerce scalar inputs to match the type
of the Expression(s) in the arguments, call
<code>cast_scalars_to_common_type(args)</code> on the
args. For example, <code>Expression$create("add", args = list(int16_field, 1))</code>
would result in a <code>float64</code> type output because <code>1</code> is a <code>double</code> in R.
To prevent casting all of the data in <code>int16_field</code> to float and to
preserve it as int16, do
<code>Expression$create("add", args = cast_scalars_to_common_type(list(int16_field, 1)))</code>
</p>
</li>
<li><p> Inside your function, you can call any other binding with <code>call_binding()</code>.
</p>
</li></ul>


<hr>
<h2 id='register_scalar_function'>Register user-defined functions</h2><span id='topic+register_scalar_function'></span>

<h3>Description</h3>

<p>These functions support calling R code from query engine execution
(i.e., a <code><a href="dplyr.html#topic+mutate">dplyr::mutate()</a></code> or <code><a href="dplyr.html#topic+filter">dplyr::filter()</a></code> on a <a href="#topic+Table">Table</a> or <a href="#topic+Dataset">Dataset</a>).
Use <code><a href="#topic+register_scalar_function">register_scalar_function()</a></code> attach Arrow input and output types to an
R function and make it available for use in the dplyr interface and/or
<code><a href="#topic+call_function">call_function()</a></code>. Scalar functions are currently the only type of
user-defined function supported. In Arrow, scalar functions must be
stateless and return output with the same shape (i.e., the same number
of rows) as the input.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>register_scalar_function(name, fun, in_type, out_type, auto_convert = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="register_scalar_function_+3A_name">name</code></td>
<td>
<p>The function name to be used in the dplyr bindings</p>
</td></tr>
<tr><td><code id="register_scalar_function_+3A_fun">fun</code></td>
<td>
<p>An R function or rlang-style lambda expression. The function
will be called with a first argument <code>context</code> which is a <code>list()</code>
with elements <code>batch_size</code> (the expected length of the output) and
<code>output_type</code> (the required <a href="#topic+DataType">DataType</a> of the output) that may be used
to ensure that the output has the correct type and length. Subsequent
arguments are passed by position as specified by <code>in_types</code>. If
<code>auto_convert</code> is <code>TRUE</code>, subsequent arguments are converted to
R vectors before being passed to <code>fun</code> and the output is automatically
constructed with the expected output type via <code><a href="#topic+as_arrow_array">as_arrow_array()</a></code>.</p>
</td></tr>
<tr><td><code id="register_scalar_function_+3A_in_type">in_type</code></td>
<td>
<p>A <a href="#topic+DataType">DataType</a> of the input type or a <code><a href="#topic+schema">schema()</a></code>
for functions with more than one argument. This signature will be used
to determine if this function is appropriate for a given set of arguments.
If this function is appropriate for more than one signature, pass a
<code>list()</code> of the above.</p>
</td></tr>
<tr><td><code id="register_scalar_function_+3A_out_type">out_type</code></td>
<td>
<p>A <a href="#topic+DataType">DataType</a> of the output type or a function accepting
a single argument (<code>types</code>), which is a <code>list()</code> of <a href="#topic+DataType">DataType</a>s. If a
function it must return a <a href="#topic+DataType">DataType</a>.</p>
</td></tr>
<tr><td><code id="register_scalar_function_+3A_auto_convert">auto_convert</code></td>
<td>
<p>Use <code>TRUE</code> to convert inputs before passing to <code>fun</code>
and construct an Array of the correct type from the output. Use this
option to write functions of R objects as opposed to functions of
Arrow R6 objects.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>NULL</code>, invisibly
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(dplyr, warn.conflicts = FALSE)

some_model &lt;- lm(mpg ~ disp + cyl, data = mtcars)
register_scalar_function(
  "mtcars_predict_mpg",
  function(context, disp, cyl) {
    predict(some_model, newdata = data.frame(disp, cyl))
  },
  in_type = schema(disp = float64(), cyl = float64()),
  out_type = float64(),
  auto_convert = TRUE
)

as_arrow_table(mtcars) %&gt;%
  transmute(mpg, mpg_predicted = mtcars_predict_mpg(disp, cyl)) %&gt;%
  collect() %&gt;%
  head()

</code></pre>

<hr>
<h2 id='repeat_value_as_array'>Take an object of length 1 and repeat it.</h2><span id='topic+repeat_value_as_array'></span>

<h3>Description</h3>

<p>Take an object of length 1 and repeat it.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>repeat_value_as_array(object, n)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="repeat_value_as_array_+3A_object">object</code></td>
<td>
<p>Object of length 1 to be repeated - vector, <code>Scalar</code>, <code>Array</code>, or <code>ChunkedArray</code></p>
</td></tr>
<tr><td><code id="repeat_value_as_array_+3A_n">n</code></td>
<td>
<p>Number of repetitions</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>Array</code> of length <code>n</code>
</p>

<hr>
<h2 id='s3_bucket'>Connect to an AWS S3 bucket</h2><span id='topic+s3_bucket'></span>

<h3>Description</h3>

<p><code>s3_bucket()</code> is a convenience function to create an <code>S3FileSystem</code> object
that automatically detects the bucket's AWS region and holding onto the its
relative path.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>s3_bucket(bucket, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="s3_bucket_+3A_bucket">bucket</code></td>
<td>
<p>string S3 bucket name or path</p>
</td></tr>
<tr><td><code id="s3_bucket_+3A_...">...</code></td>
<td>
<p>Additional connection options, passed to <code>S3FileSystem$create()</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>By default, <code><a href="#topic+s3_bucket">s3_bucket</a></code> and other
<code><a href="#topic+S3FileSystem">S3FileSystem</a></code> functions only produce output for fatal errors
or when printing their return values. When troubleshooting problems, it may
be useful to increase the log level. See the Notes section in
<code><a href="#topic+S3FileSystem">S3FileSystem</a></code> for more information or see Examples below.
</p>


<h3>Value</h3>

<p>A <code>SubTreeFileSystem</code> containing an <code>S3FileSystem</code> and the bucket's
relative path. Note that this function's success does not guarantee that you
are authorized to access the bucket's contents.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
bucket &lt;- s3_bucket("voltrondata-labs-datasets")


# Turn on debug logging. The following line of code should be run in a fresh
# R session prior to any calls to `s3_bucket()` (or other S3 functions)
Sys.setenv("ARROW_S3_LOG_LEVEL", "DEBUG")
bucket &lt;- s3_bucket("voltrondata-labs-datasets")

</code></pre>

<hr>
<h2 id='scalar'>Create an Arrow Scalar</h2><span id='topic+scalar'></span><span id='topic+StructScalar'></span>

<h3>Description</h3>

<p>Create an Arrow Scalar
</p>


<h3>Usage</h3>

<pre><code class='language-R'>scalar(x, type = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="scalar_+3A_x">x</code></td>
<td>
<p>An R vector, list, or <code>data.frame</code></p>
</td></tr>
<tr><td><code id="scalar_+3A_type">type</code></td>
<td>
<p>An optional <a href="#topic+data-type">data type</a> for <code>x</code>. If omitted, the type will be inferred from the data.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>scalar(pi)
scalar(404)
# If you pass a vector into scalar(), you get a list containing your items
scalar(c(1, 2, 3))

scalar(9) == scalar(10)

</code></pre>

<hr>
<h2 id='Scalar'>Arrow scalars</h2><span id='topic+Scalar'></span>

<h3>Description</h3>

<p>A <code>Scalar</code> holds a single value of an Arrow type.
</p>


<h3>Factory</h3>

<p>The <code>Scalar$create()</code> factory method instantiates a <code>Scalar</code> and takes the following arguments:
</p>

<ul>
<li> <p><code>x</code>: an R vector, list, or <code>data.frame</code>
</p>
</li>
<li> <p><code>type</code>: an optional <a href="#topic+data-type">data type</a> for <code>x</code>. If omitted, the type will be inferred from the data.
</p>
</li></ul>



<h3>Usage</h3>

<div class="sourceCode"><pre>a &lt;- Scalar$create(x)
length(a)

print(a)
a == a
</pre></div>


<h3>Methods</h3>


<ul>
<li> <p><code style="white-space: pre;">&#8288;$ToString()&#8288;</code>: convert to a string
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$as_vector()&#8288;</code>: convert to an R vector
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$as_array()&#8288;</code>: convert to an Arrow <code>Array</code>
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$Equals(other)&#8288;</code>: is this Scalar equal to <code>other</code>
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$ApproxEquals(other)&#8288;</code>: is this Scalar approximately equal to <code>other</code>
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$is_valid&#8288;</code>: is this Scalar valid
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$null_count&#8288;</code>: number of invalid values - 1 or 0
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$type&#8288;</code>: Scalar type
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$cast(target_type, safe = TRUE, options = cast_options(safe))&#8288;</code>: cast value
to a different type
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>Scalar$create(pi)
Scalar$create(404)
# If you pass a vector into Scalar$create, you get a list containing your items
Scalar$create(c(1, 2, 3))

# Comparisons
my_scalar &lt;- Scalar$create(99)
my_scalar$ApproxEquals(Scalar$create(99.00001)) # FALSE
my_scalar$ApproxEquals(Scalar$create(99.000009)) # TRUE
my_scalar$Equals(Scalar$create(99.000009)) # FALSE
my_scalar$Equals(Scalar$create(99L)) # FALSE (types don't match)

my_scalar$ToString()
</code></pre>

<hr>
<h2 id='Scanner'>Scan the contents of a dataset</h2><span id='topic+Scanner'></span><span id='topic+ScannerBuilder'></span>

<h3>Description</h3>

<p>A <code>Scanner</code> iterates over a <a href="#topic+Dataset">Dataset</a>'s fragments and returns data
according to given row filtering and column projection. A <code>ScannerBuilder</code>
can help create one.
</p>


<h3>Factory</h3>

<p><code>Scanner$create()</code> wraps the <code>ScannerBuilder</code> interface to make a <code>Scanner</code>.
It takes the following arguments:
</p>

<ul>
<li> <p><code>dataset</code>: A <code>Dataset</code> or <code>arrow_dplyr_query</code> object, as returned by the
<code>dplyr</code> methods on <code>Dataset</code>.
</p>
</li>
<li> <p><code>projection</code>: A character vector of column names to select columns or a
named list of expressions
</p>
</li>
<li> <p><code>filter</code>: A <code>Expression</code> to filter the scanned rows by, or <code>TRUE</code> (default)
to keep all rows.
</p>
</li>
<li> <p><code>use_threads</code>: logical: should scanning use multithreading? Default <code>TRUE</code>
</p>
</li>
<li> <p><code>...</code>: Additional arguments, currently ignored
</p>
</li></ul>



<h3>Methods</h3>

<p><code>ScannerBuilder</code> has the following methods:
</p>

<ul>
<li> <p><code style="white-space: pre;">&#8288;$Project(cols)&#8288;</code>: Indicate that the scan should only return columns given
by <code>cols</code>, a character vector of column names or a named list of <a href="#topic+Expression">Expression</a>.
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$Filter(expr)&#8288;</code>: Filter rows by an <a href="#topic+Expression">Expression</a>.
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$UseThreads(threads)&#8288;</code>: logical: should the scan use multithreading?
The method's default input is <code>TRUE</code>, but you must call the method to enable
multithreading because the scanner default is <code>FALSE</code>.
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$BatchSize(batch_size)&#8288;</code>: integer: Maximum row count of scanned record
batches, default is 32K. If scanned record batches are overflowing memory
then this method can be called to reduce their size.
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$schema&#8288;</code>: Active binding, returns the <a href="#topic+Schema">Schema</a> of the Dataset
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$Finish()&#8288;</code>: Returns a <code>Scanner</code>
</p>
</li></ul>

<p><code>Scanner</code> currently has a single method, <code style="white-space: pre;">&#8288;$ToTable()&#8288;</code>, which evaluates the
query and returns an Arrow <a href="#topic+Table">Table</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Set up directory for examples
tf &lt;- tempfile()
dir.create(tf)
on.exit(unlink(tf))

write_dataset(mtcars, tf, partitioning="cyl")

ds &lt;- open_dataset(tf)

scan_builder &lt;- ds$NewScan()
scan_builder$Filter(Expression$field_ref("hp") &gt; 100)
scan_builder$Project(list(hp_times_ten = 10 * Expression$field_ref("hp")))

# Once configured, call $Finish()
scanner &lt;- scan_builder$Finish()

# Can get results as a table
as.data.frame(scanner$ToTable())

# Or as a RecordBatchReader
scanner$ToRecordBatchReader()

</code></pre>

<hr>
<h2 id='schema'>Create a schema or extract one from an object.</h2><span id='topic+schema'></span>

<h3>Description</h3>

<p>Create a schema or extract one from an object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>schema(...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="schema_+3A_...">...</code></td>
<td>
<p><a href="#topic+field">fields</a>, field name/<a href="#topic+data-type">data type</a> pairs (or a list of), or object from which to extract
a schema</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><a href="#topic+Schema">Schema</a> for detailed documentation of the Schema R6 object
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Create schema using pairs of field names and data types
schema(a = int32(), b = float64())

# Create a schema using a list of pairs of field names and data types
schema(list(a = int8(), b = string()))

# Create schema using fields
schema(
  field("b", double()),
  field("c", bool(), nullable = FALSE),
  field("d", string())
)

# Extract schemas from objects
df &lt;- data.frame(col1 = 2:4, col2 = c(0.1, 0.3, 0.5))
tab1 &lt;- arrow_table(df)
schema(tab1)
tab2 &lt;- arrow_table(df, schema = schema(col1 = int8(), col2 = float32()))
schema(tab2)
</code></pre>

<hr>
<h2 id='Schema'>Schema class</h2><span id='topic+Schema'></span>

<h3>Description</h3>

<p>A <code>Schema</code> is an Arrow object containing <a href="#topic+Field">Field</a>s, which map names to
Arrow <a href="#topic+data-type">data types</a>. Create a <code>Schema</code> when you
want to convert an R <code>data.frame</code> to Arrow but don't want to rely on the
default mapping of R types to Arrow types, such as when you want to choose a
specific numeric precision, or when creating a <a href="#topic+Dataset">Dataset</a> and you want to
ensure a specific schema rather than inferring it from the various files.
</p>
<p>Many Arrow objects, including <a href="#topic+Table">Table</a> and <a href="#topic+Dataset">Dataset</a>, have a <code style="white-space: pre;">&#8288;$schema&#8288;</code> method
(active binding) that lets you access their schema.
</p>


<h3>Methods</h3>


<ul>
<li> <p><code style="white-space: pre;">&#8288;$ToString()&#8288;</code>: convert to a string
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$field(i)&#8288;</code>: returns the field at index <code>i</code> (0-based)
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$GetFieldByName(x)&#8288;</code>: returns the field with name <code>x</code>
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$WithMetadata(metadata)&#8288;</code>: returns a new <code>Schema</code> with the key-value
<code>metadata</code> set. Note that all list elements in <code>metadata</code> will be coerced
to <code>character</code>.
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$code(namespace)&#8288;</code>: returns the R code needed to generate this schema. Use <code>namespace=TRUE</code> to call with <code style="white-space: pre;">&#8288;arrow::&#8288;</code>.
</p>
</li></ul>



<h3>Active bindings</h3>


<ul>
<li> <p><code style="white-space: pre;">&#8288;$names&#8288;</code>: returns the field names (called in <code>names(Schema)</code>)
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$num_fields&#8288;</code>: returns the number of fields (called in <code>length(Schema)</code>)
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$fields&#8288;</code>: returns the list of <code>Field</code>s in the <code>Schema</code>, suitable for
iterating over
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$HasMetadata&#8288;</code>: logical: does this <code>Schema</code> have extra metadata?
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$metadata&#8288;</code>: returns the key-value metadata as a named list.
Modify or replace by assigning in (<code>sch$metadata &lt;- new_metadata</code>).
All list elements are coerced to string.
</p>
</li></ul>



<h3>R Metadata</h3>

<p>When converting a data.frame to an Arrow Table or RecordBatch, attributes
from the <code>data.frame</code> are saved alongside tables so that the object can be
reconstructed faithfully in R (e.g. with <code>as.data.frame()</code>). This metadata
can be both at the top-level of the <code>data.frame</code> (e.g. <code>attributes(df)</code>) or
at the column (e.g. <code>attributes(df$col_a)</code>) or for list columns only:
element level (e.g. <code>attributes(df[1, "col_a"])</code>). For example, this allows
for storing <code>haven</code> columns in a table and being able to faithfully
re-create them when pulled back into R. This metadata is separate from the
schema (column names and types) which is compatible with other Arrow
clients. The R metadata is only read by R and is ignored by other clients
(e.g. Pandas has its own custom metadata). This metadata is stored in
<code style="white-space: pre;">&#8288;$metadata$r&#8288;</code>.
</p>
<p>Since Schema metadata keys and values must be strings, this metadata is
saved by serializing R's attribute list structure to a string. If the
serialized metadata exceeds 100Kb in size, by default it is compressed
starting in version 3.0.0. To disable this compression (e.g. for tables
that are compatible with Arrow versions before 3.0.0 and include large
amounts of metadata), set the option <code>arrow.compress_metadata</code> to <code>FALSE</code>.
Files with compressed metadata are readable by older versions of arrow, but
the metadata is dropped.
</p>

<hr>
<h2 id='show_exec_plan'>Show the details of an Arrow Execution Plan</h2><span id='topic+show_exec_plan'></span>

<h3>Description</h3>

<p>This is a function which gives more details about the logical query plan
that will be executed when evaluating an <code>arrow_dplyr_query</code> object.
It calls the C++ <code>ExecPlan</code> object's print method.
Functionally, it is similar to <code>dplyr::explain()</code>. This function is used as
the <code>dplyr::explain()</code> and <code>dplyr::show_query()</code> methods.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>show_exec_plan(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="show_exec_plan_+3A_x">x</code></td>
<td>
<p>an <code>arrow_dplyr_query</code> to print the <code>ExecPlan</code> for.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>x</code>, invisibly.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(dplyr)
mtcars %&gt;%
  arrow_table() %&gt;%
  filter(mpg &gt; 20) %&gt;%
  mutate(x = gear / carb) %&gt;%
  show_exec_plan()

</code></pre>

<hr>
<h2 id='Table'>Table class</h2><span id='topic+Table'></span>

<h3>Description</h3>

<p>A Table is a sequence of <a href="#topic+ChunkedArray">chunked arrays</a>. They
have a similar interface to <a href="#topic+RecordBatch">record batches</a>, but they can be
composed from multiple record batches or chunked arrays.
</p>


<h3>S3 Methods and Usage</h3>

<p>Tables are data-frame-like, and many methods you expect to work on
a <code>data.frame</code> are implemented for <code>Table</code>. This includes <code>[</code>, <code>[[</code>,
<code>$</code>, <code>names</code>, <code>dim</code>, <code>nrow</code>, <code>ncol</code>, <code>head</code>, and <code>tail</code>. You can also pull
the data from an Arrow table into R with <code>as.data.frame()</code>. See the
examples.
</p>
<p>A caveat about the <code>$</code> method: because <code>Table</code> is an <code>R6</code> object,
<code>$</code> is also used to access the object's methods (see below). Methods take
precedence over the table's columns. So, <code>tab$Slice</code> would return the
&quot;Slice&quot; method function even if there were a column in the table called
&quot;Slice&quot;.
</p>


<h3>R6 Methods</h3>

<p>In addition to the more R-friendly S3 methods, a <code>Table</code> object has
the following R6 methods that map onto the underlying C++ methods:
</p>

<ul>
<li> <p><code style="white-space: pre;">&#8288;$column(i)&#8288;</code>: Extract a <code>ChunkedArray</code> by integer position from the table
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$ColumnNames()&#8288;</code>: Get all column names (called by <code>names(tab)</code>)
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$nbytes()&#8288;</code>: Total number of bytes consumed by the elements of the table
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$RenameColumns(value)&#8288;</code>: Set all column names (called by <code>names(tab) &lt;- value</code>)
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$GetColumnByName(name)&#8288;</code>: Extract a <code>ChunkedArray</code> by string name
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$field(i)&#8288;</code>: Extract a <code>Field</code> from the table schema by integer position
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$SelectColumns(indices)&#8288;</code>: Return new <code>Table</code> with specified columns, expressed as 0-based integers.
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$Slice(offset, length = NULL)&#8288;</code>: Create a zero-copy view starting at the
indicated integer offset and going for the given length, or to the end
of the table if <code>NULL</code>, the default.
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$Take(i)&#8288;</code>: return an <code>Table</code> with rows at positions given by
integers <code>i</code>. If <code>i</code> is an Arrow <code>Array</code> or <code>ChunkedArray</code>, it will be
coerced to an R vector before taking.
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$Filter(i, keep_na = TRUE)&#8288;</code>: return an <code>Table</code> with rows at positions where logical
vector or Arrow boolean-type <code style="white-space: pre;">&#8288;(Chunked)Array&#8288;</code> <code>i</code> is <code>TRUE</code>.
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$SortIndices(names, descending = FALSE)&#8288;</code>: return an <code>Array</code> of integer row
positions that can be used to rearrange the <code>Table</code> in ascending or descending
order by the first named column, breaking ties with further named columns.
<code>descending</code> can be a logical vector of length one or of the same length as
<code>names</code>.
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$serialize(output_stream, ...)&#8288;</code>: Write the table to the given
<a href="#topic+OutputStream">OutputStream</a>
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$cast(target_schema, safe = TRUE, options = cast_options(safe))&#8288;</code>: Alter
the schema of the record batch.
</p>
</li></ul>

<p>There are also some active bindings:
</p>

<ul>
<li> <p><code style="white-space: pre;">&#8288;$num_columns&#8288;</code>
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$num_rows&#8288;</code>
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$schema&#8288;</code>
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$metadata&#8288;</code>: Returns the key-value metadata of the <code>Schema</code> as a named list.
Modify or replace by assigning in (<code>tab$metadata &lt;- new_metadata</code>).
All list elements are coerced to string. See <code>schema()</code> for more information.
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$columns&#8288;</code>: Returns a list of <code>ChunkedArray</code>s
</p>
</li></ul>


<hr>
<h2 id='to_arrow'>Create an Arrow object from a DuckDB connection</h2><span id='topic+to_arrow'></span>

<h3>Description</h3>

<p>This can be used in pipelines that pass data back and forth between Arrow and DuckDB
</p>


<h3>Usage</h3>

<pre><code class='language-R'>to_arrow(.data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="to_arrow_+3A_.data">.data</code></td>
<td>
<p>the object to be converted</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>RecordBatchReader</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(dplyr)

ds &lt;- InMemoryDataset$create(mtcars)

ds %&gt;%
  filter(mpg &lt; 30) %&gt;%
  to_duckdb() %&gt;%
  group_by(cyl) %&gt;%
  summarize(mean_mpg = mean(mpg, na.rm = TRUE)) %&gt;%
  to_arrow() %&gt;%
  collect()

</code></pre>

<hr>
<h2 id='to_duckdb'>Create a (virtual) DuckDB table from an Arrow object</h2><span id='topic+to_duckdb'></span>

<h3>Description</h3>

<p>This will do the necessary configuration to create a (virtual) table in DuckDB
that is backed by the Arrow object given. No data is copied or modified until
<code>collect()</code> or <code>compute()</code> are called or a query is run against the table.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>to_duckdb(
  .data,
  con = arrow_duck_connection(),
  table_name = unique_arrow_tablename(),
  auto_disconnect = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="to_duckdb_+3A_.data">.data</code></td>
<td>
<p>the Arrow object (e.g. Dataset, Table) to use for the DuckDB table</p>
</td></tr>
<tr><td><code id="to_duckdb_+3A_con">con</code></td>
<td>
<p>a DuckDB connection to use (default will create one and store it
in <code>options("arrow_duck_con")</code>)</p>
</td></tr>
<tr><td><code id="to_duckdb_+3A_table_name">table_name</code></td>
<td>
<p>a name to use in DuckDB for this object. The default is a
unique string <code>"arrow_"</code> followed by numbers.</p>
</td></tr>
<tr><td><code id="to_duckdb_+3A_auto_disconnect">auto_disconnect</code></td>
<td>
<p>should the table be automatically cleaned up when the
resulting object is removed (and garbage collected)? Default: <code>TRUE</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>The result is a dbplyr-compatible object that can be used in d(b)plyr pipelines.
</p>
<p>If <code>auto_disconnect = TRUE</code>, the DuckDB table that is created will be configured
to be unregistered when the <code>tbl</code> object is garbage collected. This is helpful
if you don't want to have extra table objects in DuckDB after you've finished
using them.
</p>


<h3>Value</h3>

<p>A <code>tbl</code> of the new table in DuckDB
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(dplyr)

ds &lt;- InMemoryDataset$create(mtcars)

ds %&gt;%
  filter(mpg &lt; 30) %&gt;%
  group_by(cyl) %&gt;%
  to_duckdb() %&gt;%
  slice_min(disp)

</code></pre>

<hr>
<h2 id='unify_schemas'>Combine and harmonize schemas</h2><span id='topic+unify_schemas'></span>

<h3>Description</h3>

<p>Combine and harmonize schemas
</p>


<h3>Usage</h3>

<pre><code class='language-R'>unify_schemas(..., schemas = list(...))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="unify_schemas_+3A_...">...</code></td>
<td>
<p><a href="#topic+Schema">Schema</a>s to unify</p>
</td></tr>
<tr><td><code id="unify_schemas_+3A_schemas">schemas</code></td>
<td>
<p>Alternatively, a list of schemas</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>Schema</code> with the union of fields contained in the inputs, or
<code>NULL</code> if any of <code>schemas</code> is <code>NULL</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>a &lt;- schema(b = double(), c = bool())
z &lt;- schema(b = double(), k = utf8())
unify_schemas(a, z)
</code></pre>

<hr>
<h2 id='value_counts'><code>table</code> for Arrow objects</h2><span id='topic+value_counts'></span>

<h3>Description</h3>

<p>This function tabulates the values in the array and returns a table of counts.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>value_counts(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="value_counts_+3A_x">x</code></td>
<td>
<p><code>Array</code> or <code>ChunkedArray</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>StructArray</code> containing &quot;values&quot; (same type as <code>x</code>) and &quot;counts&quot;
<code>Int64</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>cyl_vals &lt;- Array$create(mtcars$cyl)
counts &lt;- value_counts(cyl_vals)
</code></pre>

<hr>
<h2 id='vctrs_extension_array'>Extension type for generic typed vectors</h2><span id='topic+vctrs_extension_array'></span><span id='topic+vctrs_extension_type'></span>

<h3>Description</h3>

<p>Most common R vector types are converted automatically to a suitable
Arrow <a href="#topic+data-type">data type</a> without the need for an extension type. For
vector types whose conversion is not suitably handled by default, you can
create a <code><a href="#topic+vctrs_extension_array">vctrs_extension_array()</a></code>, which passes <code><a href="vctrs.html#topic+vec_data">vctrs::vec_data()</a></code> to
<code>Array$create()</code> and calls <code><a href="vctrs.html#topic+vec_proxy">vctrs::vec_restore()</a></code> when the <a href="#topic+Array">Array</a> is
converted back into an R vector.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vctrs_extension_array(x, ptype = vctrs::vec_ptype(x), storage_type = NULL)

vctrs_extension_type(x, storage_type = infer_type(vctrs::vec_data(x)))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vctrs_extension_array_+3A_x">x</code></td>
<td>
<p>A vctr (i.e., <code><a href="vctrs.html#topic+vec_assert">vctrs::vec_is()</a></code> returns <code>TRUE</code>).</p>
</td></tr>
<tr><td><code id="vctrs_extension_array_+3A_ptype">ptype</code></td>
<td>
<p>A <code><a href="vctrs.html#topic+vec_ptype">vctrs::vec_ptype()</a></code>, which is usually a zero-length
version of the object with the appropriate attributes set. This value
will be serialized using <code><a href="base.html#topic+serialize">serialize()</a></code>, so it should not refer to any
R object that can't be saved/reloaded.</p>
</td></tr>
<tr><td><code id="vctrs_extension_array_+3A_storage_type">storage_type</code></td>
<td>
<p>The <a href="#topic+data-type">data type</a> of the underlying storage
array.</p>
</td></tr>
</table>


<h3>Value</h3>


<ul>
<li> <p><code>vctrs_extension_array()</code> returns an <a href="#topic+ExtensionArray">ExtensionArray</a> instance with a
<code>vctrs_extension_type()</code>.
</p>
</li>
<li> <p><code>vctrs_extension_type()</code> returns an <a href="#topic+ExtensionType">ExtensionType</a> instance for the
extension name &quot;arrow.r.vctrs&quot;.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>(array &lt;- vctrs_extension_array(as.POSIXlt("2022-01-02 03:45", tz = "UTC")))
array$type
as.vector(array)

temp_feather &lt;- tempfile()
write_feather(arrow_table(col = array), temp_feather)
read_feather(temp_feather)
unlink(temp_feather)
</code></pre>

<hr>
<h2 id='write_csv_arrow'>Write CSV file to disk</h2><span id='topic+write_csv_arrow'></span>

<h3>Description</h3>

<p>Write CSV file to disk
</p>


<h3>Usage</h3>

<pre><code class='language-R'>write_csv_arrow(
  x,
  sink,
  file = NULL,
  include_header = TRUE,
  col_names = NULL,
  batch_size = 1024L,
  na = "",
  write_options = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="write_csv_arrow_+3A_x">x</code></td>
<td>
<p><code>data.frame</code>, <a href="#topic+RecordBatch">RecordBatch</a>, or <a href="#topic+Table">Table</a></p>
</td></tr>
<tr><td><code id="write_csv_arrow_+3A_sink">sink</code></td>
<td>
<p>A string file path, URI, or <a href="#topic+OutputStream">OutputStream</a>, or path in a file
system (<code>SubTreeFileSystem</code>)</p>
</td></tr>
<tr><td><code id="write_csv_arrow_+3A_file">file</code></td>
<td>
<p>file name. Specify this or <code>sink</code>, not both.</p>
</td></tr>
<tr><td><code id="write_csv_arrow_+3A_include_header">include_header</code></td>
<td>
<p>Whether to write an initial header line with column names</p>
</td></tr>
<tr><td><code id="write_csv_arrow_+3A_col_names">col_names</code></td>
<td>
<p>identical to <code>include_header</code>. Specify this or
<code>include_headers</code>, not both.</p>
</td></tr>
<tr><td><code id="write_csv_arrow_+3A_batch_size">batch_size</code></td>
<td>
<p>Maximum number of rows processed at a time. Default is 1024.</p>
</td></tr>
<tr><td><code id="write_csv_arrow_+3A_na">na</code></td>
<td>
<p>value to write for NA values. Must not contain quote marks. Default
is <code>""</code>.</p>
</td></tr>
<tr><td><code id="write_csv_arrow_+3A_write_options">write_options</code></td>
<td>
<p>see <a href="#topic+csv_write_options">CSV write options</a></p>
</td></tr>
<tr><td><code id="write_csv_arrow_+3A_...">...</code></td>
<td>
<p>additional parameters</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The input <code>x</code>, invisibly. Note that if <code>sink</code> is an <a href="#topic+OutputStream">OutputStream</a>,
the stream will be left open.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tf &lt;- tempfile()
on.exit(unlink(tf))
write_csv_arrow(mtcars, tf)
</code></pre>

<hr>
<h2 id='write_dataset'>Write a dataset</h2><span id='topic+write_dataset'></span>

<h3>Description</h3>

<p>This function allows you to write a dataset. By writing to more efficient
binary storage formats, and by specifying relevant partitioning, you can
make it much faster to read and query.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>write_dataset(
  dataset,
  path,
  format = c("parquet", "feather", "arrow", "ipc", "csv", "tsv", "txt", "text"),
  partitioning = dplyr::group_vars(dataset),
  basename_template = paste0("part-{i}.", as.character(format)),
  hive_style = TRUE,
  existing_data_behavior = c("overwrite", "error", "delete_matching"),
  max_partitions = 1024L,
  max_open_files = 900L,
  max_rows_per_file = 0L,
  min_rows_per_group = 0L,
  max_rows_per_group = bitwShiftL(1, 20),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="write_dataset_+3A_dataset">dataset</code></td>
<td>
<p><a href="#topic+Dataset">Dataset</a>, <a href="#topic+RecordBatch">RecordBatch</a>, <a href="#topic+Table">Table</a>, <code>arrow_dplyr_query</code>, or
<code>data.frame</code>. If an <code>arrow_dplyr_query</code>, the query will be evaluated and
the result will be written. This means that you can <code>select()</code>, <code>filter()</code>, <code>mutate()</code>,
etc. to transform the data before it is written if you need to.</p>
</td></tr>
<tr><td><code id="write_dataset_+3A_path">path</code></td>
<td>
<p>string path, URI, or <code>SubTreeFileSystem</code> referencing a directory
to write to (directory will be created if it does not exist)</p>
</td></tr>
<tr><td><code id="write_dataset_+3A_format">format</code></td>
<td>
<p>a string identifier of the file format. Default is to use
&quot;parquet&quot; (see <a href="#topic+FileFormat">FileFormat</a>)</p>
</td></tr>
<tr><td><code id="write_dataset_+3A_partitioning">partitioning</code></td>
<td>
<p><code>Partitioning</code> or a character vector of columns to
use as partition keys (to be written as path segments). Default is to
use the current <code>group_by()</code> columns.</p>
</td></tr>
<tr><td><code id="write_dataset_+3A_basename_template">basename_template</code></td>
<td>
<p>string template for the names of files to be written.
Must contain <code>"{i}"</code>, which will be replaced with an autoincremented
integer to generate basenames of datafiles. For example, <code>"part-{i}.arrow"</code>
will yield <code style="white-space: pre;">&#8288;"part-0.arrow", ...&#8288;</code>.
If not specified, it defaults to <code>"part-{i}.&lt;default extension&gt;"</code>.</p>
</td></tr>
<tr><td><code id="write_dataset_+3A_hive_style">hive_style</code></td>
<td>
<p>logical: write partition segments as Hive-style
(<code>key1=value1/key2=value2/file.ext</code>) or as just bare values. Default is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="write_dataset_+3A_existing_data_behavior">existing_data_behavior</code></td>
<td>
<p>The behavior to use when there is already data
in the destination directory.  Must be one of &quot;overwrite&quot;, &quot;error&quot;, or
&quot;delete_matching&quot;.
</p>

<ul>
<li><p> &quot;overwrite&quot; (the default) then any new files created will overwrite
existing files
</p>
</li>
<li><p> &quot;error&quot; then the operation will fail if the destination directory is not
empty
</p>
</li>
<li><p> &quot;delete_matching&quot; then the writer will delete any existing partitions
if data is going to be written to those partitions and will leave alone
partitions which data is not written to.
</p>
</li></ul>
</td></tr>
<tr><td><code id="write_dataset_+3A_max_partitions">max_partitions</code></td>
<td>
<p>maximum number of partitions any batch may be
written into. Default is 1024L.</p>
</td></tr>
<tr><td><code id="write_dataset_+3A_max_open_files">max_open_files</code></td>
<td>
<p>maximum number of files that can be left opened
during a write operation. If greater than 0 then this will limit the
maximum number of files that can be left open. If an attempt is made to open
too many files then the least recently used file will be closed.
If this setting is set too low you may end up fragmenting your data
into many small files. The default is 900 which also allows some # of files to be
open by the scanner before hitting the default Linux limit of 1024.</p>
</td></tr>
<tr><td><code id="write_dataset_+3A_max_rows_per_file">max_rows_per_file</code></td>
<td>
<p>maximum number of rows per file.
If greater than 0 then this will limit how many rows are placed in any single file.
Default is 0L.</p>
</td></tr>
<tr><td><code id="write_dataset_+3A_min_rows_per_group">min_rows_per_group</code></td>
<td>
<p>write the row groups to the disk when this number of
rows have accumulated. Default is 0L.</p>
</td></tr>
<tr><td><code id="write_dataset_+3A_max_rows_per_group">max_rows_per_group</code></td>
<td>
<p>maximum rows allowed in a single
group and when this number of rows is exceeded, it is split and the next set
of rows is written to the next group. This value must be set such that it is
greater than <code>min_rows_per_group</code>. Default is 1024 * 1024.</p>
</td></tr>
<tr><td><code id="write_dataset_+3A_...">...</code></td>
<td>
<p>additional format-specific arguments. For available Parquet
options, see <code><a href="#topic+write_parquet">write_parquet()</a></code>. The available Feather options are:
</p>

<ul>
<li> <p><code>use_legacy_format</code> logical: write data formatted so that Arrow libraries
versions 0.14 and lower can read it. Default is <code>FALSE</code>. You can also
enable this by setting the environment variable <code>ARROW_PRE_0_15_IPC_FORMAT=1</code>.
</p>
</li>
<li> <p><code>metadata_version</code>: A string like &quot;V5&quot; or the equivalent integer indicating
the Arrow IPC MetadataVersion. Default (<code>NULL</code>) will use the latest version,
unless the environment variable <code>ARROW_PRE_1_0_METADATA_VERSION=1</code>, in
which case it will be V4.
</p>
</li>
<li> <p><code>codec</code>: A <a href="#topic+Codec">Codec</a> which will be used to compress body buffers of written
files. Default (NULL) will not compress body buffers.
</p>
</li>
<li> <p><code>null_fallback</code>: character to be used in place of missing values (<code>NA</code> or
<code>NULL</code>) when using Hive-style partitioning. See <code><a href="#topic+hive_partition">hive_partition()</a></code>.
</p>
</li></ul>
</td></tr>
</table>


<h3>Value</h3>

<p>The input <code>dataset</code>, invisibly
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# You can write datasets partitioned by the values in a column (here: "cyl").
# This creates a structure of the form cyl=X/part-Z.parquet.
one_level_tree &lt;- tempfile()
write_dataset(mtcars, one_level_tree, partitioning = "cyl")
list.files(one_level_tree, recursive = TRUE)

# You can also partition by the values in multiple columns
# (here: "cyl" and "gear").
# This creates a structure of the form cyl=X/gear=Y/part-Z.parquet.
two_levels_tree &lt;- tempfile()
write_dataset(mtcars, two_levels_tree, partitioning = c("cyl", "gear"))
list.files(two_levels_tree, recursive = TRUE)

# In the two previous examples we would have:
# X = {4,6,8}, the number of cylinders.
# Y = {3,4,5}, the number of forward gears.
# Z = {0,1,2}, the number of saved parts, starting from 0.

# You can obtain the same result as as the previous examples using arrow with
# a dplyr pipeline. This will be the same as two_levels_tree above, but the
# output directory will be different.
library(dplyr)
two_levels_tree_2 &lt;- tempfile()
mtcars %&gt;%
  group_by(cyl, gear) %&gt;%
  write_dataset(two_levels_tree_2)
list.files(two_levels_tree_2, recursive = TRUE)

# And you can also turn off the Hive-style directory naming where the column
# name is included with the values by using `hive_style = FALSE`.

# Write a structure X/Y/part-Z.parquet.
two_levels_tree_no_hive &lt;- tempfile()
mtcars %&gt;%
  group_by(cyl, gear) %&gt;%
  write_dataset(two_levels_tree_no_hive, hive_style = FALSE)
list.files(two_levels_tree_no_hive, recursive = TRUE)

</code></pre>

<hr>
<h2 id='write_delim_dataset'>Write a dataset into partitioned flat files.</h2><span id='topic+write_delim_dataset'></span><span id='topic+write_csv_dataset'></span><span id='topic+write_tsv_dataset'></span>

<h3>Description</h3>

<p>The <code style="white-space: pre;">&#8288;write_*_dataset()&#8288;</code> are a family of wrappers around <a href="#topic+write_dataset">write_dataset</a> to allow for easy switching
between functions for writing datasets.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>write_delim_dataset(
  dataset,
  path,
  partitioning = dplyr::group_vars(dataset),
  basename_template = "part-{i}.txt",
  hive_style = TRUE,
  existing_data_behavior = c("overwrite", "error", "delete_matching"),
  max_partitions = 1024L,
  max_open_files = 900L,
  max_rows_per_file = 0L,
  min_rows_per_group = 0L,
  max_rows_per_group = bitwShiftL(1, 20),
  col_names = TRUE,
  batch_size = 1024L,
  delim = ",",
  na = "",
  eol = "\n",
  quote = c("needed", "all", "none")
)

write_csv_dataset(
  dataset,
  path,
  partitioning = dplyr::group_vars(dataset),
  basename_template = "part-{i}.csv",
  hive_style = TRUE,
  existing_data_behavior = c("overwrite", "error", "delete_matching"),
  max_partitions = 1024L,
  max_open_files = 900L,
  max_rows_per_file = 0L,
  min_rows_per_group = 0L,
  max_rows_per_group = bitwShiftL(1, 20),
  col_names = TRUE,
  batch_size = 1024L,
  delim = ",",
  na = "",
  eol = "\n",
  quote = c("needed", "all", "none")
)

write_tsv_dataset(
  dataset,
  path,
  partitioning = dplyr::group_vars(dataset),
  basename_template = "part-{i}.tsv",
  hive_style = TRUE,
  existing_data_behavior = c("overwrite", "error", "delete_matching"),
  max_partitions = 1024L,
  max_open_files = 900L,
  max_rows_per_file = 0L,
  min_rows_per_group = 0L,
  max_rows_per_group = bitwShiftL(1, 20),
  col_names = TRUE,
  batch_size = 1024L,
  na = "",
  eol = "\n",
  quote = c("needed", "all", "none")
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="write_delim_dataset_+3A_dataset">dataset</code></td>
<td>
<p><a href="#topic+Dataset">Dataset</a>, <a href="#topic+RecordBatch">RecordBatch</a>, <a href="#topic+Table">Table</a>, <code>arrow_dplyr_query</code>, or
<code>data.frame</code>. If an <code>arrow_dplyr_query</code>, the query will be evaluated and
the result will be written. This means that you can <code>select()</code>, <code>filter()</code>, <code>mutate()</code>,
etc. to transform the data before it is written if you need to.</p>
</td></tr>
<tr><td><code id="write_delim_dataset_+3A_path">path</code></td>
<td>
<p>string path, URI, or <code>SubTreeFileSystem</code> referencing a directory
to write to (directory will be created if it does not exist)</p>
</td></tr>
<tr><td><code id="write_delim_dataset_+3A_partitioning">partitioning</code></td>
<td>
<p><code>Partitioning</code> or a character vector of columns to
use as partition keys (to be written as path segments). Default is to
use the current <code>group_by()</code> columns.</p>
</td></tr>
<tr><td><code id="write_delim_dataset_+3A_basename_template">basename_template</code></td>
<td>
<p>string template for the names of files to be written.
Must contain <code>"{i}"</code>, which will be replaced with an autoincremented
integer to generate basenames of datafiles. For example, <code>"part-{i}.arrow"</code>
will yield <code style="white-space: pre;">&#8288;"part-0.arrow", ...&#8288;</code>.
If not specified, it defaults to <code>"part-{i}.&lt;default extension&gt;"</code>.</p>
</td></tr>
<tr><td><code id="write_delim_dataset_+3A_hive_style">hive_style</code></td>
<td>
<p>logical: write partition segments as Hive-style
(<code>key1=value1/key2=value2/file.ext</code>) or as just bare values. Default is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="write_delim_dataset_+3A_existing_data_behavior">existing_data_behavior</code></td>
<td>
<p>The behavior to use when there is already data
in the destination directory.  Must be one of &quot;overwrite&quot;, &quot;error&quot;, or
&quot;delete_matching&quot;.
</p>

<ul>
<li><p> &quot;overwrite&quot; (the default) then any new files created will overwrite
existing files
</p>
</li>
<li><p> &quot;error&quot; then the operation will fail if the destination directory is not
empty
</p>
</li>
<li><p> &quot;delete_matching&quot; then the writer will delete any existing partitions
if data is going to be written to those partitions and will leave alone
partitions which data is not written to.
</p>
</li></ul>
</td></tr>
<tr><td><code id="write_delim_dataset_+3A_max_partitions">max_partitions</code></td>
<td>
<p>maximum number of partitions any batch may be
written into. Default is 1024L.</p>
</td></tr>
<tr><td><code id="write_delim_dataset_+3A_max_open_files">max_open_files</code></td>
<td>
<p>maximum number of files that can be left opened
during a write operation. If greater than 0 then this will limit the
maximum number of files that can be left open. If an attempt is made to open
too many files then the least recently used file will be closed.
If this setting is set too low you may end up fragmenting your data
into many small files. The default is 900 which also allows some # of files to be
open by the scanner before hitting the default Linux limit of 1024.</p>
</td></tr>
<tr><td><code id="write_delim_dataset_+3A_max_rows_per_file">max_rows_per_file</code></td>
<td>
<p>maximum number of rows per file.
If greater than 0 then this will limit how many rows are placed in any single file.
Default is 0L.</p>
</td></tr>
<tr><td><code id="write_delim_dataset_+3A_min_rows_per_group">min_rows_per_group</code></td>
<td>
<p>write the row groups to the disk when this number of
rows have accumulated. Default is 0L.</p>
</td></tr>
<tr><td><code id="write_delim_dataset_+3A_max_rows_per_group">max_rows_per_group</code></td>
<td>
<p>maximum rows allowed in a single
group and when this number of rows is exceeded, it is split and the next set
of rows is written to the next group. This value must be set such that it is
greater than <code>min_rows_per_group</code>. Default is 1024 * 1024.</p>
</td></tr>
<tr><td><code id="write_delim_dataset_+3A_col_names">col_names</code></td>
<td>
<p>Whether to write an initial header line with column names.</p>
</td></tr>
<tr><td><code id="write_delim_dataset_+3A_batch_size">batch_size</code></td>
<td>
<p>Maximum number of rows processed at a time. Default is 1024L.</p>
</td></tr>
<tr><td><code id="write_delim_dataset_+3A_delim">delim</code></td>
<td>
<p>Delimiter used to separate values. Defaults to <code>","</code> for <code>write_delim_dataset()</code> and
<code>write_csv_dataset()</code>, and <code style="white-space: pre;">&#8288;"\t&#8288;</code> for <code>write_tsv_dataset()</code>. Cannot be changed for <code>write_tsv_dataset()</code>.</p>
</td></tr>
<tr><td><code id="write_delim_dataset_+3A_na">na</code></td>
<td>
<p>a character vector of strings to interpret as missing values. Quotes are not allowed in this string.
The default is an empty string <code>""</code>.</p>
</td></tr>
<tr><td><code id="write_delim_dataset_+3A_eol">eol</code></td>
<td>
<p>the end of line character to use for ending rows. The default is <code>"\n"</code>.</p>
</td></tr>
<tr><td><code id="write_delim_dataset_+3A_quote">quote</code></td>
<td>
<p>How to handle fields which contain characters that need to be quoted.
</p>

<ul>
<li> <p><code>needed</code> - Enclose all strings and binary values in quotes which need them, because their CSV rendering can
contain quotes itself  (the default)
</p>
</li>
<li> <p><code>all</code> -   Enclose all valid values in quotes. Nulls are not quoted. May cause readers to
interpret all values as strings if schema is inferred.
</p>
</li>
<li> <p><code>none</code> -   Do not enclose any values in quotes. Prevents values from containing quotes (&quot;),
cell delimiters (,) or line endings (\r, \n), (following RFC4180). If values
contain these characters, an error is caused when attempting to write.
</p>
</li></ul>
</td></tr>
</table>


<h3>Value</h3>

<p>The input <code>dataset</code>, invisibly.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+write_dataset">write_dataset()</a></code>
</p>

<hr>
<h2 id='write_feather'>Write a Feather file (an Arrow IPC file)</h2><span id='topic+write_feather'></span><span id='topic+write_ipc_file'></span>

<h3>Description</h3>

<p>Feather provides binary columnar serialization for data frames.
It is designed to make reading and writing data frames efficient,
and to make sharing data across data analysis languages easy.
<code><a href="#topic+write_feather">write_feather()</a></code> can write both the Feather Version 1 (V1),
a legacy version available starting in 2016, and the Version 2 (V2),
which is the Apache Arrow IPC file format.
The default version is V2.
V1 files are distinct from Arrow IPC files and lack many features,
such as the ability to store all Arrow data tyeps, and compression support.
<code><a href="#topic+write_ipc_file">write_ipc_file()</a></code> can only write V2 files.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>write_feather(
  x,
  sink,
  version = 2,
  chunk_size = 65536L,
  compression = c("default", "lz4", "lz4_frame", "uncompressed", "zstd"),
  compression_level = NULL
)

write_ipc_file(
  x,
  sink,
  chunk_size = 65536L,
  compression = c("default", "lz4", "lz4_frame", "uncompressed", "zstd"),
  compression_level = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="write_feather_+3A_x">x</code></td>
<td>
<p><code>data.frame</code>, <a href="#topic+RecordBatch">RecordBatch</a>, or <a href="#topic+Table">Table</a></p>
</td></tr>
<tr><td><code id="write_feather_+3A_sink">sink</code></td>
<td>
<p>A string file path, URI, or <a href="#topic+OutputStream">OutputStream</a>, or path in a file
system (<code>SubTreeFileSystem</code>)</p>
</td></tr>
<tr><td><code id="write_feather_+3A_version">version</code></td>
<td>
<p>integer Feather file version, Version 1 or Version 2. Version 2 is the default.</p>
</td></tr>
<tr><td><code id="write_feather_+3A_chunk_size">chunk_size</code></td>
<td>
<p>For V2 files, the number of rows that each chunk of data
should have in the file. Use a smaller <code>chunk_size</code> when you need faster
random row access. Default is 64K. This option is not supported for V1.</p>
</td></tr>
<tr><td><code id="write_feather_+3A_compression">compression</code></td>
<td>
<p>Name of compression codec to use, if any. Default is
&quot;lz4&quot; if LZ4 is available in your build of the Arrow C++ library, otherwise
&quot;uncompressed&quot;. &quot;zstd&quot; is the other available codec and generally has better
compression ratios in exchange for slower read and write performance.
&quot;lz4&quot; is shorthand for the &quot;lz4_frame&quot; codec.
See <code><a href="#topic+codec_is_available">codec_is_available()</a></code> for details.
<code>TRUE</code> and <code>FALSE</code> can also be used in place of &quot;default&quot; and &quot;uncompressed&quot;.
This option is not supported for V1.</p>
</td></tr>
<tr><td><code id="write_feather_+3A_compression_level">compression_level</code></td>
<td>
<p>If <code>compression</code> is &quot;zstd&quot;, you may
specify an integer compression level. If omitted, the compression codec's
default compression level is used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The input <code>x</code>, invisibly. Note that if <code>sink</code> is an <a href="#topic+OutputStream">OutputStream</a>,
the stream will be left open.
</p>


<h3>See Also</h3>

<p><a href="#topic+RecordBatchWriter">RecordBatchWriter</a> for lower-level access to writing Arrow IPC data.
</p>
<p><a href="#topic+Schema">Schema</a> for information about schemas and metadata handling.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># We recommend the ".arrow" extension for Arrow IPC files (Feather V2).
tf1 &lt;- tempfile(fileext = ".feather")
tf2 &lt;- tempfile(fileext = ".arrow")
tf3 &lt;- tempfile(fileext = ".arrow")
on.exit({
  unlink(tf1)
  unlink(tf2)
  unlink(tf3)
})
write_feather(mtcars, tf1, version = 1)
write_feather(mtcars, tf2)
write_ipc_file(mtcars, tf3)
</code></pre>

<hr>
<h2 id='write_ipc_stream'>Write Arrow IPC stream format</h2><span id='topic+write_ipc_stream'></span>

<h3>Description</h3>

<p>Apache Arrow defines two formats for <a href="https://arrow.apache.org/docs/format/Columnar.html#serialization-and-interprocess-communication-ipc">serializing data for interprocess communication (IPC)</a>:
a &quot;stream&quot; format and a &quot;file&quot; format, known as Feather. <code>write_ipc_stream()</code>
and <code><a href="#topic+write_feather">write_feather()</a></code> write those formats, respectively.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>write_ipc_stream(x, sink, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="write_ipc_stream_+3A_x">x</code></td>
<td>
<p><code>data.frame</code>, <a href="#topic+RecordBatch">RecordBatch</a>, or <a href="#topic+Table">Table</a></p>
</td></tr>
<tr><td><code id="write_ipc_stream_+3A_sink">sink</code></td>
<td>
<p>A string file path, URI, or <a href="#topic+OutputStream">OutputStream</a>, or path in a file
system (<code>SubTreeFileSystem</code>)</p>
</td></tr>
<tr><td><code id="write_ipc_stream_+3A_...">...</code></td>
<td>
<p>extra parameters passed to <code>write_feather()</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>x</code>, invisibly.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+write_feather">write_feather()</a></code> for writing IPC files. <code><a href="#topic+write_to_raw">write_to_raw()</a></code> to
serialize data to a buffer.
<a href="#topic+RecordBatchWriter">RecordBatchWriter</a> for a lower-level interface.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tf &lt;- tempfile()
on.exit(unlink(tf))
write_ipc_stream(mtcars, tf)
</code></pre>

<hr>
<h2 id='write_parquet'>Write Parquet file to disk</h2><span id='topic+write_parquet'></span>

<h3>Description</h3>

<p><a href="https://parquet.apache.org/">Parquet</a> is a columnar storage file format.
This function enables you to write Parquet files from R.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>write_parquet(
  x,
  sink,
  chunk_size = NULL,
  version = "2.4",
  compression = default_parquet_compression(),
  compression_level = NULL,
  use_dictionary = NULL,
  write_statistics = NULL,
  data_page_size = NULL,
  use_deprecated_int96_timestamps = FALSE,
  coerce_timestamps = NULL,
  allow_truncated_timestamps = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="write_parquet_+3A_x">x</code></td>
<td>
<p><code>data.frame</code>, <a href="#topic+RecordBatch">RecordBatch</a>, or <a href="#topic+Table">Table</a></p>
</td></tr>
<tr><td><code id="write_parquet_+3A_sink">sink</code></td>
<td>
<p>A string file path, URI, or <a href="#topic+OutputStream">OutputStream</a>, or path in a file
system (<code>SubTreeFileSystem</code>)</p>
</td></tr>
<tr><td><code id="write_parquet_+3A_chunk_size">chunk_size</code></td>
<td>
<p>how many rows of data to write to disk at once. This
directly corresponds to how many rows will be in each row group in
parquet. If <code>NULL</code>, a best guess will be made for optimal size (based on
the number of columns and number of rows), though if the data has fewer
than 250 million cells (rows x cols), then the total number of rows is
used.</p>
</td></tr>
<tr><td><code id="write_parquet_+3A_version">version</code></td>
<td>
<p>parquet version: &quot;1.0&quot;, &quot;2.0&quot; (deprecated), &quot;2.4&quot; (default),
&quot;2.6&quot;, or &quot;latest&quot; (currently equivalent to 2.6). Numeric values are
coerced to character.</p>
</td></tr>
<tr><td><code id="write_parquet_+3A_compression">compression</code></td>
<td>
<p>compression algorithm. Default &quot;snappy&quot;. See details.</p>
</td></tr>
<tr><td><code id="write_parquet_+3A_compression_level">compression_level</code></td>
<td>
<p>compression level. Meaning depends on compression
algorithm</p>
</td></tr>
<tr><td><code id="write_parquet_+3A_use_dictionary">use_dictionary</code></td>
<td>
<p>logical: use dictionary encoding? Default <code>TRUE</code></p>
</td></tr>
<tr><td><code id="write_parquet_+3A_write_statistics">write_statistics</code></td>
<td>
<p>logical: include statistics? Default <code>TRUE</code></p>
</td></tr>
<tr><td><code id="write_parquet_+3A_data_page_size">data_page_size</code></td>
<td>
<p>Set a target threshold for the approximate encoded
size of data pages within a column chunk (in bytes). Default 1 MiB.</p>
</td></tr>
<tr><td><code id="write_parquet_+3A_use_deprecated_int96_timestamps">use_deprecated_int96_timestamps</code></td>
<td>
<p>logical: write timestamps to INT96
Parquet format, which has been deprecated? Default <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="write_parquet_+3A_coerce_timestamps">coerce_timestamps</code></td>
<td>
<p>Cast timestamps a particular resolution. Can be
<code>NULL</code>, &quot;ms&quot; or &quot;us&quot;. Default <code>NULL</code> (no casting)</p>
</td></tr>
<tr><td><code id="write_parquet_+3A_allow_truncated_timestamps">allow_truncated_timestamps</code></td>
<td>
<p>logical: Allow loss of data when coercing
timestamps to a particular resolution. E.g. if microsecond or nanosecond
data is lost when coercing to &quot;ms&quot;, do not raise an exception. Default
<code>FALSE</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Due to features of the format, Parquet files cannot be appended to.
If you want to use the Parquet format but also want the ability to extend
your dataset, you can write to additional Parquet files and then treat
the whole directory of files as a <a href="#topic+Dataset">Dataset</a> you can query.
See the <a href="https://arrow.apache.org/docs/r/articles/dataset.html">dataset
article</a> for examples of this.
</p>
<p>The parameters <code>compression</code>, <code>compression_level</code>, <code>use_dictionary</code> and
<code>write_statistics</code> support various patterns:
</p>

<ul>
<li><p> The default <code>NULL</code> leaves the parameter unspecified, and the C++ library
uses an appropriate default for each column (defaults listed above)
</p>
</li>
<li><p> A single, unnamed, value (e.g. a single string for <code>compression</code>) applies to all columns
</p>
</li>
<li><p> An unnamed vector, of the same size as the number of columns, to specify a
value for each column, in positional order
</p>
</li>
<li><p> A named vector, to specify the value for the named columns, the default
value for the setting is used when not supplied
</p>
</li></ul>

<p>The <code>compression</code> argument can be any of the following (case-insensitive):
&quot;uncompressed&quot;, &quot;snappy&quot;, &quot;gzip&quot;, &quot;brotli&quot;, &quot;zstd&quot;, &quot;lz4&quot;, &quot;lzo&quot; or &quot;bz2&quot;.
Only &quot;uncompressed&quot; is guaranteed to be available, but &quot;snappy&quot; and &quot;gzip&quot;
are almost always included. See <code><a href="#topic+codec_is_available">codec_is_available()</a></code>.
The default &quot;snappy&quot; is used if available, otherwise &quot;uncompressed&quot;. To
disable compression, set <code>compression = "uncompressed"</code>.
Note that &quot;uncompressed&quot; columns may still have dictionary encoding.
</p>


<h3>Value</h3>

<p>the input <code>x</code> invisibly.
</p>


<h3>See Also</h3>

<p><a href="#topic+ParquetFileWriter">ParquetFileWriter</a> for a lower-level interface to Parquet writing.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
tf1 &lt;- tempfile(fileext = ".parquet")
write_parquet(data.frame(x = 1:5), tf1)

# using compression
if (codec_is_available("gzip")) {
  tf2 &lt;- tempfile(fileext = ".gz.parquet")
  write_parquet(data.frame(x = 1:5), tf2, compression = "gzip", compression_level = 5)
}

</code></pre>

<hr>
<h2 id='write_to_raw'>Write Arrow data to a raw vector</h2><span id='topic+write_to_raw'></span>

<h3>Description</h3>

<p><code><a href="#topic+write_ipc_stream">write_ipc_stream()</a></code> and <code><a href="#topic+write_feather">write_feather()</a></code> write data to a sink and return
the data (<code>data.frame</code>, <code>RecordBatch</code>, or <code>Table</code>) they were given.
This function wraps those so that you can serialize data to a buffer and
access that buffer as a <code>raw</code> vector in R.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>write_to_raw(x, format = c("stream", "file"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="write_to_raw_+3A_x">x</code></td>
<td>
<p><code>data.frame</code>, <a href="#topic+RecordBatch">RecordBatch</a>, or <a href="#topic+Table">Table</a></p>
</td></tr>
<tr><td><code id="write_to_raw_+3A_format">format</code></td>
<td>
<p>one of <code>c("stream", "file")</code>, indicating the IPC format to use</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>raw</code> vector containing the bytes of the IPC serialized data.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># The default format is "stream"
mtcars_raw &lt;- write_to_raw(mtcars)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
