<!DOCTYPE html><html><head><title>Help for package numDeriv</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {numDeriv}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#00.numDeriv.Intro'><p>Accurate Numerical Derivatives</p></a></li>
<li><a href='#genD'><p>Generate Bates and Watts D Matrix</p></a></li>
<li><a href='#grad'><p>Numerical Gradient of a Function</p></a></li>
<li><a href='#hessian'><p>Calculate Hessian Matrix</p></a></li>
<li><a href='#jacobian'><p>Gradient of a Vector Valued Function</p></a></li>
<li><a href='#numDeriv-package'><p>Accurate Numerical Derivatives</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Version:</td>
<td>2016.8-1.1</td>
</tr>
<tr>
<td>Title:</td>
<td>Accurate Numerical Derivatives</td>
</tr>
<tr>
<td>Description:</td>
<td>Methods for calculating (usually) accurate
	numerical first and second order derivatives. Accurate calculations 
	are done using 'Richardson&rdquo;s' extrapolation or, when applicable, a
	complex step derivative is available. A simple difference 
	method is also provided. Simple difference is (usually) less accurate
	but is much quicker than 'Richardson&rdquo;s' extrapolation and provides a 
	useful cross-check. 
	Methods are provided for real scalar and vector valued functions. </td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.11.1)</td>
</tr>
<tr>
<td>LazyLoad:</td>
<td>yes</td>
</tr>
<tr>
<td>ByteCompile:</td>
<td>yes</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>Copyright:</td>
<td>2006-2011, Bank of Canada. 2012-2016, Paul Gilbert</td>
</tr>
<tr>
<td>Author:</td>
<td>Paul Gilbert and Ravi Varadhan</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Paul Gilbert &lt;pgilbert.ttv9z@ncf.ca&gt;</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="http://optimizer.r-forge.r-project.org/">http://optimizer.r-forge.r-project.org/</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2019-06-04 11:04:44 UTC; hornik</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2019-06-06 09:51:09 UTC</td>
</tr>
</table>
<hr>
<h2 id='00.numDeriv.Intro'>Accurate Numerical Derivatives</h2><span id='topic+00.numDeriv.Intro'></span>

<h3>Description</h3>

<p>Calculate (accurate) numerical approximations to derivatives.</p>


<h3>Details</h3>

<p> See <code><a href="#topic+numDeriv-package">numDeriv-package</a></code> ( in the help system use
package?numDeriv or ?&quot;numDeriv-package&quot;) for an overview.
</p>

<hr>
<h2 id='genD'>Generate Bates and Watts D Matrix</h2><span id='topic+genD'></span><span id='topic+genD.default'></span>

<h3>Description</h3>

<p>Generate a matrix of function derivative information.</p>


<h3>Usage</h3>

<pre><code class='language-R'>    genD(func, x, method="Richardson",
                   method.args=list(), ...)
    ## Default S3 method:
genD(func, x, method="Richardson",
      method.args=list(), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="genD_+3A_func">func</code></td>
<td>
<p>a function for which the first (vector) argument 
is used as a parameter vector.</p>
</td></tr>
<tr><td><code id="genD_+3A_x">x</code></td>
<td>
<p>The parameter vector first argument to <code>func</code>.</p>
</td></tr>
<tr><td><code id="genD_+3A_method">method</code></td>
<td>
<p>one of <code>"Richardson"</code> or <code>"simple"</code> indicating 
the method to use for the aproximation.</p>
</td></tr>
<tr><td><code id="genD_+3A_method.args">method.args</code></td>
<td>
<p>arguments passed to method.  See <code><a href="#topic+grad">grad</a></code>. 
(Arguments not specified remain with their default values.)</p>
</td></tr>
<tr><td><code id="genD_+3A_...">...</code></td>
<td>
<p>any additional arguments passed to <code>func</code>.
WARNING: None of these should have names matching other arguments of this function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The derivatives are calculated numerically using Richardson improvement.
Methods &quot;simple&quot; and &quot;complex&quot; are not supported in this function.
The &quot;Richardson&quot; method calculates a numerical approximation of the first 
and second derivatives of <code>func</code> at the point <code>x</code>. 
For a scalar valued function these are the gradient vector and 
Hessian matrix. (See <code><a href="#topic+grad">grad</a></code> and <code><a href="#topic+hessian">hessian</a></code>.)
For a vector valued function the first derivative is the Jacobian matrix 
(see <code><a href="#topic+jacobian">jacobian</a></code>). 
For the Richardson method 
<code>method.args=list(eps=1e-4, d=0.0001, zero.tol=sqrt(.Machine$double.eps/7e-7), 
   r=4, v=2)</code> is set as the default.
See <code><a href="#topic+grad">grad</a></code>
for more details on the Richardson's extrapolation parameters. 
</p>
<p>A simple approximation to the first order derivative with respect 
to <code class="reqn">x_i</code> is 
</p>
<p style="text-align: center;"><code class="reqn">f'_{i}(x) = &lt;f(x_{1},\dots,x_{i}+d,\dots,x_{n}) -
               f(x_{1},\dots,x_{i}-d,\dots,x_{n})&gt;/(2*d)</code>
</p>

<p>A simple approximation to the second order derivative with respect 
to <code class="reqn">x_i</code> is 
</p>
<p style="text-align: center;"><code class="reqn">f''_{i}(x) = &lt;f(x_{1},\dots,x_{i}+d,\dots,x_{n}) -
                   2 *f(x_{1},\dots,x_{n}) +
                    f(x_{1},\dots,x_{i}-d,\dots,x_{n})&gt;/(d^2) </code>
</p>
	    
<p>The second order derivative with respect to <code class="reqn">x_i, x_j</code> is 
</p>
<p style="text-align: center;"><code class="reqn">f''_{i,j}(x) = &lt;f(x_{1},\dots,x_{i}+d,\dots,x_{j}+d,\dots,x_{n}) -
                    2 *f(x_{1},\dots,x_{n}) + </code>
</p>

<p style="text-align: center;"><code class="reqn">f(x_{1},\dots,x_{i}-d,\dots,x_{j}-d,\dots,x_{n})&gt;/(2*d^2) -
		      (f''_{i}(x) + f''_{j}(x))/2 </code>
</p>

<p>Richardson's extrapolation is based on these formula with the <code>d</code> 
being reduced in the extrapolation iterations. In the code, <code>d</code> is
scaled to accommodate parameters of different magnitudes. 
</p>
<p><code>genD</code> does <code>1 + r (N^2 + N)</code> evaluations of the function
<code>f</code>, where <code>N</code> is the length of <code>x</code>.
</p>


<h3>Value</h3>

<p>A list with elements as follows:
<code>D</code> is a matrix of first and second order partial
derivatives organized in the same manner as Bates and 
Watts, the number of rows is equal to the length of the result of
<code>func</code>, the first p columns are the Jacobian, and the 
next p(p+1)/2 columns are the lower triangle of the second derivative
(which is the Hessian for a scalar valued <code>func</code>).
<code>p</code> is the length of <code>x</code> (dimension of the parameter space).
<code>f0</code> is the function value at the point where the matrix <code>D</code> 
was calculated. 
The  <code>genD</code> arguments <code>func</code>, <code>x</code>, <code>d</code>, <code>method</code>,
and  <code>method.args</code> also are returned in the list.
</p>


<h3>References</h3>

 
<p>Linfield, G.R. and Penny, J.E.T. (1989) &quot;Microcomputers in Numerical Analysis.&quot;
Halsted Press.
</p>
<p>Bates, D.M. &amp; Watts, D. (1980), &quot;Relative Curvature Measures of Nonlinearity.&quot;
J. Royal Statistics Soc. series B, 42:1-25
</p>
<p>Bates, D.M. and Watts, D. (1988) &quot;Non-linear Regression Analysis and Its Applications.&quot;
Wiley.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+hessian">hessian</a></code>, 
<code><a href="#topic+grad">grad</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>    func &lt;- function(x){c(x[1], x[1], x[2]^2)}
    z &lt;- genD(func, c(2,2,5))
</code></pre>

<hr>
<h2 id='grad'>Numerical Gradient of a Function</h2><span id='topic+grad'></span><span id='topic+grad.default'></span>

<h3>Description</h3>

<p>Calculate the gradient of a function by numerical approximation.</p>


<h3>Usage</h3>

<pre><code class='language-R'>     grad(func, x, method="Richardson", side=NULL, method.args=list(), ...) 

    ## Default S3 method:
grad(func, x, method="Richardson", side=NULL,
      method.args=list(), ...)
 </code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="grad_+3A_func">func</code></td>
<td>
<p>a function with a scalar real result (see details).</p>
</td></tr>
<tr><td><code id="grad_+3A_x">x</code></td>
<td>
<p>a real scalar or vector argument to func, indicating the 
point(s) at which the gradient is to be calculated.</p>
</td></tr>
<tr><td><code id="grad_+3A_method">method</code></td>
<td>
<p>one of <code>"Richardson"</code>, <code>"simple"</code>, or 
<code>"complex"</code> indicating the method to use for the approximation.</p>
</td></tr>
<tr><td><code id="grad_+3A_method.args">method.args</code></td>
<td>
<p>arguments passed to method. Arguments not specified 
remain with their default values as specified in details</p>
</td></tr>
<tr><td><code id="grad_+3A_side">side</code></td>
<td>
<p>an indication of whether one-sided derivatives should be
attempted (see details).</p>
</td></tr>
<tr><td><code id="grad_+3A_...">...</code></td>
<td>
<p>an additional arguments passed to <code>func</code>.
WARNING: None of these should have names matching other arguments of this function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function <code>grad</code> calculates a numerical approximation of the 
first derivative of <code>func</code> at the point <code>x</code>. Any additional 
arguments in ... are also passed to <code>func</code>, but the gradient is not
calculated with respect to these additional arguments.
It is assumed <code>func</code>  is a scalar value function. If a vector <code>x</code> 
produces a scalar
result then <code>grad</code> returns the numerical approximation of the gradient
at the point <code>x</code> (which has the same length as <code>x</code>).
If a vector <code>x</code> produces a vector result then the result must have the
same length as <code>x</code>, and it is assumed that this corresponds to applying
the function to each of its arguments (for example, <code>sin(x)</code>). 
In this case <code>grad</code> returns the
gradient at each of the points in <code>x</code> (which also has the same length 
as <code>x</code> &ndash; so be careful). An alternative for vector valued functions is
provided by <code><a href="#topic+jacobian">jacobian</a></code>.
</p>
<p>If method is &quot;simple&quot;, the calculation is done using a simple epsilon
difference. 
For method &quot;simple&quot; <code>method.args=list(eps=1e-4)</code> is the
default. Only <code>eps</code> is used by this method.
</p>
<p>If method is &quot;complex&quot;, the calculation is done using the complex step
derivative approach of Lyness and Moler, described in  Squire and Trapp. 
This method requires that the function be able to handle complex valued 
arguments and return the appropriate complex valued result, 
even though the user may only be interested in the real-valued derivatives. 
It also requires that the complex function be analytic. (This might be thought 
of as the complex equivalent of the requirement for continuity and smoothness 
of a real valued function.) 
So, while this method is extremely powerful it is applicable to
a very restricted class of functions. <em>Avoid this method if you do not 
know that your function is suitable. Your mistake may not be caught and the
results will be spurious.</em>
For cases where it can be used,
it is faster than Richardson's extrapolation, and
it also provides gradients that are correct to machine precision (16 digits).   
For method &quot;complex&quot;, <code>method.args</code> is ignored.
The algorithm uses an <code>eps</code> of <code>.Machine$double.eps</code> which cannot
(and should not) be modified. 
</p>
<p>If method is &quot;Richardson&quot;, the calculation
is done by Richardson's extrapolation (see e.g. Linfield and Penny, 1989,
or Fornberg and Sloan, 1994.)
This method should be used if accuracy, as opposed to speed, is important
(but see method &quot;complex&quot; above). 
For this method 
<code>method.args=list(eps=1e-4, d=0.0001, zero.tol=sqrt(.Machine$double.eps/7e-7), 
   r=4, v=2, show.details=FALSE)</code> is set as the default.
<code>d</code> gives the fraction of <code>x</code> to use for the initial numerical 
approximation. The default means the initial approximation uses
<code>0.0001 * x</code>.
<code>eps</code> is used instead of <code>d</code> for elements of <code>x</code> which are 
zero (absolute value less than zero.tol).
<code>zero.tol</code> tolerance used for deciding which elements of <code>x</code> are 
zero.
<code>r</code> gives the number of Richardson improvement iterations (repetitions
with successly smaller <code>d</code>. The default <code>4</code> general provides 
good results, but this can be increased to <code>6</code> for improved
accuracy at the cost of more evaluations.
<code>v</code> gives the reduction factor.
<code>show.details</code> is a logical indicating if detailed calculations should 
be shown.
</p>
<p>The general approach in the Richardson method is to iterate for <code>r</code> 
iterations from initial 
values for interval value <code>d</code>,  using reduced factor <code>v</code>.
The the first order approximation to the derivative with respect 
to <code class="reqn">x_{i}</code> is
</p>
<p style="text-align: center;"><code class="reqn">f'_{i}(x) = &lt;f(x_{1},\dots,x_{i}+d,\dots,x_{n}) -
               f(x_{1},\dots,x_{i}-d,\dots,x_{n})&gt;/(2*d)</code>
</p>

<p>This is repeated <code>r</code> times  with successively smaller <code>d</code>  and 
then Richardson extraplolation is applied.
</p>
<p>If elements of <code>x</code> are near zero the multiplicative interval calculation
using <code>d</code> does not work, and for these elements an additive calculation
using <code>eps</code> is done instead. The argument <code>zero.tol</code> is used
determine if an element should be considered too close to zero. 
In the iterations, interval is successively reduced to eventual 
be <code>d/v^r</code> and the square of this value is used in second derivative 
calculations (see <code><a href="#topic+genD">genD</a></code>) so the 
default <code>zero.tol=sqrt(.Machine$double.eps/7e-7)</code> is set to ensure the
interval is bigger than <code>.Machine$double.eps</code> with the default <code>d</code>,
<code>r</code>, and <code>v</code>.
</p>
<p>If <code>side</code> is <code>NULL</code> then it is assumed that the point at which the
calculation is being done is interior to the domain of the function. If the
point is on the boundary of the domain then <code>side</code> can be used to 
indicate which side of the point <code>x</code> should be used for the calculation.
If not <code>NULL</code> then it should be a vector of the same length as <code>x</code>
and have values <code>NA</code>, <code>+1</code>, or <code>-1</code>. <code>NA</code> indicates that
the usual calculation will be done, while <code>+1</code>, or <code>-1</code> indicate
adding or subtracting from the parameter point <code>x</code>. The argument
<code>side</code> is not supported for all methods.
</p>
<p>Since usual calculation with method &quot;simple&quot; uses only a small <code>eps</code> 
step to one side, the only effect of argument <code>side</code> is to determine the
direction of the step. The usual calculation with method &quot;Richardson&quot; is 
symmetric, using steps to both sides. The effect of argument <code>side</code> 
is to take a double sized step to one side, and no step to the other side.
This means that the center of the Richardson extrapolation steps is moving
slightly in the reduction, and is not exactly on the boundary. 
(Warning: I am not aware of theory or published
experimental evidence to support this, but the results in my limited testing
seem good.)
</p>


<h3>Value</h3>

<p>A real scalar or vector of the approximated gradient(s).</p>


<h3>References</h3>

 
<p>Linfield, G. R. and Penny, J. E. T. (1989) <em>Microcomputers in Numerical 
Analysis</em>. New York: Halsted Press.
</p>
<p>Fornberg, B. and Sloan, D, M. (1994) &ldquo;A review of pseudospectral methods 
for solving partial differential equations.&rdquo; <em>Acta Numerica</em>, 3, 203-267.
</p>
<p>Lyness, J. N. and Moler, C. B. (1967) &ldquo;Numerical Differentiation of Analytic 
Functions.&rdquo; <em>SIAM Journal for Numerical Analysis</em>,
4(2), 202-210.
</p>
<p>Squire, William and Trapp, George (1998) &ldquo;Using Complex Variables to Estimate
Derivatives of Real Functions.&rdquo; <em>SIAM Rev</em>,
40(1), 110-112.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+jacobian">jacobian</a></code>,
<code><a href="#topic+hessian">hessian</a></code>,
<code><a href="#topic+genD">genD</a></code>,
<code><a href="stats.html#topic+numericDeriv">numericDeriv</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  grad(sin, pi)
  grad(sin, (0:10)*2*pi/10)
  func0 &lt;- function(x){ sum(sin(x))  }
  grad(func0 , (0:10)*2*pi/10)

  func1 &lt;- function(x){ sin(10*x) - exp(-x) }

  curve(func1,from=0,to=5)

  x &lt;- 2.04
  numd1 &lt;- grad(func1, x)
  exact &lt;- 10*cos(10*x) + exp(-x)
  c(numd1, exact, (numd1 - exact)/exact)

  x &lt;- c(1:10)
  numd1 &lt;- grad(func1, x)
  numd2 &lt;- grad(func1, x, "complex")
  exact &lt;- 10*cos(10*x) + exp(-x)
  cbind(numd1, numd2, exact, (numd1 - exact)/exact, (numd2 - exact)/exact)

  sc2.f &lt;- function(x){
    n &lt;- length(x)
    sum((1:n) * (exp(x) - x)) / n
    }

  sc2.g &lt;- function(x){
    n &lt;- length(x)
    (1:n) * (exp(x) - 1) / n
    }

  x0 &lt;- rnorm(100)
  exact &lt;- sc2.g(x0)

  g &lt;- grad(func=sc2.f, x=x0)
  max(abs(exact - g)/(1 + abs(exact)))

  gc &lt;- grad(func=sc2.f, x=x0, method="complex")
  max(abs(exact - gc)/(1 + abs(exact)))

  f &lt;- function(x) if(x[1]&lt;=0) sum(sin(x)) else  NA
  grad(f, x=c(0,0), method="Richardson", side=c(-1,  1))
</code></pre>

<hr>
<h2 id='hessian'>Calculate Hessian Matrix</h2><span id='topic+hessian'></span><span id='topic+hessian.default'></span>

<h3>Description</h3>

<p>Calculate a numerical approximation to the Hessian matrix of a 
function at a parameter value.</p>


<h3>Usage</h3>

<pre><code class='language-R'>    hessian(func, x, method="Richardson", method.args=list(), ...)

    ## Default S3 method:
hessian(func, x, method="Richardson",
        method.args=list(), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hessian_+3A_func">func</code></td>
<td>
<p>a function for which the first (vector) argument 
is used as a parameter vector.</p>
</td></tr>
<tr><td><code id="hessian_+3A_x">x</code></td>
<td>
<p>the parameter vector first argument to func.</p>
</td></tr>
<tr><td><code id="hessian_+3A_method">method</code></td>
<td>
<p>one of <code>"Richardson"</code> or <code>"complex"</code> indicating 
the method to use for the approximation.</p>
</td></tr>
<tr><td><code id="hessian_+3A_method.args">method.args</code></td>
<td>
<p>arguments passed to method.  See <code><a href="#topic+grad">grad</a></code>. 
(Arguments not specified remain with their default values.)</p>
</td></tr>
<tr><td><code id="hessian_+3A_...">...</code></td>
<td>
<p>an additional arguments passed to <code>func</code>.
WARNING: None of these should have names matching other arguments of this function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function <code>hessian</code> calculates an numerical approximation to 
the n x n second derivative of a scalar real valued function with n-vector
argument. 
</p>
<p>The argument <code>method</code> can be <code>"Richardson"</code> or <code>"complex"</code>.
Method <code>"simple"</code> is not supported. 
</p>
<p>For method <code>"complex"</code> the Hessian matrix is calculated as the Jacobian
of the gradient. The function <code>grad</code> with method &quot;complex&quot; is used, 
and <code>method.args</code> is ignored for this (an <code>eps</code> of 
<code>.Machine$double.eps</code> is used). 
However,  <code>jacobian</code> is used in the second step, with method 
<code>"Richardson"</code> and argument <code>method.args</code> is used for this. 
The default is
<code>method.args=list(eps=1e-4, d=0.1, zero.tol=sqrt(.Machine$double.eps/7e-7), 
   r=4, v=2, show.details=FALSE)</code>. (These are the defaults for <code>hessian</code> 
with method <code>"Richardson"</code>, which are slightly different from the defaults 
for <code>jacobian</code> with method <code>"Richardson"</code>.)
See addition comments in <code><a href="#topic+grad">grad</a></code> before choosing 
method <code>"complex"</code>.
</p>
<p>Methods <code>"Richardson"</code> uses <code><a href="#topic+genD">genD</a></code> and extracts the 
second derivative. For this method 
<code>method.args=list(eps=1e-4, d=0.1, zero.tol=sqrt(.Machine$double.eps/7e-7), 
   r=4, v=2, show.details=FALSE)</code> is set as the default. <code>hessian</code> does
one evaluation of <code>func</code> in order to do some error checking before
calling <code>genD</code>, so the number of function evaluations will be one more
than indicated for <code><a href="#topic+genD">genD</a></code>.
</p>
<p>The argument <code>side</code> is not supported for second derivatives and since
... are passed to <code>func</code> there may be no error message if it is
specified.
</p>


<h3>Value</h3>

<p>An n by n matrix of the Hessian of the function calculated at the 
point <code>x</code>.</p>


<h3>See Also</h3>

<p><code><a href="#topic+jacobian">jacobian</a></code>,
<code><a href="#topic+grad">grad</a></code>,
<code><a href="#topic+genD">genD</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  sc2.f &lt;- function(x){
    n &lt;- length(x)
    sum((1:n) * (exp(x) - x)) / n
    }

  sc2.g &lt;- function(x){
    n &lt;- length(x)
    (1:n) * (exp(x) - 1) / n
    }

  x0 &lt;- rnorm(5)
  hess &lt;- hessian(func=sc2.f, x=x0)
  hessc &lt;- hessian(func=sc2.f, x=x0, "complex")
  all.equal(hess, hessc, tolerance = .Machine$double.eps)
  
#  Hessian = Jacobian of the gradient
  jac  &lt;- jacobian(func=sc2.g, x=x0)
  jacc &lt;- jacobian(func=sc2.g, x=x0, "complex")
  all.equal(hess, jac, tolerance = .Machine$double.eps)
  all.equal(hessc, jacc, tolerance = .Machine$double.eps)
</code></pre>

<hr>
<h2 id='jacobian'>Gradient of a Vector Valued Function</h2><span id='topic+jacobian'></span><span id='topic+jacobian.default'></span>

<h3>Description</h3>

<p>Calculate the m by n numerical approximation of the gradient of a real
m-vector valued function with n-vector argument.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>    jacobian(func, x, method="Richardson", side=NULL, method.args=list(), ...) 

    ## Default S3 method:
jacobian(func, x, method="Richardson", side=NULL,
       method.args=list(), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="jacobian_+3A_func">func</code></td>
<td>
<p>a function with a real (vector) result.</p>
</td></tr>
<tr><td><code id="jacobian_+3A_x">x</code></td>
<td>
<p>a real or real vector argument to func, indicating the point 
at which the gradient is to be calculated.</p>
</td></tr>
<tr><td><code id="jacobian_+3A_method">method</code></td>
<td>
<p>one of <code>"Richardson"</code>, <code>"simple"</code>, or 
<code>"complex"</code> indicating the method to use for the approximation.</p>
</td></tr>
<tr><td><code id="jacobian_+3A_method.args">method.args</code></td>
<td>
<p>arguments passed to method. See <code><a href="#topic+grad">grad</a></code>. 
(Arguments not specified remain with their default values.)</p>
</td></tr>
<tr><td><code id="jacobian_+3A_...">...</code></td>
<td>
<p>any additional arguments passed to <code>func</code>.
WARNING: None of these should have names matching other arguments of this function.</p>
</td></tr>
<tr><td><code id="jacobian_+3A_side">side</code></td>
<td>
<p>an indication of whether one-sided derivatives should be
attempted (see details in function <code><a href="#topic+grad">grad</a></code>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For <code class="reqn">f:R^n -&gt; R^m</code> calculate the <code class="reqn">m x n</code> 
Jacobian <code class="reqn">dy/dx</code>.
The function <code>jacobian</code> calculates a numerical approximation of the 
first derivative of <code>func</code> at the point <code>x</code>. Any additional 
arguments in ... are also passed to <code>func</code>, but the gradient is not
calculated with respect to these additional arguments.
</p>
<p>If method is &quot;Richardson&quot;, the calculation is done by 
Richardson's extrapolation. See <code>link{grad}</code> for more details.
For this method  <code>method.args=list(eps=1e-4, d=0.0001, 
   zero.tol=sqrt(.Machine$double.eps/7e-7), r=4, v=2, show.details=FALSE)</code> 
is set as the default.
</p>
<p>If method is &quot;simple&quot;, the calculation is done using a simple epsilon
difference. 
For method &quot;simple&quot; <code>method.args=list(eps=1e-4)</code> is the
default. Only <code>eps</code> is used by this method.
</p>
<p>If method is &quot;complex&quot;, the calculation is done using the complex step
derivative approach. See addition comments in <code><a href="#topic+grad">grad</a></code> before
choosing this method.  
For method &quot;complex&quot;, <code>method.args</code> is ignored.
The algorithm uses an <code>eps</code> of <code>.Machine$double.eps</code> which cannot
(and should not) be modified. 
</p>


<h3>Value</h3>

<p>A real m by n matrix.</p>


<h3>See Also</h3>

<p><code><a href="#topic+grad">grad</a></code>,
<code><a href="#topic+hessian">hessian</a></code>,
<code><a href="stats.html#topic+numericDeriv">numericDeriv</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>   func2 &lt;- function(x) c(sin(x), cos(x))
   x &lt;- (0:1)*2*pi
   jacobian(func2, x)
   jacobian(func2, x, "complex")
</code></pre>

<hr>
<h2 id='numDeriv-package'>Accurate Numerical Derivatives</h2><span id='topic+numDeriv-package'></span><span id='topic+numDeriv.Intro'></span>

<h3>Description</h3>

<p>Calculate (accurate) numerical approximations to derivatives.</p>


<h3>Details</h3>

<p>The main functions are
</p>
<pre>
grad	  to calculate the gradient (first derivative) of a scalar 
  	  real valued function (possibly applied to all elements 
  	  of a vector argument).

jacobian  to calculate the gradient of a real m-vector valued
  	  function with real n-vector argument.

hessian   to calculate the Hessian (second derivative) of a scalar 
  	  real valued function with real n-vector argument.

genD	  to calculate the gradient and second derivative of a
  	  real m-vector valued function with real n-vector 
	  argument.
</pre>


<h3>Author(s)</h3>

<p>Paul Gilbert, based on work by Xingqiao Liu, and Ravi Varadhan (who wrote complex-step derivative codes)</p>


<h3>References</h3>

<p>Linfield, G. R. and Penny, J. E. T. (1989) <em>Microcomputers in Numerical 
Analysis</em>. New York: Halsted Press.
</p>
<p>Fornberg, B. and Sloan, D, M. (1994) &ldquo;A review of pseudospectral methods 
for solving partial differential equations.&rdquo; <em>Acta Numerica</em>, 3, 203-267.
</p>
<p>Lyness, J. N. and Moler, C. B. (1967) &ldquo;Numerical Differentiation of Analytic 
Functions.&rdquo; <em>SIAM Journal for Numerical Analysis</em>,
4(2), 202-210.
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
