<!DOCTYPE html><html><head><title>Help for package stochQN</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {stochQN}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#adaQN'><p>adaQN guided optimizer</p></a></li>
<li><a href='#adaQN_free'><p>adaQN Free-Mode Optimizer</p></a></li>
<li><a href='#coef.stoch_logistic'><p>Retrieve fitted coefficients from stochastic logistic regression object</p></a></li>
<li><a href='#get_curr_x'><p>Get current values of the optimization variables</p></a></li>
<li><a href='#get_iteration_number'><p>Get current iteration number from the optimizer object</p></a></li>
<li><a href='#oLBFGS'><p>oLBFGS guided optimizer</p></a></li>
<li><a href='#oLBFGS_free'><p>oLBFGS Free-Mode Optimizer</p></a></li>
<li><a href='#partial_fit'><p>Partial fit stochastic model to new data</p></a></li>
<li><a href='#partial_fit_logistic'><p>Update stochastic logistic regression model with new batch of data</p></a></li>
<li><a href='#predict.stoch_logistic'><p>Prediction function for stochastic logistic regression</p></a></li>
<li><a href='#predict.stochQN_guided'><p>Predict function for stochastic optimizer object</p></a></li>
<li><a href='#print.adaQN'><p>Print summary info about adaQN guided-mode object</p></a></li>
<li><a href='#print.adaQN_free'><p>Print summary info about adaQN free-mode object</p></a></li>
<li><a href='#print.oLBFGS'><p>Print summary info about oLBFGS guided-mode object</p></a></li>
<li><a href='#print.oLBFGS_free'><p>Print summary info about oLBFGS free-mode object</p></a></li>
<li><a href='#print.SQN'><p>Print summary info about SQN guided-mode object</p></a></li>
<li><a href='#print.SQN_free'><p>Print summary info about SQN free-mode object</p></a></li>
<li><a href='#print.stoch_logistic'><p>Print general info about stochastic logistic regression object</p></a></li>
<li><a href='#run_adaQN_free'><p>Run adaQN optimizer in free-mode</p></a></li>
<li><a href='#run_oLBFGS_free'><p>Run oLBFGS optimizer in free-mode</p></a></li>
<li><a href='#run_SQN_free'><p>Run SQN optimizer in free-mode</p></a></li>
<li><a href='#SQN'><p>SQN guided optimizer</p></a></li>
<li><a href='#SQN_free'><p>SQN Free-Mode Optimizer</p></a></li>
<li><a href='#stochastic.logistic.regression'><p>Stochastic Logistic Regression</p></a></li>
<li><a href='#summary.stoch_logistic'><p>Print general info about stochastic logistic regression object</p></a></li>
<li><a href='#update_fun'><p>Update objective function value (adaQN)</p></a></li>
<li><a href='#update_gradient'><p>Update gradient (oLBFGS, SQN, adaQN)</p></a></li>
<li><a href='#update_hess_vec'><p>Update Hessian-vector product (SQN)</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Stochastic Limited Memory Quasi-Newton Optimizers</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.2-1</td>
</tr>
<tr>
<td>Author:</td>
<td>David Cortes</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>David Cortes &lt;david.cortes.rivera@gmail.com&gt;</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/david-cortes/stochQN">https://github.com/david-cortes/stochQN</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/david-cortes/stochQN/issues">https://github.com/david-cortes/stochQN/issues</a></td>
</tr>
<tr>
<td>Description:</td>
<td>Implementations of stochastic, limited-memory quasi-Newton optimizers,
	similar in spirit to the LBFGS (Limited-memory Broyden-Fletcher-Goldfarb-Shanno) algorithm,
	for smooth stochastic optimization. Implements the following methods:
	oLBFGS (online LBFGS) (Schraudolph, N.N., Yu, J. and Guenter, S., 2007 <a href="http://proceedings.mlr.press/v2/schraudolph07a.html">http://proceedings.mlr.press/v2/schraudolph07a.html</a>),
	SQN (stochastic quasi-Newton) (Byrd, R.H., Hansen, S.L., Nocedal, J. and Singer, Y., 2016 &lt;<a href="https://doi.org/10.48550/arXiv.1401.7020">doi:10.48550/arXiv.1401.7020</a>&gt;),
	adaQN (adaptive quasi-Newton) (Keskar, N.S., Berahas, A.S., 2016, &lt;<a href="https://doi.org/10.48550/arXiv.1511.01169">doi:10.48550/arXiv.1511.01169</a>&gt;).
	Provides functions for easily creating R objects
	with partial_fit/predict methods from some given objective/gradient/predict functions.
	Includes an example stochastic logistic regression using these optimizers.
	Provides header files and registered C routines for using it directly from C/C++.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/BSD-2-Clause">BSD_2_clause</a> + file LICENSE</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>6.1.1</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2021-09-25 23:55:07 UTC; david</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2021-09-26 04:10:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='adaQN'>adaQN guided optimizer</h2><span id='topic+adaQN'></span>

<h3>Description</h3>

<p>Optimizes an empirical (possibly non-convex) loss function over batches of sample data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>adaQN(x0, grad_fun, obj_fun = NULL, pred_fun = NULL,
  initial_step = 0.01, step_fun = function(iter) 1/sqrt((iter/100) +
  1), callback_iter = NULL, args_cb = NULL, verbose = TRUE,
  mem_size = 10, fisher_size = 100, bfgs_upd_freq = 20,
  max_incr = 1.01, min_curvature = 1e-04, y_reg = NULL,
  scal_reg = 1e-04, rmsprop_weight = 0.9, use_grad_diff = FALSE,
  check_nan = TRUE, nthreads = -1, X_val = NULL, y_val = NULL,
  w_val = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="adaQN_+3A_x0">x0</code></td>
<td>
<p>Initial values for the variables to optimize.</p>
</td></tr>
<tr><td><code id="adaQN_+3A_grad_fun">grad_fun</code></td>
<td>
<p>Function taking as unnamed arguments 'x_curr' (variable values), 'X' (covariates),
'y' (target variable), and 'w' (weights), plus additional arguments ('...'), and producing the expected
value of the gradient when evalauted on that data.</p>
</td></tr>
<tr><td><code id="adaQN_+3A_obj_fun">obj_fun</code></td>
<td>
<p>Function taking as unnamed arguments 'x_curr' (variable values), 'X' (covariates),
'y' (target variable), and 'w' (weights), plus additional arguments ('...'), and producing the expected
value of the objective function when evalauted on that data. Only required when using 'max_incr'.</p>
</td></tr>
<tr><td><code id="adaQN_+3A_pred_fun">pred_fun</code></td>
<td>
<p>Function taking an unnamed argument as data, another unnamed argument as the variable values,
and optional extra arguments ('...'). Will be called when using 'predict' on the object returned by this function.</p>
</td></tr>
<tr><td><code id="adaQN_+3A_initial_step">initial_step</code></td>
<td>
<p>Initial step size.</p>
</td></tr>
<tr><td><code id="adaQN_+3A_step_fun">step_fun</code></td>
<td>
<p>Function accepting the iteration number as an unnamed parameter, which will output the
number by which 'initial_step' will be multiplied at each iteration to get the step size for that
iteration.</p>
</td></tr>
<tr><td><code id="adaQN_+3A_callback_iter">callback_iter</code></td>
<td>
<p>Callback function which will be called at the end of each iteration.
Will pass three unnamed arguments: the current variable values, the current iteration number,
and 'args_cb'. Pass 'NULL' if there is no need to call a callback function.</p>
</td></tr>
<tr><td><code id="adaQN_+3A_args_cb">args_cb</code></td>
<td>
<p>Extra argument to pass to the callback function.</p>
</td></tr>
<tr><td><code id="adaQN_+3A_verbose">verbose</code></td>
<td>
<p>Whether to print information about iteration statuses when something goes wrong.</p>
</td></tr>
<tr><td><code id="adaQN_+3A_mem_size">mem_size</code></td>
<td>
<p>Number of correction pairs to store for approximation of Hessian-vector products.</p>
</td></tr>
<tr><td><code id="adaQN_+3A_fisher_size">fisher_size</code></td>
<td>
<p>Number of gradients to store for calculation of the empirical Fisher product with gradients.
If passing 'NULL', will force 'use_grad_diff' to 'TRUE'.</p>
</td></tr>
<tr><td><code id="adaQN_+3A_bfgs_upd_freq">bfgs_upd_freq</code></td>
<td>
<p>Number of iterations (batches) after which to generate a BFGS correction pair.</p>
</td></tr>
<tr><td><code id="adaQN_+3A_max_incr">max_incr</code></td>
<td>
<p>Maximum ratio of function values in the validation set under the average values of 'x' during current epoch
vs. previous epoch. If the ratio is above this threshold, the BFGS and Fisher memories will be reset, and 'x'
values reverted to their previous average.
If not using a validation set, will take a longer batch for function evaluations (same as used for gradients    
when using 'use_grad_diff' = 'TRUE').
Pass 'NULL' for no function-increase checking.</p>
</td></tr>
<tr><td><code id="adaQN_+3A_min_curvature">min_curvature</code></td>
<td>
<p>Minimum value of (s * y) / (s * s) in order to accept a correction pair. Pass 'NULL' for
no minimum.</p>
</td></tr>
<tr><td><code id="adaQN_+3A_y_reg">y_reg</code></td>
<td>
<p>Regularizer for 'y' vector (gets added y_reg * s). Pass 'NULL' for no regularization.</p>
</td></tr>
<tr><td><code id="adaQN_+3A_scal_reg">scal_reg</code></td>
<td>
<p>Regularization parameter to use in the denominator for AdaGrad and RMSProp scaling.</p>
</td></tr>
<tr><td><code id="adaQN_+3A_rmsprop_weight">rmsprop_weight</code></td>
<td>
<p>If not 'NULL', will use RMSProp formula instead of AdaGrad for approximated inverse-Hessian initialization.</p>
</td></tr>
<tr><td><code id="adaQN_+3A_use_grad_diff">use_grad_diff</code></td>
<td>
<p>Whether to create the correction pairs using differences between gradients instead of empirical Fisher matrix.
These gradients are calculated on a larger batch than the regular ones (given by batch_size * bfgs_upd_freq).</p>
</td></tr>
<tr><td><code id="adaQN_+3A_check_nan">check_nan</code></td>
<td>
<p>Whether to check for variables becoming NaN after each iteration, and reverting the step if they do
(will also reset BFGS memory).</p>
</td></tr>
<tr><td><code id="adaQN_+3A_nthreads">nthreads</code></td>
<td>
<p>Number of parallel threads to use. If set to -1, will determine the number of available threads and use
all of them. Note however that not all the computations can be parallelized, and the BLAS backend might use a different
number of threads.</p>
</td></tr>
<tr><td><code id="adaQN_+3A_x_val">X_val</code></td>
<td>
<p>Covariates to use as validation set (only used when passing 'max_incr'). If not passed, will use
a larger batch of stored data, in the same way as for Hessian-vector products in SQN.</p>
</td></tr>
<tr><td><code id="adaQN_+3A_y_val">y_val</code></td>
<td>
<p>Target variable for the covariates to use as validation set (only used when passing 'max_incr').
If not passed, will use a larger batch of stored data, in the same way as for Hessian-vector products in SQN.</p>
</td></tr>
<tr><td><code id="adaQN_+3A_w_val">w_val</code></td>
<td>
<p>Sample weights for the covariates to use as validation set (only used when passing 'max_incr').
If not passed, will use a larger batch of stored data, in the same way as for Hessian-vector products in SQN.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>an 'adaQN' object with the user-supplied functions, which can be fit to batches of data
through function 'partial_fit', and can produce predictions on new data through function 'predict'.
</p>


<h3>References</h3>

 <ul>
<li><p> Keskar, N.S. and Berahas, A.S., 2016, September.
&quot;adaQN: An Adaptive Quasi-Newton Algorithm for Training RNNs.&quot;
In Joint European Conference on Machine Learning and Knowledge Discovery in Databases (pp. 1-16). Springer, Cham.
</p>
</li>
<li><p> Wright, S. and Nocedal, J., 1999. &quot;Numerical optimization.&quot; (ch 7) Springer Science, 35(67-68), p.7.</p>
</li></ul>



<h3>See Also</h3>

<p><a href="#topic+partial_fit">partial_fit</a> , <a href="#topic+predict.stochQN_guided">predict.stochQN_guided</a> , <a href="#topic+adaQN_free">adaQN_free</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### Example regression with randomly-generated data
library(stochQN)

### Will sample data y ~ Ax + epsilon
true_coefs &lt;- c(1.12, 5.34, -6.123)

generate_data_batch &lt;- function(true_coefs, n = 100) {
  X &lt;- matrix(
    rnorm(length(true_coefs) * n),
    nrow=n, ncol=length(true_coefs))
  y &lt;- X %*% true_coefs + rnorm(n)
  return(list(X = X, y = y))
}

### Regular regression function that minimizes RMSE
eval_fun &lt;- function(coefs, X, y, weights=NULL, lambda=1e-5) {
  pred &lt;- as.numeric(X %*% coefs)
  RMSE &lt;- sqrt(mean((pred - y)^2))
  reg  &lt;- 2 * lambda * as.numeric(coefs %*% coefs)
  return(RMSE + reg)
}

eval_grad &lt;- function(coefs, X, y, weights=NULL, lambda=1e-5) {
  pred &lt;- X %*% coefs
  grad &lt;- colMeans(X * as.numeric(pred - y))
  grad &lt;- grad + 2 * lambda * as.numeric(coefs^2)
  return(grad)
}

pred_fun &lt;- function(X, coefs, ...) {
  return(as.numeric(X %*% coefs))
}

### Initialize optimizer form arbitrary values
x0 &lt;- c(1, 1, 1)
optimizer &lt;- adaQN(x0, grad_fun=eval_grad,
  pred_fun=pred_fun, obj_fun=eval_fun, initial_step=1e-0)
val_data &lt;- generate_data_batch(true_coefs, n=100)

### Fit to 50 batches of data, 100 observations each
for (i in 1:50) {
  set.seed(i)
  new_batch &lt;- generate_data_batch(true_coefs, n=100)
  partial_fit(
    optimizer,
    new_batch$X, new_batch$y,
    lambda=1e-5)
  x_curr &lt;- get_curr_x(optimizer)
  i_curr &lt;- get_iteration_number(optimizer)
  if ((i_curr %% 10)  == 0) {
    cat(sprintf(
      "Iteration %d - E[f(x)]: %f - values of x: [%f, %f, %f]\n",
      i_curr,
      eval_fun(x_curr, val_data$X, val_data$y, lambda=1e-5),
      x_curr[1], x_curr[2], x_curr[3]))
  }
}

### Predict for new data
new_batch &lt;- generate_data_batch(true_coefs, n=10)
yhat &lt;- predict(optimizer, new_batch$X)
</code></pre>

<hr>
<h2 id='adaQN_free'>adaQN Free-Mode Optimizer</h2><span id='topic+adaQN_free'></span>

<h3>Description</h3>

<p>Optimizes an empirical (perhaps non-convex) loss function over batches of sample data. Compared to
function/class 'adaQN', this version lets the user do all the calculations from the outside, only
interacting with the object by means of a function that returns a request type and is fed the
required calculation through methods 'update_gradient' and 'update_function'.
</p>
<p>Order in which requests are made:
</p>
<p>========== loop ===========
</p>
<p>* calc_grad
</p>
<p><code style="white-space: pre;">&#8288;    &#8288;</code>... (repeat calc_grad)
</p>
<p>if max_incr &gt; 0:
</p>
<p><code style="white-space: pre;">&#8288;    &#8288;</code>* calc_fun_val_batch
</p>
<p>if 'use_grad_diff':
</p>
<p><code style="white-space: pre;">&#8288;    &#8288;</code>* calc_grad_big_batch	(skipped if below max_incr)
</p>
<p>===========================
</p>
<p>After running this function, apply 'run_adaQN_free' to it to get the first requested piece of information.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>adaQN_free(mem_size = 10, fisher_size = 100, bfgs_upd_freq = 20,
  max_incr = 1.01, min_curvature = 1e-04, scal_reg = 1e-04,
  rmsprop_weight = 0.9, y_reg = NULL, use_grad_diff = FALSE,
  check_nan = TRUE, nthreads = -1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="adaQN_free_+3A_mem_size">mem_size</code></td>
<td>
<p>Number of correction pairs to store for approximation of Hessian-vector products.</p>
</td></tr>
<tr><td><code id="adaQN_free_+3A_fisher_size">fisher_size</code></td>
<td>
<p>Number of gradients to store for calculation of the empirical Fisher product with gradients.
If passing 'NULL', will force 'use_grad_diff' to 'TRUE'.</p>
</td></tr>
<tr><td><code id="adaQN_free_+3A_bfgs_upd_freq">bfgs_upd_freq</code></td>
<td>
<p>Number of iterations (batches) after which to generate a BFGS correction pair.</p>
</td></tr>
<tr><td><code id="adaQN_free_+3A_max_incr">max_incr</code></td>
<td>
<p>Maximum ratio of function values in the validation set under the average values of 'x' during current epoch
vs. previous epoch. If the ratio is above this threshold, the BFGS and Fisher memories will be reset, and 'x'
values reverted to their previous average.
Pass 'NULL' for no function-increase checking.</p>
</td></tr>
<tr><td><code id="adaQN_free_+3A_min_curvature">min_curvature</code></td>
<td>
<p>Minimum value of (s * y) / (s * s) in order to accept a correction pair. Pass 'NULL' for
no minimum.</p>
</td></tr>
<tr><td><code id="adaQN_free_+3A_scal_reg">scal_reg</code></td>
<td>
<p>Regularization parameter to use in the denominator for AdaGrad and RMSProp scaling.</p>
</td></tr>
<tr><td><code id="adaQN_free_+3A_rmsprop_weight">rmsprop_weight</code></td>
<td>
<p>If not 'NULL', will use RMSProp formula instead of AdaGrad for approximated inverse-Hessian initialization.</p>
</td></tr>
<tr><td><code id="adaQN_free_+3A_y_reg">y_reg</code></td>
<td>
<p>Regularizer for 'y' vector (gets added y_reg * s). Pass 'NULL' for no regularization.</p>
</td></tr>
<tr><td><code id="adaQN_free_+3A_use_grad_diff">use_grad_diff</code></td>
<td>
<p>Whether to create the correction pairs using differences between gradients instead of Fisher matrix.
These gradients are calculated on a larger batch than the regular ones (given by batch_size * bfgs_upd_freq).
If 'TRUE', empirical Fisher matrix will not be used.</p>
</td></tr>
<tr><td><code id="adaQN_free_+3A_check_nan">check_nan</code></td>
<td>
<p>Whether to check for variables becoming NaN after each iteration, and reverting the step if they do
(will also reset BFGS and Fisher memory).</p>
</td></tr>
<tr><td><code id="adaQN_free_+3A_nthreads">nthreads</code></td>
<td>
<p>Number of parallel threads to use. If set to -1, will determine the number of available threads and use
all of them. Note however that not all the computations can be parallelized, and the BLAS backend might use a different
number of threads.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An 'adaQN_free' object, which can be used through functions 'update_gradient', 'update_fun', and 'run_adaQN_free'
</p>


<h3>References</h3>

 <ul>
<li><p> Keskar, N.S. and Berahas, A.S., 2016, September.
&quot;adaQN: An Adaptive Quasi-Newton Algorithm for Training RNNs.&quot;
In Joint European Conference on Machine Learning and Knowledge Discovery in Databases (pp. 1-16). Springer, Cham.
</p>
</li>
<li><p> Wright, S. and Nocedal, J., 1999. &quot;Numerical optimization.&quot; (ch 7) Springer Science, 35(67-68), p.7.</p>
</li></ul>



<h3>See Also</h3>

<p><a href="#topic+update_gradient">update_gradient</a> , <a href="#topic+update_fun">update_fun</a> , <a href="#topic+run_adaQN_free">run_adaQN_free</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### Example optimizing Rosenbrock 2D function
### Note that this example is not stochastic, as the
### function is not evaluated in expectation based on
### batches of data, but rather it has a given absolute
### form that never varies.
library(stochQN)


fr &lt;- function(x) { ## Rosenbrock Banana function
	x1 &lt;- x[1]
	x2 &lt;- x[2]
	100 * (x2 - x1 * x1)^2 + (1 - x1)^2
}
grr &lt;- function(x) { ## Gradient of 'fr'
	x1 &lt;- x[1]
	x2 &lt;- x[2]
	c(-400 * x1 * (x2 - x1 * x1) - 2 * (1 - x1),
	  200 * (x2 - x1 * x1))
}


### Initial values of x
x_opt = as.numeric(c(0, 2))
cat(sprintf("Initial values of x: [%.3f, %.3f]\n",
			x_opt[1], x_opt[2]))

### Will use constant step size throughout
### (not recommended)
step_size = 1e-2

### Initialize the optimizer
optimizer = adaQN_free()

### Keep track of the iteration number
curr_iter &lt;- 0

### Run a loop for many iterations
### (Note that some iterations might require more
###  than 1 calculation request)
for (i in 1:200) {
	req &lt;- run_adaQN_free(optimizer, x_opt, step_size)
	if (req$task == "calc_grad") {
	  update_gradient(optimizer, grr(req$requested_on))
	} else if (req$task == "calc_fun_val_batch") {
	  update_fun(optimizer, fr(req$requested_on))
	}

	### Track progress every 10 iterations
	if (req$info$iteration_number &gt; curr_iter) {
		curr_iter &lt;- req$info$iteration_number
	}
	if ((curr_iter %% 10) == 0) {
		cat(sprintf(
		  "Iteration %3d - Current function value: %.3f\n",
		  req$info$iteration_number, fr(x_opt)))
	}
}
cat(sprintf("Current values of x: [%.3f, %.3f]\n",
			x_opt[1], x_opt[2]))
</code></pre>

<hr>
<h2 id='coef.stoch_logistic'>Retrieve fitted coefficients from stochastic logistic regression object</h2><span id='topic+coef.stoch_logistic'></span>

<h3>Description</h3>

<p>Retrieve fitted coefficients from stochastic logistic regression object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'stoch_logistic'
coef(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="coef.stoch_logistic_+3A_object">object</code></td>
<td>
<p>A 'stoch_logistic' object as output by function 'stochastic.logistic.regression'.
Must have already been fit to at least 1 batch of data.</p>
</td></tr>
<tr><td><code id="coef.stoch_logistic_+3A_...">...</code></td>
<td>
<p>Not used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An (n x 1) matrix with the coefficients, in the same format as those from 'glm'.
</p>


<h3>See Also</h3>

<p><a href="#topic+stochastic.logistic.regression">stochastic.logistic.regression</a>
</p>

<hr>
<h2 id='get_curr_x'>Get current values of the optimization variables</h2><span id='topic+get_curr_x'></span>

<h3>Description</h3>

<p>Get current values of the optimization variables
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_curr_x(optimizer)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_curr_x_+3A_optimizer">optimizer</code></td>
<td>
<p>An optimizer (guided-mode) from this module, as output by
functions 'oLBFGS', 'SQN', 'adaQN'.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numeric vector with the current values of the variables being optimized.
</p>


<h3>See Also</h3>

<p><a href="#topic+oLBFGS">oLBFGS</a> , <a href="#topic+SQN">SQN</a> , <a href="#topic+adaQN">adaQN</a>
</p>

<hr>
<h2 id='get_iteration_number'>Get current iteration number from the optimizer object</h2><span id='topic+get_iteration_number'></span>

<h3>Description</h3>

<p>Get current iteration number from the optimizer object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_iteration_number(optimizer)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_iteration_number_+3A_optimizer">optimizer</code></td>
<td>
<p>An optimizer (guided-mode) from this module, as output by
functions 'oLBFGS', 'SQN', 'adaQN'.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The current iteration number.
</p>


<h3>See Also</h3>

<p><a href="#topic+oLBFGS">oLBFGS</a> , <a href="#topic+SQN">SQN</a> , <a href="#topic+adaQN">adaQN</a>
</p>

<hr>
<h2 id='oLBFGS'>oLBFGS guided optimizer</h2><span id='topic+oLBFGS'></span>

<h3>Description</h3>

<p>Optimizes an empirical (convex) loss function over batches of sample data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>oLBFGS(x0, grad_fun, pred_fun = NULL, initial_step = 0.01,
  step_fun = function(iter) 1/sqrt((iter/10) + 1),
  callback_iter = NULL, args_cb = NULL, verbose = TRUE,
  mem_size = 10, hess_init = NULL, min_curvature = 1e-04,
  y_reg = NULL, check_nan = TRUE, nthreads = -1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="oLBFGS_+3A_x0">x0</code></td>
<td>
<p>Initial values for the variables to optimize.</p>
</td></tr>
<tr><td><code id="oLBFGS_+3A_grad_fun">grad_fun</code></td>
<td>
<p>Function taking as unnamed arguments 'x_curr' (variable values), 'X' (covariates),
'y' (target variable), and 'w' (weights), plus additional arguments ('...'), and producing the expected
value of the gradient when evalauted on that data.</p>
</td></tr>
<tr><td><code id="oLBFGS_+3A_pred_fun">pred_fun</code></td>
<td>
<p>Function taking an unnamed argument as data, another unnamed argument as the variable values,
and optional extra arguments ('...'). Will be called when using 'predict' on the object returned by this function.</p>
</td></tr>
<tr><td><code id="oLBFGS_+3A_initial_step">initial_step</code></td>
<td>
<p>Initial step size.</p>
</td></tr>
<tr><td><code id="oLBFGS_+3A_step_fun">step_fun</code></td>
<td>
<p>Function accepting the iteration number as an unnamed parameter, which will output the
number by which 'initial_step' will be multiplied at each iteration to get the step size for that
iteration.</p>
</td></tr>
<tr><td><code id="oLBFGS_+3A_callback_iter">callback_iter</code></td>
<td>
<p>Callback function which will be called at the end of each iteration.
Will pass three unnamed arguments: the current variable values, the current iteration number,
and 'args_cb'. Pass 'NULL' if there is no need to call a callback function.</p>
</td></tr>
<tr><td><code id="oLBFGS_+3A_args_cb">args_cb</code></td>
<td>
<p>Extra argument to pass to the callback function.</p>
</td></tr>
<tr><td><code id="oLBFGS_+3A_verbose">verbose</code></td>
<td>
<p>Whether to print information about iteration statuses when something goes wrong.</p>
</td></tr>
<tr><td><code id="oLBFGS_+3A_mem_size">mem_size</code></td>
<td>
<p>Number of correction pairs to store for approximation of Hessian-vector products.</p>
</td></tr>
<tr><td><code id="oLBFGS_+3A_hess_init">hess_init</code></td>
<td>
<p>Value to which to initialize the diagonal of H0.
If passing 'NULL', will use the same initializion as for SQN ((s_last * y_last) / (y_last * y_last)).</p>
</td></tr>
<tr><td><code id="oLBFGS_+3A_min_curvature">min_curvature</code></td>
<td>
<p>Minimum value of (s * y) / (s * s) in order to accept a correction pair. Pass 'NULL' for
no minimum.</p>
</td></tr>
<tr><td><code id="oLBFGS_+3A_y_reg">y_reg</code></td>
<td>
<p>Regularizer for 'y' vector (gets added y_reg * s). Pass 'NULL' for no regularization.</p>
</td></tr>
<tr><td><code id="oLBFGS_+3A_check_nan">check_nan</code></td>
<td>
<p>Whether to check for variables becoming NA after each iteration, and reverting the step if they do
(will also reset BFGS memory).</p>
</td></tr>
<tr><td><code id="oLBFGS_+3A_nthreads">nthreads</code></td>
<td>
<p>Number of parallel threads to use. If set to -1, will determine the number of available threads and use
all of them. Note however that not all the computations can be parallelized, and the BLAS backend might use a different
number of threads.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>an 'oLBFGS' object with the user-supplied functions, which can be fit to batches of data
through function 'partial_fit', and can produce predictions on new data through function 'predict'.
</p>


<h3>References</h3>

 <ul>
<li><p> Schraudolph, N.N., Yu, J. and Guenter, S., 2007, March.
&quot;A stochastic quasi-Newton method for online convex optimization.&quot;
In Artificial Intelligence and Statistics (pp. 436-443).
</p>
</li>
<li><p> Wright, S. and Nocedal, J., 1999. &quot;Numerical optimization.&quot; (ch 7) Springer Science, 35(67-68), p.7.</p>
</li></ul>



<h3>See Also</h3>

<p><a href="#topic+partial_fit">partial_fit</a> , <a href="#topic+predict.stochQN_guided">predict.stochQN_guided</a> , <a href="#topic+oLBFGS_free">oLBFGS_free</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### Example regression with randomly-generated data
library(stochQN)

### Will sample data y ~ Ax + epsilon
true_coefs &lt;- c(1.12, 5.34, -6.123)

generate_data_batch &lt;- function(true_coefs, n = 100) {
  X &lt;- matrix(
    rnorm(length(true_coefs) * n),
    nrow=n, ncol=length(true_coefs))
  y &lt;- X %*% true_coefs + rnorm(n)
  return(list(X = X, y = y))
}

### Regular regression function that minimizes RMSE
eval_fun &lt;- function(coefs, X, y, weights=NULL, lambda=1e-5) {
  pred &lt;- as.numeric(X %*% coefs)
  RMSE &lt;- sqrt(mean((pred - y)^2))
  reg  &lt;- lambda * as.numeric(coefs %*% coefs)
  return(RMSE + reg)
}

eval_grad &lt;- function(coefs, X, y, weights=NULL, lambda=1e-5) {
  pred &lt;- X %*% coefs
  grad &lt;- colMeans(X * as.numeric(pred - y))
  grad &lt;- grad + 2 * lambda * as.numeric(coefs^2)
  return(grad)
}

pred_fun &lt;- function(X, coefs, ...) {
  return(as.numeric(X %*% coefs))
}

### Initialize optimizer form arbitrary values
x0 &lt;- c(1, 1, 1)
optimizer &lt;- oLBFGS(x0, grad_fun=eval_grad,
  pred_fun=pred_fun, initial_step=1e-1)
val_data &lt;- generate_data_batch(true_coefs, n=100)

### Fit to 50 batches of data, 100 observations each
set.seed(1)
for (i in 1:50) {
  new_batch &lt;- generate_data_batch(true_coefs, n=100)
  partial_fit(
    optimizer,
    new_batch$X, new_batch$y,
    lambda=1e-5)
  x_curr &lt;- get_curr_x(optimizer)
  i_curr &lt;- get_iteration_number(optimizer)
  if ((i_curr %% 10)  == 0) {
    cat(sprintf(
      "Iteration %d - E[f(x)]: %f - values of x: [%f, %f, %f]\n",
      i_curr,
      eval_fun(x_curr, val_data$X, val_data$y, lambda=1e-5),
      x_curr[1], x_curr[2], x_curr[3]))
  }
}

### Predict for new data
new_batch &lt;- generate_data_batch(true_coefs, n=10)
yhat &lt;- predict(optimizer, new_batch$X)
</code></pre>

<hr>
<h2 id='oLBFGS_free'>oLBFGS Free-Mode Optimizer</h2><span id='topic+oLBFGS_free'></span>

<h3>Description</h3>

<p>Optimizes an empirical (convex) loss function over batches of sample data. Compared to
function/class 'oLBFGS', this version lets the user do all the calculations from the outside, only
interacting with the object by means of a function that returns a request type and is fed the
required calculation through a method 'update_gradient'.
</p>
<p>Order in which requests are made:
</p>
<p>========== loop ===========
</p>
<p>* calc_grad
</p>
<p>* calc_grad_same_batch		(might skip if using check_nan)
</p>
<p>===========================
</p>
<p>After running this function, apply 'run_oLBFGS_free' to it to get the first requested piece of information.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>oLBFGS_free(mem_size = 10, hess_init = NULL, min_curvature = 1e-04,
  y_reg = NULL, check_nan = TRUE, nthreads = -1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="oLBFGS_free_+3A_mem_size">mem_size</code></td>
<td>
<p>Number of correction pairs to store for approximation of Hessian-vector products.</p>
</td></tr>
<tr><td><code id="oLBFGS_free_+3A_hess_init">hess_init</code></td>
<td>
<p>Value to which to initialize the diagonal of H0.
If passing 'NULL', will use the same initializion as for SQN ((s_last * y_last) / (y_last * y_last)).</p>
</td></tr>
<tr><td><code id="oLBFGS_free_+3A_min_curvature">min_curvature</code></td>
<td>
<p>Minimum value of (s * y) / (s * s) in order to accept a correction pair. Pass 'NULL' for
no minimum.</p>
</td></tr>
<tr><td><code id="oLBFGS_free_+3A_y_reg">y_reg</code></td>
<td>
<p>Regularizer for 'y' vector (gets added y_reg * s). Pass 'NULL' for no regularization.</p>
</td></tr>
<tr><td><code id="oLBFGS_free_+3A_check_nan">check_nan</code></td>
<td>
<p>Whether to check for variables becoming NA after each iteration, and reverting the step if they do
(will also reset BFGS memory).</p>
</td></tr>
<tr><td><code id="oLBFGS_free_+3A_nthreads">nthreads</code></td>
<td>
<p>Number of parallel threads to use. If set to -1, will determine the number of available threads and use
all of them. Note however that not all the computations can be parallelized, and the BLAS backend might use a different
number of threads.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An 'oLBFGS_free' object, which can be used through functions 'update_gradient' and 'run_oLBFGS_free'
</p>


<h3>References</h3>

 <ul>
<li><p> Schraudolph, N.N., Yu, J. and Guenter, S., 2007, March.
&quot;A stochastic quasi-Newton method for online convex optimization.&quot;
In Artificial Intelligence and Statistics (pp. 436-443).
</p>
</li>
<li><p> Wright, S. and Nocedal, J., 1999. &quot;Numerical optimization.&quot; (ch 7) Springer Science, 35(67-68), p.7.</p>
</li></ul>



<h3>See Also</h3>

<p><a href="#topic+update_gradient">update_gradient</a> , <a href="#topic+run_oLBFGS_free">run_oLBFGS_free</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### Example optimizing Rosenbrock 2D function
### Note that this example is not stochastic, as the
### function is not evaluated in expectation based on
### batches of data, but rather it has a given absolute
### form that never varies.
### Warning: this optimizer is meant for convex functions
### (Rosenbrock's is not convex)
library(stochQN)


fr &lt;- function(x) { ## Rosenbrock Banana function
	x1 &lt;- x[1]
	x2 &lt;- x[2]
	100 * (x2 - x1 * x1)^2 + (1 - x1)^2
}
grr &lt;- function(x) { ## Gradient of 'fr'
	x1 &lt;- x[1]
	x2 &lt;- x[2]
	c(-400 * x1 * (x2 - x1 * x1) - 2 * (1 - x1),
	200 * (x2 - x1 * x1))
}

### Initial values of x
x_opt = as.numeric(c(0, 2))
cat(sprintf("Initial values of x: [%.3f, %.3f]\n",
	x_opt[1], x_opt[2]))
	
### Will use a constant step size throughout
### (not recommended)
step_size &lt;- 1e-1

### Initialize the optimizer
optimizer &lt;- oLBFGS_free()

### Keep track of the iteration number
curr_iter &lt;- 0

### Run a loop for 100 iterations
### (Note that each iteration requires 2 calculations,
###  hence the 200)
for (i in 1:200) {
	req &lt;- run_oLBFGS_free(optimizer, x_opt, step_size)
	if (req$task == "calc_grad") {
	  update_gradient(optimizer, grr(req$requested_on))
	} else if (req$task == "calc_grad_same_batch") {
	  update_gradient(optimizer, grr(req$requested_on))
	}
	
	### Track progress every 10 iterations
	if (req$info$iteration_number &gt; curr_iter) {
	  curr_iter &lt;- req$info$iteration_number
	  if ((curr_iter %% 10) == 0) {
	  cat(sprintf(
	   "Iteration %3d - Current function value: %.3f\n",
	  req$info$iteration_number, fr(x_opt)
	  ))
	  }
	}
}
cat(sprintf("Current values of x: [%.3f, %.3f]\n",
	x_opt[1], x_opt[2]))
</code></pre>

<hr>
<h2 id='partial_fit'>Partial fit stochastic model to new data</h2><span id='topic+partial_fit'></span>

<h3>Description</h3>

<p>Runs one iteration of the stochastic optimizer on the new data passed here.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>partial_fit(optimizer, X, y = NULL, weights = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="partial_fit_+3A_optimizer">optimizer</code></td>
<td>
<p>A stochastic optimizer from this package as output by functions 'oLBFGS', 'SQN', 'adaQN'.
Will be modified in-place.</p>
</td></tr>
<tr><td><code id="partial_fit_+3A_x">X</code></td>
<td>
<p>Covariates to pass to the user-defined gradient / objective / Hessian-vector functions.</p>
</td></tr>
<tr><td><code id="partial_fit_+3A_y">y</code></td>
<td>
<p>Target variable to pass to the user-defined gradient / objective / Hessian-vector functions.</p>
</td></tr>
<tr><td><code id="partial_fit_+3A_weights">weights</code></td>
<td>
<p>Target variable to pass to the user-defined gradient / objective / Hessian-vector functions.</p>
</td></tr>
<tr><td><code id="partial_fit_+3A_...">...</code></td>
<td>
<p>Additional arguments to pass to the user-defined gradient / objective / Hessian-vector functions.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>No return value (object is modified in-place).
</p>


<h3>See Also</h3>

<p><a href="#topic+oLBFGS">oLBFGS</a> , <a href="#topic+SQN">SQN</a> , <a href="#topic+adaQN">adaQN</a>
</p>

<hr>
<h2 id='partial_fit_logistic'>Update stochastic logistic regression model with new batch of data</h2><span id='topic+partial_fit_logistic'></span>

<h3>Description</h3>

<p>Perform a quasi-Newton iteration to update the model with new data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>partial_fit_logistic(logistic_model, X, y = NULL, w = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="partial_fit_logistic_+3A_logistic_model">logistic_model</code></td>
<td>
<p>A 'stoch_logistic' object as output by function 'stochastic.logistic.regression'.
Will be modified in-place.</p>
</td></tr>
<tr><td><code id="partial_fit_logistic_+3A_x">X</code></td>
<td>
<p>Data with covariates. If passing a 'data.frame', the model object must have been initialized
with a formula, and 'X' must also contain the target variable ('y'). If passing a matrix, must
also pass 'y'. Note that whatever factor levels are present in the first batch of data, will be taken as the
whole factor levels.</p>
</td></tr>
<tr><td><code id="partial_fit_logistic_+3A_y">y</code></td>
<td>
<p>The target variable, when using matrices. Ignored when using formula.</p>
</td></tr>
<tr><td><code id="partial_fit_logistic_+3A_w">w</code></td>
<td>
<p>Sample weights (optional). If required, must pass them at every partial fit iteration.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>No return value. Model object is updated in-place.
</p>


<h3>See Also</h3>

<p><a href="#topic+stochastic.logistic.regression">stochastic.logistic.regression</a>
</p>

<hr>
<h2 id='predict.stoch_logistic'>Prediction function for stochastic logistic regression</h2><span id='topic+predict.stoch_logistic'></span>

<h3>Description</h3>

<p>Makes predictions for new data from the fitted model. Model have already
been fit to at least 1 batch of data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'stoch_logistic'
predict(object, newdata, type = "prob", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.stoch_logistic_+3A_object">object</code></td>
<td>
<p>A 'stoch_logistic' object as output by function 'stochastic.logistic.regression'.</p>
</td></tr>
<tr><td><code id="predict.stoch_logistic_+3A_newdata">newdata</code></td>
<td>
<p>New data on which to make predictions.</p>
</td></tr>
<tr><td><code id="predict.stoch_logistic_+3A_type">type</code></td>
<td>
<p>Type of prediction to make. Can pass 'prob' to get probabilities for the positive class,
or 'class' to get the predicted class.</p>
</td></tr>
<tr><td><code id="predict.stoch_logistic_+3A_...">...</code></td>
<td>
<p>Not used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector with the predicted classes or probabilities for 'newdata'.
</p>


<h3>See Also</h3>

<p><a href="#topic+stochastic.logistic.regression">stochastic.logistic.regression</a>
</p>

<hr>
<h2 id='predict.stochQN_guided'>Predict function for stochastic optimizer object</h2><span id='topic+predict.stochQN_guided'></span>

<h3>Description</h3>

<p>Calls the user-defined predict function for an object
optimized through this package's functions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'stochQN_guided'
predict(object, newdata, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.stochQN_guided_+3A_object">object</code></td>
<td>
<p>Optimizer from this module as output by functions 'oLBFGS', 'SQN', 'adaQN'. Must
have been constructed with a predict function.</p>
</td></tr>
<tr><td><code id="predict.stochQN_guided_+3A_newdata">newdata</code></td>
<td>
<p>Data on which to make predictions (will be passed to the user-provided function).</p>
</td></tr>
<tr><td><code id="predict.stochQN_guided_+3A_...">...</code></td>
<td>
<p>Additional arguments to pass to the user-provided predict function.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><a href="#topic+oLBFGS">oLBFGS</a> , <a href="#topic+SQN">SQN</a> , <a href="#topic+adaQN">adaQN</a>
</p>

<hr>
<h2 id='print.adaQN'>Print summary info about adaQN guided-mode object</h2><span id='topic+print.adaQN'></span>

<h3>Description</h3>

<p>Print summary info about adaQN guided-mode object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'adaQN'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.adaQN_+3A_x">x</code></td>
<td>
<p>An 'adaQN' object as output by function of the same name.</p>
</td></tr>
<tr><td><code id="print.adaQN_+3A_...">...</code></td>
<td>
<p>Not used.</p>
</td></tr>
</table>

<hr>
<h2 id='print.adaQN_free'>Print summary info about adaQN free-mode object</h2><span id='topic+print.adaQN_free'></span>

<h3>Description</h3>

<p>Print summary info about adaQN free-mode object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'adaQN_free'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.adaQN_free_+3A_x">x</code></td>
<td>
<p>An 'adaQN_free' object as output by function of the same name.</p>
</td></tr>
<tr><td><code id="print.adaQN_free_+3A_...">...</code></td>
<td>
<p>Not used.</p>
</td></tr>
</table>

<hr>
<h2 id='print.oLBFGS'>Print summary info about oLBFGS guided-mode object</h2><span id='topic+print.oLBFGS'></span>

<h3>Description</h3>

<p>Print summary info about oLBFGS guided-mode object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'oLBFGS'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.oLBFGS_+3A_x">x</code></td>
<td>
<p>An 'oLBFGS' object as output by function of the same name.</p>
</td></tr>
<tr><td><code id="print.oLBFGS_+3A_...">...</code></td>
<td>
<p>Not used.</p>
</td></tr>
</table>

<hr>
<h2 id='print.oLBFGS_free'>Print summary info about oLBFGS free-mode object</h2><span id='topic+print.oLBFGS_free'></span>

<h3>Description</h3>

<p>Print summary info about oLBFGS free-mode object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'oLBFGS_free'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.oLBFGS_free_+3A_x">x</code></td>
<td>
<p>An 'oLBFGS_free' object as output by function of the same name.</p>
</td></tr>
<tr><td><code id="print.oLBFGS_free_+3A_...">...</code></td>
<td>
<p>Not used.</p>
</td></tr>
</table>

<hr>
<h2 id='print.SQN'>Print summary info about SQN guided-mode object</h2><span id='topic+print.SQN'></span>

<h3>Description</h3>

<p>Print summary info about SQN guided-mode object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'SQN'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.SQN_+3A_x">x</code></td>
<td>
<p>An 'SQN' object as output by function of the same name.</p>
</td></tr>
<tr><td><code id="print.SQN_+3A_...">...</code></td>
<td>
<p>Not used.</p>
</td></tr>
</table>

<hr>
<h2 id='print.SQN_free'>Print summary info about SQN free-mode object</h2><span id='topic+print.SQN_free'></span>

<h3>Description</h3>

<p>Print summary info about SQN free-mode object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'SQN_free'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.SQN_free_+3A_x">x</code></td>
<td>
<p>An 'SQN_free' object as output by function of the same name.</p>
</td></tr>
<tr><td><code id="print.SQN_free_+3A_...">...</code></td>
<td>
<p>Not used.</p>
</td></tr>
</table>

<hr>
<h2 id='print.stoch_logistic'>Print general info about stochastic logistic regression object</h2><span id='topic+print.stoch_logistic'></span>

<h3>Description</h3>

<p>Print general info about stochastic logistic regression object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'stoch_logistic'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.stoch_logistic_+3A_x">x</code></td>
<td>
<p>A 'stoch_logistic' object as output by function 'stochastic.logistic.regression'.</p>
</td></tr>
<tr><td><code id="print.stoch_logistic_+3A_...">...</code></td>
<td>
<p>Not used.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><a href="#topic+stochastic.logistic.regression">stochastic.logistic.regression</a>
</p>

<hr>
<h2 id='run_adaQN_free'>Run adaQN optimizer in free-mode</h2><span id='topic+run_adaQN_free'></span>

<h3>Description</h3>

<p>Run the next step of an adaQN optimization procedure, after the last requested calculation
has been fed to the optimizer. When run for the first time, there is no request, so the function just
needs to be run on the object as it is returned from function 'adaQN_free'.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>run_adaQN_free(optimizer, x, step_size)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="run_adaQN_free_+3A_optimizer">optimizer</code></td>
<td>
<p>An 'adaQN_free' optimizer, for which its last request must have been served. Will be updated in-place.</p>
</td></tr>
<tr><td><code id="run_adaQN_free_+3A_x">x</code></td>
<td>
<p>Current values of the variables being optimized. Must be a numeric vector. Will be updated in-place.</p>
</td></tr>
<tr><td><code id="run_adaQN_free_+3A_step_size">step_size</code></td>
<td>
<p>Step size for the quasi-Newton update.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A request with the next piece of required information. The output will be a list with the following levels:
</p>

<ul>
<li><p>task Requested task (one of &quot;calc_grad&quot;, &quot;calc_fun_val_batch&quot;, &quot;calc_grad_big_batch&quot;).
</p>
</li>
<li><p>requested_on Values of 'x' at which the requested information must be calculated.
</p>
</li>
<li><p>info </p>

<ul>
<li><p>x_changed_in_run Whether the 'x' vector was updated.
</p>
</li>
<li><p>iteration_number Current iteration number (in terms of quasi-Newton updates).
</p>
</li>
<li><p>iteration_info Information about potential problems encountered during the iteration.
</p>
</li></ul>
 
</li></ul>



<h3>See Also</h3>

<p><a href="#topic+adaQN_free">adaQN_free</a>
</p>

<hr>
<h2 id='run_oLBFGS_free'>Run oLBFGS optimizer in free-mode</h2><span id='topic+run_oLBFGS_free'></span>

<h3>Description</h3>

<p>Run the next step of an oLBFGS optimization procedure, after the last requested calculation
has been fed to the optimizer. When run for the first time, there is no request, so the function just
needs to be run on the object as it is returned from function 'oLBFGS_free'.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>run_oLBFGS_free(optimizer, x, step_size)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="run_oLBFGS_free_+3A_optimizer">optimizer</code></td>
<td>
<p>An 'oLBFGS_free' optimizer, for which its last request must have been served. Will be updated in-place.</p>
</td></tr>
<tr><td><code id="run_oLBFGS_free_+3A_x">x</code></td>
<td>
<p>Current values of the variables being optimized. Must be a numeric vector. Will be updated in-place.</p>
</td></tr>
<tr><td><code id="run_oLBFGS_free_+3A_step_size">step_size</code></td>
<td>
<p>Step size for the quasi-Newton update.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A request with the next piece of required information. The output will be a list with the following levels:
</p>

<ul>
<li><p>task Requested task (one of &quot;calc_grad&quot; or &quot;calc_grad_same_batch&quot;).
</p>
</li>
<li><p>requested_on Values of 'x' at which the requested information must be calculated.
</p>
</li>
<li><p>info </p>

<ul>
<li><p>x_changed_in_run Whether the 'x' vector was updated.
</p>
</li>
<li><p>iteration_number Current iteration number (in terms of quasi-Newton updates).
</p>
</li>
<li><p>iteration_info Information about potential problems encountered during the iteration.
</p>
</li></ul>
 
</li></ul>



<h3>See Also</h3>

<p><a href="#topic+oLBFGS_free">oLBFGS_free</a>
</p>

<hr>
<h2 id='run_SQN_free'>Run SQN optimizer in free-mode</h2><span id='topic+run_SQN_free'></span>

<h3>Description</h3>

<p>Run the next step of an SQN optimization procedure, after the last requested calculation
has been fed to the optimizer. When run for the first time, there is no request, so the function just
needs to be run on the object as it is returned from function 'SQN_free'.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>run_SQN_free(optimizer, x, step_size)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="run_SQN_free_+3A_optimizer">optimizer</code></td>
<td>
<p>An 'SQN_free' optimizer, for which its last request must have been served. Will be updated in-place.</p>
</td></tr>
<tr><td><code id="run_SQN_free_+3A_x">x</code></td>
<td>
<p>Current values of the variables being optimized. Must be a numeric vector. Will be updated in-place.</p>
</td></tr>
<tr><td><code id="run_SQN_free_+3A_step_size">step_size</code></td>
<td>
<p>Step size for the quasi-Newton update.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A request with the next piece of required information. The output will be a list with the following levels:
</p>

<ul>
<li><p>task Requested task (one of &quot;calc_grad&quot;, &quot;calc_grad_big_batch&quot;, &quot;calc_hess_vec&quot;).
</p>
</li>
<li><p>requested_on </p>

<ul>
<li><p>req_x Values of 'x' at which the requested information (gradient/Hessian) must be calculated.
</p>
</li>
<li><p>req_vec Vector by which the Hessian must be multiplied. Will output 'NULL' when this
calculation is not needed.
</p>
</li></ul>

</li>
<li><p>info </p>

<ul>
<li><p>x_changed_in_run Whether the 'x' vector was updated.
</p>
</li>
<li><p>iteration_number Current iteration number (in terms of quasi-Newton updates).
</p>
</li>
<li><p>iteration_info Information about potential problems encountered during the iteration.
</p>
</li></ul>
 
</li></ul>



<h3>See Also</h3>

<p><a href="#topic+SQN_free">SQN_free</a>
</p>

<hr>
<h2 id='SQN'>SQN guided optimizer</h2><span id='topic+SQN'></span>

<h3>Description</h3>

<p>Optimizes an empirical (convex) loss function over batches of sample data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SQN(x0, grad_fun, hess_vec_fun = NULL, pred_fun = NULL,
  initial_step = 0.001, step_fun = function(iter) 1/sqrt((iter/10) +
  1), callback_iter = NULL, args_cb = NULL, verbose = TRUE,
  mem_size = 10, bfgs_upd_freq = 20, min_curvature = 1e-04,
  y_reg = NULL, use_grad_diff = FALSE, check_nan = TRUE,
  nthreads = -1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SQN_+3A_x0">x0</code></td>
<td>
<p>Initial values for the variables to optimize.</p>
</td></tr>
<tr><td><code id="SQN_+3A_grad_fun">grad_fun</code></td>
<td>
<p>Function taking as unnamed arguments 'x_curr' (variable values), 'X' (covariates),
'y' (target variable), and 'w' (weights), plus additional arguments ('...'), and producing the expected
value of the gradient when evalauted on that data.</p>
</td></tr>
<tr><td><code id="SQN_+3A_hess_vec_fun">hess_vec_fun</code></td>
<td>
<p>Function taking as unnamed arguments 'x_curr' (variable values), 'vec' (numeric vector),
'X' (covariates), 'y' (target variable), and 'w' (weights), plus additional arguments ('...'), and producing
the expected value of the Hessian (with variable values at 'x_curr') when evalauted on that data, multiplied
by the vector 'vec'. Not required when using 'use_grad_diff' = 'TRUE'.</p>
</td></tr>
<tr><td><code id="SQN_+3A_pred_fun">pred_fun</code></td>
<td>
<p>Function taking an unnamed argument as data, another unnamed argument as the variable values,
and optional extra arguments ('...'). Will be called when using 'predict' on the object returned by this function.</p>
</td></tr>
<tr><td><code id="SQN_+3A_initial_step">initial_step</code></td>
<td>
<p>Initial step size.</p>
</td></tr>
<tr><td><code id="SQN_+3A_step_fun">step_fun</code></td>
<td>
<p>Function accepting the iteration number as an unnamed parameter, which will output the
number by which 'initial_step' will be multiplied at each iteration to get the step size for that
iteration.</p>
</td></tr>
<tr><td><code id="SQN_+3A_callback_iter">callback_iter</code></td>
<td>
<p>Callback function which will be called at the end of each iteration.
Will pass three unnamed arguments: the current variable values, the current iteration number,
and 'args_cb'. Pass 'NULL' if there is no need to call a callback function.</p>
</td></tr>
<tr><td><code id="SQN_+3A_args_cb">args_cb</code></td>
<td>
<p>Extra argument to pass to the callback function.</p>
</td></tr>
<tr><td><code id="SQN_+3A_verbose">verbose</code></td>
<td>
<p>Whether to print information about iteration statuses when something goes wrong.</p>
</td></tr>
<tr><td><code id="SQN_+3A_mem_size">mem_size</code></td>
<td>
<p>Number of correction pairs to store for approximation of Hessian-vector products.</p>
</td></tr>
<tr><td><code id="SQN_+3A_bfgs_upd_freq">bfgs_upd_freq</code></td>
<td>
<p>Number of iterations (batches) after which to generate a BFGS correction pair.</p>
</td></tr>
<tr><td><code id="SQN_+3A_min_curvature">min_curvature</code></td>
<td>
<p>Minimum value of (s * y) / (s * s) in order to accept a correction pair. Pass 'NULL' for
no minimum.</p>
</td></tr>
<tr><td><code id="SQN_+3A_y_reg">y_reg</code></td>
<td>
<p>Regularizer for 'y' vector (gets added y_reg * s). Pass 'NULL' for no regularization.</p>
</td></tr>
<tr><td><code id="SQN_+3A_use_grad_diff">use_grad_diff</code></td>
<td>
<p>Whether to create the correction pairs using differences between gradients instead of Hessian-vector products.
These gradients are calculated on a larger batch than the regular ones (given by batch_size * bfgs_upd_freq).</p>
</td></tr>
<tr><td><code id="SQN_+3A_check_nan">check_nan</code></td>
<td>
<p>Whether to check for variables becoming NaN after each iteration, and reverting the step if they do
(will also reset BFGS memory).</p>
</td></tr>
<tr><td><code id="SQN_+3A_nthreads">nthreads</code></td>
<td>
<p>Number of parallel threads to use. If set to -1, will determine the number of available threads and use
all of them. Note however that not all the computations can be parallelized, and the BLAS backend might use a different
number of threads.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>an 'SQN' object with the user-supplied functions, which can be fit to batches of data
through function 'partial_fit', and can produce predictions on new data through function 'predict'.
</p>


<h3>References</h3>

 <ul>
<li><p> Byrd, R.H., Hansen, S.L., Nocedal, J. and Singer, Y., 2016.
&quot;A stochastic quasi-Newton method for large-scale optimization.&quot;
SIAM Journal on Optimization, 26(2), pp.1008-1031.
</p>
</li>
<li><p> Wright, S. and Nocedal, J., 1999. &quot;Numerical optimization.&quot; (ch 7) Springer Science, 35(67-68), p.7.</p>
</li></ul>



<h3>See Also</h3>

<p><a href="#topic+partial_fit">partial_fit</a> , <a href="#topic+predict.stochQN_guided">predict.stochQN_guided</a> , <a href="#topic+SQN_free">SQN_free</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### Example logistic regression with randomly-generated data
library(stochQN)

### Will sample data y ~ Bernoulli(sigm(Ax))
true_coefs &lt;- c(1.12, 5.34, -6.123)

generate_data_batch &lt;- function(true_coefs, n = 100) {
  X &lt;- matrix(rnorm(length(true_coefs) * n), nrow=n, ncol=length(true_coefs))
  y &lt;- 1 / (1 + exp(-as.numeric(X %*% true_coefs)))
  y &lt;- as.numeric(y &gt;= runif(n))
  return(list(X = X, y = y))
}

### Logistic regression likelihood/loss
eval_fun &lt;- function(coefs, X, y, weights=NULL, lambda=1e-5) {
  pred    &lt;- 1 / (1 + exp(-as.numeric(X %*% coefs)))
  logloss &lt;- mean(-(y * log(pred) + (1 - y) * log(1 - pred)))
  reg     &lt;- lambda * as.numeric(coefs %*% coefs)
  return(logloss + reg)
}

eval_grad &lt;- function(coefs, X, y, weights=NULL, lambda=1e-5) {
  pred &lt;- 1 / (1 + exp(-(X %*% coefs)))
  grad &lt;- colMeans(X * as.numeric(pred - y))
  grad &lt;- grad + 2 * lambda * as.numeric(coefs^2)
  return(as.numeric(grad))
}

eval_Hess_vec &lt;- function(coefs, vec, X, y, weights=NULL, lambda=1e-5) {
  pred &lt;- 1 / (1 + exp(-as.numeric(X %*% coefs)))
  diag &lt;- pred * (1 - pred)
  Hp   &lt;- (t(X) * diag) %*% (X %*% vec)
  Hp   &lt;- Hp / NROW(X) + 2 * lambda * vec
  return(as.numeric(Hp))
}

pred_fun &lt;- function(X, coefs, ...) {
  return(1 / (1 + exp(-as.numeric(X %*% coefs))))
}


### Initialize optimizer form arbitrary values
x0 &lt;- c(1, 1, 1)
optimizer &lt;- SQN(x0, grad_fun=eval_grad, pred_fun=pred_fun,
  hess_vec_fun=eval_Hess_vec, initial_step=1e-0)
val_data &lt;- generate_data_batch(true_coefs, n=100)

### Fit to 250 batches of data, 100 observations each
set.seed(1)
for (i in 1:250) {
  new_batch &lt;- generate_data_batch(true_coefs, n=100)
  partial_fit(optimizer, new_batch$X, new_batch$y, lambda=1e-5)
  x_curr &lt;- get_curr_x(optimizer)
  i_curr &lt;- get_iteration_number(optimizer)
  if ((i_curr %% 10)  == 0) {
    cat(sprintf("Iteration %3d - E[f(x)]: %f - values of x: [%f, %f, %f]\n",
      i_curr, eval_fun(x_curr, val_data$X, val_data$y, lambda=1e-5),
      x_curr[1], x_curr[2], x_curr[3]))
  }
}

### Predict for new data
new_batch &lt;- generate_data_batch(true_coefs, n=10)
yhat &lt;- predict(optimizer, new_batch$X)
</code></pre>

<hr>
<h2 id='SQN_free'>SQN Free-Mode Optimizer</h2><span id='topic+SQN_free'></span>

<h3>Description</h3>

<p>Optimizes an empirical (convex) loss function over batches of sample data. Compared to
function/class 'SQN', this version lets the user do all the calculations from the outside, only
interacting with the object by means of a function that returns a request type and is fed the
required calculation through methods 'update_gradient' and 'update_hess_vec'.
</p>
<p>Order in which requests are made:
</p>
<p>========== loop ===========
</p>
<p>* calc_grad
</p>
<p><code style="white-space: pre;">&#8288;   &#8288;</code>... (repeat calc_grad)
</p>
<p>if 'use_grad_diff':
</p>
<p><code style="white-space: pre;">&#8288;    &#8288;</code>* calc_grad_big_batch
</p>
<p>else:
</p>
<p><code style="white-space: pre;">&#8288;    &#8288;</code>* calc_hess_vec
</p>
<p>===========================
</p>
<p>After running this function, apply 'run_SQN_free' to it to get the first requested piece of information.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SQN_free(mem_size = 10, bfgs_upd_freq = 20, min_curvature = 1e-04,
  y_reg = NULL, use_grad_diff = FALSE, check_nan = TRUE,
  nthreads = -1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SQN_free_+3A_mem_size">mem_size</code></td>
<td>
<p>Number of correction pairs to store for approximation of Hessian-vector products.</p>
</td></tr>
<tr><td><code id="SQN_free_+3A_bfgs_upd_freq">bfgs_upd_freq</code></td>
<td>
<p>Number of iterations (batches) after which to generate a BFGS correction pair.</p>
</td></tr>
<tr><td><code id="SQN_free_+3A_min_curvature">min_curvature</code></td>
<td>
<p>Minimum value of (s * y) / (s * s) in order to accept a correction pair. Pass 'NULL' for
no minimum.</p>
</td></tr>
<tr><td><code id="SQN_free_+3A_y_reg">y_reg</code></td>
<td>
<p>Regularizer for 'y' vector (gets added y_reg * s). Pass 'NULL' for no regularization.</p>
</td></tr>
<tr><td><code id="SQN_free_+3A_use_grad_diff">use_grad_diff</code></td>
<td>
<p>Whether to create the correction pairs using differences between gradients instead of Hessian-vector products.
These gradients are calculated on a larger batch than the regular ones (given by batch_size * bfgs_upd_freq).</p>
</td></tr>
<tr><td><code id="SQN_free_+3A_check_nan">check_nan</code></td>
<td>
<p>Whether to check for variables becoming NaN after each iteration, and reverting the step if they do
(will also reset BFGS memory).</p>
</td></tr>
<tr><td><code id="SQN_free_+3A_nthreads">nthreads</code></td>
<td>
<p>Number of parallel threads to use. If set to -1, will determine the number of available threads and use
all of them. Note however that not all the computations can be parallelized, and the BLAS backend might use a different
number of threads.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An 'SQN_free' object, which can be used through functions 'update_gradient', 'update_hess_vec',
and 'run_SQN_free'
</p>


<h3>References</h3>

 <ul>
<li><p> Byrd, R.H., Hansen, S.L., Nocedal, J. and Singer, Y., 2016.
&quot;A stochastic quasi-Newton method for large-scale optimization.&quot;
SIAM Journal on Optimization, 26(2), pp.1008-1031.
</p>
</li>
<li><p> Wright, S. and Nocedal, J., 1999. &quot;Numerical optimization.&quot; (ch 7) Springer Science, 35(67-68), p.7.</p>
</li></ul>



<h3>See Also</h3>

<p><a href="#topic+update_gradient">update_gradient</a> , <a href="#topic+update_hess_vec">update_hess_vec</a> , <a href="#topic+run_oLBFGS_free">run_oLBFGS_free</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### Example optimizing Rosenbrock 2D function
### Note that this example is not stochastic, as the
### function is not evaluated in expectation based on
### batches of data, but rather it has a given absolute
### form that never varies.
### Warning: this optimizer is meant for convex functions
### (Rosenbrock's is not convex)
library(stochQN)


fr &lt;- function(x) { ## Rosenbrock Banana function
	x1 &lt;- x[1]
	x2 &lt;- x[2]
	100 * (x2 - x1 * x1)^2 + (1 - x1)^2
}
grr &lt;- function(x) { ## Gradient of 'fr'
	x1 &lt;- x[1]
	x2 &lt;- x[2]
	c(-400 * x1 * (x2 - x1 * x1) - 2 * (1 - x1),
	  200 * (x2 - x1 * x1))
}
Hvr &lt;- function(x, v) { ## Hessian of 'fr' by vector 'v'
	x1 &lt;- x[1]
	x2 &lt;- x[2]
	H &lt;- matrix(c(1200 * x1^2 - 400*x2 + 2,
			  -400 * x1, -400 * x1, 200),
			nrow = 2)
	as.vector(H %*% v)
}

### Initial values of x
x_opt = as.numeric(c(0, 2))
cat(sprintf("Initial values of x: [%.3f, %.3f]\n",
			x_opt[1], x_opt[2]))

### Will use constant step size throughout
### (not recommended)
step_size = 1e-3

### Initialize the optimizer
optimizer = SQN_free()

### Keep track of the iteration number
curr_iter &lt;- 0

### Run a loop for severa, iterations
### (Note that some iterations might require more
###  than 1 calculation request)
for (i in 1:200) {
  req &lt;- run_SQN_free(optimizer, x_opt, step_size)
  if (req$task == "calc_grad") {
    update_gradient(optimizer, grr(req$requested_on$req_x))
  } else if (req$task == "calc_hess_vec") {
	   update_hess_vec(optimizer,
      Hvr(req$requested_on$req_x, req$requested_on$req_vec))
  }

  ### Track progress every 10 iterations
  if (req$info$iteration_number &gt; curr_iter) {
  	curr_iter &lt;- req$info$iteration_number
  }
  if ((curr_iter %% 10) == 0) {
  	cat(sprintf(
  	 "Iteration %3d - Current function value: %.3f\n",
  	 req$info$iteration_number, fr(x_opt)))
  }
}
cat(sprintf("Current values of x: [%.3f, %.3f]\n",
			x_opt[1], x_opt[2]))
</code></pre>

<hr>
<h2 id='stochastic.logistic.regression'>Stochastic Logistic Regression</h2><span id='topic+stochastic.logistic.regression'></span>

<h3>Description</h3>

<p>Stochastic Logistic Regression
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stochastic.logistic.regression(formula = NULL, pos_class = NULL,
  dim = NULL, intercept = TRUE, x0 = NULL, optimizer = "adaQN",
  optimizer_args = list(initial_step = 0.1, verbose = FALSE),
  lambda = 0.001, random_seed = 1, val_data = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="stochastic.logistic.regression_+3A_formula">formula</code></td>
<td>
<p>Formula for the model, if it is fit to data.frames instead of matrices/vectors.</p>
</td></tr>
<tr><td><code id="stochastic.logistic.regression_+3A_pos_class">pos_class</code></td>
<td>
<p>If fit to data in the form of data.frames, a string indicating which of
the classes is the positive one. If fit to data in the form of matrices/vector, pass 'NULL'.</p>
</td></tr>
<tr><td><code id="stochastic.logistic.regression_+3A_dim">dim</code></td>
<td>
<p>Dimensionality of the model (number of features). Ignored when passing 'formula' or when passing 'x0'.
If the intercept is added from the option 'intercept' here, it should not be counted towards 'dim'.</p>
</td></tr>
<tr><td><code id="stochastic.logistic.regression_+3A_intercept">intercept</code></td>
<td>
<p>Whether to add an intercept to the covariates. Only ussed when fitting to matrices/vectors.
Ignored when passing formula (for formulas without intercept, put '-1' in the RHS to get rid of the intercept).</p>
</td></tr>
<tr><td><code id="stochastic.logistic.regression_+3A_x0">x0</code></td>
<td>
<p>Initial values of the variables. If passed, will ignore 'dim' and 'random_seed'. If not passed,
will generate random starting values ~ Norm(0, 0.1).</p>
</td></tr>
<tr><td><code id="stochastic.logistic.regression_+3A_optimizer">optimizer</code></td>
<td>
<p>The optimizer to use - one of 'adaQN' (recommended), 'SQN', 'oLBFGS'.</p>
</td></tr>
<tr><td><code id="stochastic.logistic.regression_+3A_optimizer_args">optimizer_args</code></td>
<td>
<p>Arguments to pass to the optimizer (same ones as the functions of the same name).
Must be a list. See the documentation of each optimizer for the parameters they take.</p>
</td></tr>
<tr><td><code id="stochastic.logistic.regression_+3A_lambda">lambda</code></td>
<td>
<p>Regularization parameter. Be aware that the functions assume the log-likelihood (a.k.a. loss)
is divided by the number of observations, so this number should be small.</p>
</td></tr>
<tr><td><code id="stochastic.logistic.regression_+3A_random_seed">random_seed</code></td>
<td>
<p>Random seed to use for the initialization of the variables. Ignored when passing 'x0'.</p>
</td></tr>
<tr><td><code id="stochastic.logistic.regression_+3A_val_data">val_data</code></td>
<td>
<p>Validation data (only used for 'adaQN'). If passed, must be a list with entries 'X',
'y' (if passing data.frames for fitting), and optionally 'w' (sample weights).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Binary logistic regression, fit in batches using this package's own optimizers.
</p>


<h3>Value</h3>

<p>An object of class 'stoch_logistic', which can be fit to batches of data through functon 'partial_fit_logistic'.
</p>


<h3>See Also</h3>

<p><a href="#topic+partial_fit_logistic">partial_fit_logistic</a>, <a href="#topic+coef.stoch_logistic">coef.stoch_logistic</a> , <a href="#topic+predict.stoch_logistic">predict.stoch_logistic</a> , 
<a href="#topic+adaQN">adaQN</a> , <a href="#topic+SQN">SQN</a>, <a href="#topic+oLBFGS">oLBFGS</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(stochQN)

### Load Iris dataset
data("iris")

### Example with X + y interface
X &lt;- as.matrix(iris[, c("Sepal.Length", "Sepal.Width",
  "Petal.Length", "Petal.Width")])
y &lt;- as.numeric(iris$Species == "setosa")

### Initialize model with default parameters
model &lt;- stochastic.logistic.regression(dim = 4)

### Fit to 10 randomly-subsampled batches
batch_size &lt;- as.integer(nrow(X) / 3)
for (i in 1:10) {
  set.seed(i)
  batch &lt;- sample(nrow(X),
      size = batch_size, replace=TRUE)
  partial_fit_logistic(model, X, y)
}

### Check classification accuracy
cat(sprintf(
  "Accuracy after 10 iterations: %.2f%%\n",
  100 * mean(
    predict(model, X, type = "class") == y)
  ))


### Example with formula interface
iris_df &lt;- iris
levels(iris_df$Species) &lt;- c("setosa", "other", "other")

### Initialize model with default parameters
model &lt;- stochastic.logistic.regression(Species ~ .,
  pos_class="setosa")

### Fit to 10 randomly-subsampled batches
batch_size &lt;- as.integer(nrow(iris_df) / 3)
for (i in 1:10) {
  set.seed(i)
  batch &lt;- sample(nrow(iris_df),
      size=batch_size, replace=TRUE)
  partial_fit_logistic(model, iris_df)
}
cat(sprintf(
  "Accuracy after 10 iterations: %.2f%%\n",
  100 * mean(
    predict(
      model, iris_df, type = "class") == iris_df$Species
      )
  ))
</code></pre>

<hr>
<h2 id='summary.stoch_logistic'>Print general info about stochastic logistic regression object</h2><span id='topic+summary.stoch_logistic'></span>

<h3>Description</h3>

<p>Same as 'print' function. To check the fitted coefficients use function 'coef'.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'stoch_logistic'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.stoch_logistic_+3A_object">object</code></td>
<td>
<p>A 'stoch_logistic' object as output by function 'stochastic.logistic.regression'.</p>
</td></tr>
<tr><td><code id="summary.stoch_logistic_+3A_...">...</code></td>
<td>
<p>Not used.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><a href="#topic+coef.stoch_logistic">coef.stoch_logistic</a> , <a href="#topic+stochastic.logistic.regression">stochastic.logistic.regression</a>
</p>

<hr>
<h2 id='update_fun'>Update objective function value (adaQN)</h2><span id='topic+update_fun'></span>

<h3>Description</h3>

<p>Update the (expected) value of the objective function in an 'adaQN_free' object, after
it has been requested by the optimizer (do NOT update it otherwise).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>update_fun(optimizer, fun)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="update_fun_+3A_optimizer">optimizer</code></td>
<td>
<p>An 'adaQN_free' object which after the last run had requested a new function evaluation.</p>
</td></tr>
<tr><td><code id="update_fun_+3A_fun">fun</code></td>
<td>
<p>Function as evaluated (in expectation) on the values of 'x' that were returned in the request.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>No return value (object is updated in-place).
</p>

<hr>
<h2 id='update_gradient'>Update gradient (oLBFGS, SQN, adaQN)</h2><span id='topic+update_gradient'></span>

<h3>Description</h3>

<p>Update the (expected) gradient in an optimizer from this package, after
it has been requested by the optimizer (do NOT update it otherwise).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>update_gradient(optimizer, gradient)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="update_gradient_+3A_optimizer">optimizer</code></td>
<td>
<p>A free-mode optimizer from this package ('oLBFGS_free', 'SQN_free', 'adaQN_free') which
after the last run had requested a new gradient evaluation..</p>
</td></tr>
<tr><td><code id="update_gradient_+3A_gradient">gradient</code></td>
<td>
<p>The (expected value of the) gradient as evaluated on the values of 'x' that
were returned in the request. Must be a numeric vector.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>No return value (object is updated in-place).
</p>

<hr>
<h2 id='update_hess_vec'>Update Hessian-vector product (SQN)</h2><span id='topic+update_hess_vec'></span>

<h3>Description</h3>

<p>Update the (expected) values of the Hessian-vector product in an 'SQN_free' object,
after it has been requested by the optimizer (do NOT update it otherwise).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>update_hess_vec(optimizer, hess_vec)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="update_hess_vec_+3A_optimizer">optimizer</code></td>
<td>
<p>An 'SQN_free' optimizer which after the last run had requested a new Hessian-vector evaluation.</p>
</td></tr>
<tr><td><code id="update_hess_vec_+3A_hess_vec">hess_vec</code></td>
<td>
<p>The (expected) value of the Hessian evaluated at the values of 'x' that were returned in the
request, multiplied by the vector that was returned in the same request. Must be a numeric vector.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>No return value (object is updated in-place).
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
