<!DOCTYPE html><html lang="en"><head><title>Help for package UAHDataScienceUC</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {UAHDataScienceUC}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#agglomerative_clustering'><p>Agglomerative Hierarchical Clustering</p></a></li>
<li><a href='#correlation_clustering'><p>Hierarchical Correlation Clustering</p></a></li>
<li><a href='#db1'><p>Test Database 1</p></a></li>
<li><a href='#db2'><p>Test Database 2</p></a></li>
<li><a href='#db3'><p>Test Database 3</p></a></li>
<li><a href='#db4'><p>Test Database 4</p></a></li>
<li><a href='#db5'><p>Test Database 5</p></a></li>
<li><a href='#db6'><p>Test Database 6</p></a></li>
<li><a href='#dbscan'><p>Density Based Spatial Clustering of Applications with Noise (DBSCAN)</p></a></li>
<li><a href='#divisive_clustering'><p>Divisive Hierarchical Clustering</p></a></li>
<li><a href='#gaussian_mixture'><p>Gaussian mixture model</p></a></li>
<li><a href='#genetic_kmeans'><p>Genetic K-Means Clustering</p></a></li>
<li><a href='#gka_allele_mutation'><p>Allele mutation probability computation</p></a></li>
<li><a href='#gka_centers'><p>Centroid computation</p></a></li>
<li><a href='#gka_chromosome_fix'><p>Chromosome fixing method</p></a></li>
<li><a href='#gka_crossover'><p>Crossover method i.e. K-Means Operator</p></a></li>
<li><a href='#gka_fitness'><p>Fitness function</p></a></li>
<li><a href='#gka_initialization'><p>Initialization method</p></a></li>
<li><a href='#gka_mutation'><p>Mutation method</p></a></li>
<li><a href='#gka_selection'><p>Selection method</p></a></li>
<li><a href='#gka_twcv'><p>Total Within Cluster Variation (TWCV) computation</p></a></li>
<li><a href='#kmeans_'><p>K-Means Clustering</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Title:</td>
<td>Learn Clustering Techniques Through Examples and Code</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.1</td>
</tr>
<tr>
<td>Description:</td>
<td>A comprehensive educational package combining clustering algorithms with 
    detailed step-by-step explanations. Provides implementations of both traditional 
    (hierarchical, k-means) and modern (Density-Based Spatial Clustering of Applications with Noise (DBSCAN), 
    Gaussian Mixture Models (GMM), genetic k-means) clustering methods 
    as described in Ezugwu et. al., (2022) &lt;<a href="https://doi.org/10.1016%2Fj.engappai.2022.104743">doi:10.1016/j.engappai.2022.104743</a>&gt;. 
    Includes educational datasets highlighting different clustering challenges, based on 
    'scikit-learn' examples (Pedregosa et al., 2011) 
    <a href="https://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html">https://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html</a>. Features detailed 
    algorithm explanations, visualizations, and weighted distance calculations for 
    enhanced learning.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.2</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.3.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>proxy (&ge; 0.4-27), cli (&ge; 3.6.1)</td>
</tr>
<tr>
<td>Suggests:</td>
<td>deldir (&ge; 1.0-9), knitr, rmarkdown</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2025-02-17 17:17:44 UTC; andriy</td>
</tr>
<tr>
<td>Author:</td>
<td>Eduardo Ruiz Sabajanes [aut],
  Roberto Alcantara [aut],
  Juan Jose Cuadrado Gallego
    <a href="https://orcid.org/0000-0001-8178-5556"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut],
  Andriy Protsak Protsak [aut, cre],
  Universidad de Alcala [cph]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Andriy Protsak Protsak &lt;andriy.protsak@edu.uah.es&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2025-02-17 20:30:05 UTC</td>
</tr>
</table>
<hr>
<h2 id='agglomerative_clustering'>Agglomerative Hierarchical Clustering</h2><span id='topic+agglomerative_clustering'></span>

<h3>Description</h3>

<p>Perform a hierarchical agglomerative cluster analysis on a set
of observations
</p>


<h3>Usage</h3>

<pre><code class='language-R'>agglomerative_clustering(
  data,
  proximity = "single",
  distance_method = "euclidean",
  learn = FALSE,
  waiting = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="agglomerative_clustering_+3A_data">data</code></td>
<td>
<p>a set of observations, presented as a matrix-like object where
every row is a new observation.</p>
</td></tr>
<tr><td><code id="agglomerative_clustering_+3A_proximity">proximity</code></td>
<td>
<p>the proximity definition to be used. This should be one
of <code>"single"</code> (minimum/single linkage), <code>"complete"</code> (maximum/
complete linkage), <code>"average"</code> (average linkage).</p>
</td></tr>
<tr><td><code id="agglomerative_clustering_+3A_distance_method">distance_method</code></td>
<td>
<p>the distance measure to use. Supported values are:
</p>

<ul>
<li><p> 'euclidean': Standard Euclidean distance
</p>
</li>
<li><p> 'manhattan': Manhattan (city-block) distance
</p>
</li>
<li><p> 'canberra': Canberra distance
</p>
</li>
<li><p> 'chebyshev': Chebyshev (maximum) distance
</p>
</li></ul>
</td></tr>
<tr><td><code id="agglomerative_clustering_+3A_learn">learn</code></td>
<td>
<p>a Boolean determining whether intermediate logs explaining how
the algorithm works should be printed or not.</p>
</td></tr>
<tr><td><code id="agglomerative_clustering_+3A_waiting">waiting</code></td>
<td>
<p>a Boolean determining whether the intermediate logs should be
printed in chunks waiting for user input before printing the next or not.</p>
</td></tr>
<tr><td><code id="agglomerative_clustering_+3A_...">...</code></td>
<td>
<p>additional arguments passed to <code><a href="proxy.html#topic+dist">proxy::dist()</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function performs a hierarchical cluster analysis for the
<code class="reqn">n</code> objects being clustered. The definition of a set of clusters using
this method follows a <code class="reqn">n</code> step process, which repeats until a single
cluster remains:
</p>

<ol>
<li><p> Initially, each object is assigned to its own cluster. The matrix
of distances between clusters is computed.
</p>
</li>
<li><p> The two clusters with closest proximity will be joined together and
the proximity matrix updated. This is done according to the specified
<code>proximity</code>. This step is repeated until a single cluster remains.
</p>
</li></ol>

<p>The definitions of <code>proximity</code> considered by this function are:
</p>

<dl>
<dt><code>single</code></dt><dd><p><code class="reqn">\min\left\{d(x,y):x\in A,y\in B\right\}</code>. Defines
the proximity between two clusters as the distance between the closest
objects among the two clusters. It produces clusters where each object is
closest to at least one other object in the same cluster. It is known as
<strong>SLINK</strong>, <strong>single-link</strong> and <strong>minimum-link</strong>.</p>
</dd>
<dt><code>complete</code></dt><dd><p><code class="reqn">\max\left\{d(x,y):x\in A,y\in B\right\}</code>.
Defines the proximity between two clusters as the distance between the
furthest objects among the two clusters. It is known as <strong>CLINK</strong>,
<strong>complete-link</strong> and <strong>maximum-link</strong>.</p>
</dd>
<dt><code>average</code></dt><dd><p><code class="reqn">\frac{1}{\left|A\right|\cdot\left|B\right|}
 \sum_{x\in A}\sum_{y\in B} d(x,y)</code>. Defines the proximity between two
clusters as the average distance between every pair of objects, one from
each cluster. It is also known as <strong>UPGMA</strong> or <strong>average-link</strong>.</p>
</dd>
</dl>



<h3>Value</h3>

<p>An <code><a href="stats.html#topic+hclust">stats::hclust()</a></code> object which describes the tree produced by the
clustering process.
</p>


<h3>Author(s)</h3>

<p>Eduardo Ruiz Sabajanes, <a href="mailto:eduardo.ruizs@edu.uah.es">eduardo.ruizs@edu.uah.es</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
cl &lt;- agglomerative_clustering(
  db5[1:6, ],
  'single',
  learn = TRUE,
  waiting = FALSE
)

</code></pre>

<hr>
<h2 id='correlation_clustering'>Hierarchical Correlation Clustering</h2><span id='topic+correlation_clustering'></span>

<h3>Description</h3>

<p>Performs hierarchical correlation clustering by applying weights, distance metrics, and other parameters
to analyze relationships between data points and a target.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>correlation_clustering(
  data,
  target = NULL,
  weight = c(),
  distance_method = "euclidean",
  normalize = TRUE,
  labels = NULL,
  learn = FALSE,
  waiting = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="correlation_clustering_+3A_data">data</code></td>
<td>
<p>A data frame containing the main data</p>
</td></tr>
<tr><td><code id="correlation_clustering_+3A_target">target</code></td>
<td>
<p>A data frame, numeric vector or matrix to use as correlation target. Default is NULL.</p>
</td></tr>
<tr><td><code id="correlation_clustering_+3A_weight">weight</code></td>
<td>
<p>A numeric vector of weights. Default is empty vector.</p>
</td></tr>
<tr><td><code id="correlation_clustering_+3A_distance_method">distance_method</code></td>
<td>
<p>A string specifying the distance metric to use. Options are:
</p>

<ul>
<li><p> &quot;euclidean&quot; - Euclidean distance
</p>
</li>
<li><p> &quot;manhattan&quot; - Manhattan distance
</p>
</li>
<li><p> &quot;canberra&quot; - Canberra distance
</p>
</li>
<li><p> &quot;chebyshev&quot; - Chebyshev distance
</p>
</li></ul>
</td></tr>
<tr><td><code id="correlation_clustering_+3A_normalize">normalize</code></td>
<td>
<p>A boolean parameter indicating whether to normalize weights. Default is TRUE.</p>
</td></tr>
<tr><td><code id="correlation_clustering_+3A_labels">labels</code></td>
<td>
<p>A string vector for graphical solution labeling. Default is NULL.</p>
</td></tr>
<tr><td><code id="correlation_clustering_+3A_learn">learn</code></td>
<td>
<p>A boolean indicating whether to show detailed algorithm explanations. Default is FALSE.</p>
</td></tr>
<tr><td><code id="correlation_clustering_+3A_waiting">waiting</code></td>
<td>
<p>A boolean controlling pauses between explanations. Default is TRUE.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function executes the complete hierarchical correlation method in the following steps:
</p>

<ol>
<li><p> The function transforms data into useful objects
</p>
</li>
<li><p> Creates the clusters
</p>
</li>
<li><p> Calculates the distance from the target to every cluster using the specified distance metric
</p>
</li>
<li><p> Orders the distances in ascending order
</p>
</li>
<li><p> Orders the clusters according to their distance from the previous step
</p>
</li>
<li><p> Shows the sorted clusters and the distances used
</p>
</li></ol>



<h3>Value</h3>

<p>An R object containing:
</p>

<ul>
<li><p> dendrogram - A hierarchical clustering dendrogram
</p>
</li>
<li><p> sortedValues - A data frame with the sorted cluster values
</p>
</li>
<li><p> distances - A data frame with the sorted distances
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Original authors:
</p>

<ul>
<li><p> Roberto Alcantara <a href="mailto:roberto.alcantara@edu.uah.es">roberto.alcantara@edu.uah.es</a>
</p>
</li>
<li><p> Juan Jose Cuadrado <a href="mailto:jjcg@uah.es">jjcg@uah.es</a>
</p>
</li>
<li><p> Universidad de Alcala de Henares
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>data &lt;- matrix(c(1,2,1,4,5,1,8,2,9,6,3,5,8,5,4), ncol=3)
dataFrame &lt;- data.frame(data)
target1 &lt;- c(1,2,3)
target2 &lt;- dataFrame[1,]
weight1 &lt;- c(1,6,3)
weight2 &lt;- c(0.1,0.6,0.3)

# Basic usage
correlation_clustering(dataFrame, target1)

# With weights
correlation_clustering(dataFrame, target1, weight1)

# Without weight normalization
correlation_clustering(dataFrame, target1, weight1, normalize = FALSE)

# Using Canberra distance with weights
correlation_clustering(dataFrame, target1, weight2, distance = "canberra", normalize = FALSE)

# With detailed explanations
correlation_clustering(dataFrame, target1, learn = TRUE)

</code></pre>

<hr>
<h2 id='db1'>Test Database 1</h2><span id='topic+db1'></span>

<h3>Description</h3>

<p>Test Database 1
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db1
</code></pre>


<h3>Format</h3>



<h4><code>db1</code></h4>

<p>A data frame with 500 rows and 2 columns.
</p>
<p>The data points form two concentric circles.
</p>


<hr>
<h2 id='db2'>Test Database 2</h2><span id='topic+db2'></span>

<h3>Description</h3>

<p>Test Database 2
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db2
</code></pre>


<h3>Format</h3>



<h4><code>db2</code></h4>

<p>A data frame with 500 rows and 2 columns.
</p>
<p>The data points form two moons.
</p>


<hr>
<h2 id='db3'>Test Database 3</h2><span id='topic+db3'></span>

<h3>Description</h3>

<p>Test Database 3
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db3
</code></pre>


<h3>Format</h3>



<h4><code>db3</code></h4>

<p>A data frame with 500 rows and 2 columns.
</p>
<p>The data points form three overlapping elliptical clusters of varying
densities.
</p>


<hr>
<h2 id='db4'>Test Database 4</h2><span id='topic+db4'></span>

<h3>Description</h3>

<p>Test Database 4
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db4
</code></pre>


<h3>Format</h3>



<h4><code>db4</code></h4>

<p>A data frame with 500 rows and 2 columns.
</p>
<p>The data points form three diagonal parallel segments.
</p>


<hr>
<h2 id='db5'>Test Database 5</h2><span id='topic+db5'></span>

<h3>Description</h3>

<p>Test Database 5
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db5
</code></pre>


<h3>Format</h3>



<h4><code>db5</code></h4>

<p>A data frame with 500 rows and 2 columns.
</p>
<p>The data points form three non-overlapping circular clusters of similar
density.
</p>


<hr>
<h2 id='db6'>Test Database 6</h2><span id='topic+db6'></span>

<h3>Description</h3>

<p>Test Database 6
</p>


<h3>Usage</h3>

<pre><code class='language-R'>db6
</code></pre>


<h3>Format</h3>



<h4><code>db6</code></h4>

<p>A data frame with 500 rows and 2 columns.
</p>
<p>The data points are uniformly distributed on the plane.
</p>


<hr>
<h2 id='dbscan'>Density Based Spatial Clustering of Applications with Noise (DBSCAN)</h2><span id='topic+dbscan'></span>

<h3>Description</h3>

<p>Perform DBSCAN clustering on a data matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dbscan(data, epsilon, min_pts = 4, learn = FALSE, waiting = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="dbscan_+3A_data">data</code></td>
<td>
<p>a set of observations, presented as a matrix-like object where
every row is a new observation.</p>
</td></tr>
<tr><td><code id="dbscan_+3A_epsilon">epsilon</code></td>
<td>
<p>how close two observations have to be to be considered neighbors.</p>
</td></tr>
<tr><td><code id="dbscan_+3A_min_pts">min_pts</code></td>
<td>
<p>the minimum amount of neighbors for a region to be considered
dense.</p>
</td></tr>
<tr><td><code id="dbscan_+3A_learn">learn</code></td>
<td>
<p>a Boolean determining whether intermediate logs explaining how
the algorithm works should be printed or not.</p>
</td></tr>
<tr><td><code id="dbscan_+3A_waiting">waiting</code></td>
<td>
<p>a Boolean determining whether the intermediate logs should be
printed in chunks waiting for user input before printing the next or not.</p>
</td></tr>
<tr><td><code id="dbscan_+3A_...">...</code></td>
<td>
<p>additional arguments passed to <code><a href="proxy.html#topic+dist">proxy::dist()</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The data given by <code>data</code> is clustered by the DBSCAN method,
which aims to partition the points into clusters such that the points in a
cluster are close to each other and the points in different clusters are far
away from each other. The clusters are defined as dense regions of points
separated by regions of low density.
</p>
<p>The DBSCAN method follows a 2 step process:
</p>

<ol>
<li><p> For each point, the neighborhood of radius <code>epsilon</code> is computed. If
the neighborhood contains at least <code>min_pts</code> points, then the point is
considered a <strong>core point</strong>. Otherwise, the point is considered an
<strong>outlier</strong>.
</p>
</li>
<li><p> For each core point, if the core point is not already assigned to a
cluster, a new cluster is created and the core point is assigned to it.
Then, the neighborhood of the core point is explored. If a point in the
neighborhood is a core point, then the neighborhood of that point is also
explored. This process is repeated until all points in the neighborhood have
been explored. If a point in the neighborhood is not already assigned to a
cluster, then it is assigned to the cluster of the core point.
</p>
</li></ol>

<p>Whatever points are not assigned to a cluster are considered outliers.
</p>


<h3>Value</h3>

<p>A <code><a href="clustlearn.html#topic+dbscan">clustlearn::dbscan()</a></code> object. It is a list with the following
components:
</p>

<table>
<tr>
 <td style="text-align: left;">
<code>cluster</code> </td><td style="text-align: left;"> a vector of integers (from 0 to <code>max(cl$cluster)</code>)
indicating the cluster to which each point belongs. Points in cluster number
0 are considered outliers. </td>
</tr>
<tr>
 <td style="text-align: left;">
<code>epsilon</code> </td><td style="text-align: left;"> the value of <code>epsilon</code> used. </td>
</tr>
<tr>
 <td style="text-align: left;">
<code>min_pts</code> </td><td style="text-align: left;"> the value of <code>min_pts</code> used. </td>
</tr>
<tr>
 <td style="text-align: left;">
<code>size</code> </td><td style="text-align: left;"> a vector with the number of data points belonging to each
cluster (where the first element is the number of outliers). </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>



<h3>Author(s)</h3>

<p>Eduardo Ruiz Sabajanes, <a href="mailto:eduardo.ruizs@edu.uah.es">eduardo.ruizs@edu.uah.es</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Basic usage
data &lt;- db5[1:20,]
result &lt;- dbscan(data, epsilon = 0.3, min_pts = 4)

# With learning output
result &lt;- dbscan(data, epsilon = 0.3, min_pts = 4, learn = TRUE)

# Visualize results
plot(data, col = result$cluster + 1, pch = 20)

</code></pre>

<hr>
<h2 id='divisive_clustering'>Divisive Hierarchical Clustering</h2><span id='topic+divisive_clustering'></span>

<h3>Description</h3>

<p>Perform a hierarchical Divisive cluster analysis on a set of
observations
</p>


<h3>Usage</h3>

<pre><code class='language-R'>divisive_clustering(data, learn = FALSE, waiting = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="divisive_clustering_+3A_data">data</code></td>
<td>
<p>a set of observations, presented as a matrix-like object where
every row is a new observation.</p>
</td></tr>
<tr><td><code id="divisive_clustering_+3A_learn">learn</code></td>
<td>
<p>a Boolean determining whether intermediate logs explaining how
the algorithm works should be printed or not.</p>
</td></tr>
<tr><td><code id="divisive_clustering_+3A_waiting">waiting</code></td>
<td>
<p>a Boolean determining whether the intermediate logs should be
printed in chunks waiting for user input before printing the next or not.</p>
</td></tr>
<tr><td><code id="divisive_clustering_+3A_...">...</code></td>
<td>
<p>additional arguments passed to <code><a href="clustlearn.html#topic+kmeans">clustlearn::kmeans()</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function performs a hierarchical cluster analysis for the
<code class="reqn">n</code> objects being clustered. The definition of a set of clusters using
this method follows a <code class="reqn">n</code> step process, which repeats until <code class="reqn">n</code>
clusters remain:
</p>

<ol>
<li><p> Initially, each object is assigned to the same cluster. The sum of
squares of the distances between objects and their cluster center is
computed.
</p>
</li>
<li><p> The cluster with the highest sum of squares is split into two using
the k-means algorithm. This step is repeated until <code class="reqn">n</code> clusters remain.
</p>
</li></ol>



<h3>Value</h3>

<p>An <code><a href="stats.html#topic+hclust">stats::hclust()</a></code> object which describes the tree produced by the
clustering process.
</p>


<h3>Author(s)</h3>

<p>Eduardo Ruiz Sabajanes, <a href="mailto:eduardo.ruizs@edu.uah.es">eduardo.ruizs@edu.uah.es</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### !! This algorithm is very slow, so we'll only test it on some datasets !!

### Helper function
test &lt;- function(db, k) {
  # Save old par settings
  old_par &lt;- par(no.readonly = TRUE)
  # Ensure par settings are restored when function exits
  on.exit(par(old_par))

  print(cl &lt;- divisive_clustering(db, max_iterations = 5))
  par(mfrow = c(1, 2))
  plot(db, col = cutree(cl, k), asp = 1, pch = 20)
  h &lt;- rev(cl$height)[50]
  clu &lt;- as.hclust(cut(as.dendrogram(cl), h = h)$upper)
  ctr &lt;- unique(cutree(cl, k)[cl$order])
  plot(clu, labels = FALSE, hang = -1, xlab = "Cluster", sub = "", main = "")
  rect.hclust(clu, k = k, border = ctr)
}

### Example 1
# test(db1, 2)

### Example 2
# test(db2, 2)

### Example 3
# test(db3, 3)

### Example 4
# test(db4, 3)

### Example 5
test(db5, 3)

### Example 6
test(db6, 3)

### Example 7 (with explanations, no plots)
  cl &lt;- divisive_clustering(
  db5[1:6, ],
  learn = TRUE,
  waiting = FALSE
)

</code></pre>

<hr>
<h2 id='gaussian_mixture'>Gaussian mixture model</h2><span id='topic+gaussian_mixture'></span>

<h3>Description</h3>

<p>Perform Gaussian mixture model clustering on a data matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gaussian_mixture(data, k, max_iter = 10, learn = FALSE, waiting = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="gaussian_mixture_+3A_data">data</code></td>
<td>
<p>a set of observations, presented as a matrix-like object where
every row is a new observation.</p>
</td></tr>
<tr><td><code id="gaussian_mixture_+3A_k">k</code></td>
<td>
<p>the number of clusters to find.</p>
</td></tr>
<tr><td><code id="gaussian_mixture_+3A_max_iter">max_iter</code></td>
<td>
<p>the maximum number of iterations to perform.</p>
</td></tr>
<tr><td><code id="gaussian_mixture_+3A_learn">learn</code></td>
<td>
<p>a Boolean determining whether intermediate logs explaining how
the algorithm works should be printed or not.</p>
</td></tr>
<tr><td><code id="gaussian_mixture_+3A_waiting">waiting</code></td>
<td>
<p>a Boolean determining whether the intermediate logs should be
printed in chunks waiting for user input before printing the next or not.</p>
</td></tr>
<tr><td><code id="gaussian_mixture_+3A_...">...</code></td>
<td>
<p>additional arguments passed to <code><a href="clustlearn.html#topic+kmeans">clustlearn::kmeans()</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The data given by <code>data</code> is clustered by the model-based
algorithm that assumes every cluster follows a normal distribution, thus
the name &quot;Gaussian Mixture&quot;.
</p>
<p>The normal distributions are parameterized by their mean vector, covariance
matrix and mixing proportion. Initially, the mean vector is set to the
cluster centers obtained by performing a k-means clustering on the data,
the covariance matrix is set to the covariance matrix of the data points
belonging to each cluster and the mixing proportion is set to the proportion
of data points belonging to each cluster. The algorithm then optimizes the
gaussian models by means of the Expectation Maximization (EM) algorithm.
</p>
<p>The EM algorithm is an iterative algorithm that alternates between two steps:
</p>

<dl>
<dt>Expectation</dt><dd><p>Compute how much is each observation expected to belong
to each component of the GMM.</p>
</dd>
<dt>Maximization</dt><dd><p>Recompute the GMM according to the expectations from
the E-step in order to maximize them.</p>
</dd>
</dl>

<p>The algorithm stops when the changes in the expectations are sufficiently
small or when a maximum number of iterations is reached.
</p>


<h3>Value</h3>

<p>A <code><a href="clustlearn.html#topic+gaussian_mixture">clustlearn::gaussian_mixture()</a></code> object. It is a list with the
following components:
</p>

<table>
<tr>
 <td style="text-align: left;">
<code>cluster</code> </td><td style="text-align: left;"> a vector of integers (from <code>1:k</code>) indicating the
cluster to which each point belongs. </td>
</tr>
<tr>
 <td style="text-align: left;">
<code>mu</code> </td><td style="text-align: left;"> the final mean parameters. </td>
</tr>
<tr>
 <td style="text-align: left;">
<code>sigma</code> </td><td style="text-align: left;"> the final covariance matrices. </td>
</tr>
<tr>
 <td style="text-align: left;">
<code>lambda</code> </td><td style="text-align: left;"> the final mixing proportions. </td>
</tr>
<tr>
 <td style="text-align: left;">
<code>loglik</code> </td><td style="text-align: left;"> the final log likelihood. </td>
</tr>
<tr>
 <td style="text-align: left;">
<code>all.loglik</code> </td><td style="text-align: left;"> a vector of each iteration's log likelihood. </td>
</tr>
<tr>
 <td style="text-align: left;">
<code>iter</code> </td><td style="text-align: left;"> the number of iterations performed. </td>
</tr>
<tr>
 <td style="text-align: left;">
<code>size</code> </td><td style="text-align: left;"> a vector with the number of data points belonging to each
cluster. </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>



<h3>Author(s)</h3>

<p>Eduardo Ruiz Sabajanes, <a href="mailto:eduardo.ruizs@edu.uah.es">eduardo.ruizs@edu.uah.es</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### !! This algorithm is very slow, so we'll only test it on some datasets !!

### Helper functions
dmnorm &lt;- function(x, mu, sigma) {
  k &lt;- ncol(sigma)

  x  &lt;- as.matrix(x)
  diff &lt;- t(t(x) - mu)

  num &lt;- exp(-1 / 2 * diag(diff %*% solve(sigma) %*% t(diff)))
  den &lt;- sqrt(((2 * pi)^k) * det(sigma))
  num / den
}

test &lt;- function(db, k) {
  print(cl &lt;- gaussian_mixture(db, k, 100))

  x &lt;- seq(min(db[, 1]), max(db[, 1]), length.out = 100)
  y &lt;- seq(min(db[, 2]), max(db[, 2]), length.out = 100)

  plot(db, col = cl$cluster, asp = 1, pch = 20)
  for (i in seq_len(k)) {
    m &lt;- cl$mu[i, ]
    s &lt;- cl$sigma[i, , ]
    f &lt;- function(x, y) cl$lambda[i] * dmnorm(cbind(x, y), m, s)
    z &lt;- outer(x, y, f)
    contour(x, y, z, col = i, add = TRUE)
  }
}

### Example 1
test(db1[1:250,], 2)

### Example 2 (with explanations, no plots)
cl &lt;- gaussian_mixture(
  db5[1:10, ],
  3,
  learn = TRUE,
  waiting = FALSE
)

</code></pre>

<hr>
<h2 id='genetic_kmeans'>Genetic K-Means Clustering</h2><span id='topic+genetic_kmeans'></span>

<h3>Description</h3>

<p>Performs Genetic K-Means clustering on a data matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>genetic_kmeans(
  data,
  k,
  population_size = 10,
  mut_probability = 0.5,
  max_generations = 10,
  learn = FALSE,
  waiting = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="genetic_kmeans_+3A_data">data</code></td>
<td>
<p>a set of observations, presented as a matrix-like object where
every row is a new observation.</p>
</td></tr>
<tr><td><code id="genetic_kmeans_+3A_k">k</code></td>
<td>
<p>the number of clusters.</p>
</td></tr>
<tr><td><code id="genetic_kmeans_+3A_population_size">population_size</code></td>
<td>
<p>the number of individuals in the population.</p>
</td></tr>
<tr><td><code id="genetic_kmeans_+3A_mut_probability">mut_probability</code></td>
<td>
<p>the probability of a mutation occurring.</p>
</td></tr>
<tr><td><code id="genetic_kmeans_+3A_max_generations">max_generations</code></td>
<td>
<p>the maximum number of iterations allowed.</p>
</td></tr>
<tr><td><code id="genetic_kmeans_+3A_learn">learn</code></td>
<td>
<p>a Boolean determining whether intermediate logs explaining how
the algorithm works should be printed or not.</p>
</td></tr>
<tr><td><code id="genetic_kmeans_+3A_waiting">waiting</code></td>
<td>
<p>a Boolean determining whether the intermediate logs should be
printed in chunks waiting for user input before printing the next or not.</p>
</td></tr>
<tr><td><code id="genetic_kmeans_+3A_...">...</code></td>
<td>
<p>additional arguments passed to <code><a href="proxy.html#topic+dist">proxy::dist()</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A kmeans object as returned by the original kmeans function.
</p>


<h3>Author(s)</h3>

<p>Eduardo Ruiz Sabajanes, <a href="mailto:eduardo.ruizs@edu.uah.es">eduardo.ruizs@edu.uah.es</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### Example 1: Simple usage with circles dataset
result1 &lt;- genetic_kmeans(db1[1:20,], 2, learn = TRUE, waiting = FALSE)

### Example 2: Moons dataset with different population size
result2 &lt;- genetic_kmeans(db2[1:20,], 2, population_size = 20,
                         learn = TRUE, waiting = FALSE)

### Example 3: Varying density clusters with different mutation probability
result3 &lt;- genetic_kmeans(db3[1:20,], 3, mut_probability = 0.7,
                         learn = TRUE, waiting = FALSE)

### Example 4: Well-separated clusters with larger population
result5 &lt;- genetic_kmeans(db5[1:20,], 3, population_size = 30,
                         mut_probability = 0.6, learn = TRUE, waiting = FALSE)

### Example 5: Using different parameters combinations
result6 &lt;- genetic_kmeans(db1[1:20,], 2,
                         population_size = 15,
                         mut_probability = 0.8,
                         max_generations = 15,
                         learn = TRUE,
                         waiting = FALSE)

</code></pre>

<hr>
<h2 id='gka_allele_mutation'>Allele mutation probability computation</h2><span id='topic+gka_allele_mutation'></span>

<h3>Description</h3>

<p>Allele mutation probability computation
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gka_allele_mutation(data, k, centers, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="gka_allele_mutation_+3A_data">data</code></td>
<td>
<p>a set of observations, presented as a matrix-like object where
every row is a new observation. The matrix is of size <code>n</code> by <code>m</code>.</p>
</td></tr>
<tr><td><code id="gka_allele_mutation_+3A_k">k</code></td>
<td>
<p>the number of clusters.</p>
</td></tr>
<tr><td><code id="gka_allele_mutation_+3A_centers">centers</code></td>
<td>
<p>a matrix of size <code>k</code> by <code>m</code> with the cluster centers</p>
</td></tr>
<tr><td><code id="gka_allele_mutation_+3A_...">...</code></td>
<td>
<p>additional arguments passed to <code><a href="proxy.html#topic+dist">proxy::dist()</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a matrix of size <code>n</code> by <code>k</code> with the probability of each
allele mutating to a specific cluster.
</p>


<h3>Author(s)</h3>

<p>Eduardo Ruiz Sabajanes, <a href="mailto:eduardo.ruizs@edu.uah.es">eduardo.ruizs@edu.uah.es</a>
</p>

<hr>
<h2 id='gka_centers'>Centroid computation</h2><span id='topic+gka_centers'></span>

<h3>Description</h3>

<p>Centroid computation
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gka_centers(data, k, population)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="gka_centers_+3A_data">data</code></td>
<td>
<p>a set of observations, presented as a matrix-like object where
every row is a new observation. The matrix is of size <code>n</code> by <code>m</code>.</p>
</td></tr>
<tr><td><code id="gka_centers_+3A_k">k</code></td>
<td>
<p>the number of clusters.</p>
</td></tr>
<tr><td><code id="gka_centers_+3A_population">population</code></td>
<td>
<p>a matrix of size <code>p</code> by <code>n</code> with the cluster
assignments for each observation.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a 3D array of size <code>p</code> by <code>k</code> by <code>m</code>.
</p>


<h3>Author(s)</h3>

<p>Eduardo Ruiz Sabajanes, <a href="mailto:eduardo.ruizs@edu.uah.es">eduardo.ruizs@edu.uah.es</a>
</p>

<hr>
<h2 id='gka_chromosome_fix'>Chromosome fixing method</h2><span id='topic+gka_chromosome_fix'></span>

<h3>Description</h3>

<p>This method fixes chromosomes which do not have at least one
observation assigned to each cluster.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gka_chromosome_fix(population, k)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="gka_chromosome_fix_+3A_population">population</code></td>
<td>
<p>a matrix of size <code>p</code> by <code>n</code> with the cluster
assignments for each observation.</p>
</td></tr>
<tr><td><code id="gka_chromosome_fix_+3A_k">k</code></td>
<td>
<p>the number of clusters.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a matrix of size <code>p</code> by <code>n</code> with the cluster assignments
for each observation.
</p>


<h3>Author(s)</h3>

<p>Eduardo Ruiz Sabajanes, <a href="mailto:eduardo.ruizs@edu.uah.es">eduardo.ruizs@edu.uah.es</a>
</p>

<hr>
<h2 id='gka_crossover'>Crossover method i.e. K-Means Operator</h2><span id='topic+gka_crossover'></span>

<h3>Description</h3>

<p>K-Means Operator (KMO) which replaces the crossover operator in
the Genetic K-Means algorithm (GKA).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gka_crossover(data, centers)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="gka_crossover_+3A_data">data</code></td>
<td>
<p>a set of observations, presented as a matrix-like object where
every row is a new observation. The matrix is of size <code>n</code> by <code>m</code>.</p>
</td></tr>
<tr><td><code id="gka_crossover_+3A_centers">centers</code></td>
<td>
<p>a matrix of size <code>k</code> by <code>m</code> with the cluster centers
for a specific individual in the population.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a vector of size <code>n</code> with the cluster assignments for each
observation i.e. a new chromosome.
</p>


<h3>Author(s)</h3>

<p>Eduardo Ruiz Sabajanes, <a href="mailto:eduardo.ruizs@edu.uah.es">eduardo.ruizs@edu.uah.es</a>
</p>

<hr>
<h2 id='gka_fitness'>Fitness function</h2><span id='topic+gka_fitness'></span>

<h3>Description</h3>

<p>Fitness function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gka_fitness(twcv)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="gka_fitness_+3A_twcv">twcv</code></td>
<td>
<p>a vector of size <code>p</code> with the total within cluster
variation of each individual in the population.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a vector of size <code>p</code> with the fitness of each individual in the
population.
</p>


<h3>Author(s)</h3>

<p>Eduardo Ruiz Sabajanes, <a href="mailto:eduardo.ruizs@edu.uah.es">eduardo.ruizs@edu.uah.es</a>
</p>

<hr>
<h2 id='gka_initialization'>Initialization method</h2><span id='topic+gka_initialization'></span>

<h3>Description</h3>

<p>Initialization method
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gka_initialization(n, p, k)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="gka_initialization_+3A_n">n</code></td>
<td>
<p>the number of observations in the data.</p>
</td></tr>
<tr><td><code id="gka_initialization_+3A_p">p</code></td>
<td>
<p>the number of individuals in the population.</p>
</td></tr>
<tr><td><code id="gka_initialization_+3A_k">k</code></td>
<td>
<p>the number of clusters.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a matrix of size <code>p</code> by <code>n</code> with the cluster assignments
for each observation.
</p>


<h3>Author(s)</h3>

<p>Eduardo Ruiz Sabajanes, <a href="mailto:eduardo.ruizs@edu.uah.es">eduardo.ruizs@edu.uah.es</a>
</p>

<hr>
<h2 id='gka_mutation'>Mutation method</h2><span id='topic+gka_mutation'></span>

<h3>Description</h3>

<p>Mutation method
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gka_mutation(chromosome, prob, k, mut_probability)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="gka_mutation_+3A_chromosome">chromosome</code></td>
<td>
<p>a vector of size <code>n</code> with the cluster assignments for
each observation.</p>
</td></tr>
<tr><td><code id="gka_mutation_+3A_prob">prob</code></td>
<td>
<p>a matrix of size <code>n</code> by <code>k</code> with the probability of
each allele mutating to a specific cluster.</p>
</td></tr>
<tr><td><code id="gka_mutation_+3A_k">k</code></td>
<td>
<p>the number of clusters.</p>
</td></tr>
<tr><td><code id="gka_mutation_+3A_mut_probability">mut_probability</code></td>
<td>
<p>the probability of a mutation occurring.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a vector of size <code>n</code> with the cluster assignments for each
observation i.e. a new chromosome.
</p>


<h3>Author(s)</h3>

<p>Eduardo Ruiz Sabajanes, <a href="mailto:eduardo.ruizs@edu.uah.es">eduardo.ruizs@edu.uah.es</a>
</p>

<hr>
<h2 id='gka_selection'>Selection method</h2><span id='topic+gka_selection'></span>

<h3>Description</h3>

<p>Selection method
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gka_selection(p, fitness)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="gka_selection_+3A_p">p</code></td>
<td>
<p>the number of individuals in the population.</p>
</td></tr>
<tr><td><code id="gka_selection_+3A_fitness">fitness</code></td>
<td>
<p>a vector of size <code>p</code> with the fitness of each individual
in the population.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the index of the individual selected for reproduction.
</p>


<h3>Author(s)</h3>

<p>Eduardo Ruiz Sabajanes, <a href="mailto:eduardo.ruizs@edu.uah.es">eduardo.ruizs@edu.uah.es</a>
</p>

<hr>
<h2 id='gka_twcv'>Total Within Cluster Variation (TWCV) computation</h2><span id='topic+gka_twcv'></span>

<h3>Description</h3>

<p>Total Within Cluster Variation (TWCV) computation
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gka_twcv(data, k, population, centers)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="gka_twcv_+3A_data">data</code></td>
<td>
<p>a set of observations, presented as a matrix-like object where
every row is a new observation. The matrix is of size <code>n</code> by <code>m</code>.</p>
</td></tr>
<tr><td><code id="gka_twcv_+3A_k">k</code></td>
<td>
<p>the number of clusters.</p>
</td></tr>
<tr><td><code id="gka_twcv_+3A_population">population</code></td>
<td>
<p>a matrix of size <code>p</code> by <code>n</code> with the cluster
assignments for each observation.</p>
</td></tr>
<tr><td><code id="gka_twcv_+3A_centers">centers</code></td>
<td>
<p>a 3D array of size <code>p</code> by <code>k</code> by <code>m</code> with the
cluster centers for each individual in the population.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a vector of size <code>p</code> with the total within cluster variation of
each individual in the population.
</p>


<h3>Author(s)</h3>

<p>Eduardo Ruiz Sabajanes, <a href="mailto:eduardo.ruizs@edu.uah.es">eduardo.ruizs@edu.uah.es</a>
</p>

<hr>
<h2 id='kmeans_'>K-Means Clustering</h2><span id='topic+kmeans_'></span>

<h3>Description</h3>

<p>Perform K-Means clustering on a data matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kmeans_(
  data,
  centers,
  max_iterations = 10,
  initialization = "kmeans++",
  learn = FALSE,
  waiting = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="kmeans__+3A_data">data</code></td>
<td>
<p>a set of observations, presented as a matrix-like object where
every row is a new observation.</p>
</td></tr>
<tr><td><code id="kmeans__+3A_centers">centers</code></td>
<td>
<p>either the number of clusters or a set of initial cluster
centers. If a number, the centers are chosen according to the
<code>initialization</code> parameter.</p>
</td></tr>
<tr><td><code id="kmeans__+3A_max_iterations">max_iterations</code></td>
<td>
<p>the maximum number of iterations allowed.</p>
</td></tr>
<tr><td><code id="kmeans__+3A_initialization">initialization</code></td>
<td>
<p>the initialization method to be used. This should be
one of <code>"random"</code> or <code>"kmeans++"</code>. The latter is the default.</p>
</td></tr>
<tr><td><code id="kmeans__+3A_learn">learn</code></td>
<td>
<p>a Boolean determining whether intermediate logs explaining how
the algorithm works should be printed or not.</p>
</td></tr>
<tr><td><code id="kmeans__+3A_waiting">waiting</code></td>
<td>
<p>a Boolean determining whether the intermediate logs should be
printed in chunks waiting for user input before printing the next or not.</p>
</td></tr>
<tr><td><code id="kmeans__+3A_...">...</code></td>
<td>
<p>additional arguments passed to <code><a href="proxy.html#topic+dist">proxy::dist()</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The data given by <code>data</code> is clustered by the <code class="reqn">k</code>-means
method, which aims to partition the points into <code class="reqn">k</code> groups such that the
sum of squares from points to the assigned cluster centers is minimized. At
the minimum, all cluster centers are at the mean of their Voronoi sets (the
set of data points which are nearest to the cluster center).
</p>
<p>The <code class="reqn">k</code>-means method follows a 2 to <code class="reqn">n</code> step process:
</p>

<ol>
<li><p> The first step can be subdivided into 3 steps: </p>

<ol>
<li><p> Selection of the number <code class="reqn">k</code> of clusters, into which the data is
going to be grouped and of which the centers will be the representatives.
This is determined through the use of the <code>centers</code> parameter.
</p>
</li>
<li><p> Computation of the distance from each data point to each center.
</p>
</li>
<li><p> Assignment of each observation to a cluster. The observation is
assigned to the cluster represented by the nearest center.
</p>
</li></ol>

</li>
<li><p> The next steps are just like the first but for the first sub-step:
</p>

<ol>
<li><p> Computation of the new centers. The center of each cluster is
computed as the mean of the observations assigned to said cluster.
</p>
</li></ol>

</li></ol>

<p>The algorithm stops once the centers in step <code class="reqn">n+1</code> are the same as the
ones in step <code class="reqn">n</code>. However, this convergence does not always take place.
For this reason, the algorithm also stops once a maximum number of iterations
<code>max_iterations</code> is reached.
</p>
<p>The <code>initialization</code> methods provided by this function are:
</p>

<dl>
<dt><code>random</code></dt><dd><p>A set of <code>centers</code> observations is chosen at
random from the data as the initial centers.</p>
</dd>
<dt><code>kmeans++</code></dt><dd><p>The <code>centers</code> observations are chosen using the
<strong>kmeans++</strong> algorithm. This algorithm chooses the first center at
random and then chooses the next center from the remaining observations with
probability proportional to the square distance to the closest center. This
process is repeated until <code>centers</code> centers are chosen.</p>
</dd>
</dl>



<h3>Value</h3>

<p>A <code><a href="stats.html#topic+kmeans">stats::kmeans()</a></code> object.
</p>


<h3>Author(s)</h3>

<p>Eduardo Ruiz Sabajanes, <a href="mailto:eduardo.ruizs@edu.uah.es">eduardo.ruizs@edu.uah.es</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### Voronoi tesselation
voronoi &lt;- suppressMessages(suppressWarnings(require(deldir)))
cols &lt;- c(
  "#00000019",
  "#DF536B19",
  "#61D04F19",
  "#2297E619",
  "#28E2E519",
  "#CD0BBC19",
  "#F5C71019",
  "#9E9E9E19"
)

### Helper function
test &lt;- function(db, k) {
  print(cl &lt;- kmeans_(db, k, 100))
  plot(db, col = cl$cluster, asp = 1, pch = 20)
  points(cl$centers, col = seq_len(k), pch = 13, cex = 2, lwd = 2)

  if (voronoi) {
    x &lt;- c(min(db[, 1]), max(db[, 1]))
    dx &lt;- c(x[1] - x[2], x[2] - x[1])
    y &lt;- c(min(db[, 2]), max(db[, 2]))
    dy &lt;- c(y[1] - y[2], y[2] - y[1])
    tesselation &lt;- deldir(
      cl$centers[, 1],
      cl$centers[, 2],
      rw = c(x + dx, y + dy)
    )
    tiles &lt;- tile.list(tesselation)

    plot(
      tiles,
      asp = 1,
      add = TRUE,
      showpoints = FALSE,
      border = "#00000000",
      fillcol = cols
    )
  }
}

### Example 1
test(db1, 2)

### Example 2
test(db2, 2)

### Example 3
test(db3, 3)

### Example 4
test(db4, 3)

### Example 5
test(db5, 3)

### Example 6
test(db6, 3)

### Example 7 (with explanations, no plots)
cl &lt;- kmeans_(
  db5[1:20, ],
  3,
  learn = TRUE,
  waiting = FALSE
)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
