<!DOCTYPE html><html lang="en"><head><title>Help for package RWNN</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {RWNN}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#RWNN-package'><p>RWNN: Random Weight Neural Networks</p></a></li>
<li><a href='#ae_rwnn'><p>Auto-encoder pre-trained random weight neural networks</p></a></li>
<li><a href='#bag_rwnn'><p>Bagging random weight neural networks</p></a></li>
<li><a href='#boost_rwnn'><p>Boosting random weight neural networks</p></a></li>
<li><a href='#classify'><p>Classifier</p></a></li>
<li><a href='#control_rwnn'><p>rwnn control function</p></a></li>
<li><a href='#ed_rwnn'><p>Ensemble deep random weight neural networks</p></a></li>
<li><a href='#ERWNN-object'><p>An ERWNN-object</p></a></li>
<li><a href='#example_data'><p>Example data</p></a></li>
<li><a href='#predict.ERWNN'><p>Predicting targets of an ERWNN-object</p></a></li>
<li><a href='#predict.RWNN'><p>Predicting targets of an RWNN-object</p></a></li>
<li><a href='#reduce_network'><p>Reduce the weights of a random weight neural network.</p></a></li>
<li><a href='#rwnn'><p>Random weight neural networks</p></a></li>
<li><a href='#RWNN-object'><p>An RWNN-object</p></a></li>
<li><a href='#stack_rwnn'><p>Stacking random weight neural networks</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Random Weight Neural Networks</td>
</tr>
<tr>
<td>Version:</td>
<td>0.4</td>
</tr>
<tr>
<td>Date:</td>
<td>2024-08-29</td>
</tr>
<tr>
<td>Description:</td>
<td>Creation, estimation, and prediction of random weight neural networks (RWNN), Schmidt et al. (1992) &lt;<a href="https://doi.org/10.1109%2FICPR.1992.201708">doi:10.1109/ICPR.1992.201708</a>&gt;, including popular variants like extreme learning machines, Huang et al. (2006) &lt;<a href="https://doi.org/10.1016%2Fj.neucom.2005.12.126">doi:10.1016/j.neucom.2005.12.126</a>&gt;, sparse RWNN, Zhang et al. (2019) &lt;<a href="https://doi.org/10.1016%2Fj.neunet.2019.01.007">doi:10.1016/j.neunet.2019.01.007</a>&gt;, and deep RWNN, Henríquez et al. (2018) &lt;<a href="https://doi.org/10.1109%2FIJCNN.2018.8489703">doi:10.1109/IJCNN.2018.8489703</a>&gt;. It further allows for the creation of ensemble RWNNs like bagging RWNN, Sui et al. (2021) &lt;<a href="https://doi.org/10.1109%2FECCE47101.2021.9595113">doi:10.1109/ECCE47101.2021.9595113</a>&gt;, boosting RWNN, stacking RWNN, and ensemble deep RWNN, Shi et al. (2021) &lt;<a href="https://doi.org/10.1016%2Fj.patcog.2021.107978">doi:10.1016/j.patcog.2021.107978</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>Imports:</td>
<td>methods, quadprog, randtoolbox, Rcpp (&ge; 1.0.4.6), stats,
utils</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.1</td>
</tr>
<tr>
<td>Suggests:</td>
<td>tinytest</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.1.0)</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-08-29 07:03:46 UTC; svilsen</td>
</tr>
<tr>
<td>Author:</td>
<td>Søren B. Vilsen [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Søren B. Vilsen &lt;svilsen@math.aau.dk&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-09-03 14:50:06 UTC</td>
</tr>
</table>
<hr>
<h2 id='RWNN-package'>RWNN: Random Weight Neural Networks</h2><span id='topic+RWNN'></span><span id='topic+RWNN-package'></span>

<h3>Description</h3>

<p>Creation, estimation, and prediction of random weight neural networks (RWNN), Schmidt et al. (1992) <a href="https://doi.org/10.1109/ICPR.1992.201708">doi:10.1109/ICPR.1992.201708</a>, including popular variants like extreme learning machines, Huang et al. (2006) <a href="https://doi.org/10.1016/j.neucom.2005.12.126">doi:10.1016/j.neucom.2005.12.126</a>, sparse RWNN, Zhang et al. (2019) <a href="https://doi.org/10.1016/j.neunet.2019.01.007">doi:10.1016/j.neunet.2019.01.007</a>, and deep RWNN, Henríquez et al. (2018) <a href="https://doi.org/10.1109/IJCNN.2018.8489703">doi:10.1109/IJCNN.2018.8489703</a>. It further allows for the creation of ensemble RWNNs like bagging RWNN, Sui et al. (2021) <a href="https://doi.org/10.1109/ECCE47101.2021.9595113">doi:10.1109/ECCE47101.2021.9595113</a>, boosting RWNN, stacking RWNN, and ensemble deep RWNN, Shi et al. (2021) <a href="https://doi.org/10.1016/j.patcog.2021.107978">doi:10.1016/j.patcog.2021.107978</a>.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Søren B. Vilsen <a href="mailto:svilsen@math.aau.dk">svilsen@math.aau.dk</a>
</p>
<p>Søren B. Vilsen &lt;svilsen@math.aau.dk&gt;
</p>

<hr>
<h2 id='ae_rwnn'>Auto-encoder pre-trained random weight neural networks</h2><span id='topic+ae_rwnn'></span><span id='topic+ae_rwnn.formula'></span>

<h3>Description</h3>

<p>Set-up and estimate weights of a random weight neural network using an auto-encoder for unsupervised pre-training of the hidden weights.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ae_rwnn(
  formula,
  data = NULL,
  n_hidden = c(),
  lambda = NULL,
  method = "l1",
  type = NULL,
  control = list()
)

## S3 method for class 'formula'
ae_rwnn(
  formula,
  data = NULL,
  n_hidden = c(),
  lambda = NULL,
  method = "l1",
  type = NULL,
  control = list()
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ae_rwnn_+3A_formula">formula</code></td>
<td>
<p>A <a href="stats.html#topic+formula">formula</a> specifying features and targets used to estimate the parameters of the output-layer.</p>
</td></tr>
<tr><td><code id="ae_rwnn_+3A_data">data</code></td>
<td>
<p>A data-set (either a <a href="base.html#topic+data.frame">data.frame</a> or a <a href="tibble.html#topic+tibble">tibble</a>) used to estimate the parameters of the output-layer.</p>
</td></tr>
<tr><td><code id="ae_rwnn_+3A_n_hidden">n_hidden</code></td>
<td>
<p>A vector of integers designating the number of neurons in each of the hidden-layers (the length of the list is taken as the number of hidden-layers).</p>
</td></tr>
<tr><td><code id="ae_rwnn_+3A_lambda">lambda</code></td>
<td>
<p>A vector of two penalisation constants used when encoding the hidden-weights and training the output-weights, respectively.</p>
</td></tr>
<tr><td><code id="ae_rwnn_+3A_method">method</code></td>
<td>
<p>The penalisation type used for the auto-encoder (either <code>"l1"</code> or <code>"l2"</code>).</p>
</td></tr>
<tr><td><code id="ae_rwnn_+3A_type">type</code></td>
<td>
<p>A string indicating whether this is a regression or classification problem.</p>
</td></tr>
<tr><td><code id="ae_rwnn_+3A_control">control</code></td>
<td>
<p>A list of additional arguments passed to the <a href="#topic+control_rwnn">control_rwnn</a> function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An <a href="#topic+RWNN-object">RWNN-object</a>.
</p>


<h3>References</h3>

<p>Zhang Y., Wu J., Cai Z., Du B., Yu P.S. (2019) &quot;An unsupervised parameter learning model for RVFL neural network.&quot; <em>Neural Networks</em>, 112, 85-97.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n_hidden &lt;- c(20, 15, 10, 5)
lambda &lt;- c(2, 0.01)

## Using L1-norm in the auto-encoder (sparse solution)
m &lt;- ae_rwnn(y ~ ., data = example_data, n_hidden = n_hidden, lambda = lambda, method = "l1")

## Using L2-norm in the auto-encoder (dense solution)
m &lt;- ae_rwnn(y ~ ., data = example_data, n_hidden = n_hidden, lambda = lambda, method = "l2")

</code></pre>

<hr>
<h2 id='bag_rwnn'>Bagging random weight neural networks</h2><span id='topic+bag_rwnn'></span><span id='topic+bag_rwnn.formula'></span>

<h3>Description</h3>

<p>Use bootstrap aggregation to reduce the variance of random weight neural network models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bag_rwnn(
  formula,
  data = NULL,
  n_hidden = c(),
  lambda = NULL,
  B = 100,
  method = NULL,
  type = NULL,
  control = list()
)

## S3 method for class 'formula'
bag_rwnn(
  formula,
  data = NULL,
  n_hidden = c(),
  lambda = NULL,
  B = 100,
  method = NULL,
  type = NULL,
  control = list()
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="bag_rwnn_+3A_formula">formula</code></td>
<td>
<p>A <a href="stats.html#topic+formula">formula</a> specifying features and targets used to estimate the parameters of the output layer.</p>
</td></tr>
<tr><td><code id="bag_rwnn_+3A_data">data</code></td>
<td>
<p>A data-set (either a <a href="base.html#topic+data.frame">data.frame</a> or a <a href="tibble.html#topic+tibble">tibble</a>) used to estimate the parameters of the output layer.</p>
</td></tr>
<tr><td><code id="bag_rwnn_+3A_n_hidden">n_hidden</code></td>
<td>
<p>A vector of integers designating the number of neurons in each of the hidden layers (the length of the list is taken as the number of hidden layers).</p>
</td></tr>
<tr><td><code id="bag_rwnn_+3A_lambda">lambda</code></td>
<td>
<p>The penalisation constant(s) passed to either <a href="#topic+rwnn">rwnn</a> or <a href="#topic+ae_rwnn">ae_rwnn</a> (see <code>method</code> argument).</p>
</td></tr>
<tr><td><code id="bag_rwnn_+3A_b">B</code></td>
<td>
<p>The number of bootstrap samples.</p>
</td></tr>
<tr><td><code id="bag_rwnn_+3A_method">method</code></td>
<td>
<p>The penalisation type passed to <a href="#topic+ae_rwnn">ae_rwnn</a>. Set to <code>NULL</code> (default), <code>"l1"</code>, or <code>"l2"</code>. If <code>NULL</code>, <a href="#topic+rwnn">rwnn</a> is used as the base learner.</p>
</td></tr>
<tr><td><code id="bag_rwnn_+3A_type">type</code></td>
<td>
<p>A string indicating whether this is a regression or classification problem.</p>
</td></tr>
<tr><td><code id="bag_rwnn_+3A_control">control</code></td>
<td>
<p>A list of additional arguments passed to the <a href="#topic+control_rwnn">control_rwnn</a> function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An <a href="#topic+ERWNN-object">ERWNN-object</a>.
</p>


<h3>References</h3>

<p>Breiman L. (1996) &quot;Bagging Predictors.&quot; <em>Machine Learning</em>, 24, 123-140.
</p>
<p>Breiman L. (2001) &quot;Random Forests.&quot; <em>Machine Learning</em>, 45, 5-32.
</p>
<p>Sui X, He S, Vilsen SB, Teodorescu R, Stroe DI (2021) &quot;Fast and Robust Estimation of Lithium-ion Batteries State of Health Using Ensemble Learning.&quot; <em>In 2021 IEEE Energy Conversion Congress and Exposition (ECCE)</em>, 1-8.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n_hidden &lt;- 50

B &lt;- 100
lambda &lt;- 0.01

m &lt;- bag_rwnn(y ~ ., data = example_data, n_hidden = n_hidden, lambda = lambda, B = B)
</code></pre>

<hr>
<h2 id='boost_rwnn'>Boosting random weight neural networks</h2><span id='topic+boost_rwnn'></span><span id='topic+boost_rwnn.formula'></span>

<h3>Description</h3>

<p>Use gradient boosting to create ensemble random weight neural network models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>boost_rwnn(
  formula,
  data = NULL,
  n_hidden = c(),
  lambda = NULL,
  B = 100,
  epsilon = 0.1,
  method = NULL,
  type = NULL,
  control = list()
)

## S3 method for class 'formula'
boost_rwnn(
  formula,
  data = NULL,
  n_hidden = c(),
  lambda = NULL,
  B = 100,
  epsilon = 0.1,
  method = NULL,
  type = NULL,
  control = list()
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="boost_rwnn_+3A_formula">formula</code></td>
<td>
<p>A <a href="stats.html#topic+formula">formula</a> specifying features and targets used to estimate the parameters of the output layer.</p>
</td></tr>
<tr><td><code id="boost_rwnn_+3A_data">data</code></td>
<td>
<p>A data-set (either a <a href="base.html#topic+data.frame">data.frame</a> or a <a href="tibble.html#topic+tibble">tibble</a>) used to estimate the parameters of the output layer.</p>
</td></tr>
<tr><td><code id="boost_rwnn_+3A_n_hidden">n_hidden</code></td>
<td>
<p>A vector of integers designating the number of neurons in each of the hidden layers (the length of the list is taken as the number of hidden layers).</p>
</td></tr>
<tr><td><code id="boost_rwnn_+3A_lambda">lambda</code></td>
<td>
<p>The penalisation constant(s) passed to either <a href="#topic+rwnn">rwnn</a> or <a href="#topic+ae_rwnn">ae_rwnn</a> (see <code>method</code> argument).</p>
</td></tr>
<tr><td><code id="boost_rwnn_+3A_b">B</code></td>
<td>
<p>The number of levels used in the boosting tree.</p>
</td></tr>
<tr><td><code id="boost_rwnn_+3A_epsilon">epsilon</code></td>
<td>
<p>The learning rate.</p>
</td></tr>
<tr><td><code id="boost_rwnn_+3A_method">method</code></td>
<td>
<p>The penalisation type passed to <a href="#topic+ae_rwnn">ae_rwnn</a>. Set to <code>NULL</code> (default), <code>"l1"</code>, or <code>"l2"</code>. If <code>NULL</code>, <a href="#topic+rwnn">rwnn</a> is used as the base learner.</p>
</td></tr>
<tr><td><code id="boost_rwnn_+3A_type">type</code></td>
<td>
<p>A string indicating whether this is a regression or classification problem.</p>
</td></tr>
<tr><td><code id="boost_rwnn_+3A_control">control</code></td>
<td>
<p>A list of additional arguments passed to the <a href="#topic+control_rwnn">control_rwnn</a> function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An <a href="#topic+ERWNN-object">ERWNN-object</a>.
</p>


<h3>References</h3>

<p>Friedman J.H. (2001) &quot;Greedy function approximation: A gradrient boosting machine.&quot; <em>The Annals of Statistics</em>, 29, 1189-1232.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n_hidden &lt;- 10

B &lt;- 100
epsilon &lt;- 0.1
lambda &lt;- 0.01

m &lt;- boost_rwnn(y ~ ., data = example_data, n_hidden = n_hidden,
                lambda = lambda, B = B, epsilon = epsilon)
</code></pre>

<hr>
<h2 id='classify'>Classifier</h2><span id='topic+classify'></span>

<h3>Description</h3>

<p>Function classifying an observation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>classify(y, C, t = NULL, b = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="classify_+3A_y">y</code></td>
<td>
<p>A matrix of predicted classes.</p>
</td></tr>
<tr><td><code id="classify_+3A_c">C</code></td>
<td>
<p>A vector of class names corresponding to the columns of <code>y</code>.</p>
</td></tr>
<tr><td><code id="classify_+3A_t">t</code></td>
<td>
<p>The decision threshold which the predictions have to exceed (defaults to '0').</p>
</td></tr>
<tr><td><code id="classify_+3A_b">b</code></td>
<td>
<p>A buffer which the largest prediction has to exceed when compared to the second largest prediction (defaults to '0').</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of class predictions.
</p>

<hr>
<h2 id='control_rwnn'>rwnn control function</h2><span id='topic+control_rwnn'></span>

<h3>Description</h3>

<p>A function used to create a control-object for the <a href="#topic+rwnn">rwnn</a> function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>control_rwnn(
  n_hidden = NULL,
  n_features = NULL,
  lnorm = NULL,
  bias_hidden = TRUE,
  bias_output = TRUE,
  activation = NULL,
  combine_input = FALSE,
  combine_hidden = TRUE,
  include_data = TRUE,
  include_estimate = TRUE,
  rng = runif,
  rng_pars = list(min = -1, max = 1)
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="control_rwnn_+3A_n_hidden">n_hidden</code></td>
<td>
<p>A vector of integers designating the number of neurons in each of the hidden layers (the length of the list is taken as the number of hidden layers).</p>
</td></tr>
<tr><td><code id="control_rwnn_+3A_n_features">n_features</code></td>
<td>
<p>The number of randomly chosen features in the RWNN model. Note: This is meant for use in <a href="#topic+bag_rwnn">bag_rwnn</a>, and it is not recommended outside of that function.</p>
</td></tr>
<tr><td><code id="control_rwnn_+3A_lnorm">lnorm</code></td>
<td>
<p>A string indicating the type of regularisation used when estimating the weights in the output layer, <code>"l1"</code> or <code>"l2"</code> (default).</p>
</td></tr>
<tr><td><code id="control_rwnn_+3A_bias_hidden">bias_hidden</code></td>
<td>
<p>A vector of TRUE/FALSE values. The vector should have length 1, or be equal to the number of hidden layers.</p>
</td></tr>
<tr><td><code id="control_rwnn_+3A_bias_output">bias_output</code></td>
<td>
<p>TRUE/FALSE: Should a bias be added to the output layer?</p>
</td></tr>
<tr><td><code id="control_rwnn_+3A_activation">activation</code></td>
<td>
<p>A vector of strings corresponding to activation functions (see details). The vector should have length 1, or be equal to the number of hidden layers.</p>
</td></tr>
<tr><td><code id="control_rwnn_+3A_combine_input">combine_input</code></td>
<td>
<p>TRUE/FALSE: Should the input be included to predict the output?</p>
</td></tr>
<tr><td><code id="control_rwnn_+3A_combine_hidden">combine_hidden</code></td>
<td>
<p>TRUE/FALSE: Should all hidden layers be combined to predict the output?</p>
</td></tr>
<tr><td><code id="control_rwnn_+3A_include_data">include_data</code></td>
<td>
<p>TRUE/FALSE: Should the original data be included in the returned object? Note: this should almost always be set to '<code>TRUE</code>', but using '<code>FALSE</code>' is more memory efficient in <a href="#topic+ERWNN-object">ERWNN-object</a>'s.</p>
</td></tr>
<tr><td><code id="control_rwnn_+3A_include_estimate">include_estimate</code></td>
<td>
<p>TRUE/FALSE: Should the <code>rwnn</code>-function estimate the output parameters? Note: this should almost always be set to '<code>TRUE</code>', but using '<code>FALSE</code>'is more memory efficient in <a href="#topic+ERWNN-object">ERWNN-object</a>'s.</p>
</td></tr>
<tr><td><code id="control_rwnn_+3A_rng">rng</code></td>
<td>
<p>A string indicating the sampling distribution used for generating the weights of the hidden layer (defaults to <code>runif</code>).</p>
</td></tr>
<tr><td><code id="control_rwnn_+3A_rng_pars">rng_pars</code></td>
<td>
<p>A list of parameters passed to the <code>rng</code> function (defaults to <code>list(min = -1, max = 1)</code>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The possible activation functions supplied to '<code>activation</code>' are:
</p>

<dl>
<dt><code>"identity"</code></dt><dd><p style="text-align: center;"><code class="reqn">f(x) = x</code>
</p>
</dd>
<dt><code>"bentidentity"</code></dt><dd><p style="text-align: center;"><code class="reqn">f(x) = \frac{\sqrt{x^2 + 1} - 1}{2} + x</code>
</p>
</dd>
<dt><code>"sigmoid"</code></dt><dd><p style="text-align: center;"><code class="reqn">f(x) = \frac{1}{1 + \exp(-x)}</code>
</p>
</dd>
<dt><code>"tanh"</code></dt><dd><p style="text-align: center;"><code class="reqn">f(x) = \frac{\exp(x) - \exp(-x)}{\exp(x) + \exp(-x)}</code>
</p>
</dd>
<dt><code>"relu"</code></dt><dd><p style="text-align: center;"><code class="reqn">f(x) = \max\{0, x\}</code>
</p>
</dd>
<dt><code>"silu"</code> (default)</dt><dd><p style="text-align: center;"><code class="reqn">f(x) = \frac{x}{1 + \exp(-x)}</code>
</p>
</dd>
<dt><code>"softplus"</code></dt><dd><p style="text-align: center;"><code class="reqn">f(x) = \ln(1 + \exp(x))</code>
</p>
</dd>
<dt><code>"softsign"</code></dt><dd><p style="text-align: center;"><code class="reqn">f(x) = \frac{x}{1 + |x|}</code>
</p>
</dd>
<dt><code>"sqnl"</code></dt><dd><p style="text-align: center;"><code class="reqn">f(x) = -1\text{, if }x &lt; -2\text{, }f(x) = x + \frac{x^2}{4}\text{, if }-2 \le x &lt; 0\text{, }f(x) = x - \frac{x^2}{4}\text{, if }0 \le x \le 2\text{, and } f(x) = 2\text{, if }x &gt; 2</code>
</p>
</dd>
<dt><code>"gaussian"</code></dt><dd><p style="text-align: center;"><code class="reqn">f(x) = \exp(-x^2)</code>
</p>
</dd>
<dt><code>"sqrbf"</code></dt><dd><p style="text-align: center;"><code class="reqn">f(x) = 1 - \frac{x^2}{2}\text{, if }|x| \le 1\text{, }f(x) = \frac{(2 - |x|)^2}{2}\text{, if }1 &lt; |x| &lt; 2\text{, and }f(x) = 0\text{, if }|x| \ge 2</code>
</p>
</dd>
</dl>

<p>The '<code>rng</code>' argument can also be set to <code>"orthogonal"</code>, <code>"torus"</code>, <code>"halton"</code>, or <code>"sobol"</code> for added stability. The <code>"torus"</code>, <code>"halton"</code>, and <code>"sobol"</code> methods relay on the <a href="randtoolbox.html#topic+torus">torus</a>, <a href="randtoolbox.html#topic+halton">halton</a>, and <a href="randtoolbox.html#topic+sobol">sobol</a> functions. NB: this is not recommended when creating ensembles.
</p>


<h3>Value</h3>

<p>A list of control variables.
</p>


<h3>References</h3>

<p>Wang W., Liu X. (2017) &quot;The selection of input weights of extreme learning machine: A sample structure preserving point of view.&quot; <em>Neurocomputing</em>, 261, 28-36.
</p>

<hr>
<h2 id='ed_rwnn'>Ensemble deep random weight neural networks</h2><span id='topic+ed_rwnn'></span><span id='topic+ed_rwnn.formula'></span>

<h3>Description</h3>

<p>Use multiple layers to create deep ensemble random weight neural network models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ed_rwnn(
  formula,
  data = NULL,
  n_hidden,
  lambda = 0,
  method = NULL,
  type = NULL,
  control = list()
)

## S3 method for class 'formula'
ed_rwnn(
  formula,
  data = NULL,
  n_hidden,
  lambda = 0,
  method = NULL,
  type = NULL,
  control = list()
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ed_rwnn_+3A_formula">formula</code></td>
<td>
<p>A <a href="stats.html#topic+formula">formula</a> specifying features and targets used to estimate the parameters of the output layer.</p>
</td></tr>
<tr><td><code id="ed_rwnn_+3A_data">data</code></td>
<td>
<p>A data-set (either a <a href="base.html#topic+data.frame">data.frame</a> or a <a href="tibble.html#topic+tibble">tibble</a>) used to estimate the parameters of the output layer.</p>
</td></tr>
<tr><td><code id="ed_rwnn_+3A_n_hidden">n_hidden</code></td>
<td>
<p>A vector of integers designating the number of neurons in each of the hidden layers (the length of the list is taken as the number of hidden layers).</p>
</td></tr>
<tr><td><code id="ed_rwnn_+3A_lambda">lambda</code></td>
<td>
<p>The penalisation constant(s) passed to either <a href="#topic+rwnn">rwnn</a> or <a href="#topic+ae_rwnn">ae_rwnn</a> (see <code>method</code> argument).</p>
</td></tr>
<tr><td><code id="ed_rwnn_+3A_method">method</code></td>
<td>
<p>The penalisation type passed to <a href="#topic+ae_rwnn">ae_rwnn</a>. Set to <code>NULL</code> (default), <code>"l1"</code>, or <code>"l2"</code>. If <code>NULL</code>, <a href="#topic+rwnn">rwnn</a> is used as the base learner.</p>
</td></tr>
<tr><td><code id="ed_rwnn_+3A_type">type</code></td>
<td>
<p>A string indicating whether this is a regression or classification problem.</p>
</td></tr>
<tr><td><code id="ed_rwnn_+3A_control">control</code></td>
<td>
<p>A list of additional arguments passed to the <a href="#topic+control_rwnn">control_rwnn</a> function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An <a href="#topic+ERWNN-object">ERWNN-object</a>.
</p>


<h3>References</h3>

<p>Shi Q., Katuwal R., Suganthan P., Tanveer M. (2021) &quot;Random vector functional link neural network based ensemble deep learning.&quot; <em>Pattern Recognition</em>, 117, 107978.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n_hidden &lt;- c(20, 15, 10, 5)
lambda &lt;- 0.01

#
m &lt;- ed_rwnn(y ~ ., data = example_data, n_hidden = n_hidden, lambda = lambda)
</code></pre>

<hr>
<h2 id='ERWNN-object'>An ERWNN-object</h2><span id='topic+ERWNN-object'></span>

<h3>Description</h3>

<p>An ERWNN-object is a list containing the following:
</p>

<dl>
<dt><code>data</code></dt><dd><p>The original data used to estimate the weights.</p>
</dd>
<dt><code>models</code></dt><dd><p>A list with each element being an <a href="#topic+RWNN-object">RWNN-object</a>.</p>
</dd>
<dt><code>weights</code></dt><dd><p>A vector of ensemble weights.</p>
</dd>
<dt><code>method</code></dt><dd><p>A string indicating the method.</p>
</dd>
</dl>


<hr>
<h2 id='example_data'>Example data</h2><span id='topic+example_data'></span>

<h3>Description</h3>

<p>A data-set of 2000 observations were sampled independently according to the function: 
</p>
<p style="text-align: center;"><code class="reqn">y_n = \dfrac{1}{1 + \exp(-x_n^T\beta + \varepsilon_n)},</code>
</p>

<p>where <code class="reqn">x_n^T</code> is a vector containing an intercept and five input features, <code class="reqn">\beta</code> is a vector containing the parameters, <code class="reqn">(-1\;2\;1\;2\;0.5\;3)^T</code>, and <code class="reqn">\varepsilon_n</code> is normally distributed noise with mean 0 and variance 0.1. Furthermore, the five features were generated as <code class="reqn">x_1 \sim \mathcal Unif(-5, 5)</code>, <code class="reqn">x_2 \sim \mathcal Unif(0, 2)</code>, <code class="reqn">x_3 \sim \mathcal N(2, 4)</code>, <code class="reqn">x_4 \sim \mathcal Gamma(2, 4)</code>, and <code class="reqn">x_5 \sim \mathcal Beta(10, 4)</code>, respectively.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>example_data
</code></pre>


<h3>Format</h3>

<p>An object of class <code>data.frame</code> with 2000 rows and 6 columns.
</p>

<hr>
<h2 id='predict.ERWNN'>Predicting targets of an ERWNN-object</h2><span id='topic+predict.ERWNN'></span>

<h3>Description</h3>

<p>Predicting targets of an ERWNN-object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ERWNN'
predict(object, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.ERWNN_+3A_object">object</code></td>
<td>
<p>An <a href="#topic+ERWNN-object">ERWNN-object</a>.</p>
</td></tr>
<tr><td><code id="predict.ERWNN_+3A_...">...</code></td>
<td>
<p>Additional arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The additional arguments '<code>newdata</code>', '<code>type</code>', and '<code>class</code>' can be specified as follows:
</p>

<dl>
<dt><code>newdata</code></dt><dd><p>Expects a <a href="base.html#topic+matrix">matrix</a> or <a href="base.html#topic+data.frame">data.frame</a> with the same features (columns) as in the original data.</p>
</dd>
<dt><code>type</code></dt><dd><p>A string taking the following values:
</p>

<dl>
<dt><code>"mean" (default)</code></dt><dd><p>Returns the average prediction across all ensemble models.</p>
</dd>
<dt><code>"std"</code></dt><dd><p>Returns the standard deviation of the predictions across all ensemble models.</p>
</dd>
<dt><code>"all"</code></dt><dd><p>Returns all predictions for each ensemble models.</p>
</dd>
</dl>

</dd>
<dt><code>class</code></dt><dd><p>A string taking the following values:
</p>

<dl>
<dt><code>"classify"</code></dt><dd><p>Returns the predicted class of the ensemble. If used together with <code>type = "mean"</code>, the average prediction across the ensemble models are used to create the classification. However, if used with <code>type = "all"</code>, every ensemble is classified and returned.</p>
</dd>
<dt><code>"voting"</code></dt><dd><p>Returns the predicted class of the ensemble by classifying each ensemble and using majority voting to make the final prediction. NB: the <code>type</code> argument is overruled.</p>
</dd>
</dl>

</dd>
</dl>

<p>Furthermore, if '<code>class</code>' is set to either <code>"classify"</code> or <code>"voting"</code>, additional arguments '<code>t</code>' and '<code>b</code>' can be passed to the <a href="#topic+classify">classify</a>-function.
</p>
<p>NB: if the ensemble is created using the <a href="#topic+boost_rwnn">boost_rwnn</a>-function, then <code>type</code> should always be set to <code>"mean"</code>.
</p>


<h3>Value</h3>

<p>An list, matrix, or vector of predicted values depended on the arguments '<code>method</code>', '<code>type</code>', and '<code>class</code>'.
</p>

<hr>
<h2 id='predict.RWNN'>Predicting targets of an RWNN-object</h2><span id='topic+predict.RWNN'></span>

<h3>Description</h3>

<p>Predicting targets of an RWNN-object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'RWNN'
predict(object, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.RWNN_+3A_object">object</code></td>
<td>
<p>An <a href="#topic+RWNN-object">RWNN-object</a>.</p>
</td></tr>
<tr><td><code id="predict.RWNN_+3A_...">...</code></td>
<td>
<p>Additional arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The additional arguments used by the function are '<code>newdata</code>' and '<code>class</code>'. The argument '<code>newdata</code>' expects a <a href="base.html#topic+matrix">matrix</a> or <a href="base.html#topic+data.frame">data.frame</a> with the same features (columns) as in the original data. While the '<code>class</code>' argument can be set to <code>"classify"</code>. If <code>class == "classify"</code> additional arguments '<code>t</code>' and '<code>b</code>' can be passed to the <a href="#topic+classify">classify</a>-function.
</p>


<h3>Value</h3>

<p>A vector of predicted values.
</p>

<hr>
<h2 id='reduce_network'>Reduce the weights of a random weight neural network.</h2><span id='topic+reduce_network'></span><span id='topic+reduce_network.RWNN'></span><span id='topic+reduce_network.ERWNN'></span>

<h3>Description</h3>

<p>Methods for weight and neuron pruning in random weight neural networks.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>reduce_network(object, method, retrain = TRUE, ...)

## S3 method for class 'RWNN'
reduce_network(object, method, retrain = TRUE, ...)

## S3 method for class 'ERWNN'
reduce_network(object, method, retrain = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="reduce_network_+3A_object">object</code></td>
<td>
<p>An <a href="#topic+RWNN-object">RWNN-object</a> or <a href="#topic+ERWNN-object">ERWNN-object</a>.</p>
</td></tr>
<tr><td><code id="reduce_network_+3A_method">method</code></td>
<td>
<p>A string, or a function, setting the method used to reduce the network (see details).</p>
</td></tr>
<tr><td><code id="reduce_network_+3A_retrain">retrain</code></td>
<td>
<p>TRUE/FALSE: Should the output weights be retrained after reduction (defaults to <code>TRUE</code>)?</p>
</td></tr>
<tr><td><code id="reduce_network_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to the reduction method (see details).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The '<code>method</code>' and additional arguments required by the method are:
</p>

<dl>
<dt><code>"global"</code> (or <code>"glbl"</code>)</dt><dd>
<dl>
<dt><code>p</code>: The proportion of weights to remove globally based on magnitude.</dt><dd></dd>
</dl>
</dd>
<dt><code>"uniform"</code> (or <code>"unif"</code>)</dt><dd>
<dl>
<dt><code>p</code>: The proportion of weights to remove uniformly layer-by-layer based on magnitude.</dt><dd></dd>
</dl>
</dd>
<dt><code>"lamp"</code></dt><dd>
<dl>
<dt><code>p</code>: The proportion of weights to remove based on LAMP scores.</dt><dd></dd>
</dl>
</dd>
<dt><code>"apoz"</code></dt><dd>
<dl>
<dt><code>p</code>: The proportion of neurons to remove based on proportion of zeroes produced.</dt><dd></dd>
<dt><code>tolerance</code>: The tolerance used when identifying zeroes.</dt><dd></dd>
<dt><code>type</code>: A string indicating whether weights should be removed globally (<code>'global'</code>) or uniformly  (<code>'uniform'</code>).</dt><dd></dd>
</dl>
</dd>
<dt><code>"correlation"</code> (or <code>"cor"</code>)</dt><dd>
<dl>
<dt><code>type</code>: The type of correlation (argument passed to <a href="stats.html#topic+cor">cor</a> function).</dt><dd></dd>
<dt><code>rho</code>: The correlation threshold used to remove neurons.</dt><dd></dd>
</dl>
</dd>
<dt><code>"correlationtest"</code> (or <code>"cortest"</code>)</dt><dd>
<dl>
<dt><code>type</code>: The type of correlation (argument passed to <a href="stats.html#topic+cor">cor</a> function).</dt><dd></dd>
<dt><code>rho</code>: The correlation threshold used to remove neurons.</dt><dd></dd>
<dt><code>alpha</code>: The significance levels used to test whether the observed correlation between two neurons is small than <code>rho</code>.</dt><dd></dd>
</dl>
</dd>
<dt><code>"relief"</code></dt><dd>
<dl>
<dt><code>p</code>: The proportion of neurons or weights to remove based on relief scores.</dt><dd></dd>
<dt><code>type</code>: A string indicating whether neurons (<code>'neuron'</code>) or weights (<code>'weight'</code>) should be removed.</dt><dd></dd>
</dl>
</dd>
<dt><code>"output"</code></dt><dd>
<dl>
<dt><code>tolerance</code>: The tolerance used when removing zeroes from the output layer.</dt><dd></dd>
</dl>
</dd>
</dl>
 
<p>If the object is an <a href="#topic+ERWNN-object">ERWNN-object</a>, the reduction is applied to all <a href="#topic+RWNN-object">RWNN-object</a>'s in the <a href="#topic+ERWNN-object">ERWNN-object</a>. Furthermore, when
the <a href="#topic+ERWNN-object">ERWNN-object</a> is created as a stack and the weights of the stack is trained, then '<code>method</code>' can be set to:
</p>

<dl>
<dt><code>"stack"</code></dt><dd>
<dl>
<dt><code>tolerance</code>: The tolerance used when removing elements from the stack.</dt><dd></dd>
</dl>
</dd>
</dl>

<p>Lastly, '<code>method</code>' can also be passed as a function, with additional arguments passed through the <code>...</code> argument. 
NB: features and target are passed using the names <code>X</code> and <code>y</code>, respectively.
</p>


<h3>Value</h3>

<p>A reduced <a href="#topic+RWNN-object">RWNN-object</a> or <a href="#topic+ERWNN-object">ERWNN-object</a>.
</p>


<h3>References</h3>

<p>Han S., Mao H., Dally W.J. (2016) &quot;Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding.&quot; arXiv: 1510.00149.
</p>
<p>Hu H., Peng R., Tai Y.W., Tang C.K. (2016) &quot;Network Trimming: A Data-Driven Neuron Pruning Approach towards Efficient Deep Architectures.&quot; arXiv: 1607.03250.
</p>
<p>Morcos A.S., Yu H., Paganini M., Tian Y. (2019) &quot;One ticket to win them all: generalizing lottery ticket initializations across datasets and optimizers.&quot; arXiv: 1906.02773.
</p>
<p>Lee J., Park S., Mo S., Ahn S., Shin J. (2021) &quot;Layer-adaptive sparsity for the Magnitude-based Pruning.&quot; arXiv: 2010.07611.
</p>
<p>Dekhovich A., Tax D.M., Sluiter M.H., Bessa M.A. (2024) &quot;Neural network relief: a pruning algorithm based on neural activity.&quot; <em>Machine Learning</em>, 113, 2597-2618.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## RWNN-object
n_hidden &lt;- c(10, 15)
lambda &lt;- 2

m &lt;- rwnn(y ~ ., data = example_data, n_hidden = n_hidden, 
          lambda = lambda, control = list(lnorm = "l2"))

m |&gt; 
    reduce_network(method = "relief", p = 0.2, type = "neuron") |&gt; 
    (\(x) x$weights)()

m |&gt; 
    reduce_network(method = "relief", p = 0.2, type = "neuron") |&gt; 
    reduce_network(method = "correlationtest", rho = 0.995, alpha = 0.05) |&gt; 
    (\(x) x$weights)()


m |&gt; 
    reduce_network(method = "relief", p = 0.2, type = "neuron") |&gt; 
    reduce_network(method = "correlationtest", rho = 0.995, alpha = 0.05) |&gt; 
    reduce_network(method = "lamp", p = 0.2) |&gt; 
    (\(x) x$weights)()

m |&gt; 
    reduce_network(method = "relief", p = 0.4, type = "neuron") |&gt; 
    reduce_network(method = "relief", p = 0.4, type = "weight") |&gt; 
    reduce_network(method = "output") |&gt; 
    (\(x) x$weights)()

## ERWNN-object (reduction is performed element-wise on each RWNN)
n_hidden &lt;- c(10, 15)
lambda &lt;- 2
B &lt;- 100


m &lt;- bag_rwnn(y ~ ., data = example_data, n_hidden = n_hidden, 
              lambda = lambda, B = B, control = list(lnorm = "l2"))

m |&gt; 
    reduce_network(method = "relief", p = 0.2, type = "neuron") |&gt; 
    reduce_network(method = "relief", p = 0.2, type = "weight") |&gt; 
    reduce_network(method = "output")



m &lt;- stack_rwnn(y ~ ., data = example_data, n_hidden = n_hidden,
                lambda = lambda, B = B, optimise = TRUE)

# Number of models in stack
length(m$weights)
# Number of models in stack with weights &gt; .Machine$double.eps
length(m$weights[m$weights &gt; .Machine$double.eps]) 

m |&gt; 
    reduce_network(method = "stack", tolerance = 1e-8) |&gt; 
    (\(x) x$weights)()

</code></pre>

<hr>
<h2 id='rwnn'>Random weight neural networks</h2><span id='topic+rwnn'></span><span id='topic+rwnn.formula'></span>

<h3>Description</h3>

<p>Set-up and estimate weights of a random weight neural network.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rwnn(
  formula,
  data = NULL,
  n_hidden = c(),
  lambda = 0,
  type = NULL,
  control = list()
)

## S3 method for class 'formula'
rwnn(
  formula,
  data = NULL,
  n_hidden = c(),
  lambda = 0,
  type = NULL,
  control = list()
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="rwnn_+3A_formula">formula</code></td>
<td>
<p>A <a href="stats.html#topic+formula">formula</a> specifying features and targets used to estimate the parameters of the output layer.</p>
</td></tr>
<tr><td><code id="rwnn_+3A_data">data</code></td>
<td>
<p>A data-set (either a <a href="base.html#topic+data.frame">data.frame</a> or a <a href="tibble.html#topic+tibble">tibble</a>) used to estimate the parameters of the output layer.</p>
</td></tr>
<tr><td><code id="rwnn_+3A_n_hidden">n_hidden</code></td>
<td>
<p>A vector of integers designating the number of neurons in each of the hidden layers (the length of the list is taken as the number of hidden layers).</p>
</td></tr>
<tr><td><code id="rwnn_+3A_lambda">lambda</code></td>
<td>
<p>The penalisation constant used when training the output layer.</p>
</td></tr>
<tr><td><code id="rwnn_+3A_type">type</code></td>
<td>
<p>A string indicating whether this is a regression or classification problem.</p>
</td></tr>
<tr><td><code id="rwnn_+3A_control">control</code></td>
<td>
<p>A list of additional arguments passed to the <a href="#topic+control_rwnn">control_rwnn</a> function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A deep RWNN is constructed by increasing the number of elements in the vector <code>n_hidden</code>. Furthermore, if <code>type</code> is null, then the function tries to deduce it from class of target.
</p>


<h3>Value</h3>

<p>An <a href="#topic+RWNN-object">RWNN-object</a>.
</p>


<h3>References</h3>

<p>Schmidt W., Kraaijveld M., Duin R. (1992) &quot;Feedforward neural networks with random weights.&quot; <em>In Proceedings., 11th IAPR International Conference on Pattern Recognition. Vol.II. Conference B: Pattern Recognition Methodology and Systems</em>, 1–4.
</p>
<p>Pao Y., Park G., Sobajic D. (1992) &quot;Learning and generalization characteristics of random vector Functional-link net.&quot; <em>Neurocomputing</em>, 6, 163–180.
</p>
<p>Huang G.B., Zhu Q.Y., Siew C.K. (2006) &quot;Extreme learning machine: Theory and applications.&quot; <em>Neurocomputing</em>, 70(1), 489–501.
</p>
<p>Henríquez P.A., Ruz G.A. (2018) &quot;Twitter Sentiment Classification Based on Deep Random Vector Functional Link.&quot; <em>In 2018 International Joint Conference on Neural Networks (IJCNN)</em>, 1–6.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Models with a single hidden layer
n_hidden &lt;- 50
lambda &lt;- 0.01

# Regression
m &lt;- rwnn(y ~ ., data = example_data, n_hidden = n_hidden, lambda = lambda)

# Classification
m &lt;- rwnn(I(y &gt; median(y)) ~ ., data = example_data, n_hidden = n_hidden, lambda = lambda)

## Model with multiple hidden layers
n_hidden &lt;- c(20, 15, 10, 5)
lambda &lt;- 0.01

# Combining outputs from all hidden layers (default)
m &lt;- rwnn(y ~ ., data = example_data, n_hidden = n_hidden, lambda = lambda)

# Using only the output of the last hidden layer
m &lt;- rwnn(y ~ ., data = example_data, n_hidden = n_hidden,
          lambda = lambda, control = list(combine_hidden = FALSE))

</code></pre>

<hr>
<h2 id='RWNN-object'>An RWNN-object</h2><span id='topic+RWNN-object'></span>

<h3>Description</h3>

<p>An RWNN-object is a list containing the following:
</p>

<dl>
<dt><code>data</code></dt><dd><p>The original data used to estimate the weights.</p>
</dd>
<dt><code>n_hidden</code></dt><dd><p>The vector of neurons in each layer.</p>
</dd>
<dt><code>activation</code></dt><dd><p>The vector of the activation functions used in each layer.</p>
</dd>
<dt><code>lnorm</code></dt><dd><p>The norm used when estimating the output weights.</p>
</dd>
<dt><code>lambda</code></dt><dd><p>The penalisation constant used when estimating the output weights.</p>
</dd>
<dt><code>bias</code></dt><dd><p>The <code>TRUE/FALSE</code> bias vectors set by the control function for both hidden layers, and the output layer.</p>
</dd>
<dt><code>weights</code></dt><dd><p>The weigths of the neural network, split into random (stored in hidden) and estimated (stored in output) weights.</p>
</dd>
<dt><code>sigma</code></dt><dd><p>The standard deviation of the corresponding linear model.</p>
</dd>
<dt><code>type</code></dt><dd><p>A string indicating the type of modelling problem.</p>
</dd>
<dt><code>combined</code></dt><dd><p>A list of two <code>TRUE/FALSE</code> values stating whether the direct links were made to the input, and whether the output of each hidden layer was combined to make the prediction.</p>
</dd>
</dl>


<hr>
<h2 id='stack_rwnn'>Stacking random weight neural networks</h2><span id='topic+stack_rwnn'></span><span id='topic+stack_rwnn.formula'></span>

<h3>Description</h3>

<p>Use stacking to create ensemble random weight neural networks.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stack_rwnn(
  formula,
  data = NULL,
  n_hidden = c(),
  lambda = NULL,
  B = 100,
  optimise = FALSE,
  folds = 10,
  method = NULL,
  type = NULL,
  control = list()
)

## S3 method for class 'formula'
stack_rwnn(
  formula,
  data = NULL,
  n_hidden = c(),
  lambda = NULL,
  B = 100,
  optimise = FALSE,
  folds = 10,
  method = NULL,
  type = NULL,
  control = list()
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="stack_rwnn_+3A_formula">formula</code></td>
<td>
<p>A <a href="stats.html#topic+formula">formula</a> specifying features and targets used to estimate the parameters of the output layer.</p>
</td></tr>
<tr><td><code id="stack_rwnn_+3A_data">data</code></td>
<td>
<p>A data-set (either a <a href="base.html#topic+data.frame">data.frame</a> or a <a href="tibble.html#topic+tibble">tibble</a>) used to estimate the parameters of the output layer.</p>
</td></tr>
<tr><td><code id="stack_rwnn_+3A_n_hidden">n_hidden</code></td>
<td>
<p>A vector of integers designating the number of neurons in each of the hidden layers (the length of the list is taken as the number of hidden layers).</p>
</td></tr>
<tr><td><code id="stack_rwnn_+3A_lambda">lambda</code></td>
<td>
<p>The penalisation constant(s) passed to either <a href="#topic+rwnn">rwnn</a> or <a href="#topic+ae_rwnn">ae_rwnn</a> (see <code>method</code> argument).</p>
</td></tr>
<tr><td><code id="stack_rwnn_+3A_b">B</code></td>
<td>
<p>The number of models in the stack.</p>
</td></tr>
<tr><td><code id="stack_rwnn_+3A_optimise">optimise</code></td>
<td>
<p>TRUE/FALSE: Should the stacking weights be optimised (or should the stack just predict the average)?</p>
</td></tr>
<tr><td><code id="stack_rwnn_+3A_folds">folds</code></td>
<td>
<p>The number of folds used when optimising the stacking weights (see <code>optimise</code> argument).</p>
</td></tr>
<tr><td><code id="stack_rwnn_+3A_method">method</code></td>
<td>
<p>The penalisation type passed to <a href="#topic+ae_rwnn">ae_rwnn</a>. Set to <code>NULL</code> (default), <code>"l1"</code>, or <code>"l2"</code>. If <code>NULL</code>, <a href="#topic+rwnn">rwnn</a> is used as the base learner.</p>
</td></tr>
<tr><td><code id="stack_rwnn_+3A_type">type</code></td>
<td>
<p>A string indicating whether this is a regression or classification problem.</p>
</td></tr>
<tr><td><code id="stack_rwnn_+3A_control">control</code></td>
<td>
<p>A list of additional arguments passed to the <a href="#topic+control_rwnn">control_rwnn</a> function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An <a href="#topic+ERWNN-object">ERWNN-object</a>.
</p>


<h3>References</h3>

<p>Wolpert D. (1992) &quot;Stacked generalization.&quot; <em>Neural Networks</em>, 5, 241-259.
</p>
<p>Breiman L. (1996) &quot;Stacked regressions.&quot; <em>Machine Learning</em>, 24, 49-64.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n_hidden &lt;- c(20, 15, 10, 5)
lambda &lt;- 0.01
B &lt;- 100

## Using the average of the stack to predict new targets

m &lt;- stack_rwnn(y ~ ., data = example_data, n_hidden = n_hidden,
                lambda = lambda, B = B)


## Using the optimised weighting of the stack to predict new targets

m &lt;- stack_rwnn(y ~ ., data = example_data, n_hidden = n_hidden,
                lambda = lambda, B = B, optimise = TRUE)


</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
