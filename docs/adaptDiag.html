<!DOCTYPE html><html><head><title>Help for package adaptDiag</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {adaptDiag}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#binom_sample_size'><p>Calculate the minimum number of samples required for a one-sided</p>
exact binomial test</a></li>
<li><a href='#multi_trial'><p>Simulate and analyse multiple trials</p></a></li>
<li><a href='#summarise_trials'><p>Summarise results of multiple simulated trials to give the operating</p>
characteristics</a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Bayesian Adaptive Designs for Diagnostic Trials</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.0</td>
</tr>
<tr>
<td>Description:</td>
<td>Simulate clinical trials for diagnostic test devices and evaluate 
    the operating characteristics under an adaptive design with futility
    assessment determined via the posterior predictive probabilities.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/graemeleehickey/adaptDiag">https://github.com/graemeleehickey/adaptDiag</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/graemeleehickey/adaptDiag/issues">https://github.com/graemeleehickey/adaptDiag/issues</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.6.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>doParallel, extraDistr, foreach, parallel, pbmcapply, stats</td>
</tr>
<tr>
<td>Language:</td>
<td>en-US</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.1</td>
</tr>
<tr>
<td>Suggests:</td>
<td>rmarkdown, knitr, testthat (&ge; 3.0.0), VGAM, covr</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2021-08-16 16:32:46 UTC; 10318048</td>
</tr>
<tr>
<td>Author:</td>
<td>Graeme L. Hickey [cre, aut],
  Yongqiang Zhang [aut],
  Becton, Dickinson and Company [cph]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Graeme L. Hickey &lt;graemeleehickey@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2021-08-17 07:20:14 UTC</td>
</tr>
</table>
<hr>
<h2 id='binom_sample_size'>Calculate the minimum number of samples required for a one-sided
exact binomial test</h2><span id='topic+binom_sample_size'></span>

<h3>Description</h3>

<p>Calculate the minimum number of samples required for a
one-sided exact binomial test to distinguish between two success
probabilities with specified alpha and power.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>binom_sample_size(alpha = 0.05, power = 0.9, p0 = 0.9, p1 = 0.95)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="binom_sample_size_+3A_alpha">alpha</code></td>
<td>
<p>scalar. The desired false positive rate (probability of
incorrectly rejecting the null). Must be be between 0 and 1. Default value
is <code>alpha = 0.05</code>.</p>
</td></tr>
<tr><td><code id="binom_sample_size_+3A_power">power</code></td>
<td>
<p>scalar. The the minimum probability of correctly rejects the
null when the alternate is true.</p>
</td></tr>
<tr><td><code id="binom_sample_size_+3A_p0">p0</code></td>
<td>
<p>scalar. The expected proportion of successes under the null.</p>
</td></tr>
<tr><td><code id="binom_sample_size_+3A_p1">p1</code></td>
<td>
<p>scalar. The proportion of successes under the alternate
hypothesis.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a one-sided function, such that <code class="reqn">p_0 &lt; p_1</code>. It
determines the minimum sample size to evaluate the hypothesis test:
</p>
<p style="text-align: center;"><code class="reqn">H_0: \, p_1 \le p_0, \, vs.</code>
</p>

<p style="text-align: center;"><code class="reqn">H_1: \, p_1 &gt; p_0</code>
</p>



<h3>Value</h3>

<p>A list containing the required sample size and the number of
successful trials required.
</p>


<h3>References</h3>

<p>Chow S-C, Shao J, Wang H, Lokhnygina Y. (2017) <em>Sample Size
Calculations in Clinical Research</em>, Boca Raton, FL: CRC Press.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# The minimum number of reference positive cases required to demonstrate
# the true sensitivity is &gt;0.7, assuming that the true value is 0.824, with
# 90% power is

binom_sample_size(alpha = 0.05, power = 0.9, p0 = 0.7, p1 = 0.824)

# With a sample size of n = 104, if the true prevalence is 0.2, we would
# require a sample size of at least n = 520 randomly sampled subjects to
# have adequate power to demonstrate the sensitivity of the new test.

# The minimum number of reference negative cases required to demonstrate
# the true specificity is &gt;0.9, assuming that the true value is 0.963, with
# 90% power is

binom_sample_size(alpha = 0.05, power = 0.9, p0 = 0.9, p1 = 0.963)

# The proposed total sample size of n = 520 would be sufficient to
# demonstrate both endpoint goals are met.

</code></pre>

<hr>
<h2 id='multi_trial'>Simulate and analyse multiple trials</h2><span id='topic+multi_trial'></span>

<h3>Description</h3>

<p>Multiple trials and simulated and analysed up to the final
analysis stage, irrespective of whether it would have been stopped for
early success or expected futility. The output of the trials is handled
elsewhere.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>multi_trial(
  sens_true,
  spec_true,
  prev_true,
  endpoint = "both",
  sens_pg = 0.8,
  spec_pg = 0.8,
  prior_sens = c(0.1, 0.1),
  prior_spec = c(0.1, 0.1),
  prior_prev = c(0.1, 0.1),
  succ_sens = 0.95,
  succ_spec = 0.95,
  n_at_looks,
  n_mc = 10000,
  n_trials = 1000,
  ncores
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="multi_trial_+3A_sens_true">sens_true</code></td>
<td>
<p>scalar. True assumed sensitivity (must be between 0 and 1).</p>
</td></tr>
<tr><td><code id="multi_trial_+3A_spec_true">spec_true</code></td>
<td>
<p>scalar. True assumed specificity (must be between 0 and 1).</p>
</td></tr>
<tr><td><code id="multi_trial_+3A_prev_true">prev_true</code></td>
<td>
<p>scalar. True assumed prevalence as measured by the
gold-standard reference test (must be between 0 and 1).</p>
</td></tr>
<tr><td><code id="multi_trial_+3A_endpoint">endpoint</code></td>
<td>
<p>character. The endpoint(s) that must meet a performance goal
criterion. The default is <code>code = "both"</code>, which means that the
endpoint is based simultaneously on sensitivity and specificity.
Alternative options are to specify <code>code = "sens"</code> or <code>code =
"spec"</code> for sensitivity and specificity, respectively. If only a single
endpoint is selected (e.g. sensitivity), then the PG and success
probability threshold of the other statistic are set to 1, and ignored for
later analysis.</p>
</td></tr>
<tr><td><code id="multi_trial_+3A_sens_pg">sens_pg</code></td>
<td>
<p>scalar. Performance goal (PG) for the sensitivity endpoint,
such that the the posterior probability that the PG is exceeded is
calculated. Must be between 0 and 1.</p>
</td></tr>
<tr><td><code id="multi_trial_+3A_spec_pg">spec_pg</code></td>
<td>
<p>scalar. Performance goal (PG) for the specificity endpoint,
such that the the posterior probability that the PG is exceeded is
calculated. Must be between 0 and 1.</p>
</td></tr>
<tr><td><code id="multi_trial_+3A_prior_sens">prior_sens</code></td>
<td>
<p>vector. A vector of length 2 with the prior shape
parameters for the sensitivity Beta distribution.</p>
</td></tr>
<tr><td><code id="multi_trial_+3A_prior_spec">prior_spec</code></td>
<td>
<p>vector. A vector of length 2 with the prior shape
parameters for the specificity Beta distribution.</p>
</td></tr>
<tr><td><code id="multi_trial_+3A_prior_prev">prior_prev</code></td>
<td>
<p>vector. A vector of length 2 with the prior shape
parameters for the prevalence Beta distribution.</p>
</td></tr>
<tr><td><code id="multi_trial_+3A_succ_sens">succ_sens</code></td>
<td>
<p>scalar. Probability threshold for the sensitivity to exceed
in order to declare a success. Must be between 0 and 1.</p>
</td></tr>
<tr><td><code id="multi_trial_+3A_succ_spec">succ_spec</code></td>
<td>
<p>scalar. Probability threshold for the specificity to exceed
in order to declare a success. Must be between 0 and 1.</p>
</td></tr>
<tr><td><code id="multi_trial_+3A_n_at_looks">n_at_looks</code></td>
<td>
<p>vector. Sample sizes for each interim look. The final value
(or only value if no interim looks are planned) is the maximum allowable
sample size for the trial.</p>
</td></tr>
<tr><td><code id="multi_trial_+3A_n_mc">n_mc</code></td>
<td>
<p>integer. Number of Monte Carlo draws to use for sampling from the
Beta-Binomial distribution.</p>
</td></tr>
<tr><td><code id="multi_trial_+3A_n_trials">n_trials</code></td>
<td>
<p>integer. The number of clinical trials to simulate overall,
which will be used to evaluate the operating characteristics.</p>
</td></tr>
<tr><td><code id="multi_trial_+3A_ncores">ncores</code></td>
<td>
<p>integer. The number of cores to use for parallel processing. If
'ncores' is missing, it defaults to the maximum number of cores available
(spare 1).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function simulates multiple trials and analyses each stage of the trial
(i.e. at each interim analysis sample size look) irrespective of whether a
stopping rule was triggered or not. The operating characteristics are handled
by a separate function, which accounts for the stopping rules and any other
trial constraints. By enumerating each stage of the trial, additional
insights can be gained such as: for a trial that stopped early for futility,
what is the probability that it would eventually go on to be successful if
the trial had not stopped. The details on how each trial are simulated here
are described below.
</p>
<p><strong>Simulating a single trial</strong>
</p>
<p>Given true values for the test sensitivity (<code>sens_true</code>), specificity
(<code>spec_true</code>), and the prevalence (<code>prev_true</code>) of disease, along
with a sample size look strategy (<code>n_at_looks</code>), it is straightforward
to simulate a complete dataset using the binomial distribution. That is, a
data frame with true disease status (reference test), and the new diagnostic
test result.
</p>
<p><strong>Posterior probability of exceeding PG at current look</strong>
</p>
<p>At a given sample size look, the posterior probability of an endpoint (e.g.
sensitivity) exceeding the pre-specified PG (<code>sens_pg</code>) can be
calculated as follows.
</p>
<p>If we let <code class="reqn">\theta</code> be the test property of interest (e.g. sensitivity),
and if we assume a prior distribution of the form
</p>
<p style="text-align: center;"><code class="reqn">\theta ~ Beta(\alpha, \beta),</code>
</p>

<p>then with <code class="reqn">X | \theta \sim Bin(n, \theta)</code>, where <code class="reqn">X</code> is the number
of new test positive cases from the reference positive cases, the posterior
distribution of <code class="reqn">\theta</code> is
</p>
<p style="text-align: center;"><code class="reqn">\theta | X=x ~ Beta(\alpha + x, \beta + n - x).</code>
</p>

<p>The posterior probability of exceeding the PG is then calculated as
</p>
<p><code class="reqn">P[\theta \ge sens_pg | X = x, n]</code>.
</p>
<p>A similar calculation can be performed for the specificity, with
corresponding PG, <code>spec_pg</code>.
</p>
<p><strong>Posterior predictive probability of eventual success</strong>
</p>
<p>When at an interim sample size that is less the maximum
(i.e. <code>max(n_at_looks)</code>), we can calculate the probability that the trial
will go on to eventually meet the success criteria.
</p>
<p>At the <code class="reqn">j</code>-th look, we have observed <code class="reqn">n_j</code> tests, with <code class="reqn">n_j^* =
n_{max} - n_j</code> subjects yet to be enrolled for testing. For the <code class="reqn">n_j^*</code>
subjects remaining, we can simulate the number of reference positive results,
<code class="reqn">y_j^*</code>, using the posterior predictive distribution for the prevalence
(reference positive tests), which is off the form
</p>
<p style="text-align: center;"><code class="reqn">y_j^* | y_j, n_j, n_j^* ~ Beta-Bin(n_j^*, \alpha_0 + y_j, \beta + n_j - y_j),</code>
</p>

<p>where <code class="reqn">y_j</code> is the observed number of reference positive cases.
Conditional on the number of subjects with a positive reference test in the
remaining sample together with <code class="reqn">n_j^*</code>, one can simulate the complete 2x2
contingency table by using the posterior predictive distributions for
sensitivity and specificity, each of which has a Beta-Binomial form.
Combining the observed <code class="reqn">n_j</code> subjects' data with a sample of the
<code class="reqn">n_j^*</code> subjects' data drawn from the predictive distribution, one can
then calculate the posterior probability of trial success (exceeding a PG)
for a specific endpoint. Repeating this many times and calculating the
proportion of probabilities that exceed the probability success threshold
yields the probability of eventual trial success at the maximum sample size.
</p>
<p>As well as calculating the predictive posterior probability of eventual
success for sensitivity and specificity, separately, we can also calculate
the probability for both endpoints simultaneously.
</p>


<h3>Value</h3>

<p>A list containing a data frame with rows for each stage of the trial
(i.e. each sample size look), irrespective of whether the trial meets the
stopping criteria. Multiple trial simulations are stacked longways and
indicated by the 'trial' column. The data frame has the following columns:
</p>

<ul>
<li><p><code>stage</code>: Trial stage.
</p>
</li>
<li><p><code>pp_sens</code>: Posterior probability of exceeding the performance
goal for sensitivity.
</p>
</li>
<li><p><code>pp_spec</code>: Posterior probability of exceeding the performance
goal for specificity.
</p>
</li>
<li><p><code>ppp_succ_sens</code>: Posterior predictive probability of eventual
success for sensitivity at the maximum sample size.
</p>
</li>
<li><p><code>ppp_succ_spec</code>: Posterior predictive probability of eventual
success for specificity at the maximum sample size.
</p>
</li>
<li><p><code>ppp_succ_both</code>: Posterior predictive probability of eventual
success for *both* sensitivity and specificity at the maximum sample
size.
</p>
</li>
<li><p><code>tp</code>: True positive count.
</p>
</li>
<li><p><code>tn</code>: True negative count.
</p>
</li>
<li><p><code>fp</code>: False positive count.
</p>
</li>
<li><p><code>fn</code>: False negative count.
</p>
</li>
<li><p><code>sens_hat</code>: Posterior median estimate of the test
sensitivity.
</p>
</li>
<li><p><code>sens_CrI2.5</code>: Lower bound of the 95
the test sensitivity.
</p>
</li>
<li><p><code>sens_CrI97.5</code>: Upper bound of the 95
the test sensitivity.
</p>
</li>
<li><p><code>spec_hat</code>: Posterior median estimate of the test
specificity.
</p>
</li>
<li><p><code>spec_CrI2.5</code>: Lower bound of the 95
the test specificity.
</p>
</li>
<li><p><code>spec_CrI97.5</code>: Upper bound of the 95
the test specificity.
</p>
</li>
<li><p><code>n</code>: The sample size at the given look for the row.
</p>
</li>
<li><p><code>trial</code>: The trial number, which will range from 1 to
'n_trials'.
</p>
</li></ul>

<p>The list also contains the arguments used and the call.
</p>


<h3>Parallelization</h3>

<p>To use multiple cores (where available), the argument <code>ncores</code> can be
increased from the default of 1. On UNIX machines (including macOS),
parallelization is performed using the <code><a href="parallel.html#topic+mclapply">mclapply</a></code>
function with <code>ncores</code> <code class="reqn">&gt;1</code>. On Windows machines, parallel
processing is implemented via the <code><a href="foreach.html#topic+foreach">foreach</a></code> function.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
multi_trial(
  sens_true = 0.9,
  spec_true = 0.95,
  prev_true = 0.1,
  endpoint = "both",
  sens_pg = 0.8,
  spec_pg = 0.8,
  prior_sens = c(0.1, 0.1),
  prior_spec = c(0.1, 0.1),
  prior_prev = c(0.1, 0.1),
  succ_sens = 0.95,
  succ_spec = 0.95,
  n_at_looks = c(200, 400, 600, 800, 1000),
  n_mc = 10000,
  n_trials = 2,
  ncores = 1
)

</code></pre>

<hr>
<h2 id='summarise_trials'>Summarise results of multiple simulated trials to give the operating
characteristics</h2><span id='topic+summarise_trials'></span>

<h3>Description</h3>

<p>Summarise results of multiple simulated trials to give the operating
characteristics
</p>


<h3>Usage</h3>

<pre><code class='language-R'>summarise_trials(data, min_pos = 1, fut = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summarise_trials_+3A_data">data</code></td>
<td>
<p>list. Output from the <code><a href="#topic+multi_trial">multi_trial</a></code> function.</p>
</td></tr>
<tr><td><code id="summarise_trials_+3A_min_pos">min_pos</code></td>
<td>
<p>integer. The minimum number of reference positive cases before
stopping is allowed. Default is <code>min_pos = 1</code>.</p>
</td></tr>
<tr><td><code id="summarise_trials_+3A_fut">fut</code></td>
<td>
<p>scalar. A probability threshold at which the posterior predictive
probability of eventual success is compared to. If the probability is less
than <code>fut</code>, the trial stops for binding futility. Default is <code>fut
= 0</code>, which corresponds to no stopping for futility.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame of row length 1, with the following columns:
</p>

<ul>
<li><p><code>power</code>: Power is defined as the proportion of trials that
result in success, irrespective of whether it is an early stop for
success or not. Trials that stop for futility, but which subsequently go
on to be successful, are not considered as a success. In other words, the
futility decision is binding, and in practice, if a trial triggered a
futility rule, the sponsor would not see the eventual outcome if the
trial were to continue enrolling. When the performance goals are set
equal to the respective true values, the power returned is the type I
error.
</p>
</li>
<li><p><code>stop_futility</code>: The proportion of trials that stopped early
for expected futility.
</p>
</li>
<li><p><code>n_avg</code>: The average sample size for trials at the stage they
stopped.
</p>
</li>
<li><p><code>sens</code>: The average sensitivity for trials at the stage they
stopped.
</p>
</li>
<li><p><code>spec</code>: The average specificity for trials at the stage they
stopped.
</p>
</li>
<li><p><code>mean_pos</code>: The average number of reference positive cases
for trials at the stage they stopped.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>data &lt;- multi_trial(
    sens_true = 0.9,
    spec_true = 0.95,
    prev_true = 0.1,
    endpoint = "both",
    sens_pg = 0.8,
    spec_pg = 0.8,
    prior_sens = c(1, 1),
    prior_spec = c(1, 1),
    prior_prev = c(1, 1),
    succ_sens = 0.95,
    succ_spec = 0.95,
    n_at_looks = c(200, 400, 600, 800, 1000),
    n_mc = 10000,
    n_trials = 20,
    ncores = 1
    )

summarise_trials(data, fut = 0.05, min_pos = 10)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
