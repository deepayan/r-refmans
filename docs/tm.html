<!DOCTYPE html><html><head><title>Help for package tm</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {tm}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#acq'><p>50 Exemplary News Articles from the Reuters-21578 Data Set of Topic acq</p></a></li>
<li><a href='#content_transformer'><p>Content Transformers</p></a></li>
<li><a href='#Corpus'><p>Corpora</p></a></li>
<li><a href='#crude'><p>20 Exemplary News Articles from the Reuters-21578 Data Set of Topic crude</p></a></li>
<li><a href='#DataframeSource'><p>Data Frame Source</p></a></li>
<li><a href='#DirSource'><p>Directory Source</p></a></li>
<li><a href='#Docs'><p>Access Document IDs and Terms</p></a></li>
<li><a href='#findAssocs'><p>Find Associations in a Term-Document Matrix</p></a></li>
<li><a href='#findFreqTerms'><p>Find Frequent Terms</p></a></li>
<li><a href='#findMostFreqTerms'><p>Find Most Frequent Terms</p></a></li>
<li><a href='#foreign'><p>Read Document-Term Matrices</p></a></li>
<li><a href='#getTokenizers'><p>Tokenizers</p></a></li>
<li><a href='#getTransformations'><p>Transformations</p></a></li>
<li><a href='#hpc'><p>Parallelized &lsquo;lapply&rsquo;</p></a></li>
<li><a href='#inspect'><p>Inspect Objects</p></a></li>
<li><a href='#meta'><p>Metadata Management</p></a></li>
<li><a href='#PCorpus'><p>Permanent Corpora</p></a></li>
<li><a href='#PlainTextDocument'><p>Plain Text Documents</p></a></li>
<li><a href='#plot'><p>Visualize a Term-Document Matrix</p></a></li>
<li><a href='#readDataframe'><p>Read In a Text Document from a Data Frame</p></a></li>
<li><a href='#readDOC'><p>Read In a MS Word Document</p></a></li>
<li><a href='#Reader'><p>Readers</p></a></li>
<li><a href='#readPDF'><p>Read In a PDF Document</p></a></li>
<li><a href='#readPlain'><p>Read In a Text Document</p></a></li>
<li><a href='#readRCV1'><p>Read In a Reuters Corpus Volume 1 Document</p></a></li>
<li><a href='#readReut21578XML'><p>Read In a Reuters-21578 XML Document</p></a></li>
<li><a href='#readTagged'><p>Read In a POS-Tagged Word Text Document</p></a></li>
<li><a href='#readXML'><p>Read In an XML Document</p></a></li>
<li><a href='#removeNumbers'><p>Remove Numbers from a Text Document</p></a></li>
<li><a href='#removePunctuation'><p>Remove Punctuation Marks from a Text Document</p></a></li>
<li><a href='#removeSparseTerms'><p>Remove Sparse Terms from a Term-Document Matrix</p></a></li>
<li><a href='#removeWords'><p>Remove Words from a Text Document</p></a></li>
<li><a href='#SimpleCorpus'><p>Simple Corpora</p></a></li>
<li><a href='#Source'><p>Sources</p></a></li>
<li><a href='#stemCompletion'><p>Complete Stems</p></a></li>
<li><a href='#stemDocument'><p>Stem Words</p></a></li>
<li><a href='#stopwords'><p>Stopwords</p></a></li>
<li><a href='#stripWhitespace'><p>Strip Whitespace from a Text Document</p></a></li>
<li><a href='#TermDocumentMatrix'><p>Term-Document Matrix</p></a></li>
<li><a href='#termFreq'><p>Term Frequency Vector</p></a></li>
<li><a href='#TextDocument'><p>Text Documents</p></a></li>
<li><a href='#tm_combine'><p>Combine Corpora, Documents, Term-Document Matrices, and Term Frequency Vectors</p></a></li>
<li><a href='#tm_filter'><p>Filter and Index Functions on Corpora</p></a></li>
<li><a href='#tm_map'><p>Transformations on Corpora</p></a></li>
<li><a href='#tm_reduce'><p>Combine Transformations</p></a></li>
<li><a href='#tm_term_score'><p>Compute Score for Matching Terms</p></a></li>
<li><a href='#tokenizer'><p>Tokenizers</p></a></li>
<li><a href='#URISource'><p>Uniform Resource Identifier Source</p></a></li>
<li><a href='#VCorpus'><p>Volatile Corpora</p></a></li>
<li><a href='#VectorSource'><p>Vector Source</p></a></li>
<li><a href='#weightBin'><p>Weight Binary</p></a></li>
<li><a href='#WeightFunction'><p>Weighting Function</p></a></li>
<li><a href='#weightSMART'><p>SMART Weightings</p></a></li>
<li><a href='#weightTf'><p>Weight by Term Frequency</p></a></li>
<li><a href='#weightTfIdf'><p>Weight by Term Frequency - Inverse Document Frequency</p></a></li>
<li><a href='#writeCorpus'><p>Write a Corpus to Disk</p></a></li>
<li><a href='#XMLSource'><p>XML Source</p></a></li>
<li><a href='#XMLTextDocument'><p>XML Text Documents</p></a></li>
<li><a href='#Zipf_n_Heaps'><p>Explore Corpus Term Frequency Characteristics</p></a></li>
<li><a href='#ZipSource'><p>ZIP File Source</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Text Mining Package</td>
</tr>
<tr>
<td>Version:</td>
<td>0.7-12</td>
</tr>
<tr>
<td>Date:</td>
<td>2024-03-11</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.2.0), NLP (&ge; 0.2-0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp, parallel, slam (&ge; 0.1-37), stats, tools, utils,
graphics, xml2</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>BH, Rcpp</td>
</tr>
<tr>
<td>Suggests:</td>
<td>antiword, filehash, methods, pdftools, Rcampdf, Rgraphviz,
Rpoppler, SnowballC, testthat, tm.lexicon.GeneralInquirer</td>
</tr>
<tr>
<td>Description:</td>
<td>A framework for text mining applications within R.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://tm.r-forge.r-project.org/">https://tm.r-forge.r-project.org/</a></td>
</tr>
<tr>
<td>Additional_repositories:</td>
<td><a href="https://datacube.wu.ac.at">https://datacube.wu.ac.at</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-03-11 14:59:15 UTC; hornik</td>
</tr>
<tr>
<td>Author:</td>
<td>Ingo Feinerer <a href="https://orcid.org/0000-0001-7656-8338"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre],
  Kurt Hornik <a href="https://orcid.org/0000-0003-4198-9911"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut],
  Artifex Software, Inc. [ctb, cph] (pdf_info.ps taken from GPL
    Ghostscript)</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Ingo Feinerer &lt;feinerer@logic.at&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-03-11 15:24:05 UTC</td>
</tr>
</table>
<hr>
<h2 id='acq'>50 Exemplary News Articles from the Reuters-21578 Data Set of Topic acq</h2><span id='topic+acq'></span>

<h3>Description</h3>

<p>This dataset holds 50 news articles with additional meta information from the
Reuters-21578 data set. All documents belong to the topic <code>acq</code> dealing
with corporate acquisitions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("acq")</code></pre>


<h3>Format</h3>

<p>A <code><a href="#topic+VCorpus">VCorpus</a></code> of 50 text documents.</p>


<h3>Source</h3>

<p>Reuters-21578 Text Categorization Collection Distribution 1.0
(<abbr><span class="acronym">XML</span></abbr> format).</p>


<h3>References</h3>







<p>Lewis, David (1997).
<em>Reuters-21578 Text Categorization Collection Distribution</em>.
UCI Machine Learning Repository.
<a href="https://doi.org/10.24432/C52G6M">doi:10.24432/C52G6M</a>.
</p>





<h3>Examples</h3>

<pre><code class='language-R'>data("acq")
acq
</code></pre>

<hr>
<h2 id='content_transformer'>Content Transformers</h2><span id='topic+content_transformer'></span>

<h3>Description</h3>

<p>Create content transformers, i.e., functions which modify the content of an
<span class="rlang"><b>R</b></span> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>content_transformer(FUN)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="content_transformer_+3A_fun">FUN</code></td>
<td>
<p>a function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A function with two arguments:
</p>

<dl>
<dt><code>x</code></dt><dd><p>an <span class="rlang"><b>R</b></span> object with implemented content getter
(<code><a href="NLP.html#topic+content">content</a></code>) and setter (<code><a href="NLP.html#topic+content+3C-">content&lt;-</a></code>)
functions.</p>
</dd>
<dt><code>...</code></dt><dd><p>arguments passed over to <code>FUN</code>.</p>
</dd>
</dl>



<h3>See Also</h3>

<p><code><a href="#topic+tm_map">tm_map</a></code> for an interface to apply transformations to corpora.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("crude")
crude[[1]]
(f &lt;- content_transformer(function(x, pattern) gsub(pattern, "", x)))
tm_map(crude, f, "[[:digit:]]+")[[1]]
</code></pre>

<hr>
<h2 id='Corpus'>Corpora</h2><span id='topic+Corpus'></span>

<h3>Description</h3>

<p>Representing and computing on corpora.
</p>


<h3>Details</h3>

<p><em>Corpora</em> are collections of documents containing (natural language)
text. In packages which employ the infrastructure provided by package
<span class="pkg">tm</span>, such corpora are represented via the virtual S3 class
<code>Corpus</code>: such packages then provide S3 corpus classes extending the
virtual base class (such as <code><a href="#topic+VCorpus">VCorpus</a></code> provided by package <span class="pkg">tm</span>
itself).
</p>
<p>All extension classes must provide accessors to extract subsets
(<code><a href="Matrix.html#topic++5B">[</a></code>), individual documents (<code><a href="base.html#topic++5B+5B">[[</a></code>), and metadata
(<code><a href="#topic+meta">meta</a></code>). The function <code><a href="base.html#topic+length">length</a></code> must return the number
of documents, and <code><a href="base.html#topic+as.list">as.list</a></code> must construct a list holding the
documents.
</p>
<p>A corpus can have two types of metadata (accessible via <code><a href="#topic+meta">meta</a></code>).
<em>Corpus metadata</em> contains corpus specific metadata in form of tag-value
pairs. <em>Document level metadata</em> contains document specific metadata but
is stored in the corpus as a data frame. Document level metadata is typically
used for semantic reasons (e.g., classifications of documents form an own
entity due to some high-level information like the range of possible values)
or for performance reasons (single access instead of extracting metadata of
each document).
</p>
<p>The function <code>Corpus</code> is a convenience alias to <code>SimpleCorpus</code> or
<code>VCorpus</code>, depending on the arguments provided.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+SimpleCorpus">SimpleCorpus</a></code>, <code><a href="#topic+VCorpus">VCorpus</a></code>, and <code><a href="#topic+PCorpus">PCorpus</a></code>
for the corpora classes provided by package <span class="pkg">tm</span>.
</p>
<p><code><a href="tm.plugin.dc.html#topic+DCorpus">DCorpus</a></code> for a distributed corpus class provided by
package <span class="pkg">tm.plugin.dc</span>.
</p>

<hr>
<h2 id='crude'>20 Exemplary News Articles from the Reuters-21578 Data Set of Topic crude</h2><span id='topic+crude'></span>

<h3>Description</h3>

<p>This data set holds 20 news articles with additional meta information from
the Reuters-21578 data set. All documents belong to the topic <code>crude</code>
dealing with crude oil.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("crude")</code></pre>


<h3>Format</h3>

<p>A <code><a href="#topic+VCorpus">VCorpus</a></code> of 20 text documents.</p>


<h3>Source</h3>

<p>Reuters-21578 Text Categorization Collection Distribution 1.0
(<abbr><span class="acronym">XML</span></abbr> format).</p>


<h3>References</h3>







<p>Lewis, David (1997).
<em>Reuters-21578 Text Categorization Collection Distribution</em>.
UCI Machine Learning Repository.
<a href="https://doi.org/10.24432/C52G6M">doi:10.24432/C52G6M</a>.
</p>





<h3>Examples</h3>

<pre><code class='language-R'>data("crude")
crude
</code></pre>

<hr>
<h2 id='DataframeSource'>Data Frame Source</h2><span id='topic+DataframeSource'></span>

<h3>Description</h3>

<p>Create a data frame source.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>DataframeSource(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="DataframeSource_+3A_x">x</code></td>
<td>
<p>A data frame giving the texts and metadata.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A <em>data frame source</em> interprets each row of the data frame <code>x</code> as a
document. The first column must be named <code>"doc_id"</code> and contain a unique
string identifier for each document. The second column must be named
<code>"text"</code> and contain a UTF-8 encoded string representing the
document's content. Optional additional columns are used as document level
metadata.
</p>


<h3>Value</h3>

<p>An object inheriting from <code>DataframeSource</code>, <code><a href="#topic+SimpleSource">SimpleSource</a></code>,
and <code><a href="#topic+Source">Source</a></code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Source">Source</a></code> for basic information on the source infrastructure
employed by package <span class="pkg">tm</span>, and <code><a href="#topic+meta">meta</a></code> for types of metadata.
</p>
<p><code><a href="readtext.html#topic+readtext">readtext</a></code> for reading in a text in multiple formats
suitable to be processed by <code>DataframeSource</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>docs &lt;- data.frame(doc_id = c("doc_1", "doc_2"),
                   text = c("This is a text.", "This another one."),
                   dmeta1 = 1:2, dmeta2 = letters[1:2],
                   stringsAsFactors = FALSE)
(ds &lt;- DataframeSource(docs))
x &lt;- Corpus(ds)
inspect(x)
meta(x)
</code></pre>

<hr>
<h2 id='DirSource'>Directory Source</h2><span id='topic+DirSource'></span>

<h3>Description</h3>

<p>Create a directory source.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>DirSource(directory = ".",
          encoding = "",
          pattern = NULL,
          recursive = FALSE,
          ignore.case = FALSE,
          mode = "text")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="DirSource_+3A_directory">directory</code></td>
<td>
<p>A character vector of full path names; the default
corresponds to the working directory <code>getwd()</code>.</p>
</td></tr>
<tr><td><code id="DirSource_+3A_encoding">encoding</code></td>
<td>
<p>a character string describing the current encoding. It is
passed to <code><a href="base.html#topic+iconv">iconv</a></code> to convert the input to UTF-8.</p>
</td></tr>
<tr><td><code id="DirSource_+3A_pattern">pattern</code></td>
<td>
<p>an optional regular expression. Only file names which match
the regular expression will be returned.</p>
</td></tr>
<tr><td><code id="DirSource_+3A_recursive">recursive</code></td>
<td>
<p>logical. Should the listing recurse into directories?</p>
</td></tr>
<tr><td><code id="DirSource_+3A_ignore.case">ignore.case</code></td>
<td>
<p>logical. Should pattern-matching be case-insensitive?</p>
</td></tr>
<tr><td><code id="DirSource_+3A_mode">mode</code></td>
<td>
<p>a character string specifying if and how files should be read in.
Available modes are:
</p>

<dl>
<dt><code>""</code></dt><dd><p>No read. In this case <code><a href="#topic+getElem">getElem</a></code> and
<code><a href="#topic+pGetElem">pGetElem</a></code> only deliver <abbr><span class="acronym">URI</span></abbr>s.</p>
</dd>
<dt><code>"binary"</code></dt><dd><p>Files are read in binary raw mode (via
<code><a href="base.html#topic+readBin">readBin</a></code>).</p>
</dd>
<dt><code>"text"</code></dt><dd><p>Files are read as text (via
<code><a href="base.html#topic+readLines">readLines</a></code>).</p>
</dd>
</dl>

</td></tr>
</table>


<h3>Details</h3>

<p>A <em>directory source</em> acquires a list of files via <code><a href="base.html#topic+dir">dir</a></code> and
interprets each file as a document.
</p>


<h3>Value</h3>

<p>An object inheriting from <code>DirSource</code>, <code><a href="#topic+SimpleSource">SimpleSource</a></code>, and
<code><a href="#topic+Source">Source</a></code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Source">Source</a></code> for basic information on the source infrastructure
employed by package <span class="pkg">tm</span>.
</p>
<p><code><a href="base.html#topic+Encoding">Encoding</a></code> and <code><a href="base.html#topic+iconv">iconv</a></code> on encodings.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>DirSource(system.file("texts", "txt", package = "tm"))
</code></pre>

<hr>
<h2 id='Docs'>Access Document IDs and Terms</h2><span id='topic+Docs'></span><span id='topic+nDocs'></span><span id='topic+nTerms'></span><span id='topic+Terms'></span>

<h3>Description</h3>

<p>Accessing document IDs, terms, and their number of a term-document matrix or
document-term matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Docs(x)
nDocs(x)
nTerms(x)
Terms(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Docs_+3A_x">x</code></td>
<td>
<p>Either a <code><a href="#topic+TermDocumentMatrix">TermDocumentMatrix</a></code> or
<code><a href="#topic+DocumentTermMatrix">DocumentTermMatrix</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>For <code>Docs</code> and <code>Terms</code>, a character vector with document IDs and
terms, respectively.
</p>
<p>For <code>nDocs</code> and <code>nTerms</code>, an integer with the number of document IDs
and terms, respectively.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("crude")
tdm &lt;- TermDocumentMatrix(crude)[1:10,1:20]
Docs(tdm)
nDocs(tdm)
nTerms(tdm)
Terms(tdm)
</code></pre>

<hr>
<h2 id='findAssocs'>Find Associations in a Term-Document Matrix</h2><span id='topic+findAssocs'></span><span id='topic+findAssocs.DocumentTermMatrix'></span><span id='topic+findAssocs.TermDocumentMatrix'></span>

<h3>Description</h3>

<p>Find associations in a document-term or term-document matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'DocumentTermMatrix'
findAssocs(x, terms, corlimit)
## S3 method for class 'TermDocumentMatrix'
findAssocs(x, terms, corlimit)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="findAssocs_+3A_x">x</code></td>
<td>
<p>A <code><a href="#topic+DocumentTermMatrix">DocumentTermMatrix</a></code> or a
<code><a href="#topic+TermDocumentMatrix">TermDocumentMatrix</a></code>.</p>
</td></tr>
<tr><td><code id="findAssocs_+3A_terms">terms</code></td>
<td>
<p>a character vector holding terms.</p>
</td></tr>
<tr><td><code id="findAssocs_+3A_corlimit">corlimit</code></td>
<td>
<p>a numeric vector (of the same length as <code>terms</code>;
recycled otherwise) for the (inclusive) lower correlation limits of each
term in the range from zero to one.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A named list. Each list component is named after a term in <code>terms</code>
and contains a named numeric vector. Each vector holds matching terms from
<code>x</code> and their rounded correlations satisfying the inclusive lower
correlation limit of <code>corlimit</code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("crude")
tdm &lt;- TermDocumentMatrix(crude)
findAssocs(tdm, c("oil", "opec", "xyz"), c(0.7, 0.75, 0.1))
</code></pre>

<hr>
<h2 id='findFreqTerms'>Find Frequent Terms</h2><span id='topic+findFreqTerms'></span>

<h3>Description</h3>

<p>Find frequent terms in a document-term or term-document matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>findFreqTerms(x, lowfreq = 0, highfreq = Inf)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="findFreqTerms_+3A_x">x</code></td>
<td>
<p>A <code><a href="#topic+DocumentTermMatrix">DocumentTermMatrix</a></code> or
<code><a href="#topic+TermDocumentMatrix">TermDocumentMatrix</a></code>.</p>
</td></tr>
<tr><td><code id="findFreqTerms_+3A_lowfreq">lowfreq</code></td>
<td>
<p>A numeric for the lower frequency bound.</p>
</td></tr>
<tr><td><code id="findFreqTerms_+3A_highfreq">highfreq</code></td>
<td>
<p>A numeric for the upper frequency bound.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This method works for all numeric weightings but is probably
most meaningful for the standard term frequency (<code>tf</code>) weighting
of <code>x</code>.</p>


<h3>Value</h3>

<p>A character vector of terms in <code>x</code> which occur more or equal often
than <code>lowfreq</code> times and less or equal often than <code>highfreq</code>
times.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("crude")
tdm &lt;- TermDocumentMatrix(crude)
findFreqTerms(tdm, 2, 3)
</code></pre>

<hr>
<h2 id='findMostFreqTerms'>Find Most Frequent Terms</h2><span id='topic+findMostFreqTerms'></span><span id='topic+findMostFreqTerms.term_frequency'></span><span id='topic+findMostFreqTerms.DocumentTermMatrix'></span><span id='topic+findMostFreqTerms.TermDocumentMatrix'></span>

<h3>Description</h3>

<p>Find most frequent terms in a document-term or term-document matrix,
or a vector of term frequencies.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>findMostFreqTerms(x, n = 6L, ...)
## S3 method for class 'DocumentTermMatrix'
findMostFreqTerms(x, n = 6L, INDEX = NULL, ...)
## S3 method for class 'TermDocumentMatrix'
findMostFreqTerms(x, n = 6L, INDEX = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="findMostFreqTerms_+3A_x">x</code></td>
<td>
<p>A <code><a href="#topic+DocumentTermMatrix">DocumentTermMatrix</a></code> or
<code><a href="#topic+TermDocumentMatrix">TermDocumentMatrix</a></code>, or a vector of term frequencies as
obtained by <code><a href="#topic+termFreq">termFreq</a>()</code>.</p>
</td></tr>
<tr><td><code id="findMostFreqTerms_+3A_n">n</code></td>
<td>
<p>A single integer giving the maximal number of terms.</p>
</td></tr>
<tr><td><code id="findMostFreqTerms_+3A_index">INDEX</code></td>
<td>
<p>an object specifying a grouping of documents for rollup,
or <code>NULL</code> (default) in which case each document is considered
individually.</p>
</td></tr>
<tr><td><code id="findMostFreqTerms_+3A_...">...</code></td>
<td>
<p>arguments to be passed to or from methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Only terms with positive frequencies are included in the results.
</p>


<h3>Value</h3>

<p>For the document-term or term-document matrix methods, a list with the
named frequencies of the up to <code>n</code> most frequent terms occurring
in each document (group).  Otherwise, a single such vector of most
frequent terms.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("crude")

## Term frequencies:
tf &lt;- termFreq(crude[[14L]])
findMostFreqTerms(tf)

## Document-term matrices:
dtm &lt;- DocumentTermMatrix(crude)
## Most frequent terms for each document:
findMostFreqTerms(dtm)
## Most frequent terms for the first 10 the second 10 documents,
## respectively:
findMostFreqTerms(dtm, INDEX = rep(1 : 2, each = 10L))
</code></pre>

<hr>
<h2 id='foreign'>Read Document-Term Matrices</h2><span id='topic+read_dtm_Blei_et_al'></span><span id='topic+read_dtm_MC'></span>

<h3>Description</h3>

<p>Read document-term matrices stored in special file formats.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>read_dtm_Blei_et_al(file, vocab = NULL)
read_dtm_MC(file, scalingtype = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="foreign_+3A_file">file</code></td>
<td>
<p>a character string with the name of the file to read.</p>
</td></tr>
<tr><td><code id="foreign_+3A_vocab">vocab</code></td>
<td>
<p>a character string with the name of a vocabulary file
(giving the terms, one per line), or <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="foreign_+3A_scalingtype">scalingtype</code></td>
<td>
<p>a character string specifying the type of scaling
to be used, or <code>NULL</code> (default), in which case the scaling will
be inferred from the names of the files with non-zero entries found
(see <b>Details</b>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>read_dtm_Blei_et_al</code> reads the (List of Lists type sparse
matrix) format employed by the Latent Dirichlet Allocation and
Correlated Topic Model C codes by Blei et al
(<a href="http://www.cs.columbia.edu/~blei/">http://www.cs.columbia.edu/~blei/</a>).
</p>
<p>MC is a toolkit for creating vector models from text documents (see 
<a href="https://www.cs.utexas.edu/users/dml/software/mc/">https://www.cs.utexas.edu/users/dml/software/mc/</a>).  It employs a
variant of Compressed Column Storage (CCS) sparse matrix format,
writing data into several files with suitable names: e.g., a file with
&lsquo;<span class="file">_dim</span>&rsquo; appended to the base file name stores the matrix
dimensions.  The non-zero entries are stored in a file the name of
which indicates the scaling type used: e.g., &lsquo;<span class="file">_tfx_nz</span>&rsquo; indicates
scaling by term frequency (&lsquo;<span class="samp">&#8288;t&#8288;</span>&rsquo;), inverse document frequency
(&lsquo;<span class="samp">&#8288;f&#8288;</span>&rsquo;) and no normalization (&lsquo;<span class="samp">&#8288;x&#8288;</span>&rsquo;).  See &lsquo;<span class="file">README</span>&rsquo; in the
MC sources for more information.
</p>
<p><code>read_dtm_MC</code> reads such sparse matrix information with argument
<code>file</code> giving the path with the base file name.
</p>


<h3>Value</h3>

<p>A <a href="#topic+DocumentTermMatrix">document-term matrix</a>.
</p>


<h3>See Also</h3>

<p><code><a href="slam.html#topic+read_stm_MC">read_stm_MC</a></code> in package <span class="pkg">slam</span>.
</p>

<hr>
<h2 id='getTokenizers'>Tokenizers</h2><span id='topic+getTokenizers'></span>

<h3>Description</h3>

<p>Predefined tokenizers.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getTokenizers()
</code></pre>


<h3>Value</h3>

<p>A character vector with tokenizers provided by package <span class="pkg">tm</span>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Boost_tokenizer">Boost_tokenizer</a></code>, <code><a href="#topic+MC_tokenizer">MC_tokenizer</a></code> and
<code><a href="#topic+scan_tokenizer">scan_tokenizer</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>getTokenizers()
</code></pre>

<hr>
<h2 id='getTransformations'>Transformations</h2><span id='topic+getTransformations'></span>

<h3>Description</h3>

<p>Predefined transformations (mappings) which can be used with
<code><a href="#topic+tm_map">tm_map</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getTransformations()
</code></pre>


<h3>Value</h3>

<p>A character vector with transformations provided by package <span class="pkg">tm</span>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+removeNumbers">removeNumbers</a></code>, <code><a href="#topic+removePunctuation">removePunctuation</a></code>,
<code><a href="#topic+removeWords">removeWords</a></code>, <code><a href="#topic+stemDocument">stemDocument</a></code>, and
<code><a href="#topic+stripWhitespace">stripWhitespace</a></code>.
</p>
<p><code><a href="#topic+content_transformer">content_transformer</a></code> to create custom transformations.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>getTransformations()
</code></pre>

<hr>
<h2 id='hpc'>Parallelized &lsquo;lapply&rsquo;</h2><span id='topic+tm_parLapply'></span><span id='topic+tm_parLapply_engine'></span>

<h3>Description</h3>

<p>Parallelize applying a function over a list or vector according to the
registered parallelization engine.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tm_parLapply(X, FUN, ...)
tm_parLapply_engine(new)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hpc_+3A_x">X</code></td>
<td>
<p>A vector (atomic or list), or other objects suitable for the
engine in use.</p>
</td></tr>
<tr><td><code id="hpc_+3A_fun">FUN</code></td>
<td>
<p>the function to be applied to each element of <code>X</code>.</p>
</td></tr>
<tr><td><code id="hpc_+3A_...">...</code></td>
<td>
<p>optional arguments to <code>FUN</code>.</p>
</td></tr>
<tr><td><code id="hpc_+3A_new">new</code></td>
<td>
<p>an object inheriting from class <code>cluster</code> as created
by <code><a href="parallel.html#topic+makeCluster">makeCluster</a>()</code> from package
<span class="pkg">parallel</span>, or a function with formals <code>X</code>, <code>FUN</code> and
<code>...</code>, or <code>NULL</code> corresponding to the default of using no
parallelization engine.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Parallelization can be employed to speed up some of the embarrassingly
parallel computations performed in package <span class="pkg">tm</span>, specifically
<code><a href="#topic+tm_index">tm_index</a>()</code>, <code><a href="#topic+tm_map">tm_map</a>()</code> on a non-lazy-mapped
<code><a href="#topic+VCorpus">VCorpus</a></code>, and <code><a href="#topic+TermDocumentMatrix">TermDocumentMatrix</a>()</code> on a
<code><a href="#topic+VCorpus">VCorpus</a></code> or <code><a href="#topic+PCorpus">PCorpus</a></code>.
</p>
<p>Functions <code>tm_parLapply()</code> and <code>tm_parLapply_engine()</code> can
be used to customize parallelization according to the available
resources.
</p>
<p><code>tm_parLapply_engine()</code> is used for getting (with no arguments)
or setting (with argument <code>new</code>) the parallelization engine
employed (see below for examples).
</p>
<p>If an engine is set to an object inheriting from class <code>cluster</code>,
<code>tm_parLapply()</code> calls
<code><a href="parallel.html#topic+parLapply">parLapply</a>()</code> with this cluster and 
the given arguments.  If set to a function, <code>tm_parLapply()</code>
calls the function with the given arguments.  Otherwise, it simply
calls <code><a href="base.html#topic+lapply">lapply</a>()</code>.
</p>
<p>Hence, parallelization via
<code><a href="parallel.html#topic+parLapply">parLapply</a>()</code>
and a default cluster registered via
<code><a href="parallel.html#topic+setDefaultCluster">setDefaultCluster</a>()</code> can be
achieved via </p>
<pre>  tm_parLapply_engine(function(X, FUN, ...)
      parallel::parLapply(NULL, X, FUN, ...))</pre>
<p>or re-registering the cluster, say <code>cl</code>, using </p>
<pre>  tm_parLapply_engine(cl)</pre>
<p>(note that since <span class="rlang"><b>R</b></span> version 3.5.0, one can use
<code><a href="parallel.html#topic+getDefaultCluster">getDefaultCluster</a>()</code> to get
the registered default cluster).  Using </p>
<pre>  tm_parLapply_engine(function(X, FUN, ...)
      parallel::parLapplyLB(NULL, X, FUN, ...))</pre>
<p>or </p>
<pre>  tm_parLapply_engine(function(X, FUN, ...)
      parallel::parLapplyLB(cl, X, FUN, ...))</pre>
<p>gives load-balancing parallelization with the registered default or
given cluster, respectively.  To achieve parallelization via forking
(on Unix-alike platforms), one can use the above with clusters created
by <code><a href="parallel.html#topic+makeForkCluster">makeForkCluster</a>()</code>, or use </p>
<pre>  tm_parLapply_engine(parallel::mclapply)</pre>
<p>or </p>
<pre>  tm_parLapply_engine(function(X, FUN, ...)
      parallel::mclapply(X, FUN, ..., mc.cores = n))</pre>
<p>to use <code><a href="parallel.html#topic+mclapply">mclapply</a>()</code> with the default or
given number <code>n</code> of cores.
</p>


<h3>Value</h3>

<p>A list the length of <code>X</code>, with the result of applying <code>FUN</code>
together with the <code>...</code> arguments to each element of <code>X</code>.
</p>


<h3>See Also</h3>

<p><code><a href="parallel.html#topic+makeCluster">makeCluster</a>()</code>,
<code><a href="parallel.html#topic+parLapply">parLapply</a>()</code>,
<code><a href="parallel.html#topic+parLapplyLB">parLapplyLB</a>()</code>, and
<code><a href="parallel.html#topic+mclapply">mclapply</a>()</code>.
</p>

<hr>
<h2 id='inspect'>Inspect Objects</h2><span id='topic+inspect'></span><span id='topic+inspect.PCorpus'></span><span id='topic+inspect.TermDocumentMatrix'></span><span id='topic+inspect.TextDocument'></span><span id='topic+inspect.VCorpus'></span>

<h3>Description</h3>

<p>Inspect, i.e., display detailed information on a corpus, a
term-document matrix, or a text document.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'PCorpus'
inspect(x)
## S3 method for class 'VCorpus'
inspect(x)
## S3 method for class 'TermDocumentMatrix'
inspect(x)
## S3 method for class 'TextDocument'
inspect(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="inspect_+3A_x">x</code></td>
<td>
<p>Either a corpus, a term-document matrix, or a text document.</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>data("crude")
inspect(crude[1:3])
inspect(crude[[1]])
tdm &lt;- TermDocumentMatrix(crude)[1:10, 1:10]
inspect(tdm)
</code></pre>

<hr>
<h2 id='meta'>Metadata Management</h2><span id='topic+DublinCore'></span><span id='topic+DublinCore+3C-'></span><span id='topic+meta'></span><span id='topic+meta.PCorpus'></span><span id='topic+meta.SimpleCorpus'></span><span id='topic+meta.VCorpus'></span><span id='topic+meta+3C-.PCorpus'></span><span id='topic+meta+3C-.SimpleCorpus'></span><span id='topic+meta+3C-.VCorpus'></span><span id='topic+meta.PlainTextDocument'></span><span id='topic+meta+3C-.PlainTextDocument'></span><span id='topic+meta.XMLTextDocument'></span><span id='topic+meta+3C-.XMLTextDocument'></span>

<h3>Description</h3>

<p>Accessing and modifying metadata of text documents and corpora.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'PCorpus'
meta(x, tag = NULL, type = c("indexed", "corpus", "local"), ...)
## S3 replacement method for class 'PCorpus'
meta(x, tag, type = c("indexed", "corpus", "local"), ...) &lt;- value
## S3 method for class 'SimpleCorpus'
meta(x, tag = NULL, type = c("indexed", "corpus"), ...)
## S3 replacement method for class 'SimpleCorpus'
meta(x, tag, type = c("indexed", "corpus"), ...) &lt;- value
## S3 method for class 'VCorpus'
meta(x, tag = NULL, type = c("indexed", "corpus", "local"), ...)
## S3 replacement method for class 'VCorpus'
meta(x, tag, type = c("indexed", "corpus", "local"), ...) &lt;- value
## S3 method for class 'PlainTextDocument'
meta(x, tag = NULL, ...)
## S3 replacement method for class 'PlainTextDocument'
meta(x, tag = NULL, ...) &lt;- value
## S3 method for class 'XMLTextDocument'
meta(x, tag = NULL, ...)
## S3 replacement method for class 'XMLTextDocument'
meta(x, tag = NULL, ...) &lt;- value
DublinCore(x, tag = NULL)
DublinCore(x, tag) &lt;- value
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="meta_+3A_x">x</code></td>
<td>
<p>For <code>DublinCore</code> a <code><a href="#topic+TextDocument">TextDocument</a></code>, and for
<code>meta</code> a <code><a href="#topic+TextDocument">TextDocument</a></code> or a <code><a href="#topic+Corpus">Corpus</a></code>.</p>
</td></tr>
<tr><td><code id="meta_+3A_tag">tag</code></td>
<td>
<p>a character giving the name of a metadatum. No tag corresponds to
all available metadata.</p>
</td></tr>
<tr><td><code id="meta_+3A_type">type</code></td>
<td>
<p>a character specifying the kind of corpus metadata (see
<b>Details</b>).</p>
</td></tr>
<tr><td><code id="meta_+3A_...">...</code></td>
<td>
<p>Not used.</p>
</td></tr>
<tr><td><code id="meta_+3A_value">value</code></td>
<td>
<p>replacement value.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A corpus has two types of metadata. <em>Corpus metadata</em> (<code>"corpus"</code>)
contains corpus specific metadata in form of tag-value pairs.
<em>Document level metadata</em> (<code>"indexed"</code>) contains document specific
metadata but is stored in the corpus as a data frame. Document level metadata
is typically used for semantic reasons (e.g., classifications of documents
form an own entity due to some high-level information like the range of
possible values) or for performance reasons (single access instead of
extracting metadata of each document). The latter can be seen as a from of
indexing, hence the name <code>"indexed"</code>. <em>Document metadata</em>
(<code>"local"</code>) are tag-value pairs directly stored locally at the individual
documents.
</p>
<p><code>DublinCore</code> is a convenience wrapper to access and modify the metadata
of a text document using the Simple Dublin Core schema (supporting the 15
metadata elements from the Dublin Core Metadata Element Set
<a href="https://dublincore.org/documents/dces/">https://dublincore.org/documents/dces/</a>).
</p>


<h3>References</h3>

<p>Dublin Core Metadata Initiative.
<a href="https://dublincore.org/">https://dublincore.org/</a>
</p>


<h3>See Also</h3>

<p><code><a href="NLP.html#topic+meta">meta</a></code> for metadata in package <span class="pkg">NLP</span>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("crude")
meta(crude[[1]])
DublinCore(crude[[1]])
meta(crude[[1]], tag = "topics")
meta(crude[[1]], tag = "comment") &lt;- "A short comment."
meta(crude[[1]], tag = "topics") &lt;- NULL
DublinCore(crude[[1]], tag = "creator") &lt;- "Ano Nymous"
DublinCore(crude[[1]], tag = "format") &lt;- "XML"
DublinCore(crude[[1]])
meta(crude[[1]])
meta(crude)
meta(crude, type = "corpus")
meta(crude, "labels") &lt;- 21:40
meta(crude)
</code></pre>

<hr>
<h2 id='PCorpus'>Permanent Corpora</h2><span id='topic+PCorpus'></span>

<h3>Description</h3>

<p>Create permanent corpora.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>PCorpus(x,
        readerControl = list(reader = reader(x), language = "en"),
        dbControl = list(dbName = "", dbType = "DB1"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="PCorpus_+3A_x">x</code></td>
<td>
<p>A <code><a href="#topic+Source">Source</a></code> object.</p>
</td></tr>
<tr><td><code id="PCorpus_+3A_readercontrol">readerControl</code></td>
<td>
<p>a named list of control parameters for reading in content
from <code>x</code>.
</p>

<dl>
<dt><code>reader</code></dt><dd><p>a function capable of reading in and processing the
format delivered by <code>x</code>.</p>
</dd>
<dt><code>language</code></dt><dd><p>a character giving the language (preferably as
<abbr><span class="acronym">IETF</span></abbr> language tags, see <a href="NLP.html#topic+language">language</a> in
package <span class="pkg">NLP</span>).
The default language is assumed to be English (<code>"en"</code>).</p>
</dd>
</dl>

</td></tr>
<tr><td><code id="PCorpus_+3A_dbcontrol">dbControl</code></td>
<td>
<p>a named list of control parameters for the underlying
database storage provided by package <span class="pkg">filehash</span>.
</p>

<dl>
<dt><code>dbName</code></dt><dd><p>a character giving the filename for the database.</p>
</dd>
<dt><code>dbType</code></dt><dd><p>a character giving the database format (see
<code><a href="filehash.html#topic+filehashOption">filehashOption</a></code> for possible database formats).</p>
</dd>
</dl>

</td></tr>
</table>


<h3>Details</h3>

<p>A <em>permanent corpus</em> stores documents outside of <span class="rlang"><b>R</b></span> in a database. Since
multiple <code>PCorpus</code> <span class="rlang"><b>R</b></span> objects with the same underlying database can
exist simultaneously in memory, changes in one get propagated to all
corresponding objects (in contrast to the default <span class="rlang"><b>R</b></span> semantics).
</p>


<h3>Value</h3>

<p>An object inheriting from <code>PCorpus</code> and <code>Corpus</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Corpus">Corpus</a></code> for basic information on the corpus infrastructure
employed by package <span class="pkg">tm</span>.
</p>
<p><code><a href="#topic+VCorpus">VCorpus</a></code> provides an implementation with volatile storage
semantics.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>txt &lt;- system.file("texts", "txt", package = "tm")
## Not run: 
PCorpus(DirSource(txt),
        dbControl = list(dbName = "pcorpus.db", dbType = "DB1"))
## End(Not run)
</code></pre>

<hr>
<h2 id='PlainTextDocument'>Plain Text Documents</h2><span id='topic+PlainTextDocument'></span>

<h3>Description</h3>

<p>Create plain text documents.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>PlainTextDocument(x = character(0),
                  author = character(0),
                  datetimestamp = as.POSIXlt(Sys.time(), tz = "GMT"),
                  description = character(0),
                  heading = character(0),
                  id = character(0),
                  language = character(0),
                  origin = character(0),
                  ...,
                  meta = NULL,
                  class = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="PlainTextDocument_+3A_x">x</code></td>
<td>
<p>A character string giving the plain text content.</p>
</td></tr>
<tr><td><code id="PlainTextDocument_+3A_author">author</code></td>
<td>
<p>a character string or an object of class <code><a href="utils.html#topic+person">person</a></code> giving
the author names.</p>
</td></tr>
<tr><td><code id="PlainTextDocument_+3A_datetimestamp">datetimestamp</code></td>
<td>
<p>an object of class <code><a href="base.html#topic+POSIXt">POSIXt</a></code> or a character
string giving the creation date/time information.  If a character string,
exactly one of the <abbr><span class="acronym">ISO</span></abbr> 8601 formats defined by
<a href="https://www.w3.org/TR/NOTE-datetime">https://www.w3.org/TR/NOTE-datetime</a> should be used.
See <code><a href="NLP.html#topic+parse_ISO_8601_datetime">parse_ISO_8601_datetime</a></code> in package <span class="pkg">NLP</span>
for processing such date/time information.
</p>
</td></tr>
<tr><td><code id="PlainTextDocument_+3A_description">description</code></td>
<td>
<p>a character string giving a description.</p>
</td></tr>
<tr><td><code id="PlainTextDocument_+3A_heading">heading</code></td>
<td>
<p>a character string giving the title or a short heading.</p>
</td></tr>
<tr><td><code id="PlainTextDocument_+3A_id">id</code></td>
<td>
<p>a character string giving a unique identifier.</p>
</td></tr>
<tr><td><code id="PlainTextDocument_+3A_language">language</code></td>
<td>
<p>a character string giving the language (preferably as <abbr><span class="acronym">IETF</span></abbr>
language tags, see <a href="NLP.html#topic+language">language</a> in package <span class="pkg">NLP</span>).</p>
</td></tr>
<tr><td><code id="PlainTextDocument_+3A_origin">origin</code></td>
<td>
<p>a character string giving information on the source and origin.</p>
</td></tr>
<tr><td><code id="PlainTextDocument_+3A_...">...</code></td>
<td>
<p>user-defined document metadata tag-value pairs.</p>
</td></tr>
<tr><td><code id="PlainTextDocument_+3A_meta">meta</code></td>
<td>
<p>a named list or <code>NULL</code> (default) giving all metadata. If set
all other metadata arguments are ignored.</p>
</td></tr>
<tr><td><code id="PlainTextDocument_+3A_class">class</code></td>
<td>
<p>a character vector or <code>NULL</code> (default) giving
additional classes to be used for the created plain text document.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object inheriting from <code>class</code>, <code>PlainTextDocument</code> and
<code><a href="#topic+TextDocument">TextDocument</a></code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+TextDocument">TextDocument</a></code> for basic information on the text document
infrastructure employed by package <span class="pkg">tm</span>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>(ptd &lt;- PlainTextDocument("A simple plain text document",
                          heading = "Plain text document",
                          id = basename(tempfile()),
                          language = "en"))
meta(ptd)
</code></pre>

<hr>
<h2 id='plot'>Visualize a Term-Document Matrix</h2><span id='topic+plot.TermDocumentMatrix'></span>

<h3>Description</h3>

<p>Visualize correlations between terms of a term-document matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'TermDocumentMatrix'
plot(x,
     terms = sample(Terms(x), 20),
     corThreshold = 0.7,
     weighting = FALSE,
     attrs = list(graph = list(rankdir = "BT"),
                  node = list(shape = "rectangle",
                              fixedsize = FALSE)),
     ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_+3A_x">x</code></td>
<td>
<p>A term-document matrix.</p>
</td></tr>
<tr><td><code id="plot_+3A_terms">terms</code></td>
<td>
<p>Terms to be plotted. Defaults to 20 randomly chosen terms
of the term-document matrix.</p>
</td></tr>
<tr><td><code id="plot_+3A_corthreshold">corThreshold</code></td>
<td>
<p>Do not plot correlations below this
threshold. Defaults to <code>0.7</code>.</p>
</td></tr>
<tr><td><code id="plot_+3A_weighting">weighting</code></td>
<td>
<p>Define whether the line width corresponds to the
correlation.</p>
</td></tr>
<tr><td><code id="plot_+3A_attrs">attrs</code></td>
<td>
<p>Argument passed to the plot method for class
<code><a href="graph.html#topic+graphNEL-class">graphNEL</a></code>.</p>
</td></tr>
<tr><td><code id="plot_+3A_...">...</code></td>
<td>
<p>Other arguments passed to the
<code><a href="graph.html#topic+graphNEL-class">graphNEL</a></code> plot method.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Visualization requires that package <span class="pkg">Rgraphviz</span> is available.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: data(crude)
tdm &lt;- TermDocumentMatrix(crude,
                          control = list(removePunctuation = TRUE,
                                         removeNumbers = TRUE,
                                         stopwords = TRUE))
plot(tdm, corThreshold = 0.2, weighting = TRUE)
## End(Not run)
</code></pre>

<hr>
<h2 id='readDataframe'>Read In a Text Document from a Data Frame</h2><span id='topic+readDataframe'></span>

<h3>Description</h3>

<p>Read in a text document from a row in a data frame.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>readDataframe(elem, language, id)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="readDataframe_+3A_elem">elem</code></td>
<td>
<p>a named list with the component <code>content</code> which must hold
a data frame with rows as the documents to be read in. The names of the
columns holding the text content and the document identifier must be
<code>"text"</code> and <code>"doc_id"</code>, respectively.</p>
</td></tr>
<tr><td><code id="readDataframe_+3A_language">language</code></td>
<td>
<p>a string giving the language.</p>
</td></tr>
<tr><td><code id="readDataframe_+3A_id">id</code></td>
<td>
<p>Not used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code><a href="#topic+PlainTextDocument">PlainTextDocument</a></code> representing <code>elem$content</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Reader">Reader</a></code> for basic information on the reader infrastructure
employed by package <span class="pkg">tm</span>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>docs &lt;- data.frame(doc_id = c("doc_1", "doc_2"),
                   text = c("This is a text.", "This another one."),
                   stringsAsFactors = FALSE)
ds &lt;- DataframeSource(docs)
elem &lt;- getElem(stepNext(ds))
result &lt;- readDataframe(elem, "en", NULL)
inspect(result)
meta(result)
</code></pre>

<hr>
<h2 id='readDOC'>Read In a MS Word Document</h2><span id='topic+readDOC'></span>

<h3>Description</h3>

<p>Return a function which reads in a Microsoft Word document extracting
its text.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>readDOC(engine = c("antiword", "executable"), AntiwordOptions = "")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="readDOC_+3A_engine">engine</code></td>
<td>
<p>a character string for the preferred <abbr><span class="acronym">DOC</span></abbr> extraction
engine (see <b>Details</b>).</p>
</td></tr>
<tr><td><code id="readDOC_+3A_antiwordoptions">AntiwordOptions</code></td>
<td>
<p>Options passed over to <code>antiword</code> executable.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Formally this function is a function generator, i.e., it returns a
function (which reads in a text document) with a well-defined
signature, but can access passed over arguments (e.g., options to
<code>antiword</code>) via lexical scoping.
</p>
<p>Available <abbr><span class="acronym">DOC</span></abbr> extraction engines are as follows.
</p>

<dl>
<dt><code>"antiword"</code></dt><dd><p>(default) Antiword utility as provided by the
function <code><a href="antiword.html#topic+antiword">antiword</a></code> in package <span class="pkg">antiword</span>.</p>
</dd>
<dt><code>"executable"</code></dt><dd><p>command line <code>antiword</code>
executable which must be installed and accessible on your system.
This can convert documents from Microsoft Word version 2, 6, 7,
97, 2000, 2002 and 2003 to plain text.


The character vector <code>AntiwordOptions</code> is passed over to the
executable.</p>
</dd>
</dl>



<h3>Value</h3>

<p>A <code>function</code> with the following formals:
</p>

<dl>
<dt><code>elem</code></dt><dd><p>a list with the named component <code>uri</code> which must
hold a valid file name.</p>
</dd>
<dt><code>language</code></dt><dd><p>a string giving the language.</p>
</dd>
<dt><code>id</code></dt><dd><p>Not used.</p>
</dd>
</dl>

<p>The function returns a <code><a href="#topic+PlainTextDocument">PlainTextDocument</a></code> representing the text
and metadata extracted from <code>elem$uri</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Reader">Reader</a></code> for basic information on the reader infrastructure
employed by package <span class="pkg">tm</span>.
</p>

<hr>
<h2 id='Reader'>Readers</h2><span id='topic+FunctionGenerator'></span><span id='topic+Reader'></span><span id='topic+getReaders'></span>

<h3>Description</h3>

<p>Creating readers.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getReaders()
</code></pre>


<h3>Details</h3>

<p><em>Readers</em> are functions for extracting textual content and metadata out
of elements delivered by a <code><a href="#topic+Source">Source</a></code>, and for constructing a
<code><a href="#topic+TextDocument">TextDocument</a></code>. A reader must accept following arguments in
its signature:
</p>

<dl>
<dt><code>elem</code></dt><dd><p>a named list with the components <code>content</code> and
<code>uri</code> (as delivered by a <code><a href="#topic+Source">Source</a></code> via
<code><a href="#topic+getElem">getElem</a></code> or <code><a href="#topic+pGetElem">pGetElem</a></code>).</p>
</dd>
<dt><code>language</code></dt><dd><p>a character string giving the language.</p>
</dd>
<dt><code>id</code></dt><dd><p>a character giving a unique identifier for the created text
document.</p>
</dd>
</dl>

<p>The element <code>elem</code> is typically provided by a source whereas the language
and the identifier are normally provided by a corpus constructor (for the case
that <code>elem$content</code> does not give information on these two essential
items).
</p>
<p>In case a reader expects configuration arguments we can use a function
generator. A function generator is indicated by inheriting from class
<code>FunctionGenerator</code> and <code>function</code>. It allows us to process
additional arguments, store them in an environment, return a reader function
with the well-defined signature described above, and still be able to access
the additional arguments via lexical scoping. All corpus constructors in
package <span class="pkg">tm</span> check the reader function for being a function generator and
if so apply it to yield the reader with the expected signature.
</p>


<h3>Value</h3>

<p>For <code>getReaders()</code>, a character vector with readers provided by package
<span class="pkg">tm</span>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+readDOC">readDOC</a></code>, <code><a href="#topic+readPDF">readPDF</a></code>, <code><a href="#topic+readPlain">readPlain</a></code>,
<code><a href="#topic+readRCV1">readRCV1</a></code>, <code><a href="#topic+readRCV1asPlain">readRCV1asPlain</a></code>,
<code><a href="#topic+readReut21578XML">readReut21578XML</a></code>, <code><a href="#topic+readReut21578XMLasPlain">readReut21578XMLasPlain</a></code>,
and <code><a href="#topic+readXML">readXML</a></code>.
</p>

<hr>
<h2 id='readPDF'>Read In a PDF Document</h2><span id='topic+readPDF'></span>

<h3>Description</h3>

<p>Return a function which reads in a portable document format (<abbr><span class="acronym">PDF</span></abbr>)
document extracting both its text and its metadata.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>readPDF(engine = c("pdftools", "xpdf", "Rpoppler",
                   "ghostscript", "Rcampdf", "custom"),
        control = list(info = NULL, text = NULL))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="readPDF_+3A_engine">engine</code></td>
<td>
<p>a character string for the preferred <abbr><span class="acronym">PDF</span></abbr> extraction
engine (see <b>Details</b>).</p>
</td></tr>
<tr><td><code id="readPDF_+3A_control">control</code></td>
<td>
<p>a list of control options for the engine with the named
components <code>info</code> and <code>text</code> (see <b>Details</b>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Formally this function is a function generator, i.e., it returns a function
(which reads in a text document) with a well-defined signature, but can access
passed over arguments (e.g., the preferred <abbr><span class="acronym">PDF</span></abbr> extraction
<code>engine</code> and <code>control</code> options) via lexical scoping.
</p>
<p>Available <abbr><span class="acronym">PDF</span></abbr> extraction engines are as follows.
</p>

<dl>
<dt><code>"pdftools"</code></dt><dd><p>(default) Poppler <abbr><span class="acronym">PDF</span></abbr> rendering library
as provided by the functions <code><a href="pdftools.html#topic+pdf_info">pdf_info</a></code> and
<code><a href="pdftools.html#topic+pdf_text">pdf_text</a></code> in package <span class="pkg">pdftools</span>.</p>
</dd>
<dt><code>"xpdf"</code></dt><dd><p>command line <code>pdfinfo</code> and
<code>pdftotext</code> executables which must be installed and accessible on
your system. Suitable utilities are provided by the Xpdf
(<a href="http://www.xpdfreader.com/">http://www.xpdfreader.com/</a>) <abbr><span class="acronym">PDF</span></abbr> viewer or by the
Poppler (<a href="https://poppler.freedesktop.org/">https://poppler.freedesktop.org/</a>) <abbr><span class="acronym">PDF</span></abbr> rendering
library.</p>
</dd>
<dt><code>"Rpoppler"</code></dt><dd><p>Poppler <abbr><span class="acronym">PDF</span></abbr> rendering library as
provided by the functions <code><a href="Rpoppler.html#topic+PDF_info">PDF_info</a></code> and
<code><a href="Rpoppler.html#topic+PDF_text">PDF_text</a></code> in package <span class="pkg">Rpoppler</span>.</p>
</dd>
<dt><code>"ghostscript"</code></dt><dd><p>Ghostscript using &lsquo;<span class="file">pdf_info.ps</span>&rsquo; and
&lsquo;<span class="file">ps2ascii.ps</span>&rsquo;.</p>
</dd>
<dt><code>"Rcampdf"</code></dt><dd><p>Perl CAM::PDF <abbr><span class="acronym">PDF</span></abbr> manipulation library
as provided by the functions <code>pdf_info</code> and <code>pdf_text</code>
in package <span class="pkg">Rcampdf</span>, available from the repository at
<a href="http://datacube.wu.ac.at">http://datacube.wu.ac.at</a>.</p>
</dd>
<dt><code>"custom"</code></dt><dd><p>custom user-provided extraction engine.</p>
</dd>
</dl>

<p>Control parameters for engine <code>"xpdf"</code> are as follows.
</p>

<dl>
<dt><code>info</code></dt><dd><p>a character vector specifying options passed over to
the <code>pdfinfo</code> executable.</p>
</dd>
<dt><code>text</code></dt><dd><p>a character vector specifying options passed over to
the <code>pdftotext</code> executable.</p>
</dd>
</dl>

<p>Control parameters for engine <code>"custom"</code> are as follows.
</p>

<dl>
<dt><code>info</code></dt><dd><p>a function extracting metadata from a <abbr><span class="acronym">PDF</span></abbr>.
The function must accept a file path as first argument and must return a
named list with the components <code>Author</code> (as character string),
<code>CreationDate</code> (of class <code>POSIXlt</code>), <code>Subject</code> (as
character string), <code>Title</code> (as character string), and <code>Creator</code>
(as character string).</p>
</dd>
<dt><code>text</code></dt><dd><p>a function extracting content from a <abbr><span class="acronym">PDF</span></abbr>.
The function must accept a file path as first argument and must return a
character vector.</p>
</dd>
</dl>



<h3>Value</h3>

<p>A <code>function</code> with the following formals:
</p>

<dl>
<dt><code>elem</code></dt><dd><p>a named list with the component <code>uri</code> which must
hold a valid file name.</p>
</dd>
<dt><code>language</code></dt><dd><p>a string giving the language.</p>
</dd>
<dt><code>id</code></dt><dd><p>Not used.</p>
</dd>
</dl>

<p>The function returns a <code><a href="#topic+PlainTextDocument">PlainTextDocument</a></code> representing the text
and metadata extracted from <code>elem$uri</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Reader">Reader</a></code> for basic information on the reader infrastructure
employed by package <span class="pkg">tm</span>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>uri &lt;- paste0("file://",
              system.file(file.path("doc", "tm.pdf"), package = "tm"))
engine &lt;- if(nzchar(system.file(package = "pdftools"))) {
    "pdftools" 
} else {
    "ghostscript"
}
reader &lt;- readPDF(engine)
pdf &lt;- reader(elem = list(uri = uri), language = "en", id = "id1")
cat(content(pdf)[1])
VCorpus(URISource(uri, mode = ""),
        readerControl = list(reader = readPDF(engine = "ghostscript")))
</code></pre>

<hr>
<h2 id='readPlain'>Read In a Text Document</h2><span id='topic+readPlain'></span>

<h3>Description</h3>

<p>Read in a text document without knowledge about its internal structure and
possible available metadata.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>readPlain(elem, language, id)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="readPlain_+3A_elem">elem</code></td>
<td>
<p>a named list with the component <code>content</code> which must hold
the document to be read in.</p>
</td></tr>
<tr><td><code id="readPlain_+3A_language">language</code></td>
<td>
<p>a string giving the language.</p>
</td></tr>
<tr><td><code id="readPlain_+3A_id">id</code></td>
<td>
<p>a character giving a unique identifier for the created text
document.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code><a href="#topic+PlainTextDocument">PlainTextDocument</a></code> representing <code>elem$content</code>. The
argument <code>id</code> is used as fallback if <code>elem$uri</code> is null.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Reader">Reader</a></code> for basic information on the reader infrastructure
employed by package <span class="pkg">tm</span>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>docs &lt;- c("This is a text.", "This another one.")
vs &lt;- VectorSource(docs)
elem &lt;- getElem(stepNext(vs))
(result &lt;- readPlain(elem, "en", "id1"))
meta(result)
</code></pre>

<hr>
<h2 id='readRCV1'>Read In a Reuters Corpus Volume 1 Document</h2><span id='topic+readRCV1'></span><span id='topic+readRCV1asPlain'></span>

<h3>Description</h3>

<p>Read in a Reuters Corpus Volume 1 <abbr><span class="acronym">XML</span></abbr> document.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>readRCV1(elem, language, id)
readRCV1asPlain(elem, language, id)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="readRCV1_+3A_elem">elem</code></td>
<td>
<p>a named list with the component <code>content</code> which must hold
the document to be read in.</p>
</td></tr>
<tr><td><code id="readRCV1_+3A_language">language</code></td>
<td>
<p>a string giving the language.</p>
</td></tr>
<tr><td><code id="readRCV1_+3A_id">id</code></td>
<td>
<p>Not used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An <code><a href="#topic+XMLTextDocument">XMLTextDocument</a></code> for <code>readRCV1</code>, or a
<code><a href="#topic+PlainTextDocument">PlainTextDocument</a></code> for <code>readRCV1asPlain</code>, representing the
text and metadata extracted from <code>elem$content</code>.
</p>


<h3>References</h3>

<p>Lewis, D. D.; Yang, Y.; Rose, T.; and Li, F (2004).
RCV1: A New Benchmark Collection for Text Categorization Research.
<em>Journal of Machine Learning Research</em>, <b>5</b>, 361&ndash;397.
<a href="https://www.jmlr.org/papers/volume5/lewis04a/lewis04a.pdf">https://www.jmlr.org/papers/volume5/lewis04a/lewis04a.pdf</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Reader">Reader</a></code> for basic information on the reader infrastructure
employed by package <span class="pkg">tm</span>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>f &lt;- system.file("texts", "rcv1_2330.xml", package = "tm")
f_bin &lt;- readBin(f, raw(), file.size(f))
rcv1 &lt;- readRCV1(elem = list(content = f_bin), language = "en", id = "id1")
content(rcv1)
meta(rcv1)
</code></pre>

<hr>
<h2 id='readReut21578XML'>Read In a Reuters-21578 XML Document</h2><span id='topic+readReut21578XML'></span><span id='topic+readReut21578XMLasPlain'></span>

<h3>Description</h3>

<p>Read in a Reuters-21578 <abbr><span class="acronym">XML</span></abbr> document.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>readReut21578XML(elem, language, id)
readReut21578XMLasPlain(elem, language, id)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="readReut21578XML_+3A_elem">elem</code></td>
<td>
<p>a named list with the component <code>content</code> which must hold
the document to be read in.</p>
</td></tr>
<tr><td><code id="readReut21578XML_+3A_language">language</code></td>
<td>
<p>a string giving the language.</p>
</td></tr>
<tr><td><code id="readReut21578XML_+3A_id">id</code></td>
<td>
<p>Not used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An <code><a href="#topic+XMLTextDocument">XMLTextDocument</a></code> for <code>readReut21578XML</code>, or a
<code><a href="#topic+PlainTextDocument">PlainTextDocument</a></code> for <code>readReut21578XMLasPlain</code>,
representing the text and metadata extracted from <code>elem$content</code>.
</p>


<h3>References</h3>







<p>Lewis, David (1997).
<em>Reuters-21578 Text Categorization Collection Distribution</em>.
UCI Machine Learning Repository.
<a href="https://doi.org/10.24432/C52G6M">doi:10.24432/C52G6M</a>.
</p>





<h3>See Also</h3>

<p><code><a href="#topic+Reader">Reader</a></code> for basic information on the reader infrastructure
employed by package <span class="pkg">tm</span>.
</p>

<hr>
<h2 id='readTagged'>Read In a POS-Tagged Word Text Document</h2><span id='topic+readTagged'></span>

<h3>Description</h3>

<p>Return a function which reads in a text document containing POS-tagged words.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>readTagged(...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="readTagged_+3A_...">...</code></td>
<td>
<p>Arguments passed to <code><a href="NLP.html#topic+TaggedTextDocument">TaggedTextDocument</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Formally this function is a function generator, i.e., it returns a
function (which reads in a text document) with a well-defined
signature, but can access passed over arguments (<code>...</code>)
via lexical scoping.
</p>


<h3>Value</h3>

<p>A <code>function</code> with the following formals:
</p>

<dl>
<dt><code>elem</code></dt><dd><p>a named list with the component <code>content</code> which must
hold the document to be read in or the component <code>uri</code> holding a
connection object or a character string.</p>
</dd>
<dt><code>language</code></dt><dd><p>a string giving the language.</p>
</dd>
<dt><code>id</code></dt><dd><p>a character giving a unique identifier for the created
text document.</p>
</dd>
</dl>

<p>The function returns a <code><a href="NLP.html#topic+TaggedTextDocument">TaggedTextDocument</a></code> representing the
text and metadata extracted from <code>elem$content</code> or <code>elem$uri</code>. The
argument <code>id</code> is used as fallback if <code>elem$uri</code> is null.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Reader">Reader</a></code> for basic information on the reader infrastructure
employed by package <span class="pkg">tm</span>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># See http://www.nltk.org/book/ch05.html or file ca01 in the Brown corpus
x &lt;- paste("The/at grand/jj jury/nn commented/vbd on/in a/at number/nn of/in",
           "other/ap topics/nns ,/, among/in them/ppo the/at Atlanta/np and/cc",
           "Fulton/np-tl County/nn-tl purchasing/vbg departments/nns which/wdt",
           "it/pps said/vbd ``/`` are/ber well/ql operated/vbn and/cc follow/vb",
           "generally/rb accepted/vbn practices/nns which/wdt inure/vb to/in the/at",
           "best/jjt interest/nn of/in both/abx governments/nns ''/'' ./.")
vs &lt;- VectorSource(x)
elem &lt;- getElem(stepNext(vs))
(doc &lt;- readTagged()(elem, language = "en", id = "id1"))
tagged_words(doc)
</code></pre>

<hr>
<h2 id='readXML'>Read In an XML Document</h2><span id='topic+readXML'></span>

<h3>Description</h3>

<p>Return a function which reads in an <abbr><span class="acronym">XML</span></abbr> document. The structure of
the <abbr><span class="acronym">XML</span></abbr> document is described with a specification.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>readXML(spec, doc)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="readXML_+3A_spec">spec</code></td>
<td>
<p>A named list of lists each containing two components. The
constructed reader will map each list entry to the content or metadatum of
the text document as specified by the named list entry. Valid names include
<code>content</code> to access the document's content, and character strings which
are mapped to metadata entries.
</p>
<p>Each list entry must consist of two components: the first must be a string
describing the type of the second argument, and the second is the
specification entry. Valid combinations are:
</p>

<dl>
<dt><code>type = "node", spec = "XPathExpression"</code></dt><dd><p>The XPath (1.0)
expression <code>spec</code> extracts information from an <abbr><span class="acronym">XML</span></abbr> node.</p>
</dd>
<dt><code>type = "function", spec = function(doc) ...</code></dt><dd><p>The function
<code>spec</code> is called, passing over the <abbr><span class="acronym">XML</span></abbr> document (as
delivered by <code><a href="xml2.html#topic+read_xml">read_xml</a></code> from package <span class="pkg">xml2</span>) as
first argument.</p>
</dd>
<dt><code>type = "unevaluated", spec = "String"</code></dt><dd><p>The character vector
<code>spec</code> is returned without modification.</p>
</dd>
</dl>

</td></tr>
<tr><td><code id="readXML_+3A_doc">doc</code></td>
<td>
<p>An (empty) document of some subclass of <code>TextDocument</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Formally this function is a function generator, i.e., it returns a
function (which reads in a text document) with a well-defined
signature, but can access passed over arguments (e.g., the specification)
via lexical scoping.
</p>


<h3>Value</h3>

<p>A function with the following formals:
</p>

<dl>
<dt><code>elem</code></dt><dd><p>a named list with the component <code>content</code> which
must hold the document to be read in.</p>
</dd>
<dt><code>language</code></dt><dd><p>a string giving the language.</p>
</dd>
<dt><code>id</code></dt><dd><p>a character giving a unique identifier for the created
text document.</p>
</dd>
</dl>

<p>The function returns <code>doc</code> augmented by the parsed information
as described by <code>spec</code> out of the <abbr><span class="acronym">XML</span></abbr> file in
<code>elem$content</code>. The arguments <code>language</code> and <code>id</code> are used as
fallback: <code>language</code> if no corresponding metadata entry is found in
<code>elem$content</code>, and <code>id</code> if no corresponding metadata entry is found
in <code>elem$content</code> and if <code>elem$uri</code> is null.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Reader">Reader</a></code> for basic information on the reader infrastructure
employed by package <span class="pkg">tm</span>.
</p>
<p>Vignette 'Extensions: How to Handle Custom File Formats', and
<code><a href="#topic+XMLSource">XMLSource</a></code>.
</p>

<hr>
<h2 id='removeNumbers'>Remove Numbers from a Text Document</h2><span id='topic+removeNumbers'></span><span id='topic+removeNumbers.character'></span><span id='topic+removeNumbers.PlainTextDocument'></span>

<h3>Description</h3>

<p>Remove numbers from a text document.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'character'
removeNumbers(x, ucp = FALSE, ...)
## S3 method for class 'PlainTextDocument'
removeNumbers(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="removeNumbers_+3A_x">x</code></td>
<td>
<p>a character vector or text document.</p>
</td></tr>
<tr><td><code id="removeNumbers_+3A_ucp">ucp</code></td>
<td>
<p>a logical specifying whether to use Unicode character
properties for determining digit characters.  If <code>FALSE</code> 
(default), characters in the ASCII <code>[:digit:]</code> class (i.e., the
decimal digits from 0 to 9) are taken; if <code>TRUE</code>, the
characters with Unicode general category <code>Nd</code> (Decimal_Number).</p>
</td></tr>
<tr><td><code id="removeNumbers_+3A_...">...</code></td>
<td>
<p>arguments to be passed to or from methods;
in particular, from the <code>PlainTextDocument</code> method to the
<code>character</code> method.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The text document without numbers.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+getTransformations">getTransformations</a></code> to list available transformation
(mapping) functions.
</p>
<p><a href="https://unicode.org/reports/tr44/#General_Category_Values">https://unicode.org/reports/tr44/#General_Category_Values</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("crude")
crude[[1]]
removeNumbers(crude[[1]])
</code></pre>

<hr>
<h2 id='removePunctuation'>Remove Punctuation Marks from a Text Document</h2><span id='topic+removePunctuation'></span><span id='topic+removePunctuation.character'></span><span id='topic+removePunctuation.PlainTextDocument'></span>

<h3>Description</h3>

<p>Remove punctuation marks from a text document.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'character'
removePunctuation(x,
                  preserve_intra_word_contractions = FALSE,
                  preserve_intra_word_dashes = FALSE,
                  ucp = FALSE, ...)
## S3 method for class 'PlainTextDocument'
removePunctuation(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="removePunctuation_+3A_x">x</code></td>
<td>
<p>a character vector or text document.</p>
</td></tr>
<tr><td><code id="removePunctuation_+3A_preserve_intra_word_contractions">preserve_intra_word_contractions</code></td>
<td>
<p>a logical specifying whether
intra-word contractions should be kept.</p>
</td></tr>
<tr><td><code id="removePunctuation_+3A_preserve_intra_word_dashes">preserve_intra_word_dashes</code></td>
<td>
<p>a logical specifying whether
intra-word dashes should be kept.</p>
</td></tr>
<tr><td><code id="removePunctuation_+3A_ucp">ucp</code></td>
<td>
<p>a logical specifying whether to use Unicode character
properties for determining punctuation characters.  If <code>FALSE</code> 
(default), characters in the ASCII <code>[:punct:]</code> class are taken;
if <code>TRUE</code>, the characters with Unicode general category
<code>P</code> (Punctuation).</p>
</td></tr>
<tr><td><code id="removePunctuation_+3A_...">...</code></td>
<td>
<p>arguments to be passed to or from methods;
in particular, from the <code>PlainTextDocument</code> method to the
<code>character</code> method.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The character or text document <code>x</code> without punctuation marks
(besides intra-word contractions (&lsquo;<span class="samp">&#8288;'&#8288;</span>&rsquo;) and intra-word dashes
(&lsquo;<span class="samp">&#8288;-&#8288;</span>&rsquo;) if <code>preserve_intra_word_contractions</code> and
<code>preserve_intra_word_dashes</code> are set, respectively).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+getTransformations">getTransformations</a></code> to list available transformation
(mapping) functions.
</p>
<p><code><a href="base.html#topic+regex">regex</a></code> shows the class <code>[:punct:]</code> of punctuation
characters.
</p>
<p><a href="https://unicode.org/reports/tr44/#General_Category_Values">https://unicode.org/reports/tr44/#General_Category_Values</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("crude")
inspect(crude[[14]])
inspect(removePunctuation(crude[[14]]))
inspect(removePunctuation(crude[[14]],
                          preserve_intra_word_contractions = TRUE,
                          preserve_intra_word_dashes = TRUE))
</code></pre>

<hr>
<h2 id='removeSparseTerms'>Remove Sparse Terms from a Term-Document Matrix</h2><span id='topic+removeSparseTerms'></span>

<h3>Description</h3>

<p>Remove sparse terms from a document-term or term-document matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>removeSparseTerms(x, sparse)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="removeSparseTerms_+3A_x">x</code></td>
<td>
<p>A <code><a href="#topic+DocumentTermMatrix">DocumentTermMatrix</a></code> or a
<code><a href="#topic+TermDocumentMatrix">TermDocumentMatrix</a></code>.</p>
</td></tr>
<tr><td><code id="removeSparseTerms_+3A_sparse">sparse</code></td>
<td>
<p>A numeric for the maximal allowed sparsity in the range from
bigger zero to smaller one.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A term-document matrix where those terms from <code>x</code> are
removed which have at least a <code>sparse</code> percentage of empty (i.e.,
terms occurring 0 times in a document) elements. I.e., the resulting
matrix contains only terms with a sparse factor of less than
<code>sparse</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("crude")
tdm &lt;- TermDocumentMatrix(crude)
removeSparseTerms(tdm, 0.2)
</code></pre>

<hr>
<h2 id='removeWords'>Remove Words from a Text Document</h2><span id='topic+removeWords'></span><span id='topic+removeWords.character'></span><span id='topic+removeWords.PlainTextDocument'></span>

<h3>Description</h3>

<p>Remove words from a text document.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'character'
removeWords(x, words)
## S3 method for class 'PlainTextDocument'
removeWords(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="removeWords_+3A_x">x</code></td>
<td>
<p>A character or text document.</p>
</td></tr>
<tr><td><code id="removeWords_+3A_words">words</code></td>
<td>
<p>A character vector giving the words to be removed.</p>
</td></tr>
<tr><td><code id="removeWords_+3A_...">...</code></td>
<td>
<p>passed over argument <code>words</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The character or text document without the specified words.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+getTransformations">getTransformations</a></code> to list available transformation (mapping)
functions.
</p>
<p><code><a href="tau.html#topic+remove_stopwords">remove_stopwords</a></code> provided by package <span class="pkg">tau</span>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("crude")
crude[[1]]
removeWords(crude[[1]], stopwords("english"))
</code></pre>

<hr>
<h2 id='SimpleCorpus'>Simple Corpora</h2><span id='topic+SimpleCorpus'></span>

<h3>Description</h3>

<p>Create simple corpora.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SimpleCorpus(x, control = list(language = "en"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SimpleCorpus_+3A_x">x</code></td>
<td>
<p>a <code><a href="#topic+DataframeSource">DataframeSource</a></code>, <code><a href="#topic+DirSource">DirSource</a></code> or
<code><a href="#topic+VectorSource">VectorSource</a></code>.</p>
</td></tr>
<tr><td><code id="SimpleCorpus_+3A_control">control</code></td>
<td>
<p>a named list of control parameters.
</p>

<dl>
<dt><code>language</code></dt><dd><p>a character giving the language (preferably as
<abbr><span class="acronym">IETF</span></abbr> language tags, see <a href="NLP.html#topic+language">language</a> in
package <span class="pkg">NLP</span>).
The default language is assumed to be English (<code>"en"</code>).</p>
</dd>
</dl>

</td></tr>
</table>


<h3>Details</h3>

<p>A <em>simple corpus</em> is fully kept in memory. Compared to a <code>VCorpus</code>,
it is optimized for the most common usage scenario: importing plain texts from
files in a directory or directly from a vector in <span class="rlang"><b>R</b></span>, preprocessing and
transforming the texts, and finally exporting them to a term-document matrix.
It adheres to the <code><a href="#topic+Corpus">Corpus</a></code> <abbr><span class="acronym">API</span></abbr>. However, it takes
internally various shortcuts to boost performance and minimize memory
pressure; consequently it operates only under the following contraints:
</p>

<ul>
<li><p>only <code>DataframeSource</code>, <code>DirSource</code> and <code>VectorSource</code>
are supported,
</p>
</li>
<li><p>no custom readers, i.e., each document is read in and stored as plain
text (as a string, i.e., a character vector of length one),
</p>
</li>
<li><p>transformations applied via <code><a href="#topic+tm_map">tm_map</a></code> must be able to
process character vectors and return character vectors (of the same
length),
</p>
</li>
<li><p>no lazy transformations in <code><a href="#topic+tm_map">tm_map</a></code>,
</p>
</li>
<li><p>no meta data for individual documents (i.e., no <code>"local"</code> in
<code><a href="#topic+meta">meta</a></code>).
</p>
</li></ul>



<h3>Value</h3>

<p>An object inheriting from <code>SimpleCorpus</code> and <code>Corpus</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Corpus">Corpus</a></code> for basic information on the corpus infrastructure
employed by package <span class="pkg">tm</span>.
</p>
<p><code><a href="#topic+VCorpus">VCorpus</a></code> provides an implementation with volatile storage
semantics, and <code><a href="#topic+PCorpus">PCorpus</a></code> provides an implementation with
permanent storage semantics.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>txt &lt;- system.file("texts", "txt", package = "tm")
(ovid &lt;- SimpleCorpus(DirSource(txt, encoding = "UTF-8"),
                      control = list(language = "lat")))
</code></pre>

<hr>
<h2 id='Source'>Sources</h2><span id='topic+Source'></span><span id='topic+SimpleSource'></span><span id='topic+close.SimpleSource'></span><span id='topic+eoi'></span><span id='topic+eoi.SimpleSource'></span><span id='topic+getMeta'></span><span id='topic+getMeta.DataframeSource'></span><span id='topic+getElem'></span><span id='topic+getElem.DataframeSource'></span><span id='topic+getElem.DirSource'></span><span id='topic+getElem.URISource'></span><span id='topic+getElem.VectorSource'></span><span id='topic+getElem.XMLSource'></span><span id='topic+getSources'></span><span id='topic+length.SimpleSource'></span><span id='topic+open.SimpleSource'></span><span id='topic+pGetElem'></span><span id='topic+pGetElem.DataframeSource'></span><span id='topic+pGetElem.DirSource'></span><span id='topic+pGetElem.URISource'></span><span id='topic+pGetElem.VectorSource'></span><span id='topic+reader'></span><span id='topic+reader.SimpleSource'></span><span id='topic+Source'></span><span id='topic+stepNext'></span><span id='topic+stepNext.SimpleSource'></span>

<h3>Description</h3>

<p>Creating and accessing sources.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SimpleSource(encoding = "",
             length = 0,
             position = 0,
             reader = readPlain,
             ...,
             class)
getSources()
## S3 method for class 'SimpleSource'
close(con, ...)
## S3 method for class 'SimpleSource'
eoi(x)
## S3 method for class 'DataframeSource'
getMeta(x)
## S3 method for class 'DataframeSource'
getElem(x)
## S3 method for class 'DirSource'
getElem(x)
## S3 method for class 'URISource'
getElem(x)
## S3 method for class 'VectorSource'
getElem(x)
## S3 method for class 'XMLSource'
getElem(x)
## S3 method for class 'SimpleSource'
length(x)
## S3 method for class 'SimpleSource'
open(con, ...)
## S3 method for class 'DataframeSource'
pGetElem(x)
## S3 method for class 'DirSource'
pGetElem(x)
## S3 method for class 'URISource'
pGetElem(x)
## S3 method for class 'VectorSource'
pGetElem(x)
## S3 method for class 'SimpleSource'
reader(x)
## S3 method for class 'SimpleSource'
stepNext(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Source_+3A_x">x</code></td>
<td>
<p>A <code>Source</code>.</p>
</td></tr>
<tr><td><code id="Source_+3A_con">con</code></td>
<td>
<p>A <code>Source</code>.</p>
</td></tr>
<tr><td><code id="Source_+3A_encoding">encoding</code></td>
<td>
<p>a character giving the encoding of the elements delivered by
the source.</p>
</td></tr>
<tr><td><code id="Source_+3A_length">length</code></td>
<td>
<p>a non-negative integer denoting the number of elements delivered
by the source. If the length is unknown in advance set it to <code>0</code>.</p>
</td></tr>
<tr><td><code id="Source_+3A_position">position</code></td>
<td>
<p>a numeric indicating the current position in the source.</p>
</td></tr>
<tr><td><code id="Source_+3A_reader">reader</code></td>
<td>
<p>a reader function (generator).</p>
</td></tr>
<tr><td><code id="Source_+3A_...">...</code></td>
<td>
<p>For <code>SimpleSource</code> tag-value pairs for storing additional
information; not used otherwise.</p>
</td></tr>
<tr><td><code id="Source_+3A_class">class</code></td>
<td>
<p>a character vector giving additional classes to be used for the
created source.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><em>Sources</em> abstract input locations, like a directory, a connection, or
simply an <span class="rlang"><b>R</b></span> vector, in order to acquire content in a uniform way. In packages
which employ the infrastructure provided by package <span class="pkg">tm</span>, such sources are
represented via the virtual S3 class <code>Source</code>: such packages then provide
S3 source classes extending the virtual base class (such as
<code><a href="#topic+DirSource">DirSource</a></code> provided by package <span class="pkg">tm</span> itself).
</p>
<p>All extension classes must provide implementations for the functions
<code>close</code>, <code>eoi</code>, <code>getElem</code>, <code>length</code>, <code>open</code>,
<code>reader</code>, and <code>stepNext</code>. For parallel element access the
(optional) function <code>pGetElem</code> must be provided as well. If
document level metadata is available, the (optional) function <code>getMeta</code>
must be implemented.
</p>
<p>The functions <code>open</code> and <code>close</code> open and close the source,
respectively. <code>eoi</code> indicates end of input. <code>getElem</code> fetches the
element at the current position, whereas <code>pGetElem</code> retrieves all
elements in parallel at once. The function <code>length</code> gives the number of
elements. <code>reader</code> returns a default reader for processing elements.
<code>stepNext</code> increases the position in the source to acquire the next
element.
</p>
<p>The function <code>SimpleSource</code> provides a simple reference implementation
and can be used when creating custom sources.
</p>


<h3>Value</h3>

<p>For <code>SimpleSource</code>, an object inheriting from <code>class</code>,
<code>SimpleSource</code>, and <code>Source</code>.
</p>
<p>For <code>getSources</code>, a character vector with sources provided by package
<span class="pkg">tm</span>.
</p>
<p><code>open</code> and <code>close</code> return the opened and closed source,
respectively.
</p>
<p>For <code>eoi</code>, a logical indicating if the end of input of the source is
reached.
</p>
<p>For <code>getElem</code> a named list with the components <code>content</code> holding the
document and <code>uri</code> giving a uniform resource identifier (e.g., a file
path or <abbr><span class="acronym">URL</span></abbr>; <code>NULL</code> if not applicable or unavailable). For
<code>pGetElem</code> a list of such named lists.
</p>
<p>For <code>length</code>, an integer for the number of elements.
</p>
<p>For <code>reader</code>, a function for the default reader.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+DataframeSource">DataframeSource</a></code>, <code><a href="#topic+DirSource">DirSource</a></code>,
<code><a href="#topic+URISource">URISource</a></code>, <code><a href="#topic+VectorSource">VectorSource</a></code>, and
<code><a href="#topic+XMLSource">XMLSource</a></code>.
</p>

<hr>
<h2 id='stemCompletion'>Complete Stems</h2><span id='topic+stemCompletion'></span>

<h3>Description</h3>

<p>Heuristically complete stemmed words.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stemCompletion(x,
               dictionary,
               type = c("prevalent", "first", "longest",
                        "none", "random", "shortest"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="stemCompletion_+3A_x">x</code></td>
<td>
<p>A character vector of stems to be completed.</p>
</td></tr>
<tr><td><code id="stemCompletion_+3A_dictionary">dictionary</code></td>
<td>
<p>A <code><a href="#topic+Corpus">Corpus</a></code> or character vector to be searched
for possible completions.</p>
</td></tr>
<tr><td><code id="stemCompletion_+3A_type">type</code></td>
<td>
<p>A <code>character</code> naming the heuristics to be used:
</p>

<dl>
<dt><code>prevalent</code></dt><dd><p>Default. Takes the most frequent match as
completion.</p>
</dd>
<dt><code>first</code></dt><dd><p>Takes the first found completion.</p>
</dd>
<dt><code>longest</code></dt><dd><p>Takes the longest completion in terms of
characters.</p>
</dd>
<dt><code>none</code></dt><dd><p>Is the identity.</p>
</dd>
<dt><code>random</code></dt><dd><p>Takes some completion.</p>
</dd>
<dt><code>shortest</code></dt><dd><p>Takes the shortest completion in terms of
characters.</p>
</dd>
</dl>

</td></tr>
</table>


<h3>Value</h3>

<p>A character vector with completed words.
</p>


<h3>References</h3>

<p>Ingo Feinerer (2010).
Analysis and Algorithms for Stemming Inversion.
<em>Information Retrieval Technology &mdash; 6th Asia Information Retrieval Societies Conference, AIRS 2010, Taipei, Taiwan, December 1&ndash;3, 2010. Proceedings</em>, volume 6458 of <em>Lecture Notes in Computer Science</em>, pages 290&ndash;299. Springer-Verlag, December 2010.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("crude")
stemCompletion(c("compan", "entit", "suppl"), crude)
</code></pre>

<hr>
<h2 id='stemDocument'>Stem Words</h2><span id='topic+stemDocument'></span><span id='topic+stemDocument.character'></span><span id='topic+stemDocument.PlainTextDocument'></span>

<h3>Description</h3>

<p>Stem words in a text document using Porter's stemming algorithm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'character'
stemDocument(x, language = "english")
## S3 method for class 'PlainTextDocument'
stemDocument(x, language = meta(x, "language"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="stemDocument_+3A_x">x</code></td>
<td>
<p>A character vector or text document.</p>
</td></tr>
<tr><td><code id="stemDocument_+3A_language">language</code></td>
<td>
<p>A string giving the language for stemming.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The argument <code>language</code> is passed over to
<code><a href="SnowballC.html#topic+wordStem">wordStem</a></code> as the name of the Snowball stemmer.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("crude")
inspect(crude[[1]])
inspect(stemDocument(crude[[1]]))
</code></pre>

<hr>
<h2 id='stopwords'>Stopwords</h2><span id='topic+stopwords'></span>

<h3>Description</h3>

<p>Return various kinds of stopwords with support for different
languages.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stopwords(kind = "en")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="stopwords_+3A_kind">kind</code></td>
<td>
<p>A character string identifying the desired stopword list.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Available stopword lists are:
</p>

<dl>
<dt><code>catalan</code></dt><dd><p>Catalan stopwords (obtained from
<a href="http://latel.upf.edu/morgana/altres/pub/ca_stop.htm">http://latel.upf.edu/morgana/altres/pub/ca_stop.htm</a>),</p>
</dd>
<dt><code>romanian</code></dt><dd><p>Romanian stopwords (extracted from
<a href="http://snowball.tartarus.org/otherapps/romanian/romanian1.tgz">http://snowball.tartarus.org/otherapps/romanian/romanian1.tgz</a>),</p>
</dd>
<dt><code>SMART</code></dt><dd><p>English stopwords from the SMART information
retrieval system (as documented in Appendix 11 of
<a href="https://jmlr.csail.mit.edu/papers/volume5/lewis04a/">https://jmlr.csail.mit.edu/papers/volume5/lewis04a/</a>)
(which coincides with the stopword list used by the MC toolkit
(<a href="https://www.cs.utexas.edu/users/dml/software/mc/">https://www.cs.utexas.edu/users/dml/software/mc/</a>)),</p>
</dd>
</dl>

<p>and a set of stopword lists from the Snowball stemmer project in different
languages (obtained from
&lsquo;<span class="samp">&#8288;http://svn.tartarus.org/snowball/trunk/website/algorithms/*/stop.txt&#8288;</span>&rsquo;).
Supported languages are <code>danish</code>, <code>dutch</code>, <code>english</code>,
<code>finnish</code>, <code>french</code>, <code>german</code>, <code>hungarian</code>, <code>italian</code>,
<code>norwegian</code>, <code>portuguese</code>, <code>russian</code>, <code>spanish</code>, and
<code>swedish</code>. Language names are case sensitive. Alternatively, their
<abbr><span class="acronym">IETF</span></abbr> language tags may be used.




</p>


<h3>Value</h3>

<p>A character vector containing the requested stopwords. An error
is raised if no stopwords are available for the requested
<code>kind</code>.</p>


<h3>Examples</h3>

<pre><code class='language-R'>stopwords("en")
stopwords("SMART")
stopwords("german")
</code></pre>

<hr>
<h2 id='stripWhitespace'>Strip Whitespace from a Text Document</h2><span id='topic+stripWhitespace'></span><span id='topic+stripWhitespace.PlainTextDocument'></span>

<h3>Description</h3>

<p>Strip extra whitespace from a text document. Multiple whitespace
characters are collapsed to a single blank.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'PlainTextDocument'
stripWhitespace(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="stripWhitespace_+3A_x">x</code></td>
<td>
<p>A text document.</p>
</td></tr>
<tr><td><code id="stripWhitespace_+3A_...">...</code></td>
<td>
<p>Not used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The text document with multiple whitespace characters collapsed to a
single blank.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+getTransformations">getTransformations</a></code> to list available transformation (mapping)
functions.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("crude")
crude[[1]]
stripWhitespace(crude[[1]])
</code></pre>

<hr>
<h2 id='TermDocumentMatrix'>Term-Document Matrix</h2><span id='topic+TermDocumentMatrix'></span><span id='topic+DocumentTermMatrix'></span><span id='topic+as.TermDocumentMatrix'></span><span id='topic+as.DocumentTermMatrix'></span>

<h3>Description</h3>

<p>Constructs or coerces to a term-document matrix or a document-term matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>TermDocumentMatrix(x, control = list())
DocumentTermMatrix(x, control = list())
as.TermDocumentMatrix(x, ...)
as.DocumentTermMatrix(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="TermDocumentMatrix_+3A_x">x</code></td>
<td>
<p>for the constructors, a corpus or an <span class="rlang"><b>R</b></span> object from which a
corpus can be generated via <code>Corpus(VectorSource(x))</code>; for the
coercing functions, either a term-document matrix or a document-term
matrix or a <a href="slam.html#topic+matrix">simple triplet matrix</a> (package
<span class="pkg">slam</span>) or a <a href="#topic+termFreq">term frequency vector</a>.</p>
</td></tr> 
<tr><td><code id="TermDocumentMatrix_+3A_control">control</code></td>
<td>
<p>a named list of control options. There are local
options which are evaluated for each document and global options
which are evaluated once for the constructed matrix. Available local
options are documented in <code><a href="#topic+termFreq">termFreq</a></code> and are internally
delegated to a <code><a href="#topic+termFreq">termFreq</a></code> call.
</p>
<p>This is different for a <code><a href="#topic+SimpleCorpus">SimpleCorpus</a></code>. In this case all
options are processed in a fixed order in one pass to improve performance.
It always uses the Boost (<a href="https://www.boost.org">https://www.boost.org</a>) Tokenizer (via
<span class="pkg">Rcpp</span>) and takes no custom functions as option arguments.
</p>
<p>Available global options are:
</p>

<dl>
<dt><code>bounds</code></dt><dd><p>A list with a tag <code>global</code> whose value
must be an integer vector of length 2. Terms that appear in less
documents than the lower bound <code>bounds$global[1]</code> or in
more documents than the upper bound <code>bounds$global[2]</code> are
discarded. Defaults to <code>list(global = c(1, Inf))</code> (i.e., every
term will be used).</p>
</dd>
<dt><code>weighting</code></dt><dd><p>A weighting function capable of handling a
<code>TermDocumentMatrix</code>. It defaults to <code>weightTf</code> for term
frequency weighting. Available weighting functions shipped with
the <span class="pkg">tm</span> package are <code><a href="#topic+weightTf">weightTf</a></code>,
<code><a href="#topic+weightTfIdf">weightTfIdf</a></code>, <code><a href="#topic+weightBin">weightBin</a></code>, and
<code><a href="#topic+weightSMART">weightSMART</a></code>.</p>
</dd>
</dl>
</td></tr>
<tr><td><code id="TermDocumentMatrix_+3A_...">...</code></td>
<td>
<p>the additional argument <code>weighting</code> (typically a
<code><a href="#topic+WeightFunction">WeightFunction</a></code>) is allowed when coercing a
simple triplet matrix to a term-document or document-term matrix.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>TermDocumentMatrix</code> or class
<code>DocumentTermMatrix</code> (both inheriting from a
<a href="slam.html#topic+matrix">simple triplet matrix</a> in package <span class="pkg">slam</span>)
containing a sparse term-document matrix or document-term matrix. The
attribute <code>weighting</code> contains the weighting applied to the
matrix.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+termFreq">termFreq</a></code> for available local control options.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("crude")
tdm &lt;- TermDocumentMatrix(crude,
                          control = list(removePunctuation = TRUE,
                                         stopwords = TRUE))
dtm &lt;- DocumentTermMatrix(crude,
                          control = list(weighting =
                                         function(x)
                                         weightTfIdf(x, normalize =
                                                     FALSE),
                                         stopwords = TRUE))
inspect(tdm[202:205, 1:5])
inspect(tdm[c("price", "prices", "texas"), c("127", "144", "191", "194")])
inspect(dtm[1:5, 273:276])

s &lt;- SimpleCorpus(VectorSource(unlist(lapply(crude, as.character))))
m &lt;- TermDocumentMatrix(s,
                        control = list(removeNumbers = TRUE,
                                       stopwords = TRUE,
                                       stemming = TRUE))
inspect(m[c("price", "texa"), c("127", "144", "191", "194")])
</code></pre>

<hr>
<h2 id='termFreq'>Term Frequency Vector</h2><span id='topic+termFreq'></span>

<h3>Description</h3>

<p>Generate a term frequency vector from a text document.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>termFreq(doc, control = list())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="termFreq_+3A_doc">doc</code></td>
<td>
<p>An object inheriting from <code><a href="#topic+TextDocument">TextDocument</a></code> or a
character vector.</p>
</td></tr>
<tr><td><code id="termFreq_+3A_control">control</code></td>
<td>
<p>A list of control options which override default
settings.
</p>
<p>First, following two options are processed.
</p>

<dl>
<dt><code>tokenize</code></dt><dd><p>A function tokenizing a <code><a href="#topic+TextDocument">TextDocument</a></code>
into single tokens, a <code><a href="NLP.html#topic+Span_Tokenizer">Span_Tokenizer</a></code>,
<code><a href="NLP.html#topic+Token_Tokenizer">Token_Tokenizer</a></code>, or a string matching one of the
predefined tokenization functions:
</p>

<dl>
<dt><code>"Boost"</code></dt><dd><p>for <code><a href="#topic+Boost_tokenizer">Boost_tokenizer</a></code>, or</p>
</dd>
<dt><code>"MC"</code></dt><dd><p>for <code><a href="#topic+MC_tokenizer">MC_tokenizer</a></code>, or</p>
</dd>
<dt><code>"scan"</code></dt><dd><p>for <code><a href="#topic+scan_tokenizer">scan_tokenizer</a></code>, or</p>
</dd>
<dt><code>"words"</code></dt><dd><p>for <code><a href="english.html#topic+words">words</a></code>.</p>
</dd>
</dl>

<p>Defaults to <code><a href="english.html#topic+words">words</a></code>.</p>
</dd>
<dt><code>tolower</code></dt><dd><p>Either a logical value indicating whether
characters should be translated to lower case or a custom function
converting characters to lower case. Defaults to
<code><a href="base.html#topic+tolower">tolower</a></code>.</p>
</dd>
</dl>

<p>Next, a set of options which are sensitive to the order of
occurrence in the <code>control</code> list. Options are processed in the
same order as specified. User-specified options have precedence over
the default ordering so that first all user-specified options and
then all remaining options (with the default settings and in the
order as listed below) are processed.
</p>

<dl>
<dt><code>language</code></dt><dd><p>A character giving the language (preferably as
<abbr><span class="acronym">IETF</span></abbr> language tags, see <a href="NLP.html#topic+language">language</a> in package
<span class="pkg">NLP</span>) to be used for <code>stopwords</code> and <code>stemming</code> if
not provided by <code>doc</code>.</p>
</dd>
<dt><code>removePunctuation</code></dt><dd><p>A logical value indicating whether
punctuation characters should be removed from
<code>doc</code>, a custom function which performs punctuation
removal, or a list of arguments for
<code><a href="#topic+removePunctuation">removePunctuation</a></code>. Defaults to <code>FALSE</code>.</p>
</dd>
<dt><code>removeNumbers</code></dt><dd><p>A logical value indicating whether
numbers should be removed from <code>doc</code> or a custom function
for number removal. Defaults to <code>FALSE</code>.</p>
</dd>
<dt><code>stopwords</code></dt><dd><p>Either a Boolean value indicating stopword
removal using default language specific stopword lists shipped
with this package, a character vector holding custom
stopwords, or a custom function for stopword removal. Defaults
to <code>FALSE</code>.</p>
</dd>
<dt><code>stemming</code></dt><dd><p>Either a Boolean value indicating whether tokens
should be stemmed or a custom stemming function. Defaults to
<code>FALSE</code>.</p>
</dd>
</dl>

<p>Finally, following options are processed in the given order.
</p>

<dl>
<dt><code>dictionary</code></dt><dd><p>A character vector to be tabulated
against. No other terms will be listed in the result. Defaults
to <code>NULL</code> which means that all terms in <code>doc</code> are
listed.</p>
</dd>
<dt><code>bounds</code></dt><dd><p>A list with a tag <code>local</code> whose value
must be an integer vector of length 2. Terms that appear less
often in <code>doc</code> than the lower bound <code>bounds$local[1]</code>
or more often than the upper bound <code>bounds$local[2]</code> are
discarded. Defaults to <code>list(local = c(1, Inf))</code> (i.e., every
token will be used).</p>
</dd>
<dt><code>wordLengths</code></dt><dd><p>An integer vector of length 2. Words
shorter than the minimum word length <code>wordLengths[1]</code> or
longer than the maximum word length <code>wordLengths[2]</code> are
discarded. Defaults to <code>c(3, Inf)</code>, i.e., a minimum word
length of 3 characters.</p>
</dd>
</dl>

</td></tr>
</table>


<h3>Value</h3>

<p>A table of class <code>c("term_frequency", "integer")</code> with term frequencies
as values and tokens as names.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+getTokenizers">getTokenizers</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("crude")
termFreq(crude[[14]])
strsplit_space_tokenizer &lt;- function(x)
    unlist(strsplit(as.character(x), "[[:space:]]+"))
ctrl &lt;- list(tokenize = strsplit_space_tokenizer,
             removePunctuation = list(preserve_intra_word_dashes = TRUE),
             stopwords = c("reuter", "that"),
             stemming = TRUE,
             wordLengths = c(4, Inf))
termFreq(crude[[14]], control = ctrl)
</code></pre>

<hr>
<h2 id='TextDocument'>Text Documents</h2><span id='topic+TextDocument'></span>

<h3>Description</h3>

<p>Representing and computing on text documents.
</p>


<h3>Details</h3>

<p><em>Text documents</em> are documents containing (natural language) text. The
<span class="pkg">tm</span> package employs the infrastructure provided by package <span class="pkg">NLP</span> and
represents text documents via the virtual S3 class <code>TextDocument</code>.
Actual S3 text document classes then extend the virtual base class (such as
<code><a href="#topic+PlainTextDocument">PlainTextDocument</a></code>).
</p>
<p>All extension classes must provide an <code><a href="base.html#topic+as.character">as.character</a></code>
method which extracts the natural language text in documents of the
respective classes in a &ldquo;suitable&rdquo; (not necessarily structured)
form, as well as <code><a href="Biobase.html#topic+content">content</a></code> and <code><a href="#topic+meta">meta</a></code> methods
for accessing the (possibly raw) document content and metadata.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+PlainTextDocument">PlainTextDocument</a></code>, and
<code><a href="#topic+XMLTextDocument">XMLTextDocument</a></code>
for the text document classes provided by package <span class="pkg">tm</span>.
</p>
<p><code><a href="NLP.html#topic+TextDocument">TextDocument</a></code> for text documents in package <span class="pkg">NLP</span>.
</p>

<hr>
<h2 id='tm_combine'>Combine Corpora, Documents, Term-Document Matrices, and Term Frequency Vectors</h2><span id='topic+c.VCorpus'></span><span id='topic+c.TextDocument'></span><span id='topic+c.TermDocumentMatrix'></span><span id='topic+c.term_frequency'></span>

<h3>Description</h3>

<p>Combine several corpora into a single one, combine multiple
documents into a corpus, combine multiple term-document matrices
into a single one, or combine multiple term frequency vectors into a
single term-document matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'VCorpus'
c(..., recursive = FALSE)
## S3 method for class 'TextDocument'
c(..., recursive = FALSE)
## S3 method for class 'TermDocumentMatrix'
c(..., recursive = FALSE)
## S3 method for class 'term_frequency'
c(..., recursive = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tm_combine_+3A_...">...</code></td>
<td>
<p>Corpora, text documents, term-document matrices, or term
frequency vectors.</p>
</td></tr>
<tr><td><code id="tm_combine_+3A_recursive">recursive</code></td>
<td>
<p>Not used.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+VCorpus">VCorpus</a></code>, <code><a href="#topic+TextDocument">TextDocument</a></code>,
<code><a href="#topic+TermDocumentMatrix">TermDocumentMatrix</a></code>, and <code><a href="#topic+termFreq">termFreq</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("acq")
data("crude")
meta(acq, "comment", type = "corpus") &lt;- "Acquisitions"
meta(crude, "comment", type = "corpus") &lt;- "Crude oil"
meta(acq, "acqLabels") &lt;- 1:50
meta(acq, "jointLabels") &lt;- 1:50
meta(crude, "crudeLabels") &lt;- letters[1:20]
meta(crude, "jointLabels") &lt;- 1:20
c(acq, crude)
meta(c(acq, crude), type = "corpus")
meta(c(acq, crude))
c(acq[[30]], crude[[10]])
c(TermDocumentMatrix(acq), TermDocumentMatrix(crude))
</code></pre>

<hr>
<h2 id='tm_filter'>Filter and Index Functions on Corpora</h2><span id='topic+tm_filter'></span><span id='topic+tm_filter.PCorpus'></span><span id='topic+tm_filter.SimpleCorpus'></span><span id='topic+tm_filter.VCorpus'></span><span id='topic+tm_index'></span><span id='topic+tm_index.PCorpus'></span><span id='topic+tm_index.SimpleCorpus'></span><span id='topic+tm_index.VCorpus'></span>

<h3>Description</h3>

<p>Interface to apply filter and index functions to corpora.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'PCorpus'
tm_filter(x, FUN, ...)
## S3 method for class 'SimpleCorpus'
tm_filter(x, FUN, ...)
## S3 method for class 'VCorpus'
tm_filter(x, FUN, ...)
## S3 method for class 'PCorpus'
tm_index(x, FUN, ...)
## S3 method for class 'SimpleCorpus'
tm_index(x, FUN, ...)
## S3 method for class 'VCorpus'
tm_index(x, FUN, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tm_filter_+3A_x">x</code></td>
<td>
<p>A corpus.</p>
</td></tr>
<tr><td><code id="tm_filter_+3A_fun">FUN</code></td>
<td>
<p>a filter function taking a text document or a string (if
<code>x</code> is a <code>SimpleCorpus</code>) as input and returning the
logical value <code>TRUE</code> or <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="tm_filter_+3A_...">...</code></td>
<td>
<p>arguments to <code>FUN</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>tm_filter</code> returns a corpus containing documents where
<code>FUN</code> matches, whereas <code>tm_index</code> only returns the
corresponding indices.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("crude")
# Full-text search
tm_filter(crude, FUN = function(x) any(grep("co[m]?pany", content(x))))
</code></pre>

<hr>
<h2 id='tm_map'>Transformations on Corpora</h2><span id='topic+tm_map'></span><span id='topic+tm_map.VCorpus'></span><span id='topic+tm_map.SimpleCorpus'></span><span id='topic+tm_map.PCorpus'></span>

<h3>Description</h3>

<p>Interface to apply transformation functions (also denoted as mappings)
to corpora.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'PCorpus'
tm_map(x, FUN, ...)
## S3 method for class 'SimpleCorpus'
tm_map(x, FUN, ...)
## S3 method for class 'VCorpus'
tm_map(x, FUN, ..., lazy = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tm_map_+3A_x">x</code></td>
<td>
<p>A corpus.</p>
</td></tr>
<tr><td><code id="tm_map_+3A_fun">FUN</code></td>
<td>
<p>a transformation function taking a text document (a character
vector when <code>x</code> is a <code>SimpleCorpus</code>) as input and returning a text
document (a character vector of the same length as the input vector for
<code>SimpleCorpus</code>). The function <code><a href="#topic+content_transformer">content_transformer</a></code> can be
used to create a wrapper to get and set the content of text documents.</p>
</td></tr>
<tr><td><code id="tm_map_+3A_...">...</code></td>
<td>
<p>arguments to <code>FUN</code>.</p>
</td></tr>
<tr><td><code id="tm_map_+3A_lazy">lazy</code></td>
<td>
<p>a logical. Lazy mappings are mappings which are delayed
until the content is accessed. It is useful for large corpora if only few
documents will be accessed. In such a case it avoids the computationally
expensive application of the mapping to all elements in the corpus.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A corpus with <code>FUN</code> applied to each document in <code>x</code>. In case
of lazy mappings only internal flags are set. Access of individual documents
triggers the execution of the corresponding transformation function.
</p>


<h3>Note</h3>

<p>Lazy transformations change <span class="rlang"><b>R</b></span>'s standard evaluation semantics.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+getTransformations">getTransformations</a></code> for available transformations.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("crude")
## Document access triggers the stemming function
## (i.e., all other documents are not stemmed yet)
tm_map(crude, stemDocument, lazy = TRUE)[[1]]
## Use wrapper to apply character processing function
tm_map(crude, content_transformer(tolower))
## Generate a custom transformation function which takes the heading as new content
headings &lt;- function(x)
    PlainTextDocument(meta(x, "heading"),
                      id = meta(x, "id"),
                      language = meta(x, "language"))
inspect(tm_map(crude, headings))
</code></pre>

<hr>
<h2 id='tm_reduce'>Combine Transformations</h2><span id='topic+tm_reduce'></span>

<h3>Description</h3>

<p>Fold multiple transformations (mappings) into a single one.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tm_reduce(x, tmFuns, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tm_reduce_+3A_x">x</code></td>
<td>
<p>A corpus.</p>
</td></tr>
<tr><td><code id="tm_reduce_+3A_tmfuns">tmFuns</code></td>
<td>
<p>A list of <span class="pkg">tm</span> transformations.</p>
</td></tr>
<tr><td><code id="tm_reduce_+3A_...">...</code></td>
<td>
<p>Arguments to the individual transformations.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A single <span class="pkg">tm</span> transformation function obtained by folding <code>tmFuns</code>
from right to left (via <code>Reduce(..., right = TRUE)</code>).
</p>


<h3>See Also</h3>

<p><code>Reduce</code> for <span class="rlang"><b>R</b></span>'s internal folding/accumulation mechanism, and
<code><a href="#topic+getTransformations">getTransformations</a></code> to list available transformation
(mapping) functions.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(crude)
crude[[1]]
skipWords &lt;- function(x) removeWords(x, c("it", "the"))
funs &lt;- list(stripWhitespace,
             skipWords,
             removePunctuation,
             content_transformer(tolower))
tm_map(crude, FUN = tm_reduce, tmFuns = funs)[[1]]
</code></pre>

<hr>
<h2 id='tm_term_score'>Compute Score for Matching Terms</h2><span id='topic+tm_term_score'></span><span id='topic+tm_term_score.DocumentTermMatrix'></span><span id='topic+tm_term_score.term_frequency'></span><span id='topic+tm_term_score.PlainTextDocument'></span><span id='topic+tm_term_score.TermDocumentMatrix'></span>

<h3>Description</h3>

<p>Compute a score based on the number of matching terms.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'DocumentTermMatrix'
tm_term_score(x, terms, FUN = row_sums)
## S3 method for class 'PlainTextDocument'
tm_term_score(x, terms, FUN = function(x) sum(x, na.rm = TRUE))
## S3 method for class 'term_frequency'
tm_term_score(x, terms, FUN = function(x) sum(x, na.rm = TRUE))
## S3 method for class 'TermDocumentMatrix'
tm_term_score(x, terms, FUN = col_sums)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tm_term_score_+3A_x">x</code></td>
<td>
<p>Either a <code><a href="#topic+PlainTextDocument">PlainTextDocument</a></code>, a term frequency as
returned by <code><a href="#topic+termFreq">termFreq</a></code>, or a
<code><a href="#topic+TermDocumentMatrix">TermDocumentMatrix</a></code>.</p>
</td></tr>
<tr><td><code id="tm_term_score_+3A_terms">terms</code></td>
<td>
<p>A character vector of terms to be matched.</p>
</td></tr>
<tr><td><code id="tm_term_score_+3A_fun">FUN</code></td>
<td>
<p>A function computing a score from the number of terms
matching in <code>x</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A score as computed by <code>FUN</code> from the number of matching
<code>terms</code> in <code>x</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("acq")
tm_term_score(acq[[1]], c("company", "change"))
## Not run: ## Test for positive and negative sentiments
## install.packages("tm.lexicon.GeneralInquirer", repos="http://datacube.wu.ac.at", type="source")
require("tm.lexicon.GeneralInquirer")
sapply(acq[1:10], tm_term_score, terms_in_General_Inquirer_categories("Positiv"))
sapply(acq[1:10], tm_term_score, terms_in_General_Inquirer_categories("Negativ"))
tm_term_score(TermDocumentMatrix(acq[1:10],
                                control = list(removePunctuation = TRUE)),
             terms_in_General_Inquirer_categories("Positiv"))
## End(Not run)
</code></pre>

<hr>
<h2 id='tokenizer'>Tokenizers</h2><span id='topic+Boost_tokenizer'></span><span id='topic+MC_tokenizer'></span><span id='topic+scan_tokenizer'></span>

<h3>Description</h3>

<p>Tokenize a document or character vector.</p>


<h3>Usage</h3>

<pre><code class='language-R'>Boost_tokenizer(x)
MC_tokenizer(x)
scan_tokenizer(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tokenizer_+3A_x">x</code></td>
<td>
<p>A character vector, or an object that can be coerced to character by
<code>as.character</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The quality and correctness of a tokenization algorithm highly depends
on the context and application scenario. Relevant factors are the
language of the underlying text and the notions of whitespace (which
can vary with the used encoding and the language) and punctuation
marks. Consequently, for superior results you probably need a custom
tokenization function.
</p>

<dl>
<dt>Boost_tokenizer</dt><dd><p>Uses the Boost (<a href="https://www.boost.org">https://www.boost.org</a>)
Tokenizer (via <span class="pkg">Rcpp</span>).</p>
</dd>
<dt>MC_tokenizer</dt><dd><p>Implements the functionality of the tokenizer in the
MC toolkit (<a href="https://www.cs.utexas.edu/users/dml/software/mc/">https://www.cs.utexas.edu/users/dml/software/mc/</a>).</p>
</dd>
<dt>scan_tokenizer</dt><dd><p>Simulates <code>scan(..., what = "character")</code>.</p>
</dd>
</dl>



<h3>Value</h3>

<p>A character vector consisting of tokens obtained by tokenization of <code>x</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+getTokenizers">getTokenizers</a></code> to list tokenizers provided by package <span class="pkg">tm</span>.
</p>
<p><code><a href="NLP.html#topic+Regexp_Tokenizer">Regexp_Tokenizer</a></code> for tokenizers using regular expressions
provided by package <span class="pkg">NLP</span>.
</p>
<p><code><a href="tau.html#topic+tokenize">tokenize</a></code> for a simple regular expression based tokenizer
provided by package <span class="pkg">tau</span>.
</p>
<p><code><a href="tokenizers.html#topic+tokenizers">tokenizers</a></code> for a collection of tokenizers provided
by package <span class="pkg">tokenizers</span>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("crude")
Boost_tokenizer(crude[[1]])
MC_tokenizer(crude[[1]])
scan_tokenizer(crude[[1]])
strsplit_space_tokenizer &lt;- function(x)
    unlist(strsplit(as.character(x), "[[:space:]]+"))
strsplit_space_tokenizer(crude[[1]])
</code></pre>

<hr>
<h2 id='URISource'>Uniform Resource Identifier Source</h2><span id='topic+URISource'></span>

<h3>Description</h3>

<p>Create a uniform resource identifier source.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>URISource(x, encoding = "", mode = "text")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="URISource_+3A_x">x</code></td>
<td>
<p>A character vector of uniform resource identifiers (<abbr><span class="acronym">URI</span></abbr>s.</p>
</td></tr>
<tr><td><code id="URISource_+3A_encoding">encoding</code></td>
<td>
<p>A character string describing the current encoding. It is
passed to <code><a href="base.html#topic+iconv">iconv</a></code> to convert the input to UTF-8.</p>
</td></tr>
<tr><td><code id="URISource_+3A_mode">mode</code></td>
<td>
<p>a character string specifying if and how <abbr><span class="acronym">URI</span></abbr>s should be
read in. Available modes are:
</p>

<dl>
<dt><code>""</code></dt><dd><p>No read. In this case <code><a href="#topic+getElem">getElem</a></code> and
<code><a href="#topic+pGetElem">pGetElem</a></code> only deliver <abbr><span class="acronym">URI</span></abbr>s.</p>
</dd>
<dt><code>"binary"</code></dt><dd><p><abbr><span class="acronym">URI</span></abbr>s are read in binary raw mode (via
<code><a href="base.html#topic+readBin">readBin</a></code>).</p>
</dd>
<dt><code>"text"</code></dt><dd><p><abbr><span class="acronym">URI</span></abbr>s are read as text (via
<code><a href="base.html#topic+readLines">readLines</a></code>).</p>
</dd>
</dl>

</td></tr>
</table>


<h3>Details</h3>

<p>A <em>uniform resource identifier source</em> interprets each <abbr><span class="acronym">URI</span></abbr> as a
document.
</p>


<h3>Value</h3>

<p>An object inheriting from <code>URISource</code>, <code><a href="#topic+SimpleSource">SimpleSource</a></code>,
and <code><a href="#topic+Source">Source</a></code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Source">Source</a></code> for basic information on the source infrastructure
employed by package <span class="pkg">tm</span>.
</p>
<p><code><a href="base.html#topic+Encoding">Encoding</a></code> and <code><a href="base.html#topic+iconv">iconv</a></code> on encodings.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>loremipsum &lt;- system.file("texts", "loremipsum.txt", package = "tm")
ovid &lt;- system.file("texts", "txt", "ovid_1.txt", package = "tm")
us &lt;- URISource(sprintf("file://%s", c(loremipsum, ovid)))
inspect(VCorpus(us))
</code></pre>

<hr>
<h2 id='VCorpus'>Volatile Corpora</h2><span id='topic+VCorpus'></span><span id='topic+as.VCorpus'></span>

<h3>Description</h3>

<p>Create volatile corpora.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>VCorpus(x, readerControl = list(reader = reader(x), language = "en"))
as.VCorpus(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="VCorpus_+3A_x">x</code></td>
<td>
<p>For <code>VCorpus</code> a <code><a href="#topic+Source">Source</a></code> object, and for
<code>as.VCorpus</code> an <span class="rlang"><b>R</b></span> object.</p>
</td></tr>
<tr><td><code id="VCorpus_+3A_readercontrol">readerControl</code></td>
<td>
<p>a named list of control parameters for reading in content
from <code>x</code>.
</p>

<dl>
<dt><code>reader</code></dt><dd><p>a function capable of reading in and processing the
format delivered by <code>x</code>.</p>
</dd>
<dt><code>language</code></dt><dd><p>a character giving the language (preferably as
<abbr><span class="acronym">IETF</span></abbr> language tags, see <a href="NLP.html#topic+language">language</a> in
package <span class="pkg">NLP</span>).
The default language is assumed to be English (<code>"en"</code>).</p>
</dd>
</dl>

</td></tr>
</table>


<h3>Details</h3>

<p>A <em>volatile corpus</em> is fully kept in memory and thus all changes only
affect the corresponding <span class="rlang"><b>R</b></span> object.
</p>


<h3>Value</h3>

<p>An object inheriting from <code>VCorpus</code> and <code>Corpus</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Corpus">Corpus</a></code> for basic information on the corpus infrastructure
employed by package <span class="pkg">tm</span>.
</p>
<p><code><a href="#topic+PCorpus">PCorpus</a></code> provides an implementation with permanent storage
semantics.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>reut21578 &lt;- system.file("texts", "crude", package = "tm")
VCorpus(DirSource(reut21578, mode = "binary"),
        list(reader = readReut21578XMLasPlain))
</code></pre>

<hr>
<h2 id='VectorSource'>Vector Source</h2><span id='topic+VectorSource'></span>

<h3>Description</h3>

<p>Create a vector source.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>VectorSource(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="VectorSource_+3A_x">x</code></td>
<td>
<p>A vector giving the texts.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A <em>vector source</em> interprets each element of the vector <code>x</code> as a
document.
</p>


<h3>Value</h3>

<p>An object inheriting from <code>VectorSource</code>, <code><a href="#topic+SimpleSource">SimpleSource</a></code>,
and <code><a href="#topic+Source">Source</a></code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Source">Source</a></code> for basic information on the source infrastructure
employed by package <span class="pkg">tm</span>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>docs &lt;- c("This is a text.", "This another one.")
(vs &lt;- VectorSource(docs))
inspect(VCorpus(vs))
</code></pre>

<hr>
<h2 id='weightBin'>Weight Binary</h2><span id='topic+weightBin'></span>

<h3>Description</h3>

<p>Binary weight a term-document matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>weightBin(m)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="weightBin_+3A_m">m</code></td>
<td>
<p>A <code><a href="#topic+TermDocumentMatrix">TermDocumentMatrix</a></code> in term frequency format.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Formally this function is of class <code>WeightingFunction</code> with the
additional attributes <code>name</code> and <code>acronym</code>.
</p>


<h3>Value</h3>

<p>The weighted matrix.
</p>

<hr>
<h2 id='WeightFunction'>Weighting Function</h2><span id='topic+WeightFunction'></span>

<h3>Description</h3>

<p>Construct a weighting function for term-document matrices.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>WeightFunction(x, name, acronym)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="WeightFunction_+3A_x">x</code></td>
<td>
<p>A function which takes a <code><a href="#topic+TermDocumentMatrix">TermDocumentMatrix</a></code>
with term frequencies as input, weights the elements, and returns
the weighted matrix.</p>
</td></tr>
<tr><td><code id="WeightFunction_+3A_name">name</code></td>
<td>
<p>A character naming the weighting function.</p>
</td></tr>
<tr><td><code id="WeightFunction_+3A_acronym">acronym</code></td>
<td>
<p>A character giving an acronym for the name of the
weighting function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>WeightFunction</code> which extends the class
<code>function</code> representing a weighting function.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>weightCutBin &lt;- WeightFunction(function(m, cutoff) m &gt; cutoff,
                               "binary with cutoff", "bincut")
</code></pre>

<hr>
<h2 id='weightSMART'>SMART Weightings</h2><span id='topic+weightSMART'></span>

<h3>Description</h3>

<p>Weight a term-document matrix according to a combination of weights
specified in SMART notation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>weightSMART(m, spec = "nnn", control = list())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="weightSMART_+3A_m">m</code></td>
<td>
<p>A <code><a href="#topic+TermDocumentMatrix">TermDocumentMatrix</a></code> in term frequency format.</p>
</td></tr>
<tr><td><code id="weightSMART_+3A_spec">spec</code></td>
<td>
<p>a character string consisting of three characters. The first letter
specifies a term frequency schema, the second a document frequency
schema, and the third a normalization schema. See <b>Details</b> for
available built-in schemata.</p>
</td></tr>
<tr><td><code id="weightSMART_+3A_control">control</code></td>
<td>
<p>a list of control parameters. See <b>Details</b>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Formally this function is of class <code>WeightingFunction</code> with the
additional attributes <code>name</code> and <code>acronym</code>.
</p>
<p>The first letter of <code>spec</code> specifies a weighting schema for term
frequencies of <code>m</code>:
</p>

<dl>
<dt>&quot;n&quot;</dt><dd><p>(natural) <code class="reqn">\mathit{tf}_{i,j}</code> counts the number of occurrences
<code class="reqn">n_{i,j}</code> of a term <code class="reqn">t_i</code> in a document <code class="reqn">d_j</code>. The
input term-document matrix <code>m</code> is assumed to be in this
standard term frequency format already.</p>
</dd>
<dt>&quot;l&quot;</dt><dd><p>(logarithm) is defined as <code class="reqn">1 + \log_2(\mathit{tf}_{i,j})</code>.</p>
</dd>
<dt>&quot;a&quot;</dt><dd><p>(augmented) is defined as <code class="reqn">0.5 +
      \frac{0.5 * \mathit{tf}_{i,j}}{\max_i(\mathit{tf}_{i,j})}</code>.</p>
</dd>
<dt>&quot;b&quot;</dt><dd><p>(boolean) is defined as 1 if <code class="reqn">\mathit{tf}_{i,j} &gt; 0</code> and 0 otherwise.</p>
</dd>
<dt>&quot;L&quot;</dt><dd><p>(log average) is defined as <code class="reqn">\frac{1 +
      \log_2(\mathit{tf}_{i,j})}{1+\log_2(\mathrm{ave}_{i\in j}(\mathit{tf}_{i,j}))}</code>.</p>
</dd>
</dl>

<p>The second letter of <code>spec</code> specifies a weighting schema of
document frequencies for <code>m</code>:
</p>

<dl>
<dt>&quot;n&quot;</dt><dd><p>(no) is defined as 1.</p>
</dd>
<dt>&quot;t&quot;</dt><dd><p>(idf) is defined as <code class="reqn">\log_2 \frac{N}{\mathit{df}_t}</code> where
<code class="reqn">\mathit{df}_t</code> denotes how often term <code class="reqn">t</code> occurs in all
documents.</p>
</dd>
<dt>&quot;p&quot;</dt><dd><p>(prob idf) is defined as <code class="reqn">\max(0, \log_2(\frac{N - \mathit{df}_t}{\mathit{df}_t}))</code>.</p>
</dd>
</dl>

<p>The third letter of <code>spec</code> specifies a schema for normalization
of <code>m</code>:
</p>

<dl>
<dt>&quot;n&quot;</dt><dd><p>(none) is defined as 1.</p>
</dd>
<dt>&quot;c&quot;</dt><dd><p>(cosine) is defined as <code class="reqn">\sqrt{\mathrm{col\_sums}(m ^ 2)}</code>.</p>
</dd>
<dt>&quot;u&quot;</dt><dd><p>(pivoted unique) is defined as <code class="reqn">\mathit{slope} *
      \sqrt{\mathrm{col\_sums}(m ^ 2)} + (1 - \mathit{slope}) *
      \mathit{pivot}</code> where both <code>slope</code> and <code>pivot</code> must be set
via named tags in the <code>control</code> list.</p>
</dd>
<dt>&quot;b&quot;</dt><dd><p>(byte size) is defined as
<code class="reqn">\frac{1}{\mathit{CharLength}^\alpha}</code>. The parameter
<code class="reqn">\alpha</code> must be set via the named tag <code>alpha</code>
in the <code>control</code> list.</p>
</dd>
</dl>

<p>The final result is defined by multiplication of the chosen term
frequency component with the chosen document frequency component with
the chosen normalization component.
</p>


<h3>Value</h3>

<p>The weighted matrix.
</p>


<h3>References</h3>

<p>Christopher D. Manning and Prabhakar Raghavan and Hinrich Schtze (2008).
<em>Introduction to Information Retrieval</em>.
Cambridge University Press, ISBN 0521865719.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("crude")
TermDocumentMatrix(crude,
                   control = list(removePunctuation = TRUE,
                                  stopwords = TRUE,
                                  weighting = function(x)
                                  weightSMART(x, spec = "ntc")))
</code></pre>

<hr>
<h2 id='weightTf'>Weight by Term Frequency</h2><span id='topic+weightTf'></span>

<h3>Description</h3>

<p>Weight a term-document matrix by term frequency.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>weightTf(m)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="weightTf_+3A_m">m</code></td>
<td>
<p>A <code><a href="#topic+TermDocumentMatrix">TermDocumentMatrix</a></code> in term frequency format.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Formally this function is of class <code>WeightingFunction</code> with the
additional attributes <code>name</code> and <code>acronym</code>.
</p>
<p>This function acts as the identity function since the input matrix is
already in term frequency format.
</p>


<h3>Value</h3>

<p>The weighted matrix.
</p>

<hr>
<h2 id='weightTfIdf'>Weight by Term Frequency - Inverse Document Frequency</h2><span id='topic+weightTfIdf'></span>

<h3>Description</h3>

<p>Weight a term-document matrix by term frequency - inverse document
frequency.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>weightTfIdf(m, normalize = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="weightTfIdf_+3A_m">m</code></td>
<td>
<p>A <code><a href="#topic+TermDocumentMatrix">TermDocumentMatrix</a></code> in term frequency format.</p>
</td></tr>
<tr><td><code id="weightTfIdf_+3A_normalize">normalize</code></td>
<td>
<p>A Boolean value indicating whether the term
frequencies should be normalized.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Formally this function is of class <code>WeightingFunction</code> with the
additional attributes <code>name</code> and <code>acronym</code>.
</p>
<p><em>Term frequency</em> <code class="reqn">\mathit{tf}_{i,j}</code> counts the number of
occurrences <code class="reqn">n_{i,j}</code> of a term <code class="reqn">t_i</code> in a document
<code class="reqn">d_j</code>. In the case of normalization, the term frequency
<code class="reqn">\mathit{tf}_{i,j}</code> is divided by <code class="reqn">\sum_k n_{k,j}</code>.
</p>
<p><em>Inverse document frequency</em> for a term <code class="reqn">t_i</code> is defined as
</p>
<p style="text-align: center;"><code class="reqn">\mathit{idf}_i = \log_2 \frac{|D|}{|\{d \mid t_i \in d\}|}</code>
</p>
<p> where
<code class="reqn">|D|</code> denotes the total number of documents and where <code class="reqn">|\{d
  \mid t_i \in d\}|</code> is the number of documents where the term <code class="reqn">t_i</code>
appears.
</p>
<p><em>Term frequency - inverse document frequency</em> is now defined as
<code class="reqn">\mathit{tf}_{i,j} \cdot \mathit{idf}_i</code>.
</p>


<h3>Value</h3>

<p>The weighted matrix.
</p>


<h3>References</h3>

<p>Gerard Salton and Christopher Buckley (1988).
Term-weighting approaches in automatic text retrieval.
<em>Information Processing and Management</em>, <b>24</b>/5, 513&ndash;523.
</p>

<hr>
<h2 id='writeCorpus'>Write a Corpus to Disk</h2><span id='topic+writeCorpus'></span>

<h3>Description</h3>

<p>Write a plain text representation of a corpus to multiple files on
disk corresponding to the individual documents in the corpus.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>writeCorpus(x, path = ".", filenames = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="writeCorpus_+3A_x">x</code></td>
<td>
<p>A corpus.</p>
</td></tr>
<tr><td><code id="writeCorpus_+3A_path">path</code></td>
<td>
<p>A character listing the directory to be written into.</p>
</td></tr>
<tr><td><code id="writeCorpus_+3A_filenames">filenames</code></td>
<td>
<p>Either <code>NULL</code> or a character vector. In case no
filenames are provided, filenames are automatically generated by
using the documents' identifiers in <code>x</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The plain text representation of the corpus is obtained by calling
<code>as.character</code> on each document.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("crude")
## Not run: writeCorpus(crude, path = ".",
            filenames = paste(seq_along(crude), ".txt", sep = ""))
## End(Not run)
</code></pre>

<hr>
<h2 id='XMLSource'>XML Source</h2><span id='topic+XMLSource'></span>

<h3>Description</h3>

<p>Create an <abbr><span class="acronym">XML</span></abbr> source.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>XMLSource(x, parser = xml_contents, reader)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="XMLSource_+3A_x">x</code></td>
<td>
<p>a character giving a uniform resource identifier.</p>
</td></tr>
<tr><td><code id="XMLSource_+3A_parser">parser</code></td>
<td>
<p>a function accepting an <abbr><span class="acronym">XML</span></abbr> document (as delivered by
<code><a href="xml2.html#topic+read_xml">read_xml</a></code> in package <span class="pkg">xml2</span>) as input and returning
<abbr><span class="acronym">XML</span></abbr> elements/nodes.</p>
</td></tr>
<tr><td><code id="XMLSource_+3A_reader">reader</code></td>
<td>
<p>a function capable of turning <abbr><span class="acronym">XML</span></abbr> elements/nodes as
returned by <code>parser</code> into a subclass of <code><a href="#topic+TextDocument">TextDocument</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object inheriting from <code>XMLSource</code>, <code><a href="#topic+SimpleSource">SimpleSource</a></code>,
and <code><a href="#topic+Source">Source</a></code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Source">Source</a></code> for basic information on the source infrastructure
employed by package <span class="pkg">tm</span>.
</p>
<p>Vignette 'Extensions: How to Handle Custom File Formats', and
<code><a href="#topic+readXML">readXML</a></code>.
</p>

<hr>
<h2 id='XMLTextDocument'>XML Text Documents</h2><span id='topic+XMLTextDocument'></span>

<h3>Description</h3>

<p>Create <abbr><span class="acronym">XML</span></abbr> text documents.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>XMLTextDocument(x = xml_missing(),
                author = character(0),
                datetimestamp = as.POSIXlt(Sys.time(), tz = "GMT"),
                description = character(0),
                heading = character(0),
                id = character(0),
                language = character(0),
                origin = character(0),
                ...,
                meta = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="XMLTextDocument_+3A_x">x</code></td>
<td>
<p>An <code><a href="xml2.html#topic+read_xml">XMLDocument</a></code>.</p>
</td></tr>
<tr><td><code id="XMLTextDocument_+3A_author">author</code></td>
<td>
<p>a character or an object of class <code><a href="utils.html#topic+person">person</a></code> giving
the author names.</p>
</td></tr>
<tr><td><code id="XMLTextDocument_+3A_datetimestamp">datetimestamp</code></td>
<td>
<p>an object of class <code><a href="base.html#topic+POSIXt">POSIXt</a></code> or a character
string giving the creation date/time information.  If a character string,
exactly one of the <abbr><span class="acronym">ISO</span></abbr> 8601 formats defined by
<a href="https://www.w3.org/TR/NOTE-datetime">https://www.w3.org/TR/NOTE-datetime</a> should be used.
See <code><a href="NLP.html#topic+parse_ISO_8601_datetime">parse_ISO_8601_datetime</a></code> in package <span class="pkg">NLP</span>
for processing such date/time information.
</p>
</td></tr>
<tr><td><code id="XMLTextDocument_+3A_description">description</code></td>
<td>
<p>a character giving a description.</p>
</td></tr>
<tr><td><code id="XMLTextDocument_+3A_heading">heading</code></td>
<td>
<p>a character giving the title or a short heading.</p>
</td></tr>
<tr><td><code id="XMLTextDocument_+3A_id">id</code></td>
<td>
<p>a character giving a unique identifier.</p>
</td></tr>
<tr><td><code id="XMLTextDocument_+3A_language">language</code></td>
<td>
<p>a character giving the language (preferably as <abbr><span class="acronym">IETF</span></abbr>
language tags, see <a href="NLP.html#topic+language">language</a> in package <span class="pkg">NLP</span>).</p>
</td></tr>
<tr><td><code id="XMLTextDocument_+3A_origin">origin</code></td>
<td>
<p>a character giving information on the source and origin.</p>
</td></tr>
<tr><td><code id="XMLTextDocument_+3A_...">...</code></td>
<td>
<p>user-defined document metadata tag-value pairs.</p>
</td></tr>
<tr><td><code id="XMLTextDocument_+3A_meta">meta</code></td>
<td>
<p>a named list or <code>NULL</code> (default) giving all metadata. If set
all other metadata arguments are ignored.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object inheriting from <code>XMLTextDocument</code> and
<code><a href="#topic+TextDocument">TextDocument</a></code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+TextDocument">TextDocument</a></code> for basic information on the text document
infrastructure employed by package <span class="pkg">tm</span>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>xml &lt;- system.file("extdata", "order-doc.xml", package = "xml2")
(xtd &lt;- XMLTextDocument(xml2::read_xml(xml),
                        heading = "XML text document",
                        id = xml,
                        language = "en"))
content(xtd)
meta(xtd)
</code></pre>

<hr>
<h2 id='Zipf_n_Heaps'>Explore Corpus Term Frequency Characteristics</h2><span id='topic+Zipf_plot'></span><span id='topic+Heaps_plot'></span>

<h3>Description</h3>

<p>Explore Zipf's law and Heaps' law, two empirical laws in linguistics
describing commonly observed characteristics of term frequency
distributions in corpora.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Zipf_plot(x, type = "l", ...)
Heaps_plot(x, type = "l", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Zipf_n_Heaps_+3A_x">x</code></td>
<td>
<p>a document-term matrix or term-document matrix with
unweighted term frequencies.</p>
</td></tr>
<tr><td><code id="Zipf_n_Heaps_+3A_type">type</code></td>
<td>
<p>a character string indicating the type of plot to be
drawn, see <code><a href="graphics.html#topic+plot">plot</a></code>.</p>
</td></tr>
<tr><td><code id="Zipf_n_Heaps_+3A_...">...</code></td>
<td>
<p>further graphical parameters to be used for plotting.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Zipf's law (e.g., <a href="https://en.wikipedia.org/wiki/Zipf%27s_law">https://en.wikipedia.org/wiki/Zipf%27s_law</a>)
states that given some corpus of natural language utterances, the
frequency of any word is inversely proportional to its rank in the
frequency table, or, more generally, that the pmf of the term
frequencies is of the form <code class="reqn">c k^{-\beta}</code>, where <code class="reqn">k</code> is the
rank of the term (taken from the most to the least frequent one).
We can conveniently explore the degree to which the law holds by
plotting the logarithm of the frequency against the logarithm of the
rank, and inspecting the goodness of fit of a linear model.
</p>
<p>Heaps' law (e.g., <a href="https://en.wikipedia.org/wiki/Heaps%27_law">https://en.wikipedia.org/wiki/Heaps%27_law</a>)
states that the vocabulary size <code class="reqn">V</code> (i.e., the number of different
terms employed) grows polynomially with the text size <code class="reqn">T</code> (the
total number of terms in the texts), so that <code class="reqn">V = c T^\beta</code>.
We can conveniently explore the degree to which the law holds by
plotting <code class="reqn">\log(V)</code> against <code class="reqn">\log(T)</code>, and inspecting the
goodness of fit of a linear model.
</p>


<h3>Value</h3>

<p>The coefficients of the fitted linear model.  As a side effect, the
corresponding plot is produced.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("acq")
m &lt;- DocumentTermMatrix(acq)
Zipf_plot(m)
Heaps_plot(m)
</code></pre>

<hr>
<h2 id='ZipSource'>ZIP File Source</h2><span id='topic+ZipSource'></span>

<h3>Description</h3>

<p>Create a ZIP file source.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ZipSource(zipfile,
          pattern = NULL,
          recursive = FALSE,
          ignore.case = FALSE,
          mode = "text")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ZipSource_+3A_zipfile">zipfile</code></td>
<td>
<p>A character string with the full path name of a ZIP file.</p>
</td></tr>
<tr><td><code id="ZipSource_+3A_pattern">pattern</code></td>
<td>
<p>an optional regular expression. Only file names in the ZIP
file which match the regular expression will be returned.</p>
</td></tr>
<tr><td><code id="ZipSource_+3A_recursive">recursive</code></td>
<td>
<p>logical. Should the listing recurse into directories?</p>
</td></tr>
<tr><td><code id="ZipSource_+3A_ignore.case">ignore.case</code></td>
<td>
<p>logical. Should pattern-matching be case-insensitive?</p>
</td></tr>
<tr><td><code id="ZipSource_+3A_mode">mode</code></td>
<td>
<p>a character string specifying if and how files should be read in.
Available modes are:
</p>

<dl>
<dt><code>""</code></dt><dd><p>No read. In this case <code><a href="#topic+getElem">getElem</a></code> and
<code><a href="#topic+pGetElem">pGetElem</a></code> only deliver <abbr><span class="acronym">URI</span></abbr>s.</p>
</dd>
<dt><code>"binary"</code></dt><dd><p>Files are read in binary raw mode (via
<code><a href="base.html#topic+readBin">readBin</a></code>).</p>
</dd>
<dt><code>"text"</code></dt><dd><p>Files are read as text (via
<code><a href="base.html#topic+readLines">readLines</a></code>).</p>
</dd>
</dl>

</td></tr>
</table>


<h3>Details</h3>

<p>A <em>ZIP file source</em> extracts a compressed ZIP file via
<code><a href="utils.html#topic+unzip">unzip</a></code> and interprets each file as a document.
</p>


<h3>Value</h3>

<p>An object inheriting from <code>ZipSource</code>, <code><a href="#topic+SimpleSource">SimpleSource</a></code>, and
<code><a href="#topic+Source">Source</a></code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Source">Source</a></code> for basic information on the source infrastructure
employed by package <span class="pkg">tm</span>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>zipfile &lt;- tempfile()
files &lt;- Sys.glob(file.path(system.file("texts", "txt", package = "tm"), "*"))
zip(zipfile, files)
zipfile &lt;- paste0(zipfile, ".zip")
Corpus(ZipSource(zipfile, recursive = TRUE))[[1]]
file.remove(zipfile)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
