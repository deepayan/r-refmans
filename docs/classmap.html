<!DOCTYPE html><html><head><title>Help for package classmap</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {classmap}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#classmap'>
<p>Draw the class map to visualize classification results.</p></a></li>
<li><a href='#confmat.vcr'>
<p>Build a confusion matrix from the output of a function <code>vcr.*.*</code>.</p></a></li>
<li><a href='#data_bookReviews'>
<p>Amazon book reviews data</p></a></li>
<li><a href='#data_floralbuds'>
<p>Floral buds data</p></a></li>
<li><a href='#data_instagram'>
<p>Instagram data</p></a></li>
<li><a href='#data_titanic'>
<p>Titanic data</p></a></li>
<li><a href='#makeFV'>
<p>Constructs feature vectors from a kernel matrix.</p></a></li>
<li><a href='#makeKernel'>
<p>Compute kernel matrix</p></a></li>
<li><a href='#qresplot'>
<p>Draw a quasi residual plot of PAC versus a data feature</p></a></li>
<li><a href='#silplot'>
<p>Draw the silhouette plot of a classification</p></a></li>
<li><a href='#stackedplot'>
<p>Make a vertically stacked mosaic plot of class predictions.</p></a></li>
<li><a href='#vcr.da.newdata'>
<p>Carry out discriminant analysis on new data, and prepare to visualize its results.</p></a></li>
<li><a href='#vcr.da.train'>
<p>Carry out discriminant analysis on training data, and prepare to visualize its results.</p></a></li>
<li><a href='#vcr.forest.newdata'>
<p>Prepare for visualization of a random forest classification on new data.</p></a></li>
<li><a href='#vcr.forest.train'>
<p>Prepare for visualization of a random forest classification on training data</p></a></li>
<li><a href='#vcr.knn.newdata'>
<p>Carry out a k-nearest neighbor classification on new data, and prepare to visualize its results.</p></a></li>
<li><a href='#vcr.knn.train'>
<p>Carry out a k-nearest neighbor classification on training data, and prepare to visualize its results.</p></a></li>
<li><a href='#vcr.neural.newdata'>
<p>Prepare for visualization of a neural network classification on new data.</p></a></li>
<li><a href='#vcr.neural.train'>
<p>Prepare for visualization of a neural network classification on training data.</p></a></li>
<li><a href='#vcr.rpart.newdata'>
<p>Prepare for visualization of an rpart classification on new data.</p></a></li>
<li><a href='#vcr.rpart.train'>
<p>Prepare for visualization of an rpart classification on training data.</p></a></li>
<li><a href='#vcr.svm.newdata'>
<p>Prepare for visualization of a support vector machine classification on new data.</p></a></li>
<li><a href='#vcr.svm.train'>
<p>Prepare for visualization of a support vector machine classification on training data.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Visualizing Classification Results</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-04-23</td>
</tr>
<tr>
<td>Version:</td>
<td>1.2.3</td>
</tr>
<tr>
<td>Author:</td>
<td>Jakob Raymaekers [aut, cre],
  Peter Rousseeuw [aut]</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.1.0)</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, reshape2, svd, rpart.plot, nnet, robCompositions</td>
</tr>
<tr>
<td>Imports:</td>
<td>stats, graphics, ggplot2, robustbase, e1071, cellWise,
cluster, kernlab, gridExtra, rpart, randomForest</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Jakob Raymaekers &lt;jakob.raymaekers@kuleuven.be&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Tools to visualize the results of a classification of cases.
    The graphical displays include stacked plots, silhouette plots, quasi residual plots, and class maps.
    Implements the techniques described and illustrated in Raymaekers, Rousseeuw and Hubert (2021), Class maps for visualizing classification results, Technometrics, appeared online.
    &lt;<a href="https://doi.org/10.1080%2F00401706.2021.1927849">doi:10.1080/00401706.2021.1927849</a>&gt; (open access) and Raymaekers and Rousseeuw (2021),
    Silhouettes and quasi residual plots for neural nets and tree-based classifiers,
    &lt;<a href="https://doi.org/10.48550/arXiv.2106.08814">doi:10.48550/arXiv.2106.08814</a>&gt;. Examples can be found in the vignettes:
    "Discriminant_analysis_examples","K_nearest_neighbors_examples",
    "Support_vector_machine_examples", "Rpart_examples", "Random_forest_examples",
    and "Neural_net_examples".</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://doi.org/10.1080/00401706.2021.1927849">https://doi.org/10.1080/00401706.2021.1927849</a>,
<a href="https://arxiv.org/abs/2106.08814">https://arxiv.org/abs/2106.08814</a></td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.2</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-04-23 14:21:04 UTC; u0105404</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-04-23 15:30:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='classmap'>
Draw the class map to visualize classification results.
</h2><span id='topic+classmap'></span>

<h3>Description</h3>

<p>Draw the class map to visualize classification results, based on the output of one of the
<code>vcr.*.*</code> functions in this package. The vertical axis of the class map shows each case's <code>PAC</code>, the conditional probability that it belongs to an alternative class. The <code>farness</code> on the horizontal axis is the probability of a member of the given class being at most as far from the class as the case itself.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>classmap(vcrout, whichclass, classLabels = NULL, classCols = NULL,
         main = NULL, cutoff = 0.99, plotcutoff = TRUE,
         identify = FALSE, cex = 1, cex.main = 1.2, cex.lab = NULL,
         cex.axis = NULL, opacity = 1,
         squareplot = TRUE,  maxprob = NULL, maxfactor = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="classmap_+3A_vcrout">vcrout</code></td>
<td>
<p>output of <code>vcr.*.train</code> or <code>vcr.*.newdata</code>. Required.</p>
</td></tr>
<tr><td><code id="classmap_+3A_whichclass">whichclass</code></td>
<td>
<p>the number or level of the class to be displayed. Required.</p>
</td></tr>
<tr><td><code id="classmap_+3A_classlabels">classLabels</code></td>
<td>
<p>the labels (levels) of the classes. If <code>NULL</code>, they are taken from <code>vcrout</code>.</p>
</td></tr>
<tr><td><code id="classmap_+3A_classcols">classCols</code></td>
<td>
<p>a list of colors for the class labels. There should be at least as many as there are levels. If <code>NULL</code> the <code>classCols</code> are taken as 2, 3, 4, ...</p>
</td></tr>
<tr><td><code id="classmap_+3A_main">main</code></td>
<td>
<p>title for the plot.</p>
</td></tr>
<tr><td><code id="classmap_+3A_cutoff">cutoff</code></td>
<td>
<p>cases with overall farness <code>vcrout$ofarness</code> &gt; <code>cutoff</code> are flagged as outliers.</p>
</td></tr>
<tr><td><code id="classmap_+3A_plotcutoff">plotcutoff</code></td>
<td>
<p>If true, plots the cutoff on the farness values as a vertical line.</p>
</td></tr>
<tr><td><code id="classmap_+3A_identify">identify</code></td>
<td>
<p>if <code>TRUE</code>, left-click on a point to get its number, then ESC to exit.</p>
</td></tr>
<tr><td><code id="classmap_+3A_cex">cex</code></td>
<td>
<p>passed on to <code><a href="graphics.html#topic+plot">graphics::plot</a></code>.</p>
</td></tr>
<tr><td><code id="classmap_+3A_cex.main">cex.main</code></td>
<td>
<p>same, for title.</p>
</td></tr>
<tr><td><code id="classmap_+3A_cex.lab">cex.lab</code></td>
<td>
<p>same, for labels on horizontal and vertical axes.</p>
</td></tr>
<tr><td><code id="classmap_+3A_cex.axis">cex.axis</code></td>
<td>
<p>same, for axes.</p>
</td></tr>
<tr><td><code id="classmap_+3A_opacity">opacity</code></td>
<td>
<p>determines opacity of plotted dots. Value between 0 and 1, where 0 is transparent and 1 is opaque.</p>
</td></tr>
<tr><td><code id="classmap_+3A_squareplot">squareplot</code></td>
<td>
<p>If <code>TRUE</code>, makes the axes of the plot equally long.</p>
</td></tr>
<tr><td><code id="classmap_+3A_maxprob">maxprob</code></td>
<td>
<p>draws the farness axis at least upto probability maxprob. If <code>NULL</code>, the limits are obtained automatically.</p>
</td></tr>
<tr><td><code id="classmap_+3A_maxfactor">maxfactor</code></td>
<td>
<p>if not <code>NULL</code>, a number slightly higher than 1 to increase the space at the right hand side of the plot, to make room for marking points.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Executing the function plots the class map and returns
</p>
<table>
<tr><td><code>coordinates</code></td>
<td>
<p> a matrix with 2 columns containing the coordinates of the plotted points. The first coordinate is the quantile of the farness probability. This makes it easier to add text next to interesting points. If <code>identify = T</code>, the attribute <code>ids</code> of <code>coordinates</code> contains the row numbers of the identified points in the matrix <code>coordinates</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Raymaekers J., Rousseeuw P.J.
</p>


<h3>References</h3>

<p>Raymaekers J., Rousseeuw P.J., Hubert M. (2021). Class maps for visualizing classification results. <em>Technometrics</em>, appeared online. doi: <a href="https://doi.org/10.1080/00401706.2021.1927849">10.1080/00401706.2021.1927849</a>(link to open access pdf)
</p>
<p>Raymaekers J., Rousseeuw P.J.(2021). Silhouettes and quasi residual plots for neural nets and tree-based classifiers. <a href="https://arxiv.org/abs/2106.08814">(link to open access pdf)</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+vcr.da.train">vcr.da.train</a></code>, <code><a href="#topic+vcr.da.newdata">vcr.da.newdata</a></code>,<br /> <code><a href="#topic+vcr.knn.train">vcr.knn.train</a></code>, <code><a href="#topic+vcr.knn.newdata">vcr.knn.newdata</a></code>,<br /> <code><a href="#topic+vcr.svm.train">vcr.svm.train</a></code>, <code><a href="#topic+vcr.svm.newdata">vcr.svm.newdata</a></code>,<br /> <code><a href="#topic+vcr.rpart.train">vcr.rpart.train</a></code>, <code><a href="#topic+vcr.rpart.newdata">vcr.rpart.newdata</a></code>,<br /> <code><a href="#topic+vcr.forest.train">vcr.forest.train</a></code>, <code><a href="#topic+vcr.forest.newdata">vcr.forest.newdata</a></code>,<br /> <code><a href="#topic+vcr.neural.train">vcr.neural.train</a></code>, <code><a href="#topic+vcr.neural.newdata">vcr.neural.newdata</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>vcrout &lt;- vcr.da.train(iris[, 1:4], iris[, 5])
classmap(vcrout, "setosa", classCols = 2:4) # tight class
classmap(vcrout, "versicolor", classCols = 2:4) # less tight
# The cases misclassified as virginica are shown in blue.
classmap(vcrout, "virginica", classCols = 2:4)
# The case misclassified as versicolor is shown in green.

# For more examples, we refer to the vignettes:
## Not run: 
vignette("Discriminant_analysis_examples")
vignette("K_nearest_neighbors_examples")
vignette("Support_vector_machine_examples")
vignette("Rpart_examples")
vignette("Random_forest_examples")
vignette("Neural_net_examples")

## End(Not run)
</code></pre>

<hr>
<h2 id='confmat.vcr'>
Build a confusion matrix from the output of a function <code>vcr.*.*</code>.
</h2><span id='topic+confmat.vcr'></span>

<h3>Description</h3>

<p>Build a confusion matrix from the output of a function <code>vcr.*.*</code>.
Optionally, a separate column for outliers can be added to the confusion matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>confmat.vcr(vcrout, cutoff = 0.99, showClassNumbers = FALSE,
            showOutliers = TRUE, silent = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="confmat.vcr_+3A_vcrout">vcrout</code></td>
<td>
<p>output of <code>vcr.*.train</code> or <code>vcr.*.newdata</code>.</p>
</td></tr>
<tr><td><code id="confmat.vcr_+3A_cutoff">cutoff</code></td>
<td>
<p>cases with overall farness <code>vcrout$ofarness</code> &gt; <code>cutoff</code> are flagged as outliers.</p>
</td></tr>
<tr><td><code id="confmat.vcr_+3A_showclassnumbers">showClassNumbers</code></td>
<td>
<p>if <code>TRUE</code>, the row and column names are the number of each level instead of the level itself. Useful for long level names.</p>
</td></tr>
<tr><td><code id="confmat.vcr_+3A_showoutliers">showOutliers</code></td>
<td>
<p>if <code>TRUE</code> and some points were flagged as outliers, it adds an extra column on the right of the confusion matrix for these outliers, with label &quot;outl&quot;.</p>
</td></tr>
<tr><td><code id="confmat.vcr_+3A_silent">silent</code></td>
<td>
<p>if <code>FALSE</code>, the confusion matrix and accuracy are shown on the screen.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A confusion matrix
</p>


<h3>Author(s)</h3>

<p>Raymaekers J., Rousseeuw P.J.
</p>


<h3>References</h3>

<p>Raymaekers J., Rousseeuw P.J., Hubert M. (2021). Class maps for visualizing classification results. <em>Technometrics</em>, appeared online. doi: <a href="https://doi.org/10.1080/00401706.2021.1927849">10.1080/00401706.2021.1927849</a>(link to open access pdf)
</p>


<h3>See Also</h3>

<p><code><a href="#topic+vcr.da.train">vcr.da.train</a></code>, <code><a href="#topic+vcr.da.newdata">vcr.da.newdata</a></code>,<br /> <code><a href="#topic+vcr.knn.train">vcr.knn.train</a></code>, <code><a href="#topic+vcr.knn.newdata">vcr.knn.newdata</a></code>,<br /> <code><a href="#topic+vcr.svm.train">vcr.svm.train</a></code>, <code><a href="#topic+vcr.svm.newdata">vcr.svm.newdata</a></code>,<br /> <code><a href="#topic+vcr.rpart.train">vcr.rpart.train</a></code>, <code><a href="#topic+vcr.rpart.newdata">vcr.rpart.newdata</a></code>,<br /> <code><a href="#topic+vcr.forest.train">vcr.forest.train</a></code>, <code><a href="#topic+vcr.forest.newdata">vcr.forest.newdata</a></code>,<br /> <code><a href="#topic+vcr.neural.train">vcr.neural.train</a></code>, <code><a href="#topic+vcr.neural.newdata">vcr.neural.newdata</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
vcrout &lt;- vcr.knn.train(scale(iris[, 1:4]), iris[, 5], k = 5)
# The usual confusion matrix:
confmat.vcr(vcrout, showOutliers = FALSE)

# Cases with ofarness &gt; cutoff are flagged as outliers:
confmat.vcr(vcrout, cutoff = 0.98)

# With the default cutoff = 0.99 only one case is flagged here:
confmat.vcr(vcrout)
# Note that the accuracy is computed before any cases
# are flagged, so it does not depend on the cutoff.

confmat.vcr(vcrout, showClassNumbers = TRUE)
# Shows class numbers instead of labels. This option can
# be useful for long level names.

# For more examples, we refer to the vignettes:
## Not run: 
vignette("Discriminant_analysis_examples")
vignette("K_nearest_neighbors_examples")
vignette("Support_vector_machine_examples")
vignette("Rpart_examples")
vignette("Random_forest_examples")
vignette("Neural_net_examples")

## End(Not run)
</code></pre>

<hr>
<h2 id='data_bookReviews'>
Amazon book reviews data
</h2><span id='topic+data_bookReviews'></span>

<h3>Description</h3>

<p>This is a subset of the data used in the paper, which
was assembled by Prettenhofer and Stein (2010). It contains 1000 reviews
of books on Amazon, of which 500 were selected from the original training data
and 500 from the test data.
</p>
<p>The full dataset has been used for a variety of things, including
classification using svm. The subset was chosen small enough to keep the computation
time low, while still containing the examples in the paper.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("data_bookReviews")</code></pre>


<h3>Format</h3>

<p>A data frame with 1000 observations on the following 2 variables.
</p>

<dl>
<dt><code>review</code></dt><dd><p>the review in text format (character)</p>
</dd>
<dt><code>sentiment</code></dt><dd><p>factor indicating the sentiment of the review: negative (1) or positive (2)</p>
</dd>
</dl>



<h3>Source</h3>

<p>Prettenhofer, P., Stein, B. (2010). Cross-language text classification using
structural correspondence learning. <em>Proceedings of the 48th annual
meeting of the association for computational linguistics</em>,  1118-1127.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(data_bookReviews)
# Example review:
data_bookReviews[5, 1]

# The data are used in:
## Not run: 
vignette("Support_vector_machine_examples")

## End(Not run)
</code></pre>

<hr>
<h2 id='data_floralbuds'>
Floral buds data
</h2><span id='topic+data_floralbuds'></span>

<h3>Description</h3>

<p>This data on floral pear bud detection was first described by Wouters et al.
The goal is to classify the instances into buds, branches, scales and support.
The numeric vectors resulted from a multispectral vision sensor and describe the scanned images.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("data_floralbuds")</code></pre>


<h3>Format</h3>

<p>A data frame with 550 observations on the following 7 variables.
</p>

<dl>
<dt><code>X1</code></dt><dd><p>numeric vector</p>
</dd>
<dt><code>X2</code></dt><dd><p>numeric vector</p>
</dd>
<dt><code>X3</code></dt><dd><p>numeric vector</p>
</dd>
<dt><code>X4</code></dt><dd><p>numeric vector</p>
</dd>
<dt><code>X5</code></dt><dd><p>numeric vector</p>
</dd>
<dt><code>X6</code></dt><dd><p>numeric vector</p>
</dd>
<dt><code>y</code></dt><dd><p>a factor with levels <code>branch</code> <code>bud</code> <code>scales</code> <code>support</code></p>
</dd>
</dl>



<h3>Source</h3>

<p>Wouters, N., De Ketelaere, B., Deckers, T. De Baerdemaeker, J., Saeys, W. (2015).
Multispectral detection of floral buds for automated thinning of pear.
<em>Comput. Electron. Agric.</em> 113, C, 93–103. &lt;doi:10.1016/j.compag.2015.01.015&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("data_floralbuds")
str(data_floralbuds)
summary(data_floralbuds)

# The data are used in:
## Not run: 
vignette("Discriminant_analysis_examples")
vignette("Neural_net_examples")

## End(Not run)
</code></pre>

<hr>
<h2 id='data_instagram'>
Instagram data
</h2><span id='topic+data_instagram'></span>

<h3>Description</h3>

<p>This dataset contains information on fake (spam) accounts on Instagram.
The original source is https://www.kaggle.com/free4ever1/instagram-fake-spammer-genuine-accounts by Bardiya Bakhshandeh.
</p>
<p>The data contains information on 696 Instagram accounts.
For each account, 11 variables were recorded describing its characteristics.
The goal is to detect fake instagram accounts, which are used for spamming.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("data_instagram")</code></pre>


<h3>Format</h3>

<p>A data frame with 696 observations on the following variables.
</p>

<dl>
<dt>profile.pic</dt><dd><p>binary, indicates whether profile has picture.</p>
</dd>
<dt>nums.length.username</dt><dd><p>ratio of number of numerical chars in username to its length.</p>
</dd>
<dt>fullname.words</dt><dd><p>number of words in full name.</p>
</dd>
<dt>nums.length.fullname</dt><dd><p>ratio of number of numerical characters in full name to its length.</p>
</dd>
<dt>name..username</dt><dd><p>binary, indicates whether the name and username of the profile
are the same.</p>
</dd>
<dt>description.length</dt><dd><p>length of the description/biography of the profile (in number of characters).</p>
</dd>
<dt>external.URL</dt><dd><p>binary, indicates whether profile has external url.</p>
</dd>
<dt>private</dt><dd><p>binary, indicates whether profile is private or not.</p>
</dd>
<dt>X.posts</dt><dd><p>number of posts made by profile.</p>
</dd>
<dt>X.followers</dt><dd><p>number of followers.</p>
</dd>
<dt>X.follows</dt><dd><p>numbers of follows.</p>
</dd>
<dt>y</dt><dd><p>whether profile is fake or not.</p>
</dd>
<dt>dataType</dt><dd><p>vector taking the values &ldquo;train&rdquo; or &ldquo;test&rdquo; indicating
whether the observation belongs to the training or the test data.</p>
</dd>
</dl>



<h3>Source</h3>

<p>https://www.kaggle.com/free4ever1/instagram-fake-spammer-genuine-accounts
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(data_instagram)
str(data_instagram)

# The data are used in:
## Not run: 
vignette("Random_forest_examples")

## End(Not run)
</code></pre>

<hr>
<h2 id='data_titanic'>
Titanic data
</h2><span id='topic+data_titanic'></span>

<h3>Description</h3>

<p>This dataset contains information on 1309 passengers of the RMS Titanic.
The goal is to predict survival based on 11 characteristics such as the travel class, age and sex of the passengers.
</p>
<p>The original data source is
https://www.kaggle.com/c/titanic/data
</p>
<p>The data is split up in a training data consisting of 891 observations and a test data of 418 observations. The response in the test set was obtained by combining information from other data files, and has been verified by submitting it as a &lsquo;prediction&rsquo; to kaggle and getting perfect marks.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data("data_titanic")</code></pre>


<h3>Format</h3>

<p>A data frame with 1309 observations on the following variables.
</p>

<dl>
<dt>PassengerId</dt><dd><p>a unique identified for each passenger.</p>
</dd>
<dt>Pclass</dt><dd><p>travel class of the passenger.</p>
</dd>
<dt>Name</dt><dd><p>name of the passenger.</p>
</dd>
<dt>Sex</dt><dd><p>sex of the passenger.</p>
</dd>
<dt>Age</dt><dd><p>age of the passenger.</p>
</dd>
<dt>SibSp</dt><dd><p>number of siblings and spouses traveling with the passenger.</p>
</dd>
<dt>Parch</dt><dd><p>number of parents and children traveling with the passenger.</p>
</dd>
<dt>Ticket</dt><dd><p>Ticket number of the passenger.</p>
</dd>
<dt>Fare</dt><dd><p>fare paid for the ticket.</p>
</dd>
<dt>Cabin</dt><dd><p>cabin number of the passenger.</p>
</dd>
<dt>Embarked</dt><dd><p>Port of embarkation. Takes the values C (Cherbourg), Q (Queenstown) and S (Southampton).</p>
</dd>
<dt>y</dt><dd><p>factor indicating casualty or survivor.</p>
</dd>
<dt>dataType</dt><dd><p>vector taking the values &ldquo;train&rdquo; or &ldquo;test&rdquo; indicating whether the observation belongs to the training or the test data.</p>
</dd>
</dl>



<h3>Source</h3>

<p>https://www.kaggle.com/c/titanic/data
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("data_titanic")
traindata &lt;- data_titanic[which(data_titanic$dataType == "train"), -13]
testdata &lt;- data_titanic[which(data_titanic$dataType == "test"), -13]
str(traindata)
table(traindata$y)

# The data are used in:
## Not run: 
vignette("Rpart_examples")

## End(Not run)
</code></pre>

<hr>
<h2 id='makeFV'>
Constructs feature vectors from a kernel matrix.
</h2><span id='topic+makeFV'></span>

<h3>Description</h3>

<p>Constructs feature vectors from a kernel matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>makeFV(kmat, transfmat = NULL, precS = 1e-12)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="makeFV_+3A_kmat">kmat</code></td>
<td>
<p>a kernel matrix. If <code>transfmat</code> is <code>NULL</code>, we are dealing with training data and then <code>kmat</code> must be a square kernel matrix (of size <code class="reqn">n</code> by <code class="reqn">n</code> when there are <code class="reqn">n</code> cases). Such a PSD matrix kmat can e.g. be produced by <code><a href="#topic+makeKernel">makeKernel</a></code> or by <code><a href="kernlab.html#topic+kernelMatrix">kernlab::kernelMatrix</a></code>. If on the other hand <code>transfmat</code> is not <code>NULL</code>, we are dealing with a test set. See details for the precise working.</p>
</td></tr>
<tr><td><code id="makeFV_+3A_transfmat">transfmat</code></td>
<td>
<p>transformation matrix. If not <code>NULL</code>, it is the value <code>transfmat</code> of <code><a href="#topic+makeFV">makeFV</a></code> on training data. It has to be a square matrix, with as many rows as there were training data.</p>
</td></tr>
<tr><td><code id="makeFV_+3A_precs">precS</code></td>
<td>
<p>if not <code>NULL</code>, eigenvalues of <code>kmat</code> below <code>precS</code> will be set equal to precS.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>transfmat</code> is non-<code>NULL</code>, we are dealing with a test set.
Denote the number of cases in the test set by <code class="reqn">m \geq 1</code>. Each row of <code>kmat</code> of the test set then must contain the kernel values of a new case with all cases in the training set. Therefore the kernel matrix kmat must have dimensions <code class="reqn">m</code> by <code class="reqn">n</code>. The matrix <code>kmat</code> can e.g. be produced by <code><a href="#topic+makeKernel">makeKernel</a></code>. It can also be obtained by running <code><a href="kernlab.html#topic+kernelMatrix">kernlab::kernelMatrix</a></code> on the union of the training set and the test set, yielding an <code class="reqn">(n+m)</code> by <code class="reqn">(n+m)</code> matrix, from which one then takes the <code class="reqn">[(n+1):m , 1:n]</code> submatrix.
</p>


<h3>Value</h3>

<p>A list with components: <br />
</p>
<table>
<tr><td><code>Xf</code></td>
<td>
<p>When makeKV is applied to the training set, <code>Xf</code> has coordinates of <code class="reqn">n</code> points (vectors), the plain inner products of which equal the kernel matrix of the training set. That is, <code>kmat</code> = <code>Xf</code> <code>Xf</code>'. The <code>Xf</code> are expressed in an orthogonal basis in which the variance of the coordinates is decreasing, which is useful when plotting the first few coordinates. When <code><a href="#topic+makeFV">makeFV</a></code> is applied to a test set, <code>Xf</code> are coordinates of the feature vectors of the test set in the same space as those of the training set, and then <code>kmat</code> = <code>Xf</code> %*% <code>Xf of training data</code>.
</p>
</td></tr>
<tr><td><code>transfmat</code></td>
<td>
<p>square matrix for transforming kmat to <code>Xf</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Raymaekers J., Rousseeuw P.J., Hubert, M.
</p>


<h3>References</h3>

<p>Raymaekers J., Rousseeuw P.J., Hubert M. (2021). Class maps for visualizing classification results. <em>Technometrics</em>, appeared online. doi: <a href="https://doi.org/10.1080/00401706.2021.1927849">10.1080/00401706.2021.1927849</a>(link to open access pdf)
</p>


<h3>See Also</h3>

<p><code><a href="#topic+makeKernel">makeKernel</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(e1071)
set.seed(1); X &lt;- matrix(rnorm(200 * 2), ncol = 2)
X[1:100, ] &lt;- X[1:100, ] + 2
X[101:150, ] &lt;- X[101:150, ] - 2
y &lt;- as.factor(c(rep("blue", 150), rep("red", 50)))
cols &lt;- c("deepskyblue3", "red")
plot(X, col = cols[as.numeric(y)], pch = 19)
# We now fit an SVM with radial basis kernel to the data:
svmfit &lt;- svm(y~.,  data = data.frame(X = X, y = y),  scale = FALSE,
             kernel = "radial", cost = 10, gamma = 1, probability = TRUE)
Kxx &lt;- makeKernel(X, svfit = svmfit)
outFV &lt;- makeFV(Kxx)
Xf &lt;- outFV$Xf # The data matrix in this feature space.
dim(Xf) # The feature vectors are high dimensional.
# The inner products of Xf match the kernel matrix:
max(abs(as.vector(Kxx - crossprod(t(Xf), t(Xf))))) # 3.005374e-13 # tiny, OK
range(rowSums(Xf^2)) # all points in Xf lie on the unit sphere.
pairs(Xf[, 1:5], col = cols[as.numeric(y)])
# In some of these we see spherical effects, e.g.
plot(Xf[, 1], Xf[, 5], col = cols[as.numeric(y)], pch = 19)
# The data look more separable here than in the original
# two-dimensional space.

# For more examples, we refer to the vignette:
## Not run: 
vignette("Support_vector_machine_examples")

## End(Not run)
</code></pre>

<hr>
<h2 id='makeKernel'>
Compute kernel matrix
</h2><span id='topic+makeKernel'></span>

<h3>Description</h3>

<p>Computes kernel value or kernel matrix, where the kernel type is extracted from an svm trained by <code><a href="e1071.html#topic+svm">e1071::svm</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>makeKernel(X1, X2 = NULL, svfit)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="makeKernel_+3A_x1">X1</code></td>
<td>
<p>first matrix (or vector) of coordinates.</p>
</td></tr>
<tr><td><code id="makeKernel_+3A_x2">X2</code></td>
<td>
<p>if not <code>NULL</code>, second data matrix or vector. If NULL, <code>X2</code> is assumed equal to <code>X1</code>.</p>
</td></tr>
<tr><td><code id="makeKernel_+3A_svfit">svfit</code></td>
<td>
<p>output from <code><a href="e1071.html#topic+svm">e1071::svm</a></code></p>
</td></tr></table>
<p>.
</p>


<h3>Value</h3>

<p>the kernel matrix, of dimensions <code>nrow(X1)</code> by <code>nrow(X2)</code>. When both <code>X1</code> and <code>X2</code> are vectors, the result is a single number.
</p>


<h3>Author(s)</h3>

<p>Raymaekers J., Rousseeuw P.J.
</p>


<h3>References</h3>

<p>Raymaekers J., Rousseeuw P.J., Hubert M. (2021). Class maps for visualizing classification results. <em>Technometrics</em>, appeared online. doi: <a href="https://doi.org/10.1080/00401706.2021.1927849">10.1080/00401706.2021.1927849</a>(link to open access pdf)
</p>


<h3>See Also</h3>

<p><code><a href="#topic+makeFV">makeFV</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(e1071)
set.seed(1); X &lt;- matrix(rnorm(200 * 2), ncol = 2)
X[1:100, ] &lt;- X[1:100, ] + 2
X[101:150, ] &lt;- X[101:150, ] - 2
y &lt;- as.factor(c(rep("blue", 150), rep("red", 50))) # two classes
# We now fit an SVM with radial basis kernel to the data:
set.seed(1) # to make the result of svm() reproducible.
svmfit &lt;- svm(y~.,  data = data.frame(X = X, y = y),  scale = FALSE,
             kernel = "radial", cost = 10, gamma = 1, probability = TRUE)
Kxx &lt;- makeKernel(X, svfit = svmfit)
# The result is a square kernel matrix:
dim(Kxx) # 200 200
Kxx[1:5, 1:5]

# For more examples, we refer to the vignette:
## Not run: 
vignette("Support_vector_machine_examples")

## End(Not run)
</code></pre>

<hr>
<h2 id='qresplot'>
Draw a quasi residual plot of PAC versus a data feature
</h2><span id='topic+qresplot'></span>

<h3>Description</h3>

<p>Draw a quasi residual plot to visualize classification results.
The vertical axis of the quasi residual plot shows each case's probability of alternative class (PAC). The horizontal axis
shows the feature given as the second argument in the function call.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>qresplot(PAC, feat, xlab = NULL, xlim = NULL,
         main = NULL, identify = FALSE, gray = TRUE,
         opacity = 1, squareplot = FALSE, plotLoess = FALSE,
         plotErrorBars = FALSE, plotQuantiles = FALSE,
         grid = NULL, probs = c(0.5, 0.75),
         cols = NULL, fac = 1, cex = 1,
         cex.main = 1.2, cex.lab = 1,
         cex.axis = 1, pch = 19)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="qresplot_+3A_pac">PAC</code></td>
<td>
<p>vector with the PAC values of a classification,
typically the <code>$PAC</code> in the return of a call to a function <code>vcr.*.*</code></p>
</td></tr>
<tr><td><code id="qresplot_+3A_feat">feat</code></td>
<td>
<p>the PAC will be plotted versus this data
feature. Note that feat does not have to be
one of the explanatory variables of the model.
It can be another variable, a combination of
variables (like a sum or a principal component
score), the row number of the cases if they
were recorded succesively, etc.</p>
</td></tr>
<tr><td><code id="qresplot_+3A_xlab">xlab</code></td>
<td>
<p>label for the horizontal axis, i.e. the name
of variable feat.</p>
</td></tr>
<tr><td><code id="qresplot_+3A_xlim">xlim</code></td>
<td>
<p>limits for the horizontal axis. If <code>NULL</code>, the
range of feat is used.</p>
</td></tr>
<tr><td><code id="qresplot_+3A_main">main</code></td>
<td>
<p>title for the plot.</p>
</td></tr>
<tr><td><code id="qresplot_+3A_identify">identify</code></td>
<td>
<p>if <code>TRUE</code>, left-click on a point to get its number, then ESC to exit.</p>
</td></tr>
<tr><td><code id="qresplot_+3A_gray">gray</code></td>
<td>
<p>logical, if <code>TRUE</code> (the default) the plot region
where <code class="reqn">PAC &lt; 0.5</code> gets a light gray background.
Points in this region were classified into
their given class, and the points above this
region were misclassified.</p>
</td></tr>
<tr><td><code id="qresplot_+3A_opacity">opacity</code></td>
<td>
<p>determines opacity of plotted dots.
Value between 0 and 1, where 0 is transparent
and 1 is opaque.</p>
</td></tr>
<tr><td><code id="qresplot_+3A_squareplot">squareplot</code></td>
<td>
<p>if <code>TRUE</code>, the horizontal and vertical axis will
get the same length.</p>
</td></tr>
<tr><td><code id="qresplot_+3A_plotloess">plotLoess</code></td>
<td>
<p>if <code>TRUE</code>, a standard loess curve is fitted and
superimposed on the plot. May not work well if
feat is discrete with few values.
At most one of the options <code>plotLoess</code>,
<code>plotErrorbars</code>, or <code>plotQuantiles</code> can be selected.</p>
</td></tr>
<tr><td><code id="qresplot_+3A_ploterrorbars">plotErrorBars</code></td>
<td>
<p>if <code>TRUE</code>, the average PAC and its standard
error are computed on the intervals of a grid
(see option grid). Then a red curve connecting
the averages is plotted, as well as two blue
curves corresponding to the average plus or
minus one standard error. At most one of the options <code>plotLoess</code>,
<code>plotErrorbars</code>, or <code>plotQuantiles</code> can be selected.</p>
</td></tr>
<tr><td><code id="qresplot_+3A_plotquantiles">plotQuantiles</code></td>
<td>
<p>if <code>TRUE</code>, one or more quantiles of the PAC
are computed on the intervals of a grid
(see option grid). The quantiles correspond
the probabilities in option probs.
Then the curves connecting the quantiles
are plotted. At most one of the options <code>plotLoess</code>,
<code>plotErrorbars</code>, or <code>plotQuantiles</code> can be selected.</p>
</td></tr>
<tr><td><code id="qresplot_+3A_grid">grid</code></td>
<td>
<p>only used when <code>plotErrorBars</code> or <code>plotQuantiles</code>
are selected. This is a vector with increasing
feat values, forming the grid. If <code>NULL</code>, the
grid consists of the minimum and the maximum
of feat, with 9 equispaced points between them.</p>
</td></tr>
<tr><td><code id="qresplot_+3A_probs">probs</code></td>
<td>
<p>only used when <code>plotQuantiles</code> is selected. This
is a vector with probabilities determining the
quantiles. If <code>NULL</code>, defaults to <code>c(0.5, 0.75)</code>.</p>
</td></tr>
<tr><td><code id="qresplot_+3A_cols">cols</code></td>
<td>
<p>only used when plotquantiles is selected.
A vector with the colors of the quantile curves.
If <code>NULL</code> the cols are taken as 2, 3, ...</p>
</td></tr>
<tr><td><code id="qresplot_+3A_fac">fac</code></td>
<td>
<p>only used when <code>plotLoess</code>, <code>plotErrorBars</code> or
<code>plotQuantiles</code> are selected. A real number to
multiply the resulting curves. A value <code>fac &gt; 1</code>
can be useful to better visualize the curves
when they would be too close to zero.
By default (<code>fac = 1</code>) this is not done.</p>
</td></tr>
<tr><td><code id="qresplot_+3A_cex">cex</code></td>
<td>
<p>passed on to <code><a href="graphics.html#topic+plot">plot</a></code>.</p>
</td></tr>
<tr><td><code id="qresplot_+3A_cex.main">cex.main</code></td>
<td>
<p>same, for title.</p>
</td></tr>
<tr><td><code id="qresplot_+3A_cex.lab">cex.lab</code></td>
<td>
<p>same, for labels on horizontal and vertical axes.</p>
</td></tr>
<tr><td><code id="qresplot_+3A_cex.axis">cex.axis</code></td>
<td>
<p>same, for axes.</p>
</td></tr>
<tr><td><code id="qresplot_+3A_pch">pch</code></td>
<td>
<p>plot character for the points, defaults to 19.</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>coordinates</code></td>
<td>
<p>a matrix with 2 columns containing the
coordinates of the plotted points. This makes it
easier to add text next to interesting points.
If <code>identify = TRUE</code>, the attribute ids of coordinates
contains the row numbers of the identified points
in the matrix coordinates.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Raymaekers J., Rousseeuw P.J.
</p>


<h3>References</h3>

<p>Raymaekers J., Rousseeuw P.J.(2021). Silhouettes and quasi residual plots for neural nets and tree-based classifiers. <a href="https://arxiv.org/abs/2106.08814">(link to open access pdf)</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(rpart)
data("data_titanic")
traindata &lt;- data_titanic[which(data_titanic$dataType == "train"), -13]
set.seed(123) # rpart is not deterministic
rpart.out &lt;- rpart(y ~ Pclass + Sex + SibSp +
                    Parch + Fare + Embarked,
                  data = traindata, method = 'class', model = TRUE)
mytype &lt;- list(nominal = c("Name", "Sex", "Ticket", "Cabin", "Embarked"), ordratio = c("Pclass"))
x_train &lt;- traindata[, -12]
y_train &lt;- traindata[,  12]
vcrtrain &lt;- vcr.rpart.train(x_train, y_train, rpart.out, mytype)
# Quasi residual plot versus age, for males only:
PAC &lt;- vcrtrain$PAC[which(x_train$Sex == "male")]
feat &lt;- x_train$Age[which(x_train$Sex == "male")]
qresplot(PAC, feat, xlab = "Age (years)", opacity = 0.5,
         main = "quasi residual plot for male passengers",
         plotLoess = TRUE)
text(x = 14, y = 0.60, "loess curve", col = "red", cex = 1)
</code></pre>

<hr>
<h2 id='silplot'>
Draw the silhouette plot of a classification
</h2><span id='topic+silplot'></span>

<h3>Description</h3>

<p>Draw the silhouette plot to visualize classification results, based on the output of one of the <code>vcr.*.*</code> functions in this package. The horizontal axis of the silhouette plot shows each case's <code>s(i)</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>silplot(vcrout, classLabels = NULL, classCols = NULL,
        showLegend = TRUE, showClassNumbers = FALSE,
        showCases = FALSE, drawLineAtAverage = FALSE,
        topdown = TRUE, main = NULL, summary = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="silplot_+3A_vcrout">vcrout</code></td>
<td>
<p>output of <code>vcr.*.train</code> or <code>vcr.*.newdata</code>. Required.</p>
</td></tr>
<tr><td><code id="silplot_+3A_classlabels">classLabels</code></td>
<td>
<p>the labels (levels) of the classes. If <code>NULL</code>, they are taken from <code>vcrout</code>.</p>
</td></tr>
<tr><td><code id="silplot_+3A_classcols">classCols</code></td>
<td>
<p>a list of colors for the classes. There should be at least as many as there are levels. If <code>NULL</code> a default palette is used.</p>
</td></tr>
<tr><td><code id="silplot_+3A_showlegend">showLegend</code></td>
<td>
<p>if <code>TRUE</code>, a legend is shown to the right of the plot.</p>
</td></tr>
<tr><td><code id="silplot_+3A_showclassnumbers">showClassNumbers</code></td>
<td>
<p>if <code>TRUE</code>, the legend will show the class numbers
instead of the class labels.</p>
</td></tr>
<tr><td><code id="silplot_+3A_showcases">showCases</code></td>
<td>
<p>if <code>TRUE</code>, the plot shows the numbers of the cases.
They are only readable when the number of cases
is relatively small.</p>
</td></tr>
<tr><td><code id="silplot_+3A_topdown">topdown</code></td>
<td>
<p>if <code>TRUE</code> (the default), the silhouettes are
plotted from top to bottom. Otherwise they
are plotted from left to right.</p>
</td></tr>
<tr><td><code id="silplot_+3A_drawlineataverage">drawLineAtAverage</code></td>
<td>
<p>if <code>TRUE</code>, drwas a line at the average value
of the <code>s(i)</code>.</p>
</td></tr>
<tr><td><code id="silplot_+3A_main">main</code></td>
<td>
<p>title for the plot. If <code>NULL</code>, a default title is used.</p>
</td></tr>
<tr><td><code id="silplot_+3A_summary">summary</code></td>
<td>
<p>if <code>TRUE</code>, puts a summary table on the screen
with for each class its number, label, number of class members,
and the average of its <code>s(i)</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A ggplot object containing the silhouette plot.
</p>


<h3>Author(s)</h3>

<p>Raymaekers J., Rousseeuw P.J.
</p>


<h3>References</h3>

<p>Raymaekers J., Rousseeuw P.J.(2021). Silhouettes and quasi residual plots for neural nets and tree-based classifiers. <a href="https://arxiv.org/abs/2106.08814">(link to open access pdf)</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+vcr.da.train">vcr.da.train</a></code>, <code><a href="#topic+vcr.da.newdata">vcr.da.newdata</a></code>,<br /> <code><a href="#topic+vcr.knn.train">vcr.knn.train</a></code>, <code><a href="#topic+vcr.knn.newdata">vcr.knn.newdata</a></code>,<br /> <code><a href="#topic+vcr.svm.train">vcr.svm.train</a></code>, <code><a href="#topic+vcr.svm.newdata">vcr.svm.newdata</a></code>,<br /> <code><a href="#topic+vcr.rpart.train">vcr.rpart.train</a></code>, <code><a href="#topic+vcr.rpart.newdata">vcr.rpart.newdata</a></code>,<br /> <code><a href="#topic+vcr.forest.train">vcr.forest.train</a></code>, <code><a href="#topic+vcr.forest.newdata">vcr.forest.newdata</a></code>,<br /> <code><a href="#topic+vcr.neural.train">vcr.neural.train</a></code>, <code><a href="#topic+vcr.neural.newdata">vcr.neural.newdata</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>vcrout &lt;- vcr.da.train(iris[, 1:4], iris[, 5])
silplot(vcrout)
# For more examples, we refer to the vignettes:
## Not run: 
vignette("Discriminant_analysis_examples")
vignette("K_nearest_neighbors_examples")
vignette("Support_vector_machine_examples")
vignette("Rpart_examples")
vignette("Forest_examples")
vignette("Neural_net_examples")

## End(Not run)
</code></pre>

<hr>
<h2 id='stackedplot'>
Make a vertically stacked mosaic plot of class predictions.
</h2><span id='topic+stackedplot'></span>

<h3>Description</h3>

<p>Make a vertically stacked mosaic plot of class predictions from the output of
<code>vcr.*.train</code> or <code>vcr.*.newdata</code>. Optionally,
the outliers for each class can be shown as a gray rectangle at the top.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stackedplot(vcrout, cutoff = 0.99, classCols = NULL,
classLabels = NULL, separSize=1, minSize=1.5,
showOutliers = TRUE, showLegend = FALSE, main = NULL,
htitle = NULL, vtitle = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="stackedplot_+3A_vcrout">vcrout</code></td>
<td>
<p>output of <code>vcr.*.train</code> or <code>vcr.*.newdata</code>.</p>
</td></tr>
<tr><td><code id="stackedplot_+3A_cutoff">cutoff</code></td>
<td>
<p>cases with overall farness <code>vcrout$ofarness</code> &gt; <code>cutoff</code> are flagged as outliers.</p>
</td></tr>
<tr><td><code id="stackedplot_+3A_classcols">classCols</code></td>
<td>
<p>user-specified colors for the classes. If <code>NULL</code> a default palette is used.</p>
</td></tr>
<tr><td><code id="stackedplot_+3A_classlabels">classLabels</code></td>
<td>
<p>names of given labels. If <code>NULL</code> they are taken from <code>vcrout</code>.</p>
</td></tr>
<tr><td><code id="stackedplot_+3A_separsize">separSize</code></td>
<td>
<p>how much white between rectangles.</p>
</td></tr>
<tr><td><code id="stackedplot_+3A_minsize">minSize</code></td>
<td>
<p>rectangles describing less than <code>minSize</code> percent of the data, are shown as <code>minSize</code> percent.</p>
</td></tr>
<tr><td><code id="stackedplot_+3A_showoutliers">showOutliers</code></td>
<td>
<p>if <code>TRUE</code>, shows a separate class in gray with the outliers, always at the top.</p>
</td></tr>
<tr><td><code id="stackedplot_+3A_showlegend">showLegend</code></td>
<td>
<p>if <code>TRUE</code>, a legend is shown to the right of the plot. Default <code>FALSE</code>, since the legend is not necessary as the colors are already visible in the bottom part of each stack.</p>
</td></tr>
<tr><td><code id="stackedplot_+3A_main">main</code></td>
<td>
<p>title for the plot.</p>
</td></tr>
<tr><td><code id="stackedplot_+3A_htitle">htitle</code></td>
<td>
<p>title for horizontal axis (given labels). If  <code>NULL</code>, a default title is shown.</p>
</td></tr>
<tr><td><code id="stackedplot_+3A_vtitle">vtitle</code></td>
<td>
<p>title for vertical axis (predicted labels). If  <code>NULL</code>, a default title is shown.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A ggplot object.
</p>


<h3>Author(s)</h3>

<p>Raymaekers J., Rousseeuw P.J.
</p>


<h3>References</h3>

<p>Raymaekers J., Rousseeuw P.J., Hubert M. (2021). Class maps for visualizing classification results. <em>Technometrics</em>, appeared online. doi: <a href="https://doi.org/10.1080/00401706.2021.1927849">10.1080/00401706.2021.1927849</a>(link to open access pdf)
</p>


<h3>See Also</h3>

<p><code><a href="#topic+vcr.da.train">vcr.da.train</a></code>, <code><a href="#topic+vcr.da.newdata">vcr.da.newdata</a></code>,<br /> <code><a href="#topic+vcr.knn.train">vcr.knn.train</a></code>, <code><a href="#topic+vcr.knn.newdata">vcr.knn.newdata</a></code>,<br /> <code><a href="#topic+vcr.svm.train">vcr.svm.train</a></code>, <code><a href="#topic+vcr.svm.newdata">vcr.svm.newdata</a></code>,<br /> <code><a href="#topic+vcr.rpart.train">vcr.rpart.train</a></code>, <code><a href="#topic+vcr.rpart.newdata">vcr.rpart.newdata</a></code>,<br /> <code><a href="#topic+vcr.forest.train">vcr.forest.train</a></code>, <code><a href="#topic+vcr.forest.newdata">vcr.forest.newdata</a></code>,<br /> <code><a href="#topic+vcr.neural.train">vcr.neural.train</a></code>, <code><a href="#topic+vcr.neural.newdata">vcr.neural.newdata</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data("data_floralbuds")
X &lt;- data_floralbuds[, 1:6]; y &lt;- data_floralbuds[, 7]
vcrout &lt;- vcr.da.train(X, y)
cols &lt;- c("saddlebrown", "orange", "olivedrab4", "royalblue3")
stackedplot(vcrout, classCols = cols, showLegend = TRUE)

# The legend is not really needed, since we can read the
# color of a class from the bottom of its vertical bar:
stackedplot(vcrout, classCols = cols, main = "Stacked plot of QDA on foral buds data")

# If we do not wish to show outliers:
stackedplot(vcrout, classCols = cols, showOutliers = FALSE)

# For more examples, we refer to the vignettes:
## Not run: 
vignette("Discriminant_analysis_examples")
vignette("K_nearest_neighbors_examples")
vignette("Support_vector_machine_examples")
vignette("Rpart_examples")
vignette("Random_forest_examples")
vignette("Neural_net_examples")

## End(Not run)
</code></pre>

<hr>
<h2 id='vcr.da.newdata'>
Carry out discriminant analysis on new data, and prepare to visualize its results.
</h2><span id='topic+vcr.da.newdata'></span>

<h3>Description</h3>

<p>Predicts class labels for new data by discriminant analysis, using the output of <code><a href="#topic+vcr.da.train">vcr.da.train</a></code> on the training data. For new data cases whose label in <code>yintnew</code> is non-missing, additional output is produced for constructing graphical displays such as the <code><a href="#topic+classmap">classmap</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vcr.da.newdata(Xnew, ynew=NULL, vcr.da.train.out)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vcr.da.newdata_+3A_xnew">Xnew</code></td>
<td>
<p>data matrix of the new data, with the same number of columns as in the training data. Missing values are not allowed.
</p>
</td></tr>
<tr><td><code id="vcr.da.newdata_+3A_ynew">ynew</code></td>
<td>
<p>factor with class membership of each new case. Can be <code>NA</code> for some or all cases. If <code>NULL</code>, is assumed to be <code>NA</code> everywhere.</p>
</td></tr>
<tr><td><code id="vcr.da.newdata_+3A_vcr.da.train.out">vcr.da.train.out</code></td>
<td>
<p>output of <code><a href="#topic+vcr.da.train">vcr.da.train</a></code> on the training data.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with components: <br />
</p>
<table>
<tr><td><code>yintnew</code></td>
<td>
<p>number of the given class of each case. Can contain <code>NA</code>'s.</p>
</td></tr>
<tr><td><code>ynew</code></td>
<td>
<p>given class label of each case. Can contain <code>NA</code>'s.</p>
</td></tr>
<tr><td><code>levels</code></td>
<td>
<p>levels of the response, from <code>vcr.da.train.out</code>.</p>
</td></tr>
<tr><td><code>predint</code></td>
<td>
<p>predicted class number of each case. Always exists.</p>
</td></tr>
<tr><td><code>pred</code></td>
<td>
<p>predicted label of each case.</p>
</td></tr>
<tr><td><code>altint</code></td>
<td>
<p>number of the alternative class. Among the classes different from the given class, it is the one with the highest posterior probability. Is <code>NA</code> for cases whose <code>ynew</code> is missing.</p>
</td></tr>
<tr><td><code>altlab</code></td>
<td>
<p>label of the alternative class. Is <code>NA</code> for cases whose <code>ynew</code> is missing.</p>
</td></tr>
<tr><td><code>PAC</code></td>
<td>
<p>probability of the alternative class. Is <code>NA</code> for cases whose <code>ynew</code> is missing.</p>
</td></tr>
<tr><td><code>fig</code></td>
<td>
<p>distance of each case <code class="reqn">i</code> to each class <code class="reqn">g</code>. Always exists.</p>
</td></tr>
<tr><td><code>farness</code></td>
<td>
<p>farness of each case <code class="reqn">i</code> from its given class. Is <code>NA</code> for cases whose <code>ynew</code> is missing.</p>
</td></tr>
<tr><td><code>ofarness</code></td>
<td>
<p>For each case <code class="reqn">i</code>, its lowest <code>fig[i,g]</code> to any class <code>g</code>. Always exists.</p>
</td></tr>
<tr><td><code>classMS</code></td>
<td>
<p>list with center and covariance matrix of each class, from <code>vcr.da.train.out</code>.</p>
</td></tr>
<tr><td><code>lCurrent</code></td>
<td>
<p>log of mixture density of each case in its given class. Is <code>NA</code> for cases with missing <code>ynew</code>.</p>
</td></tr>
<tr><td><code>lPred</code></td>
<td>
<p>log of mixture density of each case in its predicted class. Always exists.</p>
</td></tr>
<tr><td><code>lAlt</code></td>
<td>
<p>log of mixture density of each case in its alternative class. Is <code>NA</code> for cases with missing <code>ynew</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Raymaekers J., Rousseeuw P.J.
</p>


<h3>References</h3>

<p>Raymaekers J., Rousseeuw P.J., Hubert M. (2021). Class maps for visualizing classification results. <em>Technometrics</em>, appeared online. doi: <a href="https://doi.org/10.1080/00401706.2021.1927849">10.1080/00401706.2021.1927849</a>(link to open access pdf)
</p>


<h3>See Also</h3>

<p><code><a href="#topic+vcr.da.train">vcr.da.train</a></code>, <code><a href="#topic+classmap">classmap</a></code>, <code><a href="#topic+silplot">silplot</a></code>, <code><a href="#topic+stackedplot">stackedplot</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
vcr.train &lt;- vcr.da.train(iris[, 1:4], iris[, 5])
inds &lt;- c(51:150) # a subset, containing only 2 classes
iris2 &lt;- iris[inds, ] # fake "new" data
iris2[c(1:10, 51:60), 5] &lt;- NA
vcr.test &lt;- vcr.da.newdata(iris2[, 1:4], iris2[, 5], vcr.train)
vcr.test$PAC[1:25] # between 0 and 1. Is NA where the response is.
plot(vcr.test$PAC, vcr.train$PAC[inds]); abline(0, 1) # match
plot(vcr.test$farness, vcr.train$farness[inds]); abline(0, 1) # match
confmat.vcr(vcr.train) # for comparison
confmat.vcr(vcr.test)
stackedplot(vcr.train) # for comparison
stackedplot(vcr.test)
classmap(vcr.train, "versicolor", classCols = 2:4) # for comparison
classmap(vcr.test, "versicolor", classCols = 2:4) # has fewer points

# For more examples, we refer to the vignette:
## Not run: 
vignette("Discriminant_analysis_examples")

## End(Not run)
</code></pre>

<hr>
<h2 id='vcr.da.train'>
Carry out discriminant analysis on training data, and prepare to visualize its results.
</h2><span id='topic+vcr.da.train'></span>

<h3>Description</h3>

<p>Custom DA function which prepares for graphical displays such as the <code><a href="#topic+classmap">classmap</a></code>. The disciminant analysis itself is carried out by the maximum a posteriori rule, which maximizes the density of the mixture.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vcr.da.train(X, y, rule = "QDA", estmethod = "meancov")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vcr.da.train_+3A_x">X</code></td>
<td>
<p>a numerical matrix containing the predictors in its columns. Missing values are not allowed.
</p>
</td></tr>
<tr><td><code id="vcr.da.train_+3A_y">y</code></td>
<td>
<p>a factor with the given class labels.</p>
</td></tr>
<tr><td><code id="vcr.da.train_+3A_rule">rule</code></td>
<td>
<p>either &quot;<code>QDA</code>&quot; for quadratic discriminant analysis or &quot;<code>LDA</code>&quot;  for linear discriminant analysis.</p>
</td></tr>
<tr><td><code id="vcr.da.train_+3A_estmethod">estmethod</code></td>
<td>
<p>function for location and covariance estimation.
Should return a list with the center <code>$m</code> and the covariance matrix <code>$S</code>. The default is <code>"meancov"</code> (classical mean and covariance matrix), and the option <code>"DetMCD"</code> (based on <code><a href="robustbase.html#topic+covMcd">robustbase::covMcd</a></code>) is also provided.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with components: <br />
</p>
<table>
<tr><td><code>yint</code></td>
<td>
<p>number of the given class of each case. Can contain <code>NA</code>'s.</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>given class label of each case. Can contain <code>NA</code>'s.</p>
</td></tr>
<tr><td><code>levels</code></td>
<td>
<p>levels of <code>y</code></p>
</td></tr>
<tr><td><code>predint</code></td>
<td>
<p>predicted class number of each case. For each case this is the class with the highest posterior probability. Always exists.</p>
</td></tr>
<tr><td><code>pred</code></td>
<td>
<p>predicted label of each case.</p>
</td></tr>
<tr><td><code>altint</code></td>
<td>
<p>number of the alternative class. Among the classes different from the given class, it is the one with the highest posterior probability. Is <code>NA</code> for cases whose <code>y</code> is missing.</p>
</td></tr>
<tr><td><code>altlab</code></td>
<td>
<p>label of the alternative class. Is <code>NA</code> for cases whose <code>y</code> is missing.</p>
</td></tr>
<tr><td><code>PAC</code></td>
<td>
<p>probability of the alternative class. Is <code>NA</code> for cases whose <code>y</code> is missing.</p>
</td></tr>
<tr><td><code>figparams</code></td>
<td>
<p>parameters for computing <code>fig</code>, can be used for new data.</p>
</td></tr>
<tr><td><code>fig</code></td>
<td>
<p>distance of each case <code class="reqn">i</code> from each class <code class="reqn">g</code>. Always exists.</p>
</td></tr>
<tr><td><code>farness</code></td>
<td>
<p>farness of each case from its given class. Is <code>NA</code> for cases whose <code>y</code> is missing.</p>
</td></tr>
<tr><td><code>ofarness</code></td>
<td>
<p>for each case <code>i</code>, its lowest  <code>fig[i,g]</code> to any class <code>g</code>. Always exists.</p>
</td></tr>
<tr><td><code>classMS</code></td>
<td>
<p>list with center and covariance matrix of each class</p>
</td></tr>
<tr><td><code>lCurrent</code></td>
<td>
<p>log of mixture density of each case in its given class. Is <code>NA</code> for cases with missing <code>y</code>.
</p>
</td></tr>
<tr><td><code>lPred</code></td>
<td>
<p>log of mixture density of each case in its predicted class. Always exists.
</p>
</td></tr>
<tr><td><code>lAlt</code></td>
<td>
<p>log of mixture density of each case in its alternative class. Is <code>NA</code> for cases with missing <code>y</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Raymaekers J., Rousseeuw P.J.
</p>


<h3>References</h3>

<p>Raymaekers J., Rousseeuw P.J., Hubert M. (2021). Class maps for visualizing classification results. <em>Technometrics</em>, appeared online. doi: <a href="https://doi.org/10.1080/00401706.2021.1927849">10.1080/00401706.2021.1927849</a>(link to open access pdf)
</p>


<h3>See Also</h3>

<p><code><a href="#topic+vcr.da.newdata">vcr.da.newdata</a></code>, <code><a href="#topic+classmap">classmap</a></code>, <code><a href="#topic+silplot">silplot</a></code>, <code><a href="#topic+stackedplot">stackedplot</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data("data_floralbuds")
X &lt;- data_floralbuds[, 1:6]; y &lt;- data_floralbuds[, 7]
vcrout &lt;- vcr.da.train(X, y, rule = "QDA")
# For linear discriminant analysis, put rule = "LDA".
confmat.vcr(vcrout) # There are a few outliers
cols &lt;- c("saddlebrown", "orange", "olivedrab4", "royalblue3")
stackedplot(vcrout, classCols = cols)
classmap(vcrout, "bud", classCols = cols)

# For more examples, we refer to the vignette:
## Not run: 
vignette("Discriminant_analysis_examples")

## End(Not run)
</code></pre>

<hr>
<h2 id='vcr.forest.newdata'>
Prepare for visualization of a random forest classification on new data.
</h2><span id='topic+vcr.forest.newdata'></span>

<h3>Description</h3>

<p>Produces output for the purpose of constructing graphical displays such as the <code><a href="#topic+classmap">classmap</a></code> on new data. Requires the output of
<code><a href="#topic+vcr.forest.train">vcr.forest.train</a></code> as an argument.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vcr.forest.newdata(Xnew, ynew = NULL, vcr.forest.train.out,
                   LOO = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vcr.forest.newdata_+3A_xnew">Xnew</code></td>
<td>
<p>data matrix of the new data, with the same
number of columns <code>d</code> as in the training data.
Missing values are not allowed.
</p>
</td></tr>
<tr><td><code id="vcr.forest.newdata_+3A_ynew">ynew</code></td>
<td>
<p>factor with class membership of each new case. Can be <code>NA</code> for some or all cases. If <code>NULL</code>, is assumed to be <code>NA</code> everywhere.</p>
</td></tr>
<tr><td><code id="vcr.forest.newdata_+3A_vcr.forest.train.out">vcr.forest.train.out</code></td>
<td>
<p>output of <code><a href="#topic+vcr.forest.train">vcr.forest.train</a></code> on the training data.</p>
</td></tr>
<tr><td><code id="vcr.forest.newdata_+3A_loo">LOO</code></td>
<td>
<p>leave one out. Only used when testing this function on a subset of the training data. Default is <code>LOO=FALSE</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with components: <br />
</p>
<table>
<tr><td><code>yintnew</code></td>
<td>
<p>number of the given class of each case. Can contain <code>NA</code>'s.</p>
</td></tr>
<tr><td><code>ynew</code></td>
<td>
<p>given class label of each case. Can contain <code>NA</code>'s.</p>
</td></tr>
<tr><td><code>levels</code></td>
<td>
<p>levels of the response, from <code>vcr.forest.train.out</code>.</p>
</td></tr>
<tr><td><code>predint</code></td>
<td>
<p>predicted class number of each case. Always exists.</p>
</td></tr>
<tr><td><code>pred</code></td>
<td>
<p>predicted label of each case.</p>
</td></tr>
<tr><td><code>altint</code></td>
<td>
<p>number of the alternative class. Among the classes different from the given class, it is the one with the highest posterior probability. Is <code>NA</code> for cases whose <code>ynew</code> is missing.</p>
</td></tr>
<tr><td><code>altlab</code></td>
<td>
<p>alternative label if yintnew was given, else <code>NA</code>.</p>
</td></tr>
<tr><td><code>PAC</code></td>
<td>
<p>probability of the alternative class. Is <code>NA</code> for cases whose <code>ynew</code> is missing.</p>
</td></tr>
<tr><td><code>fig</code></td>
<td>
<p>distance of each case <code class="reqn">i</code> from each class <code class="reqn">g</code>. Always exists.</p>
</td></tr>
<tr><td><code>farness</code></td>
<td>
<p>farness of each case from its given class. Is <code>NA</code> for cases whose <code>ynew</code> is missing.</p>
</td></tr>
<tr><td><code>ofarness</code></td>
<td>
<p>for each case <code class="reqn">i</code>, its lowest <code>fig[i,g]</code> to any class <code class="reqn">g</code>. Always exists.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Raymaekers J., Rousseeuw P.J.
</p>


<h3>References</h3>

<p>Raymaekers J., Rousseeuw P.J.(2021). Silhouettes and quasi residual plots for neural nets and tree-based classifiers. <a href="https://arxiv.org/abs/2106.08814">(link to open access pdf)</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+vcr.forest.train">vcr.forest.train</a></code>, <code><a href="#topic+classmap">classmap</a></code>, <code><a href="#topic+silplot">silplot</a></code>, <code><a href="#topic+stackedplot">stackedplot</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(randomForest)
data("data_instagram")
traindata &lt;- data_instagram[which(data_instagram$dataType == "train"), -13]
set.seed(71) # randomForest is not deterministic
rfout &lt;- randomForest(y ~ ., data = traindata, keep.forest = TRUE)
mytype &lt;- list(symm = c(1, 5, 7, 8)) # These 4 columns are
# (symmetric) binary variables. The variables that are not
# listed are interval-scaled by default.
x_train &lt;- traindata[, -12]
y_train &lt;- traindata[, 12]
vcrtrain &lt;- vcr.forest.train(X = x_train, y = y_train,
                            trainfit = rfout, type = mytype)
testdata &lt;- data_instagram[which(data_instagram$dataType == "test"), -13]
Xnew &lt;- testdata[, -12]
ynew &lt;- testdata[, 12]
vcrtest &lt;- vcr.forest.newdata(Xnew, ynew, vcrtrain)
confmat.vcr(vcrtest)
stackedplot(vcrtest, classCol = c(4, 2))
silplot(vcrtest, classCols = c(4, 2))
classmap(vcrtest, "genuine", classCols = c(4, 2))
classmap(vcrtest, "fake", classCols = c(4, 2))

# For more examples, we refer to the vignette:
## Not run: 
vignette("Random_forest_examples")

## End(Not run)
</code></pre>

<hr>
<h2 id='vcr.forest.train'>
Prepare for visualization of a random forest classification on training data
</h2><span id='topic+vcr.forest.train'></span>

<h3>Description</h3>

<p>Produces output for the purpose of constructing graphical displays such as the <code><a href="#topic+classmap">classmap</a></code> and <code><a href="#topic+silplot">silplot</a></code>. The user first needs to train a random forest on the data by <code><a href="randomForest.html#topic+randomForest">randomForest::randomForest</a></code>.
This then serves as an argument to  <code><a href="#topic+vcr.forest.train">vcr.forest.train</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vcr.forest.train(X, y, trainfit, type = list(),
                 k = 5, stand = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vcr.forest.train_+3A_x">X</code></td>
<td>
<p>A rectangular matrix or data frame, where the
columns (variables) may be of mixed type.
</p>
</td></tr>
<tr><td><code id="vcr.forest.train_+3A_y">y</code></td>
<td>
<p>factor with the given class labels.
It is crucial that <code>X</code> and <code>y</code> are exactly the same
as in the call to <code><a href="randomForest.html#topic+randomForest">randomForest::randomForest</a></code>.
<code>y</code> is allowed to contain <code>NA</code>'s.</p>
</td></tr>
<tr><td><code id="vcr.forest.train_+3A_trainfit">trainfit</code></td>
<td>
<p>the output of a <code><a href="randomForest.html#topic+randomForest">randomForest::randomForest</a></code> training run.</p>
</td></tr>
<tr><td><code id="vcr.forest.train_+3A_k">k</code></td>
<td>
<p>the number of nearest neighbors used in the
farness computation.</p>
</td></tr>
<tr><td><code id="vcr.forest.train_+3A_type">type</code></td>
<td>
<p>list for specifying some (or all) of the types of the
variables (columns) in <code>X</code>, used for computing the dissimilarity matrix, as in <code><a href="cluster.html#topic+daisy">cluster::daisy</a></code>.  The list may contain the following components: <code>"ordratio"</code> (ratio scaled variables to be treated as ordinal variables), <code>"logratio"</code> (ratio scaled variables that
must be logarithmically transformed), <code>"asymm"</code> (asymmetric
binary) and <code>"symm"</code> (symmetric binary variables).  Each
component's value is a vector, containing the names or the numbers
of the corresponding columns of <code>X</code>.
Variables not mentioned in the <code>type</code> list are interpreted as
usual (see argument <code>X</code>).</p>
</td></tr>
<tr><td><code id="vcr.forest.train_+3A_stand">stand</code></td>
<td>
<p>whether or not to standardize numerical (interval scaled) variables by their range as in the original <code><a href="cluster.html#topic+daisy">cluster::daisy</a></code> code for the farness computation. Defaults to <code>TRUE</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with components: <br />
</p>
<table>
<tr><td><code>X</code></td>
<td>
<p>The data used to train the forest.</p>
</td></tr>
<tr><td><code>yint</code></td>
<td>
<p>number of the given class of each case. Can contain <code>NA</code>'s.</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>given class label of each case. Can contain <code>NA</code>'s.</p>
</td></tr>
<tr><td><code>levels</code></td>
<td>
<p>levels of <code>y</code></p>
</td></tr>
<tr><td><code>predint</code></td>
<td>
<p>predicted class number of each case. For each case this is the class with the highest posterior probability. Always exists.</p>
</td></tr>
<tr><td><code>pred</code></td>
<td>
<p>predicted label of each case.</p>
</td></tr>
<tr><td><code>altint</code></td>
<td>
<p>number of the alternative class. Among the classes different from the given class, it is the one with the highest posterior probability. Is <code>NA</code> for cases whose <code>y</code> is missing.</p>
</td></tr>
<tr><td><code>altlab</code></td>
<td>
<p>label of the alternative class. Is <code>NA</code> for cases whose <code>y</code> is missing.</p>
</td></tr>
<tr><td><code>PAC</code></td>
<td>
<p>probability of the alternative class. Is <code>NA</code> for cases whose <code>y</code> is missing.</p>
</td></tr>
<tr><td><code>figparams</code></td>
<td>
<p>parameters for computing <code>fig</code>, can be used for new data.</p>
</td></tr>
<tr><td><code>fig</code></td>
<td>
<p>distance of each case <code class="reqn">i</code> from each class <code class="reqn">g</code>. Always exists.</p>
</td></tr>
<tr><td><code>farness</code></td>
<td>
<p>farness of each case from its given class. Is <code>NA</code> for cases whose <code>y</code> is missing.</p>
</td></tr>
<tr><td><code>ofarness</code></td>
<td>
<p>for each case <code>i</code>, its lowest  <code>fig[i,g]</code> to any class <code>g</code>. Always exists.</p>
</td></tr>
<tr><td><code>trainfit</code></td>
<td>
<p>The trained random forest which was given as an input to this function.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Raymaekers J., Rousseeuw P.J.
</p>


<h3>References</h3>

<p>Raymaekers J., Rousseeuw P.J.(2021). Silhouettes and quasi residual plots for neural nets and tree-based classifiers. <a href="https://arxiv.org/abs/2106.08814">(link to open access pdf)</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+vcr.forest.newdata">vcr.forest.newdata</a></code>, <code><a href="#topic+classmap">classmap</a></code>, <code><a href="#topic+silplot">silplot</a></code>, <code><a href="#topic+stackedplot">stackedplot</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(randomForest)
data("data_instagram")
traindata &lt;- data_instagram[which(data_instagram$dataType == "train"), -13]
set.seed(71) # randomForest is not deterministic
rfout &lt;- randomForest(y~., data = traindata, keep.forest = TRUE)
mytype &lt;- list(symm = c(1, 5, 7, 8)) # These 4 columns are
# (symmetric) binary variables. The variables that are not
# listed are interval-scaled by default.
x_train &lt;- traindata[, -12]
y_train &lt;- traindata[, 12]
# Prepare for visualization:
vcrtrain &lt;- vcr.forest.train(X = x_train, y = y_train,
                            trainfit = rfout, type = mytype)
confmat.vcr(vcrtrain)
stackedplot(vcrtrain, classCols = c(4, 2))
silplot(vcrtrain, classCols = c(4, 2))
classmap(vcrtrain, "genuine", classCols = c(4, 2))
classmap(vcrtrain, "fake", classCols = c(4, 2))

# For more examples, we refer to the vignette:
## Not run: 
vignette("Random_forest_examples")

## End(Not run)
</code></pre>

<hr>
<h2 id='vcr.knn.newdata'>
Carry out a k-nearest neighbor classification on new data, and prepare to visualize its results.
</h2><span id='topic+vcr.knn.newdata'></span>

<h3>Description</h3>

<p>Predicts class labels for new data by k nearest neighbors, using the output of <code><a href="#topic+vcr.knn.train">vcr.knn.train</a></code> on the training data. For cases in the new data whose given label <code>ynew</code> is not <code>NA</code>, additional output is produced for constructing graphical displays such as the <code><a href="#topic+classmap">classmap</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vcr.knn.newdata(Xnew, ynew = NULL, vcr.knn.train.out, LOO = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vcr.knn.newdata_+3A_xnew">Xnew</code></td>
<td>
<p>If the training data was a matrix of coordinates, <code>Xnew</code> must be such a matrix with the same number of columns. If the training data was a set of dissimilarities, <code>Xnew</code> must be a rectangular matrix of dissimilarities, with each row containing the dissmilarities of a new case to all training cases. Missing values are not allowed.</p>
</td></tr>
<tr><td><code id="vcr.knn.newdata_+3A_ynew">ynew</code></td>
<td>
<p>factor with class membership of each new case. Can be <code>NA</code> for some or all cases. If <code>NULL</code>, is assumed to be <code>NA</code> everywhere.</p>
</td></tr>
<tr><td><code id="vcr.knn.newdata_+3A_vcr.knn.train.out">vcr.knn.train.out</code></td>
<td>
<p>output of <code><a href="#topic+vcr.knn.train">vcr.knn.train</a></code> on the training data.</p>
</td></tr>
<tr><td><code id="vcr.knn.newdata_+3A_loo">LOO</code></td>
<td>
<p>leave one out. Only used when testing this function on a subset of the training data. Default is <code>LOO=FALSE</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with components: <br />
</p>
<table>
<tr><td><code>yintnew</code></td>
<td>
<p>number of the given class of each case. Can contain <code>NA</code>'s.</p>
</td></tr>
<tr><td><code>ynew</code></td>
<td>
<p>given class label of each case. Can contain <code>NA</code>'s.</p>
</td></tr>
<tr><td><code>levels</code></td>
<td>
<p>levels of the response, from <code>vcr.knn.train.out</code>.</p>
</td></tr>
<tr><td><code>predint</code></td>
<td>
<p>predicted class number of each case. Always exists.</p>
</td></tr>
<tr><td><code>pred</code></td>
<td>
<p>predicted label of each case.</p>
</td></tr>
<tr><td><code>altint</code></td>
<td>
<p>number of the alternative class. Among the classes different from the given class, it is the one with the highest posterior probability. Is <code>NA</code> for cases whose <code>ynew</code> is missing.</p>
</td></tr>
<tr><td><code>altlab</code></td>
<td>
<p>label of the alternative class. Is <code>NA</code> for cases whose <code>ynew</code> is missing.</p>
</td></tr>
<tr><td><code>PAC</code></td>
<td>
<p>probability of the alternative class. Is <code>NA</code> for cases whose <code>ynew</code> is missing.</p>
</td></tr>
<tr><td><code>fig</code></td>
<td>
<p>distance of each case <code class="reqn">i</code> from each class <code class="reqn">g</code>. Always exists.</p>
</td></tr>
<tr><td><code>farness</code></td>
<td>
<p>farness of each case from its given class. Is <code>NA</code> for cases whose <code>ynew</code> is missing.</p>
</td></tr>
<tr><td><code>ofarness</code></td>
<td>
<p>for each case <code>i</code>, its lowest  <code>fig[i,g]</code> to any class <code>g</code>. Always exists.</p>
</td></tr>
<tr><td><code>k</code></td>
<td>
<p>the requested number of nearest neighbors, from <code>vcr.knn.train.out</code>.</p>
</td></tr>
<tr><td><code>ktrues</code></td>
<td>
<p>for each case this contains the actual number of elements in its neighborhood. This can be higher than <code>k</code> due to ties.</p>
</td></tr>
<tr><td><code>counts</code></td>
<td>
<p>a matrix with 3 columns, each row representing a case. For the neighborhood of each case it says how many members it has from the given class, the predicted class, and the alternative class. The first and third entry is <code>NA</code> for cases whose <code>ynew</code> is missing.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Raymaekers J., Rousseeuw P.J.
</p>


<h3>References</h3>

<p>Raymaekers J., Rousseeuw P.J., Hubert M. (2021). Class maps for visualizing classification results. <em>Technometrics</em>, appeared online. doi: <a href="https://doi.org/10.1080/00401706.2021.1927849">10.1080/00401706.2021.1927849</a>(link to open access pdf)
</p>


<h3>See Also</h3>

<p><code><a href="#topic+vcr.knn.train">vcr.knn.train</a></code>, <code><a href="#topic+classmap">classmap</a></code>, <code><a href="#topic+silplot">silplot</a></code>, <code><a href="#topic+stackedplot">stackedplot</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("data_floralbuds")
X &lt;- data_floralbuds[, 1:6]; y &lt;- data_floralbuds[, 7]
set.seed(12345); trainset &lt;- sample(1:550, 275)
vcr.train &lt;- vcr.knn.train(X[trainset, ], y[trainset], k = 5)
vcr.test &lt;- vcr.knn.newdata(X[-trainset, ], y[-trainset], vcr.train)
confmat.vcr(vcr.train) # for comparison
confmat.vcr(vcr.test)
cols &lt;- c("saddlebrown", "orange", "olivedrab4", "royalblue3")
stackedplot(vcr.train, classCols = cols) # for comparison
stackedplot(vcr.test, classCols = cols)
classmap(vcr.train, "bud", classCols = cols) # for comparison
classmap(vcr.test, "bud", classCols = cols)

# For more examples, we refer to the vignette:
## Not run: 
vignette("K_nearest_neighbors_examples")

## End(Not run)
</code></pre>

<hr>
<h2 id='vcr.knn.train'>
Carry out a k-nearest neighbor classification on training data, and prepare to visualize its results.
</h2><span id='topic+vcr.knn.train'></span>

<h3>Description</h3>

<p>Carries out a k-nearest neighbor classification on the training data. Various additional output is produced for the purpose of constructing graphical displays such as the <code><a href="#topic+classmap">classmap</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vcr.knn.train(X, y, k)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vcr.knn.train_+3A_x">X</code></td>
<td>
<p>This can be a rectangular matrix or data frame of (already standardized) measurements, or a dist object obtained from <code><a href="stats.html#topic+dist">stats::dist</a></code> or <code><a href="cluster.html#topic+daisy">cluster::daisy</a></code>. Missing values are not allowed.
</p>
</td></tr>
<tr><td><code id="vcr.knn.train_+3A_y">y</code></td>
<td>
<p>factor with the given (observed) class labels. There need to be non-missing <code>y</code> in order to be able to train the classifier.</p>
</td></tr>
<tr><td><code id="vcr.knn.train_+3A_k">k</code></td>
<td>
<p>the number of nearest neighbors used. It can be selected by running cross-validation using a different package.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with components: <br />
</p>
<table>
<tr><td><code>yint</code></td>
<td>
<p>number of the given class of each case. Can contain <code>NA</code>'s.</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>given class label of each case. Can contain <code>NA</code>'s.</p>
</td></tr>
<tr><td><code>levels</code></td>
<td>
<p>levels of <code>y</code></p>
</td></tr>
<tr><td><code>predint</code></td>
<td>
<p>predicted class number of each case. Always exists.</p>
</td></tr>
<tr><td><code>pred</code></td>
<td>
<p>predicted label of each case.</p>
</td></tr>
<tr><td><code>altint</code></td>
<td>
<p>number of the alternative class. Among the classes different from the given class, it is the one with the highest posterior probability. Is <code>NA</code> for cases whose <code>y</code> is missing.</p>
</td></tr>
<tr><td><code>altlab</code></td>
<td>
<p>label of the alternative class. Is <code>NA</code> for cases whose <code>y</code> is missing.</p>
</td></tr>
<tr><td><code>PAC</code></td>
<td>
<p>probability of the alternative class. Is <code>NA</code> for cases whose <code>y</code> is missing.</p>
</td></tr>
<tr><td><code>figparams</code></td>
<td>
<p>parameters used to compute <code>fig</code>.</p>
</td></tr>
<tr><td><code>fig</code></td>
<td>
<p>distance of each case <code class="reqn">i</code> from each class <code class="reqn">g</code>. Always exists.</p>
</td></tr>
<tr><td><code>farness</code></td>
<td>
<p>farness of each case from its given class. Is <code>NA</code> for cases whose <code>y</code> is missing.</p>
</td></tr>
<tr><td><code>ofarness</code></td>
<td>
<p>for each case <code class="reqn">i</code>, its lowest  <code>fig[i,g]</code> to any class <code class="reqn">g</code>. Always exists.</p>
</td></tr>
<tr><td><code>k</code></td>
<td>
<p>the requested number of nearest neighbors, from the arguments. Will also be used for  classifying new data.</p>
</td></tr>
<tr><td><code>ktrues</code></td>
<td>
<p>for each case this contains the actual number of elements in its neighborhood. This can be higher than <code>k</code> due to ties.</p>
</td></tr>
<tr><td><code>counts</code></td>
<td>
<p>a matrix with 3 columns, each row representing a case. For the neighborhood of each case it says how many members it has from the given class, the predicted class, and the alternative class. The first and third entry is <code>NA</code> for cases whose <code>y</code> is missing.</p>
</td></tr>
<tr><td><code>X</code></td>
<td>
<p>If the argument <code>X</code> was a data frame or matrix of coordinates, <code>as.matrix(X)</code> is returned here. This is useful for classifying new data.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Raymaekers J., Rousseeuw P.J.
</p>


<h3>References</h3>

<p>Raymaekers J., Rousseeuw P.J., Hubert M. (2021). Class maps for visualizing classification results. <em>Technometrics</em>, appeared online. doi: <a href="https://doi.org/10.1080/00401706.2021.1927849">10.1080/00401706.2021.1927849</a>(link to open access pdf)
</p>


<h3>See Also</h3>

<p><code><a href="#topic+vcr.knn.newdata">vcr.knn.newdata</a></code>, <code><a href="#topic+classmap">classmap</a></code>, <code><a href="#topic+silplot">silplot</a></code>, <code><a href="#topic+stackedplot">stackedplot</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>vcrout &lt;- vcr.knn.train(iris[, 1:4], iris[, 5], k = 5)
confmat.vcr(vcrout)
stackedplot(vcrout)
classmap(vcrout, "versicolor", classCols = 2:4)
# The cases misclassified as virginica are shown in blue.

# For more examples, we refer to the vignette:
## Not run: 
vignette("K_nearest_neighbors_examples")

## End(Not run)
</code></pre>

<hr>
<h2 id='vcr.neural.newdata'>
Prepare for visualization of a neural network classification on new data.
</h2><span id='topic+vcr.neural.newdata'></span>

<h3>Description</h3>

<p>Prepares graphical display of new data fitted by a neural
net that was modeled on the training data, using the output
of <code><a href="#topic+vcr.neural.train">vcr.neural.train</a></code> on the training data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vcr.neural.newdata(Xnew, ynew = NULL, probs,
                   vcr.neural.train.out)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vcr.neural.newdata_+3A_xnew">Xnew</code></td>
<td>
<p>data matrix of the new data, with the same number of columns as in the training data. Missing values in <code>Xnew</code> are not allowed.</p>
</td></tr>
<tr><td><code id="vcr.neural.newdata_+3A_ynew">ynew</code></td>
<td>
<p>factor with class membership of each new case. Can be <code>NA</code> for some or all cases. If <code>NULL</code>, is assumed to be <code>NA</code> everywhere.</p>
</td></tr>
<tr><td><code id="vcr.neural.newdata_+3A_probs">probs</code></td>
<td>
<p>posterior probabilities obtained by running the neural net on the new data.</p>
</td></tr>
<tr><td><code id="vcr.neural.newdata_+3A_vcr.neural.train.out">vcr.neural.train.out</code></td>
<td>
<p> output of <code><a href="#topic+vcr.neural.train">vcr.neural.train</a></code> on the training data.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with components: <br />
</p>
<table>
<tr><td><code>yintnew</code></td>
<td>
<p>number of the given class of each case. Can contain <code>NA</code>'s.</p>
</td></tr>
<tr><td><code>ynew</code></td>
<td>
<p>given class label of each case. Can contain <code>NA</code>'s.</p>
</td></tr>
<tr><td><code>levels</code></td>
<td>
<p>levels of the response, from <code>vcr.svm.train.out</code>.</p>
</td></tr>
<tr><td><code>predint</code></td>
<td>
<p>predicted class number of each case. Always exists.</p>
</td></tr>
<tr><td><code>pred</code></td>
<td>
<p>predicted label of each case.</p>
</td></tr>
<tr><td><code>altint</code></td>
<td>
<p>number of the alternative class. Among the classes different from the given class, it is the one with the highest posterior probability. Is <code>NA</code> for cases whose <code>ynew</code> is missing.</p>
</td></tr>
<tr><td><code>altlab</code></td>
<td>
<p>alternative label if yintnew was given, else <code>NA</code>.</p>
</td></tr>
<tr><td><code>PAC</code></td>
<td>
<p>probability of the alternative class. Is <code>NA</code> for cases whose <code>ynew</code> is missing.</p>
</td></tr>
<tr><td><code>fig</code></td>
<td>
<p>distance of each case <code class="reqn">i</code> from each class <code class="reqn">g</code>. Always exists.</p>
</td></tr>
<tr><td><code>farness</code></td>
<td>
<p>farness of each case from its given class. Is <code>NA</code> for cases whose <code>ynew</code> is missing.</p>
</td></tr>
<tr><td><code>ofarness</code></td>
<td>
<p>for each case <code class="reqn">i</code>, its lowest <code>fig[i,g]</code> to any class <code class="reqn">g</code>. Always exists.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Raymaekers J., Rousseeuw P.J.
</p>


<h3>References</h3>

<p>Raymaekers J., Rousseeuw P.J.(2021). Silhouettes and quasi residual plots for neural nets and tree-based classifiers. <a href="https://arxiv.org/abs/2106.08814">(link to open access pdf)</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+vcr.neural.train">vcr.neural.train</a></code>, <code><a href="#topic+classmap">classmap</a></code>, <code><a href="#topic+silplot">silplot</a></code>, <code><a href="#topic+stackedplot">stackedplot</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># For examples, we refer to the vignette:
## Not run: 
vignette("Neural_net_examples")

## End(Not run)
</code></pre>

<hr>
<h2 id='vcr.neural.train'>
Prepare for visualization of a neural network classification on training data.
</h2><span id='topic+vcr.neural.train'></span>

<h3>Description</h3>

<p>Produces output for the purpose of constructing graphical displays such as the <code><a href="#topic+classmap">classmap</a></code>. The user first needs train a neural network. The representation of the data in a given layer (e.g. the final layer before applying the softmax function) then serves as the argument <code>X</code> to  <code><a href="#topic+vcr.neural.train">vcr.neural.train</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vcr.neural.train(X, y, probs, estmethod = meancov)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vcr.neural.train_+3A_x">X</code></td>
<td>
<p>the coordinates of the <code>n</code> objects of the training
data, in the layer chosen by the user. Missing
values are not allowed.
</p>
</td></tr>
<tr><td><code id="vcr.neural.train_+3A_y">y</code></td>
<td>
<p>factor with the given class labels of the objects.
Make sure that the levels are in the same order as
used in the neural net, i.e. the columns of its
binary &quot;once-hot-encoded&quot; response vectors.</p>
</td></tr>
<tr><td><code id="vcr.neural.train_+3A_probs">probs</code></td>
<td>
<p>posterior probabilities obtained by the neural
net, e.g. in keras. For each case (row of <code>X</code>),
the classes have probabilities that add up to 1.
Each row of the matrix probs contains these
probabilities. The columns of probs must be in
the same order as the levels of <code>y</code>.</p>
</td></tr>
<tr><td><code id="vcr.neural.train_+3A_estmethod">estmethod</code></td>
<td>
<p>function for location and covariance estimation.
Should return a list with <code>$m</code> and <code>$S</code>.
Can be <code>meancov</code>
(classical mean and covariance matrix) or <code>DetMCD</code>.
If one or more classes have a singular covariance
matrix, the function automatically switches to
the PCA-based farness used in <code><a href="#topic+vcr.svm.train">vcr.svm.train</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with components: <br />
</p>
<table>
<tr><td><code>X</code></td>
<td>
<p>the coordinates of the <code>n</code> objects of the training
data, in the layer chosen by the user.
</p>
</td></tr>
<tr><td><code>yint</code></td>
<td>
<p>number of the given class of each case. Can contain <code>NA</code>'s.</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>given class label of each case. Can contain <code>NA</code>'s.</p>
</td></tr>
<tr><td><code>levels</code></td>
<td>
<p>levels of <code>y</code></p>
</td></tr>
<tr><td><code>predint</code></td>
<td>
<p>predicted class number of each case. For each case this is the class with the highest posterior probability. Always exists.</p>
</td></tr>
<tr><td><code>pred</code></td>
<td>
<p>predicted label of each case.</p>
</td></tr>
<tr><td><code>altint</code></td>
<td>
<p>number of the alternative class. Among the classes different from the given class, it is the one with the highest posterior probability. Is <code>NA</code> for cases whose <code>y</code> is missing.</p>
</td></tr>
<tr><td><code>altlab</code></td>
<td>
<p>label of the alternative class. Is <code>NA</code> for cases whose <code>y</code> is missing.</p>
</td></tr>
<tr><td><code>ncolX</code></td>
<td>
<p>number of columns in <code>X</code>.  Keep??</p>
</td></tr>
<tr><td><code>PAC</code></td>
<td>
<p>probability of the alternative class. Is <code>NA</code> for cases whose <code>y</code> is missing.</p>
</td></tr>
<tr><td><code>computeMD</code></td>
<td>
<p>Whether or not the farness is computed using the Mahalanobis distance.</p>
</td></tr>
<tr><td><code>classMS</code></td>
<td>
<p>list with center and covariance matrix of each class</p>
</td></tr>
<tr><td><code>PCAfits</code></td>
<td>
<p>if not <code>NULL</code>, PCA fits to each class, estimated from the training data but also useful for new data.</p>
</td></tr>
<tr><td><code>figparams</code></td>
<td>
<p>parameters for computing <code>fig</code>, can be used for new data.</p>
</td></tr>
<tr><td><code>fig</code></td>
<td>
<p>distance of each case <code class="reqn">i</code> from each class <code class="reqn">g</code>. Always exists.</p>
</td></tr>
<tr><td><code>farness</code></td>
<td>
<p>farness of each case from its given class. Is <code>NA</code> for cases whose <code>y</code> is missing.</p>
</td></tr>
<tr><td><code>ofarness</code></td>
<td>
<p>for each case <code>i</code>, its lowest  <code>fig[i,g]</code> to any class <code>g</code>. Always exists.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Raymaekers J., Rousseeuw P.J.
</p>


<h3>References</h3>

<p>Raymaekers J., Rousseeuw P.J.(2021). Silhouettes and quasi residual plots for neural nets and tree-based classifiers. <a href="https://arxiv.org/abs/2106.08814">(link to open access pdf)</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+vcr.neural.newdata">vcr.neural.newdata</a></code>, <code><a href="#topic+classmap">classmap</a></code>, <code><a href="#topic+silplot">silplot</a></code>, <code><a href="#topic+stackedplot">stackedplot</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># For examples, we refer to the vignette:
## Not run: 
vignette("Neural_net_examples")

## End(Not run)
</code></pre>

<hr>
<h2 id='vcr.rpart.newdata'>
Prepare for visualization of an rpart classification on new data.
</h2><span id='topic+vcr.rpart.newdata'></span>

<h3>Description</h3>

<p>Produces output for the purpose of constructing graphical displays such as the <code><a href="#topic+classmap">classmap</a></code> on new data. Requires the output of
<code><a href="#topic+vcr.rpart.train">vcr.rpart.train</a></code> as an argument.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vcr.rpart.newdata(Xnew, ynew = NULL, vcr.rpart.train.out,
                  LOO = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vcr.rpart.newdata_+3A_xnew">Xnew</code></td>
<td>
<p>data matrix of the new data, with the same
number of columns <code>d</code> as in the training data. Missing values are not allowed.
</p>
</td></tr>
<tr><td><code id="vcr.rpart.newdata_+3A_ynew">ynew</code></td>
<td>
<p>factor with class membership of each new case. Can be <code>NA</code> for some or all cases. If <code>NULL</code>, is assumed to be <code>NA</code> everywhere.</p>
</td></tr>
<tr><td><code id="vcr.rpart.newdata_+3A_vcr.rpart.train.out">vcr.rpart.train.out</code></td>
<td>
<p>output of <code><a href="#topic+vcr.rpart.train">vcr.rpart.train</a></code> on the training data.</p>
</td></tr>
<tr><td><code id="vcr.rpart.newdata_+3A_loo">LOO</code></td>
<td>
<p>leave one out. Only used when testing this function on a subset of the training data. Default is <code>LOO=FALSE</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with components: <br />
</p>
<table>
<tr><td><code>yintnew</code></td>
<td>
<p>number of the given class of each case. Can contain <code>NA</code>'s.</p>
</td></tr>
<tr><td><code>ynew</code></td>
<td>
<p>given class label of each case. Can contain <code>NA</code>'s.</p>
</td></tr>
<tr><td><code>levels</code></td>
<td>
<p>levels of the response, from <code>vcr.rpart.train.out</code>.</p>
</td></tr>
<tr><td><code>predint</code></td>
<td>
<p>predicted class number of each case. Always exists.</p>
</td></tr>
<tr><td><code>pred</code></td>
<td>
<p>predicted label of each case.</p>
</td></tr>
<tr><td><code>altint</code></td>
<td>
<p>number of the alternative class. Among the classes different from the given class, it is the one with the highest posterior probability. Is <code>NA</code> for cases whose <code>ynew</code> is missing.</p>
</td></tr>
<tr><td><code>altlab</code></td>
<td>
<p>alternative label if yintnew was given, else <code>NA</code>.</p>
</td></tr>
<tr><td><code>PAC</code></td>
<td>
<p>probability of the alternative class. Is <code>NA</code> for cases whose <code>ynew</code> is missing.</p>
</td></tr>
<tr><td><code>fig</code></td>
<td>
<p>distance of each case <code class="reqn">i</code> from each class <code class="reqn">g</code>. Always exists.</p>
</td></tr>
<tr><td><code>farness</code></td>
<td>
<p>farness of each case from its given class. Is <code>NA</code> for cases whose <code>ynew</code> is missing.</p>
</td></tr>
<tr><td><code>ofarness</code></td>
<td>
<p>for each case <code class="reqn">i</code>, its lowest <code>fig[i,g]</code> to any class <code class="reqn">g</code>. Always exists.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Raymaekers J., Rousseeuw P.J.
</p>


<h3>References</h3>

<p>Raymaekers J., Rousseeuw P.J.(2021). Silhouettes and quasi residual plots for neural nets and tree-based classifiers. <a href="https://arxiv.org/abs/2106.08814">(link to open access pdf)</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+vcr.rpart.train">vcr.rpart.train</a></code>, <code><a href="#topic+classmap">classmap</a></code>, <code><a href="#topic+silplot">silplot</a></code>, <code><a href="#topic+stackedplot">stackedplot</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(rpart)
data("data_titanic")
traindata &lt;- data_titanic[which(data_titanic$dataType == "train"), -13]
str(traindata); table(traindata$y)
set.seed(123) # rpart is not deterministic
rpart.out &lt;- rpart(y ~ Pclass + Sex + SibSp +
                    Parch + Fare + Embarked,
                  data = traindata, method = 'class', model = TRUE)
y_train &lt;- traindata[, 12]
x_train &lt;- traindata[, -12]
mytype &lt;- list(nominal = c("Name", "Sex", "Ticket", "Cabin", "Embarked"), ordratio = c("Pclass"))
# These are 5 nominal columns, and one ordinal.
# The variables not listed are by default interval-scaled.
vcrtrain &lt;- vcr.rpart.train(x_train, y_train, rpart.out, mytype)
testdata &lt;- data_titanic[which(data_titanic$dataType == "test"), -13]
dim(testdata)
x_test &lt;- testdata[, -12]
y_test &lt;- testdata[, 12]
vcrtest &lt;- vcr.rpart.newdata(x_test, y_test, vcrtrain)
confmat.vcr(vcrtest)
silplot(vcrtest, classCols = c(2, 4))
classmap(vcrtest, "casualty", classCols = c(2, 4))
classmap(vcrtest, "survived", classCols = c(2, 4))

# For more examples, we refer to the vignette:
## Not run: 
vignette("Rpart_examples")

## End(Not run)
</code></pre>

<hr>
<h2 id='vcr.rpart.train'>
Prepare for visualization of an rpart classification on training data.
</h2><span id='topic+vcr.rpart.train'></span>

<h3>Description</h3>

<p>Produces output for the purpose of constructing graphical displays such as the <code><a href="#topic+classmap">classmap</a></code>. The user first needs to train a
classification tree on the data by <code><a href="rpart.html#topic+rpart">rpart::rpart</a></code>.
This then serves as an argument to  <code><a href="#topic+vcr.rpart.train">vcr.rpart.train</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vcr.rpart.train(X, y, trainfit, type = list(),
                k = 5, stand = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vcr.rpart.train_+3A_x">X</code></td>
<td>
<p>A rectangular matrix or data frame, where the
columns (variables) may be of mixed type and
may contain <code>NA</code>'s.
</p>
</td></tr>
<tr><td><code id="vcr.rpart.train_+3A_y">y</code></td>
<td>
<p>factor with the given class labels.
It is crucial that <code>X</code> and <code>y</code> are exactly the same
as in the call to <code><a href="rpart.html#topic+rpart">rpart::rpart</a></code>.
<code>y</code> is allowed to contain <code>NA</code>'s.</p>
</td></tr>
<tr><td><code id="vcr.rpart.train_+3A_k">k</code></td>
<td>
<p>the number of nearest neighbors used in the
farness computation.</p>
</td></tr>
<tr><td><code id="vcr.rpart.train_+3A_trainfit">trainfit</code></td>
<td>
<p>the output of an <code><a href="rpart.html#topic+rpart">rpart::rpart</a></code> training cycle.</p>
</td></tr>
<tr><td><code id="vcr.rpart.train_+3A_type">type</code></td>
<td>
<p>list for specifying some (or all) of the types of the
variables (columns) in <code>X</code>, used for computing the dissimilarity matrix, as in <code><a href="cluster.html#topic+daisy">cluster::daisy</a></code>.  The list may contain the following components: <code>"ordratio"</code> (ratio scaled variables to be treated as ordinal variables), <code>"logratio"</code> (ratio scaled variables that
must be logarithmically transformed), <code>"asymm"</code> (asymmetric
binary) and <code>"symm"</code> (symmetric binary variables).  Each
component's value is a vector, containing the names or the numbers
of the corresponding columns of <code>X</code>.
Variables not mentioned in the <code>type</code> list are interpreted as
usual (see argument <code>X</code>).</p>
</td></tr>
<tr><td><code id="vcr.rpart.train_+3A_stand">stand</code></td>
<td>
<p>whether or not to standardize numerical (interval scaled) variables by their range as in the original <code><a href="cluster.html#topic+daisy">cluster::daisy</a></code> code for the farness computation. Defaults to <code>TRUE</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with components: <br />
</p>
<table>
<tr><td><code>X</code></td>
<td>
<p>The input data <code>X</code>. Keep??</p>
</td></tr>
<tr><td><code>yint</code></td>
<td>
<p>number of the given class of each case. Can contain <code>NA</code>'s.</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>given class label of each case. Can contain <code>NA</code>'s.</p>
</td></tr>
<tr><td><code>levels</code></td>
<td>
<p>levels of <code>y</code></p>
</td></tr>
<tr><td><code>predint</code></td>
<td>
<p>predicted class number of each case. For each case this is the class with the highest posterior probability. Always exists.</p>
</td></tr>
<tr><td><code>pred</code></td>
<td>
<p>predicted label of each case.</p>
</td></tr>
<tr><td><code>altint</code></td>
<td>
<p>number of the alternative class. Among the classes different from the given class, it is the one with the highest posterior probability. Is <code>NA</code> for cases whose <code>y</code> is missing.</p>
</td></tr>
<tr><td><code>altlab</code></td>
<td>
<p>label of the alternative class. Is <code>NA</code> for cases whose <code>y</code> is missing.</p>
</td></tr>
<tr><td><code>PAC</code></td>
<td>
<p>probability of the alternative class. Is <code>NA</code> for cases whose <code>y</code> is missing.</p>
</td></tr>
<tr><td><code>figparams</code></td>
<td>
<p>parameters for computing <code>fig</code>, can be used for new data.</p>
</td></tr>
<tr><td><code>fig</code></td>
<td>
<p>distance of each case <code class="reqn">i</code> from each class <code class="reqn">g</code>. Always exists.</p>
</td></tr>
<tr><td><code>farness</code></td>
<td>
<p>farness of each case from its given class. Is <code>NA</code> for cases whose <code>y</code> is missing.</p>
</td></tr>
<tr><td><code>ofarness</code></td>
<td>
<p>for each case <code>i</code>, its lowest  <code>fig[i,g]</code> to any class <code>g</code>. Always exists.</p>
</td></tr>
<tr><td><code>trainfit</code></td>
<td>
<p>the trainfit used to build the VCR object.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Raymaekers J., Rousseeuw P.J.
</p>


<h3>References</h3>

<p>Raymaekers J., Rousseeuw P.J.(2021). Silhouettes and quasi residual plots for neural nets and tree-based classifiers. <a href="https://arxiv.org/abs/2106.08814">(link to open access pdf)</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+vcr.rpart.newdata">vcr.rpart.newdata</a></code>, <code><a href="#topic+classmap">classmap</a></code>, <code><a href="#topic+silplot">silplot</a></code>, <code><a href="#topic+stackedplot">stackedplot</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(rpart)
data("data_titanic")
traindata &lt;- data_titanic[which(data_titanic$dataType == "train"), -13]
str(traindata); table(traindata$y)
set.seed(123) # rpart is not deterministic
rpart.out &lt;- rpart(y ~ Pclass + Sex + SibSp +
                    Parch + Fare + Embarked,
                  data = traindata, method = 'class', model = TRUE)
y_train &lt;- traindata[, 12]
x_train &lt;- traindata[, -12]
mytype &lt;- list(nominal = c("Name", "Sex", "Ticket", "Cabin", "Embarked"), ordratio = c("Pclass"))
# These are 5 nominal columns, and one ordinal.
# The variables not listed are by default interval-scaled.
vcrtrain &lt;- vcr.rpart.train(x_train, y_train, rpart.out, mytype)
confmat.vcr(vcrtrain)
silplot(vcrtrain, classCols = c(2, 4))
classmap(vcrtrain, "casualty", classCols = c(2, 4))
classmap(vcrtrain, "survived", classCols = c(2, 4))

# For more examples, we refer to the vignette:
## Not run: 
vignette("Rpart_examples")

## End(Not run)
</code></pre>

<hr>
<h2 id='vcr.svm.newdata'>
Prepare for visualization of a support vector machine classification on new data.
</h2><span id='topic+vcr.svm.newdata'></span>

<h3>Description</h3>

<p>Carries out a support vector machine classification of new data using the output of <code><a href="#topic+vcr.svm.train">vcr.svm.train</a></code> on the training data, and computes the quantities needed for its visualization.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vcr.svm.newdata(Xnew, ynew = NULL, vcr.svm.train.out)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vcr.svm.newdata_+3A_xnew">Xnew</code></td>
<td>
<p>data matrix of the new data, with the same number of columns as in the training data. Missing values in <code>Xnew</code> are not allowed.</p>
</td></tr>
<tr><td><code id="vcr.svm.newdata_+3A_ynew">ynew</code></td>
<td>
<p>factor with class membership of each new case. Can be <code>NA</code> for some or all cases. If <code>NULL</code>, is assumed to be <code>NA</code> everywhere.</p>
</td></tr>
<tr><td><code id="vcr.svm.newdata_+3A_vcr.svm.train.out">vcr.svm.train.out</code></td>
<td>
<p> output of <code><a href="#topic+vcr.svm.train">vcr.svm.train</a></code> on the training data.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with components: <br />
</p>
<table>
<tr><td><code>yintnew</code></td>
<td>
<p>number of the given class of each case. Can contain <code>NA</code>'s.</p>
</td></tr>
<tr><td><code>ynew</code></td>
<td>
<p>given class label of each case. Can contain <code>NA</code>'s.</p>
</td></tr>
<tr><td><code>levels</code></td>
<td>
<p>levels of the response, from <code>vcr.svm.train.out</code>.</p>
</td></tr>
<tr><td><code>predint</code></td>
<td>
<p>predicted class number of each case. Always exists.</p>
</td></tr>
<tr><td><code>pred</code></td>
<td>
<p>predicted label of each case.</p>
</td></tr>
<tr><td><code>altint</code></td>
<td>
<p>number of the alternative class. Among the classes different from the given class, it is the one with the highest posterior probability. Is <code>NA</code> for cases whose <code>ynew</code> is missing.</p>
</td></tr>
<tr><td><code>altlab</code></td>
<td>
<p>alternative label if yintnew was given, else <code>NA</code>.</p>
</td></tr>
<tr><td><code>PAC</code></td>
<td>
<p>probability of the alternative class. Is <code>NA</code> for cases whose <code>ynew</code> is missing.</p>
</td></tr>
<tr><td><code>fig</code></td>
<td>
<p>distance of each case <code class="reqn">i</code> from each class <code class="reqn">g</code>. Always exists.</p>
</td></tr>
<tr><td><code>farness</code></td>
<td>
<p>farness of each case from its given class. Is <code>NA</code> for cases whose <code>ynew</code> is missing.</p>
</td></tr>
<tr><td><code>ofarness</code></td>
<td>
<p>for each case <code class="reqn">i</code>, its lowest <code>fig[i,g]</code> to any class <code class="reqn">g</code>. Always exists.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Raymaekers J., Rousseeuw P.J.
</p>


<h3>References</h3>

<p>Raymaekers J., Rousseeuw P.J., Hubert M. (2021). Class maps for visualizing classification results. <em>Technometrics</em>, appeared online. doi: <a href="https://doi.org/10.1080/00401706.2021.1927849">10.1080/00401706.2021.1927849</a>(link to open access pdf)
</p>


<h3>See Also</h3>

<p><code><a href="#topic+vcr.svm.train">vcr.svm.train</a></code>, <code><a href="#topic+classmap">classmap</a></code>, <code><a href="#topic+silplot">silplot</a></code>, <code><a href="#topic+stackedplot">stackedplot</a></code>, <code><a href="e1071.html#topic+svm">e1071::svm</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(e1071)
set.seed(1); X &lt;- matrix(rnorm(200 * 2), ncol = 2)
X[1:100, ] &lt;- X[1:100, ] + 2
X[101:150, ] &lt;- X[101:150, ] - 2
y &lt;- as.factor(c(rep("blue", 150), rep("red", 50)))
# We now fit an SVM with radial basis kernel to the data:
set.seed(1) # to make the result of svm() reproducible.
svmfit &lt;- svm(y~., data = data.frame(X = X, y = y),
scale = FALSE, kernel = "radial", cost = 10,
gamma = 1, probability = TRUE)
vcr.train &lt;- vcr.svm.train(X, y, svfit = svmfit)
# As "new" data we take a subset of the training data:
inds &lt;- c(1:25, 101:125, 151:175)
vcr.test &lt;- vcr.svm.newdata(X[inds, ], y[inds], vcr.train)
plot(vcr.test$PAC, vcr.train$PAC[inds]); abline(0, 1) # match
plot(vcr.test$farness, vcr.train$farness[inds]); abline(0, 1)
confmat.vcr(vcr.test)
cols &lt;- c("deepskyblue3", "red")
stackedplot(vcr.test, classCols = cols)
classmap(vcr.train, "blue", classCols = cols) # for comparison
classmap(vcr.test, "blue", classCols = cols)
classmap(vcr.train, "red", classCols = cols) # for comparison
classmap(vcr.test, "red", classCols = cols)


# For more examples, we refer to the vignette:
## Not run: 
vignette("Support_vector_machine_examples")

## End(Not run)
</code></pre>

<hr>
<h2 id='vcr.svm.train'>
Prepare for visualization of a support vector machine classification on training data.
</h2><span id='topic+vcr.svm.train'></span>

<h3>Description</h3>

<p>Produces output for the purpose of constructing graphical displays such as the <code><a href="#topic+classmap">classmap</a></code>. The user first needs to run a support vector machine classification on the data by <code><a href="e1071.html#topic+svm">e1071::svm</a></code>, with the option <code>probability = TRUE</code>. This classification can be with two or more classes. The output of <code><a href="e1071.html#topic+svm">e1071::svm</a></code> is then an argument to <code><a href="#topic+vcr.svm.train">vcr.svm.train</a></code>. As <code><a href="e1071.html#topic+svm">e1071::svm</a></code> does not output the data itself, it needs to be given as well, in the arguments <code>X</code> and <code>y</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vcr.svm.train(X, y, svfit, ortho = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vcr.svm.train_+3A_x">X</code></td>
<td>
<p>matrix of data coordinates, as used in <code><a href="e1071.html#topic+svm">e1071::svm</a></code>. Missing values are not allowed.</p>
</td></tr>
<tr><td><code id="vcr.svm.train_+3A_y">y</code></td>
<td>
<p>factor with the given (observed) class labels. It is crucial that X and y are exactly the same as in the call to <code><a href="e1071.html#topic+svm">e1071::svm</a></code>.</p>
</td></tr>
<tr><td><code id="vcr.svm.train_+3A_svfit">svfit</code></td>
<td>
<p>an object returned by <code><a href="e1071.html#topic+svm">e1071::svm</a></code>, called with exactly the same <code>X</code> and <code>y</code> as above.</p>
</td></tr>
<tr><td><code id="vcr.svm.train_+3A_ortho">ortho</code></td>
<td>
<p>If <code>TRUE</code>, will compute farness in the orthogonal complement of the vector beta given by <code><a href="e1071.html#topic+svm">e1071::svm</a></code>. Is only possible for 2 classes, else there would be several beta vectors.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with components: <br />
</p>
<table>
<tr><td><code>yint</code></td>
<td>
<p>number of the given class of each case. Can contain <code>NA</code>'s.</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>given class label of each case. Can contain <code>NA</code>'s.</p>
</td></tr>
<tr><td><code>levels</code></td>
<td>
<p>levels of the response <code>y</code>.</p>
</td></tr>
<tr><td><code>predint</code></td>
<td>
<p>predicted class number of each case. Always exists.</p>
</td></tr>
<tr><td><code>pred</code></td>
<td>
<p>predicted label of each case.</p>
</td></tr>
<tr><td><code>altint</code></td>
<td>
<p>number of the alternative class. Among the classes different from the given class, it is the one with the highest posterior probability. Is <code>NA</code> for cases whose <code>y</code> is missing.</p>
</td></tr>
<tr><td><code>altlab</code></td>
<td>
<p>label of the alternative class. Is <code>NA</code> for cases whose <code>y</code> is missing.</p>
</td></tr>
<tr><td><code>PAC</code></td>
<td>
<p>probability of the alternative class. Is <code>NA</code> for cases whose <code>y</code> is missing.</p>
</td></tr>
<tr><td><code>figparams</code></td>
<td>
<p>parameters used in <code>fig</code>, can be used for new data.</p>
</td></tr>
<tr><td><code>fig</code></td>
<td>
<p>distance of each case <code class="reqn">i</code> from each class <code class="reqn">g</code>. Always exists.</p>
</td></tr>
<tr><td><code>farness</code></td>
<td>
<p>farness of each case from its given class. Is <code>NA</code> for cases whose <code>y</code> is missing.</p>
</td></tr>
<tr><td><code>ofarness</code></td>
<td>
<p>for each case <code class="reqn">i</code>, its lowest <code>fig[i,g]</code> to any class <code>g</code>. Always exists.</p>
</td></tr>
<tr><td><code>svfit</code></td>
<td>
<p>as it was input, will be useful for new data.</p>
</td></tr>
<tr><td><code>X</code></td>
<td>
<p>the matrix of data coordinates from the arguments. This is useful for classifying new data.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Raymaekers J., Rousseeuw P.J.
</p>


<h3>References</h3>

<p>Raymaekers J., Rousseeuw P.J., Hubert M. (2021). Class maps for visualizing classification results. <em>Technometrics</em>, appeared online. doi: <a href="https://doi.org/10.1080/00401706.2021.1927849">10.1080/00401706.2021.1927849</a>(link to open access pdf)
</p>


<h3>See Also</h3>

<p><code><a href="#topic+vcr.knn.newdata">vcr.knn.newdata</a></code>, <code><a href="#topic+classmap">classmap</a></code>, <code><a href="#topic+silplot">silplot</a></code>, <code><a href="#topic+stackedplot">stackedplot</a></code>, <code><a href="e1071.html#topic+svm">e1071::svm</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(e1071)
set.seed(1); X &lt;- matrix(rnorm(200 * 2), ncol = 2)
X[1:100, ] &lt;- X[1:100, ] + 2
X[101:150, ] &lt;- X[101:150, ] - 2
y &lt;- as.factor(c(rep("blue", 150), rep("red", 50)))
cols &lt;- c("deepskyblue3", "red")
plot(X, col = cols[as.numeric(y)], pch = 19)
# We now fit an SVM with radial basis kernel to the data:
set.seed(1) # to make the result of svm() reproducible.
svmfit &lt;- svm(y~., data = data.frame(X = X, y = y),
scale = FALSE, kernel = "radial", cost = 10,
gamma = 1, probability = TRUE)
plot(svmfit$decision.values, col = cols[as.numeric(y)]); abline(h = 0)
# so the decision values separate the classes reasonably well.
plot(svmfit, data = data.frame(X = X, y = y), X.2~X.1, col = cols)
# The boundary is far from linear (but in feature space it is).
vcr.train &lt;- vcr.svm.train(X, y, svfit = svmfit)
confmat.vcr(vcr.train)
stackedplot(vcr.train, classCols = cols)
classmap(vcr.train, "blue", classCols = cols)
classmap(vcr.train, "red", classCols = cols)

# For more examples, we refer to the vignette:
## Not run: 
vignette("Support_vector_machine_examples")

## End(Not run)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
