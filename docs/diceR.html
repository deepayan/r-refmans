<!DOCTYPE html><html><head><title>Help for package diceR</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {diceR}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#compactness'><p>Compactness Measure</p></a></li>
<li><a href='#consensus_cluster'><p>Consensus clustering</p></a></li>
<li><a href='#consensus_combine'><p>Combine algorithms</p></a></li>
<li><a href='#consensus_evaluate'><p>Evaluate, trim, and reweigh algorithms</p></a></li>
<li><a href='#consensus_matrix'><p>Consensus matrix</p></a></li>
<li><a href='#CSPA'><p>Cluster-based Similarity Partitioning Algorithm (CSPA)</p></a></li>
<li><a href='#dice'><p>Diverse Clustering Ensemble</p></a></li>
<li><a href='#diceR-package'><p>diceR: Diverse Cluster Ensemble in R</p></a></li>
<li><a href='#external_validity'><p>External validity indices</p></a></li>
<li><a href='#graphs'><p>Graphical Displays</p></a></li>
<li><a href='#hgsc'><p>Gene expression data for High Grade Serous Carcinoma from TCGA</p></a></li>
<li><a href='#impute_knn'><p>K-Nearest Neighbours imputation</p></a></li>
<li><a href='#impute_missing'><p>Impute missing values</p></a></li>
<li><a href='#k_modes'><p>K-modes</p></a></li>
<li><a href='#LCA'><p>Latent Class Analysis</p></a></li>
<li><a href='#LCE'><p>Linkage Clustering Ensemble</p></a></li>
<li><a href='#majority_voting'><p>Majority voting</p></a></li>
<li><a href='#min_fnorm'><p>Minimize Frobenius norm for between two matrices</p></a></li>
<li><a href='#PAC'><p>Proportion of Ambiguous Clustering</p></a></li>
<li><a href='#pcn'><p>Simulate and select null distributions on empirical gene-gene correlations</p></a></li>
<li><a href='#prepare_data'><p>Prepare data for consensus clustering</p></a></li>
<li><a href='#relabel_class'><p>Relabel classes to a standard</p></a></li>
<li><a href='#sigclust'><p>Significant Testing of Clustering Results</p></a></li>
<li><a href='#similarity'><p>Similarity Matrices</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Diverse Cluster Ensemble in R</td>
</tr>
<tr>
<td>Version:</td>
<td>2.2.0</td>
</tr>
<tr>
<td>Description:</td>
<td>Performs cluster analysis using an ensemble clustering
    framework, Chiu &amp; Talhouk (2018) &lt;<a href="https://doi.org/10.1186%2Fs12859-017-1996-y">doi:10.1186/s12859-017-1996-y</a>&gt;.
    Results from a diverse set of algorithms are pooled together using
    methods such as majority voting, K-Modes, LinkCluE, and CSPA. There
    are options to compare cluster assignments across algorithms using
    internal and external indices, visualizations such as heatmaps, and
    significance testing for the existence of clusters.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/AlineTalhouk/diceR/">https://github.com/AlineTalhouk/diceR/</a>,
<a href="https://alinetalhouk.github.io/diceR/">https://alinetalhouk.github.io/diceR/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/AlineTalhouk/diceR/issues">https://github.com/AlineTalhouk/diceR/issues</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5)</td>
</tr>
<tr>
<td>Imports:</td>
<td>abind, assertthat, class, clue, clusterSim, clv, clValid,
dplyr (&ge; 0.7.5), ggplot2, infotheo, klaR, magrittr, mclust,
methods, NMF, purrr (&ge; 0.2.3), RankAggreg, Rcpp, stringr,
tidyr, yardstick</td>
</tr>
<tr>
<td>Suggests:</td>
<td>apcluster, blockcluster, cluster, covr, dbscan, e1071,
kernlab, knitr, kohonen, pander, poLCA, progress, RColorBrewer,
rlang, rmarkdown, Rtsne, sigclust, testthat</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.0</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-01-22 20:44:12 UTC; derekchiu</td>
</tr>
<tr>
<td>Author:</td>
<td>Derek Chiu [aut, cre],
  Aline Talhouk [aut],
  Johnson Liu [ctb, com]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Derek Chiu &lt;dchiu@bccrc.ca&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-01-22 21:22:46 UTC</td>
</tr>
</table>
<hr>
<h2 id='compactness'>Compactness Measure</h2><span id='topic+compactness'></span>

<h3>Description</h3>

<p>Compute the compactness validity index for a clustering result.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>compactness(data, labels)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="compactness_+3A_data">data</code></td>
<td>
<p>a dataset with rows as observations, columns as variables</p>
</td></tr>
<tr><td><code id="compactness_+3A_labels">labels</code></td>
<td>
<p>a vector of cluster labels from a clustering result</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This index is agnostic to any reference clustering results, calculating
cluster performance on the basis of compactness and separability. Smaller
values indicate a better clustering structure.
</p>


<h3>Value</h3>

<p>the compactness score
</p>


<h3>Author(s)</h3>

<p>Derek Chiu
</p>


<h3>References</h3>

<p>MATLAB function <code>valid_compactness</code> by Simon Garrett in
LinkCluE
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
E &lt;- matrix(rep(sample(1:4, 1000, replace = TRUE)), nrow = 100, byrow =
              FALSE)
set.seed(1)
dat &lt;- as.data.frame(matrix(runif(1000, -10, 10), nrow = 100, byrow = FALSE))
compactness(dat, E[, 1])
</code></pre>

<hr>
<h2 id='consensus_cluster'>Consensus clustering</h2><span id='topic+consensus_cluster'></span>

<h3>Description</h3>

<p>Runs consensus clustering across subsamples of the data, clustering
algorithms, and cluster sizes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>consensus_cluster(
  data,
  nk = 2:4,
  p.item = 0.8,
  reps = 1000,
  algorithms = NULL,
  nmf.method = c("brunet", "lee"),
  hc.method = "average",
  xdim = NULL,
  ydim = NULL,
  rlen = 200,
  alpha = c(0.05, 0.01),
  minPts = 5,
  distance = "euclidean",
  abs = TRUE,
  prep.data = c("none", "full", "sampled"),
  scale = TRUE,
  type = c("conventional", "robust", "tsne"),
  min.var = 1,
  progress = TRUE,
  seed.nmf = 123456,
  seed.data = 1,
  file.name = NULL,
  time.saved = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="consensus_cluster_+3A_data">data</code></td>
<td>
<p>data matrix with rows as samples and columns as variables</p>
</td></tr>
<tr><td><code id="consensus_cluster_+3A_nk">nk</code></td>
<td>
<p>number of clusters (k) requested; can specify a single integer or a
range of integers to compute multiple k</p>
</td></tr>
<tr><td><code id="consensus_cluster_+3A_p.item">p.item</code></td>
<td>
<p>proportion of items to be used in subsampling within an
algorithm</p>
</td></tr>
<tr><td><code id="consensus_cluster_+3A_reps">reps</code></td>
<td>
<p>number of subsamples</p>
</td></tr>
<tr><td><code id="consensus_cluster_+3A_algorithms">algorithms</code></td>
<td>
<p>vector of clustering algorithms for performing consensus
clustering. Must be any number of the following: &quot;nmf&quot;, &quot;hc&quot;, &quot;diana&quot;,
&quot;km&quot;, &quot;pam&quot;, &quot;ap&quot;, &quot;sc&quot;, &quot;gmm&quot;, &quot;block&quot;, &quot;som&quot;, &quot;cmeans&quot;, &quot;hdbscan&quot;. A
custom clustering algorithm can be used.</p>
</td></tr>
<tr><td><code id="consensus_cluster_+3A_nmf.method">nmf.method</code></td>
<td>
<p>specify NMF-based algorithms to run. By default the
&quot;brunet&quot; and &quot;lee&quot; algorithms are called. See <code><a href="NMF.html#topic+nmf">NMF::nmf()</a></code> for details.</p>
</td></tr>
<tr><td><code id="consensus_cluster_+3A_hc.method">hc.method</code></td>
<td>
<p>agglomeration method for hierarchical clustering. The
the &quot;average&quot; method is used by default. See<code><a href="stats.html#topic+hclust">stats::hclust()</a></code> for details.</p>
</td></tr>
<tr><td><code id="consensus_cluster_+3A_xdim">xdim</code></td>
<td>
<p>x dimension of the SOM grid</p>
</td></tr>
<tr><td><code id="consensus_cluster_+3A_ydim">ydim</code></td>
<td>
<p>y dimension of the SOM grid</p>
</td></tr>
<tr><td><code id="consensus_cluster_+3A_rlen">rlen</code></td>
<td>
<p>the number of times the complete data set will be presented to
the SOM network.</p>
</td></tr>
<tr><td><code id="consensus_cluster_+3A_alpha">alpha</code></td>
<td>
<p>SOM learning rate, a vector of two numbers indicating the amount
of change. Default is to decline linearly from 0.05 to 0.01 over <code>rlen</code>
updates. Not used for the batch algorithm.</p>
</td></tr>
<tr><td><code id="consensus_cluster_+3A_minpts">minPts</code></td>
<td>
<p>minimum size of clusters for HDBSCAN. Default is 5.</p>
</td></tr>
<tr><td><code id="consensus_cluster_+3A_distance">distance</code></td>
<td>
<p>a vector of distance functions. Defaults to &quot;euclidean&quot;.
Other options are given in <code><a href="stats.html#topic+dist">stats::dist()</a></code>. A custom distance function can
be used.</p>
</td></tr>
<tr><td><code id="consensus_cluster_+3A_abs">abs</code></td>
<td>
<p>only used for <code>distance = c("spearman", "pearson")</code>. If <code>TRUE</code>,
the absolute value is first applied to the distance before subtracting from
1, e.g., we use 1 - |SCD| instead of 1 - SCD for the spearman correlation
distance.</p>
</td></tr>
<tr><td><code id="consensus_cluster_+3A_prep.data">prep.data</code></td>
<td>
<p>Prepare the data on the &quot;full&quot; dataset, the &quot;sampled&quot;
dataset, or &quot;none&quot; (default).</p>
</td></tr>
<tr><td><code id="consensus_cluster_+3A_scale">scale</code></td>
<td>
<p>logical; should the data be centered and scaled?</p>
</td></tr>
<tr><td><code id="consensus_cluster_+3A_type">type</code></td>
<td>
<p>if we use &quot;conventional&quot; measures (default), then the mean and
standard deviation are used for centering and scaling, respectively. If
&quot;robust&quot; measures are specified, the median and median absolute deviation
(MAD) are used. Alternatively, we can apply &quot;tsne&quot; for dimension reduction.</p>
</td></tr>
<tr><td><code id="consensus_cluster_+3A_min.var">min.var</code></td>
<td>
<p>minimum variability measure threshold used to filter the
feature space for only highly variable features. Only features with a
minimum variability measure across all samples greater than <code>min.var</code> will
be used. If <code>type = "conventional"</code>, the standard deviation is the measure
used, and if <code>type = "robust"</code>, the MAD is the measure used.</p>
</td></tr>
<tr><td><code id="consensus_cluster_+3A_progress">progress</code></td>
<td>
<p>logical; should a progress bar be displayed?</p>
</td></tr>
<tr><td><code id="consensus_cluster_+3A_seed.nmf">seed.nmf</code></td>
<td>
<p>random seed to use for NMF-based algorithms</p>
</td></tr>
<tr><td><code id="consensus_cluster_+3A_seed.data">seed.data</code></td>
<td>
<p>seed to use to ensure each algorithm operates on the same
set of subsamples</p>
</td></tr>
<tr><td><code id="consensus_cluster_+3A_file.name">file.name</code></td>
<td>
<p>if not <code>NULL</code>, the returned array will be saved at each
iteration as well as at the end of the function call to an <code>rds</code> object
with <code>file.name</code> as the file name.</p>
</td></tr>
<tr><td><code id="consensus_cluster_+3A_time.saved">time.saved</code></td>
<td>
<p>logical; if <code>TRUE</code>, the date saved is appended to
<code>file.name</code>. Only applicable when <code>file.name</code> is not <code>NULL</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>See examples for how to use custom algorithms and distance functions. The
default clustering algorithms provided are:
</p>

<ul>
<li><p> &quot;nmf&quot;: Nonnegative Matrix Factorization (using Kullback-Leibler Divergence
or Euclidean distance; See Note for specifications.)
</p>
</li>
<li><p> &quot;hc&quot;: Hierarchical Clustering
</p>
</li>
<li><p> &quot;diana&quot;: DIvisive ANAlysis Clustering
</p>
</li>
<li><p> &quot;km&quot;: K-Means Clustering
</p>
</li>
<li><p> &quot;pam&quot;: Partition Around Medoids
</p>
</li>
<li><p> &quot;ap&quot;: Affinity Propagation
</p>
</li>
<li><p> &quot;sc&quot;: Spectral Clustering using Radial-Basis kernel function
</p>
</li>
<li><p> &quot;gmm&quot;: Gaussian Mixture Model using Bayesian Information Criterion on EM
algorithm
</p>
</li>
<li><p> &quot;block&quot;: Biclustering using a latent block model
</p>
</li>
<li><p> &quot;som&quot;: Self-Organizing Map (SOM) with Hierarchical Clustering
</p>
</li>
<li><p> &quot;cmeans&quot;: Fuzzy C-Means Clustering
</p>
</li>
<li><p> &quot;hdbscan&quot;: Hierarchical Density-based Spatial Clustering of Applications
with Noise (HDBSCAN)
</p>
</li></ul>

<p>The progress bar increments on every unit of <code>reps</code>.
</p>


<h3>Value</h3>

<p>An array of dimension <code>nrow(x)</code> by <code>reps</code> by <code>length(algorithms)</code> by
<code>length(nk)</code>. Each cube of the array represents a different k. Each slice
of a cube is a matrix showing consensus clustering results for algorithms.
The matrices have a row for each sample, and a column for each subsample.
Each entry represents a class membership.
</p>
<p>When &quot;hdbscan&quot; is part of <code>algorithms</code>, we do not include its clustering
array in the consensus result. Instead, we report two summary statistics as
attributes: the proportion of outliers and the number of clusters.
</p>


<h3>Note</h3>

<p>The <code>nmf.method</code> options are &quot;brunet&quot; (Kullback-Leibler Divergence) and
&quot;lee&quot; (Euclidean distance). When &quot;hdbscan&quot; is chosen as an algorithm to
use, its results are excluded from the rest of the consensus clusters. This
is because there is no guarantee that the cluster assignment will have
every sample clustered; more often than not there will be noise points or
outliers. In addition, the number of distinct clusters may not even be
equal to <code>nk</code>.
</p>


<h3>Author(s)</h3>

<p>Derek Chiu, Aline Talhouk
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(hgsc)
dat &lt;- hgsc[1:100, 1:50]

# Custom distance function
manh &lt;- function(x) {
  stats::dist(x, method = "manhattan")
}

# Custom clustering algorithm
agnes &lt;- function(d, k) {
  return(as.integer(stats::cutree(cluster::agnes(d, diss = TRUE), k)))
}

assign("agnes", agnes, 1)

cc &lt;- consensus_cluster(dat, reps = 6, algorithms = c("pam", "agnes"),
distance = c("euclidean", "manh"), progress = FALSE)
str(cc)
</code></pre>

<hr>
<h2 id='consensus_combine'>Combine algorithms</h2><span id='topic+consensus_combine'></span>

<h3>Description</h3>

<p>Combines results for multiple objects from <code>consensus_cluster()</code> and outputs
either the consensus matrices or consensus classes for all algorithms.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>consensus_combine(..., element = c("matrix", "class"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="consensus_combine_+3A_...">...</code></td>
<td>
<p>any number of objects outputted from <code><a href="#topic+consensus_cluster">consensus_cluster()</a></code></p>
</td></tr>
<tr><td><code id="consensus_combine_+3A_element">element</code></td>
<td>
<p>either &quot;matrix&quot; or &quot;class&quot; to extract the consensus matrix or
consensus class, respectively.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is useful for collecting summaries because the original results
from <code>consensus_cluster</code> were combined to a single object. For example,
setting <code>element = "class"</code> returns a matrix of consensus cluster
assignments, which can be visualized as a consensus matrix heatmap.
</p>


<h3>Value</h3>

<p><code>consensus_combine</code> returns either a list of all consensus matrices
or a data frame showing all the consensus classes
</p>


<h3>Author(s)</h3>

<p>Derek Chiu
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Consensus clustering for multiple algorithms
set.seed(911)
x &lt;- matrix(rnorm(500), ncol = 10)
CC1 &lt;- consensus_cluster(x, nk = 3:4, reps = 10, algorithms = "ap",
progress = FALSE)
CC2 &lt;- consensus_cluster(x, nk = 3:4, reps = 10, algorithms = "km",
progress = FALSE)

# Combine and return either matrices or classes
y1 &lt;- consensus_combine(CC1, CC2, element = "matrix")
str(y1)
y2 &lt;- consensus_combine(CC1, CC2, element = "class")
str(y2)

</code></pre>

<hr>
<h2 id='consensus_evaluate'>Evaluate, trim, and reweigh algorithms</h2><span id='topic+consensus_evaluate'></span>

<h3>Description</h3>

<p>Evaluates algorithms on internal/external validation indices. Poor performing
algorithms can be trimmed from the ensemble. The remaining algorithms can be
given weights before use in consensus functions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>consensus_evaluate(
  data,
  ...,
  cons.cl = NULL,
  ref.cl = NULL,
  k.method = NULL,
  plot = FALSE,
  trim = FALSE,
  reweigh = FALSE,
  n = 5,
  lower = 0,
  upper = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="consensus_evaluate_+3A_data">data</code></td>
<td>
<p>data matrix with rows as samples and columns as variables</p>
</td></tr>
<tr><td><code id="consensus_evaluate_+3A_...">...</code></td>
<td>
<p>any number of objects outputted from <code><a href="#topic+consensus_cluster">consensus_cluster()</a></code></p>
</td></tr>
<tr><td><code id="consensus_evaluate_+3A_cons.cl">cons.cl</code></td>
<td>
<p>matrix of cluster assignments from consensus functions such as
<code>kmodes</code> and <code>majority_voting</code></p>
</td></tr>
<tr><td><code id="consensus_evaluate_+3A_ref.cl">ref.cl</code></td>
<td>
<p>reference class</p>
</td></tr>
<tr><td><code id="consensus_evaluate_+3A_k.method">k.method</code></td>
<td>
<p>determines the method to choose k when no reference class is
given. When <code>ref.cl</code> is not <code>NULL</code>, k is the number of distinct classes of
<code>ref.cl</code>. Otherwise the input from <code>k.method</code> chooses k. The default is to
use the PAC to choose the best k(s). Specifying an integer as a
user-desired k will override the best k chosen by PAC. Finally, specifying
&quot;all&quot; will produce consensus results for all k. The &quot;all&quot; method is
implicitly performed when there is only one k used.</p>
</td></tr>
<tr><td><code id="consensus_evaluate_+3A_plot">plot</code></td>
<td>
<p>logical; if <code>TRUE</code>, <code>graph_all</code> is called</p>
</td></tr>
<tr><td><code id="consensus_evaluate_+3A_trim">trim</code></td>
<td>
<p>logical; if <code>TRUE</code>, algorithms that score low on internal indices
will be trimmed out</p>
</td></tr>
<tr><td><code id="consensus_evaluate_+3A_reweigh">reweigh</code></td>
<td>
<p>logical; if <code>TRUE</code>, after trimming out poor performing
algorithms, each algorithm is reweighed depending on its internal indices.</p>
</td></tr>
<tr><td><code id="consensus_evaluate_+3A_n">n</code></td>
<td>
<p>an integer specifying the top <code>n</code> algorithms to keep after trimming
off the poor performing ones using Rank Aggregation. If the total number of
algorithms is less than <code>n</code> no trimming is done.</p>
</td></tr>
<tr><td><code id="consensus_evaluate_+3A_lower">lower</code></td>
<td>
<p>the lower bound that determines what is ambiguous</p>
</td></tr>
<tr><td><code id="consensus_evaluate_+3A_upper">upper</code></td>
<td>
<p>the upper bound that determines what is ambiguous</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function always returns internal indices. If <code>ref.cl</code> is not <code>NULL</code>,
external indices are additionally shown. Relevant graphical displays are also
outputted. Algorithms are ranked across internal indices using Rank
Aggregation. Only the top <code>n</code> algorithms are kept, the rest are trimmed.
</p>


<h3>Value</h3>

<p><code>consensus_evaluate</code> returns a list with the following elements
</p>

<ul>
<li> <p><code>k</code>: if <code>ref.cl</code> is not <code>NULL</code>, this is the number of distinct classes
in the reference; otherwise the chosen <code>k</code> is determined by the one giving
the largest mean PAC across algorithms
</p>
</li>
<li> <p><code>pac</code>: a data frame showing the PAC for each combination of algorithm
and cluster size
</p>
</li>
<li> <p><code>ii</code>: a list of data frames for all k showing internal evaluation
indices
</p>
</li>
<li> <p><code>ei</code>: a data frame showing external evaluation indices for <code>k</code>
</p>
</li>
<li> <p><code>trim.obj</code>: A list with 4 elements
</p>

<ul>
<li> <p><code>alg.keep</code>: algorithms kept
</p>
</li>
<li> <p><code>alg.remove</code>: algorithms removed
</p>
</li>
<li> <p><code>rank.matrix</code>: a matrix of ranked algorithms for every internal
evaluation index
</p>
</li>
<li> <p><code>top.list</code>: final order of ranked algorithms
</p>
</li>
<li> <p><code>E.new</code>: A new version of a <code>consensus_cluster</code> data object
</p>
</li></ul>

</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>
# Consensus clustering for multiple algorithms
set.seed(911)
x &lt;- matrix(rnorm(500), ncol = 10)
CC &lt;- consensus_cluster(x, nk = 3:4, reps = 10, algorithms = c("ap", "km"),
progress = FALSE)

# Evaluate algorithms on internal/external indices and trim algorithms:
# remove those ranking low on internal indices
set.seed(1)
ref.cl &lt;- sample(1:4, 50, replace = TRUE)
z &lt;- consensus_evaluate(x, CC, ref.cl = ref.cl, n = 1, trim = TRUE)
str(z, max.level = 2)

</code></pre>

<hr>
<h2 id='consensus_matrix'>Consensus matrix</h2><span id='topic+consensus_matrix'></span>

<h3>Description</h3>

<p>Returns the (weighted) consensus matrix given a data matrix
</p>


<h3>Usage</h3>

<pre><code class='language-R'>consensus_matrix(data, weights = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="consensus_matrix_+3A_data">data</code></td>
<td>
<p>data matrix has rows as samples, columns as replicates</p>
</td></tr>
<tr><td><code id="consensus_matrix_+3A_weights">weights</code></td>
<td>
<p>a vector of weights for each algorithm used in meta-consensus
clustering. Must have <code>length(weights)</code> equal to <code>ncol(data)</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Given a vector of cluster assignments, we first calculate the connectivity
matrix and indicator matrix. A connectivity matrix has a 1 if both samples
are in the same cluster, and 0 otherwise. An indicator matrix has a 1 if both
samples were selected to be used in a subsample of a consensus clustering
algorithm, and 0 otherwise. Summation of connectivity matrices and indicator
matrices is performed over different subsamples of the data. The consensus
matrix is calculated by dividing the aggregated connectivity matrices by the
aggregated indicator matrices.
</p>
<p>If a meta-consensus matrix is desired, where consensus classes of different
clustering algorithms are aggregated, we can construct a weighted
meta-consensus matrix using <code>weights</code>.
</p>


<h3>Value</h3>

<p>a consensus matrix
</p>


<h3>Note</h3>

<p>When consensus is calculated over bootstrap samples, not every sample
is used in each replication. Thus, there will be scenarios where two
samples are never chosen together in any bootstrap samples. This typically
happens when the number of replications is small. The coordinate in the
consensus matrix for such pairs of samples is <code>NaN</code> from a 0 / 0
computation. These entries are coerced to 0.
</p>


<h3>Author(s)</h3>

<p>Derek Chiu
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(2)
x &lt;- replicate(100, rbinom(100, 4, 0.2))
w &lt;- rexp(100)
w &lt;- w / sum(w)
cm1 &lt;- consensus_matrix(x)
cm2 &lt;- consensus_matrix(x, weights = w)
</code></pre>

<hr>
<h2 id='CSPA'>Cluster-based Similarity Partitioning Algorithm (CSPA)</h2><span id='topic+CSPA'></span>

<h3>Description</h3>

<p>Performs hierarchical clustering on a stack of consensus matrices to obtain
consensus class labels.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CSPA(E, k)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CSPA_+3A_e">E</code></td>
<td>
<p>is an array of clustering results.</p>
</td></tr>
<tr><td><code id="CSPA_+3A_k">k</code></td>
<td>
<p>number of clusters</p>
</td></tr>
</table>


<h3>Value</h3>

<p>cluster assignments for the consensus class
</p>


<h3>Author(s)</h3>

<p>Derek Chiu
</p>


<h3>References</h3>

<p>Strehl, A., &amp; Ghosh, J. (2002). Cluster ensembles&mdash;a knowledge
reuse framework for combining multiple partitions. Journal of machine
learning research, 3(Dec), 583-617.
</p>


<h3>See Also</h3>

<p>Other consensus functions: 
<code><a href="#topic+LCA">LCA</a>()</code>,
<code><a href="#topic+LCE">LCE</a>()</code>,
<code><a href="#topic+k_modes">k_modes</a>()</code>,
<code><a href="#topic+majority_voting">majority_voting</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(hgsc)
dat &lt;- hgsc[1:100, 1:50]
x &lt;- consensus_cluster(dat, nk = 4, reps = 4, algorithms = c("hc", "diana"),
progress = FALSE)
CSPA(x, k = 4)
</code></pre>

<hr>
<h2 id='dice'>Diverse Clustering Ensemble</h2><span id='topic+dice'></span>

<h3>Description</h3>

<p>Runs consensus clustering across subsamples, algorithms, and number of
clusters (k).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dice(
  data,
  nk,
  p.item = 0.8,
  reps = 10,
  algorithms = NULL,
  k.method = NULL,
  nmf.method = c("brunet", "lee"),
  hc.method = "average",
  distance = "euclidean",
  cons.funs = c("kmodes", "majority", "CSPA", "LCE", "LCA"),
  sim.mat = c("cts", "srs", "asrs"),
  prep.data = c("none", "full", "sampled"),
  min.var = 1,
  seed = 1,
  seed.data = 1,
  trim = FALSE,
  reweigh = FALSE,
  n = 5,
  evaluate = TRUE,
  plot = FALSE,
  ref.cl = NULL,
  progress = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dice_+3A_data">data</code></td>
<td>
<p>data matrix with rows as samples and columns as variables</p>
</td></tr>
<tr><td><code id="dice_+3A_nk">nk</code></td>
<td>
<p>number of clusters (k) requested; can specify a single integer or a
range of integers to compute multiple k</p>
</td></tr>
<tr><td><code id="dice_+3A_p.item">p.item</code></td>
<td>
<p>proportion of items to be used in subsampling within an
algorithm</p>
</td></tr>
<tr><td><code id="dice_+3A_reps">reps</code></td>
<td>
<p>number of subsamples</p>
</td></tr>
<tr><td><code id="dice_+3A_algorithms">algorithms</code></td>
<td>
<p>vector of clustering algorithms for performing consensus
clustering. Must be any number of the following: &quot;nmf&quot;, &quot;hc&quot;, &quot;diana&quot;,
&quot;km&quot;, &quot;pam&quot;, &quot;ap&quot;, &quot;sc&quot;, &quot;gmm&quot;, &quot;block&quot;, &quot;som&quot;, &quot;cmeans&quot;, &quot;hdbscan&quot;. A
custom clustering algorithm can be used.</p>
</td></tr>
<tr><td><code id="dice_+3A_k.method">k.method</code></td>
<td>
<p>determines the method to choose k when no reference class is
given. When <code>ref.cl</code> is not <code>NULL</code>, k is the number of distinct classes of
<code>ref.cl</code>. Otherwise the input from <code>k.method</code> chooses k. The default is to
use the PAC to choose the best k(s). Specifying an integer as a
user-desired k will override the best k chosen by PAC. Finally, specifying
&quot;all&quot; will produce consensus results for all k. The &quot;all&quot; method is
implicitly performed when there is only one k used.</p>
</td></tr>
<tr><td><code id="dice_+3A_nmf.method">nmf.method</code></td>
<td>
<p>specify NMF-based algorithms to run. By default the
&quot;brunet&quot; and &quot;lee&quot; algorithms are called. See <code><a href="NMF.html#topic+nmf">NMF::nmf()</a></code> for details.</p>
</td></tr>
<tr><td><code id="dice_+3A_hc.method">hc.method</code></td>
<td>
<p>agglomeration method for hierarchical clustering. The
the &quot;average&quot; method is used by default. See<code><a href="stats.html#topic+hclust">stats::hclust()</a></code> for details.</p>
</td></tr>
<tr><td><code id="dice_+3A_distance">distance</code></td>
<td>
<p>a vector of distance functions. Defaults to &quot;euclidean&quot;.
Other options are given in <code><a href="stats.html#topic+dist">stats::dist()</a></code>. A custom distance function can
be used.</p>
</td></tr>
<tr><td><code id="dice_+3A_cons.funs">cons.funs</code></td>
<td>
<p>consensus functions to use. Current options are &quot;kmodes&quot;
(k-modes), &quot;majority&quot; (majority voting), &quot;CSPA&quot; (Cluster-based Similarity
Partitioning Algorithm), &quot;LCE&quot; (linkage clustering ensemble), &quot;LCA&quot; (latent
class analysis)</p>
</td></tr>
<tr><td><code id="dice_+3A_sim.mat">sim.mat</code></td>
<td>
<p>similarity matrix; choices are &quot;cts&quot;, &quot;srs&quot;, &quot;asrs&quot;.</p>
</td></tr>
<tr><td><code id="dice_+3A_prep.data">prep.data</code></td>
<td>
<p>Prepare the data on the &quot;full&quot; dataset, the &quot;sampled&quot;
dataset, or &quot;none&quot; (default).</p>
</td></tr>
<tr><td><code id="dice_+3A_min.var">min.var</code></td>
<td>
<p>minimum variability measure threshold used to filter the
feature space for only highly variable features. Only features with a
minimum variability measure across all samples greater than <code>min.var</code> will
be used. If <code>type = "conventional"</code>, the standard deviation is the measure
used, and if <code>type = "robust"</code>, the MAD is the measure used.</p>
</td></tr>
<tr><td><code id="dice_+3A_seed">seed</code></td>
<td>
<p>random seed for knn imputation reproducibility</p>
</td></tr>
<tr><td><code id="dice_+3A_seed.data">seed.data</code></td>
<td>
<p>seed to use to ensure each algorithm operates on the same
set of subsamples</p>
</td></tr>
<tr><td><code id="dice_+3A_trim">trim</code></td>
<td>
<p>logical; if <code>TRUE</code>, algorithms that score low on internal indices
will be trimmed out</p>
</td></tr>
<tr><td><code id="dice_+3A_reweigh">reweigh</code></td>
<td>
<p>logical; if <code>TRUE</code>, after trimming out poor performing
algorithms, each algorithm is reweighed depending on its internal indices.</p>
</td></tr>
<tr><td><code id="dice_+3A_n">n</code></td>
<td>
<p>an integer specifying the top <code>n</code> algorithms to keep after trimming
off the poor performing ones using Rank Aggregation. If the total number of
algorithms is less than <code>n</code> no trimming is done.</p>
</td></tr>
<tr><td><code id="dice_+3A_evaluate">evaluate</code></td>
<td>
<p>logical; if <code>TRUE</code> (default), validity indices are returned.
Internal validity indices are always computed. If <code>ref.cl</code> is not <code>NULL</code>,
then external validity indices will also be computed.</p>
</td></tr>
<tr><td><code id="dice_+3A_plot">plot</code></td>
<td>
<p>logical; if <code>TRUE</code>, <code>graph_all</code> is called and a summary
evaluation heatmap of ranked algorithms vs. internal validity indices is
plotted as well.</p>
</td></tr>
<tr><td><code id="dice_+3A_ref.cl">ref.cl</code></td>
<td>
<p>reference class</p>
</td></tr>
<tr><td><code id="dice_+3A_progress">progress</code></td>
<td>
<p>logical; should a progress bar be displayed?</p>
</td></tr>
</table>


<h3>Details</h3>

<p>There are three ways to handle the input data before clustering via argument
<code>prep.data</code>. The default is to use the raw data as-is (&quot;none&quot;). Or, we can
enact <code><a href="#topic+prepare_data">prepare_data()</a></code> on the full dataset (&quot;full&quot;), or the bootstrap sampled
datasets (&quot;sampled&quot;).
</p>


<h3>Value</h3>

<p>A list with the following elements
</p>
<table>
<tr><td><code>E</code></td>
<td>
<p>raw clustering ensemble object</p>
</td></tr>
<tr><td><code>Eknn</code></td>
<td>
<p>clustering ensemble object with knn imputation used on <code>E</code></p>
</td></tr>
<tr><td><code>Ecomp</code></td>
<td>
<p>flattened ensemble object with remaining missing entries imputed
by majority voting</p>
</td></tr>
<tr><td><code>clusters</code></td>
<td>
<p>final clustering assignment from the diverse clustering
ensemble method</p>
</td></tr>
<tr><td><code>indices</code></td>
<td>
<p>if <code>evaluate = TRUE</code>, shows cluster evaluation indices;
otherwise <code>NULL</code></p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Aline Talhouk, Derek Chiu
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(dplyr)
data(hgsc)
dat &lt;- hgsc[1:100, 1:50]
ref.cl &lt;- strsplit(rownames(dat), "_") %&gt;%
  purrr::map_chr(2) %&gt;%
  factor() %&gt;%
  as.integer()
dice.obj &lt;- dice(dat, nk = 4, reps = 5, algorithms = "hc", cons.funs =
"kmodes", ref.cl = ref.cl, progress = FALSE)
str(dice.obj, max.level = 2)
</code></pre>

<hr>
<h2 id='diceR-package'>diceR: Diverse Cluster Ensemble in R</h2><span id='topic+diceR'></span><span id='topic+diceR-package'></span>

<h3>Description</h3>

<p><img src="../help/figures/logo.png" style='float: right' alt='logo' width='120' />
</p>
<p>Performs cluster analysis using an ensemble clustering framework, Chiu &amp; Talhouk (2018) <a href="https://doi.org/10.1186/s12859-017-1996-y">doi:10.1186/s12859-017-1996-y</a>. Results from a diverse set of algorithms are pooled together using methods such as majority voting, K-Modes, LinkCluE, and CSPA. There are options to compare cluster assignments across algorithms using internal and external indices, visualizations such as heatmaps, and significance testing for the existence of clusters.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Derek Chiu <a href="mailto:dchiu@bccrc.ca">dchiu@bccrc.ca</a>
</p>
<p>Authors:
</p>

<ul>
<li><p> Aline Talhouk <a href="mailto:a.talhouk@ubc.ca">a.talhouk@ubc.ca</a>
</p>
</li></ul>

<p>Other contributors:
</p>

<ul>
<li><p> Johnson Liu <a href="mailto:gliu@bccrc.ca">gliu@bccrc.ca</a> [contributor, compiler]
</p>
</li></ul>



<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://github.com/AlineTalhouk/diceR/">https://github.com/AlineTalhouk/diceR/</a>
</p>
</li>
<li> <p><a href="https://alinetalhouk.github.io/diceR/">https://alinetalhouk.github.io/diceR/</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/AlineTalhouk/diceR/issues">https://github.com/AlineTalhouk/diceR/issues</a>
</p>
</li></ul>


<hr>
<h2 id='external_validity'>External validity indices</h2><span id='topic+external_validity'></span><span id='topic+ev_nmi'></span><span id='topic+ev_confmat'></span>

<h3>Description</h3>

<p><strong>E</strong>xternal <strong>v</strong>alidity indices compare a predicted clustering
result with a reference class or gold standard.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ev_nmi(pred.lab, ref.lab, method = "emp")

ev_confmat(pred.lab, ref.lab)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="external_validity_+3A_pred.lab">pred.lab</code></td>
<td>
<p>predicted labels generated by classifier</p>
</td></tr>
<tr><td><code id="external_validity_+3A_ref.lab">ref.lab</code></td>
<td>
<p>reference labels for the observations</p>
</td></tr>
<tr><td><code id="external_validity_+3A_method">method</code></td>
<td>
<p>method of computing the entropy. Can be any one of &quot;emp&quot;, &quot;mm&quot;,
&quot;shrink&quot;, or &quot;sg&quot;.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>ev_nmi</code> calculates the normalized mutual information
</p>
<p><code>ev_confmat</code> calculates a variety of statistics associated with
confusion matrices. Accuracy, Cohen's kappa, and Matthews correlation
coefficient have direct multiclass definitions, whereas all other
metrics use macro-averaging.
</p>


<h3>Value</h3>

<p><code>ev_nmi</code> returns the normalized mutual information.
</p>
<p><code>ev_confmat</code> returns a tibble of the following summary statistics using <code><a href="yardstick.html#topic+summary.conf_mat">yardstick::summary.conf_mat()</a></code>:
</p>

<ul>
<li> <p><code>accuracy</code>: Accuracy
</p>
</li>
<li> <p><code>kap</code>: Cohen's kappa
</p>
</li>
<li> <p><code>sens</code>: Sensitivity
</p>
</li>
<li> <p><code>spec</code>: Specificity
</p>
</li>
<li> <p><code>ppv</code>: Positive predictive value
</p>
</li>
<li> <p><code>npv</code>: Negative predictive value
</p>
</li>
<li> <p><code>mcc</code>: Matthews correlation coefficient
</p>
</li>
<li> <p><code>j_index</code>: Youden's J statistic
</p>
</li>
<li> <p><code>bal_accuracy</code>: Balanced accuracy
</p>
</li>
<li> <p><code>detection_prevalence</code>: Detection prevalence
</p>
</li>
<li> <p><code>precision</code>: alias for <code>ppv</code>
</p>
</li>
<li> <p><code>recall</code>: alias for <code>sens</code>
</p>
</li>
<li> <p><code>f_meas</code>: F Measure
</p>
</li></ul>



<h3>Note</h3>

<p><code>ev_nmi</code> is adapted from <code><a href="infotheo.html#topic+mutinformation">infotheo::mutinformation()</a></code>
</p>


<h3>Author(s)</h3>

<p>Johnson Liu, Derek Chiu
</p>


<h3>References</h3>

<p>Strehl A, Ghosh J. Cluster ensembles: a knowledge reuse framework
for combining multiple partitions. J. Mach. Learn. Res. 2002;3:583-617.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
E &lt;- matrix(rep(sample(1:4, 1000, replace = TRUE)), nrow = 100, byrow =
              FALSE)
x &lt;- sample(1:4, 100, replace = TRUE)
y &lt;- sample(1:4, 100, replace = TRUE)
ev_nmi(x, y)
ev_confmat(x, y)
</code></pre>

<hr>
<h2 id='graphs'>Graphical Displays</h2><span id='topic+graphs'></span><span id='topic+graph_cdf'></span><span id='topic+graph_delta_area'></span><span id='topic+graph_heatmap'></span><span id='topic+graph_tracking'></span><span id='topic+graph_all'></span>

<h3>Description</h3>

<p>Graph cumulative distribution function (CDF) graphs, relative change in area
under CDF curves, heatmaps, and cluster assignment tracking plots.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>graph_cdf(mat)

graph_delta_area(mat)

graph_heatmap(mat, main = NULL)

graph_tracking(cl)

graph_all(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="graphs_+3A_mat">mat</code></td>
<td>
<p>same as <code>x</code>, or a list of consensus matrices computed from <code>x</code> for
faster results</p>
</td></tr>
<tr><td><code id="graphs_+3A_main">main</code></td>
<td>
<p>heatmap title. If <code>NULL</code> (default), the titles will be taken from
names in <code>mat</code></p>
</td></tr>
<tr><td><code id="graphs_+3A_cl">cl</code></td>
<td>
<p>same as <code>x</code>, or a matrix of consensus classes computed from <code>x</code> for
faster results</p>
</td></tr>
<tr><td><code id="graphs_+3A_x">x</code></td>
<td>
<p>an object from <code><a href="#topic+consensus_cluster">consensus_cluster()</a></code></p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>graph_cdf</code> plots the CDF for consensus matrices from different algorithms.
<code>graph_delta_area</code> calculates the relative change in area under CDF curve
between algorithms. <code>graph_heatmap</code> generates consensus matrix heatmaps for
each algorithm in <code>x</code>. <code>graph_tracking</code> tracks how cluster assignments change
between algorithms. <code>graph_all</code> is a wrapper that runs all graphing
functions.
</p>


<h3>Value</h3>

<p>Various plots from <code>graph_*{}</code> functions. All plots are
generated using <code>ggplot</code>, except for <code>graph_heatmap</code>, which uses
<code><a href="NMF.html#topic+aheatmap">NMF::aheatmap()</a></code>. Colours used in <code>graph_heatmap</code> and <code>graph_tracking</code>
utilize <code><a href="RColorBrewer.html#topic+ColorBrewer">RColorBrewer::brewer.pal()</a></code> palettes.
</p>


<h3>Author(s)</h3>

<p>Derek Chiu
</p>


<h3>References</h3>

<p>https://stackoverflow.com/questions/4954507/calculate-the-area-under-a-curve
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Consensus clustering for 3 algorithms
library(ggplot2)
set.seed(911)
x &lt;- matrix(rnorm(80), ncol = 10)
CC1 &lt;- consensus_cluster(x, nk = 2:4, reps = 3,
algorithms = c("hc", "pam", "km"), progress = FALSE)

# Plot CDF
p &lt;- graph_cdf(CC1)

# Change y label and add colours
p + labs(y = "Probability") + stat_ecdf(aes(colour = k)) +
scale_color_brewer(palette = "Set2")

# Delta Area
p &lt;- graph_delta_area(CC1)

# Heatmaps with column side colours corresponding to clusters
CC2 &lt;- consensus_cluster(x, nk = 3, reps = 3, algorithms = "hc", progress =
FALSE)
graph_heatmap(CC2)

# Track how cluster assignments change between algorithms
p &lt;- graph_tracking(CC1)
</code></pre>

<hr>
<h2 id='hgsc'>Gene expression data for High Grade Serous Carcinoma from TCGA</h2><span id='topic+hgsc'></span>

<h3>Description</h3>

<p>There are 489 samples measured on 321 genes. Sample IDs are in the row names
and gene names are in the column names. This data set is used for clustering
HGSC into subtypes with prognostic significance. The cluster assignments
obtained by TCGA are indicated by the last six characters of each row name in
<code>hgsc</code>: <code>MES.C1</code>, <code>IMM.C2</code>, <code>DIF.C4</code>, and <code>PRO.C5</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hgsc
</code></pre>


<h3>Format</h3>

<p>A data frame with 489 rows and 321 columns.
</p>

<hr>
<h2 id='impute_knn'>K-Nearest Neighbours imputation</h2><span id='topic+impute_knn'></span>

<h3>Description</h3>

<p>The non-missing cases indicate the training set, and missing cases indicate
the test set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>impute_knn(x, data, seed = 123456)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="impute_knn_+3A_x">x</code></td>
<td>
<p>clustering object</p>
</td></tr>
<tr><td><code id="impute_knn_+3A_data">data</code></td>
<td>
<p>data matrix</p>
</td></tr>
<tr><td><code id="impute_knn_+3A_seed">seed</code></td>
<td>
<p>random seed for knn imputation reproducibility</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object with (potentially not all) missing values imputed with
K-Nearest Neighbours.
</p>


<h3>Note</h3>

<p>We consider 5 nearest neighbours and the minimum vote for definite
decision is 3.
</p>


<h3>Author(s)</h3>

<p>Aline Talhouk
</p>


<h3>See Also</h3>

<p>Other imputation functions: 
<code><a href="#topic+impute_missing">impute_missing</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(hgsc)
dat &lt;- hgsc[1:100, 1:50]
x &lt;- consensus_cluster(dat, nk = 4, reps = 4, algorithms = c("km", "hc",
"diana"), progress = FALSE)
x &lt;- apply(x, 2:4, impute_knn, data = dat, seed = 1)
</code></pre>

<hr>
<h2 id='impute_missing'>Impute missing values</h2><span id='topic+impute_missing'></span>

<h3>Description</h3>

<p>Impute missing values from bootstrapped subsampling
</p>


<h3>Usage</h3>

<pre><code class='language-R'>impute_missing(E, data, nk)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="impute_missing_+3A_e">E</code></td>
<td>
<p>4D array of clusterings from <code>consensus_cluster</code>. The number of rows
is equal to the number of cases to be clustered, number of columns is equal
to the clusterings obtained by different resamplings of the data, the third
dimension are the different algorithms and the fourth dimension are cluster
sizes.</p>
</td></tr>
<tr><td><code id="impute_missing_+3A_data">data</code></td>
<td>
<p>data matrix with samples as rows and genes/features as columns</p>
</td></tr>
<tr><td><code id="impute_missing_+3A_nk">nk</code></td>
<td>
<p>cluster size to extract data for (single value)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The default output from <code>consensus_cluster</code> will undoubtedly contain <code>NA</code>
entries because each replicate chooses a random subset (with replacement) of
all samples. Missing values should first be imputed using <code><a href="#topic+impute_knn">impute_knn()</a></code>. Not
all missing values are guaranteed to be imputed by KNN. See <code><a href="class.html#topic+knn">class::knn()</a></code>
for details. Thus, any remaining missing values are imputed using majority
voting.
</p>


<h3>Value</h3>

<p>If flattened matrix consists of more than one repetition, i.e. it
isn't a column vector, then the function returns a matrix of clusterings
with complete cases imputed using majority voting, and relabelled, for
chosen <code>k</code>.
</p>


<h3>Author(s)</h3>

<p>Aline Talhouk
</p>


<h3>See Also</h3>

<p>Other imputation functions: 
<code><a href="#topic+impute_knn">impute_knn</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(hgsc)
dat &lt;- hgsc[1:100, 1:50]
E &lt;- consensus_cluster(dat, nk = 3:4, reps = 10, algorithms = c("hc", "km",
"sc"), progress = FALSE)
sum(is.na(E))
E_imputed &lt;- impute_missing(E, dat, 4)
sum(is.na(E_imputed))

</code></pre>

<hr>
<h2 id='k_modes'>K-modes</h2><span id='topic+k_modes'></span>

<h3>Description</h3>

<p>Combine clustering results using K-modes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>k_modes(E, is.relabelled = TRUE, seed = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="k_modes_+3A_e">E</code></td>
<td>
<p>a matrix of clusterings with number of rows equal to the number of
cases to be clustered, number of columns equal to the clustering obtained
by different resampling of the data, and the third dimension are the
different algorithms. Matrix may already be two-dimensional.</p>
</td></tr>
<tr><td><code id="k_modes_+3A_is.relabelled">is.relabelled</code></td>
<td>
<p>logical; if <code>FALSE</code> the data will be relabelled using
the first clustering as the reference.</p>
</td></tr>
<tr><td><code id="k_modes_+3A_seed">seed</code></td>
<td>
<p>random seed for reproducibility</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Combine clustering results generated using different algorithms and different
data perturbations by k-modes. This method is the categorical data analog of
k-means clustering. Complete cases are needed: i.e. no <code>NA</code>s. If the matrix
contains <code>NA</code>s those are imputed by majority voting (after class relabeling).
</p>


<h3>Value</h3>

<p>a vector of cluster assignments based on k-modes
</p>


<h3>Author(s)</h3>

<p>Aline Talhouk
</p>


<h3>References</h3>

<p>Luo, H., Kong, F., &amp; Li, Y. (2006, August). Combining multiple
clusterings via k-modes algorithm. In International Conference on Advanced
Data Mining and Applications (pp. 308-315). Springer, Berlin, Heidelberg.
</p>


<h3>See Also</h3>

<p>Other consensus functions: 
<code><a href="#topic+CSPA">CSPA</a>()</code>,
<code><a href="#topic+LCA">LCA</a>()</code>,
<code><a href="#topic+LCE">LCE</a>()</code>,
<code><a href="#topic+majority_voting">majority_voting</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(hgsc)
dat &lt;- hgsc[1:100, 1:50]
cc &lt;- consensus_cluster(dat, nk = 4, reps = 6, algorithms = "pam", progress =
FALSE)
table(k_modes(cc[, , 1, 1, drop = FALSE], is.relabelled = FALSE))
</code></pre>

<hr>
<h2 id='LCA'>Latent Class Analysis</h2><span id='topic+LCA'></span>

<h3>Description</h3>

<p>Combine clustering results using latent class analysis.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>LCA(E, is.relabelled = TRUE, seed = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="LCA_+3A_e">E</code></td>
<td>
<p>a matrix of clusterings with number of rows equal to the number of
cases to be clustered, number of columns equal to the clustering obtained
by different resampling of the data, and the third dimension are the
different algorithms. Matrix may already be two-dimensional.</p>
</td></tr>
<tr><td><code id="LCA_+3A_is.relabelled">is.relabelled</code></td>
<td>
<p>logical; if <code>FALSE</code> the data will be relabelled using
the first clustering as the reference.</p>
</td></tr>
<tr><td><code id="LCA_+3A_seed">seed</code></td>
<td>
<p>random seed for reproducibility</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a vector of cluster assignments based on LCA
</p>


<h3>Author(s)</h3>

<p>Derek Chiu
</p>


<h3>See Also</h3>

<p>Other consensus functions: 
<code><a href="#topic+CSPA">CSPA</a>()</code>,
<code><a href="#topic+LCE">LCE</a>()</code>,
<code><a href="#topic+k_modes">k_modes</a>()</code>,
<code><a href="#topic+majority_voting">majority_voting</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(hgsc)
dat &lt;- hgsc[1:100, 1:50]
cc &lt;- consensus_cluster(dat, nk = 4, reps = 6, algorithms = "pam", progress =
FALSE)
table(LCA(cc[, , 1, 1, drop = FALSE], is.relabelled = FALSE))

</code></pre>

<hr>
<h2 id='LCE'>Linkage Clustering Ensemble</h2><span id='topic+LCE'></span>

<h3>Description</h3>

<p>Generate a cluster assignment from a CTS, SRS, or ASRS similarity matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>LCE(E, k, dc = 0.8, R = 10, sim.mat = c("cts", "srs", "asrs"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="LCE_+3A_e">E</code></td>
<td>
<p>is an array of clustering results. An error is thrown if there are
missing values. <code><a href="#topic+impute_missing">impute_missing()</a></code> can be used beforehand.</p>
</td></tr>
<tr><td><code id="LCE_+3A_k">k</code></td>
<td>
<p>requested number of clusters</p>
</td></tr>
<tr><td><code id="LCE_+3A_dc">dc</code></td>
<td>
<p>decay constant for CTS, SRS, or ASRS matrix</p>
</td></tr>
<tr><td><code id="LCE_+3A_r">R</code></td>
<td>
<p>number of repetitions for SRS matrix</p>
</td></tr>
<tr><td><code id="LCE_+3A_sim.mat">sim.mat</code></td>
<td>
<p>similarity matrix; choices are &quot;cts&quot;, &quot;srs&quot;, &quot;asrs&quot;.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a vector containing the cluster assignment from either the CTS, SRS,
or ASRS similarity matrices
</p>


<h3>Author(s)</h3>

<p>Johnson Liu
</p>


<h3>See Also</h3>

<p>Other consensus functions: 
<code><a href="#topic+CSPA">CSPA</a>()</code>,
<code><a href="#topic+LCA">LCA</a>()</code>,
<code><a href="#topic+k_modes">k_modes</a>()</code>,
<code><a href="#topic+majority_voting">majority_voting</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(hgsc)
dat &lt;- hgsc[1:100, 1:50]
x &lt;- consensus_cluster(dat, nk = 4, reps = 4, algorithms = c("km", "hc"),
progress = FALSE)
## Not run: 
LCE(E = x, k = 4, sim.mat = "asrs")

## End(Not run)

x &lt;- apply(x, 2:4, impute_knn, data = dat, seed = 1)
x_imputed &lt;- impute_missing(x, dat, nk = 4)
LCE(E = x_imputed, k = 4, sim.mat = "cts")
</code></pre>

<hr>
<h2 id='majority_voting'>Majority voting</h2><span id='topic+majority_voting'></span>

<h3>Description</h3>

<p>Combine clustering results using majority voting.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>majority_voting(E, is.relabelled = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="majority_voting_+3A_e">E</code></td>
<td>
<p>a matrix of clusterings with number of rows equal to the number of
cases to be clustered, number of columns equal to the clustering obtained
by different resampling of the data, and the third dimension are the
different algorithms. Matrix may already be two-dimensional.</p>
</td></tr>
<tr><td><code id="majority_voting_+3A_is.relabelled">is.relabelled</code></td>
<td>
<p>logical; if <code>FALSE</code> the data will be relabelled using
the first clustering as the reference.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Combine clustering results generated using different algorithms and different
data perturbations by majority voting. The class of a sample is the cluster
label which was selected most often across algorithms and subsamples.
</p>


<h3>Value</h3>

<p>a vector of cluster assignments based on majority voting
</p>


<h3>Author(s)</h3>

<p>Aline Talhouk
</p>


<h3>See Also</h3>

<p>Other consensus functions: 
<code><a href="#topic+CSPA">CSPA</a>()</code>,
<code><a href="#topic+LCA">LCA</a>()</code>,
<code><a href="#topic+LCE">LCE</a>()</code>,
<code><a href="#topic+k_modes">k_modes</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(hgsc)
dat &lt;- hgsc[1:100, 1:50]
cc &lt;- consensus_cluster(dat, nk = 4, reps = 6, algorithms = "pam", progress =
FALSE)
table(majority_voting(cc[, , 1, 1, drop = FALSE], is.relabelled = FALSE))
</code></pre>

<hr>
<h2 id='min_fnorm'>Minimize Frobenius norm for between two matrices</h2><span id='topic+min_fnorm'></span>

<h3>Description</h3>

<p>Finds a permutation of a matrix such that its Frobenius norm with another
matrix is minimized.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>min_fnorm(A, B = diag(nrow(A)))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="min_fnorm_+3A_a">A</code></td>
<td>
<p>data matrix we want to permute</p>
</td></tr>
<tr><td><code id="min_fnorm_+3A_b">B</code></td>
<td>
<p>matrix whose distance with the permuted A we want to minimize. By
default, <code>B &lt;- diag(nrow(A))</code>, so the permutation maximizes the trace of A.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Finds the permutation P of A such that <code style="white-space: pre;">&#8288;||PA - B||&#8288;</code> is minimum in Frobenius
norm. Uses the linear-sum assignment problem (LSAP) solver in the package
<code>clue</code>. The default B is the identity matrix of same dimension, so that the
permutation of A maximizes its trace. This procedure is useful for
constructing a confusion matrix when we don't know the true class labels of a
predicted class and want to compare to a reference class.
</p>


<h3>Value</h3>

<p>Permuted matrix such that it is the permutation of A closest to B
</p>


<h3>Author(s)</h3>

<p>Ravi Varadhan:
https://stat.ethz.ch/pipermail/r-help/2010-April/236664.html
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
set.seed(1)
A &lt;- matrix(sample(1:25, size = 25, rep = FALSE), 5, 5)
min_fnorm(A)
</code></pre>

<hr>
<h2 id='PAC'>Proportion of Ambiguous Clustering</h2><span id='topic+PAC'></span>

<h3>Description</h3>

<p>Given a consensus matrix, returns the proportion of ambiguous clusters (PAC).
This is a robust way to assess clustering performance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>PAC(cm, lower = 0, upper = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="PAC_+3A_cm">cm</code></td>
<td>
<p>consensus matrix. Should be symmetric and values between 0
and 1.</p>
</td></tr>
<tr><td><code id="PAC_+3A_lower">lower</code></td>
<td>
<p>the lower bound that determines what is ambiguous</p>
</td></tr>
<tr><td><code id="PAC_+3A_upper">upper</code></td>
<td>
<p>the upper bound that determines what is ambiguous</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Since a consensus matrix is symmetric, we only look at its lower (or upper)
triangular matrix. The proportion of entries strictly between <code>lower</code> and
<code>upper</code> is the PAC. In a perfect clustering, the consensus matrix would
consist of only 0s and 1s, and the PAC assessed on the (0, 1) interval would
have a perfect score of 0. Using a (0.1, 0.9) interval for defining ambiguity
is common as well.
</p>
<p>The PAC is not, strictly speaking, an internal validity index. Originally
used to choose the optimal number of clusters, here we use it to assess
cluster stability. However, PAC is still agnostic any gold standard
clustering result so we use it like an internal validity index.
</p>


<h3>Value</h3>

<p>the PAC is a score used in clustering performance. The lower it is
the better, because we want minimal ambiguity amongst the consensus.
</p>


<h3>Author(s)</h3>

<p>Derek Chiu
</p>


<h3>References</h3>

<p>Senbabaoglu, Y., Michailidis, G., &amp; Li, J. Z. (2014). Critical
limitations of consensus clustering in class discovery. Scientific reports,
4.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
x &lt;- replicate(100, rbinom(100, 4, 0.2))
y &lt;- consensus_matrix(x)
PAC(y, lower = 0.05, upper = 0.95)
</code></pre>

<hr>
<h2 id='pcn'>Simulate and select null distributions on empirical gene-gene correlations</h2><span id='topic+pcn'></span><span id='topic+pcn_simulate'></span><span id='topic+pcn_select'></span>

<h3>Description</h3>

<p>Using a principal component constructed from the sample space, we simulate
null distributions with univariate Normal distributions using <code>pcn_simulate</code>.
Then a subset of these distributions is chosen using <code>pcn_select</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pcn_simulate(data, n.sim = 50)

pcn_select(data.sim, cl, type = c("rep", "range"), int = 5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pcn_+3A_data">data</code></td>
<td>
<p>data matrix with rows as samples, columns as features</p>
</td></tr>
<tr><td><code id="pcn_+3A_n.sim">n.sim</code></td>
<td>
<p>The number of simulated datasets to simulate</p>
</td></tr>
<tr><td><code id="pcn_+3A_data.sim">data.sim</code></td>
<td>
<p>an object from <code>pcn_simulate</code></p>
</td></tr>
<tr><td><code id="pcn_+3A_cl">cl</code></td>
<td>
<p>vector of cluster memberships</p>
</td></tr>
<tr><td><code id="pcn_+3A_type">type</code></td>
<td>
<p>select either the representative dataset (&quot;rep&quot;) or a range of
datasets (&quot;range&quot;)</p>
</td></tr>
<tr><td><code id="pcn_+3A_int">int</code></td>
<td>
<p>every <code>int</code> data sets from median-ranked <code>data.sim</code> are taken.
Defaults to 5.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>pcn_simulate</code> returns a list of length <code>n.sim</code>. Each element is a
simulated matrix using this &quot;Principal Component Normal&quot; (pcn) procedure.
</p>
<p><code>pcn_select</code> returns a list with elements
</p>

<ul>
<li> <p><code>ranks</code>: When <code>type = "range"</code>, ranks of each extracted dataset shown
</p>
</li>
<li> <p><code>ind</code>: index of representative simulation
</p>
</li>
<li> <p><code>dat</code>: simulation data representation of all in pcNormal
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Derek Chiu
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(9)
A &lt;- matrix(rnorm(300), nrow = 20)
pc.dat &lt;- pcn_simulate(A, n.sim = 50)
cl &lt;- sample(1:4, 20, replace = TRUE)
pc.select &lt;- pcn_select(pc.dat, cl, "rep")
</code></pre>

<hr>
<h2 id='prepare_data'>Prepare data for consensus clustering</h2><span id='topic+prepare_data'></span>

<h3>Description</h3>

<p>Perform feature selection or dimension reduction to remove noise variables.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prepare_data(
  data,
  scale = TRUE,
  type = c("conventional", "robust", "tsne"),
  min.var = 1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="prepare_data_+3A_data">data</code></td>
<td>
<p>data matrix with rows as samples and columns as variables</p>
</td></tr>
<tr><td><code id="prepare_data_+3A_scale">scale</code></td>
<td>
<p>logical; should the data be centered and scaled?</p>
</td></tr>
<tr><td><code id="prepare_data_+3A_type">type</code></td>
<td>
<p>if we use &quot;conventional&quot; measures (default), then the mean and
standard deviation are used for centering and scaling, respectively. If
&quot;robust&quot; measures are specified, the median and median absolute deviation
(MAD) are used. Alternatively, we can apply &quot;tsne&quot; for dimension reduction.</p>
</td></tr>
<tr><td><code id="prepare_data_+3A_min.var">min.var</code></td>
<td>
<p>minimum variability measure threshold used to filter the
feature space for only highly variable features. Only features with a
minimum variability measure across all samples greater than <code>min.var</code> will
be used. If <code>type = "conventional"</code>, the standard deviation is the measure
used, and if <code>type = "robust"</code>, the MAD is the measure used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>We can apply a basic filtering method of feature selection that removes
variables with low signal and (optionally) scales before consensus
clustering. Or, we can use t-SNE dimension reduction to transform the data to
just two variables. This lower-dimensional embedding allows algorithms such
as hierarchical clustering to achieve greater performance.
</p>


<h3>Value</h3>

<p>dataset prepared for usage in <code>consensus_cluster</code>
</p>


<h3>Author(s)</h3>

<p>Derek Chiu
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(2)
x &lt;- replicate(10, rnorm(100))
x.prep &lt;- prepare_data(x)
dim(x)
dim(x.prep)
</code></pre>

<hr>
<h2 id='relabel_class'>Relabel classes to a standard</h2><span id='topic+relabel_class'></span>

<h3>Description</h3>

<p>Relabel clustering categories to match to a standard by minimizing the
Frobenius norm between the two labels.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>relabel_class(pred.cl, ref.cl)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="relabel_class_+3A_pred.cl">pred.cl</code></td>
<td>
<p>vector of predicted cluster assignments</p>
</td></tr>
<tr><td><code id="relabel_class_+3A_ref.cl">ref.cl</code></td>
<td>
<p>vector of reference labels to match to</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of relabeled cluster assignments
</p>


<h3>Author(s)</h3>

<p>Aline Talhouk
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(2)
pred &lt;- sample(1:4, 100, replace = TRUE)
true &lt;- sample(1:4, 100, replace = TRUE)
relabel_class(pred, true)
</code></pre>

<hr>
<h2 id='sigclust'>Significant Testing of Clustering Results</h2><span id='topic+sigclust'></span>

<h3>Description</h3>

<p>Uses the SigClust K-Means algorithm to assess significance of clustering
results.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sigclust(x, k, nsim, nrep = 1, labflag = 0, label = 0, icovest = 2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sigclust_+3A_x">x</code></td>
<td>
<p>data matrix, samples are rows and features are columns</p>
</td></tr>
<tr><td><code id="sigclust_+3A_k">k</code></td>
<td>
<p>cluster size to test against</p>
</td></tr>
<tr><td><code id="sigclust_+3A_nsim">nsim</code></td>
<td>
<p>number of simulations</p>
</td></tr>
<tr><td><code id="sigclust_+3A_nrep">nrep</code></td>
<td>
<p>See <code><a href="sigclust.html#topic+sigclust">sigclust::sigclust()</a></code> for details.</p>
</td></tr>
<tr><td><code id="sigclust_+3A_labflag">labflag</code></td>
<td>
<p>See <code><a href="sigclust.html#topic+sigclust">sigclust::sigclust()</a></code> for details.</p>
</td></tr>
<tr><td><code id="sigclust_+3A_label">label</code></td>
<td>
<p>true class label. See <code><a href="sigclust.html#topic+sigclust">sigclust::sigclust()</a></code> for details.</p>
</td></tr>
<tr><td><code id="sigclust_+3A_icovest">icovest</code></td>
<td>
<p>type of covariance matrix estimation</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is a wrapper for the original <code><a href="sigclust.html#topic+sigclust">sigclust::sigclust()</a></code>, except
that an additional parameter <code>k</code> is allows testing against any number of
clusters. In addition, the default type of covariance estimation is also
different.
</p>


<h3>Value</h3>

<p>An object of class <code>sigclust</code>. See <code><a href="sigclust.html#topic+sigclust">sigclust::sigclust()</a></code> for
details.
</p>


<h3>Author(s)</h3>

<p>Hanwen Huang: <a href="mailto:hanwenh@email.unc.edu">hanwenh@email.unc.edu</a>; Yufeng Liu:
<a href="mailto:yfliu@email.unc.edu">yfliu@email.unc.edu</a>; J. S. Marron: <a href="mailto:marron@email.unc.edu">marron@email.unc.edu</a>
</p>


<h3>References</h3>

<p>Liu, Yufeng, Hayes, David Neil, Nobel, Andrew and Marron, J. S,
2008, <em>Statistical Significance of Clustering for High-Dimension,
Low-Sample Size Data</em>, <em>Journal of the American Statistical Association</em>
<strong>103</strong>(483) 1281&ndash;1293.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(hgsc)
dat &lt;- hgsc[1:100, 1:50]
nk &lt;- 4
cc &lt;- consensus_cluster(dat, nk = nk, reps = 5, algorithms = "pam",
progress = FALSE)
cl.mat &lt;- consensus_combine(cc, element = "class")
lab &lt;- cl.mat$`4`[, 1]
set.seed(1)
str(sigclust(x = dat, k = nk, nsim = 50, labflag = 1, label = lab))

</code></pre>

<hr>
<h2 id='similarity'>Similarity Matrices</h2><span id='topic+similarity'></span><span id='topic+cts'></span><span id='topic+srs'></span><span id='topic+asrs'></span>

<h3>Description</h3>

<p><code>cts</code> computes the connected triple based similarity matrix, <code>srs</code> computes
the simrank based similarity matrix, and <code>asrs</code> computes the approximated
simrank based similarity matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cts(E, dc)

srs(E, dc, R)

asrs(E, dc)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="similarity_+3A_e">E</code></td>
<td>
<p>an N by M matrix of cluster ensembles</p>
</td></tr>
<tr><td><code id="similarity_+3A_dc">dc</code></td>
<td>
<p>decay factor, ranges from 0 to 1 inclusive</p>
</td></tr>
<tr><td><code id="similarity_+3A_r">R</code></td>
<td>
<p>number of iterations for <code>srs</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>an N by N CTS, SRS, or ASRS matrix
</p>


<h3>Author(s)</h3>

<p>Johnson Liu, Derek Chiu
</p>


<h3>References</h3>

<p>MATLAB functions cts, srs, asrs in package LinkCluE by Simon
Garrett
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)
E &lt;- matrix(rep(sample(1:4, 800, replace = TRUE)), nrow = 100)
CTS &lt;- cts(E = E, dc = 0.8)
SRS &lt;- srs(E = E, dc = 0.8, R = 3)
ASRS &lt;- asrs(E = E, dc = 0.8)
purrr::walk(list(CTS, SRS, ASRS), str)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
