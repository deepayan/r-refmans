<!DOCTYPE html><html lang="en"><head><title>Help for package SoftClustering</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {SoftClustering}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#createLowerMShipMatrix'><p>Create Lower Approximation</p></a></li>
<li><a href='#datatypeInteger'><p>Rough k-Means Plotting</p></a></li>
<li><a href='#DemoDataC2D2a'><p>A small two-dimensional dataset with two clusters for demonstration purposes. See examples in the Help/Description of a function, e.g. for HardKMeansDemo().</p></a></li>
<li><a href='#HardKMeans'><p>Hard k-Means</p></a></li>
<li><a href='#HardKMeansDemo'><p>Hard k-Means Demo</p></a></li>
<li><a href='#initializeMeansMatrix'><p>Initialize Means Matrix</p></a></li>
<li><a href='#initMeansC2D2a'><p>Two-dimensional dataset with two initial cluster means for the dataset DemoDataC2D2a. See examples in the Help/Description of a function, e.g. for HardKMeansDemo().</p></a></li>
<li><a href='#initMeansC3D2a'><p>Two-dimensional dataset with three initial cluster means for the dataset DemoDataC2D2a. See examples in the Help/Description of a function, e.g. for HardKMeansDemo().</p></a></li>
<li><a href='#initMeansC4D2a'><p>Two-dimensional dataset with four initial cluster means for the dataset DemoDataC2D2a. See examples in the Help/Description of a function, e.g. for HardKMeansDemo().</p></a></li>
<li><a href='#initMeansC5D2a'><p>Two-dimensional dataset with five initial cluster means for the dataset DemoDataC2D2a. See examples in the Help/Description of a function, e.g. for HardKMeansDemo().</p></a></li>
<li><a href='#normalizeMatrix'><p>Matrix Normalization</p></a></li>
<li><a href='#plotRoughKMeans'><p>Rough k-Means Plotting</p></a></li>
<li><a href='#RoughKMeans_LW'><p>Lingras &amp; West's Rough k-Means</p></a></li>
<li><a href='#RoughKMeans_PE'><p>Peters' Rough k-Means</p></a></li>
<li><a href='#RoughKMeans_PI'><p><code>PI</code> Rough k-Means</p></a></li>
<li><a href='#RoughKMeans_SHELL'><p>Rough k-Means Shell</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Soft Clustering Algorithms</td>
</tr>
<tr>
<td>Description:</td>
<td>It contains soft clustering algorithms, in particular approaches derived from rough set theory: Lingras &amp; West original rough k-means, Peters' refined rough k-means, and PI rough k-means. It also contains classic k-means and a corresponding illustrative demo.</td>
</tr>
<tr>
<td>Version:</td>
<td>2.1.3</td>
</tr>
<tr>
<td>Author:</td>
<td>G. Peters (Ed.)</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>G. Peters &lt;peters.activities@gmail.com&gt;</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.1)</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-08-17 15:09:47 UTC; myaccount</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-08-18 07:52:35 UTC</td>
</tr>
</table>
<hr>
<h2 id='createLowerMShipMatrix'>Create Lower Approximation</h2><span id='topic+createLowerMShipMatrix'></span>

<h3>Description</h3>

<p>Creates a lower approximation out of an upper approximation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>createLowerMShipMatrix(upperMShipMatrix)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="createLowerMShipMatrix_+3A_uppermshipmatrix">upperMShipMatrix</code></td>
<td>
<p>An upper approximation matrix.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns the corresponding lower approximation.
</p>


<h3>Author(s)</h3>

<p>G. Peters.
</p>

<hr>
<h2 id='datatypeInteger'>Rough k-Means Plotting</h2><span id='topic+datatypeInteger'></span>

<h3>Description</h3>

<p>Checks for integer.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>datatypeInteger(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="datatypeInteger_+3A_x">x</code></td>
<td>
<p>As a replacement for is.integer(). is.integer() delivers FALSE when the variable is numeric (as superset for integer etc.)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>TRUE if x is integer otherwise FALSE.
</p>


<h3>Author(s)</h3>

<p>G. Peters.
</p>

<hr>
<h2 id='DemoDataC2D2a'>A small two-dimensional dataset with two clusters for demonstration purposes. See examples in the Help/Description of a function, e.g. for HardKMeansDemo().</h2><span id='topic+DemoDataC2D2a'></span>

<h3>Description</h3>

<p>A small two-dimensional dataset with two clusters for demonstration purposes. See examples in the Help/Description of a function, e.g. for HardKMeansDemo().
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(DemoDataC2D2a)
</code></pre>


<h3>Format</h3>

<p>Rows: objects, columns: features
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(DemoDataC2D2a)
</code></pre>

<hr>
<h2 id='HardKMeans'>Hard k-Means</h2><span id='topic+HardKMeans'></span>

<h3>Description</h3>

<p>HardKMeans performs classic (hard) k-means.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>HardKMeans(dataMatrix, meansMatrix, nClusters, maxIterations)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="HardKMeans_+3A_datamatrix">dataMatrix</code></td>
<td>
<p>Matrix with the objects to be clustered. Dimension: [nObjects x nFeatures].</p>
</td></tr>
<tr><td><code id="HardKMeans_+3A_meansmatrix">meansMatrix</code></td>
<td>
<p>Select means derived from 1 = random (unity interval), 2 = maximum distances, matrix [nClusters x nFeatures] = self-defined means. Default: 2 = maximum distances.</p>
</td></tr>
<tr><td><code id="HardKMeans_+3A_nclusters">nClusters</code></td>
<td>
<p>Number of clusters: Integer in [2, nObjects). Note, nCluster must be set even when meansMatrix is a matrix. For transparency, nClusters will not be overridden by the number of clusters derived from meansMatrix. Default: nClusters=2.</p>
</td></tr>
<tr><td><code id="HardKMeans_+3A_maxiterations">maxIterations</code></td>
<td>
<p>Maximum number of iterations. Default: maxIterations=100.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>$upperApprox</code>: Obtained upper approximations [nObjects x nClusters]. Note: Apply function <code>createLowerMShipMatrix()</code> to obtain lower approximations; and for the boundary: <code>boundary = upperApprox - lowerApprox</code>.
</p>
<p><code>$clusterMeans</code>: Obtained means [nClusters x nFeatures].
</p>
<p><code>$nIterations</code>: Number of iterations.
</p>


<h3>Author(s)</h3>

<p>M. Goetz, G. Peters, Y. Richter, D. Sacker, T. Wochinger.
</p>


<h3>References</h3>

<p>Lloyd, S.P. (1982) Least squares quantization in PCM. <em>IEEE Transactions on Information Theory</em> <b>28</b>, 128&ndash;137. &lt;doi:10.1016/j.ijar.2012.10.003&gt;.
</p>
<p>Peters, G.; Crespo, F.; Lingras, P. and Weber, R. (2013) Soft clustering &ndash; fuzzy and rough approaches and their extensions and derivatives. <em>International Journal of Approximate Reasoning</em> <b>54</b>, 307&ndash;322. &lt;doi:10.1016/j.ijar.2012.10.003&gt;.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># An illustrative example clustering the sample data set DemoDataC2D2a.txt
HardKMeans(DemoDataC2D2a, 2, 2, 100)
</code></pre>

<hr>
<h2 id='HardKMeansDemo'>Hard k-Means Demo</h2><span id='topic+HardKMeansDemo'></span>

<h3>Description</h3>

<p>HardKMeansDemo shows how hard k-means performs stepwise. The number of features is set to 2 and the maximum number of iterations is 100.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>HardKMeansDemo(dataMatrix, meansMatrix, nClusters)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="HardKMeansDemo_+3A_datamatrix">dataMatrix</code></td>
<td>
<p>Matrix with the objects to be clustered. Dimension: [nObjects x nFeatures]. Default: no default set.</p>
</td></tr>
<tr><td><code id="HardKMeansDemo_+3A_meansmatrix">meansMatrix</code></td>
<td>
<p>Select means derived from 1 = random (unity interval), 2 = maximum distances, matrix [nClusters x nFeatures=2] = self-defined means. Default: meansMatrix=1 (random).</p>
</td></tr>
<tr><td><code id="HardKMeansDemo_+3A_nclusters">nClusters</code></td>
<td>
<p>Number of clusters: Integer in [2, min(5, nObjects-1)]. Note, nCluster must be set even when meansMatrix is a matrix. For transparency, nClusters will not be overridden by the number of clusters derived from meansMatrix. Default: nClusters=2.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>None.
</p>


<h3>Author(s)</h3>

<p>G. Peters.
</p>


<h3>References</h3>

<p>Lloyd, S.P. (1982) Least squares quantization in PCM. <em>IEEE Transactions on Information Theory</em> <b>28</b>, 128&ndash;137. &lt;doi:10.1016/j.ijar.2012.10.003&gt;.
</p>
<p>Peters, G.; Crespo, F.; Lingras, P. and Weber, R. (2013) Soft clustering &ndash; fuzzy and rough approaches and their extensions and derivatives. <em>International Journal of Approximate Reasoning</em> <b>54</b>, 307&ndash;322. &lt;doi:10.1016/j.ijar.2012.10.003&gt;.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Clustering the data set DemoDataC2D2a.txt (nClusters=2, random initial means)
HardKMeansDemo(DemoDataC2D2a,1,2)
# Clustering the data set DemoDataC2D2a.txt (nClusters=2,3,4; initially set means)
HardKMeansDemo(DemoDataC2D2a,initMeansC2D2a,2)
HardKMeansDemo(DemoDataC2D2a,initMeansC3D2a,3)
HardKMeansDemo(DemoDataC2D2a,initMeansC4D2a,4)
# Clustering the data set DemoDataC2D2a.txt (nClusters=5, initially set means)
# It leads to an empty cluster: a (rare) case for an abnormal termination of k-means.
HardKMeansDemo(DemoDataC2D2a,initMeansC5D2a,5)
</code></pre>

<hr>
<h2 id='initializeMeansMatrix'>Initialize Means Matrix</h2><span id='topic+initializeMeansMatrix'></span>

<h3>Description</h3>

<p>initializeMeansMatrix delivers an initial means matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>initializeMeansMatrix(dataMatrix, nClusters, meansMatrix)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="initializeMeansMatrix_+3A_datamatrix">dataMatrix</code></td>
<td>
<p>Matrix with the objects as basis for the means matrix.</p>
</td></tr>
<tr><td><code id="initializeMeansMatrix_+3A_nclusters">nClusters</code></td>
<td>
<p>Number of clusters.</p>
</td></tr>
<tr><td><code id="initializeMeansMatrix_+3A_meansmatrix">meansMatrix</code></td>
<td>
<p>Select means derived from 1 = random (unity interval), 2 = maximum distances, matrix [nClusters x nFeatures] = self-defined means (will be returned unchanged). Default: 2 = maximum distances.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Initial means matrix [nClusters x nFeatures].
</p>


<h3>Author(s)</h3>

<p>M. Goetz, G. Peters, Y. Richter, D. Sacker, T. Wochinger.
</p>

<hr>
<h2 id='initMeansC2D2a'>Two-dimensional dataset with two initial cluster means for the dataset DemoDataC2D2a. See examples in the Help/Description of a function, e.g. for HardKMeansDemo().</h2><span id='topic+initMeansC2D2a'></span>

<h3>Description</h3>

<p>Two-dimensional dataset with two initial cluster means for the dataset DemoDataC2D2a. See examples in the Help/Description of a function, e.g. for HardKMeansDemo().
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(initMeansC2D2a)
</code></pre>


<h3>Format</h3>

<p>Rows: objects, columns: features
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(initMeansC2D2a)
</code></pre>

<hr>
<h2 id='initMeansC3D2a'>Two-dimensional dataset with three initial cluster means for the dataset DemoDataC2D2a. See examples in the Help/Description of a function, e.g. for HardKMeansDemo().</h2><span id='topic+initMeansC3D2a'></span>

<h3>Description</h3>

<p>Two-dimensional dataset with three initial cluster means for the dataset DemoDataC2D2a. See examples in the Help/Description of a function, e.g. for HardKMeansDemo().
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(initMeansC3D2a)
</code></pre>


<h3>Format</h3>

<p>Rows: objects, columns: features
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(initMeansC3D2a)
</code></pre>

<hr>
<h2 id='initMeansC4D2a'>Two-dimensional dataset with four initial cluster means for the dataset DemoDataC2D2a. See examples in the Help/Description of a function, e.g. for HardKMeansDemo().</h2><span id='topic+initMeansC4D2a'></span>

<h3>Description</h3>

<p>Two-dimensional dataset with four initial cluster means for the dataset DemoDataC2D2a. See examples in the Help/Description of a function, e.g. for HardKMeansDemo().
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(initMeansC4D2a)
</code></pre>


<h3>Format</h3>

<p>Rows: objects, columns: features
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(initMeansC4D2a)
</code></pre>

<hr>
<h2 id='initMeansC5D2a'>Two-dimensional dataset with five initial cluster means for the dataset DemoDataC2D2a. See examples in the Help/Description of a function, e.g. for HardKMeansDemo().</h2><span id='topic+initMeansC5D2a'></span>

<h3>Description</h3>

<p>Two-dimensional dataset with five initial cluster means for the dataset DemoDataC2D2a. See examples in the Help/Description of a function, e.g. for HardKMeansDemo().
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(initMeansC5D2a)
</code></pre>


<h3>Format</h3>

<p>Rows: objects, columns: features
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(initMeansC5D2a)
</code></pre>

<hr>
<h2 id='normalizeMatrix'>Matrix Normalization</h2><span id='topic+normalizeMatrix'></span>

<h3>Description</h3>

<p>normalizeMatrix delivers a normalized matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>normalizeMatrix(dataMatrix, normMethod, bycol)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="normalizeMatrix_+3A_datamatrix">dataMatrix</code></td>
<td>
<p>Matrix with the objects to be normalized.</p>
</td></tr>
<tr><td><code id="normalizeMatrix_+3A_normmethod">normMethod</code></td>
<td>
<p>1 = unity interval, 2 = normal distribution (sample variance), 3 = normal distribution (population variance). Any other value returns the matrix unchanged. Default: meansMatrix = 1 (unity interval).</p>
</td></tr>
<tr><td><code id="normalizeMatrix_+3A_bycol">bycol</code></td>
<td>
<p>TRUE = columns are normalized, i.e., each column is considered separately (e.g., in case of the unity interval and a column colA: max(colA)=1 and min(colA)=0). For bycol = FALSE rows are normalized. Default: bycol = TRUE (columns are normalized).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Normalized matrix.
</p>


<h3>Author(s)</h3>

<p>M. Goetz, G. Peters, Y. Richter, D. Sacker, T. Wochinger.
</p>

<hr>
<h2 id='plotRoughKMeans'>Rough k-Means Plotting</h2><span id='topic+plotRoughKMeans'></span>

<h3>Description</h3>

<p>plotRoughKMeans plots the rough clustering results in 2D. Note: Plotting is limited to a maximum of 5 clusters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotRoughKMeans(dataMatrix, upperMShipMatrix, meansMatrix, plotDimensions, colouredPlot)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plotRoughKMeans_+3A_datamatrix">dataMatrix</code></td>
<td>
<p>Matrix with the objects to be plotted.</p>
</td></tr>
<tr><td><code id="plotRoughKMeans_+3A_uppermshipmatrix">upperMShipMatrix</code></td>
<td>
<p>Corresponding matrix with upper approximations.</p>
</td></tr>
<tr><td><code id="plotRoughKMeans_+3A_meansmatrix">meansMatrix</code></td>
<td>
<p>Corresponding means matrix.</p>
</td></tr>
<tr><td><code id="plotRoughKMeans_+3A_plotdimensions">plotDimensions</code></td>
<td>
<p>An integer vector of the length 2. Defines the to be plotted feature dimensions, i.e., max(plotDimensions = c(1:2)) &lt;= nFeatures. Default: plotDimensions = c(1:2).</p>
</td></tr>
<tr><td><code id="plotRoughKMeans_+3A_colouredplot">colouredPlot</code></td>
<td>
<p>Select TRUE = colouredPlot plot, FALSE = black/white plot.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>2D-plot of clustering results. The boundary objects are represented by stars (*).
</p>


<h3>Author(s)</h3>

<p>G. Peters.
</p>

<hr>
<h2 id='RoughKMeans_LW'>Lingras &amp; West's Rough k-Means</h2><span id='topic+RoughKMeans_LW'></span>

<h3>Description</h3>

<p>RoughKMeans_LW performs Lingras &amp; West's k-means clustering algorithm. The commonly accepted relative threshold is applied.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>RoughKMeans_LW(dataMatrix, meansMatrix, nClusters, maxIterations, threshold, weightLower)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="RoughKMeans_LW_+3A_datamatrix">dataMatrix</code></td>
<td>
<p>Matrix with the objects to be clustered. Dimension: [nObjects x nFeatures].</p>
</td></tr>
<tr><td><code id="RoughKMeans_LW_+3A_meansmatrix">meansMatrix</code></td>
<td>
<p>Select means derived from 1 = random (unity interval), 2 = maximum distances, matrix [nClusters x nFeatures] = self-defined means. Default: 2 = maximum distances.</p>
</td></tr>
<tr><td><code id="RoughKMeans_LW_+3A_nclusters">nClusters</code></td>
<td>
<p>Number of clusters: Integer in [2, nObjects). Note, nCluster must be set even when meansMatrix is a matrix. For transparency, nClusters will not be overridden by the number of clusters derived from meansMatrix. Default: nClusters=2.</p>
</td></tr>
<tr><td><code id="RoughKMeans_LW_+3A_maxiterations">maxIterations</code></td>
<td>
<p>Maximum number of iterations. Default: maxIterations=100.</p>
</td></tr>
<tr><td><code id="RoughKMeans_LW_+3A_threshold">threshold</code></td>
<td>
<p>Relative threshold in rough k-means algorithms (threshold &gt;= 1.0).  Default: threshold = 1.5.</p>
</td></tr>
<tr><td><code id="RoughKMeans_LW_+3A_weightlower">weightLower</code></td>
<td>
<p>Weight of the lower approximation in rough k-means algorithms (0.0 &lt;= weightLower &lt;= 1.0).  Default: weightLower = 0.7.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>$upperApprox</code>: Obtained upper approximations [nObjects x nClusters]. Note: Apply function <code>createLowerMShipMatrix()</code> to obtain lower approximations; and for the boundary: <code>boundary = upperApprox - lowerApprox</code>.
</p>
<p><code>$clusterMeans</code>: Obtained means [nClusters x nFeatures].
</p>
<p><code>$nIterations</code>: Number of iterations.
</p>


<h3>Author(s)</h3>

<p>M. Goetz, G. Peters, Y. Richter, D. Sacker, T. Wochinger.
</p>


<h3>References</h3>

<p>Lingras, P. and West, C. (2004) Interval Set Clustering of web users with rough k-means. <em>Journal of Intelligent Information Systems</em> <b>23</b>, 5&ndash;16. &lt;doi:10.1023/b:jiis.0000029668.88665.1a&gt;.
</p>
<p>Peters, G. (2006) Some refinements of rough k-means clustering. <em>Pattern Recognition</em> <b>39</b>, 1481&ndash;1491. &lt;doi:10.1016/j.patcog.2006.02.002&gt;.
</p>
<p>Lingras, P. and Peters, G. (2011) Rough Clustering. <em>WIREs Data Mining and Knowledge Discovery</em> <b>1</b>, 64&ndash;72. &lt;doi:10.1002/widm.16&gt;.
</p>
<p>Lingras, P. and Peters, G. (2012) Applying rough set concepts to clustering. In: Peters, G.; Lingras, P.; Slezak, D. and Yao, Y. Y. (Eds.) <em>Rough Sets: Selected Methods and Applications in Management and Engineering</em>, Springer, 23&ndash;37. &lt;doi:10.1007/978-1-4471-2760-4_2&gt;.
</p>
<p>Peters, G.; Crespo, F.; Lingras, P. and Weber, R. (2013) Soft clustering &ndash; fuzzy and rough approaches and their extensions and derivatives. <em>International Journal of Approximate Reasoning</em> <b>54</b>, 307&ndash;322. &lt;doi:10.1016/j.ijar.2012.10.003&gt;.
</p>
<p>Peters, G. (2014) Rough clustering utilizing the principle of indifference. <em>Information Sciences</em> <b>277</b>, 358&ndash;374. &lt;doi:10.1016/j.ins.2014.02.073&gt;.
</p>
<p>Peters, G. (2015) Is there any need for rough clustering?  <em>Pattern Recognition Letters</em> <b>53</b>, 31&ndash;37. &lt;doi:10.1016/j.patrec.2014.11.003&gt;.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># An illustrative example clustering the sample data set DemoDataC2D2a.txt
RoughKMeans_LW(DemoDataC2D2a, 2, 2, 100, 1.5, 0.7)
</code></pre>

<hr>
<h2 id='RoughKMeans_PE'>Peters' Rough k-Means</h2><span id='topic+RoughKMeans_PE'></span>

<h3>Description</h3>

<p>RoughKMeans_PE performs Peters' k-means clustering algorithm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>RoughKMeans_PE(dataMatrix, meansMatrix, nClusters, maxIterations, threshold, weightLower)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="RoughKMeans_PE_+3A_datamatrix">dataMatrix</code></td>
<td>
<p>Matrix with the objects to be clustered. Dimension: [nObjects x nFeatures].</p>
</td></tr>
<tr><td><code id="RoughKMeans_PE_+3A_meansmatrix">meansMatrix</code></td>
<td>
<p>Select means derived from 1 = random (unity interval), 2 = maximum distances, matrix [nClusters x nFeatures] = self-defined means. Default: 2 = maximum distances.</p>
</td></tr>
<tr><td><code id="RoughKMeans_PE_+3A_nclusters">nClusters</code></td>
<td>
<p>Number of clusters: Integer in [2, nObjects). Note, nCluster must be set even when meansMatrix is a matrix. For transparency, nClusters will not be overridden by the number of clusters derived from meansMatrix. Default: nClusters=2.</p>
</td></tr>
<tr><td><code id="RoughKMeans_PE_+3A_maxiterations">maxIterations</code></td>
<td>
<p>Maximum number of iterations. Default: maxIterations=100.</p>
</td></tr>
<tr><td><code id="RoughKMeans_PE_+3A_threshold">threshold</code></td>
<td>
<p>Relative threshold in rough k-means algorithms (threshold &gt;= 1.0).  Default: threshold = 1.5.</p>
</td></tr>
<tr><td><code id="RoughKMeans_PE_+3A_weightlower">weightLower</code></td>
<td>
<p>Weight of the lower approximation in rough k-means algorithms (0.0 &lt;= weightLower &lt;= 1.0).  Default: weightLower = 0.7.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>$upperApprox</code>: Obtained upper approximations [nObjects x nClusters]. Note: Apply function <code>createLowerMShipMatrix()</code> to obtain lower approximations; and for the boundary: <code>boundary = upperApprox - lowerApprox</code>.
</p>
<p><code>$clusterMeans</code>: Obtained means [nClusters x nFeatures].
</p>
<p><code>$nIterations</code>: Number of iterations.
</p>


<h3>Author(s)</h3>

<p>M. Goetz, G. Peters, Y. Richter, D. Sacker, T. Wochinger.
</p>


<h3>References</h3>

<p>Peters, G. (2006) Some refinements of rough k-means clustering. <em>Pattern Recognition</em> <b>39</b>, 1481&ndash;1491. &lt;doi:10.1016/j.patcog.2006.02.002&gt;.
</p>
<p>Peters, G.; Crespo, F.; Lingras, P. and Weber, R. (2013) Soft clustering &ndash; fuzzy and rough approaches and their extensions and derivatives. <em>International Journal of Approximate Reasoning</em> <b>54</b>, 307&ndash;322. &lt;doi:10.1016/j.ijar.2012.10.003&gt;.
</p>
<p>Peters, G. (2014) Rough clustering utilizing the principle of indifference. <em>Information Sciences</em> <b>277</b>, 358&ndash;374. &lt;doi:10.1016/j.ins.2014.02.073&gt;.
</p>
<p>Peters, G. (2015) Is there any need for rough clustering?  <em>Pattern Recognition Letters</em> <b>53</b>, 31&ndash;37. &lt;doi:10.1016/j.patrec.2014.11.003&gt;.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># An illustrative example clustering the sample data set DemoDataC2D2a.txt
RoughKMeans_PE(DemoDataC2D2a, 2, 2, 100, 1.5, 0.7)
</code></pre>

<hr>
<h2 id='RoughKMeans_PI'><code>PI</code> Rough k-Means</h2><span id='topic+RoughKMeans_PI'></span>

<h3>Description</h3>

<p>RoughKMeans_PI performs <code>pi</code> k-means clustering algorithm in its standard case. Therefore, weights are not required.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>RoughKMeans_PI(dataMatrix, meansMatrix, nClusters, maxIterations, threshold)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="RoughKMeans_PI_+3A_datamatrix">dataMatrix</code></td>
<td>
<p>Matrix with the objects to be clustered. Dimension: [nObjects x nFeatures].</p>
</td></tr>
<tr><td><code id="RoughKMeans_PI_+3A_meansmatrix">meansMatrix</code></td>
<td>
<p>Select means derived from 1 = random (unity interval), 2 = maximum distances, matrix [nClusters x nFeatures] = self-defined means. Default: 2 = maximum distances.</p>
</td></tr>
<tr><td><code id="RoughKMeans_PI_+3A_nclusters">nClusters</code></td>
<td>
<p>Number of clusters: Integer in [2, nObjects). Note, nCluster must be set even when meansMatrix is a matrix. For transparency, nClusters will not be overridden by the number of clusters derived from meansMatrix. Default: nClusters=2.</p>
</td></tr>
<tr><td><code id="RoughKMeans_PI_+3A_maxiterations">maxIterations</code></td>
<td>
<p>Maximum number of iterations. Default: maxIterations=100.</p>
</td></tr>
<tr><td><code id="RoughKMeans_PI_+3A_threshold">threshold</code></td>
<td>
<p>Relative threshold in rough k-means algorithms (threshold &gt;= 1.0).  Default: threshold = 1.5.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>$upperApprox</code>: Obtained upper approximations [nObjects x nClusters]. Note: Apply function <code>createLowerMShipMatrix()</code> to obtain lower approximations; and for the boundary: <code>boundary = upperApprox - lowerApprox</code>.
</p>
<p><code>$clusterMeans</code>: Obtained means [nClusters x nFeatures].
</p>
<p><code>$nIterations</code>: Number of iterations.
</p>


<h3>Author(s)</h3>

<p>M. Goetz, G. Peters, Y. Richter, D. Sacker, T. Wochinger.
</p>


<h3>References</h3>

<p>Peters, G. (2006) Some refinements of rough k-means clustering. <em>Pattern Recognition</em> <b>39</b>, 1481&ndash;1491. &lt;doi:10.1016/j.patcog.2006.02.002&gt;.
</p>
<p>Peters, G.; Crespo, F.; Lingras, P. and Weber, R. (2013) Soft clustering &ndash; fuzzy and rough approaches and their extensions and derivatives. <em>International Journal of Approximate Reasoning</em> <b>54</b>, 307&ndash;322. &lt;doi:10.1016/j.ijar.2012.10.003&gt;.
</p>
<p>Peters, G. (2014) Rough clustering utilizing the principle of indifference. <em>Information Sciences</em> <b>277</b>, 358&ndash;374. &lt;doi:10.1016/j.ins.2014.02.073&gt;.
</p>
<p>Peters, G. (2015) Is there any need for rough clustering?  <em>Pattern Recognition Letters</em> <b>53</b>, 31&ndash;37. &lt;doi:10.1016/j.patrec.2014.11.003&gt;.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># An illustrative example clustering the sample data set DemoDataC2D2a.txt
RoughKMeans_PI(DemoDataC2D2a, 2, 2, 100, 1.5)
</code></pre>

<hr>
<h2 id='RoughKMeans_SHELL'>Rough k-Means Shell</h2><span id='topic+RoughKMeans_SHELL'></span>

<h3>Description</h3>

<p>RoughKMeans_SHELL performs rough k-means algorithms with  options for normalization and a 2D-plot of the results.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>RoughKMeans_SHELL(clusterAlgorithm, dataMatrix, meansMatrix, nClusters, 
                  normalizationMethod, maxIterations, plotDimensions, 
                  colouredPlot, threshold, weightLower)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="RoughKMeans_SHELL_+3A_clusteralgorithm">clusterAlgorithm</code></td>
<td>
<p>Select 0 = classic k-means, 1 = Lingras &amp; West's rough k-means, 2 = Peters' rough k-means, 3 = <code class="reqn">\pi</code> rough k-means. Default: clusterAlgorithm = 3 (<code class="reqn">\pi</code> rough k-means).</p>
</td></tr>
<tr><td><code id="RoughKMeans_SHELL_+3A_datamatrix">dataMatrix</code></td>
<td>
<p>Matrix with the objects to be clustered. Dimension: [nObjects x nFeatures].</p>
</td></tr>
<tr><td><code id="RoughKMeans_SHELL_+3A_meansmatrix">meansMatrix</code></td>
<td>
<p>Select means derived from 1 = random (unity interval), 2 = maximum distances, matrix [nClusters x nFeatures] = self-defined means. Default: 2 = maximum distances.</p>
</td></tr>
<tr><td><code id="RoughKMeans_SHELL_+3A_nclusters">nClusters</code></td>
<td>
<p>Number of clusters: Integer in [2, nObjects). Note, nCluster must be set even when meansMatrix is a matrix. For transparency, nClusters will not be overridden by the number of clusters derived from meansMatrix. Default: nClusters=2. Note: Plotting is limited to a maximum of 5 clusters.</p>
</td></tr>
<tr><td><code id="RoughKMeans_SHELL_+3A_normalizationmethod">normalizationMethod</code></td>
<td>
<p>1 = unity interval, 2 = normal distribution (sample variance), 3 = normal distribution (population variance). Any other value returns the matrix unchanged. Default: meansMatrix = 1 (unity interval).</p>
</td></tr>
<tr><td><code id="RoughKMeans_SHELL_+3A_maxiterations">maxIterations</code></td>
<td>
<p>Maximum number of iterations. Default: maxIterations=100.</p>
</td></tr>
<tr><td><code id="RoughKMeans_SHELL_+3A_plotdimensions">plotDimensions</code></td>
<td>
<p>An integer vector of the length 2. Defines the to be plotted feature dimensions, i.e., max(plotDimensions = c(1:2)) &lt;= nFeatures. Default: plotDimensions = c(1:2).</p>
</td></tr>
<tr><td><code id="RoughKMeans_SHELL_+3A_colouredplot">colouredPlot</code></td>
<td>
<p>Select TRUE = colouredPlot plot, FALSE = black/white plot.</p>
</td></tr>
<tr><td><code id="RoughKMeans_SHELL_+3A_threshold">threshold</code></td>
<td>
<p>Relative threshold in rough k-means algorithms (threshold &gt;= 1.0).  Default: threshold = 1.5. Note: It can be ignored for classic k-means.</p>
</td></tr>
<tr><td><code id="RoughKMeans_SHELL_+3A_weightlower">weightLower</code></td>
<td>
<p>Weight of the lower approximation in rough k-means algorithms (0.0 &lt;= weightLower &lt;= 1.0).  Default: weightLower = 0.7. Note: It can be ignored for classic k-means and <code class="reqn">\pi</code> rough k-means</p>
</td></tr>
</table>


<h3>Value</h3>

<p>2D-plot of clustering results. The boundary objects are represented by stars (*).
</p>
<p><code>$upperApprox</code>: Obtained upper approximations [nObjects x nClusters]. Note: Apply function <code>createLowerMShipMatrix()</code> to obtain lower approximations; and for the boundary: <code>boundary = upperApprox - lowerApprox</code>.
</p>
<p><code>$clusterMeans</code>: Obtained means [nClusters x nFeatures].
</p>
<p><code>$nIterations</code>: Number of iterations.
</p>


<h3>Author(s)</h3>

<p>M. Goetz, G. Peters, Y. Richter, D. Sacker, T. Wochinger.
</p>


<h3>References</h3>

<p>Lloyd, S.P. (1982) Least squares quantization in PCM. <em>IEEE Transactions on Information Theory</em> <b>28</b>, 128&ndash;137. &lt;doi:10.1016/j.ijar.2012.10.003&gt;.
</p>
<p>Lingras, P. and West, C. (2004) Interval Set Clustering of web users with rough k-means. <em>Journal of Intelligent Information Systems</em> <b>23</b>, 5&ndash;16. &lt;doi:10.1023/b:jiis.0000029668.88665.1a&gt;.
</p>
<p>Peters, G. (2006) Some refinements of rough k-means clustering. <em>Pattern Recognition</em> <b>39</b>, 1481&ndash;1491. &lt;doi:10.1016/j.patcog.2006.02.002&gt;.
</p>
<p>Lingras, P. and Peters, G. (2011) Rough Clustering. <em>WIREs Data Mining and Knowledge Discovery</em> <b>1</b>, 64&ndash;72. &lt;doi:10.1002/widm.16&gt;.
</p>
<p>Lingras, P. and Peters, G. (2012) Applying rough set concepts to clustering. In: Peters, G.; Lingras, P.; Slezak, D. and Yao, Y. Y. (Eds.) <em>Rough Sets: Selected Methods and Applications in Management and Engineering</em>, Springer, 23&ndash;37. &lt;doi:10.1007/978-1-4471-2760-4_2&gt;.
</p>
<p>Peters, G.; Crespo, F.; Lingras, P. and Weber, R. (2013) Soft clustering &ndash; fuzzy and rough approaches and their extensions and derivatives. <em>International Journal of Approximate Reasoning</em> <b>54</b>, 307&ndash;322. &lt;doi:10.1016/j.ijar.2012.10.003&gt;.
</p>
<p>Peters, G. (2014) Rough clustering utilizing the principle of indifference. <em>Information Sciences</em> <b>277</b>, 358&ndash;374. &lt;doi:10.1016/j.ins.2014.02.073&gt;.
</p>
<p>Peters, G. (2015) Is there any need for rough clustering?  <em>Pattern Recognition Letters</em> <b>53</b>, 31&ndash;37. &lt;doi:10.1016/j.patrec.2014.11.003&gt;.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># An illustrative example clustering the sample data set DemoDataC2D2a.txt
RoughKMeans_SHELL(3, DemoDataC2D2a, 2, 2, 1, 100, c(1:2), TRUE, 1.5, 0.7)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
