<!DOCTYPE html><html><head><title>Help for package ICtest</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {ICtest}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#components'>
<p>Generic Components Extraction Function</p></a></li>
<li><a href='#covSIR'>
<p>Supervised Scatter Matrix as Used in Sliced Inverse Regression</p></a></li>
<li><a href='#FOBIasymp'>
<p>Testing for the Number of Gaussian Components in NGCA or ICA Using FOBI</p></a></li>
<li><a href='#FOBIboot'>
<p>Boostrap-based Testing for the Number of Gaussian Components in ICA Using FOBI</p></a></li>
<li><a href='#FOBIladle'>
<p>Ladle Estimate to Estimate the Number of Gaussian Components in ICA or NGCA</p></a></li>
<li><a href='#ggladleplot'>
<p>Ladle Plot for an Object of Class ladle Using ggplot2</p></a></li>
<li><a href='#ggplot.ictest'>
<p>Scatterplot Matrix for a ictest Object using ggplot2</p></a></li>
<li><a href='#ggplot.ladle'>
<p>Scatterplot Matrix for a ladle Object using ggplot2</p></a></li>
<li><a href='#ggscreeplot'>
<p>ggplot2-style screeplot</p></a></li>
<li><a href='#ggscreeplot.ictest'>
<p>Screeplot for an ictest Object Using ggplot2</p></a></li>
<li><a href='#ICSboot'>
<p>Boostrap-based Testing for the Number of Gaussian Components in NGCA Using Two Scatter Matrices</p></a></li>
<li><a href='#ladle'>
<p>Ladle estimate for an arbitrary matrix</p></a></li>
<li><a href='#ladleplot'>
<p>Ladle Plot for an Object of Class ladle</p></a></li>
<li><a href='#NGPP'><p>Non-Gaussian Projection Pursuit</p></a></li>
<li><a href='#NGPPest'><p>Signal Subspace Dimension Testing Using non-Gaussian Projection Pursuit</p></a></li>
<li><a href='#NGPPsim'><p>Signal Subspace Dimension Testing Using non-Gaussian Projection Pursuit</p></a></li>
<li><a href='#PCAasymp'>
<p>Testing for Subsphericity using the Covariance Matrix or Tyler's Shape Matrix</p></a></li>
<li><a href='#PCAaug'>
<p>Augmentation Estimate for PCA</p></a></li>
<li><a href='#PCAboot'>
<p>Bootstrap-Based Testing for Subsphericity</p></a></li>
<li><a href='#PCAladle'>
<p>Ladle Estimate for PCA</p></a></li>
<li><a href='#PCAschott'>
<p>Testing for Subsphericity using the Schott's test</p></a></li>
<li><a href='#plot.ictest'>
<p>Scatterplot Matrix for a ictest Object</p></a></li>
<li><a href='#plot.ladle'>
<p>Plotting an Object of Class ladle</p></a></li>
<li><a href='#print.ladle'>
<p>Printing an Object of Class ladle</p></a></li>
<li><a href='#rMU'>
<p>Greek Letter mu Shaped Bivariate Data Generation</p></a></li>
<li><a href='#rOMEGA'>
<p>Greek Letter Omega Shaped Bivariate Data Generation</p></a></li>
<li><a href='#rorth'>
<p>Random Orthogonal Matrix Creation Uniform WRT the Haar Measure.</p></a></li>
<li><a href='#screeplot.ictest'>
<p>Screeplot for an ictest Object</p></a></li>
<li><a href='#SIRasymp'>
<p>Testing the Subspace Dimension for Sliced Inverse Regression.</p></a></li>
<li><a href='#SIRboot'>
<p>Testing the Subspace Dimension for Sliced Inverse Regression Using Bootstrapping.</p></a></li>
<li><a href='#SIRladle'>
<p>Ladle Estimate for SIR</p></a></li>
<li><a href='#summary.ladle'>
<p>Summarizing an Object of Class ladle</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Estimating and Testing the Number of Interesting Components in
Linear Dimension Reduction</td>
</tr>
<tr>
<td>Version:</td>
<td>0.3-5</td>
</tr>
<tr>
<td>Date:</td>
<td>2022-05-18</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Klaus Nordhausen &lt;klaus.k.nordhausen@jyu.fi&gt;</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo</td>
</tr>
<tr>
<td>Depends:</td>
<td>JADE, ICS (&ge; 1.3-0), ggplot2</td>
</tr>
<tr>
<td>Imports:</td>
<td>stats, graphics, Rcpp (&ge; 0.12.3), ICSNP, survey, GGally, png,
zoo, xts, RcppRoll, mvtnorm</td>
</tr>
<tr>
<td>Description:</td>
<td>For different linear dimension reduction methods like principal components analysis (PCA), independent components analysis (ICA) and supervised linear dimension reduction tests and estimates for the number of interesting components (ICs) are provided.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown, fICA</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-05-18 06:43:57 UTC; knordhau</td>
</tr>
<tr>
<td>Author:</td>
<td>Klaus Nordhausen <a href="https://orcid.org/0000-0002-3758-8501"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre],
  Hannu Oja <a href="https://orcid.org/0000-0002-4945-5976"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut],
  David E. Tyler [aut],
  Joni Virta <a href="https://orcid.org/0000-0002-2150-2769"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-05-18 07:30:29 UTC</td>
</tr>
</table>
<hr>
<h2 id='components'>
Generic Components Extraction Function
</h2><span id='topic+components'></span><span id='topic+components.ictest'></span><span id='topic+components.ladle'></span>

<h3>Description</h3>

<p>Function to extract components from an object. If the object is of class ictest or ladle the user can choose if all components
are extracted or only those which were interesting under the null hypothesis.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>components(x, ...)
## S3 method for class 'ictest'
components(x, which = "all", ...)
## S3 method for class 'ladle'
components(x, which = "all", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="components_+3A_x">x</code></td>
<td>
<p>an object which has a components method, like for example an ictest object.</p>
</td></tr>  
<tr><td><code id="components_+3A_which">which</code></td>
<td>
<p>for an object of class ictest. If <code>"all"</code>, then all components S in the ictest object are extracted. If <code>"k"</code>, then only the first k components are extracted,
where the value of <code>k</code> is taken from the ictest object. This is only meaningful if <code>k</code> was at least 1.</p>
</td></tr>
<tr><td><code id="components_+3A_...">...</code></td>
<td>
<p>arguments passed on to other methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a matrix with the components.
</p>


<h3>Author(s)</h3>

<p>Klaus Nordhausen
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n &lt;- 200
X &lt;- cbind(rnorm(n, sd = 2), rnorm(n, sd = 1.5), rnorm(n), rnorm(n), rnorm(n))

TestCov &lt;- PCAasymp(X, k = 2)
head(components(TestCov))
head(components(TestCov, which = "k"))
</code></pre>

<hr>
<h2 id='covSIR'>
Supervised Scatter Matrix as Used in Sliced Inverse Regression
</h2><span id='topic+covSIR'></span>

<h3>Description</h3>

<p>Sliced Inverse Regression (SIR) can be seen as  special case of Supervised ICS (SICS)
and this function gives the supervised scatter matrix for SIR
</p>


<h3>Usage</h3>

<pre><code class='language-R'>covSIR(X, y, h = 10, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="covSIR_+3A_x">X</code></td>
<td>

<p>a numeric data matrix.
</p>
</td></tr>
<tr><td><code id="covSIR_+3A_y">y</code></td>
<td>

<p>a numeric response vector.
</p>
</td></tr>
<tr><td><code id="covSIR_+3A_h">h</code></td>
<td>

<p>the number of slices.
</p>
</td></tr>
<tr><td><code id="covSIR_+3A_...">...</code></td>
<td>

<p>arguments passed on to <code><a href="stats.html#topic+quantile">quantile</a></code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This supervised scatter matrix is usually used as the second scatter matrix in SICS to obtain a SIR type supervised linear dimension reduction.
For that purpose <code>covSIR</code> first divides the response <code>y</code> into <code>h</code> slices using the corresponding quantiles as cut points.
Then for each slice the mean vector of <code>X</code> is computed and the resulting supervised scatter matrix consist of the covariance matrix of these mean vectors.
</p>
<p>The function might have problems if the sample size is too small.
</p>


<h3>Value</h3>

<p>a supervised scatter matrix
</p>


<h3>Author(s)</h3>

<p>Klaus Nordhausen
</p>


<h3>References</h3>

<p><cite>Liski, E., Nordhausen, K. and Oja, H. (2014), Supervised invariant coordinate selection, <em>Statistics: A Journal of Theoretical and Applied Statistics</em>, <b>48</b>,  711&ndash;731. &lt;doi:10.1080/02331888.2013.800067&gt;.</cite>
</p>
<p><cite>Nordhausen, K., Oja, H. and Tyler, D.E. (2022), Asymptotic and Bootstrap Tests for Subspace Dimension, Journal of Multivariate Analysis, 188, 104830. &lt;doi:10.1016/j.jmva.2021.104830&gt;.</cite>
</p>


<h3>See Also</h3>

<p><code><a href="ICS.html#topic+ics">ics</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>X &lt;- matrix(rnorm(1000), ncol = 5)
eps &lt;- rnorm(200, sd = 0.1)
y &lt;- 2 + 0.5 * X[, 1] + 2 * X[, 3] + eps

covSIR(X, y)
</code></pre>

<hr>
<h2 id='FOBIasymp'>
Testing for the Number of Gaussian Components in NGCA or ICA Using FOBI 
</h2><span id='topic+FOBIasymp'></span>

<h3>Description</h3>

<p>In non-gaussian component analysis (NGCA) and independent components analysis (ICA) gaussian components are considered as uninteresting.
The function tests, based on FOBI, if there are <code>p-k</code> gaussian components where <code>p</code> is the dimension of the data.
The function offers three different test versions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>FOBIasymp(X, k, type = "S3", model = "NGCA", method = "satterthwaite")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="FOBIasymp_+3A_x">X</code></td>
<td>
<p>numeric data matrix.</p>
</td></tr>
<tr><td><code id="FOBIasymp_+3A_k">k</code></td>
<td>
<p>the number of non-gaussian components under the null.</p>
</td></tr>
<tr><td><code id="FOBIasymp_+3A_type">type</code></td>
<td>
<p>which of the three tests to perform. Options are <code>"S1"</code>, <code>"S2"</code> and <code>"S3"</code>. For the differences see the details section.</p>
</td></tr>
<tr><td><code id="FOBIasymp_+3A_model">model</code></td>
<td>
<p>What is the underlying assumption of the non-gaussian parts. Options are general <code>"NGCA"</code> model and <code>"ICA"</code> model.</p>
</td></tr>
<tr><td><code id="FOBIasymp_+3A_method">method</code></td>
<td>
<p>if <code>type = "S1"</code> the teststatistic has as limiting distribution a weighted sum of chisquare distributions. To compute the p-value then the 
function used is <code><a href="survey.html#topic+pchisqsum">pchisqsum</a></code>. The <code>method</code> argument specifies which method  <code><a href="survey.html#topic+pchisqsum">pchisqsum</a></code> uses for the computation.
Options are <code>"satterthwaite"</code>, <code>"integration"</code> and  <code>"saddlepoint"</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function jointly diagonalizes the regular covariance and the matrix of fourth moments. Note however that in this case the matrix of fourth moments
is not made consistent under the normal model by dividing it by <code class="reqn">p+2</code>, as for example done by the function <code><a href="ICS.html#topic+cov4">cov4</a></code> where <code class="reqn">p</code> denotes the dimension
of the data. Therefore 
the eigenvalues of this generalized eigenvector-eigenvalue problem which correspond to normally distributed components should be <code>p+2</code>.
</p>
<p>Given eigenvalues <code class="reqn">d_1,...,d_p</code> the function thus orders the components in decending order according to the values of <code class="reqn">(d_i-(p+2))^2</code>.
</p>
<p>Under the null it is then assumed that the first <code>k</code> interesting components are mutually independent and non-normal and the last <code>p-k</code> are gaussian.
</p>
<p>Three possible tests are then available to test this null hypothesis for a sample of size n:
</p>

<ol>
<li> <p><code>type="S1"</code>: The test statistic T is the variance of the last p-k eigenvalues around p+2:
</p>
<p style="text-align: center;"><code class="reqn">T = n \sum_{i=k+1}^p (d_i-(p+2))^2</code>
</p>

<p>the limiting distribution of which under the null is the sum of two weighted chisquare distributions with weights: 
</p>
<p><code class="reqn">w_1 = 2 \sigma_1 / (p-k)</code> and <code class="reqn">w_2 = 2 \sigma_1 / (p-k)  +  \sigma_2</code>.
</p>
<p>and degrees of freedom:
</p>
<p><code class="reqn">df_1 = (p-k-1)(p-k+2)/2</code> and <code class="reqn">df_2 = 1</code>.
</p>
</li>
<li> <p><code>type="S2"</code>: Another possible version for the test statistic is a scaled sum of the variance of the eigenvalues around the mean plus the variance around
the expected value under normality (p+2). Denote <code class="reqn">VAR_{dpk}</code> as the variance of the last p-k eigenvalues  and  <code class="reqn">VAR2_{dpk}</code> as the variance of these eigenvalues around <code class="reqn">p+2</code>.
Then the test statistic is:
</p>
<p style="text-align: center;"><code class="reqn">T = (n (p-k) VAR_{dpk}) / (2 \sigma_1) + (n VAR2_{dpk}) / (2 \sigma_1 / (p-k) + \sigma_2)</code>
</p>

<p>This test statistic has a limiting chisquare distribution with <code class="reqn">(p-k-1)(p-q+2)/2 + 1</code> degrees of freedom.
</p>
</li>
<li> <p><code>type="S3"</code>: The third possible test statistic just checks the equality of the last p-k eigenvalues using only the first part of the test statistic of <code>type="S2"</code>.
The test statistic is then:
</p>
<p style="text-align: center;"><code class="reqn">T = (n (p-k) VAR_{dpk}) / (2 \sigma_1)</code>
</p>

<p>and has a limiting chisquare distribution with <code class="reqn">(p-k-1)(p-q+2)/2</code> degrees of freedom.
</p>
</li></ol>

<p>The constants <code class="reqn">\sigma_1</code> and <code class="reqn">\sigma_2</code> depend on the underlying model assumptions as specified in argument <code>model</code> and are estimated from the data.
</p>


<h3>Value</h3>

<p>A list of class ictest inheriting from class htest containing:
</p>
<table>
<tr><td><code>statistic</code></td>
<td>
<p>the value of the test statistic.</p>
</td></tr>
<tr><td><code>p.value</code></td>
<td>
<p>the p-value of the test.</p>
</td></tr>
<tr><td><code>parameter</code></td>
<td>
<p>the degrees of freedom of the test or the degrees of freedoms and the corresponding weights of the test in case the test
has as its limiting distribution a weighted sum of chisquare distributions.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>character string denoting which test was performed.</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>character string giving the name of the data.</p>
</td></tr>
<tr><td><code>alternative</code></td>
<td>
<p>character string specifying the alternative hypothesis.</p>
</td></tr>
<tr><td><code>k</code></td>
<td>
<p>the number or non-gaussian components used in the testing problem.</p>
</td></tr>
<tr><td><code>W</code></td>
<td>
<p>the transformation matrix to the independent components. Also known as unmixing matrix.</p>
</td></tr>
<tr><td><code>S</code></td>
<td>
<p>data matrix with the centered independent components.</p>
</td></tr>
<tr><td><code>D</code></td>
<td>
<p>the underlying FOBI eigenvalues.</p>
</td></tr>
<tr><td><code>MU</code></td>
<td>
<p>the location of the data which was substracted before calculating the independent components.</p>
</td></tr>
<tr><td><code>sigma1</code></td>
<td>
<p>the asymptotic constant sigma1 needed for the asymptotic test(s).</p>
</td></tr>
<tr><td><code>sigma2</code></td>
<td>
<p>the asymptotic constant sigma2 needed for the asymptotic test(s).</p>
</td></tr>
<tr><td><code>type</code></td>
<td>
<p>the value of <code>type</code>.</p>
</td></tr>
<tr><td><code>model</code></td>
<td>
<p>the value of <code>model</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Klaus Nordhausen
</p>


<h3>References</h3>

<p><cite>Nordhausen, K., Oja, H. and Tyler, D.E. (2022), Asymptotic and Bootstrap Tests for Subspace Dimension, Journal of Multivariate Analysis, 188, 104830. &lt;doi:10.1016/j.jmva.2021.104830&gt;.</cite>
</p>
<p><cite>Nordhausen, K., Oja, H., Tyler, D.E. and Virta, J. (2017), Asymptotic and Bootstrap Tests for the Dimension of the Non-Gaussian Subspace, Signal Processing Letters, 24, 887&ndash;891. &lt;doi:10.1109/LSP.2017.2696880 &gt;.</cite>
</p>


<h3>See Also</h3>

<p><code><a href="JADE.html#topic+FOBI">FOBI</a></code>,  <code><a href="#topic+FOBIboot">FOBIboot</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n &lt;- 1500
S &lt;- cbind(runif(n), rchisq(n, 2), rexp(n), rnorm(n), rnorm(n), rnorm(n))
A &lt;- matrix(rnorm(36), ncol = 6)
X &lt;- S %*% t(A)

FOBIasymp(X, k = 2)
FOBIasymp(X, k = 3, type = "S1")
FOBIasymp(X, k = 0, type = "S2", model = "ICA")
</code></pre>

<hr>
<h2 id='FOBIboot'>
Boostrap-based Testing for the Number of Gaussian Components in ICA Using FOBI 
</h2><span id='topic+FOBIboot'></span>

<h3>Description</h3>

<p>In independent components analysis (ICA) gaussian components are considered as uninteresting.
The function uses boostrappping tests, based on FOBI, to decide if there are <code>p-k</code> gaussian components where <code>p</code> is the dimension of the data.
The function offers two different boostrapping strategies.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>FOBIboot(X, k, n.boot = 200, s.boot = "B1")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="FOBIboot_+3A_x">X</code></td>
<td>
<p>a numeric data matrix with p&gt;1 columns.</p>
</td></tr>
<tr><td><code id="FOBIboot_+3A_k">k</code></td>
<td>
<p>the number of non-gaussian components under the null.</p>
</td></tr>
<tr><td><code id="FOBIboot_+3A_n.boot">n.boot</code></td>
<td>
<p>number of bootstrapping samples.</p>
</td></tr>
<tr><td><code id="FOBIboot_+3A_s.boot">s.boot</code></td>
<td>
<p>bootstrapping strategy to be used. Possible values are <code>"B1"</code>, <code>"B2"</code>. See details for further information.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>As in <code><a href="#topic+FOBIasymp">FOBIasymp</a></code> the function jointly diagonalizes the regular covariance and the matrix of fourth moments. Note that in this case the matrix of fourth moments
is not made consistent under the normal model by dividing it by <code class="reqn">p+2</code>, as for example done by the function <code><a href="ICS.html#topic+cov4">cov4</a></code> where <code class="reqn">p</code> denotes the dimension
of the data. Therefore the eigenvalues of this generalized eigenvector-eigenvalue problem which correspond to normally distributed components should be <code>p+2</code>.
Given eigenvalues <code class="reqn">d_1,...,d_p</code> the function thus orders the components in descending order according to the values of <code class="reqn">(d_i-(p+2))^2</code>.
</p>
<p>Under the null it is then assumed that the first <code>k</code> interesting components are mutually independent and non-normal and the last <code>p-k</code> components are gaussian.
</p>
<p>Let <code class="reqn">d_1,...,d_p</code> be the ordered eigenvalues, <code class="reqn">W</code> the correspondingly ordered unmixing matrix, <code class="reqn">s_i = W (x_i-MU)</code> the corresponding
source vectors which give the source matrix <code class="reqn">S</code> which can be decomposed into <code class="reqn">S_1</code> and <code class="reqn">S_2</code> where <code class="reqn">S_1</code> is the matrix with the <code class="reqn">k</code> non-gaussian components
and <code class="reqn">S_2</code> the matrix with the gaussian components (under the null).
</p>
<p>The test statistic is then <code class="reqn">T = n \sum_{i=k+1}^p (d_i-(p+2))^2</code>
</p>
<p>Two possible bootstrap tests are provided for testing that the last <code>p-k</code> components are gaussian and independent from the first k components:
</p>

<ol>
<li> <p><code>s.boot="B1"</code>: 
The first strategy has the followong steps:
</p>

<ol>
<li><p> Take a bootstrap sample <code class="reqn">S_1^*</code> of size <code class="reqn">n</code> from <code class="reqn">S_1</code>.
</p>
</li>
<li><p> Take a bootstrap sample <code class="reqn">S_2^*</code> consisting of a matrix of standard normally distributed elements.
</p>
</li>
<li><p> Combine <code class="reqn">S^*=(S_1^*, S_2^*)</code> and create <code class="reqn">X^*= S^* W</code>.
</p>
</li>
<li><p> Compute the test statistic based on <code class="reqn">X^*</code>. 
</p>
</li>
<li><p> Repeat the previous steps <code>n.boot</code> times.
</p>
</li></ol>

<p>Note that in this bootstrapping test the assumption of &rdquo;independent components&rdquo; is not used, it is only used that the last <code class="reqn">p-k</code> components are gaussian and independent from the first <code class="reqn">k</code> components. Therefore this strategy can be applied in an independent component analysis (ICA) framework
and in a non-gaussian components analysis (NGCA) framework.
</p>
</li>
<li> <p><code>s.boot="B2"</code>: 
The second strategy has the following steps:
</p>

<ol>
<li><p> Take a bootstrap sample <code class="reqn">S_1^*</code> of size <code class="reqn">n</code> from <code class="reqn">S_1</code> where the subsampling is done separately for each independent component.
</p>
</li>
<li><p> Take a bootstrap sample <code class="reqn">S_2^*</code> consisting of a matrix of standard normally distributed elemenets.
</p>
</li>
<li><p> Combine <code class="reqn">S^*=(S_1^*, S_2^*)</code> and create <code class="reqn">X^*= S^* W</code>.
</p>
</li>
<li><p> Compute the test statistic based on <code class="reqn">X^*</code>. 
</p>
</li>
<li><p> Repeat the previous steps <code>n.boot</code> times.
</p>
</li></ol>

<p>This bootstrapping strategy assumes a full ICA model and cannot be used in an NGCA framework.
</p>
</li></ol>



<h3>Value</h3>

<p>A list of class ictest inheriting from class htest containing:
</p>
<table>
<tr><td><code>statistic</code></td>
<td>
<p>the value of the test statistic.</p>
</td></tr>
<tr><td><code>p.value</code></td>
<td>
<p>the p-value of the test.</p>
</td></tr>
<tr><td><code>parameter</code></td>
<td>
<p>the number of boostrapping samples used to obtain the p-value.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>character string which test was performed.</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>character string giving the name of the data.</p>
</td></tr>
<tr><td><code>alternative</code></td>
<td>
<p>character string specifying the alternative hypothesis.</p>
</td></tr>
<tr><td><code>k</code></td>
<td>
<p>the number or non-gaussian components used in the testing problem.</p>
</td></tr>
<tr><td><code>W</code></td>
<td>
<p>the transformation matrix to the independent components. Also known as unmixing matrix.</p>
</td></tr>
<tr><td><code>S</code></td>
<td>
<p>data matrix with the centered independent components.</p>
</td></tr>
<tr><td><code>D</code></td>
<td>
<p>the underlying FOBI eigenvalues.</p>
</td></tr>
<tr><td><code>MU</code></td>
<td>
<p>the location of the data which was substracted before calculating the independent components.</p>
</td></tr>
<tr><td><code>s.boot</code></td>
<td>
<p>character string which boostrapping strategy was used.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Klaus Nordhausen
</p>


<h3>References</h3>

<p><cite>Nordhausen, K., Oja, H. and Tyler, D.E. (2022), Asymptotic and Bootstrap Tests for Subspace Dimension, Journal of Multivariate Analysis, 188, 104830. &lt;doi:10.1016/j.jmva.2021.104830&gt;.</cite>
</p>
<p><cite>Nordhausen, K., Oja, H., Tyler, D.E. and Virta, J. (2017), Asymptotic and Bootstrap Tests for the Dimension of the Non-Gaussian Subspace, Signal Processing Letters, 24, 887&ndash;891. &lt;doi:10.1109/LSP.2017.2696880 &gt;.</cite>
</p>


<h3>See Also</h3>

<p><code><a href="JADE.html#topic+FOBI">FOBI</a></code>,  <code><a href="#topic+FOBIasymp">FOBIasymp</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n &lt;- 1500
S &lt;- cbind(runif(n), rchisq(n, 2), rexp(n), rnorm(n), rnorm(n), rnorm(n))
A &lt;- matrix(rnorm(36), ncol = 6)
X &lt;- S %*% t(A)

FOBIboot(X, k = 2)
FOBIboot(X, k = 3, s.boot = "B1")
FOBIboot(X, k = 0, s.boot = "B2")
</code></pre>

<hr>
<h2 id='FOBIladle'>
Ladle Estimate to Estimate the Number of Gaussian Components in ICA or NGCA
</h2><span id='topic+FOBIladle'></span>

<h3>Description</h3>

<p>The ladle estimator uses the eigenvalues and eigenvectors of FOBI to estimate the number of Gaussian components in ICA or NGCA.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>FOBIladle(X, n.boot = 200, 
          ncomp = ifelse(ncol(X) &gt; 10, floor(ncol(X)/log(ncol(X))), ncol(X) - 1))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="FOBIladle_+3A_x">X</code></td>
<td>
<p>numeric data matrix.</p>
</td></tr>
<tr><td><code id="FOBIladle_+3A_n.boot">n.boot</code></td>
<td>
<p>number of bootstrapping samples to be used.</p>
</td></tr>
<tr><td><code id="FOBIladle_+3A_ncomp">ncomp</code></td>
<td>
<p>The number of components among which the ladle estimator is to be searched. The default here follows
the recommendation of Luo and Li 2016.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The model here assumes that in ICA or NGCA there are k non-gaussian components and p-k gaussian components.
The idea is then to decide which eigenvalues differ from p+2. The ladle estimate for this purpose combines the values of the 
scaled eigenvalues and the variation of the eigenvectors based on bootstrapping. The idea there is that for distinct eigenvales the variation of the eigenvectors
is small and for equal eigenvalues the corresponding eigenvectors have large variation.
</p>
<p>This measure is then computed assuming k=0,..., <code>ncomp</code> and the ladle estimate for k is the value where the measure takes its minimum. 
</p>


<h3>Value</h3>

<p>A list of class ladle containing:
</p>
<table>
<tr><td><code>method</code></td>
<td>
<p>the string FOBI.</p>
</td></tr>
<tr><td><code>k</code></td>
<td>
<p>the estimated number of non-gaussian components.</p>
</td></tr>
<tr><td><code>fn</code></td>
<td>
<p>vector giving the measures of variation of the eigenvectors using the bootstrapped eigenvectors for the different number of components.</p>
</td></tr>
<tr><td><code>phin</code></td>
<td>
<p>normalized eigenvalues of the FOBI matrix.</p>
</td></tr>
<tr><td><code>gn</code></td>
<td>
<p>the main criterion for the ladle estimate - the sum of fn and phin. k is the value where gn takes its minimum</p>
</td></tr>
<tr><td><code>lambda</code></td>
<td>
<p>the eigenvalues of the FOBI matrix.</p>
</td></tr>
<tr><td><code>W</code></td>
<td>
<p>the transformation matrix to the independent components. Also known as unmixing matrix.</p>
</td></tr>
<tr><td><code>S</code></td>
<td>
<p>data matrix with the centered independent components.</p>
</td></tr>
<tr><td><code>MU</code></td>
<td>
<p>the location of the data which was substracted before calculating the independent components.</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>the name of the data for which the ladle estimate was computed.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Klaus Nordhausen
</p>


<h3>References</h3>

<p><cite>Luo, W. and Li, B. (2016), Combining Eigenvalues and Variation of Eigenvectors for Order Determination, Biometrika, 103. 875&ndash;887. &lt;doi:10.1093/biomet/asw051&gt;</cite>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ladleplot">ladleplot</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n &lt;- 1000
X &lt;- cbind(rexp(n), rt(n,5), rnorm(n), rnorm(n), rnorm(n), rnorm(n))

test &lt;- FOBIladle(X)
test
summary(test)
plot(test)
ladleplot(test)
</code></pre>

<hr>
<h2 id='ggladleplot'>
Ladle Plot for an Object of Class ladle Using ggplot2
</h2><span id='topic+ggladleplot'></span>

<h3>Description</h3>

<p>The ladle plot is a measure to decide about the number of interesting components. Of interest for the ladle criterion is the minimum.
The function here offers however also to plot other criterion values which are part of the actual ladle criterion. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ggladleplot(x, crit = "gn", type="l", ylab = crit, 
          xlab = "component", main = deparse(substitute(x)), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ggladleplot_+3A_x">x</code></td>
<td>
<p>an object of class ladle.</p>
</td></tr>
<tr><td><code id="ggladleplot_+3A_crit">crit</code></td>
<td>
<p>the criterion to be plotted, options are <code>"gn"</code>, <code>"fn"</code>, <code>"phin"</code> and <code>"lambda"</code>.</p>
</td></tr>
<tr><td><code id="ggladleplot_+3A_type">type</code></td>
<td>
<p>plotting type.</p>
</td></tr>
<tr><td><code id="ggladleplot_+3A_ylab">ylab</code></td>
<td>
<p>default ylab value.</p>
</td></tr>
<tr><td><code id="ggladleplot_+3A_xlab">xlab</code></td>
<td>
<p>default xlab value.</p>
</td></tr>
<tr><td><code id="ggladleplot_+3A_main">main</code></td>
<td>
<p>default title.</p>
</td></tr>
<tr><td><code id="ggladleplot_+3A_...">...</code></td>
<td>
<p>other arguments for the plotting functions.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The main criterion of the ladle is the scaled sum of the eigenvalues and the measure of variation of the eigenvectors up to the component of interest.
</p>
<p>The sum is denoted <code>"gn"</code> and the individual parts are <code>"fn"</code> for the measure of the eigenvector variation and <code>"phin"</code> for the scaled eigenvalues.
The last option <code>"lambda"</code> corresponds to the unscaled eigenvalues yielding then a screeplot.
</p>


<h3>Author(s)</h3>

<p>Klaus Nordhausen, Joni Virta
</p>


<h3>References</h3>

<p><cite>Luo, W. and Li, B. (2016), Combining Eigenvalues and Variation of Eigenvectors for Order Determination, Biometrika, 103. 875&ndash;887. &lt;doi:10.1093/biomet/asw051&gt;</cite>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+FOBIladle">FOBIladle</a></code>, <code><a href="#topic+PCAladle">PCAladle</a></code>, <code><a href="#topic+SIRladle">SIRladle</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n &lt;- 1000
X &lt;- cbind(rexp(n), rt(n,5), rnorm(n), rnorm(n), rnorm(n), rnorm(n))
test &lt;- FOBIladle(X)
ggladleplot(test)
ggladleplot(test, crit="fn")
ggladleplot(test, crit="phin")
ggladleplot(test, crit="lambda")
</code></pre>

<hr>
<h2 id='ggplot.ictest'>
Scatterplot Matrix for a ictest Object using ggplot2
</h2><span id='topic+ggplot.ictest'></span>

<h3>Description</h3>

<p>For an object of class ictest, plots either the pairwise scatter plot matrix using <code>ggpairs</code> from GGally, or the time series plots of the underlying components using ggplot2. The user can choose if only the components considered interesting or all of them should be plotted. Aesthetics can be passed to ggpairs as well.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ictest'
ggplot(data, mapping = aes(), mapvar = NULL, which = "all", ..., 
       environment=parent.frame())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ggplot.ictest_+3A_data">data</code></td>
<td>
<p>object of class ictest</p>
</td></tr>
<tr><td><code id="ggplot.ictest_+3A_mapping">mapping</code></td>
<td>
<p>aesthetic mapping, see documentation for <code><a href="GGally.html#topic+ggpairs">ggpairs</a></code>. If <code>x</code> has the class <code>mts</code> then this argument is not used.</p>
</td></tr>
<tr><td><code id="ggplot.ictest_+3A_mapvar">mapvar</code></td>
<td>
<p>data.frame of the external variables used by the aesthetic mappings. If <code>x</code> has the class <code>mts</code> then this argument is not used.</p>
</td></tr>
<tr><td><code id="ggplot.ictest_+3A_which">which</code></td>
<td>
<p>if <code>"all"</code>, then all components of S in the ictest object are plotted. If <code>"k"</code>, then only the first k components are plotted,
where the value of <code>k</code> is taken from the ictest object. This is only meaningful if <code>k</code> was at least 2.</p>
</td></tr>
<tr><td><code id="ggplot.ictest_+3A_...">...</code></td>
<td>
<p>arguments passed on to <code><a href="GGally.html#topic+ggpairs">ggpairs</a></code>. If the component matrix has the class <code>mts</code>, <code>xts</code> or <code>zoo</code> then this argument is not used.</p>
</td></tr>
<tr><td><code id="ggplot.ictest_+3A_environment">environment</code></td>
<td>
<p>not used but needed for consistency.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If the component matrix has the class <code>mts</code>, <code>xts</code> or <code>zoo</code> then a time series plot will be plotted using ggplot2. Otherwise, a pairwise scatter plot matrix will be plotted using GGally.
</p>


<h3>Author(s)</h3>

<p>Klaus Nordhausen, Joni Virta
</p>


<h3>See Also</h3>

<p><code><a href="#topic+plot.ictest">plot.ictest</a>, <a href="graphics.html#topic+pairs">pairs</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># The data
X &lt;- iris[, 1:4]

# The aesthetics variables
mapvar &lt;- data.frame(iris[, 5])
colnames(mapvar) &lt;- "species"

TestCov &lt;- PCAasymp(X, k = 2)
ggplot(TestCov)
ggplot(TestCov, aes(color = species), mapvar = mapvar, which = "k")
</code></pre>

<hr>
<h2 id='ggplot.ladle'>
Scatterplot Matrix for a ladle Object using ggplot2
</h2><span id='topic+ggplot.ladle'></span>

<h3>Description</h3>

<p>For an object of class ladle, plots either the pairwise scatter plot matrix using <code>ggpairs</code> from GGally, or the time series plots of the underlying components using ggplot2. The user can choose if only the components considered interesting or all of them should be plotted. Aesthetics can be passed to ggpairs as well.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ladle'
ggplot(data, mapping = aes(), mapvar = NULL, which = "all", ..., 
       environment=parent.frame())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ggplot.ladle_+3A_data">data</code></td>
<td>
<p>object of class ladle</p>
</td></tr>
<tr><td><code id="ggplot.ladle_+3A_mapping">mapping</code></td>
<td>
<p>aesthetic mapping, see documentation for <code><a href="GGally.html#topic+ggpairs">ggpairs</a></code>. If <code>x</code> has the class <code>mts</code> then this argument is not used.</p>
</td></tr>
<tr><td><code id="ggplot.ladle_+3A_mapvar">mapvar</code></td>
<td>
<p>data.frame of the external variables used by the aesthetic mappings. If <code>x</code> has the class <code>mts</code> then this argument is not used.</p>
</td></tr>
<tr><td><code id="ggplot.ladle_+3A_which">which</code></td>
<td>
<p>if <code>"all"</code>, then all components of S in the ladle object are plotted. If <code>"k"</code>, then only the first k components are plotted,
where the value of <code>k</code> is taken from the ladle object. This is only meaningful if <code>k</code> was at least 2.</p>
</td></tr>
<tr><td><code id="ggplot.ladle_+3A_...">...</code></td>
<td>
<p>arguments passed on to <code><a href="GGally.html#topic+ggpairs">ggpairs</a></code>. If the component matrix has the class <code>mts</code>, <code>xts</code> or <code>zoo</code> then this argument is not used.</p>
</td></tr>
<tr><td><code id="ggplot.ladle_+3A_environment">environment</code></td>
<td>
<p>not used but needed for consistency.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If the component matrix has the class <code>mts</code>, <code>xts</code> or <code>zoo</code> then a time series plot will be plotted using ggplot2. Otherwise, a pairwise scatter plot matrix will be plotted using GGally.
</p>


<h3>Author(s)</h3>

<p>Klaus Nordhausen, Joni Virta
</p>


<h3>See Also</h3>

<p><code><a href="#topic+plot.ladle">plot.ladle</a>, <a href="graphics.html#topic+pairs">pairs</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># The data
X &lt;- as.matrix(iris[, 1:4])

# The aesthetics variables
mapvar &lt;- data.frame(iris[, 5])
colnames(mapvar) &lt;- "species"

ladle_res &lt;- PCAladle(X)

# The estimate
summary(ladle_res)

# Plots of the components
ggplot(ladle_res)
ggplot(ladle_res, aes(color = species), mapvar = mapvar, which = "k")
</code></pre>

<hr>
<h2 id='ggscreeplot'>
ggplot2-style screeplot
</h2><span id='topic+ggscreeplot'></span>

<h3>Description</h3>

<p>A generic method for ggplot2-style screeplots.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ggscreeplot(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ggscreeplot_+3A_x">x</code></td>
<td>
<p>An object of an appropriate class.</p>
</td></tr>
<tr><td><code id="ggscreeplot_+3A_...">...</code></td>
<td>
<p>Additional arguments.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Joni Virta, Klaus Nordhausen
</p>

<hr>
<h2 id='ggscreeplot.ictest'>
Screeplot for an ictest Object Using ggplot2
</h2><span id='topic+ggscreeplot.ictest'></span>

<h3>Description</h3>

<p>Plots the criterion values of an <code>ictest</code> object against its index number using ggplot2. Two versions of this screeplot are available.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ictest'
ggscreeplot(x, type = "barplot", main = deparse(substitute(x)),
            ylab = "criterion", xlab = "component", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ggscreeplot.ictest_+3A_x">x</code></td>
<td>
<p> object of class <code>ictest</code>.</p>
</td></tr>
<tr><td><code id="ggscreeplot.ictest_+3A_type">type</code></td>
<td>
 <p><code>barplot</code> if a barplot or <code>lines</code> if a line plot is preferred.</p>
</td></tr>
<tr><td><code id="ggscreeplot.ictest_+3A_main">main</code></td>
<td>
<p> main title of the plot. </p>
</td></tr>
<tr><td><code id="ggscreeplot.ictest_+3A_ylab">ylab</code></td>
<td>
<p> y-axis label. </p>
</td></tr>
<tr><td><code id="ggscreeplot.ictest_+3A_xlab">xlab</code></td>
<td>
<p> x-axis label. </p>
</td></tr>
<tr><td><code id="ggscreeplot.ictest_+3A_...">...</code></td>
<td>
<p>arguments passed to and from methods.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Klaus Nordhausen, Joni Virta
</p>


<h3>See Also</h3>

<p><code><a href="#topic+screeplot.ictest">screeplot.ictest</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n &lt;- 200
X &lt;- cbind(rnorm(n, sd = 2), rnorm(n, sd = 1.5), rnorm(n), rnorm(n), rnorm(n))

TestCov &lt;- PCAasymp(X, k = 2)
ggscreeplot(TestCov)
</code></pre>

<hr>
<h2 id='ICSboot'>
Boostrap-based Testing for the Number of Gaussian Components in NGCA Using Two Scatter Matrices
</h2><span id='topic+ICSboot'></span>

<h3>Description</h3>

<p>In independent components analysis (ICA) gaussian components are considered as uninteresting.
The function uses boostrappping tests, based on ICS using any combination of two scatter matrices, to decide if there are <code>p-k</code> gaussian components where <code>p</code> is the dimension of the data.
The function offers two different boostrapping strategies.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ICSboot(X, k, S1=cov, S2=cov4, S1args=NULL, S2args=NULL, n.boot = 200, s.boot = "B1")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ICSboot_+3A_x">X</code></td>
<td>
<p>a numeric data matrix with p&gt;1 columns.</p>
</td></tr>
<tr><td><code id="ICSboot_+3A_k">k</code></td>
<td>
<p>the number of non-gaussian components under the null.</p>
</td></tr>
<tr><td><code id="ICSboot_+3A_s1">S1</code></td>
<td>
<p>name of the first scatter matrix function. Can only return a matrix. Default is <code><a href="stats.html#topic+cov">cov</a></code></p>
</td></tr></table>
<p>.
</p>
<table>
<tr><td><code id="ICSboot_+3A_s2">S2</code></td>
<td>
<p>name of the second scatter matrix function. Can only return a matrix. Default is <code><a href="ICS.html#topic+cov4">cov4</a></code></p>
</td></tr>
<tr><td><code id="ICSboot_+3A_s1args">S1args</code></td>
<td>
<p>list with optional additional arguments for <code>S1</code>.</p>
</td></tr>
<tr><td><code id="ICSboot_+3A_s2args">S2args</code></td>
<td>
<p>list with optional additional arguments for <code>S2</code>.</p>
</td></tr>
<tr><td><code id="ICSboot_+3A_n.boot">n.boot</code></td>
<td>
<p>number of bootstrapping samples.</p>
</td></tr>
<tr><td><code id="ICSboot_+3A_s.boot">s.boot</code></td>
<td>
<p>bootstrapping strategy to be used. Possible values are <code>"B1"</code>, <code>"B2"</code>. See details for further information.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>While in <code><a href="#topic+FOBIasymp">FOBIasymp</a></code> and <code><a href="#topic+FOBIboot">FOBIboot</a></code> the two scatters used are always <code>cov</code> and  <code><a href="ICS.html#topic+cov4">cov4</a></code> this function can be used with any two scatter functions. In that case however the value of the Gaussian eigenvalues are in general not known and depend on the scatter functions used. Therefore the test uses as test statistic the <code>k</code> successive eigenvalues with the smallest variance. Which means the default here might differ from <code><a href="#topic+FOBIasymp">FOBIasymp</a></code> and <code><a href="#topic+FOBIboot">FOBIboot</a></code>.
</p>
<p>Given eigenvalues <code class="reqn">d_1,...,d_p</code> the function thus orders the components in descending order according to the &quot;variance&quot; criterion .
</p>
<p>Under the null it is then assumed that the first <code>k</code> interesting components are mutually independent and non-normal and the last <code>p-k</code> components are gaussian.
</p>
<p>Let <code class="reqn">d_1,...,d_p</code> be the ordered eigenvalues, <code class="reqn">W</code> the correspondingly ordered unmixing matrix, <code class="reqn">s_i = W (x_i-MU)</code> the corresponding
source vectors which give the source matrix <code class="reqn">S</code> which can be decomposed into <code class="reqn">S_1</code> and <code class="reqn">S_2</code> where <code class="reqn">S_1</code> is the matrix with the <code class="reqn">k</code> non-gaussian components
and <code class="reqn">S_2</code> the matrix with the gaussian components (under the null).
</p>
<p>Two possible bootstrap tests are provided for testing that the last <code>p-k</code> components are gaussian and independent from the first k components:
</p>

<ol>
<li> <p><code>s.boot="B1"</code>: 
The first strategy has the followong steps:
</p>

<ol>
<li><p> Take a bootstrap sample <code class="reqn">S_1^*</code> of size <code class="reqn">n</code> from <code class="reqn">S_1</code>.
</p>
</li>
<li><p> Take a bootstrap sample <code class="reqn">S_2^*</code> consisting of a matrix with gaussian random variables having <code class="reqn">cov(S_2)</code>.
</p>
</li>
<li><p> Combine <code class="reqn">S^*=(S_1^*, S_2^*)</code> and create <code class="reqn">X^*= S^* W</code>.
</p>
</li>
<li><p> Compute the test statistic based on <code class="reqn">X^*</code>. 
</p>
</li>
<li><p> Repeat the previous steps <code>n.boot</code> times.
</p>
</li></ol>

<p>Note that in this bootstrapping test the assumption of &rdquo;independent components&rdquo; is not used, it is only used that the last <code class="reqn">p-k</code> components are gaussian and independent from the first <code class="reqn">k</code> components. Therefore this strategy can be applied in an independent component analysis (ICA) framework
and in a non-gaussian components analysis (NGCA) framework.
</p>
</li>
<li> <p><code>s.boot="B2"</code>: 
The second strategy has the following steps:
</p>

<ol>
<li><p> Take a bootstrap sample <code class="reqn">S_1^*</code> of size <code class="reqn">n</code> from <code class="reqn">S_1</code> where the subsampling is done separately for each independent component.
</p>
</li>
<li><p> Take a bootstrap sample <code class="reqn">S_2^*</code>  consisting of a matrix with gaussian random variables having <code class="reqn">cov(S_2)</code>
</p>
</li>
<li><p> Combine <code class="reqn">S^*=(S_1^*, S_2^*)</code> and create <code class="reqn">X^*= S^* W</code>.
</p>
</li>
<li><p> Compute the test statistic based on <code class="reqn">X^*</code>. 
</p>
</li>
<li><p> Repeat the previous steps <code>n.boot</code> times.
</p>
</li></ol>

<p>This bootstrapping strategy assumes a full ICA model and cannot be used in an NGCA framework. Note that when the goal is to recover the non-gaussian independent components both scatters used must have the independence property.
</p>
</li></ol>



<h3>Value</h3>

<p>A list of class ictest inheriting from class htest containing:
</p>
<table>
<tr><td><code>statistic</code></td>
<td>
<p>the value of the test statistic.</p>
</td></tr>
<tr><td><code>p.value</code></td>
<td>
<p>the p-value of the test.</p>
</td></tr>
<tr><td><code>parameter</code></td>
<td>
<p>the number of boostrapping samples used to obtain the p-value.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>character string which test was performed and which scatters were used.</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>character string giving the name of the data.</p>
</td></tr>
<tr><td><code>alternative</code></td>
<td>
<p>character string specifying the alternative hypothesis.</p>
</td></tr>
<tr><td><code>k</code></td>
<td>
<p>the number or non-gaussian components used in the testing problem.</p>
</td></tr>
<tr><td><code>W</code></td>
<td>
<p>the transformation matrix to the independent components. Also known as unmixing matrix.</p>
</td></tr>
<tr><td><code>S</code></td>
<td>
<p>data matrix with the centered independent components.</p>
</td></tr>
<tr><td><code>D</code></td>
<td>
<p>the underlying eigenvalues.</p>
</td></tr>
<tr><td><code>MU</code></td>
<td>
<p>the location of the data which was substracted before calculating the independent components.</p>
</td></tr>
<tr><td><code>s.boot</code></td>
<td>
<p>character string which boostrapping strategy was used.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Klaus Nordhausen
</p>


<h3>References</h3>

<p><cite>Nordhausen, K., Oja, H. and Tyler, D.E. (2022), Asymptotic and Bootstrap Tests for Subspace Dimension, Journal of Multivariate Analysis, 188, 104830. &lt;doi:10.1016/j.jmva.2021.104830&gt;.</cite>
</p>
<p><cite>Nordhausen, K., Oja, H., Tyler, D.E. and Virta, J. (2017), Asymptotic and Bootstrap Tests for the Dimension of the Non-Gaussian Subspace, Signal Processing Letters, 24, 887&ndash;891. &lt;doi:10.1109/LSP.2017.2696880&gt;.</cite>
</p>
<p><cite>Radojicic, U. and Nordhausen, K. (2020),  Non-Gaussian Component Analysis: Testing the Dimension of the Signal Subspace. In Maciak, M., Pestas, M. and Schindler, M. (editors) &quot;Analytical Methods in Statistics. AMISTAT 2019&quot;, 101&ndash;123, Springer, Cham. &lt;doi:10.1007/978-3-030-48814-7_6&gt;.</cite>
</p>


<h3>See Also</h3>

<p><code><a href="ICS.html#topic+ics">ics</a></code>, <code><a href="#topic+FOBIboot">FOBIboot</a></code>,  <code><a href="#topic+FOBIasymp">FOBIasymp</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n &lt;- 750
S &lt;- cbind(runif(n), rchisq(n, 2), rexp(n), rnorm(n), rnorm(n), rnorm(n))
A &lt;- matrix(rnorm(36), ncol = 6)
X &lt;- S %*% t(A)

# n.boot is small for demonstration purpose, should be larger
ICSboot(X, k=1, n.boot=20)

if(require("ICSNP")){

myTyl &lt;- function(X,...) HR.Mest(X,...)$scatter
myT &lt;- function(X,...) tM(X,...)$V

# n.boot is small for demonstration purpose, should be larger
ICSboot(X, k=3, S1=myT, S2=myTyl, s.boot = "B2", n.boot=20)
}
</code></pre>

<hr>
<h2 id='ladle'>
Ladle estimate for an arbitrary matrix
</h2><span id='topic+ladle'></span>

<h3>Description</h3>

<p>The ladle estimates the rank of a symmetric matrix <code class="reqn">S</code> by combining the classical screeplot with an estimate of the rank from the bootstrap eigenvector variability of <code class="reqn">S</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ladle(x, S, n.boots = 200, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ladle_+3A_x">x</code></td>
<td>
<p><code>n</code> x <code>p</code> data matrix.</p>
</td></tr>
<tr><td><code id="ladle_+3A_s">S</code></td>
<td>
<p>Function for computing a <code>q</code> x <code>q</code> symmetric matrix from the data <code>x</code>.</p>
</td></tr>
<tr><td><code id="ladle_+3A_n.boots">n.boots</code></td>
<td>
<p>The number of bootstrap samples.</p>
</td></tr>
<tr><td><code id="ladle_+3A_...">...</code></td>
<td>
<p>Furhter parameters passed to <code>S</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Assume that the eigenvalues of the population version of <code>S</code> are <code class="reqn">\lambda_1 &gt;= ... &gt;= \lambda_k &gt; \lambda_k+1 = ... = \lambda_p</code>. The ladle estimates the true value of <code class="reqn">k</code> (for example the rank of <code>S</code>) by combining the classical screeplot with estimate of <code class="reqn">k</code> from the bootstrap eigenvector variability of <code>S</code>.
</p>
<p>For applying the ladle to either PCA, FOBI or SIR, see the dedicated functions <code><a href="#topic+PCAladle">PCAladle</a></code>, <code><a href="#topic+FOBIladle">FOBIladle</a></code>, <code><a href="#topic+SIRladle">SIRladle</a></code>.
</p>


<h3>Value</h3>

<p>A list of class <code>ladle</code> containing:
</p>
<table>
<tr><td><code>method</code></td>
<td>
<p>The string &ldquo;general&rdquo;.</p>
</td></tr>
<tr><td><code>k</code></td>
<td>
<p>The estimated value of k.</p>
</td></tr>
<tr><td><code>fn</code></td>
<td>
<p>A vector giving the measures of variation of the eigenvectors using the bootstrapped eigenvectors for the different number of components.</p>
</td></tr>
<tr><td><code>phin</code></td>
<td>
<p>The normalized eigenvalues of the S matrix.</p>
</td></tr>
<tr><td><code>gn</code></td>
<td>
<p>The main criterion for the ladle estimate - the sum of fn and phin. k is the value where gn takes its minimum.</p>
</td></tr>
<tr><td><code>lambda</code></td>
<td>
<p>The eigenvalues of the covariance matrix.</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>The name of the data for which the ladle estimate was computed.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Joni Virta
</p>


<h3>References</h3>

<p>Luo, W. and Li, B. (2016), Combining Eigenvalues and Variation of Eigenvectors for Order Determination, Biometrika, 103. 875-887. &lt;doi:10.1093/biomet/asw051&gt;
</p>


<h3>See Also</h3>

<p><code><a href="#topic+PCAladle">PCAladle</a></code>, <code><a href="#topic+FOBIladle">FOBIladle</a></code>, <code><a href="#topic+SIRladle">SIRladle</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Function for computing the left CCA matrix
S_CCA &lt;- function(x, dim){
  x1 &lt;- x[, 1:dim]
  x2 &lt;- x[, -(1:dim)]
  stand &lt;- function(x){
    x &lt;- as.matrix(x)
    x &lt;- sweep(x, 2, colMeans(x), "-")
    eigcov &lt;- eigen(cov(x), symmetric = TRUE)
    x%*%(eigcov$vectors%*%diag((eigcov$values)^(-1/2))%*%t(eigcov$vectors))
  }
  
  x1stand &lt;- stand(x1)
  x2stand &lt;- stand(x2)
  
  crosscov &lt;- cov(x1stand, x2stand)
  
  tcrossprod(crosscov)
}

# Toy data with two canonical components
n &lt;- 200
x1 &lt;- matrix(rnorm(n*5), n, 5)
x2 &lt;- cbind(x1[, 1] + rnorm(n, sd = sqrt(0.5)),
            -1*x1[, 1] + x1[, 2] + rnorm(n, sd = sqrt(0.5)),
            matrix(rnorm(n*3), n, 3))
x &lt;- cbind(x1, x2)

# The ladle estimate
ladle_1 &lt;- ladle(x, S_CCA, dim = 5)
ladleplot(ladle_1)
</code></pre>

<hr>
<h2 id='ladleplot'>
Ladle Plot for an Object of Class ladle 
</h2><span id='topic+ladleplot'></span>

<h3>Description</h3>

<p>The ladle plot is a measure to decide about the number of interesting components. Of interest for the ladle criterion is the minimum.
The function here offers however also to plot other criterion values which are part of the actual ladle criterion. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ladleplot(x, crit = "gn", type="l", ylab = crit, 
          xlab = "component", main = deparse(substitute(x)), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ladleplot_+3A_x">x</code></td>
<td>
<p>an object of class ladle.</p>
</td></tr>
<tr><td><code id="ladleplot_+3A_crit">crit</code></td>
<td>
<p>the criterion to be plotted, options are <code>"gn"</code>, <code>"fn"</code>, <code>"phin"</code> and <code>"lambda"</code>.</p>
</td></tr>
<tr><td><code id="ladleplot_+3A_type">type</code></td>
<td>
<p>plotting type.</p>
</td></tr>
<tr><td><code id="ladleplot_+3A_ylab">ylab</code></td>
<td>
<p>default ylab value.</p>
</td></tr>
<tr><td><code id="ladleplot_+3A_xlab">xlab</code></td>
<td>
<p>default xlab value.</p>
</td></tr>
<tr><td><code id="ladleplot_+3A_main">main</code></td>
<td>
<p>default title.</p>
</td></tr>
<tr><td><code id="ladleplot_+3A_...">...</code></td>
<td>
<p>other arguments for the plotting functions.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The main criterion of the ladle is the scaled sum of the eigenvalues and the measure of variation of the eigenvectors up to the component of interest.
</p>
<p>The sum is denoted <code>"gn"</code> and the individual parts are <code>"fn"</code> for the measure of the eigenvector variation and <code>"phin"</code> for the scaled eigenvalues.
The last option <code>"lambda"</code> corresponds to the unscaled eigenvalues yielding then a screeplot.
</p>


<h3>Author(s)</h3>

<p>Klaus Nordhausen
</p>


<h3>References</h3>

<p><cite>Luo, W. and Li, B. (2016), Combining Eigenvalues and Variation of Eigenvectors for Order Determination, Biometrika, 103. 875&ndash;887. &lt;doi:10.1093/biomet/asw051&gt;</cite>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+FOBIladle">FOBIladle</a></code>, <code><a href="#topic+PCAladle">PCAladle</a></code>, <code><a href="#topic+SIRladle">SIRladle</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n &lt;- 1000
X &lt;- cbind(rexp(n), rt(n,5), rnorm(n), rnorm(n), rnorm(n), rnorm(n))
test &lt;- FOBIladle(X)
ladleplot(test)
ladleplot(test, crit="fn")
ladleplot(test, crit="phin")
ladleplot(test, crit="lambda")
</code></pre>

<hr>
<h2 id='NGPP'>Non-Gaussian Projection Pursuit 
</h2><span id='topic+NGPP'></span>

<h3>Description</h3>

<p>Estimates <code class="reqn">k</code> non-Gaussian signal components using projection pursuit. The projection index can be chosen among convex combinations of squares of one or several standard projection indices used in ICA.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>NGPP(X, k, nl = c("skew", "pow3"), alpha = 0.8, method = "symm", eps = 1e-6,
     verbose = FALSE, maxiter = 100)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="NGPP_+3A_x">X</code></td>
<td>
<p>Numeric matrix with n rows corresponding to the observations and p columns corresponding to the variables.</p>
</td></tr>
<tr><td><code id="NGPP_+3A_k">k</code></td>
<td>
<p>Number of components to estimate, <code class="reqn">1 \leq k \leq p</code>.</p>
</td></tr>
<tr><td><code id="NGPP_+3A_nl">nl</code></td>
<td>
<p>Vector of non-linearities, a convex combination of the corresponding squared objective functions of which is then used as the projection index. The choices include <code>"skew"</code> (skewness), <code>"pow3"</code> (excess kurtosis), <code>"tanh"</code> (<code class="reqn">log(cosh)</code>) and <code>"gauss"</code> (Gaussian function).</p>
</td></tr>
<tr><td><code id="NGPP_+3A_alpha">alpha</code></td>
<td>
<p>Vector of positive weights between 0 and 1 given to the non-linearities. The length of <code>alpha</code> should be either one less than the number of non-linearities in which case the missing weight is chosen so that <code>alpha</code> sums to one, or equal to the number of non-linearities in which case the weights are used as such. No boundary checks for the weights are done.</p>
</td></tr>
<tr><td><code id="NGPP_+3A_method">method</code></td>
<td>
<p>If <code>"symm"</code> the <code>k</code> signals are estimated simultaneously (symmetric projection pursuit) and if <code>"defl"</code> they are estimated one-by-one (deflation-based projection pursuit).</p>
</td></tr>
<tr><td><code id="NGPP_+3A_eps">eps</code></td>
<td>
<p>Convergence tolerance.</p>
</td></tr>
<tr><td><code id="NGPP_+3A_verbose">verbose</code></td>
<td>
<p>If <code>TRUE</code> the numbers of iterations will be printed.</p>
</td></tr>
<tr><td><code id="NGPP_+3A_maxiter">maxiter</code></td>
<td>
<p>Maximum number of iterations.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>It is assumed that the data is a random sample from the model <code class="reqn">x = m + A s</code> where the latent vector <code class="reqn">s = (s_1^T, s_2^T)^T</code> consists of <code class="reqn">k</code>-dimensional non-Gaussian subvector (the signal) and <code class="reqn">p - k</code>-dimensional Gaussian subvector (the noise) and the components of <code class="reqn">s</code> are mutually independent. Without loss of generality we further assume that the components of <code class="reqn">s</code> have zero means and unit variances.
</p>
<p>The objective is to estimate an inverse for the mixing matrix <code class="reqn">A</code> and in non-Gaussian projection pursuit this is done by first standardizaing the observations and then finding mutually orthogonal directions maximizing a convex combination of the chosen squared objective functions.
</p>
<p>After estimation the found signals are ordered in decreasing order with respect to their objective function values.
</p>


<h3>Value</h3>

<p>A list with class 'bss' containing the following components: 
</p>
<table>
<tr><td><code>W</code></td>
<td>
<p>Estimated unmixing matrix</p>
</td></tr>
<tr><td><code>S</code></td>
<td>
<p>Matrix of size <code class="reqn">n \times k</code> containing the estimated signals.</p>
</td></tr>
<tr><td><code>D</code></td>
<td>
<p>Vector of the objective function values of the signals</p>
</td></tr>
<tr><td><code>MU</code></td>
<td>
<p>Location vector of the data which was substracted before estimating the signal components.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Joni Virta
</p>


<h3>References</h3>

<p><cite>Virta, J., Nordhausen, K. and Oja, H., (2016), Projection Pursuit for non-Gaussian Independent Components, &lt;https://arxiv.org/abs/1612.05445&gt;.</cite> 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+NGPPsim">NGPPsim</a>, <a href="#topic+NGPPest">NGPPest</a>, <a href="fICA.html#topic+fICA">fICA</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Simulated data with 2 signals

n &lt;- 500
S &lt;- cbind(rexp(n), runif(n),  rnorm(n))
A &lt;- matrix(rnorm(9), ncol = 3)
X &lt;- S %*% t(A)

res &lt;- NGPP(X, 2)
res$W %*% A


# Iris data

X &lt;- as.matrix(iris[, 1:4])

res &lt;- NGPP(X, 2, nl = c("pow3", "tanh"), alpha = 0.5)
plot(res, col = iris[, 5])
</code></pre>

<hr>
<h2 id='NGPPest'>Signal Subspace Dimension Testing Using non-Gaussian Projection Pursuit 
</h2><span id='topic+NGPPest'></span>

<h3>Description</h3>

<p>Estimates the dimension of the signal subspace using NGPP to conduct sequential hypothesis testing. The test statistic is a multivariate extension of the classical Jarque-Bera statistic and the distribution of it under the null hypothesis is obtained by simulation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>NGPPest(X, nl = c("skew", "pow3"), alpha = 0.8, N = 500, eps = 1e-6,
        verbose = FALSE, maxiter = 100)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="NGPPest_+3A_x">X</code></td>
<td>
<p>Numeric matrix with n rows corresponding to the observations and p columns corresponding to the variables.</p>
</td></tr>
<tr><td><code id="NGPPest_+3A_nl">nl</code></td>
<td>
<p>Vector of non-linearities, a convex combination of the corresponding squared objective functions of which is then used as the projection index. The choices include <code>"skew"</code> (skewness), <code>"pow3"</code> (excess kurtosis), <code>"tanh"</code> (<code class="reqn">log(cosh)</code>) and <code>"gauss"</code> (Gaussian function).</p>
</td></tr>
<tr><td><code id="NGPPest_+3A_alpha">alpha</code></td>
<td>
<p>Vector of positive weights between 0 and 1 given to the non-linearities. The length of <code>alpha</code> should be either one less than the number of non-linearities in which case the missing weight is chosen so that <code>alpha</code> sums to one, or equal to the number of non-linearities in which case the weights are used as such. No boundary checks for the weights are done.</p>
</td></tr>
<tr><td><code id="NGPPest_+3A_n">N</code></td>
<td>
<p>Number of normal samples to be used in simulating the distribution of the test statistic under the null hypothesis.</p>
</td></tr>
<tr><td><code id="NGPPest_+3A_eps">eps</code></td>
<td>
<p>Convergence tolerance.</p>
</td></tr>
<tr><td><code id="NGPPest_+3A_verbose">verbose</code></td>
<td>
<p>If <code>TRUE</code> the numbers of iterations will be printed.</p>
</td></tr>
<tr><td><code id="NGPPest_+3A_maxiter">maxiter</code></td>
<td>
<p>Maximum number of iterations.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>It is assumed that the data is a random sample from the model <code class="reqn">x = m + A s</code> where the latent vector <code class="reqn">s = (s_1^T, s_2^T)^T</code> consists of <code class="reqn">k</code>-dimensional non-Gaussian subvector (the signal) and <code class="reqn">p - k</code>-dimensional Gaussian subvector (the noise) and the components of <code class="reqn">s</code> are mutually independent. Without loss of generality we further assume that the components of <code class="reqn">s</code> have zero means and unit variances.
</p>
<p>The algorithm first estimates full <code class="reqn">p</code> components from the data using deflation-based NGPP with the chosen non-linearities and weighting and then tests the null hypothesis <code class="reqn">H_0: k_{true} \leq k</code> for each <code class="reqn">k = 0, \ldots , p - 1</code>. The testing is based on the fact that under the null hypothesis <code class="reqn">H_0: k_{true} \leq k</code> the distribution of the final <code class="reqn">p - k</code> components is standard multivariate normal and the significance of the test can be obtained by comparing the objective function value of the <code class="reqn">(k + 1)</code>th estimated components to the same quantity estimated from <code>N</code> samples of size <code class="reqn">n</code> from <code class="reqn">(p - k)</code>-dimensional standard multivariate normal distribution.
</p>
<p>Note that if <code>maxiter</code> is reached at any step of the algorithm it will use the current estimated direction and continue to the next step. 
</p>


<h3>Value</h3>

<p>A list with class 'icest' containing the following components:
</p>
<table>
<tr><td><code>statistic</code></td>
<td>
<p>Test statistic, i.e. the objective function values of all estimated component.</p>
</td></tr>
<tr><td><code>p.value</code></td>
<td>
<p>Obtained vector of <code class="reqn">p</code>-values.</p>
</td></tr>
<tr><td><code>parameter</code></td>
<td>
<p>Number <code>N</code> of simulated normal samples.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>Character string <code>"Estimation the signal subspace dimension using NGPP"</code>.</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>Character string giving the name of the data.</p>
</td></tr>
<tr><td><code>W</code></td>
<td>
<p>Estimated unmixing matrix</p>
</td></tr>
<tr><td><code>S</code></td>
<td>
<p>Matrix of size <code class="reqn">n \times p</code> containing the estimated signals.</p>
</td></tr>
<tr><td><code>D</code></td>
<td>
<p>Vector of the objective function values of the signals</p>
</td></tr>
<tr><td><code>MU</code></td>
<td>
<p>Location vector of the data which was substracted before estimating the signal components.</p>
</td></tr>
<tr><td><code>conv</code></td>
<td>
<p>Boolean vector telling for which components the algorithm converged (<code>TRUE</code>) and for which not (<code>FALSE</code>).</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Joni Virta
</p>


<h3>References</h3>

<p><cite>Virta, J., Nordhausen, K. and Oja, H., (2016), Projection Pursuit for non-Gaussian Independent Components, &lt;https://arxiv.org/abs/1612.05445&gt;.</cite> 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+NGPP">NGPP</a>, <a href="#topic+NGPPsim">NGPPsim</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Iris data

X &lt;- as.matrix(iris[, 1:4])

# The number of simulations N should be increased in practical situations
# Now we settle for N = 100

res &lt;- NGPPest(X, N = 100)
res$statistic
res$p.value
res$conv
</code></pre>

<hr>
<h2 id='NGPPsim'>Signal Subspace Dimension Testing Using non-Gaussian Projection Pursuit 
</h2><span id='topic+NGPPsim'></span>

<h3>Description</h3>

<p>Tests whether the true dimension of the signal subspace is less than or equal to a given <code class="reqn">k</code>. The test statistic is a multivariate extension of the classical Jarque-Bera statistic and the distribution of it under the null hypothesis is obtained by simulation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>NGPPsim(X, k, nl = c("skew", "pow3"), alpha = 0.8, N = 1000, eps = 1e-6,
        verbose = FALSE, maxiter = 100)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="NGPPsim_+3A_x">X</code></td>
<td>
<p>Numeric matrix with n rows corresponding to the observations and p columns corresponding to the variables.</p>
</td></tr>
<tr><td><code id="NGPPsim_+3A_k">k</code></td>
<td>
<p>Number of components to estimate, <code class="reqn">1 \leq k \leq p</code>.</p>
</td></tr>
<tr><td><code id="NGPPsim_+3A_nl">nl</code></td>
<td>
<p>Vector of non-linearities, a convex combination of the corresponding squared objective functions of which is then used as the projection index. The choices include <code>"skew"</code> (skewness), <code>"pow3"</code> (excess kurtosis), <code>"tanh"</code> (<code class="reqn">log(cosh)</code>) and <code>"gauss"</code> (Gaussian function).</p>
</td></tr>
<tr><td><code id="NGPPsim_+3A_alpha">alpha</code></td>
<td>
<p>Vector of positive weights between 0 and 1 given to the non-linearities. The length of <code>alpha</code> should be either one less than the number of non-linearities in which case the missing weight is chosen so that <code>alpha</code> sums to one, or equal to the number of non-linearities in which case the weights are used as such. No boundary checks for the weights are done.</p>
</td></tr>
<tr><td><code id="NGPPsim_+3A_n">N</code></td>
<td>
<p>Number of normal samples to be used in simulating the distribution of the test statistic under the null hypothesis.</p>
</td></tr>
<tr><td><code id="NGPPsim_+3A_eps">eps</code></td>
<td>
<p>Convergence tolerance.</p>
</td></tr>
<tr><td><code id="NGPPsim_+3A_verbose">verbose</code></td>
<td>
<p>If <code>TRUE</code> the numbers of iterations will be printed.</p>
</td></tr>
<tr><td><code id="NGPPsim_+3A_maxiter">maxiter</code></td>
<td>
<p>Maximum number of iterations.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>It is assumed that the data is a random sample from the model <code class="reqn">x = m + A s</code> where the latent vector <code class="reqn">s = (s_1^T, s_2^T)^T</code> consists of <code class="reqn">k</code>-dimensional non-Gaussian subvector (the signal) and <code class="reqn">p - k</code>-dimensional Gaussian subvector (the noise) and the components of <code class="reqn">s</code> are mutually independent. Without loss of generality we further assume that the components of <code class="reqn">s</code> have zero means and unit variances.
</p>
<p>To test the null hypothesis <code class="reqn">H_0: k_{true} \leq k</code> the algorithm first estimates <code class="reqn">k + 1</code> components using delfation-based NGPP with the chosen non-linearities and weighting. Under the null hypothesis the distribution of the final <code class="reqn">p - k</code> components is standard multivariate normal and the significance of the test is obtained by comparing the objective function value of the <code class="reqn">(k + 1)</code>th estimated components to the same quantity estimated from <code>N</code> samples of size <code class="reqn">n</code> from <code class="reqn">(p - k)</code>-dimensional standard multivariate normal distribution.
</p>
<p>Note that if <code>maxiter</code> is reached at any step of the algorithm it will use the current estimated direction and continue to the next step. 
</p>


<h3>Value</h3>

<p>A list with class 'ictest', inheriting from the class 'hctest', containing the following components:
</p>
<table>
<tr><td><code>statistic</code></td>
<td>
<p>Test statistic, i.e. the objective function value of the (<code>k</code> + 1)th estimated component.</p>
</td></tr>
<tr><td><code>p.value</code></td>
<td>
<p>Obtained <code class="reqn">p</code>-value.</p>
</td></tr>
<tr><td><code>parameter</code></td>
<td>
<p>Number <code>N</code> of simulated normal samples.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>Character string denoting which test was performed.</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>Character string giving the name of the data.</p>
</td></tr>
<tr><td><code>alternative</code></td>
<td>
<p>Alternative hypothesis, i.e. <code>"There are less than p - k Gaussian components"</code>.</p>
</td></tr>
<tr><td><code>k</code></td>
<td>
<p>Tested dimension <code>k</code>.</p>
</td></tr>
<tr><td><code>W</code></td>
<td>
<p>Estimated unmixing matrix</p>
</td></tr>
<tr><td><code>S</code></td>
<td>
<p>Matrix of size <code class="reqn">n \times (k + 1)</code> containing the estimated signals.</p>
</td></tr>
<tr><td><code>D</code></td>
<td>
<p>Vector of the objective function values of the signals</p>
</td></tr>
<tr><td><code>MU</code></td>
<td>
<p>Location vector of the data which was substracted before estimating the signal components.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Joni Virta
</p>


<h3>References</h3>

<p><cite>Virta, J., Nordhausen, K. and Oja, H., (2016), Projection Pursuit for non-Gaussian Independent Components, &lt;https://arxiv.org/abs/1612.05445&gt;.</cite> 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+NGPP">NGPP</a>, <a href="#topic+NGPPest">NGPPest</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Simulated data with 2 signals and 2 noise components

n &lt;- 200
S &lt;- cbind(rexp(n), rbeta(n, 1, 2), rnorm(n), rnorm(n))
A &lt;- matrix(rnorm(16), ncol = 4)
X &lt;- S %*% t(A)

# The number of simulations N should be increased in practical situations
# Now we settle for N = 100

res1 &lt;- NGPPsim(X, 1, N = 100)
res1
screeplot(res1)

res2 &lt;- NGPPsim(X, 2, N = 100)
res2
screeplot(res2)
</code></pre>

<hr>
<h2 id='PCAasymp'>
Testing for Subsphericity using the Covariance Matrix or Tyler's Shape Matrix
</h2><span id='topic+PCAasymp'></span>

<h3>Description</h3>

<p>The function tests, assuming an elliptical model, that the last <code>p-k</code> eigenvalues of
a scatter matrix are equal and the <code>k</code> interesting components are those with a larger variance. 
The scatter matrices that can be used here are the regular covariance matrix and Tyler's shape matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>PCAasymp(X, k, scatter = "cov", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="PCAasymp_+3A_x">X</code></td>
<td>
<p>a numeric data matrix with p&gt;1 columns.</p>
</td></tr>
<tr><td><code id="PCAasymp_+3A_k">k</code></td>
<td>
<p>the number of eigenvalues larger than the equal ones. Can be between 0 and p-2.</p>
</td></tr>
<tr><td><code id="PCAasymp_+3A_scatter">scatter</code></td>
<td>
<p>the scatter matrix to be used. Can be <code>"cov"</code> or <code>"tyler"</code>. For <code>"cov"</code> the regular covariance matrix is computed and for
<code>"tyler"</code> the function <code><a href="ICSNP.html#topic+HR.Mest">HR.Mest</a></code> is used to compute Tyler's shape matrix.</p>
</td></tr>
<tr><td><code id="PCAasymp_+3A_...">...</code></td>
<td>
<p>arguments passed on to <code><a href="ICSNP.html#topic+HR.Mest">HR.Mest</a></code> if <code>scatter = "tyler"</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The functions assumes an elliptical model and tests if the last <code class="reqn">p-k</code> eigenvalues of PCA are equal. PCA can here be either be based on the regular covariance matrix or on Tyler's shape matrix.
</p>
<p>For a sample of size <code class="reqn">n</code>, the test statistic is
</p>
<p style="text-align: center;"><code class="reqn">T = n / (2 \bar{d}^2 \sigma_1) \sum_{k+1}^p (d_i - \bar{d})^2,</code>
</p>
 
<p>where <code class="reqn">\bar{d}</code> is the mean of the last <code class="reqn">p-k</code> PCA eigenvalues. 
</p>
<p>The constant <code class="reqn">\sigma_1</code> is for the regular covariance matrix estimated from the data whereas for Tyler's shape matrix it is simply a function of the dimension of the data.
</p>
<p>The test statistic has a limiting chisquare distribution with <code class="reqn">(p-k-1)(p-k+2)/2</code> degrees of freedom.
</p>
<p>Note that the regular covariance matrix is here divided by <code class="reqn">n</code> and not by <code class="reqn">n-1</code>. 
</p>


<h3>Value</h3>

<p>A list of class ictest inheriting from class htest containing:
</p>
<table>
<tr><td><code>statistic</code></td>
<td>
<p>the value of the test statistic.</p>
</td></tr>
<tr><td><code>p.value</code></td>
<td>
<p>the p-value of the test.</p>
</td></tr>
<tr><td><code>parameter</code></td>
<td>
<p>the degrees of freedom of the test.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>character string which test was performed.</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>character string giving the name of the data.</p>
</td></tr>
<tr><td><code>alternative</code></td>
<td>
<p>character string specifying the alternative hypothesis.</p>
</td></tr>
<tr><td><code>k</code></td>
<td>
<p>the number or larger eigenvalues used in the testing problem.</p>
</td></tr>
<tr><td><code>W</code></td>
<td>
<p>the transformation matrix to the principal components.</p>
</td></tr>
<tr><td><code>S</code></td>
<td>
<p>data matrix with the centered principal components.</p>
</td></tr>
<tr><td><code>D</code></td>
<td>
<p>the underlying eigenvalues.</p>
</td></tr>
<tr><td><code>MU</code></td>
<td>
<p>the location of the data which was substracted before calculating the principal components.</p>
</td></tr>
<tr><td><code>SCATTER</code></td>
<td>
<p>the computed scatter matrix.</p>
</td></tr>
<tr><td><code>sigma1</code></td>
<td>
<p>the asymptotic constant needed for the asymptotic test.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Klaus Nordhausen
</p>


<h3>References</h3>

<p><cite>Nordhausen, K., Oja, H. and Tyler, D.E. (2022), Asymptotic and Bootstrap Tests for Subspace Dimension, Journal of Multivariate Analysis, 188, 104830. &lt;doi:10.1016/j.jmva.2021.104830&gt;.</cite>
</p>


<h3>See Also</h3>

<p><code><a href="ICSNP.html#topic+HR.Mest">HR.Mest</a></code>, <code><a href="#topic+PCAboot">PCAboot</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n &lt;- 200
X &lt;- cbind(rnorm(n, sd = 2), rnorm(n, sd = 1.5), rnorm(n), rnorm(n), rnorm(n))

TestCov &lt;- PCAasymp(X, k = 2)
TestCov
TestTyler &lt;- PCAasymp(X, k = 1, scatter = "tyler")
TestTyler
</code></pre>

<hr>
<h2 id='PCAaug'>
Augmentation Estimate for PCA
</h2><span id='topic+PCAaug'></span>

<h3>Description</h3>

<p>For p-variate data, the augmentation estimate for PCA assumes that the last p-k eigenvalues are equal. Combining information from the eigenvalues and eigenvectors
of the covariance matrix the augmentation estimator yields an estimate for k. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>PCAaug(X, noise = "median", naug = 1, nrep = 1, sigma2 = NULL, alpha = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="PCAaug_+3A_x">X</code></td>
<td>
<p>numeric data matrix.</p>
</td></tr>
<tr><td><code id="PCAaug_+3A_noise">noise</code></td>
<td>
<p>name of the method to be used to estimate the noise variance. Options are <code>"median"</code>, <code>"last"</code>, <code>"quantile"</code> or <code>"known"</code>. See details.</p>
</td></tr>
<tr><td><code id="PCAaug_+3A_naug">naug</code></td>
<td>
<p>number of components to be augmented.</p>
</td></tr>
<tr><td><code id="PCAaug_+3A_nrep">nrep</code></td>
<td>
<p>number of repetitions for the augmentation procedure.</p>
</td></tr>
<tr><td><code id="PCAaug_+3A_sigma2">sigma2</code></td>
<td>
<p>value of the noise variance when <code>noise = "known"</code>.</p>
</td></tr>
<tr><td><code id="PCAaug_+3A_alpha">alpha</code></td>
<td>
<p>the quantile to be used when <code>noise = "quantile"</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The model here assumes that the eigenvalues of the covariance matrix are of the form <code class="reqn">\lambda_1 \geq ... \geq \lambda_{k} &gt; \lambda_{k+1} =  ... = \lambda_p</code>
and the goal is to estimate the value of k. The value <code class="reqn">\lambda_{k+1}</code> corresponds then to the noise variance. 
</p>
<p>The augmented estimator adds for that purpose <code>naug</code> Gaussian components with the provided noise variance which needs to be provided
(<code>noise = "known"</code>) or estimated from the data. Three estimation methods are available. In the case of <code>noise = "median"</code> the estimate is the median of the eigenvalues of the covariance matrix, in the case of <code>noise = "last"</code> it corresponds to the last eigenvalue of the covariance matrix and in the case of <code>noise = "quantile"</code> it is the mean of the eigenvalues smaller or equal to the <code>alpha</code>-quantile of the eigenvalues of the covariance matrix.
</p>
<p>The augmentation estimator uses then the augmented components to measure the variation of the eigenvalues. For a more stable result it is recommened to repeat the augmentation process several times and Lue and Li (2021) recommend to use for <code>naug</code> approximately <code>p/5</code> or <code>p/10</code> where <code>p</code> is the number of columns of <code>X</code>. 
</p>
<p>The augmented estimator for this purpose combines then the values of the 
scaled eigenvalues and the variation measured via augmentation. The main idea there is that for distinct eigenvales the variation of the eigenvectors
is small and for equal eigenvalues the corresponding eigenvectors have large variation.
</p>
<p>The augmented estimate for k is the value where the measure takes its minimum and can be also visualized as a ladle.
</p>
<p>For further details see Luo and Li (2021) and Radojicic et al. (2021).
</p>


<h3>Value</h3>

<p>A list of class ladle containing:
</p>
<table>
<tr><td><code>method</code></td>
<td>
<p>the string PCA.</p>
</td></tr>
<tr><td><code>k</code></td>
<td>
<p>the estimated value of k.</p>
</td></tr>
<tr><td><code>fn</code></td>
<td>
<p>vector giving the measures of variation of the eigenvectors using the bootstrapped eigenvectors for the different number of components.</p>
</td></tr>
<tr><td><code>phin</code></td>
<td>
<p>normalized eigenvalues of the covariance matrix.</p>
</td></tr>
<tr><td><code>gn</code></td>
<td>
<p>the main criterion for the augmented estimate - the sum of fn and phin. k is the value where gn takes its minimum</p>
</td></tr>
<tr><td><code>lambda</code></td>
<td>
<p>the eigenvalues of the covariance matrix.</p>
</td></tr>
<tr><td><code>W</code></td>
<td>
<p>the transformation matrix to the principal components.</p>
</td></tr>
<tr><td><code>S</code></td>
<td>
<p>data matrix with the centered principal components.</p>
</td></tr>
<tr><td><code>MU</code></td>
<td>
<p>the location of the data which was substracted before calculating the principal components.</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>the name of the data for which the augmented estimate was computed.</p>
</td></tr>
<tr><td><code>sigma2</code></td>
<td>
<p>the value used as noise variance when simulating the augmented components.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Klaus Nordhausen
</p>


<h3>References</h3>

<p><cite>Luo, W. and Li, B. (2021), On Order Determination by Predictor Augmentation, Biometrika, 108, 557&ndash;574. &lt;doi:10.1093/biomet/asaa077&gt;</cite>
</p>
<p><cite>Radojicic, U., Lietzen, N., Nordhausen, K. and Virta, J. (2021), Dimension Estimation in Two-Dimensional PCA. In S. Loncaric, T. Petkovic and D. Petrinovic (editors) &quot;Proceedings of the 12 International Symposium on Image and Signal Processing and Analysis (ISPA 2021)&quot;, 16&ndash;22. &lt;doi:10.1109/ISPA52656.2021.9552114&gt;</cite>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ladleplot">ladleplot</a></code>, <code><a href="#topic+PCAladle">PCAladle</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n &lt;- 1000
Y &lt;- cbind(rnorm(n, sd=2), rnorm(n,sd=2), rnorm(n), rnorm(n), rnorm(n), rnorm(n))

testPCA &lt;- PCAaug(Y) 
testPCA
summary(testPCA)
plot(testPCA)
ladleplot(testPCA)
ladleplot(testPCA, crit = "fn")
ladleplot(testPCA, crit = "lambda")
ladleplot(testPCA, crit = "phin")
</code></pre>

<hr>
<h2 id='PCAboot'>
Bootstrap-Based Testing for Subsphericity 
</h2><span id='topic+PCAboot'></span>

<h3>Description</h3>

<p>The function tests, assuming an elliptical model, that the last <code>p-k</code> eigenvalues of
a scatter matrix are equal and the <code>k</code> interesting components are those with a larger variance. 
To obtain p-values two different bootstrapping strategies are available and the user can provide the scatter matrix to be used
as a function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>PCAboot(X, k, n.boot = 200, s.boot = "B1", S = MeanCov, Sargs = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="PCAboot_+3A_x">X</code></td>
<td>
<p>a numeric data matrix with p&gt;1 columns.</p>
</td></tr>
<tr><td><code id="PCAboot_+3A_k">k</code></td>
<td>
<p>the number of eigenvalues larger than the equal ones. Can be between 0 and p-2.</p>
</td></tr>
<tr><td><code id="PCAboot_+3A_n.boot">n.boot</code></td>
<td>
<p>number of bootstrapping samples.</p>
</td></tr>
<tr><td><code id="PCAboot_+3A_s.boot">s.boot</code></td>
<td>
<p>bootstrapping strategy to be used. Possible values are <code>"B1"</code>, <code>"B2"</code>. See details for further information.</p>
</td></tr>
<tr><td><code id="PCAboot_+3A_s">S</code></td>
<td>
<p>A function which returns a list that has as its first element a location vector and as the second element the scatter matrix.</p>
</td></tr>
<tr><td><code id="PCAboot_+3A_sargs">Sargs</code></td>
<td>
<p>list of further arguments passed on to the function specified in <code>S</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Here the function <code>S</code> needs to return a list where the first argument is a location vector and the second one a scatter matrix.
</p>
<p>The location is used to center the data and the scatter matrix is used to perform PCA.
</p>
<p>Consider X as the centered data and denote by W the transformation matrix to the principal components. The corresponding eigenvalues
from PCA are <code class="reqn">d_1,...,d_p</code>. Under the null, <code class="reqn">d_k &gt; d_{k+1} = ... = d_{p}</code>.
Denote further by <code class="reqn">\bar{d}</code> the mean of the last <code>p-k</code> eigenvalues and by <code class="reqn">D^* = diag(d_1,...,d_k,\bar{d},...,\bar{d})</code> a <code class="reqn">p \times p</code> diagonal matrix. Assume that <code class="reqn">S</code> is the matrix with principal components which can be decomposed into <code class="reqn">S_1</code> and <code class="reqn">S_2</code> where 
<code class="reqn">S_1</code> contains the k interesting principal components and <code class="reqn">S_2</code> the last <code class="reqn">p-k</code> principal components.
</p>
<p>For a sample of size <code class="reqn">n</code>, the test statistic used for the boostrapping tests is
</p>
<p style="text-align: center;"><code class="reqn">T = n / (\bar{d}^2) \sum_{k+1}^p (d_i - \bar{d})^2.</code>
</p>
 
<p>The function offers then two boostrapping strategies:
</p>

<ol>
<li> <p><code>s.boot="B1"</code>: 
The first strategy has the following steps:
</p>

<ol>
<li><p> Take a bootstrap sample <code class="reqn">S^*</code> of size <code class="reqn">n</code> from <code class="reqn">S</code> and decompose it into <code class="reqn">S_1^*</code> and <code class="reqn">S_2^*</code>.
</p>
</li>
<li><p> Every observation in <code class="reqn">S_2^*</code> is transformed with a different random orthogonal matrix.
</p>
</li>
<li><p> Recombine <code class="reqn">S^*=(S_1^*, S_2^*)</code> and create <code class="reqn">X^*= S^* W</code>.
</p>
</li>
<li><p> Compute the test statistic based on <code class="reqn">X^*</code>. 
</p>
</li>
<li><p> Repeat the previous steps <code>n.boot</code> times.
</p>
</li></ol>

</li>
<li> <p><code>s.boot="B2"</code>: 
The second strategy has the following steps:
</p>

<ol>
<li><p> Scale each principal component using the matrix <code class="reqn">D</code>, i.e. <code class="reqn">Z = S D</code>. 
</p>
</li>
<li><p> Take a bootstrap sample <code class="reqn">Z^*</code> of size <code class="reqn">n</code> from <code class="reqn">Z</code>.
</p>
</li>
<li><p> Every observation in <code class="reqn">Z^*</code> is transformed with a different random orthogonal matrix.
</p>
</li>
<li><p> Recreate <code class="reqn">X^*= Z^* {D^*}^{-1} W</code>.
</p>
</li>
<li><p> Compute the test statistic based on <code class="reqn">X^*</code>. 
</p>
</li>
<li><p> Repeat the previous steps <code>n.boot</code> times.
</p>
</li></ol>

<p>To create the random orthogonal matrices the function <code><a href="#topic+rorth">rorth</a></code> is used.
</p>
</li></ol>



<h3>Value</h3>

<p>A list of class ictest inheriting from class htest containing:
</p>
<table>
<tr><td><code>statistic</code></td>
<td>
<p>the value of the test statistic.</p>
</td></tr>
<tr><td><code>p.value</code></td>
<td>
<p>the p-value of the test.</p>
</td></tr>
<tr><td><code>parameter</code></td>
<td>
<p>the degrees of freedom of the test.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>character string which test was performed.</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>character string giving the name of the data.</p>
</td></tr>
<tr><td><code>alternative</code></td>
<td>
<p>character string specifying the alternative hypothesis.</p>
</td></tr>
<tr><td><code>k</code></td>
<td>
<p>the number or larger eigenvalues used in the testing problem.</p>
</td></tr>
<tr><td><code>W</code></td>
<td>
<p>the transformation matrix to the principal components.</p>
</td></tr>
<tr><td><code>S</code></td>
<td>
<p>data matrix with the centered principal components.</p>
</td></tr>
<tr><td><code>D</code></td>
<td>
<p>the underlying eigenvalues.</p>
</td></tr>
<tr><td><code>MU</code></td>
<td>
<p>the location of the data which was substracted before calculating the principal components.</p>
</td></tr>
<tr><td><code>SCATTER</code></td>
<td>
<p>The computed scatter matrix.</p>
</td></tr>
<tr><td><code>scatter</code></td>
<td>
<p>character string denoting which scatter function was used.</p>
</td></tr>
<tr><td><code>s.boot</code></td>
<td>
<p>character string denoting which bootstrapping test version was used.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Klaus Nordhausen
</p>


<h3>References</h3>

<p><cite>Nordhausen, K., Oja, H. and Tyler, D.E. (2022), Asymptotic and Bootstrap Tests for Subspace Dimension, Journal of Multivariate Analysis, 188, 104830. &lt;doi:10.1016/j.jmva.2021.104830&gt;.</cite>
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+cov">cov</a></code>, <code><a href="ICS.html#topic+MeanCov">MeanCov</a></code>, <code><a href="#topic+PCAasymp">PCAasymp</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n &lt;- 200
X &lt;- cbind(rnorm(n, sd = 2), rnorm(n, sd = 1.5), rnorm(n), rnorm(n), rnorm(n))

# for demonstration purpose the n.boot is chosen small, should be larger in real applications

TestCov &lt;- PCAboot(X, k = 2, n.boot=30)
TestCov


TestTM &lt;- PCAboot(X, k = 1, n.boot=30, s.boot = "B2", S = "tM", Sargs = list(df=2))
TestTM

</code></pre>

<hr>
<h2 id='PCAladle'>
Ladle Estimate for PCA
</h2><span id='topic+PCAladle'></span>

<h3>Description</h3>

<p>For p-variate data, the Ladle estimate for PCA assumes that the last p-k eigenvalues are equal. Combining information from the eigenvalues and eigenvectors
of the covariance matrix the ladle estimator yields an estimate for k. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>PCAladle(X, n.boot = 200, 
         ncomp = ifelse(ncol(X) &gt; 10, floor(ncol(X)/log(ncol(X))), ncol(X) - 1))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="PCAladle_+3A_x">X</code></td>
<td>
<p>numeric data matrix.</p>
</td></tr>
<tr><td><code id="PCAladle_+3A_n.boot">n.boot</code></td>
<td>
<p>number of bootstrapping samples to be used.</p>
</td></tr>
<tr><td><code id="PCAladle_+3A_ncomp">ncomp</code></td>
<td>
<p>The number of components among which the ladle estimator is to be searched. The default here follows
the recommendation of Luo and Li 2016.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The model here assumes that the eigenvalues of the covariance matrix are of the form <code class="reqn">\lambda_1 \geq ... \geq \lambda_{k} &gt; \lambda_{k+1} =  ... = \lambda_p</code>
and the goal is to estimate the value of k. The ladle estimate for this purpose combines the values of the 
scaled eigenvalues and the variation of the eigenvectors based on bootstrapping. The idea there is that for distinct eigenvales the variation of the eigenvectors
is small and for equal eigenvalues the corresponding eigenvectors have large variation.
</p>
<p>This measure is then computed assuming k=0,..., <code>ncomp</code> and the ladle estimate for k is the value where the measure takes its minimum. 
</p>


<h3>Value</h3>

<p>A list of class ladle containing:
</p>
<table>
<tr><td><code>method</code></td>
<td>
<p>the string PCA.</p>
</td></tr>
<tr><td><code>k</code></td>
<td>
<p>the estimated value of k.</p>
</td></tr>
<tr><td><code>fn</code></td>
<td>
<p>vector giving the measures of variation of the eigenvectors using the bootstrapped eigenvectors for the different number of components.</p>
</td></tr>
<tr><td><code>phin</code></td>
<td>
<p>normalized eigenvalues of the covariance matrix.</p>
</td></tr>
<tr><td><code>gn</code></td>
<td>
<p>the main criterion for the ladle estimate - the sum of fn and phin. k is the value where gn takes its minimum</p>
</td></tr>
<tr><td><code>lambda</code></td>
<td>
<p>the eigenvalues of the covariance matrix.</p>
</td></tr>
<tr><td><code>W</code></td>
<td>
<p>the transformation matrix to the principal components.</p>
</td></tr>
<tr><td><code>S</code></td>
<td>
<p>data matrix with the centered principal components.</p>
</td></tr>
<tr><td><code>MU</code></td>
<td>
<p>the location of the data which was substracted before calculating the principal components.</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>the name of the data for which the ladle estimate was computed.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Klaus Nordhausen
</p>


<h3>References</h3>

<p><cite>Luo, W. and Li, B. (2016), Combining Eigenvalues and Variation of Eigenvectors for Order Determination, Biometrika, 103, 875&ndash;887. &lt;doi:10.1093/biomet/asw051&gt;</cite>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ladleplot">ladleplot</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n &lt;- 1000
Y &lt;- cbind(rnorm(n, sd=2), rnorm(n,sd=2), rnorm(n), rnorm(n), rnorm(n), rnorm(n))

testPCA &lt;- PCAladle(Y)
testPCA
summary(testPCA)
plot(testPCA)
ladleplot(testPCA)
ladleplot(testPCA, crit = "fn")
ladleplot(testPCA, crit = "lambda")
ladleplot(testPCA, crit = "phin")
</code></pre>

<hr>
<h2 id='PCAschott'>
Testing for Subsphericity using the Schott's test
</h2><span id='topic+PCAschott'></span>

<h3>Description</h3>

<p>The test tests the equality of the last eigenvalues assuming normal distributed data using the regular covariance matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>PCAschott(X, k)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="PCAschott_+3A_x">X</code></td>
<td>
<p>a numeric data matrix with p&gt;1 columns.</p>
</td></tr>
<tr><td><code id="PCAschott_+3A_k">k</code></td>
<td>
<p>the number of eigenvalues larger than the equal ones. Can be between 0 and p-2.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The functions assumes multivariate normal data and tests if the last <code class="reqn">p-k</code> eigenvalues of PCA are equal.
</p>


<h3>Value</h3>

<p>A list of class ictest inheriting from class htest containing:
</p>
<table>
<tr><td><code>statistic</code></td>
<td>
<p>the value of the test statistic.</p>
</td></tr>
<tr><td><code>p.value</code></td>
<td>
<p>the p-value of the test.</p>
</td></tr>
<tr><td><code>parameter</code></td>
<td>
<p>the degrees of freedom of the test.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>character string which test was performed.</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>character string giving the name of the data.</p>
</td></tr>
<tr><td><code>alternative</code></td>
<td>
<p>character string specifying the alternative hypothesis.</p>
</td></tr>
<tr><td><code>k</code></td>
<td>
<p>the number or larger eigenvalues used in the testing problem.</p>
</td></tr>
<tr><td><code>W</code></td>
<td>
<p>the transformation matrix to the principal components.</p>
</td></tr>
<tr><td><code>S</code></td>
<td>
<p>data matrix with the centered principal components.</p>
</td></tr>
<tr><td><code>D</code></td>
<td>
<p>the underlying eigenvalues.</p>
</td></tr>
<tr><td><code>MU</code></td>
<td>
<p>the mean vector of the data which was substracted before calculating the principal components.</p>
</td></tr>
<tr><td><code>SCATTER</code></td>
<td>
<p>the computed covariance matrix matrix.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Klaus Nordhausen
</p>


<h3>References</h3>

<p><cite>Schott, J.R. (2006), A High-Dimensional Test for the Equality of the Smallest Eigenvalues of a Covariance Matrix, Journal of Multivariate Analysis, 97, 827&ndash;843. &lt;doi:10.1016/j.jmva.2005.05.003&gt;</cite>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+PCAasymp">PCAasymp</a></code>, <code><a href="#topic+PCAboot">PCAboot</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n &lt;- 200
X &lt;- cbind(rnorm(n, sd = 2), rnorm(n, sd = 1.5), rnorm(n), rnorm(n), rnorm(n))
PCAschott(X, 2)
</code></pre>

<hr>
<h2 id='plot.ictest'>
Scatterplot Matrix for a ictest Object
</h2><span id='topic+plot.ictest'></span>

<h3>Description</h3>

<p>For an object of class ictest, plots either the pairwise scatter plot matrix, or the time series plots of the underlying components. The user can choose if only the components considered interesting or all of them should be plotted.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ictest'
plot(x, which = "all", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.ictest_+3A_x">x</code></td>
<td>
<p>object of class ictest</p>
</td></tr>
<tr><td><code id="plot.ictest_+3A_which">which</code></td>
<td>
<p>if <code>"all"</code>, then all components of S in the ictest object are plotted. If <code>"k"</code>, then only the first k components are plotted,
where the value of <code>k</code> is taken from the ictest object. This is only meaningful if <code>k</code> was at least 2.</p>
</td></tr>
<tr><td><code id="plot.ictest_+3A_...">...</code></td>
<td>
<p>other arguments passed on to <code><a href="graphics.html#topic+pairs">pairs</a></code> if the components are a numeric matrix or to <code><a href="stats.html#topic+plot.ts">plot.ts</a></code>, <code><a href="zoo.html#topic+plot.zoo">plot.zoo</a></code> or <code><a href="xts.html#topic+plot.xts">plot.xts</a></code> if the components are from the corresponding class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If the component matrix has the class <code>mts</code>, <code>xts</code> or <code>zoo</code>, then a time series plot will be plotted. Otherwise, the pairwise scatter plot matrix will be plotted.
</p>


<h3>Author(s)</h3>

<p>Klaus Nordhausen
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ggplot.ictest">ggplot.ictest</a>, <a href="graphics.html#topic+pairs">pairs</a>, <a href="stats.html#topic+plot.ts">plot.ts</a>, <a href="zoo.html#topic+plot.zoo">plot.zoo</a>, <a href="xts.html#topic+plot.xts">plot.xts</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n &lt;- 200
X &lt;- cbind(rnorm(n, sd = 2), rnorm(n, sd = 1.5), rnorm(n), rnorm(n), rnorm(n))

TestCov &lt;- PCAasymp(X, k = 2)
plot(TestCov)
plot(TestCov, which = "k")
</code></pre>

<hr>
<h2 id='plot.ladle'>
Plotting an Object of Class ladle
</h2><span id='topic+plot.ladle'></span>

<h3>Description</h3>

<p>An object of class ladle contains always the source components as estimated by the corresponding statistical method.
This function either plots all of the components or only this considered interesting according to the ladle estimate.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ladle'
plot(x, which = "all", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.ladle_+3A_x">x</code></td>
<td>
<p>an object of class ladle.</p>
</td></tr>
<tr><td><code id="plot.ladle_+3A_which">which</code></td>
<td>
<p>if <code>"all"</code>, then all components of S in the ladle object are
plotted. If <code>"k"</code>, then only the  k components are
plotted, which are considered interesting according to the ladle estimator. 
This is only meaningful if the estimated 'k' is at least 2.</p>
</td></tr>
<tr><td><code id="plot.ladle_+3A_...">...</code></td>
<td>
<p>other arguments passed on to <code><a href="graphics.html#topic+pairs">pairs</a></code> if the components are a numeric matrix or to <code><a href="stats.html#topic+plot.ts">plot.ts</a></code>, <code><a href="zoo.html#topic+plot.zoo">plot.zoo</a></code> or <code><a href="xts.html#topic+plot.xts">plot.xts</a></code> if the components are from the corresponding class.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If the component matrix has the class <code>mts</code>, <code>xts</code> or <code>zoo</code>, then a time series plot will be plotted. Otherwise, the pairwise scatter plot matrix will be plotted.
</p>


<h3>Author(s)</h3>

<p>Klaus Nordhausen
</p>


<h3>See Also</h3>

<p><code><a href="graphics.html#topic+pairs">pairs</a></code>, <code><a href="stats.html#topic+plot.ts">plot.ts</a></code>, <code><a href="zoo.html#topic+plot.zoo">plot.zoo</a></code>, <code><a href="xts.html#topic+plot.xts">plot.xts</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n &lt;- 1000
X &lt;- cbind(rexp(n), rt(n,5), rnorm(n), rnorm(n), rnorm(n), rnorm(n))
test &lt;- FOBIladle(X)
plot(test)
</code></pre>

<hr>
<h2 id='print.ladle'>
Printing an Object of Class ladle
</h2><span id='topic+print.ladle'></span>

<h3>Description</h3>

<p>Basic printing of an object of class ladle. Prints basically everything but the estimated components.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ladle'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.ladle_+3A_x">x</code></td>
<td>
<p>an object of class ladle.</p>
</td></tr>
<tr><td><code id="print.ladle_+3A_...">...</code></td>
<td>
<p>further arguments to be passed to or from methods.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Klaus Nordhausen
</p>


<h3>See Also</h3>

<p><code><a href="#topic+summary.ladle">summary.ladle</a></code>, <code><a href="#topic+plot.ladle">plot.ladle</a></code>, <code><a href="#topic+ladleplot">ladleplot</a></code>, <code><a href="#topic+FOBIladle">FOBIladle</a></code>, <code><a href="#topic+PCAladle">PCAladle</a></code>, <code><a href="#topic+SIRladle">SIRladle</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n &lt;- 1000
X &lt;- cbind(rexp(n), rt(n,5), rnorm(n), rnorm(n), rnorm(n), rnorm(n))
test &lt;- FOBIladle(X)
test
</code></pre>

<hr>
<h2 id='rMU'>
Greek Letter mu Shaped Bivariate Data Generation
</h2><span id='topic+rMU'></span>

<h3>Description</h3>

<p>A function to generate bivariate data with the scatterplot resembling the greek letter mu.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rMU(n)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rMU_+3A_n">n</code></td>
<td>
<p>the sample size.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>n</code> times <code>2</code> matrix
</p>


<h3>Author(s)</h3>

<p>Klaus Nordhausen, Joni Virta
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- rMU(1000)

plot(x)
</code></pre>

<hr>
<h2 id='rOMEGA'>
Greek Letter Omega Shaped Bivariate Data Generation
</h2><span id='topic+rOMEGA'></span>

<h3>Description</h3>

<p>A function to generate bivariate data with the scatterplot resembling the greek letter Omega.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rOMEGA(n)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rOMEGA_+3A_n">n</code></td>
<td>
<p>the sample size.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>n</code> times <code>2</code> matrix
</p>


<h3>Author(s)</h3>

<p>Klaus Nordhausen, Joni Virta
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- rOMEGA(1000)

plot(x)
</code></pre>

<hr>
<h2 id='rorth'>
Random Orthogonal Matrix Creation Uniform WRT the Haar Measure.
</h2><span id='topic+rorth'></span>

<h3>Description</h3>

<p>A function to create a random orthogonal matrix uniformly distributed with respect to the Haar measure.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rorth(k)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rorth_+3A_k">k</code></td>
<td>
<p>the desired numer of columns (=rows) of the orthogonal matrix.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function fills a <code>k</code>x<code>k</code> matrix with N(0,1) random variables and perfroms then a QR decompoistion using <code><a href="base.html#topic+qr">qr</a></code>. If the diagonal elements of R are all positive the resulting orthogonal matrix Q is uniform distributed wrt to the Haar measure. Note that the function currently does not check if
all diagonal measurements are indeed positive (however this will happen with probability 1 in theory). 
</p>


<h3>Value</h3>

<p>An orthogonal <code>k</code> times <code>k</code> matrix
</p>


<h3>Author(s)</h3>

<p>Klaus Nordhausen
</p>


<h3>References</h3>

<p><cite>Stewart, G.W. (1980), The efficient generation of random orthogonal matrices with an application to condition estimators, <em>SIAM Journal on Numerical Analysis</em>, <b>17</b>,  403&ndash;409. &lt;doi:10.1137/0717034&gt;.</cite>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>Orth &lt;- rorth(4)

crossprod(Orth)
tcrossprod(Orth)
</code></pre>

<hr>
<h2 id='screeplot.ictest'>
Screeplot for an ictest Object
</h2><span id='topic+screeplot.ictest'></span>

<h3>Description</h3>

<p>Plots the criterion values of an <code>ictest</code> object against its index number. Two versions of this screeplot are available.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ictest'
screeplot(x, type = "barplot", main = deparse(substitute(x)), 
  ylab = "criterion", xlab = "component", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="screeplot.ictest_+3A_x">x</code></td>
<td>
<p> object of class <code>ictest</code>.</p>
</td></tr>
<tr><td><code id="screeplot.ictest_+3A_type">type</code></td>
<td>
 <p><code>barplot</code> if a barplot or <code>lines</code> if a line plot is preferred.</p>
</td></tr>
<tr><td><code id="screeplot.ictest_+3A_main">main</code></td>
<td>
<p> main title of the plot. </p>
</td></tr>
<tr><td><code id="screeplot.ictest_+3A_ylab">ylab</code></td>
<td>
<p> y-axis label. </p>
</td></tr>
<tr><td><code id="screeplot.ictest_+3A_xlab">xlab</code></td>
<td>
<p> x-axis label. </p>
</td></tr>
<tr><td><code id="screeplot.ictest_+3A_...">...</code></td>
<td>
<p>other arguments for the plotting functions. </p>
</td></tr>  
</table>


<h3>Author(s)</h3>

<p>Klaus Nordhausen
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ggscreeplot">ggscreeplot</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n &lt;- 200
X &lt;- cbind(rnorm(n, sd = 2), rnorm(n, sd = 1.5), rnorm(n), rnorm(n), rnorm(n))

TestCov &lt;- PCAasymp(X, k = 2)
screeplot(TestCov)
</code></pre>

<hr>
<h2 id='SIRasymp'>
Testing the Subspace Dimension for Sliced Inverse Regression.
</h2><span id='topic+SIRasymp'></span>

<h3>Description</h3>

<p>Using the two scatter matrices approach (SICS) for sliced inversion regression (SIR), the function tests
if the last <code>p-k</code> components have zero eigenvalues, where <code>p</code> is the number of explaining variables. Hence the assumption is that the first <code>k</code> 
components are relevant for modelling the response <code>y</code> and the remaining components are not.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SIRasymp(X, y, k, h = 10, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SIRasymp_+3A_x">X</code></td>
<td>
<p>a numeric data matrix of explaining variables.</p>
</td></tr>
<tr><td><code id="SIRasymp_+3A_y">y</code></td>
<td>
<p>a numeric vector specifying the response.</p>
</td></tr>
<tr><td><code id="SIRasymp_+3A_k">k</code></td>
<td>
<p>the number of relevant components under the null hypothesis.</p>
</td></tr>
<tr><td><code id="SIRasymp_+3A_h">h</code></td>
<td>
<p>the number of slices used in SIR. Passed on to function <code><a href="#topic+covSIR">covSIR</a></code>.</p>
</td></tr>
<tr><td><code id="SIRasymp_+3A_...">...</code></td>
<td>
<p>other arguments passed on to <code><a href="#topic+covSIR">covSIR</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Under the null the first <code>k</code> eigenvalues contained in <code>D</code> are non-zero and the remaining <code>p-k</code> are zero.
</p>
<p>For a sample of size <code class="reqn">n</code>, the test statistic <code class="reqn">T</code> is then n times the sum of these last p-k eigenvalue and has under the null a chisquare distribution with <code class="reqn">(p-k)(h-k-1)</code> degrees of freedom,
therefore it is required that <code class="reqn">k &lt; h-1</code>.
</p>


<h3>Value</h3>

<p>A list of class ictest inheriting from class htest containing:
</p>
<table>
<tr><td><code>statistic</code></td>
<td>
<p>the value of the test statistic.</p>
</td></tr>
<tr><td><code>p.value</code></td>
<td>
<p>the p-value of the test.</p>
</td></tr>
<tr><td><code>parameter</code></td>
<td>
<p>the degrees of freedom of the test.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>character string which test was performed.</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>character string giving the name of the data.</p>
</td></tr>
<tr><td><code>alternative</code></td>
<td>
<p>character string specifying the alternative hypothesis.</p>
</td></tr>
<tr><td><code>k</code></td>
<td>
<p>the number of non-zero eigenvalues used in the testing problem.</p>
</td></tr>
<tr><td><code>W</code></td>
<td>
<p>the transformation matrix to the underlying components.</p>
</td></tr>
<tr><td><code>S</code></td>
<td>
<p>data matrix with the centered underlying components.</p>
</td></tr>
<tr><td><code>D</code></td>
<td>
<p>the underlying eigenvalues.</p>
</td></tr>
<tr><td><code>MU</code></td>
<td>
<p>the location of the data which was substracted before calculating the components.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Klaus Nordhausen
</p>


<h3>References</h3>

<p><cite>Nordhausen, K., Oja, H. and Tyler, D.E. (2022), Asymptotic and Bootstrap Tests for Subspace Dimension, Journal of Multivariate Analysis, 188, 104830. &lt;doi:10.1016/j.jmva.2021.104830&gt;.</cite>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+covSIR">covSIR</a></code>,  <code><a href="#topic+SIRboot">SIRboot</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>X &lt;- matrix(rnorm(1000), ncol = 5)
eps &lt;- rnorm(200, sd = 0.1)
y &lt;- 2 + 0.5 * X[, 1] + 2 * X[, 3] + eps
  
SIRasymp(X, y, k = 0) 
SIRasymp(X, y, k = 1)    
</code></pre>

<hr>
<h2 id='SIRboot'>
Testing the Subspace Dimension for Sliced Inverse Regression Using Bootstrapping.
</h2><span id='topic+SIRboot'></span>

<h3>Description</h3>

<p>Using the two scatter matrices approach (SICS) for sliced inversion regression (SIR) the function tests
if the last <code>p-k</code> components have zero eigenvalues, where <code>p</code> is the number of explaining variables. Hence the assumption is that the first <code>k</code> 
components are relevant for modelling the response <code>y</code> and the remaining components are not. The function performs bootstrapping to obtain a p-value.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SIRboot(X, y, k, h = 10, n.boot = 200, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SIRboot_+3A_x">X</code></td>
<td>
<p>a numeric data matrix of explaining variables.</p>
</td></tr>
<tr><td><code id="SIRboot_+3A_y">y</code></td>
<td>
<p>a numeric vector specifying the response.</p>
</td></tr>
<tr><td><code id="SIRboot_+3A_k">k</code></td>
<td>
<p>the number of relevant components under the null hypothesis.</p>
</td></tr>
<tr><td><code id="SIRboot_+3A_h">h</code></td>
<td>
<p>the number of slices used in SIR. Passed on to function <code><a href="#topic+covSIR">covSIR</a></code>.</p>
</td></tr>
<tr><td><code id="SIRboot_+3A_n.boot">n.boot</code></td>
<td>
<p>number of bootstrapping samples.</p>
</td></tr>
<tr><td><code id="SIRboot_+3A_...">...</code></td>
<td>
<p>other arguments passed on to <code><a href="#topic+covSIR">covSIR</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Under the null hypthesis the last p-k eigenvalue as given in D are zero. The test statistic is then the sum of these eigenvalues.
</p>
<p>Denote W as the transformation matrix to the supervised invariant coordinates (SIC) <code class="reqn">s_i</code>, <code class="reqn">i=1,\ldots,n</code>, i.e.
</p>
<p style="text-align: center;"><code class="reqn">s_i = W (x_i-MU),</code>
</p>

<p>where <code>MU</code> is the location.
</p>
<p>Let <code class="reqn">S_1</code> be the submatrix of the SICs which are relevant and <code class="reqn">S_2</code> the submatrix of the SICs which are irrelevant for the response y under the null.
</p>
<p>The boostrapping has then the following steps:
</p>

<ol>
<li><p> Take a boostrap sample <code class="reqn">(y^*, S_1^*)</code> of size <code class="reqn">n</code> from <code class="reqn">(y, S_1)</code>.
</p>
</li>
<li><p> Take a boostrap sample <code class="reqn">S_2^*</code> of size <code class="reqn">n</code> from <code class="reqn">S_2</code>.
</p>
</li>
<li><p> Combine <code class="reqn">S^*=(S_1^*, S_2^*)</code> and create <code class="reqn">X^*= S^* W</code>.
</p>
</li>
<li><p> Compute the test statistic based on <code class="reqn">X^*</code>. 
</p>
</li>
<li><p> Repeat the previous steps <code>n.boot</code> times.
</p>
</li></ol>



<h3>Value</h3>

<p>A list of class ictest inheriting from class htest containing:
</p>
<table>
<tr><td><code>statistic</code></td>
<td>
<p>the value of the test statistic.</p>
</td></tr>
<tr><td><code>p.value</code></td>
<td>
<p>the p-value of the test.</p>
</td></tr>
<tr><td><code>parameter</code></td>
<td>
<p>the number of boostrapping samples used to compute the p-value.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>character string which test was performed.</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>character string giving the name of the data.</p>
</td></tr>
<tr><td><code>alternative</code></td>
<td>
<p>character string specifying the alternative hypothesis.</p>
</td></tr>
<tr><td><code>k</code></td>
<td>
<p>the number of non-zero eigenvalues used in the testing problem.</p>
</td></tr>
<tr><td><code>W</code></td>
<td>
<p>the transformation matrix to the underlying components.</p>
</td></tr>
<tr><td><code>S</code></td>
<td>
<p>data matrix with the centered underlying components.</p>
</td></tr>
<tr><td><code>D</code></td>
<td>
<p>the underlying eigenvalues.</p>
</td></tr>
<tr><td><code>MU</code></td>
<td>
<p>the location of the data which was substracted before calculating the components.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Klaus Nordhausen
</p>


<h3>References</h3>

<p><cite>Nordhausen, K., Oja, H. and Tyler, D.E. (2022), Asymptotic and Bootstrap Tests for Subspace Dimension, Journal of Multivariate Analysis, 188, 104830. &lt;doi:10.1016/j.jmva.2021.104830&gt;.</cite>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+covSIR">covSIR</a></code>,  <code><a href="#topic+SIRasymp">SIRasymp</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>X &lt;- matrix(rnorm(1000), ncol = 5)
eps &lt;- rnorm(200, sd = 0.1)
y &lt;- 2 + 0.5 * X[, 1] + 2 * X[, 3] + eps
  
SIRboot(X, y, k = 0) 
SIRboot(X, y, k = 1)    
</code></pre>

<hr>
<h2 id='SIRladle'>
Ladle Estimate for SIR
</h2><span id='topic+SIRladle'></span>

<h3>Description</h3>

<p>In the supervised dimension reduction context with response y and explaining variables x, this functions provides the ladle estimate 
for the dimension of the central subspace for SIR.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SIRladle(X, y, h = 10, n.boot = 200, 
         ncomp = ifelse(ncol(X) &gt; 10, floor(ncol(X)/log(ncol(X))), ncol(X) - 1), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SIRladle_+3A_x">X</code></td>
<td>
<p>numeric data matrix.</p>
</td></tr>
<tr><td><code id="SIRladle_+3A_y">y</code></td>
<td>
<p>numeric response vector.</p>
</td></tr>
<tr><td><code id="SIRladle_+3A_h">h</code></td>
<td>
<p>number of slices in SIR.</p>
</td></tr>
<tr><td><code id="SIRladle_+3A_n.boot">n.boot</code></td>
<td>
<p>number of bootstrapping samples to be used.</p>
</td></tr>
<tr><td><code id="SIRladle_+3A_ncomp">ncomp</code></td>
<td>
<p>The number of components among which the ladle estimator is to be searched. The default here follows
the recommendation of Luo and Li 2016.</p>
</td></tr>
<tr><td><code id="SIRladle_+3A_...">...</code></td>
<td>
<p>arguments passed on to <code><a href="stats.html#topic+quantile">quantile</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The idea here is that the eigenvalues of the SIR-M matrix are of the form  <code class="reqn">\lambda_1 \geq ... \geq \lambda_k &gt; 0 =  ... = 0</code> and the eigenvectors
of the non-zero eigenvalue span the central subspace.
</p>
<p>The ladle estimate for k for this purpose combines the values of the 
scaled eigenvalues and the variation of the eigenvectors based on bootstrapping. The idea there is that for distinct eigenvales the variation of the eigenvectors
is small and for equal eigenvalues the corresponding eigenvectors have large variation.
</p>
<p>This measure is then computed assuming k=0,..., <code>ncomp</code> and the ladle estimate for k is the value where the measure takes its minimum. 
</p>


<h3>Value</h3>

<p>A list of class ladle containing:
</p>
<table>
<tr><td><code>method</code></td>
<td>
<p>the string SIR.</p>
</td></tr>
<tr><td><code>k</code></td>
<td>
<p>the estimated value of k.</p>
</td></tr>
<tr><td><code>fn</code></td>
<td>
<p>vector giving the measures of variation of the eigenvectors using the bootstrapped eigenvectors for the different number of components.</p>
</td></tr>
<tr><td><code>phin</code></td>
<td>
<p>normalized eigenvalues of the M matrix in the SIR case.</p>
</td></tr>
<tr><td><code>gn</code></td>
<td>
<p>the main criterion for the ladle estimate - the sum of fn and phin. k is the value where gn takes its minimum</p>
</td></tr>
<tr><td><code>lambda</code></td>
<td>
<p>the eigenvalues of the M matrix in the SIR case.</p>
</td></tr>
<tr><td><code>W</code></td>
<td>
<p>the transformation matrix to supervised components.</p>
</td></tr>
<tr><td><code>S</code></td>
<td>
<p>data matrix with the centered supervised components.</p>
</td></tr>
<tr><td><code>MU</code></td>
<td>
<p>the location of the data which was substracted before calculating the supervised components.</p>
</td></tr>
<tr><td><code>data.name</code></td>
<td>
<p>the name of the data for which the ladle estimate was computed.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Klaus Nordhausen
</p>


<h3>References</h3>

<p><cite>Luo, W. and Li, B. (2016), Combining Eigenvalues and Variation of Eigenvectors for Order Determination, Biometrika, 103. 875&ndash;887. &lt;doi:10.1093/biomet/asw051&gt;</cite>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ladleplot">ladleplot</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n &lt;- 1000
X &lt;- cbind(rnorm(n), rnorm(n), rnorm(n), rnorm(n), rnorm(n))
eps &lt;- rnorm(n, sd=0.02)
y &lt;- 4*X[,1] + 2*X[,2] + eps


test &lt;- SIRladle(X, y)
test
summary(test)
plot(test)
pairs(cbind(y, components(test)))
ladleplot(test)
ladleplot(test, crit = "fn")
ladleplot(test, crit = "lambda")
ladleplot(test, crit = "phin")
</code></pre>

<hr>
<h2 id='summary.ladle'>
Summarizing an Object of Class ladle
</h2><span id='topic+summary.ladle'></span>

<h3>Description</h3>

<p>Summarizes an ladle object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ladle'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.ladle_+3A_object">object</code></td>
<td>
<p>an object of class ladle.</p>
</td></tr>
<tr><td><code id="summary.ladle_+3A_...">...</code></td>
<td>
<p>further arguments to be passed to or from methods.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Klaus Nordhausen
</p>


<h3>See Also</h3>

<p><code><a href="#topic+print.ladle">print.ladle</a></code>, <code><a href="#topic+plot.ladle">plot.ladle</a></code>, <code><a href="#topic+ladleplot">ladleplot</a></code>, <code><a href="#topic+FOBIladle">FOBIladle</a></code>, <code><a href="#topic+PCAladle">PCAladle</a></code>, <code><a href="#topic+SIRladle">SIRladle</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>n &lt;- 1000
X &lt;- cbind(rexp(n), rt(n,5), rnorm(n), rnorm(n), rnorm(n), rnorm(n))

test &lt;- FOBIladle(X)
summary(test)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
