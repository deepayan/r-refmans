<!DOCTYPE html><html lang="en"><head><title>Help for package ZVCV</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {ZVCV}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#ZVCV_package'><p>Zero-Variance Control Variates</p></a></li>
<li><a href='#aSECF'><p>Approximate semi-exact control functionals (aSECF)</p></a></li>
<li><a href='#aSECF_crossval'><p>Approximate semi-exact control functionals (aSECF) with cross-validation</p></a></li>
<li><a href='#CF'><p>Control functionals (CF)</p></a></li>
<li><a href='#CF_crossval'><p>Control functionals (CF) with cross-validation</p></a></li>
<li><a href='#evidence'><p>Evidence estimation with ZV-CV</p></a></li>
<li><a href='#Expand_Temperatures'><p>Adjusting the temperature schedule</p></a></li>
<li><a href='#getX'><p>ZV-CV design matrix</p></a></li>
<li><a href='#K0_fn'><p>Kernel matrix calculation</p></a></li>
<li><a href='#logsumexp'><p>Stable log sum of exponential calculations</p></a></li>
<li><a href='#medianTune'><p>Median heuristic</p></a></li>
<li><a href='#nearPD'><p>Nearest symmetric positive definite matrix</p></a></li>
<li><a href='#Phi_fn'><p>Phi matrix calculation</p></a></li>
<li><a href='#SECF'><p>Semi-exact control functionals (SECF)</p></a></li>
<li><a href='#SECF_crossval'><p>Semi-exact control functionals (SECF) with cross-validation</p></a></li>
<li><a href='#squareNorm'><p>Squared norm matrix calculation</p></a></li>
<li><a href='#VDP'><p>Example of estimation using SMC</p></a></li>
<li><a href='#zvcv'><p>ZV-CV for general expectations</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Zero-Variance Control Variates</td>
</tr>
<tr>
<td>Version:</td>
<td>2.1.2</td>
</tr>
<tr>
<td>Date:</td>
<td>2022-11-02</td>
</tr>
<tr>
<td>Description:</td>
<td>Stein control variates can be used to improve Monte Carlo estimates of expectations when the derivatives of the log target are available. This package implements a variety of such methods, including zero-variance control variates (ZV-CV, Mira et al. (2013) &lt;<a href="https://doi.org/10.1007%2Fs11222-012-9344-6">doi:10.1007/s11222-012-9344-6</a>&gt;), regularised ZV-CV (South et al., 2018 &lt;<a href="https://doi.org/10.48550/arXiv.1811.05073">doi:10.48550/arXiv.1811.05073</a>&gt;), control functionals (CF, Oates et al. (2017) &lt;<a href="https://doi.org/10.1111%2Frssb.12185">doi:10.1111/rssb.12185</a>&gt;) and semi-exact control functionals (SECF, South et al., 2020 &lt;<a href="https://doi.org/10.48550/arXiv.2002.00033">doi:10.48550/arXiv.2002.00033</a>&gt;). ZV-CV is a parametric approach that is exact for (low order) polynomial integrands with Gaussian targets. CF is a non-parametric alternative that offers better than the standard Monte Carlo convergence rates. SECF has both a parametric and a non-parametric component and it offers the advantages of both for an additional computational cost. Functions for applying ZV-CV and CF to two estimators for the normalising constant of the posterior distribution in Bayesian statistics are also supplied in this package. The basic requirements for using the package are a set of samples, derivatives and function evaluations. </td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/LeahPrice/ZVCV/issues">https://github.com/LeahPrice/ZVCV/issues</a></td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>LazyLoad:</td>
<td>yes</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp (&ge; 0.11.0), glmnet, abind, mvtnorm, stats, Rlinsolve,
magrittr, dplyr</td>
</tr>
<tr>
<td>Suggests:</td>
<td>partitions, ggplot2, ggthemes</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo, BH</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.1</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-11-02 05:22:03 UTC; southl</td>
</tr>
<tr>
<td>Author:</td>
<td>Leah F. South <a href="https://orcid.org/0000-0002-5646-2963"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Leah F. South &lt;leah.south@hdr.qut.edu.au&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-11-02 09:30:19 UTC</td>
</tr>
</table>
<hr>
<h2 id='ZVCV_package'>Zero-Variance Control Variates</h2><span id='topic+ZVCV'></span><span id='topic+ZVCV-package'></span>

<h3>Description</h3>

<p>This package can be used to perform post-hoc variance reduction of Monte Carlo estimators when the derivatives of the log target are available.
The main functionality is available through the following functions. 
All of these use a set of <code class="reqn">N</code> <code class="reqn">d</code>-dimensional samples along with the associated derivatives of the log target. 
You can evaluate posterior expectations of <code class="reqn">k</code> functions.
</p>

<ul>
<li> <p><code><a href="#topic+zvcv">zvcv</a></code>: For estimating expectations using (regularised) zero-variance control variates (ZV-CV, Mira et al, 2013; South et al, 2018).
This function can also be used to choose between various versions of ZV-CV using cross-validation.
</p>
</li>
<li> <p><code><a href="#topic+CF">CF</a></code>: For estimating expectations using control functionals (CF, Oates et al, 2017). 
</p>
</li>
<li> <p><code><a href="#topic+SECF">SECF</a></code>: For estimating expectations using semi-exact control functionals (SECF, South et al, 2020).
</p>
</li>
<li> <p><code><a href="#topic+aSECF">aSECF</a></code>: For estimating expectations using approximate semi-exact control functionals (aSECF, South et al, 2020). 
</p>
</li>
<li> <p><code><a href="#topic+CF_crossval">CF_crossval</a></code>: CF with cross-validation tuning.
</p>
</li>
<li> <p><code><a href="#topic+SECF_crossval">SECF_crossval</a></code>: SECF with cross-validation tuning.
</p>
</li>
<li> <p><code><a href="#topic+aSECF_crossval">aSECF_crossval</a></code>: aSECF with cross-validation tuning.
</p>
</li></ul>

<p>ZV-CV is exact for polynomials of order at most <code>polyorder</code> under Gaussian targets and is fast for large <code class="reqn">N</code> (although
setting a limit on <code>polyorder</code> through <code>polyorder_max</code> is recommended for large <code class="reqn">N</code>).
CF is a non-parametric approach that offers better than the standard Monte Carlo convergence rates. 
SECF has both a parametric and a non-parametric component and it offers the advantages of both for an additional computational cost. The cost of
SECF is reduced in aSECF using nystrom approximations and conjugate gradient.
</p>


<h3>Helper functions</h3>


<ul>
<li> <p><code><a href="#topic+getX">getX</a></code>: Calculates the design matrix for ZV-CV (without the column of 1's for the intercept)
</p>
</li>
<li> <p><code><a href="#topic+medianTune">medianTune</a></code>: Calculates the median heuristic for use in e.g. the Gaussian, Matern and rational quadratic kernels. Using the median heuristic is an alternative to cross-validation.
</p>
</li>
<li> <p><code><a href="#topic+K0_fn">K0_fn</a></code>: Calculates the <code class="reqn">K_0</code> matrix. The output of this function can be used as an argument to <code><a href="#topic+CF">CF</a></code>, <code><a href="#topic+CF_crossval">CF_crossval</a></code>,
<code><a href="#topic+SECF">SECF</a></code>, <code><a href="#topic+SECF_crossval">SECF_crossval</a></code>, <code><a href="#topic+aSECF">aSECF</a></code> and <code><a href="#topic+aSECF_crossval">aSECF_crossval</a></code>.
The kernel matrix is automatically computed in all of the above methods, but it is faster to calculate
in advance when using more than one of the above functions and when using any of the crossval functions.
</p>
</li>
<li> <p><code><a href="#topic+Phi_fn">Phi_fn</a></code>: Calculates the Phi matrix for SECF and aSECF (similar to <code>getX</code> but with different arguments and it includes the column of 1's)
</p>
</li>
<li> <p><code><a href="#topic+squareNorm">squareNorm</a></code>: Gets the matrix of square norms which is needed for all kernels.
Calculating this can help to save time if you are also interested in calculating the median heuristic, handling multiple tuning parameters or trying other kernels.
</p>
</li>
<li> <p><code><a href="#topic+nearPD">nearPD</a></code>: Finds the nearest symmetric positive definite matrix to the given matrix, for handling numerical issues.
</p>
</li>
<li> <p><code><a href="#topic+logsumexp">logsumexp</a></code>: Performs stable computation of the log sum of exponential (useful when handling the sum of weights)
</p>
</li></ul>



<h3>Evidence estimation</h3>

<p>The following functions are used to estimate the evidence (the normalisiing constant of the posterior) as described in South et al (2018). They are relevant when
sequential Monte Carlo with an annealing schedule has been used to collect the samples, and therefore are not of interest to those who are interested in
variance reduction based on vanilla MCMC.
</p>

<ul>
<li> <p><code><a href="#topic+evidence_CTI">evidence_CTI</a></code> and <code><a href="#topic+evidence_CTI_CF">evidence_CTI_CF</a></code>: Functions to estimate the evidence using thermodynamic integration (TI) with ZV-CV and CF, respectively
</p>
</li>
<li> <p><code><a href="#topic+evidence_SMC">evidence_SMC</a></code> and <code><a href="#topic+evidence_SMC_CF">evidence_SMC_CF</a></code>: Function to estimate the evidence using the SMC evidence identity with ZV-CV and CF, respectively.
</p>
</li></ul>

<p>The function <code><a href="#topic+Expand_Temperatures">Expand_Temperatures</a></code> can be used to adjust the temperature schedule so that it is more (or less) strict than the original schedule of <code class="reqn">T</code> temperatures.
</p>


<h3>Author(s)</h3>

<p>Leah F. South
</p>


<h3>References</h3>

<p>Mira, A., Solgi, R., &amp; Imparato, D. (2013). Zero variance Markov chain Monte Carlo for Bayesian estimators. Statistics and Computing, 23(5), 653-662.
</p>
<p>South, L. F., Karvonen, T., Nemeth, C., Girolami, M. and Oates, C. J. (2020). Semi-Exact Control Functionals From Sard's Method.  <a href="https://arxiv.org/abs/2002.00033">https://arxiv.org/abs/2002.00033</a>
</p>
<p>South, L. F., Oates, C. J., Mira, A., &amp; Drovandi, C. (2018). Regularised zero-variance control variates for high-dimensional variance reduction. <a href="https://arxiv.org/abs/1811.05073">https://arxiv.org/abs/1811.05073</a>
</p>


<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li><p> Report bugs at <a href="https://github.com/LeahPrice/ZVCV/issues">https://github.com/LeahPrice/ZVCV/issues</a>
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'># A real data example using ZV-CV is available at \link{VDP}.
# This involves estimating posterior expectations and the evidence from SMC samples.

# The remainder of this section is duplicating (albeit with a different random
# seed) Figure 2a of South et al. (2020).

N_repeats &lt;- 2 # For speed, the actual code uses 100 
N_all &lt;- 25 # For speed, the actual code uses c(10,25,50,100,250,500,1000) 
sigma_list &lt;- list(10^(-1.5),10^(-1),10^(-0.5),1,10^(0.5),10)
nfolds &lt;- 4 # For speed, the actual code uses 10
folds &lt;- 2 # For speed, the actual code uses 5
d &lt;- 4

integrand_fn &lt;- function(x){
  return (1 + x[,2] + 0.1*x[,1]*x[,2]*x[,3] + sin(x[,1])*exp(-(x[,2]*x[,3])^2))
}

results &lt;- data.frame()
for (N in N_all){

  # identify the largest polynomial order that can be fit without regularisation for auto ZV-CV
  max_r &lt;- 0
  while (choose(d + max_r + 1,d)&lt;((folds-1)/folds*N)){
  	max_r &lt;- max_r + 1
  }

  MC &lt;- ZV1 &lt;- ZV2 &lt;- ZVchoose &lt;- rep(NaN,N_repeats)
  CF &lt;- SECF1 &lt;- aSECF1 &lt;- SECF2 &lt;- aSECF2 &lt;- rep(NaN,N_repeats)
  CF_medHeur &lt;- SECF1_medHeur &lt;- aSECF1_medHeur &lt;- rep(NaN,N_repeats)
  SECF2_medHeur &lt;- aSECF2_medHeur &lt;- rep(NaN,N_repeats)
  for (i in 1:N_repeats){     
    x &lt;- matrix(rnorm(N*d),ncol=d)
    u &lt;- -x
    f &lt;- integrand_fn(x)
    
    MC[i] &lt;- mean(f)
    ZV1[i] &lt;- zvcv(f,x,u,options=list(polyorder=1,regul_reg=FALSE))$expectation
    # Checking if the sample size is large enough to accommodation a second order polynomial
    if (N &gt; choose(d+2,d)){
      ZV2[i] &lt;- zvcv(f,x,u,options=list(polyorder=2,regul_reg=FALSE))$expectation
    }
    myopts &lt;- list(list(polyorder=Inf,regul_reg=FALSE,polyorder_max=max_r),
        list(polyorder=Inf,nfolds=nfolds))
    ZVchoose[i] &lt;- zvcv(f,x,u,options=myopts,folds = folds)$expectation
    
    # Calculating the kernel matrix in advance for CF and SECF
    K0_list &lt;- list()
    for (j in 1:length(sigma_list)){
      K0_list[[j]] &lt;- K0_fn(x,u,sigma_list[[j]],steinOrder=2,kernel_function="RQ")
    }
    
    CF[i] &lt;- CF_crossval(f,x,u,K0_list=K0_list,folds = folds)$expectation
    SECF1[i] &lt;- SECF_crossval(f,x,u,K0_list=K0_list,folds = folds)$expectation
    aSECF1[i] &lt;- aSECF_crossval(f,x,u,steinOrder=2,kernel_function="RQ",
        sigma_list=sigma_list,reltol=1e-05,folds = folds)$expectation
    if (max_r&gt;=2){
      SECF2[i] &lt;- SECF_crossval(f,x,u,polyorder=2,K0_list=K0_list,folds = folds)$expectation
      aSECF2[i] &lt;- aSECF_crossval(f,x,u,polyorder=2,steinOrder=2,kernel_function="RQ",
          sigma_list=sigma_list,reltol=1e-05,folds = folds)$expectation
    }

    medHeur &lt;- medianTune(x)
    K0_medHeur &lt;- K0_fn(x,u,medHeur,steinOrder=2,kernel_function="RQ")
    CF_medHeur[i] &lt;- CF(f,x,u,K0=K0_medHeur)$expectation
    SECF1_medHeur[i] &lt;- SECF(f,x,u,K0=K0_medHeur)$expectation
    aSECF1_medHeur[i] &lt;- aSECF(f,x,u,steinOrder=2,kernel_function="RQ",
          sigma=medHeur,reltol=1e-05)$expectation
    if (max_r&gt;=2){
      SECF2_medHeur[i] &lt;- SECF(f,x,u,polyorder=2,K0=K0_medHeur)$expectation
      aSECF2_medHeur[i] &lt;- aSECF(f,x,u,polyorder=2,steinOrder=2,kernel_function="RQ",
          sigma=medHeur,reltol=1e-05)$expectation
    }
    
    # print(sprintf("--%d",i))
  }
  # Adding the results to a data frame
  MSE_crude &lt;- mean((MC - 1)^2)
  results &lt;- rbind(results,data.frame(N=N, order = "1 or NA",
      efficiency = 1, type = "MC")) 
  results &lt;- rbind(results,data.frame(N=N, order = "1 or NA",
      efficiency = MSE_crude/mean((ZV1 - 1)^2), type = "ZV")) 
  results &lt;- rbind(results,data.frame(N=N, order = "2",
      efficiency = MSE_crude/mean((ZV2 - 1)^2), type = "ZV")) 
  results &lt;- rbind(results,data.frame(N=N, order = "1 or NA",
      efficiency = MSE_crude/mean((ZVchoose - 1)^2), type = "ZVchoose")) 
  results &lt;- rbind(results,data.frame(N=N, order = "1 or NA",
      efficiency = MSE_crude/mean((CF - 1)^2), type = "CF")) 
  results &lt;- rbind(results,data.frame(N=N, order = "1 or NA",
      efficiency = MSE_crude/mean((SECF1 - 1)^2), type = "SECF")) 
  results &lt;- rbind(results,data.frame(N=N, order = "1 or NA",
      efficiency = MSE_crude/mean((aSECF1 - 1)^2), type = "aSECF")) 
  if (((folds-1)/folds*N) &gt; choose(d+2,d)){
    results &lt;- rbind(results,data.frame(N=N, order = "2",
      efficiency = MSE_crude/mean((SECF2 - 1)^2), type = "SECF")) 
    results &lt;- rbind(results,data.frame(N=N, order = "2",
      efficiency = MSE_crude/mean((aSECF2 - 1)^2), type = "aSECF")) 
  }

  results &lt;- rbind(results,data.frame(N=N, order = "1 or NA",
      efficiency = MSE_crude/mean((CF_medHeur - 1)^2), type = "CF_medHeur")) 
  results &lt;- rbind(results,data.frame(N=N, order = "1 or NA",
      efficiency = MSE_crude/mean((SECF1_medHeur - 1)^2), type = "SECF_medHeur")) 
  results &lt;- rbind(results,data.frame(N=N, order = "1 or NA",
      efficiency = MSE_crude/mean((aSECF1_medHeur - 1)^2), type = "aSECF_medHeur")) 
  if (((folds-1)/folds*N) &gt; choose(d+2,d)){
    results &lt;- rbind(results,data.frame(N=N, order = "2",
      efficiency = MSE_crude/mean((SECF2_medHeur - 1)^2), type = "SECF_medHeur")) 
    results &lt;- rbind(results,data.frame(N=N, order = "2",
      efficiency = MSE_crude/mean((aSECF2_medHeur - 1)^2), type = "aSECF_medHeur")) 
  }
  # print(N)
}


## Not run: 
# Plotting results where cross-validation is used for kernel methods
require(ggplot2)
require(ggthemes)
a &lt;- ggplot(data=subset(results,!(type %in% c("CF_medHeur","SECF_medHeur",
  "aSECF_medHeur","SECF_medHeur","aSECF_medHeur"))),
  aes(x=N,y=efficiency,col=type,linetype=order)) + scale_color_pander() + 
  ggtitle("") + geom_line(size=1.5) + scale_x_log10() + scale_y_log10() + 
  annotation_logticks(base=10) + labs(x="N",y="Efficiency",color="Method",
  linetype="Polynomial Order") + theme_minimal(base_size = 15) +
  theme(legend.key.size = unit(0.5, "cm"),legend.key.width =  unit(1, "cm")) +
  guides(linetype = guide_legend(override.aes = list(size=1),title.position = "top"),
  color = guide_legend(override.aes = list(size=1),title.position = "top"))
print(a)


# Plotting results where the median heuristic is used for kernel methods
b &lt;- ggplot(data=subset(results,!(type %in% c("CF","SECF","aSECF","SECF","aSECF"))),
            aes(x=N,y=efficiency,col=type,linetype=order)) + scale_color_pander() + 
  ggtitle("") + geom_line(size=1.5) + scale_x_log10() + scale_y_log10() + 
  annotation_logticks(base=10) + labs(x="N",y="Efficiency",color="Method",
  linetype="Polynomial Order") + theme_minimal(base_size = 15) +
  theme(legend.key.size = unit(0.5, "cm"),legend.key.width =  unit(1, "cm")) +
  guides(linetype = guide_legend(override.aes = list(size=1),title.position = "top"),
  color = guide_legend(override.aes = list(size=1),title.position = "top"))
print(b)

## End(Not run)

</code></pre>

<hr>
<h2 id='aSECF'>Approximate semi-exact control functionals (aSECF)</h2><span id='topic+aSECF'></span>

<h3>Description</h3>

<p>This function performs approximate semi-exact control functionals as described in South et al (2020). It uses a nystrom approximation and conjugate gradient to speed up SECF.
This is faster than <code><a href="#topic+SECF">SECF</a></code> for large <code class="reqn">N</code>. If you would like to choose
between different kernels using cross-validation, then you can use <code><a href="#topic+aSECF_crossval">aSECF_crossval</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>aSECF(
  integrands,
  samples,
  derivatives,
  polyorder = NULL,
  steinOrder = NULL,
  kernel_function = NULL,
  sigma = NULL,
  K0 = NULL,
  nystrom_inds = NULL,
  est_inds = NULL,
  apriori = NULL,
  conjugate_gradient = TRUE,
  reltol = 0.01,
  diagnostics = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="aSECF_+3A_integrands">integrands</code></td>
<td>
<p>An <code class="reqn">N</code> by <code class="reqn">k</code> matrix of integrands (evaluations of the function of interest)</p>
</td></tr>
<tr><td><code id="aSECF_+3A_samples">samples</code></td>
<td>
<p>An <code class="reqn">N</code> by <code class="reqn">d</code> matrix of samples from the target</p>
</td></tr>
<tr><td><code id="aSECF_+3A_derivatives">derivatives</code></td>
<td>
<p>An <code class="reqn">N</code> by <code class="reqn">d</code> matrix of derivatives of the log target with respect to the parameters</p>
</td></tr>
<tr><td><code id="aSECF_+3A_polyorder">polyorder</code></td>
<td>
<p>(optional)        The order of the polynomial to be used in the parametric component, with a default of <code class="reqn">1</code>. We recommend keeping this value low (e.g. only 1-2).</p>
</td></tr>
<tr><td><code id="aSECF_+3A_steinorder">steinOrder</code></td>
<td>
<p>(optional)    This is the order of the Stein operator. The default is <code>1</code> in the control functionals paper (Oates et al, 2017) and <code>2</code> in the semi-exact control functionals paper (South et al, 2020).  The following values are currently available: <code>1</code> for all kernels and <code>2</code> for &quot;gaussian&quot;, &quot;matern&quot; and &quot;RQ&quot;. See below for further details.</p>
</td></tr>
<tr><td><code id="aSECF_+3A_kernel_function">kernel_function</code></td>
<td>
<p>(optional)        Choose between &quot;gaussian&quot;, &quot;matern&quot;, &quot;RQ&quot;, &quot;product&quot; or &quot;prodsim&quot;. See below for further details.</p>
</td></tr>
<tr><td><code id="aSECF_+3A_sigma">sigma</code></td>
<td>
<p>(optional)            The tuning parameters of the specified kernel. This involves a single length-scale parameter in &quot;gaussian&quot; and &quot;RQ&quot;, a length-scale and a smoothness parameter in &quot;matern&quot; and two parameters in &quot;product&quot; and &quot;prodsim&quot;. See below for further details.</p>
</td></tr>
<tr><td><code id="aSECF_+3A_k0">K0</code></td>
<td>
<p>(optional) The kernel matrix. One can specify either this or all of <code>sigma</code>, <code>steinOrder</code> and <code>kernel_function</code>. The former involves pre-computing the kernel matrix using <code><a href="#topic+K0_fn">K0_fn</a></code> and is more efficient when using multiple estimators out of <code><a href="#topic+CF">CF</a></code>, <code><a href="#topic+SECF">SECF</a></code> and  <code><a href="#topic+aSECF">aSECF</a></code> or when using the cross-validation functions.</p>
</td></tr>
<tr><td><code id="aSECF_+3A_nystrom_inds">nystrom_inds</code></td>
<td>
<p>(optional) The sample indices to be used in the Nystrom approximation.</p>
</td></tr>
<tr><td><code id="aSECF_+3A_est_inds">est_inds</code></td>
<td>
<p>(optional) A vector of indices for the estimation-only samples. The default when <code>est_inds</code> is missing or <code>NULL</code> is to perform both estimation of the control variates and evaluation of the integral using all samples. Otherwise, the samples from <code>est_inds</code> are used in estimating the control variates and the remainder are used in evaluating the integral. Splitting the indices in this way can be used to reduce bias from adaption and to make computation feasible for very large sample sizes (small <code>est_inds</code> is faster), but in general in will increase the variance of the estimator.</p>
</td></tr>
<tr><td><code id="aSECF_+3A_apriori">apriori</code></td>
<td>
<p>(optional) A vector containing the subset of parameter indices to use in the polynomial. Typically this argument would only be used if the dimension of the problem is very large or if prior information about parameter dependencies is known. The default is to use all parameters <code class="reqn">1:d</code> where <code class="reqn">d</code> is the dimension of the target.</p>
</td></tr>
<tr><td><code id="aSECF_+3A_conjugate_gradient">conjugate_gradient</code></td>
<td>
<p>(optional) A flag for whether to perform conjugate gradient to further speed up the nystrom approximation (the default is true).</p>
</td></tr>
<tr><td><code id="aSECF_+3A_reltol">reltol</code></td>
<td>
<p>(optional) The relative tolerance for choosing when the stop conjugate gradient iterations (the default is 1e-02).
using <code><a href="#topic+squareNorm">squareNorm</a></code>, as long as the <code>nystrom_inds</code> are <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="aSECF_+3A_diagnostics">diagnostics</code></td>
<td>
<p>(optional) A flag for whether to return the necessary outputs for plotting or estimating using the fitted model. The default is <code>false</code> since this requires some additional computation when <code>est_inds</code> is <code>NULL</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with the following elements:
</p>

<ul>
<li> <p><code>expectation</code>: The estimate(s) of the (<code class="reqn">k</code>) expectations(s).
</p>
</li>
<li> <p><code>cond_no</code>: (Only if <code>conjugate_gradient</code> = <code>TRUE</code>) The condition number of the matrix being solved using conjugate gradient.
</p>
</li>
<li> <p><code>iter</code>: (Only if <code>conjugate_gradient</code> = <code>TRUE</code>) The number of conjugate gradient iterations
</p>
</li>
<li> <p><code>f_true</code>: (Only if <code>est_inds</code> is not <code>NULL</code>) The integrands for the evaluation set. This should be the same as integrands[setdiff(1:N,est_inds),].
</p>
</li>
<li> <p><code>f_hat</code>: (Only if <code>est_inds</code> is not <code>NULL</code>) The fitted values for the integrands in the evaluation set. This can be used to help assess the performance of the Gaussian process model.
</p>
</li>
<li> <p><code>a</code>: (Only if <code>diagnostics</code> = <code>TRUE</code>) The value of <code class="reqn">a</code> as described in South et al (2020), where predictions are of the form <code class="reqn">f_hat = K0*a + Phi*b</code> for heldout K0 and Phi matrices and estimators using heldout samples are of the form <code class="reqn">mean(f - f_hat) + b[1]</code>.
</p>
</li>
<li> <p><code>b</code>: (Only if <code>diagnostics</code> = <code>TRUE</code>) The value of <code class="reqn">b</code> as described in South et al (2020), where predictions are of the form <code class="reqn">f_hat = K0*a + Phi*b</code> for heldout K0 and Phi matrices and estimators using heldout samples are of the form <code class="reqn">mean(f - f_hat) + b[1]</code>.
</p>
</li>
<li> <p><code>ny_inds</code>: (Only if <code>diagnostics</code> = <code>TRUE</code>) The indices of the samples used in the nystrom approximation (this will match nystrom_inds if this argument was not <code>NULL</code>).
</p>
</li></ul>



<h3>On the choice of <code class="reqn">\sigma</code>, the kernel and the Stein order</h3>

<p>The kernel in Stein-based kernel methods is <code class="reqn">L_x L_y k(x,y)</code> where <code class="reqn">L_x</code> is a first or second order Stein operator in <code class="reqn">x</code> and <code class="reqn">k(x,y)</code> is some generic kernel to be specified.
</p>
<p>The Stein operators for distribution <code class="reqn">p(x)</code> are defined as:
</p>

<ul>
<li> <p><strong><code>steinOrder=1</code></strong>: <code class="reqn">L_x g(x) = \nabla_x^T g(x) + \nabla_x \log p(x)^T g(x)</code> (see e.g. Oates el al (2017))
</p>
</li>
<li> <p><strong><code>steinOrder=2</code></strong>: <code class="reqn">L_x g(x) = \Delta_x g(x) + \nabla_x log p(x)^T \nabla_x g(x)</code> (see e.g. South el al (2020))
</p>
</li></ul>

<p>Here <code class="reqn">\nabla_x</code> is the first order derivative wrt <code class="reqn">x</code> and <code class="reqn">\Delta_x = \nabla_x^T \nabla_x</code> is the Laplacian operator.
</p>
<p>The generic kernels which are implemented in this package are listed below.  Note that the input parameter <strong><code>sigma</code></strong> defines the kernel parameters <code class="reqn">\sigma</code>. 
</p>

<ul>
<li> <p><strong><code>"gaussian"</code></strong>: A Gaussian kernel,
</p>
<p style="text-align: center;"><code class="reqn">k(x,y) = exp(-z(x,y)/\sigma^2)</code>
</p>

</li>
<li> <p><strong><code>"matern"</code></strong>: A Matern kernel with <code class="reqn">\sigma = (\lambda,\nu)</code>,
</p>
<p style="text-align: center;"><code class="reqn">k(x,y) = bc^{\nu}z(x,y)^{\nu/2}K_{\nu}(c z(x,y)^{0.5})</code>
</p>
<p> where <code class="reqn">b=2^{1-\nu}(\Gamma(\nu))^{-1}</code>, <code class="reqn">c=(2\nu)^{0.5}\lambda^{-1}</code> and <code class="reqn">K_{\nu}(x)</code> is the modified Bessel function of the second kind. Note that <code class="reqn">\lambda</code> is the length-scale parameter and <code class="reqn">\nu</code> is the smoothness parameter (which defaults to 2.5 for <code class="reqn">steinOrder=1</code> and 4.5 for <code class="reqn">steinOrder=2</code>).
</p>
</li>
<li> <p><strong><code>"RQ"</code></strong>: A rational quadratic kernel,
</p>
<p style="text-align: center;"><code class="reqn">k(x,y) = (1+\sigma^{-2}z(x,y))^{-1}</code>
</p>

</li>
<li> <p><strong><code>"product"</code></strong>: The product kernel that appears in Oates et al (2017) with <code class="reqn">\sigma = (a,b)</code>
</p>
<p style="text-align: center;"><code class="reqn">k(x,y) = (1+a z(x) + a z(y))^{-1} exp(-0.5 b^{-2} z(x,y)) </code>
</p>

</li>
<li> <p><strong><code>"prodsim"</code></strong>: A slightly different product kernel with <code class="reqn">\sigma = (a,b)</code> (see e.g. <a href="https://www.imperial.ac.uk/inference-group/projects/monte-carlo-methods/control-functionals/">https://www.imperial.ac.uk/inference-group/projects/monte-carlo-methods/control-functionals/</a>),
</p>
<p style="text-align: center;"><code class="reqn">k(x,y) = (1+a z(x))^{-1}(1 + a z(y))^{-1} exp(-0.5 b^{-2} z(x,y)) </code>
</p>

</li></ul>

<p>In the above equations, <code class="reqn">z(x) = \sum_j x[j]^2</code> and <code class="reqn">z(x,y) = \sum_j (x[j] - y[j])^2</code>. For the last two kernels, the code only has implementations for <code>steinOrder</code>=<code>1</code>. Each combination of <code>steinOrder</code> and <code>kernel_function</code> above is currently hard-coded but it may be possible to extend this to other kernels in future versions using autodiff. The calculations for the first three kernels above are detailed in South et al (2020).
</p>


<h3>Author(s)</h3>

<p>Leah F. South
</p>


<h3>References</h3>

<p>South, L. F., Karvonen, T., Nemeth, C., Girolami, M. and Oates, C. J. (2020). Semi-Exact Control Functionals From Sard's Method.  <a href="https://arxiv.org/abs/2002.00033">https://arxiv.org/abs/2002.00033</a>
</p>


<h3>See Also</h3>

<p>See <a href="#topic+ZVCV">ZVCV</a> for examples and related functions. See <code><a href="#topic+aSECF_crossval">aSECF_crossval</a></code> for a function to choose between different kernels for this estimator.
</p>

<hr>
<h2 id='aSECF_crossval'>Approximate semi-exact control functionals (aSECF) with cross-validation</h2><span id='topic+aSECF_crossval'></span>

<h3>Description</h3>

<p>This function chooses between a list of kernel tuning parameters (<code>sigma_list</code>) or a list of K0 matrices (<code>K0_list</code>) for
the approximate semi-exact control functionals method described in South et al (2020). The latter requires
calculating and storing kernel matrices using <code><a href="#topic+K0_fn">K0_fn</a></code> but it is more flexible
because it can be used to choose the Stein operator order and the kernel function, in addition
to its parameters. It is also faster to pre-specify <code><a href="#topic+K0_fn">K0_fn</a></code>.
For estimation with fixed kernel parameters, use <code><a href="#topic+aSECF">aSECF</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>aSECF_crossval(
  integrands,
  samples,
  derivatives,
  polyorder = NULL,
  steinOrder = NULL,
  kernel_function = NULL,
  sigma_list = NULL,
  est_inds = NULL,
  apriori = NULL,
  num_nystrom = NULL,
  conjugate_gradient = TRUE,
  reltol = 0.01,
  folds = NULL,
  diagnostics = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="aSECF_crossval_+3A_integrands">integrands</code></td>
<td>
<p>An <code class="reqn">N</code> by <code class="reqn">k</code> matrix of integrands (evaluations of the function of interest)</p>
</td></tr>
<tr><td><code id="aSECF_crossval_+3A_samples">samples</code></td>
<td>
<p>An <code class="reqn">N</code> by <code class="reqn">d</code> matrix of samples from the target</p>
</td></tr>
<tr><td><code id="aSECF_crossval_+3A_derivatives">derivatives</code></td>
<td>
<p>An <code class="reqn">N</code> by <code class="reqn">d</code> matrix of derivatives of the log target with respect to the parameters</p>
</td></tr>
<tr><td><code id="aSECF_crossval_+3A_polyorder">polyorder</code></td>
<td>
<p>(optional)        The order of the polynomial to be used in the parametric component, with a default of <code class="reqn">1</code>. We recommend keeping this value low (e.g. only 1-2).</p>
</td></tr>
<tr><td><code id="aSECF_crossval_+3A_steinorder">steinOrder</code></td>
<td>
<p>(optional)    This is the order of the Stein operator. The default is <code>1</code> in the control functionals paper (Oates et al, 2017) and <code>2</code> in the semi-exact control functionals paper (South et al, 2020).  The following values are currently available: <code>1</code> for all kernels and <code>2</code> for &quot;gaussian&quot;, &quot;matern&quot; and &quot;RQ&quot;. See below for further details.</p>
</td></tr>
<tr><td><code id="aSECF_crossval_+3A_kernel_function">kernel_function</code></td>
<td>
<p>(optional)        Choose between &quot;gaussian&quot;, &quot;matern&quot;, &quot;RQ&quot;, &quot;product&quot; or &quot;prodsim&quot;. See below for further details.</p>
</td></tr>
<tr><td><code id="aSECF_crossval_+3A_sigma_list">sigma_list</code></td>
<td>
<p>(optional between this and <code>K0_list</code>)            A list of tuning parameters for the specified kernel. This involves a list of single length-scale parameter in &quot;gaussian&quot; and &quot;RQ&quot;, a list of vectors containing length-scale and smoothness parameters in &quot;matern&quot; and a list of vectors of the two parameters in &quot;product&quot; and &quot;prodsim&quot;. See below for further details. When <code>sigma_list</code> is specified and not <code>K0_list</code>, the <code class="reqn">K0</code> matrix is computed twice for each selected tuning parameter.</p>
</td></tr>
<tr><td><code id="aSECF_crossval_+3A_est_inds">est_inds</code></td>
<td>
<p>(optional) A vector of indices for the estimation-only samples. The default when <code>est_inds</code> is missing or <code>NULL</code> is to perform both estimation of the control variates and evaluation of the integral using all samples. Otherwise, the samples from <code>est_inds</code> are used in estimating the control variates and the remainder are used in evaluating the integral. Splitting the indices in this way can be used to reduce bias from adaption and to make computation feasible for very large sample sizes (small <code>est_inds</code> is faster), but in general in will increase the variance of the estimator.</p>
</td></tr>
<tr><td><code id="aSECF_crossval_+3A_apriori">apriori</code></td>
<td>
<p>(optional) A vector containing the subset of parameter indices to use in the polynomial. Typically this argument would only be used if the dimension of the problem is very large or if prior information about parameter dependencies is known. The default is to use all parameters <code class="reqn">1:d</code> where <code class="reqn">d</code> is the dimension of the target.</p>
</td></tr>
<tr><td><code id="aSECF_crossval_+3A_num_nystrom">num_nystrom</code></td>
<td>
<p>(optional) The number of samples to use in the Nystrom approximation, with a default of ceiling(sqrt(N)). The nystrom indices cannot be passed in here because of the way the cross-validation has been set up.</p>
</td></tr>
<tr><td><code id="aSECF_crossval_+3A_conjugate_gradient">conjugate_gradient</code></td>
<td>
<p>(optional) A flag for whether to perform conjugate gradient to further speed up the nystrom approximation (the default is true).</p>
</td></tr>
<tr><td><code id="aSECF_crossval_+3A_reltol">reltol</code></td>
<td>
<p>(optional) The relative tolerance for choosing when the stop conjugate gradient iterations (the default is 1e-02).
using <code><a href="#topic+squareNorm">squareNorm</a></code>, as long as the <code>nystrom_inds</code> are <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="aSECF_crossval_+3A_folds">folds</code></td>
<td>
<p>(optional) The number of folds for cross-validation. The default is five.</p>
</td></tr>
<tr><td><code id="aSECF_crossval_+3A_diagnostics">diagnostics</code></td>
<td>
<p>(optional) A flag for whether to return the necessary outputs for plotting or estimating using the fitted model. The default is <code>false</code> since this requires some additional computation when <code>est_inds</code> is <code>NULL</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with the following elements:
</p>

<ul>
<li> <p><code>expectation</code>: The estimate(s) of the (<code class="reqn">k</code>) expectations(s).
</p>
</li>
<li> <p><code>mse</code>: A matrix of the cross-validation mean square prediction errors. The number of columns is the number of tuning options given and the number of rows is <code class="reqn">k</code>, the number of integrands of interest.
</p>
</li>
<li> <p><code>optinds</code>: The optimal indices from the list for each expectation.
</p>
</li>
<li> <p><code>cond_no</code>: (Only if <code>conjugate_gradient</code> = <code>TRUE</code>) The condition number of the matrix being solved using conjugate gradient.
</p>
</li>
<li> <p><code>iter</code>: (Only if <code>conjugate_gradient</code> = <code>TRUE</code>) The number of conjugate gradient iterations
</p>
</li>
<li> <p><code>f_true</code>: (Only if <code>est_inds</code> is not <code>NULL</code>) The integrands for the evaluation set. This should be the same as integrands[setdiff(1:N,est_inds),].
</p>
</li>
<li> <p><code>f_hat</code>: (Only if <code>est_inds</code> is not <code>NULL</code>) The fitted values for the integrands in the evaluation set. This can be used to help assess the performance of the Gaussian process model.
</p>
</li>
<li> <p><code>a</code>: (Only if <code>diagnostics</code> = <code>TRUE</code>) The value of <code class="reqn">a</code> as described in South et al (2020), where predictions are of the form <code class="reqn">f_hat = K0*a + Phi*b</code> for heldout K0 and Phi matrices and estimators using heldout samples are of the form <code class="reqn">mean(f - f_hat) + b[1]</code>.
</p>
</li>
<li> <p><code>b</code>: (Only if <code>diagnostics</code> = <code>TRUE</code>) The value of <code class="reqn">b</code> as described in South et al (2020), where predictions are of the form <code class="reqn">f_hat = K0*a + Phi*b</code> for heldout K0 and Phi matrices and estimators using heldout samples are of the form <code class="reqn">mean(f - f_hat) + b[1]</code>.
</p>
</li>
<li> <p><code>ny_inds</code>: (Only if <code>diagnostics</code> = <code>TRUE</code>) The indices of the samples used in the nystrom approximation (this will match nystrom_inds if this argument was not <code>NULL</code>).
</p>
</li></ul>



<h3>On the choice of <code class="reqn">\sigma</code>, the kernel and the Stein order</h3>

<p>The kernel in Stein-based kernel methods is <code class="reqn">L_x L_y k(x,y)</code> where <code class="reqn">L_x</code> is a first or second order Stein operator in <code class="reqn">x</code> and <code class="reqn">k(x,y)</code> is some generic kernel to be specified.
</p>
<p>The Stein operators for distribution <code class="reqn">p(x)</code> are defined as:
</p>

<ul>
<li> <p><strong><code>steinOrder=1</code></strong>: <code class="reqn">L_x g(x) = \nabla_x^T g(x) + \nabla_x \log p(x)^T g(x)</code> (see e.g. Oates el al (2017))
</p>
</li>
<li> <p><strong><code>steinOrder=2</code></strong>: <code class="reqn">L_x g(x) = \Delta_x g(x) + \nabla_x log p(x)^T \nabla_x g(x)</code> (see e.g. South el al (2020))
</p>
</li></ul>

<p>Here <code class="reqn">\nabla_x</code> is the first order derivative wrt <code class="reqn">x</code> and <code class="reqn">\Delta_x = \nabla_x^T \nabla_x</code> is the Laplacian operator.
</p>
<p>The generic kernels which are implemented in this package are listed below.  Note that the input parameter <strong><code>sigma</code></strong> defines the kernel parameters <code class="reqn">\sigma</code>. 
</p>

<ul>
<li> <p><strong><code>"gaussian"</code></strong>: A Gaussian kernel,
</p>
<p style="text-align: center;"><code class="reqn">k(x,y) = exp(-z(x,y)/\sigma^2)</code>
</p>

</li>
<li> <p><strong><code>"matern"</code></strong>: A Matern kernel with <code class="reqn">\sigma = (\lambda,\nu)</code>,
</p>
<p style="text-align: center;"><code class="reqn">k(x,y) = bc^{\nu}z(x,y)^{\nu/2}K_{\nu}(c z(x,y)^{0.5})</code>
</p>
<p> where <code class="reqn">b=2^{1-\nu}(\Gamma(\nu))^{-1}</code>, <code class="reqn">c=(2\nu)^{0.5}\lambda^{-1}</code> and <code class="reqn">K_{\nu}(x)</code> is the modified Bessel function of the second kind. Note that <code class="reqn">\lambda</code> is the length-scale parameter and <code class="reqn">\nu</code> is the smoothness parameter (which defaults to 2.5 for <code class="reqn">steinOrder=1</code> and 4.5 for <code class="reqn">steinOrder=2</code>).
</p>
</li>
<li> <p><strong><code>"RQ"</code></strong>: A rational quadratic kernel,
</p>
<p style="text-align: center;"><code class="reqn">k(x,y) = (1+\sigma^{-2}z(x,y))^{-1}</code>
</p>

</li>
<li> <p><strong><code>"product"</code></strong>: The product kernel that appears in Oates et al (2017) with <code class="reqn">\sigma = (a,b)</code>
</p>
<p style="text-align: center;"><code class="reqn">k(x,y) = (1+a z(x) + a z(y))^{-1} exp(-0.5 b^{-2} z(x,y)) </code>
</p>

</li>
<li> <p><strong><code>"prodsim"</code></strong>: A slightly different product kernel with <code class="reqn">\sigma = (a,b)</code> (see e.g. <a href="https://www.imperial.ac.uk/inference-group/projects/monte-carlo-methods/control-functionals/">https://www.imperial.ac.uk/inference-group/projects/monte-carlo-methods/control-functionals/</a>),
</p>
<p style="text-align: center;"><code class="reqn">k(x,y) = (1+a z(x))^{-1}(1 + a z(y))^{-1} exp(-0.5 b^{-2} z(x,y)) </code>
</p>

</li></ul>

<p>In the above equations, <code class="reqn">z(x) = \sum_j x[j]^2</code> and <code class="reqn">z(x,y) = \sum_j (x[j] - y[j])^2</code>. For the last two kernels, the code only has implementations for <code>steinOrder</code>=<code>1</code>. Each combination of <code>steinOrder</code> and <code>kernel_function</code> above is currently hard-coded but it may be possible to extend this to other kernels in future versions using autodiff. The calculations for the first three kernels above are detailed in South et al (2020).
</p>


<h3>Author(s)</h3>

<p>Leah F. South
</p>


<h3>References</h3>

<p>South, L. F., Karvonen, T., Nemeth, C., Girolami, M. and Oates, C. J. (2020). Semi-Exact Control Functionals From Sard's Method.  <a href="https://arxiv.org/abs/2002.00033">https://arxiv.org/abs/2002.00033</a>
</p>


<h3>See Also</h3>

<p>See <a href="#topic+ZVCV">ZVCV</a> for examples and related functions. See <code><a href="#topic+aSECF_crossval">aSECF_crossval</a></code> for a function to choose between different kernels for this estimator.
</p>

<hr>
<h2 id='CF'>Control functionals (CF)</h2><span id='topic+CF'></span>

<h3>Description</h3>

<p>This function performs control functionals as described in Oates et al (2017).
To choose between different kernels using cross-validation, use <code><a href="#topic+CF_crossval">CF_crossval</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CF(
  integrands,
  samples,
  derivatives,
  steinOrder = NULL,
  kernel_function = NULL,
  sigma = NULL,
  K0 = NULL,
  est_inds = NULL,
  one_in_denom = FALSE,
  diagnostics = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="CF_+3A_integrands">integrands</code></td>
<td>
<p>An <code class="reqn">N</code> by <code class="reqn">k</code> matrix of integrands (evaluations of the function of interest)</p>
</td></tr>
<tr><td><code id="CF_+3A_samples">samples</code></td>
<td>
<p>An <code class="reqn">N</code> by <code class="reqn">d</code> matrix of samples from the target</p>
</td></tr>
<tr><td><code id="CF_+3A_derivatives">derivatives</code></td>
<td>
<p>An <code class="reqn">N</code> by <code class="reqn">d</code> matrix of derivatives of the log target with respect to the parameters</p>
</td></tr>
<tr><td><code id="CF_+3A_steinorder">steinOrder</code></td>
<td>
<p>(optional)    This is the order of the Stein operator. The default is <code>1</code> in the control functionals paper (Oates et al, 2017) and <code>2</code> in the semi-exact control functionals paper (South et al, 2020).  The following values are currently available: <code>1</code> for all kernels and <code>2</code> for &quot;gaussian&quot;, &quot;matern&quot; and &quot;RQ&quot;. See below for further details.</p>
</td></tr>
<tr><td><code id="CF_+3A_kernel_function">kernel_function</code></td>
<td>
<p>(optional)        Choose between &quot;gaussian&quot;, &quot;matern&quot;, &quot;RQ&quot;, &quot;product&quot; or &quot;prodsim&quot;. See below for further details.</p>
</td></tr>
<tr><td><code id="CF_+3A_sigma">sigma</code></td>
<td>
<p>(optional)            The tuning parameters of the specified kernel. This involves a single length-scale parameter in &quot;gaussian&quot; and &quot;RQ&quot;, a length-scale and a smoothness parameter in &quot;matern&quot; and two parameters in &quot;product&quot; and &quot;prodsim&quot;. See below for further details.</p>
</td></tr>
<tr><td><code id="CF_+3A_k0">K0</code></td>
<td>
<p>(optional) The kernel matrix. One can specify either this or all of <code>sigma</code>, <code>steinOrder</code> and <code>kernel_function</code>. The former involves pre-computing the kernel matrix using <code><a href="#topic+K0_fn">K0_fn</a></code> and is more efficient when using multiple estimators out of <code><a href="#topic+CF">CF</a></code>, <code><a href="#topic+SECF">SECF</a></code> and  <code><a href="#topic+aSECF">aSECF</a></code> or when using the cross-validation functions.</p>
</td></tr>
<tr><td><code id="CF_+3A_est_inds">est_inds</code></td>
<td>
<p>(optional) A vector of indices for the estimation-only samples. The default when <code>est_inds</code> is missing or <code>NULL</code> is to perform both estimation of the control variates and evaluation of the integral using all samples. Otherwise, the samples from <code>est_inds</code> are used in estimating the control variates and the remainder are used in evaluating the integral. Splitting the indices in this way can be used to reduce bias from adaption and to make computation feasible for very large sample sizes (small <code>est_inds</code> is faster), but in general in will increase the variance of the estimator.</p>
</td></tr>
<tr><td><code id="CF_+3A_one_in_denom">one_in_denom</code></td>
<td>
<p>(optional) Whether or not to include a <code class="reqn">1 + </code> in the denominator of the control functionals estimator, as in equation 2 on p703 of Oates et al (2017). The <code class="reqn">1 +</code> in the denominator is an arbitrary choice so we set it to zero by default.</p>
</td></tr>
<tr><td><code id="CF_+3A_diagnostics">diagnostics</code></td>
<td>
<p>(optional) A flag for whether to return the necessary outputs for plotting or estimating using the fitted model. The default is <code>false</code> since this requires some additional computation when <code>est_inds</code> is <code>NULL</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with the following elements:
</p>

<ul>
<li> <p><code>expectation</code>: The estimate(s) of the (<code class="reqn">k</code>) expectation(s).
</p>
</li>
<li> <p><code>f_true</code>: (Only if <code>est_inds</code> is not <code>NULL</code>) The integrands for the evaluation set. This should be the same as integrands[setdiff(1:N,est_inds),].
</p>
</li>
<li> <p><code>f_hat</code>: (Only if <code>est_inds</code> is not <code>NULL</code>) The fitted values for the integrands in the evaluation set. This can be used to help assess the performance of the Gaussian process model.
</p>
</li>
<li> <p><code>a</code>: (Only if <code>diagnostics</code> = <code>TRUE</code>) The value of <code class="reqn">a</code> as described in South et al (2020), where predictions are of the form <code class="reqn">f_hat = K0*a + 1*b</code> for heldout K0 and estimators using heldout samples are of the form <code class="reqn">mean(f - f_hat) + b</code>.
</p>
</li>
<li> <p><code>b</code>: (Only if <code>diagnostics</code> = <code>TRUE</code>) The value of <code class="reqn">b</code> as described in South et al (2020), where predictions are of the form <code class="reqn">f_hat = K0*a + 1*b</code> for heldout K0 and estimators using heldout samples are of the form <code class="reqn">mean(f - f_hat) + b</code>.
</p>
</li>
<li> <p><code>ksd</code>: (Only if <code>diagnostics</code> = <code>TRUE</code>) An estimated kernel Stein discrepancy based on the fitted model that can be used for diagnostic purposes. See South et al (2020) for further details.
</p>
</li>
<li> <p><code>bound_const</code>: (Only if <code>diagnostics</code> = <code>TRUE</code> and <code>est_inds</code>=<code>NULL</code>) This is such that the absolute error for the estimator should be less than <code class="reqn">ksd \times bound_const</code>.
</p>
</li></ul>



<h3>Warning</h3>

<p>Solving the linear system in CF has <code class="reqn">O(N^3)</code> complexity and is therefore not suited to large <code class="reqn">N</code>. Using <code class="reqn">est_inds</code> will instead have an <code class="reqn">O(N_0^3)</code> cost in solving the linear system and an <code class="reqn">O((N-N_0)^2)</code> cost in handling the remaining samples, where <code class="reqn">N_0</code> is the length of <code class="reqn">est_inds</code>. This can be much cheaper for large <code class="reqn">N</code>.
</p>


<h3>On the choice of <code class="reqn">\sigma</code>, the kernel and the Stein order</h3>

<p>The kernel in Stein-based kernel methods is <code class="reqn">L_x L_y k(x,y)</code> where <code class="reqn">L_x</code> is a first or second order Stein operator in <code class="reqn">x</code> and <code class="reqn">k(x,y)</code> is some generic kernel to be specified.
</p>
<p>The Stein operators for distribution <code class="reqn">p(x)</code> are defined as:
</p>

<ul>
<li> <p><strong><code>steinOrder=1</code></strong>: <code class="reqn">L_x g(x) = \nabla_x^T g(x) + \nabla_x \log p(x)^T g(x)</code> (see e.g. Oates el al (2017))
</p>
</li>
<li> <p><strong><code>steinOrder=2</code></strong>: <code class="reqn">L_x g(x) = \Delta_x g(x) + \nabla_x log p(x)^T \nabla_x g(x)</code> (see e.g. South el al (2020))
</p>
</li></ul>

<p>Here <code class="reqn">\nabla_x</code> is the first order derivative wrt <code class="reqn">x</code> and <code class="reqn">\Delta_x = \nabla_x^T \nabla_x</code> is the Laplacian operator.
</p>
<p>The generic kernels which are implemented in this package are listed below.  Note that the input parameter <strong><code>sigma</code></strong> defines the kernel parameters <code class="reqn">\sigma</code>. 
</p>

<ul>
<li> <p><strong><code>"gaussian"</code></strong>: A Gaussian kernel,
</p>
<p style="text-align: center;"><code class="reqn">k(x,y) = exp(-z(x,y)/\sigma^2)</code>
</p>

</li>
<li> <p><strong><code>"matern"</code></strong>: A Matern kernel with <code class="reqn">\sigma = (\lambda,\nu)</code>,
</p>
<p style="text-align: center;"><code class="reqn">k(x,y) = bc^{\nu}z(x,y)^{\nu/2}K_{\nu}(c z(x,y)^{0.5})</code>
</p>
<p> where <code class="reqn">b=2^{1-\nu}(\Gamma(\nu))^{-1}</code>, <code class="reqn">c=(2\nu)^{0.5}\lambda^{-1}</code> and <code class="reqn">K_{\nu}(x)</code> is the modified Bessel function of the second kind. Note that <code class="reqn">\lambda</code> is the length-scale parameter and <code class="reqn">\nu</code> is the smoothness parameter (which defaults to 2.5 for <code class="reqn">steinOrder=1</code> and 4.5 for <code class="reqn">steinOrder=2</code>).
</p>
</li>
<li> <p><strong><code>"RQ"</code></strong>: A rational quadratic kernel,
</p>
<p style="text-align: center;"><code class="reqn">k(x,y) = (1+\sigma^{-2}z(x,y))^{-1}</code>
</p>

</li>
<li> <p><strong><code>"product"</code></strong>: The product kernel that appears in Oates et al (2017) with <code class="reqn">\sigma = (a,b)</code>
</p>
<p style="text-align: center;"><code class="reqn">k(x,y) = (1+a z(x) + a z(y))^{-1} exp(-0.5 b^{-2} z(x,y)) </code>
</p>

</li>
<li> <p><strong><code>"prodsim"</code></strong>: A slightly different product kernel with <code class="reqn">\sigma = (a,b)</code> (see e.g. <a href="https://www.imperial.ac.uk/inference-group/projects/monte-carlo-methods/control-functionals/">https://www.imperial.ac.uk/inference-group/projects/monte-carlo-methods/control-functionals/</a>),
</p>
<p style="text-align: center;"><code class="reqn">k(x,y) = (1+a z(x))^{-1}(1 + a z(y))^{-1} exp(-0.5 b^{-2} z(x,y)) </code>
</p>

</li></ul>

<p>In the above equations, <code class="reqn">z(x) = \sum_j x[j]^2</code> and <code class="reqn">z(x,y) = \sum_j (x[j] - y[j])^2</code>. For the last two kernels, the code only has implementations for <code>steinOrder</code>=<code>1</code>. Each combination of <code>steinOrder</code> and <code>kernel_function</code> above is currently hard-coded but it may be possible to extend this to other kernels in future versions using autodiff. The calculations for the first three kernels above are detailed in South et al (2020).
</p>


<h3>Author(s)</h3>

<p>Leah F. South
</p>


<h3>References</h3>

<p>Oates, C. J., Girolami, M. &amp; Chopin, N. (2017). Control functionals for Monte Carlo integration. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 79(3), 695-718.
</p>
<p>South, L. F., Karvonen, T., Nemeth, C., Girolami, M. and Oates, C. J. (2020). Semi-Exact Control Functionals From Sard's Method.  <a href="https://arxiv.org/abs/2002.00033">https://arxiv.org/abs/2002.00033</a>
</p>


<h3>See Also</h3>

<p>See <a href="#topic+ZVCV">ZVCV</a> for examples and related functions. See <code><a href="#topic+CF_crossval">CF_crossval</a></code> for a function to choose between different kernels for this estimator.
</p>

<hr>
<h2 id='CF_crossval'>Control functionals (CF) with cross-validation</h2><span id='topic+CF_crossval'></span>

<h3>Description</h3>

<p>This function chooses between a list of kernel tuning parameters (<code>sigma_list</code>) or a list of K0 matrices (<code>K0_list</code>) for
the control functionals method described in Oates et al (2017). The latter requires
calculating and storing kernel matrices using <code><a href="#topic+K0_fn">K0_fn</a></code> but it is more flexible
because it can be used to choose the Stein operator order and the kernel function, in addition
to its parameters. It is also faster to pre-specify <code><a href="#topic+K0_fn">K0_fn</a></code>.
For estimation with fixed kernel parameters, use <code><a href="#topic+CF">CF</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CF_crossval(
  integrands,
  samples,
  derivatives,
  steinOrder = NULL,
  kernel_function = NULL,
  sigma_list = NULL,
  K0_list = NULL,
  est_inds = NULL,
  log_weights = NULL,
  one_in_denom = FALSE,
  folds = NULL,
  diagnostics = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="CF_crossval_+3A_integrands">integrands</code></td>
<td>
<p>An <code class="reqn">N</code> by <code class="reqn">k</code> matrix of integrands (evaluations of the function of interest)</p>
</td></tr>
<tr><td><code id="CF_crossval_+3A_samples">samples</code></td>
<td>
<p>An <code class="reqn">N</code> by <code class="reqn">d</code> matrix of samples from the target</p>
</td></tr>
<tr><td><code id="CF_crossval_+3A_derivatives">derivatives</code></td>
<td>
<p>An <code class="reqn">N</code> by <code class="reqn">d</code> matrix of derivatives of the log target with respect to the parameters</p>
</td></tr>
<tr><td><code id="CF_crossval_+3A_steinorder">steinOrder</code></td>
<td>
<p>(optional)    This is the order of the Stein operator. The default is <code>1</code> in the control functionals paper (Oates et al, 2017) and <code>2</code> in the semi-exact control functionals paper (South et al, 2020).  The following values are currently available: <code>1</code> for all kernels and <code>2</code> for &quot;gaussian&quot;, &quot;matern&quot; and &quot;RQ&quot;. See below for further details.</p>
</td></tr>
<tr><td><code id="CF_crossval_+3A_kernel_function">kernel_function</code></td>
<td>
<p>(optional)        Choose between &quot;gaussian&quot;, &quot;matern&quot;, &quot;RQ&quot;, &quot;product&quot; or &quot;prodsim&quot;. See below for further details.</p>
</td></tr>
<tr><td><code id="CF_crossval_+3A_sigma_list">sigma_list</code></td>
<td>
<p>(optional between this and <code>K0_list</code>)            A list of tuning parameters for the specified kernel. This involves a list of single length-scale parameter in &quot;gaussian&quot; and &quot;RQ&quot;, a list of vectors containing length-scale and smoothness parameters in &quot;matern&quot; and a list of vectors of the two parameters in &quot;product&quot; and &quot;prodsim&quot;. See below for further details. When <code>sigma_list</code> is specified and not <code>K0_list</code>, the <code class="reqn">K0</code> matrix is computed twice for each selected tuning parameter.</p>
</td></tr>
<tr><td><code id="CF_crossval_+3A_k0_list">K0_list</code></td>
<td>
<p>(optional between this and <code>sigma_list</code>) A list of kernel matrices, which can be calculated using <code><a href="#topic+K0_fn">K0_fn</a></code>.</p>
</td></tr>
<tr><td><code id="CF_crossval_+3A_est_inds">est_inds</code></td>
<td>
<p>(optional) A vector of indices for the estimation-only samples. The default when <code>est_inds</code> is missing or <code>NULL</code> is to perform both estimation of the control variates and evaluation of the integral using all samples. Otherwise, the samples from <code>est_inds</code> are used in estimating the control variates and the remainder are used in evaluating the integral. Splitting the indices in this way can be used to reduce bias from adaption and to make computation feasible for very large sample sizes (small <code>est_inds</code> is faster), but in general in will increase the variance of the estimator.</p>
</td></tr>
<tr><td><code id="CF_crossval_+3A_log_weights">log_weights</code></td>
<td>
<p>(optional) A vector of length <code class="reqn">N</code> containing the logged weights of the samples. The default is equal weights. The weights are only used in estimating the cross-validation error. This method is not implemented for the case where <code>est_inds</code> is specified becausing specifying <code>est_inds</code> typically indicates a desire for an unbiased estimator and using self-normalised importance weights introduces bias.</p>
</td></tr>
<tr><td><code id="CF_crossval_+3A_one_in_denom">one_in_denom</code></td>
<td>
<p>(optional) Whether or not to include a <code class="reqn">1 + </code> in the denominator of the control functionals estimator, as in equation 2 on p703 of Oates et al (2017). The <code class="reqn">1 +</code> in the denominator is an arbitrary choice so we set it to zero by default.</p>
</td></tr>
<tr><td><code id="CF_crossval_+3A_folds">folds</code></td>
<td>
<p>(optional) The number of folds for cross-validation. The default is five.</p>
</td></tr>
<tr><td><code id="CF_crossval_+3A_diagnostics">diagnostics</code></td>
<td>
<p>(optional) A flag for whether to return the necessary outputs for plotting or estimating using the fitted model. The default is <code>false</code> since this requires some additional computation when <code>est_inds</code> is <code>NULL</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with the following elements:
</p>

<ul>
<li> <p><code>expectation</code>: The estimate(s) of the (<code class="reqn">k</code>) expectation(s).
</p>
</li>
<li> <p><code>mse</code>: A matrix of the cross-validation mean square prediction errors. The number of columns is the number of tuning options given and the number of rows is <code class="reqn">k</code>, the number of integrands of interest.
</p>
</li>
<li> <p><code>optinds</code>: The optimal indices from the list for each expectation.
</p>
</li>
<li> <p><code>f_true</code>: (Only if <code>est_inds</code> is not <code>NULL</code>) The integrands for the evaluation set. This should be the same as integrands[setdiff(1:N,est_inds),].
</p>
</li>
<li> <p><code>f_hat</code>: (Only if <code>est_inds</code> is not <code>NULL</code>) The fitted values for the integrands in the evaluation set. This can be used to help assess the performance of the Gaussian process model.
</p>
</li>
<li> <p><code>a</code>: (Only if <code>diagnostics</code> = <code>TRUE</code>) The value of <code class="reqn">a</code> as described in South et al (2020), where predictions are of the form <code class="reqn">f_hat = K0*a + 1*b</code> for heldout K0 and estimators using heldout samples are of the form <code class="reqn">mean(f - f_hat) + b</code>.
</p>
</li>
<li> <p><code>b</code>: (Only if <code>diagnostics</code> = <code>TRUE</code>) The value of <code class="reqn">b</code> as described in South et al (2020), where predictions are of the form <code class="reqn">f_hat = K0*a + 1*b</code> for heldout K0 and estimators using heldout samples are of the form <code class="reqn">mean(f - f_hat) + b</code>.
</p>
</li>
<li> <p><code>ksd</code>: (Only if <code>diagnostics</code> = <code>TRUE</code>) An estimated kernel Stein discrepancy based on the fitted model that can be used for diagnostic purposes. See South et al (2020) for further details.
</p>
</li>
<li> <p><code>bound_const</code>: (Only if <code>diagnostics</code> = <code>TRUE</code> and <code>est_inds</code>=<code>NULL</code>) This is such that the absolute error for the estimator should be less than <code class="reqn">ksd \times bound_const</code>.
</p>
</li></ul>



<h3>Warning</h3>

<p>Solving the linear system in CF has <code class="reqn">O(N^3)</code> complexity and is therefore not suited to large <code class="reqn">N</code>. Using <code class="reqn">est_inds</code> will instead have an <code class="reqn">O(N_0^3)</code> cost in solving the linear system and an <code class="reqn">O((N-N_0)^2)</code> cost in handling the remaining samples, where <code class="reqn">N_0</code> is the length of <code class="reqn">est_inds</code>. This can be much cheaper for large <code class="reqn">N</code>.
</p>


<h3>On the choice of <code class="reqn">\sigma</code>, the kernel and the Stein order</h3>

<p>The kernel in Stein-based kernel methods is <code class="reqn">L_x L_y k(x,y)</code> where <code class="reqn">L_x</code> is a first or second order Stein operator in <code class="reqn">x</code> and <code class="reqn">k(x,y)</code> is some generic kernel to be specified.
</p>
<p>The Stein operators for distribution <code class="reqn">p(x)</code> are defined as:
</p>

<ul>
<li> <p><strong><code>steinOrder=1</code></strong>: <code class="reqn">L_x g(x) = \nabla_x^T g(x) + \nabla_x \log p(x)^T g(x)</code> (see e.g. Oates el al (2017))
</p>
</li>
<li> <p><strong><code>steinOrder=2</code></strong>: <code class="reqn">L_x g(x) = \Delta_x g(x) + \nabla_x log p(x)^T \nabla_x g(x)</code> (see e.g. South el al (2020))
</p>
</li></ul>

<p>Here <code class="reqn">\nabla_x</code> is the first order derivative wrt <code class="reqn">x</code> and <code class="reqn">\Delta_x = \nabla_x^T \nabla_x</code> is the Laplacian operator.
</p>
<p>The generic kernels which are implemented in this package are listed below.  Note that the input parameter <strong><code>sigma</code></strong> defines the kernel parameters <code class="reqn">\sigma</code>. 
</p>

<ul>
<li> <p><strong><code>"gaussian"</code></strong>: A Gaussian kernel,
</p>
<p style="text-align: center;"><code class="reqn">k(x,y) = exp(-z(x,y)/\sigma^2)</code>
</p>

</li>
<li> <p><strong><code>"matern"</code></strong>: A Matern kernel with <code class="reqn">\sigma = (\lambda,\nu)</code>,
</p>
<p style="text-align: center;"><code class="reqn">k(x,y) = bc^{\nu}z(x,y)^{\nu/2}K_{\nu}(c z(x,y)^{0.5})</code>
</p>
<p> where <code class="reqn">b=2^{1-\nu}(\Gamma(\nu))^{-1}</code>, <code class="reqn">c=(2\nu)^{0.5}\lambda^{-1}</code> and <code class="reqn">K_{\nu}(x)</code> is the modified Bessel function of the second kind. Note that <code class="reqn">\lambda</code> is the length-scale parameter and <code class="reqn">\nu</code> is the smoothness parameter (which defaults to 2.5 for <code class="reqn">steinOrder=1</code> and 4.5 for <code class="reqn">steinOrder=2</code>).
</p>
</li>
<li> <p><strong><code>"RQ"</code></strong>: A rational quadratic kernel,
</p>
<p style="text-align: center;"><code class="reqn">k(x,y) = (1+\sigma^{-2}z(x,y))^{-1}</code>
</p>

</li>
<li> <p><strong><code>"product"</code></strong>: The product kernel that appears in Oates et al (2017) with <code class="reqn">\sigma = (a,b)</code>
</p>
<p style="text-align: center;"><code class="reqn">k(x,y) = (1+a z(x) + a z(y))^{-1} exp(-0.5 b^{-2} z(x,y)) </code>
</p>

</li>
<li> <p><strong><code>"prodsim"</code></strong>: A slightly different product kernel with <code class="reqn">\sigma = (a,b)</code> (see e.g. <a href="https://www.imperial.ac.uk/inference-group/projects/monte-carlo-methods/control-functionals/">https://www.imperial.ac.uk/inference-group/projects/monte-carlo-methods/control-functionals/</a>),
</p>
<p style="text-align: center;"><code class="reqn">k(x,y) = (1+a z(x))^{-1}(1 + a z(y))^{-1} exp(-0.5 b^{-2} z(x,y)) </code>
</p>

</li></ul>

<p>In the above equations, <code class="reqn">z(x) = \sum_j x[j]^2</code> and <code class="reqn">z(x,y) = \sum_j (x[j] - y[j])^2</code>. For the last two kernels, the code only has implementations for <code>steinOrder</code>=<code>1</code>. Each combination of <code>steinOrder</code> and <code>kernel_function</code> above is currently hard-coded but it may be possible to extend this to other kernels in future versions using autodiff. The calculations for the first three kernels above are detailed in South et al (2020).
</p>


<h3>Author(s)</h3>

<p>Leah F. South
</p>


<h3>References</h3>

<p>Oates, C. J., Girolami, M. &amp; Chopin, N. (2017). Control functionals for Monte Carlo integration. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 79(3), 695-718.
</p>
<p>South, L. F., Karvonen, T., Nemeth, C., Girolami, M. and Oates, C. J. (2020). Semi-Exact Control Functionals From Sard's Method.  <a href="https://arxiv.org/abs/2002.00033">https://arxiv.org/abs/2002.00033</a>
</p>


<h3>See Also</h3>

<p>See <a href="#topic+ZVCV">ZVCV</a> for examples and related functions. See <code><a href="#topic+CF">CF</a></code> for a function to perform control functionals with fixed kernel specifications.
</p>

<hr>
<h2 id='evidence'>Evidence estimation with ZV-CV</h2><span id='topic+evidence'></span><span id='topic+evidence_CTI'></span><span id='topic+evidence_CTI_CF'></span><span id='topic+evidence_SMC'></span><span id='topic+evidence_SMC_CF'></span>

<h3>Description</h3>

<p>The functions <code>evidence_CTI</code> and <code>evidence_CTI_CF</code> can be used to improve upon the thermodynamic integration (TI) estimate of the normalising constant with ZV-CV and CF, respectively. The functions <code>evidence_SMC</code> and <code>evidence_SMC_CF</code> do the same thing for the sequential Monte Carlo (SMC) normalising constant identity.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>evidence_CTI(
  samples,
  loglike,
  der_loglike,
  der_logprior,
  temperatures,
  temperatures_all,
  most_recent,
  est_inds,
  options,
  folds = 5
)

evidence_CTI_CF(
  samples,
  loglike,
  der_loglike,
  der_logprior,
  temperatures,
  temperatures_all,
  most_recent,
  est_inds,
  steinOrder,
  kernel_function,
  sigma_list,
  folds = 5
)

evidence_SMC(
  samples,
  loglike,
  der_loglike,
  der_logprior,
  temperatures,
  temperatures_all,
  most_recent,
  est_inds,
  options,
  folds = 5
)

evidence_SMC_CF(
  samples,
  loglike,
  der_loglike,
  der_logprior,
  temperatures,
  temperatures_all,
  most_recent,
  est_inds,
  steinOrder,
  kernel_function,
  sigma_list,
  folds = 5
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="evidence_+3A_samples">samples</code></td>
<td>
<p>An <code class="reqn">N</code> by <code class="reqn">d</code> by <code class="reqn">T</code> matrix of samples from the <code class="reqn">T</code> power posteriors, where <code class="reqn">N</code> is the number of samples and <code class="reqn">d</code> is the dimension of the target distribution</p>
</td></tr>
<tr><td><code id="evidence_+3A_loglike">loglike</code></td>
<td>
<p>An <code class="reqn">N</code> by <code class="reqn">T</code> matrix of log likelihood values corresponding to <code>samples</code></p>
</td></tr>
<tr><td><code id="evidence_+3A_der_loglike">der_loglike</code></td>
<td>
<p>An <code class="reqn">N</code> by <code class="reqn">d</code> by <code class="reqn">T</code> matrix of the derivatives of the log likelihood with respect to the parameters, with parameter values corresponding to <code>samples</code></p>
</td></tr>
<tr><td><code id="evidence_+3A_der_logprior">der_logprior</code></td>
<td>
<p>An <code class="reqn">N</code> by <code class="reqn">d</code> by <code class="reqn">T</code> matrix of the derivatives of the log prior with respect to the parameters, with parameter values corresponding to <code>samples</code></p>
</td></tr>
<tr><td><code id="evidence_+3A_temperatures">temperatures</code></td>
<td>
<p>A vector of length <code class="reqn">T</code> of temperatures for the power posterior temperatures</p>
</td></tr>
<tr><td><code id="evidence_+3A_temperatures_all">temperatures_all</code></td>
<td>
<p>An adjusted vector of length <code class="reqn">tau</code> of temperatures. Better performance should be obtained with a more conservative temperature schedule. See <code><a href="#topic+Expand_Temperatures">Expand_Temperatures</a></code> for a function to adjust the temperatures.</p>
</td></tr>
<tr><td><code id="evidence_+3A_most_recent">most_recent</code></td>
<td>
<p>A vector of length <code class="reqn">tau</code> which gives the indices in the original temperatures that the new temperatures correspond to.</p>
</td></tr>
<tr><td><code id="evidence_+3A_est_inds">est_inds</code></td>
<td>
<p>(optional) A vector of indices for the estimation-only samples. The default when <code>est_inds</code> is missing or <code>NULL</code> is to perform both estimation of the control variates and evaluation of the integral using all samples. Otherwise, the samples from <code>est_inds</code> are used in estimating the control variates and the remainder are used in evaluating the integral. Splitting the indices in this way can be used to reduce bias from adaption and to make computation feasible for very large sample sizes (small <code>est_inds</code> is faster), but in general in will increase the variance of the estimator.</p>
</td></tr>
<tr><td><code id="evidence_+3A_options">options</code></td>
<td>
<p>A list of control variate specifications for ZV-CV. This can be a single list containing the elements below (the defaults are used for elements which are not specified). Alternatively, it can be a list of lists containing any or all of the elements below. Where the latter is used, the function <code>zvcv</code> automatically selects the best performing option based on cross-validation.</p>
</td></tr>
<tr><td><code id="evidence_+3A_folds">folds</code></td>
<td>
<p>The number of folds used in k-fold cross-validation for selecting the optimal control variate. For ZV-CV, this may include selection of the optimal polynomial order, regression type and subset of parameters depending on <code>options</code>. For CF, this includes the selection of the optimal tuning parameters in <code>sigma_list</code>. The default is five.</p>
</td></tr>
<tr><td><code id="evidence_+3A_steinorder">steinOrder</code></td>
<td>
<p>(optional)    This is the order of the Stein operator. The default is <code>1</code> in the control functionals paper (Oates et al, 2017) and <code>2</code> in the semi-exact control functionals paper (South et al, 2020).  The following values are currently available: <code>1</code> for all kernels and <code>2</code> for &quot;gaussian&quot;, &quot;matern&quot; and &quot;RQ&quot;. See below for further details.</p>
</td></tr>
<tr><td><code id="evidence_+3A_kernel_function">kernel_function</code></td>
<td>
<p>(optional)        Choose between &quot;gaussian&quot;, &quot;matern&quot;, &quot;RQ&quot;, &quot;product&quot; or &quot;prodsim&quot;. See below for further details.</p>
</td></tr>
<tr><td><code id="evidence_+3A_sigma_list">sigma_list</code></td>
<td>
<p>(optional between this and <code>K0_list</code>)            A list of tuning parameters for the specified kernel. This involves a list of single length-scale parameter in &quot;gaussian&quot; and &quot;RQ&quot;, a list of vectors containing length-scale and smoothness parameters in &quot;matern&quot; and a list of vectors of the two parameters in &quot;product&quot; and &quot;prodsim&quot;. See below for further details. When <code>sigma_list</code> is specified and not <code>K0_list</code>, the <code class="reqn">K0</code> matrix is computed twice for each selected tuning parameter.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The function <code>evidence_CTI</code>  returns a list, containing the following components:
</p>

<ul>
<li> <p><code>log_evidence_PS1</code>: The 1st order quadrature estimate for the log normalising constant
</p>
</li>
<li> <p><code>log_evidence_PS2</code>: The 2nd order quadrature estimate for the log normalising constant
</p>
</li>
<li> <p><code>regression_LL</code>: The set of <code class="reqn">tau</code> <code>zvcv</code> type returns for the 1st order quadrature expectations
</p>
</li>
<li> <p><code>regression_vLL</code>: The set of <code class="reqn">tau</code> <code>zvcv</code> type returns for the 2nd order quadrature expectations
</p>
</li></ul>

<p>The function <code>evidence_CTI_CF</code>  returns a list, containing the following components:
</p>

<ul>
<li> <p><code>log_evidence_PS1</code>: The 1st order quadrature estimate for the log normalising constant
</p>
</li>
<li> <p><code>log_evidence_PS2</code>: The 2nd order quadrature estimate for the log normalising constant
</p>
</li>
<li> <p><code>regression_LL</code>: The set of <code class="reqn">tau</code> <code>CF_crossval</code> type returns for the 1st order quadrature expectations
</p>
</li>
<li> <p><code>regression_vLL</code>: The set of <code class="reqn">tau</code> <code>CF_crossval</code> type returns for the 2nd order quadrature expectations
</p>
</li>
<li> <p><code>selected_LL_CF</code>: The set of <code class="reqn">tau</code> selected tuning parameters from <code>sigma_list</code> for the 1st order quadrature expectations.
</p>
</li>
<li> <p><code>selected_vLL_CF</code>: The set of <code class="reqn">tau</code> selected tuning parameters from <code>sigma_list</code> for the 2nd order quadrature expectations.
</p>
</li></ul>

<p>The function <code>evidence_SMC</code>  returns a list, containing the following components:
</p>

<ul>
<li> <p><code>log_evidence</code>: The logged SMC estimate for the normalising constant
</p>
</li>
<li> <p><code>regression_SMC</code>: The set of <code class="reqn">tau</code> <code>zvcv</code> type returns for the expectations
</p>
</li></ul>

<p>The function <code>evidence_SMC_CF</code>  returns a list, containing the following components:
</p>

<ul>
<li> <p><code>log_evidence</code>: The logged SMC estimate for the normalising constant
</p>
</li>
<li> <p><code>regression_SMC</code>: The set of <code class="reqn">tau</code> <code>CF_crossval</code> type returns for the expectations
</p>
</li>
<li> <p><code>selected_CF</code>: The set of <code class="reqn">tau</code> selected tuning parameters from <code>sigma_list</code> for the expectations
</p>
</li></ul>



<h3>On the choice of <code class="reqn">\sigma</code>, the kernel and the Stein order</h3>

<p>The kernel in Stein-based kernel methods is <code class="reqn">L_x L_y k(x,y)</code> where <code class="reqn">L_x</code> is a first or second order Stein operator in <code class="reqn">x</code> and <code class="reqn">k(x,y)</code> is some generic kernel to be specified.
</p>
<p>The Stein operators for distribution <code class="reqn">p(x)</code> are defined as:
</p>

<ul>
<li> <p><strong><code>steinOrder=1</code></strong>: <code class="reqn">L_x g(x) = \nabla_x^T g(x) + \nabla_x \log p(x)^T g(x)</code> (see e.g. Oates el al (2017))
</p>
</li>
<li> <p><strong><code>steinOrder=2</code></strong>: <code class="reqn">L_x g(x) = \Delta_x g(x) + \nabla_x log p(x)^T \nabla_x g(x)</code> (see e.g. South el al (2020))
</p>
</li></ul>

<p>Here <code class="reqn">\nabla_x</code> is the first order derivative wrt <code class="reqn">x</code> and <code class="reqn">\Delta_x = \nabla_x^T \nabla_x</code> is the Laplacian operator.
</p>
<p>The generic kernels which are implemented in this package are listed below.  Note that the input parameter <strong><code>sigma</code></strong> defines the kernel parameters <code class="reqn">\sigma</code>. 
</p>

<ul>
<li> <p><strong><code>"gaussian"</code></strong>: A Gaussian kernel,
</p>
<p style="text-align: center;"><code class="reqn">k(x,y) = exp(-z(x,y)/\sigma^2)</code>
</p>

</li>
<li> <p><strong><code>"matern"</code></strong>: A Matern kernel with <code class="reqn">\sigma = (\lambda,\nu)</code>,
</p>
<p style="text-align: center;"><code class="reqn">k(x,y) = bc^{\nu}z(x,y)^{\nu/2}K_{\nu}(c z(x,y)^{0.5})</code>
</p>
<p> where <code class="reqn">b=2^{1-\nu}(\Gamma(\nu))^{-1}</code>, <code class="reqn">c=(2\nu)^{0.5}\lambda^{-1}</code> and <code class="reqn">K_{\nu}(x)</code> is the modified Bessel function of the second kind. Note that <code class="reqn">\lambda</code> is the length-scale parameter and <code class="reqn">\nu</code> is the smoothness parameter (which defaults to 2.5 for <code class="reqn">steinOrder=1</code> and 4.5 for <code class="reqn">steinOrder=2</code>).
</p>
</li>
<li> <p><strong><code>"RQ"</code></strong>: A rational quadratic kernel,
</p>
<p style="text-align: center;"><code class="reqn">k(x,y) = (1+\sigma^{-2}z(x,y))^{-1}</code>
</p>

</li>
<li> <p><strong><code>"product"</code></strong>: The product kernel that appears in Oates et al (2017) with <code class="reqn">\sigma = (a,b)</code>
</p>
<p style="text-align: center;"><code class="reqn">k(x,y) = (1+a z(x) + a z(y))^{-1} exp(-0.5 b^{-2} z(x,y)) </code>
</p>

</li>
<li> <p><strong><code>"prodsim"</code></strong>: A slightly different product kernel with <code class="reqn">\sigma = (a,b)</code> (see e.g. <a href="https://www.imperial.ac.uk/inference-group/projects/monte-carlo-methods/control-functionals/">https://www.imperial.ac.uk/inference-group/projects/monte-carlo-methods/control-functionals/</a>),
</p>
<p style="text-align: center;"><code class="reqn">k(x,y) = (1+a z(x))^{-1}(1 + a z(y))^{-1} exp(-0.5 b^{-2} z(x,y)) </code>
</p>

</li></ul>

<p>In the above equations, <code class="reqn">z(x) = \sum_j x[j]^2</code> and <code class="reqn">z(x,y) = \sum_j (x[j] - y[j])^2</code>. For the last two kernels, the code only has implementations for <code>steinOrder</code>=<code>1</code>. Each combination of <code>steinOrder</code> and <code>kernel_function</code> above is currently hard-coded but it may be possible to extend this to other kernels in future versions using autodiff. The calculations for the first three kernels above are detailed in South et al (2020).
</p>


<h3>Author(s)</h3>

<p>Leah F. South
</p>


<h3>References</h3>

<p>Mira, A., Solgi, R., &amp; Imparato, D. (2013). Zero variance Markov chain Monte Carlo for Bayesian estimators. Statistics and Computing, 23(5), 653-662.
</p>
<p>South, L. F., Oates, C. J., Mira, A., &amp; Drovandi, C. (2019). Regularised zero variance control variates for high-dimensional variance reduction. <a href="https://arxiv.org/abs/1811.05073">https://arxiv.org/abs/1811.05073</a>
</p>


<h3>See Also</h3>

<p>See an example at <code><a href="#topic+VDP">VDP</a></code> and see <a href="#topic+ZVCV">ZVCV</a> for more package details. See <code><a href="#topic+Expand_Temperatures">Expand_Temperatures</a></code> for a function that can be used to find stricter (or less stricter) temperature schedules based on the conditional effective sample size.
</p>

<hr>
<h2 id='Expand_Temperatures'>Adjusting the temperature schedule</h2><span id='topic+Expand_Temperatures'></span>

<h3>Description</h3>

<p>This function is used to adjust the temperature schedule so that it is more (or less) strict than the original.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Expand_Temperatures(
  temperatures,
  loglike,
  rho,
  bisec_tol = .Machine$double.eps^0.25
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Expand_Temperatures_+3A_temperatures">temperatures</code></td>
<td>
<p>A vector of length <code class="reqn">T</code> temperatures for the power posterior temperatures.</p>
</td></tr>
<tr><td><code id="Expand_Temperatures_+3A_loglike">loglike</code></td>
<td>
<p>An <code class="reqn">N</code> by <code class="reqn">T</code> matrix of log likelihood values corresponding to the samples.</p>
</td></tr>
<tr><td><code id="Expand_Temperatures_+3A_rho">rho</code></td>
<td>
<p>The tolerance for the new temperatures. Temperatures are selected so that the conditional effective sample size (CESS) at each temperature is <code class="reqn">\rho*N</code> where <code class="reqn">N</code> is the population size.</p>
</td></tr>
<tr><td><code id="Expand_Temperatures_+3A_bisec_tol">bisec_tol</code></td>
<td>
<p>The tolerance for the bisection method used in selecting temperatures. The default is <code>.Machine$double.eps^0.25</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list is returned, containing the following components:
</p>

<ul>
<li> <p><code>temperatures_all</code>: The new set of temperatures of length <code class="reqn">tau</code>.
</p>
</li>
<li> <p><code>relevant_samples</code>: A vector of length <code class="reqn">tau</code> containing indices to show which particle sets the new temperatures are based on.
</p>
</li>
<li> <p><code>logw</code>: An <code class="reqn">N</code> by <code class="reqn">tau</code> matrix of log normalised weights of the particles
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Leah F. South
</p>


<h3>References</h3>

<p>South, L. F., Oates, C. J., Mira, A., &amp; Drovandi, C. (2019). Regularised zero variance control variates for high-dimensional variance reduction. <a href="https://arxiv.org/abs/1811.05073">https://arxiv.org/abs/1811.05073</a>
</p>


<h3>See Also</h3>

<p>See <code><a href="#topic+evidence">evidence</a></code> for functions to estimate the evidence, <code><a href="#topic+VDP">VDP</a></code> for an example and <a href="#topic+ZVCV">ZVCV</a> for more package details.
</p>

<hr>
<h2 id='getX'>ZV-CV design matrix</h2><span id='topic+getX'></span>

<h3>Description</h3>

<p>The function <code>getX</code> is used to get the matrix of covariates for the regression based on a specified polynomial order.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>getX(samples, derivatives, polyorder)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="getX_+3A_samples">samples</code></td>
<td>
<p>An <code class="reqn">N</code> by <code class="reqn">d</code> matrix of samples from the target</p>
</td></tr>
<tr><td><code id="getX_+3A_derivatives">derivatives</code></td>
<td>
<p>An <code class="reqn">N</code> by <code class="reqn">d</code> matrix of derivatives of the log target with respect to the parameters</p>
</td></tr>
<tr><td><code id="getX_+3A_polyorder">polyorder</code></td>
<td>
<p>The order of the polynomial.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The design matrix for the regression (except for the column of 1's for the intercept).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Phi_fn">Phi_fn</a></code> for a very similar function for use in semi-exact control functionals. The function <code><a href="#topic+Phi_fn">Phi_fn</a></code> essentially gets the same matrix but with a column of ones added.
</p>

<hr>
<h2 id='K0_fn'>Kernel matrix calculation</h2><span id='topic+K0_fn'></span>

<h3>Description</h3>

<p>This function calculates the full <code class="reqn">K_0</code> matrix, which is a first or second order Stein operator applied to
a standard kernel. 
The output of this function can be used as an argument to <code><a href="#topic+CF">CF</a></code>, <code><a href="#topic+CF_crossval">CF_crossval</a></code>,
<code><a href="#topic+SECF">SECF</a></code>, <code><a href="#topic+SECF_crossval">SECF_crossval</a></code>, <code><a href="#topic+aSECF">aSECF</a></code> and <code><a href="#topic+aSECF_crossval">aSECF_crossval</a></code>.
The kernel matrix is automatically computed in all of the above methods, but it is faster to calculate
in advance when using more than one of the above functions and when using any of the crossval functions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>K0_fn(
  samples,
  derivatives,
  sigma,
  steinOrder,
  kernel_function,
  Z = NULL,
  nystrom_inds = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="K0_fn_+3A_samples">samples</code></td>
<td>
<p>An <code class="reqn">N</code> by <code class="reqn">d</code> matrix of samples from the target</p>
</td></tr>
<tr><td><code id="K0_fn_+3A_derivatives">derivatives</code></td>
<td>
<p>An <code class="reqn">N</code> by <code class="reqn">d</code> matrix of derivatives of the log target with respect to the parameters</p>
</td></tr>
<tr><td><code id="K0_fn_+3A_sigma">sigma</code></td>
<td>
<p>The tuning parameters of the specified kernel. This involves a single length-scale parameter in &quot;gaussian&quot; and &quot;RQ&quot;, a length-scale and a smoothness parameter in &quot;matern&quot; and two parameters in &quot;product&quot; and &quot;prodsim&quot;. See below for further details.</p>
</td></tr>
<tr><td><code id="K0_fn_+3A_steinorder">steinOrder</code></td>
<td>
<p>This is the order of the Stein operator. The default is <code>1</code> in the control functionals paper (Oates et al, 2017) and <code>2</code> in the semi-exact control functionals paper (South et al, 2020).  The following values are currently available: <code>1</code> for all kernels and <code>2</code> for &quot;gaussian&quot;, &quot;matern&quot; and &quot;RQ&quot;. See below for further details.</p>
</td></tr>
<tr><td><code id="K0_fn_+3A_kernel_function">kernel_function</code></td>
<td>
<p>Choose between &quot;gaussian&quot;, &quot;matern&quot;, &quot;RQ&quot;, &quot;product&quot; or &quot;prodsim&quot;. See below for further details.</p>
</td></tr>
<tr><td><code id="K0_fn_+3A_z">Z</code></td>
<td>
<p>(optional) An <code class="reqn">N</code> by <code class="reqn">N</code> (or <code class="reqn">N</code> by <code class="reqn">m</code> where <code class="reqn">m</code> is the length of <code>nystrom_inds</code>). This can be calculated using <code><a href="#topic+squareNorm">squareNorm</a></code>.</p>
</td></tr>
<tr><td><code id="K0_fn_+3A_nystrom_inds">nystrom_inds</code></td>
<td>
<p>(optional) The sample indices to be used in the Nystrom approximation (for when using aSECF).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An <code class="reqn">N</code> by <code class="reqn">N</code> kernel matrix (or <code class="reqn">N</code> by <code class="reqn">m</code> where <code class="reqn">m</code> is the length of <code>nystrom_inds</code>).
</p>


<h3>On the choice of <code class="reqn">\sigma</code>, the kernel and the Stein order</h3>

<p>The kernel in Stein-based kernel methods is <code class="reqn">L_x L_y k(x,y)</code> where <code class="reqn">L_x</code> is a first or second order Stein operator in <code class="reqn">x</code> and <code class="reqn">k(x,y)</code> is some generic kernel to be specified.
</p>
<p>The Stein operators for distribution <code class="reqn">p(x)</code> are defined as:
</p>

<ul>
<li> <p><strong><code>steinOrder=1</code></strong>: <code class="reqn">L_x g(x) = \nabla_x^T g(x) + \nabla_x \log p(x)^T g(x)</code> (see e.g. Oates el al (2017))
</p>
</li>
<li> <p><strong><code>steinOrder=2</code></strong>: <code class="reqn">L_x g(x) = \Delta_x g(x) + \nabla_x log p(x)^T \nabla_x g(x)</code> (see e.g. South el al (2020))
</p>
</li></ul>

<p>Here <code class="reqn">\nabla_x</code> is the first order derivative wrt <code class="reqn">x</code> and <code class="reqn">\Delta_x = \nabla_x^T \nabla_x</code> is the Laplacian operator.
</p>
<p>The generic kernels which are implemented in this package are listed below.  Note that the input parameter <strong><code>sigma</code></strong> defines the kernel parameters <code class="reqn">\sigma</code>. 
</p>

<ul>
<li> <p><strong><code>"gaussian"</code></strong>: A Gaussian kernel,
</p>
<p style="text-align: center;"><code class="reqn">k(x,y) = exp(-z(x,y)/\sigma^2)</code>
</p>

</li>
<li> <p><strong><code>"matern"</code></strong>: A Matern kernel with <code class="reqn">\sigma = (\lambda,\nu)</code>,
</p>
<p style="text-align: center;"><code class="reqn">k(x,y) = bc^{\nu}z(x,y)^{\nu/2}K_{\nu}(c z(x,y)^{0.5})</code>
</p>
<p> where <code class="reqn">b=2^{1-\nu}(\Gamma(\nu))^{-1}</code>, <code class="reqn">c=(2\nu)^{0.5}\lambda^{-1}</code> and <code class="reqn">K_{\nu}(x)</code> is the modified Bessel function of the second kind. Note that <code class="reqn">\lambda</code> is the length-scale parameter and <code class="reqn">\nu</code> is the smoothness parameter (which defaults to 2.5 for <code class="reqn">steinOrder=1</code> and 4.5 for <code class="reqn">steinOrder=2</code>).
</p>
</li>
<li> <p><strong><code>"RQ"</code></strong>: A rational quadratic kernel,
</p>
<p style="text-align: center;"><code class="reqn">k(x,y) = (1+\sigma^{-2}z(x,y))^{-1}</code>
</p>

</li>
<li> <p><strong><code>"product"</code></strong>: The product kernel that appears in Oates et al (2017) with <code class="reqn">\sigma = (a,b)</code>
</p>
<p style="text-align: center;"><code class="reqn">k(x,y) = (1+a z(x) + a z(y))^{-1} exp(-0.5 b^{-2} z(x,y)) </code>
</p>

</li>
<li> <p><strong><code>"prodsim"</code></strong>: A slightly different product kernel with <code class="reqn">\sigma = (a,b)</code> (see e.g. <a href="https://www.imperial.ac.uk/inference-group/projects/monte-carlo-methods/control-functionals/">https://www.imperial.ac.uk/inference-group/projects/monte-carlo-methods/control-functionals/</a>),
</p>
<p style="text-align: center;"><code class="reqn">k(x,y) = (1+a z(x))^{-1}(1 + a z(y))^{-1} exp(-0.5 b^{-2} z(x,y)) </code>
</p>

</li></ul>

<p>In the above equations, <code class="reqn">z(x) = \sum_j x[j]^2</code> and <code class="reqn">z(x,y) = \sum_j (x[j] - y[j])^2</code>. For the last two kernels, the code only has implementations for <code>steinOrder</code>=<code>1</code>. Each combination of <code>steinOrder</code> and <code>kernel_function</code> above is currently hard-coded but it may be possible to extend this to other kernels in future versions using autodiff. The calculations for the first three kernels above are detailed in South et al (2020).
</p>


<h3>Author(s)</h3>

<p>Leah F. South
</p>


<h3>References</h3>

<p>Oates, C. J., Girolami, M. &amp; Chopin, N. (2017). Control functionals for Monte Carlo integration. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 79(3), 695-718.
</p>
<p>South, L. F., Karvonen, T., Nemeth, C., Girolami, M. and Oates, C. J. (2020). Semi-Exact Control Functionals From Sard's Method.  <a href="https://arxiv.org/abs/2002.00033">https://arxiv.org/abs/2002.00033</a>
</p>

<hr>
<h2 id='logsumexp'>Stable log sum of exponential calculations</h2><span id='topic+logsumexp'></span>

<h3>Description</h3>

<p>The function <code>logsumexp</code> is used for stable computation of log(sum(exp(x))), which is useful when summing weights for example.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>logsumexp(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="logsumexp_+3A_x">x</code></td>
<td>
<p>The values for which you want to compute log(sum(exp(x)))</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The stable result of log(sum(exp(x)))
</p>


<h3>See Also</h3>

<p>See <a href="#topic+ZVCV">ZVCV</a> for more package details.
</p>

<hr>
<h2 id='medianTune'>Median heuristic</h2><span id='topic+medianTune'></span>

<h3>Description</h3>

<p>This function calculates the median heuristic for use in e.g. the Gaussian, Matern and rational quadratic kernels.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>medianTune(samples, Z = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="medianTune_+3A_samples">samples</code></td>
<td>
<p>An <code class="reqn">N</code> by <code class="reqn">d</code> matrix of samples from the target</p>
</td></tr>
<tr><td><code id="medianTune_+3A_z">Z</code></td>
<td>
<p>(optional) An NxN matrix of square norms, which can be calculated
using <code><a href="#topic+squareNorm">squareNorm</a></code>, as long as the <code>nystrom_inds</code> are <code>NULL</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The median heuristic, which can then be used as the length-scale parameter in the Gaussian, Matern and rational quadratic kernels
</p>


<h3>Author(s)</h3>

<p>Leah F. South
</p>


<h3>References</h3>

<p>Garreau, D., Jitkrittum, W. and Kanagawa, M. (2017). Large sample analysis of the median heuristic.  <a href="https://arxiv.org/abs/1707.07269">https://arxiv.org/abs/1707.07269</a>
</p>


<h3>See Also</h3>

<p>See <code><a href="#topic+medianTune">medianTune</a></code> and <code><a href="#topic+K0_fn">K0_fn</a></code> for functions which use this.
</p>

<hr>
<h2 id='nearPD'>Nearest symmetric positive definite matrix</h2><span id='topic+nearPD'></span>

<h3>Description</h3>

<p>This function finds the nearest symmetric positive definite matrix to the given matrix.
It is used throughout the package to handle numerical issues in matrix inverses
and cholesky decompositions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nearPD(K0)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="nearPD_+3A_k0">K0</code></td>
<td>
<p>A square matrix</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The closest symmetric positive definite matrix to K0.
</p>


<h3>Author(s)</h3>

<p>Adapted from Matlab code by John D'Errico
</p>


<h3>References</h3>

<p>Higham, N. J. (1988). Computing a nearest symmetric positive semidefinite matrix. Linear Algebra and its Applications, 103, 103-118.
</p>
<p>D'Errico, J. (2013). nearestSPD Matlab function. <a href="https://uk.mathworks.com/matlabcentral/fileexchange/42885-nearestspd">https://uk.mathworks.com/matlabcentral/fileexchange/42885-nearestspd</a>.
</p>

<hr>
<h2 id='Phi_fn'>Phi matrix calculation</h2><span id='topic+Phi_fn'></span>

<h3>Description</h3>

<p>This function calculates the <code class="reqn">\Phi</code> matrix, which is a second order Stein operator applied
to a polynomial. See South et al (2020) for further details. This function is not required for
estimation but may be useful when evaluation samples are not initially available since
estimators using heldout samples are of the form <code class="reqn">mean(f - f_hat) + b[1]</code> where <code class="reqn">f_hat = K0*a + Phi*b</code> for heldout K0 and Phi matrices.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Phi_fn(samples, derivatives, polyorder = NULL, apriori = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Phi_fn_+3A_samples">samples</code></td>
<td>
<p>An <code class="reqn">N</code> by <code class="reqn">d</code> matrix of samples from the target</p>
</td></tr>
<tr><td><code id="Phi_fn_+3A_derivatives">derivatives</code></td>
<td>
<p>An <code class="reqn">N</code> by <code class="reqn">d</code> matrix of derivatives of the log target with respect to the parameters</p>
</td></tr>
<tr><td><code id="Phi_fn_+3A_polyorder">polyorder</code></td>
<td>
<p>(optional)        The order of the polynomial to be used in the parametric component, with a default of <code class="reqn">1</code>. We recommend keeping this value low (e.g. only 1-2).</p>
</td></tr>
<tr><td><code id="Phi_fn_+3A_apriori">apriori</code></td>
<td>
<p>(optional) A vector containing the subset of parameter indices to use in the polynomial. Typically this argument would only be used if the dimension of the problem is very large or if prior information about parameter dependencies is known. The default is to use all parameters <code class="reqn">1:d</code> where <code class="reqn">d</code> is the dimension of the target.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An <code class="reqn">N</code> by <code class="reqn">Q</code> matrix (where Q is determined by the polynomial order and the apriori).
</p>


<h3>Author(s)</h3>

<p>Leah F. South
</p>


<h3>References</h3>

<p>South, L. F., Karvonen, T., Nemeth, C., Girolami, M. and Oates, C. J. (2020). Semi-Exact Control Functionals From Sard's Method.  <a href="https://arxiv.org/abs/2002.00033">https://arxiv.org/abs/2002.00033</a>
</p>

<hr>
<h2 id='SECF'>Semi-exact control functionals (SECF)</h2><span id='topic+SECF'></span>

<h3>Description</h3>

<p>This function performs semi-exact control functionals as described in South et al (2020).
To choose between different kernels using cross-validation, use <code><a href="#topic+SECF_crossval">SECF_crossval</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SECF(
  integrands,
  samples,
  derivatives,
  polyorder = NULL,
  steinOrder = NULL,
  kernel_function = NULL,
  sigma = NULL,
  K0 = NULL,
  est_inds = NULL,
  apriori = NULL,
  diagnostics = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="SECF_+3A_integrands">integrands</code></td>
<td>
<p>An <code class="reqn">N</code> by <code class="reqn">k</code> matrix of integrands (evaluations of the function of interest)</p>
</td></tr>
<tr><td><code id="SECF_+3A_samples">samples</code></td>
<td>
<p>An <code class="reqn">N</code> by <code class="reqn">d</code> matrix of samples from the target</p>
</td></tr>
<tr><td><code id="SECF_+3A_derivatives">derivatives</code></td>
<td>
<p>An <code class="reqn">N</code> by <code class="reqn">d</code> matrix of derivatives of the log target with respect to the parameters</p>
</td></tr>
<tr><td><code id="SECF_+3A_polyorder">polyorder</code></td>
<td>
<p>(optional)        The order of the polynomial to be used in the parametric component, with a default of <code class="reqn">1</code>. We recommend keeping this value low (e.g. only 1-2).</p>
</td></tr>
<tr><td><code id="SECF_+3A_steinorder">steinOrder</code></td>
<td>
<p>(optional)    This is the order of the Stein operator. The default is <code>1</code> in the control functionals paper (Oates et al, 2017) and <code>2</code> in the semi-exact control functionals paper (South et al, 2020).  The following values are currently available: <code>1</code> for all kernels and <code>2</code> for &quot;gaussian&quot;, &quot;matern&quot; and &quot;RQ&quot;. See below for further details.</p>
</td></tr>
<tr><td><code id="SECF_+3A_kernel_function">kernel_function</code></td>
<td>
<p>(optional)        Choose between &quot;gaussian&quot;, &quot;matern&quot;, &quot;RQ&quot;, &quot;product&quot; or &quot;prodsim&quot;. See below for further details.</p>
</td></tr>
<tr><td><code id="SECF_+3A_sigma">sigma</code></td>
<td>
<p>(optional)            The tuning parameters of the specified kernel. This involves a single length-scale parameter in &quot;gaussian&quot; and &quot;RQ&quot;, a length-scale and a smoothness parameter in &quot;matern&quot; and two parameters in &quot;product&quot; and &quot;prodsim&quot;. See below for further details.</p>
</td></tr>
<tr><td><code id="SECF_+3A_k0">K0</code></td>
<td>
<p>(optional) The kernel matrix. One can specify either this or all of <code>sigma</code>, <code>steinOrder</code> and <code>kernel_function</code>. The former involves pre-computing the kernel matrix using <code><a href="#topic+K0_fn">K0_fn</a></code> and is more efficient when using multiple estimators out of <code><a href="#topic+CF">CF</a></code>, <code><a href="#topic+SECF">SECF</a></code> and  <code><a href="#topic+aSECF">aSECF</a></code> or when using the cross-validation functions.</p>
</td></tr>
<tr><td><code id="SECF_+3A_est_inds">est_inds</code></td>
<td>
<p>(optional) A vector of indices for the estimation-only samples. The default when <code>est_inds</code> is missing or <code>NULL</code> is to perform both estimation of the control variates and evaluation of the integral using all samples. Otherwise, the samples from <code>est_inds</code> are used in estimating the control variates and the remainder are used in evaluating the integral. Splitting the indices in this way can be used to reduce bias from adaption and to make computation feasible for very large sample sizes (small <code>est_inds</code> is faster), but in general in will increase the variance of the estimator.</p>
</td></tr>
<tr><td><code id="SECF_+3A_apriori">apriori</code></td>
<td>
<p>(optional) A vector containing the subset of parameter indices to use in the polynomial. Typically this argument would only be used if the dimension of the problem is very large or if prior information about parameter dependencies is known. The default is to use all parameters <code class="reqn">1:d</code> where <code class="reqn">d</code> is the dimension of the target.</p>
</td></tr>
<tr><td><code id="SECF_+3A_diagnostics">diagnostics</code></td>
<td>
<p>(optional) A flag for whether to return the necessary outputs for plotting or estimating using the fitted model. The default is <code>false</code> since this requires some additional computation when <code>est_inds</code> is <code>NULL</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with the following elements:
</p>

<ul>
<li> <p><code>expectation</code>: The estimate(s) of the (<code class="reqn">k</code>) expectation(s).
</p>
</li>
<li> <p><code>f_true</code>: (Only if <code>est_inds</code> is not <code>NULL</code>) The integrands for the evaluation set. This should be the same as integrands[setdiff(1:N,est_inds),].
</p>
</li>
<li> <p><code>f_hat</code>: (Only if <code>est_inds</code> is not <code>NULL</code>) The fitted values for the integrands in the evaluation set. This can be used to help assess the performance of the Gaussian process model.
</p>
</li>
<li> <p><code>a</code>: (Only if <code>diagnostics</code> = <code>TRUE</code>) The value of <code class="reqn">a</code> as described in South et al (2020), where predictions are of the form <code class="reqn">f_hat = K0*a + Phi*b</code> for heldout K0 and Phi matrices and estimators using heldout samples are of the form <code class="reqn">mean(f - f_hat) + b[1]</code>.
</p>
</li>
<li> <p><code>b</code>: (Only if <code>diagnostics</code> = <code>TRUE</code>) The value of <code class="reqn">b</code> as described in South et al (2020), where predictions are of the form <code class="reqn">f_hat = K0*a + Phi*b</code> for heldout K0 and Phi matrices and estimators using heldout samples are of the form <code class="reqn">mean(f - f_hat) + b[1]</code>.
</p>
</li>
<li> <p><code>ksd</code>: (Only if <code>diagnostics</code> = <code>TRUE</code>) An estimated kernel Stein discrepancy based on the fitted model that can be used for diagnostic purposes. See South et al (2020) for further details.
</p>
</li>
<li> <p><code>bound_const</code>: (Only if <code>diagnostics</code> = <code>TRUE</code> and <code>est_inds</code>=<code>NULL</code>) This is such that the absolute error for the estimator should be less than <code class="reqn">ksd \times bound_const</code>.
</p>
</li></ul>



<h3>Warning</h3>

<p>Solving the linear system in SECF has <code class="reqn">O(N^3+Q^3)</code> complexity where <code class="reqn">N</code> is the sample size and <code class="reqn">Q</code> is the number of terms in the polynomial.
Standard SECF is therefore not suited to large <code class="reqn">N</code>. The method aSECF is designed for larger <code class="reqn">N</code> and details can be found at <code><a href="#topic+aSECF">aSECF</a></code> and in South et al (2020).
An alternative would be to use <code class="reqn">est_inds</code> which has <code class="reqn">O(N_0^3 + Q^3)</code> complexity in solving the linear system and <code class="reqn">O((N-N_0)^2)</code> complexity in
handling the remaining samples, where <code class="reqn">N_0</code> is the length of <code class="reqn">est_inds</code>. This can be much cheaper for small <code class="reqn">N_0</code> but the estimation of the
Gaussian process model is only done using <code class="reqn">N_0</code> samples and the evaluation of the integral only uses <code class="reqn">N-N_0</code> samples.
</p>


<h3>On the choice of <code class="reqn">\sigma</code>, the kernel and the Stein order</h3>

<p>The kernel in Stein-based kernel methods is <code class="reqn">L_x L_y k(x,y)</code> where <code class="reqn">L_x</code> is a first or second order Stein operator in <code class="reqn">x</code> and <code class="reqn">k(x,y)</code> is some generic kernel to be specified.
</p>
<p>The Stein operators for distribution <code class="reqn">p(x)</code> are defined as:
</p>

<ul>
<li> <p><strong><code>steinOrder=1</code></strong>: <code class="reqn">L_x g(x) = \nabla_x^T g(x) + \nabla_x \log p(x)^T g(x)</code> (see e.g. Oates el al (2017))
</p>
</li>
<li> <p><strong><code>steinOrder=2</code></strong>: <code class="reqn">L_x g(x) = \Delta_x g(x) + \nabla_x log p(x)^T \nabla_x g(x)</code> (see e.g. South el al (2020))
</p>
</li></ul>

<p>Here <code class="reqn">\nabla_x</code> is the first order derivative wrt <code class="reqn">x</code> and <code class="reqn">\Delta_x = \nabla_x^T \nabla_x</code> is the Laplacian operator.
</p>
<p>The generic kernels which are implemented in this package are listed below.  Note that the input parameter <strong><code>sigma</code></strong> defines the kernel parameters <code class="reqn">\sigma</code>. 
</p>

<ul>
<li> <p><strong><code>"gaussian"</code></strong>: A Gaussian kernel,
</p>
<p style="text-align: center;"><code class="reqn">k(x,y) = exp(-z(x,y)/\sigma^2)</code>
</p>

</li>
<li> <p><strong><code>"matern"</code></strong>: A Matern kernel with <code class="reqn">\sigma = (\lambda,\nu)</code>,
</p>
<p style="text-align: center;"><code class="reqn">k(x,y) = bc^{\nu}z(x,y)^{\nu/2}K_{\nu}(c z(x,y)^{0.5})</code>
</p>
<p> where <code class="reqn">b=2^{1-\nu}(\Gamma(\nu))^{-1}</code>, <code class="reqn">c=(2\nu)^{0.5}\lambda^{-1}</code> and <code class="reqn">K_{\nu}(x)</code> is the modified Bessel function of the second kind. Note that <code class="reqn">\lambda</code> is the length-scale parameter and <code class="reqn">\nu</code> is the smoothness parameter (which defaults to 2.5 for <code class="reqn">steinOrder=1</code> and 4.5 for <code class="reqn">steinOrder=2</code>).
</p>
</li>
<li> <p><strong><code>"RQ"</code></strong>: A rational quadratic kernel,
</p>
<p style="text-align: center;"><code class="reqn">k(x,y) = (1+\sigma^{-2}z(x,y))^{-1}</code>
</p>

</li>
<li> <p><strong><code>"product"</code></strong>: The product kernel that appears in Oates et al (2017) with <code class="reqn">\sigma = (a,b)</code>
</p>
<p style="text-align: center;"><code class="reqn">k(x,y) = (1+a z(x) + a z(y))^{-1} exp(-0.5 b^{-2} z(x,y)) </code>
</p>

</li>
<li> <p><strong><code>"prodsim"</code></strong>: A slightly different product kernel with <code class="reqn">\sigma = (a,b)</code> (see e.g. <a href="https://www.imperial.ac.uk/inference-group/projects/monte-carlo-methods/control-functionals/">https://www.imperial.ac.uk/inference-group/projects/monte-carlo-methods/control-functionals/</a>),
</p>
<p style="text-align: center;"><code class="reqn">k(x,y) = (1+a z(x))^{-1}(1 + a z(y))^{-1} exp(-0.5 b^{-2} z(x,y)) </code>
</p>

</li></ul>

<p>In the above equations, <code class="reqn">z(x) = \sum_j x[j]^2</code> and <code class="reqn">z(x,y) = \sum_j (x[j] - y[j])^2</code>. For the last two kernels, the code only has implementations for <code>steinOrder</code>=<code>1</code>. Each combination of <code>steinOrder</code> and <code>kernel_function</code> above is currently hard-coded but it may be possible to extend this to other kernels in future versions using autodiff. The calculations for the first three kernels above are detailed in South et al (2020).
</p>


<h3>Author(s)</h3>

<p>Leah F. South
</p>


<h3>References</h3>

<p>Oates, C. J., Girolami, M. &amp; Chopin, N. (2017). Control functionals for Monte Carlo integration. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 79(3), 695-718.
</p>
<p>South, L. F., Karvonen, T., Nemeth, C., Girolami, M. and Oates, C. J. (2020). Semi-Exact Control Functionals From Sard's Method.  <a href="https://arxiv.org/abs/2002.00033">https://arxiv.org/abs/2002.00033</a>
</p>


<h3>See Also</h3>

<p>See <a href="#topic+ZVCV">ZVCV</a> for examples and related functions. See <code><a href="#topic+SECF_crossval">SECF_crossval</a></code> for a function to choose between different kernels for this estimator.
</p>

<hr>
<h2 id='SECF_crossval'>Semi-exact control functionals (SECF) with cross-validation</h2><span id='topic+SECF_crossval'></span>

<h3>Description</h3>

<p>This function chooses between a list of kernel tuning parameters (<code>sigma_list</code>) or a list of K0 matrices (<code>K0_list</code>) for
the semi-exact control functionals method described in South et al (2020). The latter requires
calculating and storing kernel matrices using <code><a href="#topic+K0_fn">K0_fn</a></code> but it is more flexible
because it can be used to choose the Stein operator order and the kernel function, in addition
to its parameters. It is also faster to pre-specify <code><a href="#topic+K0_fn">K0_fn</a></code>.
For estimation with fixed kernel parameters, use <code><a href="#topic+SECF">SECF</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SECF_crossval(
  integrands,
  samples,
  derivatives,
  polyorder = NULL,
  steinOrder = NULL,
  kernel_function = NULL,
  sigma_list = NULL,
  K0_list = NULL,
  est_inds = NULL,
  apriori = NULL,
  folds = NULL,
  diagnostics = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="SECF_crossval_+3A_integrands">integrands</code></td>
<td>
<p>An <code class="reqn">N</code> by <code class="reqn">k</code> matrix of integrands (evaluations of the function of interest)</p>
</td></tr>
<tr><td><code id="SECF_crossval_+3A_samples">samples</code></td>
<td>
<p>An <code class="reqn">N</code> by <code class="reqn">d</code> matrix of samples from the target</p>
</td></tr>
<tr><td><code id="SECF_crossval_+3A_derivatives">derivatives</code></td>
<td>
<p>An <code class="reqn">N</code> by <code class="reqn">d</code> matrix of derivatives of the log target with respect to the parameters</p>
</td></tr>
<tr><td><code id="SECF_crossval_+3A_polyorder">polyorder</code></td>
<td>
<p>(optional)        The order of the polynomial to be used in the parametric component, with a default of <code class="reqn">1</code>. We recommend keeping this value low (e.g. only 1-2).</p>
</td></tr>
<tr><td><code id="SECF_crossval_+3A_steinorder">steinOrder</code></td>
<td>
<p>(optional)    This is the order of the Stein operator. The default is <code>1</code> in the control functionals paper (Oates et al, 2017) and <code>2</code> in the semi-exact control functionals paper (South et al, 2020).  The following values are currently available: <code>1</code> for all kernels and <code>2</code> for &quot;gaussian&quot;, &quot;matern&quot; and &quot;RQ&quot;. See below for further details.</p>
</td></tr>
<tr><td><code id="SECF_crossval_+3A_kernel_function">kernel_function</code></td>
<td>
<p>(optional)        Choose between &quot;gaussian&quot;, &quot;matern&quot;, &quot;RQ&quot;, &quot;product&quot; or &quot;prodsim&quot;. See below for further details.</p>
</td></tr>
<tr><td><code id="SECF_crossval_+3A_sigma_list">sigma_list</code></td>
<td>
<p>(optional between this and <code>K0_list</code>)            A list of tuning parameters for the specified kernel. This involves a list of single length-scale parameter in &quot;gaussian&quot; and &quot;RQ&quot;, a list of vectors containing length-scale and smoothness parameters in &quot;matern&quot; and a list of vectors of the two parameters in &quot;product&quot; and &quot;prodsim&quot;. See below for further details. When <code>sigma_list</code> is specified and not <code>K0_list</code>, the <code class="reqn">K0</code> matrix is computed twice for each selected tuning parameter.</p>
</td></tr>
<tr><td><code id="SECF_crossval_+3A_k0_list">K0_list</code></td>
<td>
<p>(optional between this and <code>sigma_list</code>) A list of kernel matrices, which can be calculated using <code><a href="#topic+K0_fn">K0_fn</a></code>.</p>
</td></tr>
<tr><td><code id="SECF_crossval_+3A_est_inds">est_inds</code></td>
<td>
<p>(optional) A vector of indices for the estimation-only samples. The default when <code>est_inds</code> is missing or <code>NULL</code> is to perform both estimation of the control variates and evaluation of the integral using all samples. Otherwise, the samples from <code>est_inds</code> are used in estimating the control variates and the remainder are used in evaluating the integral. Splitting the indices in this way can be used to reduce bias from adaption and to make computation feasible for very large sample sizes (small <code>est_inds</code> is faster), but in general in will increase the variance of the estimator.</p>
</td></tr>
<tr><td><code id="SECF_crossval_+3A_apriori">apriori</code></td>
<td>
<p>(optional) A vector containing the subset of parameter indices to use in the polynomial. Typically this argument would only be used if the dimension of the problem is very large or if prior information about parameter dependencies is known. The default is to use all parameters <code class="reqn">1:d</code> where <code class="reqn">d</code> is the dimension of the target.</p>
</td></tr>
<tr><td><code id="SECF_crossval_+3A_folds">folds</code></td>
<td>
<p>(optional) The number of folds for cross-validation. The default is five.</p>
</td></tr>
<tr><td><code id="SECF_crossval_+3A_diagnostics">diagnostics</code></td>
<td>
<p>(optional) A flag for whether to return the necessary outputs for plotting or estimating using the fitted model. The default is <code>false</code> since this requires some additional computation when <code>est_inds</code> is <code>NULL</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with the following elements:
</p>

<ul>
<li> <p><code>expectation</code>: The estimate(s) of the (<code class="reqn">k</code>) expectation(s).
</p>
</li>
<li> <p><code>mse</code>: A matrix of the cross-validation mean square prediction errors. The number of columns is the number of tuning options given and the number of rows is <code class="reqn">k</code>, the number of integrands of interest.
</p>
</li>
<li> <p><code>optinds</code>: The optimal indices from the list for each expectation.
</p>
</li>
<li> <p><code>f_true</code>: (Only if <code>est_inds</code> is not <code>NULL</code>) The integrands for the evaluation set. This should be the same as integrands[setdiff(1:N,est_inds),].
</p>
</li>
<li> <p><code>f_hat</code>: (Only if <code>est_inds</code> is not <code>NULL</code>) The fitted values for the integrands in the evaluation set. This can be used to help assess the performance of the Gaussian process model.
</p>
</li>
<li> <p><code>a</code>: (Only if <code>diagnostics</code> = <code>TRUE</code>) The value of <code class="reqn">a</code> as described in South et al (2020), where predictions are of the form <code class="reqn">f_hat = K0*a + Phi*b</code> for heldout K0 and Phi matrices and estimators using heldout samples are of the form <code class="reqn">mean(f - f_hat) + b[1]</code>.
</p>
</li>
<li> <p><code>b</code>: (Only if <code>diagnostics</code> = <code>TRUE</code>) The value of <code class="reqn">b</code> as described in South et al (2020), where predictions are of the form <code class="reqn">f_hat = K0*a + Phi*b</code> for heldout K0 and Phi matrices and estimators using heldout samples are of the form <code class="reqn">mean(f - f_hat) + b[1]</code>.
</p>
</li>
<li> <p><code>ksd</code>: (Only if <code>diagnostics</code> = <code>TRUE</code>) An estimated kernel Stein discrepancy based on the fitted model that can be used for diagnostic purposes. See South et al (2020) for further details.
</p>
</li>
<li> <p><code>bound_const</code>: (Only if <code>diagnostics</code> = <code>TRUE</code> and <code>est_inds</code>=<code>NULL</code>) This is such that the absolute error for the estimator should be less than <code class="reqn">ksd \times bound_const</code>.
</p>
</li></ul>



<h3>Warning</h3>

<p>Solving the linear system in SECF has <code class="reqn">O(N^3+Q^3)</code> complexity where <code class="reqn">N</code> is the sample size and <code class="reqn">Q</code> is the number of terms in the polynomial.
Standard SECF is therefore not suited to large <code class="reqn">N</code>. The method aSECF is designed for larger <code class="reqn">N</code> and details can be found at <code><a href="#topic+aSECF">aSECF</a></code> and in South et al (2020).
An alternative would be to use <code class="reqn">est_inds</code> which has <code class="reqn">O(N_0^3 + Q^3)</code> complexity in solving the linear system and <code class="reqn">O((N-N_0)^2)</code> complexity in
handling the remaining samples, where <code class="reqn">N_0</code> is the length of <code class="reqn">est_inds</code>. This can be much cheaper for large <code class="reqn">N</code> but the estimation of the
Gaussian process model is only done using <code class="reqn">N_0</code> samples and the evaluation of the integral only uses <code class="reqn">N-N_0</code> samples.
</p>


<h3>On the choice of <code class="reqn">\sigma</code>, the kernel and the Stein order</h3>

<p>The kernel in Stein-based kernel methods is <code class="reqn">L_x L_y k(x,y)</code> where <code class="reqn">L_x</code> is a first or second order Stein operator in <code class="reqn">x</code> and <code class="reqn">k(x,y)</code> is some generic kernel to be specified.
</p>
<p>The Stein operators for distribution <code class="reqn">p(x)</code> are defined as:
</p>

<ul>
<li> <p><strong><code>steinOrder=1</code></strong>: <code class="reqn">L_x g(x) = \nabla_x^T g(x) + \nabla_x \log p(x)^T g(x)</code> (see e.g. Oates el al (2017))
</p>
</li>
<li> <p><strong><code>steinOrder=2</code></strong>: <code class="reqn">L_x g(x) = \Delta_x g(x) + \nabla_x log p(x)^T \nabla_x g(x)</code> (see e.g. South el al (2020))
</p>
</li></ul>

<p>Here <code class="reqn">\nabla_x</code> is the first order derivative wrt <code class="reqn">x</code> and <code class="reqn">\Delta_x = \nabla_x^T \nabla_x</code> is the Laplacian operator.
</p>
<p>The generic kernels which are implemented in this package are listed below.  Note that the input parameter <strong><code>sigma</code></strong> defines the kernel parameters <code class="reqn">\sigma</code>. 
</p>

<ul>
<li> <p><strong><code>"gaussian"</code></strong>: A Gaussian kernel,
</p>
<p style="text-align: center;"><code class="reqn">k(x,y) = exp(-z(x,y)/\sigma^2)</code>
</p>

</li>
<li> <p><strong><code>"matern"</code></strong>: A Matern kernel with <code class="reqn">\sigma = (\lambda,\nu)</code>,
</p>
<p style="text-align: center;"><code class="reqn">k(x,y) = bc^{\nu}z(x,y)^{\nu/2}K_{\nu}(c z(x,y)^{0.5})</code>
</p>
<p> where <code class="reqn">b=2^{1-\nu}(\Gamma(\nu))^{-1}</code>, <code class="reqn">c=(2\nu)^{0.5}\lambda^{-1}</code> and <code class="reqn">K_{\nu}(x)</code> is the modified Bessel function of the second kind. Note that <code class="reqn">\lambda</code> is the length-scale parameter and <code class="reqn">\nu</code> is the smoothness parameter (which defaults to 2.5 for <code class="reqn">steinOrder=1</code> and 4.5 for <code class="reqn">steinOrder=2</code>).
</p>
</li>
<li> <p><strong><code>"RQ"</code></strong>: A rational quadratic kernel,
</p>
<p style="text-align: center;"><code class="reqn">k(x,y) = (1+\sigma^{-2}z(x,y))^{-1}</code>
</p>

</li>
<li> <p><strong><code>"product"</code></strong>: The product kernel that appears in Oates et al (2017) with <code class="reqn">\sigma = (a,b)</code>
</p>
<p style="text-align: center;"><code class="reqn">k(x,y) = (1+a z(x) + a z(y))^{-1} exp(-0.5 b^{-2} z(x,y)) </code>
</p>

</li>
<li> <p><strong><code>"prodsim"</code></strong>: A slightly different product kernel with <code class="reqn">\sigma = (a,b)</code> (see e.g. <a href="https://www.imperial.ac.uk/inference-group/projects/monte-carlo-methods/control-functionals/">https://www.imperial.ac.uk/inference-group/projects/monte-carlo-methods/control-functionals/</a>),
</p>
<p style="text-align: center;"><code class="reqn">k(x,y) = (1+a z(x))^{-1}(1 + a z(y))^{-1} exp(-0.5 b^{-2} z(x,y)) </code>
</p>

</li></ul>

<p>In the above equations, <code class="reqn">z(x) = \sum_j x[j]^2</code> and <code class="reqn">z(x,y) = \sum_j (x[j] - y[j])^2</code>. For the last two kernels, the code only has implementations for <code>steinOrder</code>=<code>1</code>. Each combination of <code>steinOrder</code> and <code>kernel_function</code> above is currently hard-coded but it may be possible to extend this to other kernels in future versions using autodiff. The calculations for the first three kernels above are detailed in South et al (2020).
</p>


<h3>Author(s)</h3>

<p>Leah F. South
</p>


<h3>References</h3>

<p>Oates, C. J., Girolami, M. &amp; Chopin, N. (2017). Control functionals for Monte Carlo integration. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 79(3), 695-718.
</p>
<p>South, L. F., Karvonen, T., Nemeth, C., Girolami, M. and Oates, C. J. (2020). Semi-Exact Control Functionals From Sard's Method.  <a href="https://arxiv.org/abs/2002.00033">https://arxiv.org/abs/2002.00033</a>
</p>


<h3>See Also</h3>

<p>See <a href="#topic+ZVCV">ZVCV</a> for examples and related functions. See <code><a href="#topic+SECF">SECF</a></code> for a function to perform semi-exact control functionals with fixed kernel specifications.
</p>

<hr>
<h2 id='squareNorm'>Squared norm matrix calculation</h2><span id='topic+squareNorm'></span>

<h3>Description</h3>

<p>This function gets the matrix of square norms which is needed for all kernels.
Calculating this can help to save time if you are also interested in calculating the median heuristic, handling multiple tuning parameters
or trying other kernels.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>squareNorm(samples, nystrom_inds = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="squareNorm_+3A_samples">samples</code></td>
<td>
<p>An <code class="reqn">N</code> by <code class="reqn">d</code> matrix of samples from the target</p>
</td></tr>
<tr><td><code id="squareNorm_+3A_nystrom_inds">nystrom_inds</code></td>
<td>
<p>The (optional) sample indices to be used in the Nystrom approximation (for when using aSECF).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An <code class="reqn">N</code> by <code class="reqn">N</code> matrix of squared norms between samples (or <code class="reqn">N</code> by <code class="reqn">m</code> where <code class="reqn">m</code> is the length of <code>nystrom_inds</code>).
</p>


<h3>Author(s)</h3>

<p>Leah F. South
</p>


<h3>See Also</h3>

<p>See <code><a href="#topic+medianTune">medianTune</a></code> and <code><a href="#topic+K0_fn">K0_fn</a></code> for functions which use this.
</p>

<hr>
<h2 id='VDP'>Example of estimation using SMC</h2><span id='topic+VDP'></span>

<h3>Description</h3>

<p>This example illustrates how ZV-CV can be used for post-processing of results from likelihood-annealing SMC. In particular, we use ZV-CV to estimate posterior expectations and the evidence for a single SMC run of this example based on the Van der Pol oscillatory differential equations (Van der Pol, 1926). Further details about this example and applications to ZV-CV can be found in Oates et al. (2017) and South et al. (2019).
</p>
<p>Given that the focus of this R package is on ZV-CV, we assume that samples have already been obtained from SMC and put into the correct format. One could use the R package <code>RcppSMC</code> or implement their own sampler in order to obtain results like this. The key is to make sure the derivatives of the log likelihood and log prior are stored, along with the inverse temperatures.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(VDP)
</code></pre>


<h3>Format</h3>

<p>A list containing the following :
</p>

<dl>
<dt>N</dt><dd><p>The size of the SMC population</p>
</dd>
<dt>rho</dt><dd><p>The tolerance for the new temperatures, which are selected so that the CESS at each temperature is <code class="reqn">\rho*N</code> where <code class="reqn">N</code> is the population size.</p>
</dd>
<dt>temperatures</dt><dd><p>A vector of length <code class="reqn">T</code> of inverse power posterior temperatures</p>
</dd>
<dt>samples</dt><dd><p>An <code class="reqn">N</code> by <code class="reqn">d</code> by <code class="reqn">T</code> matrix of samples from the <code class="reqn">T</code> power posteriors, where <code class="reqn">d</code> is the dimension of the target distribution. The samples are transformed to be on the log scale and all derivatives are with respect to log samples.</p>
</dd>
<dt>loglike</dt><dd><p>An <code class="reqn">N</code> by <code class="reqn">T</code> matrix of log likelihood values corresponding to <code>samples</code></p>
</dd>
<dt>logprior</dt><dd><p>An <code class="reqn">N</code> by <code class="reqn">T</code> matrix of log prior values corresponding to <code>samples</code></p>
</dd>
<dt>der_loglike</dt><dd><p>An <code class="reqn">N</code> by <code class="reqn">d</code> by <code class="reqn">T</code> matrix of the derivatives of the log likelihood with respect to the parameters, with parameter values corresponding to <code>samples</code></p>
</dd>
<dt>der_logprior</dt><dd><p> An <code class="reqn">N</code> by <code class="reqn">d</code> by <code class="reqn">T</code> matrix of the derivatives of the log prior with respect to the parameters, with parameter values corresponding to <code>samples</code></p>
</dd>
</dl>


<h3>References</h3>

 
<p>Oates, C. J., Girolami, M. &amp; Chopin, N. (2017). Control functionals for Monte Carlo integration. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 79(3), 695-718.
</p>
<p>South, L. F., Oates, C. J., Mira, A., &amp; Drovandi, C. (2019). Regularised zero-variance control variates for high-dimensional variance reduction.
</p>
<p>Van der Pol, B. (1926). On relaxation-oscillations. The London, Edinburgh and Dublin Philosophical Magazine and Journal of Science, 2(11), 978-992.
</p>


<h3>See Also</h3>

<p>See <a href="#topic+ZVCV">ZVCV</a> for more package details.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(1)

# Load the SMC results
data(VDP) 

# Set up the list of control variates to choose from 
options &lt;- list()
# Vanilla Monte Carlo
options[[1]] &lt;- list(polyorder = 0)
# Standard ZV-CV with polynomial order selected through cross-validation
options[[2]] &lt;- list(polyorder = Inf, regul_reg = FALSE)

##############################
# Posterior expectation - The true expectation is 0.9852 to 4 decimal places
##############################

# Note the exp() because samples and derivatives were stored on the log scale
# but we are interested in the expectation on the original scale
posterior &lt;- zvcv(exp(VDP$samples[,,8]), VDP$samples[,,8],
VDP$der_loglike[,,8] + VDP$der_logprior[,,8], options = options) 
posterior$expectation # The posterior expectation estimate
posterior$polyorder # The selected polynomial order

##############################
# Evidence estimation - The true logged evidence is 10.36 to 2 decimal places
##############################

# Getting additional temperatures based on maintaing a CESS of 0.91N rather than 0.9N.
# The value 0.91 is used for speed but South et al. (2019) use 0.99.
temp &lt;- Expand_Temperatures(VDP$temperatures, VDP$loglike, 0.91)
VDP$temperatures_new &lt;- temp$temperatures_all # the new temperatures
VDP$most_recent &lt;- temp$relevant_samples # the samples associated with the new temperatures

n_sigma &lt;- 3 # For speed, South et al. (2019) uses 15
sigma_list &lt;- as.list( 10^(0.5*seq(-3,4,length.out=n_sigma)) )

# Evidence estimation using the SMC identity
Z_SMC &lt;- evidence_SMC(VDP$samples, VDP$loglike, VDP$der_loglike, VDP$der_logprior,
VDP$temperatures, VDP$temperatures_new, VDP$most_recent, options = options)
Z_SMC$log_evidence

# Evidence estimation using the SMC identity
Z_SMC_CF &lt;- evidence_SMC_CF(VDP$samples, VDP$loglike, VDP$der_loglike, VDP$der_logprior,
VDP$temperatures, VDP$temperatures_new, VDP$most_recent, steinOrder = 2,
kernel_function = "gaussian", sigma_list = sigma_list, folds = 2)
Z_SMC_CF$log_evidence

# Evidence estimation using the CTI identity
Z_CTI &lt;- evidence_CTI(VDP$samples, VDP$loglike, VDP$der_loglike, VDP$der_logprior,
VDP$temperatures, VDP$temperatures_new, VDP$most_recent, options = options)
Z_CTI$log_evidence_PS2

# Evidence estimation using the CTI identity
Z_CTI_CF &lt;- evidence_CTI_CF(VDP$samples, VDP$loglike, VDP$der_loglike, VDP$der_logprior,
VDP$temperatures, VDP$temperatures_new, VDP$most_recent, steinOrder = 2,
kernel_function = "gaussian", sigma_list = sigma_list, folds = 2)
Z_CTI_CF$log_evidence_PS2
</code></pre>

<hr>
<h2 id='zvcv'>ZV-CV for general expectations</h2><span id='topic+zvcv'></span>

<h3>Description</h3>

<p>The function <code>zvcv</code> is used to perform (regularised) ZV-CV given a set of samples, derivatives and function evaluations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>zvcv(
  integrand,
  samples,
  derivatives,
  log_weights,
  integrand_logged = FALSE,
  est_inds,
  options = list(polyorder = 2, regul_reg = TRUE, alpha_elnet = 1, nfolds = 10, apriori
    = (1:NCOL(samples)), intercept = TRUE, polyorder_max = Inf),
  folds = 5
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="zvcv_+3A_integrand">integrand</code></td>
<td>
<p>An <code class="reqn">N</code> by <code class="reqn">k</code> matrix of integrands (evaluations of the functions of interest)</p>
</td></tr>
<tr><td><code id="zvcv_+3A_samples">samples</code></td>
<td>
<p>An <code class="reqn">N</code> by <code class="reqn">d</code> matrix of samples from the target</p>
</td></tr>
<tr><td><code id="zvcv_+3A_derivatives">derivatives</code></td>
<td>
<p>An <code class="reqn">N</code> by <code class="reqn">d</code> matrix of derivatives of the log target with respect to the parameters</p>
</td></tr>
<tr><td><code id="zvcv_+3A_log_weights">log_weights</code></td>
<td>
<p>(optional) A vector of length <code class="reqn">N</code> containing log weights of the samples. The default is equal weights.</p>
</td></tr>
<tr><td><code id="zvcv_+3A_integrand_logged">integrand_logged</code></td>
<td>
<p>(optional) Sometimes it is better to input the integrand on the logged scale for stability. If the actual integrand is the exponential of <code>integrand</code>, then <code>integrand_logged = TRUE</code>. Otherwise, the default of <code>integrand_logged = FALSE</code> should be used.</p>
</td></tr>
<tr><td><code id="zvcv_+3A_est_inds">est_inds</code></td>
<td>
<p>(optional) A vector of indices for the estimation-only samples. The default when <code>est_inds</code> is missing or <code>NULL</code> is to perform both estimation of the control variates and evaluation of the integral using all samples. Otherwise, the samples from <code>est_inds</code> are used in estimating the control variates and the remainder are used in evaluating the integral. Splitting the indices in this way can be used to reduce bias from adaption and to make computation feasible for very large sample sizes (small <code>est_inds</code> is faster), but in general in will increase the variance of the estimator.</p>
</td></tr>
<tr><td><code id="zvcv_+3A_options">options</code></td>
<td>
<p>A list of control variate specifications. This can be a single list containing the elements below (the defaults are used for elements which are not specified). Alternatively, it can be a list of lists containing any or all of the elements below. Where the latter is used, the function <code>zvcv</code> automatically selects the best performing option based on cross-validation. 
</p>

<ul>
<li> <p><code>polyorder</code>:   The order of the polynomial, with a default of 2. A value of <code>Inf</code> will get the cross-validation method to choose between orders.
</p>
</li>
<li> <p><code>regul_reg</code>:   A flag for whether regularised regression is to be used. The default is TRUE, i.e. regularised regression is used.
</p>
</li>
<li> <p><code>alpha_elnet</code>:   The alpha parameter for elastic net. The default is 1, which correponds to LASSO. A value of 0 would correspond to ridge regression.
</p>
</li>
<li> <p><code>nfolds</code>:   The number of folds used in cross-validation to select lambda for LASSO or elastic net. The default is 10.
</p>
</li>
<li> <p><code>apriori</code>:   A vector containing the subset of parameter indices to use in the polynomial. Typically this argument would only be used if the dimension of the problem is very large or if prior information about parameter dependencies is known. The default is to use all parameters <code class="reqn">1:d</code> where <code class="reqn">d</code> is the dimension of the target. In <code>zvcv</code>, this is equivalent to using only the relevant columns in <code>samples</code> and <code>derivatives</code>).
</p>
</li>
<li> <p><code>intercept</code>:   A flag for whether the intercept should be estimated or fixed to the empirical mean of the integrand in the estimation set. The default is to include an intercept (<code>intercept = TRUE</code>) as this tends to lead to better variance reductions. Note that an <code>intercept = TRUE</code> flag may be changed to <code>intercept = FALSE</code> within the function if <code>integrand_logged = TRUE</code> and a <code>NaN</code> is encountered. See South et al. (2018) for further details.
</p>
</li>
<li> <p><code>polyorder_max</code>:   The maximum allowable polynomial order. This may be used to prevent memory issues in the case that the polynomial order is selected automatically. A default maximum polynomial order based on the regression design matrix having no more than ten million elements will be selected if the <code>polyorder</code> is infinite and in this case a warning will be given. Recall that setting your default R settings to <code>options(warn=1)</code> will ensure that you receive these warnings in real time. Optimal polynomial order selection may go to at most this maximum value, or it may stop earlier. 
</p>
</li></ul>
</td></tr>
<tr><td><code id="zvcv_+3A_folds">folds</code></td>
<td>
<p>The number of folds used in k-fold cross-validation for selecting the optimal control variate. Depending on the <code>options</code>, this may include selection of the optimal polynomial order, regression type and subset of parameters in the polynomial. The default is five.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list is returned, containing the following components:
</p>

<ul>
<li> <p><code>expectation</code>: The estimates of the expectations.
</p>
</li>
<li> <p><code>num_select</code>: The number of non-zero coefficients in the polynomial.
</p>
</li>
<li> <p><code>mse</code>: The mean square error for the evaluation set.
</p>
</li>
<li> <p><code>coefs</code>: The estimated coefficients for the regression (columns are for the different integrands).
</p>
</li>
<li> <p><code>integrand_logged</code>: The <code>integrand_logged</code> input stored for reference.
</p>
</li>
<li> <p><code>est_inds</code>: The <code>est_inds</code> input stored for reference.
</p>
</li>
<li> <p><code>polyorder</code>: The <code>polyorder</code> value used in the final estimate.
</p>
</li>
<li> <p><code>regul_reg</code>: The <code>regul_reg</code> flag used in the final estimate.
</p>
</li>
<li> <p><code>alpha_elnet</code>: The <code>alpha_elnet</code> value used in the final estimate.
</p>
</li>
<li> <p><code>nfolds</code>: The <code>nfolds</code> value used in the final estimate.
</p>
</li>
<li> <p><code>apriori</code>:  The <code>apriori</code> vector used in the final estimate.
</p>
</li>
<li> <p><code>intercept</code>: The <code>intercept</code> flag used in the final estimate.
</p>
</li>
<li> <p><code>polyorder_max</code>: The <code>polyorder_max</code> flag used in the final estimate, if multiple <code>options</code> are specified.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Leah F. South
</p>


<h3>References</h3>

<p>Mira, A., Solgi, R., &amp; Imparato, D. (2013). Zero variance Markov chain Monte Carlo for Bayesian estimators. Statistics and Computing, 23(5), 653-662.
</p>
<p>South, L. F., Oates, C. J., Mira, A., &amp; Drovandi, C. (2019). Regularised zero variance control variates for high-dimensional variance reduction. <a href="https://arxiv.org/abs/1811.05073">https://arxiv.org/abs/1811.05073</a>
</p>


<h3>See Also</h3>

<p>See <a href="#topic+ZVCV">ZVCV</a> and <code><a href="#topic+VDP">VDP</a></code> for additional examples. See <code><a href="#topic+evidence">evidence</a></code> for functions which use <code>zvcv</code> to estimate the normalising constant of the posterior.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># An example where ZV-CV can result in zero-variance estimators

# Estimating some expectations when theta is bivariate normally distributed with:
mymean &lt;- c(-1.5,1.5)
mycov &lt;- matrix(c(1,0.5,0.5,2),nrow=2)

# Perfect draws from the target distribution (could be replaced with
# approximate draws from e.g. MCMC or SMC)
N &lt;- 30
require(mvtnorm)
set.seed(1)
samples &lt;- rmvnorm(N, mean = mymean, sigma = mycov)
# derivatives of Gaussian wrt x
derivatives &lt;- t( apply(samples,1,function(x) -solve(mycov)%*%(x - mymean)) )

# The integrands are the marginal posterior means of theta, the variances and the
# covariance (true values are c(-1.5,1.5,1,2,0.5))
integrand &lt;- cbind(samples[,1],samples[,2],(samples[,1] - mymean[1])^2,
    (samples[,2] - mymean[2])^2, (samples[,1] - mymean[1])*(samples[,2] - mymean[2]))

# Estimates without ZV-CV (i.e. vanilla Monte Carlo integration)
# Vanilla Monte Carlo
sprintf("%.15f",colMeans(integrand))

# ZV-CV with fixed specifications
# For this example, polyorder = 1 with OLS is exact for the first two integrands and
# polyorder = 2 with OLS is exact for the last three integrands

# ZV-CV with 2nd order polynomial, OLS and a polynomial based on only x_1.
# For diagonal mycov, this would be exact for the first and third expectations.
sprintf("%.15f",zvcv(integrand, samples, derivatives,
    options = list(polyorder = 2, regul_reg = FALSE, apriori = 1))$expectation)

# ZV-CV with 1st order polynomial and OLS (exact for the first two integrands)
sprintf("%.15f",zvcv(integrand, samples, derivatives,
    options = list(polyorder = 1, regul_reg = FALSE))$expectation)

# ZV-CV with 2nd order polynomial and OLS (exact for all)
sprintf("%.15f",zvcv(integrand, samples, derivatives,
    options = list(polyorder = 2, regul_reg = FALSE))$expectation) 

# ZV-CV with cross validation
myopts &lt;- list(list(polyorder = Inf, regul_reg = FALSE),list(polyorder = Inf, nfolds = 4)) 
temp &lt;- zvcv(integrand,samples,derivatives,options = myopts, folds = 2) 
temp$polyorder # The chosen control variate order
temp$regul_reg # Flag for if the chosen control variate uses regularisation
# Cross-val ZV-CV to choose the polynomial order and whether to perform OLS or LASSO
sprintf("%.15f",temp$expectation) # Estimate based on the chosen control variate


</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
