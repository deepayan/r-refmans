<!DOCTYPE html><html lang="en"><head><title>Help for package medflex</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {medflex}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#expData'><p>Expanded dataset</p></a></li>
<li><a href='#expData-methods'><p>Methods for expanded datasets</p></a></li>
<li><a href='#neImpute'><p>Expand the dataset and impute nested counterfactual outcomes</p></a></li>
<li><a href='#neImpute.default'><p>Expand the dataset and impute nested counterfactual outcomes</p></a></li>
<li><a href='#neImpute.formula'><p>Expand the dataset and impute nested counterfactual outcomes</p></a></li>
<li><a href='#neLht'><p>Linear hypotheses for natural effect models</p></a></li>
<li><a href='#neLht-methods'><p>Methods for linear hypotheses in natural effect models</p></a></li>
<li><a href='#neModel'><p>Fit a natural effect model</p></a></li>
<li><a href='#neModel-methods'><p>Methods for natural effect models</p></a></li>
<li><a href='#neWeight'><p>Expand the dataset and calculate ratio-of-mediator probability weights</p></a></li>
<li><a href='#neWeight.default'><p>Expand the dataset and calculate ratio-of-mediator probability weights</p></a></li>
<li><a href='#neWeight.formula'><p>Expand the dataset and calculate ratio-of-mediator probability weights</p></a></li>
<li><a href='#plot.neLht'><p>Confidence interval plots for linear hypotheses in natural effect models</p></a></li>
<li><a href='#plot.neModel'><p>Confidence interval plots for natural effect components</p></a></li>
<li><a href='#UPBdata'><p>UPB data</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Title:</td>
<td>Flexible Mediation Analysis Using Natural Effect Models</td>
</tr>
<tr>
<td>Version:</td>
<td>0.6-10</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-06-22</td>
</tr>
<tr>
<td>Description:</td>
<td>Run flexible mediation analyses using natural effect models as described in 
  Lange, Vansteelandt and Bekaert (2012) &lt;<a href="https://doi.org/10.1093%2Faje%2Fkwr525">doi:10.1093/aje/kwr525</a>&gt;, 
  Vansteelandt, Bekaert and Lange (2012) &lt;<a href="https://doi.org/10.1515%2F2161-962X.1014">doi:10.1515/2161-962X.1014</a>&gt; 
  and Loeys, Moerkerke, De Smet, Buysse, Steen and Vansteelandt (2013) &lt;<a href="https://doi.org/10.1080%2F00273171.2013.832132">doi:10.1080/00273171.2013.832132</a>&gt;.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.1.2), multcomp (&ge; 1.3-6)</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/jmpsteen/medflex">https://github.com/jmpsteen/medflex</a></td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>Imports:</td>
<td>boot (&ge; 1.3-8), car (&ge; 2.0-21), Matrix (&ge; 1.1-4), graphics
(&ge; 3.1.2), sandwich (&ge; 2.3-2), stats (&ge; 3.1.2), utils (&ge;
3.1.2)</td>
</tr>
<tr>
<td>Suggests:</td>
<td>arm (&ge; 1.7-05), gam (&ge; 1.09.1), glmnet (&ge; 1.9-8), mice (&ge;
2.22), mitools (&ge; 2.3), rpart (&ge; 4.1-8), SuperLearner (&ge;
2.0-15), VGAM (&ge; 1.0-0)</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>ByteCompile:</td>
<td>yes</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Author:</td>
<td>Johan Steen [aut, cre],
  Tom Loeys [aut],
  Beatrijs Moerkerke [aut],
  Stijn Vansteelandt [aut],
  Joris Meys [ctb] (technical support),
  Theis Lange [ctb] (valuable suggestions),
  Joscha Legewie [ctb],
  Paul Fink [ctb],
  Sanford Weisberg [ctb],
  Yves Rosseel [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Johan Steen &lt;johan.steen@gmail.com&gt;</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-06-22 13:35:26 UTC; jsteen</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-06-22 16:52:38 UTC</td>
</tr>
</table>
<hr>
<h2 id='expData'>Expanded dataset</h2><span id='topic+expData'></span>

<h3>Description</h3>

<p>Expanded dataset including either ratio-of-mediator probability weights or imputed nested counterfactual outcomes.
</p>


<h3>Value</h3>

<p>A data frame, resulting from applying <code><a href="#topic+neWeight">neWeight</a></code> or <code><a href="#topic+neImpute">neImpute</a></code> on an original dataset <code>data</code>.
This data frame has <code>nRep * length(data)</code> rows, containing all original variables (except the original exposure variable)
and two variables reflecting observed and hypothetical values of the exposure for each observation unit.
</p>
<p>These auxiliary variables (<em>x</em> and <em>x*</em>) are named after the exposure variable and carry integers as suffixes.
Suffixes <code>0</code> and <code>1</code> are used for variables whose corresponding parameters in the final natural effect model index natural direct and indirect effects, respectively.
</p>
<p>This object also stores some additional attributes, which are used as input for <code><a href="#topic+neModel">neModel</a></code>, such as
</p>
<table role = "presentation">
<tr><td><code>model</code></td>
<td>
<p>the fitted working model object</p>
</td></tr>
<tr><td><code>data</code></td>
<td>
<p>original dataset</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>the matched call</p>
</td></tr>
<tr><td><code>terms</code></td>
<td>
<p>the <code>neTerms</code> (internal class) object used</p>
</td></tr>
<tr><td><code>weights</code></td>
<td>
<p>ratio-of-mediator probability weights (only stored if object inherits from class <code>weightData</code>)</p>
</td></tr>
</table>


<h3>Note</h3>

<p>If the weighting-based approach (<code><a href="#topic+neWeight">neWeight</a></code>) is applied, the original outcome values are copied for the nested counterfactual outcomes
and the object stores an additional attribute, <code>"weights"</code>, containing a vector with ratio-of-mediator probability weights.
</p>
<p>If the imputation-based approach (<code><a href="#topic+neImpute">neImpute</a></code>) is applied, the nested counterfactual outcomes are imputed by predictions from the imputation model.
</p>
<p>In the former case, this object inherits from classes <code>c("data.frame", "expData", "impData")</code>, whereas in the latter case it inherits from classes <code>c("data.frame", "expData", "weightData")</code>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+neImpute">neImpute</a></code>, <code><a href="#topic+neWeight">neWeight</a></code>
</p>

<hr>
<h2 id='expData-methods'>Methods for expanded datasets</h2><span id='topic+expData-methods'></span><span id='topic+residuals.expData'></span><span id='topic+residualPlot.expData'></span><span id='topic+residualPlots.expData'></span><span id='topic+weights.expData'></span>

<h3>Description</h3>

<p>Regression weights, residuals and residual plots for expanded datasets.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'expData'
residuals(object, ...)

## S3 method for class 'expData'
residualPlot(model, ...)

## S3 method for class 'expData'
residualPlots(model, ...)

## S3 method for class 'expData'
weights(object, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="expData-methods_+3A_object">object</code></td>
<td>
<p>an expanded dataset (of class <code>"<a href="#topic+expData">expData</a>"</code>).</p>
</td></tr>
<tr><td><code id="expData-methods_+3A_...">...</code></td>
<td>
<p>additional arguments.</p>
</td></tr>
<tr><td><code id="expData-methods_+3A_model">model</code></td>
<td>
<p>an expanded dataset (of class <code>"<a href="#topic+expData">expData</a>"</code>) (for use with <code>residualPlot</code> and <code>residualPlots</code>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>weights</code> extracts regression weights (to be used in the natural effect model) for each observation of an expanded dataset.
</p>
<p><code>residuals</code> extracts residuals from the working model which is stored as an attribute of the expanded dataset. These can be used to assess normality of the residuals of the mediator working model when using the weighting-based approach (see example).
</p>
<p><code>residualPlot</code> and <code>residualPlots</code> are convenience functions from the <span class="pkg">car</span> package. These can be used to assess the adequacy of the working model.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+expData">expData</a></code>, <code><a href="#topic+neWeight">neWeight</a></code>, <code><a href="car.html#topic+residualPlot">residualPlot</a></code>, <code><a href="car.html#topic+residualPlots">residualPlots</a></code>, <code><a href="stats.html#topic+residuals">residuals</a></code>, <code><a href="stats.html#topic+weights">weights</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(UPBdata)

weightData &lt;- neWeight(negaff ~ att + gender + educ + age, 
                       data = UPBdata, nRep = 2)

## extract regression weights for natural effect model
head(weights(weightData)) 

## assess normality
qqnorm(residuals(weightData))

## assess model adequacy
library(car)
residualPlots(weightData)
</code></pre>

<hr>
<h2 id='neImpute'>Expand the dataset and impute nested counterfactual outcomes</h2><span id='topic+neImpute'></span>

<h3>Description</h3>

<p>This function both expands the data along hypothetical exposure values and imputes nested counterfactual outcomes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>neImpute(object, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="neImpute_+3A_object">object</code></td>
<td>
<p>an object used to select a method.</p>
</td></tr>
<tr><td><code id="neImpute_+3A_...">...</code></td>
<td>
<p>additional arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Generic function that both expands the data along hypothetical exposure values (for each observation unit <em>i</em>) and imputes nested counterfactual outcomes in this expanded dataset in a single run.
Imputed counterfactual outcomes 
</p>
<p style="text-align: center;"><code class="reqn">\hat E(Y_i \vert X_i = x, M_i, C_i)</code>
</p>

<p>are predictions from the imputation model that can be specified either externally as a fitted model object (<code><a href="#topic+neImpute.default">neImpute.default</a></code>)
or internally (<code><a href="#topic+neImpute.formula">neImpute.formula</a></code>).
</p>


<h3>Value</h3>

<p>A data frame of class <code>c("data.frame", "expData", "impData")</code>. See <code><a href="#topic+expData">expData</a></code> for its structure.
</p>


<h3>References</h3>

<p>Vansteelandt, S., Bekaert, M., &amp; Lange, T. (2012). Imputation Strategies for the Estimation of Natural Direct and Indirect Effects. <em>Epidemiologic Methods</em>, <b>1</b>(1), Article 7.
</p>
<p>Loeys, T., Moerkerke, B., De Smet, O., Buysse, A., Steen, J., &amp; Vansteelandt, S. (2013). Flexible Mediation Analysis in the Presence of Nonlinear Relations: Beyond the Mediation Formula. <em>Multivariate Behavioral Research</em>, <b>48</b>(6), 871-894.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+neImpute.default">neImpute.default</a></code>, <code><a href="#topic+neImpute.formula">neImpute.formula</a></code>, <code><a href="#topic+neModel">neModel</a></code>, <code><a href="#topic+expData">expData</a></code>
</p>

<hr>
<h2 id='neImpute.default'>Expand the dataset and impute nested counterfactual outcomes</h2><span id='topic+neImpute.default'></span>

<h3>Description</h3>

<p>This function both expands the data along hypothetical exposure values and imputes nested counterfactual outcomes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## Default S3 method:
neImpute(
  object,
  formula,
  data,
  nMed = 1,
  nRep = 5,
  xSampling = c("quantiles", "random"),
  xFit,
  percLim = c(0.05, 0.95),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="neImpute.default_+3A_object">object</code></td>
<td>
<p>fitted model object representing the imputation model.</p>
</td></tr>
<tr><td><code id="neImpute.default_+3A_formula">formula</code></td>
<td>
<p>a <code><a href="stats.html#topic+formula">formula</a></code> object providing a symbolic description of the imputation model. Redundant if already specified in call for fitted model specified in <code>object</code> (see details).</p>
</td></tr>
<tr><td><code id="neImpute.default_+3A_data">data</code></td>
<td>
<p>data, as matrix or data frame, containing the exposure (and other relevant) variables. Redundant if already specified in call for fitted model specified in <code>object</code> (see details).</p>
</td></tr>
<tr><td><code id="neImpute.default_+3A_nmed">nMed</code></td>
<td>
<p>number of mediators.</p>
</td></tr>
<tr><td><code id="neImpute.default_+3A_nrep">nRep</code></td>
<td>
<p>number of replications or hypothetical values of the exposure to sample for each observation unit.</p>
</td></tr>
<tr><td><code id="neImpute.default_+3A_xsampling">xSampling</code></td>
<td>
<p>character string indicating how to sample from the conditional exposure distribution.
Possible values are <code>"quantiles"</code> or <code>"random"</code> (see details).</p>
</td></tr>
<tr><td><code id="neImpute.default_+3A_xfit">xFit</code></td>
<td>
<p>an optional fitted object (preferably <code>glm</code>) for the conditional exposure distribution (see details).</p>
</td></tr>
<tr><td><code id="neImpute.default_+3A_perclim">percLim</code></td>
<td>
<p>a numerical vector of the form <code>c(lower, upper)</code> indicating the extreme percentiles to sample when using <code>"quantiles"</code> as sampling method to sample from the conditional exposure distribution (see details).</p>
</td></tr>
<tr><td><code id="neImpute.default_+3A_...">...</code></td>
<td>
<p>additional arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Imputed counterfactual outcomes are predictions from the imputation model that needs to be specified as a fitted object in the <code>object</code> argument.
</p>
<p>If the model-fitting function used to fit the imputation model does not require specification of a <code>formula</code> or <code>data</code> argument (when using e.g. <code><a href="SuperLearner.html#topic+SuperLearner">SuperLearner</a></code>),
these need to be specified explicitly in order to enable <code>neImpute.default</code> to extract pointers to variable types relevant for mediation analysis.
</p>
<p>Whether a <code><a href="stats.html#topic+formula">formula</a></code> is specified externally (in the call for the fitted imputation model object which is specified in <code>object</code>) or internally (via the <code>formula</code> argument),
it always needs to be of the form <code>Y ~ X + M1 + M2 + M3 + C1 + C2</code>, with the same outcome as in the final natural effect model and with predictor variables entered in the following prespecified order:
</p>

<ol>
<li><p> exposure <code>X</code>: The first predictor is coded as exposure or treatment.
</p>
</li>
<li><p> mediator(s) <code>M</code>: The second predictor is coded as mediator. In case of multiple mediators (<code>nMed &gt; 1</code>), then predictors <code>2:(nMed + 1)</code> are coded as mediators.
</p>
</li>
<li><p> baseline covariates <code>C</code>: All remaining predictor variables are automatically coded as baseline covariates.
</p>
</li></ol>

<p>It is important to adhere to this prespecified order to enable <code>neImpute</code> to create valid pointers to these different types of predictor variables.
This requirement extends to the use of operators different than the <code>+</code> operator, such as the <code>:</code> and <code>*</code> operators (when e.g. adding interaction terms). 
For instance, the formula specifications <code>Y ~ X * M + C1 + C2</code>, <code>Y ~ X + M + X:M + C1 + C2</code> and <code>Y ~ X + X:M + M + C1 + C2</code> will create identical pointers to the different types of variables,
as the order of the unique predictor variables is identical in all three specifications. 
</p>
<p>Furthermore, categorical exposures that are not coded as factors in the original dataset, should be specified as factors in the formula, 
using the <code><a href="base.html#topic+factor">factor</a></code> function, e.g. <code>Y ~ factor(X) + M + C1 + C2</code>. 
Quadratic or higher-order polynomial terms can be included as well, by making use of the <code><a href="base.html#topic+I">I</a></code> function or by using the <code><a href="stats.html#topic+poly">poly</a></code> function.
For instance, <code>Y ~ X + I(X^2) + M + C1 + C2</code> and <code>Y ~ poly(X, 2, raw = TRUE) + M + C1 + C2</code> are equivalent and result in identical pointers to the different types of variables.
</p>
<p>The command <code>terms(object, "vartype")</code> (with <code>object</code> replaced by the name of the resulting expanded dataset) can be used to check whether valid pointers have been created.
</p>
<p>If multiple mediators are specified (<code>nMed &gt; 1</code>), the natural indirect effect parameter in the natural effect model captures the joint mediated effect. That is, the effect of the exposure on the outcome via these mediators considered jointly. 
The remaining effect of the exposure on the outcome (not mediated through the specified mediators) is then captured by the natural indirect effect parameter.
</p>
<p>In contrast to imputation models with categorical exposures, additional arguments need to be specified if the exposure is continuous.
All of these additional arguments are related to the sampling procedure for the exposure.
</p>
<p>Whereas the number of replications <code>nRep</code> for categorical variables equals the number of levels for the exposure coded as a factor (i.e. the number of hypothetical exposure values), the number of desired replications needs to be specified explicitly for continuous exposures. 
Its default is 5.
</p>
<p>If <code>xFit</code> is left unspecified, the hypothetical exposure levels are automatically sampled from a linear model for the exposure, conditional on a linear combination of all covariates.
If one wishes to use another model for the exposure, this default model specification can be overruled by referring to a fitted model object in the <code>xFit</code> argument. 
Misspecification of this sampling model does not induce bias in the estimated coefficients and standard errors of the natural effect model.
</p>
<p>The <code>xSampling</code> argument allows to specify how the hypothetical exposure levels should be sampled from the conditional exposure distribution (which is either entered explicitly using the <code>xFit</code> argument or fitted automatically as described in the previous paragraph).
The <code>"random"</code> option randomly samples <code>nRep</code> draws from the exposure distribution, whereas the <code>"quantiles"</code> option (default) samples <code>nRep</code> quantiles at equal-sized probability intervals. Only the latter hence yields fixed exposure levels given <code>nRep</code> and <code>xFit</code>. <br /><br />
In order to guarantee that the entire support of the distribution is being sampled (which might be a concern if <code>nRep</code> is chosen to be small), the default lower and upper sampled quantiles are the 5th and 95th percentiles.
The intermittent quantiles correspond to equal-sized probability intervals. So, for instance, if <code>nRep = 4</code>, then the sampled quantiles will correspond to probabilities 0.05, 0.35, 0.65 and 0.95.
These default 'outer' quantiles can be changed by specifying the <code>percLim</code> argument accordingly. By specifying <code>percLim = NULL</code>, the standard quantiles will be sampled (e.g., 0.2, 0.4, 0.6 and 0.8 if <code>nRep = 4</code>).
</p>


<h3>Value</h3>

<p>A data frame of class <code>c("data.frame", "expData", "impData")</code>. See <code><a href="#topic+expData">expData</a></code> for its structure.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+neImpute">neImpute</a></code>, <code><a href="#topic+neImpute.formula">neImpute.formula</a></code>, <code><a href="#topic+neModel">neModel</a></code>, <code><a href="#topic+expData">expData</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(UPBdata)

## example using glm imputation model with binary exposure
fit.glm &lt;- glm(UPB ~ factor(attbin) + negaff + gender + educ + age, 
               family = binomial, data = UPBdata)
impData &lt;- neImpute(fit.glm)
head(impData)

## example using glm imputation model with continuous exposure
fit.glm &lt;- glm(UPB ~ att + negaff + gender + educ + age, 
               family = binomial, data = UPBdata)
impData &lt;- neImpute(fit.glm, nRep = 2)
head(impData)

## example using vglm (yielding identical results as with glm)
library(VGAM)
fit.vglm &lt;- vglm(UPB ~ att + negaff + gender + educ + age, 
                 family = binomialff, data = UPBdata)
impData2 &lt;- neImpute(fit.vglm, nRep = 2)
head(impData2)

## Not run: 
## example using SuperLearner
library(Matrix)
library(SuperLearner)
SL.library &lt;- c("SL.glm", "SL.glm.interaction", "SL.rpart",
                "SL.step", "SL.stepAIC", "SL.step.interaction",
                "SL.bayesglm", "SL.glmnet")
pred &lt;- c("att", "negaff", "gender", "educ", "age")
fit.SL &lt;- SuperLearner(Y = UPBdata$UPB, X = subset(UPBdata, select = pred),
                       SL.library = SL.library, family = binomial())
impSL &lt;- neImpute(fit.SL, 
                  formula = UPB ~ att + negaff + gender + educ + age, 
                  data = UPBdata)
head(impSL)

## End(Not run)


</code></pre>

<hr>
<h2 id='neImpute.formula'>Expand the dataset and impute nested counterfactual outcomes</h2><span id='topic+neImpute.formula'></span>

<h3>Description</h3>

<p>This function both expands the data along hypothetical exposure values and imputes nested counterfactual outcomes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'formula'
neImpute(
  object,
  family,
  data,
  FUN = glm,
  nMed = 1,
  nRep = 5,
  xSampling = c("quantiles", "random"),
  xFit,
  percLim = c(0.05, 0.95),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="neImpute.formula_+3A_object">object</code></td>
<td>
<p>a <code><a href="stats.html#topic+formula">formula</a></code> object providing a symbolic description of the imputation model (see details).</p>
</td></tr>
<tr><td><code id="neImpute.formula_+3A_family">family</code></td>
<td>
<p>a description of the error distribution and link function to be used in the model. Consult the help files of the model-fitting function specified in <code>FUN</code> for more details on appropriate argument specification.</p>
</td></tr>
<tr><td><code id="neImpute.formula_+3A_data">data</code></td>
<td>
<p>data, as matrix or data frame, containing the exposure (and other relevant) variables. Redundant if already specified in call for fitted model specified in <code>object</code> (see details).</p>
</td></tr>
<tr><td><code id="neImpute.formula_+3A_fun">FUN</code></td>
<td>
<p>function used to fit model specified in <code>formula</code>.</p>
</td></tr>
<tr><td><code id="neImpute.formula_+3A_nmed">nMed</code></td>
<td>
<p>number of mediators.</p>
</td></tr>
<tr><td><code id="neImpute.formula_+3A_nrep">nRep</code></td>
<td>
<p>number of replications or hypothetical values of the exposure to sample for each observation unit.</p>
</td></tr>
<tr><td><code id="neImpute.formula_+3A_xsampling">xSampling</code></td>
<td>
<p>character string indicating how to sample from the conditional exposure distribution.
Possible values are <code>"quantiles"</code> or <code>"random"</code> (see details).</p>
</td></tr>
<tr><td><code id="neImpute.formula_+3A_xfit">xFit</code></td>
<td>
<p>an optional fitted object (preferably <code>glm</code>) for the conditional exposure distribution (see details).</p>
</td></tr>
<tr><td><code id="neImpute.formula_+3A_perclim">percLim</code></td>
<td>
<p>a numerical vector of the form <code>c(lower, upper)</code> indicating the extreme percentiles to sample when using <code>"quantiles"</code> as sampling method to sample from the conditional exposure distribution (see details).</p>
</td></tr>
<tr><td><code id="neImpute.formula_+3A_...">...</code></td>
<td>
<p>additional arguments (passed to <code>FUN</code>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Imputed counterfactual outcomes are predictions from the imputation model that is fitted internally by extracting information from the arguments <code>object</code>, <code>family</code>, <code>data</code>, <code>FUN</code> and <code>...</code>.
</p>
<p>For imputation model specification via the <code>object</code> argument, use a <code><a href="stats.html#topic+formula">formula</a></code> of the form 
</p>
<p><code>Y ~ X + M1 + M2 + M3 + C1 + C2</code>,
</p>
<p>with the same outcome as in the final natural effect model and with predictor variables entered in the following prespecified order:
</p>

<ol>
<li><p> exposure <code>X</code>: The first predictor is coded as exposure or treatment.
</p>
</li>
<li><p> mediator(s) <code>M</code>: The second predictor is coded as mediator. In case of multiple mediators (<code>nMed &gt; 1</code>), then predictors <code>2:(nMed + 1)</code> are coded as mediators.
</p>
</li>
<li><p> baseline covariates <code>C</code>: All remaining predictor variables are automatically coded as baseline covariates.
</p>
</li></ol>

<p>It is important to adhere to this prespecified order to enable <code>neImpute</code> to create valid pointers to these different types of predictor variables.
This requirement extends to the use of operators different than the <code>+</code> operator, such as the <code>:</code> and <code>*</code> operators (when e.g. adding interaction terms). 
For instance, the formula specifications <code>Y ~ X * M + C1 + C2</code>, <code>Y ~ X + M + X:M + C1 + C2</code> and <code>Y ~ X + X:M + M + C1 + C2</code> will create identical pointers to the different types of variables,
as the order of the unique predictor variables is identical in all three specifications. 
</p>
<p>Furthermore, categorical exposures that are not coded as factors in the original dataset, should be specified as factors in the formula, 
using the <code><a href="base.html#topic+factor">factor</a></code> function, e.g. <code>Y ~ factor(X) + M + C1 + C2</code>. 
Quadratic or higher-order polynomial terms can be included as well, by making use of the <code><a href="base.html#topic+I">I</a></code> function or by using the <code><a href="stats.html#topic+poly">poly</a></code> function.
For instance, <code>Y ~ X + I(X^2) + M + C1 + C2</code> and <code>Y ~ poly(X, 2, raw = TRUE) + M + C1 + C2</code> are equivalent and result in identical pointers to the different types of variables.
</p>
<p>The command <code>terms(object, "vartype")</code> (with <code>object</code> replaced by the name of the resulting expanded dataset) can be used to check whether valid pointers have been created.
</p>
<p>If multiple mediators are specified (<code>nMed &gt; 1</code>), the natural indirect effect parameter in the natural effect model captures the joint mediated effect. That is, the effect of the exposure on the outcome via these mediators considered jointly. 
The remaining effect of the exposure on the outcome (not mediated through the specified mediators) is then captured by the natural indirect effect parameter.
</p>
<p>The type of imputation model can be defined by specifying an appropriate model-fitting function via the <code>FUN</code> argument (its default is <code><a href="stats.html#topic+glm">glm</a></code>).
This method can only be used with model-fitting functions that require a <code>formula</code> argument (so not when using e.g. <code><a href="SuperLearner.html#topic+SuperLearner">SuperLearner</a></code>).
</p>
<p>In contrast to imputation models with categorical exposures, additional arguments need to be specified if the exposure is continuous.
All of these additional arguments are related to the sampling procedure for the exposure.
</p>
<p>Whereas the number of replications <code>nRep</code> for categorical variables equals the number of levels for the exposure coded as a factor (i.e. the number of hypothetical exposure values), the number of desired replications needs to be specified explicitly for continuous exposures.
Its default is 5.
</p>
<p>If <code>xFit</code> is left unspecified, the hypothetical exposure levels are automatically sampled from a linear model for the exposure, conditional on a linear combination of all covariates.
If one wishes to use another model for the exposure, this default model specification can be overruled by referring to a fitted model object in the <code>xFit</code> argument.
Misspecification of this sampling model does not induce bias in the estimated coefficients and standard errors of the natural effect model.
</p>
<p>The <code>xSampling</code> argument allows to specify how the hypothetical exposure levels should be sampled from the conditional exposure distribution (which is either entered explicitly using the <code>xFit</code> argument or fitted automatically as described in the previous paragraph).
The <code>"random"</code> option randomly samples <code>nRep</code> draws from the exposure distribution, whereas the <code>"quantiles"</code> option (default) samples <code>nRep</code> quantiles at equal-sized probability intervals. Only the latter hence yields fixed exposure levels given <code>nRep</code> and <code>xFit</code>. <br /><br />
In order to guarantee that the entire support of the distribution is being sampled (which might be a concern if <code>nRep</code> is chosen to be small), the default lower and upper sampled quantiles are the 5th and 95th percentiles.
The intermittent quantiles correspond to equal-sized probability intervals. So, for instance, if <code>nRep = 4</code>, then the sampled quantiles will correspond to probabilities 0.05, 0.35, 0.65 and 0.95.
These default 'outer' quantiles can be changed by specifying the <code>percLim</code> argument accordingly. By specifying <code>percLim = NULL</code>, the standard quantiles will be sampled (e.g., 0.2, 0.4, 0.6 and 0.8 if <code>nRep = 4</code>).
</p>


<h3>Value</h3>

<p>A data frame of class <code>c("data.frame", "expData", "impData")</code>. See <code><a href="#topic+expData">expData</a></code> for its structure.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+neImpute">neImpute</a></code>, <code><a href="#topic+neImpute.default">neImpute.default</a></code>, <code><a href="#topic+neModel">neModel</a></code>, <code><a href="#topic+expData">expData</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(UPBdata)

## example using glm imputation model with binary exposure
impData &lt;- neImpute(UPB ~ factor(attbin) + negaff + gender + educ + age, 
                    family = binomial, data = UPBdata)
head(impData)

## example using glm imputation model with continuous exposure
impData &lt;- neImpute(UPB ~ att + negaff + gender + educ + age, 
                    family = binomial, data = UPBdata, nRep = 2)
head(impData)

## example using vglm (yielding identical results as with glm)
library(VGAM)
impData2 &lt;- neImpute(UPB ~ att + negaff + gender + educ + age, 
                     family = binomialff, data = UPBdata, 
                     nRep = 2, FUN = vglm)
head(impData2)
</code></pre>

<hr>
<h2 id='neLht'>Linear hypotheses for natural effect models</h2><span id='topic+neLht'></span><span id='topic+neEffdecomp'></span><span id='topic+neEffdecomp.neModel'></span><span id='topic+neLht.neModel'></span>

<h3>Description</h3>

<p><code>neLht</code> allows to calculate linear combinations of natural effect model parameter estimates.<br /> <code>neEffdecomp</code> automatically extracts relevant causal parameter estimates from a natural effect model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>neEffdecomp(model, xRef, covLev, ...)

## S3 method for class 'neModel'
neEffdecomp(model, xRef, covLev, ...)

neLht(model, ...)

## S3 method for class 'neModel'
neLht(model, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="neLht_+3A_model">model</code></td>
<td>
<p>a fitted natural effect model object.</p>
</td></tr>
<tr><td><code id="neLht_+3A_xref">xRef</code></td>
<td>
<p>a vector including reference levels for the exposure, <em>x*</em> and <em>x</em>, at which natural effect components need to be evaluated (see details).</p>
</td></tr>
<tr><td><code id="neLht_+3A_covlev">covLev</code></td>
<td>
<p>a vector including covariate levels at which natural effect components need to be evaluated (see details).</p>
</td></tr>
<tr><td><code id="neLht_+3A_...">...</code></td>
<td>
<p>additional arguments (passed to <code><a href="multcomp.html#topic+glht">glht</a></code>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>neLht</code> is a wrapper of <code><a href="multcomp.html#topic+glht">glht</a></code> and offers the same functionality (see &lsquo;Details&rsquo; section of  <code><a href="multcomp.html#topic+glht">glht</a></code> for details on argument specification). 
It returns objects that inherit from the class <code>"neLht"</code> in order to make output of their corresponding methods (see <code><a href="#topic+neLht-methods">neLht-methods</a></code>) more compatible for natural effect models
containing bootstrap variance-covariance matrices and standard errors.
</p>
<p><code>neEffdecomp</code> is a convenience function that automatically extracts causal parameter estimates from a natural effect model
and derives natural effect components.
That is, natural direct, natural indirect and total causal effect estimates are returned if no exposure-mediator interaction is modelled (i.e. two-way decomposition). 
If mediated interaction is allowed for in the natural effect model, there are two ways of decomposing the total effect into (natural) direct and indirect effects components: 
either as the sum of the pure direct and the total indirect effect or as the sum of the pure indirect and the total direct effect (i.e. three-way decomposition).
In total, five causal effect estimates are returned in this case.
</p>
<p>For continuous exposures, default exposure levels at which natural effect components are evaluated are <em>x*</em> = 0 and <em>x</em> = 1.
For multicategorical exposures, default levels are the reference level of the factor that encodes the exposure variable and the level corresponding to its first dummy variable for <em>x*</em> and <em>x</em>, respectively.  
If one wishes to evaluate natural effect components at different reference levels (e.g. if the natural effect model includes mediated interaction, quadratic or higher-order polynomial terms for the exposure; see examples), 
these can be specified as a vector of the form <code>c(x*,x)</code> via the <code>xRef</code> argument.
</p>
<p>If applicable, covariate levels at which natural effect components are evaluated can also be specified. This is particularly useful for natural effect models encoding effect modification by baseline covariates (e.g. moderated mediation).
By default, these levels are set to 0 for continuous covariates and to the reference level for categorical covariates coded as factors. 
Different covariate levels can be specified via the <code>covLev</code> argument, which requires a vector including valid levels for covariates that are specified in the natural effect model (or a subset of covariates that are specified as modifiers of either the natural direct or indirect effect or both).
Levels need to be preceded by the name of the corresponding covariate, e.g., <code>covLev = c(gender = "M", age = 30)</code>. Covariates for which the levels are left unspecified are set to their default levels (see examples). 
The <code><a href="base.html#topic+print">print</a></code> and <code><a href="#topic+summary.neLht">summary</a></code> functions for <code>neEffdecomp</code> objects return the covariate levels at which natural effect components are evaluated. 
No specific levels are returned for covariates that are not specified as modifier since effect decomposition is independent of the level of these covariates (see examples).
</p>


<h3>Value</h3>

<p>An object of class <code>c("neLht", "glht")</code> (see <code><a href="multcomp.html#topic+glht">glht</a></code>). 
If the bootstrap is used for obtaining standard errors when fitting the <code><a href="#topic+neModel">neModel</a></code> object, the returned object additionally inherits from class <code>"neLhtBoot"</code>. 
<code>neEffdecomp</code> returns an object that additionally inherits from class <code>"neEffdecomp"</code>.
</p>
<p>See <code><a href="#topic+neLht-methods">neLht-methods</a></code> for methods for <code>neLht</code> objects (and <code>glht-methods</code> for additional methods for <code>glht</code> objects).
</p>


<h3>Note</h3>

<p><code>neEffdecomp</code> is internally called by <code><a href="#topic+plot.neModel">plot.neModel</a></code> to create confidence interval plots for <code>neModel</code> objects.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+plot.neLht">plot.neLht</a></code>, <code><a href="#topic+neLht-methods">neLht-methods</a></code>, <code><a href="multcomp.html#topic+glht">glht</a></code>, <code>glht-methods</code>, <code><a href="#topic+neModel">neModel</a></code>, <code><a href="#topic+plot.neModel">plot.neModel</a></code>, <code><a href="base.html#topic+summary">summary</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(UPBdata)

impData &lt;- neImpute(UPB ~ att * negaff + gender + educ + age, 
                    family = binomial, data = UPBdata)
neMod &lt;- neModel(UPB ~ att0 * att1 + gender + educ + age, 
                 family = binomial, expData = impData, se = "robust")

lht &lt;- neLht(neMod, linfct = c("att0 = 0", "att0 + att0:att1 = 0", 
                               "att1 = 0", "att1 + att0:att1 = 0", 
                               "att0 + att1 + att0:att1 = 0"))
summary(lht)

## or obtain directly via neEffdecomp
eff &lt;- neEffdecomp(neMod)
summary(eff)

## changing reference levels for multicategorical exposures
UPBdata$attcat &lt;- factor(cut(UPBdata$att, 3), labels = c("L", "M", "H"))
impData &lt;- neImpute(UPB ~ attcat * negaff + gender + educ + age,
                    family = binomial, data = UPBdata)
neMod &lt;- neModel(UPB ~ attcat0 * attcat1 + gender + educ + age,
                 family = binomial, expData = impData, se = "robust")
                 
neEffdecomp(neMod)
neEffdecomp(neMod, xRef = c("L", "H"))
neEffdecomp(neMod, xRef = c("M", "H"))


## changing reference levels for continuous exposures
impData &lt;- neImpute(UPB ~ (att + I(att^2)) * negaff + gender + educ + age,
                    family = binomial, data = UPBdata)
neMod &lt;- neModel(UPB ~ (att0 + I(att0^2)) * (att1 + I(att1^2)) + gender + educ + age,
                 family = binomial, expData = impData, se = "robust")
neEffdecomp(neMod)
neEffdecomp(neMod, xRef = c(-1, 0))

## changing covariate levels when allowing for modification 
## of the indirect effect by baseline covariates
impData &lt;- neImpute(UPB ~ (att + negaff + gender + educ + age)^2,
                    family = binomial, data = UPBdata)
neMod &lt;- neModel(UPB ~ att0 * att1 + gender + educ + age + att1:gender + att1:age,
                 family = binomial, expData = impData, se = "robust")
neEffdecomp(neMod)
neEffdecomp(neMod, covLev = c(gender = "F", age = 0)) # default covariate levels
neEffdecomp(neMod, covLev = c(gender = "M", age = 40))
neEffdecomp(neMod, covLev = c(gender = "M", age = 40, educ = "L"))
neEffdecomp(neMod, covLev = c(gender = "M", age = 40, educ = "M"))
neEffdecomp(neMod, covLev = c(gender = "M", age = 40, educ = "H"))
# effect decomposition is independent of education level
neEffdecomp(neMod, covLev = c(gender = "M")) 
# age is set to its default level when left unspecified

</code></pre>

<hr>
<h2 id='neLht-methods'>Methods for linear hypotheses in natural effect models</h2><span id='topic+neLht-methods'></span><span id='topic+confint.neLhtBoot'></span><span id='topic+confint.neLht'></span><span id='topic+summary.neLht'></span>

<h3>Description</h3>

<p>Obtain confidence intervals and statistical tests for linear hypotheses in natural effect models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'neLhtBoot'
confint(object, parm, level = 0.95, type = "norm", ...)

## S3 method for class 'neLht'
confint(object, parm, level = 0.95, calpha = univariate_calpha(), ...)

## S3 method for class 'neLht'
summary(object, test = univariate(), ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="neLht-methods_+3A_object">object</code></td>
<td>
<p>an object of class <code>neLht</code>.</p>
</td></tr>
<tr><td><code id="neLht-methods_+3A_parm">parm</code></td>
<td>
<p>a specification of which parameters are to be given
confidence intervals, either a vector of numbers or a vector of
names.  If missing, all parameters are considered.</p>
</td></tr>
<tr><td><code id="neLht-methods_+3A_level">level</code></td>
<td>
<p>the confidence level required.</p>
</td></tr>
<tr><td><code id="neLht-methods_+3A_type">type</code></td>
<td>
<p>the type of bootstrap intervals required. The default <code>"norm"</code> returns normal approximation bootstrap confidence intervals. Currently, <code>"norm"</code>, <code>"basic"</code>, <code>"perc"</code> and <code>"bca"</code> are supported (see <code><a href="boot.html#topic+boot.ci">boot.ci</a></code>).</p>
</td></tr>
<tr><td><code id="neLht-methods_+3A_...">...</code></td>
<td>
<p>additional arguments.</p>
</td></tr>
<tr><td><code id="neLht-methods_+3A_calpha">calpha</code></td>
<td>
<p>a function computing the critical value. The default <code>univariate_calpha()</code> returns unadjusted confidence intervals, whereas <code>adjusted_calpha()</code> returns adjusted confidence intervals.</p>
</td></tr>
<tr><td><code id="neLht-methods_+3A_test">test</code></td>
<td>
<p>a function for computing p-values. The default <code>univariate()</code> does not apply a multiple testing correction. The function <code>adjusted()</code> allows to correct for multiple testing (see <code><a href="multcomp.html#topic+summary.glht">summary.glht</a></code> and <code><a href="multcomp.html#topic+adjusted">adjusted</a></code>) and <code>Chisquare()</code> allows to test global linear hypotheses.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>confint</code> yields bootstrap confidence intervals  or confidence intervals based on the sandwich estimator (depending on the type of standard errors requested when fitting the <code><a href="#topic+neModel">neModel</a></code> object). 
Bootstrap confidence intervals are internally called via the <code><a href="boot.html#topic+boot.ci">boot.ci</a></code> function from the <span class="pkg">boot</span> package.
Confidence intervals based on the sandwich estimator are internally called via the corresponding <code><a href="multcomp.html#topic+confint.glht">confint.glht</a></code> function from the <span class="pkg">multcomp</span> package.
The default confidence level specified in <code>level</code> (which corresponds to the <code>conf</code> argument in <code><a href="boot.html#topic+boot.ci">boot.ci</a></code>) is 0.95
and the default type of bootstrap confidence interval, <code>"norm"</code>, is based on the normal approximation.
Bias-corrected and accelerated (<code>"bca"</code>) bootstrap confidence intervals require a sufficiently large number of bootstrap replicates (for more details see <code><a href="boot.html#topic+boot.ci">boot.ci</a></code>).
</p>
<p>A summary table with large sample tests, similar to that for <code><a href="multcomp.html#topic+glht">glht</a></code>, can be obtained using <code>summary</code>.
</p>
<p>In contrast to <code><a href="multcomp.html#topic+summary.glht">summary.glht</a></code>, which by default returns <em>p</em>-values that are adjusted for multiple testing,
the summary function returns unadjusted <em>p</em>-values. Adjusted <em>p</em>-values can also be obtained by specifying the <code>test</code> argument 
(see <code><a href="multcomp.html#topic+adjusted">adjusted</a></code> for more details).
</p>
<p>Global Wald tests considering all linear hypotheses simultaneously (i.e. testing the global null hypothesis) 
can be requested by specifying <code>test = Chisqtest()</code>.
</p>
<p>See <code>glht-methods</code> for additional methods for <code>glht</code> objects.
</p>


<h3>Note</h3>

<p>For the bootstrap, <em>z</em>-values in the summary table are simply calculated by dividing the parameter estimate by its corresponding bootstrap standard error. 
Corresponding <em>p</em>-values in the summary table are only indicative, since the null distribution for each statistic is assumed to be approximately standard normal.
Therefore, whenever possible, it is recommended to focus mainly on bootstrap confidence intervals for inference, rather than the provided <em>p</em>-values.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+neLht">neLht</a></code>, <code><a href="#topic+plot.neLht">plot.neLht</a></code>, <code><a href="multcomp.html#topic+glht">glht</a></code>, <code>glht-methods</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(UPBdata)

impData &lt;- neImpute(UPB ~ att * negaff + gender + educ + age, 
                    family = binomial, data = UPBdata)
neMod &lt;- neModel(UPB ~ att0 * att1 + gender + educ + age, 
                 family = binomial, expData = impData, se = "robust")

lht &lt;- neLht(neMod, linfct = c("att0 = 0", "att0 + att0:att1 = 0", 
                               "att1 = 0", "att1 + att0:att1 = 0", 
                               "att0 + att1 + att0:att1 = 0"))

## obtain confidence intervals
confint(lht)
confint(lht, parm = c("att0", "att0 + att0:att1"))
confint(lht, parm = 1:2, level = 0.90)

## summary table
summary(lht)

## summary table with omnibus Chisquare test
summary(lht, test = Chisqtest())
</code></pre>

<hr>
<h2 id='neModel'>Fit a natural effect model</h2><span id='topic+neModel'></span>

<h3>Description</h3>

<p><code>neModel</code> is used to fit a natural effect model on the expanded dataset.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>neModel(
  formula,
  family = gaussian,
  expData,
  xFit,
  se = c("bootstrap", "robust"),
  nBoot = 1000,
  parallel = c("no", "multicore", "snow"),
  ncpus = getOption("boot.ncpus", 1L),
  progress = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="neModel_+3A_formula">formula</code></td>
<td>
<p>a <code><a href="stats.html#topic+formula">formula</a></code> object providing a symbolic description of the natural effect model.</p>
</td></tr>
<tr><td><code id="neModel_+3A_family">family</code></td>
<td>
<p>a description of the error distribution and link
function to be used in the model.  For <code>glm</code> this can be a
character string naming a family function, a family function or the
result of a call to a family function.  For <code>glm.fit</code> only the
third option is supported.  (See <code><a href="stats.html#topic+family">family</a></code> for details of
family functions.)</p>
</td></tr>
<tr><td><code id="neModel_+3A_expdata">expData</code></td>
<td>
<p>the expanded dataset (of class <code>"<a href="#topic+expData">expData</a>"</code>).</p>
</td></tr>
<tr><td><code id="neModel_+3A_xfit">xFit</code></td>
<td>
<p>fitted model object representing a model for the exposure (used for inverse treatment (exposure) probability weighting).</p>
</td></tr>
<tr><td><code id="neModel_+3A_se">se</code></td>
<td>
<p>character string indicating the type of standard errors to be calculated. The default type is based on the bootstrap (see details).</p>
</td></tr>
<tr><td><code id="neModel_+3A_nboot">nBoot</code></td>
<td>
<p>number of bootstrap replicates (see <code>R</code> argument of <code><a href="boot.html#topic+boot">boot</a></code>).</p>
</td></tr>
<tr><td><code id="neModel_+3A_parallel">parallel</code></td>
<td>
<p>(only for bootstrap) The type of parallel operation to be used (if any). If missing, the default is taken from the option <code>"boot.parallel"</code> (and if that is not set, <code>"no"</code>).</p>
</td></tr>
<tr><td><code id="neModel_+3A_ncpus">ncpus</code></td>
<td>
<p>(only for bootstrap) integer: number of processes to be used in parallel operation: typically one would chose this to the number of available CPUs (see details).</p>
</td></tr>
<tr><td><code id="neModel_+3A_progress">progress</code></td>
<td>
<p>(only for bootstrap) logical value indicating whether or not a progress bar should be displayed. Progress bars are automatically disabled for multicore processing.</p>
</td></tr>
<tr><td><code id="neModel_+3A_...">...</code></td>
<td>
<p>additional arguments (passed to <code><a href="stats.html#topic+glm">glm</a></code>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is a wrapper for <code><a href="stats.html#topic+glm">glm</a></code>, providing unbiased bootstrap (<code>se = "bootstrap"</code>, the default) or robust (<code>se = "robust"</code>) standard errors for the parameter estimates (see below for more details).
</p>
<p>The <code>formula</code> argument requires to be specified in function of the variables from the expanded dataset (specified in <code>expData</code>) whose corresponding parameters index the direct and indirect effect.
Stratum-specific natural effects can be estimated by additionally modeling the relation between the outcome and baseline covariates.
If the set of baseline covariates adjusted for in the <code>formula</code> argument is not sufficient to control for confounding (e.g. when fitting a population-average natural effect model),
an adequate model for the exposure (conditioning on a sufficient set of baseline covariates) should be specified in the <code>xFit</code> argument.
In this case, such a model for the exposure distribution is needed to weight by the reciprocal of the probability (density) of the exposure (i.e. inverse probability weighting) in order to adjust for confounding.
Just as for ratio-of-mediator probability weighting (see paragraph below), this kind of weighting is done internally.
</p>
<p>Quadratic or higher-order polynomial terms can be included in the <code>formula</code> by making use of the <code><a href="base.html#topic+I">I</a></code> function or by using the <code><a href="stats.html#topic+poly">poly</a></code> function.
However, we do not recommend the use of orthogonal polynomials (i.e. using the default argument specification <code>raw = FALSE</code> in <code>poly</code>), as these are not compatible with the <code><a href="#topic+neEffdecomp">neEffdecomp</a></code> function.
</p>
<p>In contrast to <code><a href="stats.html#topic+glm">glm</a></code>, the <code>expData</code> argument (rather than <code>data</code> argument) requires specification of a data frame that inherits from class <code>"<a href="#topic+expData">expData</a>"</code>,
which contains additional information about e.g. the fitted working model, the variable types or terms of this working model 
and possibly ratio-of-mediator probability weights.
The latter are automatically extracted from the <code><a href="#topic+expData">expData</a></code> object and weighting is done internally.
</p>
<p>As the default <code><a href="stats.html#topic+glm">glm</a></code> standard errors fail to reflect the uncertainty inherent to the working model(s) (i.e. either a model for the mediator or an imputation model for the outcome and possibly a model for the exposure),
bootstrap standard errors (using the <code><a href="boot.html#topic+boot">boot</a></code> function from the <span class="pkg">boot</span> package) or robust standard errors are calculated. The default type of standard errors is bootstrap standard errors. 
Robust standard errors (based on the sandwich estimator) can be requested (to be calculated) instead by specifying <code>se = "robust"</code>.
</p>


<h3>Value</h3>

<p>An object of class <code>"<a href="#topic+neModel-methods">neModel</a>"</code> (which additionally inherits from class <code>"neModelBoot"</code> if the bootstrap is used) consisting of a list of 3 objects:
</p>
<table role = "presentation">
<tr><td><code>neModelFit</code></td>
<td>
<p>the fitted natural model object (of class <code>"<a href="stats.html#topic+glm">glm</a>"</code>) with downwardly biased standard errors</p>
</td></tr>
<tr><td><code>bootRes</code>, <code>vcov</code></td>
<td>
<p>the bootstrap results (of class <code>"<a href="boot.html#topic+boot">boot</a>"</code>; if <code>se = "bootstrap"</code>) or the robust variance-covariance matrix (if <code>se = "robust"</code>)</p>
</td></tr>
<tr><td><code>terms</code></td>
<td>
<p>the <code>neTerms</code> (internal class) object used. This object is equivalent to the <code><a href="stats.html#topic+terms.object">terms</a></code> object returned by the <code><a href="stats.html#topic+glm">glm</a></code> function, 
but has an additional <code>"vartype"</code> attribute, a list including pointers to the names of the outcome variable (<code>Y</code>), exposure (<code>X</code>), mediator (<code>M</code>), covariates (<code>C</code>) and auxiliary hypothetical variables <em>x</em> and <em>x*</em> (<code>Xexp</code>).</p>
</td></tr>
</table>
<p>See <code><a href="#topic+neModel-methods">neModel-methods</a></code> for methods for <code>neModel</code> objects.
</p>


<h3>Bootstrap standard errors</h3>

<p>The bootstrap procedure entails refitting all working models on each bootstrap sample, reconstructing the expanded dataset and subsequently refitting the specified natural effect model on this dataset.
In order to obtain stable standard errors, the number of bootstrap samples (specified via the <code>nBoot</code> argument) should be chosen relatively high (default is 1000).
</p>
<p>To speed up the bootstrap procedure, parallel processing can be used by specifying the desired type of parallel operation via the <code>parallel</code> argument (for more details, see <code><a href="boot.html#topic+boot">boot</a></code>).
The number of parallel processes (<code>ncpus</code>) is suggested to be specified explicitly (its default is 1, unless the global option <code>options("boot.cpus")</code> is specified). 
The function <code><a href="parallel.html#topic+detectCores">detectCores</a></code> from the <span class="pkg">parallel</span> package can be helpful at determining the number of available cores (although this may not always correspond to the number of <em>allowed</em> cores).
</p>


<h3>Robust standard errors</h3>

<p>Robust variance-covariance matrices for the model parameters, based on the sandwich estimator, are calculated using core functions from the <span class="pkg">sandwich</span> package.
Additional details and derivations for the sandwich estimator for natural effect models can be found in the corresponding vignette that can be obtained by the command <code>vignette("sandwich", package = "medflex")</code>.
</p>


<h3>Note</h3>

<p>It is important to note that the original mediator(s) should not be specified in the <code>formula</code> argument, as the natural indirect effect in natural effect models
should be captured solely by parameter(s) corresponding to the auxiliary hypothetical variable <em>x*</em> in the expanded dataset (see <code><a href="#topic+expData">expData</a></code>).
</p>


<h3>References</h3>

<p>Lange, T., Vansteelandt, S., &amp; Bekaert, M. (2012). A Simple Unified Approach for Estimating Natural Direct and Indirect Effects. <em>American Journal of Epidemiology</em>, <b>176</b>(3), 190-195.
</p>
<p>Vansteelandt, S., Bekaert, M., &amp; Lange, T. (2012). Imputation Strategies for the Estimation of Natural Direct and Indirect Effects. <em>Epidemiologic Methods</em>, <b>1</b>(1), Article 7.
</p>
<p>Loeys, T., Moerkerke, B., De Smet, O., Buysse, A., Steen, J., &amp; Vansteelandt, S. (2013). Flexible Mediation Analysis in the Presence of Nonlinear Relations: Beyond the Mediation Formula. <em>Multivariate Behavioral Research</em>, <b>48</b>(6), 871-894.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+neModel-methods">neModel-methods</a></code>, <code><a href="#topic+plot.neModel">plot.neModel</a></code>, <code><a href="#topic+neImpute">neImpute</a></code>, <code><a href="#topic+neWeight">neWeight</a></code>, <code><a href="#topic+neLht">neLht</a></code>, <code><a href="#topic+neEffdecomp">neEffdecomp</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(UPBdata)

##############################
## weighting-based approach ##
##############################
weightData &lt;- neWeight(negaff ~ att + gender + educ + age, 
                       data = UPBdata)

## stratum-specific natural effects
# bootstrap SE
## Not run: 
weightFit1b &lt;- neModel(UPB ~ att0 * att1 + gender + educ + age, 
                       family = binomial, expData = weightData)
summary(weightFit1b)

## End(Not run)
# robust SE
weightFit1r &lt;- neModel(UPB ~ att0 * att1 + gender + educ + age, 
                       family = binomial, expData = weightData, se = "robust")
summary(weightFit1r)

## population-average natural effects
expFit &lt;- glm(att ~ gender + educ + age, data = UPBdata)
# bootstrap SE
## Not run: 
weightFit2b &lt;- neModel(UPB ~ att0 * att1, family = binomial, 
                       expData = weightData, xFit = expFit)
summary(weightFit2b)

## End(Not run)
# robust SE
weightFit2r &lt;- neModel(UPB ~ att0 * att1, family = binomial, 
                       expData = weightData, xFit = expFit, se = "robust")
summary(weightFit2r)

###############################
## imputation-based approach ##
###############################
impData &lt;- neImpute(UPB ~ att * negaff + gender + educ + age, 
                    family = binomial, data = UPBdata)

## stratum-specific natural effects
# bootstrap SE
## Not run: 
impFit1b &lt;- neModel(UPB ~ att0 * att1 + gender + educ + age, 
                    family = binomial, expData = impData)
summary(impFit1b)

## End(Not run)
# robust SE
impFit1r &lt;- neModel(UPB ~ att0 * att1 + gender + educ + age, 
                    family = binomial, expData = impData, se = "robust")
summary(impFit1r)

## population-average natural effects
# bootstrap SE
## Not run: 
impFit2b &lt;- neModel(UPB ~ att0 * att1, family = binomial, 
                    expData = impData, xFit = expFit)
summary(impFit2b)

## End(Not run)
# robust SE
impFit2r &lt;- neModel(UPB ~ att0 * att1, family = binomial, 
                    expData = impData, xFit = expFit, se = "robust")
summary(impFit2r)


</code></pre>

<hr>
<h2 id='neModel-methods'>Methods for natural effect models</h2><span id='topic+neModel-methods'></span><span id='topic+coef.neModel'></span><span id='topic+confint.neModelBoot'></span><span id='topic+confint.neModel'></span><span id='topic+residualPlot.neModel'></span><span id='topic+residualPlots.neModel'></span><span id='topic+summary.neModel'></span><span id='topic+vcov.neModel'></span><span id='topic+weights.neModel'></span>

<h3>Description</h3>

<p>Extractor functions, confidence intervals, residual plots and statistical tests for natural effect models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'neModel'
coef(object, ...)

## S3 method for class 'neModelBoot'
confint(object, parm, level = 0.95, type = "norm", ...)

## S3 method for class 'neModel'
confint(object, parm, level = 0.95, ...)

## S3 method for class 'neModel'
residualPlot(model, ...)

## S3 method for class 'neModel'
residualPlots(model, ...)

## S3 method for class 'neModel'
summary(object, ...)

## S3 method for class 'neModel'
vcov(object, ...)

## S3 method for class 'neModel'
weights(object, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="neModel-methods_+3A_object">object</code></td>
<td>
<p>a fitted natural effect model object.</p>
</td></tr>
<tr><td><code id="neModel-methods_+3A_...">...</code></td>
<td>
<p>additional arguments.</p>
</td></tr>
<tr><td><code id="neModel-methods_+3A_parm">parm</code></td>
<td>
<p>a specification of which parameters are to be given
confidence intervals, either a vector of numbers or a vector of
names.  If missing, all parameters are considered.</p>
</td></tr>
<tr><td><code id="neModel-methods_+3A_level">level</code></td>
<td>
<p>the confidence level required.</p>
</td></tr>
<tr><td><code id="neModel-methods_+3A_type">type</code></td>
<td>
<p>the type of bootstrap intervals required. The default <code>"norm"</code> returns normal approximation bootstrap confidence intervals. Currently, <code>"norm"</code>, <code>"basic"</code>, <code>"perc"</code> and <code>"bca"</code> are supported (see <code><a href="boot.html#topic+boot.ci">boot.ci</a></code>).</p>
</td></tr>
<tr><td><code id="neModel-methods_+3A_model">model</code></td>
<td>
<p>a fitted natural effect model object (for use with <code>residualPlot</code> and <code>residualPlots</code>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>confint</code> yields bootstrap confidence intervals or confidence intervals based on the sandwich estimator (depending on the type of standard errors requested when fitting the <code><a href="#topic+neModel">neModel</a></code> object). 
Bootstrap confidence intervals are internally called via the <code><a href="boot.html#topic+boot.ci">boot.ci</a></code> function from the <span class="pkg">boot</span> package.
Confidence intervals based on the sandwich estimator are internally called via <code><a href="stats.html#topic+confint.default">confint.default</a></code>.
The default confidence level specified in <code>level</code> (which corresponds to the <code>conf</code> argument in <code><a href="boot.html#topic+boot.ci">boot.ci</a></code>) is 0.95
and the default type of bootstrap confidence interval, <code>"norm"</code>, is based on the normal approximation.
Bias-corrected and accelerated (<code>"bca"</code>) bootstrap confidence intervals require a sufficiently large number of bootstrap replicates (for more details see <code><a href="boot.html#topic+boot.ci">boot.ci</a></code>).
</p>
<p>A summary table with large sample tests, similar to that for <code><a href="stats.html#topic+glm">glm</a></code> output, can be obtained using <code>summary</code>.
</p>
<p><code>vcov</code> returns either the bootstrap variance-covariance matrix (calculated from the bootstrap samples stored in<br /> <code>object$bootRes</code>; see <code><a href="#topic+neModel">neModel</a></code>)
or the robust variance-covariance matrix (which is a diagonal block matrix of the original sandwich covariance matrix).
</p>
<p><code>weights</code> returns a vector containing the regression weights used to fit the natural effect model.
These weights can be based on
</p>

<ol>
<li><p> ratio-of-mediator probability (density) weights (only if the weighting-based approach is used)
</p>
</li>
<li><p> inverse probability of treatment (exposure) weights (only if <code>xFit</code> was specified in <code><a href="#topic+neModel">neModel</a></code>)
</p>
</li></ol>

<p><code>residualPlot</code> and <code>residualPlots</code> are convenience functions from the <span class="pkg">car</span> package. These can be used to assess model adequacy.
</p>


<h3>Note</h3>

<p>For the bootstrap, <em>z</em>-values in the summary table are calculated by dividing the parameter estimate by its corresponding bootstrap standard error. 
Corresponding <em>p</em>-values in the summary table are indicative, since the null distribution for each statistic is assumed to be approximately standard normal.
Therefore, whenever possible, it is recommended to focus mainly on bootstrap confidence intervals for inference, rather than the provided <em>p</em>-values.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+neModel">neModel</a></code>, <code><a href="#topic+plot.neModel">plot.neModel</a></code>, <code><a href="car.html#topic+residualPlot">residualPlot</a></code>, <code><a href="car.html#topic+residualPlots">residualPlots</a></code>, <code><a href="stats.html#topic+weights">weights</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(UPBdata)

weightData &lt;- neWeight(negaff ~ att + educ + gender + age,
                       data = UPBdata)
neMod &lt;- neModel(UPB ~ att0 * att1 + educ + gender + age, 
                 family = binomial, expData = weightData, se = "robust")

## extract coefficients
coef(neMod)

## extract variance-covariance matrix
vcov(neMod)

## extract regression weights
w &lt;- weights(neMod)
head(w)

## obtain bootstrap confidence intervals
confint(neMod)
confint(neMod, parm = c("att0"))
confint(neMod, type = "perc", level = 0.90)

## summary table
summary(neMod)

## residual plots
library(car)
residualPlots(neMod)
</code></pre>

<hr>
<h2 id='neWeight'>Expand the dataset and calculate ratio-of-mediator probability weights</h2><span id='topic+neWeight'></span>

<h3>Description</h3>

<p>This function both expands the data along hypothetical exposure values and calculates ratio-of-mediator probability weights.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>neWeight(object, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="neWeight_+3A_object">object</code></td>
<td>
<p>an object used to select a method.</p>
</td></tr>
<tr><td><code id="neWeight_+3A_...">...</code></td>
<td>
<p>additional arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Generic function that both expands the data along hypothetical exposure values and
calculates ratio-of-mediator probability weights 
</p>
<p style="text-align: center;"><code class="reqn">\frac{\hat P(M_i \vert X_i = x^*, C_i)}{\hat P(M_i \vert X_i = x, C_i)}</code>
</p>

<p>for each observation unit <em>i</em> in this expanded dataset in a single run.
These weights are ratios of probabilities or probability densities from the mediator model distribution, which can be specified either externally as a fitted model object (<code><a href="#topic+neWeight.default">neWeight.default</a></code>)
or internally (<code><a href="#topic+neWeight.formula">neWeight.formula</a></code>).
</p>


<h3>Value</h3>

<p>A data frame of class <code>c("data.frame", "expData", "weightData")</code>. See <code><a href="#topic+expData">expData</a></code> for its structure.
</p>


<h3>References</h3>

<p>Hong, G. (2010). Ratio of mediator probability weighting for estimating natural direct and indirect effects. In <em>Proceedings of the American Statistical Association, Biometrics Section</em>, pp. 2401-2415. American Statistical Association, Alexandria, VA.
</p>
<p>Lange, T., Vansteelandt, S., &amp; Bekaert, M. (2012). A Simple Unified Approach for Estimating Natural Direct and Indirect Effects. <em>American Journal of Epidemiology</em>, <b>176</b>(3), 190-195.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+neWeight.default">neWeight.default</a></code>, <code><a href="#topic+neWeight.formula">neWeight.formula</a></code>, <code><a href="#topic+expData">expData</a></code>
</p>

<hr>
<h2 id='neWeight.default'>Expand the dataset and calculate ratio-of-mediator probability weights</h2><span id='topic+neWeight.default'></span>

<h3>Description</h3>

<p>This function both expands the data along hypothetical exposure values and calculates ratio-of-mediator probability weights.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## Default S3 method:
neWeight(
  object,
  formula,
  data,
  nRep = 5,
  xSampling = c("quantiles", "random"),
  xFit,
  percLim = c(0.05, 0.95),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="neWeight.default_+3A_object">object</code></td>
<td>
<p>fitted model object representing the mediator model.</p>
</td></tr>
<tr><td><code id="neWeight.default_+3A_formula">formula</code></td>
<td>
<p>a <code><a href="stats.html#topic+formula">formula</a></code> object providing a symbolic description of the mediator model. Redundant if already specified in call for fitted model specified in <code>object</code> (see details).</p>
</td></tr>
<tr><td><code id="neWeight.default_+3A_data">data</code></td>
<td>
<p>data, as matrix or data frame, containing the exposure (and other relevant) variables. Redundant if already specified in call for fitted model specified in <code>object</code> (see details).</p>
</td></tr>
<tr><td><code id="neWeight.default_+3A_nrep">nRep</code></td>
<td>
<p>number of replications or hypothetical values of the exposure to sample for each observation unit.</p>
</td></tr>
<tr><td><code id="neWeight.default_+3A_xsampling">xSampling</code></td>
<td>
<p>character string indicating how to sample from the conditional exposure distribution.
Possible values are <code>"quantiles"</code> or <code>"random"</code> (see details).</p>
</td></tr>
<tr><td><code id="neWeight.default_+3A_xfit">xFit</code></td>
<td>
<p>an optional fitted object (preferably <code>glm</code>) for the conditional exposure distribution (see details).</p>
</td></tr>
<tr><td><code id="neWeight.default_+3A_perclim">percLim</code></td>
<td>
<p>a numerical vector of the form <code>c(lower, upper)</code> indicating the extreme percentiles to sample when using <code>"quantiles"</code> as sampling method to sample from the conditional exposure distribution (see details).</p>
</td></tr>
<tr><td><code id="neWeight.default_+3A_...">...</code></td>
<td>
<p>additional arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The calculated weights are ratios of fitted probabilities or probability densities from the distribution of the mediator model.
This model needs to be specified as a fitted object in the <code>object</code> argument.
</p>
<p>If the model-fitting function used to fit the mediator model does not require specification of a <code>formula</code> or <code>data</code> argument,
these need to be specified explicitly in order to enable <code>neWeight.default</code> to extract pointers to variable types relevant for mediation analysis.
</p>
<p>Whether a <code><a href="stats.html#topic+formula">formula</a></code> is specified externally (in the call for the fitted mediator model object which is specified in <code>object</code>) or internally (via the <code>formula</code> argument),
it always needs to be of the form <code>M ~ X + C1 + C2</code>, with predictor variables entered in the following prespecified order:
</p>

<ol>
<li><p> exposure <code>X</code>: The first predictor is coded as exposure or treatment.
</p>
</li>
<li><p> baseline covariates <code>C</code>: All remaining predictor variables are automatically coded as baseline covariates.
</p>
</li></ol>

<p>It is important to adhere to this prespecified order to enable <code>neWeight</code> to create valid pointers to these different types of predictor variables.
This requirement extends to the use of operators different than the <code>+</code> operator, such as the <code>:</code> and <code>*</code> operators (when e.g. adding interaction terms). 
For instance, the formula specifications <code>M ~ X * C1 + C2</code>, <code>M ~ X + C1 + X:C1 + C2</code> and <code>Y ~ X + X:C1 + C1 + C2</code> will create identical pointers to the different types of variables,
as the order of the unique predictor variables is identical in all three specifications. 
</p>
<p>Furthermore, categorical exposures that are not coded as factors in the original dataset, should be specified as factors in the formula, 
using the <code><a href="base.html#topic+factor">factor</a></code> function, e.g. <code>M ~ factor(X) + C1 + C2</code>. 
Quadratic or higher-order polynomial terms can be included as well, by making use of the <code><a href="base.html#topic+I">I</a></code> function or by using the <code><a href="stats.html#topic+poly">poly</a></code> function.
For instance, <code>M ~ X + I(X^2) + C1 + C2</code> and <code>M ~ poly(X, 2, raw = TRUE) + C1 + C2</code> are equivalent and result in identical pointers to the different types of variables.
</p>
<p>The command <code>terms(object, "vartype")</code> (with <code>object</code> replaced by the name of the resulting expanded dataset) can be used to check whether valid pointers have been created.
</p>
<p>In contrast to imputation models with categorical exposures, additional arguments need to be specified if the exposure is continuous.
All of these additional arguments are related to the sampling procedure for the exposure.
</p>
<p>Whereas the number of replications <code>nRep</code> for categorical variables equals the number of levels for the exposure coded as a factor (i.e. the number of hypothetical exposure values), the number of desired replications needs to be specified explicitly for continuous exposures.
Its default is 5.
</p>
<p>If <code>xFit</code> is left unspecified, the hypothetical exposure levels are automatically sampled from a linear model for the exposure, conditional on a linear combination of all covariates.
If one wishes to use another model for the exposure, this default model specification can be overruled by referring to a fitted model object in the <code>xFit</code> argument.
Misspecification of this sampling model does not induce bias in the estimated coefficients and standard errors of the natural effect model.
</p>
<p>The <code>xSampling</code> argument allows to specify how the hypothetical exposure levels should be sampled from the conditional exposure distribution (which is either entered explicitly using the <code>xFit</code> argument or fitted automatically as described in the previous paragraph).
The <code>"random"</code> option randomly samples <code>nRep</code> draws from the exposure distribution, whereas the <code>"quantiles"</code> option (default) samples <code>nRep</code> quantiles at equal-sized probability intervals. Only the latter hence yields fixed exposure levels given <code>nRep</code> and <code>xFit</code>. <br /><br />
In order to guarantee that the entire support of the distribution is being sampled (which might be a concern if <code>nRep</code> is chosen to be small), the default lower and upper sampled quantiles are the 5th and 95th percentiles.
The intermittent quantiles correspond to equal-sized probability intervals. So, for instance, if <code>nRep = 4</code>, then the sampled quantiles will correspond to probabilities 0.05, 0.35, 0.65 and 0.95.
These default 'outer' quantiles can be changed by specifying the <code>percLim</code> argument accordingly. By specifying <code>percLim = NULL</code>, the standard quantiles will be sampled (e.g., 0.2, 0.4, 0.6 and 0.8 if <code>nRep = 4</code>).
</p>


<h3>Value</h3>

<p>A data frame of class <code>c("data.frame", "expData", "weightData")</code>. See <code><a href="#topic+expData">expData</a></code> for its structure.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+neWeight">neWeight</a></code>, <code><a href="#topic+neWeight.formula">neWeight.formula</a></code>, <code><a href="#topic+expData">expData</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(UPBdata)

## example using glm
fit.glm &lt;- glm(negaff ~ att + gender + educ + age, data = UPBdata)
weightData &lt;- neWeight(fit.glm, nRep = 2)

</code></pre>

<hr>
<h2 id='neWeight.formula'>Expand the dataset and calculate ratio-of-mediator probability weights</h2><span id='topic+neWeight.formula'></span>

<h3>Description</h3>

<p>This function both expands the data along hypothetical exposure values and calculates ratio-of-mediator probability weights.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'formula'
neWeight(
  object,
  family,
  data,
  FUN = glm,
  nRep = 5,
  xSampling = c("quantiles", "random"),
  xFit,
  percLim = c(0.05, 0.95),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="neWeight.formula_+3A_object">object</code></td>
<td>
<p>a <code><a href="stats.html#topic+formula">formula</a></code> object providing a symbolic description of the mediator model (see details).</p>
</td></tr>
<tr><td><code id="neWeight.formula_+3A_family">family</code></td>
<td>
<p>a description of the error distribution and link function to be used in the model. Consult the help files of the model-fitting function specified in <code>FUN</code> for more details on appropriate argument specification.</p>
</td></tr>
<tr><td><code id="neWeight.formula_+3A_data">data</code></td>
<td>
<p>data, as matrix or data frame, containing the exposure (and other relevant) variables. Redundant if already specified in call for fitted model specified in <code>object</code> (see details).</p>
</td></tr>
<tr><td><code id="neWeight.formula_+3A_fun">FUN</code></td>
<td>
<p>function used to fit model specified in <code>formula</code>.</p>
</td></tr>
<tr><td><code id="neWeight.formula_+3A_nrep">nRep</code></td>
<td>
<p>number of replications or hypothetical values of the exposure to sample for each observation unit.</p>
</td></tr>
<tr><td><code id="neWeight.formula_+3A_xsampling">xSampling</code></td>
<td>
<p>character string indicating how to sample from the conditional exposure distribution.
Possible values are <code>"quantiles"</code> or <code>"random"</code> (see details).</p>
</td></tr>
<tr><td><code id="neWeight.formula_+3A_xfit">xFit</code></td>
<td>
<p>an optional fitted object (preferably <code>glm</code>) for the conditional exposure distribution (see details).</p>
</td></tr>
<tr><td><code id="neWeight.formula_+3A_perclim">percLim</code></td>
<td>
<p>a numerical vector of the form <code>c(lower, upper)</code> indicating the extreme percentiles to sample when using <code>"quantiles"</code> as sampling method to sample from the conditional exposure distribution (see details).</p>
</td></tr>
<tr><td><code id="neWeight.formula_+3A_...">...</code></td>
<td>
<p>additional arguments (passed to <code>FUN</code>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The calculated weights are ratios of fitted probabilities or probability densities from the distribution of the mediator model.
This model is fitted internally by extracting information from the arguments <code>object</code>, <code>family</code>, <code>data</code>, <code>FUN</code> and <code>...</code>.
</p>
<p>For mediation model specification via the <code>object</code> argument, use a <code><a href="stats.html#topic+formula">formula</a></code> of the form<br /><code>M ~ X + C1 + C2</code>,
with predictor variables entered in the following prespecified order:
</p>

<ol>
<li><p> exposure <code>X</code>: The first predictor is coded as exposure or treatment.
</p>
</li>
<li><p> baseline covariates <code>C</code>: All remaining predictor variables are automatically coded as baseline covariates.
</p>
</li></ol>

<p>It is important to adhere to this prespecified order to enable <code>neWeight</code> to create valid pointers to these different types of predictor variables.
This requirement extends to the use of operators different than the <code>+</code> operator, such as the <code>:</code> and <code>*</code> operators (when e.g. adding interaction terms). 
For instance, the formula specifications <code>M ~ X * C1 + C2</code>, <code>M ~ X + C1 + X:C1 + C2</code> and <code>Y ~ X + X:C1 + C1 + C2</code> will create identical pointers to the different types of variables,
as the order of the unique predictor variables is identical in all three specifications. 
</p>
<p>Furthermore, categorical exposures that are not coded as factors in the original dataset, should be specified as factors in the formula, 
using the <code><a href="base.html#topic+factor">factor</a></code> function, e.g. <code>M ~ factor(X) + C1 + C2</code>. 
Quadratic or higher-order polynomial terms can be included as well, by making use of the <code><a href="base.html#topic+I">I</a></code> function or by using the <code><a href="stats.html#topic+poly">poly</a></code> function.
For instance, <code>M ~ X + I(X^2) + C1 + C2</code> and <code>M ~ poly(X, 2, raw = TRUE) + C1 + C2</code> are equivalent and result in identical pointers to the different types of variables.
</p>
<p>The command <code>terms(object, "vartype")</code> (with <code>object</code> replaced by the name of the resulting expanded dataset) can be used to check whether valid pointers have been created.
</p>
<p>The type of mediator model can be defined by specifying an appropriate model-fitting function via the <code>FUN</code> argument (its default is <code><a href="stats.html#topic+glm">glm</a></code>).
This method can only be used with model-fitting functions that require a <code>formula</code> argument.
</p>
<p>In contrast to imputation models with categorical exposures, additional arguments need to be specified if the exposure is continuous.
All of these additional arguments are related to the sampling procedure for the exposure.
</p>
<p>Whereas the number of replications <code>nRep</code> for categorical variables equals the number of levels for the exposure coded as a factor (i.e. the number of hypothetical exposure values), the number of desired replications needs to be specified explicitly for continuous exposures.
Its default is 5.
</p>
<p>If <code>xFit</code> is left unspecified, the hypothetical exposure levels are automatically sampled from a linear model for the exposure, conditional on a linear combination of all covariates.
If one wishes to use another model for the exposure, this default model specification can be overruled by referring to a fitted model object in the <code>xFit</code> argument.
Misspecification of this sampling model does not induce bias in the estimated coefficients and standard errors of the natural effect model.
</p>
<p>The <code>xSampling</code> argument allows to specify how the hypothetical exposure levels should be sampled from the conditional exposure distribution (which is either entered explicitly using the <code>xFit</code> argument or fitted automatically as described in the previous paragraph).
The <code>"random"</code> option randomly samples <code>nRep</code> draws from the exposure distribution, whereas the <code>"quantiles"</code> option (default) samples <code>nRep</code> quantiles at equal-sized probability intervals. Only the latter hence yields fixed exposure levels given <code>nRep</code> and <code>xFit</code>. <br /><br />
In order to guarantee that the entire support of the distribution is being sampled (which might be a concern if <code>nRep</code> is chosen to be small), the default lower and upper sampled quantiles are the 5th and 95th percentiles.
The intermittent quantiles correspond to equal-sized probability intervals. So, for instance, if <code>nRep = 4</code>, then the sampled quantiles will correspond to probabilities 0.05, 0.35, 0.65 and 0.95.
These default 'outer' quantiles can be changed by specifying the <code>percLim</code> argument accordingly. By specifying <code>percLim = NULL</code>, the standard quantiles will be sampled (e.g., 0.2, 0.4, 0.6 and 0.8 if <code>nRep = 4</code>).
</p>


<h3>Value</h3>

<p>A data frame of class <code>c("data.frame", "expData", "weightData"))</code>. See <code><a href="#topic+expData">expData</a></code> for its structure.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+neWeight.default">neWeight.default</a></code>, <code><a href="#topic+expData">expData</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(UPBdata)

## example using glm
weightData &lt;- neWeight(negaff ~ att + gender + educ + age, 
                       data = UPBdata, nRep = 2)
</code></pre>

<hr>
<h2 id='plot.neLht'>Confidence interval plots for linear hypotheses in natural effect models</h2><span id='topic+plot.neLht'></span><span id='topic+plot.neEffdecomp'></span><span id='topic+plot.neLhtBoot'></span>

<h3>Description</h3>

<p>Confidence interval plots for linear hypotheses in natural effect models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'neEffdecomp'
plot(x, level = 0.95, transf = identity, ylabels, yticks.at, ...)

## S3 method for class 'neLht'
plot(x, level = 0.95, transf = identity, ylabels, yticks.at, ...)

## S3 method for class 'neLhtBoot'
plot(
  x,
  level = 0.95,
  ci.type = "norm",
  transf = identity,
  ylabels,
  yticks.at,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.neLht_+3A_x">x</code></td>
<td>
<p>an object of class <code>neLht</code>.</p>
</td></tr>
<tr><td><code id="plot.neLht_+3A_level">level</code></td>
<td>
<p>the confidence level required.</p>
</td></tr>
<tr><td><code id="plot.neLht_+3A_transf">transf</code></td>
<td>
<p>transformation function to be applied internally on the (linear hypothesis) estimates and their confidence intervals (e.g. <code>exp</code> for logit or Poisson regression). The default is <code>identity</code> (i.e. no transformation).</p>
</td></tr>
<tr><td><code id="plot.neLht_+3A_ylabels">ylabels</code></td>
<td>
<p>character vector containing the labels for the (linear hypothesis) estimates to be plotted on the y-axis.</p>
</td></tr>
<tr><td><code id="plot.neLht_+3A_yticks.at">yticks.at</code></td>
<td>
<p>numeric vector containing the y-coordinates (from 0 to 1) to draw the tick marks for the different estimates and their corresponding confidence intervals.</p>
</td></tr>
<tr><td><code id="plot.neLht_+3A_...">...</code></td>
<td>
<p>additional arguments.</p>
</td></tr>
<tr><td><code id="plot.neLht_+3A_ci.type">ci.type</code></td>
<td>
<p>the type of bootstrap intervals required (see <code>type</code> argument in <code><a href="#topic+neModel-methods">neModel-methods</a></code>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is an adapted version of <code><a href="multcomp.html#topic+plot.glht">plot.glht</a></code> from the <span class="pkg">multcomp</span> package and
yields confidence interval plots for each of the linear hypothesis parameters.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+neModel">neModel</a></code>, <code><a href="#topic+neLht">neLht</a></code>, <code><a href="#topic+neEffdecomp">neEffdecomp</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(UPBdata)

impData &lt;- neImpute(UPB ~ att * negaff + gender + educ + age, 
                    family = binomial, data = UPBdata)
neMod &lt;- neModel(UPB ~ att0 * att1 + gender + educ + age, 
                 family = binomial, expData = impData, se = "robust")

lht &lt;- neLht(neMod, linfct = c("att0 = 0", "att0 + att0:att1 = 0", 
                               "att1 = 0", "att1 + att0:att1 = 0", 
                               "att0 + att1 + att0:att1 = 0"))

## all pairs return identical output
plot(confint(lht), transf = exp)
plot(lht, transf = exp)

plot(neEffdecomp(neMod), transf = exp)
plot(neMod, transf = exp)


</code></pre>

<hr>
<h2 id='plot.neModel'>Confidence interval plots for natural effect components</h2><span id='topic+plot.neModel'></span><span id='topic+plot.neModelBoot'></span>

<h3>Description</h3>

<p>Obtain effect decomposition confidence interval plots for natural effect models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'neModel'
plot(x, xRef, covLev, level = 0.95, transf = identity, ylabels, yticks.at, ...)

## S3 method for class 'neModelBoot'
plot(
  x,
  xRef,
  covLev,
  level = 0.95,
  ci.type = "norm",
  transf = identity,
  ylabels,
  yticks.at,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.neModel_+3A_x">x</code></td>
<td>
<p>a fitted natural effect model object.</p>
</td></tr>
<tr><td><code id="plot.neModel_+3A_xref">xRef</code></td>
<td>
<p>a vector including reference levels for the exposure, <em>x*</em> and <em>x</em>, at which natural effect components need to be evaluated (see details).</p>
</td></tr>
<tr><td><code id="plot.neModel_+3A_covlev">covLev</code></td>
<td>
<p>a vector including covariate levels at which natural effect components need to be evaluated (see details).</p>
</td></tr>
<tr><td><code id="plot.neModel_+3A_level">level</code></td>
<td>
<p>the confidence level required.</p>
</td></tr>
<tr><td><code id="plot.neModel_+3A_transf">transf</code></td>
<td>
<p>transformation function to be applied internally on the (linear hypothesis) estimates and their confidence intervals (e.g. <code>exp</code> for logit or Poisson regression). The default is <code>identity</code> (i.e. no transformation).</p>
</td></tr>
<tr><td><code id="plot.neModel_+3A_ylabels">ylabels</code></td>
<td>
<p>character vector containing the labels for the (linear hypothesis) estimates to be plotted on the y-axis.</p>
</td></tr>
<tr><td><code id="plot.neModel_+3A_yticks.at">yticks.at</code></td>
<td>
<p>numeric vector containing the y-coordinates (from 0 to 1) to draw the tick marks for the different estimates and their corresponding confidence intervals.</p>
</td></tr>
<tr><td><code id="plot.neModel_+3A_...">...</code></td>
<td>
<p>additional arguments.</p>
</td></tr>
<tr><td><code id="plot.neModel_+3A_ci.type">ci.type</code></td>
<td>
<p>the type of bootstrap intervals required (see <code>type</code> argument in <code><a href="#topic+neModel-methods">neModel-methods</a></code>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function yields confidence interval plots for the natural effect components.
These causal parameter estimates are first internally extracted from the <code>neModel</code> object by applying the effect decomposition function <code><a href="#topic+neEffdecomp">neEffdecomp</a>(x, xRef, covLev)</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(UPBdata)

impData &lt;- neImpute(UPB ~ att * negaff + educ + gender + age, 
                    family = binomial, data = UPBdata)
neMod &lt;- neModel(UPB ~ att0 * att1 + educ + gender + age, 
                 family = binomial, expData = impData, se = "robust")

plot(neMod)
plot(neMod, transf = exp, 
     ylabels = c("PDE", "TDE", "PIE", "TIE", "TE"))
plot(neMod, level = 0.9, xRef = c(-1, 0))
</code></pre>

<hr>
<h2 id='UPBdata'>UPB data</h2><span id='topic+UPBdata'></span>

<h3>Description</h3>

<p>Data from a survey study that was part of the Interdisciplinary Project for the Optimization of Separation trajectories (IPOS). This large-scale project involved the recruitment of individuals who divorced between March 2008 and March 2009 in four major courts in Flanders. It aimed to improve the quality of life in families during and after the divorce by translating research findings into practical guidelines for separation specialists and by promoting evidence-based policy.
This dataset involves a subsample of 385 individuals, namely those who responded to a battery of questionnaires related to romantic relationship and breakup characteristics (De Smet, 2012).
</p>


<h3>Format</h3>

<p>A data frame with 385 rows and 9 variables:
</p>

<dl>
<dt>att</dt><dd><p>self-reported anxious attachment level (standardized)</p>
</dd>
<dt>attbin</dt><dd><p>binary version of self-reported anxious attachment level: 1 = higher than sample mean, 0 = lower than sample mean</p>
</dd>
<dt>attcat</dt><dd><p>multicategorical version of self-reported anxious attachment level: <code>L</code> = low, <code>M</code> = intermediate, <code>H</code> = high</p>
</dd>
<dt>negaff</dt><dd><p>level of self-reported experienced negative affectivity (standardized)</p>
</dd>
<dt>initiator</dt><dd><p>initiator of the divorce</p>
</dd>
<dt>gender</dt><dd><p>gender: <code>F</code> = female, <code>M</code> = male</p>
</dd>
<dt>educ</dt><dd><p>education level: either <code>H</code> = high (at least a bachelor's degree), <code>M</code> = intermediate (having finished secondary school) or <code>L</code> = low (otherwise)</p>
</dd>
<dt>age</dt><dd><p>age (in years)</p>
</dd>
<dt>UPB</dt><dd><p>binary variable indicating whether the individual reported having displayed unwanted pursuit behavior(s) towards the ex-partner</p>
</dd>
</dl>



<h3>Source</h3>

<p>Ghent University and Catholic University of Louvain (2010). <em>Interdisciplinary Project for the Optimisation of Separation trajectories - divorce and separation in Flanders</em>.
</p>


<h3>References</h3>

<p>De Smet, O., Loeys, T., &amp; Buysse, A. (2012). Post-Breakup Unwanted Pursuit: A Refined Analysis of the Role of Romantic Relationship Characteristics. <em>Journal of Family Violence</em>, <b>27</b>(5), 437-452.
</p>
<p>Loeys, T., Moerkerke, B., De Smet, O., Buysse, A., Steen, J., &amp; Vansteelandt, S. (2013). Flexible Mediation Analysis in the Presence of Nonlinear Relations: Beyond the Mediation Formula. <em>Multivariate Behavioral Research</em>, <b>48</b>(6), 871-894.
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
