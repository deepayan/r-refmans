<!DOCTYPE html><html lang="en"><head><title>Help for package BT</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {BT}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#.BT_cv_errors'><p>Cross-validation errors.</p></a></li>
<li><a href='#.BT_relative_influence'><p>Method for estimating the relative influence.</p></a></li>
<li><a href='#BT'><p>(Adaptive) Boosting Trees (ABT/BT) Algorithm.</p></a></li>
<li><a href='#BT_call'><p>(Adaptive) Boosting Trees (ABT/BT) fit.</p></a></li>
<li><a href='#BT_devTweedie'><p>Deviance function for the Tweedie family.</p></a></li>
<li><a href='#BT_more'><p>Perform additional boosting iterations.</p></a></li>
<li><a href='#BT_perf'><p>Performance assessment.</p></a></li>
<li><a href='#BT_Simulated_Data'><p>Simulated Database.</p></a></li>
<li><a href='#BTCVFit'><p>BTCVFit</p></a></li>
<li><a href='#BTFit'><p>BTFit</p></a></li>
<li><a href='#predict.BTCVFit'><p>Predictions for CV fitted BT models.</p></a></li>
<li><a href='#predict.BTFit'><p>Predict method for BT Model fits.</p></a></li>
<li><a href='#print.BTFit'><p>Printing function.</p></a></li>
<li><a href='#summary.BTFit'><p>Summary of a BTFit object.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Title:</td>
<td>(Adaptive) Boosting Trees Algorithm</td>
</tr>
<tr>
<td>Version:</td>
<td>0.4</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-08-19</td>
</tr>
<tr>
<td>Author:</td>
<td>Gireg Willame [aut, cre, cph]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Gireg Willame &lt;gireg.willame@gmail.com&gt;</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>rpart, stats, statmod, parallel</td>
</tr>
<tr>
<td>Suggests:</td>
<td>rmarkdown, knitr, testthat (&ge; 3.0.0)</td>
</tr>
<tr>
<td>Description:</td>
<td>Performs (Adaptive) Boosting Trees for Poisson distributed response variables, using log-link function.
  The code approach is similar to the one used in 'gbm'/'gbm3'. Moreover, each tree in the expansion is built thanks to the 'rpart' package.
  This package is based on following books and articles
  Denuit, M., Hainaut, D., Trufin, J. (2019) &lt;<a href="https://doi.org/10.1007%2F978-3-030-25820-7">doi:10.1007/978-3-030-25820-7</a>&gt;
  Denuit, M., Hainaut, D., Trufin, J. (2019) &lt;<a href="https://doi.org/10.1007%2F978-3-030-57556-4">doi:10.1007/978-3-030-57556-4</a>&gt;
  Denuit, M., Hainaut, D., Trufin, J. (2019) &lt;<a href="https://doi.org/10.1007%2F978-3-030-25827-6">doi:10.1007/978-3-030-25827-6</a>&gt;
  Denuit, M., Hainaut, D., Trufin, J. (2022) &lt;<a href="https://doi.org/10.1080%2F03461238.2022.2037016">doi:10.1080/03461238.2022.2037016</a>&gt;
  Denuit, M., Huyghe, J., Trufin, J. (2022) <a href="https://dial.uclouvain.be/pr/boreal/fr/object/boreal%3A244325/datastream/PDF_01/view">https://dial.uclouvain.be/pr/boreal/fr/object/boreal%3A244325/datastream/PDF_01/view</a>
  Denuit, M., Trufin, J., Verdebout, T. (2022) <a href="https://dial.uclouvain.be/pr/boreal/fr/object/boreal%3A268577">https://dial.uclouvain.be/pr/boreal/fr/object/boreal%3A268577</a>.</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/GiregWillame/BT/">https://github.com/GiregWillame/BT/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/GiregWillame/BT/issues/">https://github.com/GiregWillame/BT/issues/</a></td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-08-19 08:57:10 UTC; Gireg Willame</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-08-19 09:22:33 UTC</td>
</tr>
</table>
<hr>
<h2 id='.BT_cv_errors'>Cross-validation errors.</h2><span id='topic+.BT_cv_errors'></span>

<h3>Description</h3>

<p>Function to compute the cross-validation error.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.BT_cv_errors(BT_cv_fit, cv.folds, folds)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id=".BT_cv_errors_+3A_bt_cv_fit">BT_cv_fit</code></td>
<td>
<p>a <code><a href="#topic+BTCVFit">BTCVFit</a></code> object.</p>
</td></tr>
<tr><td><code id=".BT_cv_errors_+3A_cv.folds">cv.folds</code></td>
<td>
<p>a numeric corresponding to the number of folds.</p>
</td></tr>
<tr><td><code id=".BT_cv_errors_+3A_folds">folds</code></td>
<td>
<p>a numerical vector containing the different <code>folds.id</code>. Note that if the latter was not defined by the user, those are randomly generated based on the <code>cv.folds</code> input.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function computes the global cross-validation error as a function of the boosting iteration. Differently said, this measure is obtained by
computing the average of out-of-fold errors.
</p>


<h3>Value</h3>

<p>Vector containing the cross-validation errors w.r.t. the boosting iteration.
</p>


<h3>Author(s)</h3>

<p>Gireg Willame <a href="mailto:gireg.willame@gmail.com">gireg.willame@gmail.com</a>
</p>
<p><em>This package is inspired by the <code>gbm3</code> package. For more details, see <a href="https://github.com/gbm-developers/gbm3/">https://github.com/gbm-developers/gbm3/</a></em>.
</p>


<h3>References</h3>

<p>M. Denuit, D. Hainaut and J. Trufin (2019). <strong>Effective Statistical Learning Methods for Actuaries |: GLMs and Extensions</strong>, <em>Springer Actuarial</em>.
</p>
<p>M. Denuit, D. Hainaut and J. Trufin (2019). <strong>Effective Statistical Learning Methods for Actuaries ||: Tree-Based Methods and Extensions</strong>, <em>Springer Actuarial</em>.
</p>
<p>M. Denuit, D. Hainaut and J. Trufin (2019). <strong>Effective Statistical Learning Methods for Actuaries |||: Neural Networks and Extensions</strong>, <em>Springer Actuarial</em>.
</p>
<p>M. Denuit, D. Hainaut and J. Trufin (2022). <strong>Response versus gradient boosting trees, GLMs and neural networks under Tweedie loss and log-link</strong>.
Accepted for publication in <em>Scandinavian Actuarial Journal</em>.
</p>
<p>M. Denuit, J. Huyghe and J. Trufin (2022). <strong>Boosting cost-complexity pruned trees on Tweedie responses: The ABT machine for insurance ratemaking</strong>.
Paper submitted for publication.
</p>
<p>M. Denuit, J. Trufin and T. Verdebout (2022). <strong>Boosting on the responses with Tweedie loss functions</strong>. Paper submitted for publication.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+BT">BT</a></code>.
</p>

<hr>
<h2 id='.BT_relative_influence'>Method for estimating the relative influence.</h2><span id='topic+.BT_relative_influence'></span>

<h3>Description</h3>

<p>Helper function for computing the relative influence of each variable in the BT object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.BT_relative_influence(
  BTFit_object,
  n.iter,
  rescale = FALSE,
  sort.it = FALSE,
  consider.competing = FALSE,
  consider.surrogates = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id=".BT_relative_influence_+3A_btfit_object">BTFit_object</code></td>
<td>
<p>a <code><a href="#topic+BTFit">BTFit</a></code> object.</p>
</td></tr>
<tr><td><code id=".BT_relative_influence_+3A_n.iter">n.iter</code></td>
<td>
<p>number of boosting iterations used for computation. If not provided, the function will perform a best guess approach to determine the optimal number of iterations. In fact,
if a validation set was used during the fitting, the retained number of iterations is the one corresponding to the lowest validation set error ; otherwise, if cross-validation was performed, the
number of iterations resulting in lowest cross-validation error will be used; otherwise, if the out-of-bag parameter was defined, the OOB error will be used to determine the optimal
number of iterations; otherwise, all iterations will be used.</p>
</td></tr>
<tr><td><code id=".BT_relative_influence_+3A_rescale">rescale</code></td>
<td>
<p>whether or not the results should be rescaled (divided by the maximum observation). Default set to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id=".BT_relative_influence_+3A_sort.it">sort.it</code></td>
<td>
<p>whether or not the results should be (reverse) sorted. Default set to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id=".BT_relative_influence_+3A_consider.competing">consider.competing</code></td>
<td>
<p>whether or not competing split should be considered in the relative influence computation. Default set to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id=".BT_relative_influence_+3A_consider.surrogates">consider.surrogates</code></td>
<td>
<p>whether or not surrogates should be considered in the relative influence computation. Default set to <code>FALSE</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is not intended for end-user use. It performs the relative influence computation and is called during the summary function.
Note that a permutation approach is not yet implemented.
</p>


<h3>Value</h3>

<p>Returns by default an unprocessed vector of estimated relative influences. If the <code>rescale</code> and <code>sort.it</code> arguments are used, it returns
a processed version of the same vector.
</p>


<h3>Author(s)</h3>

<p>Gireg Willame <a href="mailto:gireg.willame@gmail.com">gireg.willame@gmail.com</a>
</p>
<p><em>This package is inspired by the <code>gbm3</code> package. For more details, see <a href="https://github.com/gbm-developers/gbm3/">https://github.com/gbm-developers/gbm3/</a></em>.
</p>


<h3>References</h3>

<p>M. Denuit, D. Hainaut and J. Trufin (2019). <strong>Effective Statistical Learning Methods for Actuaries |: GLMs and Extensions</strong>, <em>Springer Actuarial</em>.
</p>
<p>M. Denuit, D. Hainaut and J. Trufin (2019). <strong>Effective Statistical Learning Methods for Actuaries ||: Tree-Based Methods and Extensions</strong>, <em>Springer Actuarial</em>.
</p>
<p>M. Denuit, D. Hainaut and J. Trufin (2019). <strong>Effective Statistical Learning Methods for Actuaries |||: Neural Networks and Extensions</strong>, <em>Springer Actuarial</em>.
</p>
<p>M. Denuit, D. Hainaut and J. Trufin (2022). <strong>Response versus gradient boosting trees, GLMs and neural networks under Tweedie loss and log-link</strong>.
Accepted for publication in <em>Scandinavian Actuarial Journal</em>.
</p>
<p>M. Denuit, J. Huyghe and J. Trufin (2022). <strong>Boosting cost-complexity pruned trees on Tweedie responses: The ABT machine for insurance ratemaking</strong>.
Paper submitted for publication.
</p>
<p>M. Denuit, J. Trufin and T. Verdebout (2022). <strong>Boosting on the responses with Tweedie loss functions</strong>. Paper submitted for publication.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+BT">BT</a></code>, <code><a href="#topic+BTFit">BTFit</a></code>, <code><a href="#topic+BT_perf">BT_perf</a></code>.
</p>

<hr>
<h2 id='BT'>(Adaptive) Boosting Trees (ABT/BT) Algorithm.</h2><span id='topic+BT'></span>

<h3>Description</h3>

<p>Performs the (Adaptive) Boosting Trees algorithm. This code prepares the inputs and calls the function <code><a href="#topic+BT_call">BT_call</a></code>.
Each tree in the process is built thanks to the <code><a href="rpart.html#topic+rpart">rpart</a></code> function.
In case of cross-validation, this function prepares the folds and performs multiple calls to the fitting function <code><a href="#topic+BT_call">BT_call</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>BT(
  formula = formula(data),
  data = list(),
  tweedie.power = 1,
  ABT = TRUE,
  n.iter = 100,
  train.fraction = 1,
  interaction.depth = 4,
  shrinkage = 1,
  bag.fraction = 1,
  colsample.bytree = NULL,
  keep.data = TRUE,
  is.verbose = FALSE,
  cv.folds = 1,
  folds.id = NULL,
  n.cores = 1,
  tree.control = rpart.control(xval = 0, maxdepth = (if (!is.null(interaction.depth)) {
 
       interaction.depth
 } else {
     10
 }), cp = -Inf, minsplit = 2),
  weights = NULL,
  seed = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="BT_+3A_formula">formula</code></td>
<td>
<p>a symbolic description of the model to be fit. Note that the offset isn't supported in this algorithm.
Instead, everything is performed with a log-link function and a direct relationship exist between response, offset and weights.</p>
</td></tr>
<tr><td><code id="BT_+3A_data">data</code></td>
<td>
<p>an optional data frame containing the variables in the model. By default the variables are taken from <code>environment(formula)</code>, typically the environment from which
<code>BT</code> is called. If <code>keep.data=TRUE</code> in the initial call to <code>BT</code> then <code>BT</code> stores a copy with the object (up to the variables used).</p>
</td></tr>
<tr><td><code id="BT_+3A_tweedie.power">tweedie.power</code></td>
<td>
<p>Experimental parameter currently not used - Set to 1 referring to Poisson distribution.</p>
</td></tr>
<tr><td><code id="BT_+3A_abt">ABT</code></td>
<td>
<p>a boolean parameter. If <code>ABT=TRUE</code> an adaptive boosting tree algorithm is built whereas if <code>ABT=FALSE</code> an usual boosting tree algorithm is run.
By default, it is set to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="BT_+3A_n.iter">n.iter</code></td>
<td>
<p>the total number of iterations to fit. This is equivalent to the number of trees and the number of basis functions in the additive expansion.
Please note that the initialization is not taken into account in the <code>n.iter</code>. More explicitly, a weighted average initializes the algorithm and then <code>n.iter</code> trees
are built. Moreover, note that the <code>bag.fraction</code>, <code>colsample.bytree</code>, ... are not used for this initializing phase.
By default, it is set to 100.</p>
</td></tr>
<tr><td><code id="BT_+3A_train.fraction">train.fraction</code></td>
<td>
<p>the first <code>train.fraction * nrows(data)</code> observations are used to fit the <code>BT</code> and the remainder are used for
computing out-of-sample estimates (also known as validation error) of the loss function. By default, it is set to 1 meaning no out-of-sample estimates.</p>
</td></tr>
<tr><td><code id="BT_+3A_interaction.depth">interaction.depth</code></td>
<td>
<p>the maximum depth of variable interactions: 1 builds an additive model, 2 builds a model with up to two-way interactions, etc.
This parameter can also be interpreted as the maximum number of non-terminal nodes. By default, it is set to 4.
Please note that if this parameter is <code>NULL</code>, all the trees in the expansion are built based on the <code>tree.control</code> parameter only, independently
of the <code>ABT</code> value.
This option is devoted to advanced users only and allows them to benefit from the full flexibility of the implemented algorithm.</p>
</td></tr>
<tr><td><code id="BT_+3A_shrinkage">shrinkage</code></td>
<td>
<p>a shrinkage parameter (in the interval (0,1]) applied to each tree in the expansion. Also known as the learning rate or step-size reduction. By default, it is set to 1.</p>
</td></tr>
<tr><td><code id="BT_+3A_bag.fraction">bag.fraction</code></td>
<td>
<p>the fraction of independent training observations randomly selected to propose the next tree in the expansion.
This introduces randomness into the model fit. If <code>bag.fraction</code>&lt;1 then running the same model twice will result in similar but different fits.
Please note that if this parameter is used the <code>BTErrors$training.error</code> corresponds to the normalized in-bag error and the out-of-bag improvements
are computed and stored in <code>BTErrors$oob.improvement</code>. See <code><a href="#topic+BTFit">BTFit</a></code> for more details.
By default, it is set to 1.</p>
</td></tr>
<tr><td><code id="BT_+3A_colsample.bytree">colsample.bytree</code></td>
<td>
<p>each tree will be trained on a random subset of <code>colsample.bytree</code> number of features. Each tree will consider a new
random subset of features from the formula, adding variability to the algorithm and reducing computation time. <code>colsample.bytree</code> will be bounded between
1 and the number of features considered in the formula. By default, it is set to <code>NULL</code> meaning no effect.</p>
</td></tr>
<tr><td><code id="BT_+3A_keep.data">keep.data</code></td>
<td>
<p>a boolean variable indicating whether to keep the data frames. This is particularly useful if one wants to keep track of the initial data frames
and is further used for predicting in case any data frame is specified.
Note that in case of cross-validation, if <code>keep.data=TRUE</code> the initial data frames are saved whereas the cross-validation samples are not.
By default, it is set to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="BT_+3A_is.verbose">is.verbose</code></td>
<td>
<p>if <code>is.verbose=TRUE</code>, the <code>BT</code> will print out the algorithm progress. By default, it is set to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="BT_+3A_cv.folds">cv.folds</code></td>
<td>
<p>a positive integer representing the number of cross-validation folds to perform. If <code>cv.folds</code>&gt;1 then <code>BT</code>, in addition to the usual fit,
will perform a cross-validation and calculate an estimate of generalization error returned in <code>BTErrors$cv.error</code>. By default, it is set to 1 meaning no cross-validation.</p>
</td></tr>
<tr><td><code id="BT_+3A_folds.id">folds.id</code></td>
<td>
<p>an optional vector of values identifying what fold each observation is in. If supplied, this parameter prevails over <code>cv.folds</code>.
By default, <code>folds.id = NULL</code> meaning that no folds are defined.</p>
</td></tr>
<tr><td><code id="BT_+3A_n.cores">n.cores</code></td>
<td>
<p>the number of cores to use for parallelization. This parameter is used during the cross-validation.
This parameter is bounded between 1 and the maximum number of available cores.
By default, it is set to 1 leading to a sequential approach.</p>
</td></tr>
<tr><td><code id="BT_+3A_tree.control">tree.control</code></td>
<td>
<p>for advanced user only. It allows to define additional tree parameters that will be used at each iteration.
See <code><a href="rpart.html#topic+rpart.control">rpart.control</a></code> for more information.</p>
</td></tr>
<tr><td><code id="BT_+3A_weights">weights</code></td>
<td>
<p>optional vector of weights used in the fitting process. These weights must be positive but do not need to be normalized.
By default, it is set to <code>NULL</code> which corresponds to an uniform weight of 1 for each observation.</p>
</td></tr>
<tr><td><code id="BT_+3A_seed">seed</code></td>
<td>
<p>optional number used as seed. Please note that if <code>cv.folds</code>&gt;1, the <code>parLapply</code> function is called.
Therefore, the seed (if defined) used inside each fold will be a multiple of the <code>seed</code> parameter.</p>
</td></tr>
<tr><td><code id="BT_+3A_...">...</code></td>
<td>
<p>not currently used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The NA values are currently dropped using <code>na.omit</code>.
</p>


<h3>Value</h3>

<p>a <code><a href="#topic+BTFit">BTFit</a></code> object.
</p>


<h3>Author(s)</h3>

<p>Gireg Willame <a href="mailto:gireg.willame@gmail.com">gireg.willame@gmail.com</a>
</p>
<p><em>This package is inspired by the <code>gbm3</code> package. For more details, see <a href="https://github.com/gbm-developers/gbm3/">https://github.com/gbm-developers/gbm3/</a></em>.
</p>


<h3>References</h3>

<p>M. Denuit, D. Hainaut and J. Trufin (2019). <strong>Effective Statistical Learning Methods for Actuaries |: GLMs and Extensions</strong>, <em>Springer Actuarial</em>.
</p>
<p>M. Denuit, D. Hainaut and J. Trufin (2019). <strong>Effective Statistical Learning Methods for Actuaries ||: Tree-Based Methods and Extensions</strong>, <em>Springer Actuarial</em>.
</p>
<p>M. Denuit, D. Hainaut and J. Trufin (2019). <strong>Effective Statistical Learning Methods for Actuaries |||: Neural Networks and Extensions</strong>, <em>Springer Actuarial</em>.
</p>
<p>M. Denuit, D. Hainaut and J. Trufin (2022). <strong>Response versus gradient boosting trees, GLMs and neural networks under Tweedie loss and log-link</strong>.
Accepted for publication in <em>Scandinavian Actuarial Journal</em>.
</p>
<p>M. Denuit, J. Huyghe and J. Trufin (2022). <strong>Boosting cost-complexity pruned trees on Tweedie responses: The ABT machine for insurance ratemaking</strong>.
Paper submitted for publication.
</p>
<p>M. Denuit, J. Trufin and T. Verdebout (2022). <strong>Boosting on the responses with Tweedie loss functions</strong>. Paper submitted for publication.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+BTFit">BTFit</a></code>, <code><a href="#topic+BTCVFit">BTCVFit</a></code>, <code><a href="#topic+BT_call">BT_call</a></code>, <code><a href="#topic+BT_perf">BT_perf</a></code>, <code><a href="#topic+predict.BTFit">predict.BTFit</a></code>,
<code><a href="#topic+summary.BTFit">summary.BTFit</a></code>, <code><a href="#topic+print.BTFit">print.BTFit</a></code>, <code><a href="#topic+.BT_cv_errors">.BT_cv_errors</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Load dataset.
dataset &lt;- BT::BT_Simulated_Data

## Fit a Boosting Tree model.
BT_algo &lt;- BT(formula = Y_normalized ~ Age + Sport + Split + Gender, # formula
              data = dataset, # data
              ABT = FALSE, # Classical Boosting Tree
              n.iter = 200,
              train.fraction = 0.8,
              interaction.depth = 3,
              shrinkage = 0.01,
              bag.fraction = 0.5,
              colsample.bytree = 2, # 2 explanatory variable used at each iteration.
              keep.data = FALSE, # Do not keep a data copy.
              is.verbose = FALSE, # Do not print progress.
              cv.folds = 3, # 3-cv will be performed.
              folds.id = NULL ,
              n.cores = 1,
              weights = ExpoR, # &lt;=&gt; Poisson model on response Y with ExpoR in offset.
              seed = NULL)

## Determine the model performance and plot results.
best_iter_val &lt;- BT_perf(BT_algo, method='validation')
best_iter_oob &lt;- BT_perf(BT_algo, method='OOB', oobag.curve = TRUE)
best_iter_cv &lt;- BT_perf(BT_algo, method ='cv', oobag.curve = TRUE)

best_iter &lt;- best_iter_val

## Variable influence and plot results.
# Based on the first iteration.
variable_influence1 &lt;- summary(BT_algo, n.iter = 1)
# Using all iterations up to best_iter.
variable_influence_best_iter &lt;- summary(BT_algo, n.iter = best_iter)

##  Print results : call, best_iters and summarized relative influence.
print(BT_algo)

## Model predictions.
# Predict on the link scale, using only the best_iter tree.
pred_single_iter &lt;- predict(BT_algo, newdata = dataset,
                            n.iter = best_iter, type = 'link', single.iter = TRUE)
# Predict on the response scale, using the first best_iter.
pred_best_iter &lt;- predict(BT_algo, newdata = dataset,
                          n.iter = best_iter, type = 'response')


</code></pre>

<hr>
<h2 id='BT_call'>(Adaptive) Boosting Trees (ABT/BT) fit.</h2><span id='topic+BT_call'></span><span id='topic+BT_callInit'></span><span id='topic+BT_callBoosting'></span>

<h3>Description</h3>

<p>Fit a (Adaptive) Boosting Trees algorithm. This is for &quot;power&quot; users who have a large number of variables and wish to avoid calling
<code>model.frame</code> which can be slow in this instance. This function is in particular called by <code><a href="#topic+BT">BT</a></code>.
It is mainly split in two parts, the first one considers the initialization (see <code>BT_callInit</code>) whereas the second performs all the boosting iterations (see <code>BT_callBoosting</code>).
By default, this function does not perform input checks (those are all done in <code><a href="#topic+BT">BT</a></code>) and all the parameters should be given in the right format. We therefore
suppose that the user is aware of all the choices made.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>BT_call(
  training.set,
  validation.set,
  tweedie.power,
  respVar,
  w,
  explVar,
  ABT,
  tree.control,
  train.fraction,
  interaction.depth,
  bag.fraction,
  shrinkage,
  n.iter,
  colsample.bytree,
  keep.data,
  is.verbose
)

BT_callInit(training.set, validation.set, tweedie.power, respVar, w)

BT_callBoosting(
  training.set,
  validation.set,
  tweedie.power,
  ABT,
  tree.control,
  interaction.depth,
  bag.fraction,
  shrinkage,
  n.iter,
  colsample.bytree,
  train.fraction,
  keep.data,
  is.verbose,
  respVar,
  w,
  explVar
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="BT_call_+3A_training.set">training.set</code></td>
<td>
<p>a data frame containing all the related variables on which one wants to fit the algorithm.</p>
</td></tr>
<tr><td><code id="BT_call_+3A_validation.set">validation.set</code></td>
<td>
<p>a held-out data frame containing all the related variables on which one wants to assess the algorithm performance. This can be NULL.</p>
</td></tr>
<tr><td><code id="BT_call_+3A_tweedie.power">tweedie.power</code></td>
<td>
<p>Experimental parameter currently not used - Set to 1 referring to Poisson distribution.</p>
</td></tr>
<tr><td><code id="BT_call_+3A_respvar">respVar</code></td>
<td>
<p>the name of the target/response variable.</p>
</td></tr>
<tr><td><code id="BT_call_+3A_w">w</code></td>
<td>
<p>a vector of weights.</p>
</td></tr>
<tr><td><code id="BT_call_+3A_explvar">explVar</code></td>
<td>
<p>a vector containing the name of explanatory variables.</p>
</td></tr>
<tr><td><code id="BT_call_+3A_abt">ABT</code></td>
<td>
<p>a boolean parameter. If <code>ABT=TRUE</code> an adaptive boosting tree algorithm is built whereas if <code>ABT=FALSE</code> an usual boosting tree algorithm is run.</p>
</td></tr>
<tr><td><code id="BT_call_+3A_tree.control">tree.control</code></td>
<td>
<p>allows to define additional tree parameters that will be used at each iteration. See <code><a href="rpart.html#topic+rpart.control">rpart.control</a></code> for more information.</p>
</td></tr>
<tr><td><code id="BT_call_+3A_train.fraction">train.fraction</code></td>
<td>
<p>the first <code>train.fraction * nrows(data)</code> observations are used to fit the <code>BT</code> and the remainder are used for
computing out-of-sample estimates (also known as validation error) of the loss function. It is mainly used to report the value in the <code>BTFit</code> object.</p>
</td></tr>
<tr><td><code id="BT_call_+3A_interaction.depth">interaction.depth</code></td>
<td>
<p>the maximum depth of variable interactions: 1 builds an additive model, 2 builds a model with up to two-way interactions, etc.
This parameter can also be interpreted as the maximum number of non-terminal nodes. By default, it is set to 4.
Please note that if this parameter is <code>NULL</code>, all the trees in the expansion are built based on the <code>tree.control</code> parameter only.
This option is devoted to advanced users only and allows them to benefit from the full flexibility of the implemented algorithm.</p>
</td></tr>
<tr><td><code id="BT_call_+3A_bag.fraction">bag.fraction</code></td>
<td>
<p>the fraction of independent training observations randomly selected to propose the next tree in the expansion.
This introduces randomness into the model fit. If <code>bag.fraction</code>&lt;1 then running the same model twice will result in similar but different fits.
<code>BT</code> uses the R random number generator, so <code>set.seed</code> ensures the same model can be reconstructed. Please note that if this parameter is used the <code>BTErrors$training.error</code>
corresponds to the normalized in-bag error.</p>
</td></tr>
<tr><td><code id="BT_call_+3A_shrinkage">shrinkage</code></td>
<td>
<p>a shrinkage parameter applied to each tree in the expansion. Also known as the learning rate or step-size reduction.</p>
</td></tr>
<tr><td><code id="BT_call_+3A_n.iter">n.iter</code></td>
<td>
<p>the total number of iterations to fit. This is equivalent to the number of trees and the number of basis functions in the additive expansion.
Please note that the initialization is not taken into account in the <code>n.iter</code>. More explicitly, a weighted average initializes the algorithm and then <code>n.iter</code> trees
are built. Moreover, note that the <code>bag.fraction</code>, <code>colsample.bytree</code>, ... are not used for this initializing phase.</p>
</td></tr>
<tr><td><code id="BT_call_+3A_colsample.bytree">colsample.bytree</code></td>
<td>
<p>each tree will be trained on a random subset of <code>colsample.bytree</code> number of features. Each tree will consider a new
random subset of features from the formula, adding variability to the algorithm and reducing computation time. <code>colsample.bytree</code> will be bounded between
1 and the number of features considered.</p>
</td></tr>
<tr><td><code id="BT_call_+3A_keep.data">keep.data</code></td>
<td>
<p>a boolean variable indicating whether to keep the data frames. This is particularly useful if one wants to keep track of the initial data frames
and is further used for predicting in case any data frame is specified.
Note that in case of cross-validation, if <code>keep.data=TRUE</code> the initial data frames are saved whereas the cross-validation samples are not.</p>
</td></tr>
<tr><td><code id="BT_call_+3A_is.verbose">is.verbose</code></td>
<td>
<p>if <code>is.verbose=TRUE</code>, the <code>BT</code> will print out the algorithm progress.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code><a href="#topic+BTFit">BTFit</a></code> object.
</p>


<h3>Author(s)</h3>

<p>Gireg Willame <a href="mailto:gireg.willame@gmail.com">gireg.willame@gmail.com</a>
</p>
<p><em>This package is inspired by the <code>gbm3</code> package. For more details, see <a href="https://github.com/gbm-developers/gbm3/">https://github.com/gbm-developers/gbm3/</a></em>.
</p>


<h3>References</h3>

<p>M. Denuit, D. Hainaut and J. Trufin (2019). <strong>Effective Statistical Learning Methods for Actuaries |: GLMs and Extensions</strong>, <em>Springer Actuarial</em>.
</p>
<p>M. Denuit, D. Hainaut and J. Trufin (2019). <strong>Effective Statistical Learning Methods for Actuaries ||: Tree-Based Methods and Extensions</strong>, <em>Springer Actuarial</em>.
</p>
<p>M. Denuit, D. Hainaut and J. Trufin (2019). <strong>Effective Statistical Learning Methods for Actuaries |||: Neural Networks and Extensions</strong>, <em>Springer Actuarial</em>.
</p>
<p>M. Denuit, D. Hainaut and J. Trufin (2022). <strong>Response versus gradient boosting trees, GLMs and neural networks under Tweedie loss and log-link</strong>.
Accepted for publication in <em>Scandinavian Actuarial Journal</em>.
</p>
<p>M. Denuit, J. Huyghe and J. Trufin (2022). <strong>Boosting cost-complexity pruned trees on Tweedie responses: The ABT machine for insurance ratemaking</strong>.
Paper submitted for publication.
</p>
<p>M. Denuit, J. Trufin and T. Verdebout (2022). <strong>Boosting on the responses with Tweedie loss functions</strong>. Paper submitted for publication.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+BTFit">BTFit</a></code>, <code><a href="#topic+BTCVFit">BTCVFit</a></code>, <code><a href="#topic+BT_perf">BT_perf</a></code>, <code><a href="#topic+predict.BTFit">predict.BTFit</a></code>,
<code><a href="#topic+summary.BTFit">summary.BTFit</a></code>, <code><a href="#topic+print.BTFit">print.BTFit</a></code>, <code><a href="#topic+.BT_cv_errors">.BT_cv_errors</a></code>.
</p>

<hr>
<h2 id='BT_devTweedie'>Deviance function for the Tweedie family.</h2><span id='topic+BT_devTweedie'></span>

<h3>Description</h3>

<p>Compute the deviance for the Tweedie family case.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>BT_devTweedie(y, mu, tweedieVal, w = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="BT_devTweedie_+3A_y">y</code></td>
<td>
<p>a vector containing the observed values.</p>
</td></tr>
<tr><td><code id="BT_devTweedie_+3A_mu">mu</code></td>
<td>
<p>a vector containing the fitted values.</p>
</td></tr>
<tr><td><code id="BT_devTweedie_+3A_tweedieval">tweedieVal</code></td>
<td>
<p>a numeric representing the Tweedie Power. It has to be a positive number outside of the interval ]0,1[.</p>
</td></tr>
<tr><td><code id="BT_devTweedie_+3A_w">w</code></td>
<td>
<p>an optional vector of weights.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function computes the Tweedie related deviance. The latter is defined as:
</p>
<p style="text-align: center;"><code class="reqn">d(y, mu, w) = w (y-mu)^2, if tweedieVal = 0;</code>
</p>

<p style="text-align: center;"><code class="reqn">d(y, mu, w) = 2 w (y log(y/mu) + mu - y), if tweedieVal = 1;</code>
</p>

<p style="text-align: center;"><code class="reqn">d(y, mu, w) = 2 w (log(mu/y) + y/mu - 1), if tweedieVal = 2;</code>
</p>

<p style="text-align: center;"><code class="reqn">d(y, mu, w) = 2 w (max(y,0)^(2-p)/((1-p)(2-p)) - y mu^(1-p)/(1-p) + mu^(2-p)/(2-p)), else.</code>
</p>



<h3>Value</h3>

<p>A vector of individual deviance contribution.
</p>


<h3>Author(s)</h3>

<p>Gireg Willame <a href="mailto:gireg.willame@gmail.com">gireg.willame@gmail.com</a>
</p>
<p><em>This package is inspired by the <code>gbm3</code> package. For more details, see <a href="https://github.com/gbm-developers/gbm3/">https://github.com/gbm-developers/gbm3/</a></em>.
</p>


<h3>References</h3>

<p>M. Denuit, D. Hainaut and J. Trufin (2019). <strong>Effective Statistical Learning Methods for Actuaries |: GLMs and Extensions</strong>, <em>Springer Actuarial</em>.
</p>
<p>M. Denuit, D. Hainaut and J. Trufin (2019). <strong>Effective Statistical Learning Methods for Actuaries ||: Tree-Based Methods and Extensions</strong>, <em>Springer Actuarial</em>.
</p>
<p>M. Denuit, D. Hainaut and J. Trufin (2019). <strong>Effective Statistical Learning Methods for Actuaries |||: Neural Networks and Extensions</strong>, <em>Springer Actuarial</em>.
</p>
<p>M. Denuit, D. Hainaut and J. Trufin (2022). <strong>Response versus gradient boosting trees, GLMs and neural networks under Tweedie loss and log-link</strong>.
Accepted for publication in <em>Scandinavian Actuarial Journal</em>.
</p>
<p>M. Denuit, J. Huyghe and J. Trufin (2022). <strong>Boosting cost-complexity pruned trees on Tweedie responses: The ABT machine for insurance ratemaking</strong>.
Paper submitted for publication.
</p>
<p>M. Denuit, J. Trufin and T. Verdebout (2022). <strong>Boosting on the responses with Tweedie loss functions</strong>. Paper submitted for publication.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+BT">BT</a></code>, <code><a href="#topic+BT_call">BT_call</a></code>.
</p>

<hr>
<h2 id='BT_more'>Perform additional boosting iterations.</h2><span id='topic+BT_more'></span>

<h3>Description</h3>

<p>Method to perform additional iterations of the Boosting Tree algorithm, starting from an initial <code><a href="#topic+BTFit">BTFit</a></code> object.
This does not support further cross-validation. Moreover, this approach is only allowed if <code>keep.data=TRUE</code> in the original call.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>BT_more(BTFit_object, new.n.iter = 100, is.verbose = FALSE, seed = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="BT_more_+3A_btfit_object">BTFit_object</code></td>
<td>
<p>a <code><a href="#topic+BTFit">BTFit</a></code> object.</p>
</td></tr>
<tr><td><code id="BT_more_+3A_new.n.iter">new.n.iter</code></td>
<td>
<p>number of new boosting iterations to perform.</p>
</td></tr>
<tr><td><code id="BT_more_+3A_is.verbose">is.verbose</code></td>
<td>
<p>a logical specifying whether or not the additional fitting should run &quot;noisely&quot; with feedback on progress provided to the user.</p>
</td></tr>
<tr><td><code id="BT_more_+3A_seed">seed</code></td>
<td>
<p>optional seed used to perform the new iterations. By default, no seed is set.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a new <code><a href="#topic+BTFit">BTFit</a></code> object containing the initial call as well as the new iterations performed.
</p>


<h3>Author(s)</h3>

<p>Gireg Willame <a href="mailto:gireg.willame@gmail.com">gireg.willame@gmail.com</a>
</p>
<p><em>This package is inspired by the <code>gbm3</code> package. For more details, see <a href="https://github.com/gbm-developers/gbm3/">https://github.com/gbm-developers/gbm3/</a></em>.
</p>


<h3>References</h3>

<p>M. Denuit, D. Hainaut and J. Trufin (2019). <strong>Effective Statistical Learning Methods for Actuaries |: GLMs and Extensions</strong>, <em>Springer Actuarial</em>.
</p>
<p>M. Denuit, D. Hainaut and J. Trufin (2019). <strong>Effective Statistical Learning Methods for Actuaries ||: Tree-Based Methods and Extensions</strong>, <em>Springer Actuarial</em>.
</p>
<p>M. Denuit, D. Hainaut and J. Trufin (2019). <strong>Effective Statistical Learning Methods for Actuaries |||: Neural Networks and Extensions</strong>, <em>Springer Actuarial</em>.
</p>
<p>M. Denuit, D. Hainaut and J. Trufin (2022). <strong>Response versus gradient boosting trees, GLMs and neural networks under Tweedie loss and log-link</strong>.
Accepted for publication in <em>Scandinavian Actuarial Journal</em>.
</p>
<p>M. Denuit, J. Huyghe and J. Trufin (2022). <strong>Boosting cost-complexity pruned trees on Tweedie responses: The ABT machine for insurance ratemaking</strong>.
Paper submitted for publication.
</p>
<p>M. Denuit, J. Trufin and T. Verdebout (2022). <strong>Boosting on the responses with Tweedie loss functions</strong>. Paper submitted for publication.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+BT">BT</a></code>, <code><a href="#topic+BTFit">BTFit</a></code>.
</p>

<hr>
<h2 id='BT_perf'>Performance assessment.</h2><span id='topic+BT_perf'></span>

<h3>Description</h3>

<p>Function to compute the performances of a fitted boosting tree.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>BT_perf(
  BTFit_object,
  plot.it = TRUE,
  oobag.curve = FALSE,
  overlay = TRUE,
  method,
  main = ""
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="BT_perf_+3A_btfit_object">BTFit_object</code></td>
<td>
<p>a <code><a href="#topic+BTFit">BTFit</a></code> object resulting from an initial call to <code><a href="#topic+BT">BT</a></code></p>
</td></tr>
<tr><td><code id="BT_perf_+3A_plot.it">plot.it</code></td>
<td>
<p>a boolean indicating whether to plot the performance measure. Setting <code>plot.it = TRUE</code> creates two plots.
The first one plots the <code>object$BTErrors$training.error</code> (in black) as well as the <code>object$BTErrors$validation.error</code> (in red) and/or the <code>object$BTErrors$cv.error</code> (in green) depending on the <code>method</code> and
parametrization. These values are plotted as a function of the iteration number. The scale of the error measurement, shown on the left vertical axis, depends on the arguments used in the
initial call to <code><a href="#topic+BT">BT</a></code> and the chosen <code>method</code>.</p>
</td></tr>
<tr><td><code id="BT_perf_+3A_oobag.curve">oobag.curve</code></td>
<td>
<p>indicates whether to plot the out-of-bag performance measures in a second plot. Note that this option makes sense if the <code>bag.fraction</code> was properly defined in the
initial call to <code><a href="#topic+BT">BT</a></code>.</p>
</td></tr>
<tr><td><code id="BT_perf_+3A_overlay">overlay</code></td>
<td>
<p>if set to <code>TRUE</code> and <code>oobag.curve=TRUE</code> then a right y-axis is added and the estimated cumulative improvement in the loss function is
plotted versus the iteration number.</p>
</td></tr>
<tr><td><code id="BT_perf_+3A_method">method</code></td>
<td>
<p>indicates the method used to estimate the optimal number of boosting iterations. Setting <code>method = "OOB"</code> computes the out-of-bag estimate and <code>method = "validation"</code>
uses the validation dataset to compute an out-of-sample estimate. Finally, setting <code>method = "cv"</code> extracts the optimal number of iterations using cross-validation, if
<code><a href="#topic+BT">BT</a></code> was called with <code>cv.folds &gt; 1</code>. If missing, a guessing method is applied.</p>
</td></tr>
<tr><td><code id="BT_perf_+3A_main">main</code></td>
<td>
<p>optional parameter that allows the user to define specific plot title.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns the estimated optimal number of iterations. The method of computation depends on the <code>method</code> argument.
</p>


<h3>Author(s)</h3>

<p>Gireg Willame <a href="mailto:g.willame@detralytics.eu">g.willame@detralytics.eu</a>
</p>
<p><em>This package is inspired by the <code>gbm3</code> package. For more details, see <a href="https://github.com/gbm-developers/gbm3/">https://github.com/gbm-developers/gbm3/</a></em>.
</p>


<h3>References</h3>

<p>M. Denuit, D. Hainaut and J. Trufin (2019). <strong>Effective Statistical Learning Methods for Actuaries |: GLMs and Extensions</strong>, <em>Springer Actuarial</em>.
</p>
<p>M. Denuit, D. Hainaut and J. Trufin (2019). <strong>Effective Statistical Learning Methods for Actuaries ||: Tree-Based Methods and Extensions</strong>, <em>Springer Actuarial</em>.
</p>
<p>M. Denuit, D. Hainaut and J. Trufin (2019). <strong>Effective Statistical Learning Methods for Actuaries |||: Neural Networks and Extensions</strong>, <em>Springer Actuarial</em>.
</p>
<p>M. Denuit, D. Hainaut and J. Trufin (2022). <strong>Response versus gradient boosting trees, GLMs and neural networks under Tweedie loss and log-link</strong>.
Accepted for publication in <em>Scandinavian Actuarial Journal</em>.
</p>
<p>M. Denuit, J. Huyghe and J. Trufin (2022). <strong>Boosting cost-complexity pruned trees on Tweedie responses: The ABT machine for insurance ratemaking</strong>.
Paper submitted for publication.
</p>
<p>M. Denuit, J. Trufin and T. Verdebout (2022). <strong>Boosting on the responses with Tweedie loss functions</strong>. Paper submitted for publication.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+BT">BT</a></code>, <code><a href="#topic+BT_call">BT_call</a></code>.
</p>

<hr>
<h2 id='BT_Simulated_Data'>Simulated Database.</h2><span id='topic+BT_Simulated_Data'></span>

<h3>Description</h3>

<p>A simulated database used for examples and vignettes.
The variables are related to a motor insurance pricing context.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>BT_Simulated_Data
</code></pre>


<h3>Format</h3>

<p>A simulated data frame with 50,000 rows and 7 columns, containing simulation of different policyholders:
</p>

<dl>
<dt>Gender</dt><dd><p>Gender, varying between male and female.</p>
</dd>
<dt>Age</dt><dd><p>Age, varying from 18 to 65years old.</p>
</dd>
<dt>Split</dt><dd><p>Noisy variable, not used to simulate the response variable. It allows to assess how the algorithm handle these features.</p>
</dd>
<dt>Sport</dt><dd><p>Car type, varying between yes (sport car) or no.</p>
</dd>
<dt>ExpoR</dt><dd><p>Yearly exposure-to-risk, varying between 0 and 1.</p>
</dd>
<dt>Y</dt><dd><p>Yearly claim number, simulated thanks to Poisson distribution.</p>
</dd>
<dt>Y_normalized</dt><dd><p>Yearly claim frequency, corresponding to the ratio between Y and ExpoR.</p>
</dd>
</dl>


<hr>
<h2 id='BTCVFit'>BTCVFit</h2><span id='topic+BTCVFit'></span>

<h3>Description</h3>

<p>These are objects representing CV fitted boosting trees.
</p>


<h3>Details</h3>

<p>CV (Adaptive) Boosting Tree Model Object.
</p>


<h3>Value</h3>

<p>a list of <code><a href="#topic+BTFit">BTFit</a></code> objects with each element corresponding to a specific BT fit on a particular fold
</p>


<h3>Structure </h3>

<p>The following components must be included in a legitimate <code>BTCVFit</code> object.
</p>


<h3>Author(s)</h3>

<p>Gireg Willame <a href="mailto:gireg.willame@gmail.com">gireg.willame@gmail.com</a>
</p>
<p><em>This package is inspired by the <code>gbm3</code> package. For more details, see <a href="https://github.com/gbm-developers/gbm3/">https://github.com/gbm-developers/gbm3/</a></em>.
</p>


<h3>References</h3>

<p>M. Denuit, D. Hainaut and J. Trufin (2019). <strong>Effective Statistical Learning Methods for Actuaries |: GLMs and Extensions</strong>, <em>Springer Actuarial</em>.
</p>
<p>M. Denuit, D. Hainaut and J. Trufin (2019). <strong>Effective Statistical Learning Methods for Actuaries ||: Tree-Based Methods and Extensions</strong>, <em>Springer Actuarial</em>.
</p>
<p>M. Denuit, D. Hainaut and J. Trufin (2019). <strong>Effective Statistical Learning Methods for Actuaries |||: Neural Networks and Extensions</strong>, <em>Springer Actuarial</em>.
</p>
<p>M. Denuit, D. Hainaut and J. Trufin (2022). <strong>Response versus gradient boosting trees, GLMs and neural networks under Tweedie loss and log-link</strong>.
Accepted for publication in <em>Scandinavian Actuarial Journal</em>.
</p>
<p>M. Denuit, J. Huyghe and J. Trufin (2022). <strong>Boosting cost-complexity pruned trees on Tweedie responses: The ABT machine for insurance ratemaking</strong>.
Paper submitted for publication.
</p>
<p>M. Denuit, J. Trufin and T. Verdebout (2022). <strong>Boosting on the responses with Tweedie loss functions</strong>. Paper submitted for publication.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+BT">BT</a></code>.
</p>

<hr>
<h2 id='BTFit'>BTFit</h2><span id='topic+BTFit'></span>

<h3>Description</h3>

<p>These are objects representing fitted boosting trees.
</p>


<h3>Details</h3>

<p>Boosting Tree Model Object.
</p>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>BTInit</code></td>
<td>
<p>an object of class <code>BTInit</code> containing the initial fitted value <code>initFit</code>, the initial <code>training.error</code> and the initial <code>validation.error</code> if any.</p>
</td></tr>
<tr><td><code>BTErrors</code></td>
<td>
<p>an object of class <code>BTErrors</code> containing the vectors of errors for each iteration performed (excl. the initialization). More precisely, it contains the <code>training.error</code>,
<code>validation.error</code> if <code>train.fraction</code>&lt;1 and the <code>oob.improvement</code> if <code>bag.fraction</code> &lt; 1.
Moreover, if a cross-validation approach was performed, a vector of cross-validation errors <code>cv.error</code> as a function of boosting iteration is also stored in this object.</p>
</td></tr>
<tr><td><code>BTIndivFits</code></td>
<td>
<p>an object of class <code>BTIndivFits</code> containing the list of each individual tree fitted at each boosting iteration.</p>
</td></tr>
<tr><td><code>distribution</code></td>
<td>
<p>the Tweedie power (and so the distribution) that has been used to perform the algorithm. It will currently always output 1.</p>
</td></tr>
<tr><td><code>var.names</code></td>
<td>
<p>a vector containing the names of the explanatory variables.</p>
</td></tr>
<tr><td><code>response</code></td>
<td>
<p>the name of the target/response variable.</p>
</td></tr>
<tr><td><code>w</code></td>
<td>
<p>a vector containing the weights used.</p>
</td></tr>
<tr><td><code>seed</code></td>
<td>
<p>the used seed, if any.</p>
</td></tr>
<tr><td><code>BTData</code></td>
<td>
<p>if <code>keep.data=TRUE</code>, an object of class <code>BTData</code> containing the <code>training.set</code> and <code>validation.set</code> (can be NULL if not used). These data frames are reduced
to the used variables, that are the response and explanatory variables. Note that in case of cross-validation, even if <code>keep.data=TRUE</code> the folds will not be kept. In fact, only the data
frames related to the original fit (i.e. on the whole training set) will be saved.</p>
</td></tr>
<tr><td><code>BTParams</code></td>
<td>
<p>an object of class <code>BTParams</code> containing all the (Adaptive) boosting tree parameters. More precisely, it contains the <code>ABT</code>, <code>train.fraction</code>,
<code>shrinkage</code>, <code>interaction.depth</code>, <code>bag.fraction</code>, <code>n.iter</code>, <code>colsample.bytree</code> and <code>tree.control</code> parameter values.</p>
</td></tr>
<tr><td><code>keep.data</code></td>
<td>
<p>the <code>keep.data</code> parameter value.</p>
</td></tr>
<tr><td><code>is.verbose</code></td>
<td>
<p>the <code>is.verbose</code> parameter value.</p>
</td></tr>
<tr><td><code>fitted.values</code></td>
<td>
<p>the training set fitted values on the score scale using all the <code>n.iter</code> (and initialization) iterations.</p>
</td></tr>
<tr><td><code>cv.folds</code></td>
<td>
<p>the number of cross-validation folds. Set to 1 if no cross-validation performed.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>the original call to the <code>BT</code> algorithm.</p>
</td></tr>
<tr><td><code>Terms</code></td>
<td>
<p>the <code>model.frame</code> terms argument.</p>
</td></tr>
<tr><td><code>folds</code></td>
<td>
<p>a vector of values identifying to which fold each observation is in. This argument is not present if there is no cross-validation. On the other hand, it corresponds
to <code>folds.id</code> if it was initially defined by the user.</p>
</td></tr>
<tr><td><code>cv.fitted</code></td>
<td>
<p>a vector containing the cross-validation fitted values, if a cross-validation was performed. More precisely, for a given observation, the prediction will be furnished by the cv-model
for which this specific observation was out-of-fold. See <code><a href="#topic+predict.BTCVFit">predict.BTCVFit</a></code> for more details.</p>
</td></tr>
</table>


<h3>Structure </h3>

<p>The following components must be included in a legitimate <code>BTFit</code> object.
</p>


<h3>Author(s)</h3>

<p>Gireg Willame <a href="mailto:gireg.willame@gmail.com">gireg.willame@gmail.com</a>
</p>
<p><em>This package is inspired by the <code>gbm3</code> package. For more details, see <a href="https://github.com/gbm-developers/gbm3/">https://github.com/gbm-developers/gbm3/</a></em>.
</p>


<h3>References</h3>

<p>M. Denuit, D. Hainaut and J. Trufin (2019). <strong>Effective Statistical Learning Methods for Actuaries |: GLMs and Extensions</strong>, <em>Springer Actuarial</em>.
</p>
<p>M. Denuit, D. Hainaut and J. Trufin (2019). <strong>Effective Statistical Learning Methods for Actuaries ||: Tree-Based Methods and Extensions</strong>, <em>Springer Actuarial</em>.
</p>
<p>M. Denuit, D. Hainaut and J. Trufin (2019). <strong>Effective Statistical Learning Methods for Actuaries |||: Neural Networks and Extensions</strong>, <em>Springer Actuarial</em>.
</p>
<p>M. Denuit, D. Hainaut and J. Trufin (2022). <strong>Response versus gradient boosting trees, GLMs and neural networks under Tweedie loss and log-link</strong>.
Accepted for publication in <em>Scandinavian Actuarial Journal</em>.
</p>
<p>M. Denuit, J. Huyghe and J. Trufin (2022). <strong>Boosting cost-complexity pruned trees on Tweedie responses: The ABT machine for insurance ratemaking</strong>.
Paper submitted for publication.
</p>
<p>M. Denuit, J. Trufin and T. Verdebout (2022). <strong>Boosting on the responses with Tweedie loss functions</strong>. Paper submitted for publication.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+BT">BT</a></code>.
</p>

<hr>
<h2 id='predict.BTCVFit'>Predictions for CV fitted BT models.</h2><span id='topic+predict.BTCVFit'></span>

<h3>Description</h3>

<p>Compute predictions from cross-validated Boosting Trees model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'BTCVFit'
predict(object, data, cv.folds, folds, best.iter.cv, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.BTCVFit_+3A_object">object</code></td>
<td>
<p>a <code><a href="#topic+BTCVFit">BTCVFit</a></code> object containing CV BT models.</p>
</td></tr>
<tr><td><code id="predict.BTCVFit_+3A_data">data</code></td>
<td>
<p>the database on which one wants to predict the different CV BT models.</p>
</td></tr>
<tr><td><code id="predict.BTCVFit_+3A_cv.folds">cv.folds</code></td>
<td>
<p>a positive integer specifying the number of folds to be used in cross-validation of the BT fit.</p>
</td></tr>
<tr><td><code id="predict.BTCVFit_+3A_folds">folds</code></td>
<td>
<p>vector of integers specifying which row of data belongs to which cv.folds.</p>
</td></tr>
<tr><td><code id="predict.BTCVFit_+3A_best.iter.cv">best.iter.cv</code></td>
<td>
<p>the optimal number of trees with a CV approach.</p>
</td></tr>
<tr><td><code id="predict.BTCVFit_+3A_...">...</code></td>
<td>
<p>not currently used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function has not been coded for public usage but rather to assess the cross-validation performances.
</p>


<h3>Value</h3>

<p>Returns a vector of predictions for each cv folds.
</p>


<h3>Author(s)</h3>

<p>Gireg Willame <a href="mailto:gireg.willame@gmail.com">gireg.willame@gmail.com</a>
</p>
<p><em>This package is inspired by the <code>gbm3</code> package. For more details, see <a href="https://github.com/gbm-developers/gbm3/">https://github.com/gbm-developers/gbm3/</a></em>.
</p>


<h3>References</h3>

<p>M. Denuit, D. Hainaut and J. Trufin (2019). <strong>Effective Statistical Learning Methods for Actuaries |: GLMs and Extensions</strong>, <em>Springer Actuarial</em>.
</p>
<p>M. Denuit, D. Hainaut and J. Trufin (2019). <strong>Effective Statistical Learning Methods for Actuaries ||: Tree-Based Methods and Extensions</strong>, <em>Springer Actuarial</em>.
</p>
<p>M. Denuit, D. Hainaut and J. Trufin (2019). <strong>Effective Statistical Learning Methods for Actuaries |||: Neural Networks and Extensions</strong>, <em>Springer Actuarial</em>.
</p>
<p>M. Denuit, D. Hainaut and J. Trufin (2022). <strong>Response versus gradient boosting trees, GLMs and neural networks under Tweedie loss and log-link</strong>.
Accepted for publication in <em>Scandinavian Actuarial Journal</em>.
</p>
<p>M. Denuit, J. Huyghe and J. Trufin (2022). <strong>Boosting cost-complexity pruned trees on Tweedie responses: The ABT machine for insurance ratemaking</strong>.
Paper submitted for publication.
</p>
<p>M. Denuit, J. Trufin and T. Verdebout (2022). <strong>Boosting on the responses with Tweedie loss functions</strong>. Paper submitted for publication.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+BT">BT</a></code>, <code><a href="#topic+BTFit">BTFit</a></code>.
</p>

<hr>
<h2 id='predict.BTFit'>Predict method for BT Model fits.</h2><span id='topic+predict.BTFit'></span>

<h3>Description</h3>

<p>Predicted values based on a boosting tree model object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'BTFit'
predict(object, newdata, n.iter, type = "link", single.iter = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.BTFit_+3A_object">object</code></td>
<td>
<p>a <code><a href="#topic+BTFit">BTFit</a></code> object.</p>
</td></tr>
<tr><td><code id="predict.BTFit_+3A_newdata">newdata</code></td>
<td>
<p>data frame of observations for which to make predictions. If missing or not a data frame, if <code>keep.data=TRUE</code> in the initial fit then the original training set will be used.</p>
</td></tr>
<tr><td><code id="predict.BTFit_+3A_n.iter">n.iter</code></td>
<td>
<p>number of boosting iterations used for the prediction. This parameter can be a vector in which case predictions are returned for each iteration specified.</p>
</td></tr>
<tr><td><code id="predict.BTFit_+3A_type">type</code></td>
<td>
<p>the scale on which the BT makes the predictions. Can either be &quot;link&quot; or &quot;response&quot;. Note that, by construction, a log-link function is used during the fit.</p>
</td></tr>
<tr><td><code id="predict.BTFit_+3A_single.iter">single.iter</code></td>
<td>
<p>if <code>single.iter=TRUE</code> then <code>predict.BTFit</code> returns the predictions from the single tree <code>n.iter</code>.</p>
</td></tr>
<tr><td><code id="predict.BTFit_+3A_...">...</code></td>
<td>
<p>not currently used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>predict.BTFit</code> produces a predicted values for each observation in <code>newdata</code> using the first <code>n.iter</code> boosting iterations.
If <code>n.iter</code> is a vector then the result is a matrix with each column corresponding to the <code>BT</code> predictions with <code>n.iter[1]</code> boosting iterations, <code>n.iter[2]</code> boosting
iterations, and so on.
</p>
<p>As for the fit, the predictions do not include any offset term.
In the Poisson case, please remind that a weighted approach is initially favored.
</p>


<h3>Value</h3>

<p>Returns a vector of predictions. By default, the predictions are on the score scale.
If <code>type = "response"</code>, then <code>BT</code> converts back to the same scale as the outcome. Note that, a log-link is supposed by construction.
</p>


<h3>Author(s)</h3>

<p>Gireg Willame <a href="mailto:gireg.willame@gmail.com">gireg.willame@gmail.com</a>
</p>
<p><em>This package is inspired by the <code>gbm3</code> package. For more details, see <a href="https://github.com/gbm-developers/gbm3/">https://github.com/gbm-developers/gbm3/</a></em>.
</p>


<h3>References</h3>

<p>M. Denuit, D. Hainaut and J. Trufin (2019). <strong>Effective Statistical Learning Methods for Actuaries |: GLMs and Extensions</strong>, <em>Springer Actuarial</em>.
</p>
<p>M. Denuit, D. Hainaut and J. Trufin (2019). <strong>Effective Statistical Learning Methods for Actuaries ||: Tree-Based Methods and Extensions</strong>, <em>Springer Actuarial</em>.
</p>
<p>M. Denuit, D. Hainaut and J. Trufin (2019). <strong>Effective Statistical Learning Methods for Actuaries |||: Neural Networks and Extensions</strong>, <em>Springer Actuarial</em>.
</p>
<p>M. Denuit, D. Hainaut and J. Trufin (2022). <strong>Response versus gradient boosting trees, GLMs and neural networks under Tweedie loss and log-link</strong>.
Accepted for publication in <em>Scandinavian Actuarial Journal</em>.
</p>
<p>M. Denuit, J. Huyghe and J. Trufin (2022). <strong>Boosting cost-complexity pruned trees on Tweedie responses: The ABT machine for insurance ratemaking</strong>.
Paper submitted for publication.
</p>
<p>M. Denuit, J. Trufin and T. Verdebout (2022). <strong>Boosting on the responses with Tweedie loss functions</strong>. Paper submitted for publication.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+BT">BT</a></code>, <code><a href="#topic+BTFit">BTFit</a></code>.
</p>

<hr>
<h2 id='print.BTFit'>Printing function.</h2><span id='topic+print.BTFit'></span>

<h3>Description</h3>

<p>Function to print the BT results.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'BTFit'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print.BTFit_+3A_x">x</code></td>
<td>
<p>a <code><a href="#topic+BTFit">BTFit</a></code> object.</p>
</td></tr>
<tr><td><code id="print.BTFit_+3A_...">...</code></td>
<td>
<p>arguments passed to <code>print.default</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Print the different input parameters as well as obtained results (best iteration/performance &amp; relative influence) given the chosen approach.
</p>


<h3>Value</h3>

<p>No value returned.
</p>


<h3>Author(s)</h3>

<p>Gireg Willame <a href="mailto:gireg.willame@gmail.com">gireg.willame@gmail.com</a>
</p>
<p><em>This package is inspired by the <code>gbm3</code> package. For more details, see <a href="https://github.com/gbm-developers/gbm3/">https://github.com/gbm-developers/gbm3/</a></em>.
</p>


<h3>References</h3>

<p>M. Denuit, D. Hainaut and J. Trufin (2019). <strong>Effective Statistical Learning Methods for Actuaries |: GLMs and Extensions</strong>, <em>Springer Actuarial</em>.
</p>
<p>M. Denuit, D. Hainaut and J. Trufin (2019). <strong>Effective Statistical Learning Methods for Actuaries ||: Tree-Based Methods and Extensions</strong>, <em>Springer Actuarial</em>.
</p>
<p>M. Denuit, D. Hainaut and J. Trufin (2019). <strong>Effective Statistical Learning Methods for Actuaries |||: Neural Networks and Extensions</strong>, <em>Springer Actuarial</em>.
</p>
<p>M. Denuit, D. Hainaut and J. Trufin (2022). <strong>Response versus gradient boosting trees, GLMs and neural networks under Tweedie loss and log-link</strong>.
Accepted for publication in <em>Scandinavian Actuarial Journal</em>.
</p>
<p>M. Denuit, J. Huyghe and J. Trufin (2022). <strong>Boosting cost-complexity pruned trees on Tweedie responses: The ABT machine for insurance ratemaking</strong>.
Paper submitted for publication.
</p>
<p>M. Denuit, J. Trufin and T. Verdebout (2022). <strong>Boosting on the responses with Tweedie loss functions</strong>. Paper submitted for publication.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+BT">BT</a></code>, <code><a href="#topic+.BT_relative_influence">.BT_relative_influence</a></code>, <code><a href="#topic+BT_perf">BT_perf</a></code>.
</p>

<hr>
<h2 id='summary.BTFit'>Summary of a BTFit object.</h2><span id='topic+summary.BTFit'></span>

<h3>Description</h3>

<p>Computes the relative influence of each variable in the BTFit object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'BTFit'
summary(
  object,
  cBars = length(object$var.names),
  n.iter = object$BTParams$n.iter,
  plot_it = TRUE,
  order_it = TRUE,
  method = .BT_relative_influence,
  normalize = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="summary.BTFit_+3A_object">object</code></td>
<td>
<p>a <code><a href="#topic+BTFit">BTFit</a></code> object.</p>
</td></tr>
<tr><td><code id="summary.BTFit_+3A_cbars">cBars</code></td>
<td>
<p>the number of bars to plot. If <code>order=TRUE</code> only the variables with the <code>cBars</code> largest relative influence will appear in the barplot.
If <code>order=FALSE</code> then the first <code>cBars</code> variables will appear in the barplot.</p>
</td></tr>
<tr><td><code id="summary.BTFit_+3A_n.iter">n.iter</code></td>
<td>
<p>the number of trees used to compute the relative influence. Only the first <code>n.iter</code> trees will be used.</p>
</td></tr>
<tr><td><code id="summary.BTFit_+3A_plot_it">plot_it</code></td>
<td>
<p>an indicator as to whether the plot is generated.</p>
</td></tr>
<tr><td><code id="summary.BTFit_+3A_order_it">order_it</code></td>
<td>
<p>an indicator as to whether the plotted and/or returned relative influences are sorted.</p>
</td></tr>
<tr><td><code id="summary.BTFit_+3A_method">method</code></td>
<td>
<p>the function used to compute the relative influence. Currently, only <code><a href="#topic+.BT_relative_influence">.BT_relative_influence</a></code> is available (default value as well).</p>
</td></tr>
<tr><td><code id="summary.BTFit_+3A_normalize">normalize</code></td>
<td>
<p>if <code>TRUE</code> returns the normalized relative influence.</p>
</td></tr>
<tr><td><code id="summary.BTFit_+3A_...">...</code></td>
<td>
<p>additional argument passed to the plot function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Please note that the relative influence for variables having an original <strong>negative</strong> relative influence is forced to 0.
</p>


<h3>Value</h3>

<p>Returns a data frame where the first component is the variable name and the second one is the computed relative influence, normalized to sum up to 100.
Depending on the <code>plot_it</code> value, the relative influence plot will be performed.
</p>


<h3>Author(s)</h3>

<p>Gireg Willame <a href="mailto:gireg.willame@gmail.com">gireg.willame@gmail.com</a>
</p>
<p><em>This package is inspired by the <code>gbm3</code> package. For more details, see <a href="https://github.com/gbm-developers/gbm3/">https://github.com/gbm-developers/gbm3/</a></em>.
</p>


<h3>References</h3>

<p>M. Denuit, D. Hainaut and J. Trufin (2019). <strong>Effective Statistical Learning Methods for Actuaries |: GLMs and Extensions</strong>, <em>Springer Actuarial</em>.
</p>
<p>M. Denuit, D. Hainaut and J. Trufin (2019). <strong>Effective Statistical Learning Methods for Actuaries ||: Tree-Based Methods and Extensions</strong>, <em>Springer Actuarial</em>.
</p>
<p>M. Denuit, D. Hainaut and J. Trufin (2019). <strong>Effective Statistical Learning Methods for Actuaries |||: Neural Networks and Extensions</strong>, <em>Springer Actuarial</em>.
</p>
<p>M. Denuit, D. Hainaut and J. Trufin (2022). <strong>Response versus gradient boosting trees, GLMs and neural networks under Tweedie loss and log-link</strong>.
Accepted for publication in <em>Scandinavian Actuarial Journal</em>.
</p>
<p>M. Denuit, J. Huyghe and J. Trufin (2022). <strong>Boosting cost-complexity pruned trees on Tweedie responses: The ABT machine for insurance ratemaking</strong>.
Paper submitted for publication.
</p>
<p>M. Denuit, J. Trufin and T. Verdebout (2022). <strong>Boosting on the responses with Tweedie loss functions</strong>. Paper submitted for publication.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+BT">BT</a></code>, <code><a href="#topic+.BT_relative_influence">.BT_relative_influence</a></code>.
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
