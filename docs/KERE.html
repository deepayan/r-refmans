<!DOCTYPE html><html><head><title>Help for package KERE</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {KERE}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#as.kernelMatrix'><p>Assing kernelMatrix class to matrix objects</p></a></li>
<li><a href='#cv.KERE'><p>Cross-validation for KERE</p></a></li>
<li><a href='#dots'><p>Kernel Functions</p></a></li>
<li><a href='#KERE'>
<p>Fits the regularization paths for the kernel expectile regression.</p></a></li>
<li><a href='#kernel-class'><p>Class &quot;kernel&quot; &quot;rbfkernel&quot; &quot;polykernel&quot;, &quot;tanhkernel&quot;, &quot;vanillakernel&quot;</p></a></li>
<li><a href='#kernelMatrix'><p>Kernel Matrix functions</p></a></li>
<li><a href='#plot.KERE'><p>Plot coefficients from a &quot;KERE&quot; object</p></a></li>
<li><a href='#predict.KERE'>
<p>make predictions from a &quot;KERE&quot; object.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Expectile Regression in Reproducing Kernel Hilbert Space</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.0</td>
</tr>
<tr>
<td>Date:</td>
<td>2015-8-26</td>
</tr>
<tr>
<td>Author:</td>
<td>Yi Yang, Teng Zhang, Hui Zou</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Yi Yang &lt;yiyang@umn.edu&gt;</td>
</tr>
<tr>
<td>Depends:</td>
<td>methods</td>
</tr>
<tr>
<td>Description:</td>
<td>An efficient algorithm inspired by majorization-minimization principle for solving the entire solution path of a flexible nonparametric expectile regression estimator constructed in a reproducing kernel Hilbert space.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>Packaged:</td>
<td>2015-08-27 13:38:41 UTC; emeryyi</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2015-08-28 00:35:09</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
</table>
<hr>
<h2 id='as.kernelMatrix'>Assing kernelMatrix class to matrix objects</h2><span id='topic+kernelMatrix-class'></span><span id='topic+as.kernelMatrix'></span><span id='topic+as.kernelMatrix-methods'></span><span id='topic+as.kernelMatrix+2Cmatrix-method'></span>

<h3>Description</h3>

<p><code>as.kernelMatrix</code> in package <span class="pkg">KERE</span> can be used 
to coerce the kernelMatrix class to matrix objects representing a
kernel matrix.  These matrices can then be used with the kernelMatrix
interfaces which most of the functions in <span class="pkg">KERE</span> support.</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'matrix'
as.kernelMatrix(x, center = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="as.kernelMatrix_+3A_x">x</code></td>
<td>
<p>matrix to be assigned the <code>kernelMatrix</code> class </p>
</td></tr>
<tr><td><code id="as.kernelMatrix_+3A_center">center</code></td>
<td>
<p>center the kernel matrix in feature space (default: FALSE) </p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Alexandros Karatzoglou <br />
<a href="mailto:alexandros.karatzoglou@ci.tuwien.ac.at">alexandros.karatzoglou@ci.tuwien.ac.at</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+kernelMatrix">kernelMatrix</a></code>, <code><a href="#topic+dots">dots</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## Create toy data
x &lt;- rbind(matrix(rnorm(10),,2),matrix(rnorm(10,mean=3),,2))
y &lt;- matrix(c(rep(1,5),rep(-1,5)))

### Use as.kernelMatrix to label the cov. matrix as a kernel matrix
### which is eq. to using a linear kernel 

K &lt;- as.kernelMatrix(crossprod(t(x)))

K

</code></pre>

<hr>
<h2 id='cv.KERE'>Cross-validation for KERE
</h2><span id='topic+cv.KERE'></span>

<h3>Description</h3>

<p>Does k-fold cross-validation for <code>KERE</code>, produces a plot, and returns a value for <code>lambda</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'KERE'
cv(x, y, kern, lambda = NULL, nfolds = 5, foldid, omega = 0.5, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cv.KERE_+3A_x">x</code></td>
<td>
<p>matrix of predictors, of dimension <code class="reqn">N \times p</code>; each row is an observation vector.</p>
</td></tr>
<tr><td><code id="cv.KERE_+3A_y">y</code></td>
<td>
<p>response variable.
</p>
</td></tr>
<tr><td><code id="cv.KERE_+3A_kern">kern</code></td>
<td>
<p>the built-in kernel classes in <span class="pkg">KERE</span>. 
The <code>kern</code> parameter can be set to any function, of class
kernel, which computes the inner product in feature space between two
vector arguments. <span class="pkg">KERE</span> provides the most popular kernel functions
which can be initialized by using the following
functions:
</p>

<ul>
<li><p><code>rbfdot</code> Radial Basis kernel function,
</p>
</li>
<li><p><code>polydot</code> Polynomial kernel function,
</p>
</li>
<li><p><code>vanilladot</code> Linear kernel function,
</p>
</li>
<li><p><code>tanhdot</code> Hyperbolic tangent kernel function,
</p>
</li>
<li><p><code>laplacedot</code> Laplacian kernel function,
</p>
</li>
<li><p><code>besseldot</code> Bessel kernel function,
</p>
</li>
<li><p><code>anovadot</code> ANOVA RBF kernel function,
</p>
</li>
<li><p><code>splinedot</code> the Spline kernel. 
</p>
</li></ul>

<p>Objects can be created by calling the rbfdot, polydot, tanhdot, vanilladot, anovadot, besseldot, laplacedot, splinedot functions etc. (see example.)
</p>
</td></tr>
<tr><td><code id="cv.KERE_+3A_lambda">lambda</code></td>
<td>
<p>a user supplied <code>lambda</code> sequence. It is better to supply a decreasing sequence of <code>lambda</code> values, if not, the program will sort user-defined <code>lambda</code> sequence in decreasing order automatically.
</p>
</td></tr>
<tr><td><code id="cv.KERE_+3A_nfolds">nfolds</code></td>
<td>

<p>number of folds - default is 5. Although <code>nfolds</code>
can be as large as the sample size (leave-one-out CV), it is not
recommended for large datasets. Smallest value allowable is <code>nfolds=3</code>.
</p>
</td></tr>
<tr><td><code id="cv.KERE_+3A_foldid">foldid</code></td>
<td>
<p>	an optional vector of values between 1 and <code>nfold</code>
identifying what fold each observation is in. If supplied,
<code>nfold</code> can be missing.</p>
</td></tr>
<tr><td><code id="cv.KERE_+3A_omega">omega</code></td>
<td>
<p>the parameter <code class="reqn">\omega</code> in the expectile regression model. The value must be in (0,1). Default is 0.5.
</p>
</td></tr>
<tr><td><code id="cv.KERE_+3A_...">...</code></td>
<td>
<p>other arguments that can be passed to <code>KERE</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function runs <code><a href="#topic+KERE">KERE</a></code> <code>nfolds</code>+1 times; the
first to get the <code>lambda</code> sequence, and then the remainder to
compute the fit with each of the folds omitted. The average error and standard deviation over the
folds are computed.
</p>


<h3>Value</h3>

<p>an object of class <code><a href="#topic+cv.KERE">cv.KERE</a></code> is returned, which is a
list with the ingredients of the cross-validation fit.
</p>
<table>
<tr><td><code>lambda</code></td>
<td>
<p>the values of <code>lambda</code> used in the fits.</p>
</td></tr>
<tr><td><code>cvm</code></td>
<td>
<p>the mean cross-validated error - a vector of length
<code>length(lambda)</code>.</p>
</td></tr>
<tr><td><code>cvsd</code></td>
<td>
<p>estimate of standard error of <code>cvm</code>.</p>
</td></tr>
<tr><td><code>cvupper</code></td>
<td>
<p>upper curve = <code>cvm+cvsd</code>.</p>
</td></tr>
<tr><td><code>cvlo</code></td>
<td>
<p>lower curve = <code>cvm-cvsd</code>.</p>
</td></tr>
<tr><td><code>name</code></td>
<td>
<p>a character string &quot;Expectile Loss&quot;</p>
</td></tr>
<tr><td><code>lambda.min</code></td>
<td>
<p>the optimal value of <code>lambda</code> that gives minimum
cross validation error <code>cvm</code>.</p>
</td></tr>
<tr><td><code>cvm.min</code></td>
<td>
<p>the minimum
cross validation error <code>cvm</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Yi Yang, Teng Zhang and Hui Zou<br />
Maintainer: Yi Yang  &lt;yiyang@umn.edu&gt;
</p>


<h3>References</h3>

<p>Y. Yang, T. Zhang, and H. Zou. &quot;Flexible Expectile Regression in Reproducing Kernel Hilbert Space.&quot; ArXiv e-prints: stat.ME/1508.05987, August 2015.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
N &lt;- 200
X1 &lt;- runif(N)
X2 &lt;- 2*runif(N)
X3 &lt;- 3*runif(N)
SNR &lt;- 10 # signal-to-noise ratio
Y &lt;- X1**1.5 + 2 * (X2**.5) + X1*X3
sigma &lt;- sqrt(var(Y)/SNR)
Y &lt;- Y + X2*rnorm(N,0,sigma)
X &lt;- cbind(X1,X2,X3)

# set gaussian kernel 
kern &lt;- rbfdot(sigma=0.1)

# define lambda sequence
lambda &lt;- exp(seq(log(0.5),log(0.01),len=10))

cv.KERE(x=X, y=Y, kern, lambda = lambda, nfolds = 5, omega = 0.5)
</code></pre>

<hr>
<h2 id='dots'>Kernel Functions</h2><span id='topic+dots'></span><span id='topic+kernels'></span><span id='topic+rbfdot'></span><span id='topic+polydot'></span><span id='topic+tanhdot'></span><span id='topic+vanilladot'></span><span id='topic+laplacedot'></span><span id='topic+besseldot'></span><span id='topic+anovadot'></span><span id='topic+fourierdot'></span><span id='topic+splinedot'></span><span id='topic+kpar'></span><span id='topic+kfunction'></span><span id='topic+show+2Ckernel-method'></span>

<h3>Description</h3>

<p>The kernel generating functions provided in KERE. <br />
The Gaussian RBF kernel <code class="reqn">k(x,x') = \exp(-\sigma \|x - x'\|^2)</code> <br />
The Polynomial kernel <code class="reqn">k(x,x') = (scale &lt;x, x'&gt; + offset)^{degree}</code><br />
The Linear kernel <code class="reqn">k(x,x') = &lt;x, x'&gt;</code><br />
The Hyperbolic tangent kernel <code class="reqn">k(x, x') = \tanh(scale &lt;x, x'&gt; +  offset)</code><br />
The Laplacian kernel <code class="reqn">k(x,x') = \exp(-\sigma \|x - x'\|)</code> <br />
The Bessel kernel <code class="reqn">k(x,x') = (- Bessel_{(\nu+1)}^n \sigma \|x - x'\|^2)</code> <br />
The ANOVA RBF kernel <code class="reqn">k(x,x') = \sum_{1\leq i_1 \ldots &lt; i_D \leq
      N} \prod_{d=1}^D k(x_{id}, {x'}_{id})</code> where k(x,x) is a Gaussian
RBF kernel. <br />
The Spline kernel <code class="reqn"> \prod_{d=1}^D 1 + x_i x_j + x_i x_j min(x_i,
    x_j)  - \frac{x_i + x_j}{2} min(x_i,x_j)^2 +
    \frac{min(x_i,x_j)^3}{3}</code> \
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rbfdot(sigma = 1)

polydot(degree = 1, scale = 1, offset = 1)

tanhdot(scale = 1, offset = 1)

vanilladot()

laplacedot(sigma = 1)

besseldot(sigma = 1, order = 1, degree = 1)

anovadot(sigma = 1, degree = 1)

splinedot()
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dots_+3A_sigma">sigma</code></td>
<td>
<p>The inverse kernel width used by the Gaussian the
Laplacian, the Bessel and the ANOVA kernel </p>
</td></tr>
<tr><td><code id="dots_+3A_degree">degree</code></td>
<td>
<p>The degree of the polynomial, bessel or ANOVA
kernel function. This has to be an positive integer.</p>
</td></tr>
<tr><td><code id="dots_+3A_scale">scale</code></td>
<td>
<p>The scaling parameter of the polynomial and tangent
kernel is a convenient way of normalizing
patterns without the need to modify the data itself</p>
</td></tr>
<tr><td><code id="dots_+3A_offset">offset</code></td>
<td>
<p>The offset used in a polynomial or hyperbolic tangent
kernel</p>
</td></tr>
<tr><td><code id="dots_+3A_order">order</code></td>
<td>
<p>The order of the Bessel function to be used as a kernel</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The kernel generating functions are used to initialize a kernel
function
which calculates the dot (inner) product between two feature vectors in a
Hilbert Space. These functions can be passed as a <code>kernel</code> argument on almost all
functions in <span class="pkg">KERE</span>(e.g., <code>ksvm</code>, <code>kpca</code>  etc).
</p>
<p>Although using one of the existing kernel functions as a
<code>kernel</code> argument in various functions in <span class="pkg">KERE</span> has the
advantage that optimized code is used to calculate various kernel expressions,
any other function implementing a dot product of class <code>kernel</code> can also be used as a kernel
argument. This allows the user to use, test and develop special kernels
for a given data set or algorithm.
</p>


<h3>Value</h3>

<p>Return an S4 object of class <code>kernel</code> which extents the
<code>function</code> class. The resulting function implements the given
kernel calculating the inner (dot) product between two vectors.
</p>
<table>
<tr><td><code>kpar</code></td>
<td>
<p>a list containing the kernel parameters (hyperparameters)
used.</p>
</td></tr>
</table>
<p>The kernel parameters can be accessed by the <code>kpar</code> function.
</p>


<h3>Note</h3>

<p>If the offset in the Polynomial kernel is set to 0, we obtain homogeneous polynomial
kernels, for positive values, we have inhomogeneous
kernels. Note that for negative values the kernel does not satisfy Mercer's
condition and thus the optimizers may fail. <br />
</p>
<p>In the Hyperbolic tangent kernel if the offset is negative the likelihood of obtaining a kernel
matrix that is not positive definite is much higher (since then even some
diagonal elements may be negative), hence if this kernel has to be used, the
offset should always be positive. Note, however, that this is no guarantee
that the kernel will be positive.
</p>


<h3>Author(s)</h3>

<p>Alexandros Karatzoglou<br />
<a href="mailto:alexandros.karatzoglou@ci.tuwien.ac.at">alexandros.karatzoglou@ci.tuwien.ac.at</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+kernelMatrix">kernelMatrix</a> </code>, <code><a href="#topic+kernelMult">kernelMult</a></code>, <code><a href="#topic+kernelPol">kernelPol</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>rbfkernel &lt;- rbfdot(sigma = 0.1)
rbfkernel

kpar(rbfkernel)

## create two vectors
x &lt;- rnorm(10)
y &lt;- rnorm(10)

## calculate dot product
rbfkernel(x,y)

</code></pre>

<hr>
<h2 id='KERE'>
Fits the regularization paths for the kernel expectile regression.</h2><span id='topic+KERE'></span>

<h3>Description</h3>

<p>Fits a regularization path for the kernel expectile regression at a sequence of regularization parameters lambda.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>KERE(x, y, kern, lambda = NULL, eps = 1e-08, maxit = 1e4,
omega = 0.5, gamma = 1e-06, option = c("fast", "normal"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="KERE_+3A_x">x</code></td>
<td>
<p>matrix of predictors, of dimension <code class="reqn">N \times p</code>; each row is an observation vector.</p>
</td></tr>
<tr><td><code id="KERE_+3A_y">y</code></td>
<td>
<p>response variable.
</p>
</td></tr>
<tr><td><code id="KERE_+3A_kern">kern</code></td>
<td>
<p>the built-in kernel classes in <span class="pkg">KERE</span>. 
The <code>kern</code> parameter can be set to any function, of class
kernel, which computes the inner product in feature space between two
vector arguments. <span class="pkg">KERE</span> provides the most popular kernel functions
which can be initialized by using the following
functions:
</p>

<ul>
<li><p><code>rbfdot</code> Radial Basis kernel function,
</p>
</li>
<li><p><code>polydot</code> Polynomial kernel function,
</p>
</li>
<li><p><code>vanilladot</code> Linear kernel function,
</p>
</li>
<li><p><code>tanhdot</code> Hyperbolic tangent kernel function,
</p>
</li>
<li><p><code>laplacedot</code> Laplacian kernel function,
</p>
</li>
<li><p><code>besseldot</code> Bessel kernel function,
</p>
</li>
<li><p><code>anovadot</code> ANOVA RBF kernel function,
</p>
</li>
<li><p><code>splinedot</code> the Spline kernel. 
</p>
</li></ul>

<p>Objects can be created by calling the rbfdot, polydot, tanhdot, vanilladot, anovadot, besseldot, laplacedot, splinedot functions etc. (see example.)
</p>
</td></tr>
<tr><td><code id="KERE_+3A_lambda">lambda</code></td>
<td>
<p>a user supplied <code>lambda</code> sequence. It is better to supply a decreasing sequence of <code>lambda</code> values, if not, the program will sort user-defined <code>lambda</code> sequence in decreasing order automatically.
</p>
</td></tr>
<tr><td><code id="KERE_+3A_eps">eps</code></td>
<td>

<p>convergence threshold for majorization minimization algorithm. Each majorization descent loop continues until the relative change in any coefficient <code class="reqn">||alpha(new)-\alpha(old)||_2^2/||\alpha(old)||_2^2</code> is less than <code>eps</code>. Defaults value is <code>1e-8</code>.
</p>
</td></tr>
<tr><td><code id="KERE_+3A_maxit">maxit</code></td>
<td>

<p>maximum number of loop iterations allowed at fixed lambda value. Default is 1e4. If models do not converge, consider increasing <code>maxit</code>.
</p>
</td></tr>
<tr><td><code id="KERE_+3A_omega">omega</code></td>
<td>
<p>the parameter <code class="reqn">\omega</code> in the expectile regression model. The value must be in (0,1). Default is 0.5.
</p>
</td></tr>
<tr><td><code id="KERE_+3A_gamma">gamma</code></td>
<td>

<p>a scalar number. If it is specified, the number will be added to each diagonal element of the kernel matrix as perturbation. The default is <code>1e-06</code>.
</p>
</td></tr>
<tr><td><code id="KERE_+3A_option">option</code></td>
<td>
<p>users can choose which method to use to update the inverse matrix in the MM algorithm. <code>"fast"</code> uses a trick described in Yang, Zhang and Zou (2015) to update estimates for each lambda. <code>"normal"</code> uses a naive way for the computation.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note that the objective function in <code>KERE</code> is
</p>
<p style="text-align: center;"><code class="reqn">Loss(y- \alpha_0 - K * \alpha )) + \lambda * \alpha^T * K * \alpha,</code>
</p>

<p>where the <code class="reqn">\alpha_0</code> is the intercept, <code class="reqn">\alpha</code> is the solution vector, and <code class="reqn">K</code> is the kernel matrix with <code class="reqn">K_{ij}=K(x_i,x_j)</code>. Users can specify the kernel function to use, options include Radial Basis kernel, Polynomial kernel, Linear kernel, Hyperbolic tangent kernel, Laplacian kernel, Bessel kernel, ANOVA RBF kernel, the Spline kernel. Users can also tweak the penalty by choosing different <code class="reqn">lambda</code>. 
</p>
<p>For computing speed reason, if models are not converging or running slow, consider increasing <code>eps</code> before increasing <code>maxit</code>.
</p>


<h3>Value</h3>

<p>An object with S3 class <code><a href="#topic+KERE">KERE</a></code>.
</p>
<table>
<tr><td><code>call</code></td>
<td>
<p>the call that produced this object.</p>
</td></tr>
<tr><td><code>alpha</code></td>
<td>
<p>a <code>nrow(x)*length(lambda)</code> matrix of coefficients. Each column is a solution vector corresponding to a lambda value in the lambda sequence.</p>
</td></tr>
<tr><td><code>lambda</code></td>
<td>
<p>the actual sequence of <code>lambda</code> values used.</p>
</td></tr>
<tr><td><code>npass</code></td>
<td>
<p>total number of loop iterations corresponding to each lambda value.</p>
</td></tr>
<tr><td><code>jerr</code></td>
<td>
<p>error flag, for warnings and errors, 0 if no error.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Yi Yang, Teng Zhang and Hui Zou<br />
Maintainer: Yi Yang  &lt;yiyang@umn.edu&gt;
</p>


<h3>References</h3>

<p>Y. Yang, T. Zhang, and H. Zou. &quot;Flexible Expectile Regression in Reproducing Kernel Hilbert Space.&quot; ArXiv e-prints: stat.ME/1508.05987, August 2015.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># create data
N &lt;- 200
X1 &lt;- runif(N)
X2 &lt;- 2*runif(N)
X3 &lt;- 3*runif(N)
SNR &lt;- 10 # signal-to-noise ratio
Y &lt;- X1**1.5 + 2 * (X2**.5) + X1*X3
sigma &lt;- sqrt(var(Y)/SNR)
Y &lt;- Y + X2*rnorm(N,0,sigma)
X &lt;- cbind(X1,X2,X3)

# set gaussian kernel 
kern &lt;- rbfdot(sigma=0.1)

# define lambda sequence
lambda &lt;- exp(seq(log(0.5),log(0.01),len=10))

# run KERE
m1 &lt;- KERE(x=X, y=Y, kern=kern, lambda = lambda, omega = 0.5) 

# plot the solution paths
plot(m1)

</code></pre>

<hr>
<h2 id='kernel-class'>Class &quot;kernel&quot; &quot;rbfkernel&quot; &quot;polykernel&quot;, &quot;tanhkernel&quot;, &quot;vanillakernel&quot;</h2><span id='topic+rbfkernel-class'></span><span id='topic+polykernel-class'></span><span id='topic+vanillakernel-class'></span><span id='topic+tanhkernel-class'></span><span id='topic+anovakernel-class'></span><span id='topic+besselkernel-class'></span><span id='topic+laplacekernel-class'></span><span id='topic+splinekernel-class'></span><span id='topic+fourierkernel-class'></span><span id='topic+kfunction-class'></span><span id='topic+kernel-class'></span><span id='topic+kpar+2Ckernel-method'></span>

<h3>Description</h3>

<p>  The built-in kernel classes in <span class="pkg">KERE</span></p>


<h3>Objects from the Class</h3>

<p>Objects can be created by calls of the form <code>new("rbfkernel")</code>,
<code>new{"polykernel"}</code>, <code>new{"tanhkernel"}</code>,
<code>new{"vanillakernel"}</code>, <code>new{"anovakernel"}</code>,
<code>new{"besselkernel"}</code>, <code>new{"laplacekernel"}</code>,
<code>new{"splinekernel"}</code> or by calling the <code>rbfdot</code>, <code>polydot</code>, <code>tanhdot</code>,
<code>vanilladot</code>, <code>anovadot</code>, <code>besseldot</code>, <code>laplacedot</code>,
<code>splinedot</code> functions etc..
</p>


<h3>Slots</h3>


<dl>
<dt><code>.Data</code>:</dt><dd><p>Object of class <code>"function"</code> containing
the kernel function </p>
</dd>
<dt><code>kpar</code>:</dt><dd><p>Object of class <code>"list"</code> containing the
kernel parameters </p>
</dd>
</dl>



<h3>Extends</h3>

<p>Class <code>"kernel"</code>, directly.
Class <code>"function"</code>, by class <code>"kernel"</code>.
</p>


<h3>Methods</h3>


<dl>
<dt>kernelMatrix</dt><dd><p><code>signature(kernel = "rbfkernel", x =
	"matrix")</code>: computes the kernel matrix</p>
</dd>
<dt>kernelMult</dt><dd><p><code>signature(kernel = "rbfkernel", x =
	"matrix")</code>: computes the quadratic kernel expression</p>
</dd>
<dt>kernelPol</dt><dd><p><code>signature(kernel = "rbfkernel", x =
	"matrix")</code>: computes the kernel expansion</p>
</dd>
<dt>kernelFast</dt><dd><p><code>signature(kernel = "rbfkernel", x =
	"matrix"),,a</code>: computes parts or the full kernel matrix, mainly
used in kernel algorithms where columns of the kernel matrix are
computed per invocation </p>
</dd>
</dl>



<h3>Author(s)</h3>

<p>Alexandros Karatzoglou<br /> <a href="mailto:alexandros.karatzoglou@ci.tuwien.ac.at">alexandros.karatzoglou@ci.tuwien.ac.at</a> </p>


<h3>See Also</h3>

<p><code><a href="#topic+dots">dots</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
rbfkernel &lt;- rbfdot(sigma = 0.1)
rbfkernel
is(rbfkernel)
kpar(rbfkernel)

</code></pre>

<hr>
<h2 id='kernelMatrix'>Kernel Matrix functions</h2><span id='topic+kernelMatrix'></span><span id='topic+kernelMult'></span><span id='topic+kernelPol'></span><span id='topic+kernelFast'></span><span id='topic+kernelPol+2Ckernel-method'></span><span id='topic+kernelMatrix+2Ckernel-method'></span><span id='topic+kernelMult+2Ckernel-method'></span><span id='topic+kernelFast+2Ckernel-method'></span><span id='topic+kernelMatrix+2Crbfkernel-method'></span><span id='topic+kernelMatrix+2Cpolykernel-method'></span><span id='topic+kernelMatrix+2Cvanillakernel-method'></span><span id='topic+kernelMatrix+2Ctanhkernel-method'></span><span id='topic+kernelMatrix+2Claplacekernel-method'></span><span id='topic+kernelMatrix+2Canovakernel-method'></span><span id='topic+kernelMatrix+2Csplinekernel-method'></span><span id='topic+kernelMatrix+2Cbesselkernel-method'></span><span id='topic+kernelMult+2Crbfkernel+2CANY-method'></span><span id='topic+kernelMult+2Csplinekernel+2CANY-method'></span><span id='topic+kernelMult+2Cpolykernel+2CANY-method'></span><span id='topic+kernelMult+2Ctanhkernel+2CANY-method'></span><span id='topic+kernelMult+2Claplacekernel+2CANY-method'></span><span id='topic+kernelMult+2Cbesselkernel+2CANY-method'></span><span id='topic+kernelMult+2Canovakernel+2CANY-method'></span><span id='topic+kernelMult+2Cvanillakernel+2CANY-method'></span><span id='topic+kernelMult+2Ccharacter+2CkernelMatrix-method'></span><span id='topic+kernelPol+2Crbfkernel-method'></span><span id='topic+kernelPol+2Csplinekernel-method'></span><span id='topic+kernelPol+2Cpolykernel-method'></span><span id='topic+kernelPol+2Ctanhkernel-method'></span><span id='topic+kernelPol+2Cvanillakernel-method'></span><span id='topic+kernelPol+2Canovakernel-method'></span><span id='topic+kernelPol+2Cbesselkernel-method'></span><span id='topic+kernelPol+2Claplacekernel-method'></span><span id='topic+kernelFast+2Crbfkernel-method'></span><span id='topic+kernelFast+2Csplinekernel-method'></span><span id='topic+kernelFast+2Cpolykernel-method'></span><span id='topic+kernelFast+2Ctanhkernel-method'></span><span id='topic+kernelFast+2Cvanillakernel-method'></span><span id='topic+kernelFast+2Canovakernel-method'></span><span id='topic+kernelFast+2Cbesselkernel-method'></span><span id='topic+kernelFast+2Claplacekernel-method'></span><span id='topic+kernelFast+2Csplinekernel-method'></span>

<h3>Description</h3>

<p><code>kernelMatrix</code> calculates the kernel matrix <code class="reqn">K_{ij} = k(x_i,x_j)</code> or <code class="reqn">K_{ij} =
    k(x_i,y_j)</code>.<br />
<code>kernelPol</code> computes the quadratic kernel expression  <code class="reqn">H = z_i z_j
    k(x_i,x_j)</code>, <code class="reqn">H = z_i k_j k(x_i,y_j)</code>.<br />
<code>kernelMult</code> calculates the kernel expansion <code class="reqn">f(x_i) =
      \sum_{i=1}^m z_i  k(x_i,x_j)</code><br />
<code>kernelFast</code> computes the kernel matrix, identical
to <code>kernelMatrix</code>, except that it also requires the squared
norm of the first argument as additional input, useful in iterative
kernel matrix calculations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'kernel'
kernelMatrix(kernel, x, y = NULL)

## S4 method for signature 'kernel'
kernelPol(kernel, x, y = NULL, z, k = NULL)

## S4 method for signature 'kernel'
kernelMult(kernel, x, y = NULL, z, blocksize = 256)

## S4 method for signature 'kernel'
kernelFast(kernel, x, y, a)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kernelMatrix_+3A_kernel">kernel</code></td>
<td>
<p>the kernel function to be used to calculate the kernel
matrix.
This has to be a function of class <code>kernel</code>, i.e. which can be
generated either one of the build in 
kernel generating functions (e.g., <code>rbfdot</code> etc.) or a user defined
function of class <code>kernel</code> taking two vector arguments and returning a scalar.</p>
</td></tr>
<tr><td><code id="kernelMatrix_+3A_x">x</code></td>
<td>
<p>a data matrix to be used to calculate the kernel matrix.</p>
</td></tr>
<tr><td><code id="kernelMatrix_+3A_y">y</code></td>
<td>
<p>second data matrix to calculate the kernel matrix.</p>
</td></tr>
<tr><td><code id="kernelMatrix_+3A_z">z</code></td>
<td>
<p>a suitable vector or matrix</p>
</td></tr>
<tr><td><code id="kernelMatrix_+3A_k">k</code></td>
<td>
<p>a suitable vector or matrix</p>
</td></tr>
<tr><td><code id="kernelMatrix_+3A_a">a</code></td>
<td>
<p>the squared norm of <code>x</code>, e.g., <code>rowSums(x^2)</code></p>
</td></tr>
<tr><td><code id="kernelMatrix_+3A_blocksize">blocksize</code></td>
<td>
<p>the kernel expansion computations are done block wise
to avoid storing the kernel matrix into memory. <code>blocksize</code>
defines the size of the computational blocks.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Common functions used during kernel based computations.<br />
The <code>kernel</code> parameter can be set to any function, of class
kernel, which computes the inner product in feature space between two
vector arguments. <span class="pkg">KERE</span> provides the most popular kernel functions
which can be initialized by using the following
functions:
</p>

<ul>
<li> <p><code>rbfdot</code> Radial Basis kernel function
</p>
</li>
<li> <p><code>polydot</code> Polynomial kernel function
</p>
</li>
<li> <p><code>vanilladot</code> Linear kernel function
</p>
</li>
<li> <p><code>tanhdot</code> Hyperbolic tangent kernel function
</p>
</li>
<li> <p><code>laplacedot</code> Laplacian kernel function
</p>
</li>
<li> <p><code>besseldot</code> Bessel kernel function
</p>
</li>
<li> <p><code>anovadot</code> ANOVA RBF kernel function
</p>
</li>
<li> <p><code>splinedot</code> the Spline kernel 
</p>
</li></ul>
<p>  (see example.)
</p>
<p><code>kernelFast</code> is mainly used in situations where columns of the
kernel matrix are computed per invocation. In these cases,
evaluating the norm of each row-entry over and over again would
cause significant computational overhead.
</p>


<h3>Value</h3>

<p><code>kernelMatrix</code> returns a symmetric diagonal semi-definite matrix.<br />
<code>kernelPol</code> returns a matrix.<br />
<code>kernelMult</code> usually returns a one-column matrix.
</p>


<h3>Author(s)</h3>

<p>Alexandros Karatzoglou <br />
<a href="mailto:alexandros.karatzoglou@ci.tuwien.ac.at">alexandros.karatzoglou@ci.tuwien.ac.at</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+rbfdot">rbfdot</a></code>, <code><a href="#topic+polydot">polydot</a></code>,
<code><a href="#topic+tanhdot">tanhdot</a></code>, <code><a href="#topic+vanilladot">vanilladot</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>## use the spam data
x &lt;- matrix(rnorm(10*10),10,10)

## initialize kernel function 
rbf &lt;- rbfdot(sigma = 0.05)
rbf

## calculate kernel matrix
kernelMatrix(rbf, x)

y &lt;- matrix(rnorm(10*1),10,1)


## calculate the quadratic kernel expression
kernelPol(rbf, x, ,y)

## calculate the kernel expansion
kernelMult(rbf, x, ,y)
</code></pre>

<hr>
<h2 id='plot.KERE'>Plot coefficients from a &quot;KERE&quot; object
</h2><span id='topic+plot.KERE'></span>

<h3>Description</h3>

<p>Produces a coefficient profile plot of the coefficient paths for a
fitted <code><a href="#topic+KERE">KERE</a></code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'KERE'
plot(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.KERE_+3A_x">x</code></td>
<td>
<p>fitted <code><a href="#topic+KERE">KERE</a></code> model.</p>
</td></tr>
<tr><td><code id="plot.KERE_+3A_...">...</code></td>
<td>
<p>other graphical parameters to plot.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A coefficient profile plot is produced. The x-axis is <code class="reqn">log(\lambda)</code>. The y-axis is the value of fitted <code class="reqn">\alpha</code>'s.
</p>


<h3>Author(s)</h3>

<p>Yi Yang, Teng Zhang and Hui Zou<br />
Maintainer: Yi Yang  &lt;yiyang@umn.edu&gt;
</p>


<h3>References</h3>

<p>Y. Yang, T. Zhang, and H. Zou. &quot;Flexible Expectile Regression in Reproducing Kernel Hilbert Space.&quot; ArXiv e-prints: stat.ME/1508.05987, August 2015.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># create data
N &lt;- 200
X1 &lt;- runif(N)
X2 &lt;- 2*runif(N)
X3 &lt;- 3*runif(N)
SNR &lt;- 10 # signal-to-noise ratio
Y &lt;- X1**1.5 + 2 * (X2**.5) + X1*X3
sigma &lt;- sqrt(var(Y)/SNR)
Y &lt;- Y + X2*rnorm(N,0,sigma)
X &lt;- cbind(X1,X2,X3)

# set gaussian kernel 
kern &lt;- rbfdot(sigma=0.1)

# define lambda sequence
lambda &lt;- exp(seq(log(0.5),log(0.01),len=10))

# run KERE
m1 &lt;- KERE(x=X, y=Y, kern=kern, lambda = lambda, omega = 0.5) 

# plot the solution paths
plot(m1)
</code></pre>

<hr>
<h2 id='predict.KERE'>
make predictions from a &quot;KERE&quot; object.</h2><span id='topic+predict.KERE'></span>

<h3>Description</h3>

<p>Similar to other predict methods, this functions predicts fitted values and class labels from a fitted <code><a href="#topic+KERE">KERE</a></code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'KERE'
predict(object, kern, x, newx,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.KERE_+3A_object">object</code></td>
<td>
<p>fitted <code><a href="#topic+KERE">KERE</a></code> model object.</p>
</td></tr>
<tr><td><code id="predict.KERE_+3A_kern">kern</code></td>
<td>
<p>the built-in kernel classes in <span class="pkg">KERE</span>. Objects can be created by calling the rbfdot, polydot, tanhdot, vanilladot, anovadot, besseldot, laplacedot, splinedot functions etc. (see example.)
</p>
</td></tr>
<tr><td><code id="predict.KERE_+3A_x">x</code></td>
<td>
<p>the original design matrix for training <code>KERE</code>.
</p>
</td></tr>
<tr><td><code id="predict.KERE_+3A_newx">newx</code></td>
<td>
<p>matrix of new values for <code>x</code> at which predictions are
to be made. NOTE: <code>newx</code> must be a matrix with each row as an observation. <code>predict</code> function does not accept a vector or other formats of <code>newx</code>.</p>
</td></tr>
<tr><td><code id="predict.KERE_+3A_...">...</code></td>
<td>
<p>other parameters to <code>predict</code> function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The fitted <code class="reqn">\alpha_0 + K * \alpha</code> at newx is returned as a size <code>nrow(newx)*length(lambda)</code> matrix for various lambda values where the <code>KERE</code> model was fitted.
</p>


<h3>Value</h3>

<p>The fitted <code class="reqn">\alpha_0 + K * \alpha</code> is returned as a size <code>nrow(newx)*length(lambda)</code> matrix. The row represents the index for observations of newx. The column represents the index for the lambda sequence.
</p>


<h3>Author(s)</h3>

<p>Yi Yang, Teng Zhang and Hui Zou<br />
Maintainer: Yi Yang  &lt;yiyang@umn.edu&gt;
</p>


<h3>References</h3>

<p>Y. Yang, T. Zhang, and H. Zou. &quot;Flexible Expectile Regression in Reproducing Kernel Hilbert Space.&quot; ArXiv e-prints: stat.ME/1508.05987, August 2015.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># create data
N &lt;- 100
X1 &lt;- runif(N)
X2 &lt;- 2*runif(N)
X3 &lt;- 3*runif(N)
SNR &lt;- 10 # signal-to-noise ratio
Y &lt;- X1**1.5 + 2 * (X2**.5) + X1*X3
sigma &lt;- sqrt(var(Y)/SNR)
Y &lt;- Y + X2*rnorm(N,0,sigma)
X &lt;- cbind(X1,X2,X3)

# set gaussian kernel 
kern &lt;- rbfdot(sigma=0.1)

# define lambda sequence
lambda &lt;- exp(seq(log(0.5),log(0.01),len=10))

# run KERE
m1 &lt;- KERE(x=X, y=Y, kern=kern, lambda = lambda, omega = 0.5) 

# create newx for prediction
N1 &lt;- 5
X1 &lt;- runif(N1)
X2 &lt;- 2*runif(N1)
X3 &lt;- 3*runif(N1)
newx &lt;- cbind(X1,X2,X3)

# make prediction
p1 &lt;- predict.KERE(m1, kern, X, newx)
p1
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
