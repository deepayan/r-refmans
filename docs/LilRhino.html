<!DOCTYPE html><html><head><title>Help for package LilRhino</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {LilRhino}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#Binary_Network'><p>Binary Decision Neural Network Wrapper</p>
</a></li>
<li><a href='#Bootstrap_Data_Frame'><p>A function for bootstraping textual data so that all levels have the same number of entries.</p>
</a></li>
<li><a href='#Bootstrap_Vocab'><p>An internal function for Bootstrap_Data_Frame.</p>
</a></li>
<li><a href='#Codes_done'><p> For announcing when code is done.</p>
</a></li>
<li><a href='#Cross_val_maker'><p> For Creating a test and train set from a whole set</p></a></li>
<li><a href='#Feed_Reduction'><p>A Function for converting data into approximations of probability space.</p>
</a></li>
<li><a href='#Load_Glove_Embeddings'><p>Function for loading in pre-trained or personal word embedding softwares.</p></a></li>
<li><a href='#Monty_Hall'><p> Monty Hall Simulator</p>
</a></li>
<li><a href='#Nearest_Centroid'><p> For performing the nearest centroid problem (with modifications) on MNST data specifically (general to come)</p>
</a></li>
<li><a href='#Num_Al_Sep'>
<p>Number/alpha numeric seperator for strings.</p></a></li>
<li><a href='#Percent'><p> Percent of confusion matrix</p>
</a></li>
<li><a href='#Pretreatment'>
<p>Pretreatment of textual documents for NLP.</p></a></li>
<li><a href='#Random_Brains'><p>Random Brains: Neural Network Implementation of Random Forest</p>
</a></li>
<li><a href='#Sentence_Vector'><p>Function for extracting the sentence vector from an embeddings matrix.</p>
</a></li>
<li><a href='#Stopword_Maker'>
<p>For the finding of the $N$ most populous words in a corpus.</p></a></li>
<li><a href='#Table_percent'><p> Table Percent</p>
</a></li>
<li><a href='#Vector_Puller'><p>Function for extacting word vectors from embeddings.</p>
</a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>For Implementation of Feed Reduction, Learning Examples, NLP and
Code Management</td>
</tr>
<tr>
<td>Version:</td>
<td>1.2.2</td>
</tr>
<tr>
<td>Author:</td>
<td>Travis Barton (2018) </td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Travis Barton &lt;travisdatabarton@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>This is for code management functions, NLP tools, a Monty Hall simulator, and for implementing my own variable reduction technique called Feed Reduction. The Feed Reduction technique is not yet published, but is merely a tool for implementing a series of binary neural networks meant for reducing data into N dimensions, where N is the number of possible values of the response variable.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Suggests:</td>
<td>textclean</td>
</tr>
<tr>
<td>Imports:</td>
<td>FNN, stringi, beepr, ggplot2, keras, dplyr, readr, parallel,
tm, e1071, SnowballC, data.table, fastmatch, neuralnet</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-04-27 20:27:08 UTC; travisbarton</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-04-27 22:10:14 UTC</td>
</tr>
</table>
<hr>
<h2 id='Binary_Network'>Binary Decision Neural Network Wrapper

</h2><span id='topic+Binary_Network'></span>

<h3>Description</h3>

<p>Used as a function of Feed_Reduction, Binary_Networt uses a 3 layer neural network with an adam optimizer, leaky RELU for the first two activation functions, followed by a softmax on the last layer. The loss function is binary_crossentropy. This is a keras wrapper, and uses tensorflow in the backend.

</p>


<h3>Usage</h3>

<pre><code class='language-R'>Binary_Network(X, Y, X_test, val_split, nodes, epochs, batch_size, verbose = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Binary_Network_+3A_x">X</code></td>
<td>
<p>Training data.

</p>
</td></tr>
<tr><td><code id="Binary_Network_+3A_y">Y</code></td>
<td>
<p>Training Labels. These must be binary.

</p>
</td></tr>
<tr><td><code id="Binary_Network_+3A_x_test">X_test</code></td>
<td>
<p>The test Data

</p>
</td></tr>
<tr><td><code id="Binary_Network_+3A_val_split">val_split</code></td>
<td>
<p>The validation split for keras.

</p>
</td></tr>
<tr><td><code id="Binary_Network_+3A_nodes">nodes</code></td>
<td>
<p>The number of nodes in the hidden layers.

</p>
</td></tr>
<tr><td><code id="Binary_Network_+3A_epochs">epochs</code></td>
<td>
<p>The number of epochs for the network

</p>
</td></tr>
<tr><td><code id="Binary_Network_+3A_batch_size">batch_size</code></td>
<td>
<p> The batch size for the network

</p>
</td></tr>
<tr><td><code id="Binary_Network_+3A_verbose">verbose</code></td>
<td>
<p> Weither or not you want details about the run as its happening. 0 = silent, 1 = progress bar, 2 = one line per epoch.

</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is a subset for the larger function Feed_Network. The output is the list containing the training and testing data converted into an approximation of probability space for that binary decision.

</p>


<h3>Value</h3>

<table>
<tr><td><code>Train</code></td>
<td>
<p>The training data in approximate probability space</p>
</td></tr>
<tr><td><code>Test</code></td>
<td>
<p>The testing data in 'double' approximate probability space</p>
</td></tr>





</table>


<h3>Author(s)</h3>

<p> Travis Barton

</p>


<h3>References</h3>

<p>Check out http://wbbpredictions.com/wp-content/uploads/2018/12/Redditbot_Paper.pdf and Keras for details

</p>


<h3>See Also</h3>

<p> Feed_Network

</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Not run: 
if(8 * .Machine$sizeof.pointer == 64){
  #Feed Network Testing
  library(keras)
  library(dplyr)
    install_keras()
    dat &lt;- keras::dataset_mnist()
    X_train = array_reshape(dat$train$x/255, c(nrow(dat$train$x/255), 784))
    y_train = to_categorical(dat$train$y, 10)
    X_test = array_reshape(dat$test$x/255, c(nrow(dat$test$x/255), 784))
    y_test =to_categorical(dat$test$y, 10)


    index_train = which(dat$train$y == 6 | dat$train$y == 5)
    index_train = sample(index_train, length(index_train))
    index_test = which(dat$test$y == 6 | dat$test$y == 5)
    index_test = sample(index_test, length(index_test))

    temp = Binary_Network(X_train[index_train,],
    y_train[index_train,c(7, 6)], X_test[index_test,], .3, 350, 30, 50)
  }
  
## End(Not run)

</code></pre>

<hr>
<h2 id='Bootstrap_Data_Frame'>A function for bootstraping textual data so that all levels have the same number of entries.

</h2><span id='topic+Bootstrap_Data_Frame'></span>

<h3>Description</h3>

<p> This function takes a corpus and a set of labels and uses Bootstrap_Vocab to increase the size of each label until they are all the same length. Stop words are not bootstrapped.

</p>


<h3>Usage</h3>

<pre><code class='language-R'>Bootstrap_Data_Frame(text, tags, stopwords, min_length = 7, max_length = 15)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Bootstrap_Data_Frame_+3A_text">text</code></td>
<td>
<p>text is the collection of textual data to bootstrap up.

</p>
</td></tr>
<tr><td><code id="Bootstrap_Data_Frame_+3A_tags">tags</code></td>
<td>
<p>tags are the collection of tags that will be used to bootstrap. There should be one for every entry in 'text'. They do not have to be unique.
</p>
</td></tr>
<tr><td><code id="Bootstrap_Data_Frame_+3A_stopwords">stopwords</code></td>
<td>
<p>stopwords to make sure are not apart of the bootstrapping process. It is advised to eliminate the most common words. See Stop_Word_Maker()</p>
</td></tr>
<tr><td><code id="Bootstrap_Data_Frame_+3A_min_length">min_length</code></td>
<td>
<p>The shortest length allowable for bootstrapped words</p>
</td></tr>
<tr><td><code id="Bootstrap_Data_Frame_+3A_max_length">max_length</code></td>
<td>
<p>The longest length allowable for bootstrapped words</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Most of the bootstrapped words will be nonseneical. The intention of this package is not to create new sentences, but to instead trick your model into thinking it has equal lengthed levels. This method is meant for bag of words style models.

</p>


<h3>Value</h3>

<p>A data frame of your original documents along with the bootstrapped ones (column 1) along with their tags (column 2).





</p>


<h3>Author(s)</h3>

<p>Travis Barton

</p>


<h3>Examples</h3>

<pre><code class='language-R'>test_set = c('I like cats', 'I like dogs', 'we love animals', 'I am a vet',
             'US politics bore me', 'I dont like to vote',
             'The rainbow looked nice today dont you think tommy')
test_tags = c('animals', 'animals', 'animals', 'animals',
             'politics', 'politics',
             'misc')

Bootstrap_Data_Frame(test_set, test_tags, c("I", "we"), min_length = 3, max_length = 8)
</code></pre>

<hr>
<h2 id='Bootstrap_Vocab'>An internal function for Bootstrap_Data_Frame.

</h2><span id='topic+Bootstrap_Vocab'></span>

<h3>Description</h3>

<p>This function takes a selection of documents and bootstraps words from said sentences until there are N total sentences (both sudo and original).

</p>


<h3>Usage</h3>

<pre><code class='language-R'>Bootstrap_Vocab(vocab, N, stopwds, min_length = 7, max_length = 15)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Bootstrap_Vocab_+3A_vocab">vocab</code></td>
<td>
<p>The collection of documents to boostrap.

</p>
</td></tr>
<tr><td><code id="Bootstrap_Vocab_+3A_n">N</code></td>
<td>
<p>The total amount of sentences to end up with</p>
</td></tr>
<tr><td><code id="Bootstrap_Vocab_+3A_stopwds">stopwds</code></td>
<td>
<p>A list of stopwords to not include in the bootstrapping proccess</p>
</td></tr>
<tr><td><code id="Bootstrap_Vocab_+3A_min_length">min_length</code></td>
<td>
<p>The shortest allowable bootstrapped doument</p>
</td></tr>
<tr><td><code id="Bootstrap_Vocab_+3A_max_length">max_length</code></td>
<td>
<p>The longest allowable bootstrapped document</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The min and max length arguements to not gaurantee that a sentence will reach that length. These senteces will be nonsensical.

</p>


<h3>Value</h3>

<p>A vector of bootstrapped sentences.





</p>


<h3>Author(s)</h3>

<p>Travis Barton

</p>


<h3>Examples</h3>

<pre><code class='language-R'>

testing_set = c(paste('this is test',  as.character(seq(1, 10, 1))))

Bootstrap_Vocab(testing_set, 20, c('this'))

</code></pre>

<hr>
<h2 id='Codes_done'> For announcing when code is done.

</h2><span id='topic+Codes_done'></span>

<h3>Description</h3>

<p> for alerting you when your code is done.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Codes_done(title, msg, sound = FALSE, effect = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Codes_done_+3A_title">title</code></td>
<td>
<p> The title of the notification</p>
</td></tr>
<tr><td><code id="Codes_done_+3A_msg">msg</code></td>
<td>
<p> The message to be sent</p>
</td></tr>
<tr><td><code id="Codes_done_+3A_sound">sound</code></td>
<td>
<p> Optional sound to blurt as well</p>
</td></tr>
<tr><td><code id="Codes_done_+3A_effect">effect</code></td>
<td>
<p> If sound it blurted, what should it be? (check beepr package for sound options)</p>
</td></tr>

</table>


<h3>Details</h3>

<p> Only for Linix (as far as I know)

</p>


<h3>Author(s)</h3>

<p> smacdonald (stack overflow) with modificaion by Travis Barton

</p>


<h3>References</h3>

<p> https://stackoverflow.com/questions/3365657/is-there-a-way-to-make-r-beep-play-a-sound-at-the-end-of-a-script

</p>


<h3>Examples</h3>

<pre><code class='language-R'>Codes_done("done", "check it", sound = TRUE, effect = 1)
</code></pre>

<hr>
<h2 id='Cross_val_maker'> For Creating a test and train set from a whole set
</h2><span id='topic+Cross_val_maker'></span>

<h3>Description</h3>

<p> for making one dataset into two (test and train)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Cross_val_maker(data, alpha)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Cross_val_maker_+3A_data">data</code></td>
<td>
<p> matrix of data you want to split</p>
</td></tr>
<tr><td><code id="Cross_val_maker_+3A_alpha">alpha</code></td>
<td>
<p> the percent of data to split
</p>
</td></tr>
</table>


<h3>Value</h3>

<p> returns a list with accessable with the '$' sign. Test and Train are labeled as such.
</p>


<h3>Author(s)</h3>

<p> Travis Barton
</p>


<h3>Examples</h3>

<pre><code class='language-R'>dat &lt;- Cross_val_maker(iris, .1)
train &lt;- dat$Train
test &lt;- dat$Test
</code></pre>

<hr>
<h2 id='Feed_Reduction'>A Function for converting data into approximations of probability space.

</h2><span id='topic+Feed_Reduction'></span>

<h3>Description</h3>

<p>It takes the number of unique labels in the training data and tries to predict a one vs all binary neural network for each unique label. The output is an approximation of the probability that each individual input does not not match the label.
Travis Barton (2018) http://wbbpredictions.com/wp-content/uploads/2018/12/Redditbot_Paper.pdf

</p>


<h3>Usage</h3>

<pre><code class='language-R'>Feed_Reduction(X, Y, X_test, val_split = .1,
               nodes = NULL, epochs = 15,
               batch_size = 30, verbose = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Feed_Reduction_+3A_x">X</code></td>
<td>
<p> Training data

</p>
</td></tr>
<tr><td><code id="Feed_Reduction_+3A_y">Y</code></td>
<td>
<p>Training labels</p>
</td></tr>
<tr><td><code id="Feed_Reduction_+3A_x_test">X_test</code></td>
<td>
<p>Testing data</p>
</td></tr>
<tr><td><code id="Feed_Reduction_+3A_val_split">val_split</code></td>
<td>
<p>The validation split for the keras, binary, neural networks</p>
</td></tr>
<tr><td><code id="Feed_Reduction_+3A_nodes">nodes</code></td>
<td>
<p>The number nodes for the hidden layers, default is 1/4 of the length of the training data.</p>
</td></tr>
<tr><td><code id="Feed_Reduction_+3A_epochs">epochs</code></td>
<td>
<p>The number of epochs for the fitting of the networks</p>
</td></tr>
<tr><td><code id="Feed_Reduction_+3A_batch_size">batch_size</code></td>
<td>
<p>The batch size for the networks</p>
</td></tr>
<tr><td><code id="Feed_Reduction_+3A_verbose">verbose</code></td>
<td>
<p>Weither or not you want details about the run as its happening. 0 = silent, 1 = progress bar, 2 = one line per epoch.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a new technique for dimensionality reduction of my own creation. Data is converted to the same number of dimensions as there are unique labels. Each dimension is an approximation of the probability that the data point is inside the a unique label. The return value is a list the training and test data with their dimensionality reduced.
</p>


<h3>Value</h3>

<table>
<tr><td><code>Train</code></td>
<td>
<p>The training data in the new probability space</p>
</td></tr>
<tr><td><code>Test</code></td>
<td>
<p>The testing data in the new probability space</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Travis Barton.

</p>


<h3>References</h3>

<p>Check out http://wbbpredictions.com/wp-content/uploads/2018/12/Redditbot_Paper.pdf for details on the proccess

</p>


<h3>See Also</h3>

<p>Binary_Network

</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
if(8 * .Machine$sizeof.pointer == 64){
#Feed Network Testing
library(keras)

  install_keras()
  dat &lt;- keras::dataset_mnist()
  X_train = array_reshape(dat$train$x/255, c(nrow(dat$train$x/255), 784))
  y_train = dat$train$y
  X_test = array_reshape(dat$test$x/255, c(nrow(dat$test$x/255), 784))
  y_test = dat$test$y

  Reduced_Data2 = Feed_Reduction(X_train, y_train, X_test,
                                val_split = .3, nodes = 350,
                                30, 50, verbose = 1)

  library(e1071)
  names(Reduced_Data2$test) = names(Reduced_Data2$train)
  newdat = as.data.frame(cbind(rbind(Reduced_Data2$train, Reduced_Data2$test), c(y_train, y_test)))
  colnames(newdat) = c(paste("V", c(1:11), sep = ""))
  mod = svm(V11~., data = newdat, subset = c(1:60000),
           kernel = 'linear', cost = 1, type = 'C-classification')
  preds = predict(mod, newdat[60001:70000,-11])
  sum(preds == y_test)/10000

}

## End(Not run)
</code></pre>

<hr>
<h2 id='Load_Glove_Embeddings'>Function for loading in pre-trained or personal word embedding softwares.
</h2><span id='topic+Load_Glove_Embeddings'></span>

<h3>Description</h3>

<p>Loads in GloVes' pretrained 42 billion token embeddings, trained on the common crawl.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Load_Glove_Embeddings(path = 'glove.42B.300d.txt', d = 300)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Load_Glove_Embeddings_+3A_path">path</code></td>
<td>

<p>The path to the embeddings file.
</p>
</td></tr>
<tr><td><code id="Load_Glove_Embeddings_+3A_d">d</code></td>
<td>

<p>The dimension of the embeddings file.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The embeddings file should be the word, followed by numeric values, ending with a carriage return.
</p>


<h3>Value</h3>

<p>The embeddings matrix.
</p>


<h3>Author(s)</h3>

<p>Travis Barton
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#This code only works if you have the 5g file found here: &lt;https://nlp.stanford.edu/projects/glove/&gt;

## Not run: emb = Load_Glove_Embeddings()
</code></pre>

<hr>
<h2 id='Monty_Hall'> Monty Hall Simulator

</h2><span id='topic+Monty_Hall'></span>

<h3>Description</h3>

<p> A simulator for the famous Monty Hall Problem

</p>


<h3>Usage</h3>

<pre><code class='language-R'>Monty_Hall(Games = 10, Choice = "Stay")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Monty_Hall_+3A_games">Games</code></td>
<td>
<p> The number of games to run on the simulation

</p>
</td></tr>
<tr><td><code id="Monty_Hall_+3A_choice">Choice</code></td>
<td>
<p>Wether you would like the simulation to either 'Stay' with the first chosen door, 'Switch' to the other door, or 'Random' where you randomly decide to either stay or switch.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p> This is just a toy example of the famous Monty Hall problem. It returns a ggplot bar chart showing the counts for wins or loses in the simulation.

</p>


<h3>Value</h3>

<p> A ggplot graph is produced. There is no return value.





</p>


<h3>Author(s)</h3>

<p> Travis Barton

</p>


<h3>Examples</h3>

<pre><code class='language-R'>Monty_Hall(100, 'Stay')
</code></pre>

<hr>
<h2 id='Nearest_Centroid'> For performing the nearest centroid problem (with modifications) on MNST data specifically (general to come)

</h2><span id='topic+Nearest_Centroid'></span>

<h3>Description</h3>

<p> For Chen's homework, I'll change this when I generalize it.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Nearest_Centroid(X_train, X_test, Y_train)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Nearest_Centroid_+3A_x_train">X_train</code></td>
<td>
<p> Training data</p>
</td></tr>
<tr><td><code id="Nearest_Centroid_+3A_x_test">X_test</code></td>
<td>
<p>data to be tested</p>
</td></tr>
<tr><td><code id="Nearest_Centroid_+3A_y_train">Y_train</code></td>
<td>
<p>training labels

</p>
</td></tr>
</table>


<h3>Note</h3>

<p> Based on homework from Guangling Chen's M251 class at SJSU
</p>


<h3>Author(s)</h3>

<p> Travis Barton
</p>

<hr>
<h2 id='Num_Al_Sep'>
Number/alpha numeric seperator for strings.
</h2><span id='topic+Num_Al_Sep'></span>

<h3>Description</h3>

<p>A Function for the separating of numbers from letters. 'b4' for example would be converted to 'b 4'.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Num_Al_Sep(vec)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Num_Al_Sep_+3A_vec">vec</code></td>
<td>

<p>The string vector in which you wish to separate the numbers from the letters.
</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>output</code></td>
<td>
<p>The separated vector.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>This is a really simple function really used inside other functions.
</p>


<h3>Author(s)</h3>

<p>Travis Barton
</p>


<h3>Examples</h3>

<pre><code class='language-R'>test_vec = 'The most iconic American weapon has to be the AR15'
res = Num_Al_Sep(test_vec)
print(res)
</code></pre>

<hr>
<h2 id='Percent'> Percent of confusion matrix

</h2><span id='topic+Percent'></span>

<h3>Description</h3>

<p> For finding the accuracy of confusion matricies with true/pred values

</p>


<h3>Usage</h3>

<pre><code class='language-R'>Percent(true, test)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Percent_+3A_true">true</code></td>
<td>
<p> The true values

</p>
</td></tr>
<tr><td><code id="Percent_+3A_test">test</code></td>
<td>
<p>the test values</p>
</td></tr>
</table>


<h3>Details</h3>

<p> Make sure your strings have the right values and create a square matrix.

</p>


<h3>Value</h3>

<p> the percent acc.





</p>


<h3>Author(s)</h3>

<p> Travis Barton

</p>


<h3>Examples</h3>

<pre><code class='language-R'>true &lt;- rep(1:10, 10)
test &lt;- rep(1:10, 10)
test[c(2, 22, 33, 89)] = 1
Percent(true, test)
#or
#percent(table(true, test))
</code></pre>

<hr>
<h2 id='Pretreatment'>
Pretreatment of textual documents for NLP.
</h2><span id='topic+Pretreatment'></span>

<h3>Description</h3>

<p>This function goes through a number of pretreatment steps in preparation for vectorization. These steps are designed to help the data become more standard so that there are fewer outliers when training during NLP. The following effects are applied:
1. Non-alpha/numerics are removed.
2. Numbers are separated from letters.
3. Numbers are replaced with their word equivalents.
4. Words are stemmed (optional).
5. Words are lowercased (optinal).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Pretreatment(title_vec, stem = TRUE, lower = TRUE, parallel = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Pretreatment_+3A_title_vec">title_vec</code></td>
<td>

<p>Vector of documents to be pre-treated.
</p>
</td></tr>
<tr><td><code id="Pretreatment_+3A_stem">stem</code></td>
<td>

<p>Boolian variable to decide whether to stem or not.
</p>
</td></tr>
<tr><td><code id="Pretreatment_+3A_lower">lower</code></td>
<td>

<p>Boolian variable to decide whether to lowercase words or not.
</p>
</td></tr>
<tr><td><code id="Pretreatment_+3A_parallel">parallel</code></td>
<td>

<p>Boolian variable to decide whether to run this function in parallel or not.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function returns a list. It should be able to accept any format that the function lapply would accept. The parallelization is done with the function Mcapply from the package 'parallel' and will only work on systems that allow forking (Sorry windows users). Future updates will allow for socketing.
</p>


<h3>Value</h3>

<table>
<tr><td><code>output</code></td>
<td>
<p>The list of character strings post-pretreatment</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Travis Barton
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run:  # for some reason it takes longer than 5 seconds on CRAN's computers
test_vec = c('This is a test', 'Ahoy!', 'my battle-ship is on... b6!')
res = Pretreatment(test_vec)
print(res)

## End(Not run)
</code></pre>

<hr>
<h2 id='Random_Brains'>Random Brains: Neural Network Implementation of Random Forest

</h2><span id='topic+Random_Brains'></span>

<h3>Description</h3>

<p>Creates a random forest style collection of neural networks for classification

</p>


<h3>Usage</h3>

<pre><code class='language-R'>Random_Brains(data, y, x_test,
variables = ceiling(ncol(data)/10),
brains = floor(sqrt(ncol(data))),
hiddens = c(3, 4))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Random_Brains_+3A_data">data</code></td>
<td>
<p>The data that holds the predictors ONLY.</p>
</td></tr>
<tr><td><code id="Random_Brains_+3A_y">y</code></td>
<td>
<p>The responce variable</p>
</td></tr>
<tr><td><code id="Random_Brains_+3A_x_test">x_test</code></td>
<td>
<p>The testing predictors</p>
</td></tr>
<tr><td><code id="Random_Brains_+3A_variables">variables</code></td>
<td>
<p>The number of predictors to select for each brain in 'data'. The default is one tenth of the number of columns in 'data'.</p>
</td></tr>
<tr><td><code id="Random_Brains_+3A_brains">brains</code></td>
<td>
<p>The number of neural networks to create. The default is the square root of the number of columns in 'data'.</p>
</td></tr>
<tr><td><code id="Random_Brains_+3A_hiddens">hiddens</code></td>
<td>
<p>The is a vector with length equal to the desired number of hidden layers. Each entry in the vector corresponds to the number of nodes in that layer. The default is c(3, 4) which is a two layer network with 3 and 4 nodes in the layers respectively.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is meant to mirror the classic random forest function exctly. The only difference being that it uses shallow neural networks to build the forest instead of decision trees.

</p>


<h3>Value</h3>

<table>
<tr><td><code>predictions</code></td>
<td>
<p>The predictions for x_test.</p>
</td></tr>
<tr><td><code>num_brains</code></td>
<td>
<p>The number of neural networks used to decide the predictions.</p>
</td></tr>
<tr><td><code>predictors_per_brain</code></td>
<td>
<p>The number of variabled used for the neural networks used to decide the predictions.</p>
</td></tr>
<tr><td><code>hidden_layers</code></td>
<td>
<p>The vector describing the number of layers, as well as how many there were.</p>
</td></tr>
<tr><td><code>preds_per_brain</code></td>
<td>
<p>This matrix describes which columns where selected by each brain. Each row is a new brain. each column describes the index of the column used.</p>
</td></tr>
<tr><td><code>raw_results</code></td>
<td>
<p>The matrix of raw predictions from the brains. Each row is the cummulative predictions of all the brains. Which prediciton won by majority vote can be seen in 'predictions</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The neural networks are created using the neuralnet package!

</p>


<h3>Author(s)</h3>

<p>Travis Barton

</p>


<h3>Examples</h3>

<pre><code class='language-R'>
dat = Cross_val_maker(iris, .2)

train = dat$Train
test = dat$Test

Final_Test = Random_Brains(train[,-5],
  train$Species, as.matrix(test[,-5]),
  variables = 3, brains = 2)
table(Final_Test$predictions, as.numeric(test$Species))


</code></pre>

<hr>
<h2 id='Sentence_Vector'>Function for extracting the sentence vector from an embeddings matrix.

</h2><span id='topic+Sentence_Vector'></span>

<h3>Description</h3>

<p>Function for extracting the sentence vector from an embeddings matrix in a fast and convenient manner.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Sentence_Vector(Sentence, emb_matrix, dimension, stopwords)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Sentence_Vector_+3A_sentence">Sentence</code></td>
<td>
<p>The sentence to find the vector of.

</p>
</td></tr>
<tr><td><code id="Sentence_Vector_+3A_emb_matrix">emb_matrix</code></td>
<td>
<p>The embeddings matrix to search.

</p>
</td></tr>
<tr><td><code id="Sentence_Vector_+3A_dimension">dimension</code></td>
<td>
<p>The dimension of the vector to return.

</p>
</td></tr>
<tr><td><code id="Sentence_Vector_+3A_stopwords">stopwords</code></td>
<td>
<p>Words that should not be included in the averaging proccess.

</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function splits the sentence into words, eliminates all stopwords, finds the vectors of each word, then averages the word vectors into a sentence vector.

</p>


<h3>Value</h3>

<p>The sentence vector from an embeddings matrix.





</p>


<h3>Author(s)</h3>

<p>Travis Barton

</p>


<h3>Examples</h3>

<pre><code class='language-R'>  emb = data.frame(matrix(c(1, 2, 3, 4, 5, 5,
  4, 3, 2, 1, 1, 5, 3, 2, 4), nrow = 3),
  row.names = c('sentence', 'in', 'question'))

  Sentence_Vector(c('this is the sentence in question'), emb, 5, c('this', 'is', 'the'))


</code></pre>

<hr>
<h2 id='Stopword_Maker'>
For the finding of the $N$ most populous words in a corpus.
</h2><span id='topic+Stopword_Maker'></span>

<h3>Description</h3>

<p>This function finds the $N$ most used words in a corpus. This is done to identify stop words to better prune data sets before training.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Stopword_Maker(titles, cutoff = 20)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Stopword_Maker_+3A_titles">titles</code></td>
<td>

<p>The documents in which the most populous words are sought.
</p>
</td></tr>
<tr><td><code id="Stopword_Maker_+3A_cutoff">cutoff</code></td>
<td>

<p>The number of $N$ top most used words to keep as stop words.
</p>
</td></tr>
</table>


<h3>Value</h3>

<table>
<tr><td><code>output</code></td>
<td>
<p>A vector of the $N$ most populous words.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Travis Barton
</p>


<h3>Examples</h3>

<pre><code class='language-R'>test_set = c('this is a testset', 'I am searching for a list of words',
'I like turtles',
'A rocket would be a fast way of getting to work, but I do not think it is very practical')
res = Stopword_Maker(test_set, 4)
print(res)
</code></pre>

<hr>
<h2 id='Table_percent'> Table Percent

</h2><span id='topic+Table_percent'></span>

<h3>Description</h3>

<p> Finds the acc of square tables.

</p>


<h3>Usage</h3>

<pre><code class='language-R'>Table_percent(in_table)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Table_percent_+3A_in_table">in_table</code></td>
<td>
<p> a confusion matrix

</p>
</td></tr>
</table>


<h3>Details</h3>

<p> The table must be square

</p>


<h3>Note</h3>

<p> make sure its square.

</p>


<h3>Author(s)</h3>

<p> Travis Barton

</p>


<h3>Examples</h3>

<pre><code class='language-R'>
true &lt;- rep(1:10, 10)
test &lt;- rep(1:10, 10)
test[c(2, 22, 33, 89)] = 1
Table_percent(table(true, test))
</code></pre>

<hr>
<h2 id='Vector_Puller'>Function for extacting word vectors from embeddings.

</h2><span id='topic+Vector_Puller'></span>

<h3>Description</h3>

<p>Function for extacting word vectors from embeddings. This function is an internal function for 'Sentence_Puller'. It averages the word vectors and returns the average of these vectors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Vector_Puller(words, emb_matrix, dimension)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Vector_Puller_+3A_words">words</code></td>
<td>
<p>The word to be extracted.

</p>
</td></tr>
<tr><td><code id="Vector_Puller_+3A_emb_matrix">emb_matrix</code></td>
<td>
<p>The embeddings matrix. It must be a data frame.

</p>
</td></tr>
<tr><td><code id="Vector_Puller_+3A_dimension">dimension</code></td>
<td>
<p>The Dimension of the embeddings to extract. They do not have to match that of the matrix, but they cannot exceed its maximum column count.

</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a simple and fast internal function.

</p>


<h3>Value</h3>

<p>The vector that corresponds to the average of the word vectors.
</p>


<h3>Author(s)</h3>

<p>Travis Barton
</p>


<h3>Examples</h3>

<pre><code class='language-R'># This is an example emb_matrix

emb = data.frame(matrix(c(1, 2, 3, 4, 5, 5, 4, 3, 2, 1), nrow = 2), row.names = c('cow', 'moo'))

Vector_Puller(c('cow', 'moo'), emb, 5)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
