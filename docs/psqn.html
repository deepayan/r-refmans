<!DOCTYPE html><html><head><title>Help for package psqn</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {psqn}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#psqn-package'><p>psqn: Partially Separable Quasi-Newton</p></a></li>
<li><a href='#psqn'><p>Partially Separable Function Optimization</p></a></li>
<li><a href='#psqn_bfgs'><p>BFGS Implementation Used Internally in the psqn Package</p></a></li>
<li><a href='#psqn_generic'><p>Generic Partially Separable Function Optimization</p></a></li>
<li><a href='#psqn_hess'><p>Computes the Hessian.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Partially Separable Quasi-Newton</td>
</tr>
<tr>
<td>Version:</td>
<td>0.3.1</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Benjamin Christoffersen &lt;boennecd@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Provides quasi-Newton methods to minimize partially separable
    functions. The methods are largely described by  
    Nocedal and Wright (2006) &lt;<a href="https://doi.org/10.1007%2F978-0-387-40065-5">doi:10.1007/978-0-387-40065-5</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.apache.org/licenses/LICENSE-2.0">Apache License (&ge; 2)</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.2</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0), Matrix</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/boennecd/psqn">https://github.com/boennecd/psqn</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/boennecd/psqn/issues">https://github.com/boennecd/psqn/issues</a></td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppEigen, testthat</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp</td>
</tr>
<tr>
<td>Suggests:</td>
<td>R.rsp, rmarkdown, RcppArmadillo, RcppEigen, bench, testthat,
numDeriv, lbfgsb3c, lbfgs, alabama</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>R.rsp</td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>C++11</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-03-02 13:10:38 UTC; boennecd</td>
</tr>
<tr>
<td>Author:</td>
<td>Benjamin Christoffersen
    <a href="https://orcid.org/0000-0002-7182-1346"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [cre, aut]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-03-02 14:20:10 UTC</td>
</tr>
</table>
<hr>
<h2 id='psqn-package'>psqn: Partially Separable Quasi-Newton</h2><span id='topic+psqn-package'></span><span id='topic+_PACKAGE'></span>

<h3>Description</h3>

<p>The main methods in the psqn package are the <code><a href="#topic+psqn">psqn</a></code> and
<code><a href="#topic+psqn_generic">psqn_generic</a></code> function.
Notice that it is also possible to use the package from C++. This may
yield a large reduction in the computation time. See the vignette for
details e.g. by calling <code>vignette("psqn", package = "psqn")</code>.
A brief introduction is provided in the &quot;quick-intro&quot; vignette
(see <code>vignette("quick-intro", package = "psqn")</code>).
</p>
<p>This package is fairly new. Thus, results may change and
contributions and feedback is much appreciated.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Benjamin Christoffersen <a href="mailto:boennecd@gmail.com">boennecd@gmail.com</a> (<a href="https://orcid.org/0000-0002-7182-1346">ORCID</a>)
</p>


<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://github.com/boennecd/psqn">https://github.com/boennecd/psqn</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/boennecd/psqn/issues">https://github.com/boennecd/psqn/issues</a>
</p>
</li></ul>


<hr>
<h2 id='psqn'>Partially Separable Function Optimization</h2><span id='topic+psqn'></span><span id='topic+psqn_aug_Lagrang'></span>

<h3>Description</h3>

<p>Optimization method for specially structured partially separable
functions. The <code>psqn_aug_Lagrang</code> function supports non-linear
equality constraints using an augmented Lagrangian method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>psqn(
  par,
  fn,
  n_ele_func,
  rel_eps = 1e-08,
  max_it = 100L,
  n_threads = 1L,
  c1 = 1e-04,
  c2 = 0.9,
  use_bfgs = TRUE,
  trace = 0L,
  cg_tol = 0.5,
  strong_wolfe = TRUE,
  env = NULL,
  max_cg = 0L,
  pre_method = 1L,
  mask = as.integer(c()),
  gr_tol = -1
)

psqn_aug_Lagrang(
  par,
  fn,
  n_ele_func,
  consts,
  n_constraints,
  multipliers = as.numeric(c()),
  penalty_start = 1L,
  rel_eps = 1e-08,
  max_it = 100L,
  max_it_outer = 100L,
  violations_norm_thresh = 1e-06,
  n_threads = 1L,
  c1 = 1e-04,
  c2 = 0.9,
  tau = 1.5,
  use_bfgs = TRUE,
  trace = 0L,
  cg_tol = 0.5,
  strong_wolfe = TRUE,
  env = NULL,
  max_cg = 0L,
  pre_method = 1L,
  mask = as.integer(c()),
  gr_tol = -1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="psqn_+3A_par">par</code></td>
<td>
<p>Initial values for the parameters. It is a concatenated
vector of the global parameters and all the private parameters.</p>
</td></tr>
<tr><td><code id="psqn_+3A_fn">fn</code></td>
<td>
<p>Function to compute the element functions and their
derivatives. Each call computes an element function. See the examples
section.</p>
</td></tr>
<tr><td><code id="psqn_+3A_n_ele_func">n_ele_func</code></td>
<td>
<p>Number of element functions.</p>
</td></tr>
<tr><td><code id="psqn_+3A_rel_eps">rel_eps</code></td>
<td>
<p>Relative convergence threshold.</p>
</td></tr>
<tr><td><code id="psqn_+3A_max_it">max_it</code></td>
<td>
<p>Maximum number of iterations.</p>
</td></tr>
<tr><td><code id="psqn_+3A_n_threads">n_threads</code></td>
<td>
<p>Number of threads to use.</p>
</td></tr>
<tr><td><code id="psqn_+3A_c1">c1</code>, <code id="psqn_+3A_c2">c2</code></td>
<td>
<p>Thresholds for the Wolfe condition.</p>
</td></tr>
<tr><td><code id="psqn_+3A_use_bfgs">use_bfgs</code></td>
<td>
<p>Logical for whether to use BFGS updates or SR1 updates.</p>
</td></tr>
<tr><td><code id="psqn_+3A_trace">trace</code></td>
<td>
<p>Integer where larger values gives more information during the
optimization.</p>
</td></tr>
<tr><td><code id="psqn_+3A_cg_tol">cg_tol</code></td>
<td>
<p>Threshold for the conjugate gradient method.</p>
</td></tr>
<tr><td><code id="psqn_+3A_strong_wolfe">strong_wolfe</code></td>
<td>
<p><code>TRUE</code> if the strong Wolfe condition should be used.</p>
</td></tr>
<tr><td><code id="psqn_+3A_env">env</code></td>
<td>
<p>Environment to evaluate <code>fn</code> in. <code>NULL</code> yields the
global environment.</p>
</td></tr>
<tr><td><code id="psqn_+3A_max_cg">max_cg</code></td>
<td>
<p>Maximum number of conjugate gradient iterations in each
iteration. Use zero if there should not be a limit.</p>
</td></tr>
<tr><td><code id="psqn_+3A_pre_method">pre_method</code></td>
<td>
<p>Preconditioning method in the conjugate gradient method.
Zero yields no preconditioning, one yields diagonal preconditioning,
two yields the incomplete Cholesky factorization from Eigen, and
three yields a block diagonal preconditioning. One and three are fast
options with three seeming to work well for some poorly conditioned
problems.</p>
</td></tr>
<tr><td><code id="psqn_+3A_mask">mask</code></td>
<td>
<p>zero based indices for parameters to mask (i.e. fix).</p>
</td></tr>
<tr><td><code id="psqn_+3A_gr_tol">gr_tol</code></td>
<td>
<p>convergence tolerance for the Euclidean norm of the gradient. A negative
value yields no check.</p>
</td></tr>
<tr><td><code id="psqn_+3A_consts">consts</code></td>
<td>
<p>Function to compute the constraints which must be equal to
zero. See the example Section.</p>
</td></tr>
<tr><td><code id="psqn_+3A_n_constraints">n_constraints</code></td>
<td>
<p>The number of constraints.</p>
</td></tr>
<tr><td><code id="psqn_+3A_multipliers">multipliers</code></td>
<td>
<p>Staring values for the multipliers in the augmented
Lagrangian method. There needs to be the same number of multipliers as the
number of constraints. An empty vector, <code>numeric()</code>, yields zero as
the starting value for all multipliers.</p>
</td></tr>
<tr><td><code id="psqn_+3A_penalty_start">penalty_start</code></td>
<td>
<p>Starting value for the penalty parameterin the
augmented Lagrangian method.</p>
</td></tr>
<tr><td><code id="psqn_+3A_max_it_outer">max_it_outer</code></td>
<td>
<p>Maximum number of augmented Lagrangian steps.</p>
</td></tr>
<tr><td><code id="psqn_+3A_violations_norm_thresh">violations_norm_thresh</code></td>
<td>
<p>Threshold for the norm of the constraint
violations.</p>
</td></tr>
<tr><td><code id="psqn_+3A_tau">tau</code></td>
<td>
<p>Multiplier used for the penalty parameter between each outer
iterations.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function follows the method described by Nocedal and Wright (2006)
and mainly what is described in Section 7.4. Details are provided
in the psqn vignette. See <code>vignette("psqn", package = "psqn")</code>.
</p>
<p>The partially separable function we consider are special in that the
function to be minimized is a sum of so-called element functions which
only depend on few shared (global) parameters and some
private parameters which are particular to each element function. A generic
method for other partially separable functions is available through the
<code><a href="#topic+psqn_generic">psqn_generic</a></code> function.
</p>
<p>The optimization function is also available in C++ as a header-only
library. Using C++ may reduce the computation time substantially. See
the vignette in the package for examples.
</p>
<p>You have to define the <code>PSQN_USE_EIGEN</code> macro variable in C++ if you want
to use the incomplete Cholesky factorization from Eigen. You will also have
to include Eigen or RcppEigen. This is not needed when you use the R
functions documented here. The incomplete Cholesky factorization comes
with some additional overhead because of the allocations of the
factorization,
forming the factorization, and the assignment of the sparse version of
the Hessian approximation.
However, it may substantially reduce the required number of conjugate
gradient iterations.
</p>


<h3>Value</h3>

<p><code>pqne</code>: An object with the following elements:
</p>
<table>
<tr><td><code>par</code></td>
<td>
<p>the estimated global and private parameters.</p>
</td></tr>
<tr><td><code>value</code></td>
<td>
<p>function value at <code>par</code>.</p>
</td></tr>
<tr><td><code>info</code></td>
<td>
<p>information code. 0 implies convergence.
-1 implies that the maximum number iterations is reached.
-2 implies that the conjugate gradient method failed.
-3 implies that the line search failed.
-4 implies that the user interrupted the optimization.</p>
</td></tr>
<tr><td><code>counts</code></td>
<td>
<p>An integer vector with the number of function evaluations,
gradient evaluations, and the number of conjugate gradient iterations.</p>
</td></tr>
<tr><td><code>convergence</code></td>
<td>
<p><code>TRUE</code> if <code>info == 0</code>.</p>
</td></tr>
</table>
<p><code>psqn_aug_Lagrang</code>: Like <code>psqn</code> with a few exceptions:
</p>
<table>
<tr><td><code>multipliers</code></td>
<td>
<p>final multipliers from the augmented Lagrangian
method.</p>
</td></tr>
<tr><td><code>counts</code></td>
<td>
<p>has an additional element called <code>n_aug_Lagrang</code> with the
number of augmented Lagrangian iterations.</p>
</td></tr>
<tr><td><code>penalty</code></td>
<td>
<p>the final penalty parameter from the augmented Lagrangian
method.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Nocedal, J. and Wright, S. J. (2006). <em>Numerical Optimization</em>
(2nd ed.). Springer.
</p>
<p>Lin, C. and Moré, J. J. (1999). <em>Incomplete Cholesky factorizations
with limited memory</em>. SIAM Journal on Scientific Computing.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># example with inner problem in a Taylor approximation for a GLMM as in the
# vignette

# assign model parameters, number of random effects, and fixed effects
q &lt;- 2 # number of private parameters per cluster
p &lt;- 1 # number of global parameters
beta &lt;- sqrt((1:p) / sum(1:p))
Sigma &lt;- diag(q)

# simulate a data set
set.seed(66608927)
n_clusters &lt;- 20L # number of clusters
sim_dat &lt;- replicate(n_clusters, {
  n_members &lt;- sample.int(8L, 1L) + 2L
  X &lt;- matrix(runif(p * n_members, -sqrt(6 / 2), sqrt(6 / 2)),
              p)
  u &lt;- drop(rnorm(q) %*% chol(Sigma))
  Z &lt;- matrix(runif(q * n_members, -sqrt(6 / 2 / q), sqrt(6 / 2 / q)),
              q)
  eta &lt;- drop(beta %*% X + u %*% Z)
  y &lt;- as.numeric((1 + exp(-eta))^(-1) &gt; runif(n_members))

  list(X = X, Z = Z, y = y, u = u, Sigma_inv = solve(Sigma))
}, simplify = FALSE)

# evaluates the negative log integrand.
#
# Args:
#   i cluster/element function index.
#   par the global and private parameter for this cluster. It has length
#       zero if the number of parameters is requested. That is, a 2D integer
#       vector the number of global parameters as the first element and the
#       number of private parameters as the second element.
#   comp_grad logical for whether to compute the gradient.
r_func &lt;- function(i, par, comp_grad){
  dat &lt;- sim_dat[[i]]
  X &lt;- dat$X
  Z &lt;- dat$Z

  if(length(par) &lt; 1)
    # requested the dimension of the parameter
    return(c(global_dim = NROW(dat$X), private_dim = NROW(dat$Z)))

  y &lt;- dat$y
  Sigma_inv &lt;- dat$Sigma_inv

  beta &lt;- par[1:p]
  uhat &lt;- par[1:q + p]
  eta &lt;- drop(beta %*% X + uhat %*% Z)
  exp_eta &lt;- exp(eta)

  out &lt;- -sum(y * eta) + sum(log(1 + exp_eta)) +
    sum(uhat * (Sigma_inv %*% uhat)) / 2
  if(comp_grad){
    d_eta &lt;- -y + exp_eta / (1 + exp_eta)
    grad &lt;- c(X %*% d_eta,
              Z %*% d_eta + dat$Sigma_inv %*% uhat)
    attr(out, "grad") &lt;- grad
  }

  out
}

# optimize the log integrand
res &lt;- psqn(par = rep(0, p + q * n_clusters), fn = r_func,
            n_ele_func = n_clusters)
head(res$par, p)              # the estimated global parameters
tail(res$par, n_clusters * q) # the estimated private parameters

# compare with
beta
c(sapply(sim_dat, "[[", "u"))

# add equality constraints
idx_constrained &lt;- list(c(2L, 19L), c(1L, 5L, 8L))

# evaluates the c(x) in equalities c(x) = 0.
#
# Args:
#   i constrain index.
#   par the constrained parameters. It has length zero if we need to pass the
#       one-based indices of the parameters that the i'th constrain depends on.
#   what integer which is zero if the function should be returned and one if the
#        gradient should be computed.
consts &lt;- function(i, par, what){
  if(length(par) == 0)
    # need to return the indices
    return(idx_constrained[[i]])

  if(i == 1){
    # a linear equality constrain. It is implemented as a non-linear constrain
    # though
    out &lt;- sum(par) - 3
    if(what == 1)
      attr(out, "grad") &lt;- rep(1, length(par))

  } else if(i == 2){
    # the parameters need to be on a circle
    out &lt;- sum(par^2) - 1
    if(what == 1)
      attr(out, "grad") &lt;- 2 * par
  }

  out
}

# optimize with the constraints
res_consts &lt;- psqn_aug_Lagrang(
  par = rep(0, p + q * n_clusters), fn = r_func, consts = consts,
  n_ele_func = n_clusters, n_constraints = length(idx_constrained))

res_consts
res_consts$multipliers # the estimated multipliers
res_consts$penalty # the penalty parameter

# the function value is higher (worse) as expected
res$value - res_consts$value

# the two constraints are satisfied
sum(res_consts$par[idx_constrained[[1]]]) - 3   # ~ 0
sum(res_consts$par[idx_constrained[[2]]]^2) - 1 # ~ 0

# we can also use another pre conditioner
res_consts_chol &lt;- psqn_aug_Lagrang(
  par = rep(0, p + q * n_clusters), fn = r_func, consts = consts,
  n_ele_func = n_clusters, n_constraints = length(idx_constrained),
  pre_method = 2L)

res_consts_chol

</code></pre>

<hr>
<h2 id='psqn_bfgs'>BFGS Implementation Used Internally in the psqn Package</h2><span id='topic+psqn_bfgs'></span>

<h3>Description</h3>

<p>The method seems to mainly differ from <code><a href="stats.html#topic+optim">optim</a></code> by the line search
method. This version uses the interpolation method with a zoom phase
using cubic interpolation as described by Nocedal and Wright (2006).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>psqn_bfgs(
  par,
  fn,
  gr,
  rel_eps = 1e-08,
  max_it = 100L,
  c1 = 1e-04,
  c2 = 0.9,
  trace = 0L,
  env = NULL,
  gr_tol = -1,
  abs_eps = -1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="psqn_bfgs_+3A_par">par</code></td>
<td>
<p>Initial values for the parameters.</p>
</td></tr>
<tr><td><code id="psqn_bfgs_+3A_fn">fn</code></td>
<td>
<p>Function to evaluate the function to be minimized.</p>
</td></tr>
<tr><td><code id="psqn_bfgs_+3A_gr">gr</code></td>
<td>
<p>Gradient of <code>fn</code>. Should return the function value as an
attribute called <code>"value"</code>.</p>
</td></tr>
<tr><td><code id="psqn_bfgs_+3A_rel_eps">rel_eps</code></td>
<td>
<p>Relative convergence threshold.</p>
</td></tr>
<tr><td><code id="psqn_bfgs_+3A_max_it">max_it</code></td>
<td>
<p>Maximum number of iterations.</p>
</td></tr>
<tr><td><code id="psqn_bfgs_+3A_c1">c1</code></td>
<td>
<p>Thresholds for the Wolfe condition.</p>
</td></tr>
<tr><td><code id="psqn_bfgs_+3A_c2">c2</code></td>
<td>
<p>Thresholds for the Wolfe condition.</p>
</td></tr>
<tr><td><code id="psqn_bfgs_+3A_trace">trace</code></td>
<td>
<p>Integer where larger values gives more information during the
optimization.</p>
</td></tr>
<tr><td><code id="psqn_bfgs_+3A_env">env</code></td>
<td>
<p>Environment to evaluate <code>fn</code> and <code>gr</code> in.
<code>NULL</code> yields the global environment.</p>
</td></tr>
<tr><td><code id="psqn_bfgs_+3A_gr_tol">gr_tol</code></td>
<td>
<p>Convergence tolerance for the Euclidean norm of the gradient. A negative
value yields no check.</p>
</td></tr>
<tr><td><code id="psqn_bfgs_+3A_abs_eps">abs_eps</code></td>
<td>
<p>Absolute convergence threshold. A negative values yields no
check.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object like the object returned by <code><a href="#topic+psqn">psqn</a></code>.
</p>


<h3>References</h3>

<p>Nocedal, J. and Wright, S. J. (2006). <em>Numerical Optimization</em>
(2nd ed.). Springer.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># declare function and gradient from the example from help(optim)
fn &lt;- function(x) {
  x1 &lt;- x[1]
  x2 &lt;- x[2]
  100 * (x2 - x1 * x1)^2 + (1 - x1)^2
}
gr &lt;- function(x) {
  x1 &lt;- x[1]
  x2 &lt;- x[2]
  c(-400 * x1 * (x2 - x1 * x1) - 2 * (1 - x1),
     200 *      (x2 - x1 * x1))
}

# we need a different function for the method in this package
gr_psqn &lt;- function(x) {
  x1 &lt;- x[1]
  x2 &lt;- x[2]
  out &lt;- c(-400 * x1 * (x2 - x1 * x1) - 2 * (1 - x1),
            200 *      (x2 - x1 * x1))
  attr(out, "value") &lt;- 100 * (x2 - x1 * x1)^2 + (1 - x1)^2
  out
}

# we get the same
optim    (c(-1.2, 1), fn, gr, method = "BFGS")
psqn_bfgs(c(-1.2, 1), fn, gr_psqn)

# compare the computation time
system.time(replicate(1000,
                      optim    (c(-1.2, 1), fn, gr, method = "BFGS")))
system.time(replicate(1000,
                      psqn_bfgs(c(-1.2, 1), fn, gr_psqn)))

# we can use an alternative convergence criterion
org &lt;- psqn_bfgs(c(-1.2, 1), fn, gr_psqn, rel_eps = 1e-4)
sqrt(sum(gr_psqn(org$par)^2))

new_res &lt;- psqn_bfgs(c(-1.2, 1), fn, gr_psqn, rel_eps = 1e-4, gr_tol = 1e-8)
sqrt(sum(gr_psqn(new_res$par)^2))

new_res &lt;- psqn_bfgs(c(-1.2, 1), fn, gr_psqn, rel_eps = 1, abs_eps = 1e-2)
new_res$value - org$value # ~ there (but this is not guaranteed)
</code></pre>

<hr>
<h2 id='psqn_generic'>Generic Partially Separable Function Optimization</h2><span id='topic+psqn_generic'></span><span id='topic+psqn_aug_Lagrang_generic'></span>

<h3>Description</h3>

<p>Optimization method for generic partially separable functions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>psqn_generic(
  par,
  fn,
  n_ele_func,
  rel_eps = 1e-08,
  max_it = 100L,
  n_threads = 1L,
  c1 = 1e-04,
  c2 = 0.9,
  use_bfgs = TRUE,
  trace = 0L,
  cg_tol = 0.5,
  strong_wolfe = TRUE,
  env = NULL,
  max_cg = 0L,
  pre_method = 1L,
  mask = as.integer(c()),
  gr_tol = -1
)

psqn_aug_Lagrang_generic(
  par,
  fn,
  n_ele_func,
  consts,
  n_constraints,
  multipliers = as.numeric(c()),
  penalty_start = 1L,
  rel_eps = 1e-08,
  max_it = 100L,
  max_it_outer = 100L,
  violations_norm_thresh = 1e-06,
  n_threads = 1L,
  c1 = 1e-04,
  c2 = 0.9,
  tau = 1.5,
  use_bfgs = TRUE,
  trace = 0L,
  cg_tol = 0.5,
  strong_wolfe = TRUE,
  env = NULL,
  max_cg = 0L,
  pre_method = 1L,
  mask = as.integer(c()),
  gr_tol = -1
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="psqn_generic_+3A_par">par</code></td>
<td>
<p>Initial values for the parameters.</p>
</td></tr>
<tr><td><code id="psqn_generic_+3A_fn">fn</code></td>
<td>
<p>Function to compute the element functions and their
derivatives. Each call computes an element function. See the examples
section.</p>
</td></tr>
<tr><td><code id="psqn_generic_+3A_n_ele_func">n_ele_func</code></td>
<td>
<p>Number of element functions.</p>
</td></tr>
<tr><td><code id="psqn_generic_+3A_rel_eps">rel_eps</code></td>
<td>
<p>Relative convergence threshold.</p>
</td></tr>
<tr><td><code id="psqn_generic_+3A_max_it">max_it</code></td>
<td>
<p>Maximum number of iterations.</p>
</td></tr>
<tr><td><code id="psqn_generic_+3A_n_threads">n_threads</code></td>
<td>
<p>Number of threads to use.</p>
</td></tr>
<tr><td><code id="psqn_generic_+3A_c1">c1</code></td>
<td>
<p>Thresholds for the Wolfe condition.</p>
</td></tr>
<tr><td><code id="psqn_generic_+3A_c2">c2</code></td>
<td>
<p>Thresholds for the Wolfe condition.</p>
</td></tr>
<tr><td><code id="psqn_generic_+3A_use_bfgs">use_bfgs</code></td>
<td>
<p>Logical for whether to use BFGS updates or SR1 updates.</p>
</td></tr>
<tr><td><code id="psqn_generic_+3A_trace">trace</code></td>
<td>
<p>Integer where larger values gives more information during the
optimization.</p>
</td></tr>
<tr><td><code id="psqn_generic_+3A_cg_tol">cg_tol</code></td>
<td>
<p>Threshold for the conjugate gradient method.</p>
</td></tr>
<tr><td><code id="psqn_generic_+3A_strong_wolfe">strong_wolfe</code></td>
<td>
<p><code>TRUE</code> if the strong Wolfe condition should be used.</p>
</td></tr>
<tr><td><code id="psqn_generic_+3A_env">env</code></td>
<td>
<p>Environment to evaluate <code>fn</code> in. <code>NULL</code> yields the
global environment.</p>
</td></tr>
<tr><td><code id="psqn_generic_+3A_max_cg">max_cg</code></td>
<td>
<p>Maximum number of conjugate gradient iterations in each
iteration. Use zero if there should not be a limit.</p>
</td></tr>
<tr><td><code id="psqn_generic_+3A_pre_method">pre_method</code></td>
<td>
<p>Preconditioning method in the conjugate gradient method.
Zero yields no preconditioning, one yields diagonal preconditioning,
two yields the incomplete Cholesky factorization from Eigen, and
three yields a block diagonal preconditioning. One and three are fast
options with three seeming to work well for some poorly conditioned
problems.</p>
</td></tr>
<tr><td><code id="psqn_generic_+3A_mask">mask</code></td>
<td>
<p>zero based indices for parameters to mask (i.e. fix).</p>
</td></tr>
<tr><td><code id="psqn_generic_+3A_gr_tol">gr_tol</code></td>
<td>
<p>convergence tolerance for the Euclidean norm of the gradient. A negative
value yields no check.</p>
</td></tr>
<tr><td><code id="psqn_generic_+3A_consts">consts</code></td>
<td>
<p>Function to compute the constraints which must be equal to
zero. See the example Section.</p>
</td></tr>
<tr><td><code id="psqn_generic_+3A_n_constraints">n_constraints</code></td>
<td>
<p>The number of constraints.</p>
</td></tr>
<tr><td><code id="psqn_generic_+3A_multipliers">multipliers</code></td>
<td>
<p>Staring values for the multipliers in the augmented
Lagrangian method. There needs to be the same number of multipliers as the
number of constraints. An empty vector, <code>numeric()</code>, yields zero as
the starting value for all multipliers.</p>
</td></tr>
<tr><td><code id="psqn_generic_+3A_penalty_start">penalty_start</code></td>
<td>
<p>Starting value for the penalty parameterin the
augmented Lagrangian method.</p>
</td></tr>
<tr><td><code id="psqn_generic_+3A_max_it_outer">max_it_outer</code></td>
<td>
<p>Maximum number of augmented Lagrangian steps.</p>
</td></tr>
<tr><td><code id="psqn_generic_+3A_violations_norm_thresh">violations_norm_thresh</code></td>
<td>
<p>Threshold for the norm of the constraint
violations.</p>
</td></tr>
<tr><td><code id="psqn_generic_+3A_tau">tau</code></td>
<td>
<p>Multiplier used for the penalty parameter between each outer
iterations.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function follows the method described by Nocedal and Wright (2006)
and mainly what is described in Section 7.4. Details are provided
in the psqn vignette. See <code>vignette("psqn", package = "psqn")</code>.
</p>
<p>The partially separable function we consider can be quite general and the
only restriction is that we can write the function to be minimized as a sum
of so-called element functions each of which only depends on a small number
of the parameters. A more restricted version is available through the
<code><a href="#topic+psqn">psqn</a></code> function.
</p>
<p>The optimization function is also available in C++ as a header-only
library. Using C++ may reduce the computation time substantially. See
the vignette in the package for examples.
</p>


<h3>Value</h3>

<p>A list like <code><a href="#topic+psqn">psqn</a></code> and <code><a href="#topic+psqn_aug_Lagrang">psqn_aug_Lagrang</a></code>.
</p>


<h3>References</h3>

<p>Nocedal, J. and Wright, S. J. (2006). <em>Numerical Optimization</em>
(2nd ed.). Springer.
</p>
<p>Lin, C. and Moré, J. J. (1999). <em>Incomplete Cholesky factorizations
with limited memory</em>. SIAM Journal on Scientific Computing.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># example with a GLM as in the vignette

# assign the number of parameters and number of observations
set.seed(1)
K &lt;- 20L
n &lt;- 5L * K

# simulate the data
truth_limit &lt;- runif(K, -1, 1)
dat &lt;- replicate(
  n, {
    # sample the indices
    n_samp &lt;- sample.int(5L, 1L) + 1L
    indices &lt;- sort(sample.int(K, n_samp))

    # sample the outcome, y, and return
    list(y = rpois(1, exp(sum(truth_limit[indices]))),
         indices = indices)
  }, simplify = FALSE)

# we need each parameter to be present at least once
stopifnot(length(unique(unlist(
  lapply(dat, `[`, "indices")
))) == K) # otherwise we need to change the code

# assign the function we need to pass to psqn_generic
#
# Args:
#   i cluster/element function index.
#   par the parameters that this element function depends on. It has length zero
#       if we need to pass the one-based indices of the parameters that the i'th
#       element function depends on.
#   comp_grad TRUE of the gradient should be computed.
r_func &lt;- function(i, par, comp_grad){
  z &lt;- dat[[i]]
  if(length(par) == 0L)
    # return the indices
    return(z$indices)

  eta &lt;- sum(par)
  exp_eta &lt;- exp(eta)
  out &lt;- -z$y * eta + exp_eta
  if(comp_grad)
    attr(out, "grad") &lt;- rep(-z$y + exp_eta, length(z$indices))
  out
}

# minimize the function
R_res &lt;- psqn_generic(
  par = numeric(K), fn = r_func, n_ele_func = length(dat), c1 = 1e-4, c2 = .1,
  trace = 0L, rel_eps = 1e-9, max_it = 1000L, env = environment())

# get the same as if we had used optim
R_func &lt;- function(x){
  out &lt;- vapply(dat, function(z){
    eta &lt;- sum(x[z$indices])
    -z$y * eta + exp(eta)
  }, 0.)
  sum(out)
}
R_func_gr &lt;- function(x){
  out &lt;- numeric(length(x))
  for(z in dat){
    idx_i &lt;- z$indices
    eta &lt;- sum(x[idx_i])
    out[idx_i] &lt;- out[idx_i] -z$y + exp(eta)
  }
  out
}

opt &lt;- optim(numeric(K), R_func, R_func_gr, method = "BFGS",
             control = list(maxit = 1000L))

# we got the same
all.equal(opt$value, R_res$value)

# also works if we fix some parameters
to_fix &lt;- c(7L, 1L, 18L)
par_fix &lt;- numeric(K)
par_fix[to_fix] &lt;- c(-1, -.5, 0)

R_res &lt;- psqn_generic(
  par = par_fix, fn = r_func, n_ele_func = length(dat), c1 = 1e-4, c2 = .1,
  trace = 0L, rel_eps = 1e-9, max_it = 1000L, env = environment(),
  mask = to_fix - 1L) # notice the -1L because of the zero based indices

# the equivalent optim version is
opt &lt;- optim(
  numeric(K - length(to_fix)),
  function(par) { par_fix[-to_fix] &lt;- par; R_func   (par_fix) },
  function(par) { par_fix[-to_fix] &lt;- par; R_func_gr(par_fix)[-to_fix] },
  method = "BFGS", control = list(maxit = 1000L))

res_optim &lt;- par_fix
res_optim[-to_fix] &lt;- opt$par

# we got the same
all.equal(res_optim, R_res$par, tolerance = 1e-5)
all.equal(R_res$par[to_fix], par_fix[to_fix]) # the parameters are fixed

# add equality constraints
idx_constrained &lt;- list(c(2L, 19L, 11L, 7L), c(3L, 5L, 8L), 9:7)

# evaluates the c(x) in equalities c(x) = 0.
#
# Args:
#   i constrain index.
#   par the constrained parameters. It has length zero if we need to pass the
#       one-based indices of the parameters that the i'th constrain depends on.
#   what integer which is zero if the function should be returned and one if the
#        gradient should be computed.
consts &lt;- function(i, par, what){
  if(length(par) == 0)
    # need to return the indices
    return(idx_constrained[[i]])

  if(i == 1){
    out &lt;- exp(sum(par[1:2])) + exp(sum(par[3:4])) - 1
    if(what == 1)
      attr(out, "grad") &lt;- c(rep(exp(sum(par[1:2])), 2),
                             rep(exp(sum(par[3:4])), 2))

  } else if(i == 2){
    # the parameters need to be on a circle
    out &lt;- sum(par^2) - 1
    if(what == 1)
      attr(out, "grad") &lt;- 2 * par
  } else if(i == 3){
    out &lt;- sum(par) - .5
    if(what == 1)
      attr(out, "grad") &lt;- rep(1, length(par))
  }

  out
}

# optimize with the constraints and masking
res_consts &lt;- psqn_aug_Lagrang_generic(
  par = par_fix, fn = r_func, n_ele_func = length(dat), c1 = 1e-4, c2 = .1,
  trace = 0L, rel_eps = 1e-8, max_it = 1000L, env = environment(),
  consts = consts, n_constraints = length(idx_constrained),
  mask = to_fix - 1L)

res_consts

# the constraints are satisfied
consts(1, res_consts$par[idx_constrained[[1]]], 0) # ~ 0
consts(2, res_consts$par[idx_constrained[[2]]], 0) # ~ 0
consts(3, res_consts$par[idx_constrained[[3]]], 0) # ~ 0

# compare with the alabama package
if(require(alabama)){
    ala_fit &lt;- auglag(
      par_fix, R_func, R_func_gr,
      heq = function(x){
        c(x[to_fix] - par_fix[to_fix],
          consts(1, x[idx_constrained[[1]]], 0),
          consts(2, x[idx_constrained[[2]]], 0),
          consts(3, x[idx_constrained[[3]]], 0))
      }, control.outer = list(trace = 0L))

    cat(sprintf("Difference in objective value is %.6f. Parametes are\n",
                ala_fit$value - res_consts$value))
    print(rbind(alabama = ala_fit$par,
                psqn = res_consts$par))

    cat("\nOutput from all.equal\n")
    print(all.equal(ala_fit$par, res_consts$par))
}

# the overhead here is though quite large with the R interface from the psqn
# package. A C++ implementation is much faster as shown in
# vignette("psqn", package = "psqn"). The reason it is that it is very fast
# to evaluate the element functions in this case

</code></pre>

<hr>
<h2 id='psqn_hess'>Computes the Hessian.</h2><span id='topic+psqn_hess'></span><span id='topic+psqn_generic_hess'></span>

<h3>Description</h3>

<p>Computes the Hessian using numerical differentiation with Richardson
extrapolation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>psqn_hess(
  val,
  fn,
  n_ele_func,
  n_threads = 1L,
  env = NULL,
  eps = 0.001,
  scale = 2,
  tol = 1e-09,
  order = 6L
)

psqn_generic_hess(
  val,
  fn,
  n_ele_func,
  n_threads = 1L,
  env = NULL,
  eps = 0.001,
  scale = 2,
  tol = 1e-09,
  order = 6L
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="psqn_hess_+3A_val">val</code></td>
<td>
<p>Where to evaluate the function at.</p>
</td></tr>
<tr><td><code id="psqn_hess_+3A_fn">fn</code></td>
<td>
<p>Function to compute the element functions and their derivatives.
See <code><a href="#topic+psqn">psqn</a></code> and <code><a href="#topic+psqn_generic">psqn_generic</a></code>.</p>
</td></tr>
<tr><td><code id="psqn_hess_+3A_n_ele_func">n_ele_func</code></td>
<td>
<p>Number of element functions.</p>
</td></tr>
<tr><td><code id="psqn_hess_+3A_n_threads">n_threads</code></td>
<td>
<p>Number of threads to use.</p>
</td></tr>
<tr><td><code id="psqn_hess_+3A_env">env</code></td>
<td>
<p>Environment to evaluate <code>fn</code> in. <code>NULL</code> yields the
global environment.</p>
</td></tr>
<tr><td><code id="psqn_hess_+3A_eps">eps</code></td>
<td>
<p>Determines the step size. See the details.</p>
</td></tr>
<tr><td><code id="psqn_hess_+3A_scale">scale</code></td>
<td>
<p>Scaling factor in the Richardson extrapolation. See the
details.</p>
</td></tr>
<tr><td><code id="psqn_hess_+3A_tol">tol</code></td>
<td>
<p>Relative convergence criteria. See the details.</p>
</td></tr>
<tr><td><code id="psqn_hess_+3A_order">order</code></td>
<td>
<p>Maximum number of iteration of the Richardson extrapolation.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function computes the Hessian using numerical differentiation with
centered differences and subsequent use of Richardson
extrapolation to refine the estimate.
</p>
<p>The additional arguments are as follows: The numerical differentiation
is applied for each argument with a step size of
<code>s = max(eps, |x| * eps)</code>.
The Richardson extrapolation at iteration <code>i</code> uses a step size of
<code>s * scale^(-i)</code>. The convergence threshold for each comportment of
the gradient is <code>max(tol, |gr(x)[j]| * tol)</code>.
</p>
<p>The numerical differentiation is done on each element function and thus
much more efficient then doing it on the whole gradient.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># assign model parameters, number of random effects, and fixed effects
q &lt;- 2 # number of private parameters per cluster
p &lt;- 1 # number of global parameters
beta &lt;- sqrt((1:p) / sum(1:p))
Sigma &lt;- diag(q)

# simulate a data set
set.seed(66608927)
n_clusters &lt;- 20L # number of clusters
sim_dat &lt;- replicate(n_clusters, {
  n_members &lt;- sample.int(8L, 1L) + 2L
  X &lt;- matrix(runif(p * n_members, -sqrt(6 / 2), sqrt(6 / 2)),
              p)
  u &lt;- drop(rnorm(q) %*% chol(Sigma))
  Z &lt;- matrix(runif(q * n_members, -sqrt(6 / 2 / q), sqrt(6 / 2 / q)),
              q)
  eta &lt;- drop(beta %*% X + u %*% Z)
  y &lt;- as.numeric((1 + exp(-eta))^(-1) &gt; runif(n_members))

  list(X = X, Z = Z, y = y, u = u, Sigma_inv = solve(Sigma))
}, simplify = FALSE)

# evaluates the negative log integrand.
#
# Args:
#   i cluster/element function index.
#   par the global and private parameter for this cluster. It has length
#       zero if the number of parameters is requested. That is, a 2D integer
#       vector the number of global parameters as the first element and the
#       number of private parameters as the second element.
#   comp_grad logical for whether to compute the gradient.
r_func &lt;- function(i, par, comp_grad){
  dat &lt;- sim_dat[[i]]
  X &lt;- dat$X
  Z &lt;- dat$Z

  if(length(par) &lt; 1)
    # requested the dimension of the parameter
    return(c(global_dim = NROW(dat$X), private_dim = NROW(dat$Z)))

  y &lt;- dat$y
  Sigma_inv &lt;- dat$Sigma_inv

  beta &lt;- par[1:p]
  uhat &lt;- par[1:q + p]
  eta &lt;- drop(beta %*% X + uhat %*% Z)
  exp_eta &lt;- exp(eta)

  out &lt;- -sum(y * eta) + sum(log(1 + exp_eta)) +
    sum(uhat * (Sigma_inv %*% uhat)) / 2
  if(comp_grad){
    d_eta &lt;- -y + exp_eta / (1 + exp_eta)
    grad &lt;- c(X %*% d_eta,
              Z %*% d_eta + dat$Sigma_inv %*% uhat)
    attr(out, "grad") &lt;- grad
  }

  out
}

# compute the hessian
set.seed(1)
par &lt;- runif(p + q * n_clusters, -1)

hess &lt;- psqn_hess(val = par, fn = r_func, n_ele_func = n_clusters)

# compare with numerical differentiation from R
if(require(numDeriv)){
    hess_num &lt;- jacobian(function(x){
        out &lt;- numeric(length(x))
        for(i in seq_len(n_clusters)){
            out_i &lt;- r_func(i, x[c(1:p, 1:q + (i - 1L) * q + p)], TRUE)
            out[1:p] &lt;- out[1:p] + attr(out_i, "grad")[1:p]
            out[1:q + (i - 1L) * q + p] &lt;- attr(out_i, "grad")[1:q + p]
        }
        out
    }, par)

    cat("Output of all.equal\n")
    print(all.equal(Matrix(hess_num, sparse = TRUE), hess))
}

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
