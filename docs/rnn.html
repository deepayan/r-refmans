<!DOCTYPE html><html lang="en"><head><title>Help for package rnn</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {rnn}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#backprop_gru'><p>backprop_gru</p></a></li>
<li><a href='#backprop_lstm'><p>backprop_lstm</p></a></li>
<li><a href='#backprop_r'><p>backprop_r</p></a></li>
<li><a href='#backprop_rnn'><p>backprop_rnn</p></a></li>
<li><a href='#bin2int'><p>Binary to Integer</p></a></li>
<li><a href='#clean_lstm'><p>clean_lstm</p></a></li>
<li><a href='#clean_r'><p>init_r</p></a></li>
<li><a href='#clean_rnn'><p>clean_rnn</p></a></li>
<li><a href='#epoch_annealing'><p>epoch annealing</p></a></li>
<li><a href='#epoch_print'><p>epoch printing for trainr</p></a></li>
<li><a href='#init_gru'><p>init_gru</p></a></li>
<li><a href='#init_lstm'><p>init_lstm</p></a></li>
<li><a href='#init_r'><p>init_r</p></a></li>
<li><a href='#init_rnn'><p>init_rnn</p></a></li>
<li><a href='#int2bin'><p>Integer to Binary</p></a></li>
<li><a href='#loss_L1'><p>L1 loss</p></a></li>
<li><a href='#predict_gru'><p>gru prediction function</p></a></li>
<li><a href='#predict_lstm'><p>gru prediction function</p></a></li>
<li><a href='#predict_rnn'><p>Recurrent Neural Network</p></a></li>
<li><a href='#predictr'><p>Recurrent Neural Network</p></a></li>
<li><a href='#rnn'><p>Recurrent Neural Network</p></a></li>
<li><a href='#trainr'><p>Recurrent Neural Network</p></a></li>
<li><a href='#update_adagrad'><p>update_adagrad</p></a></li>
<li><a href='#update_r'><p>update_r</p></a></li>
<li><a href='#update_sgd'><p>update_sgd</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Title:</td>
<td>Recurrent Neural Network</td>
</tr>
<tr>
<td>Version:</td>
<td>1.9.0</td>
</tr>
<tr>
<td>Description:</td>
<td>Implementation of a Recurrent Neural Network architectures in native R, including Long Short-Term Memory (Hochreiter and Schmidhuber, &lt;<a href="https://doi.org/10.1162%2Fneco.1997.9.8.1735">doi:10.1162/neco.1997.9.8.1735</a>&gt;), Gated Recurrent Unit (Chung et al., &lt;<a href="https://doi.org/10.48550/arXiv.1412.3555">doi:10.48550/arXiv.1412.3555</a>&gt;) and vanilla RNN.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.2.2)</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://qua.st/rnn/">https://qua.st/rnn/</a>, <a href="https://github.com/bquast/rnn">https://github.com/bquast/rnn</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/bquast/rnn/issues">https://github.com/bquast/rnn/issues</a></td>
</tr>
<tr>
<td>Imports:</td>
<td>attention, sigmoid (&ge; 1.4.0)</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat, knitr, rmarkdown</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-04-21 15:57:27 UTC; bquast</td>
</tr>
<tr>
<td>Author:</td>
<td>Bastiaan Quast <a href="https://orcid.org/0000-0002-2951-3577"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Bastiaan Quast &lt;bquast@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-04-22 00:50:19 UTC</td>
</tr>
</table>
<hr>
<h2 id='backprop_gru'>backprop_gru</h2><span id='topic+backprop_gru'></span>

<h3>Description</h3>

<p>backpropagate the error in a model object of type gru
</p>


<h3>Usage</h3>

<pre><code class='language-R'>backprop_gru(model, a, c, j, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="backprop_gru_+3A_model">model</code></td>
<td>
<p>the output model object</p>
</td></tr>
<tr><td><code id="backprop_gru_+3A_a">a</code></td>
<td>
<p>the input of this learning batch</p>
</td></tr>
<tr><td><code id="backprop_gru_+3A_c">c</code></td>
<td>
<p>the output of this learning batch</p>
</td></tr>
<tr><td><code id="backprop_gru_+3A_j">j</code></td>
<td>
<p>the indexes of the sample in the current batch</p>
</td></tr>
<tr><td><code id="backprop_gru_+3A_...">...</code></td>
<td>
<p>argument to be passed to method</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the updated model
</p>

<hr>
<h2 id='backprop_lstm'>backprop_lstm</h2><span id='topic+backprop_lstm'></span>

<h3>Description</h3>

<p>backpropagate the error in a model object of type rlstm
</p>


<h3>Usage</h3>

<pre><code class='language-R'>backprop_lstm(model, a, c, j, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="backprop_lstm_+3A_model">model</code></td>
<td>
<p>the output model object</p>
</td></tr>
<tr><td><code id="backprop_lstm_+3A_a">a</code></td>
<td>
<p>the input of this learning batch</p>
</td></tr>
<tr><td><code id="backprop_lstm_+3A_c">c</code></td>
<td>
<p>the output of this learning batch</p>
</td></tr>
<tr><td><code id="backprop_lstm_+3A_j">j</code></td>
<td>
<p>the indexes of the sample in the current batch</p>
</td></tr>
<tr><td><code id="backprop_lstm_+3A_...">...</code></td>
<td>
<p>argument to be passed to method</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the updated model
</p>

<hr>
<h2 id='backprop_r'>backprop_r</h2><span id='topic+backprop_r'></span>

<h3>Description</h3>

<p>backpropagate the error in a model object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>backprop_r(model, a, c, j, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="backprop_r_+3A_model">model</code></td>
<td>
<p>the output model object</p>
</td></tr>
<tr><td><code id="backprop_r_+3A_a">a</code></td>
<td>
<p>the input of this learning batch</p>
</td></tr>
<tr><td><code id="backprop_r_+3A_c">c</code></td>
<td>
<p>the output of this learning batch</p>
</td></tr>
<tr><td><code id="backprop_r_+3A_j">j</code></td>
<td>
<p>the indexes of the sample in the current batch</p>
</td></tr>
<tr><td><code id="backprop_r_+3A_...">...</code></td>
<td>
<p>argument to be passed to method</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the updated model
</p>

<hr>
<h2 id='backprop_rnn'>backprop_rnn</h2><span id='topic+backprop_rnn'></span>

<h3>Description</h3>

<p>backpropagate the error in a model object of type rnn
</p>


<h3>Usage</h3>

<pre><code class='language-R'>backprop_rnn(model, a, c, j, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="backprop_rnn_+3A_model">model</code></td>
<td>
<p>the output model object</p>
</td></tr>
<tr><td><code id="backprop_rnn_+3A_a">a</code></td>
<td>
<p>the input of this learning batch</p>
</td></tr>
<tr><td><code id="backprop_rnn_+3A_c">c</code></td>
<td>
<p>the output of this learning batch</p>
</td></tr>
<tr><td><code id="backprop_rnn_+3A_j">j</code></td>
<td>
<p>the indexes of the sample in the current batch</p>
</td></tr>
<tr><td><code id="backprop_rnn_+3A_...">...</code></td>
<td>
<p>argument to be passed to method</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the updated model
</p>

<hr>
<h2 id='bin2int'>Binary to Integer</h2><span id='topic+bin2int'></span><span id='topic+b2i'></span>

<h3>Description</h3>

<p>Binary to Integer
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bin2int(binary)

b2i(binary)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="bin2int_+3A_binary">binary</code></td>
<td>
<p>input binary</p>
</td></tr>
</table>


<h3>Value</h3>

<p>integer representation
</p>


<h3>Functions</h3>


<ul>
<li> <p><code>b2i()</code>: individual Binary to Integer
</p>
</li></ul>

<hr>
<h2 id='clean_lstm'>clean_lstm</h2><span id='topic+clean_lstm'></span>

<h3>Description</h3>

<p>clean the model for lighter output
</p>


<h3>Usage</h3>

<pre><code class='language-R'>clean_lstm(model)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="clean_lstm_+3A_model">model</code></td>
<td>
<p>the output model object</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the updated model
</p>

<hr>
<h2 id='clean_r'>init_r</h2><span id='topic+clean_r'></span>

<h3>Description</h3>

<p>Initialize the weight parameters
</p>


<h3>Usage</h3>

<pre><code class='language-R'>clean_r(model)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="clean_r_+3A_model">model</code></td>
<td>
<p>the output model object</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the updated model
</p>

<hr>
<h2 id='clean_rnn'>clean_rnn</h2><span id='topic+clean_rnn'></span>

<h3>Description</h3>

<p>clean the model for lighter output
</p>


<h3>Usage</h3>

<pre><code class='language-R'>clean_rnn(model)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="clean_rnn_+3A_model">model</code></td>
<td>
<p>the output model object</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the updated model
</p>

<hr>
<h2 id='epoch_annealing'>epoch annealing</h2><span id='topic+epoch_annealing'></span>

<h3>Description</h3>

<p>Apply the learning rate decay to the learning rate, called in epoch_model_function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>epoch_annealing(model)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="epoch_annealing_+3A_model">model</code></td>
<td>
<p>the output model object</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the updated model
</p>

<hr>
<h2 id='epoch_print'>epoch printing for trainr</h2><span id='topic+epoch_print'></span>

<h3>Description</h3>

<p>Print the error adn learning rate at each epoch of the trainr learning, called in epoch_function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>epoch_print(model)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="epoch_print_+3A_model">model</code></td>
<td>
<p>the output model object</p>
</td></tr>
</table>


<h3>Value</h3>

<p>nothing
</p>

<hr>
<h2 id='init_gru'>init_gru</h2><span id='topic+init_gru'></span>

<h3>Description</h3>

<p>Initialize the weight parameter for a gru
</p>


<h3>Usage</h3>

<pre><code class='language-R'>init_gru(model)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="init_gru_+3A_model">model</code></td>
<td>
<p>the output model object</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the updated model
</p>

<hr>
<h2 id='init_lstm'>init_lstm</h2><span id='topic+init_lstm'></span>

<h3>Description</h3>

<p>Initialize the weight parameter for a lstm
</p>


<h3>Usage</h3>

<pre><code class='language-R'>init_lstm(model)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="init_lstm_+3A_model">model</code></td>
<td>
<p>the output model object</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the updated model
</p>

<hr>
<h2 id='init_r'>init_r</h2><span id='topic+init_r'></span>

<h3>Description</h3>

<p>Initialize the weight parameters
</p>


<h3>Usage</h3>

<pre><code class='language-R'>init_r(model)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="init_r_+3A_model">model</code></td>
<td>
<p>the output model object</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the updated model
</p>

<hr>
<h2 id='init_rnn'>init_rnn</h2><span id='topic+init_rnn'></span>

<h3>Description</h3>

<p>Initialize the weight parameter for a rnn
</p>


<h3>Usage</h3>

<pre><code class='language-R'>init_rnn(model)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="init_rnn_+3A_model">model</code></td>
<td>
<p>the output model object</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the updated model
</p>

<hr>
<h2 id='int2bin'>Integer to Binary</h2><span id='topic+int2bin'></span><span id='topic+i2b'></span>

<h3>Description</h3>

<p>Integer to Binary
</p>


<h3>Usage</h3>

<pre><code class='language-R'>int2bin(integer, length = 8)

i2b(integer, length = 8)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="int2bin_+3A_integer">integer</code></td>
<td>
<p>input integer</p>
</td></tr>
<tr><td><code id="int2bin_+3A_length">length</code></td>
<td>
<p>binary representation length</p>
</td></tr>
</table>


<h3>Value</h3>

<p>binary representation
</p>


<h3>Functions</h3>


<ul>
<li> <p><code>i2b()</code>: individual Integer to Binary
</p>
</li></ul>

<hr>
<h2 id='loss_L1'>L1 loss</h2><span id='topic+loss_L1'></span>

<h3>Description</h3>

<p>Apply the learning rate to the weight update, vocabulary to verify !!
</p>


<h3>Usage</h3>

<pre><code class='language-R'>loss_L1(model)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="loss_L1_+3A_model">model</code></td>
<td>
<p>the output model object</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the updated model
</p>

<hr>
<h2 id='predict_gru'>gru prediction function</h2><span id='topic+predict_gru'></span>

<h3>Description</h3>

<p>predict the output of a gru model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>predict_gru(model, X, hidden = FALSE, real_output = T, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict_gru_+3A_model">model</code></td>
<td>
<p>output of the trainr function</p>
</td></tr>
<tr><td><code id="predict_gru_+3A_x">X</code></td>
<td>
<p>array of input values, dim 1: samples, dim 2: time, dim 3: variables (could be 1 or more, if a matrix, will be coerce to array)</p>
</td></tr>
<tr><td><code id="predict_gru_+3A_hidden">hidden</code></td>
<td>
<p>should the function output the hidden units states</p>
</td></tr>
<tr><td><code id="predict_gru_+3A_real_output">real_output</code></td>
<td>
<p>option used when the function in called inside trainr, do not drop factor for 2 dimension array output</p>
</td></tr>
<tr><td><code id="predict_gru_+3A_...">...</code></td>
<td>
<p>arguments to pass on to sigmoid function</p>
</td></tr>
</table>


<h3>Value</h3>

<p>array or matrix of predicted values
</p>

<hr>
<h2 id='predict_lstm'>gru prediction function</h2><span id='topic+predict_lstm'></span>

<h3>Description</h3>

<p>predict the output of a lstm model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>predict_lstm(model, X, hidden = FALSE, real_output = T, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict_lstm_+3A_model">model</code></td>
<td>
<p>output of the trainr function</p>
</td></tr>
<tr><td><code id="predict_lstm_+3A_x">X</code></td>
<td>
<p>array of input values, dim 1: samples, dim 2: time, dim 3: variables (could be 1 or more, if a matrix, will be coerce to array)</p>
</td></tr>
<tr><td><code id="predict_lstm_+3A_hidden">hidden</code></td>
<td>
<p>should the function output the hidden units states</p>
</td></tr>
<tr><td><code id="predict_lstm_+3A_real_output">real_output</code></td>
<td>
<p>option used when the function in called inside trainr, do not drop factor for 2 dimension array output</p>
</td></tr>
<tr><td><code id="predict_lstm_+3A_...">...</code></td>
<td>
<p>arguments to pass on to sigmoid function</p>
</td></tr>
</table>


<h3>Value</h3>

<p>array or matrix of predicted values
</p>

<hr>
<h2 id='predict_rnn'>Recurrent Neural Network</h2><span id='topic+predict_rnn'></span>

<h3>Description</h3>

<p>predict the output of a RNN model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>predict_rnn(model, X, hidden = FALSE, real_output = T, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict_rnn_+3A_model">model</code></td>
<td>
<p>output of the trainr function</p>
</td></tr>
<tr><td><code id="predict_rnn_+3A_x">X</code></td>
<td>
<p>array of input values, dim 1: samples, dim 2: time, dim 3: variables (could be 1 or more, if a matrix, will be coerce to array)</p>
</td></tr>
<tr><td><code id="predict_rnn_+3A_hidden">hidden</code></td>
<td>
<p>should the function output the hidden units states</p>
</td></tr>
<tr><td><code id="predict_rnn_+3A_real_output">real_output</code></td>
<td>
<p>option used when the function in called inside trainr, do not drop factor for 2 dimension array output</p>
</td></tr>
<tr><td><code id="predict_rnn_+3A_...">...</code></td>
<td>
<p>arguments to pass on to sigmoid function</p>
</td></tr>
</table>


<h3>Value</h3>

<p>array or matrix of predicted values
</p>

<hr>
<h2 id='predictr'>Recurrent Neural Network</h2><span id='topic+predictr'></span>

<h3>Description</h3>

<p>predict the output of a RNN model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>predictr(model, X, hidden = FALSE, real_output = T, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predictr_+3A_model">model</code></td>
<td>
<p>output of the trainr function</p>
</td></tr>
<tr><td><code id="predictr_+3A_x">X</code></td>
<td>
<p>array of input values, dim 1: samples, dim 2: time, dim 3: variables (could be 1 or more, if a matrix, will be coerce to array)</p>
</td></tr>
<tr><td><code id="predictr_+3A_hidden">hidden</code></td>
<td>
<p>should the function output the hidden units states</p>
</td></tr>
<tr><td><code id="predictr_+3A_real_output">real_output</code></td>
<td>
<p>option used when the function in called inside trainr, do not drop factor for 2 dimension array output and other actions. Let it to TRUE, the default, to let the function take care of the data.</p>
</td></tr>
<tr><td><code id="predictr_+3A_...">...</code></td>
<td>
<p>arguments to pass on to sigmoid function</p>
</td></tr>
</table>


<h3>Value</h3>

<p>array or matrix of predicted values
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run:  
# create training numbers
X1 = sample(0:127, 10000, replace=TRUE)
X2 = sample(0:127, 10000, replace=TRUE)

# create training response numbers
Y &lt;- X1 + X2

# convert to binary
X1 &lt;- int2bin(X1)
X2 &lt;- int2bin(X2)
Y  &lt;- int2bin(Y)

# Create 3d array: dim 1: samples; dim 2: time; dim 3: variables.
X &lt;- array( c(X1,X2), dim=c(dim(X1),2) )

# train the model
model &lt;- trainr(Y=Y[,dim(Y)[2]:1],
                X=X[,dim(X)[2]:1,],
                learningrate   =  1,
                hidden_dim     = 16 )
             
# create test inputs
A1 = int2bin( sample(0:127, 7000, replace=TRUE) )
A2 = int2bin( sample(0:127, 7000, replace=TRUE) )

# create 3d array: dim 1: samples; dim 2: time; dim 3: variables
A &lt;- array( c(A1,A2), dim=c(dim(A1),2) )
    
# predict
B  &lt;- predictr(model,
               A[,dim(A)[2]:1,]     )
B = B[,dim(B)[2]:1]
# convert back to integers
A1 &lt;- bin2int(A1)
A2 &lt;- bin2int(A2)
B  &lt;- bin2int(B)
 
# inspect the differences              
table( B-(A1+A2) )

# plot the difference
hist(  B-(A1+A2) )

## End(Not run)

</code></pre>

<hr>
<h2 id='rnn'>Recurrent Neural Network</h2><span id='topic+rnn'></span>

<h3>Description</h3>

<p>A Recurrent Neural Network in native R, transforms numbers to binaries before adding bit by bit, teaching itself how to carry.
</p>


<h3>Author(s)</h3>

<p>Bastiaan Quast <a href="mailto:bquast@gmail.com">bquast@gmail.com</a>
</p>


<h3>References</h3>

<p>https://qua.st/rnn/
</p>


<h3>See Also</h3>

<p><code><a href="#topic+trainr">trainr</a></code> for training a model and <code><a href="#topic+predictr">predictr</a></code> for using a model to make predictions. https://qua.st/rnn
</p>

<hr>
<h2 id='trainr'>Recurrent Neural Network</h2><span id='topic+trainr'></span>

<h3>Description</h3>

<p>Trains a Recurrent Neural Network.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>trainr(
  Y,
  X,
  model = NULL,
  learningrate,
  learningrate_decay = 1,
  momentum = 0,
  hidden_dim = c(10),
  network_type = "rnn",
  numepochs = 1,
  sigmoid = c("logistic", "Gompertz", "tanh"),
  use_bias = F,
  batch_size = 1,
  seq_to_seq_unsync = F,
  update_rule = "sgd",
  epoch_function = c(epoch_print, epoch_annealing),
  loss_function = loss_L1,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="trainr_+3A_y">Y</code></td>
<td>
<p>array of output values, dim 1: samples (must be equal to dim 1 of X), dim 2: time (must be equal to dim 2 of X), dim 3: variables (could be 1 or more, if a matrix, will be coerce to array)</p>
</td></tr>
<tr><td><code id="trainr_+3A_x">X</code></td>
<td>
<p>array of input values, dim 1: samples, dim 2: time, dim 3: variables (could be 1 or more, if a matrix, will be coerce to array)</p>
</td></tr>
<tr><td><code id="trainr_+3A_model">model</code></td>
<td>
<p>a model trained before, used for retraining purpose.</p>
</td></tr>
<tr><td><code id="trainr_+3A_learningrate">learningrate</code></td>
<td>
<p>learning rate to be applied for weight iteration</p>
</td></tr>
<tr><td><code id="trainr_+3A_learningrate_decay">learningrate_decay</code></td>
<td>
<p>coefficient to apply to the learning rate at each epoch, via the epoch_annealing function</p>
</td></tr>
<tr><td><code id="trainr_+3A_momentum">momentum</code></td>
<td>
<p>coefficient of the last weight iteration to keep for faster learning</p>
</td></tr>
<tr><td><code id="trainr_+3A_hidden_dim">hidden_dim</code></td>
<td>
<p>dimension(s) of hidden layer(s)</p>
</td></tr>
<tr><td><code id="trainr_+3A_network_type">network_type</code></td>
<td>
<p>type of network, could be rnn, gru or lstm. gru and lstm are experimentale.</p>
</td></tr>
<tr><td><code id="trainr_+3A_numepochs">numepochs</code></td>
<td>
<p>number of iteration, i.e. number of time the whole dataset is presented to the network</p>
</td></tr>
<tr><td><code id="trainr_+3A_sigmoid">sigmoid</code></td>
<td>
<p>method to be passed to the sigmoid function</p>
</td></tr>
<tr><td><code id="trainr_+3A_use_bias">use_bias</code></td>
<td>
<p>should the network use bias</p>
</td></tr>
<tr><td><code id="trainr_+3A_batch_size">batch_size</code></td>
<td>
<p>batch size: number of samples used at each weight iteration, only 1 supported for the moment</p>
</td></tr>
<tr><td><code id="trainr_+3A_seq_to_seq_unsync">seq_to_seq_unsync</code></td>
<td>
<p>if TRUE, the network will be trained to backpropagate only the second half of the output error. If many to one is the target, just make Y have a time dim of 1. The X and Y data are modify at first to fit a classic learning, error are set to 0 during back propagation, input for the second part is also set to 0.</p>
</td></tr>
<tr><td><code id="trainr_+3A_update_rule">update_rule</code></td>
<td>
<p>rule to update the weight, &quot;sgd&quot;, the default, is stochastic gradient descent, other available options are &quot;adagrad&quot; (experimentale, do not learn yet)</p>
</td></tr>
<tr><td><code id="trainr_+3A_epoch_function">epoch_function</code></td>
<td>
<p>vector of functions to applied at each epoch loop. Use it to intereact with the objects inside the list model or to print and plot at each epoch. Should return the model.</p>
</td></tr>
<tr><td><code id="trainr_+3A_loss_function">loss_function</code></td>
<td>
<p>loss function, applied in each sample loop, vocabulary to verify.</p>
</td></tr>
<tr><td><code id="trainr_+3A_...">...</code></td>
<td>
<p>Arguments to be passed to methods, to be used in user defined functions</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a model to be used by the predictr function
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# create training numbers
X1 = sample(0:127, 10000, replace=TRUE)
X2 = sample(0:127, 10000, replace=TRUE)

# create training response numbers
Y &lt;- X1 + X2

# convert to binary
X1 &lt;- int2bin(X1, length=8)
X2 &lt;- int2bin(X2, length=8)
Y  &lt;- int2bin(Y,  length=8)

# create 3d array: dim 1: samples; dim 2: time; dim 3: variables
X &lt;- array( c(X1,X2), dim=c(dim(X1),2) )

# train the model
model &lt;- trainr(Y=Y,
                X=X,
                learningrate   =  1,
                hidden_dim     = 16  )

## End(Not run)
    
</code></pre>

<hr>
<h2 id='update_adagrad'>update_adagrad</h2><span id='topic+update_adagrad'></span>

<h3>Description</h3>

<p>Apply the update with adagrad, not working yet
</p>


<h3>Usage</h3>

<pre><code class='language-R'>update_adagrad(model)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="update_adagrad_+3A_model">model</code></td>
<td>
<p>the output model object</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the updated model
</p>

<hr>
<h2 id='update_r'>update_r</h2><span id='topic+update_r'></span>

<h3>Description</h3>

<p>Apply the update
</p>


<h3>Usage</h3>

<pre><code class='language-R'>update_r(model)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="update_r_+3A_model">model</code></td>
<td>
<p>the output model object</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the updated model
</p>

<hr>
<h2 id='update_sgd'>update_sgd</h2><span id='topic+update_sgd'></span>

<h3>Description</h3>

<p>Apply the update with stochastic gradient descent
</p>


<h3>Usage</h3>

<pre><code class='language-R'>update_sgd(model)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="update_sgd_+3A_model">model</code></td>
<td>
<p>the output model object</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the updated model
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
