<!DOCTYPE html><html><head><title>Help for package aggTrees</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {aggTrees}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#avg_characteristics_rpart'><p>Leaves Average Characteristics</p></a></li>
<li><a href='#balance_measures'><p>Balance Measures</p></a></li>
<li><a href='#build_aggtree'><p>Aggregation Trees</p></a></li>
<li><a href='#causal_ols_rpart'><p>Estimation and Inference about the GATEs with rpart Objects</p></a></li>
<li><a href='#descriptive_arm'><p>Descriptive Statistics by Treatment Arm</p></a></li>
<li><a href='#dr_scores'><p>Doubly-Robust Scores</p></a></li>
<li><a href='#estimate_rpart'><p>GATE Estimation with rpart Objects</p></a></li>
<li><a href='#expand_df'><p>Covariate Matrix Expansion</p></a></li>
<li><a href='#get_leaves'><p>Number of Leaves</p></a></li>
<li><a href='#leaf_membership'><p>Leaf Membership</p></a></li>
<li><a href='#log_ratio_sd'><p>Log Ratio of Standard Deviations</p></a></li>
<li><a href='#node_membership'><p>Node Membership</p></a></li>
<li><a href='#normalized_diff'><p>Normalized Differences</p></a></li>
<li><a href='#plot.aggTrees'><p>Plot Method for aggTrees Objects</p></a></li>
<li><a href='#print.aggTrees'><p>Print Method for aggTrees Objects</p></a></li>
<li><a href='#print.aggTrees.inference'><p>Print Method for aggTrees.inference Objects</p></a></li>
<li><a href='#rename_latex'><p>Renaming Variables for LATEX Usage</p></a></li>
<li><a href='#sample_split'><p>Sample Splitting</p></a></li>
<li><a href='#subtree'><p>Subtree</p></a></li>
<li><a href='#summary.aggTrees'><p>Summary Method for aggTrees Objects</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Aggregation Trees</td>
</tr>
<tr>
<td>Version:</td>
<td>2.0.2</td>
</tr>
<tr>
<td>Description:</td>
<td>Nonparametric data-driven approach to discovering heterogeneous subgroups in a selection-on-observables framework. 
    aggTrees allows researchers to assess whether there exists relevant heterogeneity in treatment effects by generating a sequence of optimal groupings, 
    one for each level of granularity. For each grouping, we obtain point estimation and inference about the Group Average Treatment Effects. 
    Please reference the use as Di Francesco (2022) &lt;<a href="https://doi.org/10.2139%2Fssrn.4304256">doi:10.2139/ssrn.4304256</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.10)</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>Imports:</td>
<td>boot, broom, car, caret, estimatr, grf, rpart, rpart.plot,
stats, stringr</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://riccardo-df.github.io/aggTrees/">https://riccardo-df.github.io/aggTrees/</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-09-20 06:54:41 UTC; difra</td>
</tr>
<tr>
<td>Author:</td>
<td>Riccardo Di Francesco [aut, cre, cph]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Riccardo Di Francesco &lt;difrancesco.riccardo96@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-09-20 07:20:09 UTC</td>
</tr>
</table>
<hr>
<h2 id='avg_characteristics_rpart'>Leaves Average Characteristics</h2><span id='topic+avg_characteristics_rpart'></span>

<h3>Description</h3>

<p>Computes the average characteristics of units in each leaf of an <code><a href="rpart.html#topic+rpart">rpart</a></code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>avg_characteristics_rpart(tree, X)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="avg_characteristics_rpart_+3A_tree">tree</code></td>
<td>
<p>An <code>rpart</code> object.</p>
</td></tr>
<tr><td><code id="avg_characteristics_rpart_+3A_x">X</code></td>
<td>
<p>Covariate matrix (no intercept).</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code><a href="#topic+avg_characteristics_rpart">avg_characteristics_rpart</a></code> regresses each covariate on a set of dummies denoting leaf membership.
This way, we get the average characteristics of units in each leaf, together with a standard error.<br />
</p>
<p>Leaves are ordered in increasing order of their predictions (from most negative to most positive).<br />
</p>
<p>Standard errors are estimated via the Eicker-Huber-White estimator.
</p>


<h3>Value</h3>

<p>A list storing each regression as an <code><a href="estimatr.html#topic+lm_robust">lm_robust</a></code> object.
</p>


<h3>Author(s)</h3>

<p>Riccardo Di Francesco
</p>


<h3>References</h3>


<ul>
<li><p> R Di Francesco (2022). Aggregation Trees. CEIS Research Paper, 546. <a href="https://doi.org/10.2139/ssrn.4304256">doi:10.2139/ssrn.4304256</a>.
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+causal_ols_rpart">causal_ols_rpart</a></code>, <code><a href="#topic+estimate_rpart">estimate_rpart</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Generate data.
set.seed(1986)

n &lt;- 1000
k &lt;- 3

X &lt;- matrix(rnorm(n * k), ncol = k)
colnames(X) &lt;- paste0("x", seq_len(k))
D &lt;- rbinom(n, size = 1, prob = 0.5)
mu0 &lt;- 0.5 * X[, 1]
mu1 &lt;- 0.5 * X[, 1] + X[, 2]
y &lt;- mu0 + D * (mu1 - mu0) + rnorm(n)

## Construct a tree.
library(rpart)
tree &lt;- rpart(y ~ ., data = data.frame("y" = y, X), maxdepth = 2)

## Compute average characteristics in each leaf.
results &lt;- avg_characteristics_rpart(tree, X)
results

</code></pre>

<hr>
<h2 id='balance_measures'>Balance Measures</h2><span id='topic+balance_measures'></span>

<h3>Description</h3>

<p>Compute several balance measures to check whether the covariate distributions are balanced across
treatment arms.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>balance_measures(X, D)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="balance_measures_+3A_x">X</code></td>
<td>
<p>Covariate matrix (no intercept).</p>
</td></tr>
<tr><td><code id="balance_measures_+3A_d">D</code></td>
<td>
<p>Treatment assignment vector.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For each covariate in <code>X</code>, <code>balance_measures</code> computes sample averages and standard deviations
for both treatment arms. Additionally, two balance measures are computed:
</p>

<dl>
<dt><code>Norm. Diff.</code></dt><dd><p>Normalized differences, computed as the differences in the means of each covariate
across treatment arms, normalized by the sum of the within-arm variances. They provide a measure of the
discrepancy between locations of the covariate distributions across treatment arms.</p>
</dd>
<dt><code>Log S.D.</code></dt><dd><p>Log ratio of standard deviations are computed as the logarithm of the ratio of the
within-arm standard deviations. They provide a measure of the
discrepancy in the dispersion of the covariate distributions across treatment arms.</p>
</dd>
</dl>

<p>Compilation of the LATEX code requires the following packages: <code>booktabs</code>, <code>float</code>, <code>adjustbox</code>.
</p>


<h3>Value</h3>

<p>Prints LATEX code in the console.
</p>


<h3>Author(s)</h3>

<p>Elena Dal Torrione, Riccardo Di Francesco
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Generate data.
set.seed(1986)

n &lt;- 1000
k &lt;- 3

X &lt;- matrix(rnorm(n * k), ncol = k)
colnames(X) &lt;- paste0("x", seq_len(k))
D &lt;- rbinom(n, size = 1, prob = 0.5)
mu0 &lt;- 0.5 * X[, 1]
mu1 &lt;- 0.5 * X[, 1] + X[, 2]
y &lt;- mu0 + D * (mu1 - mu0) + rnorm(n)

## Print table.
balance_measures(X, D)

</code></pre>

<hr>
<h2 id='build_aggtree'>Aggregation Trees</h2><span id='topic+build_aggtree'></span><span id='topic+inference_aggtree'></span>

<h3>Description</h3>

<p>Nonparametric data-driven approach to discovering heterogeneous subgroups in a selection-on-observables framework.
The approach constructs a sequence of groupings, one for each level of granularity. Groupings are nested and
feature an optimality property. For each grouping, we obtain point estimation and standard errors for the group average
treatment effects (GATEs). Additionally, we assess whether systematic heterogeneity is found by testing the hypotheses
that the differences in the GATEs across all pairs of groups are zero. Finally, we investigate the driving mechanisms of
effect heterogeneity by computing the average characteristics of units in each group.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>build_aggtree(
  y,
  D,
  X,
  honest_frac = 0.5,
  method = "aipw",
  scores = NULL,
  cates = NULL,
  is_honest = NULL,
  ...
)

inference_aggtree(object, n_groups, boot_ci = FALSE, boot_R = 2000)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="build_aggtree_+3A_y">y</code></td>
<td>
<p>Outcome vector.</p>
</td></tr>
<tr><td><code id="build_aggtree_+3A_d">D</code></td>
<td>
<p>Treatment vector.</p>
</td></tr>
<tr><td><code id="build_aggtree_+3A_x">X</code></td>
<td>
<p>Covariate matrix (no intercept).</p>
</td></tr>
<tr><td><code id="build_aggtree_+3A_honest_frac">honest_frac</code></td>
<td>
<p>Fraction of observations to be allocated to honest sample.</p>
</td></tr>
<tr><td><code id="build_aggtree_+3A_method">method</code></td>
<td>
<p>Either <code>"raw"</code> or <code>"aipw"</code>, controls how node predictions are computed.</p>
</td></tr>
<tr><td><code id="build_aggtree_+3A_scores">scores</code></td>
<td>
<p>Optional, vector of scores to be used in computing node predictions. Useful to save computational time if scores have already been estimated. Ignored if <code>method == "raw"</code>.</p>
</td></tr>
<tr><td><code id="build_aggtree_+3A_cates">cates</code></td>
<td>
<p>Optional, estimated CATEs. If not provided by the user, CATEs are estimated internally via a <code><a href="grf.html#topic+causal_forest">causal_forest</a></code>.</p>
</td></tr>
<tr><td><code id="build_aggtree_+3A_is_honest">is_honest</code></td>
<td>
<p>Logical vector denoting which observations belong to the honest sample. Required only if the <code>cates</code> argument is used.</p>
</td></tr>
<tr><td><code id="build_aggtree_+3A_...">...</code></td>
<td>
<p>Further arguments from <code><a href="rpart.html#topic+rpart.control">rpart.control</a></code>.</p>
</td></tr>
<tr><td><code id="build_aggtree_+3A_object">object</code></td>
<td>
<p>An <code>aggTrees</code> object.</p>
</td></tr>
<tr><td><code id="build_aggtree_+3A_n_groups">n_groups</code></td>
<td>
<p>Number of desired groups.</p>
</td></tr>
<tr><td><code id="build_aggtree_+3A_boot_ci">boot_ci</code></td>
<td>
<p>Logical, whether to compute bootstrap confidence intervals.</p>
</td></tr>
<tr><td><code id="build_aggtree_+3A_boot_r">boot_R</code></td>
<td>
<p>Number of bootstrap replications. Ignored if <code>boot_ci == FALSE</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Aggregation trees are a three-step procedure. First, the conditional average treatment effects (CATEs) are estimated using any
estimator. Second, a tree is grown to approximate the CATEs. Third, the tree is pruned to derive a nested sequence of optimal
groupings, one for each granularity level. For each level of granularity, we can obtain point estimation and inference about
the GATEs.<br />
</p>
<p>To implement this methodology, the user can rely on two core functions that handle the various steps.<br />
</p>


<h4>Constructing the Sequence of Groupings</h4>

<p><code><a href="#topic+build_aggtree">build_aggtree</a></code> constructs the sequence of groupings (i.e., the tree) and estimate the GATEs in each node. The
GATEs can be estimated in several ways. This is controlled by the <code>method</code> argument. If <code>method == "raw"</code>, we
compute the difference in mean outcomes between treated and control observations in each node. This is an unbiased estimator
in randomized experiment. If <code>method == "aipw"</code>, we construct doubly-robust scores and average them in each node. This
is unbiased also in observational studies. Honest regression forests and 5-fold cross fitting are used to estimate the
propensity score and the conditional mean function of the outcome (unless the user specifies the argument <code>scores</code>).<br />
</p>
<p>The user can provide a vector of the estimated CATEs via the <code>cates</code> argument. If so, the user needs to specify a logical
vector to denote which observations belong to the honest sample. If honesty is not desired, <code>is_honest</code> must be a
vector of <code>FALSE</code>s. If no vector of CATEs is provided, these are estimated internally via a
<code><a href="grf.html#topic+causal_forest">causal_forest</a></code>.<br />
</p>



<h4>GATEs Estimation and Inference</h4>

<p><code><a href="#topic+inference_aggtree">inference_aggtree</a></code> takes as input an <code>aggTrees</code> object constructed by <code><a href="#topic+build_aggtree">build_aggtree</a></code>. Then, for
the desired granularity level, chosen via the <code>n_groups</code> argument, it provides point estimation and standard errors for
the GATEs. Additionally, it performs some hypothesis testing to assess whether we find systematic heterogeneity and computes
the average characteristics of the units in each group to investigate the driving mechanisms.
</p>


<h5>Point estimates and standard errors for the GATEs</h5>

<p>GATEs and their standard errors are obtained by fitting an appropriate linear model. If <code>method == "raw"</code>, we estimate
via OLS the following:
</p>
<p style="text-align: center;"><code class="reqn">Y_i = \sum_{l = 1}^{|T|} L_{i, l} \gamma_l + \sum_{l = 1}^{|T|} L_{i, l} D_i \beta_l + \epsilon_i</code>
</p>

<p>with <code>L_{i, l}</code> a dummy variable equal to one if the i-th unit falls in the l-th group, and |T| the
number of groups. If the treatment is randomly assigned, one can show that the betas identify the GATE of
each group. However, this is not true in observational studies due to selection into treatment. In this case, the user is
expected to use <code>method == "aipw"</code> when calling <code><a href="#topic+build_aggtree">build_aggtree</a></code>. In this case,
<code><a href="#topic+inference_aggtree">inference_aggtree</a></code> uses the scores in the following regression:
</p>
<p style="text-align: center;"><code class="reqn">score_i = \sum_{l = 1}^{|T|} L_{i, l} \beta_l + \epsilon_i</code>
</p>

<p>This way, betas again identify the GATEs.<br />
</p>
<p>Regardless of <code>method</code>, standard errors are estimated via the Eicker-Huber-White estimator.<br />
</p>
<p>If <code>boot_ci == TRUE</code>, the routine also computes asymmetric bias-corrected and accelerated 95% confidence intervals using 2000 bootstrap
samples. Particularly useful when the honest sample is small-ish.
</p>



<h5>Hypothesis testing</h5>

<p><code><a href="#topic+inference_aggtree">inference_aggtree</a></code> uses the standard errors obtained by fitting the linear models above to test the hypotheses
that the GATEs are different across all pairs of leaves. Here, we adjust p-values to account for multiple hypotheses testing
using Holm's procedure.
</p>



<h5>Average Characteristics</h5>

<p><code><a href="#topic+inference_aggtree">inference_aggtree</a></code> regresses each covariate on a set of dummies denoting group membership. This way, we get the
average characteristics of units in each leaf, together with a standard error. Leaves are ordered in increasing order of their
predictions (from most negative to most positive). Standard errors are estimated via the Eicker-Huber-White estimator.
</p>




<h4>Caution on Inference</h4>

<p>Regardless of the chosen <code>method</code>, both functions estimate the GATEs, the linear models, and the average characteristics
of units in each group using only observations in the honest sample. If the honest sample is empty (this happens because the
user either sets <code>honest_frac = 0</code> or passes a vector of <code>FALSE</code>s as <code>is_honest</code> when calling
<code><a href="#topic+build_aggtree">build_aggtree</a></code>), the same data used to construct the tree are used to estimate the above quantities. This is
fine for prediction but invalidates inference.
</p>



<h3>Value</h3>

<p><code><a href="#topic+build_aggtree">build_aggtree</a></code> returns an <code>aggTrees</code> object.<br />
</p>
<p><code><a href="#topic+inference_aggtree">inference_aggtree</a></code> returns an <code>aggTrees.inference</code> object, which in turn contains the <code>aggTrees</code> object used
in the call.
</p>


<h3>Author(s)</h3>

<p>Riccardo Di Francesco
</p>


<h3>References</h3>


<ul>
<li><p> R Di Francesco (2022). Aggregation Trees. CEIS Research Paper, 546. <a href="https://doi.org/10.2139/ssrn.4304256">doi:10.2139/ssrn.4304256</a>.
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+plot.aggTrees">plot.aggTrees</a></code> <code><a href="#topic+print.aggTrees.inference">print.aggTrees.inference</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Generate data.
set.seed(1986)

n &lt;- 1000
k &lt;- 3

X &lt;- matrix(rnorm(n * k), ncol = k)
colnames(X) &lt;- paste0("x", seq_len(k))
D &lt;- rbinom(n, size = 1, prob = 0.5)
mu0 &lt;- 0.5 * X[, 1]
mu1 &lt;- 0.5 * X[, 1] + X[, 2]
y &lt;- mu0 + D * (mu1 - mu0) + rnorm(n)

## Construct sequence of groupings. CATEs estimated internally.
groupings &lt;- build_aggtree(y, D, X, method = "aipw")

## Alternatively, we can estimate the CATEs and pass them.
splits &lt;- sample_split(length(y), training_frac = 0.5)
training_idx &lt;- splits$training_idx
honest_idx &lt;- splits$honest_idx

y_tr &lt;- y[training_idx]
D_tr &lt;- D[training_idx]
X_tr &lt;- X[training_idx, ]

y_hon &lt;- y[honest_idx]
D_hon &lt;- D[honest_idx]
X_hon &lt;- X[honest_idx, ]

library(grf)
forest &lt;- causal_forest(X_tr, y_tr, D_tr) # Use training sample.
cates &lt;- predict(forest, X)$predictions

groupings &lt;- build_aggtree(y, D, X, method = "aipw", cates = cates,
                           is_honest = 1:length(y) %in% honest_idx)

## We have compatibility with generic S3-methods.
summary(groupings)
print(groupings)
plot(groupings) # Try also setting 'sequence = TRUE'.

## To predict, do the following.
tree &lt;- subtree(groupings$tree, cv = TRUE) # Select by cross-validation.
head(predict(tree, data.frame(X)))

## Inference with 4 groups.
results &lt;- inference_aggtree(groupings, n_groups = 4)

summary(results$model) # Coefficient of leafk is GATE in k-th leaf.

results$gates_diff_pairs$gates_diff # GATEs differences.
results$gates_diff_pairs$holm_pvalues # leaves 1-2 not statistically different.

## LATEX.
print(results, table = "diff")
print(results, table = "avg_char")

</code></pre>

<hr>
<h2 id='causal_ols_rpart'>Estimation and Inference about the GATEs with rpart Objects</h2><span id='topic+causal_ols_rpart'></span>

<h3>Description</h3>

<p>Obtains point estimates and standard errors for the group average treatment effects (GATEs), where groups correspond to the
leaves of an <code><a href="rpart.html#topic+rpart">rpart</a></code> object. Additionally, performs some hypothesis testing.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>causal_ols_rpart(
  tree,
  y,
  D,
  X,
  method = "aipw",
  scores = NULL,
  boot_ci = FALSE,
  boot_R = 2000
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="causal_ols_rpart_+3A_tree">tree</code></td>
<td>
<p>An <code><a href="rpart.html#topic+rpart">rpart</a></code> object.</p>
</td></tr>
<tr><td><code id="causal_ols_rpart_+3A_y">y</code></td>
<td>
<p>Outcome vector.</p>
</td></tr>
<tr><td><code id="causal_ols_rpart_+3A_d">D</code></td>
<td>
<p>Treatment assignment vector</p>
</td></tr>
<tr><td><code id="causal_ols_rpart_+3A_x">X</code></td>
<td>
<p>Covariate matrix (no intercept).</p>
</td></tr>
<tr><td><code id="causal_ols_rpart_+3A_method">method</code></td>
<td>
<p>Either <code>"raw"</code> or <code>"aipw"</code>, defines the outcome used in the regression.</p>
</td></tr>
<tr><td><code id="causal_ols_rpart_+3A_scores">scores</code></td>
<td>
<p>Optional, vector of scores to be used in the regression. Useful to save computational time if scores have already been estimated. Ignored if <code>method == "raw"</code>.</p>
</td></tr>
<tr><td><code id="causal_ols_rpart_+3A_boot_ci">boot_ci</code></td>
<td>
<p>Logical, whether to compute bootstrap confidence intervals.</p>
</td></tr>
<tr><td><code id="causal_ols_rpart_+3A_boot_r">boot_R</code></td>
<td>
<p>Number of bootstrap replications. Ignored if <code>boot_ci == FALSE</code>.</p>
</td></tr>
</table>


<h3>Details</h3>



<h4>Point estimates and standard errors for the GATEs</h4>

<p>The GATEs and their standard errors are obtained by fitting an appropriate linear model. If <code>method == "raw"</code>, we
estimate via OLS the following:
</p>
<p style="text-align: center;"><code class="reqn">Y_i = \sum_{l = 1}^{|T|} L_{i, l} \gamma_l + \sum_{l = 1}^{|T|} L_{i, l} D_i \beta_l + \epsilon_i</code>
</p>

<p>with <code>L_{i, l}</code> a dummy variable equal to one if the i-th unit falls in the l-th leaf of <code>tree</code>, and |T| the number of
groups. If the treatment is randomly assigned, one can show that the betas identify the GATE in each leaf. However, this is not true
in observational studies due to selection into treatment. In this case, the user is expected to use <code>method == "aipw"</code> to run
the following regression:
</p>
<p style="text-align: center;"><code class="reqn">score_i = \sum_{l = 1}^{|T|} L_{i, l} \beta_l + \epsilon_i</code>
</p>

<p>where score_i are doubly-robust scores constructed via honest regression forests and 5-fold cross fitting (unless the user specifies
the argument <code>scores</code>). This way, betas again identify the GATEs.<br />
</p>
<p>Regardless of <code>method</code>, standard errors are estimated via the Eicker-Huber-White estimator.<br />
</p>
<p>If <code>boot_ci == TRUE</code>, the routine also computes asymmetric bias-corrected and accelerated 95% confidence intervals using 2000 bootstrap
samples.<br />
</p>
<p>If <code>tree</code> consists of a root only, <code>causal_ols_rpart</code> regresses <code>y</code> on a constant and <code>D</code> if
<code>method == "raw"</code>, or regresses the doubly-robust scores on a constant if <code>method == "aipw"</code>. This way,
we get an estimate of the overall average treatment effect.
</p>



<h4>Hypothesis testing</h4>

<p><code><a href="#topic+causal_ols_rpart">causal_ols_rpart</a></code> uses the standard errors obtained by fitting the linear models above to test the hypotheses
that the GATEs are different across all pairs of leaves. Here, we adjust p-values to account for multiple hypotheses testing
using Holm's procedure.
</p>



<h4>Caution on Inference</h4>

<p>&quot;honesty&quot; is a necessary requirement to get valid inference. Thus, observations in <code>y</code>, <code>D</code>, and
<code>X</code> must not have been used to construct the <code>tree</code> and the <code>scores</code>.<br />
</p>



<h3>Value</h3>

<p>A list storing:
</p>
<table>
<tr><td><code>model</code></td>
<td>
<p>The model fitted to get point estimates and standard errors for the GATEs, as an <code><a href="estimatr.html#topic+lm_robust">lm_robust</a></code> object.</p>
</td></tr>
<tr><td><code>gates_diff_pairs</code></td>
<td>
<p>Results of testing whether GATEs differ across all pairs of leaves. This is a list storing GATEs differences and p-values adjusted using Holm's procedure (check <code><a href="stats.html#topic+p.adjust">p.adjust</a></code>). <code>NULL</code> if the tree consists of a root only.</p>
</td></tr>
<tr><td><code>boot_ci</code></td>
<td>
<p>Bootstrap confidence intervals (this is an empty list if <code>boot_ci == FALSE</code>.</p>
</td></tr>
<tr><td><code>scores</code></td>
<td>
<p>Vector of doubly robust scores. <code>NULL</code> if <code>method == 'raw'</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Riccardo Di Francesco
</p>


<h3>References</h3>


<ul>
<li><p> R Di Francesco (2022). Aggregation Trees. CEIS Research Paper, 546. <a href="https://doi.org/10.2139/ssrn.4304256">doi:10.2139/ssrn.4304256</a>.
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+estimate_rpart">estimate_rpart</a></code> <code><a href="#topic+avg_characteristics_rpart">avg_characteristics_rpart</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Generate data.
set.seed(1986)

n &lt;- 1000
k &lt;- 3

X &lt;- matrix(rnorm(n * k), ncol = k)
colnames(X) &lt;- paste0("x", seq_len(k))
D &lt;- rbinom(n, size = 1, prob = 0.5)
mu0 &lt;- 0.5 * X[, 1]
mu1 &lt;- 0.5 * X[, 1] + X[, 2]
y &lt;- mu0 + D * (mu1 - mu0) + rnorm(n)

## Split the sample.
splits &lt;- sample_split(length(y), training_frac = 0.5)
training_idx &lt;- splits$training_idx
honest_idx &lt;- splits$honest_idx

y_tr &lt;- y[training_idx]
D_tr &lt;- D[training_idx]
X_tr &lt;- X[training_idx, ]

y_hon &lt;- y[honest_idx]
D_hon &lt;- D[honest_idx]
X_hon &lt;- X[honest_idx, ]

## Construct a tree using training sample.
library(rpart)
tree &lt;- rpart(y ~ ., data = data.frame("y" = y_tr, X_tr), maxdepth = 2)

## Estimate GATEs in each node (internal and terminal) using honest sample.
results &lt;- causal_ols_rpart(tree, y_hon, D_hon, X_hon, method = "raw")

summary(results$model) # Coefficient of leafk:D is GATE in k-th leaf.

results$gates_diff_pair$gates_diff # GATEs differences.
results$gates_diff_pair$holm_pvalues # leaves 1-2 and 3-4 not statistically different.

</code></pre>

<hr>
<h2 id='descriptive_arm'>Descriptive Statistics by Treatment Arm</h2><span id='topic+descriptive_arm'></span>

<h3>Description</h3>

<p>Computes sample averages and standard deviations of the covariates across treatment arms.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>descriptive_arm(X, D)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="descriptive_arm_+3A_x">X</code></td>
<td>
<p>Covariate matrix (no intercept).</p>
</td></tr>
<tr><td><code id="descriptive_arm_+3A_d">D</code></td>
<td>
<p>Treatment assignment vector.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Sample means and standard deviations across treatment arms are a first, useful insight to assess covariate balance.
</p>


<h3>Value</h3>

<p>4xp array, storing the desired statistics.
</p>

<hr>
<h2 id='dr_scores'>Doubly-Robust Scores</h2><span id='topic+dr_scores'></span>

<h3>Description</h3>

<p>Constructs doubly-robust scores via K-fold cross-fitting.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dr_scores(y, D, X, k = 5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dr_scores_+3A_y">y</code></td>
<td>
<p>Outcome vector.</p>
</td></tr>
<tr><td><code id="dr_scores_+3A_d">D</code></td>
<td>
<p>Treatment assignment vector.</p>
</td></tr>
<tr><td><code id="dr_scores_+3A_x">X</code></td>
<td>
<p>Covariate matrix (no intercept).</p>
</td></tr>
<tr><td><code id="dr_scores_+3A_k">k</code></td>
<td>
<p>Number of folds.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Honest regression forests are used to estimate the propensity score and the conditional mean function of the outcome.
</p>


<h3>Value</h3>

<p>A vector of scores.
</p>


<h3>Author(s)</h3>

<p>Riccardo Di Francesco
</p>

<hr>
<h2 id='estimate_rpart'>GATE Estimation with rpart Objects</h2><span id='topic+estimate_rpart'></span>

<h3>Description</h3>

<p>Replaces node predictions of an <code><a href="rpart.html#topic+rpart">rpart</a></code> object using external data to estimate the group average treatment
effects (GATEs).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>estimate_rpart(tree, y, D, X, method = "aipw", scores = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="estimate_rpart_+3A_tree">tree</code></td>
<td>
<p>An <code><a href="rpart.html#topic+rpart">rpart</a></code> object.</p>
</td></tr>
<tr><td><code id="estimate_rpart_+3A_y">y</code></td>
<td>
<p>Outcome vector.</p>
</td></tr>
<tr><td><code id="estimate_rpart_+3A_d">D</code></td>
<td>
<p>Treatment assignment vector.</p>
</td></tr>
<tr><td><code id="estimate_rpart_+3A_x">X</code></td>
<td>
<p>Covariate matrix (no intercept).</p>
</td></tr>
<tr><td><code id="estimate_rpart_+3A_method">method</code></td>
<td>
<p>Either <code>"raw"</code> or <code>"aipw"</code>, controls how node predictions are replaced.</p>
</td></tr>
<tr><td><code id="estimate_rpart_+3A_scores">scores</code></td>
<td>
<p>Optional, vector of scores to be used in replacing node predictions. Useful to save computational time if scores have already been estimated. Ignored if <code>method == "raw"</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>method == "raw"</code>, <code>estimate_rpart</code> replaces node predictions with the differences between the sample average
of the observed outcomes of treated units and the sample average of the observed outcomes of control units in each node,
which is an unbiased estimator of the GATEs if the assignment to treatment is randomized.<br />
</p>
<p>If <code>method == "aipw"</code>, <code>estimate_rpart</code> replaces node predictions with sample averages of doubly-robust
scores in each node. This is a valid estimator of the GATEs in observational studies. Honest regression forests
and 5-fold cross fitting are used to estimate the propensity score and the conditional mean function of the outcome
(unless the user specifies the argument <code>scores</code>).<br />
</p>
<p><code>estimate_rpart</code> allows the user to implement &quot;honest&quot; estimation. If observations in <code>y</code>, <code>D</code> and <code>X</code>
have not been used to construct the <code>tree</code>, then the new predictions are honest in the sense of Athey and Imbens (2016).
To get standard errors for the tree's estimates, please use <code><a href="#topic+causal_ols_rpart">causal_ols_rpart</a></code>.
</p>


<h3>Value</h3>

<p>A tree with node predictions replaced, as an <code><a href="rpart.html#topic+rpart">rpart</a></code> object, and the scores (if <code>method == "raw"</code>,
this is <code>NULL</code>).
</p>


<h3>Author(s)</h3>

<p>Riccardo Di Francesco
</p>


<h3>References</h3>


<ul>
<li><p> R Di Francesco (2022). Aggregation Trees. CEIS Research Paper, 546. <a href="https://doi.org/10.2139/ssrn.4304256">doi:10.2139/ssrn.4304256</a>.
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+causal_ols_rpart">causal_ols_rpart</a></code> <code><a href="#topic+avg_characteristics_rpart">avg_characteristics_rpart</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Generate data.
set.seed(1986)

n &lt;- 1000
k &lt;- 3

X &lt;- matrix(rnorm(n * k), ncol = k)
colnames(X) &lt;- paste0("x", seq_len(k))
D &lt;- rbinom(n, size = 1, prob = 0.5)
mu0 &lt;- 0.5 * X[, 1]
mu1 &lt;- 0.5 * X[, 1] + X[, 2]
y &lt;- mu0 + D * (mu1 - mu0) + rnorm(n)

## Split the sample.
splits &lt;- sample_split(length(y), training_frac = 0.5)
training_idx &lt;- splits$training_idx
honest_idx &lt;- splits$honest_idx

y_tr &lt;- y[training_idx]
D_tr &lt;- D[training_idx]
X_tr &lt;- X[training_idx, ]

y_hon &lt;- y[honest_idx]
D_hon &lt;- D[honest_idx]
X_hon &lt;- X[honest_idx, ]

## Construct a tree using training sample.
library(rpart)
tree &lt;- rpart(y ~ ., data = data.frame("y" = y_tr, X_tr), maxdepth = 2)

## Estimate GATEs in each node (internal and terminal) using honest sample.
new_tree &lt;- estimate_rpart(tree, y_hon, D_hon, X_hon, method = "raw")
new_tree$tree

</code></pre>

<hr>
<h2 id='expand_df'>Covariate Matrix Expansion</h2><span id='topic+expand_df'></span>

<h3>Description</h3>

<p>Expands the covariate matrix, adding interactions and polynomials. This is particularly useful for penalized regressions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>expand_df(X, int_order = 2, poly_order = 4, threshold = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="expand_df_+3A_x">X</code></td>
<td>
<p>Covariate matrix (no intercept).</p>
</td></tr>
<tr><td><code id="expand_df_+3A_int_order">int_order</code></td>
<td>
<p>Order of interactions to be added. Set equal to one if no interactions are desired.</p>
</td></tr>
<tr><td><code id="expand_df_+3A_poly_order">poly_order</code></td>
<td>
<p>Order of the polynomials to be added. Set equal to one if no polynomials are desired.</p>
</td></tr>
<tr><td><code id="expand_df_+3A_threshold">threshold</code></td>
<td>
<p>Drop binary variables representing less than <code>threshold</code>% of the population. Useful to speed up computation.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>expand_df</code> assumes that categorical variables are coded as <code>factors</code>. Also, no missing values are allowed.<br />
</p>
<p><code>expand_df</code> uses <code><a href="stats.html#topic+model.matrix">model.matrix</a></code> to expand factors to a set of dummy variables. Then, it identifies continuous covariates as those
not having 0 and 1 as unique values.<br />
</p>
<p><code>expand_df</code> first introduces all the <code>int_order</code>-way interactions between the variables (using the expanded set of dummies), and then adds
<code>poly_order</code>-order polynomials for continuous covariates.
</p>


<h3>Value</h3>

<p>The expanded covariate matrix, as a data frame.
</p>


<h3>Author(s)</h3>

<p>Riccardo Di Francesco
</p>

<hr>
<h2 id='get_leaves'>Number of Leaves</h2><span id='topic+get_leaves'></span>

<h3>Description</h3>

<p>Extracts the number of leaves of an <code><a href="rpart.html#topic+rpart">rpart</a></code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_leaves(tree)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_leaves_+3A_tree">tree</code></td>
<td>
<p>An <code><a href="rpart.html#topic+rpart">rpart</a></code> object.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The number of leaves.
</p>


<h3>Author(s)</h3>

<p>Riccardo Di Francesco
</p>


<h3>See Also</h3>

<p><code><a href="#topic+subtree">subtree</a></code> <code><a href="#topic+node_membership">node_membership</a></code> <code><a href="#topic+leaf_membership">leaf_membership</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Generate data.
set.seed(1986)

n &lt;- 3000
k &lt;- 3

X &lt;- matrix(rnorm(n * k), ncol = k)
colnames(X) &lt;- paste0("x", seq_len(k))

y &lt;- exp(X[, 1]) + 2 * X[, 2] * X[, 2] &gt; 0 + rnorm(n)

## Construct tree.
library(rpart)
tree &lt;- rpart(y ~ ., data = data.frame(y, X))

## Extract number of leaves.
n_leaves &lt;- get_leaves(tree)
n_leaves

</code></pre>

<hr>
<h2 id='leaf_membership'>Leaf Membership</h2><span id='topic+leaf_membership'></span>

<h3>Description</h3>

<p>Constructs a variable that encodes in which leaf of an <code><a href="rpart.html#topic+rpart">rpart</a></code> object the units in a given data frame fall.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>leaf_membership(tree, X)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="leaf_membership_+3A_tree">tree</code></td>
<td>
<p>An <code><a href="rpart.html#topic+rpart">rpart</a></code> object.</p>
</td></tr>
<tr><td><code id="leaf_membership_+3A_x">X</code></td>
<td>
<p>Covariate matrix (no intercept).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A factor whose levels denote in which leaf each unit falls. Leaves are ordered in increasing order of their predictions
(from most negative to most positive).
</p>


<h3>Author(s)</h3>

<p>Riccardo Di Francesco
</p>


<h3>See Also</h3>

<p><code><a href="#topic+subtree">subtree</a></code> <code><a href="#topic+node_membership">node_membership</a></code> <code><a href="#topic+get_leaves">get_leaves</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Generate data.
set.seed(1986)

n &lt;- 3000
k &lt;- 3

X &lt;- matrix(rnorm(n * k), ncol = k)
colnames(X) &lt;- paste0("x", seq_len(k))

y &lt;- exp(X[, 1]) + 2 * X[, 2] * X[, 2] &gt; 0 + rnorm(n)

## Construct tree.
library(rpart)
tree &lt;- rpart(y ~ ., data = data.frame(y, X))

## Extract number of leaves.
leaves_factor &lt;- leaf_membership(tree, X)
head(leaves_factor)

</code></pre>

<hr>
<h2 id='log_ratio_sd'>Log Ratio of Standard Deviations</h2><span id='topic+log_ratio_sd'></span>

<h3>Description</h3>

<p>Computes a measure of the difference in the dispersion of the
covariate distributions across treatment arms.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>log_ratio_sd(X, D)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="log_ratio_sd_+3A_x">X</code></td>
<td>
<p>Covariate matrix (no intercept).</p>
</td></tr>
<tr><td><code id="log_ratio_sd_+3A_d">D</code></td>
<td>
<p>Treatment assignment vector.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Log ratio of standard deviations are computed as the logarithm of the ratio of the within-arm standard deviations.
</p>


<h3>Value</h3>

<p>1xp data frame storing logarithm of the ratio of standard deviations of each covariate.
</p>

<hr>
<h2 id='node_membership'>Node Membership</h2><span id='topic+node_membership'></span>

<h3>Description</h3>

<p>Constructs a binary variable that encodes whether each observation falls into a particular node of an
<code><a href="rpart.html#topic+rpart">rpart</a></code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>node_membership(tree, X, node)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="node_membership_+3A_tree">tree</code></td>
<td>
<p>An <code><a href="rpart.html#topic+rpart">rpart</a></code> object.</p>
</td></tr>
<tr><td><code id="node_membership_+3A_x">X</code></td>
<td>
<p>Covariate matrix (no intercept).</p>
</td></tr>
<tr><td><code id="node_membership_+3A_node">node</code></td>
<td>
<p>Number of node.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Logical vector denoting whether each observation in <code>X</code> falls into <code>node</code>.
</p>


<h3>Author(s)</h3>

<p>Riccardo Di Francesco
</p>


<h3>See Also</h3>

<p><code><a href="#topic+subtree">subtree</a></code> <code><a href="#topic+leaf_membership">leaf_membership</a></code> <code><a href="#topic+get_leaves">get_leaves</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Generate data.
set.seed(1986)

n &lt;- 3000
k &lt;- 3

X &lt;- matrix(rnorm(n * k), ncol = k)
colnames(X) &lt;- paste0("x", seq_len(k))

y &lt;- exp(X[, 1]) + 2 * X[, 2] * X[, 2] &gt; 0 + rnorm(n)

## Construct tree.
library(rpart)
tree &lt;- rpart(y ~ ., data = data.frame(y, X))

## Extract number of leaves.
is_in_third_node &lt;- node_membership(tree, X, 3)
head(is_in_third_node)

</code></pre>

<hr>
<h2 id='normalized_diff'>Normalized Differences</h2><span id='topic+normalized_diff'></span>

<h3>Description</h3>

<p>Computes a measure of the difference between locations of the covariate distributions
across treatment arms.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>normalized_diff(X, D)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="normalized_diff_+3A_x">X</code></td>
<td>
<p>Covariate matrix (no intercept).</p>
</td></tr>
<tr><td><code id="normalized_diff_+3A_d">D</code></td>
<td>
<p>Treatment assignment vector.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Normalized differences are computed as the difference in the means of each covariate across treatment arms, normalized
by the sum of the within-arm variances.
</p>


<h3>Value</h3>

<p>1xp data frame storing the normalized difference of each covariate.
</p>

<hr>
<h2 id='plot.aggTrees'>Plot Method for aggTrees Objects</h2><span id='topic+plot.aggTrees'></span>

<h3>Description</h3>

<p>Plots an <code>aggTrees</code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'aggTrees'
plot(x, leaves = get_leaves(x$tree), sequence = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.aggTrees_+3A_x">x</code></td>
<td>
<p>An <code>aggTrees</code> object.</p>
</td></tr>
<tr><td><code id="plot.aggTrees_+3A_leaves">leaves</code></td>
<td>
<p>Number of leaves of the desired tree. This can be used to plot subtrees.</p>
</td></tr>
<tr><td><code id="plot.aggTrees_+3A_sequence">sequence</code></td>
<td>
<p>If <code>TRUE</code>, the whole sequence of optimal groupings is displayed in a short animation.</p>
</td></tr>
<tr><td><code id="plot.aggTrees_+3A_...">...</code></td>
<td>
<p>Further arguments from <code><a href="rpart.plot.html#topic+prp">prp</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Nodes are colored using a diverging palette. Nodes with predictions smaller than the ATE (i.e., the root
prediction) are colored in blue shades, and nodes with predictions larger than the ATE are colored in red
shades. Moreover, predictions that are more distant in absolute value from the ATE get darker shades.
This way, we have an immediate understanding of the groups with extreme GATEs.
</p>


<h3>Value</h3>

<p>Plots an <code>aggTrees</code> object.
</p>


<h3>Author(s)</h3>

<p>Riccardo Di Francesco
</p>


<h3>References</h3>


<ul>
<li><p> R Di Francesco (2022). Aggregation Trees. CEIS Research Paper, 546. <a href="https://doi.org/10.2139/ssrn.4304256">doi:10.2139/ssrn.4304256</a>.
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+build_aggtree">build_aggtree</a></code>, <code><a href="#topic+inference_aggtree">inference_aggtree</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Generate data.
set.seed(1986)

n &lt;- 1000
k &lt;- 3

X &lt;- matrix(rnorm(n * k), ncol = k)
colnames(X) &lt;- paste0("x", seq_len(k))
D &lt;- rbinom(n, size = 1, prob = 0.5)
mu0 &lt;- 0.5 * X[, 1]
mu1 &lt;- 0.5 * X[, 1] + X[, 2]
y &lt;- mu0 + D * (mu1 - mu0) + rnorm(n)

## Construct sequence of groupings. CATEs estimated internally,
groupings &lt;- build_aggtree(y, D, X, method = "aipw")

## Plot.
plot(groupings)
plot(groupings, leaves = 3)
plot(groupings, sequence = TRUE)

</code></pre>

<hr>
<h2 id='print.aggTrees'>Print Method for aggTrees Objects</h2><span id='topic+print.aggTrees'></span>

<h3>Description</h3>

<p>Prints an <code>aggTrees</code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'aggTrees'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.aggTrees_+3A_x">x</code></td>
<td>
<p><code>aggTrees</code> object.</p>
</td></tr>
<tr><td><code id="print.aggTrees_+3A_...">...</code></td>
<td>
<p>Further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Prints an <code>aggTrees</code> object.
</p>


<h3>Author(s)</h3>

<p>Riccardo Di Francesco
</p>


<h3>References</h3>


<ul>
<li><p> R Di Francesco (2022). Aggregation Trees. CEIS Research Paper, 546. <a href="https://doi.org/10.2139/ssrn.4304256">doi:10.2139/ssrn.4304256</a>.
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+build_aggtree">build_aggtree</a></code>, <code><a href="#topic+inference_aggtree">inference_aggtree</a></code>
</p>

<hr>
<h2 id='print.aggTrees.inference'>Print Method for aggTrees.inference Objects</h2><span id='topic+print.aggTrees.inference'></span>

<h3>Description</h3>

<p>Prints an <code>aggTrees.inference</code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'aggTrees.inference'
print(x, table = "avg_char", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.aggTrees.inference_+3A_x">x</code></td>
<td>
<p><code>aggTrees.inference</code> object.</p>
</td></tr>
<tr><td><code id="print.aggTrees.inference_+3A_table">table</code></td>
<td>
<p>Either <code>"avg_char"</code> or <code>"diff"</code>, controls which table must be produced.</p>
</td></tr>
<tr><td><code id="print.aggTrees.inference_+3A_...">...</code></td>
<td>
<p>Further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A description of each table is provided in its caption.<br />
</p>
<p>Some covariates may feature zero variation in some leaf. This generally happens to dummy variables used to split some
nodes. In this case, when <code>table == "avg_char"</code> a warning message is produced displaying the names of the covariates
with zero variation in one or more leaves. The user should correct the table by removing the associated standard errors.<br />
</p>
<p>Compilation of the LATEX code requires the following packages: <code>booktabs</code>, <code>float</code>, <code>adjustbox</code>,
<code>multirow</code>.
</p>


<h3>Value</h3>

<p>Prints LATEX code.
</p>


<h3>Author(s)</h3>

<p>Riccardo Di Francesco
</p>


<h3>References</h3>


<ul>
<li><p> R Di Francesco (2022). Aggregation Trees. CEIS Research Paper, 546. <a href="https://doi.org/10.2139/ssrn.4304256">doi:10.2139/ssrn.4304256</a>.
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+build_aggtree">build_aggtree</a></code>, <code><a href="#topic+inference_aggtree">inference_aggtree</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Generate data.
set.seed(1986)

n &lt;- 1000
k &lt;- 3

X &lt;- matrix(rnorm(n * k), ncol = k)
colnames(X) &lt;- paste0("x", seq_len(k))
D &lt;- rbinom(n, size = 1, prob = 0.5)
mu0 &lt;- 0.5 * X[, 1]
mu1 &lt;- 0.5 * X[, 1] + X[, 2]
y &lt;- mu0 + D * (mu1 - mu0) + rnorm(n)

## Construct sequence of groupings. CATEs estimated internally,
groupings &lt;- build_aggtree(y, D, X, method = "aipw")

## Analyze results with 4 groups.
results &lt;- inference_aggtree(groupings, n_groups = 4)

## Print results.
print(results, table = "diff")
print(results, table = "avg_char")

</code></pre>

<hr>
<h2 id='rename_latex'>Renaming Variables for LATEX Usage</h2><span id='topic+rename_latex'></span>

<h3>Description</h3>

<p>Renames variables where the character &quot;_&quot; is used, which causes clashes in LATEX.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rename_latex(names)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rename_latex_+3A_names">names</code></td>
<td>
<p>string vector.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The renamed string vector. Strings where &quot;_&quot; is not found are not modified by <code>rename_latex</code>.
</p>

<hr>
<h2 id='sample_split'>Sample Splitting</h2><span id='topic+sample_split'></span>

<h3>Description</h3>

<p>Splits the sample into training and honest subsamples.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sample_split(n, training_frac = 0.5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sample_split_+3A_n">n</code></td>
<td>
<p>Size of the sample to be split.</p>
</td></tr>
<tr><td><code id="sample_split_+3A_training_frac">training_frac</code></td>
<td>
<p>Fraction of units for the training sample.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list storing the indexes for the two different subsamples.
</p>


<h3>Author(s)</h3>

<p>Riccardo Di Francesco
</p>

<hr>
<h2 id='subtree'>Subtree</h2><span id='topic+subtree'></span>

<h3>Description</h3>

<p>Extracts a subtree with a user-specified number of leaves from an <code><a href="rpart.html#topic+rpart">rpart</a></code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>subtree(tree, leaves = NULL, cv = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="subtree_+3A_tree">tree</code></td>
<td>
<p>An <code><a href="rpart.html#topic+rpart">rpart</a></code> object.</p>
</td></tr>
<tr><td><code id="subtree_+3A_leaves">leaves</code></td>
<td>
<p>Number of leaves of the desired subtree.</p>
</td></tr>
<tr><td><code id="subtree_+3A_cv">cv</code></td>
<td>
<p>If <code>TRUE</code>, <code>leaves</code> is ignored and a cross-validation criterion is used to select a partition.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The subtree, as an <code><a href="rpart.html#topic+rpart">rpart</a></code> object.
</p>


<h3>Author(s)</h3>

<p>Riccardo Di Francesco
</p>


<h3>See Also</h3>

<p><code><a href="#topic+get_leaves">get_leaves</a></code> <code><a href="#topic+node_membership">node_membership</a></code> <code><a href="#topic+leaf_membership">leaf_membership</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Generate data.
set.seed(1986)

n &lt;- 3000
k &lt;- 3

X &lt;- matrix(rnorm(n * k), ncol = k)
colnames(X) &lt;- paste0("x", seq_len(k))

y &lt;- exp(X[, 1]) + 2 * X[, 2] * X[, 2] &gt; 0 + rnorm(n)

## Construct tree.
library(rpart)
tree &lt;- rpart(y ~ ., data = data.frame(y, X), cp = 0)

## Extract subtree.
sub_tree &lt;- subtree(tree, leaves = 4)
sub_tree_cv &lt;- subtree(tree, cv = TRUE)

</code></pre>

<hr>
<h2 id='summary.aggTrees'>Summary Method for aggTrees Objects</h2><span id='topic+summary.aggTrees'></span>

<h3>Description</h3>

<p>Summarizes an <code>aggTrees</code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'aggTrees'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.aggTrees_+3A_object">object</code></td>
<td>
<p><code>aggTrees</code> object.</p>
</td></tr>
<tr><td><code id="summary.aggTrees_+3A_...">...</code></td>
<td>
<p>Further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Prints the summary of an <code>aggTrees</code> object.
</p>


<h3>Author(s)</h3>

<p>Riccardo Di Francesco
</p>


<h3>References</h3>


<ul>
<li><p> R Di Francesco (2022). Aggregation Trees. CEIS Research Paper, 546. <a href="https://doi.org/10.2139/ssrn.4304256">doi:10.2139/ssrn.4304256</a>.
</p>
</li></ul>



<h3>See Also</h3>

<p><code><a href="#topic+build_aggtree">build_aggtree</a></code>, <code><a href="#topic+inference_aggtree">inference_aggtree</a></code>
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
