<!DOCTYPE html><html><head><title>Help for package nnTensor</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {nnTensor}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#GabrielNMF'>
<p>Gabriel-type Bi-Cross-Validation for Non-negative Matrix Factorization</p></a></li>
<li><a href='#jNMF'>
<p>Joint Non-negative Matrix Factorization Algorithms (jNMF)</p></a></li>
<li><a href='#kFoldMaskTensor'>
<p>Mask tensors generator to perform k-fold cross validation</p></a></li>
<li><a href='#NMF'>
<p>Non-negative Matrix Factorization Algorithms (NMF)</p></a></li>
<li><a href='#NMTF'>
<p>Non-negative Matrix Tri-Factorization Algorithms (NMTF)</p></a></li>
<li><a href='#nnTensor-package'>
<p>Non-Negative Tensor Decomposition</p></a></li>
<li><a href='#NTD'>
<p>Non-negative Tucker Decomposition Algorithms (NTD)</p></a></li>
<li><a href='#NTF'>
<p>Non-negative CP Decomposition Algorithms (NTF)</p></a></li>
<li><a href='#plot.NMF'>
<p>Plot function of the result of NMF function</p></a></li>
<li><a href='#plotTensor2D'>
<p>Plot function for visualization of matrix data structure</p></a></li>
<li><a href='#plotTensor3D'>
<p>Plot function for visualization of tensor data structure</p></a></li>
<li><a href='#recTensor'>
<p>Tensor Reconstruction from core tensor (S) and factor matrices (A)</p></a></li>
<li><a href='#siNMF'>
<p>Simultaneous Non-negative Matrix Factorization Algorithms (siNMF)</p></a></li>
<li><a href='#toyModel'>
<p>Toy model data for using NMF, NTF, and NTD</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Non-Negative Tensor Decomposition</td>
</tr>
<tr>
<td>Version:</td>
<td>1.2.0</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.4.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>methods, MASS, fields, rTensor, plot3D, tagcloud, ggplot2</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown, testthat, dplyr</td>
</tr>
<tr>
<td>Description:</td>
<td>Some functions for performing non-negative matrix factorization, non-negative CANDECOMP/PARAFAC (CP) decomposition, non-negative Tucker decomposition, and generating toy model data. See Andrzej Cichock et al (2009) and the reference section of GitHub README.md <a href="https://github.com/rikenbit/nnTensor">https://github.com/rikenbit/nnTensor</a>, for details of the methods.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/rikenbit/nnTensor">https://github.com/rikenbit/nnTensor</a></td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-06-22 08:32:30 UTC; root</td>
</tr>
<tr>
<td>Author:</td>
<td>Koki Tsuyuzaki [aut, cre],
  Itoshi Nikaido [aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Koki Tsuyuzaki &lt;k.t.the-answer@hotmail.co.jp&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-06-22 13:22:39 UTC</td>
</tr>
</table>
<hr>
<h2 id='GabrielNMF'>
Gabriel-type Bi-Cross-Validation for Non-negative Matrix Factorization
</h2><span id='topic+GabrielNMF'></span>

<h3>Description</h3>

<p>The input data is assumed to be non-negative matrix.
GabrielNMF devides the input file into four matrices (A, B, C, and D)
and perform cross validation by the prediction of A from the matrices
B, C, and D.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>GabrielNMF(X, J = 3, nx = 5, ny = 5, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="GabrielNMF_+3A_x">X</code></td>
<td>

<p>The input matrix which has N-rows and M-columns.
</p>
</td></tr>
<tr><td><code id="GabrielNMF_+3A_j">J</code></td>
<td>

<p>The number of low-dimension (J &lt; N, M).
</p>
</td></tr>
<tr><td><code id="GabrielNMF_+3A_nx">nx</code></td>
<td>

<p>The number of hold-out in row-wise direction (2 &lt; nx &lt; N).
</p>
</td></tr>
<tr><td><code id="GabrielNMF_+3A_ny">ny</code></td>
<td>

<p>The number of hold-out in row-wise direction (2 &lt; ny &lt; M).
</p>
</td></tr>
<tr><td><code id="GabrielNMF_+3A_...">...</code></td>
<td>

<p>Other parameters for NMF function.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>TestRecError : The reconstruction error calculated by Gabriel-style Bi-Cross
Validation.
</p>


<h3>Author(s)</h3>

<p>Koki Tsuyuzaki</p>


<h3>References</h3>

<p>Art B. Owen et. al., (2009). Bi-Cross-Validation of the SVD and the Nonnegative Matrix Factorization. <em>The Annals of Applied Statistics</em>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  if(interactive()){
    # Test data
    matdata &lt;- toyModel(model = "NMF")

    # Bi-Cross-Validation
    BCV &lt;- rep(0, length=5)
    names(BCV) &lt;- 2:6
    for(j in seq(BCV)){
      print(j+1)
      BCV[j] &lt;- mean(GabrielNMF(matdata, J=j+1, nx=2, ny=2)$TestRecError)
    }
    proper.rank &lt;- as.numeric(names(BCV)[which(BCV == min(BCV))])
    
    # NMF
    out &lt;- NMF(matdata, J=proper.rank)
  }
</code></pre>

<hr>
<h2 id='jNMF'>
Joint Non-negative Matrix Factorization Algorithms (jNMF)
</h2><span id='topic+jNMF'></span>

<h3>Description</h3>

<p>The input data objects are assumed to be non-negative matrices.
jNMF decompose the matrices to two low-dimensional factor matices simultaneously.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>jNMF(X, M=NULL, pseudocount=.Machine$double.eps,
  initW=NULL, initV=NULL, initH=NULL, fixW=FALSE, fixV=FALSE,
  fixH=FALSE,
  L1_W=1e-10, L1_V=1e-10, L1_H=1e-10,
  L2_W=1e-10, L2_V=1e-10, L2_H=1e-10,
  J = 3, w=NULL, algorithm = c("Frobenius", "KL", "IS", "PLTF"),
  p=1, thr = 1e-10, num.iter = 100, viz = FALSE,
  figdir = NULL, verbose = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="jNMF_+3A_x">X</code></td>
<td>

<p>A list containing input matrices (X_k, &lt;N*Mk&gt;, k=1..K).
</p>
</td></tr>
<tr><td><code id="jNMF_+3A_m">M</code></td>
<td>

<p>A list containing the mask matrices (X_k, &lt;N*Mk&gt;, k=1..K). If the input matrix
has missing values, specify the element as 0 (otherwise 1).
</p>
</td></tr>
<tr><td><code id="jNMF_+3A_pseudocount">pseudocount</code></td>
<td>

<p>The pseudo count to avoid zero division, when the element is zero (Default: Machine Epsilon).
</p>
</td></tr>
<tr><td><code id="jNMF_+3A_initw">initW</code></td>
<td>

<p>The initial values of factor matrix W, which has N-rows and J-columns
(Default: NULL).
</p>
</td></tr>
<tr><td><code id="jNMF_+3A_initv">initV</code></td>
<td>

<p>A list containing the initial values of multiple factor matrices
(V_k, &lt;N*J&gt;, k=1..K, Default: NULL).
</p>
</td></tr>
<tr><td><code id="jNMF_+3A_inith">initH</code></td>
<td>

<p>A list containing the initial values of multiple factor matrices
(H_k, &lt;Mk*J&gt;, k=1..K, Default: NULL).
</p>
</td></tr>
<tr><td><code id="jNMF_+3A_fixw">fixW</code></td>
<td>

<p>Whether the factor matrix W is updated in each iteration step (Default: FALSE).
</p>
</td></tr>
<tr><td><code id="jNMF_+3A_fixv">fixV</code></td>
<td>

<p>Whether the factor matrices Vk are updated in each iteration step
(Default: FALSE).
</p>
</td></tr>
<tr><td><code id="jNMF_+3A_fixh">fixH</code></td>
<td>

<p>Whether the factor matrices Hk are updated in each iteration step
(Default: FALSE).
</p>
</td></tr>
<tr><td><code id="jNMF_+3A_l1_w">L1_W</code></td>
<td>

<p>Paramter for L1 regularitation (Default: 1e-10).
This also works as small positive constant to prevent division by zero,
so should be set as 0.
</p>
</td></tr>
<tr><td><code id="jNMF_+3A_l1_v">L1_V</code></td>
<td>

<p>Paramter for L1 regularitation (Default: 1e-10).
This also works as small positive constant to prevent division by zero,
so should be set as 0.
</p>
</td></tr>
<tr><td><code id="jNMF_+3A_l1_h">L1_H</code></td>
<td>

<p>Paramter for L1 regularitation (Default: 1e-10).
This also works as small positive constant to prevent division by zero,
so should be set as 0.
</p>
</td></tr>
<tr><td><code id="jNMF_+3A_l2_w">L2_W</code></td>
<td>

<p>Paramter for L2 regularitation (Default: 1e-10).
</p>
</td></tr>
<tr><td><code id="jNMF_+3A_l2_v">L2_V</code></td>
<td>

<p>Paramter for L2 regularitation (Default: 1e-10).
</p>
</td></tr>
<tr><td><code id="jNMF_+3A_l2_h">L2_H</code></td>
<td>

<p>Paramter for L2 regularitation (Default: 1e-10).
</p>
</td></tr>
<tr><td><code id="jNMF_+3A_j">J</code></td>
<td>

<p>Number of low-dimension (J &lt; N, Mk).
</p>
</td></tr>
<tr><td><code id="jNMF_+3A_w">w</code></td>
<td>

<p>Weight vector (Default: NULL)
</p>
</td></tr>
<tr><td><code id="jNMF_+3A_algorithm">algorithm</code></td>
<td>

<p>Divergence between X and X_bar. &quot;Frobenius&quot;, &quot;KL&quot;, and &quot;IS&quot; are available
(Default: &quot;KL&quot;).
</p>
</td></tr>
<tr><td><code id="jNMF_+3A_p">p</code></td>
<td>

<p>The parameter of Probabilistic Latent Tensor Factorization
(p=0: Frobenius, p=1: KL, p=2: IS)
</p>
</td></tr>
<tr><td><code id="jNMF_+3A_thr">thr</code></td>
<td>

<p>When error change rate is lower than thr, the iteration is terminated
(Default: 1E-10).
</p>
</td></tr>
<tr><td><code id="jNMF_+3A_num.iter">num.iter</code></td>
<td>

<p>The number of interation step (Default: 100).
</p>
</td></tr>
<tr><td><code id="jNMF_+3A_viz">viz</code></td>
<td>

<p>If viz == TRUE, internal reconstructed matrix can be visualized.
</p>
</td></tr>
<tr><td><code id="jNMF_+3A_figdir">figdir</code></td>
<td>

<p>the directory for saving the figure, when viz == TRUE.
</p>
</td></tr>
<tr><td><code id="jNMF_+3A_verbose">verbose</code></td>
<td>

<p>If verbose == TRUE, Error change rate is generated in console windos.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>W : A matrix which has N-rows and J-columns (J &lt; N, Mk).
V : A list which has multiple elements containing N-rows and J-columns
(J &lt; N, Mk).
H : A list which has multiple elements containing Mk-rows and J-columns matrix
(J &lt; N, Mk).
RecError : The reconstruction error between data matrix and reconstructed matrix
from W and H.
TrainRecError : The reconstruction error calculated by training set
(observed values specified by M).
TestRecError : The reconstruction error calculated by test set
(missing values specified by M).
RelChange : The relative change of the error.
</p>


<h3>Author(s)</h3>

<p>Koki Tsuyuzaki</p>


<h3>References</h3>

<p>Liviu Badea, (2008) Extracting Gene Expression Profiles Common to Colon and
Pancreatic Adenocarcinoma using Simultaneous nonnegative matrix factorization.
<em>Pacific Symposium on Biocomputing</em> 13:279-290
</p>
<p>Shihua Zhang, et al. (2012) Discovery of multi-dimensional modules by
integrative analysis of cancer genomic data. <em>Nucleic Acids Research</em>
40(19), 9379-9391
</p>
<p>Zi Yang, et al. (2016) A non-negative matrix factorization method for
detecting modules in heterogeneous omics multi-modal data,
<em>Bioinformatics</em> 32(1), 1-8
</p>
<p>Y. Kenan Yilmaz et al., (2010) Probabilistic Latent Tensor Factorization,
<em>International Conference on Latent Variable Analysis and Signal Separation</em>
346-353
</p>
<p>N. Fujita et al., (2018) Biomarker discovery by integrated joint non-negative matrix factorization and pathway signature analyses, <em>Scientific Report</em>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>matdata &lt;- toyModel(model = "siNMF_Hard")
out &lt;- jNMF(matdata, J=2, num.iter=2)
</code></pre>

<hr>
<h2 id='kFoldMaskTensor'>
Mask tensors generator to perform k-fold cross validation
</h2><span id='topic+kFoldMaskTensor'></span>

<h3>Description</h3>

<p>The output multiple mask tensors can be immediately specified as the argument M for NTF() or NTD().
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kFoldMaskTensor(X, k=3, seeds=123)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="kFoldMaskTensor_+3A_x">X</code></td>
<td>

<p>An rTensor object.
</p>
</td></tr>
<tr><td><code id="kFoldMaskTensor_+3A_k">k</code></td>
<td>

<p>Number of split for k-fold cross validation (Default: 3).
</p>
</td></tr>
<tr><td><code id="kFoldMaskTensor_+3A_seeds">seeds</code></td>
<td>

<p>Random seed to use for set.seed() (Default: 123).
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Koki Tsuyuzaki</p>


<h3>Examples</h3>

<pre><code class='language-R'>tensordata &lt;- toyModel(model = "CP")

str(kFoldMaskTensor(tensordata, k=5))
</code></pre>

<hr>
<h2 id='NMF'>
Non-negative Matrix Factorization Algorithms (NMF)
</h2><span id='topic+NMF'></span>

<h3>Description</h3>

<p>The input data is assumed to be non-negative matrix.
NMF decompose the matrix to two low-dimensional factor matices.
This function is also used as initialization step of tensor decomposition
(see also NTF and NTD).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>NMF(X, M=NULL, pseudocount=.Machine$double.eps, initU=NULL, initV=NULL,
  fixU=FALSE, fixV=FALSE,
  L1_U=1e-10, L1_V=1e-10, L2_U=1e-10, L2_V=1e-10, J = 3,
  rank.method=c("all", "ccc", "dispersion", "rss", "evar", "residuals",
    "sparseness.basis", "sparseness.coef", "sparseness2.basis",
    "sparseness2.coef", "norm.info.gain.basis", "norm.info.gain.coef",
    "singular",  "volume", "condition"), runtime=30,
  algorithm = c("Frobenius", "KL", "IS", "Pearson", "Hellinger", "Neyman",
    "Alpha", "Beta", "ALS", "PGD", "HALS", "GCD", "Projected", "NHR", "DTPP",
    "Orthogonal", "OrthReg"), Alpha = 1, Beta = 2,
  eta = 1e-04, thr1 = 1e-10, thr2 = 1e-10, tol = 1e-04,
  num.iter = 100, viz = FALSE, figdir = NULL, verbose = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="NMF_+3A_x">X</code></td>
<td>

<p>The input matrix which has N-rows and M-columns.
</p>
</td></tr>
<tr><td><code id="NMF_+3A_m">M</code></td>
<td>

<p>The mask matrix which has N-rows and M-columns. If the input matrix has
missing values, specify the elements as 0 (otherwise 1).
</p>
</td></tr>
<tr><td><code id="NMF_+3A_pseudocount">pseudocount</code></td>
<td>

<p>The pseudo count to avoid zero division, when the element is zero (Default: Machine Epsilon).
</p>
</td></tr>
<tr><td><code id="NMF_+3A_initu">initU</code></td>
<td>

<p>The initial values of factor matrix U, which has N-rows and J-columns
(Default: NULL).
</p>
</td></tr>
<tr><td><code id="NMF_+3A_initv">initV</code></td>
<td>
<p> The initial values of factor matrix V, which
has M-rows and J-columns (Default: NULL).
</p>
</td></tr>
<tr><td><code id="NMF_+3A_fixu">fixU</code></td>
<td>
<p> Whether the factor matrix U is updated in each iteration step
(Default: FALSE).
</p>
</td></tr>
<tr><td><code id="NMF_+3A_fixv">fixV</code></td>
<td>

<p>Whether the factor matrix V is updated in each iteration step
(Default: FALSE).
</p>
</td></tr>
<tr><td><code id="NMF_+3A_l1_u">L1_U</code></td>
<td>

<p>Paramter for L1 regularitation (Default: 1e-10).
This also works as small positive constant to prevent division by zero,
so should be set as 0.
</p>
</td></tr>
<tr><td><code id="NMF_+3A_l1_v">L1_V</code></td>
<td>

<p>Paramter for L1 regularitation (Default: 1e-10).
This also works as small positive constant to prevent division by zero,
so should be set as 0.
</p>
</td></tr>
<tr><td><code id="NMF_+3A_l2_u">L2_U</code></td>
<td>

<p>Paramter for L2 regularitation (Default: 1e-10).
</p>
</td></tr>
<tr><td><code id="NMF_+3A_l2_v">L2_V</code></td>
<td>

<p>Paramter for L2 regularitation (Default: 1e-10).
</p>
</td></tr>
<tr><td><code id="NMF_+3A_j">J</code></td>
<td>

<p>The number of low-dimension (J &lt; N, M). If a numerical vector is specified
(e.g. 2:6), the appropriate rank is estimated.
</p>
</td></tr>
<tr><td><code id="NMF_+3A_rank.method">rank.method</code></td>
<td>

<p>The rank estimation method (Default: &quot;all&quot;). Only if the J option is
specified as a numerical vector longer than two, this option will be active.
</p>
</td></tr>
<tr><td><code id="NMF_+3A_runtime">runtime</code></td>
<td>

<p>The number of trials to estimate rank (Default: 10).
</p>
</td></tr>
<tr><td><code id="NMF_+3A_algorithm">algorithm</code></td>
<td>

<p>NMF algorithms. &quot;Frobenius&quot;, &quot;KL&quot;, &quot;IS&quot;, &quot;Pearson&quot;, &quot;Hellinger&quot;, &quot;Neyman&quot;, &quot;Alpha&quot;, &quot;Beta&quot;, &quot;ALS&quot;, &quot;PGD&quot;, &quot;HALS&quot;, &quot;GCD&quot;, &quot;Projected&quot;, &quot;NHR&quot;, &quot;DTPP&quot;, &quot;Orthogonal&quot;, and &quot;OrthReg&quot; are available (Default: &quot;Frobenius&quot;).
</p>
</td></tr>
<tr><td><code id="NMF_+3A_alpha">Alpha</code></td>
<td>

<p>The parameter of Alpha-divergence.
</p>
</td></tr>
<tr><td><code id="NMF_+3A_beta">Beta</code></td>
<td>

<p>The parameter of Beta-divergence.
</p>
</td></tr>
<tr><td><code id="NMF_+3A_eta">eta</code></td>
<td>

<p>The stepsize for PGD algorithm (Default: 0.0001).
</p>
</td></tr>
<tr><td><code id="NMF_+3A_thr1">thr1</code></td>
<td>

<p>When error change rate is lower than thr1, the iteration is terminated
(Default: 1E-10).
</p>
</td></tr>
<tr><td><code id="NMF_+3A_thr2">thr2</code></td>
<td>

<p>If the minus-value is generated, replaced as thr2 (Default: 1E-10).
This value is used within the internal function .positive().
</p>
</td></tr>
<tr><td><code id="NMF_+3A_tol">tol</code></td>
<td>

<p>The tolerance parameter used in GCD algorithm.
</p>
</td></tr>
<tr><td><code id="NMF_+3A_num.iter">num.iter</code></td>
<td>

<p>The number of interation step (Default: 100).
</p>
</td></tr>
<tr><td><code id="NMF_+3A_viz">viz</code></td>
<td>

<p>If viz == TRUE, internal reconstructed matrix can be visualized.
</p>
</td></tr>
<tr><td><code id="NMF_+3A_figdir">figdir</code></td>
<td>

<p>The directory for saving the figure, when viz == TRUE.
</p>
</td></tr>
<tr><td><code id="NMF_+3A_verbose">verbose</code></td>
<td>

<p>If verbose == TRUE, Error change rate is generated in console window.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>U : A matrix which has N-rows and J-columns (J &lt; N, M).
V : A matrix which has M-rows and J-columns (J &lt; N, M).
J : The number of dimension (J &lt; N, M).
RecError : The reconstruction error between data tensor and reconstructed
tensor from U and V.
TrainRecError : The reconstruction error calculated by training set
(observed values specified by M).
TestRecError : The reconstruction error calculated by test set
(missing values specified by M).
RelChange : The relative change of the error.
Trial : All the results of the trials to estimate the rank.
Runtime : The number of the trials to estimate the rank.
RankMethod : The rank estimation method.
</p>


<h3>Author(s)</h3>

<p>Koki Tsuyuzaki</p>


<h3>References</h3>

<p>Andrzej CICHOCK, et. al., (2009). Nonnegative Matrix and Tensor Factorizations.
<em>John Wiley &amp; Sons, Ltd</em>
</p>
<p>Keigo Kimura, (2017). A Study on Efficient Algorithms for Nonnegative Matrix/
Tensor Factorization.
<em>Hokkaido University Collection of Scholarly and Academic Papers</em>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  if(interactive()){
    # Test data
    matdata &lt;- toyModel(model = "NMF")

    # Simple usage
    out &lt;- NMF(matdata, J=5)

    # Rank estimation mode (single method)
    out2 &lt;- NMF(matdata, J=2:10, rank.method="ccc", runtime=3)
    plot(out2)

    # Rank estimation mode (all method)
    out3 &lt;- NMF(matdata, J=2:10, rank.method="all", runtime=10)
    plot(out3)
  }
</code></pre>

<hr>
<h2 id='NMTF'>
Non-negative Matrix Tri-Factorization Algorithms (NMTF)
</h2><span id='topic+NMTF'></span>

<h3>Description</h3>

<p>The input data is assumed to be non-negative matrix.
NMTF decompose the matrix to three low-dimensional factor matices.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>NMTF(X, M=NULL, pseudocount=.Machine$double.eps,
    initU=NULL, initS=NULL, initV=NULL,
    fixU=FALSE, fixS=FALSE, fixV=FALSE,
    L1_U=1e-10, L1_S=1e-10, L1_V=1e-10,
    L2_U=1e-10, L2_S=1e-10, L2_V=1e-10,
    orthU=FALSE, orthV=FALSE,
    rank = c(3, 4),
    algorithm = c("Frobenius", "KL", "IS", "ALS", "PG", "COD", "Beta"),
    Beta = 2, root = FALSE, thr = 1e-10, num.iter = 100,
    viz = FALSE, figdir = NULL, verbose = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="NMTF_+3A_x">X</code></td>
<td>

<p>The input matrix which has N-rows and M-columns.
</p>
</td></tr>
<tr><td><code id="NMTF_+3A_m">M</code></td>
<td>

<p>The mask matrix which has N-rows and M-columns. If the input matrix has
missing values, specify the elements as 0 (otherwise 1).
</p>
</td></tr>
<tr><td><code id="NMTF_+3A_pseudocount">pseudocount</code></td>
<td>

<p>The pseudo count to avoid zero division, when the element is zero (Default: Machine Epsilon).
</p>
</td></tr>
<tr><td><code id="NMTF_+3A_initu">initU</code></td>
<td>

<p>The initial values of factor matrix U, which has N-rows and J1-columns
(Default: NULL).
</p>
</td></tr>
<tr><td><code id="NMTF_+3A_inits">initS</code></td>
<td>

<p>The initial values of factor matrix S, which has J1-rows and J2-columns
(Default: NULL).
</p>
</td></tr>
<tr><td><code id="NMTF_+3A_initv">initV</code></td>
<td>
<p> The initial values of factor matrix V, which
has M-rows and J2-columns (Default: NULL).
</p>
</td></tr>
<tr><td><code id="NMTF_+3A_fixu">fixU</code></td>
<td>
<p> Whether the factor matrix U is updated in each iteration step
(Default: FALSE).
</p>
</td></tr>
<tr><td><code id="NMTF_+3A_fixs">fixS</code></td>
<td>
<p> Whether the factor matrix S is updated in each iteration step
(Default: FALSE).
</p>
</td></tr>
<tr><td><code id="NMTF_+3A_fixv">fixV</code></td>
<td>

<p>Whether the factor matrix V is updated in each iteration step
(Default: FALSE).
</p>
</td></tr>
<tr><td><code id="NMTF_+3A_l1_u">L1_U</code></td>
<td>

<p>Paramter for L1 regularitation (Default: 1e-10).
</p>
</td></tr>
<tr><td><code id="NMTF_+3A_l1_s">L1_S</code></td>
<td>

<p>Paramter for L1 regularitation (Default: 1e-10).
</p>
</td></tr>
<tr><td><code id="NMTF_+3A_l1_v">L1_V</code></td>
<td>

<p>Paramter for L1 regularitation (Default: 1e-10).
</p>
</td></tr>
<tr><td><code id="NMTF_+3A_l2_u">L2_U</code></td>
<td>

<p>Paramter for L2 regularitation (Default: 1e-10).
</p>
</td></tr>
<tr><td><code id="NMTF_+3A_l2_s">L2_S</code></td>
<td>

<p>Paramter for L2 regularitation (Default: 1e-10).
</p>
</td></tr>
<tr><td><code id="NMTF_+3A_l2_v">L2_V</code></td>
<td>

<p>Paramter for L2 regularitation (Default: 1e-10).
</p>
</td></tr>
<tr><td><code id="NMTF_+3A_orthu">orthU</code></td>
<td>

<p>Whether the column vectors of matrix U are orthogonalized (Default: FALSE).
</p>
</td></tr>
<tr><td><code id="NMTF_+3A_orthv">orthV</code></td>
<td>

<p>Whether the column vectors of matrix V are orthogonalized (Default: FALSE).
</p>
</td></tr>
<tr><td><code id="NMTF_+3A_rank">rank</code></td>
<td>

<p>The number of low-dimension (J1 (&lt; N) and J2 (&lt; M)) (Default: c(3,4)).
</p>
</td></tr>
<tr><td><code id="NMTF_+3A_algorithm">algorithm</code></td>
<td>

<p>NMTF algorithms. &quot;Frobenius&quot;, &quot;KL&quot;, &quot;IS&quot;, &quot;ALS&quot;, &quot;PG&quot;, &quot;COD&quot;, and &quot;Beta&quot; are available (Default: &quot;Frobenius&quot;).
</p>
</td></tr>
<tr><td><code id="NMTF_+3A_beta">Beta</code></td>
<td>

<p>The parameter of Beta-divergence (Default: 2, which means &quot;Frobenius&quot;).
</p>
</td></tr>
<tr><td><code id="NMTF_+3A_root">root</code></td>
<td>

<p>Whether square root is calculed in each iteration (Default: FALSE).
</p>
</td></tr>
<tr><td><code id="NMTF_+3A_thr">thr</code></td>
<td>

<p>When error change rate is lower than thr, the iteration is terminated (Default: 1E-10).
</p>
</td></tr>
<tr><td><code id="NMTF_+3A_num.iter">num.iter</code></td>
<td>

<p>The number of interation step (Default: 100).
</p>
</td></tr>
<tr><td><code id="NMTF_+3A_viz">viz</code></td>
<td>

<p>If viz == TRUE, internal reconstructed matrix can be visualized.
</p>
</td></tr>
<tr><td><code id="NMTF_+3A_figdir">figdir</code></td>
<td>

<p>The directory for saving the figure, when viz == TRUE.
</p>
</td></tr>
<tr><td><code id="NMTF_+3A_verbose">verbose</code></td>
<td>

<p>If verbose == TRUE, Error change rate is generated in console window.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>U : A matrix which has N-rows and J1-columns (J1 &lt; N).
S : A matrix which has J1-rows and J2-columns.
V : A matrix which has M-rows and J2-columns (J2 &lt; M).
rank : The number of low-dimension (J1 (&lt; N) and J2 (&lt; M)).
RecError : The reconstruction error between data tensor and reconstructed
tensor from U and V.
TrainRecError : The reconstruction error calculated by training set
(observed values specified by M).
TestRecError : The reconstruction error calculated by test set
(missing values specified by M).
RelChange : The relative change of the error.
algorithm: algorithm specified.
</p>


<h3>Author(s)</h3>

<p>Koki Tsuyuzaki</p>


<h3>References</h3>

<p>Fast Optimization of Non-Negative Matrix Tri-Factorization: Supporting Information, Andrej Copar, et. al., <em>PLOS ONE</em>, 14(6), e0217994, 2019
</p>
<p>Co-clustering by Block Value Decomposition, Bo Long et al., <em>SIGKDD'05</em>, 2005
</p>
<p>Orthogonal Nonnegative Matrix Tri-Factorizations for Clustering, Chris Ding et. al., <em>12th ACM SIGKDD</em>, 2006
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  if(interactive()){
    # Test data
    matdata &lt;- toyModel(model = "NMF")

    # Simple usage
    out &lt;- NMTF(matdata, rank=c(4,4))
  }
</code></pre>

<hr>
<h2 id='nnTensor-package'>
Non-Negative Tensor Decomposition
</h2><span id='topic+nnTensor-package'></span><span id='topic+nnTensor'></span>

<h3>Description</h3>

<p>Some functions for performing non-negative matrix factorization, non-negative CANDECOMP/PARAFAC (CP) decomposition, non-negative Tucker decomposition, and generating toy model data. See Andrzej Cichock et al (2009) and the reference section of GitHub README.md &lt;https://github.com/rikenbit/nnTensor&gt;, for details of the methods.
</p>


<h3>Details</h3>

<p>The DESCRIPTION file:
</p>

<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> nnTensor</td>
</tr>
<tr>
 <td style="text-align: left;">
Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
Title: </td><td style="text-align: left;"> Non-Negative Tensor Decomposition</td>
</tr>
<tr>
 <td style="text-align: left;">
Version: </td><td style="text-align: left;"> 1.2.0</td>
</tr>
<tr>
 <td style="text-align: left;">
Authors@R: </td><td style="text-align: left;"> c(person("Koki", "Tsuyuzaki", role = c("aut", "cre"),
                      email = "k.t.the-answer@hotmail.co.jp"),
               person("Itoshi", "Nikaido", role = "aut"))</td>
</tr>
<tr>
 <td style="text-align: left;">
Depends: </td><td style="text-align: left;"> R (&gt;= 3.4.0)</td>
</tr>
<tr>
 <td style="text-align: left;">
Imports: </td><td style="text-align: left;"> methods,
MASS,
fields,
rTensor,
plot3D,
tagcloud,
ggplot2</td>
</tr>
<tr>
 <td style="text-align: left;">
Suggests: </td><td style="text-align: left;"> knitr,
rmarkdown,
testthat,
dplyr</td>
</tr>
<tr>
 <td style="text-align: left;">
Description: </td><td style="text-align: left;"> Some functions for performing non-negative matrix factorization, non-negative CANDECOMP/PARAFAC (CP) decomposition, non-negative Tucker decomposition, and generating toy model data. See Andrzej Cichock et al (2009) and the reference section of GitHub README.md &lt;https://github.com/rikenbit/nnTensor&gt;, for details of the methods.</td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> MIT + file LICENSE</td>
</tr>
<tr>
 <td style="text-align: left;">
URL: </td><td style="text-align: left;"> https://github.com/rikenbit/nnTensor</td>
</tr>
<tr>
 <td style="text-align: left;">
VignetteBuilder: </td><td style="text-align: left;"> knitr</td>
</tr>
<tr>
 <td style="text-align: left;">
Author: </td><td style="text-align: left;"> Koki Tsuyuzaki [aut, cre],
  Itoshi Nikaido [aut]</td>
</tr>
<tr>
 <td style="text-align: left;">
Maintainer: </td><td style="text-align: left;"> Koki Tsuyuzaki &lt;k.t.the-answer@hotmail.co.jp&gt;</td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>


<p>Index of help topics:
</p>
<pre>
GabrielNMF              Gabriel-type Bi-Cross-Validation for
                        Non-negative Matrix Factorization
NMF                     Non-negative Matrix Factorization Algorithms
                        (NMF)
NMTF                    Non-negative Matrix Tri-Factorization
                        Algorithms (NMTF)
NTD                     Non-negative Tucker Decomposition Algorithms
                        (NTD)
NTF                     Non-negative CP Decomposition Algorithms (NTF)
jNMF                    Joint Non-negative Matrix Factorization
                        Algorithms (jNMF)
kFoldMaskTensor         Mask tensors generator to perform k-fold cross
                        validation
nnTensor-package        Non-Negative Tensor Decomposition
plot.NMF                Plot function of the result of NMF function
plotTensor2D            Plot function for visualization of matrix data
                        structure
plotTensor3D            Plot function for visualization of tensor data
                        structure
recTensor               Tensor Reconstruction from core tensor (S) and
                        factor matrices (A)
siNMF                   Simultaneous Non-negative Matrix Factorization
                        Algorithms (siNMF)
toyModel                Toy model data for using NMF, NTF, and NTD
</pre>


<h3>Author(s)</h3>

<p>NA
</p>
<p>Maintainer: NA
</p>


<h3>References</h3>

<p>Andrzej CICHOCK, et. al., (2009). Nonnegative Matrix and Tensor Factorizations. <em>John Wiley &amp; Sons, Ltd</em>
</p>
<p>Keigo Kimura, (2017). A Study on Efficient Algorithms for Nonnegative Matrix/Tensor Factorization. <em>Hokkaido University Collection of Scholarly and Academic Papers</em>
</p>
<p>Andrzej CICHOCKI et. al., (2007). Non-negative Tensor Factorization using Alpha and Beta Divergence. <em>IEEE ICASSP 2007</em>
</p>
<p>Anh Huy PHAN et. al., (2008). Multi-way Nonnegative Tensor Factorization Using Fast Hierarchical Alternating Least Squares Algorithm (HALS). <em>NOLTA2008</em>
</p>
<p>Andrzej CICHOCKI et. al., (2008). Fast Local Algorithms for Large Scale Nonnegative Matrix and Tensor Factorizations. <em>IEICE Transactions on Fundamentals of Electronics, Communications and Computer Sciences</em>
</p>
<p>Yong-Deok Kim et. al., (2007). Nonnegative Tucker Decomposition. <em>IEEE Conference on Computer Vision and Pattern Recognition</em>
</p>
<p>Yong-Deok Kim et. al., (2008). Nonneegative Tucker Decomposition With Alpha-Divergence. <em>IEEE International Conference on Acoustics, Speech and Signal Processing</em>
</p>
<p>Anh Huy Phan, (2008). Fast and efficient algorithms for nonnegative Tucker decomposition. <em>Advances in Neural Networks - ISNN2008</em>
</p>
<p>Anh Hyu Phan et. al. (2011). Extended HALS algorithm for nonnegative Tucker decomposition and its applications for multiway analysis and classification. <em>Neurocomputing</em>
</p>
<p>Jean-Philippe Brunet. et. al., (2004). Metagenes and molecular pattern discovery using matrix factorization. <em>PNAS</em>
</p>
<p>Xiaoxu Han. (2007). CANCER MOLECULAR PATTERN DISCOVERY BY SUBSPACE CONSENSUS KERNEL CLASSIFICATION
</p>
<p>Attila Frigyesi. et. al., (2008). Non-Negative Matrix Factorization for the Analysis of Complex Gene Expression Data: Identification of Clinically Relevant Tumor Subtypes. <em>Cancer Informatics</em>
</p>
<p>Haesun Park. et. al., (2019). Lecture 3: Nonnegative Matrix Factorization: Algorithms and Applications. <em>SIAM Gene Golub Summer School, Aussois France, June 18, 2019</em>
</p>
<p>Chunxuan Shao. et. al., (2017). Robust classification of single-cell transcriptome data by nonnegative matrix factorization. <em>Bioinformatics</em>
</p>
<p>Paul Fogel (2013). Permuted NMF: A Simple Algorithm Intended to Minimize the Volume of the Score Matrix
</p>
<p>Philip M. Kim. et. al., (2003). Subsystem Identification Through Dimensionality Reduction of Large-Scale Gene Expression Data. <em>Genome Research</em>
</p>
<p>Lucie N. Hutchins. et. al., (2008). Position-dependent motif characterization using non-negative matrix factorization. <em>Bioinformatics</em>
</p>
<p>Patrik O. Hoyer (2004). Non-negative Matrix Factorization with Sparseness Constraints. <em>Journal of Machine Learning 5</em>
</p>
<p>N. Fujita et al., (2018) Biomarker discovery by integrated joint non-negative matrix factorization and pathway signature analyses, <em>Scientific Report</em>
</p>
<p>Art B. Owen et. al., (2009). Bi-Cross-Validation of the SVD and the Nonnegative Matrix Factorization. <em>The Annals of Applied Statistics</em>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+toyModel">toyModel</a></code>,<code><a href="#topic+NMF">NMF</a></code>,<code><a href="#topic+NTF">NTF</a></code>,<code><a href="#topic+NTD">NTD</a></code>,<code><a href="#topic+recTensor">recTensor</a></code>,<code><a href="#topic+plotTensor3D">plotTensor3D</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>ls("package:nnTensor")
</code></pre>

<hr>
<h2 id='NTD'>
Non-negative Tucker Decomposition Algorithms (NTD)
</h2><span id='topic+NTD'></span>

<h3>Description</h3>

<p>The input data is assumed to be non-negative tensor.
NTD decompose the tensor to the dense core tensor (S) and low-dimensional
factor matices (A).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>NTD(X, M=NULL, pseudocount=.Machine$double.eps, initS=NULL, initA=NULL,
  fixS=FALSE, fixA=FALSE, L1_A=1e-10, L2_A=1e-10,
  rank = rep(3, length=length(dim(X))),
  modes = seq_along(dim(X)),
  algorithm = c("Frobenius", "KL", "IS", "Pearson", "Hellinger", "Neyman",
    "HALS", "Alpha", "Beta", "NMF"), init = c("NMF", "ALS", "Random"),
  nmf.algorithm = c("Frobenius", "KL", "IS", "Pearson", "Hellinger", "Neyman",
    "Alpha", "Beta", "ALS", "PGD", "HALS", "GCD", "Projected", "NHR", "DTPP",
    "Orthogonal", "OrthReg"),
  Alpha = 1,
  Beta = 2, thr = 1e-10, num.iter = 100, num.iter2 = 10, viz = FALSE,
  figdir = NULL, verbose = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="NTD_+3A_x">X</code></td>
<td>

<p>K-order input tensor which has I_1, I_2, ..., and I_K dimensions.
</p>
</td></tr>
<tr><td><code id="NTD_+3A_m">M</code></td>
<td>

<p>K-order mask tensor which has I_1, I_2, ..., and I_K dimensions. If the mask tensor has
missing values, specify the element as 0 (otherwise 1).
</p>
</td></tr>
<tr><td><code id="NTD_+3A_pseudocount">pseudocount</code></td>
<td>

<p>The pseudo count to avoid zero division, when the element is zero (Default: Machine Epsilon).
</p>
</td></tr>
<tr><td><code id="NTD_+3A_inits">initS</code></td>
<td>

<p>The initial values of core tensor which has I_1, I_2, ..., and I_K dimensions
(Default: NULL).
</p>
</td></tr>
<tr><td><code id="NTD_+3A_inita">initA</code></td>
<td>

<p>A list containing the initial values of K factor matrices
(A_k, &lt;Ik*Jk&gt;, k=1..K, Default: NULL).
</p>
</td></tr>
<tr><td><code id="NTD_+3A_fixs">fixS</code></td>
<td>

<p>Whether the core tensor S is updated in each iteration step (Default: FALSE).
</p>
</td></tr>
<tr><td><code id="NTD_+3A_fixa">fixA</code></td>
<td>

<p>Whether the factor matrices Ak are updated in each iteration step
(Default: FALSE).
</p>
</td></tr>
<tr><td><code id="NTD_+3A_l1_a">L1_A</code></td>
<td>

<p>Paramter for L1 regularitation (Default: 1e-10).
This also works as small positive constant to prevent division by zero,
so should be set as 0.
</p>
</td></tr>
<tr><td><code id="NTD_+3A_l2_a">L2_A</code></td>
<td>

<p>Paramter for L2 regularitation (Default: 1e-10).
</p>
</td></tr>
<tr><td><code id="NTD_+3A_rank">rank</code></td>
<td>

<p>The number of low-dimension in each mode (Default: 3 for each mode).
</p>
</td></tr>
<tr><td><code id="NTD_+3A_modes">modes</code></td>
<td>

<p>The vector of the modes on which to perform the decomposition
(Default: 1:K &lt;all modes&gt;).
</p>
</td></tr>
<tr><td><code id="NTD_+3A_algorithm">algorithm</code></td>
<td>

<p>NTD algorithms. &quot;Frobenius&quot;, &quot;KL&quot;, &quot;IS&quot;, &quot;Pearson&quot;, &quot;Hellinger&quot;, &quot;Neyman&quot;,
&quot;HALS&quot;, &quot;Alpha&quot;, &quot;Beta&quot;, &quot;NMF&quot; are available (Default: &quot;Frobenius&quot;).
</p>
</td></tr>
<tr><td><code id="NTD_+3A_nmf.algorithm">nmf.algorithm</code></td>
<td>

<p>NMF algorithms, when the algorithm is &quot;NMF&quot;.
&quot;Frobenius&quot;, &quot;KL&quot;, &quot;IS&quot;, &quot;Pearson&quot;, &quot;Hellinger&quot;, &quot;Neyman&quot;, &quot;Alpha&quot;, &quot;Beta&quot;,
&quot;ALS&quot;, &quot;PGD&quot;, &quot;HALS&quot;, &quot;GCD&quot;, &quot;Projected&quot;, &quot;NHR&quot;, &quot;DTPP&quot;, &quot;Orthogonal&quot;,
and &quot;OrthReg&quot; are available (Default: &quot;Frobenius&quot;).
</p>
</td></tr>
<tr><td><code id="NTD_+3A_init">init</code></td>
<td>

<p>The initialization algorithms. &quot;NMF&quot;, &quot;ALS&quot;, and &quot;Random&quot; are available
(Default: &quot;NMF&quot;).
</p>
</td></tr>
<tr><td><code id="NTD_+3A_alpha">Alpha</code></td>
<td>

<p>The parameter of Alpha-divergence.
</p>
</td></tr>
<tr><td><code id="NTD_+3A_beta">Beta</code></td>
<td>

<p>The parameter of Beta-divergence.
</p>
</td></tr>
<tr><td><code id="NTD_+3A_thr">thr</code></td>
<td>

<p>When error change rate is lower than thr1, the iteration is terminated
(Default: 1E-10).
</p>
</td></tr>
<tr><td><code id="NTD_+3A_num.iter">num.iter</code></td>
<td>

<p>The number of interation step (Default: 100).
</p>
</td></tr>
<tr><td><code id="NTD_+3A_num.iter2">num.iter2</code></td>
<td>

<p>The number of NMF interation step, when the algorithm is &quot;NMF&quot; (Default: 10).
</p>
</td></tr>
<tr><td><code id="NTD_+3A_viz">viz</code></td>
<td>

<p>If viz == TRUE, internal reconstructed tensor can be visualized.
</p>
</td></tr>
<tr><td><code id="NTD_+3A_figdir">figdir</code></td>
<td>

<p>the directory for saving the figure, when viz == TRUE (Default: NULL).
</p>
</td></tr>
<tr><td><code id="NTD_+3A_verbose">verbose</code></td>
<td>

<p>If verbose == TRUE, Error change rate is generated in console windos.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>S : K-order tensor object, which is defined as S4 class of rTensor package.
A : A list containing K factor matrices.
RecError : The reconstruction error between data tensor and reconstructed
tensor from S and A.
TrainRecError : The reconstruction error calculated by training set
(observed values specified by M).
TestRecError : The reconstruction error calculated by test set
(missing values specified by M).
RelChange : The relative change of the error.
</p>


<h3>Author(s)</h3>

<p>Koki Tsuyuzaki</p>


<h3>References</h3>

<p>Yong-Deok Kim et. al., (2007). Nonnegative Tucker Decomposition.
<em>IEEE Conference on Computer Vision and Pattern Recognition</em>
</p>
<p>Yong-Deok Kim et. al., (2008). Nonneegative Tucker Decomposition With
Alpha-Divergence. <em>IEEE International Conference on Acoustics,
Speech and Signal Processing</em>
</p>
<p>Anh Huy Phan, (2008). Fast and efficient algorithms for nonnegative
Tucker decomposition. <em>Advances in Neural Networks - ISNN2008</em>
</p>
<p>Anh Hyu Phan et. al. (2011). Extended HALS algorithm for nonnegative
Tucker decomposition and its applications for multiway analysis and classification.
<em>Neurocomputing</em>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+plotTensor3D">plotTensor3D</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tensordata &lt;- toyModel(model = "Tucker")
out &lt;- NTD(tensordata, rank=c(2,2,2), algorithm="Frobenius",
  init="Random", num.iter=2)
</code></pre>

<hr>
<h2 id='NTF'>
Non-negative CP Decomposition Algorithms (NTF)
</h2><span id='topic+NTF'></span>

<h3>Description</h3>

<p>The input data is assumed to be non-negative tensor.
NTF decompose the tensor to the diagonal core tensor (S) and low-dimensional
factor matices (A).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>NTF(X, M=NULL, pseudocount=.Machine$double.eps, initA=NULL,
  fixA=FALSE, L1_A=1e-10, L2_A=1e-10, rank = 3,
  algorithm = c("Frobenius", "KL", "IS", "Pearson", "Hellinger", "Neyman",
    "HALS", "Alpha-HALS", "Beta-HALS", "Alpha", "Beta"),
  init = c("NMF", "ABS-SVD", "ALS", "Random"), Alpha = 1,
  Beta = 2, thr = 1e-10, num.iter = 100, viz = FALSE,
  figdir = NULL, verbose = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="NTF_+3A_x">X</code></td>
<td>

<p>K-order input tensor which has I_1, I_2, ..., and I_K dimensions.
</p>
</td></tr>
<tr><td><code id="NTF_+3A_m">M</code></td>
<td>

<p>K-order mask tensor which has I_1, I_2, ..., and I_K dimensions. If the mask tensor has
missing values, specify the element as 0 (otherwise 1).
</p>
</td></tr>
<tr><td><code id="NTF_+3A_pseudocount">pseudocount</code></td>
<td>

<p>The pseudo count to avoid zero division, when the element is zero (Default: Machine Epsilon).
</p>
</td></tr>
<tr><td><code id="NTF_+3A_inita">initA</code></td>
<td>

<p>A list containing the initial values of K factor matrices
(A_k, &lt;Ik*Jk&gt;, k=1..K, Default: NULL).
</p>
</td></tr>
<tr><td><code id="NTF_+3A_fixa">fixA</code></td>
<td>

<p>Whether the factor matrices Ak are updated in each iteration step
(Default: FALSE).
</p>
</td></tr>
<tr><td><code id="NTF_+3A_l1_a">L1_A</code></td>
<td>

<p>Paramter for L1 regularitation (Default: 1e-10).
This also works as small positive constant to prevent division by zero,
so should be set as 0.
</p>
</td></tr>
<tr><td><code id="NTF_+3A_l2_a">L2_A</code></td>
<td>

<p>Paramter for L2 regularitation (Default: 1e-10).
</p>
</td></tr>
<tr><td><code id="NTF_+3A_rank">rank</code></td>
<td>

<p>The number of low-dimension in each mode (Default: 3).
</p>
</td></tr>
<tr><td><code id="NTF_+3A_algorithm">algorithm</code></td>
<td>

<p>NTF algorithms. &quot;Frobenius&quot;, &quot;KL&quot;, &quot;IS&quot;, &quot;Pearson&quot;, &quot;Hellinger&quot;, &quot;Neyman&quot;, &quot;HALS&quot;, &quot;Alpha-HALS&quot;, &quot;Beta-HALS&quot;, &quot;Alpha&quot;, and &quot;Beta&quot; are available
(Default: &quot;Frobenius&quot;).
</p>
</td></tr>
<tr><td><code id="NTF_+3A_init">init</code></td>
<td>

<p>The initialization algorithms. &quot;NMF&quot;, &quot;ABS-SVD&quot;, &quot;ALS&quot;, and &quot;Random&quot; are available
(Default: &quot;NMF&quot;).
</p>
</td></tr>
<tr><td><code id="NTF_+3A_alpha">Alpha</code></td>
<td>

<p>The parameter of Alpha-divergence.
</p>
</td></tr>
<tr><td><code id="NTF_+3A_beta">Beta</code></td>
<td>

<p>The parameter of Beta-divergence.
</p>
</td></tr>
<tr><td><code id="NTF_+3A_thr">thr</code></td>
<td>

<p>When error change rate is lower than thr1, the iteration is terminated
(Default: 1E-10).
</p>
</td></tr>
<tr><td><code id="NTF_+3A_num.iter">num.iter</code></td>
<td>

<p>The number of interation step (Default: 100).
</p>
</td></tr>
<tr><td><code id="NTF_+3A_viz">viz</code></td>
<td>

<p>If viz == TRUE, internal reconstructed tensor can be visualized.
</p>
</td></tr>
<tr><td><code id="NTF_+3A_figdir">figdir</code></td>
<td>

<p>the directory for saving the figure, when viz == TRUE (Default: NULL).
</p>
</td></tr>
<tr><td><code id="NTF_+3A_verbose">verbose</code></td>
<td>

<p>If verbose == TRUE, Error change rate is generated in console windos.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>S : K-order tensor object, which is defined as S4 class of rTensor package.
A : A list containing K factor matrices.
RecError : The reconstruction error between data tensor and reconstructed
tensor from S and A.
TrainRecError : The reconstruction error calculated by training set
(observed values specified by M).
TestRecError : The reconstruction error calculated by test set
(missing values specified by M).
RelChange : The relative change of the error.
</p>


<h3>Author(s)</h3>

<p>Koki Tsuyuzaki</p>


<h3>References</h3>

<p>Andrzej CICHOCKI et. al., (2007). Non-negative Tensor Factorization using
Alpha and Beta Divergence. <em>IEEE ICASSP 2007</em>
</p>
<p>Anh Huy PHAN et. al., (2008). Multi-way Nonnegative Tensor Factorization
Using Fast Hierarchical Alternating Least Squares Algorithm (HALS). <em>NOLTA2008</em>
</p>
<p>Andrzej CICHOCKI et. al., (2008). Fast Local Algorithms for Large Scale
Nonnegative Matrix and Tensor Factorizations.
<em>IEICE Transactions on Fundamentals of Electronics, Communications
and Computer Sciences</em>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+plotTensor3D">plotTensor3D</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tensordata &lt;- toyModel(model = "CP")
out &lt;- NTF(tensordata, rank=3, algorithm="Beta-HALS", num.iter=2)
</code></pre>

<hr>
<h2 id='plot.NMF'>
Plot function of the result of NMF function
</h2><span id='topic+plot.NMF'></span><span id='topic+plot'></span>

<h3>Description</h3>

<p>Only if J is specified as a vector longer than 1, this function will be active.
</p>


<h3>Author(s)</h3>

<p>Koki Tsuyuzaki</p>


<h3>References</h3>

<p>Jean-Philippe Brunet. et. al., (2004). Metagenes and molecular pattern discovery using matrix factorization. <em>PNAS</em>
</p>
<p>Xiaoxu Han. (2007). CANCER MOLECULAR PATTERN DISCOVERY BY SUBSPACE CONSENSUS KERNEL CLASSIFICATION
</p>
<p>Attila Frigyesi. et. al., (2008). Non-Negative Matrix Factorization for the Analysis of Complex Gene Expression Data: Identification of Clinically Relevant Tumor Subtypes. <em>Cancer Informatics</em>
</p>
<p>Haesun Park. et. al., (2019). Lecture 3: Nonnegative Matrix Factorization: Algorithms and Applications. <em>SIAM Gene Golub Summer School, Aussois France, June 18, 2019</em>
</p>
<p>Chunxuan Shao. et. al., (2017). Robust classification of single-cell transcriptome data by nonnegative matrix factorization. <em>Bioinformatics</em>
</p>
<p>Paul Fogel (2013). Permuted NMF: A Simple Algorithm Intended to Minimize the Volume of the Score Matrix
</p>
<p>Philip M. Kim. et. al., (2003). Subsystem Identification Through Dimensionality Reduction of Large-Scale Gene Expression Data. <em>Genome Research</em>
</p>
<p>Lucie N. Hutchins. et. al., (2008). Position-dependent motif characterization using non-negative matrix factorization. <em>Bioinformatics</em>
</p>
<p>Patrik O. Hoyer (2004). Non-negative Matrix Factorization with Sparseness Constraints. <em>Journal of Machine Learning 5</em>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>methods(class = "NMF")
</code></pre>

<hr>
<h2 id='plotTensor2D'>
Plot function for visualization of matrix data structure
</h2><span id='topic+plotTensor2D'></span>

<h3>Description</h3>

<p>Combined with recTensor function and the result of NTF or NTD, the reconstructed tensor structure can be visullized.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotTensor2D(X = NULL, method=c("sd", "mad"),
    sign=c("positive", "negative", "both"), thr=2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plotTensor2D_+3A_x">X</code></td>
<td>

<p>Matrix object.
</p>
</td></tr>
<tr><td><code id="plotTensor2D_+3A_method">method</code></td>
<td>

<p>Cutoff method to focus on large/small value in the tensor data (Default: &quot;sd&quot;).
</p>
</td></tr>
<tr><td><code id="plotTensor2D_+3A_sign">sign</code></td>
<td>

<p>Direction to cutoff the large/small value in the tensor data (Default: &quot;positive&quot;).
</p>
</td></tr>
<tr><td><code id="plotTensor2D_+3A_thr">thr</code></td>
<td>

<p>Threshold of cutoff method (Default: 2).
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Koki Tsuyuzaki</p>


<h3>Examples</h3>

<pre><code class='language-R'>tensordata &lt;- toyModel(model = "CP")

out &lt;- NTF(tensordata, rank=3, num.iter=2)

tmp &lt;- tempdir()

png(filename=paste0(tmp, "/NTF.png"))
plotTensor2D(out$A[[1]])
dev.off()
</code></pre>

<hr>
<h2 id='plotTensor3D'>
Plot function for visualization of tensor data structure
</h2><span id='topic+plotTensor3D'></span>

<h3>Description</h3>

<p>Combined with recTensor function and the result of NTF or NTD, the reconstructed tensor structure can be visullized.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotTensor3D(X = NULL, method=c("sd", "mad"),
  sign=c("positive", "negative", "both"), thr=2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plotTensor3D_+3A_x">X</code></td>
<td>

<p>Tensor object, which is defined as S4 class of rTensor package.
</p>
</td></tr>
<tr><td><code id="plotTensor3D_+3A_method">method</code></td>
<td>

<p>Cutoff method to focus on large/small value in the tensor data (Default: &quot;sd&quot;).
</p>
</td></tr>
<tr><td><code id="plotTensor3D_+3A_sign">sign</code></td>
<td>

<p>Direction to cutoff the large/small value in the tensor data (Default: &quot;positive&quot;).
</p>
</td></tr>
<tr><td><code id="plotTensor3D_+3A_thr">thr</code></td>
<td>

<p>Threshold of cutoff method (Default: 2).
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Koki Tsuyuzaki</p>


<h3>Examples</h3>

<pre><code class='language-R'>tensordata &lt;- toyModel(model = "CP")

out &lt;- NTF(tensordata, rank=3, algorithm="Beta-HALS", num.iter=2)

tmp &lt;- tempdir()

png(filename=paste0(tmp, "/NTF.png"))
plotTensor3D(recTensor(out$S, out$A))
dev.off()
</code></pre>

<hr>
<h2 id='recTensor'>
Tensor Reconstruction from core tensor (S) and factor matrices (A)
</h2><span id='topic+recTensor'></span>

<h3>Description</h3>

<p>Combined with plotTensor3D function and the result of NTF or NTD, the reconstructed tesor structure can be visullized.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>recTensor(S = NULL, A = NULL, idx = seq_along(dim(S)), reverse = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="recTensor_+3A_s">S</code></td>
<td>

<p>K-order tensor object, which is defined as S4 class of rTensor package.
</p>
</td></tr>
<tr><td><code id="recTensor_+3A_a">A</code></td>
<td>

<p>A list containing K factor matrices.
</p>
</td></tr>
<tr><td><code id="recTensor_+3A_idx">idx</code></td>
<td>

<p>The direction of mode-n muliplication (Default: 1:K).
For example idx=1 is defined. S x_1 A is calculated (x_1 : mode-1 multiplication).
</p>
</td></tr>
<tr><td><code id="recTensor_+3A_reverse">reverse</code></td>
<td>

<p>If reverse = TRUE, t(A[[n]]) is multiplicated to S (Default: FALSE).
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Tensor object, which is defined as S4 class of rTensor package.
</p>


<h3>Author(s)</h3>

<p>Koki Tsuyuzaki</p>


<h3>See Also</h3>

<p><code><a href="rTensor.html#topic+Tensor-class">Tensor-class</a></code>, <code><a href="#topic+NTF">NTF</a></code>, <code><a href="#topic+NTD">NTD</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>tensordata &lt;- toyModel(model = "CP")
out &lt;- NTF(tensordata, rank=3, algorithm="Beta-HALS", num.iter=2)
rec &lt;- recTensor(out$S, out$A)
</code></pre>

<hr>
<h2 id='siNMF'>
Simultaneous Non-negative Matrix Factorization Algorithms (siNMF)
</h2><span id='topic+siNMF'></span>

<h3>Description</h3>

<p>The input data objects are assumed to be non-negative matrices.
siNMF decompose the matrices to two low-dimensional factor
matices simultaneously.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>siNMF(X, M=NULL, pseudocount=.Machine$double.eps, initW=NULL, initH=NULL,
  fixW=FALSE, fixH=FALSE,
  L1_W=1e-10, L1_H=1e-10, L2_W=1e-10, L2_H=1e-10, J = 3,
  w=NULL, algorithm = c("Frobenius", "KL", "IS", "PLTF"), p=1,
    thr = 1e-10, num.iter = 100,
    viz = FALSE, figdir = NULL, verbose = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="siNMF_+3A_x">X</code></td>
<td>

<p>A list containing the input matrices (X_k, &lt;N*Mk&gt;, k=1..K).
</p>
</td></tr>
<tr><td><code id="siNMF_+3A_m">M</code></td>
<td>

<p>A list containing the mask matrices (X_k, &lt;N*Mk&gt;, k=1..K). If the input matrix
has missing values, specify the element as 0 (otherwise 1).
</p>
</td></tr>
<tr><td><code id="siNMF_+3A_pseudocount">pseudocount</code></td>
<td>

<p>The pseudo count to avoid zero division, when the element is zero (Default: Machine Epsilon).
</p>
</td></tr>
<tr><td><code id="siNMF_+3A_initw">initW</code></td>
<td>

<p>The initial values of factor matrix W, which has N-rows and J-columns
(Default: NULL).
</p>
</td></tr>
<tr><td><code id="siNMF_+3A_inith">initH</code></td>
<td>

<p>A list containing the initial values of multiple factor matrices
(H_k, &lt;Mk*J&gt;, k=1..K, Default: NULL).
</p>
</td></tr>
<tr><td><code id="siNMF_+3A_fixw">fixW</code></td>
<td>

<p>Whether the factor matrix W is updated in each iteration step (Default: FALSE).
</p>
</td></tr>
<tr><td><code id="siNMF_+3A_fixh">fixH</code></td>
<td>

<p>Whether the factor matrices Hk are updated in each iteration step
(Default: FALSE).
</p>
</td></tr>
<tr><td><code id="siNMF_+3A_l1_w">L1_W</code></td>
<td>

<p>Paramter for L1 regularitation (Default: 1e-10).
This also works as small positive constant to prevent division by zero,
so should be set as 0.
</p>
</td></tr>
<tr><td><code id="siNMF_+3A_l1_h">L1_H</code></td>
<td>

<p>Paramter for L1 regularitation (Default: 1e-10).
This also works as small positive constant to prevent division by zero,
so should be set as 0.
</p>
</td></tr>
<tr><td><code id="siNMF_+3A_l2_w">L2_W</code></td>
<td>

<p>Paramter for L2 regularitation (Default: 1e-10).
</p>
</td></tr>
<tr><td><code id="siNMF_+3A_l2_h">L2_H</code></td>
<td>

<p>Paramter for L2 regularitation (Default: 1e-10).
</p>
</td></tr>
<tr><td><code id="siNMF_+3A_j">J</code></td>
<td>

<p>Number of low-dimension (J &lt; N, Mk).
</p>
</td></tr>
<tr><td><code id="siNMF_+3A_w">w</code></td>
<td>

<p>Weight vector (Default: NULL)
</p>
</td></tr>
<tr><td><code id="siNMF_+3A_algorithm">algorithm</code></td>
<td>

<p>Divergence between X and X_bar. &quot;Frobenius&quot;, &quot;KL&quot;, and &quot;IS&quot; are available
(Default: &quot;KL&quot;).
</p>
</td></tr>
<tr><td><code id="siNMF_+3A_p">p</code></td>
<td>

<p>The parameter of Probabilistic Latent Tensor Factorization
(p=0: Frobenius, p=1: KL, p=2: IS)
</p>
</td></tr>
<tr><td><code id="siNMF_+3A_thr">thr</code></td>
<td>

<p>When error change rate is lower than thr, the iteration is terminated
(Default: 1E-10).
</p>
</td></tr>
<tr><td><code id="siNMF_+3A_num.iter">num.iter</code></td>
<td>

<p>The number of interation step (Default: 100).
</p>
</td></tr>
<tr><td><code id="siNMF_+3A_viz">viz</code></td>
<td>

<p>If viz == TRUE, internal reconstructed matrix can be visualized.
</p>
</td></tr>
<tr><td><code id="siNMF_+3A_figdir">figdir</code></td>
<td>

<p>the directory for saving the figure, when viz == TRUE.
</p>
</td></tr>
<tr><td><code id="siNMF_+3A_verbose">verbose</code></td>
<td>

<p>If verbose == TRUE, Error change rate is generated in console windos.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>W : A matrix which has N-rows and J-columns (J &lt; N, Mk).
H : A list which has multiple elements containing Mk-rows and
J-columns matrix (J &lt; N, Mk).
RecError : The reconstruction error between data matrix and
reconstructed matrix from W and H.
TrainRecError : The reconstruction error calculated by training set
(observed values specified by M).
TestRecError : The reconstruction error calculated by test set
(missing values specified by M).
RelChange : The relative change of the error.
</p>


<h3>Author(s)</h3>

<p>Koki Tsuyuzaki</p>


<h3>References</h3>

<p>Liviu Badea, (2008) Extracting Gene Expression Profiles Common to Colon
and Pancreatic Adenocarcinoma using Simultaneous nonnegative matrix
factorization. <em>Pacific Symposium on Biocomputing</em> 13:279-290
</p>
<p>Shihua Zhang, et al. (2012) Discovery of multi-dimensional modules by
integrative analysis of cancer genomic data. <em>Nucleic Acids Research</em>
40(19), 9379-9391
</p>
<p>Zi Yang, et al. (2016) A non-negative matrix factorization method for
detecting modules in heterogeneous omics multi-modal data,
<em>Bioinformatics</em> 32(1), 1-8
</p>
<p>Y. Kenan Yilmaz et al., (2010) Probabilistic Latent Tensor Factorization,
<em>International Conference on Latent Variable Analysis and Signal Separation</em>
346-353
</p>
<p>N. Fujita et al., (2018) Biomarker discovery by integrated joint non-negative matrix factorization and pathway signature analyses, <em>Scientific Report</em>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>matdata &lt;- toyModel(model = "siNMF_Easy")
out &lt;- siNMF(matdata, J=2, num.iter=2)
</code></pre>

<hr>
<h2 id='toyModel'>
Toy model data for using NMF, NTF, and NTD
</h2><span id='topic+toyModel'></span>

<h3>Description</h3>

<p>The data is used for confirming the algorithm are properly working.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>toyModel(model = "CP", seeds=123)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="toyModel_+3A_model">model</code></td>
<td>

<p>Single character string is specified.
&quot;NMF&quot;, &quot;CP&quot;, and &quot;Tucker&quot; are available (Default: &quot;CP&quot;).
</p>
</td></tr>
<tr><td><code id="toyModel_+3A_seeds">seeds</code></td>
<td>

<p>Random number for setting set.seeds in the function (Default: 123).
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>If model is specified as &quot;NMF&quot;, a matrix is generated.
Otherwise, a tensor is generated.
</p>


<h3>Author(s)</h3>

<p>Koki Tsuyuzaki</p>


<h3>See Also</h3>

<p><code><a href="#topic+NMF">NMF</a></code>, <code><a href="#topic+NTF">NTF</a></code>, <code><a href="#topic+NTD">NTD</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>matdata &lt;- toyModel(model = "NMF", seeds=123)
tensordata1 &lt;- toyModel(model = "CP", seeds=123)
tensordata2 &lt;- toyModel(model = "Tucker", seeds=123)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
