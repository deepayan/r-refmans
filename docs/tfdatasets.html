<!DOCTYPE html><html><head><title>Help for package tfdatasets</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {tfdatasets}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#%&gt;%'><p>Pipe operator</p></a></li>
<li><a href='#all_nominal'><p>Find all nominal variables.</p></a></li>
<li><a href='#all_numeric'><p>Speciy all numeric variables.</p></a></li>
<li><a href='#as_array_iterator'><p>Convert tf_dataset to an iterator that yields R arrays.</p></a></li>
<li><a href='#as_tensor.tensorflow.python.data.ops.dataset_ops.DatasetV2'><p>Get the single element of the dataset.</p></a></li>
<li><a href='#as_tf_dataset'><p>Add the tf_dataset class to a dataset</p></a></li>
<li><a href='#choose_from_datasets'><p>Creates a dataset that deterministically chooses elements from datasets.</p></a></li>
<li><a href='#dataset_batch'><p>Combines consecutive elements of this dataset into batches.</p></a></li>
<li><a href='#dataset_bucket_by_sequence_length'><p>A transformation that buckets elements in a <code>Dataset</code> by length</p></a></li>
<li><a href='#dataset_cache'><p>Caches the elements in this dataset.</p></a></li>
<li><a href='#dataset_collect'><p>Collects a dataset</p></a></li>
<li><a href='#dataset_concatenate'><p>Creates a dataset by concatenating given dataset with this dataset.</p></a></li>
<li><a href='#dataset_decode_delim'><p>Transform a dataset with delimted text lines into a dataset with named</p>
columns</a></li>
<li><a href='#dataset_enumerate'><p>Enumerates the elements of this dataset</p></a></li>
<li><a href='#dataset_filter'><p>Filter a dataset by a predicate</p></a></li>
<li><a href='#dataset_flat_map'><p>Maps map_func across this dataset and flattens the result.</p></a></li>
<li><a href='#dataset_group_by_window'><p>Group windows of elements by key and reduce them</p></a></li>
<li><a href='#dataset_interleave'><p>Maps map_func across this dataset, and interleaves the results</p></a></li>
<li><a href='#dataset_map'><p>Map a function across a dataset.</p></a></li>
<li><a href='#dataset_map_and_batch'><p>Fused implementation of dataset_map() and dataset_batch()</p></a></li>
<li><a href='#dataset_options'><p>Get or Set Dataset Options</p></a></li>
<li><a href='#dataset_padded_batch'><p>Combines consecutive elements of this dataset into padded batches.</p></a></li>
<li><a href='#dataset_prefetch'><p>Creates a Dataset that prefetches elements from this dataset.</p></a></li>
<li><a href='#dataset_prefetch_to_device'><p>A transformation that prefetches dataset values to the given <code>device</code></p></a></li>
<li><a href='#dataset_prepare'><p>Prepare a dataset for analysis</p></a></li>
<li><a href='#dataset_reduce'><p>Reduces the input dataset to a single element.</p></a></li>
<li><a href='#dataset_rejection_resample'><p>A transformation that resamples a dataset to a target distribution.</p></a></li>
<li><a href='#dataset_repeat'><p>Repeats a dataset count times.</p></a></li>
<li><a href='#dataset_scan'><p>A transformation that scans a function across an input dataset</p></a></li>
<li><a href='#dataset_shard'><p>Creates a dataset that includes only 1 / num_shards of this dataset.</p></a></li>
<li><a href='#dataset_shuffle'><p>Randomly shuffles the elements of this dataset.</p></a></li>
<li><a href='#dataset_shuffle_and_repeat'><p>Shuffles and repeats a dataset returning a new permutation for each epoch.</p></a></li>
<li><a href='#dataset_skip'><p>Creates a dataset that skips count elements from this dataset</p></a></li>
<li><a href='#dataset_snapshot'><p>Persist the output of a dataset</p></a></li>
<li><a href='#dataset_take'><p>Creates a dataset with at most count elements from this dataset</p></a></li>
<li><a href='#dataset_take_while'><p>A transformation that stops dataset iteration based on a predicate.</p></a></li>
<li><a href='#dataset_unbatch'><p>Unbatch a dataset</p></a></li>
<li><a href='#dataset_unique'><p>A transformation that discards duplicate elements of a Dataset.</p></a></li>
<li><a href='#dataset_use_spec'><p>Transform the dataset using the provided spec.</p></a></li>
<li><a href='#dataset_window'><p>Combines input elements into a dataset of windows.</p></a></li>
<li><a href='#delim_record_spec'><p>Specification for reading a record from a text file with delimited values</p></a></li>
<li><a href='#dense_features'><p>Dense Features</p></a></li>
<li><a href='#feature_spec'><p>Creates a feature specification.</p></a></li>
<li><a href='#file_list_dataset'><p>A dataset of all files matching a pattern</p></a></li>
<li><a href='#fit.FeatureSpec'><p>Fits a feature specification.</p></a></li>
<li><a href='#fixed_length_record_dataset'><p>A dataset of fixed-length records from one or more binary files.</p></a></li>
<li><a href='#has_type'><p>Identify the type of the variable.</p></a></li>
<li><a href='#hearts'><p>Heart Disease Data Set</p></a></li>
<li><a href='#input_fn.tf_dataset'><p>Construct a tfestimators input function from a dataset</p></a></li>
<li><a href='#iterator_get_next'><p>Get next element from iterator</p></a></li>
<li><a href='#iterator_initializer'><p>An operation that should be run to initialize this iterator.</p></a></li>
<li><a href='#iterator_make_initializer'><p>Create an operation that can be run to initialize this iterator</p></a></li>
<li><a href='#iterator_string_handle'><p>String-valued tensor that represents this iterator</p></a></li>
<li><a href='#layer_input_from_dataset'><p>Creates a list of inputs from a dataset</p></a></li>
<li><a href='#length.tf_dataset'><p>Get Dataset length</p></a></li>
<li><a href='#make_csv_dataset'><p>Reads CSV files into a batched dataset</p></a></li>
<li><a href='#make-iterator'><p>Creates an iterator for enumerating the elements of this dataset.</p></a></li>
<li><a href='#next_batch'><p>Tensor(s) for retrieving the next batch from a dataset</p></a></li>
<li><a href='#output_types'><p>Output types and shapes</p></a></li>
<li><a href='#random_integer_dataset'><p>Creates a <code>Dataset</code> of pseudorandom values</p></a></li>
<li><a href='#range_dataset'><p>Creates a dataset of a step-separated range of values.</p></a></li>
<li><a href='#read_files'><p>Read a dataset from a set of files</p></a></li>
<li><a href='#reexports'><p>Objects exported from other packages</p></a></li>
<li><a href='#sample_from_datasets'><p>Samples elements at random from the datasets in <code>datasets</code>.</p></a></li>
<li><a href='#scaler'><p>List of pre-made scalers</p></a></li>
<li><a href='#scaler_min_max'><p>Creates an instance of a min max scaler</p></a></li>
<li><a href='#scaler_standard'><p>Creates an instance of a standard scaler</p></a></li>
<li><a href='#selectors'><p>Selectors</p></a></li>
<li><a href='#sparse_tensor_slices_dataset'><p>Splits each rank-N <code>tf$SparseTensor</code> in this dataset row-wise.</p></a></li>
<li><a href='#sql_record_spec'><p>A dataset consisting of the results from a SQL query</p></a></li>
<li><a href='#step_bucketized_column'><p>Creates bucketized columns</p></a></li>
<li><a href='#step_categorical_column_with_hash_bucket'><p>Creates a categorical column with hash buckets specification</p></a></li>
<li><a href='#step_categorical_column_with_identity'><p>Create a categorical column with identity</p></a></li>
<li><a href='#step_categorical_column_with_vocabulary_file'><p>Creates a categorical column with vocabulary file</p></a></li>
<li><a href='#step_categorical_column_with_vocabulary_list'><p>Creates a categorical column specification</p></a></li>
<li><a href='#step_crossed_column'><p>Creates crosses of categorical columns</p></a></li>
<li><a href='#step_embedding_column'><p>Creates embeddings columns</p></a></li>
<li><a href='#step_indicator_column'><p>Creates Indicator Columns</p></a></li>
<li><a href='#step_numeric_column'><p>Creates a numeric column specification</p></a></li>
<li><a href='#step_remove_column'><p>Creates a step that can remove columns</p></a></li>
<li><a href='#step_shared_embeddings_column'><p>Creates shared embeddings for categorical columns</p></a></li>
<li><a href='#steps'><p>Steps for feature columns specification.</p></a></li>
<li><a href='#tensor_slices_dataset'><p>Creates a dataset whose elements are slices of the given tensors.</p></a></li>
<li><a href='#tensors_dataset'><p>Creates a dataset with a single element, comprising the given tensors.</p></a></li>
<li><a href='#text_line_dataset'><p>A dataset comprising lines from one or more text files.</p></a></li>
<li><a href='#tfrecord_dataset'><p>A dataset comprising records from one or more TFRecord files.</p></a></li>
<li><a href='#until_out_of_range'><p>Execute code that traverses a dataset until an out of range condition occurs</p></a></li>
<li><a href='#with_dataset'><p>Execute code that traverses a dataset</p></a></li>
<li><a href='#zip_datasets'><p>Creates a dataset by zipping together the given datasets.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Interface to 'TensorFlow' Datasets</td>
</tr>
<tr>
<td>Version:</td>
<td>2.9.0</td>
</tr>
<tr>
<td>Description:</td>
<td>Interface to 'TensorFlow' Datasets, a high-level library for
    building complex input pipelines from simple, re-usable pieces.
    See <a href="https://www.tensorflow.org/guide">https://www.tensorflow.org/guide</a> for additional
    details.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.apache.org/licenses/LICENSE-2.0">Apache License 2.0</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/rstudio/tfdatasets">https://github.com/rstudio/tfdatasets</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/rstudio/tfdatasets/issues">https://github.com/rstudio/tfdatasets/issues</a></td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>TensorFlow &gt;= 1.4 (https://www.tensorflow.org/)</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.1)</td>
</tr>
<tr>
<td>Imports:</td>
<td>reticulate (&ge; 1.10), tensorflow (&ge; 1.13.1), magrittr, rlang,
tidyselect, stats, generics, vctrs</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.0</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat, knitr, keras, rsample, rmarkdown, Metrics, dplyr,
tfestimators</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-06-29 20:47:07 UTC; tomasz</td>
</tr>
<tr>
<td>Author:</td>
<td>Tomasz Kalinowski [ctb, cph, cre],
  Daniel Falbel [ctb, cph],
  JJ Allaire [aut, cph],
  Yuan Tang <a href="https://orcid.org/0000-0001-5243-233X"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut],
  Kevin Ushey [aut],
  RStudio [cph, fnd],
  Google Inc. [cph]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Tomasz Kalinowski &lt;tomasz.kalinowski@rstudio.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-06-29 21:10:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='+25+26gt+3B+25'>Pipe operator</h2><span id='topic++25+3E+25'></span>

<h3>Description</h3>

<p>See <code><a href="magrittr.html#topic++25+3E+25">%&gt;%</a></code> for more details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lhs %&gt;% rhs
</code></pre>

<hr>
<h2 id='all_nominal'>Find all nominal variables.</h2><span id='topic+all_nominal'></span>

<h3>Description</h3>

<p>Currently we only consider &quot;string&quot; type as nominal.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>all_nominal()
</code></pre>


<h3>See Also</h3>

<p>Other Selectors: 
<code><a href="#topic+all_numeric">all_numeric</a>()</code>,
<code><a href="#topic+has_type">has_type</a>()</code>
</p>

<hr>
<h2 id='all_numeric'>Speciy all numeric variables.</h2><span id='topic+all_numeric'></span>

<h3>Description</h3>

<p>Find all the variables with the following types:
&quot;float16&quot;, &quot;float32&quot;, &quot;float64&quot;, &quot;int16&quot;, &quot;int32&quot;, &quot;int64&quot;,
&quot;half&quot;, &quot;double&quot;.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>all_numeric()
</code></pre>


<h3>See Also</h3>

<p>Other Selectors: 
<code><a href="#topic+all_nominal">all_nominal</a>()</code>,
<code><a href="#topic+has_type">has_type</a>()</code>
</p>

<hr>
<h2 id='as_array_iterator'>Convert tf_dataset to an iterator that yields R arrays.</h2><span id='topic+as_array_iterator'></span>

<h3>Description</h3>

<p>Convert tf_dataset to an iterator that yields R arrays.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as_array_iterator(dataset)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="as_array_iterator_+3A_dataset">dataset</code></td>
<td>
<p>A tensorflow dataset</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An iterable. Use <code><a href="#topic+iterate">iterate()</a></code> or <code><a href="#topic+iter_next">iter_next()</a></code> to access values from the iterator.
</p>

<hr>
<h2 id='as_tensor.tensorflow.python.data.ops.dataset_ops.DatasetV2'>Get the single element of the dataset.</h2><span id='topic+as_tensor.tensorflow.python.data.ops.dataset_ops.DatasetV2'></span><span id='topic+get_single_element'></span><span id='topic+as.array.tensorflow.python.data.ops.dataset_ops.DatasetV2'></span>

<h3>Description</h3>

<p>The function enables you to use a TF Dataset in a stateless &quot;tensor-in
tensor-out&quot; expression, without creating an iterator. This facilitates the
ease of data transformation on tensors using the optimized TF Dataset
abstraction on top of them.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'tensorflow.python.data.ops.dataset_ops.DatasetV2'
as_tensor(x, ..., name = NULL)

## S3 method for class 'tensorflow.python.data.ops.dataset_ops.DatasetV2'
as.array(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="as_tensor.tensorflow.python.data.ops.dataset_ops.DatasetV2_+3A_x">x</code></td>
<td>
<p>A TF Dataset</p>
</td></tr>
<tr><td><code id="as_tensor.tensorflow.python.data.ops.dataset_ops.DatasetV2_+3A_...">...</code></td>
<td>
<p>passed on to <code>tensorflow::as_tensor()</code></p>
</td></tr>
<tr><td><code id="as_tensor.tensorflow.python.data.ops.dataset_ops.DatasetV2_+3A_name">name</code></td>
<td>
<p>(Optional.) A name for the TensorFlow operation.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For example, consider a <code>preprocess_batch()</code> which would take as an input
a batch of raw features and returns the processed feature.
</p>
<div class="sourceCode r"><pre>preprocess_one_case &lt;- function(x) x + 100

preprocess_batch   &lt;- function(raw_features) {
  batch_size &lt;- dim(raw_features)[1]
  ds &lt;- raw_features %&gt;%
    tensor_slices_dataset() %&gt;%
    dataset_map(preprocess_one_case, num_parallel_calls = batch_size) %&gt;%
    dataset_batch(batch_size)
  as_tensor(ds)
}

raw_features &lt;- array(seq(prod(4, 5)), c(4, 5))
preprocess_batch(raw_features)
</pre></div>
<p>In the above example, the batch of <code>raw_features</code> was converted to a TF
Dataset. Next, each of the raw_feature cases in the batch was mapped using
the preprocess_one_case and the processed features were grouped into a single
batch. The final dataset contains only one element which is a batch of all
the processed features.
</p>
<p>Note: The dataset should contain only one element. Now, instead of creating
an iterator for the dataset and retrieving the batch of features, the
<code>as_tensor()</code> function is used to skip the iterator creation process and
directly output the batch of features.
</p>
<p>This can be particularly useful when your tensor transformations are
expressed as TF Dataset operations, and you want to use those transformations
while serving your model.
</p>


<h3>See Also</h3>


<ul>
<li> <p><a href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset#get_single_element">https://www.tensorflow.org/api_docs/python/tf/data/Dataset#get_single_element</a>
</p>
</li></ul>


<hr>
<h2 id='as_tf_dataset'>Add the tf_dataset class to a dataset</h2><span id='topic+as_tf_dataset'></span>

<h3>Description</h3>

<p>Calling this function on a dataset adds the &quot;tf_dataset&quot; class to the dataset
object. All datasets returned by functions in the <span class="pkg">tfdatasets</span> package
call this function on the dataset before returning it.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as_tf_dataset(dataset)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="as_tf_dataset_+3A_dataset">dataset</code></td>
<td>
<p>A dataset</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A dataset with class &quot;tf_dataset&quot;
</p>

<hr>
<h2 id='choose_from_datasets'>Creates a dataset that deterministically chooses elements from datasets.</h2><span id='topic+choose_from_datasets'></span>

<h3>Description</h3>

<p>Creates a dataset that deterministically chooses elements from datasets.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>choose_from_datasets(datasets, choice_dataset, stop_on_empty_dataset = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="choose_from_datasets_+3A_datasets">datasets</code></td>
<td>
<p>A non-empty list of tf.data.Dataset objects with compatible
structure.</p>
</td></tr>
<tr><td><code id="choose_from_datasets_+3A_choice_dataset">choice_dataset</code></td>
<td>
<p>A <code>tf.data.Dataset</code> of scalar <code>tf.int64</code> tensors
between <code>0</code> and <code>length(datasets) - 1</code>.</p>
</td></tr>
<tr><td><code id="choose_from_datasets_+3A_stop_on_empty_dataset">stop_on_empty_dataset</code></td>
<td>
<p>If <code>TRUE</code>, selection stops if it encounters an
empty dataset. If <code>FALSE</code>, it skips empty datasets. It is recommended to
set it to <code>TRUE</code>. Otherwise, the selected elements start off as the user
intends, but may change as input datasets become empty. This can be
difficult to detect since the dataset starts off looking correct. Defaults
to <code>TRUE</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a dataset that interleaves elements from datasets according
to the values of choice_dataset.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
datasets &lt;- list(tensors_dataset("foo") %&gt;% dataset_repeat(),
                 tensors_dataset("bar") %&gt;% dataset_repeat(),
                 tensors_dataset("baz") %&gt;% dataset_repeat())

# Define a dataset containing `[0, 1, 2, 0, 1, 2, 0, 1, 2]`.
choice_dataset &lt;- range_dataset(0, 3) %&gt;% dataset_repeat(3)
result &lt;- choose_from_datasets(datasets, choice_dataset)
result %&gt;% as_array_iterator() %&gt;% iterate(function(s) s$decode()) %&gt;% print()
# [1] "foo" "bar" "baz" "foo" "bar" "baz" "foo" "bar" "baz"

## End(Not run)
</code></pre>

<hr>
<h2 id='dataset_batch'>Combines consecutive elements of this dataset into batches.</h2><span id='topic+dataset_batch'></span>

<h3>Description</h3>

<p>The components of the resulting element will have an additional outer
dimension, which will be <code>batch_size</code> (or <code>N %% batch_size</code> for the last
element if <code>batch_size</code> does not divide the number of input elements <code>N</code>
evenly and <code>drop_remainder</code> is <code>FALSE</code>). If your program depends on the
batches having the same outer dimension, you should set the <code>drop_remainder</code>
argument to <code>TRUE</code> to prevent the smaller batch from being produced.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dataset_batch(
  dataset,
  batch_size,
  drop_remainder = FALSE,
  num_parallel_calls = NULL,
  deterministic = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dataset_batch_+3A_dataset">dataset</code></td>
<td>
<p>A dataset</p>
</td></tr>
<tr><td><code id="dataset_batch_+3A_batch_size">batch_size</code></td>
<td>
<p>An integer, representing the number of consecutive elements
of this dataset to combine in a single batch.</p>
</td></tr>
<tr><td><code id="dataset_batch_+3A_drop_remainder">drop_remainder</code></td>
<td>
<p>(Optional.) A boolean, representing whether the last
batch should be dropped in the case it has fewer than <code>batch_size</code>
elements; the default behavior is not to drop the smaller batch.</p>
</td></tr>
<tr><td><code id="dataset_batch_+3A_num_parallel_calls">num_parallel_calls</code></td>
<td>
<p>(Optional.) A scalar integer, representing the
number of batches to compute asynchronously in parallel. If not specified,
batches will be computed sequentially. If the value <code>tf$data$AUTOTUNE</code> is
used, then the number of parallel calls is set dynamically based on
available resources.</p>
</td></tr>
<tr><td><code id="dataset_batch_+3A_deterministic">deterministic</code></td>
<td>
<p>(Optional.) When <code>num_parallel_calls</code> is specified, if
this boolean is specified (<code>TRUE</code> or <code>FALSE</code>), it controls the order in
which the transformation produces elements. If set to <code>FALSE</code>, the
transformation is allowed to yield elements out of order to trade
determinism for performance. If not specified, the
<code>tf.data.Options.experimental_deterministic</code> option (<code>TRUE</code> by default)
controls the behavior. See <code>dataset_options()</code> for how to set dataset
options.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A dataset
</p>


<h3>Note</h3>

<p>If your program requires data to have a statically known shape (e.g.,
when using XLA), you should use <code>drop_remainder=TRUE</code>. Without
<code>drop_remainder=TRUE</code> the shape of the output dataset will have an unknown
leading dimension due to the possibility of a smaller final batch.
</p>


<h3>See Also</h3>

<p>Other dataset methods: 
<code><a href="#topic+dataset_cache">dataset_cache</a>()</code>,
<code><a href="#topic+dataset_collect">dataset_collect</a>()</code>,
<code><a href="#topic+dataset_concatenate">dataset_concatenate</a>()</code>,
<code><a href="#topic+dataset_decode_delim">dataset_decode_delim</a>()</code>,
<code><a href="#topic+dataset_filter">dataset_filter</a>()</code>,
<code><a href="#topic+dataset_interleave">dataset_interleave</a>()</code>,
<code><a href="#topic+dataset_map_and_batch">dataset_map_and_batch</a>()</code>,
<code><a href="#topic+dataset_map">dataset_map</a>()</code>,
<code><a href="#topic+dataset_padded_batch">dataset_padded_batch</a>()</code>,
<code><a href="#topic+dataset_prefetch_to_device">dataset_prefetch_to_device</a>()</code>,
<code><a href="#topic+dataset_prefetch">dataset_prefetch</a>()</code>,
<code><a href="#topic+dataset_reduce">dataset_reduce</a>()</code>,
<code><a href="#topic+dataset_repeat">dataset_repeat</a>()</code>,
<code><a href="#topic+dataset_shuffle_and_repeat">dataset_shuffle_and_repeat</a>()</code>,
<code><a href="#topic+dataset_shuffle">dataset_shuffle</a>()</code>,
<code><a href="#topic+dataset_skip">dataset_skip</a>()</code>,
<code><a href="#topic+dataset_take_while">dataset_take_while</a>()</code>,
<code><a href="#topic+dataset_take">dataset_take</a>()</code>,
<code><a href="#topic+dataset_window">dataset_window</a>()</code>
</p>

<hr>
<h2 id='dataset_bucket_by_sequence_length'>A transformation that buckets elements in a <code>Dataset</code> by length</h2><span id='topic+dataset_bucket_by_sequence_length'></span>

<h3>Description</h3>

<p>A transformation that buckets elements in a <code>Dataset</code> by length
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dataset_bucket_by_sequence_length(
  dataset,
  element_length_func,
  bucket_boundaries,
  bucket_batch_sizes,
  padded_shapes = NULL,
  padding_values = NULL,
  pad_to_bucket_boundary = FALSE,
  no_padding = FALSE,
  drop_remainder = FALSE,
  name = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dataset_bucket_by_sequence_length_+3A_dataset">dataset</code></td>
<td>
<p>A <code>tf_dataset</code></p>
</td></tr>
<tr><td><code id="dataset_bucket_by_sequence_length_+3A_element_length_func">element_length_func</code></td>
<td>
<p>function from element in <code>Dataset</code> to <code>tf$int32</code>,
determines the length of the element, which will determine the bucket it
goes into.</p>
</td></tr>
<tr><td><code id="dataset_bucket_by_sequence_length_+3A_bucket_boundaries">bucket_boundaries</code></td>
<td>
<p>integers, upper length boundaries of the buckets.</p>
</td></tr>
<tr><td><code id="dataset_bucket_by_sequence_length_+3A_bucket_batch_sizes">bucket_batch_sizes</code></td>
<td>
<p>integers, batch size per bucket. Length should be
<code>length(bucket_boundaries) + 1</code>.</p>
</td></tr>
<tr><td><code id="dataset_bucket_by_sequence_length_+3A_padded_shapes">padded_shapes</code></td>
<td>
<p>Nested structure of <code>tf.TensorShape</code> (returned by <code><a href="tensorflow.html#topic+shape">tensorflow::shape()</a></code>)
to pass to <code>tf.data.Dataset.padded_batch</code>. If not provided, will use
<code>dataset.output_shapes</code>, which will result in variable length dimensions
being padded out to the maximum length in each batch.</p>
</td></tr>
<tr><td><code id="dataset_bucket_by_sequence_length_+3A_padding_values">padding_values</code></td>
<td>
<p>Values to pad with, passed to
<code>tf.data.Dataset.padded_batch</code>. Defaults to padding with 0.</p>
</td></tr>
<tr><td><code id="dataset_bucket_by_sequence_length_+3A_pad_to_bucket_boundary">pad_to_bucket_boundary</code></td>
<td>
<p>bool, if <code>FALSE</code>, will pad dimensions with unknown
size to maximum length in batch. If <code>TRUE</code>, will pad dimensions with
unknown size to bucket boundary minus 1 (i.e., the maximum length in
each bucket), and caller must ensure that the source <code>Dataset</code> does not
contain any elements with length longer than <code>max(bucket_boundaries)</code>.</p>
</td></tr>
<tr><td><code id="dataset_bucket_by_sequence_length_+3A_no_padding">no_padding</code></td>
<td>
<p>boolean, indicates whether to pad the batch features (features
need to be either of type <code>tf.sparse.SparseTensor</code> or of same shape).</p>
</td></tr>
<tr><td><code id="dataset_bucket_by_sequence_length_+3A_drop_remainder">drop_remainder</code></td>
<td>
<p>(Optional.) A logical scalar, representing
whether the last batch should be dropped in the case it has fewer than
<code>batch_size</code> elements; the default behavior is not to drop the smaller
batch.</p>
</td></tr>
<tr><td><code id="dataset_bucket_by_sequence_length_+3A_name">name</code></td>
<td>
<p>(Optional.) A name for the tf.data operation.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Elements of the <code>Dataset</code> are grouped together by length and then are padded
and batched.
</p>
<p>This is useful for sequence tasks in which the elements have variable
length. Grouping together elements that have similar lengths reduces the
total fraction of padding in a batch which increases training step
efficiency.
</p>
<p>Below is an example to bucketize the input data to the 3 buckets
&quot;[0, 3), [3, 5), [5, Inf)&quot; based on sequence length, with batch size 2.
</p>


<h3>See Also</h3>


<ul>
<li> <p><a href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset#bucket_by_sequence_length">https://www.tensorflow.org/api_docs/python/tf/data/Dataset#bucket_by_sequence_length</a>
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
dataset &lt;- list(c(0),
                c(1, 2, 3, 4),
                c(5, 6, 7),
                c(7, 8, 9, 10, 11),
                c(13, 14, 15, 16, 17, 18, 19, 20),
                c(21, 22)) %&gt;%
  lapply(as.array) %&gt;% lapply(as_tensor, "int32") %&gt;%
  lapply(tensors_dataset) %&gt;%
  Reduce(dataset_concatenate, .)

dataset %&gt;%
  dataset_bucket_by_sequence_length(
    element_length_func = function(elem) tf$shape(elem)[1],
    bucket_boundaries = c(3, 5),
    bucket_batch_sizes = c(2, 2, 2)
  ) %&gt;%
  as_array_iterator() %&gt;%
  iterate(print)
#      [,1] [,2] [,3] [,4]
# [1,]    1    2    3    4
# [2,]    5    6    7    0
#      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8]
# [1,]    7    8    9   10   11    0    0    0
# [2,]   13   14   15   16   17   18   19   20
#      [,1] [,2]
# [1,]    0    0
# [2,]   21   22

## End(Not run)
</code></pre>

<hr>
<h2 id='dataset_cache'>Caches the elements in this dataset.</h2><span id='topic+dataset_cache'></span>

<h3>Description</h3>

<p>Caches the elements in this dataset.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dataset_cache(dataset, filename = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dataset_cache_+3A_dataset">dataset</code></td>
<td>
<p>A dataset</p>
</td></tr>
<tr><td><code id="dataset_cache_+3A_filename">filename</code></td>
<td>
<p>String with the name of a directory on the filesystem to use
for caching tensors in this Dataset. If a filename is not provided, the
dataset will be cached in memory.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A dataset
</p>


<h3>See Also</h3>

<p>Other dataset methods: 
<code><a href="#topic+dataset_batch">dataset_batch</a>()</code>,
<code><a href="#topic+dataset_collect">dataset_collect</a>()</code>,
<code><a href="#topic+dataset_concatenate">dataset_concatenate</a>()</code>,
<code><a href="#topic+dataset_decode_delim">dataset_decode_delim</a>()</code>,
<code><a href="#topic+dataset_filter">dataset_filter</a>()</code>,
<code><a href="#topic+dataset_interleave">dataset_interleave</a>()</code>,
<code><a href="#topic+dataset_map_and_batch">dataset_map_and_batch</a>()</code>,
<code><a href="#topic+dataset_map">dataset_map</a>()</code>,
<code><a href="#topic+dataset_padded_batch">dataset_padded_batch</a>()</code>,
<code><a href="#topic+dataset_prefetch_to_device">dataset_prefetch_to_device</a>()</code>,
<code><a href="#topic+dataset_prefetch">dataset_prefetch</a>()</code>,
<code><a href="#topic+dataset_reduce">dataset_reduce</a>()</code>,
<code><a href="#topic+dataset_repeat">dataset_repeat</a>()</code>,
<code><a href="#topic+dataset_shuffle_and_repeat">dataset_shuffle_and_repeat</a>()</code>,
<code><a href="#topic+dataset_shuffle">dataset_shuffle</a>()</code>,
<code><a href="#topic+dataset_skip">dataset_skip</a>()</code>,
<code><a href="#topic+dataset_take_while">dataset_take_while</a>()</code>,
<code><a href="#topic+dataset_take">dataset_take</a>()</code>,
<code><a href="#topic+dataset_window">dataset_window</a>()</code>
</p>

<hr>
<h2 id='dataset_collect'>Collects a dataset</h2><span id='topic+dataset_collect'></span>

<h3>Description</h3>

<p>Iterates throught the dataset collecting every element into a list.
It's useful for looking at the full result of the dataset.
Note: You may run out of memory if your dataset is too big.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dataset_collect(dataset, iter_max = Inf)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dataset_collect_+3A_dataset">dataset</code></td>
<td>
<p>A dataset</p>
</td></tr>
<tr><td><code id="dataset_collect_+3A_iter_max">iter_max</code></td>
<td>
<p>Maximum number of iterations. <code>Inf</code> until the end of the
dataset</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other dataset methods: 
<code><a href="#topic+dataset_batch">dataset_batch</a>()</code>,
<code><a href="#topic+dataset_cache">dataset_cache</a>()</code>,
<code><a href="#topic+dataset_concatenate">dataset_concatenate</a>()</code>,
<code><a href="#topic+dataset_decode_delim">dataset_decode_delim</a>()</code>,
<code><a href="#topic+dataset_filter">dataset_filter</a>()</code>,
<code><a href="#topic+dataset_interleave">dataset_interleave</a>()</code>,
<code><a href="#topic+dataset_map_and_batch">dataset_map_and_batch</a>()</code>,
<code><a href="#topic+dataset_map">dataset_map</a>()</code>,
<code><a href="#topic+dataset_padded_batch">dataset_padded_batch</a>()</code>,
<code><a href="#topic+dataset_prefetch_to_device">dataset_prefetch_to_device</a>()</code>,
<code><a href="#topic+dataset_prefetch">dataset_prefetch</a>()</code>,
<code><a href="#topic+dataset_reduce">dataset_reduce</a>()</code>,
<code><a href="#topic+dataset_repeat">dataset_repeat</a>()</code>,
<code><a href="#topic+dataset_shuffle_and_repeat">dataset_shuffle_and_repeat</a>()</code>,
<code><a href="#topic+dataset_shuffle">dataset_shuffle</a>()</code>,
<code><a href="#topic+dataset_skip">dataset_skip</a>()</code>,
<code><a href="#topic+dataset_take_while">dataset_take_while</a>()</code>,
<code><a href="#topic+dataset_take">dataset_take</a>()</code>,
<code><a href="#topic+dataset_window">dataset_window</a>()</code>
</p>

<hr>
<h2 id='dataset_concatenate'>Creates a dataset by concatenating given dataset with this dataset.</h2><span id='topic+dataset_concatenate'></span>

<h3>Description</h3>

<p>Creates a dataset by concatenating given dataset with this dataset.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dataset_concatenate(dataset, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dataset_concatenate_+3A_dataset">dataset</code>, <code id="dataset_concatenate_+3A_...">...</code></td>
<td>
<p><code>tf_dataset</code>s to be concatenated</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A dataset
</p>


<h3>Note</h3>

<p>Input dataset and dataset to be concatenated should have same nested
structures and output types.
</p>


<h3>See Also</h3>

<p>Other dataset methods: 
<code><a href="#topic+dataset_batch">dataset_batch</a>()</code>,
<code><a href="#topic+dataset_cache">dataset_cache</a>()</code>,
<code><a href="#topic+dataset_collect">dataset_collect</a>()</code>,
<code><a href="#topic+dataset_decode_delim">dataset_decode_delim</a>()</code>,
<code><a href="#topic+dataset_filter">dataset_filter</a>()</code>,
<code><a href="#topic+dataset_interleave">dataset_interleave</a>()</code>,
<code><a href="#topic+dataset_map_and_batch">dataset_map_and_batch</a>()</code>,
<code><a href="#topic+dataset_map">dataset_map</a>()</code>,
<code><a href="#topic+dataset_padded_batch">dataset_padded_batch</a>()</code>,
<code><a href="#topic+dataset_prefetch_to_device">dataset_prefetch_to_device</a>()</code>,
<code><a href="#topic+dataset_prefetch">dataset_prefetch</a>()</code>,
<code><a href="#topic+dataset_reduce">dataset_reduce</a>()</code>,
<code><a href="#topic+dataset_repeat">dataset_repeat</a>()</code>,
<code><a href="#topic+dataset_shuffle_and_repeat">dataset_shuffle_and_repeat</a>()</code>,
<code><a href="#topic+dataset_shuffle">dataset_shuffle</a>()</code>,
<code><a href="#topic+dataset_skip">dataset_skip</a>()</code>,
<code><a href="#topic+dataset_take_while">dataset_take_while</a>()</code>,
<code><a href="#topic+dataset_take">dataset_take</a>()</code>,
<code><a href="#topic+dataset_window">dataset_window</a>()</code>
</p>

<hr>
<h2 id='dataset_decode_delim'>Transform a dataset with delimted text lines into a dataset with named
columns</h2><span id='topic+dataset_decode_delim'></span>

<h3>Description</h3>

<p>Transform a dataset with delimted text lines into a dataset with named
columns
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dataset_decode_delim(dataset, record_spec, parallel_records = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dataset_decode_delim_+3A_dataset">dataset</code></td>
<td>
<p>Dataset containing delimited text lines (e.g. a CSV)</p>
</td></tr>
<tr><td><code id="dataset_decode_delim_+3A_record_spec">record_spec</code></td>
<td>
<p>Specification of column names and types (see <code><a href="#topic+delim_record_spec">delim_record_spec()</a></code>).</p>
</td></tr>
<tr><td><code id="dataset_decode_delim_+3A_parallel_records">parallel_records</code></td>
<td>
<p>(Optional) An integer, representing the number of
records to decode in parallel. If not specified, records will be
processed sequentially.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other dataset methods: 
<code><a href="#topic+dataset_batch">dataset_batch</a>()</code>,
<code><a href="#topic+dataset_cache">dataset_cache</a>()</code>,
<code><a href="#topic+dataset_collect">dataset_collect</a>()</code>,
<code><a href="#topic+dataset_concatenate">dataset_concatenate</a>()</code>,
<code><a href="#topic+dataset_filter">dataset_filter</a>()</code>,
<code><a href="#topic+dataset_interleave">dataset_interleave</a>()</code>,
<code><a href="#topic+dataset_map_and_batch">dataset_map_and_batch</a>()</code>,
<code><a href="#topic+dataset_map">dataset_map</a>()</code>,
<code><a href="#topic+dataset_padded_batch">dataset_padded_batch</a>()</code>,
<code><a href="#topic+dataset_prefetch_to_device">dataset_prefetch_to_device</a>()</code>,
<code><a href="#topic+dataset_prefetch">dataset_prefetch</a>()</code>,
<code><a href="#topic+dataset_reduce">dataset_reduce</a>()</code>,
<code><a href="#topic+dataset_repeat">dataset_repeat</a>()</code>,
<code><a href="#topic+dataset_shuffle_and_repeat">dataset_shuffle_and_repeat</a>()</code>,
<code><a href="#topic+dataset_shuffle">dataset_shuffle</a>()</code>,
<code><a href="#topic+dataset_skip">dataset_skip</a>()</code>,
<code><a href="#topic+dataset_take_while">dataset_take_while</a>()</code>,
<code><a href="#topic+dataset_take">dataset_take</a>()</code>,
<code><a href="#topic+dataset_window">dataset_window</a>()</code>
</p>

<hr>
<h2 id='dataset_enumerate'>Enumerates the elements of this dataset</h2><span id='topic+dataset_enumerate'></span>

<h3>Description</h3>

<p>Enumerates the elements of this dataset
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dataset_enumerate(dataset, start = 0L)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dataset_enumerate_+3A_dataset">dataset</code></td>
<td>
<p>A tensorflow dataset</p>
</td></tr>
<tr><td><code id="dataset_enumerate_+3A_start">start</code></td>
<td>
<p>An integer (coerced to a <code>tf$int64</code> scalar <code>tf.Tensor</code>),
representing the start value for enumeration.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>It is similar to python's <code>enumerate</code>, this transforms a sequence of
elements into a sequence of <code>list(index, element)</code>, where index is an integer
that indicates the position of the element in the sequence.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
dataset &lt;- tensor_slices_dataset(100:103) %&gt;%
  dataset_enumerate()

iterator &lt;- reticulate::as_iterator(dataset)
reticulate::iter_next(iterator) # list(0, 100)
reticulate::iter_next(iterator) # list(1, 101)
reticulate::iter_next(iterator) # list(2, 102)
reticulate::iter_next(iterator) # list(3, 103)
reticulate::iter_next(iterator) # NULL (iterator exhausted)
reticulate::iter_next(iterator) # NULL (iterator exhausted)

## End(Not run)
</code></pre>

<hr>
<h2 id='dataset_filter'>Filter a dataset by a predicate</h2><span id='topic+dataset_filter'></span>

<h3>Description</h3>

<p>Filter a dataset by a predicate
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dataset_filter(dataset, predicate)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dataset_filter_+3A_dataset">dataset</code></td>
<td>
<p>A dataset</p>
</td></tr>
<tr><td><code id="dataset_filter_+3A_predicate">predicate</code></td>
<td>
<p>A function mapping a nested structure of tensors (having
shapes and types defined by <code><a href="#topic+output_shapes">output_shapes()</a></code> and <code><a href="#topic+output_types">output_types()</a></code> to a
scalar <code>tf$bool</code> tensor.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note that the functions used inside the predicate must be
tensor operations (e.g. <code>tf$not_equal</code>, <code>tf$less</code>, etc.). R
generic methods for relational operators (e.g. <code>&lt;</code>, <code>&gt;</code>, <code>&lt;=</code>,
etc.) and logical operators (e.g. <code>!</code>, <code>&amp;</code>, <code>|</code>, etc.) are
provided so you can use shorthand syntax for most common
comparisions (this is illustrated by the example below).
</p>


<h3>Value</h3>

<p>A dataset composed of records that matched the predicate.
</p>


<h3>See Also</h3>

<p>Other dataset methods: 
<code><a href="#topic+dataset_batch">dataset_batch</a>()</code>,
<code><a href="#topic+dataset_cache">dataset_cache</a>()</code>,
<code><a href="#topic+dataset_collect">dataset_collect</a>()</code>,
<code><a href="#topic+dataset_concatenate">dataset_concatenate</a>()</code>,
<code><a href="#topic+dataset_decode_delim">dataset_decode_delim</a>()</code>,
<code><a href="#topic+dataset_interleave">dataset_interleave</a>()</code>,
<code><a href="#topic+dataset_map_and_batch">dataset_map_and_batch</a>()</code>,
<code><a href="#topic+dataset_map">dataset_map</a>()</code>,
<code><a href="#topic+dataset_padded_batch">dataset_padded_batch</a>()</code>,
<code><a href="#topic+dataset_prefetch_to_device">dataset_prefetch_to_device</a>()</code>,
<code><a href="#topic+dataset_prefetch">dataset_prefetch</a>()</code>,
<code><a href="#topic+dataset_reduce">dataset_reduce</a>()</code>,
<code><a href="#topic+dataset_repeat">dataset_repeat</a>()</code>,
<code><a href="#topic+dataset_shuffle_and_repeat">dataset_shuffle_and_repeat</a>()</code>,
<code><a href="#topic+dataset_shuffle">dataset_shuffle</a>()</code>,
<code><a href="#topic+dataset_skip">dataset_skip</a>()</code>,
<code><a href="#topic+dataset_take_while">dataset_take_while</a>()</code>,
<code><a href="#topic+dataset_take">dataset_take</a>()</code>,
<code><a href="#topic+dataset_window">dataset_window</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

dataset &lt;- text_line_dataset("mtcars.csv", record_spec = mtcars_spec) %&gt;%
  dataset_filter(function(record) {
    record$mpg &gt;= 20
})

dataset &lt;- text_line_dataset("mtcars.csv", record_spec = mtcars_spec) %&gt;%
  dataset_filter(function(record) {
    record$mpg &gt;= 20 &amp; record$cyl &gt;= 6L
  })


## End(Not run)

</code></pre>

<hr>
<h2 id='dataset_flat_map'>Maps map_func across this dataset and flattens the result.</h2><span id='topic+dataset_flat_map'></span>

<h3>Description</h3>

<p>Maps map_func across this dataset and flattens the result.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dataset_flat_map(dataset, map_func)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dataset_flat_map_+3A_dataset">dataset</code></td>
<td>
<p>A dataset</p>
</td></tr>
<tr><td><code id="dataset_flat_map_+3A_map_func">map_func</code></td>
<td>
<p>A function mapping a nested structure of tensors (having
shapes and types defined by <code><a href="#topic+output_shapes">output_shapes()</a></code> and <code><a href="#topic+output_types">output_types()</a></code> to a
dataset.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A dataset
</p>

<hr>
<h2 id='dataset_group_by_window'>Group windows of elements by key and reduce them</h2><span id='topic+dataset_group_by_window'></span>

<h3>Description</h3>

<p>Group windows of elements by key and reduce them
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dataset_group_by_window(
  dataset,
  key_func,
  reduce_func,
  window_size = NULL,
  window_size_func = NULL,
  name = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dataset_group_by_window_+3A_dataset">dataset</code></td>
<td>
<p>a TF Dataset</p>
</td></tr>
<tr><td><code id="dataset_group_by_window_+3A_key_func">key_func</code></td>
<td>
<p>A function mapping a nested structure of tensors (having
shapes and types defined by <code>self$output_shapes</code> and <code>self$output_types</code>)
to a scalar <code>tf.int64</code> tensor.</p>
</td></tr>
<tr><td><code id="dataset_group_by_window_+3A_reduce_func">reduce_func</code></td>
<td>
<p>A function mapping a key and a dataset of up to
<code>window_size</code> consecutive elements matching that key to another dataset.</p>
</td></tr>
<tr><td><code id="dataset_group_by_window_+3A_window_size">window_size</code></td>
<td>
<p>A <code>tf.int64</code> scalar <code>tf.Tensor</code>, representing the number
of consecutive elements matching the same key to combine in a single batch,
which will be passed to <code>reduce_func</code>. Mutually exclusive with
<code>window_size_func</code>.</p>
</td></tr>
<tr><td><code id="dataset_group_by_window_+3A_window_size_func">window_size_func</code></td>
<td>
<p>A function mapping a key to a <code>tf.int64</code> scalar
<code>tf.Tensor</code>, representing the number of consecutive elements matching the
same key to combine in a single batch, which will be passed to
<code>reduce_func</code>. Mutually exclusive with <code>window_size</code>.</p>
</td></tr>
<tr><td><code id="dataset_group_by_window_+3A_name">name</code></td>
<td>
<p>(Optional.) A name for the Tensorflow operation.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This transformation maps each consecutive element in a dataset to a
key using <code>key_func()</code> and groups the elements by key. It then applies
<code>reduce_func()</code> to at most <code>window_size_func(key)</code> elements matching the same
key. All except the final window for each key will contain
<code>window_size_func(key)</code> elements; the final window may be smaller.
</p>
<p>You may provide either a constant <code>window_size</code> or a window size determined
by the key through <code>window_size_func</code>.
</p>
<div class="sourceCode r"><pre>window_size &lt;-  5
dataset &lt;- range_dataset(to = 10) %&gt;%
  dataset_group_by_window(
    key_func = function(x) x %% 2,
    reduce_func = function(key, ds) dataset_batch(ds, window_size),
    window_size = window_size
  )

it &lt;- as_array_iterator(dataset)
while (!is.null(elem &lt;- iter_next(it)))
  print(elem)
#&gt; tf.Tensor([0 2 4 6 8], shape=(5), dtype=int64)
#&gt; tf.Tensor([1 3 5 7 9], shape=(5), dtype=int64)
</pre></div>


<h3>See Also</h3>


<ul>
<li> <p><a href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset#group_by_window">https://www.tensorflow.org/api_docs/python/tf/data/Dataset#group_by_window</a>
</p>
</li></ul>


<hr>
<h2 id='dataset_interleave'>Maps map_func across this dataset, and interleaves the results</h2><span id='topic+dataset_interleave'></span>

<h3>Description</h3>

<p>Maps map_func across this dataset, and interleaves the results
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dataset_interleave(dataset, map_func, cycle_length, block_length = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dataset_interleave_+3A_dataset">dataset</code></td>
<td>
<p>A dataset</p>
</td></tr>
<tr><td><code id="dataset_interleave_+3A_map_func">map_func</code></td>
<td>
<p>A function mapping a nested structure of tensors (having
shapes and types defined by <code><a href="#topic+output_shapes">output_shapes()</a></code> and <code><a href="#topic+output_types">output_types()</a></code> to a
dataset.</p>
</td></tr>
<tr><td><code id="dataset_interleave_+3A_cycle_length">cycle_length</code></td>
<td>
<p>The number of elements from this dataset that will be
processed concurrently.</p>
</td></tr>
<tr><td><code id="dataset_interleave_+3A_block_length">block_length</code></td>
<td>
<p>The number of consecutive elements to produce from each
input element before cycling to another input element.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>cycle_length</code> and <code>block_length</code> arguments control the order in which
elements are produced. <code>cycle_length</code> controls the number of input elements
that are processed concurrently. In general, this transformation will apply
<code>map_func</code> to <code>cycle_length</code> input elements, open iterators on the returned
dataset objects, and cycle through them producing <code>block_length</code> consecutive
elements from each iterator, and consuming the next input element each time
it reaches the end of an iterator.
</p>


<h3>See Also</h3>

<p>Other dataset methods: 
<code><a href="#topic+dataset_batch">dataset_batch</a>()</code>,
<code><a href="#topic+dataset_cache">dataset_cache</a>()</code>,
<code><a href="#topic+dataset_collect">dataset_collect</a>()</code>,
<code><a href="#topic+dataset_concatenate">dataset_concatenate</a>()</code>,
<code><a href="#topic+dataset_decode_delim">dataset_decode_delim</a>()</code>,
<code><a href="#topic+dataset_filter">dataset_filter</a>()</code>,
<code><a href="#topic+dataset_map_and_batch">dataset_map_and_batch</a>()</code>,
<code><a href="#topic+dataset_map">dataset_map</a>()</code>,
<code><a href="#topic+dataset_padded_batch">dataset_padded_batch</a>()</code>,
<code><a href="#topic+dataset_prefetch_to_device">dataset_prefetch_to_device</a>()</code>,
<code><a href="#topic+dataset_prefetch">dataset_prefetch</a>()</code>,
<code><a href="#topic+dataset_reduce">dataset_reduce</a>()</code>,
<code><a href="#topic+dataset_repeat">dataset_repeat</a>()</code>,
<code><a href="#topic+dataset_shuffle_and_repeat">dataset_shuffle_and_repeat</a>()</code>,
<code><a href="#topic+dataset_shuffle">dataset_shuffle</a>()</code>,
<code><a href="#topic+dataset_skip">dataset_skip</a>()</code>,
<code><a href="#topic+dataset_take_while">dataset_take_while</a>()</code>,
<code><a href="#topic+dataset_take">dataset_take</a>()</code>,
<code><a href="#topic+dataset_window">dataset_window</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

dataset &lt;- tensor_slices_dataset(c(1,2,3,4,5)) %&gt;%
 dataset_interleave(cycle_length = 2, block_length = 4, function(x) {
   tensors_dataset(x) %&gt;%
     dataset_repeat(6)
 })

# resulting dataset (newlines indicate "block" boundaries):
c(1, 1, 1, 1,
  2, 2, 2, 2,
  1, 1,
  2, 2,
  3, 3, 3, 3,
  4, 4, 4, 4,
  3, 3,
  4, 4,
  5, 5, 5, 5,
  5, 5,
)


## End(Not run)

</code></pre>

<hr>
<h2 id='dataset_map'>Map a function across a dataset.</h2><span id='topic+dataset_map'></span>

<h3>Description</h3>

<p>Map a function across a dataset.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dataset_map(dataset, map_func, num_parallel_calls = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dataset_map_+3A_dataset">dataset</code></td>
<td>
<p>A dataset</p>
</td></tr>
<tr><td><code id="dataset_map_+3A_map_func">map_func</code></td>
<td>
<p>A function mapping a nested structure of tensors (having
shapes and types defined by <code><a href="#topic+output_shapes">output_shapes()</a></code> and <code><a href="#topic+output_types">output_types()</a></code> to
another nested structure of tensors. It also supports <code>purrr</code> style
lambda functions powered by <code><a href="rlang.html#topic+as_function">rlang::as_function()</a></code>.</p>
</td></tr>
<tr><td><code id="dataset_map_+3A_num_parallel_calls">num_parallel_calls</code></td>
<td>
<p>(Optional) An integer, representing the
number of elements to process in parallel If not specified, elements will
be processed sequentially.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A dataset
</p>


<h3>See Also</h3>

<p>Other dataset methods: 
<code><a href="#topic+dataset_batch">dataset_batch</a>()</code>,
<code><a href="#topic+dataset_cache">dataset_cache</a>()</code>,
<code><a href="#topic+dataset_collect">dataset_collect</a>()</code>,
<code><a href="#topic+dataset_concatenate">dataset_concatenate</a>()</code>,
<code><a href="#topic+dataset_decode_delim">dataset_decode_delim</a>()</code>,
<code><a href="#topic+dataset_filter">dataset_filter</a>()</code>,
<code><a href="#topic+dataset_interleave">dataset_interleave</a>()</code>,
<code><a href="#topic+dataset_map_and_batch">dataset_map_and_batch</a>()</code>,
<code><a href="#topic+dataset_padded_batch">dataset_padded_batch</a>()</code>,
<code><a href="#topic+dataset_prefetch_to_device">dataset_prefetch_to_device</a>()</code>,
<code><a href="#topic+dataset_prefetch">dataset_prefetch</a>()</code>,
<code><a href="#topic+dataset_reduce">dataset_reduce</a>()</code>,
<code><a href="#topic+dataset_repeat">dataset_repeat</a>()</code>,
<code><a href="#topic+dataset_shuffle_and_repeat">dataset_shuffle_and_repeat</a>()</code>,
<code><a href="#topic+dataset_shuffle">dataset_shuffle</a>()</code>,
<code><a href="#topic+dataset_skip">dataset_skip</a>()</code>,
<code><a href="#topic+dataset_take_while">dataset_take_while</a>()</code>,
<code><a href="#topic+dataset_take">dataset_take</a>()</code>,
<code><a href="#topic+dataset_window">dataset_window</a>()</code>
</p>

<hr>
<h2 id='dataset_map_and_batch'>Fused implementation of dataset_map() and dataset_batch()</h2><span id='topic+dataset_map_and_batch'></span>

<h3>Description</h3>

<p>Maps 'map_func&ldquo; across batch_size consecutive elements of this dataset and then combines
them into a batch. Functionally, it is equivalent to map followed by batch. However, by
fusing the two transformations together, the implementation can be more efficient.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dataset_map_and_batch(
  dataset,
  map_func,
  batch_size,
  num_parallel_batches = NULL,
  drop_remainder = FALSE,
  num_parallel_calls = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dataset_map_and_batch_+3A_dataset">dataset</code></td>
<td>
<p>A dataset</p>
</td></tr>
<tr><td><code id="dataset_map_and_batch_+3A_map_func">map_func</code></td>
<td>
<p>A function mapping a nested structure of tensors (having
shapes and types defined by <code><a href="#topic+output_shapes">output_shapes()</a></code> and <code><a href="#topic+output_types">output_types()</a></code> to
another nested structure of tensors. It also supports <code>purrr</code> style
lambda functions powered by <code><a href="rlang.html#topic+as_function">rlang::as_function()</a></code>.</p>
</td></tr>
<tr><td><code id="dataset_map_and_batch_+3A_batch_size">batch_size</code></td>
<td>
<p>An integer, representing the number of consecutive elements
of this dataset to combine in a single batch.</p>
</td></tr>
<tr><td><code id="dataset_map_and_batch_+3A_num_parallel_batches">num_parallel_batches</code></td>
<td>
<p>(Optional) An integer, representing the number of batches
to create in parallel. On one hand, higher values can help mitigate the effect of
stragglers. On the other hand, higher values can increase contention if CPU is
scarce.</p>
</td></tr>
<tr><td><code id="dataset_map_and_batch_+3A_drop_remainder">drop_remainder</code></td>
<td>
<p>(Optional.) A boolean, representing whether the last
batch should be dropped in the case it has fewer than <code>batch_size</code>
elements; the default behavior is not to drop the smaller batch.</p>
</td></tr>
<tr><td><code id="dataset_map_and_batch_+3A_num_parallel_calls">num_parallel_calls</code></td>
<td>
<p>(Optional) An integer, representing the
number of elements to process in parallel If not specified, elements will
be processed sequentially.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other dataset methods: 
<code><a href="#topic+dataset_batch">dataset_batch</a>()</code>,
<code><a href="#topic+dataset_cache">dataset_cache</a>()</code>,
<code><a href="#topic+dataset_collect">dataset_collect</a>()</code>,
<code><a href="#topic+dataset_concatenate">dataset_concatenate</a>()</code>,
<code><a href="#topic+dataset_decode_delim">dataset_decode_delim</a>()</code>,
<code><a href="#topic+dataset_filter">dataset_filter</a>()</code>,
<code><a href="#topic+dataset_interleave">dataset_interleave</a>()</code>,
<code><a href="#topic+dataset_map">dataset_map</a>()</code>,
<code><a href="#topic+dataset_padded_batch">dataset_padded_batch</a>()</code>,
<code><a href="#topic+dataset_prefetch_to_device">dataset_prefetch_to_device</a>()</code>,
<code><a href="#topic+dataset_prefetch">dataset_prefetch</a>()</code>,
<code><a href="#topic+dataset_reduce">dataset_reduce</a>()</code>,
<code><a href="#topic+dataset_repeat">dataset_repeat</a>()</code>,
<code><a href="#topic+dataset_shuffle_and_repeat">dataset_shuffle_and_repeat</a>()</code>,
<code><a href="#topic+dataset_shuffle">dataset_shuffle</a>()</code>,
<code><a href="#topic+dataset_skip">dataset_skip</a>()</code>,
<code><a href="#topic+dataset_take_while">dataset_take_while</a>()</code>,
<code><a href="#topic+dataset_take">dataset_take</a>()</code>,
<code><a href="#topic+dataset_window">dataset_window</a>()</code>
</p>

<hr>
<h2 id='dataset_options'>Get or Set Dataset Options</h2><span id='topic+dataset_options'></span>

<h3>Description</h3>

<p>Get or Set Dataset Options
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dataset_options(dataset, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dataset_options_+3A_dataset">dataset</code></td>
<td>
<p>a tensorflow dataset</p>
</td></tr>
<tr><td><code id="dataset_options_+3A_...">...</code></td>
<td>
<p>Valid values include:
</p>

<ul>
<li><p> A set of named arguments setting options. Names of nested attributes can
be separated with a <code>"."</code> (see examples). The set of named arguments can be
supplied individually to <code>...</code>, or as a single named list.
</p>
</li>
<li><p> a <code>tf$data$Options()</code> instance.
</p>
</li></ul>
</td></tr>
</table>


<h3>Details</h3>

<p>The options are &quot;global&quot; in the sense they apply to the entire
dataset. If options are set multiple times, they are merged as long as
different options do not use different non-default values.
</p>


<h3>Value</h3>

<p>If values are supplied to <code>...</code>, returns a <code>tf.data.Dataset</code> with the
given options set/updated. Otherwise, returns the currently set options for
the dataset.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# pass options directly:
range_dataset(0, 10) %&gt;%
  dataset_options(
    experimental_deterministic = FALSE,
    threading.private_threadpool_size = 10
  )

# pass options as a named list:
opts &lt;- list(
  experimental_deterministic = FALSE,
  threading.private_threadpool_size = 10
)
range_dataset(0, 10) %&gt;%
  dataset_options(opts)

# pass a tf.data.Options() instance
opts &lt;- tf$data$Options()
opts$experimental_deterministic &lt;- FALSE
opts$threading$private_threadpool_size &lt;- 10L
range_dataset(0, 10) %&gt;%
  dataset_options(opts)

# get currently set options
range_dataset(0, 10) %&gt;% dataset_options()

## End(Not run)
</code></pre>

<hr>
<h2 id='dataset_padded_batch'>Combines consecutive elements of this dataset into padded batches.</h2><span id='topic+dataset_padded_batch'></span>

<h3>Description</h3>

<p>Combines consecutive elements of this dataset into padded batches.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dataset_padded_batch(
  dataset,
  batch_size,
  padded_shapes = NULL,
  padding_values = NULL,
  drop_remainder = FALSE,
  name = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dataset_padded_batch_+3A_dataset">dataset</code></td>
<td>
<p>A dataset</p>
</td></tr>
<tr><td><code id="dataset_padded_batch_+3A_batch_size">batch_size</code></td>
<td>
<p>An integer, representing the number of
consecutive elements of this dataset to combine in a single batch.</p>
</td></tr>
<tr><td><code id="dataset_padded_batch_+3A_padded_shapes">padded_shapes</code></td>
<td>
<p>(Optional.) A (nested) structure of
<code>tf.TensorShape</code> (returned by <code><a href="tensorflow.html#topic+shape">tensorflow::shape()</a></code>) or
<code>tf$int64</code> vector tensor-like objects representing the shape to which
the respective component of each input element should be padded prior
to batching. Any unknown dimensions will be padded to the maximum size
of that dimension in each batch. If unset, all dimensions of all
components are padded to the maximum size in the batch. <code>padded_shapes</code>
must be set if any component has an unknown rank.</p>
</td></tr>
<tr><td><code id="dataset_padded_batch_+3A_padding_values">padding_values</code></td>
<td>
<p>(Optional.) A (nested) structure of scalar-shaped
<code>tf.Tensor</code>, representing the padding values to use for the respective
components. <code>NULL</code> represents that the (nested) structure should be padded
with default values.  Defaults are <code>0</code> for numeric types and the empty
string <code>""</code> for string types. The <code>padding_values</code> should have the same
(nested) structure as the input dataset. If <code>padding_values</code> is a single
element and the input dataset has multiple components, then the same
<code>padding_values</code> will be used to pad every component of the dataset.
If <code>padding_values</code> is a scalar, then its value will be broadcasted
to match the shape of each component.</p>
</td></tr>
<tr><td><code id="dataset_padded_batch_+3A_drop_remainder">drop_remainder</code></td>
<td>
<p>(Optional.) A boolean scalar, representing
whether the last batch should be dropped in the case it has fewer than
<code>batch_size</code> elements; the default behavior is not to drop the smaller
batch.</p>
</td></tr>
<tr><td><code id="dataset_padded_batch_+3A_name">name</code></td>
<td>
<p>(Optional.) A name for the tf.data operation. Requires tensorflow version &gt;= 2.7.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This transformation combines multiple consecutive elements of the input
dataset into a single element.
</p>
<p>Like <code><a href="#topic+dataset_batch">dataset_batch()</a></code>, the components of the resulting element will
have an additional outer dimension, which will be <code>batch_size</code> (or
<code>N %% batch_size</code> for the last element if <code>batch_size</code> does not divide the
number of input elements <code>N</code> evenly and <code>drop_remainder</code> is <code>FALSE</code>). If
your program depends on the batches having the same outer dimension, you
should set the <code>drop_remainder</code> argument to <code>TRUE</code> to prevent the smaller
batch from being produced.
</p>
<p>Unlike <code><a href="#topic+dataset_batch">dataset_batch()</a></code>, the input elements to be batched may have
different shapes, and this transformation will pad each component to the
respective shape in <code>padded_shapes</code>. The <code>padded_shapes</code> argument
determines the resulting shape for each dimension of each component in an
output element:
</p>

<ul>
<li><p> If the dimension is a constant, the component will be padded out to that
length in that dimension.
</p>
</li>
<li><p> If the dimension is unknown, the component will be padded out to the
maximum length of all elements in that dimension.
</p>
</li></ul>

<p>See also <code>tf$data$experimental$dense_to_sparse_batch</code>, which combines
elements that may have different shapes into a <code>tf$sparse$SparseTensor</code>.
</p>


<h3>Value</h3>

<p>A tf_dataset
</p>


<h3>See Also</h3>


<ul>
<li> <p><a href="https://www.tensorflow.org/api_docs/python/tf/data/Dataset#padded_batch">https://www.tensorflow.org/api_docs/python/tf/data/Dataset#padded_batch</a>
</p>
</li></ul>

<p>Other dataset methods: 
<code><a href="#topic+dataset_batch">dataset_batch</a>()</code>,
<code><a href="#topic+dataset_cache">dataset_cache</a>()</code>,
<code><a href="#topic+dataset_collect">dataset_collect</a>()</code>,
<code><a href="#topic+dataset_concatenate">dataset_concatenate</a>()</code>,
<code><a href="#topic+dataset_decode_delim">dataset_decode_delim</a>()</code>,
<code><a href="#topic+dataset_filter">dataset_filter</a>()</code>,
<code><a href="#topic+dataset_interleave">dataset_interleave</a>()</code>,
<code><a href="#topic+dataset_map_and_batch">dataset_map_and_batch</a>()</code>,
<code><a href="#topic+dataset_map">dataset_map</a>()</code>,
<code><a href="#topic+dataset_prefetch_to_device">dataset_prefetch_to_device</a>()</code>,
<code><a href="#topic+dataset_prefetch">dataset_prefetch</a>()</code>,
<code><a href="#topic+dataset_reduce">dataset_reduce</a>()</code>,
<code><a href="#topic+dataset_repeat">dataset_repeat</a>()</code>,
<code><a href="#topic+dataset_shuffle_and_repeat">dataset_shuffle_and_repeat</a>()</code>,
<code><a href="#topic+dataset_shuffle">dataset_shuffle</a>()</code>,
<code><a href="#topic+dataset_skip">dataset_skip</a>()</code>,
<code><a href="#topic+dataset_take_while">dataset_take_while</a>()</code>,
<code><a href="#topic+dataset_take">dataset_take</a>()</code>,
<code><a href="#topic+dataset_window">dataset_window</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
A &lt;- range_dataset(1, 5, dtype = tf$int32) %&gt;%
  dataset_map(function(x) tf$fill(list(x), x))

# Pad to the smallest per-batch size that fits all elements.
B &lt;- A %&gt;% dataset_padded_batch(2)
B %&gt;% as_array_iterator() %&gt;% iterate(print)

# Pad to a fixed size.
C &lt;- A %&gt;% dataset_padded_batch(2, padded_shapes=5)
C %&gt;% as_array_iterator() %&gt;% iterate(print)

# Pad with a custom value.
D &lt;- A %&gt;% dataset_padded_batch(2, padded_shapes=5, padding_values = -1L)
D %&gt;% as_array_iterator() %&gt;% iterate(print)

# Pad with a single value and multiple components.
E &lt;- zip_datasets(A, A) %&gt;%  dataset_padded_batch(2, padding_values = -1L)
E %&gt;% as_array_iterator() %&gt;% iterate(print)

## End(Not run)
</code></pre>

<hr>
<h2 id='dataset_prefetch'>Creates a Dataset that prefetches elements from this dataset.</h2><span id='topic+dataset_prefetch'></span>

<h3>Description</h3>

<p>Creates a Dataset that prefetches elements from this dataset.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dataset_prefetch(dataset, buffer_size = tf$data$AUTOTUNE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dataset_prefetch_+3A_dataset">dataset</code></td>
<td>
<p>A dataset</p>
</td></tr>
<tr><td><code id="dataset_prefetch_+3A_buffer_size">buffer_size</code></td>
<td>
<p>An integer, representing the maximum number elements that
will be buffered when prefetching.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A dataset
</p>


<h3>See Also</h3>

<p>Other dataset methods: 
<code><a href="#topic+dataset_batch">dataset_batch</a>()</code>,
<code><a href="#topic+dataset_cache">dataset_cache</a>()</code>,
<code><a href="#topic+dataset_collect">dataset_collect</a>()</code>,
<code><a href="#topic+dataset_concatenate">dataset_concatenate</a>()</code>,
<code><a href="#topic+dataset_decode_delim">dataset_decode_delim</a>()</code>,
<code><a href="#topic+dataset_filter">dataset_filter</a>()</code>,
<code><a href="#topic+dataset_interleave">dataset_interleave</a>()</code>,
<code><a href="#topic+dataset_map_and_batch">dataset_map_and_batch</a>()</code>,
<code><a href="#topic+dataset_map">dataset_map</a>()</code>,
<code><a href="#topic+dataset_padded_batch">dataset_padded_batch</a>()</code>,
<code><a href="#topic+dataset_prefetch_to_device">dataset_prefetch_to_device</a>()</code>,
<code><a href="#topic+dataset_reduce">dataset_reduce</a>()</code>,
<code><a href="#topic+dataset_repeat">dataset_repeat</a>()</code>,
<code><a href="#topic+dataset_shuffle_and_repeat">dataset_shuffle_and_repeat</a>()</code>,
<code><a href="#topic+dataset_shuffle">dataset_shuffle</a>()</code>,
<code><a href="#topic+dataset_skip">dataset_skip</a>()</code>,
<code><a href="#topic+dataset_take_while">dataset_take_while</a>()</code>,
<code><a href="#topic+dataset_take">dataset_take</a>()</code>,
<code><a href="#topic+dataset_window">dataset_window</a>()</code>
</p>

<hr>
<h2 id='dataset_prefetch_to_device'>A transformation that prefetches dataset values to the given <code>device</code></h2><span id='topic+dataset_prefetch_to_device'></span>

<h3>Description</h3>

<p>A transformation that prefetches dataset values to the given <code>device</code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dataset_prefetch_to_device(dataset, device, buffer_size = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dataset_prefetch_to_device_+3A_dataset">dataset</code></td>
<td>
<p>A dataset</p>
</td></tr>
<tr><td><code id="dataset_prefetch_to_device_+3A_device">device</code></td>
<td>
<p>A string. The name of a device to which elements will be prefetched
(e.g. &quot;/gpu:0&quot;).</p>
</td></tr>
<tr><td><code id="dataset_prefetch_to_device_+3A_buffer_size">buffer_size</code></td>
<td>
<p>(Optional.) The number of elements to buffer on device.
Defaults to an automatically chosen value.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A dataset
</p>


<h3>Note</h3>

<p>Although the transformation creates a dataset, the transformation must be the
final dataset in the input pipeline.
</p>


<h3>See Also</h3>

<p>Other dataset methods: 
<code><a href="#topic+dataset_batch">dataset_batch</a>()</code>,
<code><a href="#topic+dataset_cache">dataset_cache</a>()</code>,
<code><a href="#topic+dataset_collect">dataset_collect</a>()</code>,
<code><a href="#topic+dataset_concatenate">dataset_concatenate</a>()</code>,
<code><a href="#topic+dataset_decode_delim">dataset_decode_delim</a>()</code>,
<code><a href="#topic+dataset_filter">dataset_filter</a>()</code>,
<code><a href="#topic+dataset_interleave">dataset_interleave</a>()</code>,
<code><a href="#topic+dataset_map_and_batch">dataset_map_and_batch</a>()</code>,
<code><a href="#topic+dataset_map">dataset_map</a>()</code>,
<code><a href="#topic+dataset_padded_batch">dataset_padded_batch</a>()</code>,
<code><a href="#topic+dataset_prefetch">dataset_prefetch</a>()</code>,
<code><a href="#topic+dataset_reduce">dataset_reduce</a>()</code>,
<code><a href="#topic+dataset_repeat">dataset_repeat</a>()</code>,
<code><a href="#topic+dataset_shuffle_and_repeat">dataset_shuffle_and_repeat</a>()</code>,
<code><a href="#topic+dataset_shuffle">dataset_shuffle</a>()</code>,
<code><a href="#topic+dataset_skip">dataset_skip</a>()</code>,
<code><a href="#topic+dataset_take_while">dataset_take_while</a>()</code>,
<code><a href="#topic+dataset_take">dataset_take</a>()</code>,
<code><a href="#topic+dataset_window">dataset_window</a>()</code>
</p>

<hr>
<h2 id='dataset_prepare'>Prepare a dataset for analysis</h2><span id='topic+dataset_prepare'></span>

<h3>Description</h3>

<p>Transform a dataset with named columns into a list with features (<code>x</code>) and
response (<code>y</code>) elements.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dataset_prepare(
  dataset,
  x,
  y = NULL,
  named = TRUE,
  named_features = FALSE,
  parallel_records = NULL,
  batch_size = NULL,
  num_parallel_batches = NULL,
  drop_remainder = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dataset_prepare_+3A_dataset">dataset</code></td>
<td>
<p>A dataset</p>
</td></tr>
<tr><td><code id="dataset_prepare_+3A_x">x</code></td>
<td>
<p>Features to include. When <code>named_features</code> is <code>FALSE</code> all features
will be stacked into a single tensor so must have an identical data type.</p>
</td></tr>
<tr><td><code id="dataset_prepare_+3A_y">y</code></td>
<td>
<p>(Optional). Response variable.</p>
</td></tr>
<tr><td><code id="dataset_prepare_+3A_named">named</code></td>
<td>
<p><code>TRUE</code> to name the dataset elements &quot;x&quot; and &quot;y&quot;, <code>FALSE</code> to
not name the dataset elements.</p>
</td></tr>
<tr><td><code id="dataset_prepare_+3A_named_features">named_features</code></td>
<td>
<p><code>TRUE</code> to yield features as a named list; <code>FALSE</code> to
stack features into a single array. Note that in the case of <code>FALSE</code> (the
default) all features will be stacked into a single 2D tensor so need to
have the same underlying data type.</p>
</td></tr>
<tr><td><code id="dataset_prepare_+3A_parallel_records">parallel_records</code></td>
<td>
<p>(Optional) An integer, representing the number of
records to decode in parallel. If not specified, records will be
processed sequentially.</p>
</td></tr>
<tr><td><code id="dataset_prepare_+3A_batch_size">batch_size</code></td>
<td>
<p>(Optional). Batch size if you would like to fuse the
<code>dataset_prepare()</code> operation together with a <code>dataset_batch()</code> (fusing
generally improves overall training performance).</p>
</td></tr>
<tr><td><code id="dataset_prepare_+3A_num_parallel_batches">num_parallel_batches</code></td>
<td>
<p>(Optional) An integer, representing the number of batches
to create in parallel. On one hand, higher values can help mitigate the effect of
stragglers. On the other hand, higher values can increase contention if CPU is
scarce.</p>
</td></tr>
<tr><td><code id="dataset_prepare_+3A_drop_remainder">drop_remainder</code></td>
<td>
<p>(Optional.) A boolean, representing whether the last
batch should be dropped in the case it has fewer than <code>batch_size</code>
elements; the default behavior is not to drop the smaller batch.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A dataset. The dataset will have a structure of either:
</p>

<ul>
<li><p> When <code>named_features</code> is <code>TRUE</code>: <code>list(x = list(feature_name = feature_values, ...), y = response_values)</code>
</p>
</li>
<li><p> When <code>named_features</code> is <code>FALSE</code>: <code>list(x = features_array, y = response_values)</code>,
where <code>features_array</code> is a Rank 2 array of <code style="white-space: pre;">&#8288;(batch_size, num_features)&#8288;</code>.
</p>
</li></ul>

<p>Note that the <code>y</code> element will be omitted when <code>y</code> is <code>NULL</code>.
</p>


<h3>See Also</h3>

<p><a href="#topic+input_fn.tf_dataset">input_fn()</a> for use with <span class="pkg">tfestimators</span>.
</p>

<hr>
<h2 id='dataset_reduce'>Reduces the input dataset to a single element.</h2><span id='topic+dataset_reduce'></span>

<h3>Description</h3>

<p>The transformation calls reduce_func successively on every element of the input dataset
until the dataset is exhausted, aggregating information in its internal state.
The initial_state argument is used for the initial state and the final state is returned as the result.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dataset_reduce(dataset, initial_state, reduce_func)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dataset_reduce_+3A_dataset">dataset</code></td>
<td>
<p>A dataset</p>
</td></tr>
<tr><td><code id="dataset_reduce_+3A_initial_state">initial_state</code></td>
<td>
<p>An element representing the initial state of the transformation.</p>
</td></tr>
<tr><td><code id="dataset_reduce_+3A_reduce_func">reduce_func</code></td>
<td>
<p>A function that maps <code style="white-space: pre;">&#8288;(old_state, input_element)&#8288;</code> to new_state.
It must take two arguments and return a new element.
The structure of new_state must match the structure of initial_state.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A dataset element.
</p>


<h3>See Also</h3>

<p>Other dataset methods: 
<code><a href="#topic+dataset_batch">dataset_batch</a>()</code>,
<code><a href="#topic+dataset_cache">dataset_cache</a>()</code>,
<code><a href="#topic+dataset_collect">dataset_collect</a>()</code>,
<code><a href="#topic+dataset_concatenate">dataset_concatenate</a>()</code>,
<code><a href="#topic+dataset_decode_delim">dataset_decode_delim</a>()</code>,
<code><a href="#topic+dataset_filter">dataset_filter</a>()</code>,
<code><a href="#topic+dataset_interleave">dataset_interleave</a>()</code>,
<code><a href="#topic+dataset_map_and_batch">dataset_map_and_batch</a>()</code>,
<code><a href="#topic+dataset_map">dataset_map</a>()</code>,
<code><a href="#topic+dataset_padded_batch">dataset_padded_batch</a>()</code>,
<code><a href="#topic+dataset_prefetch_to_device">dataset_prefetch_to_device</a>()</code>,
<code><a href="#topic+dataset_prefetch">dataset_prefetch</a>()</code>,
<code><a href="#topic+dataset_repeat">dataset_repeat</a>()</code>,
<code><a href="#topic+dataset_shuffle_and_repeat">dataset_shuffle_and_repeat</a>()</code>,
<code><a href="#topic+dataset_shuffle">dataset_shuffle</a>()</code>,
<code><a href="#topic+dataset_skip">dataset_skip</a>()</code>,
<code><a href="#topic+dataset_take_while">dataset_take_while</a>()</code>,
<code><a href="#topic+dataset_take">dataset_take</a>()</code>,
<code><a href="#topic+dataset_window">dataset_window</a>()</code>
</p>

<hr>
<h2 id='dataset_rejection_resample'>A transformation that resamples a dataset to a target distribution.</h2><span id='topic+dataset_rejection_resample'></span>

<h3>Description</h3>

<p>A transformation that resamples a dataset to a target distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dataset_rejection_resample(
  dataset,
  class_func,
  target_dist,
  initial_dist = NULL,
  seed = NULL,
  name = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dataset_rejection_resample_+3A_dataset">dataset</code></td>
<td>
<p>A <code>tf.Dataset</code></p>
</td></tr>
<tr><td><code id="dataset_rejection_resample_+3A_class_func">class_func</code></td>
<td>
<p>A function mapping an element of the input dataset to a
scalar <code>tf.int32</code> tensor. Values should be in <code style="white-space: pre;">&#8288;[0, num_classes)&#8288;</code>.</p>
</td></tr>
<tr><td><code id="dataset_rejection_resample_+3A_target_dist">target_dist</code></td>
<td>
<p>A floating point type tensor, shaped <code style="white-space: pre;">&#8288;[num_classes]&#8288;</code>.</p>
</td></tr>
<tr><td><code id="dataset_rejection_resample_+3A_initial_dist">initial_dist</code></td>
<td>
<p>(Optional.) A floating point type tensor, shaped
<code style="white-space: pre;">&#8288;[num_classes]&#8288;</code>. If not provided, the true class distribution is estimated
live in a streaming fashion.</p>
</td></tr>
<tr><td><code id="dataset_rejection_resample_+3A_seed">seed</code></td>
<td>
<p>(Optional.) Integer seed for the resampler.</p>
</td></tr>
<tr><td><code id="dataset_rejection_resample_+3A_name">name</code></td>
<td>
<p>(Optional.) A name for the tf.data operation.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>tf.Dataset</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
initial_dist &lt;- c(.5, .5)
target_dist &lt;- c(.6, .4)
num_classes &lt;- length(initial_dist)
num_samples &lt;- 100000
data &lt;- sample.int(num_classes, num_samples, prob = initial_dist, replace = TRUE)
dataset &lt;- tensor_slices_dataset(data)
tally &lt;- c(0, 0)
`add&lt;-` &lt;- function (x, value) x + value
# tfautograph::autograph({
#   for(i in dataset)
#     add(tally[as.numeric(i)]) &lt;- 1
# })
dataset %&gt;%
  as_array_iterator() %&gt;%
  iterate(function(i) {
    add(tally[i]) &lt;&lt;- 1
  }, simplify = FALSE)
# The value of `tally` will be close to c(50000, 50000) as
# per the `initial_dist` distribution.
tally # c(50287, 49713)

tally &lt;- c(0, 0)
dataset %&gt;%
  dataset_rejection_resample(
    class_func = function(x) (x-1) %% 2,
    target_dist = target_dist,
    initial_dist = initial_dist
  ) %&gt;%
  as_array_iterator() %&gt;%
  iterate(function(element) {
    names(element) &lt;- c("class_id", "i")
    add(tally[element$i]) &lt;&lt;- 1
  }, simplify = FALSE)
# The value of tally will be now be close to c(75000, 50000)
# thus satisfying the target_dist distribution.
tally # c(74822, 49921)

## End(Not run)
</code></pre>

<hr>
<h2 id='dataset_repeat'>Repeats a dataset count times.</h2><span id='topic+dataset_repeat'></span>

<h3>Description</h3>

<p>Repeats a dataset count times.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dataset_repeat(dataset, count = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dataset_repeat_+3A_dataset">dataset</code></td>
<td>
<p>A dataset</p>
</td></tr>
<tr><td><code id="dataset_repeat_+3A_count">count</code></td>
<td>
<p>(Optional.) An integer value representing the number of times
the elements of this dataset should be repeated. The default behavior (if
<code>count</code> is <code>NULL</code> or <code>-1</code>) is for the elements to be repeated indefinitely.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A dataset
</p>


<h3>See Also</h3>

<p>Other dataset methods: 
<code><a href="#topic+dataset_batch">dataset_batch</a>()</code>,
<code><a href="#topic+dataset_cache">dataset_cache</a>()</code>,
<code><a href="#topic+dataset_collect">dataset_collect</a>()</code>,
<code><a href="#topic+dataset_concatenate">dataset_concatenate</a>()</code>,
<code><a href="#topic+dataset_decode_delim">dataset_decode_delim</a>()</code>,
<code><a href="#topic+dataset_filter">dataset_filter</a>()</code>,
<code><a href="#topic+dataset_interleave">dataset_interleave</a>()</code>,
<code><a href="#topic+dataset_map_and_batch">dataset_map_and_batch</a>()</code>,
<code><a href="#topic+dataset_map">dataset_map</a>()</code>,
<code><a href="#topic+dataset_padded_batch">dataset_padded_batch</a>()</code>,
<code><a href="#topic+dataset_prefetch_to_device">dataset_prefetch_to_device</a>()</code>,
<code><a href="#topic+dataset_prefetch">dataset_prefetch</a>()</code>,
<code><a href="#topic+dataset_reduce">dataset_reduce</a>()</code>,
<code><a href="#topic+dataset_shuffle_and_repeat">dataset_shuffle_and_repeat</a>()</code>,
<code><a href="#topic+dataset_shuffle">dataset_shuffle</a>()</code>,
<code><a href="#topic+dataset_skip">dataset_skip</a>()</code>,
<code><a href="#topic+dataset_take_while">dataset_take_while</a>()</code>,
<code><a href="#topic+dataset_take">dataset_take</a>()</code>,
<code><a href="#topic+dataset_window">dataset_window</a>()</code>
</p>

<hr>
<h2 id='dataset_scan'>A transformation that scans a function across an input dataset</h2><span id='topic+dataset_scan'></span>

<h3>Description</h3>

<p>A transformation that scans a function across an input dataset
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dataset_scan(dataset, initial_state, scan_func)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dataset_scan_+3A_dataset">dataset</code></td>
<td>
<p>A tensorflow dataset</p>
</td></tr>
<tr><td><code id="dataset_scan_+3A_initial_state">initial_state</code></td>
<td>
<p>A nested structure of tensors, representing the initial
state of the accumulator.</p>
</td></tr>
<tr><td><code id="dataset_scan_+3A_scan_func">scan_func</code></td>
<td>
<p>A function that maps <code style="white-space: pre;">&#8288;(old_state, input_element)&#8288;</code> to
<code style="white-space: pre;">&#8288;(new_state, output_element)&#8288;</code>. It must take two arguments and return a
pair of nested structures of tensors. The <code>new_state</code> must match the
structure of <code>initial_state</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This transformation is a stateful relative of <code>dataset_map()</code>.
In addition to mapping <code>scan_func</code> across the elements of the input dataset,
<code>scan()</code> accumulates one or more state tensors, whose initial values are
<code>initial_state</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
initial_state &lt;- as_tensor(0, dtype="int64")
scan_func &lt;- function(state, i) list(state + i, state + i)
dataset &lt;- range_dataset(0, 10) %&gt;%
  dataset_scan(initial_state, scan_func)

reticulate::iterate(dataset, as.array) %&gt;%
  unlist()
# 0  1  3  6 10 15 21 28 36 45

## End(Not run)
</code></pre>

<hr>
<h2 id='dataset_shard'>Creates a dataset that includes only 1 / num_shards of this dataset.</h2><span id='topic+dataset_shard'></span>

<h3>Description</h3>

<p>This dataset operator is very useful when running distributed training, as it
allows each worker to read a unique subset.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dataset_shard(dataset, num_shards, index)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dataset_shard_+3A_dataset">dataset</code></td>
<td>
<p>A dataset</p>
</td></tr>
<tr><td><code id="dataset_shard_+3A_num_shards">num_shards</code></td>
<td>
<p>A integer representing the number of shards operating in
parallel.</p>
</td></tr>
<tr><td><code id="dataset_shard_+3A_index">index</code></td>
<td>
<p>A integer, representing the worker index.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A dataset
</p>

<hr>
<h2 id='dataset_shuffle'>Randomly shuffles the elements of this dataset.</h2><span id='topic+dataset_shuffle'></span>

<h3>Description</h3>

<p>Randomly shuffles the elements of this dataset.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dataset_shuffle(
  dataset,
  buffer_size,
  seed = NULL,
  reshuffle_each_iteration = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dataset_shuffle_+3A_dataset">dataset</code></td>
<td>
<p>A dataset</p>
</td></tr>
<tr><td><code id="dataset_shuffle_+3A_buffer_size">buffer_size</code></td>
<td>
<p>An integer, representing the number of elements from this
dataset from which the new dataset will sample.</p>
</td></tr>
<tr><td><code id="dataset_shuffle_+3A_seed">seed</code></td>
<td>
<p>(Optional) An integer, representing the random seed that will be
used to create the distribution.</p>
</td></tr>
<tr><td><code id="dataset_shuffle_+3A_reshuffle_each_iteration">reshuffle_each_iteration</code></td>
<td>
<p>(Optional) A boolean, which if true indicates
that the dataset should be pseudorandomly reshuffled each time it is iterated
over. (Defaults to <code>TRUE</code>). Not used if TF version &lt; 1.15</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A dataset
</p>


<h3>See Also</h3>

<p>Other dataset methods: 
<code><a href="#topic+dataset_batch">dataset_batch</a>()</code>,
<code><a href="#topic+dataset_cache">dataset_cache</a>()</code>,
<code><a href="#topic+dataset_collect">dataset_collect</a>()</code>,
<code><a href="#topic+dataset_concatenate">dataset_concatenate</a>()</code>,
<code><a href="#topic+dataset_decode_delim">dataset_decode_delim</a>()</code>,
<code><a href="#topic+dataset_filter">dataset_filter</a>()</code>,
<code><a href="#topic+dataset_interleave">dataset_interleave</a>()</code>,
<code><a href="#topic+dataset_map_and_batch">dataset_map_and_batch</a>()</code>,
<code><a href="#topic+dataset_map">dataset_map</a>()</code>,
<code><a href="#topic+dataset_padded_batch">dataset_padded_batch</a>()</code>,
<code><a href="#topic+dataset_prefetch_to_device">dataset_prefetch_to_device</a>()</code>,
<code><a href="#topic+dataset_prefetch">dataset_prefetch</a>()</code>,
<code><a href="#topic+dataset_reduce">dataset_reduce</a>()</code>,
<code><a href="#topic+dataset_repeat">dataset_repeat</a>()</code>,
<code><a href="#topic+dataset_shuffle_and_repeat">dataset_shuffle_and_repeat</a>()</code>,
<code><a href="#topic+dataset_skip">dataset_skip</a>()</code>,
<code><a href="#topic+dataset_take_while">dataset_take_while</a>()</code>,
<code><a href="#topic+dataset_take">dataset_take</a>()</code>,
<code><a href="#topic+dataset_window">dataset_window</a>()</code>
</p>

<hr>
<h2 id='dataset_shuffle_and_repeat'>Shuffles and repeats a dataset returning a new permutation for each epoch.</h2><span id='topic+dataset_shuffle_and_repeat'></span>

<h3>Description</h3>

<p>Shuffles and repeats a dataset returning a new permutation for each epoch.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dataset_shuffle_and_repeat(dataset, buffer_size, count = NULL, seed = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dataset_shuffle_and_repeat_+3A_dataset">dataset</code></td>
<td>
<p>A dataset</p>
</td></tr>
<tr><td><code id="dataset_shuffle_and_repeat_+3A_buffer_size">buffer_size</code></td>
<td>
<p>An integer, representing the number of elements from this
dataset from which the new dataset will sample.</p>
</td></tr>
<tr><td><code id="dataset_shuffle_and_repeat_+3A_count">count</code></td>
<td>
<p>(Optional.) An integer value representing the number of times
the elements of this dataset should be repeated. The default behavior (if
<code>count</code> is <code>NULL</code> or <code>-1</code>) is for the elements to be repeated indefinitely.</p>
</td></tr>
<tr><td><code id="dataset_shuffle_and_repeat_+3A_seed">seed</code></td>
<td>
<p>(Optional) An integer, representing the random seed that will be
used to create the distribution.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other dataset methods: 
<code><a href="#topic+dataset_batch">dataset_batch</a>()</code>,
<code><a href="#topic+dataset_cache">dataset_cache</a>()</code>,
<code><a href="#topic+dataset_collect">dataset_collect</a>()</code>,
<code><a href="#topic+dataset_concatenate">dataset_concatenate</a>()</code>,
<code><a href="#topic+dataset_decode_delim">dataset_decode_delim</a>()</code>,
<code><a href="#topic+dataset_filter">dataset_filter</a>()</code>,
<code><a href="#topic+dataset_interleave">dataset_interleave</a>()</code>,
<code><a href="#topic+dataset_map_and_batch">dataset_map_and_batch</a>()</code>,
<code><a href="#topic+dataset_map">dataset_map</a>()</code>,
<code><a href="#topic+dataset_padded_batch">dataset_padded_batch</a>()</code>,
<code><a href="#topic+dataset_prefetch_to_device">dataset_prefetch_to_device</a>()</code>,
<code><a href="#topic+dataset_prefetch">dataset_prefetch</a>()</code>,
<code><a href="#topic+dataset_reduce">dataset_reduce</a>()</code>,
<code><a href="#topic+dataset_repeat">dataset_repeat</a>()</code>,
<code><a href="#topic+dataset_shuffle">dataset_shuffle</a>()</code>,
<code><a href="#topic+dataset_skip">dataset_skip</a>()</code>,
<code><a href="#topic+dataset_take_while">dataset_take_while</a>()</code>,
<code><a href="#topic+dataset_take">dataset_take</a>()</code>,
<code><a href="#topic+dataset_window">dataset_window</a>()</code>
</p>

<hr>
<h2 id='dataset_skip'>Creates a dataset that skips count elements from this dataset</h2><span id='topic+dataset_skip'></span>

<h3>Description</h3>

<p>Creates a dataset that skips count elements from this dataset
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dataset_skip(dataset, count)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dataset_skip_+3A_dataset">dataset</code></td>
<td>
<p>A dataset</p>
</td></tr>
<tr><td><code id="dataset_skip_+3A_count">count</code></td>
<td>
<p>An integer, representing the number of elements of this dataset
that should be skipped to form the new dataset. If count is greater than
the size of this dataset, the new dataset will contain no elements. If
count is -1, skips the entire dataset.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A dataset
</p>


<h3>See Also</h3>

<p>Other dataset methods: 
<code><a href="#topic+dataset_batch">dataset_batch</a>()</code>,
<code><a href="#topic+dataset_cache">dataset_cache</a>()</code>,
<code><a href="#topic+dataset_collect">dataset_collect</a>()</code>,
<code><a href="#topic+dataset_concatenate">dataset_concatenate</a>()</code>,
<code><a href="#topic+dataset_decode_delim">dataset_decode_delim</a>()</code>,
<code><a href="#topic+dataset_filter">dataset_filter</a>()</code>,
<code><a href="#topic+dataset_interleave">dataset_interleave</a>()</code>,
<code><a href="#topic+dataset_map_and_batch">dataset_map_and_batch</a>()</code>,
<code><a href="#topic+dataset_map">dataset_map</a>()</code>,
<code><a href="#topic+dataset_padded_batch">dataset_padded_batch</a>()</code>,
<code><a href="#topic+dataset_prefetch_to_device">dataset_prefetch_to_device</a>()</code>,
<code><a href="#topic+dataset_prefetch">dataset_prefetch</a>()</code>,
<code><a href="#topic+dataset_reduce">dataset_reduce</a>()</code>,
<code><a href="#topic+dataset_repeat">dataset_repeat</a>()</code>,
<code><a href="#topic+dataset_shuffle_and_repeat">dataset_shuffle_and_repeat</a>()</code>,
<code><a href="#topic+dataset_shuffle">dataset_shuffle</a>()</code>,
<code><a href="#topic+dataset_take_while">dataset_take_while</a>()</code>,
<code><a href="#topic+dataset_take">dataset_take</a>()</code>,
<code><a href="#topic+dataset_window">dataset_window</a>()</code>
</p>

<hr>
<h2 id='dataset_snapshot'>Persist the output of a dataset</h2><span id='topic+dataset_snapshot'></span>

<h3>Description</h3>

<p>Persist the output of a dataset
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dataset_snapshot(
  dataset,
  path,
  compression = c("AUTO", "GZIP", "SNAPPY", "None"),
  reader_func = NULL,
  shard_func = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dataset_snapshot_+3A_dataset">dataset</code></td>
<td>
<p>A tensorflow dataset</p>
</td></tr>
<tr><td><code id="dataset_snapshot_+3A_path">path</code></td>
<td>
<p>Required. A directory to use for storing/loading the snapshot to/from.</p>
</td></tr>
<tr><td><code id="dataset_snapshot_+3A_compression">compression</code></td>
<td>
<p>Optional. The type of compression to apply to the snapshot
written to disk. Supported options are <code>"GZIP"</code>, <code>"SNAPPY"</code>, <code>"AUTO"</code> or
<code>NULL</code> (values of <code>""</code>, <code>NA</code>, and <code>"None"</code> are synonymous with <code>NULL</code>)
Defaults to <code>AUTO</code>, which attempts to pick an appropriate compression
algorithm for the dataset.</p>
</td></tr>
<tr><td><code id="dataset_snapshot_+3A_reader_func">reader_func</code></td>
<td>
<p>Optional. A function to control how to read data from
snapshot shards.</p>
</td></tr>
<tr><td><code id="dataset_snapshot_+3A_shard_func">shard_func</code></td>
<td>
<p>Optional. A function to control how to shard data when writing
a snapshot.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The snapshot API allows users to transparently persist the output of their
preprocessing pipeline to disk, and materialize the pre-processed data on a
different training run.
</p>
<p>This API enables repeated preprocessing steps to be consolidated, and allows
re-use of already processed data, trading off disk storage and network
bandwidth for freeing up more valuable CPU resources and accelerator compute
time.
</p>
<p>https://github.com/tensorflow/community/blob/master/rfcs/20200107-tf-data-snapshot.md
has detailed design documentation of this feature.
</p>
<p>Users can specify various options to control the behavior of snapshot,
including how snapshots are read from and written to by passing in
user-defined functions to the <code>reader_func</code> and <code>shard_func</code> parameters.
</p>
<p><code>shard_func</code> is a user specified function that maps input elements to
snapshot shards.
</p>
<div class="sourceCode R"><pre>NUM_SHARDS &lt;- parallel::detectCores()
dataset %&gt;%
  dataset_enumerate() %&gt;%
  dataset_snapshot(
    "/path/to/snapshot/dir",
    shard_func = function(index, ds_elem) x %% NUM_SHARDS) %&gt;%
  dataset_map(function(index, ds_elem) ds_elem)
</pre></div>
<p><code>reader_func</code> is a user specified function that accepts a single argument:
a Dataset of Datasets, each representing a &quot;split&quot; of elements of the
original dataset. The cardinality of the input dataset matches the
number of the shards specified in the <code>shard_func</code>. The function
should return a Dataset of elements of the original dataset.
</p>
<p>Users may want specify this function to control how snapshot files should be
read from disk, including the amount of shuffling and parallelism.
</p>
<p>Here is an example of a standard reader function a user can define. This
function enables both dataset shuffling and parallel reading of datasets:
</p>
<div class="sourceCode R"><pre>user_reader_func &lt;- function(datasets) {
  num_cores &lt;- parallel::detectCores()
  datasets %&gt;%
    dataset_shuffle(num_cores) %&gt;%
    dataset_interleave(function(x) x, num_parallel_calls=AUTOTUNE)
}

dataset &lt;- dataset %&gt;%
  dataset_snapshot("/path/to/snapshot/dir",
                   reader_func = user_reader_func)
</pre></div>
<p>By default, snapshot parallelizes reads by the number of cores available on
the system, but will not attempt to shuffle the data.
</p>

<hr>
<h2 id='dataset_take'>Creates a dataset with at most count elements from this dataset</h2><span id='topic+dataset_take'></span>

<h3>Description</h3>

<p>Creates a dataset with at most count elements from this dataset
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dataset_take(dataset, count)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dataset_take_+3A_dataset">dataset</code></td>
<td>
<p>A dataset</p>
</td></tr>
<tr><td><code id="dataset_take_+3A_count">count</code></td>
<td>
<p>Integer representing the number of elements of this dataset that
should be taken to form the new dataset. If <code>count</code> is -1, or if <code>count</code> is
greater than the size of this dataset, the new dataset will contain all
elements of this dataset.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A dataset
</p>


<h3>See Also</h3>

<p>Other dataset methods: 
<code><a href="#topic+dataset_batch">dataset_batch</a>()</code>,
<code><a href="#topic+dataset_cache">dataset_cache</a>()</code>,
<code><a href="#topic+dataset_collect">dataset_collect</a>()</code>,
<code><a href="#topic+dataset_concatenate">dataset_concatenate</a>()</code>,
<code><a href="#topic+dataset_decode_delim">dataset_decode_delim</a>()</code>,
<code><a href="#topic+dataset_filter">dataset_filter</a>()</code>,
<code><a href="#topic+dataset_interleave">dataset_interleave</a>()</code>,
<code><a href="#topic+dataset_map_and_batch">dataset_map_and_batch</a>()</code>,
<code><a href="#topic+dataset_map">dataset_map</a>()</code>,
<code><a href="#topic+dataset_padded_batch">dataset_padded_batch</a>()</code>,
<code><a href="#topic+dataset_prefetch_to_device">dataset_prefetch_to_device</a>()</code>,
<code><a href="#topic+dataset_prefetch">dataset_prefetch</a>()</code>,
<code><a href="#topic+dataset_reduce">dataset_reduce</a>()</code>,
<code><a href="#topic+dataset_repeat">dataset_repeat</a>()</code>,
<code><a href="#topic+dataset_shuffle_and_repeat">dataset_shuffle_and_repeat</a>()</code>,
<code><a href="#topic+dataset_shuffle">dataset_shuffle</a>()</code>,
<code><a href="#topic+dataset_skip">dataset_skip</a>()</code>,
<code><a href="#topic+dataset_take_while">dataset_take_while</a>()</code>,
<code><a href="#topic+dataset_window">dataset_window</a>()</code>
</p>

<hr>
<h2 id='dataset_take_while'>A transformation that stops dataset iteration based on a predicate.</h2><span id='topic+dataset_take_while'></span>

<h3>Description</h3>

<p>A transformation that stops dataset iteration based on a predicate.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dataset_take_while(dataset, predicate, name = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dataset_take_while_+3A_dataset">dataset</code></td>
<td>
<p>A TF dataset</p>
</td></tr>
<tr><td><code id="dataset_take_while_+3A_predicate">predicate</code></td>
<td>
<p>A function that maps a nested structure of tensors (having
shapes and types defined by <code>self$output_shapes</code> and <code>self$output_types</code>)
to a scalar <code>tf.bool</code> tensor.</p>
</td></tr>
<tr><td><code id="dataset_take_while_+3A_name">name</code></td>
<td>
<p>(Optional.) A name for the tf.data operation.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Example usage:
</p>
<div class="sourceCode r"><pre> range_dataset(from = 0, to = 10) %&gt;%
   dataset_take_while( ~ .x &lt; 5) %&gt;%
   as_array_iterator() %&gt;%
   iterate(simplify = FALSE) %&gt;% str()
 #&gt; List of 5
 #&gt; $ : num 0
 #&gt; $ : num 1
 #&gt; $ : num 2
 #&gt; $ : num 3
 #&gt; $ : num 4
</pre></div>


<h3>Value</h3>

<p>A TF Dataset
</p>


<h3>See Also</h3>

<p>Other dataset methods: 
<code><a href="#topic+dataset_batch">dataset_batch</a>()</code>,
<code><a href="#topic+dataset_cache">dataset_cache</a>()</code>,
<code><a href="#topic+dataset_collect">dataset_collect</a>()</code>,
<code><a href="#topic+dataset_concatenate">dataset_concatenate</a>()</code>,
<code><a href="#topic+dataset_decode_delim">dataset_decode_delim</a>()</code>,
<code><a href="#topic+dataset_filter">dataset_filter</a>()</code>,
<code><a href="#topic+dataset_interleave">dataset_interleave</a>()</code>,
<code><a href="#topic+dataset_map_and_batch">dataset_map_and_batch</a>()</code>,
<code><a href="#topic+dataset_map">dataset_map</a>()</code>,
<code><a href="#topic+dataset_padded_batch">dataset_padded_batch</a>()</code>,
<code><a href="#topic+dataset_prefetch_to_device">dataset_prefetch_to_device</a>()</code>,
<code><a href="#topic+dataset_prefetch">dataset_prefetch</a>()</code>,
<code><a href="#topic+dataset_reduce">dataset_reduce</a>()</code>,
<code><a href="#topic+dataset_repeat">dataset_repeat</a>()</code>,
<code><a href="#topic+dataset_shuffle_and_repeat">dataset_shuffle_and_repeat</a>()</code>,
<code><a href="#topic+dataset_shuffle">dataset_shuffle</a>()</code>,
<code><a href="#topic+dataset_skip">dataset_skip</a>()</code>,
<code><a href="#topic+dataset_take">dataset_take</a>()</code>,
<code><a href="#topic+dataset_window">dataset_window</a>()</code>
</p>

<hr>
<h2 id='dataset_unbatch'>Unbatch a dataset</h2><span id='topic+dataset_unbatch'></span>

<h3>Description</h3>

<p>Splits elements of a dataset into multiple elements.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dataset_unbatch(dataset, name = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dataset_unbatch_+3A_dataset">dataset</code></td>
<td>
<p>A dataset</p>
</td></tr>
<tr><td><code id="dataset_unbatch_+3A_name">name</code></td>
<td>
<p>(Optional.) A name for the tf.data operation.</p>
</td></tr>
</table>

<hr>
<h2 id='dataset_unique'>A transformation that discards duplicate elements of a Dataset.</h2><span id='topic+dataset_unique'></span>

<h3>Description</h3>

<p>Use this transformation to produce a dataset that contains one instance of
each unique element in the input (See example).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dataset_unique(dataset, name = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dataset_unique_+3A_dataset">dataset</code></td>
<td>
<p>A tf.Dataset.</p>
</td></tr>
<tr><td><code id="dataset_unique_+3A_name">name</code></td>
<td>
<p>(Optional.) A name for the tf.data operation.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A tf.Dataset
</p>


<h3>Note</h3>

<p>This transformation only supports datasets which fit into memory and
have elements of either tf.int32, tf.int64 or tf.string type.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
c(0, 37, 2, 37, 2, 1) %&gt;% as_tensor("int32") %&gt;%
  tensor_slices_dataset() %&gt;%
  dataset_unique() %&gt;%
  as_array_iterator() %&gt;% iterate() %&gt;% sort()
# [1]  0  1  2 37

## End(Not run)
</code></pre>

<hr>
<h2 id='dataset_use_spec'>Transform the dataset using the provided spec.</h2><span id='topic+dataset_use_spec'></span>

<h3>Description</h3>

<p>Prepares the dataset to be used directly in a model.The transformed
dataset is prepared to return tuples (x,y) that can be used directly
in Keras.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dataset_use_spec(dataset, spec)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dataset_use_spec_+3A_dataset">dataset</code></td>
<td>
<p>A TensorFlow dataset.</p>
</td></tr>
<tr><td><code id="dataset_use_spec_+3A_spec">spec</code></td>
<td>
<p>A feature specification created with <code><a href="#topic+feature_spec">feature_spec()</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A TensorFlow dataset.
</p>


<h3>See Also</h3>


<ul>
<li> <p><code><a href="#topic+feature_spec">feature_spec()</a></code> to initialize the feature specification.
</p>
</li>
<li> <p><code><a href="#topic+fit.FeatureSpec">fit.FeatureSpec()</a></code> to create a tensorflow dataset prepared to modeling.
</p>
</li>
<li> <p><a href="#topic+steps">steps</a> to a list of all implemented steps.
</p>
</li></ul>

<p>Other Feature Spec Functions: 
<code><a href="#topic+feature_spec">feature_spec</a>()</code>,
<code><a href="#topic+fit.FeatureSpec">fit.FeatureSpec</a>()</code>,
<code><a href="#topic+step_bucketized_column">step_bucketized_column</a>()</code>,
<code><a href="#topic+step_categorical_column_with_hash_bucket">step_categorical_column_with_hash_bucket</a>()</code>,
<code><a href="#topic+step_categorical_column_with_identity">step_categorical_column_with_identity</a>()</code>,
<code><a href="#topic+step_categorical_column_with_vocabulary_file">step_categorical_column_with_vocabulary_file</a>()</code>,
<code><a href="#topic+step_categorical_column_with_vocabulary_list">step_categorical_column_with_vocabulary_list</a>()</code>,
<code><a href="#topic+step_crossed_column">step_crossed_column</a>()</code>,
<code><a href="#topic+step_embedding_column">step_embedding_column</a>()</code>,
<code><a href="#topic+step_indicator_column">step_indicator_column</a>()</code>,
<code><a href="#topic+step_numeric_column">step_numeric_column</a>()</code>,
<code><a href="#topic+step_remove_column">step_remove_column</a>()</code>,
<code><a href="#topic+step_shared_embeddings_column">step_shared_embeddings_column</a>()</code>,
<code><a href="#topic+steps">steps</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library(tfdatasets)
data(hearts)
hearts &lt;- tensor_slices_dataset(hearts) %&gt;% dataset_batch(32)

# use the formula interface
spec &lt;- feature_spec(hearts, target ~ age) %&gt;%
  step_numeric_column(age)

spec_fit &lt;- fit(spec)
final_dataset &lt;- hearts %&gt;% dataset_use_spec(spec_fit)

## End(Not run)
</code></pre>

<hr>
<h2 id='dataset_window'>Combines input elements into a dataset of windows.</h2><span id='topic+dataset_window'></span>

<h3>Description</h3>

<p>Combines input elements into a dataset of windows.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dataset_window(dataset, size, shift = NULL, stride = 1, drop_remainder = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dataset_window_+3A_dataset">dataset</code></td>
<td>
<p>A dataset</p>
</td></tr>
<tr><td><code id="dataset_window_+3A_size">size</code></td>
<td>
<p>representing the number of elements of the input dataset to
combine into a window.</p>
</td></tr>
<tr><td><code id="dataset_window_+3A_shift">shift</code></td>
<td>
<p>epresenting the forward shift of the sliding window in each
iteration. Defaults to <code>size</code>.</p>
</td></tr>
<tr><td><code id="dataset_window_+3A_stride">stride</code></td>
<td>
<p>representing the stride of the input elements in the sliding
window.</p>
</td></tr>
<tr><td><code id="dataset_window_+3A_drop_remainder">drop_remainder</code></td>
<td>
<p>representing whether a window should be dropped in
case its size is smaller <code style="white-space: pre;">&#8288;than window_size&#8288;</code>.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other dataset methods: 
<code><a href="#topic+dataset_batch">dataset_batch</a>()</code>,
<code><a href="#topic+dataset_cache">dataset_cache</a>()</code>,
<code><a href="#topic+dataset_collect">dataset_collect</a>()</code>,
<code><a href="#topic+dataset_concatenate">dataset_concatenate</a>()</code>,
<code><a href="#topic+dataset_decode_delim">dataset_decode_delim</a>()</code>,
<code><a href="#topic+dataset_filter">dataset_filter</a>()</code>,
<code><a href="#topic+dataset_interleave">dataset_interleave</a>()</code>,
<code><a href="#topic+dataset_map_and_batch">dataset_map_and_batch</a>()</code>,
<code><a href="#topic+dataset_map">dataset_map</a>()</code>,
<code><a href="#topic+dataset_padded_batch">dataset_padded_batch</a>()</code>,
<code><a href="#topic+dataset_prefetch_to_device">dataset_prefetch_to_device</a>()</code>,
<code><a href="#topic+dataset_prefetch">dataset_prefetch</a>()</code>,
<code><a href="#topic+dataset_reduce">dataset_reduce</a>()</code>,
<code><a href="#topic+dataset_repeat">dataset_repeat</a>()</code>,
<code><a href="#topic+dataset_shuffle_and_repeat">dataset_shuffle_and_repeat</a>()</code>,
<code><a href="#topic+dataset_shuffle">dataset_shuffle</a>()</code>,
<code><a href="#topic+dataset_skip">dataset_skip</a>()</code>,
<code><a href="#topic+dataset_take_while">dataset_take_while</a>()</code>,
<code><a href="#topic+dataset_take">dataset_take</a>()</code>
</p>

<hr>
<h2 id='delim_record_spec'>Specification for reading a record from a text file with delimited values</h2><span id='topic+delim_record_spec'></span><span id='topic+csv_record_spec'></span><span id='topic+tsv_record_spec'></span>

<h3>Description</h3>

<p>Specification for reading a record from a text file with delimited values
</p>


<h3>Usage</h3>

<pre><code class='language-R'>delim_record_spec(
  example_file,
  delim = ",",
  skip = 0,
  names = NULL,
  types = NULL,
  defaults = NULL
)

csv_record_spec(
  example_file,
  skip = 0,
  names = NULL,
  types = NULL,
  defaults = NULL
)

tsv_record_spec(
  example_file,
  skip = 0,
  names = NULL,
  types = NULL,
  defaults = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="delim_record_spec_+3A_example_file">example_file</code></td>
<td>
<p>File that provides an example of the records to be read.
If you don't explicitly specify names and types (or defaults) then this
file will be read to generate default values.</p>
</td></tr>
<tr><td><code id="delim_record_spec_+3A_delim">delim</code></td>
<td>
<p>Character delimiter to separate fields in a record (defaults to
&quot;,&quot;)</p>
</td></tr>
<tr><td><code id="delim_record_spec_+3A_skip">skip</code></td>
<td>
<p>Number of lines to skip before reading data. Note that if
<code>names</code> is explicitly provided and there are column names witin the
file then <code>skip</code> should be set to 1 to ensure that the column names are
bypassed.</p>
</td></tr>
<tr><td><code id="delim_record_spec_+3A_names">names</code></td>
<td>
<p>Character vector with column names (or <code>NULL</code> to automatically
detect the column names from the first row of <code>example_file</code>).
</p>
<p>If <code>names</code> is a character vector, the values will be used as the names of
the columns, and the first row of the input will be read into the first row
of the datset. Note that if the underlying text file also includes column
names in it's first row, this row should be skipped explicitly with <code>skip = 1</code>.
</p>
<p>If <code>NULL</code>, the first row of the example_file will be used as the column
names, and will be skipped when reading the dataset.</p>
</td></tr>
<tr><td><code id="delim_record_spec_+3A_types">types</code></td>
<td>
<p>Column types. If <code>NULL</code> and <code>defaults</code> is specified then types
will be imputed from the defaults. Otherwise, all column types will be
imputed from the first 1000 rows of the <code>example_file</code>. This is convenient
(and fast), but not robust. If the imputation fails, you'll need to supply
the correct types yourself.
</p>
<p>Types can be explicitliy specified in a character vector as &quot;integer&quot;,
&quot;double&quot;, and &quot;character&quot; (e.g. <code style="white-space: pre;">&#8288;col_types = c("double", "double", "integer"&#8288;</code>).
</p>
<p>Alternatively, you can use a compact string representation where each
character represents one column: c = character, i = integer, d = double
(e.g. <code style="white-space: pre;">&#8288;types = &#8288;</code>ddi').</p>
</td></tr>
<tr><td><code id="delim_record_spec_+3A_defaults">defaults</code></td>
<td>
<p>List of default values which are used when data is
missing from a record (e.g. <code style="white-space: pre;">&#8288;list(0, 0, 0L&#8288;</code>). If <code>NULL</code> then defaults will
be automatically provided based on <code>types</code> (<code>0</code> for numeric columns and
<code>""</code> for character columns).</p>
</td></tr>
</table>

<hr>
<h2 id='dense_features'>Dense Features</h2><span id='topic+dense_features'></span>

<h3>Description</h3>

<p>Retrives the Dense Features from a spec.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dense_features(spec)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dense_features_+3A_spec">spec</code></td>
<td>
<p>A feature specification created with <code><a href="#topic+feature_spec">feature_spec()</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of feature columns.
</p>

<hr>
<h2 id='feature_spec'>Creates a feature specification.</h2><span id='topic+feature_spec'></span>

<h3>Description</h3>

<p>Used to create initialize a feature columns specification.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>feature_spec(dataset, x, y = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="feature_spec_+3A_dataset">dataset</code></td>
<td>
<p>A TensorFlow dataset.</p>
</td></tr>
<tr><td><code id="feature_spec_+3A_x">x</code></td>
<td>
<p>Features to include can use <code><a href="tidyselect.html#topic+language">tidyselect::select_helpers()</a></code> or
a <code>formula</code>.</p>
</td></tr>
<tr><td><code id="feature_spec_+3A_y">y</code></td>
<td>
<p>(Optional) The response variable. Can also be specified using
a <code>formula</code> in the <code>x</code> argument.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>After creating the <code>feature_spec</code> object you can add steps using the
<code>step</code> functions.
</p>


<h3>Value</h3>

<p>a <code>FeatureSpec</code> object.
</p>


<h3>See Also</h3>


<ul>
<li> <p><code><a href="#topic+fit.FeatureSpec">fit.FeatureSpec()</a></code> to fit the FeatureSpec
</p>
</li>
<li> <p><code><a href="#topic+dataset_use_spec">dataset_use_spec()</a></code> to create a tensorflow dataset prepared to modeling.
</p>
</li>
<li> <p><a href="#topic+steps">steps</a> to a list of all implemented steps.
</p>
</li></ul>

<p>Other Feature Spec Functions: 
<code><a href="#topic+dataset_use_spec">dataset_use_spec</a>()</code>,
<code><a href="#topic+fit.FeatureSpec">fit.FeatureSpec</a>()</code>,
<code><a href="#topic+step_bucketized_column">step_bucketized_column</a>()</code>,
<code><a href="#topic+step_categorical_column_with_hash_bucket">step_categorical_column_with_hash_bucket</a>()</code>,
<code><a href="#topic+step_categorical_column_with_identity">step_categorical_column_with_identity</a>()</code>,
<code><a href="#topic+step_categorical_column_with_vocabulary_file">step_categorical_column_with_vocabulary_file</a>()</code>,
<code><a href="#topic+step_categorical_column_with_vocabulary_list">step_categorical_column_with_vocabulary_list</a>()</code>,
<code><a href="#topic+step_crossed_column">step_crossed_column</a>()</code>,
<code><a href="#topic+step_embedding_column">step_embedding_column</a>()</code>,
<code><a href="#topic+step_indicator_column">step_indicator_column</a>()</code>,
<code><a href="#topic+step_numeric_column">step_numeric_column</a>()</code>,
<code><a href="#topic+step_remove_column">step_remove_column</a>()</code>,
<code><a href="#topic+step_shared_embeddings_column">step_shared_embeddings_column</a>()</code>,
<code><a href="#topic+steps">steps</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library(tfdatasets)
data(hearts)
hearts &lt;- tensor_slices_dataset(hearts) %&gt;% dataset_batch(32)

# use the formula interface
spec &lt;- feature_spec(hearts, target ~ .)

# select using `tidyselect` helpers
spec &lt;- feature_spec(hearts, x = c(thal, age), y = target)

## End(Not run)
</code></pre>

<hr>
<h2 id='file_list_dataset'>A dataset of all files matching a pattern</h2><span id='topic+file_list_dataset'></span>

<h3>Description</h3>

<p>A dataset of all files matching a pattern
</p>


<h3>Usage</h3>

<pre><code class='language-R'>file_list_dataset(file_pattern, shuffle = NULL, seed = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="file_list_dataset_+3A_file_pattern">file_pattern</code></td>
<td>
<p>A string, representing the filename pattern that will be matched.</p>
</td></tr>
<tr><td><code id="file_list_dataset_+3A_shuffle">shuffle</code></td>
<td>
<p>(Optional) If <code style="white-space: pre;">&#8288;TRUE``, the file names will be shuffled randomly. Defaults to &#8288;</code>TRUE'</p>
</td></tr>
<tr><td><code id="file_list_dataset_+3A_seed">seed</code></td>
<td>
<p>(Optional) An integer, representing the random seed that
will be used to create the distribution.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For example, if we had the following files on our filesystem: - /path/to/dir/a.txt -
/path/to/dir/b.csv - /path/to/dir/c.csv
</p>
<p>If we pass &quot;/path/to/dir/*.csv&quot; as the <code>file_pattern</code>, the dataset would produce: -
/path/to/dir/b.csv - /path/to/dir/c.csv
</p>


<h3>Value</h3>

<p>A dataset of string correponding to file names
</p>


<h3>Note</h3>

<p>The <code>shuffle</code> and <code>seed</code> arguments only apply for TensorFlow &gt;= v1.8
</p>

<hr>
<h2 id='fit.FeatureSpec'>Fits a feature specification.</h2><span id='topic+fit.FeatureSpec'></span>

<h3>Description</h3>

<p>This function will <code>fit</code> the specification. Depending
on the steps added to the specification it will compute
for example, the levels of categorical features, normalization
constants, etc.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'FeatureSpec'
fit(object, dataset = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fit.FeatureSpec_+3A_object">object</code></td>
<td>
<p>A feature specification created with <code><a href="#topic+feature_spec">feature_spec()</a></code>.</p>
</td></tr>
<tr><td><code id="fit.FeatureSpec_+3A_dataset">dataset</code></td>
<td>
<p>(Optional) A TensorFlow dataset. If <code>NULL</code> it will use
the dataset provided when initilializing the <code>feature_spec</code>.</p>
</td></tr>
<tr><td><code id="fit.FeatureSpec_+3A_...">...</code></td>
<td>
<p>(unused)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a fitted <code>FeatureSpec</code> object.
</p>


<h3>See Also</h3>


<ul>
<li> <p><code><a href="#topic+feature_spec">feature_spec()</a></code> to initialize the feature specification.
</p>
</li>
<li> <p><code><a href="#topic+dataset_use_spec">dataset_use_spec()</a></code> to create a tensorflow dataset prepared to modeling.
</p>
</li>
<li> <p><a href="#topic+steps">steps</a> to a list of all implemented steps.
</p>
</li></ul>

<p>Other Feature Spec Functions: 
<code><a href="#topic+dataset_use_spec">dataset_use_spec</a>()</code>,
<code><a href="#topic+feature_spec">feature_spec</a>()</code>,
<code><a href="#topic+step_bucketized_column">step_bucketized_column</a>()</code>,
<code><a href="#topic+step_categorical_column_with_hash_bucket">step_categorical_column_with_hash_bucket</a>()</code>,
<code><a href="#topic+step_categorical_column_with_identity">step_categorical_column_with_identity</a>()</code>,
<code><a href="#topic+step_categorical_column_with_vocabulary_file">step_categorical_column_with_vocabulary_file</a>()</code>,
<code><a href="#topic+step_categorical_column_with_vocabulary_list">step_categorical_column_with_vocabulary_list</a>()</code>,
<code><a href="#topic+step_crossed_column">step_crossed_column</a>()</code>,
<code><a href="#topic+step_embedding_column">step_embedding_column</a>()</code>,
<code><a href="#topic+step_indicator_column">step_indicator_column</a>()</code>,
<code><a href="#topic+step_numeric_column">step_numeric_column</a>()</code>,
<code><a href="#topic+step_remove_column">step_remove_column</a>()</code>,
<code><a href="#topic+step_shared_embeddings_column">step_shared_embeddings_column</a>()</code>,
<code><a href="#topic+steps">steps</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library(tfdatasets)
data(hearts)
hearts &lt;- tensor_slices_dataset(hearts) %&gt;% dataset_batch(32)

# use the formula interface
spec &lt;- feature_spec(hearts, target ~ age) %&gt;%
  step_numeric_column(age)

spec_fit &lt;- fit(spec)
spec_fit

## End(Not run)
</code></pre>

<hr>
<h2 id='fixed_length_record_dataset'>A dataset of fixed-length records from one or more binary files.</h2><span id='topic+fixed_length_record_dataset'></span>

<h3>Description</h3>

<p>A dataset of fixed-length records from one or more binary files.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fixed_length_record_dataset(
  filenames,
  record_bytes,
  header_bytes = NULL,
  footer_bytes = NULL,
  buffer_size = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fixed_length_record_dataset_+3A_filenames">filenames</code></td>
<td>
<p>A string tensor containing one or more filenames.</p>
</td></tr>
<tr><td><code id="fixed_length_record_dataset_+3A_record_bytes">record_bytes</code></td>
<td>
<p>An integer representing the number of bytes in each
record.</p>
</td></tr>
<tr><td><code id="fixed_length_record_dataset_+3A_header_bytes">header_bytes</code></td>
<td>
<p>(Optional) An integer scalar representing the number of
bytes to skip at the start of a file.</p>
</td></tr>
<tr><td><code id="fixed_length_record_dataset_+3A_footer_bytes">footer_bytes</code></td>
<td>
<p>(Optional) A integer scalar representing the number of
bytes to ignore at the end of a file.</p>
</td></tr>
<tr><td><code id="fixed_length_record_dataset_+3A_buffer_size">buffer_size</code></td>
<td>
<p>(Optional) A integer scalar representing the number of
bytes to buffer when reading.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A dataset
</p>

<hr>
<h2 id='has_type'>Identify the type of the variable.</h2><span id='topic+has_type'></span>

<h3>Description</h3>

<p>Can only be used inside the <a href="#topic+steps">steps</a> specifications to find
variables by type.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>has_type(match = "float32")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="has_type_+3A_match">match</code></td>
<td>
<p>A list of types to match.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other Selectors: 
<code><a href="#topic+all_nominal">all_nominal</a>()</code>,
<code><a href="#topic+all_numeric">all_numeric</a>()</code>
</p>

<hr>
<h2 id='hearts'>Heart Disease Data Set</h2><span id='topic+hearts'></span>

<h3>Description</h3>

<p>Heart disease (angiographic disease status) dataset.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hearts
</code></pre>


<h3>Format</h3>

<p>A data frame with 303 rows and 14 variables:
</p>

<dl>
<dt>age</dt><dd><p>age in years</p>
</dd>
<dt>sex</dt><dd><p>sex (1 = male; 0 = female)</p>
</dd>
<dt>cp</dt><dd><p>chest pain type: Value 1: typical angina, Value 2: atypical angina,
Value 3: non-anginal pain, Value 4: asymptomatic</p>
</dd>
<dt>trestbps</dt><dd><p>resting blood pressure (in mm Hg on admission to the hospital)</p>
</dd>
<dt>chol</dt><dd><p> serum cholestoral in mg/dl </p>
</dd>
<dt>fbs</dt><dd><p>(fasting blood sugar &gt; 120 mg/dl) (1 = true; 0 = false)</p>
</dd>
<dt>restecg</dt><dd><p>resting electrocardiographic results: Value 0: normal, Value 1:
having ST-T wave abnormality (T wave inversions and/or ST elevation or
depression of &gt; 0.05 mV), Value 2: showing probable or definite left ventricular
hypertrophy by Estes' criteria</p>
</dd>
<dt>thalach</dt><dd><p>maximum heart rate achieved</p>
</dd>
<dt>exang</dt><dd><p>exercise induced angina (1 = yes; 0 = no)</p>
</dd>
<dt>oldpeak</dt><dd><p>ST depression induced by exercise relative to rest</p>
</dd>
<dt>slope</dt><dd><p>the slope of the peak exercise ST segment: Value 1: upsloping,
Value 2: flat, Value 3: downsloping</p>
</dd>
<dt>ca</dt><dd><p>number of major vessels (0-3) colored by flourosopy</p>
</dd>
<dt>thal</dt><dd><p>3 = normal; 6 = fixed defect; 7 = reversable defect</p>
</dd>
<dt>target</dt><dd><p>diagnosis of heart disease angiographic</p>
</dd>
</dl>



<h3>Source</h3>

<p><a href="https://archive.ics.uci.edu/ml/datasets/heart+Disease">https://archive.ics.uci.edu/ml/datasets/heart+Disease</a>
</p>


<h3>References</h3>

<p>The authors of the databases have requested that any publications resulting
from the use of the data include the names of the principal investigator
responsible for the data collection at each institution. They would be:
</p>

<ol>
<li><p> Hungarian Institute of Cardiology. Budapest: Andras Janosi, M.D.
</p>
</li>
<li><p> University Hospital, Zurich, Switzerland: William Steinbrunn, M.D.
</p>
</li>
<li><p> University Hospital, Basel, Switzerland: Matthias Pfisterer, M.D.
</p>
</li>
<li><p> V.A. Medical Center, Long Beach and Cleveland Clinic Foundation:Robert Detrano, M.D., Ph.D.
</p>
</li></ol>


<hr>
<h2 id='input_fn.tf_dataset'>Construct a tfestimators input function from a dataset</h2><span id='topic+input_fn.tf_dataset'></span><span id='topic+input_fn'></span>

<h3>Description</h3>

<p>Construct a tfestimators input function from a dataset
</p>


<h3>Usage</h3>

<pre><code class='language-R'>input_fn.tf_dataset(dataset, features, response = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="input_fn.tf_dataset_+3A_dataset">dataset</code></td>
<td>
<p>A dataset</p>
</td></tr>
<tr><td><code id="input_fn.tf_dataset_+3A_features">features</code></td>
<td>
<p>The names of feature variables to be used.</p>
</td></tr>
<tr><td><code id="input_fn.tf_dataset_+3A_response">response</code></td>
<td>
<p>The name of the response variable.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Creating an input_fn from a dataset requires that the dataset
consist of a set of named output tensors (e.g. like the dataset
produced by the <code><a href="#topic+tfrecord_dataset">tfrecord_dataset()</a></code> or <code><a href="#topic+text_line_dataset">text_line_dataset()</a></code> function).
</p>


<h3>Value</h3>

<p>An input_fn suitable for use with tfestimators <a href="tfestimators.html#topic+train.tf_estimator">train</a>,
<a href="tfestimators.html#topic+evaluate.tf_estimator">evaluate</a>, and <a href="tfestimators.html#topic+predict.tf_estimator">predict</a> methods
</p>

<hr>
<h2 id='iterator_get_next'>Get next element from iterator</h2><span id='topic+iterator_get_next'></span>

<h3>Description</h3>

<p>Returns a nested list of tensors that when evaluated will yield
the next element(s) in the dataset.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>iterator_get_next(iterator, name = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="iterator_get_next_+3A_iterator">iterator</code></td>
<td>
<p>An iterator</p>
</td></tr>
<tr><td><code id="iterator_get_next_+3A_name">name</code></td>
<td>
<p>(Optional) A name for the created operation.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A nested list of tensors
</p>


<h3>See Also</h3>

<p>Other iterator functions: 
<code><a href="#topic+iterator_initializer">iterator_initializer</a>()</code>,
<code><a href="#topic+iterator_make_initializer">iterator_make_initializer</a>()</code>,
<code><a href="#topic+iterator_string_handle">iterator_string_handle</a>()</code>,
<code><a href="#topic+make-iterator">make-iterator</a></code>
</p>

<hr>
<h2 id='iterator_initializer'>An operation that should be run to initialize this iterator.</h2><span id='topic+iterator_initializer'></span>

<h3>Description</h3>

<p>An operation that should be run to initialize this iterator.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>iterator_initializer(iterator)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="iterator_initializer_+3A_iterator">iterator</code></td>
<td>
<p>An iterator</p>
</td></tr>
</table>


<h3>See Also</h3>

<p>Other iterator functions: 
<code><a href="#topic+iterator_get_next">iterator_get_next</a>()</code>,
<code><a href="#topic+iterator_make_initializer">iterator_make_initializer</a>()</code>,
<code><a href="#topic+iterator_string_handle">iterator_string_handle</a>()</code>,
<code><a href="#topic+make-iterator">make-iterator</a></code>
</p>

<hr>
<h2 id='iterator_make_initializer'>Create an operation that can be run to initialize this iterator</h2><span id='topic+iterator_make_initializer'></span>

<h3>Description</h3>

<p>Create an operation that can be run to initialize this iterator
</p>


<h3>Usage</h3>

<pre><code class='language-R'>iterator_make_initializer(iterator, dataset, name = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="iterator_make_initializer_+3A_iterator">iterator</code></td>
<td>
<p>An iterator</p>
</td></tr>
<tr><td><code id="iterator_make_initializer_+3A_dataset">dataset</code></td>
<td>
<p>A dataset</p>
</td></tr>
<tr><td><code id="iterator_make_initializer_+3A_name">name</code></td>
<td>
<p>(Optional) A name for the created operation.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A tf$Operation that can be run to initialize this iterator on the
given dataset.
</p>


<h3>See Also</h3>

<p>Other iterator functions: 
<code><a href="#topic+iterator_get_next">iterator_get_next</a>()</code>,
<code><a href="#topic+iterator_initializer">iterator_initializer</a>()</code>,
<code><a href="#topic+iterator_string_handle">iterator_string_handle</a>()</code>,
<code><a href="#topic+make-iterator">make-iterator</a></code>
</p>

<hr>
<h2 id='iterator_string_handle'>String-valued tensor that represents this iterator</h2><span id='topic+iterator_string_handle'></span>

<h3>Description</h3>

<p>String-valued tensor that represents this iterator
</p>


<h3>Usage</h3>

<pre><code class='language-R'>iterator_string_handle(iterator, name = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="iterator_string_handle_+3A_iterator">iterator</code></td>
<td>
<p>An iterator</p>
</td></tr>
<tr><td><code id="iterator_string_handle_+3A_name">name</code></td>
<td>
<p>(Optional) A name for the created operation.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Scalar tensor of type string
</p>


<h3>See Also</h3>

<p>Other iterator functions: 
<code><a href="#topic+iterator_get_next">iterator_get_next</a>()</code>,
<code><a href="#topic+iterator_initializer">iterator_initializer</a>()</code>,
<code><a href="#topic+iterator_make_initializer">iterator_make_initializer</a>()</code>,
<code><a href="#topic+make-iterator">make-iterator</a></code>
</p>

<hr>
<h2 id='layer_input_from_dataset'>Creates a list of inputs from a dataset</h2><span id='topic+layer_input_from_dataset'></span>

<h3>Description</h3>

<p>Create a list ok Keras input layers that can be used together
with <code><a href="keras.html#topic+layer_dense_features">keras::layer_dense_features()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>layer_input_from_dataset(dataset)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="layer_input_from_dataset_+3A_dataset">dataset</code></td>
<td>
<p>a TensorFlow dataset or a data.frame</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a list of Keras input layers
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library(tfdatasets)
data(hearts)
hearts &lt;- tensor_slices_dataset(hearts) %&gt;% dataset_batch(32)

# use the formula interface
spec &lt;- feature_spec(hearts, target ~ age + slope) %&gt;%
  step_numeric_column(age, slope) %&gt;%
  step_bucketized_column(age, boundaries = c(10, 20, 30))

spec &lt;- fit(spec)
dataset &lt;- hearts %&gt;% dataset_use_spec(spec)

input &lt;- layer_input_from_dataset(dataset)

## End(Not run)

</code></pre>

<hr>
<h2 id='length.tf_dataset'>Get Dataset length</h2><span id='topic+length.tf_dataset'></span><span id='topic+length.tensorflow.python.data.ops.dataset_ops.DatasetV2'></span>

<h3>Description</h3>

<p>Returns the length of the dataset.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'tf_dataset'
length(x)

## S3 method for class 'tensorflow.python.data.ops.dataset_ops.DatasetV2'
length(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="length.tf_dataset_+3A_x">x</code></td>
<td>
<p>a <code>tf.data.Dataset</code> object.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Either <code>Inf</code> if the dataset is infinite, <code>NA</code> if the dataset length
is unknown, or an R numeric if it is known.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
range_dataset(0, 42) %&gt;% length()
# 42

range_dataset(0, 42) %&gt;% dataset_repeat() %&gt;% length()
# Inf

range_dataset(0, 42) %&gt;% dataset_repeat() %&gt;%
  dataset_filter(function(x) TRUE) %&gt;% length()
# NA

## End(Not run)
</code></pre>

<hr>
<h2 id='make_csv_dataset'>Reads CSV files into a batched dataset</h2><span id='topic+make_csv_dataset'></span>

<h3>Description</h3>

<p>Reads CSV files into a dataset, where each element is a (features, labels) list that
corresponds to a batch of CSV rows. The features dictionary maps feature column names
to tensors containing the corresponding feature data, and labels is a tensor
containing the batch's label data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>make_csv_dataset(
  file_pattern,
  batch_size,
  column_names = NULL,
  column_defaults = NULL,
  label_name = NULL,
  select_columns = NULL,
  field_delim = ",",
  use_quote_delim = TRUE,
  na_value = "",
  header = TRUE,
  num_epochs = NULL,
  shuffle = TRUE,
  shuffle_buffer_size = 10000,
  shuffle_seed = NULL,
  prefetch_buffer_size = 1,
  num_parallel_reads = 1,
  num_parallel_parser_calls = 2,
  sloppy = FALSE,
  num_rows_for_inference = 100
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="make_csv_dataset_+3A_file_pattern">file_pattern</code></td>
<td>
<p>List of files or glob patterns of file paths containing CSV records.</p>
</td></tr>
<tr><td><code id="make_csv_dataset_+3A_batch_size">batch_size</code></td>
<td>
<p>An integer representing the number of records to combine in a single
batch.</p>
</td></tr>
<tr><td><code id="make_csv_dataset_+3A_column_names">column_names</code></td>
<td>
<p>An optional list of strings that corresponds to the CSV columns, in
order. One per column of the input record. If this is not provided, infers the column
names from the first row of the records. These names will be the keys of the features
dict of each dataset element.</p>
</td></tr>
<tr><td><code id="make_csv_dataset_+3A_column_defaults">column_defaults</code></td>
<td>
<p>A optional list of default values for the CSV fields. One item
per selected column of the input record. Each item in the list is either a valid CSV
dtype (integer, numeric, or string), or a tensor with one of the
aforementioned types. The tensor can either be a scalar default value (if the column
is optional), or an empty tensor (if the column is required). If a dtype is provided
instead of a tensor, the column is also treated as required. If this list is not
provided, tries to infer types based on reading the first <code>num_rows_for_inference</code> rows
of files specified, and assumes all columns are optional, defaulting to <code>0</code> for
numeric values and <code>""</code> for string values. If both this and <code>select_columns</code> are
specified, these must have the same lengths, and <code>column_defaults</code> is assumed to be
sorted in order of increasing column index.</p>
</td></tr>
<tr><td><code id="make_csv_dataset_+3A_label_name">label_name</code></td>
<td>
<p>A optional string corresponding to the label column. If provided, the
data for this column is returned as a separate tensor from the features dictionary,
so that the dataset complies with the format expected by a TF Estiamtors and Keras.</p>
</td></tr>
<tr><td><code id="make_csv_dataset_+3A_select_columns">select_columns</code></td>
<td>
<p>(Ignored if using TensorFlow version 1.8.) An optional list of
integer indices or string column names, that specifies a subset of columns of CSV data
to select. If column names are provided, these must correspond to names provided in
<code>column_names</code> or inferred from the file header lines. When this argument is specified,
only a subset of CSV columns will be parsed and returned, corresponding to the columns
specified. Using this results in faster parsing and lower memory usage. If both this
and <code>column_defaults</code> are specified, these must have the same lengths, and
<code>column_defaults</code> is assumed to be sorted in order of increasing column index.</p>
</td></tr>
<tr><td><code id="make_csv_dataset_+3A_field_delim">field_delim</code></td>
<td>
<p>An optional string. Defaults to <code>","</code>. Char delimiter to separate
fields in a record.</p>
</td></tr>
<tr><td><code id="make_csv_dataset_+3A_use_quote_delim">use_quote_delim</code></td>
<td>
<p>An optional bool. Defaults to <code>TRUE</code>. If false, treats double
quotation marks as regular characters inside of the string fields.</p>
</td></tr>
<tr><td><code id="make_csv_dataset_+3A_na_value">na_value</code></td>
<td>
<p>Additional string to recognize as NA/NaN.</p>
</td></tr>
<tr><td><code id="make_csv_dataset_+3A_header">header</code></td>
<td>
<p>A bool that indicates whether the first rows of provided CSV files
correspond to header lines with column names, and should not be included in the data.</p>
</td></tr>
<tr><td><code id="make_csv_dataset_+3A_num_epochs">num_epochs</code></td>
<td>
<p>An integer specifying the number of times this dataset is repeated. If
NULL, cycles through the dataset forever.</p>
</td></tr>
<tr><td><code id="make_csv_dataset_+3A_shuffle">shuffle</code></td>
<td>
<p>A bool that indicates whether the input should be shuffled.</p>
</td></tr>
<tr><td><code id="make_csv_dataset_+3A_shuffle_buffer_size">shuffle_buffer_size</code></td>
<td>
<p>Buffer size to use for shuffling. A large buffer size
ensures better shuffling, but increases memory usage and startup time.</p>
</td></tr>
<tr><td><code id="make_csv_dataset_+3A_shuffle_seed">shuffle_seed</code></td>
<td>
<p>Randomization seed to use for shuffling.</p>
</td></tr>
<tr><td><code id="make_csv_dataset_+3A_prefetch_buffer_size">prefetch_buffer_size</code></td>
<td>
<p>An int specifying the number of feature batches to prefetch
for performance improvement. Recommended value is the number of batches consumed per
training step.</p>
</td></tr>
<tr><td><code id="make_csv_dataset_+3A_num_parallel_reads">num_parallel_reads</code></td>
<td>
<p>Number of threads used to read CSV records from files. If &gt;1,
the results will be interleaved.</p>
</td></tr>
<tr><td><code id="make_csv_dataset_+3A_num_parallel_parser_calls">num_parallel_parser_calls</code></td>
<td>
<p>(Ignored if using TensorFlow version 1.11 or later.)
Number of parallel invocations of the CSV parsing function on CSV records.</p>
</td></tr>
<tr><td><code id="make_csv_dataset_+3A_sloppy">sloppy</code></td>
<td>
<p>If <code>TRUE</code>, reading performance will be improved at the cost of
non-deterministic ordering. If <code>FALSE</code>, the order of elements produced is
deterministic prior to shuffling (elements are still randomized if <code>shuffle=TRUE</code>.
Note that if the seed is set, then order of elements after shuffling is
deterministic). Defaults to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="make_csv_dataset_+3A_num_rows_for_inference">num_rows_for_inference</code></td>
<td>
<p>Number of rows of a file to use for type inference if
record_defaults is not provided. If <code>NULL</code>, reads all the rows of all the files.
Defaults to 100.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A dataset, where each element is a (features, labels) list that corresponds to
a batch of <code>batch_size</code> CSV rows. The features dictionary maps feature column names
to tensors containing the corresponding column data, and labels is a tensor
containing the column data for the label column specified by <code>label_name</code>.
</p>

<hr>
<h2 id='make-iterator'>Creates an iterator for enumerating the elements of this dataset.</h2><span id='topic+make-iterator'></span><span id='topic+make_iterator_one_shot'></span><span id='topic+make_iterator_initializable'></span><span id='topic+make_iterator_from_structure'></span><span id='topic+make_iterator_from_string_handle'></span>

<h3>Description</h3>

<p>Creates an iterator for enumerating the elements of this dataset.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>make_iterator_one_shot(dataset)

make_iterator_initializable(dataset, shared_name = NULL)

make_iterator_from_structure(
  output_types,
  output_shapes = NULL,
  shared_name = NULL
)

make_iterator_from_string_handle(
  string_handle,
  output_types,
  output_shapes = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="make-iterator_+3A_dataset">dataset</code></td>
<td>
<p>A dataset</p>
</td></tr>
<tr><td><code id="make-iterator_+3A_shared_name">shared_name</code></td>
<td>
<p>(Optional) If non-empty, the returned iterator will be
shared under the given name across multiple sessions that share the same
devices (e.g. when using a remote server).</p>
</td></tr>
<tr><td><code id="make-iterator_+3A_output_types">output_types</code></td>
<td>
<p>A nested structure of tf$DType objects corresponding to
each component of an element of this iterator.</p>
</td></tr>
<tr><td><code id="make-iterator_+3A_output_shapes">output_shapes</code></td>
<td>
<p>(Optional) A nested structure of tf$TensorShape objects
corresponding to each component of an element of this dataset. If omitted,
each component will have an unconstrainted shape.</p>
</td></tr>
<tr><td><code id="make-iterator_+3A_string_handle">string_handle</code></td>
<td>
<p>A scalar tensor of type string that evaluates
to a handle produced by the <code><a href="#topic+iterator_string_handle">iterator_string_handle()</a></code> method.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An Iterator over the elements of this dataset.
</p>


<h3>Initialization</h3>

<p>For <code>make_iterator_one_shot()</code>, the returned
iterator will be initialized automatically. A &quot;one-shot&quot; iterator does not
currently support re-initialization.
</p>
<p>For <code>make_iterator_initializable()</code>,
the returned iterator will be in an uninitialized state, and you must run
the object returned from <code><a href="#topic+iterator_initializer">iterator_initializer()</a></code> before using it.
</p>
<p>For <code>make_iterator_from_structure()</code>, the returned iterator is not bound
to a particular dataset, and it has no initializer. To initialize the
iterator, run the operation returned by <code><a href="#topic+iterator_make_initializer">iterator_make_initializer()</a></code>.
</p>


<h3>See Also</h3>

<p>Other iterator functions: 
<code><a href="#topic+iterator_get_next">iterator_get_next</a>()</code>,
<code><a href="#topic+iterator_initializer">iterator_initializer</a>()</code>,
<code><a href="#topic+iterator_make_initializer">iterator_make_initializer</a>()</code>,
<code><a href="#topic+iterator_string_handle">iterator_string_handle</a>()</code>
</p>

<hr>
<h2 id='next_batch'>Tensor(s) for retrieving the next batch from a dataset</h2><span id='topic+next_batch'></span>

<h3>Description</h3>

<p>Tensor(s) for retrieving the next batch from a dataset
</p>


<h3>Usage</h3>

<pre><code class='language-R'>next_batch(dataset)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="next_batch_+3A_dataset">dataset</code></td>
<td>
<p>A dataset</p>
</td></tr>
</table>


<h3>Details</h3>

<p>To access the underlying data within the dataset you iteratively evaluate the
tensor(s) to read batches of data.
</p>
<p>Note that in many cases you won't need to explicitly evaluate the tensors.
Rather, you will pass the tensors to another function that will perform
the evaluation (e.g. the Keras <a href="keras.html#topic+layer_input">layer_input()</a> and
<a href="keras.html#topic+reexports">compile()</a> functions).
</p>
<p>If you do need to perform iteration manually by evaluating the tensors, there
are a couple of possible approaches to controlling/detecting when iteration should
end.
</p>
<p>One approach is to create a dataset that yields batches infinitely (traversing
the dataset multiple times with different batches randomly drawn). In this case you'd
use another mechanism like a global step counter or detecting a learning plateau.
</p>
<p>Another approach is to detect when all batches have been yielded
from the dataset. When the tensor reaches the end of iteration a runtime
error will occur. You can catch and ignore the error when it occurs by wrapping
your iteration code in the <code>with_dataset()</code> function.
</p>
<p>See the examples below for a demonstration of each of these methods of iteration.
</p>


<h3>Value</h3>

<p>Tensor(s) that can be evaluated to yield the next batch of training data.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

# iteration with 'infinite' dataset and explicit step counter

library(tfdatasets)
dataset &lt;- text_line_dataset("mtcars.csv", record_spec = mtcars_spec) %&gt;%
  dataset_prepare(x = c(mpg, disp), y = cyl) %&gt;%
  dataset_shuffle(5000) %&gt;%
  dataset_batch(128) %&gt;%
  dataset_repeat() # repeat infinitely
batch &lt;- next_batch(dataset)
steps &lt;- 200
for (i in 1:steps) {
  # use batch$x and batch$y tensors
}

# iteration that detects and ignores end of iteration error

library(tfdatasets)
dataset &lt;- text_line_dataset("mtcars.csv", record_spec = mtcars_spec) %&gt;%
  dataset_prepare(x = c(mpg, disp), y = cyl) %&gt;%
  dataset_batch(128) %&gt;%
  dataset_repeat(10)
batch &lt;- next_batch(dataset)
with_dataset({
  while(TRUE) {
    # use batch$x and batch$y tensors
  }
})

## End(Not run)

</code></pre>

<hr>
<h2 id='output_types'>Output types and shapes</h2><span id='topic+output_types'></span><span id='topic+output_shapes'></span>

<h3>Description</h3>

<p>Output types and shapes
</p>


<h3>Usage</h3>

<pre><code class='language-R'>output_types(object)

output_shapes(object)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="output_types_+3A_object">object</code></td>
<td>
<p>A dataset or iterator</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>output_types()</code> returns the type of each component of an element of
this object; <code>output_shapes()</code> returns the shape of each component of an
element of this object
</p>

<hr>
<h2 id='random_integer_dataset'>Creates a <code>Dataset</code> of pseudorandom values</h2><span id='topic+random_integer_dataset'></span>

<h3>Description</h3>

<p>Creates a <code>Dataset</code> of pseudorandom values
</p>


<h3>Usage</h3>

<pre><code class='language-R'>random_integer_dataset(seed = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="random_integer_dataset_+3A_seed">seed</code></td>
<td>
<p>(Optional) If specified, the dataset produces a deterministic
sequence of values.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The dataset generates a sequence of uniformly distributed integer values (dtype int64).
</p>

<hr>
<h2 id='range_dataset'>Creates a dataset of a step-separated range of values.</h2><span id='topic+range_dataset'></span>

<h3>Description</h3>

<p>Creates a dataset of a step-separated range of values.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>range_dataset(from = 0, to = 0, by = 1, ..., dtype = tf$int64)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="range_dataset_+3A_from">from</code></td>
<td>
<p>Range start</p>
</td></tr>
<tr><td><code id="range_dataset_+3A_to">to</code></td>
<td>
<p>Range end (exclusive)</p>
</td></tr>
<tr><td><code id="range_dataset_+3A_by">by</code></td>
<td>
<p>Increment of the sequence</p>
</td></tr>
<tr><td><code id="range_dataset_+3A_...">...</code></td>
<td>
<p>ignored</p>
</td></tr>
<tr><td><code id="range_dataset_+3A_dtype">dtype</code></td>
<td>
<p>Output dtype. (Optional, default: <code>tf$int64</code>).</p>
</td></tr>
</table>

<hr>
<h2 id='read_files'>Read a dataset from a set of files</h2><span id='topic+read_files'></span>

<h3>Description</h3>

<p>Read files into a dataset, optionally processing them in parallel.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>read_files(
  files,
  reader,
  ...,
  parallel_files = 1,
  parallel_interleave = 1,
  num_shards = NULL,
  shard_index = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="read_files_+3A_files">files</code></td>
<td>
<p>List of filenames or glob pattern for files (e.g. &quot;*.csv&quot;)</p>
</td></tr>
<tr><td><code id="read_files_+3A_reader">reader</code></td>
<td>
<p>Function that maps a file into a dataset (e.g.
<code><a href="#topic+text_line_dataset">text_line_dataset()</a></code> or <code><a href="#topic+tfrecord_dataset">tfrecord_dataset()</a></code>).</p>
</td></tr>
<tr><td><code id="read_files_+3A_...">...</code></td>
<td>
<p>Additional arguments to pass to <code>reader</code> function</p>
</td></tr>
<tr><td><code id="read_files_+3A_parallel_files">parallel_files</code></td>
<td>
<p>An integer, number of files to process in parallel</p>
</td></tr>
<tr><td><code id="read_files_+3A_parallel_interleave">parallel_interleave</code></td>
<td>
<p>An integer, number of consecutive records to
produce from each file before cycling to another file.</p>
</td></tr>
<tr><td><code id="read_files_+3A_num_shards">num_shards</code></td>
<td>
<p>An integer representing the number of shards operating in
parallel.</p>
</td></tr>
<tr><td><code id="read_files_+3A_shard_index">shard_index</code></td>
<td>
<p>An integer, representing the worker index. Shared indexes
are 0 based so for e.g. 8 shards valid indexes would be 0-7.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A dataset
</p>

<hr>
<h2 id='reexports'>Objects exported from other packages</h2><span id='topic+reexports'></span><span id='topic+as_tensor'></span><span id='topic+starts_with'></span><span id='topic+ends_with'></span><span id='topic+contains'></span><span id='topic+everything'></span><span id='topic+matches'></span><span id='topic+num_range'></span><span id='topic+one_of'></span><span id='topic+tf'></span><span id='topic+shape'></span><span id='topic+install_tensorflow'></span><span id='topic+fit'></span><span id='topic+as_iterator'></span><span id='topic+iter_next'></span><span id='topic+iterate'></span>

<h3>Description</h3>

<p>These objects are imported from other packages. Follow the links
below to see their documentation.
</p>

<dl>
<dt>generics</dt><dd><p><code><a href="generics.html#topic+fit">fit</a></code></p>
</dd>
<dt>reticulate</dt><dd><p><code><a href="reticulate.html#topic+iterate">as_iterator</a></code>, <code><a href="reticulate.html#topic+iterate">iter_next</a></code>, <code><a href="reticulate.html#topic+iterate">iterate</a></code></p>
</dd>
<dt>tensorflow</dt><dd><p><code><a href="tensorflow.html#topic+as_tensor">as_tensor</a></code>, <code><a href="tensorflow.html#topic+install_tensorflow">install_tensorflow</a></code>, <code><a href="tensorflow.html#topic+shape">shape</a></code>, <code><a href="tensorflow.html#topic+tf">tf</a></code></p>
</dd>
<dt>tidyselect</dt><dd><p><code><a href="tidyselect.html#topic+starts_with">contains</a></code>, <code><a href="tidyselect.html#topic+starts_with">ends_with</a></code>, <code><a href="tidyselect.html#topic+everything">everything</a></code>, <code><a href="tidyselect.html#topic+starts_with">matches</a></code>, <code><a href="tidyselect.html#topic+starts_with">num_range</a></code>, <code><a href="tidyselect.html#topic+one_of">one_of</a></code>, <code><a href="tidyselect.html#topic+starts_with">starts_with</a></code></p>
</dd>
</dl>

<hr>
<h2 id='sample_from_datasets'>Samples elements at random from the datasets in <code>datasets</code>.</h2><span id='topic+sample_from_datasets'></span>

<h3>Description</h3>

<p>Samples elements at random from the datasets in <code>datasets</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sample_from_datasets(
  datasets,
  weights = NULL,
  seed = NULL,
  stop_on_empty_dataset = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sample_from_datasets_+3A_datasets">datasets</code></td>
<td>
<p>A list ofobjects with compatible structure.</p>
</td></tr>
<tr><td><code id="sample_from_datasets_+3A_weights">weights</code></td>
<td>
<p>(Optional.) A list of <code>length(datasets)</code> floating-point values where
<code>weights[[i]]</code> represents the probability with which an element should be sampled
from <code>datasets[[i]]</code>, or a dataset object where each element is such a list.
Defaults to a uniform distribution across <code>datasets</code>.</p>
</td></tr>
<tr><td><code id="sample_from_datasets_+3A_seed">seed</code></td>
<td>
<p>(Optional.) An integer, representing the random seed
that will be used to create the distribution.</p>
</td></tr>
<tr><td><code id="sample_from_datasets_+3A_stop_on_empty_dataset">stop_on_empty_dataset</code></td>
<td>
<p>If <code>TRUE</code>, selection stops if it encounters an
empty dataset. If <code>FALSE</code>, it skips empty datasets. It is recommended to
set it to <code>TRUE</code>. Otherwise, the selected elements start off as the user
intends, but may change as input datasets become empty. This can be
difficult to detect since the dataset starts off looking correct. Defaults
to <code>TRUE</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A dataset that interleaves elements from <code>datasets</code> at random, according to
<code>weights</code> if provided, otherwise with uniform probability.
</p>

<hr>
<h2 id='scaler'>List of pre-made scalers</h2><span id='topic+scaler'></span>

<h3>Description</h3>


<ul>
<li> <p><a href="#topic+scaler_standard">scaler_standard</a>: mean and standard deviation normalizer.
</p>
</li>
<li> <p><a href="#topic+scaler_min_max">scaler_min_max</a>: min max normalizer
</p>
</li></ul>



<h3>See Also</h3>

<p><a href="#topic+step_numeric_column">step_numeric_column</a>
</p>

<hr>
<h2 id='scaler_min_max'>Creates an instance of a min max scaler</h2><span id='topic+scaler_min_max'></span>

<h3>Description</h3>

<p>This scaler will learn the min and max of the numeric variable
and use this to create a <code>normalizer_fn</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>scaler_min_max()
</code></pre>


<h3>See Also</h3>

<p><a href="#topic+scaler">scaler</a> to a complete list of normalizers
</p>
<p>Other scaler: 
<code><a href="#topic+scaler_standard">scaler_standard</a>()</code>
</p>

<hr>
<h2 id='scaler_standard'>Creates an instance of a standard scaler</h2><span id='topic+scaler_standard'></span>

<h3>Description</h3>

<p>This scaler will learn the mean and the standard deviation
and use this to create a <code>normalizer_fn</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>scaler_standard()
</code></pre>


<h3>See Also</h3>

<p><a href="#topic+scaler">scaler</a> to a complete list of normalizers
</p>
<p>Other scaler: 
<code><a href="#topic+scaler_min_max">scaler_min_max</a>()</code>
</p>

<hr>
<h2 id='selectors'>Selectors</h2><span id='topic+selectors'></span><span id='topic+cur_info_env'></span>

<h3>Description</h3>

<p>List of selectors that can be used to specify variables inside
steps.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cur_info_env
</code></pre>


<h3>Format</h3>

<p>An object of class <code>environment</code> of length 0.
</p>


<h3>Selectors</h3>


<ul>
<li> <p><code><a href="#topic+has_type">has_type()</a></code>
</p>
</li>
<li> <p><code><a href="#topic+all_numeric">all_numeric()</a></code>
</p>
</li>
<li> <p><code><a href="#topic+all_nominal">all_nominal()</a></code>
</p>
</li>
<li> <p><code><a href="#topic+starts_with">starts_with()</a></code>
</p>
</li>
<li> <p><code><a href="#topic+ends_with">ends_with()</a></code>
</p>
</li>
<li> <p><code><a href="#topic+one_of">one_of()</a></code>
</p>
</li>
<li> <p><code><a href="#topic+matches">matches()</a></code>
</p>
</li>
<li> <p><code><a href="#topic+contains">contains()</a></code>
</p>
</li>
<li> <p><code><a href="#topic+everything">everything()</a></code>
</p>
</li></ul>


<hr>
<h2 id='sparse_tensor_slices_dataset'>Splits each rank-N <code>tf$SparseTensor</code> in this dataset row-wise.</h2><span id='topic+sparse_tensor_slices_dataset'></span>

<h3>Description</h3>

<p>Splits each rank-N <code>tf$SparseTensor</code> in this dataset row-wise.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sparse_tensor_slices_dataset(sparse_tensor)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sparse_tensor_slices_dataset_+3A_sparse_tensor">sparse_tensor</code></td>
<td>
<p>A <code>tf$SparseTensor</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A dataset of rank-(N-1) sparse tensors.
</p>


<h3>See Also</h3>

<p>Other tensor datasets: 
<code><a href="#topic+tensor_slices_dataset">tensor_slices_dataset</a>()</code>,
<code><a href="#topic+tensors_dataset">tensors_dataset</a>()</code>
</p>

<hr>
<h2 id='sql_record_spec'>A dataset consisting of the results from a SQL query</h2><span id='topic+sql_record_spec'></span><span id='topic+sql_dataset'></span><span id='topic+sqlite_dataset'></span>

<h3>Description</h3>

<p>A dataset consisting of the results from a SQL query
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sql_record_spec(names, types)

sql_dataset(driver_name, data_source_name, query, record_spec)

sqlite_dataset(filename, query, record_spec)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sql_record_spec_+3A_names">names</code></td>
<td>
<p>Names of columns returned from the query</p>
</td></tr>
<tr><td><code id="sql_record_spec_+3A_types">types</code></td>
<td>
<p>List of <code>tf$DType</code> objects (e.g. <code>tf$int32</code>,
<code>tf$double</code>, <code>tf$string</code>) representing the types of the columns
returned by the query.</p>
</td></tr>
<tr><td><code id="sql_record_spec_+3A_driver_name">driver_name</code></td>
<td>
<p>String containing the database type. Currently, the only
supported value is 'sqlite'.</p>
</td></tr>
<tr><td><code id="sql_record_spec_+3A_data_source_name">data_source_name</code></td>
<td>
<p>String containing a connection string to connect to
the database.</p>
</td></tr>
<tr><td><code id="sql_record_spec_+3A_query">query</code></td>
<td>
<p>String containing the SQL query to execute.</p>
</td></tr>
<tr><td><code id="sql_record_spec_+3A_record_spec">record_spec</code></td>
<td>
<p>Names and types of database columns</p>
</td></tr>
<tr><td><code id="sql_record_spec_+3A_filename">filename</code></td>
<td>
<p>Filename for the database</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A dataset
</p>

<hr>
<h2 id='step_bucketized_column'>Creates bucketized columns</h2><span id='topic+step_bucketized_column'></span>

<h3>Description</h3>

<p>Use this step to create bucketized columns from numeric columns.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>step_bucketized_column(spec, ..., boundaries)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="step_bucketized_column_+3A_spec">spec</code></td>
<td>
<p>A feature specification created with <code><a href="#topic+feature_spec">feature_spec()</a></code>.</p>
</td></tr>
<tr><td><code id="step_bucketized_column_+3A_...">...</code></td>
<td>
<p>Comma separated list of variable names to apply the step. <a href="#topic+selectors">selectors</a> can also be used.</p>
</td></tr>
<tr><td><code id="step_bucketized_column_+3A_boundaries">boundaries</code></td>
<td>
<p>A sorted list or tuple of floats specifying the boundaries.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code>FeatureSpec</code> object.
</p>


<h3>See Also</h3>

<p><a href="#topic+steps">steps</a> for a complete list of allowed steps.
</p>
<p>Other Feature Spec Functions: 
<code><a href="#topic+dataset_use_spec">dataset_use_spec</a>()</code>,
<code><a href="#topic+feature_spec">feature_spec</a>()</code>,
<code><a href="#topic+fit.FeatureSpec">fit.FeatureSpec</a>()</code>,
<code><a href="#topic+step_categorical_column_with_hash_bucket">step_categorical_column_with_hash_bucket</a>()</code>,
<code><a href="#topic+step_categorical_column_with_identity">step_categorical_column_with_identity</a>()</code>,
<code><a href="#topic+step_categorical_column_with_vocabulary_file">step_categorical_column_with_vocabulary_file</a>()</code>,
<code><a href="#topic+step_categorical_column_with_vocabulary_list">step_categorical_column_with_vocabulary_list</a>()</code>,
<code><a href="#topic+step_crossed_column">step_crossed_column</a>()</code>,
<code><a href="#topic+step_embedding_column">step_embedding_column</a>()</code>,
<code><a href="#topic+step_indicator_column">step_indicator_column</a>()</code>,
<code><a href="#topic+step_numeric_column">step_numeric_column</a>()</code>,
<code><a href="#topic+step_remove_column">step_remove_column</a>()</code>,
<code><a href="#topic+step_shared_embeddings_column">step_shared_embeddings_column</a>()</code>,
<code><a href="#topic+steps">steps</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library(tfdatasets)
data(hearts)
file &lt;- tempfile()
writeLines(unique(hearts$thal), file)
hearts &lt;- tensor_slices_dataset(hearts) %&gt;% dataset_batch(32)

# use the formula interface
spec &lt;- feature_spec(hearts, target ~ age) %&gt;%
  step_numeric_column(age) %&gt;%
  step_bucketized_column(age, boundaries = c(10, 20, 30))
spec_fit &lt;- fit(spec)
final_dataset &lt;- hearts %&gt;% dataset_use_spec(spec_fit)

## End(Not run)
</code></pre>

<hr>
<h2 id='step_categorical_column_with_hash_bucket'>Creates a categorical column with hash buckets specification</h2><span id='topic+step_categorical_column_with_hash_bucket'></span>

<h3>Description</h3>

<p>Represents sparse feature where ids are set by hashing.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>step_categorical_column_with_hash_bucket(
  spec,
  ...,
  hash_bucket_size,
  dtype = tf$string
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="step_categorical_column_with_hash_bucket_+3A_spec">spec</code></td>
<td>
<p>A feature specification created with <code><a href="#topic+feature_spec">feature_spec()</a></code>.</p>
</td></tr>
<tr><td><code id="step_categorical_column_with_hash_bucket_+3A_...">...</code></td>
<td>
<p>Comma separated list of variable names to apply the step. <a href="#topic+selectors">selectors</a> can also be used.</p>
</td></tr>
<tr><td><code id="step_categorical_column_with_hash_bucket_+3A_hash_bucket_size">hash_bucket_size</code></td>
<td>
<p>An int &gt; 1. The number of buckets.</p>
</td></tr>
<tr><td><code id="step_categorical_column_with_hash_bucket_+3A_dtype">dtype</code></td>
<td>
<p>The type of features. Only string and integer types are supported.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code>FeatureSpec</code> object.
</p>


<h3>See Also</h3>

<p><a href="#topic+steps">steps</a> for a complete list of allowed steps.
</p>
<p>Other Feature Spec Functions: 
<code><a href="#topic+dataset_use_spec">dataset_use_spec</a>()</code>,
<code><a href="#topic+feature_spec">feature_spec</a>()</code>,
<code><a href="#topic+fit.FeatureSpec">fit.FeatureSpec</a>()</code>,
<code><a href="#topic+step_bucketized_column">step_bucketized_column</a>()</code>,
<code><a href="#topic+step_categorical_column_with_identity">step_categorical_column_with_identity</a>()</code>,
<code><a href="#topic+step_categorical_column_with_vocabulary_file">step_categorical_column_with_vocabulary_file</a>()</code>,
<code><a href="#topic+step_categorical_column_with_vocabulary_list">step_categorical_column_with_vocabulary_list</a>()</code>,
<code><a href="#topic+step_crossed_column">step_crossed_column</a>()</code>,
<code><a href="#topic+step_embedding_column">step_embedding_column</a>()</code>,
<code><a href="#topic+step_indicator_column">step_indicator_column</a>()</code>,
<code><a href="#topic+step_numeric_column">step_numeric_column</a>()</code>,
<code><a href="#topic+step_remove_column">step_remove_column</a>()</code>,
<code><a href="#topic+step_shared_embeddings_column">step_shared_embeddings_column</a>()</code>,
<code><a href="#topic+steps">steps</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library(tfdatasets)
data(hearts)
hearts &lt;- tensor_slices_dataset(hearts) %&gt;% dataset_batch(32)

# use the formula interface
spec &lt;- feature_spec(hearts, target ~ thal) %&gt;%
  step_categorical_column_with_hash_bucket(thal, hash_bucket_size = 3)

spec_fit &lt;- fit(spec)
final_dataset &lt;- hearts %&gt;% dataset_use_spec(spec_fit)

## End(Not run)

</code></pre>

<hr>
<h2 id='step_categorical_column_with_identity'>Create a categorical column with identity</h2><span id='topic+step_categorical_column_with_identity'></span>

<h3>Description</h3>

<p>Use this when your inputs are integers in the range <code style="white-space: pre;">&#8288;[0-num_buckets)&#8288;</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>step_categorical_column_with_identity(
  spec,
  ...,
  num_buckets,
  default_value = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="step_categorical_column_with_identity_+3A_spec">spec</code></td>
<td>
<p>A feature specification created with <code><a href="#topic+feature_spec">feature_spec()</a></code>.</p>
</td></tr>
<tr><td><code id="step_categorical_column_with_identity_+3A_...">...</code></td>
<td>
<p>Comma separated list of variable names to apply the step. <a href="#topic+selectors">selectors</a> can also be used.</p>
</td></tr>
<tr><td><code id="step_categorical_column_with_identity_+3A_num_buckets">num_buckets</code></td>
<td>
<p>Range of inputs and outputs is <code style="white-space: pre;">&#8288;[0, num_buckets)&#8288;</code>.</p>
</td></tr>
<tr><td><code id="step_categorical_column_with_identity_+3A_default_value">default_value</code></td>
<td>
<p>If <code>NULL</code>, this column's graph operations will fail
for out-of-range inputs. Otherwise, this value must be in the range
<code style="white-space: pre;">&#8288;[0, num_buckets)&#8288;</code>, and will replace inputs in that range.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code>FeatureSpec</code> object.
</p>


<h3>See Also</h3>

<p><a href="#topic+steps">steps</a> for a complete list of allowed steps.
</p>
<p>Other Feature Spec Functions: 
<code><a href="#topic+dataset_use_spec">dataset_use_spec</a>()</code>,
<code><a href="#topic+feature_spec">feature_spec</a>()</code>,
<code><a href="#topic+fit.FeatureSpec">fit.FeatureSpec</a>()</code>,
<code><a href="#topic+step_bucketized_column">step_bucketized_column</a>()</code>,
<code><a href="#topic+step_categorical_column_with_hash_bucket">step_categorical_column_with_hash_bucket</a>()</code>,
<code><a href="#topic+step_categorical_column_with_vocabulary_file">step_categorical_column_with_vocabulary_file</a>()</code>,
<code><a href="#topic+step_categorical_column_with_vocabulary_list">step_categorical_column_with_vocabulary_list</a>()</code>,
<code><a href="#topic+step_crossed_column">step_crossed_column</a>()</code>,
<code><a href="#topic+step_embedding_column">step_embedding_column</a>()</code>,
<code><a href="#topic+step_indicator_column">step_indicator_column</a>()</code>,
<code><a href="#topic+step_numeric_column">step_numeric_column</a>()</code>,
<code><a href="#topic+step_remove_column">step_remove_column</a>()</code>,
<code><a href="#topic+step_shared_embeddings_column">step_shared_embeddings_column</a>()</code>,
<code><a href="#topic+steps">steps</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library(tfdatasets)
data(hearts)

hearts$thal &lt;- as.integer(as.factor(hearts$thal)) - 1L

hearts &lt;- tensor_slices_dataset(hearts) %&gt;% dataset_batch(32)

# use the formula interface
spec &lt;- feature_spec(hearts, target ~ thal) %&gt;%
  step_categorical_column_with_identity(thal, num_buckets = 5)

spec_fit &lt;- fit(spec)
final_dataset &lt;- hearts %&gt;% dataset_use_spec(spec_fit)

## End(Not run)

</code></pre>

<hr>
<h2 id='step_categorical_column_with_vocabulary_file'>Creates a categorical column with vocabulary file</h2><span id='topic+step_categorical_column_with_vocabulary_file'></span>

<h3>Description</h3>

<p>Use this function when the vocabulary of a categorical variable
is written to a file.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>step_categorical_column_with_vocabulary_file(
  spec,
  ...,
  vocabulary_file,
  vocabulary_size = NULL,
  dtype = tf$string,
  default_value = NULL,
  num_oov_buckets = 0L
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="step_categorical_column_with_vocabulary_file_+3A_spec">spec</code></td>
<td>
<p>A feature specification created with <code><a href="#topic+feature_spec">feature_spec()</a></code>.</p>
</td></tr>
<tr><td><code id="step_categorical_column_with_vocabulary_file_+3A_...">...</code></td>
<td>
<p>Comma separated list of variable names to apply the step. <a href="#topic+selectors">selectors</a> can also be used.</p>
</td></tr>
<tr><td><code id="step_categorical_column_with_vocabulary_file_+3A_vocabulary_file">vocabulary_file</code></td>
<td>
<p>The vocabulary file name.</p>
</td></tr>
<tr><td><code id="step_categorical_column_with_vocabulary_file_+3A_vocabulary_size">vocabulary_size</code></td>
<td>
<p>Number of the elements in the vocabulary. This
must be no greater than length of <code>vocabulary_file</code>, if less than
length, later values are ignored. If None, it is set to the length of
<code>vocabulary_file</code>.</p>
</td></tr>
<tr><td><code id="step_categorical_column_with_vocabulary_file_+3A_dtype">dtype</code></td>
<td>
<p>The type of features. Only string and integer types are
supported.</p>
</td></tr>
<tr><td><code id="step_categorical_column_with_vocabulary_file_+3A_default_value">default_value</code></td>
<td>
<p>The integer ID value to return for out-of-vocabulary
feature values, defaults to <code>-1</code>. This can not be specified with a
positive <code>num_oov_buckets</code>.</p>
</td></tr>
<tr><td><code id="step_categorical_column_with_vocabulary_file_+3A_num_oov_buckets">num_oov_buckets</code></td>
<td>
<p>Non-negative integer, the number of out-of-vocabulary
buckets. All out-of-vocabulary inputs will be assigned IDs in the range
<code style="white-space: pre;">&#8288;[vocabulary_size, vocabulary_size+num_oov_buckets)&#8288;</code> based on a hash of
the input value. A positive <code>num_oov_buckets</code> can not be specified with
default_value.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code>FeatureSpec</code> object.
</p>


<h3>See Also</h3>

<p><a href="#topic+steps">steps</a> for a complete list of allowed steps.
</p>
<p>Other Feature Spec Functions: 
<code><a href="#topic+dataset_use_spec">dataset_use_spec</a>()</code>,
<code><a href="#topic+feature_spec">feature_spec</a>()</code>,
<code><a href="#topic+fit.FeatureSpec">fit.FeatureSpec</a>()</code>,
<code><a href="#topic+step_bucketized_column">step_bucketized_column</a>()</code>,
<code><a href="#topic+step_categorical_column_with_hash_bucket">step_categorical_column_with_hash_bucket</a>()</code>,
<code><a href="#topic+step_categorical_column_with_identity">step_categorical_column_with_identity</a>()</code>,
<code><a href="#topic+step_categorical_column_with_vocabulary_list">step_categorical_column_with_vocabulary_list</a>()</code>,
<code><a href="#topic+step_crossed_column">step_crossed_column</a>()</code>,
<code><a href="#topic+step_embedding_column">step_embedding_column</a>()</code>,
<code><a href="#topic+step_indicator_column">step_indicator_column</a>()</code>,
<code><a href="#topic+step_numeric_column">step_numeric_column</a>()</code>,
<code><a href="#topic+step_remove_column">step_remove_column</a>()</code>,
<code><a href="#topic+step_shared_embeddings_column">step_shared_embeddings_column</a>()</code>,
<code><a href="#topic+steps">steps</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library(tfdatasets)
data(hearts)
file &lt;- tempfile()
writeLines(unique(hearts$thal), file)
hearts &lt;- tensor_slices_dataset(hearts) %&gt;% dataset_batch(32)

# use the formula interface
spec &lt;- feature_spec(hearts, target ~ thal) %&gt;%
  step_categorical_column_with_vocabulary_file(thal, vocabulary_file = file)

spec_fit &lt;- fit(spec)
final_dataset &lt;- hearts %&gt;% dataset_use_spec(spec_fit)

## End(Not run)

</code></pre>

<hr>
<h2 id='step_categorical_column_with_vocabulary_list'>Creates a categorical column specification</h2><span id='topic+step_categorical_column_with_vocabulary_list'></span>

<h3>Description</h3>

<p>Creates a categorical column specification
</p>


<h3>Usage</h3>

<pre><code class='language-R'>step_categorical_column_with_vocabulary_list(
  spec,
  ...,
  vocabulary_list = NULL,
  dtype = NULL,
  default_value = -1L,
  num_oov_buckets = 0L
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="step_categorical_column_with_vocabulary_list_+3A_spec">spec</code></td>
<td>
<p>A feature specification created with <code><a href="#topic+feature_spec">feature_spec()</a></code>.</p>
</td></tr>
<tr><td><code id="step_categorical_column_with_vocabulary_list_+3A_...">...</code></td>
<td>
<p>Comma separated list of variable names to apply the step. <a href="#topic+selectors">selectors</a> can also be used.</p>
</td></tr>
<tr><td><code id="step_categorical_column_with_vocabulary_list_+3A_vocabulary_list">vocabulary_list</code></td>
<td>
<p>An ordered iterable defining the vocabulary. Each
feature is mapped to the index of its value (if present) in vocabulary_list.
Must be castable to <code>dtype</code>. If <code>NULL</code> the vocabulary will be defined as
all unique values in the dataset provided when fitting the specification.</p>
</td></tr>
<tr><td><code id="step_categorical_column_with_vocabulary_list_+3A_dtype">dtype</code></td>
<td>
<p>The type of features. Only string and integer types are supported.
If <code>NULL</code>, it will be inferred from <code>vocabulary_list</code>.</p>
</td></tr>
<tr><td><code id="step_categorical_column_with_vocabulary_list_+3A_default_value">default_value</code></td>
<td>
<p>The integer ID value to return for out-of-vocabulary feature
values, defaults to <code>-1</code>. This can not be specified with a positive
num_oov_buckets.</p>
</td></tr>
<tr><td><code id="step_categorical_column_with_vocabulary_list_+3A_num_oov_buckets">num_oov_buckets</code></td>
<td>
<p>Non-negative integer, the number of out-of-vocabulary buckets.
All out-of-vocabulary inputs will be assigned IDs in the range
<code style="white-space: pre;">&#8288;[lenght(vocabulary_list), length(vocabulary_list)+num_oov_buckets)&#8288;</code> based on a hash of
the input value. A positive num_oov_buckets can not be specified with
default_value.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code>FeatureSpec</code> object.
</p>


<h3>See Also</h3>

<p><a href="#topic+steps">steps</a> for a complete list of allowed steps.
</p>
<p>Other Feature Spec Functions: 
<code><a href="#topic+dataset_use_spec">dataset_use_spec</a>()</code>,
<code><a href="#topic+feature_spec">feature_spec</a>()</code>,
<code><a href="#topic+fit.FeatureSpec">fit.FeatureSpec</a>()</code>,
<code><a href="#topic+step_bucketized_column">step_bucketized_column</a>()</code>,
<code><a href="#topic+step_categorical_column_with_hash_bucket">step_categorical_column_with_hash_bucket</a>()</code>,
<code><a href="#topic+step_categorical_column_with_identity">step_categorical_column_with_identity</a>()</code>,
<code><a href="#topic+step_categorical_column_with_vocabulary_file">step_categorical_column_with_vocabulary_file</a>()</code>,
<code><a href="#topic+step_crossed_column">step_crossed_column</a>()</code>,
<code><a href="#topic+step_embedding_column">step_embedding_column</a>()</code>,
<code><a href="#topic+step_indicator_column">step_indicator_column</a>()</code>,
<code><a href="#topic+step_numeric_column">step_numeric_column</a>()</code>,
<code><a href="#topic+step_remove_column">step_remove_column</a>()</code>,
<code><a href="#topic+step_shared_embeddings_column">step_shared_embeddings_column</a>()</code>,
<code><a href="#topic+steps">steps</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library(tfdatasets)
data(hearts)
hearts &lt;- tensor_slices_dataset(hearts) %&gt;% dataset_batch(32)

# use the formula interface
spec &lt;- feature_spec(hearts, target ~ thal) %&gt;%
  step_categorical_column_with_vocabulary_list(thal)

spec_fit &lt;- fit(spec)
final_dataset &lt;- hearts %&gt;% dataset_use_spec(spec_fit)

## End(Not run)

</code></pre>

<hr>
<h2 id='step_crossed_column'>Creates crosses of categorical columns</h2><span id='topic+step_crossed_column'></span>

<h3>Description</h3>

<p>Use this step to create crosses between categorical columns.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>step_crossed_column(spec, ..., hash_bucket_size, hash_key = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="step_crossed_column_+3A_spec">spec</code></td>
<td>
<p>A feature specification created with <code><a href="#topic+feature_spec">feature_spec()</a></code>.</p>
</td></tr>
<tr><td><code id="step_crossed_column_+3A_...">...</code></td>
<td>
<p>Comma separated list of variable names to apply the step. <a href="#topic+selectors">selectors</a> can also be used.</p>
</td></tr>
<tr><td><code id="step_crossed_column_+3A_hash_bucket_size">hash_bucket_size</code></td>
<td>
<p>An int &gt; 1. The number of buckets.</p>
</td></tr>
<tr><td><code id="step_crossed_column_+3A_hash_key">hash_key</code></td>
<td>
<p>(optional) Specify the hash_key that will be used by the
FingerprintCat64 function to combine the crosses fingerprints on
SparseCrossOp.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code>FeatureSpec</code> object.
</p>


<h3>See Also</h3>

<p><a href="#topic+steps">steps</a> for a complete list of allowed steps.
</p>
<p>Other Feature Spec Functions: 
<code><a href="#topic+dataset_use_spec">dataset_use_spec</a>()</code>,
<code><a href="#topic+feature_spec">feature_spec</a>()</code>,
<code><a href="#topic+fit.FeatureSpec">fit.FeatureSpec</a>()</code>,
<code><a href="#topic+step_bucketized_column">step_bucketized_column</a>()</code>,
<code><a href="#topic+step_categorical_column_with_hash_bucket">step_categorical_column_with_hash_bucket</a>()</code>,
<code><a href="#topic+step_categorical_column_with_identity">step_categorical_column_with_identity</a>()</code>,
<code><a href="#topic+step_categorical_column_with_vocabulary_file">step_categorical_column_with_vocabulary_file</a>()</code>,
<code><a href="#topic+step_categorical_column_with_vocabulary_list">step_categorical_column_with_vocabulary_list</a>()</code>,
<code><a href="#topic+step_embedding_column">step_embedding_column</a>()</code>,
<code><a href="#topic+step_indicator_column">step_indicator_column</a>()</code>,
<code><a href="#topic+step_numeric_column">step_numeric_column</a>()</code>,
<code><a href="#topic+step_remove_column">step_remove_column</a>()</code>,
<code><a href="#topic+step_shared_embeddings_column">step_shared_embeddings_column</a>()</code>,
<code><a href="#topic+steps">steps</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library(tfdatasets)
data(hearts)
file &lt;- tempfile()
writeLines(unique(hearts$thal), file)
hearts &lt;- tensor_slices_dataset(hearts) %&gt;% dataset_batch(32)

# use the formula interface
spec &lt;- feature_spec(hearts, target ~ age) %&gt;%
  step_numeric_column(age) %&gt;%
  step_bucketized_column(age, boundaries = c(10, 20, 30))
spec_fit &lt;- fit(spec)
final_dataset &lt;- hearts %&gt;% dataset_use_spec(spec_fit)

## End(Not run)
</code></pre>

<hr>
<h2 id='step_embedding_column'>Creates embeddings columns</h2><span id='topic+step_embedding_column'></span>

<h3>Description</h3>

<p>Use this step to create ambeddings columns from categorical
columns.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>step_embedding_column(
  spec,
  ...,
  dimension = function(x) {
     as.integer(x^0.25)
 },
  combiner = "mean",
  initializer = NULL,
  ckpt_to_load_from = NULL,
  tensor_name_in_ckpt = NULL,
  max_norm = NULL,
  trainable = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="step_embedding_column_+3A_spec">spec</code></td>
<td>
<p>A feature specification created with <code><a href="#topic+feature_spec">feature_spec()</a></code>.</p>
</td></tr>
<tr><td><code id="step_embedding_column_+3A_...">...</code></td>
<td>
<p>Comma separated list of variable names to apply the step. <a href="#topic+selectors">selectors</a> can also be used.</p>
</td></tr>
<tr><td><code id="step_embedding_column_+3A_dimension">dimension</code></td>
<td>
<p>An integer specifying dimension of the embedding, must be &gt; 0.
Can also be a function of the size of the vocabulary.</p>
</td></tr>
<tr><td><code id="step_embedding_column_+3A_combiner">combiner</code></td>
<td>
<p>A string specifying how to reduce if there are multiple entries in
a single row. Currently 'mean', 'sqrtn' and 'sum' are supported, with 'mean' the
default. 'sqrtn' often achieves good accuracy, in particular with bag-of-words
columns. Each of this can be thought as example level normalizations on
the column. For more information, see <code>tf.embedding_lookup_sparse</code>.</p>
</td></tr>
<tr><td><code id="step_embedding_column_+3A_initializer">initializer</code></td>
<td>
<p>A variable initializer function to be used in embedding
variable initialization. If not specified, defaults to
<code>tf.truncated_normal_initializer</code> with mean <code>0.0</code> and standard deviation
<code>1/sqrt(dimension)</code>.</p>
</td></tr>
<tr><td><code id="step_embedding_column_+3A_ckpt_to_load_from">ckpt_to_load_from</code></td>
<td>
<p>String representing checkpoint name/pattern from
which to restore column weights. Required if <code>tensor_name_in_ckpt</code> is
not <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="step_embedding_column_+3A_tensor_name_in_ckpt">tensor_name_in_ckpt</code></td>
<td>
<p>Name of the Tensor in ckpt_to_load_from from which to
restore the column weights. Required if <code>ckpt_to_load_from</code> is not <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="step_embedding_column_+3A_max_norm">max_norm</code></td>
<td>
<p>If not <code>NULL</code>, embedding values are l2-normalized to this value.</p>
</td></tr>
<tr><td><code id="step_embedding_column_+3A_trainable">trainable</code></td>
<td>
<p>Whether or not the embedding is trainable. Default is <code>TRUE</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code>FeatureSpec</code> object.
</p>


<h3>See Also</h3>

<p><a href="#topic+steps">steps</a> for a complete list of allowed steps.
</p>
<p>Other Feature Spec Functions: 
<code><a href="#topic+dataset_use_spec">dataset_use_spec</a>()</code>,
<code><a href="#topic+feature_spec">feature_spec</a>()</code>,
<code><a href="#topic+fit.FeatureSpec">fit.FeatureSpec</a>()</code>,
<code><a href="#topic+step_bucketized_column">step_bucketized_column</a>()</code>,
<code><a href="#topic+step_categorical_column_with_hash_bucket">step_categorical_column_with_hash_bucket</a>()</code>,
<code><a href="#topic+step_categorical_column_with_identity">step_categorical_column_with_identity</a>()</code>,
<code><a href="#topic+step_categorical_column_with_vocabulary_file">step_categorical_column_with_vocabulary_file</a>()</code>,
<code><a href="#topic+step_categorical_column_with_vocabulary_list">step_categorical_column_with_vocabulary_list</a>()</code>,
<code><a href="#topic+step_crossed_column">step_crossed_column</a>()</code>,
<code><a href="#topic+step_indicator_column">step_indicator_column</a>()</code>,
<code><a href="#topic+step_numeric_column">step_numeric_column</a>()</code>,
<code><a href="#topic+step_remove_column">step_remove_column</a>()</code>,
<code><a href="#topic+step_shared_embeddings_column">step_shared_embeddings_column</a>()</code>,
<code><a href="#topic+steps">steps</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library(tfdatasets)
data(hearts)
file &lt;- tempfile()
writeLines(unique(hearts$thal), file)
hearts &lt;- tensor_slices_dataset(hearts) %&gt;% dataset_batch(32)

# use the formula interface
spec &lt;- feature_spec(hearts, target ~ thal) %&gt;%
  step_categorical_column_with_vocabulary_list(thal) %&gt;%
  step_embedding_column(thal, dimension = 3)
spec_fit &lt;- fit(spec)
final_dataset &lt;- hearts %&gt;% dataset_use_spec(spec_fit)

## End(Not run)
</code></pre>

<hr>
<h2 id='step_indicator_column'>Creates Indicator Columns</h2><span id='topic+step_indicator_column'></span>

<h3>Description</h3>

<p>Use this step to create indicator columns from categorical columns.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>step_indicator_column(spec, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="step_indicator_column_+3A_spec">spec</code></td>
<td>
<p>A feature specification created with <code><a href="#topic+feature_spec">feature_spec()</a></code>.</p>
</td></tr>
<tr><td><code id="step_indicator_column_+3A_...">...</code></td>
<td>
<p>Comma separated list of variable names to apply the step. <a href="#topic+selectors">selectors</a> can also be used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code>FeatureSpec</code> object.
</p>


<h3>See Also</h3>

<p><a href="#topic+steps">steps</a> for a complete list of allowed steps.
</p>
<p>Other Feature Spec Functions: 
<code><a href="#topic+dataset_use_spec">dataset_use_spec</a>()</code>,
<code><a href="#topic+feature_spec">feature_spec</a>()</code>,
<code><a href="#topic+fit.FeatureSpec">fit.FeatureSpec</a>()</code>,
<code><a href="#topic+step_bucketized_column">step_bucketized_column</a>()</code>,
<code><a href="#topic+step_categorical_column_with_hash_bucket">step_categorical_column_with_hash_bucket</a>()</code>,
<code><a href="#topic+step_categorical_column_with_identity">step_categorical_column_with_identity</a>()</code>,
<code><a href="#topic+step_categorical_column_with_vocabulary_file">step_categorical_column_with_vocabulary_file</a>()</code>,
<code><a href="#topic+step_categorical_column_with_vocabulary_list">step_categorical_column_with_vocabulary_list</a>()</code>,
<code><a href="#topic+step_crossed_column">step_crossed_column</a>()</code>,
<code><a href="#topic+step_embedding_column">step_embedding_column</a>()</code>,
<code><a href="#topic+step_numeric_column">step_numeric_column</a>()</code>,
<code><a href="#topic+step_remove_column">step_remove_column</a>()</code>,
<code><a href="#topic+step_shared_embeddings_column">step_shared_embeddings_column</a>()</code>,
<code><a href="#topic+steps">steps</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library(tfdatasets)
data(hearts)
file &lt;- tempfile()
writeLines(unique(hearts$thal), file)
hearts &lt;- tensor_slices_dataset(hearts) %&gt;% dataset_batch(32)

# use the formula interface
spec &lt;- feature_spec(hearts, target ~ thal) %&gt;%
  step_categorical_column_with_vocabulary_list(thal) %&gt;%
  step_indicator_column(thal)
spec_fit &lt;- fit(spec)
final_dataset &lt;- hearts %&gt;% dataset_use_spec(spec_fit)

## End(Not run)
</code></pre>

<hr>
<h2 id='step_numeric_column'>Creates a numeric column specification</h2><span id='topic+step_numeric_column'></span>

<h3>Description</h3>

<p><code>step_numeric_column</code> creates a numeric column specification. It can also be
used to normalize numeric columns.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>step_numeric_column(
  spec,
  ...,
  shape = 1L,
  default_value = NULL,
  dtype = tf$float32,
  normalizer_fn = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="step_numeric_column_+3A_spec">spec</code></td>
<td>
<p>A feature specification created with <code><a href="#topic+feature_spec">feature_spec()</a></code>.</p>
</td></tr>
<tr><td><code id="step_numeric_column_+3A_...">...</code></td>
<td>
<p>Comma separated list of variable names to apply the step. <a href="#topic+selectors">selectors</a> can also be used.</p>
</td></tr>
<tr><td><code id="step_numeric_column_+3A_shape">shape</code></td>
<td>
<p>An iterable of integers specifies the shape of the Tensor. An integer can be given
which means a single dimension Tensor with given width. The Tensor representing the column will
have the shape of <code>batch_size</code> + <code>shape</code>.</p>
</td></tr>
<tr><td><code id="step_numeric_column_+3A_default_value">default_value</code></td>
<td>
<p>A single value compatible with <code>dtype</code> or an iterable of values compatible
with <code>dtype</code> which the column takes on during <code>tf.Example</code> parsing if data is missing. A
default value of <code>NULL</code> will cause <code>tf.parse_example</code> to fail if an example does not contain
this column. If a single value is provided, the same value will be applied as
the default value for every item. If an iterable of values is provided, the shape
of the default_value should be equal to the given shape.</p>
</td></tr>
<tr><td><code id="step_numeric_column_+3A_dtype">dtype</code></td>
<td>
<p>defines the type of values. Default value is <code>tf$float32</code>. Must be a non-quantized,
real integer or floating point type.</p>
</td></tr>
<tr><td><code id="step_numeric_column_+3A_normalizer_fn">normalizer_fn</code></td>
<td>
<p>If not <code>NULL</code>, a function that can be used to normalize the value
of the tensor after default_value is applied for parsing. Normalizer function takes the
input Tensor as its argument, and returns the output Tensor. (e.g. <code style="white-space: pre;">&#8288;function(x) (x - 3.0) / 4.2)&#8288;</code>.
Please note that even though the most common use case of this function is normalization, it
can be used for any kind of Tensorflow transformations. You can also a pre-made <a href="#topic+scaler">scaler</a>, in
this case a function will be created after <a href="#topic+fit.FeatureSpec">fit.FeatureSpec</a> is called on the feature specification.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code>FeatureSpec</code> object.
</p>


<h3>See Also</h3>

<p><a href="#topic+steps">steps</a> for a complete list of allowed steps.
</p>
<p>Other Feature Spec Functions: 
<code><a href="#topic+dataset_use_spec">dataset_use_spec</a>()</code>,
<code><a href="#topic+feature_spec">feature_spec</a>()</code>,
<code><a href="#topic+fit.FeatureSpec">fit.FeatureSpec</a>()</code>,
<code><a href="#topic+step_bucketized_column">step_bucketized_column</a>()</code>,
<code><a href="#topic+step_categorical_column_with_hash_bucket">step_categorical_column_with_hash_bucket</a>()</code>,
<code><a href="#topic+step_categorical_column_with_identity">step_categorical_column_with_identity</a>()</code>,
<code><a href="#topic+step_categorical_column_with_vocabulary_file">step_categorical_column_with_vocabulary_file</a>()</code>,
<code><a href="#topic+step_categorical_column_with_vocabulary_list">step_categorical_column_with_vocabulary_list</a>()</code>,
<code><a href="#topic+step_crossed_column">step_crossed_column</a>()</code>,
<code><a href="#topic+step_embedding_column">step_embedding_column</a>()</code>,
<code><a href="#topic+step_indicator_column">step_indicator_column</a>()</code>,
<code><a href="#topic+step_remove_column">step_remove_column</a>()</code>,
<code><a href="#topic+step_shared_embeddings_column">step_shared_embeddings_column</a>()</code>,
<code><a href="#topic+steps">steps</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library(tfdatasets)
data(hearts)
hearts &lt;- tensor_slices_dataset(hearts) %&gt;% dataset_batch(32)

# use the formula interface
spec &lt;- feature_spec(hearts, target ~ age) %&gt;%
  step_numeric_column(age, normalizer_fn = standard_scaler())

spec_fit &lt;- fit(spec)
final_dataset &lt;- hearts %&gt;% dataset_use_spec(spec_fit)

## End(Not run)

</code></pre>

<hr>
<h2 id='step_remove_column'>Creates a step that can remove columns</h2><span id='topic+step_remove_column'></span>

<h3>Description</h3>

<p>Removes features of the feature specification.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>step_remove_column(spec, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="step_remove_column_+3A_spec">spec</code></td>
<td>
<p>A feature specification created with <code><a href="#topic+feature_spec">feature_spec()</a></code>.</p>
</td></tr>
<tr><td><code id="step_remove_column_+3A_...">...</code></td>
<td>
<p>Comma separated list of variable names to apply the step. <a href="#topic+selectors">selectors</a> can also be used.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code>FeatureSpec</code> object.
</p>


<h3>See Also</h3>

<p><a href="#topic+steps">steps</a> for a complete list of allowed steps.
</p>
<p>Other Feature Spec Functions: 
<code><a href="#topic+dataset_use_spec">dataset_use_spec</a>()</code>,
<code><a href="#topic+feature_spec">feature_spec</a>()</code>,
<code><a href="#topic+fit.FeatureSpec">fit.FeatureSpec</a>()</code>,
<code><a href="#topic+step_bucketized_column">step_bucketized_column</a>()</code>,
<code><a href="#topic+step_categorical_column_with_hash_bucket">step_categorical_column_with_hash_bucket</a>()</code>,
<code><a href="#topic+step_categorical_column_with_identity">step_categorical_column_with_identity</a>()</code>,
<code><a href="#topic+step_categorical_column_with_vocabulary_file">step_categorical_column_with_vocabulary_file</a>()</code>,
<code><a href="#topic+step_categorical_column_with_vocabulary_list">step_categorical_column_with_vocabulary_list</a>()</code>,
<code><a href="#topic+step_crossed_column">step_crossed_column</a>()</code>,
<code><a href="#topic+step_embedding_column">step_embedding_column</a>()</code>,
<code><a href="#topic+step_indicator_column">step_indicator_column</a>()</code>,
<code><a href="#topic+step_numeric_column">step_numeric_column</a>()</code>,
<code><a href="#topic+step_shared_embeddings_column">step_shared_embeddings_column</a>()</code>,
<code><a href="#topic+steps">steps</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library(tfdatasets)
data(hearts)
hearts &lt;- tensor_slices_dataset(hearts) %&gt;% dataset_batch(32)

# use the formula interface
spec &lt;- feature_spec(hearts, target ~ age) %&gt;%
  step_numeric_column(age, normalizer_fn = scaler_standard()) %&gt;%
  step_bucketized_column(age, boundaries = c(20, 50)) %&gt;%
  step_remove_column(age)

spec_fit &lt;- fit(spec)
final_dataset &lt;- hearts %&gt;% dataset_use_spec(spec_fit)

## End(Not run)

</code></pre>

<hr>
<h2 id='step_shared_embeddings_column'>Creates shared embeddings for categorical columns</h2><span id='topic+step_shared_embeddings_column'></span>

<h3>Description</h3>

<p>This is similar to <a href="#topic+step_embedding_column">step_embedding_column</a>, except that it produces a list of
embedding columns that share the same embedding weights.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>step_shared_embeddings_column(
  spec,
  ...,
  dimension,
  combiner = "mean",
  initializer = NULL,
  shared_embedding_collection_name = NULL,
  ckpt_to_load_from = NULL,
  tensor_name_in_ckpt = NULL,
  max_norm = NULL,
  trainable = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="step_shared_embeddings_column_+3A_spec">spec</code></td>
<td>
<p>A feature specification created with <code><a href="#topic+feature_spec">feature_spec()</a></code>.</p>
</td></tr>
<tr><td><code id="step_shared_embeddings_column_+3A_...">...</code></td>
<td>
<p>Comma separated list of variable names to apply the step. <a href="#topic+selectors">selectors</a> can also be used.</p>
</td></tr>
<tr><td><code id="step_shared_embeddings_column_+3A_dimension">dimension</code></td>
<td>
<p>An integer specifying dimension of the embedding, must be &gt; 0.
Can also be a function of the size of the vocabulary.</p>
</td></tr>
<tr><td><code id="step_shared_embeddings_column_+3A_combiner">combiner</code></td>
<td>
<p>A string specifying how to reduce if there are multiple entries in
a single row. Currently 'mean', 'sqrtn' and 'sum' are supported, with 'mean' the
default. 'sqrtn' often achieves good accuracy, in particular with bag-of-words
columns. Each of this can be thought as example level normalizations on
the column. For more information, see <code>tf.embedding_lookup_sparse</code>.</p>
</td></tr>
<tr><td><code id="step_shared_embeddings_column_+3A_initializer">initializer</code></td>
<td>
<p>A variable initializer function to be used in embedding
variable initialization. If not specified, defaults to
<code>tf.truncated_normal_initializer</code> with mean <code>0.0</code> and standard deviation
<code>1/sqrt(dimension)</code>.</p>
</td></tr>
<tr><td><code id="step_shared_embeddings_column_+3A_shared_embedding_collection_name">shared_embedding_collection_name</code></td>
<td>
<p>Optional collective name of
these columns. If not given, a reasonable name will be chosen based on
the names of categorical_columns.</p>
</td></tr>
<tr><td><code id="step_shared_embeddings_column_+3A_ckpt_to_load_from">ckpt_to_load_from</code></td>
<td>
<p>String representing checkpoint name/pattern from
which to restore column weights. Required if <code>tensor_name_in_ckpt</code> is
not <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="step_shared_embeddings_column_+3A_tensor_name_in_ckpt">tensor_name_in_ckpt</code></td>
<td>
<p>Name of the Tensor in ckpt_to_load_from from which to
restore the column weights. Required if <code>ckpt_to_load_from</code> is not <code>NULL</code>.</p>
</td></tr>
<tr><td><code id="step_shared_embeddings_column_+3A_max_norm">max_norm</code></td>
<td>
<p>If not <code>NULL</code>, embedding values are l2-normalized to this value.</p>
</td></tr>
<tr><td><code id="step_shared_embeddings_column_+3A_trainable">trainable</code></td>
<td>
<p>Whether or not the embedding is trainable. Default is <code>TRUE</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a <code>FeatureSpec</code> object.
</p>


<h3>Note</h3>

<p>Does not work in the eager mode.
</p>


<h3>See Also</h3>

<p><a href="#topic+steps">steps</a> for a complete list of allowed steps.
</p>
<p>Other Feature Spec Functions: 
<code><a href="#topic+dataset_use_spec">dataset_use_spec</a>()</code>,
<code><a href="#topic+feature_spec">feature_spec</a>()</code>,
<code><a href="#topic+fit.FeatureSpec">fit.FeatureSpec</a>()</code>,
<code><a href="#topic+step_bucketized_column">step_bucketized_column</a>()</code>,
<code><a href="#topic+step_categorical_column_with_hash_bucket">step_categorical_column_with_hash_bucket</a>()</code>,
<code><a href="#topic+step_categorical_column_with_identity">step_categorical_column_with_identity</a>()</code>,
<code><a href="#topic+step_categorical_column_with_vocabulary_file">step_categorical_column_with_vocabulary_file</a>()</code>,
<code><a href="#topic+step_categorical_column_with_vocabulary_list">step_categorical_column_with_vocabulary_list</a>()</code>,
<code><a href="#topic+step_crossed_column">step_crossed_column</a>()</code>,
<code><a href="#topic+step_embedding_column">step_embedding_column</a>()</code>,
<code><a href="#topic+step_indicator_column">step_indicator_column</a>()</code>,
<code><a href="#topic+step_numeric_column">step_numeric_column</a>()</code>,
<code><a href="#topic+step_remove_column">step_remove_column</a>()</code>,
<code><a href="#topic+steps">steps</a></code>
</p>

<hr>
<h2 id='steps'>Steps for feature columns specification.</h2><span id='topic+steps'></span>

<h3>Description</h3>

<p>List of steps that can be used to specify columns in the <code>feature_spec</code> interface.
</p>


<h3>Steps</h3>


<ul>
<li> <p><code><a href="#topic+step_numeric_column">step_numeric_column()</a></code> to define numeric columns.
</p>
</li>
<li> <p><code><a href="#topic+step_categorical_column_with_vocabulary_list">step_categorical_column_with_vocabulary_list()</a></code> to define categorical columns.
</p>
</li>
<li> <p><code><a href="#topic+step_categorical_column_with_hash_bucket">step_categorical_column_with_hash_bucket()</a></code> to define categorical columns
where ids are set by hashing.
</p>
</li>
<li> <p><code><a href="#topic+step_categorical_column_with_identity">step_categorical_column_with_identity()</a></code> to define categorical columns
represented by integers in the range <code style="white-space: pre;">&#8288;[0-num_buckets)&#8288;</code>.
</p>
</li>
<li> <p><code><a href="#topic+step_categorical_column_with_vocabulary_file">step_categorical_column_with_vocabulary_file()</a></code> to define categorical columns
when their vocabulary is available in a file.
</p>
</li>
<li> <p><code><a href="#topic+step_indicator_column">step_indicator_column()</a></code> to create indicator columns from categorical columns.
</p>
</li>
<li> <p><code><a href="#topic+step_embedding_column">step_embedding_column()</a></code> to create embeddings columns from categorical columns.
</p>
</li>
<li> <p><code><a href="#topic+step_bucketized_column">step_bucketized_column()</a></code> to create bucketized columns from numeric columns.
</p>
</li>
<li> <p><code><a href="#topic+step_crossed_column">step_crossed_column()</a></code> to perform crosses of categorical columns.
</p>
</li>
<li> <p><code><a href="#topic+step_shared_embeddings_column">step_shared_embeddings_column()</a></code> to share embeddings between a list of
categorical columns.
</p>
</li>
<li> <p><code><a href="#topic+step_remove_column">step_remove_column()</a></code> to remove columns from the specification.
</p>
</li></ul>



<h3>See Also</h3>


<ul>
<li> <p><a href="#topic+selectors">selectors</a> for a list of selectors that can be used to specify variables.
</p>
</li></ul>

<p>Other Feature Spec Functions: 
<code><a href="#topic+dataset_use_spec">dataset_use_spec</a>()</code>,
<code><a href="#topic+feature_spec">feature_spec</a>()</code>,
<code><a href="#topic+fit.FeatureSpec">fit.FeatureSpec</a>()</code>,
<code><a href="#topic+step_bucketized_column">step_bucketized_column</a>()</code>,
<code><a href="#topic+step_categorical_column_with_hash_bucket">step_categorical_column_with_hash_bucket</a>()</code>,
<code><a href="#topic+step_categorical_column_with_identity">step_categorical_column_with_identity</a>()</code>,
<code><a href="#topic+step_categorical_column_with_vocabulary_file">step_categorical_column_with_vocabulary_file</a>()</code>,
<code><a href="#topic+step_categorical_column_with_vocabulary_list">step_categorical_column_with_vocabulary_list</a>()</code>,
<code><a href="#topic+step_crossed_column">step_crossed_column</a>()</code>,
<code><a href="#topic+step_embedding_column">step_embedding_column</a>()</code>,
<code><a href="#topic+step_indicator_column">step_indicator_column</a>()</code>,
<code><a href="#topic+step_numeric_column">step_numeric_column</a>()</code>,
<code><a href="#topic+step_remove_column">step_remove_column</a>()</code>,
<code><a href="#topic+step_shared_embeddings_column">step_shared_embeddings_column</a>()</code>
</p>

<hr>
<h2 id='tensor_slices_dataset'>Creates a dataset whose elements are slices of the given tensors.</h2><span id='topic+tensor_slices_dataset'></span>

<h3>Description</h3>

<p>Creates a dataset whose elements are slices of the given tensors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tensor_slices_dataset(tensors)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tensor_slices_dataset_+3A_tensors">tensors</code></td>
<td>
<p>A nested structure of tensors, each having the same size in
the 0th dimension.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A dataset.
</p>


<h3>See Also</h3>

<p>Other tensor datasets: 
<code><a href="#topic+sparse_tensor_slices_dataset">sparse_tensor_slices_dataset</a>()</code>,
<code><a href="#topic+tensors_dataset">tensors_dataset</a>()</code>
</p>

<hr>
<h2 id='tensors_dataset'>Creates a dataset with a single element, comprising the given tensors.</h2><span id='topic+tensors_dataset'></span>

<h3>Description</h3>

<p>Creates a dataset with a single element, comprising the given tensors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tensors_dataset(tensors)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tensors_dataset_+3A_tensors">tensors</code></td>
<td>
<p>A nested structure of tensors.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A dataset.
</p>


<h3>See Also</h3>

<p>Other tensor datasets: 
<code><a href="#topic+sparse_tensor_slices_dataset">sparse_tensor_slices_dataset</a>()</code>,
<code><a href="#topic+tensor_slices_dataset">tensor_slices_dataset</a>()</code>
</p>

<hr>
<h2 id='text_line_dataset'>A dataset comprising lines from one or more text files.</h2><span id='topic+text_line_dataset'></span>

<h3>Description</h3>

<p>A dataset comprising lines from one or more text files.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>text_line_dataset(
  filenames,
  compression_type = NULL,
  record_spec = NULL,
  parallel_records = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="text_line_dataset_+3A_filenames">filenames</code></td>
<td>
<p>String(s) specifying one or more filenames</p>
</td></tr>
<tr><td><code id="text_line_dataset_+3A_compression_type">compression_type</code></td>
<td>
<p>A string, one of: <code>NULL</code> (no compression), <code>"ZLIB"</code>, or
<code>"GZIP"</code>.</p>
</td></tr>
<tr><td><code id="text_line_dataset_+3A_record_spec">record_spec</code></td>
<td>
<p>(Optional) Specification used to decode delimimted text lines
into records (see <code><a href="#topic+delim_record_spec">delim_record_spec()</a></code>).</p>
</td></tr>
<tr><td><code id="text_line_dataset_+3A_parallel_records">parallel_records</code></td>
<td>
<p>(Optional) An integer, representing the number of
records to decode in parallel. If not specified, records will be
processed sequentially.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A dataset
</p>

<hr>
<h2 id='tfrecord_dataset'>A dataset comprising records from one or more TFRecord files.</h2><span id='topic+tfrecord_dataset'></span>

<h3>Description</h3>

<p>A dataset comprising records from one or more TFRecord files.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tfrecord_dataset(
  filenames,
  compression_type = NULL,
  buffer_size = NULL,
  num_parallel_reads = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tfrecord_dataset_+3A_filenames">filenames</code></td>
<td>
<p>String(s) specifying one or more filenames</p>
</td></tr>
<tr><td><code id="tfrecord_dataset_+3A_compression_type">compression_type</code></td>
<td>
<p>A string, one of: <code>NULL</code> (no compression), <code>"ZLIB"</code>, or
<code>"GZIP"</code>.</p>
</td></tr>
<tr><td><code id="tfrecord_dataset_+3A_buffer_size">buffer_size</code></td>
<td>
<p>An integer representing the number of bytes in the read buffer. (0
means no buffering).</p>
</td></tr>
<tr><td><code id="tfrecord_dataset_+3A_num_parallel_reads">num_parallel_reads</code></td>
<td>
<p>An integer representing the number of files to read in
parallel. Defaults to reading files sequentially.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If the dataset encodes a set of TFExample instances, then they can be decoded
into named records using the <code><a href="#topic+dataset_map">dataset_map()</a></code> function (see example below).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

# Creates a dataset that reads all of the examples from two files, and extracts
# the image and label features.
filenames &lt;- c("/var/data/file1.tfrecord", "/var/data/file2.tfrecord")
dataset &lt;- tfrecord_dataset(filenames) %&gt;%
  dataset_map(function(example_proto) {
    features &lt;- list(
      image = tf$FixedLenFeature(shape(), tf$string, default_value = ""),
      label = tf$FixedLenFeature(shape(), tf$int32, default_value = 0L)
    )
    tf$parse_single_example(example_proto, features)
  })

## End(Not run)

</code></pre>

<hr>
<h2 id='until_out_of_range'>Execute code that traverses a dataset until an out of range condition occurs</h2><span id='topic+until_out_of_range'></span><span id='topic+out_of_range_handler'></span>

<h3>Description</h3>

<p>Execute code that traverses a dataset until an out of range condition occurs
</p>


<h3>Usage</h3>

<pre><code class='language-R'>until_out_of_range(expr)

out_of_range_handler(e)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="until_out_of_range_+3A_expr">expr</code></td>
<td>
<p>Expression to execute (will be executed multiple times until
the condition occurs)</p>
</td></tr>
<tr><td><code id="until_out_of_range_+3A_e">e</code></td>
<td>
<p>Error object</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When a dataset iterator reaches the end, an out of range runtime error
will occur. This function will catch and ignore the error when it occurs.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library(tfdatasets)
dataset &lt;- text_line_dataset("mtcars.csv", record_spec = mtcars_spec) %&gt;%
  dataset_batch(128) %&gt;%
  dataset_repeat(10) %&gt;%
  dataset_prepare(x = c(mpg, disp), y = cyl)

iter &lt;- make_iterator_one_shot(dataset)
next_batch &lt;- iterator_get_next(iter)

until_out_of_range({
  batch &lt;- sess$run(next_batch)
  # use batch$x and batch$y tensors
})

## End(Not run)

</code></pre>

<hr>
<h2 id='with_dataset'>Execute code that traverses a dataset</h2><span id='topic+with_dataset'></span>

<h3>Description</h3>

<p>Execute code that traverses a dataset
</p>


<h3>Usage</h3>

<pre><code class='language-R'>with_dataset(expr)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="with_dataset_+3A_expr">expr</code></td>
<td>
<p>Expression to execute</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When a dataset iterator reaches the end, an out of range runtime error
will occur. You can catch and ignore the error when it occurs by wrapping
your iteration code in a call to <code>with_dataset()</code> (see the example
below for an illustration).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library(tfdatasets)
dataset &lt;- text_line_dataset("mtcars.csv", record_spec = mtcars_spec) %&gt;%
  dataset_prepare(x = c(mpg, disp), y = cyl) %&gt;%
  dataset_batch(128) %&gt;%
  dataset_repeat(10)

iter &lt;- make_iterator_one_shot(dataset)
next_batch &lt;- iterator_get_next(iter)

with_dataset({
  while(TRUE) {
    batch &lt;- sess$run(next_batch)
    # use batch$x and batch$y tensors
  }
})

## End(Not run)

</code></pre>

<hr>
<h2 id='zip_datasets'>Creates a dataset by zipping together the given datasets.</h2><span id='topic+zip_datasets'></span>

<h3>Description</h3>

<p>Merges datasets together into pairs or tuples that contain an element from
each dataset.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>zip_datasets(...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="zip_datasets_+3A_...">...</code></td>
<td>
<p>Datasets to zip (or a single argument with a list or list of lists of datasets).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A dataset
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
