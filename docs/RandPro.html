<!DOCTYPE html><html><head><title>Help for package RandPro</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {RandPro}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#classify'><p>Classification Function</p></a></li>
<li><a href='#dimension'><p>Function to determine the required number of dimension for generating the projection matrix</p></a></li>
<li><a href='#form_matrix'><p>Forms the Projection Matrix</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Random Projection with Classification</td>
</tr>
<tr>
<td>Version:</td>
<td>0.2.2</td>
</tr>
<tr>
<td>Author:</td>
<td>Aghila G, Siddharth R </td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Siddharth R &lt;r.siddharthcse@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Performs random projection using Johnson-Lindenstrauss (JL) Lemma (see William B.Johnson and Joram Lindenstrauss (1984) &lt;<a href="https://doi.org/10.1090%2Fconm%2F026%2F737400">doi:10.1090/conm/026/737400</a>&gt;). Random Projection is a dimension reduction technique, where the data in the high dimensional space is projected into the low dimensional space using JL transform. The original high dimensional data matrix is multiplied with the low dimensional projection matrix which results in reduced matrix. The projection matrix can be generated using the projection function that is independent to the original data. Then finally apply the classification task on the projected data.  </td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>Depends:</td>
<td>caret</td>
</tr>
<tr>
<td>Imports:</td>
<td>stats, e1071</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>TRUE</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.1</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2020-07-14 18:59:44 UTC; siddhu</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2020-07-19 14:50:03 UTC</td>
</tr>
</table>
<hr>
<h2 id='classify'>Classification Function</h2><span id='topic+classify'></span>

<h3>Description</h3>

<p>The classify() function allows the user to combine the task of random projection based dimension reduction
and classification within a single function. The dimension of the training data and test data was reduced
by the  value returned from the dimension() method. Then the projection matrix was generated using
form_matrix() function based on the input paramater &quot;projection&quot;.Then the training data and test data was
projected into the low dimensional space by multiplying with the projection matrix. At last the reduced matrix
was given to the classifier. The confusion matrix is the output of the classifier where we can calculate
the performance of the classifier.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>classify(
  train_data,
  test_data,
  train_label,
  test_label,
  eps = 0.1,
  projection = "gaussian",
  classifier = "knn"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="classify_+3A_train_data">train_data</code></td>
<td>
<p>- Training data of either matrix or data frame</p>
</td></tr>
<tr><td><code id="classify_+3A_test_data">test_data</code></td>
<td>
<p>- Test data of either matrix or data frame</p>
</td></tr>
<tr><td><code id="classify_+3A_train_label">train_label</code></td>
<td>
<p>- Training label of either vector or data frame</p>
</td></tr>
<tr><td><code id="classify_+3A_test_label">test_label</code></td>
<td>
<p>- Test label of either vector or data frame</p>
</td></tr>
<tr><td><code id="classify_+3A_eps">eps</code></td>
<td>
<p>- Epsilon with default 0.1</p>
</td></tr>
<tr><td><code id="classify_+3A_projection">projection</code></td>
<td>
<p>- projection function with default &quot;gaussian&quot;</p>
</td></tr>
<tr><td><code id="classify_+3A_classifier">classifier</code></td>
<td>
<p>- classifier with default &quot;knn&quot;</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The parameters train_data,test_data,train_label and test_label are mandatory arguments. The eps is the error
tolerance paramater. The value of eps must be <code class="reqn">0.0&lt;eps&lt;1.0</code>. The default value of eps is 0.1 that means 10 percentage of
error is acceptable during projection. The supported projection functions are gaussian, probability, li, and
achlioptas.The default projection method is &quot;gaussian&quot;. The complete detail of the projection function is given in
form_matrix() function. The final argument &quot;classifier&quot; in the function defines the classifier to train the model.
The supported classifier for classification task are
</p>
<p>&quot;knn&quot; - k-nearest neighbor classification
</p>
<p>&quot;svmlinear&quot; - Support Vector Machine
</p>
<p>&quot;nb&quot; - Naive Bayes Classifier
</p>


<h3>Value</h3>

<p>Confusion Matrix
</p>


<h3>Author(s)</h3>

<p>Aghila G
</p>
<p>Siddharth R
</p>


<h3>References</h3>

<p>[1] Cannings, T. I. and Samworth, R. J. &quot;Random projection ensemble classification(2015)&quot;.
</p>
<p>[2] Ella Bingham and Heikki Mannila, &quot;Random projection in dimensionality reduction: Applications to image and text data(2001)&quot;.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load Library
library(RandPro)

#Load Iris Data
data("iris")

#Split the data into training set and test set of 75:25 ratio.
set.seed(101)
sample &lt;- sample.int(n = nrow(iris), size = floor(.75*nrow(iris)), replace = FALSE)
trainn &lt;- iris[sample, ]
testt  &lt;- iris[-sample,]

#Extract the train label and test label
trainl &lt;- trainn$Species
testl &lt;- testt$Species
typeof(trainl)

#Remove the label from training set and test set
trainn &lt;- trainn[,1:4]
testt &lt;- testt[,1:4]

#classify the Iris data with default K-NN Classifier.
res &lt;- classify(trainn,testt,trainl,testl)
res

</code></pre>

<hr>
<h2 id='dimension'>Function to determine the required number of dimension for generating the projection matrix</h2><span id='topic+dimension'></span>

<h3>Description</h3>

<p>Johnson-Lindenstrauss (JL) lemma is the heart of random projection.
The lemma states that a small set of points in a high-dimensional space
can be embedded into low dimensional space in such a way that distances between the
points are nearly preserved.
The lemma has been used in dimensionality reduction, compressed sensing, manifold learning and graph embedding.
The epsilon is the error tolerant parameter and it is inversely
proportional to the accuracy of the result. The higher error tolerant level decreases the number of
dimension and also the computation complexity with the marginal loss of accuracy.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dimension(sample, epsilon = 0.1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dimension_+3A_sample">sample</code></td>
<td>
<p>- number of samples</p>
</td></tr>
<tr><td><code id="dimension_+3A_epsilon">epsilon</code></td>
<td>
<p>- error tolerance level with default value 0.1</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function dimension() is used to find the minimum dimension required to project the data from high dimensional
space to low dimensional space. The number of sample and error tolerant level has been passed as an input argument
to the function dimension() .
It will return the size of the random subspace to guarantee a bounded distortion introduced by
the random projection.
</p>


<h3>Value</h3>

<p>minimum number of dimension required to maintain the pair wise distance between any two
points with the controlled amount of error(eps)
</p>


<h3>Author(s)</h3>

<p>Aghila G
</p>
<p>Siddharth R
</p>


<h3>References</h3>

<p>[1] William B.Johnson, Joram Lindenstrauss, &quot;Extension of Lipschitz mappings into a Hilbert space (1984)&quot;
</p>
<p>[2] Sanjoy Dasgupta , Anupam Gupta &quot;An elementary proof of a theorem of Johnson and Lindenstrauss (2003)&quot;
</p>


<h3>See Also</h3>

<p><a href="http://cseweb.ucsd.edu/~dasgupta/papers/jl.pdf">Johnson-Lindenstrauss Elementary Proof</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#load library
library(RandPro)

#Calculate minimum dimension using eps =0.5 for 1000000 sample
y &lt;- dimension(1000000,0.5)

#Calculating minimum dimension using different epsilon value for 1000000 sample
d &lt;-  c(0.5,0.1)
x&lt;- dimension(103260,d)

</code></pre>

<hr>
<h2 id='form_matrix'>Forms the Projection Matrix</h2><span id='topic+form_matrix'></span>

<h3>Description</h3>

<p>The projection function is used to generate the random projection matrix. It will form either dense or spare
projection matrix. The Package supports 4 projection functions namely gaussian, probability, achlioptas and li.
The number of rows and columns of the input sample is passed with the boolean value JLT. If JLT is set to TRUE,
the dimension of the input data is reduced to the value returned by dimension() method.
For Dense Matrix -  &quot;gaussian&quot; method
For Sparse Matrix - &quot;probability, achlioptas and li&quot; method.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>form_matrix(rows, cols, JLT, eps = 0.1, projection = "gaussian")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="form_matrix_+3A_rows">rows</code></td>
<td>
<p>- number of rows</p>
</td></tr>
<tr><td><code id="form_matrix_+3A_cols">cols</code></td>
<td>
<p>- number of columns</p>
</td></tr>
<tr><td><code id="form_matrix_+3A_jlt">JLT</code></td>
<td>
<p>- Boolean to set JL transform (TRUE or FALSE)</p>
</td></tr>
<tr><td><code id="form_matrix_+3A_eps">eps</code></td>
<td>
<p>- error tolerance level with default value 0.1</p>
</td></tr>
<tr><td><code id="form_matrix_+3A_projection">projection</code></td>
<td>
<p>- projection function with default value &quot;gaussian&quot;</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The 4 projection functions are
</p>
<p>1.&quot;gaussian&quot; - The default projection function is &quot;gaussian&quot;. In probability theory, Gaussian distribution is also
called as normal distribution. It is a continuous probability distribution used to represent real-valued random
variables. The elements in the random matrix are drawn from N(0,1/k), N is a Natural number and
k value calculated based on JL - Lemma using dimension() function.
</p>
<p>2. &quot;probability&quot; - In this method, the matrix was generated using the equal probability
distribution with the elements [-1, 1].
</p>
<p>3. &quot;achlioptas&quot; - Achlioptas matrix is easy to generate and also the 2/3rd of the matrix was filled
with zero which makes it as more sparse and cut-off the 2/3rd computation.
</p>
<p>4. &quot;li&quot; - This method generalizes the achlioptas method and generate very sparse random matrix
to improve the computational speed up of random projection.
</p>
<p>When comparing to gaussian function,  the other projection functions creates sparse matrix by filling with
zero's or one's to reduce the computation even more.
</p>


<h3>Value</h3>

<p>Projection Matrix
</p>


<h3>Author(s)</h3>

<p>Aghila G
</p>
<p>Siddharth R
</p>


<h3>References</h3>

<p>[1] N.I.R. Ailon and B.Chazelle, &quot;The Fast Johnson Lindenstrauss Transform and Approximate Nearest Neighbors(2009)&quot;
</p>
<p>[2] Ping Li, Trevor J. Hastie, and Kenneth W. Church,  &quot;Very sparse random projections(2006)&quot;.
</p>
<p>[3] D. Achlioptas, &quot;Database-friendly random projections(2002)&quot;
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load Library
library(RandPro)

# Default Gaussian projection matrix without JL transform
mat &lt;- form_matrix(600,1000,FALSE)

# Default Gaussian projection matrix with JL transform of 50% Error tolerance
mat &lt;- form_matrix(300,100000,TRUE,0.5)

# Projection matrix with probability distribution of 50% Error tolerance
mat &lt;- form_matrix(250,1000000,TRUE,0.5,"probability")

# Projection matrix with li distribution of 50% Error tolerance
mat &lt;- form_matrix(250,1000000,TRUE,0.5,"li")

# Projection matrix with achlioptas distribution of 50% Error tolerance
mat &lt;- form_matrix(250,1000000,TRUE,0.5,"achlioptas")


</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
