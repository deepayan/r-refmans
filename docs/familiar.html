<!DOCTYPE html><html><head><title>Help for package familiar</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {familiar}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#.check_class_level_plausibility'><p>Internal function to test plausibility of provided class levels</p></a></li>
<li><a href='#.check_feature_availability'><p>Internal function to check whether feature columns are found in the data</p></a></li>
<li><a href='#.check_input_identifier_column'><p>Internal function for checking consistency of the identifier columns</p></a></li>
<li><a href='#.check_outcome_type_plausibility'><p>Internal function for checking if the outcome type fits well to the data</p></a></li>
<li><a href='#.check_survival_time_plausibility'><p>Internal function to test plausibility of provided survival times.</p></a></li>
<li><a href='#.finish_data_preparation'><p>Internal function for finalising generic data processing</p></a></li>
<li><a href='#.get_default_sign_size'><p>Internal function for obtaining a default signature size parameter</p></a></li>
<li><a href='#.get_iteration_data'><p>Internal function for creating or retrieving iteration data</p></a></li>
<li><a href='#.impute_outcome_type'><p>Internal imputation function for the outcome type.</p></a></li>
<li><a href='#.load_iterations'><p>Internal function for loading iteration data from the file system</p></a></li>
<li><a href='#.parse_categorical_features'><p>Internal function for setting categorical features</p></a></li>
<li><a href='#.parse_evaluation_settings'><p>Internal function for parsing settings related to model evaluation</p></a></li>
<li><a href='#.parse_experiment_settings'><p>Internal function for parsing settings related to the computational setup</p></a></li>
<li><a href='#.parse_feature_selection_settings'><p>Internal function for parsing settings related to feature selection</p></a></li>
<li><a href='#.parse_file_paths'><p>Internal function for parsing file paths</p></a></li>
<li><a href='#.parse_general_settings'><p>Internal function for parsing settings that configure various aspects of the</p>
worklow</a></li>
<li><a href='#.parse_hyperparameter_optimisation_settings'><p>Internal function for parsing settings related to model hyperparameter</p>
optimisation</a></li>
<li><a href='#.parse_initial_settings'><p>Internal function for parsing settings required to parse the input data</p>
and define the experiment</a></li>
<li><a href='#.parse_model_development_settings'><p>Internal function for parsing settings related to model development</p></a></li>
<li><a href='#.parse_preprocessing_settings'><p>Internal function for parsing settings related to preprocessing</p></a></li>
<li><a href='#.parse_setup_settings'><p>Internal function for parsing settings related to the computational setup</p></a></li>
<li><a href='#.plot_permutation_variable_importance'><p>Internal plotting function for permutation variable importance plots</p></a></li>
<li><a href='#.plot_univariate_importance'><p>Internal plotting function for univariate plots</p></a></li>
<li><a href='#.prepare_familiar_data_sets'><p>Prepare familiarData objects for evaluation at runtime.</p></a></li>
<li><a href='#.update_experimental_design_settings'><p>Internal function to check batch assignment to development and validation</p></a></li>
<li><a href='#.update_initial_settings'><p>Internal check and update of settings related to data set parsing</p></a></li>
<li><a href='#aggregate_vimp_table'><p>Aggregate variable importance from multiple variable importance</p>
objects.</a></li>
<li><a href='#as_data_object'><p>Creates a valid data object from input data.</p></a></li>
<li><a href='#as_familiar_collection'><p>Conversion to familiarCollection object.</p></a></li>
<li><a href='#as_familiar_data'><p>Conversion to familiarData object.</p></a></li>
<li><a href='#as_familiar_ensemble'><p>Conversion to familiarEnsemble object.</p></a></li>
<li><a href='#coef'><p>Extract model coefficients</p></a></li>
<li><a href='#create_randomised_groups'><p>Create randomised groups</p>
Creates randomised groups, e.g. for tests that depend on splitting (continuous) data into groups, such as the Hosmer-Lemeshow test</a></li>
<li><a href='#dataObject-class'><p>Data object</p></a></li>
<li><a href='#encapsulate_path'><p>Encapsulate path</p></a></li>
<li><a href='#experimentData-class'><p>Experiment data</p></a></li>
<li><a href='#export_all'><p>Extract and export all data.</p></a></li>
<li><a href='#export_auc_data'><p>Extract and export ROC and Precision-Recall curves.</p></a></li>
<li><a href='#export_calibration_data'><p>Extract and export calibration and goodness-of-fit tests.</p></a></li>
<li><a href='#export_calibration_info'><p>Extract and export calibration information.</p></a></li>
<li><a href='#export_confusion_matrix_data'><p>Extract and export confusion matrices.</p></a></li>
<li><a href='#export_decision_curve_analysis_data'><p>Extract and export decision curve analysis data.</p></a></li>
<li><a href='#export_feature_expressions'><p>Extract and export feature expressions.</p></a></li>
<li><a href='#export_feature_similarity'><p>Extract and export mutual correlation between features.</p></a></li>
<li><a href='#export_fs_vimp'><p>Extract and export feature selection variable importance.</p></a></li>
<li><a href='#export_hyperparameters'><p>Extract and export model hyperparameters.</p></a></li>
<li><a href='#export_ice_data'><p>Extract and export individual conditional expectation data.</p></a></li>
<li><a href='#export_model_performance'><p>Extract and export metrics for model performance.</p></a></li>
<li><a href='#export_model_vimp'><p>Extract and export model-based variable importance.</p></a></li>
<li><a href='#export_partial_dependence_data'><p>Extract and export partial dependence data.</p></a></li>
<li><a href='#export_permutation_vimp'><p>Extract and export permutation variable importance.</p></a></li>
<li><a href='#export_prediction_data'><p>Extract and export predicted values.</p></a></li>
<li><a href='#export_risk_stratification_data'><p>Extract and export sample risk group stratification and associated</p>
tests.</a></li>
<li><a href='#export_risk_stratification_info'><p>Extract and export cut-off values for risk group stratification.</p></a></li>
<li><a href='#export_sample_similarity'><p>Extract and export mutual correlation between features.</p></a></li>
<li><a href='#export_univariate_analysis_data'><p>Extract and export univariate analysis data of features.</p></a></li>
<li><a href='#extract_auc_data'><p>Internal function to extract area under the ROC curve information.</p></a></li>
<li><a href='#extract_calibration_data'><p>Internal function to extract calibration data.</p></a></li>
<li><a href='#extract_calibration_info'><p>Internal function to extract calibration info from data.</p></a></li>
<li><a href='#extract_confusion_matrix'><p>Internal function to extract the confusion matrix.</p></a></li>
<li><a href='#extract_data'><p>Internal function to create a familiarData object.</p></a></li>
<li><a href='#extract_decision_curve_data'><p>Internal function to extract decision curve analysis data.</p></a></li>
<li><a href='#extract_dispatcher,familiarEnsemble,familiarDataElement-method'><p>Internal function to dispatch extraction functions.</p></a></li>
<li><a href='#extract_experimental_setup'><p>Parse experimental design</p></a></li>
<li><a href='#extract_feature_expression'><p>Internal function to extract feature expressions.</p></a></li>
<li><a href='#extract_feature_similarity'><p>Internal function to extract the feature distance table.</p></a></li>
<li><a href='#extract_fs_vimp'><p>Internal function to extract feature selection variable importance.</p></a></li>
<li><a href='#extract_hyperparameters'><p>Internal function to extract hyperparameters from models.</p></a></li>
<li><a href='#extract_ice'><p>Internal function to extract data for individual conditional</p>
expectation plots.</a></li>
<li><a href='#extract_model_vimp'><p>Internal function to extract variable importance from models.</p></a></li>
<li><a href='#extract_performance'><p>Internal function to extract performance metrics.</p></a></li>
<li><a href='#extract_permutation_vimp'><p>Internal function to extract permutation variable importance.</p></a></li>
<li><a href='#extract_predictions'><p>Internal function to extract predicted values from models.</p></a></li>
<li><a href='#extract_risk_stratification_data'><p>Internal function to extract stratification data.</p></a></li>
<li><a href='#extract_risk_stratification_info'><p>Internal function to extract risk stratification info from data.</p></a></li>
<li><a href='#extract_sample_similarity'><p>Internal function to extract the sample distance table.</p></a></li>
<li><a href='#extract_univariate_analysis'><p>Internal function to extract data from a univariate analysis.</p></a></li>
<li><a href='#familiar'><p>familiar: Fully Automated Machine Learning with Interpretable Analysis of Results</p></a></li>
<li><a href='#familiarCollection-class'><p>Collection of familiar data.</p></a></li>
<li><a href='#familiarData-class'><p>Dataset obtained after evaluating models on a dataset.</p></a></li>
<li><a href='#familiarDataElement-class'><p>Data container for evaluation data.</p></a></li>
<li><a href='#familiarEnsemble-class'><p>Ensemble of familiar models.</p></a></li>
<li><a href='#familiarHyperparameterLearner-class'><p>Hyperparameter learner.</p></a></li>
<li><a href='#familiarMetric-class'><p>Model performance metric.</p></a></li>
<li><a href='#familiarModel-class'><p>Familiar model.</p></a></li>
<li><a href='#familiarNoveltyDetector-class'><p>Novelty detector.</p></a></li>
<li><a href='#familiarVimpMethod-class'><p>Variable importance method object.</p></a></li>
<li><a href='#featureInfo-class'><p>Feature information object.</p></a></li>
<li><a href='#featureInfoParameters-class'><p>Feature information parameters object.</p></a></li>
<li><a href='#get_class_names,familiarCollection-method'><p>Get outcome class labels</p></a></li>
<li><a href='#get_data_set_names,familiarCollection-method'><p>Get current name of datasets</p></a></li>
<li><a href='#get_feature_names,familiarCollection-method'><p>Get current feature labels</p></a></li>
<li><a href='#get_fs_method_names,familiarCollection-method'><p>Get current feature selection method name labels</p></a></li>
<li><a href='#get_learner_names,familiarCollection-method'><p>Get current learner name labels</p></a></li>
<li><a href='#get_risk_group_names,familiarCollection-method'><p>Get current risk group labels</p></a></li>
<li><a href='#get_vimp_table'><p>Extract variable importance table.</p></a></li>
<li><a href='#get_xml_config'><p>Create an empty xml configuration file</p></a></li>
<li><a href='#is.encapsulated_path'><p>Internal test for encapsulated_path</p></a></li>
<li><a href='#is.waive'><p>Internal test to see if an object is a waiver</p></a></li>
<li><a href='#outcomeInfo-class'><p>Outcome information object.</p></a></li>
<li><a href='#plot_auc_precision_recall_curve'><p>Plot the precision-recall curve.</p></a></li>
<li><a href='#plot_auc_roc_curve'><p>Plot the receiver operating characteristic curve.</p></a></li>
<li><a href='#plot_calibration_data'><p>Plot calibration figures.</p></a></li>
<li><a href='#plot_confusion_matrix'><p>Plot confusion matrix.</p></a></li>
<li><a href='#plot_decision_curve'><p>Plot decision curves.</p></a></li>
<li><a href='#plot_feature_similarity'><p>Plot heatmaps for pairwise similarity between features.</p></a></li>
<li><a href='#plot_ice'><p>Plot individual conditional expectation plots.</p></a></li>
<li><a href='#plot_kaplan_meier'><p>Plot Kaplan-Meier survival curves.</p></a></li>
<li><a href='#plot_model_performance'><p>Plot model performance.</p></a></li>
<li><a href='#plot_pd'><p>Plot partial dependence.</p></a></li>
<li><a href='#plot_permutation_variable_importance'><p>Plot permutation variable importance.</p></a></li>
<li><a href='#plot_sample_clustering'><p>Plot heatmaps for pairwise similarity between features.</p></a></li>
<li><a href='#plot_univariate_importance'><p>Plot univariate importance.</p></a></li>
<li><a href='#plot_variable_importance'><p>Plot variable importance scores of features during feature selection or</p>
after training a model.</a></li>
<li><a href='#plotting.check_data_handling'><p>Checks and sanitizes splitting variables for plotting.</p></a></li>
<li><a href='#plotting.check_input_args'><p>Internal checks on common plot input arguments</p></a></li>
<li><a href='#precompute_data_assignment'><p>Pre-compute data assignment</p></a></li>
<li><a href='#precompute_feature_info'><p>Pre-compute feature information</p></a></li>
<li><a href='#precompute_vimp'><p>Pre-compute variable importance</p></a></li>
<li><a href='#predict'><p>Model predictions for familiar models and model ensembles</p></a></li>
<li><a href='#set_class_names,familiarCollection-method'><p>Rename outcome classes for plotting and export</p></a></li>
<li><a href='#set_data_set_names,familiarCollection-method'><p>Name datasets for plotting and export</p></a></li>
<li><a href='#set_feature_names,familiarCollection-method'><p>Rename features for plotting and export</p></a></li>
<li><a href='#set_fs_method_names,familiarCollection-method'><p>Rename feature selection methods for plotting and export</p></a></li>
<li><a href='#set_learner_names,familiarCollection-method'><p>Rename learners for plotting and export</p></a></li>
<li><a href='#set_object_name,familiarData-method'><p>Set the name of a <code>familiarData</code> object.</p></a></li>
<li><a href='#set_object_name,familiarEnsemble-method'><p>Set the name of a <code>familiarEnsemble</code> object.</p></a></li>
<li><a href='#set_object_name,familiarModel-method'><p>Set the name of a <code>familiarModel</code> object.</p></a></li>
<li><a href='#set_risk_group_names,familiarCollection-method'><p>Rename risk groups for plotting and export</p></a></li>
<li><a href='#summary'><p>Model summaries</p></a></li>
<li><a href='#summon_familiar'><p>Perform end-to-end machine learning and data analysis</p></a></li>
<li><a href='#theme_familiar'><p>Familiar ggplot2 theme</p></a></li>
<li><a href='#train_familiar'><p>Create models using end-to-end machine learning</p></a></li>
<li><a href='#update_model_dir_path'><p>Updates model directory path for ensemble objects.</p></a></li>
<li><a href='#update_object'><p>Update familiar S4 objects to the most recent version.</p></a></li>
<li><a href='#vcov'><p>Calculate variance-covariance matrix for a model</p></a></li>
<li><a href='#vimpTable-class'><p>Variable importance table</p></a></li>
<li><a href='#waiver'><p>Create a waiver object</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>End-to-End Automated Machine Learning and Model Evaluation</td>
</tr>
<tr>
<td>Version:</td>
<td>1.4.6</td>
</tr>
<tr>
<td>Description:</td>
<td>Single unified interface for end-to-end modelling of regression, 
    categorical and time-to-event (survival) outcomes. Models created using
    familiar are self-containing, and their use does not require additional
    information such as baseline survival, feature clustering, or feature
    transformation and normalisation parameters. Model performance,
    calibration, risk group stratification, (permutation) variable importance,
    individual conditional expectation, partial dependence, and more, are
    assessed automatically as part of the evaluation process and exported in
    tabular format and plotted, and may also be computed manually using export
    and plot functions. Where possible, metrics and values obtained during the
    evaluation process come with confidence intervals.</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/alexzwanenburg/familiar">https://github.com/alexzwanenburg/familiar</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/alexzwanenburg/familiar/issues">https://github.com/alexzwanenburg/familiar/issues</a></td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.0.0)</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://joinup.ec.europa.eu/software/page/eupl">EUPL</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Imports:</td>
<td>data.table, methods, rlang (&ge; 0.3.4), rstream, survival</td>
</tr>
<tr>
<td>Suggests:</td>
<td>BART, callr (&ge; 3.4.3), cluster, CORElearn, coro,
dynamicTreeCut, e1071 (&ge; 1.7.5), Ecdat, fastcluster, fastglm,
ggplot2 (&ge; 3.0.0), glmnet, gtable, harmonicmeanp, isotree (&ge;
0.3.0), knitr, labeling, laGP, MASS, maxstat, mboost (&ge;
2.9.0), microbenchmark, nnet, partykit, praznik, proxy, qvalue,
randomForestSRC, ranger, rmarkdown, scales, testthat (&ge;
3.0.0), xml2, VGAM, xgboost</td>
</tr>
<tr>
<td>Collate:</td>
<td>'FamiliarS4Classes.R' 'FamiliarS4Generics.R'
'BatchNormalisation.R' 'BootstrapConfidenceInterval.R'
'CheckArguments.R' 'CheckHyperparameters.R' 'CheckPackages.R'
'ClassBalance.R' 'ClusteringMethod.R' 'Clustering.R'
'ClusterRepresentation.R' 'Normalisation.R'
'CombatNormalisation.R' 'LearnerS4Naive.R' 'DataObject.R'
'DataParameterChecks.R' 'DataPreProcessing.R'
'DataProcessing.R' 'DataServerBackend.R' 'ErrorMessages.R'
'Evaluation.R' 'ExperimentData.R' 'ExperimentSetup.R'
'Familiar.R' 'FamiliarCollection.R'
'FamiliarCollectionExport.R' 'FamiliarData.R'
'FamiliarDataComputation.R'
'FamiliarDataComputationAUCCurves.R'
'FamiliarDataComputationCalibrationData.R'
'FamiliarDataComputationCalibrationInfo.R'
'FamiliarDataComputationConfusionMatrix.R'
'FamiliarDataComputationDecisionCurveAnalysis.R'
'FamiliarDataComputationFeatureExpression.R'
'FamiliarDataComputationFeatureSimilarity.R'
'FamiliarDataComputationHyperparameters.R'
'FamiliarDataComputationICE.R'
'FamiliarDataComputationModelPerformance.R'
'FamiliarDataComputationPermutationVimp.R'
'FamiliarDataComputationPredictionData.R'
'FamiliarDataComputationRiskStratificationData.R'
'FamiliarDataComputationRiskStratificationInfo.R'
'FamiliarDataComputationSampleSimilarity.R'
'FamiliarDataComputationUnivariateAnalysis.R'
'FamiliarDataComputationVimp.R' 'FamiliarDataElement.R'
'FamiliarEnsemble.R' 'HyperparameterS4Ranger.R'
'HyperparameterS4RandomSearch.R'
'HyperparameterS4GaussianProcess.R'
'HyperparameterS4BayesianAdditiveRegressionTrees.R'
'FamiliarHyperparameterLearner.R' 'FamiliarModel.R'
'FamiliarNoveltyDetector.R' 'FamiliarObjectConversion.R'
'FamiliarObjectUpdate.R' 'FamiliarSharedS4Methods.R'
'FamiliarVimpMethod.R' 'FeatureInfo.R'
'FeatureInfoParameters.R' 'FeatureSelection.R'
'FunctionWrapperUtilities.R' 'HyperparameterOptimisation.R'
'HyperparameterOptimisationMetaLearners.R'
'HyperparameterOptimisationUtilities.R' 'Imputation.R'
'Iterations.R' 'LearnerS4XGBoost.R'
'LearnerS4SurvivalRegression.R' 'LearnerS4SVM.R'
'LearnerS4RFSRC.R' 'LearnerS4Ranger.R' 'LearnerS4MBoost.R'
'LearnerS4NaiveBayes.R' 'LearnerS4KNN.R' 'LearnerS4GLMnet.R'
'LearnerS4GLM.R' 'LearnerS4Cox.R' 'LearnerMain.R'
'LearnerRecalibration.R' 'LearnerSurvivalGrouping.R'
'LearnerSurvivalProbability.R' 'Logger.R'
'MetricS4Regression.R' 'MetricS4ConfusionMatrixMetrics.R'
'MetricS4Brier.R' 'MetricS4AUC.R' 'MetricS4.R'
'MetricS4ConcordanceIndex.R' 'ModelBuilding.R'
'NoveltyDetectorS4IsolationTree.R' 'NoveltyDetectorMain.R'
'NoveltyDetectorS4NoneNoveltyDetector.R' 'OutcomeInfo.R'
'PairwiseSimilarity.R' 'ParallelFunctions.R' 'ParseData.R'
'ParseSettings.R' 'PlotAUCcurves.R' 'PlotAll.R'
'PlotCalibration.R' 'PlotColours.R' 'PlotConfusionMatrix.R'
'PlotDecisionCurves.R' 'PlotFeatureRanking.R'
'PlotFeatureSimilarity.R' 'PlotGTable.R' 'PlotICE.R'
'PlotInputArguments.R' 'PlotKaplanMeier.R'
'PlotModelPerformance.R' 'PlotPermutationVariableImportance.R'
'PlotSampleClustering.R' 'PlotUnivariateImportance.R'
'PlotUtilities.R' 'PredictS4Methods.R' 'ProcessTimeUtilites.R'
'Random.R' 'RandomGrouping.R' 'RankBordaAggregation.R'
'RankMain.R' 'RankSimpleAggregation.R'
'RankStabilityAggregation.R' 'SocketServer.R'
'StringUtilities.R' 'TestDataCreators.R' 'TestFunctions.R'
'TrainS4Methods.R' 'Transformation.R' 'TrimUtilities.R'
'Utilities.R' 'UtilitiesS4.R' 'VimpS4Regression.R'
'VimpS4OtherMethods.R' 'VimpS4MutualInformation.R'
'VimpS4Correlation.R' 'VimpS4Concordance.R' 'VimpMain.R'
'VimpS4CoreLearn.R' 'VimpTable.R' 'aaa.R'</td>
</tr>
<tr>
<td>Config/testthat/parallel:</td>
<td>true</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-01-24 12:38:17 UTC; alexz</td>
</tr>
<tr>
<td>Author:</td>
<td>Alex Zwanenburg <a href="https://orcid.org/0000-0002-0342-9545"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre],
  Steffen Löck [aut],
  Stefan Leger [ctb],
  Iram Shahzadi [ctb],
  Asier Rabasco Meneghetti [ctb],
  Sebastian Starke [ctb],
  Technische Universität Dresden [cph],
  German Cancer Research Center (DKFZ) [cph]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Alex Zwanenburg &lt;alexander.zwanenburg@nct-dresden.de&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-01-24 13:40:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='.check_class_level_plausibility'>Internal function to test plausibility of provided class levels</h2><span id='topic+.check_class_level_plausibility'></span>

<h3>Description</h3>

<p>This function checks whether categorical levels are present in the data that
are not found in the user-provided class levels.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.check_class_level_plausibility(
  data,
  outcome_type,
  outcome_column,
  class_levels,
  check_stringency = "strict"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".check_class_level_plausibility_+3A_data">data</code></td>
<td>
<p>Data set as loaded using the <code>.load_data</code> function.</p>
</td></tr>
<tr><td><code id=".check_class_level_plausibility_+3A_outcome_type">outcome_type</code></td>
<td>
<p>(<strong>recommended</strong>) Type of outcome found in the outcome
column. The outcome type determines many aspects of the overall process,
e.g. the available feature selection methods and learners, but also the
type of assessments that can be conducted to evaluate the resulting models.
Implemented outcome types are:
</p>

<ul>
<li> <p><code>binomial</code>: categorical outcome with 2 levels.
</p>
</li>
<li> <p><code>multinomial</code>: categorical outcome with 2 or more levels.
</p>
</li>
<li> <p><code>count</code>: Poisson-distributed numeric outcomes.
</p>
</li>
<li> <p><code>continuous</code>: general continuous numeric outcomes.
</p>
</li>
<li> <p><code>survival</code>: survival outcome for time-to-event data.
</p>
</li></ul>

<p>If not provided, the algorithm will attempt to obtain outcome_type from
contents of the outcome column. This may lead to unexpected results, and we
therefore advise to provide this information manually.
</p>
<p>Note that <code>competing_risk</code> survival analysis are not fully supported, and
is currently not a valid choice for <code>outcome_type</code>.</p>
</td></tr>
<tr><td><code id=".check_class_level_plausibility_+3A_outcome_column">outcome_column</code></td>
<td>
<p>(<strong>recommended</strong>) Name of the column containing the
outcome of interest. May be identified from a formula, if a formula is
provided as an argument. Otherwise an error is raised. Note that <code>survival</code>
and <code>competing_risk</code> outcome type outcomes require two columns that
indicate the time-to-event or the time of last follow-up and the event
status.</p>
</td></tr>
<tr><td><code id=".check_class_level_plausibility_+3A_class_levels">class_levels</code></td>
<td>
<p>(<em>optional</em>) Class levels for <code>binomial</code> or <code>multinomial</code>
outcomes. This argument can be used to specify the ordering of levels for
categorical outcomes. These class levels must exactly match the levels
present in the outcome column.</p>
</td></tr>
<tr><td><code id=".check_class_level_plausibility_+3A_check_stringency">check_stringency</code></td>
<td>
<p>Specifies stringency of various checks. This is mostly:
</p>

<ul>
<li> <p><code>strict</code>: default value used for <code>summon_familiar</code>. Thoroughly checks
input data. Used internally for checking development data.
</p>
</li>
<li> <p><code>external_warn</code>: value used for <code>extract_data</code> and related methods. Less
stringent checks, but will warn for possible issues. Used internally for
checking data for evaluation and explanation.
</p>
</li>
<li> <p><code>external</code>: value used for external methods such as <code>predict</code>. Less
stringent checks, particularly for identifier and outcome columns, which may
be completely absent. Used internally for <code>predict</code>.
</p>
</li></ul>
</td></tr>
</table>

<hr>
<h2 id='.check_feature_availability'>Internal function to check whether feature columns are found in the data</h2><span id='topic+.check_feature_availability'></span>

<h3>Description</h3>

<p>This function checks whether feature columns can be found in the data set.
It will raise an error if any feature columns are missing from the data set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.check_feature_availability(data, feature)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".check_feature_availability_+3A_data">data</code></td>
<td>
<p>Data set as loaded using the <code>.load_data</code> function.</p>
</td></tr>
<tr><td><code id=".check_feature_availability_+3A_feature">feature</code></td>
<td>
<p>Character string(s) indicating one or more features.</p>
</td></tr>
</table>

<hr>
<h2 id='.check_input_identifier_column'>Internal function for checking consistency of the identifier columns</h2><span id='topic+.check_input_identifier_column'></span>

<h3>Description</h3>

<p>This function checks whether an identifier column is consistent, i.e. appears
it exists, there is only one, and there is no overlap with any user-provided
feature columns, identifiers, or
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.check_input_identifier_column(
  id_column,
  data,
  signature = NULL,
  exclude_features = NULL,
  include_features = NULL,
  other_id_column = NULL,
  outcome_column = NULL,
  col_type,
  check_stringency = "strict"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".check_input_identifier_column_+3A_id_column">id_column</code></td>
<td>
<p>Character string indicating the currently inspected
identifier column.</p>
</td></tr>
<tr><td><code id=".check_input_identifier_column_+3A_data">data</code></td>
<td>
<p>Data set as loaded using the <code>.load_data</code> function.</p>
</td></tr>
<tr><td><code id=".check_input_identifier_column_+3A_signature">signature</code></td>
<td>
<p>(<em>optional</em>) One or more names of feature columns that are
considered part of a specific signature. Features specified here will
always be used for modelling. Ranking from feature selection has no effect
for these features.</p>
</td></tr>
<tr><td><code id=".check_input_identifier_column_+3A_exclude_features">exclude_features</code></td>
<td>
<p>(<em>optional</em>) Feature columns that will be removed
from the data set. Cannot overlap with features in <code>signature</code>,
<code>novelty_features</code> or <code>include_features</code>.</p>
</td></tr>
<tr><td><code id=".check_input_identifier_column_+3A_include_features">include_features</code></td>
<td>
<p>(<em>optional</em>) Feature columns that are specifically
included in the data set. By default all features are included. Cannot
overlap with <code>exclude_features</code>, but may overlap <code>signature</code>. Features in
<code>signature</code> and <code>novelty_features</code> are always included. If both
<code>exclude_features</code> and <code>include_features</code> are provided, <code>include_features</code>
takes precedence, provided that there is no overlap between the two.</p>
</td></tr>
<tr><td><code id=".check_input_identifier_column_+3A_other_id_column">other_id_column</code></td>
<td>
<p>Character string indicating another identifier column.</p>
</td></tr>
<tr><td><code id=".check_input_identifier_column_+3A_outcome_column">outcome_column</code></td>
<td>
<p>Character string indicating the outcome column(s).</p>
</td></tr>
<tr><td><code id=".check_input_identifier_column_+3A_col_type">col_type</code></td>
<td>
<p>Character string indicating the type of column, i.e. <code>sample</code>
or <code>batch</code>.</p>
</td></tr>
<tr><td><code id=".check_input_identifier_column_+3A_check_stringency">check_stringency</code></td>
<td>
<p>Specifies stringency of various checks. This is mostly:
</p>

<ul>
<li> <p><code>strict</code>: default value used for <code>summon_familiar</code>. Thoroughly checks
input data. Used internally for checking development data.
</p>
</li>
<li> <p><code>external_warn</code>: value used for <code>extract_data</code> and related methods. Less
stringent checks, but will warn for possible issues. Used internally for
checking data for evaluation and explanation.
</p>
</li>
<li> <p><code>external</code>: value used for external methods such as <code>predict</code>. Less
stringent checks, particularly for identifier and outcome columns, which may
be completely absent. Used internally for <code>predict</code>.
</p>
</li></ul>
</td></tr>
</table>

<hr>
<h2 id='.check_outcome_type_plausibility'>Internal function for checking if the outcome type fits well to the data</h2><span id='topic+.check_outcome_type_plausibility'></span>

<h3>Description</h3>

<p>This function may help identify if the outcome type is plausible
given the outcome data. In practice it also tests whether the outcome column
is actually correct given the outcome type.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.check_outcome_type_plausibility(
  data,
  outcome_type,
  outcome_column,
  censoring_indicator,
  event_indicator,
  competing_risk_indicator,
  check_stringency = "strict"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".check_outcome_type_plausibility_+3A_data">data</code></td>
<td>
<p>Data set as loaded using the <code>.load_data</code> function.</p>
</td></tr>
<tr><td><code id=".check_outcome_type_plausibility_+3A_outcome_type">outcome_type</code></td>
<td>
<p>Character string indicating the type of outcome being
assessed.</p>
</td></tr>
<tr><td><code id=".check_outcome_type_plausibility_+3A_outcome_column">outcome_column</code></td>
<td>
<p>Name of the outcome column in the data set.</p>
</td></tr>
<tr><td><code id=".check_outcome_type_plausibility_+3A_censoring_indicator">censoring_indicator</code></td>
<td>
<p>Name of censoring indicator.</p>
</td></tr>
<tr><td><code id=".check_outcome_type_plausibility_+3A_event_indicator">event_indicator</code></td>
<td>
<p>Name of event indicator.</p>
</td></tr>
<tr><td><code id=".check_outcome_type_plausibility_+3A_competing_risk_indicator">competing_risk_indicator</code></td>
<td>
<p>Name of competing risk indicator.</p>
</td></tr>
<tr><td><code id=".check_outcome_type_plausibility_+3A_check_stringency">check_stringency</code></td>
<td>
<p>Specifies stringency of various checks. This is mostly:
</p>

<ul>
<li> <p><code>strict</code>: default value used for <code>summon_familiar</code>. Thoroughly checks
input data. Used internally for checking development data.
</p>
</li>
<li> <p><code>external_warn</code>: value used for <code>extract_data</code> and related methods. Less
stringent checks, but will warn for possible issues. Used internally for
checking data for evaluation and explanation.
</p>
</li>
<li> <p><code>external</code>: value used for external methods such as <code>predict</code>. Less
stringent checks, particularly for identifier and outcome columns, which may
be completely absent. Used internally for <code>predict</code>.
</p>
</li></ul>
</td></tr>
</table>

<hr>
<h2 id='.check_survival_time_plausibility'>Internal function to test plausibility of provided survival times.</h2><span id='topic+.check_survival_time_plausibility'></span>

<h3>Description</h3>

<p>This function checks whether non-positive outcome time is present in the
data. This may produce unexpected results for some packages. For example,
glmnet will not train if an instance has a survival time of 0 or lower.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.check_survival_time_plausibility(
  data,
  outcome_type,
  outcome_column,
  check_stringency = "strict"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".check_survival_time_plausibility_+3A_data">data</code></td>
<td>
<p>Data set as loaded using the <code>.load_data</code> function.</p>
</td></tr>
<tr><td><code id=".check_survival_time_plausibility_+3A_outcome_type">outcome_type</code></td>
<td>
<p>(<strong>recommended</strong>) Type of outcome found in the outcome
column. The outcome type determines many aspects of the overall process,
e.g. the available feature selection methods and learners, but also the
type of assessments that can be conducted to evaluate the resulting models.
Implemented outcome types are:
</p>

<ul>
<li> <p><code>binomial</code>: categorical outcome with 2 levels.
</p>
</li>
<li> <p><code>multinomial</code>: categorical outcome with 2 or more levels.
</p>
</li>
<li> <p><code>count</code>: Poisson-distributed numeric outcomes.
</p>
</li>
<li> <p><code>continuous</code>: general continuous numeric outcomes.
</p>
</li>
<li> <p><code>survival</code>: survival outcome for time-to-event data.
</p>
</li></ul>

<p>If not provided, the algorithm will attempt to obtain outcome_type from
contents of the outcome column. This may lead to unexpected results, and we
therefore advise to provide this information manually.
</p>
<p>Note that <code>competing_risk</code> survival analysis are not fully supported, and
is currently not a valid choice for <code>outcome_type</code>.</p>
</td></tr>
<tr><td><code id=".check_survival_time_plausibility_+3A_outcome_column">outcome_column</code></td>
<td>
<p>(<strong>recommended</strong>) Name of the column containing the
outcome of interest. May be identified from a formula, if a formula is
provided as an argument. Otherwise an error is raised. Note that <code>survival</code>
and <code>competing_risk</code> outcome type outcomes require two columns that
indicate the time-to-event or the time of last follow-up and the event
status.</p>
</td></tr>
</table>

<hr>
<h2 id='.finish_data_preparation'>Internal function for finalising generic data processing</h2><span id='topic+.finish_data_preparation'></span>

<h3>Description</h3>

<p>Internal function for finalising generic data processing
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.finish_data_preparation(
  data,
  sample_id_column,
  batch_id_column,
  series_id_column,
  outcome_column,
  outcome_type,
  include_features,
  class_levels,
  censoring_indicator,
  event_indicator,
  competing_risk_indicator,
  check_stringency = "strict",
  reference_method = "auto"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".finish_data_preparation_+3A_data">data</code></td>
<td>
<p>data.table with feature data</p>
</td></tr>
<tr><td><code id=".finish_data_preparation_+3A_sample_id_column">sample_id_column</code></td>
<td>
<p>(<strong>recommended</strong>) Name of the column containing
sample or subject identifiers. See <code>batch_id_column</code> above for more
details.
</p>
<p>If unset, every row will be identified as a single sample.</p>
</td></tr>
<tr><td><code id=".finish_data_preparation_+3A_batch_id_column">batch_id_column</code></td>
<td>
<p>(<strong>recommended</strong>) Name of the column containing batch
or cohort identifiers. This parameter is required if more than one dataset
is provided, or if external validation is performed.
</p>
<p>In familiar any row of data is organised by four identifiers:
</p>

<ul>
<li><p> The batch identifier <code>batch_id_column</code>: This denotes the group to which a
set of samples belongs, e.g. patients from a single study, samples measured
in a batch, etc. The batch identifier is used for batch normalisation, as
well as selection of development and validation datasets.
</p>
</li>
<li><p> The sample identifier <code>sample_id_column</code>: This denotes the sample level,
e.g. data from a single individual. Subsets of data, e.g. bootstraps or
cross-validation folds, are created at this level.
</p>
</li>
<li><p> The series identifier <code>series_id_column</code>: Indicates measurements on a
single sample that may not share the same outcome value, e.g. a time
series, or the number of cells in a view.
</p>
</li>
<li><p> The repetition identifier: Indicates repeated measurements in a single
series where any feature values may differ, but the outcome does not.
Repetition identifiers are always implicitly set when multiple entries for
the same series of the same sample in the same batch that share the same
outcome are encountered.
</p>
</li></ul>
</td></tr>
<tr><td><code id=".finish_data_preparation_+3A_series_id_column">series_id_column</code></td>
<td>
<p>(<strong>optional</strong>) Name of the column containing series
identifiers, which distinguish between measurements that are part of a
series for a single sample. See <code>batch_id_column</code> above for more details.
</p>
<p>If unset, rows which share the same batch and sample identifiers but have a
different outcome are assigned unique series identifiers.</p>
</td></tr>
<tr><td><code id=".finish_data_preparation_+3A_outcome_column">outcome_column</code></td>
<td>
<p>(<strong>recommended</strong>) Name of the column containing the
outcome of interest. May be identified from a formula, if a formula is
provided as an argument. Otherwise an error is raised. Note that <code>survival</code>
and <code>competing_risk</code> outcome type outcomes require two columns that
indicate the time-to-event or the time of last follow-up and the event
status.</p>
</td></tr>
<tr><td><code id=".finish_data_preparation_+3A_outcome_type">outcome_type</code></td>
<td>
<p>(<strong>recommended</strong>) Type of outcome found in the outcome
column. The outcome type determines many aspects of the overall process,
e.g. the available feature selection methods and learners, but also the
type of assessments that can be conducted to evaluate the resulting models.
Implemented outcome types are:
</p>

<ul>
<li> <p><code>binomial</code>: categorical outcome with 2 levels.
</p>
</li>
<li> <p><code>multinomial</code>: categorical outcome with 2 or more levels.
</p>
</li>
<li> <p><code>count</code>: Poisson-distributed numeric outcomes.
</p>
</li>
<li> <p><code>continuous</code>: general continuous numeric outcomes.
</p>
</li>
<li> <p><code>survival</code>: survival outcome for time-to-event data.
</p>
</li></ul>

<p>If not provided, the algorithm will attempt to obtain outcome_type from
contents of the outcome column. This may lead to unexpected results, and we
therefore advise to provide this information manually.
</p>
<p>Note that <code>competing_risk</code> survival analysis are not fully supported, and
is currently not a valid choice for <code>outcome_type</code>.</p>
</td></tr>
<tr><td><code id=".finish_data_preparation_+3A_include_features">include_features</code></td>
<td>
<p>(<em>optional</em>) Feature columns that are specifically
included in the data set. By default all features are included. Cannot
overlap with <code>exclude_features</code>, but may overlap <code>signature</code>. Features in
<code>signature</code> and <code>novelty_features</code> are always included. If both
<code>exclude_features</code> and <code>include_features</code> are provided, <code>include_features</code>
takes precedence, provided that there is no overlap between the two.</p>
</td></tr>
<tr><td><code id=".finish_data_preparation_+3A_class_levels">class_levels</code></td>
<td>
<p>(<em>optional</em>) Class levels for <code>binomial</code> or <code>multinomial</code>
outcomes. This argument can be used to specify the ordering of levels for
categorical outcomes. These class levels must exactly match the levels
present in the outcome column.</p>
</td></tr>
<tr><td><code id=".finish_data_preparation_+3A_censoring_indicator">censoring_indicator</code></td>
<td>
<p>(<strong>recommended</strong>) Indicator for right-censoring in
<code>survival</code> and <code>competing_risk</code> analyses. <code>familiar</code> will automatically
recognise <code>0</code>, <code>false</code>, <code>f</code>, <code>n</code>, <code>no</code> as censoring indicators, including
different capitalisations. If this parameter is set, it replaces the
default values.</p>
</td></tr>
<tr><td><code id=".finish_data_preparation_+3A_event_indicator">event_indicator</code></td>
<td>
<p>(<strong>recommended</strong>) Indicator for events in <code>survival</code>
and <code>competing_risk</code> analyses. <code>familiar</code> will automatically recognise <code>1</code>,
<code>true</code>, <code>t</code>, <code>y</code> and <code>yes</code> as event indicators, including different
capitalisations. If this parameter is set, it replaces the default values.</p>
</td></tr>
<tr><td><code id=".finish_data_preparation_+3A_competing_risk_indicator">competing_risk_indicator</code></td>
<td>
<p>(<strong>recommended</strong>) Indicator for competing
risks in <code>competing_risk</code> analyses. There are no default values, and if
unset, all values other than those specified by the <code>event_indicator</code> and
<code>censoring_indicator</code> parameters are considered to indicate competing
risks.</p>
</td></tr>
<tr><td><code id=".finish_data_preparation_+3A_check_stringency">check_stringency</code></td>
<td>
<p>Specifies stringency of various checks. This is mostly:
</p>

<ul>
<li> <p><code>strict</code>: default value used for <code>summon_familiar</code>. Thoroughly checks
input data. Used internally for checking development data.
</p>
</li>
<li> <p><code>external_warn</code>: value used for <code>extract_data</code> and related methods. Less
stringent checks, but will warn for possible issues. Used internally for
checking data for evaluation and explanation.
</p>
</li>
<li> <p><code>external</code>: value used for external methods such as <code>predict</code>. Less
stringent checks, particularly for identifier and outcome columns, which may
be completely absent. Used internally for <code>predict</code>.
</p>
</li></ul>
</td></tr>
<tr><td><code id=".finish_data_preparation_+3A_reference_method">reference_method</code></td>
<td>
<p>(<em>optional</em>) Method used to set reference levels for
categorical features. There are several options:
</p>

<ul>
<li> <p><code>auto</code> (default): Categorical features that are not explicitly set by the
user, i.e. columns containing boolean values or characters, use the most
frequent level as reference. Categorical features that are explicitly set,
i.e. as factors, are used as is.
</p>
</li>
<li> <p><code>always</code>: Both automatically detected and user-specified categorical
features have the reference level set to the most frequent level. Ordinal
features are not altered, but are used as is.
</p>
</li>
<li> <p><code>never</code>: User-specified categorical features are used as is.
Automatically detected categorical features are simply sorted, and the
first level is then used as the reference level. This was the behaviour
prior to familiar version 1.3.0.
</p>
</li></ul>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is used to update data.table provided by loading the
data. When part of the main familiar workflow, this function is used after
.parse_initial_settings &ndash;&gt; .load_data &ndash;&gt; .update_initial_settings.
</p>
<p>When used to parse external data (e.g. in conjunction with familiarModel)
it follows after .load_data. Hence the function contains several checks
which are otherwise part of .update_initial_settings.
</p>


<h3>Value</h3>

<p>data.table with expected column names.
</p>

<hr>
<h2 id='.get_default_sign_size'>Internal function for obtaining a default signature size parameter</h2><span id='topic+.get_default_sign_size'></span>

<h3>Description</h3>

<p>Internal function for obtaining a default signature size parameter
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.get_default_sign_size(data_obj, restrict_samples = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".get_default_sign_size_+3A_data_obj">data_obj</code></td>
<td>
<p>dataObject class object which contains the data on which the
preset parameters are determined.</p>
</td></tr>
<tr><td><code id=".get_default_sign_size_+3A_restrict_samples">restrict_samples</code></td>
<td>
<p>Logical indicating whether the signature size should
be limited by the number of samples in addition to the number of available
features. This may help convergence of OLS-based methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List containing the preset values for the signature size parameter.
</p>

<hr>
<h2 id='.get_iteration_data'>Internal function for creating or retrieving iteration data</h2><span id='topic+.get_iteration_data'></span>

<h3>Description</h3>

<p>Internal function for creating or retrieving iteration data
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.get_iteration_data(
  file_paths,
  data,
  experiment_setup,
  settings,
  message_indent = 0L,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".get_iteration_data_+3A_file_paths">file_paths</code></td>
<td>
<p>Set of paths to relevant files and directories.</p>
</td></tr>
<tr><td><code id=".get_iteration_data_+3A_data">data</code></td>
<td>
<p>Data set as loaded using the <code>.load_data</code> function.</p>
</td></tr>
<tr><td><code id=".get_iteration_data_+3A_experiment_setup">experiment_setup</code></td>
<td>
<p>data.table with subsampler information at different
levels of the experimental design.</p>
</td></tr>
<tr><td><code id=".get_iteration_data_+3A_settings">settings</code></td>
<td>
<p>List of parameter settings. Some of these parameters are
relevant to creating iterations.</p>
</td></tr>
<tr><td><code id=".get_iteration_data_+3A_message_indent">message_indent</code></td>
<td>
<p>Indenting of messages.</p>
</td></tr>
<tr><td><code id=".get_iteration_data_+3A_verbose">verbose</code></td>
<td>
<p>Sets verbosity.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with the following elements:
</p>

<ul>
<li> <p><code>iter_list</code>: A list containing iteration data at the different levels of
the experiment.
</p>
</li>
<li> <p><code>project_id</code>: The unique project identifier.
</p>
</li>
<li> <p><code>experiment_setup</code>: data.table with subsampler information at different
levels of the experimental design.
</p>
</li></ul>


<hr>
<h2 id='.impute_outcome_type'>Internal imputation function for the outcome type.</h2><span id='topic+.impute_outcome_type'></span>

<h3>Description</h3>

<p>This function allows for imputation of the most plausible outcome type.
This imputation is only done for trivial cases, where there is little doubt.
As a consequence <code>count</code> and <code>continuous</code> outcome types are never imputed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.impute_outcome_type(
  data,
  outcome_column,
  class_levels,
  censoring_indicator,
  event_indicator,
  competing_risk_indicator
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".impute_outcome_type_+3A_data">data</code></td>
<td>
<p>Data set as loaded using the <code>.load_data</code> function.</p>
</td></tr>
<tr><td><code id=".impute_outcome_type_+3A_outcome_column">outcome_column</code></td>
<td>
<p>Name of the outcome column in the data set.</p>
</td></tr>
<tr><td><code id=".impute_outcome_type_+3A_class_levels">class_levels</code></td>
<td>
<p>User-provided class levels for the outcome.</p>
</td></tr>
<tr><td><code id=".impute_outcome_type_+3A_censoring_indicator">censoring_indicator</code></td>
<td>
<p>Name of censoring indicator.</p>
</td></tr>
<tr><td><code id=".impute_outcome_type_+3A_event_indicator">event_indicator</code></td>
<td>
<p>Name of event indicator.</p>
</td></tr>
<tr><td><code id=".impute_outcome_type_+3A_competing_risk_indicator">competing_risk_indicator</code></td>
<td>
<p>Name of competing risk indicator.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The imputed outcome type.
</p>


<h3>Note</h3>

<p>It is highly recommended that the user provides the outcome type.
</p>

<hr>
<h2 id='.load_iterations'>Internal function for loading iteration data from the file system</h2><span id='topic+.load_iterations'></span>

<h3>Description</h3>

<p>Loads iterations generated by <code>.create_iterations</code> that were created in a
previous session. If these are not available, this is indicated by setting a
return flag.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.load_iterations(file_dir, iteration_file = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".load_iterations_+3A_file_dir">file_dir</code></td>
<td>
<p>Path to directory where iteration files are stored.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List containing:
</p>

<ul>
<li> <p><code>iteration_file_exists</code>: An indicator whether an iteration file was found.
</p>
</li>
<li> <p><code>iteration_list</code>: The list of iterations (if available).
</p>
</li>
<li> <p><code>project_id</code>: The unique project identifier (if available).
</p>
</li></ul>


<hr>
<h2 id='.parse_categorical_features'>Internal function for setting categorical features</h2><span id='topic+.parse_categorical_features'></span>

<h3>Description</h3>

<p>Internal function for setting categorical features
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.parse_categorical_features(data, outcome_type, reference_method = "auto")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".parse_categorical_features_+3A_data">data</code></td>
<td>
<p>data.table with feature data</p>
</td></tr>
<tr><td><code id=".parse_categorical_features_+3A_outcome_type">outcome_type</code></td>
<td>
<p>character, indicating the type of outcome</p>
</td></tr>
<tr><td><code id=".parse_categorical_features_+3A_reference_method">reference_method</code></td>
<td>
<p>character, indicating the type of method used to set
the reference level.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function parses columns containing feature data to factors if
the data contained therein have logical (TRUE, FALSE), character, or factor
classes.  Unless passed as feature names with <code>reference</code>, numerical data,
including integers, are not converted to factors.
</p>


<h3>Value</h3>

<p>data.table with several features converted to factor.
</p>

<hr>
<h2 id='.parse_evaluation_settings'>Internal function for parsing settings related to model evaluation</h2><span id='topic+.parse_evaluation_settings'></span>

<h3>Description</h3>

<p>Internal function for parsing settings related to model evaluation
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.parse_evaluation_settings(
  config = NULL,
  data,
  parallel,
  outcome_type,
  hpo_metric,
  development_batch_id,
  vimp_aggregation_method,
  vimp_aggregation_rank_threshold,
  prep_cluster_method,
  prep_cluster_linkage_method,
  prep_cluster_cut_method,
  prep_cluster_similarity_threshold,
  prep_cluster_similarity_metric,
  evaluate_top_level_only = waiver(),
  skip_evaluation_elements = waiver(),
  ensemble_method = waiver(),
  evaluation_metric = waiver(),
  sample_limit = waiver(),
  detail_level = waiver(),
  estimation_type = waiver(),
  aggregate_results = waiver(),
  confidence_level = waiver(),
  bootstrap_ci_method = waiver(),
  feature_cluster_method = waiver(),
  feature_cluster_cut_method = waiver(),
  feature_linkage_method = waiver(),
  feature_similarity_metric = waiver(),
  feature_similarity_threshold = waiver(),
  sample_cluster_method = waiver(),
  sample_linkage_method = waiver(),
  sample_similarity_metric = waiver(),
  eval_aggregation_method = waiver(),
  eval_aggregation_rank_threshold = waiver(),
  eval_icc_type = waiver(),
  stratification_method = waiver(),
  stratification_threshold = waiver(),
  time_max = waiver(),
  evaluation_times = waiver(),
  dynamic_model_loading = waiver(),
  parallel_evaluation = waiver(),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".parse_evaluation_settings_+3A_config">config</code></td>
<td>
<p>A list of settings, e.g. from an xml file.</p>
</td></tr>
<tr><td><code id=".parse_evaluation_settings_+3A_data">data</code></td>
<td>
<p>Data set as loaded using the <code>.load_data</code> function.</p>
</td></tr>
<tr><td><code id=".parse_evaluation_settings_+3A_parallel">parallel</code></td>
<td>
<p>Logical value that whether familiar uses parallelisation. If
<code>FALSE</code> it will override <code>parallel_evaluation</code>.</p>
</td></tr>
<tr><td><code id=".parse_evaluation_settings_+3A_outcome_type">outcome_type</code></td>
<td>
<p>Type of outcome found in the data set.</p>
</td></tr>
<tr><td><code id=".parse_evaluation_settings_+3A_hpo_metric">hpo_metric</code></td>
<td>
<p>Metric defined for hyperparameter optimisation.</p>
</td></tr>
<tr><td><code id=".parse_evaluation_settings_+3A_development_batch_id">development_batch_id</code></td>
<td>
<p>Identifiers of batches used for model development.
These identifiers are used to determine the cohorts used to determine a
setting for <code>time_max</code>, if the <code>outcome_type</code> is <code>survival</code>, and both
<code>time_max</code> and <code>evaluation_times</code> are not provided.</p>
</td></tr>
<tr><td><code id=".parse_evaluation_settings_+3A_vimp_aggregation_method">vimp_aggregation_method</code></td>
<td>
<p>Method for variable importance aggregation that
was used for feature selection.</p>
</td></tr>
<tr><td><code id=".parse_evaluation_settings_+3A_vimp_aggregation_rank_threshold">vimp_aggregation_rank_threshold</code></td>
<td>
<p>Rank threshold for variable importance
aggregation used during feature selection.</p>
</td></tr>
<tr><td><code id=".parse_evaluation_settings_+3A_prep_cluster_method">prep_cluster_method</code></td>
<td>
<p>Cluster method used during pre-processing.</p>
</td></tr>
<tr><td><code id=".parse_evaluation_settings_+3A_prep_cluster_linkage_method">prep_cluster_linkage_method</code></td>
<td>
<p>Cluster linkage method used during
pre-processing.</p>
</td></tr>
<tr><td><code id=".parse_evaluation_settings_+3A_prep_cluster_cut_method">prep_cluster_cut_method</code></td>
<td>
<p>Cluster cut method used during pre-processing.</p>
</td></tr>
<tr><td><code id=".parse_evaluation_settings_+3A_prep_cluster_similarity_threshold">prep_cluster_similarity_threshold</code></td>
<td>
<p>Cluster similarity threshold used
during pre-processing.</p>
</td></tr>
<tr><td><code id=".parse_evaluation_settings_+3A_prep_cluster_similarity_metric">prep_cluster_similarity_metric</code></td>
<td>
<p>Cluster similarity metric used during
pre-processing.</p>
</td></tr>
<tr><td><code id=".parse_evaluation_settings_+3A_evaluate_top_level_only">evaluate_top_level_only</code></td>
<td>
<p>(<em>optional</em>) Flag that signals that only
evaluation at the most global experiment level is required. Consider a
cross-validation experiment with additional external validation. The global
experiment level consists of data that are used for development, internal
validation and external validation. The next lower experiment level are the
individual cross-validation iterations.
</p>
<p>When the flag is <code>true</code>, evaluations take place on the global level only,
and no results are generated for the next lower experiment levels. In our
example, this means that results from individual cross-validation iterations
are not computed and shown. When the flag is <code>false</code>, results are computed
from both the global layer and the next lower level.
</p>
<p>Setting the flag to <code>true</code> saves computation time.</p>
</td></tr>
<tr><td><code id=".parse_evaluation_settings_+3A_skip_evaluation_elements">skip_evaluation_elements</code></td>
<td>
<p>(<em>optional</em>) Specifies which evaluation steps,
if any, should be skipped as part of the evaluation process. Defaults to
<code>none</code>, which means that all relevant evaluation steps are performed. It can
have one or more of the following values:
</p>

<ul>
<li> <p><code>none</code>, <code>false</code>: no steps are skipped.
</p>
</li>
<li> <p><code>all</code>, <code>true</code>: all steps are skipped.
</p>
</li>
<li> <p><code>auc_data</code>: data for assessing and plotting the area under the receiver
operating characteristic curve are not computed.
</p>
</li>
<li> <p><code>calibration_data</code>: data for assessing and plotting model calibration are
not computed.
</p>
</li>
<li> <p><code>calibration_info</code>: data required to assess calibration, such as baseline
survival curves, are not collected. These data will still be present in the
models.
</p>
</li>
<li> <p><code>confusion_matrix</code>: data for assessing and plotting a confusion matrix are
not collected.
</p>
</li>
<li> <p><code>decision_curve_analyis</code>: data for performing a decision curve analysis
are not computed.
</p>
</li>
<li> <p><code>feature_expressions</code>: data for assessing and plotting sample clustering
are not computed.
</p>
</li>
<li> <p><code>feature_similarity</code>: data for assessing and plotting feature clusters are
not computed.
</p>
</li>
<li> <p><code>fs_vimp</code>: data for assessing and plotting feature selection-based
variable importance are not collected.
</p>
</li>
<li> <p><code>hyperparameters</code>: data for assessing model hyperparameters are not
collected. These data will still be present in the models.
</p>
</li>
<li> <p><code>ice_data</code>: data for individual conditional expectation and partial
dependence plots are not created.
</p>
</li>
<li> <p><code>model_performance</code>: data for assessing and visualising model performance
are not created.
</p>
</li>
<li> <p><code>model_vimp</code>: data for assessing and plotting model-based variable
importance are not collected.
</p>
</li>
<li> <p><code>permutation_vimp</code>: data for assessing and plotting model-agnostic
permutation variable importance are not computed.
</p>
</li>
<li> <p><code>prediction_data</code>: predictions for each sample are not made and exported.
</p>
</li>
<li> <p><code>risk_stratification_data</code>: data for assessing and plotting Kaplan-Meier
survival curves are not collected.
</p>
</li>
<li> <p><code>risk_stratification_info</code>: data for assessing stratification into risk
groups are not computed.
</p>
</li>
<li> <p><code>univariate_analysis</code>: data for assessing and plotting univariate feature
importance are not computed.
</p>
</li></ul>
</td></tr>
<tr><td><code id=".parse_evaluation_settings_+3A_ensemble_method">ensemble_method</code></td>
<td>
<p>(<em>optional</em>) Method for ensembling predictions from
models for the same sample. Available methods are:
</p>

<ul>
<li> <p><code>median</code> (default): Use the median of the predicted values as the ensemble
value for a sample.
</p>
</li>
<li> <p><code>mean</code>: Use the mean of the predicted values as the ensemble value for a
sample.
</p>
</li></ul>

<p>This parameter is only used if <code>detail_level</code> is <code>ensemble</code>.</p>
</td></tr>
<tr><td><code id=".parse_evaluation_settings_+3A_evaluation_metric">evaluation_metric</code></td>
<td>
<p>(<em>optional</em>) One or more metrics for assessing model
performance. See the vignette on performance metrics for the available
metrics.
</p>
<p>Confidence intervals (or rather credibility intervals) are computed for each
metric during evaluation. This is done using bootstraps, the number of which
depends on the value of <code>confidence_level</code> (Davison and Hinkley, 1997).
</p>
<p>If unset, the metric in the <code>optimisation_metric</code> variable is used.</p>
</td></tr>
<tr><td><code id=".parse_evaluation_settings_+3A_sample_limit">sample_limit</code></td>
<td>
<p>(<em>optional</em>) Set the upper limit of the number of samples
that are used during evaluation steps. Cannot be less than 20.
</p>
<p>This setting can be specified per data element by providing a parameter
value in a named list with data elements, e.g.
<code>list("sample_similarity"=100, "permutation_vimp"=1000)</code>.
</p>
<p>This parameter can be set for the following data elements:
<code>sample_similarity</code> and <code>ice_data</code>.</p>
</td></tr>
<tr><td><code id=".parse_evaluation_settings_+3A_detail_level">detail_level</code></td>
<td>
<p>(<em>optional</em>) Sets the level at which results are computed
and aggregated.
</p>

<ul>
<li> <p><code>ensemble</code>: Results are computed at the ensemble level, i.e. over all
models in the ensemble. This means that, for example, bias-corrected
estimates of model performance are assessed by creating (at least) 20
bootstraps and computing the model performance of the ensemble model for
each bootstrap.
</p>
</li>
<li> <p><code>hybrid</code> (default): Results are computed at the level of models in an
ensemble. This means that, for example, bias-corrected estimates of model
performance are directly computed using the models in the ensemble. If there
are at least 20 trained models in the ensemble, performance is computed for
each model, in contrast to <code>ensemble</code> where performance is computed for the
ensemble of models. If there are less than 20 trained models in the
ensemble, bootstraps are created so that at least 20 point estimates can be
made.
</p>
</li>
<li> <p><code>model</code>: Results are computed at the model level. This means that, for
example, bias-corrected estimates of model performance are assessed by
creating (at least) 20 bootstraps and computing the performance of the model
for each bootstrap.
</p>
</li></ul>

<p>Note that each level of detail has a different interpretation for bootstrap
confidence intervals. For <code>ensemble</code> and <code>model</code> these are the confidence
intervals for the ensemble and an individual model, respectively. That is,
the confidence interval describes the range where an estimate produced by a
respective ensemble or model trained on a repeat of the experiment may be
found with the probability of the confidence level. For <code>hybrid</code>, it
represents the range where any single model trained on a repeat of the
experiment may be found with the probability of the confidence level. By
definition, confidence intervals obtained using <code>hybrid</code> are at least as
wide as those for <code>ensemble</code>. <code>hybrid</code> offers the correct interpretation if
the goal of the analysis is to assess the result of a single, unspecified,
model.
</p>
<p><code>hybrid</code> is generally computationally less expensive then <code>ensemble</code>, which
in turn is somewhat less expensive than <code>model</code>.
</p>
<p>A non-default <code>detail_level</code> parameter can be specified for separate
evaluation steps by providing a parameter value in a named list with data
elements, e.g. <code>list("auc_data"="ensemble", "model_performance"="hybrid")</code>.
This parameter can be set for the following data elements: <code>auc_data</code>,
<code>decision_curve_analyis</code>, <code>model_performance</code>, <code>permutation_vimp</code>,
<code>ice_data</code>, <code>prediction_data</code> and <code>confusion_matrix</code>.</p>
</td></tr>
<tr><td><code id=".parse_evaluation_settings_+3A_estimation_type">estimation_type</code></td>
<td>
<p>(<em>optional</em>) Sets the type of estimation that should be
possible. This has the following options:
</p>

<ul>
<li> <p><code>point</code>: Point estimates.
</p>
</li>
<li> <p><code>bias_correction</code> or <code>bc</code>: Bias-corrected estimates. A bias-corrected
estimate is computed from (at least) 20 point estimates, and <code>familiar</code> may
bootstrap the data to create them.
</p>
</li>
<li> <p><code>bootstrap_confidence_interval</code> or <code>bci</code> (default): Bias-corrected
estimates with bootstrap confidence intervals (Efron and Hastie, 2016). The
number of point estimates required depends on the <code>confidence_level</code>
parameter, and <code>familiar</code> may bootstrap the data to create them.
</p>
</li></ul>

<p>As with <code>detail_level</code>, a non-default <code>estimation_type</code> parameter can be
specified for separate evaluation steps by providing a parameter value in a
named list with data elements, e.g. <code>list("auc_data"="bci", "model_performance"="point")</code>. This parameter can be set for the following
data elements: <code>auc_data</code>, <code>decision_curve_analyis</code>, <code>model_performance</code>,
<code>permutation_vimp</code>, <code>ice_data</code>, and <code>prediction_data</code>.</p>
</td></tr>
<tr><td><code id=".parse_evaluation_settings_+3A_aggregate_results">aggregate_results</code></td>
<td>
<p>(<em>optional</em>) Flag that signifies whether results
should be aggregated during evaluation. If <code>estimation_type</code> is
<code>bias_correction</code> or <code>bc</code>, aggregation leads to a single bias-corrected
estimate. If <code>estimation_type</code> is <code>bootstrap_confidence_interval</code> or <code>bci</code>,
aggregation leads to a single bias-corrected estimate with lower and upper
boundaries of the confidence interval. This has no effect if
<code>estimation_type</code> is <code>point</code>.
</p>
<p>The default value is equal to <code>TRUE</code> except when assessing metrics to assess
model performance, as the default violin plot requires underlying data.
</p>
<p>As with <code>detail_level</code> and <code>estimation_type</code>, a non-default
<code>aggregate_results</code> parameter can be specified for separate evaluation steps
by providing a parameter value in a named list with data elements, e.g.
<code>list("auc_data"=TRUE, , "model_performance"=FALSE)</code>. This parameter exists
for the same elements as <code>estimation_type</code>.</p>
</td></tr>
<tr><td><code id=".parse_evaluation_settings_+3A_confidence_level">confidence_level</code></td>
<td>
<p>(<em>optional</em>) Numeric value for the level at which
confidence intervals are determined. In the case bootstraps are used to
determine the confidence intervals bootstrap estimation, <code>familiar</code> uses the
rule of thumb <code class="reqn">n = 20 / ci.level</code> to determine the number of required
bootstraps.
</p>
<p>The default value is <code>0.95</code>.</p>
</td></tr>
<tr><td><code id=".parse_evaluation_settings_+3A_bootstrap_ci_method">bootstrap_ci_method</code></td>
<td>
<p>(<em>optional</em>) Method used to determine bootstrap
confidence intervals (Efron and Hastie, 2016). The following methods are
implemented:
</p>

<ul>
<li> <p><code>percentile</code> (default): Confidence intervals obtained using the percentile
method.
</p>
</li>
<li> <p><code>bc</code>: Bias-corrected confidence intervals.
</p>
</li></ul>

<p>Note that the standard method is not implemented because this method is
often not suitable due to non-normal distributions. The bias-corrected and
accelerated (BCa) method is not implemented yet.</p>
</td></tr>
<tr><td><code id=".parse_evaluation_settings_+3A_feature_cluster_method">feature_cluster_method</code></td>
<td>
<p>(<em>optional</em>) Method used to perform clustering
of features. The same methods as for the <code>cluster_method</code> configuration
parameter are available: <code>none</code>, <code>hclust</code>, <code>agnes</code>, <code>diana</code> and <code>pam</code>.
</p>
<p>The value for the <code>cluster_method</code> configuration parameter is used by
default. When generating clusters for the purpose of determining mutual
correlation and ordering feature expressions, <code>none</code> is ignored and <code>hclust</code>
is used instead.</p>
</td></tr>
<tr><td><code id=".parse_evaluation_settings_+3A_feature_cluster_cut_method">feature_cluster_cut_method</code></td>
<td>
<p>(<em>optional</em>) Method used to divide features
into separate clusters. The available methods are the same as for the
<code>cluster_cut_method</code> configuration parameter: <code>silhouette</code>, <code>fixed_cut</code> and
<code>dynamic_cut</code>.
</p>
<p><code>silhouette</code> is available for all cluster methods, but <code>fixed_cut</code> only
applies to methods that create hierarchical trees (<code>hclust</code>, <code>agnes</code> and
<code>diana</code>). <code>dynamic_cut</code> requires the <code>dynamicTreeCut</code> package and can only
be used with <code>agnes</code> and <code>hclust</code>.
</p>
<p>The value for the <code>cluster_cut_method</code> configuration parameter is used by
default.</p>
</td></tr>
<tr><td><code id=".parse_evaluation_settings_+3A_feature_linkage_method">feature_linkage_method</code></td>
<td>
<p>(<em>optional</em>) Method used for agglomerative
clustering with <code>hclust</code> and <code>agnes</code>. Linkage determines how features are
sequentially combined into clusters based on distance. The methods are
shared with the <code>cluster_linkage_method</code> configuration parameter: <code>average</code>,
<code>single</code>, <code>complete</code>, <code>weighted</code>, and <code>ward</code>.
</p>
<p>The value for the <code>cluster_linkage_method</code> configuration parameters is used
by default.</p>
</td></tr>
<tr><td><code id=".parse_evaluation_settings_+3A_feature_similarity_metric">feature_similarity_metric</code></td>
<td>
<p>(<em>optional</em>) Metric to determine pairwise
similarity between features. Similarity is computed in the same manner as
for clustering, and <code>feature_similarity_metric</code> therefore has the same
options as <code>cluster_similarity_metric</code>: <code>mcfadden_r2</code>, <code>cox_snell_r2</code>,
<code>nagelkerke_r2</code>, <code>mutual_information</code>, <code>spearman</code>, <code>kendall</code> and <code>pearson</code>.
</p>
<p>The value used for the <code>cluster_similarity_metric</code> configuration parameter
is used by default.</p>
</td></tr>
<tr><td><code id=".parse_evaluation_settings_+3A_feature_similarity_threshold">feature_similarity_threshold</code></td>
<td>
<p>(<em>optional</em>) The threshold level for
pair-wise similarity that is required to form feature clusters with the
<code>fixed_cut</code> method. This threshold functions in the same manner as the one
defined using the <code>cluster_similarity_threshold</code> parameter.
</p>
<p>By default, the value for the <code>cluster_similarity_threshold</code> configuration
parameter is used.
</p>
<p>Unlike for <code>cluster_similarity_threshold</code>, more than one value can be
supplied here.</p>
</td></tr>
<tr><td><code id=".parse_evaluation_settings_+3A_sample_cluster_method">sample_cluster_method</code></td>
<td>
<p>(<em>optional</em>) The method used to perform
clustering based on distance between samples. These are the same methods as
for the <code>cluster_method</code> configuration parameter: <code>hclust</code>, <code>agnes</code>, <code>diana</code>
and <code>pam</code>.
</p>
<p>The value for the <code>cluster_method</code> configuration parameter is used by
default. When generating clusters for the purpose of ordering samples in
feature expressions, <code>none</code> is ignored and <code>hclust</code> is used instead.</p>
</td></tr>
<tr><td><code id=".parse_evaluation_settings_+3A_sample_linkage_method">sample_linkage_method</code></td>
<td>
<p>(<em>optional</em>) The method used for agglomerative
clustering in <code>hclust</code> and <code>agnes</code>. These are the same methods as for the
<code>cluster_linkage_method</code> configuration parameter: <code>average</code>, <code>single</code>,
<code>complete</code>, <code>weighted</code>, and <code>ward</code>.
</p>
<p>The value for the <code>cluster_linkage_method</code> configuration parameters is used
by default.</p>
</td></tr>
<tr><td><code id=".parse_evaluation_settings_+3A_sample_similarity_metric">sample_similarity_metric</code></td>
<td>
<p>(<em>optional</em>) Metric to determine pairwise
similarity between samples. Similarity is computed in the same manner as for
clustering, but <code>sample_similarity_metric</code> has different options that are
better suited to computing distance between samples instead of between
features. The following metrics are available.
</p>

<ul>
<li> <p><code>gower</code> (default): compute Gower's distance between samples. By default,
Gower's distance is computed based on winsorised data to reduce the effect
of outliers (see below).
</p>
</li>
<li> <p><code>euclidean</code>: compute the Euclidean distance between samples.
</p>
</li></ul>

<p>The underlying feature data for numerical features is scaled to the
<code class="reqn">[0,1]</code> range using the feature values across the samples. The
normalisation parameters required can optionally be computed from feature
data with the outer 5% (on both sides) of feature values trimmed or
winsorised. To do so append <code style="white-space: pre;">&#8288;_trim&#8288;</code> (trimming) or <code style="white-space: pre;">&#8288;_winsor&#8288;</code> (winsorising) to
the metric name. This reduces the effect of outliers somewhat.
</p>
<p>Regardless of metric, all categorical features are handled as for the
Gower's distance: distance is 0 if the values in a pair of samples match,
and 1 if they do not.</p>
</td></tr>
<tr><td><code id=".parse_evaluation_settings_+3A_eval_aggregation_method">eval_aggregation_method</code></td>
<td>
<p>(<em>optional</em>) Method for aggregating variable
importances for the purpose of evaluation. Variable importances are
determined during feature selection steps and after training the model. Both
types are evaluated, but feature selection variable importance is only
evaluated at run-time.
</p>
<p>See the documentation for the <code>vimp_aggregation_method</code> argument for
information concerning the different methods available.</p>
</td></tr>
<tr><td><code id=".parse_evaluation_settings_+3A_eval_aggregation_rank_threshold">eval_aggregation_rank_threshold</code></td>
<td>
<p>(<em>optional</em>) The threshold used to
define the subset of highly important features during evaluation.
</p>
<p>See the documentation for the <code>vimp_aggregation_rank_threshold</code> argument for
more information.</p>
</td></tr>
<tr><td><code id=".parse_evaluation_settings_+3A_eval_icc_type">eval_icc_type</code></td>
<td>
<p>(<em>optional</em>) String indicating the type of intraclass
correlation coefficient (<code>1</code>, <code>2</code> or <code>3</code>) that should be used to compute
robustness for features in repeated measurements during the evaluation of
univariate importance. These types correspond to the types in Shrout and
Fleiss (1979). The default value is <code>1</code>.</p>
</td></tr>
<tr><td><code id=".parse_evaluation_settings_+3A_stratification_method">stratification_method</code></td>
<td>
<p>(<em>optional</em>) Method for determining the
stratification threshold for creating survival groups. The actual,
model-dependent, threshold value is obtained from the development data, and
can afterwards be used to perform stratification on validation data.
</p>
<p>The following stratification methods are available:
</p>

<ul>
<li> <p><code>median</code> (default): The median predicted value in the development cohort
is used to stratify the samples into two risk groups. For predicted outcome
values that build a continuous spectrum, the two risk groups in the
development cohort will be roughly equal in size.
</p>
</li>
<li> <p><code>mean</code>: The mean predicted value in the development cohort is used to
stratify the samples into two risk groups.
</p>
</li>
<li> <p><code>mean_trim</code>: As <code>mean</code>, but based on the set of predicted values
where the 5% lowest and 5% highest values are discarded. This reduces the
effect of outliers.
</p>
</li>
<li> <p><code>mean_winsor</code>: As <code>mean</code>, but based on the set of predicted values where
the 5% lowest and 5% highest values are winsorised. This reduces the effect
of outliers.
</p>
</li>
<li> <p><code>fixed</code>: Samples are stratified based on the sample quantiles of the
predicted values. These quantiles are defined using the
<code>stratification_threshold</code> parameter.
</p>
</li>
<li> <p><code>optimised</code>: Use maximally selected rank statistics to determine the
optimal threshold (Lausen and Schumacher, 1992; Hothorn et al., 2003) to
stratify samples into two optimally separated risk groups.
</p>
</li></ul>

<p>One or more stratification methods can be selected simultaneously.
</p>
<p>This parameter is only relevant for <code>survival</code> outcomes.</p>
</td></tr>
<tr><td><code id=".parse_evaluation_settings_+3A_stratification_threshold">stratification_threshold</code></td>
<td>
<p>(<em>optional</em>) Numeric value(s) signifying the
sample quantiles for stratification using the <code>fixed</code> method. The number of
risk groups will be the number of values +1.
</p>
<p>The default value is <code>c(1/3, 2/3)</code>, which will yield two thresholds that
divide samples into three equally sized groups. If <code>fixed</code> is not among the
selected stratification methods, this parameter is ignored.
</p>
<p>This parameter is only relevant for <code>survival</code> outcomes.</p>
</td></tr>
<tr><td><code id=".parse_evaluation_settings_+3A_time_max">time_max</code></td>
<td>
<p>(<em>optional</em>) Time point which is used as the benchmark for
e.g. cumulative risks generated by random forest, or the cutoff for Uno's
concordance index.
</p>
<p>If <code>time_max</code> is not provided, but <code>evaluation_times</code> is, the largest value
of <code>evaluation_times</code> is used. If both are not provided, <code>time_max</code> is set
to the 98th percentile of the distribution of survival times for samples
with an event in the development data set.
</p>
<p>This parameter is only relevant for <code>survival</code> outcomes.</p>
</td></tr>
<tr><td><code id=".parse_evaluation_settings_+3A_evaluation_times">evaluation_times</code></td>
<td>
<p>(<em>optional</em>) One or more time points that are used for
assessing calibration in survival problems. This is done as expected and
observed survival probabilities depend on time.
</p>
<p>If unset, <code>evaluation_times</code> will be equal to <code>time_max</code>.
</p>
<p>This parameter is only relevant for <code>survival</code> outcomes.</p>
</td></tr>
<tr><td><code id=".parse_evaluation_settings_+3A_dynamic_model_loading">dynamic_model_loading</code></td>
<td>
<p>(<em>optional</em>) Enables dynamic loading of models
during the evaluation process, if <code>TRUE</code>. Defaults to <code>FALSE</code>. Dynamic
loading of models may reduce the overall memory footprint, at the cost of
increased disk or network IO. Models can only be dynamically loaded if they
are found at an accessible disk or network location. Setting this parameter
to <code>TRUE</code> may help if parallel processing causes out-of-memory issues during
evaluation.</p>
</td></tr>
<tr><td><code id=".parse_evaluation_settings_+3A_parallel_evaluation">parallel_evaluation</code></td>
<td>
<p>(<em>optional</em>) Enable parallel processing for
hyperparameter optimisation. Defaults to <code>TRUE</code>. When set to <code>FALSE</code>, this
will disable the use of parallel processing while performing optimisation,
regardless of the settings of the <code>parallel</code> parameter. The parameter
moreover specifies whether parallelisation takes place within the evaluation
process steps (<code>inner</code>, default), or in an outer loop ( <code>outer</code>) over
learners, data subsamples, etc.
</p>
<p><code>parallel_evaluation</code> is ignored if <code>parallel=FALSE</code>.</p>
</td></tr>
<tr><td><code id=".parse_evaluation_settings_+3A_...">...</code></td>
<td>
<p>Unused arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List of parameters related to model evaluation.
</p>


<h3>References</h3>


<ol>
<li><p> Davison, A. C. &amp; Hinkley, D. V. Bootstrap methods and their
application. (Cambridge University Press, 1997).
</p>
</li>
<li><p> Efron, B. &amp; Hastie, T. Computer Age Statistical Inference. (Cambridge
University Press, 2016).
</p>
</li>
<li><p> Lausen, B. &amp; Schumacher, M. Maximally Selected Rank Statistics.
Biometrics 48, 73 (1992).
</p>
</li>
<li><p> Hothorn, T. &amp; Lausen, B. On the exact distribution of maximally selected
rank statistics. Comput. Stat. Data Anal. 43, 121–137 (2003).
</p>
</li></ol>


<hr>
<h2 id='.parse_experiment_settings'>Internal function for parsing settings related to the computational setup</h2><span id='topic+.parse_experiment_settings'></span>

<h3>Description</h3>

<p>Internal function for parsing settings related to the computational setup
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.parse_experiment_settings(
  config = NULL,
  batch_id_column = waiver(),
  sample_id_column = waiver(),
  series_id_column = waiver(),
  development_batch_id = waiver(),
  validation_batch_id = waiver(),
  outcome_name = waiver(),
  outcome_column = waiver(),
  outcome_type = waiver(),
  event_indicator = waiver(),
  censoring_indicator = waiver(),
  competing_risk_indicator = waiver(),
  class_levels = waiver(),
  signature = waiver(),
  novelty_features = waiver(),
  exclude_features = waiver(),
  include_features = waiver(),
  reference_method = waiver(),
  experimental_design = waiver(),
  imbalance_correction_method = waiver(),
  imbalance_n_partitions = waiver(),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".parse_experiment_settings_+3A_config">config</code></td>
<td>
<p>A list of settings, e.g. from an xml file.</p>
</td></tr>
<tr><td><code id=".parse_experiment_settings_+3A_batch_id_column">batch_id_column</code></td>
<td>
<p>(<strong>recommended</strong>) Name of the column containing batch
or cohort identifiers. This parameter is required if more than one dataset
is provided, or if external validation is performed.
</p>
<p>In familiar any row of data is organised by four identifiers:
</p>

<ul>
<li><p> The batch identifier <code>batch_id_column</code>: This denotes the group to which a
set of samples belongs, e.g. patients from a single study, samples measured
in a batch, etc. The batch identifier is used for batch normalisation, as
well as selection of development and validation datasets.
</p>
</li>
<li><p> The sample identifier <code>sample_id_column</code>: This denotes the sample level,
e.g. data from a single individual. Subsets of data, e.g. bootstraps or
cross-validation folds, are created at this level.
</p>
</li>
<li><p> The series identifier <code>series_id_column</code>: Indicates measurements on a
single sample that may not share the same outcome value, e.g. a time
series, or the number of cells in a view.
</p>
</li>
<li><p> The repetition identifier: Indicates repeated measurements in a single
series where any feature values may differ, but the outcome does not.
Repetition identifiers are always implicitly set when multiple entries for
the same series of the same sample in the same batch that share the same
outcome are encountered.
</p>
</li></ul>
</td></tr>
<tr><td><code id=".parse_experiment_settings_+3A_sample_id_column">sample_id_column</code></td>
<td>
<p>(<strong>recommended</strong>) Name of the column containing
sample or subject identifiers. See <code>batch_id_column</code> above for more
details.
</p>
<p>If unset, every row will be identified as a single sample.</p>
</td></tr>
<tr><td><code id=".parse_experiment_settings_+3A_series_id_column">series_id_column</code></td>
<td>
<p>(<strong>optional</strong>) Name of the column containing series
identifiers, which distinguish between measurements that are part of a
series for a single sample. See <code>batch_id_column</code> above for more details.
</p>
<p>If unset, rows which share the same batch and sample identifiers but have a
different outcome are assigned unique series identifiers.</p>
</td></tr>
<tr><td><code id=".parse_experiment_settings_+3A_development_batch_id">development_batch_id</code></td>
<td>
<p>(<em>optional</em>) One or more batch or cohort
identifiers to constitute data sets for development. Defaults to all, or
all minus the identifiers in <code>validation_batch_id</code> for external validation.
Required if external validation is performed and <code>validation_batch_id</code> is
not provided.</p>
</td></tr>
<tr><td><code id=".parse_experiment_settings_+3A_validation_batch_id">validation_batch_id</code></td>
<td>
<p>(<em>optional</em>) One or more batch or cohort
identifiers to constitute data sets for external validation. Defaults to
all data sets except those in <code>development_batch_id</code> for external
validation, or none if not. Required if <code>development_batch_id</code> is not
provided.</p>
</td></tr>
<tr><td><code id=".parse_experiment_settings_+3A_outcome_name">outcome_name</code></td>
<td>
<p>(<em>optional</em>) Name of the modelled outcome. This name will
be used in figures created by <code>familiar</code>.
</p>
<p>If not set, the column name in <code>outcome_column</code> will be used for
<code>binomial</code>, <code>multinomial</code>, <code>count</code> and <code>continuous</code> outcomes. For other
outcomes (<code>survival</code> and <code>competing_risk</code>) no default is used.</p>
</td></tr>
<tr><td><code id=".parse_experiment_settings_+3A_outcome_column">outcome_column</code></td>
<td>
<p>(<strong>recommended</strong>) Name of the column containing the
outcome of interest. May be identified from a formula, if a formula is
provided as an argument. Otherwise an error is raised. Note that <code>survival</code>
and <code>competing_risk</code> outcome type outcomes require two columns that
indicate the time-to-event or the time of last follow-up and the event
status.</p>
</td></tr>
<tr><td><code id=".parse_experiment_settings_+3A_outcome_type">outcome_type</code></td>
<td>
<p>(<strong>recommended</strong>) Type of outcome found in the outcome
column. The outcome type determines many aspects of the overall process,
e.g. the available feature selection methods and learners, but also the
type of assessments that can be conducted to evaluate the resulting models.
Implemented outcome types are:
</p>

<ul>
<li> <p><code>binomial</code>: categorical outcome with 2 levels.
</p>
</li>
<li> <p><code>multinomial</code>: categorical outcome with 2 or more levels.
</p>
</li>
<li> <p><code>count</code>: Poisson-distributed numeric outcomes.
</p>
</li>
<li> <p><code>continuous</code>: general continuous numeric outcomes.
</p>
</li>
<li> <p><code>survival</code>: survival outcome for time-to-event data.
</p>
</li></ul>

<p>If not provided, the algorithm will attempt to obtain outcome_type from
contents of the outcome column. This may lead to unexpected results, and we
therefore advise to provide this information manually.
</p>
<p>Note that <code>competing_risk</code> survival analysis are not fully supported, and
is currently not a valid choice for <code>outcome_type</code>.</p>
</td></tr>
<tr><td><code id=".parse_experiment_settings_+3A_event_indicator">event_indicator</code></td>
<td>
<p>(<strong>recommended</strong>) Indicator for events in <code>survival</code>
and <code>competing_risk</code> analyses. <code>familiar</code> will automatically recognise <code>1</code>,
<code>true</code>, <code>t</code>, <code>y</code> and <code>yes</code> as event indicators, including different
capitalisations. If this parameter is set, it replaces the default values.</p>
</td></tr>
<tr><td><code id=".parse_experiment_settings_+3A_censoring_indicator">censoring_indicator</code></td>
<td>
<p>(<strong>recommended</strong>) Indicator for right-censoring in
<code>survival</code> and <code>competing_risk</code> analyses. <code>familiar</code> will automatically
recognise <code>0</code>, <code>false</code>, <code>f</code>, <code>n</code>, <code>no</code> as censoring indicators, including
different capitalisations. If this parameter is set, it replaces the
default values.</p>
</td></tr>
<tr><td><code id=".parse_experiment_settings_+3A_competing_risk_indicator">competing_risk_indicator</code></td>
<td>
<p>(<strong>recommended</strong>) Indicator for competing
risks in <code>competing_risk</code> analyses. There are no default values, and if
unset, all values other than those specified by the <code>event_indicator</code> and
<code>censoring_indicator</code> parameters are considered to indicate competing
risks.</p>
</td></tr>
<tr><td><code id=".parse_experiment_settings_+3A_class_levels">class_levels</code></td>
<td>
<p>(<em>optional</em>) Class levels for <code>binomial</code> or <code>multinomial</code>
outcomes. This argument can be used to specify the ordering of levels for
categorical outcomes. These class levels must exactly match the levels
present in the outcome column.</p>
</td></tr>
<tr><td><code id=".parse_experiment_settings_+3A_signature">signature</code></td>
<td>
<p>(<em>optional</em>) One or more names of feature columns that are
considered part of a specific signature. Features specified here will
always be used for modelling. Ranking from feature selection has no effect
for these features.</p>
</td></tr>
<tr><td><code id=".parse_experiment_settings_+3A_novelty_features">novelty_features</code></td>
<td>
<p>(<em>optional</em>) One or more names of feature columns
that should be included for the purpose of novelty detection.</p>
</td></tr>
<tr><td><code id=".parse_experiment_settings_+3A_exclude_features">exclude_features</code></td>
<td>
<p>(<em>optional</em>) Feature columns that will be removed
from the data set. Cannot overlap with features in <code>signature</code>,
<code>novelty_features</code> or <code>include_features</code>.</p>
</td></tr>
<tr><td><code id=".parse_experiment_settings_+3A_include_features">include_features</code></td>
<td>
<p>(<em>optional</em>) Feature columns that are specifically
included in the data set. By default all features are included. Cannot
overlap with <code>exclude_features</code>, but may overlap <code>signature</code>. Features in
<code>signature</code> and <code>novelty_features</code> are always included. If both
<code>exclude_features</code> and <code>include_features</code> are provided, <code>include_features</code>
takes precedence, provided that there is no overlap between the two.</p>
</td></tr>
<tr><td><code id=".parse_experiment_settings_+3A_reference_method">reference_method</code></td>
<td>
<p>(<em>optional</em>) Method used to set reference levels for
categorical features. There are several options:
</p>

<ul>
<li> <p><code>auto</code> (default): Categorical features that are not explicitly set by the
user, i.e. columns containing boolean values or characters, use the most
frequent level as reference. Categorical features that are explicitly set,
i.e. as factors, are used as is.
</p>
</li>
<li> <p><code>always</code>: Both automatically detected and user-specified categorical
features have the reference level set to the most frequent level. Ordinal
features are not altered, but are used as is.
</p>
</li>
<li> <p><code>never</code>: User-specified categorical features are used as is.
Automatically detected categorical features are simply sorted, and the
first level is then used as the reference level. This was the behaviour
prior to familiar version 1.3.0.
</p>
</li></ul>
</td></tr>
<tr><td><code id=".parse_experiment_settings_+3A_experimental_design">experimental_design</code></td>
<td>
<p>(<strong>required</strong>) Defines what the experiment looks
like, e.g. <code>cv(bt(fs,20)+mb,3,2)+ev</code> for 2 times repeated 3-fold
cross-validation with nested feature selection on 20 bootstraps and
model-building, and external validation. The basic workflow components are:
</p>

<ul>
<li> <p><code>fs</code>: (required) feature selection step.
</p>
</li>
<li> <p><code>mb</code>: (required) model building step.
</p>
</li>
<li> <p><code>ev</code>: (optional) external validation. Note that internal validation due
to subsampling will always be conducted if the subsampling methods create
any validation data sets.
</p>
</li></ul>

<p>The different components are linked using <code>+</code>.
</p>
<p>Different subsampling methods can be used in conjunction with the basic
workflow components:
</p>

<ul>
<li> <p><code>bs(x,n)</code>: (stratified) .632 bootstrap, with <code>n</code> the number of
bootstraps. In contrast to <code>bt</code>, feature pre-processing parameters and
hyperparameter optimisation are conducted on individual bootstraps.
</p>
</li>
<li> <p><code>bt(x,n)</code>: (stratified) .632 bootstrap, with <code>n</code> the number of
bootstraps. Unlike <code>bs</code> and other subsampling methods, no separate
pre-processing parameters or optimised hyperparameters will be determined
for each bootstrap.
</p>
</li>
<li> <p><code>cv(x,n,p)</code>: (stratified) <code>n</code>-fold cross-validation, repeated <code>p</code> times.
Pre-processing parameters are determined for each iteration.
</p>
</li>
<li> <p><code>lv(x)</code>: leave-one-out-cross-validation. Pre-processing parameters are
determined for each iteration.
</p>
</li>
<li> <p><code>ip(x)</code>: imbalance partitioning for addressing class imbalances on the
data set. Pre-processing parameters are determined for each partition. The
number of partitions generated depends on the imbalance correction method
(see the <code>imbalance_correction_method</code> parameter). Imbalance partitioning
does not generate validation sets.
</p>
</li></ul>

<p>As shown in the example above, sampling algorithms can be nested.
</p>
<p>The simplest valid experimental design is <code>fs+mb</code>, which corresponds to a
TRIPOD type 1a analysis. Type 1b analyses are only possible using
bootstraps, e.g. <code>bt(fs+mb,100)</code>. Type 2a analyses can be conducted using
cross-validation, e.g. <code>cv(bt(fs,100)+mb,10,1)</code>. Depending on the origin of
the external validation data, designs such as <code>fs+mb+ev</code> or
<code>cv(bt(fs,100)+mb,10,1)+ev</code> constitute type 2b or type 3 analyses. Type 4
analyses can be done by obtaining one or more <code>familiarModel</code> objects from
others and applying them to your own data set.
</p>
<p>Alternatively, the <code>experimental_design</code> parameter may be used to provide a
path to a file containing iterations, which is named <code style="white-space: pre;">&#8288;####_iterations.RDS&#8288;</code>
by convention. This path can be relative to the directory of the current
experiment (<code>experiment_dir</code>), or an absolute path. The absolute path may
thus also point to a file from a different experiment.</p>
</td></tr>
<tr><td><code id=".parse_experiment_settings_+3A_imbalance_correction_method">imbalance_correction_method</code></td>
<td>
<p>(<em>optional</em>) Type of method used to
address class imbalances. Available options are:
</p>

<ul>
<li> <p><code>full_undersampling</code> (default): All data will be used in an ensemble
fashion. The full minority class will appear in each partition, but
majority classes are undersampled until all data have been used.
</p>
</li>
<li> <p><code>random_undersampling</code>: Randomly undersamples majority classes. This is
useful in cases where full undersampling would lead to the formation of
many models due major overrepresentation of the largest class.
</p>
</li></ul>

<p>This parameter is only used in combination with imbalance partitioning in
the experimental design, and <code>ip</code> should therefore appear in the string
that defines the design.</p>
</td></tr>
<tr><td><code id=".parse_experiment_settings_+3A_imbalance_n_partitions">imbalance_n_partitions</code></td>
<td>
<p>(<em>optional</em>) Number of times random
undersampling should be repeated. 10 undersampled subsets with balanced
classes are formed by default.</p>
</td></tr>
<tr><td><code id=".parse_experiment_settings_+3A_...">...</code></td>
<td>
<p>Unused arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List of parameters related to data parsing and the experiment.
</p>

<hr>
<h2 id='.parse_feature_selection_settings'>Internal function for parsing settings related to feature selection</h2><span id='topic+.parse_feature_selection_settings'></span>

<h3>Description</h3>

<p>Internal function for parsing settings related to feature selection
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.parse_feature_selection_settings(
  config = NULL,
  data,
  parallel,
  outcome_type,
  fs_method = waiver(),
  fs_method_parameter = waiver(),
  vimp_aggregation_method = waiver(),
  vimp_aggregation_rank_threshold = waiver(),
  parallel_feature_selection = waiver(),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".parse_feature_selection_settings_+3A_config">config</code></td>
<td>
<p>A list of settings, e.g. from an xml file.</p>
</td></tr>
<tr><td><code id=".parse_feature_selection_settings_+3A_data">data</code></td>
<td>
<p>Data set as loaded using the <code>.load_data</code> function.</p>
</td></tr>
<tr><td><code id=".parse_feature_selection_settings_+3A_parallel">parallel</code></td>
<td>
<p>Logical value that whether familiar uses parallelisation. If
<code>FALSE</code> it will override <code>parallel_feature_selection</code>.</p>
</td></tr>
<tr><td><code id=".parse_feature_selection_settings_+3A_outcome_type">outcome_type</code></td>
<td>
<p>Type of outcome found in the data set.</p>
</td></tr>
<tr><td><code id=".parse_feature_selection_settings_+3A_fs_method">fs_method</code></td>
<td>
<p>(<strong>required</strong>) Feature selection method to be used for
determining variable importance. <code>familiar</code> implements various feature
selection methods. Please refer to the vignette on feature selection
methods for more details.
</p>
<p>More than one feature selection method can be chosen. The experiment will
then repeated for each feature selection method.
</p>
<p>Feature selection methods determines the ranking of features. Actual
selection of features is done by optimising the signature size model
hyperparameter during the hyperparameter optimisation step.</p>
</td></tr>
<tr><td><code id=".parse_feature_selection_settings_+3A_fs_method_parameter">fs_method_parameter</code></td>
<td>
<p>(<em>optional</em>) List of lists containing parameters
for feature selection methods. Each sublist should have the name of the
feature selection method it corresponds to.
</p>
<p>Most feature selection methods do not have parameters that can be set.
Please refer to the vignette on feature selection methods for more details.
Note that if the feature selection method is based on a learner (e.g. lasso
regression), hyperparameter optimisation may be performed prior to
assessing variable importance.</p>
</td></tr>
<tr><td><code id=".parse_feature_selection_settings_+3A_vimp_aggregation_method">vimp_aggregation_method</code></td>
<td>
<p>(<em>optional</em>) The method used to aggregate
variable importances over different data subsets, e.g. bootstraps. The
following methods can be selected:
</p>

<ul>
<li> <p><code>none</code>: Don't aggregate ranks, but rather aggregate the variable
importance scores themselves.
</p>
</li>
<li> <p><code>mean</code>: Use the mean rank of a feature over the subsets to
determine the aggregated feature rank.
</p>
</li>
<li> <p><code>median</code>: Use the median rank of a feature over the subsets to determine
the aggregated feature rank.
</p>
</li>
<li> <p><code>best</code>: Use the best rank the feature obtained in any subset to determine
the aggregated feature rank.
</p>
</li>
<li> <p><code>worst</code>: Use the worst rank the feature obtained in any subset to
determine the aggregated feature rank.
</p>
</li>
<li> <p><code>stability</code>: Use the frequency of the feature being in the subset of
highly ranked features as measure for the aggregated feature rank
(Meinshausen and Buehlmann, 2010).
</p>
</li>
<li> <p><code>exponential</code>: Use a rank-weighted frequence of occurrence in the subset
of highly ranked features as measure for the aggregated feature rank (Haury
et al., 2011).
</p>
</li>
<li> <p><code>borda</code> (default): Use the borda count as measure for the aggregated
feature rank (Wald et al., 2012).
</p>
</li>
<li> <p><code>enhanced_borda</code>: Use an occurrence frequency-weighted borda count as
measure for the aggregated feature rank (Wald et al., 2012).
</p>
</li>
<li> <p><code>truncated_borda</code>: Use borda count computed only on features within the
subset of highly ranked features.
</p>
</li>
<li> <p><code>enhanced_truncated_borda</code>: Apply both the enhanced borda method and the
truncated borda method and use the resulting borda count as the aggregated
feature rank.
</p>
</li></ul>

<p>The <em>feature selection methods</em> vignette provides additional information.</p>
</td></tr>
<tr><td><code id=".parse_feature_selection_settings_+3A_vimp_aggregation_rank_threshold">vimp_aggregation_rank_threshold</code></td>
<td>
<p>(<em>optional</em>) The threshold used to
define the subset of highly important features. If not set, this threshold
is determined by maximising the variance in the occurrence value over all
features over the subset size.
</p>
<p>This parameter is only relevant for <code>stability</code>, <code>exponential</code>,
<code>enhanced_borda</code>, <code>truncated_borda</code> and <code>enhanced_truncated_borda</code> methods.</p>
</td></tr>
<tr><td><code id=".parse_feature_selection_settings_+3A_parallel_feature_selection">parallel_feature_selection</code></td>
<td>
<p>(<em>optional</em>) Enable parallel processing for
the feature selection workflow. Defaults to <code>TRUE</code>. When set to <code>FALSE</code>,
this will disable the use of parallel processing while performing feature
selection, regardless of the settings of the <code>parallel</code> parameter.
<code>parallel_feature_selection</code> is ignored if <code>parallel=FALSE</code>.</p>
</td></tr>
<tr><td><code id=".parse_feature_selection_settings_+3A_...">...</code></td>
<td>
<p>Unused arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List of parameters related to feature selection.
</p>


<h3>References</h3>


<ol>
<li><p> Wald, R., Khoshgoftaar, T. M., Dittman, D., Awada, W. &amp;
Napolitano, A. An extensive comparison of feature ranking aggregation
techniques in bioinformatics. in 2012 IEEE 13th International Conference on
Information Reuse Integration (IRI) 377–384 (2012).
</p>
</li>
<li><p> Meinshausen, N. &amp; Buehlmann, P. Stability selection. J. R. Stat. Soc.
Series B Stat. Methodol. 72, 417–473 (2010).
</p>
</li>
<li><p> Haury, A.-C., Gestraud, P. &amp; Vert, J.-P. The influence of feature
selection methods on accuracy, stability and interpretability of molecular
signatures. PLoS One 6, e28210 (2011).
</p>
</li></ol>


<hr>
<h2 id='.parse_file_paths'>Internal function for parsing file paths</h2><span id='topic+.parse_file_paths'></span>

<h3>Description</h3>

<p>Internal function for parsing file paths
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.parse_file_paths(
  config = NULL,
  project_dir = waiver(),
  experiment_dir = waiver(),
  data_file = waiver(),
  verbose = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".parse_file_paths_+3A_config">config</code></td>
<td>
<p>A list of settings, e.g. from an xml file.</p>
</td></tr>
<tr><td><code id=".parse_file_paths_+3A_project_dir">project_dir</code></td>
<td>
<p>(<em>optional</em>) Path to the project directory. <code>familiar</code>
checks if the directory indicated by <code>experiment_dir</code> and data files in
<code>data_file</code> are relative to the <code>project_dir</code>.</p>
</td></tr>
<tr><td><code id=".parse_file_paths_+3A_experiment_dir">experiment_dir</code></td>
<td>
<p>(<strong>recommended</strong>) Path to the directory where all
intermediate and final results produced by <code>familiar</code> are written to.
</p>
<p>The <code>experiment_dir</code> can be a path relative to <code>project_dir</code> or an absolute
path.
</p>
<p>In case no project directory is provided and the experiment directory is
not on an absolute path, a directory will be created in the temporary R
directory indicated by <code>tempdir()</code>. This directory is deleted after closing
the R session or once data analysis has finished. All information will be
lost afterwards. Hence, it is recommended to provide either
<code>experiment_dir</code> as an absolute path, or provide both <code>project_dir</code> and
<code>experiment_dir</code>.</p>
</td></tr>
<tr><td><code id=".parse_file_paths_+3A_data_file">data_file</code></td>
<td>
<p>(<em>optional</em>) Path to files containing data that should be
analysed. The paths can be relative to <code>project_dir</code> or absolute paths. An
error will be raised if the file cannot be found.
</p>
<p>The following types of data are supported.
</p>

<ul>
<li> <p><code>csv</code> files containing column headers on the first row, and samples per
row. <code>csv</code> files are read using <code>data.table::fread</code>.
</p>
</li>
<li> <p><code>rds</code> files that contain a <code>data.table</code> or <code>data.frame</code> object. <code>rds</code>
files are imported using <code>base::readRDS</code>.
</p>
</li>
<li> <p><code>RData</code> files that contain a single <code>data.table</code> or <code>data.frame</code> object.
<code>RData</code> files are imported using <code>base::load</code>.
</p>
</li></ul>

<p>All data are expected in wide format, with sample information organised
row-wise.
</p>
<p>More than one data file can be provided. <code>familiar</code> will try to combine
data files based on column names and identifier columns.
</p>
<p>Alternatively, data can be provided using the <code>data</code> argument. These data
are expected to be <code>data.frame</code> or <code>data.table</code> objects or paths to data
files. The latter are handled in the same way as file paths provided to
<code>data_file</code>.</p>
</td></tr>
<tr><td><code id=".parse_file_paths_+3A_verbose">verbose</code></td>
<td>
<p>Sets verbosity.</p>
</td></tr>
<tr><td><code id=".parse_file_paths_+3A_...">...</code></td>
<td>
<p>Unused arguments</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List of paths to important directories and files.
</p>

<hr>
<h2 id='.parse_general_settings'>Internal function for parsing settings that configure various aspects of the
worklow</h2><span id='topic+.parse_general_settings'></span>

<h3>Description</h3>

<p>Internal function for parsing settings that configure various aspects of the
worklow
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.parse_general_settings(settings, config = NULL, data, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".parse_general_settings_+3A_settings">settings</code></td>
<td>
<p>List of settings that was previously generated by
<code>.parse_initial_settings</code>.</p>
</td></tr>
<tr><td><code id=".parse_general_settings_+3A_config">config</code></td>
<td>
<p>A list of settings, e.g. from an xml file.</p>
</td></tr>
<tr><td><code id=".parse_general_settings_+3A_data">data</code></td>
<td>
<p>Data set as loaded using the <code>.load_data</code> function.</p>
</td></tr>
<tr><td><code id=".parse_general_settings_+3A_...">...</code></td>
<td>

<p>Arguments passed on to <code><a href="#topic+.parse_setup_settings">.parse_setup_settings</a></code>, <code><a href="#topic+.parse_preprocessing_settings">.parse_preprocessing_settings</a></code>, <code><a href="#topic+.parse_feature_selection_settings">.parse_feature_selection_settings</a></code>, <code><a href="#topic+.parse_model_development_settings">.parse_model_development_settings</a></code>, <code><a href="#topic+.parse_hyperparameter_optimisation_settings">.parse_hyperparameter_optimisation_settings</a></code>, <code><a href="#topic+.parse_evaluation_settings">.parse_evaluation_settings</a></code>
</p>

<dl>
<dt><code>parallel</code></dt><dd><p>(<em>optional</em>) Enable parallel processing. Defaults to <code>TRUE</code>.
When set to <code>FALSE</code>, this disables all parallel processing, regardless of
specific parameters such as <code>parallel_preprocessing</code>. However, when
<code>parallel</code> is <code>TRUE</code>, parallel processing of different parts of the
workflow can be disabled by setting respective flags to <code>FALSE</code>.</p>
</dd>
<dt><code>parallel_nr_cores</code></dt><dd><p>(<em>optional</em>) Number of cores available for
parallelisation. Defaults to 2. This setting does nothing if
parallelisation is disabled.</p>
</dd>
<dt><code>restart_cluster</code></dt><dd><p>(<em>optional</em>) Restart nodes used for parallel computing
to free up memory prior to starting a parallel process. Note that it does
take time to set up the clusters. Therefore setting this argument to <code>TRUE</code>
may impact processing speed. This argument is ignored if <code>parallel</code> is
<code>FALSE</code> or the cluster was initialised outside of familiar. Default is
<code>FALSE</code>, which causes the clusters to be initialised only once.</p>
</dd>
<dt><code>cluster_type</code></dt><dd><p>(<em>optional</em>) Selection of the cluster type for parallel
processing. Available types are the ones supported by the parallel package
that is part of the base R distribution: <code>psock</code> (default), <code>fork</code>, <code>mpi</code>,
<code>nws</code>, <code>sock</code>. In addition, <code>none</code> is available, which also disables
parallel processing.</p>
</dd>
<dt><code>backend_type</code></dt><dd><p>(<em>optional</em>) Selection of the backend for distributing
copies of the data. This backend ensures that only a single master copy is
kept in memory. This limits memory usage during parallel processing.
</p>
<p>Several backend options are available, notably <code>socket_server</code>, and <code>none</code>
(default). <code>socket_server</code> is based on the callr package and R sockets,
comes with <code>familiar</code> and is available for any OS. <code>none</code> uses the package
environment of familiar to store data, and is available for any OS.
However, <code>none</code> requires copying of data to any parallel process, and has a
larger memory footprint.</p>
</dd>
<dt><code>server_port</code></dt><dd><p>(<em>optional</em>) Integer indicating the port on which the
socket server or RServe process should communicate. Defaults to port 6311.
Note that ports 0 to 1024 and 49152 to 65535 cannot be used.</p>
</dd>
<dt><code>feature_max_fraction_missing</code></dt><dd><p>(<em>optional</em>) Numeric value between <code>0.0</code>
and <code>0.95</code> that determines the meximum fraction of missing values that
still allows a feature to be included in the data set. All features with a
missing value fraction over this threshold are not processed further. The
default value is <code>0.30</code>.</p>
</dd>
<dt><code>sample_max_fraction_missing</code></dt><dd><p>(<em>optional</em>) Numeric value between <code>0.0</code>
and <code>0.95</code> that determines the maximum fraction of missing values that
still allows a sample to be included in the data set. All samples with a
missing value fraction over this threshold are excluded and not processed
further. The default value is <code>0.30</code>.</p>
</dd>
<dt><code>filter_method</code></dt><dd><p>(<em>optional</em>) One or methods used to reduce
dimensionality of the data set by removing irrelevant or poorly
reproducible features.
</p>
<p>Several method are available:
</p>

<ul>
<li> <p><code>none</code> (default): None of the features will be filtered.
</p>
</li>
<li> <p><code>low_variance</code>: Features with a variance below the
<code>low_var_minimum_variance_threshold</code> are filtered. This can be useful to
filter, for example, genes that are not differentially expressed.
</p>
</li>
<li> <p><code>univariate_test</code>: Features undergo a univariate regression using an
outcome-appropriate regression model. The p-value of the model coefficient
is collected. Features with coefficient p or q-value above the
<code>univariate_test_threshold</code> are subsequently filtered.
</p>
</li>
<li> <p><code>robustness</code>: Features that are not sufficiently robust according to the
intraclass correlation coefficient are filtered. Use of this method
requires that repeated measurements are present in the data set, i.e. there
should be entries for which the sample and cohort identifiers are the same.
</p>
</li></ul>

<p>More than one method can be used simultaneously. Features with singular
values are always filtered, as these do not contain information.</p>
</dd>
<dt><code>univariate_test_threshold</code></dt><dd><p>(<em>optional</em>) Numeric value between <code>1.0</code> and
<code>0.0</code> that determines which features are irrelevant and will be filtered by
the <code>univariate_test</code>. The p or q-values are compared to this threshold.
All features with values above the threshold are filtered. The default
value is <code>0.20</code>.</p>
</dd>
<dt><code>univariate_test_threshold_metric</code></dt><dd><p>(<em>optional</em>) Metric used with the to
compare the <code>univariate_test_threshold</code> against. The following metrics can
be chosen:
</p>

<ul>
<li> <p><code>p_value</code> (default): The unadjusted p-value of each feature is used for
to filter features.
</p>
</li>
<li> <p><code>q_value</code>: The q-value (Story, 2002), is used to filter features. Some
data sets may have insufficient samples to compute the q-value. The
<code>qvalue</code> package must be installed from Bioconductor to use this method.
</p>
</li></ul>
</dd>
<dt><code>univariate_test_max_feature_set_size</code></dt><dd><p>(<em>optional</em>) Maximum size of the
feature set after the univariate test. P or q values of features are
compared against the threshold, but if the resulting data set would be
larger than this setting, only the most relevant features up to the desired
feature set size are selected.
</p>
<p>The default value is <code>NULL</code>, which causes features to be filtered based on
their relevance only.</p>
</dd>
<dt><code>low_var_minimum_variance_threshold</code></dt><dd><p>(required, if used) Numeric value
that determines which features will be filtered by the <code>low_variance</code>
method. The variance of each feature is computed and compared to the
threshold. If it is below the threshold, the feature is removed.
</p>
<p>This parameter has no default value and should be set if <code>low_variance</code> is
used.</p>
</dd>
<dt><code>low_var_max_feature_set_size</code></dt><dd><p>(<em>optional</em>) Maximum size of the feature
set after filtering features with a low variance. All features are first
compared against <code>low_var_minimum_variance_threshold</code>. If the resulting
feature set would be larger than specified, only the most strongly varying
features will be selected, up to the desired size of the feature set.
</p>
<p>The default value is <code>NULL</code>, which causes features to be filtered based on
their variance only.</p>
</dd>
<dt><code>robustness_icc_type</code></dt><dd><p>(<em>optional</em>) String indicating the type of
intraclass correlation coefficient (<code>1</code>, <code>2</code> or <code>3</code>) that should be used to
compute robustness for features in repeated measurements. These types
correspond to the types in Shrout and Fleiss (1979). The default value is
<code>1</code>.</p>
</dd>
<dt><code>robustness_threshold_metric</code></dt><dd><p>(<em>optional</em>) String indicating which
specific intraclass correlation coefficient (ICC) metric should be used to
filter features. This should be one of:
</p>

<ul>
<li> <p><code>icc</code>: The estimated ICC value itself.
</p>
</li>
<li> <p><code>icc_low</code> (default): The estimated lower limit of the 95% confidence
interval of the ICC, as suggested by Koo and Li (2016).
</p>
</li>
<li> <p><code>icc_panel</code>: The estimated ICC value over the panel average, i.e. the ICC
that would be obtained if all repeated measurements were averaged.
</p>
</li>
<li> <p><code>icc_panel_low</code>: The estimated lower limit of the 95% confidence interval
of the panel ICC.
</p>
</li></ul>
</dd>
<dt><code>robustness_threshold_value</code></dt><dd><p>(<em>optional</em>) The intraclass correlation
coefficient value that is as threshold. The default value is <code>0.70</code>.</p>
</dd>
<dt><code>transformation_method</code></dt><dd><p>(<em>optional</em>) The transformation method used to
change the distribution of the data to be more normal-like. The following
methods are available:
</p>

<ul>
<li> <p><code>none</code>: This disables transformation of features.
</p>
</li>
<li> <p><code>yeo_johnson</code> (default): Transformation using the Yeo-Johnson
transformation (Yeo and Johnson, 2000). The algorithm tests various lambda
values and selects the lambda that maximises the log-likelihood.
</p>
</li>
<li> <p><code>yeo_johnson_trim</code>: As <code>yeo_johnson</code>, but based on the set of feature
values where the 5% lowest and 5% highest values are discarded. This
reduces the effect of outliers.
</p>
</li>
<li> <p><code>yeo_johnson_winsor</code>: As <code>yeo_johnson</code>, but based on the set of feature
values where the 5% lowest and 5% highest values are winsorised. This
reduces the effect of outliers.
</p>
</li>
<li> <p><code>yeo_johnson_robust</code>: A robust version of <code>yeo_johnson</code> after Raymaekers
and Rousseeuw (2021). This method is less sensitive to outliers.
</p>
</li>
<li> <p><code>box_cox</code>: Transformation using the Box-Cox transformation (Box and Cox,
1964). Unlike the Yeo-Johnson transformation, the Box-Cox transformation
requires that all data are positive. Features that contain zero or negative
values cannot be transformed using this transformation. The algorithm tests
various lambda values and selects the lambda that maximises the
log-likelihood.
</p>
</li>
<li> <p><code>box_cox_trim</code>: As <code>box_cox</code>, but based on the set of feature values
where the 5% lowest and 5% highest values are discarded. This reduces the
effect of outliers.
</p>
</li>
<li> <p><code>box_cox_winsor</code>: As <code>box_cox</code>, but based on the set of feature values
where the 5% lowest and 5% highest values are winsorised. This reduces the
effect of outliers.
</p>
</li>
<li> <p><code>box_cox_robust</code>: A robust verson of <code>box_cox</code> after Raymaekers and
Rousseew (2021). This method is less sensitive to outliers.
</p>
</li></ul>

<p>Only features that contain numerical data are transformed. Transformation
parameters obtained in development data are stored within <code>featureInfo</code>
objects for later use with validation data sets.</p>
</dd>
<dt><code>normalisation_method</code></dt><dd><p>(<em>optional</em>) The normalisation method used to
improve the comparability between numerical features that may have very
different scales. The following normalisation methods can be chosen:
</p>

<ul>
<li> <p><code>none</code>: This disables feature normalisation.
</p>
</li>
<li> <p><code>standardisation</code>: Features are normalised by subtraction of their mean
values and division by their standard deviations. This causes every feature
to be have a center value of 0.0 and standard deviation of 1.0.
</p>
</li>
<li> <p><code>standardisation_trim</code>: As <code>standardisation</code>, but based on the set of
feature values where the 5% lowest and 5% highest values are discarded.
This reduces the effect of outliers.
</p>
</li>
<li> <p><code>standardisation_winsor</code>: As <code>standardisation</code>, but based on the set of
feature values where the 5% lowest and 5% highest values are winsorised.
This reduces the effect of outliers.
</p>
</li>
<li> <p><code>standardisation_robust</code> (default): A robust version of <code>standardisation</code>
that relies on computing Huber's M-estimators for location and scale.
</p>
</li>
<li> <p><code>normalisation</code>: Features are normalised by subtraction of their minimum
values and division by their ranges. This maps all feature values to a
<code class="reqn">[0, 1]</code> interval.
</p>
</li>
<li> <p><code>normalisation_trim</code>: As <code>normalisation</code>, but based on the set of feature
values where the 5% lowest and 5% highest values are discarded. This
reduces the effect of outliers.
</p>
</li>
<li> <p><code>normalisation_winsor</code>: As <code>normalisation</code>, but based on the set of
feature values where the 5% lowest and 5% highest values are winsorised.
This reduces the effect of outliers.
</p>
</li>
<li> <p><code>quantile</code>: Features are normalised by subtraction of their median values
and division by their interquartile range.
</p>
</li>
<li> <p><code>mean_centering</code>: Features are centered by substracting the mean, but do
not undergo rescaling.
</p>
</li></ul>

<p>Only features that contain numerical data are normalised. Normalisation
parameters obtained in development data are stored within <code>featureInfo</code>
objects for later use with validation data sets.</p>
</dd>
<dt><code>batch_normalisation_method</code></dt><dd><p>(<em>optional</em>) The method used for batch
normalisation. Available methods are:
</p>

<ul>
<li> <p><code>none</code> (default): This disables batch normalisation of features.
</p>
</li>
<li> <p><code>standardisation</code>: Features within each batch are normalised by
subtraction of the mean value and division by the standard deviation in
each batch.
</p>
</li>
<li> <p><code>standardisation_trim</code>: As <code>standardisation</code>, but based on the set of
feature values where the 5% lowest and 5% highest values are discarded.
This reduces the effect of outliers.
</p>
</li>
<li> <p><code>standardisation_winsor</code>: As <code>standardisation</code>, but based on the set of
feature values where the 5% lowest and 5% highest values are winsorised.
This reduces the effect of outliers.
</p>
</li>
<li> <p><code>standardisation_robust</code>: A robust version of <code>standardisation</code> that
relies on computing Huber's M-estimators for location and scale within each
batch.
</p>
</li>
<li> <p><code>normalisation</code>: Features within each batch are normalised by subtraction
of their minimum values and division by their range in each batch. This
maps all feature values in each batch to a <code class="reqn">[0, 1]</code> interval.
</p>
</li>
<li> <p><code>normalisation_trim</code>: As <code>normalisation</code>, but based on the set of feature
values where the 5% lowest and 5% highest values are discarded. This
reduces the effect of outliers.
</p>
</li>
<li> <p><code>normalisation_winsor</code>: As <code>normalisation</code>, but based on the set of
feature values where the 5% lowest and 5% highest values are winsorised.
This reduces the effect of outliers.
</p>
</li>
<li> <p><code>quantile</code>: Features in each batch are normalised by subtraction of the
median value and division by the interquartile range of each batch.
</p>
</li>
<li> <p><code>mean_centering</code>: Features in each batch are centered on 0.0 by
substracting the mean value in each batch, but are not rescaled.
</p>
</li>
<li> <p><code>combat_parametric</code>: Batch adjustments using parametric empirical Bayes
(Johnson et al, 2007). <code>combat_p</code> leads to the same method.
</p>
</li>
<li> <p><code>combat_non_parametric</code>: Batch adjustments using non-parametric empirical
Bayes (Johnson et al, 2007). <code>combat_np</code> and <code>combat</code> lead to the same
method. Note that we reduced complexity from O(<code class="reqn">n^2</code>) to O(<code class="reqn">n</code>) by
only computing batch adjustment parameters for each feature on a subset of
50 randomly selected features, instead of all features.
</p>
</li></ul>

<p>Only features that contain numerical data are normalised using batch
normalisation. Batch normalisation parameters obtained in development data
are stored within <code>featureInfo</code> objects for later use with validation data
sets, in case the validation data is from the same batch.
</p>
<p>If validation data contains data from unknown batches, normalisation
parameters are separately determined for these batches.
</p>
<p>Note that for both empirical Bayes methods, the batch effect is assumed to
produce results across the features. This is often true for things such as
gene expressions, but the assumption may not hold generally.
</p>
<p>When performing batch normalisation, it is moreover important to check that
differences between batches or cohorts are not related to the studied
endpoint.</p>
</dd>
<dt><code>imputation_method</code></dt><dd><p>(<em>optional</em>) Method used for imputing missing
feature values. Two methods are implemented:
</p>

<ul>
<li> <p><code>simple</code>: Simple replacement of a missing value by the median value (for
numeric features) or the modal value (for categorical features).
</p>
</li>
<li> <p><code>lasso</code>: Imputation of missing value by lasso regression (using <code>glmnet</code>)
based on information contained in other features.
</p>
</li></ul>

<p><code>simple</code> imputation precedes <code>lasso</code> imputation to ensure that any missing
values in predictors required for <code>lasso</code> regression are resolved. The
<code>lasso</code> estimate is then used to replace the missing value.
</p>
<p>The default value depends on the number of features in the dataset. If the
number is lower than 100, <code>lasso</code> is used by default, and <code>simple</code>
otherwise.
</p>
<p>Only single imputation is performed. Imputation models and parameters are
stored within <code>featureInfo</code> objects for later use with validation data
sets.</p>
</dd>
<dt><code>cluster_method</code></dt><dd><p>(<em>optional</em>) Clustering is performed to identify and
replace redundant features, for example those that are highly correlated.
Such features do not carry much additional information and may be removed
or replaced instead (Park et al., 2007; Tolosi and Lengauer, 2011).
</p>
<p>The cluster method determines the algorithm used to form the clusters. The
following cluster methods are implemented:
</p>

<ul>
<li> <p><code>none</code>: No clustering is performed.
</p>
</li>
<li> <p><code>hclust</code> (default): Hierarchical agglomerative clustering. If the
<code>fastcluster</code> package is installed, <code>fastcluster::hclust</code> is used (Muellner
2013), otherwise <code>stats::hclust</code> is used.
</p>
</li>
<li> <p><code>agnes</code>: Hierarchical clustering using agglomerative nesting (Kaufman and
Rousseeuw, 1990). This algorithm is similar to <code>hclust</code>, but uses the
<code>cluster::agnes</code> implementation.
</p>
</li>
<li> <p><code>diana</code>: Divisive analysis hierarchical clustering. This method uses
divisive instead of agglomerative clustering (Kaufman and Rousseeuw, 1990).
<code>cluster::diana</code> is used.
</p>
</li>
<li> <p><code>pam</code>: Partioning around medioids. This partitions the data into $k$
clusters around medioids (Kaufman and Rousseeuw, 1990). $k$ is selected
using the <code>silhouette</code> metric. <code>pam</code> is implemented using the
<code>cluster::pam</code> function.
</p>
</li></ul>

<p>Clusters and cluster information is stored within <code>featureInfo</code> objects for
later use with validation data sets. This enables reproduction of the same
clusters as formed in the development data set.</p>
</dd>
<dt><code>cluster_linkage_method</code></dt><dd><p>(<em>optional</em>) Linkage method used for
agglomerative clustering in <code>hclust</code> and <code>agnes</code>. The following linkage
methods can be used:
</p>

<ul>
<li> <p><code>average</code> (default): Average linkage.
</p>
</li>
<li> <p><code>single</code>: Single linkage.
</p>
</li>
<li> <p><code>complete</code>: Complete linkage.
</p>
</li>
<li> <p><code>weighted</code>: Weighted linkage, also known as McQuitty linkage.
</p>
</li>
<li> <p><code>ward</code>: Linkage using Ward's minimum variance method.
</p>
</li></ul>

<p><code>diana</code> and <code>pam</code> do not require a linkage method.</p>
</dd>
<dt><code>cluster_cut_method</code></dt><dd><p>(<em>optional</em>) The method used to define the actual
clusters. The following methods can be used:
</p>

<ul>
<li> <p><code>silhouette</code>: Clusters are formed based on the silhouette score
(Rousseeuw, 1987). The average silhouette score is computed from 2 to
<code class="reqn">n</code> clusters, with <code class="reqn">n</code> the number of features. Clusters are only
formed if the average silhouette exceeds 0.50, which indicates reasonable
evidence for structure. This procedure may be slow if the number of
features is large (&gt;100s).
</p>
</li>
<li> <p><code>fixed_cut</code>: Clusters are formed by cutting the hierarchical tree at the
point indicated by the <code>cluster_similarity_threshold</code>, e.g. where features
in a cluster have an average Spearman correlation of 0.90. <code>fixed_cut</code> is
only available for <code>agnes</code>, <code>diana</code> and <code>hclust</code>.
</p>
</li>
<li> <p><code>dynamic_cut</code>: Dynamic cluster formation using the cutting algorithm in
the <code>dynamicTreeCut</code> package. This package should be installed to select
this option. <code>dynamic_cut</code> can only be used with <code>agnes</code> and <code>hclust</code>.
</p>
</li></ul>

<p>The default options are <code>silhouette</code> for partioning around medioids (<code>pam</code>)
and <code>fixed_cut</code> otherwise.</p>
</dd>
<dt><code>cluster_similarity_metric</code></dt><dd><p>(<em>optional</em>) Clusters are formed based on
feature similarity. All features are compared in a pair-wise fashion to
compute similarity, for example correlation. The resulting similarity grid
is converted into a distance matrix that is subsequently used for
clustering. The following metrics are supported to compute pairwise
similarities:
</p>

<ul>
<li> <p><code>mutual_information</code> (default): normalised mutual information.
</p>
</li>
<li> <p><code>mcfadden_r2</code>: McFadden's pseudo R-squared (McFadden, 1974).
</p>
</li>
<li> <p><code>cox_snell_r2</code>: Cox and Snell's pseudo R-squared (Cox and Snell, 1989).
</p>
</li>
<li> <p><code>nagelkerke_r2</code>: Nagelkerke's pseudo R-squared (Nagelkerke, 1991).
</p>
</li>
<li> <p><code>spearman</code>: Spearman's rank order correlation.
</p>
</li>
<li> <p><code>kendall</code>: Kendall rank correlation.
</p>
</li>
<li> <p><code>pearson</code>: Pearson product-moment correlation.
</p>
</li></ul>

<p>The pseudo R-squared metrics can be used to assess similarity between mixed
pairs of numeric and categorical features, as these are based on the
log-likelihood of regression models. In <code>familiar</code>, the more informative
feature is used as the predictor and the other feature as the reponse
variable. In numeric-categorical pairs, the numeric feature is considered
to be more informative and is thus used as the predictor. In
categorical-categorical pairs, the feature with most levels is used as the
predictor.
</p>
<p>In case any of the classical correlation coefficients (<code>pearson</code>,
<code>spearman</code> and <code>kendall</code>) are used with (mixed) categorical features, the
categorical features are one-hot encoded and the mean correlation over all
resulting pairs is used as similarity.</p>
</dd>
<dt><code>cluster_similarity_threshold</code></dt><dd><p>(<em>optional</em>) The threshold level for
pair-wise similarity that is required to form clusters using <code>fixed_cut</code>.
This should be a numerical value between 0.0 and 1.0. Note however, that a
reasonable threshold value depends strongly on the similarity metric. The
following are the default values used:
</p>

<ul>
<li> <p><code>mcfadden_r2</code> and <code>mutual_information</code>: <code>0.30</code>
</p>
</li>
<li> <p><code>cox_snell_r2</code> and <code>nagelkerke_r2</code>: <code>0.75</code>
</p>
</li>
<li> <p><code>spearman</code>, <code>kendall</code> and <code>pearson</code>: <code>0.90</code>
</p>
</li></ul>

<p>Alternatively, if the <code style="white-space: pre;">&#8288;fixed cut&#8288;</code> method is not used, this value determines
whether any clustering should be performed, because the data may not
contain highly similar features. The default values in this situation are:
</p>

<ul>
<li> <p><code>mcfadden_r2</code>  and <code>mutual_information</code>: <code>0.25</code>
</p>
</li>
<li> <p><code>cox_snell_r2</code> and <code>nagelkerke_r2</code>: <code>0.40</code>
</p>
</li>
<li> <p><code>spearman</code>, <code>kendall</code> and <code>pearson</code>: <code>0.70</code>
</p>
</li></ul>

<p>The threshold value is converted to a distance (1-similarity) prior to
cutting hierarchical trees.</p>
</dd>
<dt><code>cluster_representation_method</code></dt><dd><p>(<em>optional</em>) Method used to determine
how the information of co-clustered features is summarised and used to
represent the cluster. The following methods can be selected:
</p>

<ul>
<li> <p><code>best_predictor</code> (default): The feature with the highest importance
according to univariate regression with the outcome is used to represent
the cluster.
</p>
</li>
<li> <p><code>medioid</code>: The feature closest to the cluster center, i.e. the feature
that is most similar to the remaining features in the cluster, is used to
represent the feature.
</p>
</li>
<li> <p><code>mean</code>: A meta-feature is generated by averaging the feature values for
all features in a cluster. This method aligns all features so that all
features will be positively correlated prior to averaging. Should a cluster
contain one or more categorical features, the <code>medioid</code> method will be used
instead, as averaging is not possible. Note that if this method is chosen,
the <code>normalisation_method</code> parameter should be one of <code>standardisation</code>,
<code>standardisation_trim</code>, <code>standardisation_winsor</code> or <code>quantile</code>.'
</p>
</li></ul>

<p>If the <code>pam</code> cluster method is selected, only the <code>medioid</code> method can be
used. In that case 1 medioid is used by default.</p>
</dd>
<dt><code>parallel_preprocessing</code></dt><dd><p>(<em>optional</em>) Enable parallel processing for the
preprocessing workflow. Defaults to <code>TRUE</code>. When set to <code>FALSE</code>, this will
disable the use of parallel processing while preprocessing, regardless of
the settings of the <code>parallel</code> parameter. <code>parallel_preprocessing</code> is
ignored if <code>parallel=FALSE</code>.</p>
</dd>
<dt><code>fs_method</code></dt><dd><p>(<strong>required</strong>) Feature selection method to be used for
determining variable importance. <code>familiar</code> implements various feature
selection methods. Please refer to the vignette on feature selection
methods for more details.
</p>
<p>More than one feature selection method can be chosen. The experiment will
then repeated for each feature selection method.
</p>
<p>Feature selection methods determines the ranking of features. Actual
selection of features is done by optimising the signature size model
hyperparameter during the hyperparameter optimisation step.</p>
</dd>
<dt><code>fs_method_parameter</code></dt><dd><p>(<em>optional</em>) List of lists containing parameters
for feature selection methods. Each sublist should have the name of the
feature selection method it corresponds to.
</p>
<p>Most feature selection methods do not have parameters that can be set.
Please refer to the vignette on feature selection methods for more details.
Note that if the feature selection method is based on a learner (e.g. lasso
regression), hyperparameter optimisation may be performed prior to
assessing variable importance.</p>
</dd>
<dt><code>vimp_aggregation_method</code></dt><dd><p>(<em>optional</em>) The method used to aggregate
variable importances over different data subsets, e.g. bootstraps. The
following methods can be selected:
</p>

<ul>
<li> <p><code>none</code>: Don't aggregate ranks, but rather aggregate the variable
importance scores themselves.
</p>
</li>
<li> <p><code>mean</code>: Use the mean rank of a feature over the subsets to
determine the aggregated feature rank.
</p>
</li>
<li> <p><code>median</code>: Use the median rank of a feature over the subsets to determine
the aggregated feature rank.
</p>
</li>
<li> <p><code>best</code>: Use the best rank the feature obtained in any subset to determine
the aggregated feature rank.
</p>
</li>
<li> <p><code>worst</code>: Use the worst rank the feature obtained in any subset to
determine the aggregated feature rank.
</p>
</li>
<li> <p><code>stability</code>: Use the frequency of the feature being in the subset of
highly ranked features as measure for the aggregated feature rank
(Meinshausen and Buehlmann, 2010).
</p>
</li>
<li> <p><code>exponential</code>: Use a rank-weighted frequence of occurrence in the subset
of highly ranked features as measure for the aggregated feature rank (Haury
et al., 2011).
</p>
</li>
<li> <p><code>borda</code> (default): Use the borda count as measure for the aggregated
feature rank (Wald et al., 2012).
</p>
</li>
<li> <p><code>enhanced_borda</code>: Use an occurrence frequency-weighted borda count as
measure for the aggregated feature rank (Wald et al., 2012).
</p>
</li>
<li> <p><code>truncated_borda</code>: Use borda count computed only on features within the
subset of highly ranked features.
</p>
</li>
<li> <p><code>enhanced_truncated_borda</code>: Apply both the enhanced borda method and the
truncated borda method and use the resulting borda count as the aggregated
feature rank.
</p>
</li></ul>

<p>The <em>feature selection methods</em> vignette provides additional information.</p>
</dd>
<dt><code>vimp_aggregation_rank_threshold</code></dt><dd><p>(<em>optional</em>) The threshold used to
define the subset of highly important features. If not set, this threshold
is determined by maximising the variance in the occurrence value over all
features over the subset size.
</p>
<p>This parameter is only relevant for <code>stability</code>, <code>exponential</code>,
<code>enhanced_borda</code>, <code>truncated_borda</code> and <code>enhanced_truncated_borda</code> methods.</p>
</dd>
<dt><code>parallel_feature_selection</code></dt><dd><p>(<em>optional</em>) Enable parallel processing for
the feature selection workflow. Defaults to <code>TRUE</code>. When set to <code>FALSE</code>,
this will disable the use of parallel processing while performing feature
selection, regardless of the settings of the <code>parallel</code> parameter.
<code>parallel_feature_selection</code> is ignored if <code>parallel=FALSE</code>.</p>
</dd>
<dt><code>learner</code></dt><dd><p>(<strong>required</strong>) One or more algorithms used for model
development. A sizeable number learners is supported in <code>familiar</code>. Please
see the vignette on learners for more information concerning the available
learners.</p>
</dd>
<dt><code>hyperparameter</code></dt><dd><p>(<em>optional</em>) List of lists containing hyperparameters
for learners. Each sublist should have the name of the learner method it
corresponds to, with list elements being named after the intended
hyperparameter, e.g. <code>"glm_logistic"=list("sign_size"=3)</code>
</p>
<p>All learners have hyperparameters. Please refer to the vignette on learners
for more details. If no parameters are provided, sequential model-based
optimisation is used to determine optimal hyperparameters.
</p>
<p>Hyperparameters provided by the user are never optimised. However, if more
than one value is provided for a single hyperparameter, optimisation will
be conducted using these values.</p>
</dd>
<dt><code>novelty_detector</code></dt><dd><p>(<em>optional</em>) Specify the algorithm used for training
a novelty detector. This detector can be used to identify
out-of-distribution data prospectively.</p>
</dd>
<dt><code>detector_parameters</code></dt><dd><p>(<em>optional</em>) List lists containing hyperparameters
for novelty detectors. Currently not used.</p>
</dd>
<dt><code>parallel_model_development</code></dt><dd><p>(<em>optional</em>) Enable parallel processing for
the model development workflow. Defaults to <code>TRUE</code>. When set to <code>FALSE</code>,
this will disable the use of parallel processing while developing models,
regardless of the settings of the <code>parallel</code> parameter.
<code>parallel_model_development</code> is ignored if <code>parallel=FALSE</code>.</p>
</dd>
<dt><code>optimisation_bootstraps</code></dt><dd><p>(<em>optional</em>) Number of bootstraps that should
be generated from the development data set. During the optimisation
procedure one or more of these bootstraps (indicated by
<code>smbo_step_bootstraps</code>) are used for model development using different
combinations of hyperparameters. The effect of the hyperparameters is then
assessed by comparing in-bag and out-of-bag model performance.
</p>
<p>The default number of bootstraps is <code>50</code>. Hyperparameter optimisation may
finish before exhausting the set of bootstraps.</p>
</dd>
<dt><code>optimisation_determine_vimp</code></dt><dd><p>(<em>optional</em>) Logical value that indicates
whether variable importance is determined separately for each of the
bootstraps created during the optimisation process (<code>TRUE</code>) or the
applicable results from the feature selection step are used (<code>FALSE</code>).
</p>
<p>Determining variable importance increases the initial computational
overhead. However, it prevents positive biases for the out-of-bag data due
to overlap of these data with the development data set used for the feature
selection step. In this case, any hyperparameters of the variable
importance method are not determined separately for each bootstrap, but
those obtained during the feature selection step are used instead. In case
multiple of such hyperparameter sets could be applicable, the set that will
be used is randomly selected for each bootstrap.
</p>
<p>This parameter only affects hyperparameter optimisation of learners. The
default is <code>TRUE</code>.</p>
</dd>
<dt><code>smbo_random_initialisation</code></dt><dd><p>(<em>optional</em>) String indicating the
initialisation method for the hyperparameter space. Can be one of
<code>fixed_subsample</code> (default), <code>fixed</code>, or <code>random</code>. <code>fixed</code> and
<code>fixed_subsample</code> first create hyperparameter sets from a range of default
values set by familiar. <code>fixed_subsample</code> then randomly draws up to
<code>smbo_n_random_sets</code> from the grid. <code>random</code> does not rely upon a fixed
grid, and randomly draws up to <code>smbo_n_random_sets</code> hyperparameter sets
from the hyperparameter space.</p>
</dd>
<dt><code>smbo_n_random_sets</code></dt><dd><p>(<em>optional</em>) Number of random or subsampled
hyperparameters drawn during the initialisation process. Default: <code>100</code>.
Cannot be smaller than <code>10</code>. The parameter is not used when
<code>smbo_random_initialisation</code> is <code>fixed</code>, as the entire pre-defined grid
will be explored.</p>
</dd>
<dt><code>max_smbo_iterations</code></dt><dd><p>(<em>optional</em>) Maximum number of intensify
iterations of the SMBO algorithm. During an intensify iteration a run-off
occurs between the current <em>best</em> hyperparameter combination and either 10
challenger combination with the highest expected improvement or a set of 20
random combinations.
</p>
<p>Run-off with random combinations is used to force exploration of the
hyperparameter space, and is performed every second intensify iteration, or
if there is no expected improvement for any challenger combination.
</p>
<p>If a combination of hyperparameters leads to better performance on the same
data than the incumbent <em>best</em> set of hyperparameters, it replaces the
incumbent set at the end of the intensify iteration.
</p>
<p>The default number of intensify iteration is <code>20</code>. Iterations may be
stopped early if the incumbent set of hyperparameters remains the same for
<code>smbo_stop_convergent_iterations</code> iterations, or performance improvement is
minimal. This behaviour is suppressed during the first 4 iterations to
enable the algorithm to explore the hyperparameter space.</p>
</dd>
<dt><code>smbo_stop_convergent_iterations</code></dt><dd><p>(<em>optional</em>) The number of subsequent
convergent SMBO iterations required to stop hyperparameter optimisation
early. An iteration is convergent if the <em>best</em> parameter set has not
changed or the optimisation score over the 4 most recent iterations has not
changed beyond the tolerance level in <code>smbo_stop_tolerance</code>.
</p>
<p>The default value is <code>3</code>.</p>
</dd>
<dt><code>smbo_stop_tolerance</code></dt><dd><p>(<em>optional</em>) Tolerance for early stopping due to
convergent optimisation score.
</p>
<p>The default value depends on the square root of the number of samples (at
the series level), and is <code>0.01</code> for 100 samples. This value is computed as
<code>0.1 * 1 / sqrt(n_samples)</code>. The upper limit is <code>0.0001</code> for 1M or more
samples.</p>
</dd>
<dt><code>smbo_time_limit</code></dt><dd><p>(<em>optional</em>) Time limit (in minutes) for the
optimisation process. Optimisation is stopped after this limit is exceeded.
Time taken to determine variable importance for the optimisation process
(see the <code>optimisation_determine_vimp</code> parameter) does not count.
</p>
<p>The default is <code>NULL</code>, indicating that there is no time limit for the
optimisation process. The time limit cannot be less than 1 minute.</p>
</dd>
<dt><code>smbo_initial_bootstraps</code></dt><dd><p>(<em>optional</em>) The number of bootstraps taken
from the set of <code>optimisation_bootstraps</code> as the bootstraps assessed
initially.
</p>
<p>The default value is <code>1</code>. The value cannot be larger than
<code>optimisation_bootstraps</code>.</p>
</dd>
<dt><code>smbo_step_bootstraps</code></dt><dd><p>(<em>optional</em>) The number of bootstraps taken from
the set of <code>optimisation_bootstraps</code> bootstraps as the bootstraps assessed
during the steps of each intensify iteration.
</p>
<p>The default value is <code>3</code>. The value cannot be larger than
<code>optimisation_bootstraps</code>.</p>
</dd>
<dt><code>smbo_intensify_steps</code></dt><dd><p>(<em>optional</em>) The number of steps in each SMBO
intensify iteration. Each step a new set of <code>smbo_step_bootstraps</code>
bootstraps is drawn and used in the run-off between the incumbent <em>best</em>
hyperparameter combination and its challengers.
</p>
<p>The default value is <code>5</code>. Higher numbers allow for a more detailed
comparison, but this comes with added computational cost.</p>
</dd>
<dt><code>optimisation_metric</code></dt><dd><p>(<em>optional</em>) One or more metrics used to compute
performance scores. See the vignette on performance metrics for the
available metrics.
</p>
<p>If unset, the following metrics are used by default:
</p>

<ul>
<li> <p><code>auc_roc</code>: For <code>binomial</code> and <code>multinomial</code> models.
</p>
</li>
<li> <p><code>mse</code>: Mean squared error for <code>continuous</code> models.
</p>
</li>
<li> <p><code>msle</code>: Mean squared logarithmic error for <code>count</code> models.
</p>
</li>
<li> <p><code>concordance_index</code>: For <code>survival</code> models.
</p>
</li></ul>

<p>Multiple optimisation metrics can be specified. Actual metric values are
converted to an objective value by comparison with a baseline metric value
that derives from a trivial model, i.e. majority class for binomial and
multinomial outcomes, the median outcome for count and continuous outcomes
and a fixed risk or time for survival outcomes.</p>
</dd>
<dt><code>optimisation_function</code></dt><dd><p>(<em>optional</em>) Type of optimisation function used
to quantify the performance of a hyperparameter set. Model performance is
assessed using the metric(s) specified by <code>optimisation_metric</code> on the
in-bag (IB) and out-of-bag (OOB) samples of a bootstrap. These values are
converted to objective scores with a standardised interval of <code class="reqn">[-1.0,
  1.0]</code>. Each pair of objective is subsequently used to compute an
optimisation score. The optimisation score across different bootstraps is
than aggregated to a summary score. This summary score is used to rank
hyperparameter sets, and select the optimal set.
</p>
<p>The combination of optimisation score and summary score is determined by
the optimisation function indicated by this parameter:
</p>

<ul>
<li> <p><code>validation</code> or <code>max_validation</code> (default): seeks to maximise OOB score.
</p>
</li>
<li> <p><code>balanced</code>: seeks to balance IB and OOB score.
</p>
</li>
<li> <p><code>stronger_balance</code>: similar to <code>balanced</code>, but with stronger penalty for
differences between IB and OOB scores.
</p>
</li>
<li> <p><code>validation_minus_sd</code>: seeks to optimise the average OOB score minus its
standard deviation.
</p>
</li>
<li> <p><code>validation_25th_percentile</code>: seeks to optimise the 25th percentile of
OOB scores, and is conceptually similar to <code>validation_minus_sd</code>.
</p>
</li>
<li> <p><code>model_estimate</code>: seeks to maximise the OOB score estimate predicted by
the hyperparameter learner (not available for random search).
</p>
</li>
<li> <p><code>model_estimate_minus_sd</code>: seeks to maximise the OOB score estimate minus
its estimated standard deviation, as predicted by the hyperparameter
learner (not available for random search).
</p>
</li>
<li> <p><code>model_balanced_estimate</code>: seeks to maximise the estimate of the balanced
IB and OOB score. This is similar to the <code>balanced</code> score, and in fact uses
a hyperparameter learner to predict said score (not available for random
search).
</p>
</li>
<li> <p><code>model_balanced_estimate_minus_sd</code>: seeks to maximise the estimate of the
balanced IB and OOB score, minus its estimated standard deviation. This is
similar to the <code>balanced</code> score, but takes into account its estimated
spread.
</p>
</li></ul>

<p>Additional detail are provided in the <em>Learning algorithms and
hyperparameter optimisation</em> vignette.</p>
</dd>
<dt><code>hyperparameter_learner</code></dt><dd><p>(<em>optional</em>) Any point in the hyperparameter
space has a single, scalar, optimisation score value that is <em>a priori</em>
unknown. During the optimisation process, the algorithm samples from the
hyperparameter space by selecting hyperparameter sets and computing the
optimisation score value for one or more bootstraps. For each
hyperparameter set the resulting values are distributed around the actual
value. The learner indicated by <code>hyperparameter_learner</code> is then used to
infer optimisation score estimates for unsampled parts of the
hyperparameter space.
</p>
<p>The following models are available:
</p>

<ul>
<li> <p><code>bayesian_additive_regression_trees</code> or <code>bart</code>: Uses Bayesian Additive
Regression Trees (Sparapani et al., 2021) for inference. Unlike standard
random forests, BART allows for estimating posterior distributions directly
and can extrapolate.
</p>
</li>
<li> <p><code>gaussian_process</code> (default): Creates a localised approximate Gaussian
process for inference (Gramacy, 2016). This allows for better scaling than
deterministic Gaussian Processes.
</p>
</li>
<li> <p><code>random_forest</code>: Creates a random forest for inference. Originally
suggested by Hutter et al. (2011). A weakness of random forests is their
lack of extrapolation beyond observed values, which limits their usefulness
in exploiting promising areas of hyperparameter space.
</p>
</li>
<li> <p><code>random</code> or <code>random_search</code>: Forgoes the use of models to steer
optimisation. Instead, a random search is performed.
</p>
</li></ul>
</dd>
<dt><code>acquisition_function</code></dt><dd><p>(<em>optional</em>) The acquisition function influences
how new hyperparameter sets are selected. The algorithm uses the model
learned by the learner indicated by <code>hyperparameter_learner</code> to search the
hyperparameter space for hyperparameter sets that are either likely better
than the best known set (<em>exploitation</em>) or where there is considerable
uncertainty (<em>exploration</em>). The acquisition function quantifies this
(Shahriari et al., 2016).
</p>
<p>The following acquisition functions are available, and are described in
more detail in the <em>learner algorithms</em> vignette:
</p>

<ul>
<li> <p><code>improvement_probability</code>: The probability of improvement quantifies the
probability that the expected optimisation score for a set is better than
the best observed optimisation score
</p>
</li>
<li> <p><code>improvement_empirical_probability</code>: Similar to
<code>improvement_probability</code>, but based directly on optimisation scores
predicted by the individual decision trees.
</p>
</li>
<li> <p><code>expected_improvement</code> (default): Computes expected improvement.
</p>
</li>
<li> <p><code>upper_confidence_bound</code>: This acquisition function is based on the upper
confidence bound of the distribution (Srinivas et al., 2012).
</p>
</li>
<li> <p><code>bayes_upper_confidence_bound</code>: This acquisition function is based on the
upper confidence bound of the distribution (Kaufmann et al., 2012).
</p>
</li></ul>
</dd>
<dt><code>exploration_method</code></dt><dd><p>(<em>optional</em>) Method used to steer exploration in
post-initialisation intensive searching steps. As stated earlier, each SMBO
iteration step compares suggested alternative parameter sets with an
incumbent <strong>best</strong> set in a series of steps. The exploration method
controls how the set of alternative parameter sets is pruned after each
step in an iteration. Can be one of the following:
</p>

<ul>
<li> <p><code>single_shot</code> (default): The set of alternative parameter sets is not
pruned, and each intensification iteration contains only a single
intensification step that only uses a single bootstrap. This is the fastest
exploration method, but only superficially tests each parameter set.
</p>
</li>
<li> <p><code>successive_halving</code>: The set of alternative parameter sets is
pruned by removing the worst performing half of the sets after each step
(Jamieson and Talwalkar, 2016).
</p>
</li>
<li> <p><code>stochastic_reject</code>: The set of alternative parameter sets is pruned by
comparing the performance of each parameter set with that of the incumbent
<strong>best</strong> parameter set using a paired Wilcoxon test based on shared
bootstraps. Parameter sets that perform significantly worse, at an alpha
level indicated by <code>smbo_stochastic_reject_p_value</code>, are pruned.
</p>
</li>
<li> <p><code>none</code>: The set of alternative parameter sets is not pruned.
</p>
</li></ul>
</dd>
<dt><code>smbo_stochastic_reject_p_value</code></dt><dd><p>(<em>optional</em>) The p-value threshold used
for the <code>stochastic_reject</code> exploration method.
</p>
<p>The default value is <code>0.05</code>.</p>
</dd>
<dt><code>parallel_hyperparameter_optimisation</code></dt><dd><p>(<em>optional</em>) Enable parallel
processing for hyperparameter optimisation. Defaults to <code>TRUE</code>. When set to
<code>FALSE</code>, this will disable the use of parallel processing while performing
optimisation, regardless of the settings of the <code>parallel</code> parameter. The
parameter moreover specifies whether parallelisation takes place within the
optimisation algorithm (<code>inner</code>, default), or in an outer loop ( <code>outer</code>)
over learners, data subsamples, etc.
</p>
<p><code>parallel_hyperparameter_optimisation</code> is ignored if <code>parallel=FALSE</code>.</p>
</dd>
<dt><code>evaluate_top_level_only</code></dt><dd><p>(<em>optional</em>) Flag that signals that only
evaluation at the most global experiment level is required. Consider a
cross-validation experiment with additional external validation. The global
experiment level consists of data that are used for development, internal
validation and external validation. The next lower experiment level are the
individual cross-validation iterations.
</p>
<p>When the flag is <code>true</code>, evaluations take place on the global level only,
and no results are generated for the next lower experiment levels. In our
example, this means that results from individual cross-validation iterations
are not computed and shown. When the flag is <code>false</code>, results are computed
from both the global layer and the next lower level.
</p>
<p>Setting the flag to <code>true</code> saves computation time.</p>
</dd>
<dt><code>skip_evaluation_elements</code></dt><dd><p>(<em>optional</em>) Specifies which evaluation steps,
if any, should be skipped as part of the evaluation process. Defaults to
<code>none</code>, which means that all relevant evaluation steps are performed. It can
have one or more of the following values:
</p>

<ul>
<li> <p><code>none</code>, <code>false</code>: no steps are skipped.
</p>
</li>
<li> <p><code>all</code>, <code>true</code>: all steps are skipped.
</p>
</li>
<li> <p><code>auc_data</code>: data for assessing and plotting the area under the receiver
operating characteristic curve are not computed.
</p>
</li>
<li> <p><code>calibration_data</code>: data for assessing and plotting model calibration are
not computed.
</p>
</li>
<li> <p><code>calibration_info</code>: data required to assess calibration, such as baseline
survival curves, are not collected. These data will still be present in the
models.
</p>
</li>
<li> <p><code>confusion_matrix</code>: data for assessing and plotting a confusion matrix are
not collected.
</p>
</li>
<li> <p><code>decision_curve_analyis</code>: data for performing a decision curve analysis
are not computed.
</p>
</li>
<li> <p><code>feature_expressions</code>: data for assessing and plotting sample clustering
are not computed.
</p>
</li>
<li> <p><code>feature_similarity</code>: data for assessing and plotting feature clusters are
not computed.
</p>
</li>
<li> <p><code>fs_vimp</code>: data for assessing and plotting feature selection-based
variable importance are not collected.
</p>
</li>
<li> <p><code>hyperparameters</code>: data for assessing model hyperparameters are not
collected. These data will still be present in the models.
</p>
</li>
<li> <p><code>ice_data</code>: data for individual conditional expectation and partial
dependence plots are not created.
</p>
</li>
<li> <p><code>model_performance</code>: data for assessing and visualising model performance
are not created.
</p>
</li>
<li> <p><code>model_vimp</code>: data for assessing and plotting model-based variable
importance are not collected.
</p>
</li>
<li> <p><code>permutation_vimp</code>: data for assessing and plotting model-agnostic
permutation variable importance are not computed.
</p>
</li>
<li> <p><code>prediction_data</code>: predictions for each sample are not made and exported.
</p>
</li>
<li> <p><code>risk_stratification_data</code>: data for assessing and plotting Kaplan-Meier
survival curves are not collected.
</p>
</li>
<li> <p><code>risk_stratification_info</code>: data for assessing stratification into risk
groups are not computed.
</p>
</li>
<li> <p><code>univariate_analysis</code>: data for assessing and plotting univariate feature
importance are not computed.
</p>
</li></ul>
</dd>
<dt><code>ensemble_method</code></dt><dd><p>(<em>optional</em>) Method for ensembling predictions from
models for the same sample. Available methods are:
</p>

<ul>
<li> <p><code>median</code> (default): Use the median of the predicted values as the ensemble
value for a sample.
</p>
</li>
<li> <p><code>mean</code>: Use the mean of the predicted values as the ensemble value for a
sample.
</p>
</li></ul>

<p>This parameter is only used if <code>detail_level</code> is <code>ensemble</code>.</p>
</dd>
<dt><code>evaluation_metric</code></dt><dd><p>(<em>optional</em>) One or more metrics for assessing model
performance. See the vignette on performance metrics for the available
metrics.
</p>
<p>Confidence intervals (or rather credibility intervals) are computed for each
metric during evaluation. This is done using bootstraps, the number of which
depends on the value of <code>confidence_level</code> (Davison and Hinkley, 1997).
</p>
<p>If unset, the metric in the <code>optimisation_metric</code> variable is used.</p>
</dd>
<dt><code>sample_limit</code></dt><dd><p>(<em>optional</em>) Set the upper limit of the number of samples
that are used during evaluation steps. Cannot be less than 20.
</p>
<p>This setting can be specified per data element by providing a parameter
value in a named list with data elements, e.g.
<code>list("sample_similarity"=100, "permutation_vimp"=1000)</code>.
</p>
<p>This parameter can be set for the following data elements:
<code>sample_similarity</code> and <code>ice_data</code>.</p>
</dd>
<dt><code>detail_level</code></dt><dd><p>(<em>optional</em>) Sets the level at which results are computed
and aggregated.
</p>

<ul>
<li> <p><code>ensemble</code>: Results are computed at the ensemble level, i.e. over all
models in the ensemble. This means that, for example, bias-corrected
estimates of model performance are assessed by creating (at least) 20
bootstraps and computing the model performance of the ensemble model for
each bootstrap.
</p>
</li>
<li> <p><code>hybrid</code> (default): Results are computed at the level of models in an
ensemble. This means that, for example, bias-corrected estimates of model
performance are directly computed using the models in the ensemble. If there
are at least 20 trained models in the ensemble, performance is computed for
each model, in contrast to <code>ensemble</code> where performance is computed for the
ensemble of models. If there are less than 20 trained models in the
ensemble, bootstraps are created so that at least 20 point estimates can be
made.
</p>
</li>
<li> <p><code>model</code>: Results are computed at the model level. This means that, for
example, bias-corrected estimates of model performance are assessed by
creating (at least) 20 bootstraps and computing the performance of the model
for each bootstrap.
</p>
</li></ul>

<p>Note that each level of detail has a different interpretation for bootstrap
confidence intervals. For <code>ensemble</code> and <code>model</code> these are the confidence
intervals for the ensemble and an individual model, respectively. That is,
the confidence interval describes the range where an estimate produced by a
respective ensemble or model trained on a repeat of the experiment may be
found with the probability of the confidence level. For <code>hybrid</code>, it
represents the range where any single model trained on a repeat of the
experiment may be found with the probability of the confidence level. By
definition, confidence intervals obtained using <code>hybrid</code> are at least as
wide as those for <code>ensemble</code>. <code>hybrid</code> offers the correct interpretation if
the goal of the analysis is to assess the result of a single, unspecified,
model.
</p>
<p><code>hybrid</code> is generally computationally less expensive then <code>ensemble</code>, which
in turn is somewhat less expensive than <code>model</code>.
</p>
<p>A non-default <code>detail_level</code> parameter can be specified for separate
evaluation steps by providing a parameter value in a named list with data
elements, e.g. <code>list("auc_data"="ensemble", "model_performance"="hybrid")</code>.
This parameter can be set for the following data elements: <code>auc_data</code>,
<code>decision_curve_analyis</code>, <code>model_performance</code>, <code>permutation_vimp</code>,
<code>ice_data</code>, <code>prediction_data</code> and <code>confusion_matrix</code>.</p>
</dd>
<dt><code>estimation_type</code></dt><dd><p>(<em>optional</em>) Sets the type of estimation that should be
possible. This has the following options:
</p>

<ul>
<li> <p><code>point</code>: Point estimates.
</p>
</li>
<li> <p><code>bias_correction</code> or <code>bc</code>: Bias-corrected estimates. A bias-corrected
estimate is computed from (at least) 20 point estimates, and <code>familiar</code> may
bootstrap the data to create them.
</p>
</li>
<li> <p><code>bootstrap_confidence_interval</code> or <code>bci</code> (default): Bias-corrected
estimates with bootstrap confidence intervals (Efron and Hastie, 2016). The
number of point estimates required depends on the <code>confidence_level</code>
parameter, and <code>familiar</code> may bootstrap the data to create them.
</p>
</li></ul>

<p>As with <code>detail_level</code>, a non-default <code>estimation_type</code> parameter can be
specified for separate evaluation steps by providing a parameter value in a
named list with data elements, e.g. <code>list("auc_data"="bci", "model_performance"="point")</code>. This parameter can be set for the following
data elements: <code>auc_data</code>, <code>decision_curve_analyis</code>, <code>model_performance</code>,
<code>permutation_vimp</code>, <code>ice_data</code>, and <code>prediction_data</code>.</p>
</dd>
<dt><code>aggregate_results</code></dt><dd><p>(<em>optional</em>) Flag that signifies whether results
should be aggregated during evaluation. If <code>estimation_type</code> is
<code>bias_correction</code> or <code>bc</code>, aggregation leads to a single bias-corrected
estimate. If <code>estimation_type</code> is <code>bootstrap_confidence_interval</code> or <code>bci</code>,
aggregation leads to a single bias-corrected estimate with lower and upper
boundaries of the confidence interval. This has no effect if
<code>estimation_type</code> is <code>point</code>.
</p>
<p>The default value is equal to <code>TRUE</code> except when assessing metrics to assess
model performance, as the default violin plot requires underlying data.
</p>
<p>As with <code>detail_level</code> and <code>estimation_type</code>, a non-default
<code>aggregate_results</code> parameter can be specified for separate evaluation steps
by providing a parameter value in a named list with data elements, e.g.
<code>list("auc_data"=TRUE, , "model_performance"=FALSE)</code>. This parameter exists
for the same elements as <code>estimation_type</code>.</p>
</dd>
<dt><code>confidence_level</code></dt><dd><p>(<em>optional</em>) Numeric value for the level at which
confidence intervals are determined. In the case bootstraps are used to
determine the confidence intervals bootstrap estimation, <code>familiar</code> uses the
rule of thumb <code class="reqn">n = 20 / ci.level</code> to determine the number of required
bootstraps.
</p>
<p>The default value is <code>0.95</code>.</p>
</dd>
<dt><code>bootstrap_ci_method</code></dt><dd><p>(<em>optional</em>) Method used to determine bootstrap
confidence intervals (Efron and Hastie, 2016). The following methods are
implemented:
</p>

<ul>
<li> <p><code>percentile</code> (default): Confidence intervals obtained using the percentile
method.
</p>
</li>
<li> <p><code>bc</code>: Bias-corrected confidence intervals.
</p>
</li></ul>

<p>Note that the standard method is not implemented because this method is
often not suitable due to non-normal distributions. The bias-corrected and
accelerated (BCa) method is not implemented yet.</p>
</dd>
<dt><code>feature_cluster_method</code></dt><dd><p>(<em>optional</em>) Method used to perform clustering
of features. The same methods as for the <code>cluster_method</code> configuration
parameter are available: <code>none</code>, <code>hclust</code>, <code>agnes</code>, <code>diana</code> and <code>pam</code>.
</p>
<p>The value for the <code>cluster_method</code> configuration parameter is used by
default. When generating clusters for the purpose of determining mutual
correlation and ordering feature expressions, <code>none</code> is ignored and <code>hclust</code>
is used instead.</p>
</dd>
<dt><code>feature_linkage_method</code></dt><dd><p>(<em>optional</em>) Method used for agglomerative
clustering with <code>hclust</code> and <code>agnes</code>. Linkage determines how features are
sequentially combined into clusters based on distance. The methods are
shared with the <code>cluster_linkage_method</code> configuration parameter: <code>average</code>,
<code>single</code>, <code>complete</code>, <code>weighted</code>, and <code>ward</code>.
</p>
<p>The value for the <code>cluster_linkage_method</code> configuration parameters is used
by default.</p>
</dd>
<dt><code>feature_cluster_cut_method</code></dt><dd><p>(<em>optional</em>) Method used to divide features
into separate clusters. The available methods are the same as for the
<code>cluster_cut_method</code> configuration parameter: <code>silhouette</code>, <code>fixed_cut</code> and
<code>dynamic_cut</code>.
</p>
<p><code>silhouette</code> is available for all cluster methods, but <code>fixed_cut</code> only
applies to methods that create hierarchical trees (<code>hclust</code>, <code>agnes</code> and
<code>diana</code>). <code>dynamic_cut</code> requires the <code>dynamicTreeCut</code> package and can only
be used with <code>agnes</code> and <code>hclust</code>.
</p>
<p>The value for the <code>cluster_cut_method</code> configuration parameter is used by
default.</p>
</dd>
<dt><code>feature_similarity_metric</code></dt><dd><p>(<em>optional</em>) Metric to determine pairwise
similarity between features. Similarity is computed in the same manner as
for clustering, and <code>feature_similarity_metric</code> therefore has the same
options as <code>cluster_similarity_metric</code>: <code>mcfadden_r2</code>, <code>cox_snell_r2</code>,
<code>nagelkerke_r2</code>, <code>mutual_information</code>, <code>spearman</code>, <code>kendall</code> and <code>pearson</code>.
</p>
<p>The value used for the <code>cluster_similarity_metric</code> configuration parameter
is used by default.</p>
</dd>
<dt><code>feature_similarity_threshold</code></dt><dd><p>(<em>optional</em>) The threshold level for
pair-wise similarity that is required to form feature clusters with the
<code>fixed_cut</code> method. This threshold functions in the same manner as the one
defined using the <code>cluster_similarity_threshold</code> parameter.
</p>
<p>By default, the value for the <code>cluster_similarity_threshold</code> configuration
parameter is used.
</p>
<p>Unlike for <code>cluster_similarity_threshold</code>, more than one value can be
supplied here.</p>
</dd>
<dt><code>sample_cluster_method</code></dt><dd><p>(<em>optional</em>) The method used to perform
clustering based on distance between samples. These are the same methods as
for the <code>cluster_method</code> configuration parameter: <code>hclust</code>, <code>agnes</code>, <code>diana</code>
and <code>pam</code>.
</p>
<p>The value for the <code>cluster_method</code> configuration parameter is used by
default. When generating clusters for the purpose of ordering samples in
feature expressions, <code>none</code> is ignored and <code>hclust</code> is used instead.</p>
</dd>
<dt><code>sample_linkage_method</code></dt><dd><p>(<em>optional</em>) The method used for agglomerative
clustering in <code>hclust</code> and <code>agnes</code>. These are the same methods as for the
<code>cluster_linkage_method</code> configuration parameter: <code>average</code>, <code>single</code>,
<code>complete</code>, <code>weighted</code>, and <code>ward</code>.
</p>
<p>The value for the <code>cluster_linkage_method</code> configuration parameters is used
by default.</p>
</dd>
<dt><code>sample_similarity_metric</code></dt><dd><p>(<em>optional</em>) Metric to determine pairwise
similarity between samples. Similarity is computed in the same manner as for
clustering, but <code>sample_similarity_metric</code> has different options that are
better suited to computing distance between samples instead of between
features. The following metrics are available.
</p>

<ul>
<li> <p><code>gower</code> (default): compute Gower's distance between samples. By default,
Gower's distance is computed based on winsorised data to reduce the effect
of outliers (see below).
</p>
</li>
<li> <p><code>euclidean</code>: compute the Euclidean distance between samples.
</p>
</li></ul>

<p>The underlying feature data for numerical features is scaled to the
<code class="reqn">[0,1]</code> range using the feature values across the samples. The
normalisation parameters required can optionally be computed from feature
data with the outer 5% (on both sides) of feature values trimmed or
winsorised. To do so append <code style="white-space: pre;">&#8288;_trim&#8288;</code> (trimming) or <code style="white-space: pre;">&#8288;_winsor&#8288;</code> (winsorising) to
the metric name. This reduces the effect of outliers somewhat.
</p>
<p>Regardless of metric, all categorical features are handled as for the
Gower's distance: distance is 0 if the values in a pair of samples match,
and 1 if they do not.</p>
</dd>
<dt><code>eval_aggregation_method</code></dt><dd><p>(<em>optional</em>) Method for aggregating variable
importances for the purpose of evaluation. Variable importances are
determined during feature selection steps and after training the model. Both
types are evaluated, but feature selection variable importance is only
evaluated at run-time.
</p>
<p>See the documentation for the <code>vimp_aggregation_method</code> argument for
information concerning the different methods available.</p>
</dd>
<dt><code>eval_aggregation_rank_threshold</code></dt><dd><p>(<em>optional</em>) The threshold used to
define the subset of highly important features during evaluation.
</p>
<p>See the documentation for the <code>vimp_aggregation_rank_threshold</code> argument for
more information.</p>
</dd>
<dt><code>eval_icc_type</code></dt><dd><p>(<em>optional</em>) String indicating the type of intraclass
correlation coefficient (<code>1</code>, <code>2</code> or <code>3</code>) that should be used to compute
robustness for features in repeated measurements during the evaluation of
univariate importance. These types correspond to the types in Shrout and
Fleiss (1979). The default value is <code>1</code>.</p>
</dd>
<dt><code>stratification_method</code></dt><dd><p>(<em>optional</em>) Method for determining the
stratification threshold for creating survival groups. The actual,
model-dependent, threshold value is obtained from the development data, and
can afterwards be used to perform stratification on validation data.
</p>
<p>The following stratification methods are available:
</p>

<ul>
<li> <p><code>median</code> (default): The median predicted value in the development cohort
is used to stratify the samples into two risk groups. For predicted outcome
values that build a continuous spectrum, the two risk groups in the
development cohort will be roughly equal in size.
</p>
</li>
<li> <p><code>mean</code>: The mean predicted value in the development cohort is used to
stratify the samples into two risk groups.
</p>
</li>
<li> <p><code>mean_trim</code>: As <code>mean</code>, but based on the set of predicted values
where the 5% lowest and 5% highest values are discarded. This reduces the
effect of outliers.
</p>
</li>
<li> <p><code>mean_winsor</code>: As <code>mean</code>, but based on the set of predicted values where
the 5% lowest and 5% highest values are winsorised. This reduces the effect
of outliers.
</p>
</li>
<li> <p><code>fixed</code>: Samples are stratified based on the sample quantiles of the
predicted values. These quantiles are defined using the
<code>stratification_threshold</code> parameter.
</p>
</li>
<li> <p><code>optimised</code>: Use maximally selected rank statistics to determine the
optimal threshold (Lausen and Schumacher, 1992; Hothorn et al., 2003) to
stratify samples into two optimally separated risk groups.
</p>
</li></ul>

<p>One or more stratification methods can be selected simultaneously.
</p>
<p>This parameter is only relevant for <code>survival</code> outcomes.</p>
</dd>
<dt><code>stratification_threshold</code></dt><dd><p>(<em>optional</em>) Numeric value(s) signifying the
sample quantiles for stratification using the <code>fixed</code> method. The number of
risk groups will be the number of values +1.
</p>
<p>The default value is <code>c(1/3, 2/3)</code>, which will yield two thresholds that
divide samples into three equally sized groups. If <code>fixed</code> is not among the
selected stratification methods, this parameter is ignored.
</p>
<p>This parameter is only relevant for <code>survival</code> outcomes.</p>
</dd>
<dt><code>time_max</code></dt><dd><p>(<em>optional</em>) Time point which is used as the benchmark for
e.g. cumulative risks generated by random forest, or the cutoff for Uno's
concordance index.
</p>
<p>If <code>time_max</code> is not provided, but <code>evaluation_times</code> is, the largest value
of <code>evaluation_times</code> is used. If both are not provided, <code>time_max</code> is set
to the 98th percentile of the distribution of survival times for samples
with an event in the development data set.
</p>
<p>This parameter is only relevant for <code>survival</code> outcomes.</p>
</dd>
<dt><code>evaluation_times</code></dt><dd><p>(<em>optional</em>) One or more time points that are used for
assessing calibration in survival problems. This is done as expected and
observed survival probabilities depend on time.
</p>
<p>If unset, <code>evaluation_times</code> will be equal to <code>time_max</code>.
</p>
<p>This parameter is only relevant for <code>survival</code> outcomes.</p>
</dd>
<dt><code>dynamic_model_loading</code></dt><dd><p>(<em>optional</em>) Enables dynamic loading of models
during the evaluation process, if <code>TRUE</code>. Defaults to <code>FALSE</code>. Dynamic
loading of models may reduce the overall memory footprint, at the cost of
increased disk or network IO. Models can only be dynamically loaded if they
are found at an accessible disk or network location. Setting this parameter
to <code>TRUE</code> may help if parallel processing causes out-of-memory issues during
evaluation.</p>
</dd>
<dt><code>parallel_evaluation</code></dt><dd><p>(<em>optional</em>) Enable parallel processing for
hyperparameter optimisation. Defaults to <code>TRUE</code>. When set to <code>FALSE</code>, this
will disable the use of parallel processing while performing optimisation,
regardless of the settings of the <code>parallel</code> parameter. The parameter
moreover specifies whether parallelisation takes place within the evaluation
process steps (<code>inner</code>, default), or in an outer loop ( <code>outer</code>) over
learners, data subsamples, etc.
</p>
<p><code>parallel_evaluation</code> is ignored if <code>parallel=FALSE</code>.</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of settings to be used within the workflow
</p>


<h3>References</h3>


<ol>
<li><p> Storey, J. D. A direct approach to false discovery rates. J.
R. Stat. Soc. Series B Stat. Methodol. 64, 479–498 (2002).
</p>
</li>
<li><p> Shrout, P. E. &amp; Fleiss, J. L. Intraclass correlations: uses in assessing
rater reliability. Psychol. Bull. 86, 420–428 (1979).
</p>
</li>
<li><p> Koo, T. K. &amp; Li, M. Y. A guideline of selecting and reporting intraclass
correlation coefficients for reliability research. J. Chiropr. Med. 15,
155–163 (2016).
</p>
</li>
<li><p> Yeo, I. &amp; Johnson, R. A. A new family of power transformations to
improve normality or symmetry. Biometrika 87, 954–959 (2000).
</p>
</li>
<li><p> Box, G. E. P. &amp; Cox, D. R. An analysis of transformations. J. R. Stat.
Soc. Series B Stat. Methodol. 26, 211–252 (1964).
</p>
</li>
<li><p> Raymaekers, J., Rousseeuw,  P. J. Transforming variables to central
normality. Mach Learn. (2021).
</p>
</li>
<li><p> Park, M. Y., Hastie, T. &amp; Tibshirani, R. Averaged gene expressions for
regression. Biostatistics 8, 212–227 (2007).
</p>
</li>
<li><p> Tolosi, L. &amp; Lengauer, T. Classification with correlated features:
unreliability of feature ranking and solutions. Bioinformatics 27,
1986–1994 (2011).
</p>
</li>
<li><p> Johnson, W. E., Li, C. &amp; Rabinovic, A. Adjusting batch effects in
microarray expression data using empirical Bayes methods. Biostatistics 8,
118–127 (2007)
</p>
</li>
<li><p> Kaufman, L. &amp; Rousseeuw, P. J. Finding groups in data: an introduction
to cluster analysis. (John Wiley &amp; Sons, 2009).
</p>
</li>
<li><p> Muellner, D. fastcluster: fast hierarchical, agglomerative clustering
routines for R and Python. J. Stat. Softw. 53, 1–18 (2013).
</p>
</li>
<li><p> Rousseeuw, P. J. Silhouettes: A graphical aid to the interpretation and
validation of cluster analysis. J. Comput. Appl. Math. 20, 53–65 (1987).
</p>
</li>
<li><p> Langfelder, P., Zhang, B. &amp; Horvath, S. Defining clusters from a
hierarchical cluster tree: the Dynamic Tree Cut package for R.
Bioinformatics 24, 719–720 (2008).
</p>
</li>
<li><p> McFadden, D. Conditional logit analysis of qualitative choice behavior.
in Frontiers in Econometrics (ed. Zarembka, P.) 105–142 (Academic Press,
1974).
</p>
</li>
<li><p> Cox, D. R. &amp; Snell, E. J. Analysis of binary data. (Chapman and Hall,
1989).
</p>
</li>
<li><p> Nagelkerke, N. J. D. A note on a general definition of the coefficient
of determination. Biometrika 78, 691–692 (1991).
</p>
</li>
<li><p> Meinshausen, N. &amp; Buehlmann, P. Stability selection. J. R. Stat. Soc.
Series B Stat. Methodol. 72, 417–473 (2010).
</p>
</li>
<li><p> Haury, A.-C., Gestraud, P. &amp; Vert, J.-P. The influence of feature
selection methods on accuracy, stability and interpretability of molecular
signatures. PLoS One 6, e28210 (2011).
</p>
</li>
<li><p> Wald, R., Khoshgoftaar, T. M., Dittman, D., Awada, W. &amp; Napolitano,A. An
extensive comparison of feature ranking aggregation techniques in
bioinformatics. in 2012 IEEE 13th International Conference on Information
Reuse Integration (IRI) 377–384 (2012).
</p>
</li>
<li><p> Hutter, F., Hoos, H. H. &amp; Leyton-Brown, K. Sequential model-based
optimization for general algorithm configuration. in Learning and
Intelligent Optimization (ed. Coello, C. A. C.) 6683, 507–523 (Springer
Berlin Heidelberg, 2011).
</p>
</li>
<li><p> Shahriari, B., Swersky, K., Wang, Z., Adams, R. P. &amp; de Freitas, N.
Taking the Human Out of the Loop: A Review of Bayesian Optimization. Proc.
IEEE 104, 148–175 (2016)
</p>
</li>
<li><p> Srinivas, N., Krause, A., Kakade, S. M. &amp; Seeger, M. W.
Information-Theoretic Regret Bounds for Gaussian Process Optimization in
the Bandit Setting. IEEE Trans. Inf. Theory 58, 3250–3265 (2012)
</p>
</li>
<li><p> Kaufmann, E., Cappé, O. &amp; Garivier, A. On Bayesian upper confidence
bounds for bandit problems. in Artificial intelligence and statistics
592–600 (2012).
</p>
</li>
<li><p> Jamieson, K. &amp; Talwalkar, A. Non-stochastic Best Arm Identification and
Hyperparameter Optimization. in Proceedings of the 19th International
Conference on Artificial Intelligence and Statistics (eds. Gretton, A. &amp;
Robert, C. C.) vol. 51 240–248 (PMLR, 2016).
</p>
</li>
<li><p> Gramacy, R. B. laGP: Large-Scale Spatial Modeling via Local Approximate
Gaussian Processes in R. Journal of Statistical Software 72, 1–46 (2016)
</p>
</li>
<li><p> Sparapani, R., Spanbauer, C. &amp; McCulloch, R. Nonparametric Machine
Learning and Efficient Computation with Bayesian Additive Regression Trees:
The BART R Package. Journal of Statistical Software 97, 1–66 (2021)
</p>
</li>
<li><p> Davison, A. C. &amp; Hinkley, D. V. Bootstrap methods and their application.
(Cambridge University Press, 1997).
</p>
</li>
<li><p> Efron, B. &amp; Hastie, T. Computer Age Statistical Inference. (Cambridge
University Press, 2016).
</p>
</li>
<li><p> Lausen, B. &amp; Schumacher, M. Maximally Selected Rank Statistics.
Biometrics 48, 73 (1992).
</p>
</li>
<li><p> Hothorn, T. &amp; Lausen, B. On the exact distribution of maximally selected
rank statistics. Comput. Stat. Data Anal. 43, 121–137 (2003).
</p>
</li></ol>


<hr>
<h2 id='.parse_hyperparameter_optimisation_settings'>Internal function for parsing settings related to model hyperparameter
optimisation</h2><span id='topic+.parse_hyperparameter_optimisation_settings'></span>

<h3>Description</h3>

<p>Internal function for parsing settings related to model hyperparameter
optimisation
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.parse_hyperparameter_optimisation_settings(
  config = NULL,
  parallel,
  outcome_type,
  optimisation_bootstraps = waiver(),
  optimisation_determine_vimp = waiver(),
  smbo_random_initialisation = waiver(),
  smbo_n_random_sets = waiver(),
  max_smbo_iterations = waiver(),
  smbo_stop_convergent_iterations = waiver(),
  smbo_stop_tolerance = waiver(),
  smbo_time_limit = waiver(),
  smbo_initial_bootstraps = waiver(),
  smbo_step_bootstraps = waiver(),
  smbo_intensify_steps = waiver(),
  smbo_stochastic_reject_p_value = waiver(),
  optimisation_function = waiver(),
  optimisation_metric = waiver(),
  acquisition_function = waiver(),
  exploration_method = waiver(),
  hyperparameter_learner = waiver(),
  parallel_hyperparameter_optimisation = waiver(),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".parse_hyperparameter_optimisation_settings_+3A_config">config</code></td>
<td>
<p>A list of settings, e.g. from an xml file.</p>
</td></tr>
<tr><td><code id=".parse_hyperparameter_optimisation_settings_+3A_parallel">parallel</code></td>
<td>
<p>Logical value that whether familiar uses parallelisation. If
<code>FALSE</code> it will override <code>parallel_hyperparameter_optimisation</code>.</p>
</td></tr>
<tr><td><code id=".parse_hyperparameter_optimisation_settings_+3A_outcome_type">outcome_type</code></td>
<td>
<p>Type of outcome found in the data set.</p>
</td></tr>
<tr><td><code id=".parse_hyperparameter_optimisation_settings_+3A_optimisation_bootstraps">optimisation_bootstraps</code></td>
<td>
<p>(<em>optional</em>) Number of bootstraps that should
be generated from the development data set. During the optimisation
procedure one or more of these bootstraps (indicated by
<code>smbo_step_bootstraps</code>) are used for model development using different
combinations of hyperparameters. The effect of the hyperparameters is then
assessed by comparing in-bag and out-of-bag model performance.
</p>
<p>The default number of bootstraps is <code>50</code>. Hyperparameter optimisation may
finish before exhausting the set of bootstraps.</p>
</td></tr>
<tr><td><code id=".parse_hyperparameter_optimisation_settings_+3A_optimisation_determine_vimp">optimisation_determine_vimp</code></td>
<td>
<p>(<em>optional</em>) Logical value that indicates
whether variable importance is determined separately for each of the
bootstraps created during the optimisation process (<code>TRUE</code>) or the
applicable results from the feature selection step are used (<code>FALSE</code>).
</p>
<p>Determining variable importance increases the initial computational
overhead. However, it prevents positive biases for the out-of-bag data due
to overlap of these data with the development data set used for the feature
selection step. In this case, any hyperparameters of the variable
importance method are not determined separately for each bootstrap, but
those obtained during the feature selection step are used instead. In case
multiple of such hyperparameter sets could be applicable, the set that will
be used is randomly selected for each bootstrap.
</p>
<p>This parameter only affects hyperparameter optimisation of learners. The
default is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id=".parse_hyperparameter_optimisation_settings_+3A_smbo_random_initialisation">smbo_random_initialisation</code></td>
<td>
<p>(<em>optional</em>) String indicating the
initialisation method for the hyperparameter space. Can be one of
<code>fixed_subsample</code> (default), <code>fixed</code>, or <code>random</code>. <code>fixed</code> and
<code>fixed_subsample</code> first create hyperparameter sets from a range of default
values set by familiar. <code>fixed_subsample</code> then randomly draws up to
<code>smbo_n_random_sets</code> from the grid. <code>random</code> does not rely upon a fixed
grid, and randomly draws up to <code>smbo_n_random_sets</code> hyperparameter sets
from the hyperparameter space.</p>
</td></tr>
<tr><td><code id=".parse_hyperparameter_optimisation_settings_+3A_smbo_n_random_sets">smbo_n_random_sets</code></td>
<td>
<p>(<em>optional</em>) Number of random or subsampled
hyperparameters drawn during the initialisation process. Default: <code>100</code>.
Cannot be smaller than <code>10</code>. The parameter is not used when
<code>smbo_random_initialisation</code> is <code>fixed</code>, as the entire pre-defined grid
will be explored.</p>
</td></tr>
<tr><td><code id=".parse_hyperparameter_optimisation_settings_+3A_max_smbo_iterations">max_smbo_iterations</code></td>
<td>
<p>(<em>optional</em>) Maximum number of intensify
iterations of the SMBO algorithm. During an intensify iteration a run-off
occurs between the current <em>best</em> hyperparameter combination and either 10
challenger combination with the highest expected improvement or a set of 20
random combinations.
</p>
<p>Run-off with random combinations is used to force exploration of the
hyperparameter space, and is performed every second intensify iteration, or
if there is no expected improvement for any challenger combination.
</p>
<p>If a combination of hyperparameters leads to better performance on the same
data than the incumbent <em>best</em> set of hyperparameters, it replaces the
incumbent set at the end of the intensify iteration.
</p>
<p>The default number of intensify iteration is <code>20</code>. Iterations may be
stopped early if the incumbent set of hyperparameters remains the same for
<code>smbo_stop_convergent_iterations</code> iterations, or performance improvement is
minimal. This behaviour is suppressed during the first 4 iterations to
enable the algorithm to explore the hyperparameter space.</p>
</td></tr>
<tr><td><code id=".parse_hyperparameter_optimisation_settings_+3A_smbo_stop_convergent_iterations">smbo_stop_convergent_iterations</code></td>
<td>
<p>(<em>optional</em>) The number of subsequent
convergent SMBO iterations required to stop hyperparameter optimisation
early. An iteration is convergent if the <em>best</em> parameter set has not
changed or the optimisation score over the 4 most recent iterations has not
changed beyond the tolerance level in <code>smbo_stop_tolerance</code>.
</p>
<p>The default value is <code>3</code>.</p>
</td></tr>
<tr><td><code id=".parse_hyperparameter_optimisation_settings_+3A_smbo_stop_tolerance">smbo_stop_tolerance</code></td>
<td>
<p>(<em>optional</em>) Tolerance for early stopping due to
convergent optimisation score.
</p>
<p>The default value depends on the square root of the number of samples (at
the series level), and is <code>0.01</code> for 100 samples. This value is computed as
<code>0.1 * 1 / sqrt(n_samples)</code>. The upper limit is <code>0.0001</code> for 1M or more
samples.</p>
</td></tr>
<tr><td><code id=".parse_hyperparameter_optimisation_settings_+3A_smbo_time_limit">smbo_time_limit</code></td>
<td>
<p>(<em>optional</em>) Time limit (in minutes) for the
optimisation process. Optimisation is stopped after this limit is exceeded.
Time taken to determine variable importance for the optimisation process
(see the <code>optimisation_determine_vimp</code> parameter) does not count.
</p>
<p>The default is <code>NULL</code>, indicating that there is no time limit for the
optimisation process. The time limit cannot be less than 1 minute.</p>
</td></tr>
<tr><td><code id=".parse_hyperparameter_optimisation_settings_+3A_smbo_initial_bootstraps">smbo_initial_bootstraps</code></td>
<td>
<p>(<em>optional</em>) The number of bootstraps taken
from the set of <code>optimisation_bootstraps</code> as the bootstraps assessed
initially.
</p>
<p>The default value is <code>1</code>. The value cannot be larger than
<code>optimisation_bootstraps</code>.</p>
</td></tr>
<tr><td><code id=".parse_hyperparameter_optimisation_settings_+3A_smbo_step_bootstraps">smbo_step_bootstraps</code></td>
<td>
<p>(<em>optional</em>) The number of bootstraps taken from
the set of <code>optimisation_bootstraps</code> bootstraps as the bootstraps assessed
during the steps of each intensify iteration.
</p>
<p>The default value is <code>3</code>. The value cannot be larger than
<code>optimisation_bootstraps</code>.</p>
</td></tr>
<tr><td><code id=".parse_hyperparameter_optimisation_settings_+3A_smbo_intensify_steps">smbo_intensify_steps</code></td>
<td>
<p>(<em>optional</em>) The number of steps in each SMBO
intensify iteration. Each step a new set of <code>smbo_step_bootstraps</code>
bootstraps is drawn and used in the run-off between the incumbent <em>best</em>
hyperparameter combination and its challengers.
</p>
<p>The default value is <code>5</code>. Higher numbers allow for a more detailed
comparison, but this comes with added computational cost.</p>
</td></tr>
<tr><td><code id=".parse_hyperparameter_optimisation_settings_+3A_smbo_stochastic_reject_p_value">smbo_stochastic_reject_p_value</code></td>
<td>
<p>(<em>optional</em>) The p-value threshold used
for the <code>stochastic_reject</code> exploration method.
</p>
<p>The default value is <code>0.05</code>.</p>
</td></tr>
<tr><td><code id=".parse_hyperparameter_optimisation_settings_+3A_optimisation_function">optimisation_function</code></td>
<td>
<p>(<em>optional</em>) Type of optimisation function used
to quantify the performance of a hyperparameter set. Model performance is
assessed using the metric(s) specified by <code>optimisation_metric</code> on the
in-bag (IB) and out-of-bag (OOB) samples of a bootstrap. These values are
converted to objective scores with a standardised interval of <code class="reqn">[-1.0,
  1.0]</code>. Each pair of objective is subsequently used to compute an
optimisation score. The optimisation score across different bootstraps is
than aggregated to a summary score. This summary score is used to rank
hyperparameter sets, and select the optimal set.
</p>
<p>The combination of optimisation score and summary score is determined by
the optimisation function indicated by this parameter:
</p>

<ul>
<li> <p><code>validation</code> or <code>max_validation</code> (default): seeks to maximise OOB score.
</p>
</li>
<li> <p><code>balanced</code>: seeks to balance IB and OOB score.
</p>
</li>
<li> <p><code>stronger_balance</code>: similar to <code>balanced</code>, but with stronger penalty for
differences between IB and OOB scores.
</p>
</li>
<li> <p><code>validation_minus_sd</code>: seeks to optimise the average OOB score minus its
standard deviation.
</p>
</li>
<li> <p><code>validation_25th_percentile</code>: seeks to optimise the 25th percentile of
OOB scores, and is conceptually similar to <code>validation_minus_sd</code>.
</p>
</li>
<li> <p><code>model_estimate</code>: seeks to maximise the OOB score estimate predicted by
the hyperparameter learner (not available for random search).
</p>
</li>
<li> <p><code>model_estimate_minus_sd</code>: seeks to maximise the OOB score estimate minus
its estimated standard deviation, as predicted by the hyperparameter
learner (not available for random search).
</p>
</li>
<li> <p><code>model_balanced_estimate</code>: seeks to maximise the estimate of the balanced
IB and OOB score. This is similar to the <code>balanced</code> score, and in fact uses
a hyperparameter learner to predict said score (not available for random
search).
</p>
</li>
<li> <p><code>model_balanced_estimate_minus_sd</code>: seeks to maximise the estimate of the
balanced IB and OOB score, minus its estimated standard deviation. This is
similar to the <code>balanced</code> score, but takes into account its estimated
spread.
</p>
</li></ul>

<p>Additional detail are provided in the <em>Learning algorithms and
hyperparameter optimisation</em> vignette.</p>
</td></tr>
<tr><td><code id=".parse_hyperparameter_optimisation_settings_+3A_optimisation_metric">optimisation_metric</code></td>
<td>
<p>(<em>optional</em>) One or more metrics used to compute
performance scores. See the vignette on performance metrics for the
available metrics.
</p>
<p>If unset, the following metrics are used by default:
</p>

<ul>
<li> <p><code>auc_roc</code>: For <code>binomial</code> and <code>multinomial</code> models.
</p>
</li>
<li> <p><code>mse</code>: Mean squared error for <code>continuous</code> models.
</p>
</li>
<li> <p><code>msle</code>: Mean squared logarithmic error for <code>count</code> models.
</p>
</li>
<li> <p><code>concordance_index</code>: For <code>survival</code> models.
</p>
</li></ul>

<p>Multiple optimisation metrics can be specified. Actual metric values are
converted to an objective value by comparison with a baseline metric value
that derives from a trivial model, i.e. majority class for binomial and
multinomial outcomes, the median outcome for count and continuous outcomes
and a fixed risk or time for survival outcomes.</p>
</td></tr>
<tr><td><code id=".parse_hyperparameter_optimisation_settings_+3A_acquisition_function">acquisition_function</code></td>
<td>
<p>(<em>optional</em>) The acquisition function influences
how new hyperparameter sets are selected. The algorithm uses the model
learned by the learner indicated by <code>hyperparameter_learner</code> to search the
hyperparameter space for hyperparameter sets that are either likely better
than the best known set (<em>exploitation</em>) or where there is considerable
uncertainty (<em>exploration</em>). The acquisition function quantifies this
(Shahriari et al., 2016).
</p>
<p>The following acquisition functions are available, and are described in
more detail in the <em>learner algorithms</em> vignette:
</p>

<ul>
<li> <p><code>improvement_probability</code>: The probability of improvement quantifies the
probability that the expected optimisation score for a set is better than
the best observed optimisation score
</p>
</li>
<li> <p><code>improvement_empirical_probability</code>: Similar to
<code>improvement_probability</code>, but based directly on optimisation scores
predicted by the individual decision trees.
</p>
</li>
<li> <p><code>expected_improvement</code> (default): Computes expected improvement.
</p>
</li>
<li> <p><code>upper_confidence_bound</code>: This acquisition function is based on the upper
confidence bound of the distribution (Srinivas et al., 2012).
</p>
</li>
<li> <p><code>bayes_upper_confidence_bound</code>: This acquisition function is based on the
upper confidence bound of the distribution (Kaufmann et al., 2012).
</p>
</li></ul>
</td></tr>
<tr><td><code id=".parse_hyperparameter_optimisation_settings_+3A_exploration_method">exploration_method</code></td>
<td>
<p>(<em>optional</em>) Method used to steer exploration in
post-initialisation intensive searching steps. As stated earlier, each SMBO
iteration step compares suggested alternative parameter sets with an
incumbent <strong>best</strong> set in a series of steps. The exploration method
controls how the set of alternative parameter sets is pruned after each
step in an iteration. Can be one of the following:
</p>

<ul>
<li> <p><code>single_shot</code> (default): The set of alternative parameter sets is not
pruned, and each intensification iteration contains only a single
intensification step that only uses a single bootstrap. This is the fastest
exploration method, but only superficially tests each parameter set.
</p>
</li>
<li> <p><code>successive_halving</code>: The set of alternative parameter sets is
pruned by removing the worst performing half of the sets after each step
(Jamieson and Talwalkar, 2016).
</p>
</li>
<li> <p><code>stochastic_reject</code>: The set of alternative parameter sets is pruned by
comparing the performance of each parameter set with that of the incumbent
<strong>best</strong> parameter set using a paired Wilcoxon test based on shared
bootstraps. Parameter sets that perform significantly worse, at an alpha
level indicated by <code>smbo_stochastic_reject_p_value</code>, are pruned.
</p>
</li>
<li> <p><code>none</code>: The set of alternative parameter sets is not pruned.
</p>
</li></ul>
</td></tr>
<tr><td><code id=".parse_hyperparameter_optimisation_settings_+3A_hyperparameter_learner">hyperparameter_learner</code></td>
<td>
<p>(<em>optional</em>) Any point in the hyperparameter
space has a single, scalar, optimisation score value that is <em>a priori</em>
unknown. During the optimisation process, the algorithm samples from the
hyperparameter space by selecting hyperparameter sets and computing the
optimisation score value for one or more bootstraps. For each
hyperparameter set the resulting values are distributed around the actual
value. The learner indicated by <code>hyperparameter_learner</code> is then used to
infer optimisation score estimates for unsampled parts of the
hyperparameter space.
</p>
<p>The following models are available:
</p>

<ul>
<li> <p><code>bayesian_additive_regression_trees</code> or <code>bart</code>: Uses Bayesian Additive
Regression Trees (Sparapani et al., 2021) for inference. Unlike standard
random forests, BART allows for estimating posterior distributions directly
and can extrapolate.
</p>
</li>
<li> <p><code>gaussian_process</code> (default): Creates a localised approximate Gaussian
process for inference (Gramacy, 2016). This allows for better scaling than
deterministic Gaussian Processes.
</p>
</li>
<li> <p><code>random_forest</code>: Creates a random forest for inference. Originally
suggested by Hutter et al. (2011). A weakness of random forests is their
lack of extrapolation beyond observed values, which limits their usefulness
in exploiting promising areas of hyperparameter space.
</p>
</li>
<li> <p><code>random</code> or <code>random_search</code>: Forgoes the use of models to steer
optimisation. Instead, a random search is performed.
</p>
</li></ul>
</td></tr>
<tr><td><code id=".parse_hyperparameter_optimisation_settings_+3A_parallel_hyperparameter_optimisation">parallel_hyperparameter_optimisation</code></td>
<td>
<p>(<em>optional</em>) Enable parallel
processing for hyperparameter optimisation. Defaults to <code>TRUE</code>. When set to
<code>FALSE</code>, this will disable the use of parallel processing while performing
optimisation, regardless of the settings of the <code>parallel</code> parameter. The
parameter moreover specifies whether parallelisation takes place within the
optimisation algorithm (<code>inner</code>, default), or in an outer loop ( <code>outer</code>)
over learners, data subsamples, etc.
</p>
<p><code>parallel_hyperparameter_optimisation</code> is ignored if <code>parallel=FALSE</code>.</p>
</td></tr>
<tr><td><code id=".parse_hyperparameter_optimisation_settings_+3A_...">...</code></td>
<td>
<p>Unused arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List of parameters related to model hyperparameter optimisation.
</p>


<h3>References</h3>


<ol>
<li><p> Hutter, F., Hoos, H. H. &amp; Leyton-Brown, K. Sequential
model-based optimization for general algorithm configuration. in Learning
and Intelligent Optimization (ed. Coello, C. A. C.) 6683, 507–523 (Springer
Berlin Heidelberg, 2011).
</p>
</li>
<li><p> Shahriari, B., Swersky, K., Wang, Z., Adams, R. P. &amp; de Freitas, N.
Taking the Human Out of the Loop: A Review of Bayesian Optimization. Proc.
IEEE 104, 148–175 (2016)
</p>
</li>
<li><p> Srinivas, N., Krause, A., Kakade, S. M. &amp; Seeger, M. W.
Information-Theoretic Regret Bounds for Gaussian Process Optimization in
the Bandit Setting. IEEE Trans. Inf. Theory 58, 3250–3265 (2012)
</p>
</li>
<li><p> Kaufmann, E., Cappé, O. &amp; Garivier, A. On Bayesian upper confidence
bounds for bandit problems. in Artificial intelligence and statistics
592–600 (2012).
</p>
</li>
<li><p> Jamieson, K. &amp; Talwalkar, A. Non-stochastic Best Arm Identification and
Hyperparameter Optimization. in Proceedings of the 19th International
Conference on Artificial Intelligence and Statistics (eds. Gretton, A. &amp;
Robert, C. C.) vol. 51 240–248 (PMLR, 2016).
</p>
</li>
<li><p> Gramacy, R. B. laGP: Large-Scale Spatial Modeling via Local Approximate
Gaussian Processes in R. Journal of Statistical Software 72, 1–46 (2016)
</p>
</li>
<li><p> Sparapani, R., Spanbauer, C. &amp; McCulloch, R. Nonparametric Machine
Learning and Efficient Computation with Bayesian Additive Regression Trees:
The BART R Package. Journal of Statistical Software 97, 1–66 (2021)
</p>
</li></ol>


<hr>
<h2 id='.parse_initial_settings'>Internal function for parsing settings required to parse the input data
and define the experiment</h2><span id='topic+.parse_initial_settings'></span>

<h3>Description</h3>

<p>This function parses settings required to parse the data set, e.g. determine
which columns are identfier columns, what column contains outcome data, which
type of outcome is it?
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.parse_initial_settings(config = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".parse_initial_settings_+3A_config">config</code></td>
<td>
<p>A list of settings, e.g. from an xml file.</p>
</td></tr>
<tr><td><code id=".parse_initial_settings_+3A_...">...</code></td>
<td>

<p>Arguments passed on to <code><a href="#topic+.parse_experiment_settings">.parse_experiment_settings</a></code>
</p>

<dl>
<dt><code>batch_id_column</code></dt><dd><p>(<strong>recommended</strong>) Name of the column containing batch
or cohort identifiers. This parameter is required if more than one dataset
is provided, or if external validation is performed.
</p>
<p>In familiar any row of data is organised by four identifiers:
</p>

<ul>
<li><p> The batch identifier <code>batch_id_column</code>: This denotes the group to which a
set of samples belongs, e.g. patients from a single study, samples measured
in a batch, etc. The batch identifier is used for batch normalisation, as
well as selection of development and validation datasets.
</p>
</li>
<li><p> The sample identifier <code>sample_id_column</code>: This denotes the sample level,
e.g. data from a single individual. Subsets of data, e.g. bootstraps or
cross-validation folds, are created at this level.
</p>
</li>
<li><p> The series identifier <code>series_id_column</code>: Indicates measurements on a
single sample that may not share the same outcome value, e.g. a time
series, or the number of cells in a view.
</p>
</li>
<li><p> The repetition identifier: Indicates repeated measurements in a single
series where any feature values may differ, but the outcome does not.
Repetition identifiers are always implicitly set when multiple entries for
the same series of the same sample in the same batch that share the same
outcome are encountered.
</p>
</li></ul>
</dd>
<dt><code>sample_id_column</code></dt><dd><p>(<strong>recommended</strong>) Name of the column containing
sample or subject identifiers. See <code>batch_id_column</code> above for more
details.
</p>
<p>If unset, every row will be identified as a single sample.</p>
</dd>
<dt><code>series_id_column</code></dt><dd><p>(<strong>optional</strong>) Name of the column containing series
identifiers, which distinguish between measurements that are part of a
series for a single sample. See <code>batch_id_column</code> above for more details.
</p>
<p>If unset, rows which share the same batch and sample identifiers but have a
different outcome are assigned unique series identifiers.</p>
</dd>
<dt><code>development_batch_id</code></dt><dd><p>(<em>optional</em>) One or more batch or cohort
identifiers to constitute data sets for development. Defaults to all, or
all minus the identifiers in <code>validation_batch_id</code> for external validation.
Required if external validation is performed and <code>validation_batch_id</code> is
not provided.</p>
</dd>
<dt><code>validation_batch_id</code></dt><dd><p>(<em>optional</em>) One or more batch or cohort
identifiers to constitute data sets for external validation. Defaults to
all data sets except those in <code>development_batch_id</code> for external
validation, or none if not. Required if <code>development_batch_id</code> is not
provided.</p>
</dd>
<dt><code>outcome_name</code></dt><dd><p>(<em>optional</em>) Name of the modelled outcome. This name will
be used in figures created by <code>familiar</code>.
</p>
<p>If not set, the column name in <code>outcome_column</code> will be used for
<code>binomial</code>, <code>multinomial</code>, <code>count</code> and <code>continuous</code> outcomes. For other
outcomes (<code>survival</code> and <code>competing_risk</code>) no default is used.</p>
</dd>
<dt><code>outcome_column</code></dt><dd><p>(<strong>recommended</strong>) Name of the column containing the
outcome of interest. May be identified from a formula, if a formula is
provided as an argument. Otherwise an error is raised. Note that <code>survival</code>
and <code>competing_risk</code> outcome type outcomes require two columns that
indicate the time-to-event or the time of last follow-up and the event
status.</p>
</dd>
<dt><code>outcome_type</code></dt><dd><p>(<strong>recommended</strong>) Type of outcome found in the outcome
column. The outcome type determines many aspects of the overall process,
e.g. the available feature selection methods and learners, but also the
type of assessments that can be conducted to evaluate the resulting models.
Implemented outcome types are:
</p>

<ul>
<li> <p><code>binomial</code>: categorical outcome with 2 levels.
</p>
</li>
<li> <p><code>multinomial</code>: categorical outcome with 2 or more levels.
</p>
</li>
<li> <p><code>count</code>: Poisson-distributed numeric outcomes.
</p>
</li>
<li> <p><code>continuous</code>: general continuous numeric outcomes.
</p>
</li>
<li> <p><code>survival</code>: survival outcome for time-to-event data.
</p>
</li></ul>

<p>If not provided, the algorithm will attempt to obtain outcome_type from
contents of the outcome column. This may lead to unexpected results, and we
therefore advise to provide this information manually.
</p>
<p>Note that <code>competing_risk</code> survival analysis are not fully supported, and
is currently not a valid choice for <code>outcome_type</code>.</p>
</dd>
<dt><code>class_levels</code></dt><dd><p>(<em>optional</em>) Class levels for <code>binomial</code> or <code>multinomial</code>
outcomes. This argument can be used to specify the ordering of levels for
categorical outcomes. These class levels must exactly match the levels
present in the outcome column.</p>
</dd>
<dt><code>event_indicator</code></dt><dd><p>(<strong>recommended</strong>) Indicator for events in <code>survival</code>
and <code>competing_risk</code> analyses. <code>familiar</code> will automatically recognise <code>1</code>,
<code>true</code>, <code>t</code>, <code>y</code> and <code>yes</code> as event indicators, including different
capitalisations. If this parameter is set, it replaces the default values.</p>
</dd>
<dt><code>censoring_indicator</code></dt><dd><p>(<strong>recommended</strong>) Indicator for right-censoring in
<code>survival</code> and <code>competing_risk</code> analyses. <code>familiar</code> will automatically
recognise <code>0</code>, <code>false</code>, <code>f</code>, <code>n</code>, <code>no</code> as censoring indicators, including
different capitalisations. If this parameter is set, it replaces the
default values.</p>
</dd>
<dt><code>competing_risk_indicator</code></dt><dd><p>(<strong>recommended</strong>) Indicator for competing
risks in <code>competing_risk</code> analyses. There are no default values, and if
unset, all values other than those specified by the <code>event_indicator</code> and
<code>censoring_indicator</code> parameters are considered to indicate competing
risks.</p>
</dd>
<dt><code>signature</code></dt><dd><p>(<em>optional</em>) One or more names of feature columns that are
considered part of a specific signature. Features specified here will
always be used for modelling. Ranking from feature selection has no effect
for these features.</p>
</dd>
<dt><code>novelty_features</code></dt><dd><p>(<em>optional</em>) One or more names of feature columns
that should be included for the purpose of novelty detection.</p>
</dd>
<dt><code>exclude_features</code></dt><dd><p>(<em>optional</em>) Feature columns that will be removed
from the data set. Cannot overlap with features in <code>signature</code>,
<code>novelty_features</code> or <code>include_features</code>.</p>
</dd>
<dt><code>include_features</code></dt><dd><p>(<em>optional</em>) Feature columns that are specifically
included in the data set. By default all features are included. Cannot
overlap with <code>exclude_features</code>, but may overlap <code>signature</code>. Features in
<code>signature</code> and <code>novelty_features</code> are always included. If both
<code>exclude_features</code> and <code>include_features</code> are provided, <code>include_features</code>
takes precedence, provided that there is no overlap between the two.</p>
</dd>
<dt><code>reference_method</code></dt><dd><p>(<em>optional</em>) Method used to set reference levels for
categorical features. There are several options:
</p>

<ul>
<li> <p><code>auto</code> (default): Categorical features that are not explicitly set by the
user, i.e. columns containing boolean values or characters, use the most
frequent level as reference. Categorical features that are explicitly set,
i.e. as factors, are used as is.
</p>
</li>
<li> <p><code>always</code>: Both automatically detected and user-specified categorical
features have the reference level set to the most frequent level. Ordinal
features are not altered, but are used as is.
</p>
</li>
<li> <p><code>never</code>: User-specified categorical features are used as is.
Automatically detected categorical features are simply sorted, and the
first level is then used as the reference level. This was the behaviour
prior to familiar version 1.3.0.
</p>
</li></ul>
</dd>
<dt><code>experimental_design</code></dt><dd><p>(<strong>required</strong>) Defines what the experiment looks
like, e.g. <code>cv(bt(fs,20)+mb,3,2)+ev</code> for 2 times repeated 3-fold
cross-validation with nested feature selection on 20 bootstraps and
model-building, and external validation. The basic workflow components are:
</p>

<ul>
<li> <p><code>fs</code>: (required) feature selection step.
</p>
</li>
<li> <p><code>mb</code>: (required) model building step.
</p>
</li>
<li> <p><code>ev</code>: (optional) external validation. Note that internal validation due
to subsampling will always be conducted if the subsampling methods create
any validation data sets.
</p>
</li></ul>

<p>The different components are linked using <code>+</code>.
</p>
<p>Different subsampling methods can be used in conjunction with the basic
workflow components:
</p>

<ul>
<li> <p><code>bs(x,n)</code>: (stratified) .632 bootstrap, with <code>n</code> the number of
bootstraps. In contrast to <code>bt</code>, feature pre-processing parameters and
hyperparameter optimisation are conducted on individual bootstraps.
</p>
</li>
<li> <p><code>bt(x,n)</code>: (stratified) .632 bootstrap, with <code>n</code> the number of
bootstraps. Unlike <code>bs</code> and other subsampling methods, no separate
pre-processing parameters or optimised hyperparameters will be determined
for each bootstrap.
</p>
</li>
<li> <p><code>cv(x,n,p)</code>: (stratified) <code>n</code>-fold cross-validation, repeated <code>p</code> times.
Pre-processing parameters are determined for each iteration.
</p>
</li>
<li> <p><code>lv(x)</code>: leave-one-out-cross-validation. Pre-processing parameters are
determined for each iteration.
</p>
</li>
<li> <p><code>ip(x)</code>: imbalance partitioning for addressing class imbalances on the
data set. Pre-processing parameters are determined for each partition. The
number of partitions generated depends on the imbalance correction method
(see the <code>imbalance_correction_method</code> parameter). Imbalance partitioning
does not generate validation sets.
</p>
</li></ul>

<p>As shown in the example above, sampling algorithms can be nested.
</p>
<p>The simplest valid experimental design is <code>fs+mb</code>, which corresponds to a
TRIPOD type 1a analysis. Type 1b analyses are only possible using
bootstraps, e.g. <code>bt(fs+mb,100)</code>. Type 2a analyses can be conducted using
cross-validation, e.g. <code>cv(bt(fs,100)+mb,10,1)</code>. Depending on the origin of
the external validation data, designs such as <code>fs+mb+ev</code> or
<code>cv(bt(fs,100)+mb,10,1)+ev</code> constitute type 2b or type 3 analyses. Type 4
analyses can be done by obtaining one or more <code>familiarModel</code> objects from
others and applying them to your own data set.
</p>
<p>Alternatively, the <code>experimental_design</code> parameter may be used to provide a
path to a file containing iterations, which is named <code style="white-space: pre;">&#8288;####_iterations.RDS&#8288;</code>
by convention. This path can be relative to the directory of the current
experiment (<code>experiment_dir</code>), or an absolute path. The absolute path may
thus also point to a file from a different experiment.</p>
</dd>
<dt><code>imbalance_correction_method</code></dt><dd><p>(<em>optional</em>) Type of method used to
address class imbalances. Available options are:
</p>

<ul>
<li> <p><code>full_undersampling</code> (default): All data will be used in an ensemble
fashion. The full minority class will appear in each partition, but
majority classes are undersampled until all data have been used.
</p>
</li>
<li> <p><code>random_undersampling</code>: Randomly undersamples majority classes. This is
useful in cases where full undersampling would lead to the formation of
many models due major overrepresentation of the largest class.
</p>
</li></ul>

<p>This parameter is only used in combination with imbalance partitioning in
the experimental design, and <code>ip</code> should therefore appear in the string
that defines the design.</p>
</dd>
<dt><code>imbalance_n_partitions</code></dt><dd><p>(<em>optional</em>) Number of times random
undersampling should be repeated. 10 undersampled subsets with balanced
classes are formed by default.</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Details</h3>

<p>Three variants of parameters exist:
</p>

<ul>
<li><p> required: this parameter is required and must be set by the user.
</p>
</li>
<li><p> recommended: not setting this parameter might cause an error to be thrown,
dependent on other input.
</p>
</li>
<li><p> optional: these parameters have default values that may be altered if
required.
</p>
</li></ul>



<h3>Value</h3>

<p>A list of settings to be used for configuring the experiments.
</p>

<hr>
<h2 id='.parse_model_development_settings'>Internal function for parsing settings related to model development</h2><span id='topic+.parse_model_development_settings'></span>

<h3>Description</h3>

<p>Internal function for parsing settings related to model development
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.parse_model_development_settings(
  config = NULL,
  data,
  parallel,
  outcome_type,
  learner = waiver(),
  hyperparameter = waiver(),
  novelty_detector = waiver(),
  detector_parameters = waiver(),
  parallel_model_development = waiver(),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".parse_model_development_settings_+3A_config">config</code></td>
<td>
<p>A list of settings, e.g. from an xml file.</p>
</td></tr>
<tr><td><code id=".parse_model_development_settings_+3A_data">data</code></td>
<td>
<p>Data set as loaded using the <code>.load_data</code> function.</p>
</td></tr>
<tr><td><code id=".parse_model_development_settings_+3A_parallel">parallel</code></td>
<td>
<p>Logical value that whether familiar uses parallelisation. If
<code>FALSE</code> it will override <code>parallel_model_development</code>.</p>
</td></tr>
<tr><td><code id=".parse_model_development_settings_+3A_outcome_type">outcome_type</code></td>
<td>
<p>Type of outcome found in the data set.</p>
</td></tr>
<tr><td><code id=".parse_model_development_settings_+3A_learner">learner</code></td>
<td>
<p>(<strong>required</strong>) One or more algorithms used for model
development. A sizeable number learners is supported in <code>familiar</code>. Please
see the vignette on learners for more information concerning the available
learners.</p>
</td></tr>
<tr><td><code id=".parse_model_development_settings_+3A_hyperparameter">hyperparameter</code></td>
<td>
<p>(<em>optional</em>) List of lists containing hyperparameters
for learners. Each sublist should have the name of the learner method it
corresponds to, with list elements being named after the intended
hyperparameter, e.g. <code>"glm_logistic"=list("sign_size"=3)</code>
</p>
<p>All learners have hyperparameters. Please refer to the vignette on learners
for more details. If no parameters are provided, sequential model-based
optimisation is used to determine optimal hyperparameters.
</p>
<p>Hyperparameters provided by the user are never optimised. However, if more
than one value is provided for a single hyperparameter, optimisation will
be conducted using these values.</p>
</td></tr>
<tr><td><code id=".parse_model_development_settings_+3A_novelty_detector">novelty_detector</code></td>
<td>
<p>(<em>optional</em>) Specify the algorithm used for training
a novelty detector. This detector can be used to identify
out-of-distribution data prospectively.</p>
</td></tr>
<tr><td><code id=".parse_model_development_settings_+3A_detector_parameters">detector_parameters</code></td>
<td>
<p>(<em>optional</em>) List lists containing hyperparameters
for novelty detectors. Currently not used.</p>
</td></tr>
<tr><td><code id=".parse_model_development_settings_+3A_parallel_model_development">parallel_model_development</code></td>
<td>
<p>(<em>optional</em>) Enable parallel processing for
the model development workflow. Defaults to <code>TRUE</code>. When set to <code>FALSE</code>,
this will disable the use of parallel processing while developing models,
regardless of the settings of the <code>parallel</code> parameter.
<code>parallel_model_development</code> is ignored if <code>parallel=FALSE</code>.</p>
</td></tr>
<tr><td><code id=".parse_model_development_settings_+3A_...">...</code></td>
<td>
<p>Unused arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List of parameters related to model development.
</p>

<hr>
<h2 id='.parse_preprocessing_settings'>Internal function for parsing settings related to preprocessing</h2><span id='topic+.parse_preprocessing_settings'></span>

<h3>Description</h3>

<p>Internal function for parsing settings related to preprocessing
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.parse_preprocessing_settings(
  config = NULL,
  data,
  parallel,
  outcome_type,
  feature_max_fraction_missing = waiver(),
  sample_max_fraction_missing = waiver(),
  filter_method = waiver(),
  univariate_test_threshold = waiver(),
  univariate_test_threshold_metric = waiver(),
  univariate_test_max_feature_set_size = waiver(),
  low_var_minimum_variance_threshold = waiver(),
  low_var_max_feature_set_size = waiver(),
  robustness_icc_type = waiver(),
  robustness_threshold_metric = waiver(),
  robustness_threshold_value = waiver(),
  transformation_method = waiver(),
  normalisation_method = waiver(),
  batch_normalisation_method = waiver(),
  imputation_method = waiver(),
  cluster_method = waiver(),
  cluster_linkage_method = waiver(),
  cluster_cut_method = waiver(),
  cluster_similarity_metric = waiver(),
  cluster_similarity_threshold = waiver(),
  cluster_representation_method = waiver(),
  parallel_preprocessing = waiver(),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".parse_preprocessing_settings_+3A_config">config</code></td>
<td>
<p>A list of settings, e.g. from an xml file.</p>
</td></tr>
<tr><td><code id=".parse_preprocessing_settings_+3A_data">data</code></td>
<td>
<p>Data set as loaded using the <code>.load_data</code> function.</p>
</td></tr>
<tr><td><code id=".parse_preprocessing_settings_+3A_parallel">parallel</code></td>
<td>
<p>Logical value that whether familiar uses parallelisation. If
<code>FALSE</code> it will override <code>parallel_preprocessing</code>.</p>
</td></tr>
<tr><td><code id=".parse_preprocessing_settings_+3A_outcome_type">outcome_type</code></td>
<td>
<p>Type of outcome found in the data set.</p>
</td></tr>
<tr><td><code id=".parse_preprocessing_settings_+3A_feature_max_fraction_missing">feature_max_fraction_missing</code></td>
<td>
<p>(<em>optional</em>) Numeric value between <code>0.0</code>
and <code>0.95</code> that determines the meximum fraction of missing values that
still allows a feature to be included in the data set. All features with a
missing value fraction over this threshold are not processed further. The
default value is <code>0.30</code>.</p>
</td></tr>
<tr><td><code id=".parse_preprocessing_settings_+3A_sample_max_fraction_missing">sample_max_fraction_missing</code></td>
<td>
<p>(<em>optional</em>) Numeric value between <code>0.0</code>
and <code>0.95</code> that determines the maximum fraction of missing values that
still allows a sample to be included in the data set. All samples with a
missing value fraction over this threshold are excluded and not processed
further. The default value is <code>0.30</code>.</p>
</td></tr>
<tr><td><code id=".parse_preprocessing_settings_+3A_filter_method">filter_method</code></td>
<td>
<p>(<em>optional</em>) One or methods used to reduce
dimensionality of the data set by removing irrelevant or poorly
reproducible features.
</p>
<p>Several method are available:
</p>

<ul>
<li> <p><code>none</code> (default): None of the features will be filtered.
</p>
</li>
<li> <p><code>low_variance</code>: Features with a variance below the
<code>low_var_minimum_variance_threshold</code> are filtered. This can be useful to
filter, for example, genes that are not differentially expressed.
</p>
</li>
<li> <p><code>univariate_test</code>: Features undergo a univariate regression using an
outcome-appropriate regression model. The p-value of the model coefficient
is collected. Features with coefficient p or q-value above the
<code>univariate_test_threshold</code> are subsequently filtered.
</p>
</li>
<li> <p><code>robustness</code>: Features that are not sufficiently robust according to the
intraclass correlation coefficient are filtered. Use of this method
requires that repeated measurements are present in the data set, i.e. there
should be entries for which the sample and cohort identifiers are the same.
</p>
</li></ul>

<p>More than one method can be used simultaneously. Features with singular
values are always filtered, as these do not contain information.</p>
</td></tr>
<tr><td><code id=".parse_preprocessing_settings_+3A_univariate_test_threshold">univariate_test_threshold</code></td>
<td>
<p>(<em>optional</em>) Numeric value between <code>1.0</code> and
<code>0.0</code> that determines which features are irrelevant and will be filtered by
the <code>univariate_test</code>. The p or q-values are compared to this threshold.
All features with values above the threshold are filtered. The default
value is <code>0.20</code>.</p>
</td></tr>
<tr><td><code id=".parse_preprocessing_settings_+3A_univariate_test_threshold_metric">univariate_test_threshold_metric</code></td>
<td>
<p>(<em>optional</em>) Metric used with the to
compare the <code>univariate_test_threshold</code> against. The following metrics can
be chosen:
</p>

<ul>
<li> <p><code>p_value</code> (default): The unadjusted p-value of each feature is used for
to filter features.
</p>
</li>
<li> <p><code>q_value</code>: The q-value (Story, 2002), is used to filter features. Some
data sets may have insufficient samples to compute the q-value. The
<code>qvalue</code> package must be installed from Bioconductor to use this method.
</p>
</li></ul>
</td></tr>
<tr><td><code id=".parse_preprocessing_settings_+3A_univariate_test_max_feature_set_size">univariate_test_max_feature_set_size</code></td>
<td>
<p>(<em>optional</em>) Maximum size of the
feature set after the univariate test. P or q values of features are
compared against the threshold, but if the resulting data set would be
larger than this setting, only the most relevant features up to the desired
feature set size are selected.
</p>
<p>The default value is <code>NULL</code>, which causes features to be filtered based on
their relevance only.</p>
</td></tr>
<tr><td><code id=".parse_preprocessing_settings_+3A_low_var_minimum_variance_threshold">low_var_minimum_variance_threshold</code></td>
<td>
<p>(required, if used) Numeric value
that determines which features will be filtered by the <code>low_variance</code>
method. The variance of each feature is computed and compared to the
threshold. If it is below the threshold, the feature is removed.
</p>
<p>This parameter has no default value and should be set if <code>low_variance</code> is
used.</p>
</td></tr>
<tr><td><code id=".parse_preprocessing_settings_+3A_low_var_max_feature_set_size">low_var_max_feature_set_size</code></td>
<td>
<p>(<em>optional</em>) Maximum size of the feature
set after filtering features with a low variance. All features are first
compared against <code>low_var_minimum_variance_threshold</code>. If the resulting
feature set would be larger than specified, only the most strongly varying
features will be selected, up to the desired size of the feature set.
</p>
<p>The default value is <code>NULL</code>, which causes features to be filtered based on
their variance only.</p>
</td></tr>
<tr><td><code id=".parse_preprocessing_settings_+3A_robustness_icc_type">robustness_icc_type</code></td>
<td>
<p>(<em>optional</em>) String indicating the type of
intraclass correlation coefficient (<code>1</code>, <code>2</code> or <code>3</code>) that should be used to
compute robustness for features in repeated measurements. These types
correspond to the types in Shrout and Fleiss (1979). The default value is
<code>1</code>.</p>
</td></tr>
<tr><td><code id=".parse_preprocessing_settings_+3A_robustness_threshold_metric">robustness_threshold_metric</code></td>
<td>
<p>(<em>optional</em>) String indicating which
specific intraclass correlation coefficient (ICC) metric should be used to
filter features. This should be one of:
</p>

<ul>
<li> <p><code>icc</code>: The estimated ICC value itself.
</p>
</li>
<li> <p><code>icc_low</code> (default): The estimated lower limit of the 95% confidence
interval of the ICC, as suggested by Koo and Li (2016).
</p>
</li>
<li> <p><code>icc_panel</code>: The estimated ICC value over the panel average, i.e. the ICC
that would be obtained if all repeated measurements were averaged.
</p>
</li>
<li> <p><code>icc_panel_low</code>: The estimated lower limit of the 95% confidence interval
of the panel ICC.
</p>
</li></ul>
</td></tr>
<tr><td><code id=".parse_preprocessing_settings_+3A_robustness_threshold_value">robustness_threshold_value</code></td>
<td>
<p>(<em>optional</em>) The intraclass correlation
coefficient value that is as threshold. The default value is <code>0.70</code>.</p>
</td></tr>
<tr><td><code id=".parse_preprocessing_settings_+3A_transformation_method">transformation_method</code></td>
<td>
<p>(<em>optional</em>) The transformation method used to
change the distribution of the data to be more normal-like. The following
methods are available:
</p>

<ul>
<li> <p><code>none</code>: This disables transformation of features.
</p>
</li>
<li> <p><code>yeo_johnson</code> (default): Transformation using the Yeo-Johnson
transformation (Yeo and Johnson, 2000). The algorithm tests various lambda
values and selects the lambda that maximises the log-likelihood.
</p>
</li>
<li> <p><code>yeo_johnson_trim</code>: As <code>yeo_johnson</code>, but based on the set of feature
values where the 5% lowest and 5% highest values are discarded. This
reduces the effect of outliers.
</p>
</li>
<li> <p><code>yeo_johnson_winsor</code>: As <code>yeo_johnson</code>, but based on the set of feature
values where the 5% lowest and 5% highest values are winsorised. This
reduces the effect of outliers.
</p>
</li>
<li> <p><code>yeo_johnson_robust</code>: A robust version of <code>yeo_johnson</code> after Raymaekers
and Rousseeuw (2021). This method is less sensitive to outliers.
</p>
</li>
<li> <p><code>box_cox</code>: Transformation using the Box-Cox transformation (Box and Cox,
1964). Unlike the Yeo-Johnson transformation, the Box-Cox transformation
requires that all data are positive. Features that contain zero or negative
values cannot be transformed using this transformation. The algorithm tests
various lambda values and selects the lambda that maximises the
log-likelihood.
</p>
</li>
<li> <p><code>box_cox_trim</code>: As <code>box_cox</code>, but based on the set of feature values
where the 5% lowest and 5% highest values are discarded. This reduces the
effect of outliers.
</p>
</li>
<li> <p><code>box_cox_winsor</code>: As <code>box_cox</code>, but based on the set of feature values
where the 5% lowest and 5% highest values are winsorised. This reduces the
effect of outliers.
</p>
</li>
<li> <p><code>box_cox_robust</code>: A robust verson of <code>box_cox</code> after Raymaekers and
Rousseew (2021). This method is less sensitive to outliers.
</p>
</li></ul>

<p>Only features that contain numerical data are transformed. Transformation
parameters obtained in development data are stored within <code>featureInfo</code>
objects for later use with validation data sets.</p>
</td></tr>
<tr><td><code id=".parse_preprocessing_settings_+3A_normalisation_method">normalisation_method</code></td>
<td>
<p>(<em>optional</em>) The normalisation method used to
improve the comparability between numerical features that may have very
different scales. The following normalisation methods can be chosen:
</p>

<ul>
<li> <p><code>none</code>: This disables feature normalisation.
</p>
</li>
<li> <p><code>standardisation</code>: Features are normalised by subtraction of their mean
values and division by their standard deviations. This causes every feature
to be have a center value of 0.0 and standard deviation of 1.0.
</p>
</li>
<li> <p><code>standardisation_trim</code>: As <code>standardisation</code>, but based on the set of
feature values where the 5% lowest and 5% highest values are discarded.
This reduces the effect of outliers.
</p>
</li>
<li> <p><code>standardisation_winsor</code>: As <code>standardisation</code>, but based on the set of
feature values where the 5% lowest and 5% highest values are winsorised.
This reduces the effect of outliers.
</p>
</li>
<li> <p><code>standardisation_robust</code> (default): A robust version of <code>standardisation</code>
that relies on computing Huber's M-estimators for location and scale.
</p>
</li>
<li> <p><code>normalisation</code>: Features are normalised by subtraction of their minimum
values and division by their ranges. This maps all feature values to a
<code class="reqn">[0, 1]</code> interval.
</p>
</li>
<li> <p><code>normalisation_trim</code>: As <code>normalisation</code>, but based on the set of feature
values where the 5% lowest and 5% highest values are discarded. This
reduces the effect of outliers.
</p>
</li>
<li> <p><code>normalisation_winsor</code>: As <code>normalisation</code>, but based on the set of
feature values where the 5% lowest and 5% highest values are winsorised.
This reduces the effect of outliers.
</p>
</li>
<li> <p><code>quantile</code>: Features are normalised by subtraction of their median values
and division by their interquartile range.
</p>
</li>
<li> <p><code>mean_centering</code>: Features are centered by substracting the mean, but do
not undergo rescaling.
</p>
</li></ul>

<p>Only features that contain numerical data are normalised. Normalisation
parameters obtained in development data are stored within <code>featureInfo</code>
objects for later use with validation data sets.</p>
</td></tr>
<tr><td><code id=".parse_preprocessing_settings_+3A_batch_normalisation_method">batch_normalisation_method</code></td>
<td>
<p>(<em>optional</em>) The method used for batch
normalisation. Available methods are:
</p>

<ul>
<li> <p><code>none</code> (default): This disables batch normalisation of features.
</p>
</li>
<li> <p><code>standardisation</code>: Features within each batch are normalised by
subtraction of the mean value and division by the standard deviation in
each batch.
</p>
</li>
<li> <p><code>standardisation_trim</code>: As <code>standardisation</code>, but based on the set of
feature values where the 5% lowest and 5% highest values are discarded.
This reduces the effect of outliers.
</p>
</li>
<li> <p><code>standardisation_winsor</code>: As <code>standardisation</code>, but based on the set of
feature values where the 5% lowest and 5% highest values are winsorised.
This reduces the effect of outliers.
</p>
</li>
<li> <p><code>standardisation_robust</code>: A robust version of <code>standardisation</code> that
relies on computing Huber's M-estimators for location and scale within each
batch.
</p>
</li>
<li> <p><code>normalisation</code>: Features within each batch are normalised by subtraction
of their minimum values and division by their range in each batch. This
maps all feature values in each batch to a <code class="reqn">[0, 1]</code> interval.
</p>
</li>
<li> <p><code>normalisation_trim</code>: As <code>normalisation</code>, but based on the set of feature
values where the 5% lowest and 5% highest values are discarded. This
reduces the effect of outliers.
</p>
</li>
<li> <p><code>normalisation_winsor</code>: As <code>normalisation</code>, but based on the set of
feature values where the 5% lowest and 5% highest values are winsorised.
This reduces the effect of outliers.
</p>
</li>
<li> <p><code>quantile</code>: Features in each batch are normalised by subtraction of the
median value and division by the interquartile range of each batch.
</p>
</li>
<li> <p><code>mean_centering</code>: Features in each batch are centered on 0.0 by
substracting the mean value in each batch, but are not rescaled.
</p>
</li>
<li> <p><code>combat_parametric</code>: Batch adjustments using parametric empirical Bayes
(Johnson et al, 2007). <code>combat_p</code> leads to the same method.
</p>
</li>
<li> <p><code>combat_non_parametric</code>: Batch adjustments using non-parametric empirical
Bayes (Johnson et al, 2007). <code>combat_np</code> and <code>combat</code> lead to the same
method. Note that we reduced complexity from O(<code class="reqn">n^2</code>) to O(<code class="reqn">n</code>) by
only computing batch adjustment parameters for each feature on a subset of
50 randomly selected features, instead of all features.
</p>
</li></ul>

<p>Only features that contain numerical data are normalised using batch
normalisation. Batch normalisation parameters obtained in development data
are stored within <code>featureInfo</code> objects for later use with validation data
sets, in case the validation data is from the same batch.
</p>
<p>If validation data contains data from unknown batches, normalisation
parameters are separately determined for these batches.
</p>
<p>Note that for both empirical Bayes methods, the batch effect is assumed to
produce results across the features. This is often true for things such as
gene expressions, but the assumption may not hold generally.
</p>
<p>When performing batch normalisation, it is moreover important to check that
differences between batches or cohorts are not related to the studied
endpoint.</p>
</td></tr>
<tr><td><code id=".parse_preprocessing_settings_+3A_imputation_method">imputation_method</code></td>
<td>
<p>(<em>optional</em>) Method used for imputing missing
feature values. Two methods are implemented:
</p>

<ul>
<li> <p><code>simple</code>: Simple replacement of a missing value by the median value (for
numeric features) or the modal value (for categorical features).
</p>
</li>
<li> <p><code>lasso</code>: Imputation of missing value by lasso regression (using <code>glmnet</code>)
based on information contained in other features.
</p>
</li></ul>

<p><code>simple</code> imputation precedes <code>lasso</code> imputation to ensure that any missing
values in predictors required for <code>lasso</code> regression are resolved. The
<code>lasso</code> estimate is then used to replace the missing value.
</p>
<p>The default value depends on the number of features in the dataset. If the
number is lower than 100, <code>lasso</code> is used by default, and <code>simple</code>
otherwise.
</p>
<p>Only single imputation is performed. Imputation models and parameters are
stored within <code>featureInfo</code> objects for later use with validation data
sets.</p>
</td></tr>
<tr><td><code id=".parse_preprocessing_settings_+3A_cluster_method">cluster_method</code></td>
<td>
<p>(<em>optional</em>) Clustering is performed to identify and
replace redundant features, for example those that are highly correlated.
Such features do not carry much additional information and may be removed
or replaced instead (Park et al., 2007; Tolosi and Lengauer, 2011).
</p>
<p>The cluster method determines the algorithm used to form the clusters. The
following cluster methods are implemented:
</p>

<ul>
<li> <p><code>none</code>: No clustering is performed.
</p>
</li>
<li> <p><code>hclust</code> (default): Hierarchical agglomerative clustering. If the
<code>fastcluster</code> package is installed, <code>fastcluster::hclust</code> is used (Muellner
2013), otherwise <code>stats::hclust</code> is used.
</p>
</li>
<li> <p><code>agnes</code>: Hierarchical clustering using agglomerative nesting (Kaufman and
Rousseeuw, 1990). This algorithm is similar to <code>hclust</code>, but uses the
<code>cluster::agnes</code> implementation.
</p>
</li>
<li> <p><code>diana</code>: Divisive analysis hierarchical clustering. This method uses
divisive instead of agglomerative clustering (Kaufman and Rousseeuw, 1990).
<code>cluster::diana</code> is used.
</p>
</li>
<li> <p><code>pam</code>: Partioning around medioids. This partitions the data into $k$
clusters around medioids (Kaufman and Rousseeuw, 1990). $k$ is selected
using the <code>silhouette</code> metric. <code>pam</code> is implemented using the
<code>cluster::pam</code> function.
</p>
</li></ul>

<p>Clusters and cluster information is stored within <code>featureInfo</code> objects for
later use with validation data sets. This enables reproduction of the same
clusters as formed in the development data set.</p>
</td></tr>
<tr><td><code id=".parse_preprocessing_settings_+3A_cluster_linkage_method">cluster_linkage_method</code></td>
<td>
<p>(<em>optional</em>) Linkage method used for
agglomerative clustering in <code>hclust</code> and <code>agnes</code>. The following linkage
methods can be used:
</p>

<ul>
<li> <p><code>average</code> (default): Average linkage.
</p>
</li>
<li> <p><code>single</code>: Single linkage.
</p>
</li>
<li> <p><code>complete</code>: Complete linkage.
</p>
</li>
<li> <p><code>weighted</code>: Weighted linkage, also known as McQuitty linkage.
</p>
</li>
<li> <p><code>ward</code>: Linkage using Ward's minimum variance method.
</p>
</li></ul>

<p><code>diana</code> and <code>pam</code> do not require a linkage method.</p>
</td></tr>
<tr><td><code id=".parse_preprocessing_settings_+3A_cluster_cut_method">cluster_cut_method</code></td>
<td>
<p>(<em>optional</em>) The method used to define the actual
clusters. The following methods can be used:
</p>

<ul>
<li> <p><code>silhouette</code>: Clusters are formed based on the silhouette score
(Rousseeuw, 1987). The average silhouette score is computed from 2 to
<code class="reqn">n</code> clusters, with <code class="reqn">n</code> the number of features. Clusters are only
formed if the average silhouette exceeds 0.50, which indicates reasonable
evidence for structure. This procedure may be slow if the number of
features is large (&gt;100s).
</p>
</li>
<li> <p><code>fixed_cut</code>: Clusters are formed by cutting the hierarchical tree at the
point indicated by the <code>cluster_similarity_threshold</code>, e.g. where features
in a cluster have an average Spearman correlation of 0.90. <code>fixed_cut</code> is
only available for <code>agnes</code>, <code>diana</code> and <code>hclust</code>.
</p>
</li>
<li> <p><code>dynamic_cut</code>: Dynamic cluster formation using the cutting algorithm in
the <code>dynamicTreeCut</code> package. This package should be installed to select
this option. <code>dynamic_cut</code> can only be used with <code>agnes</code> and <code>hclust</code>.
</p>
</li></ul>

<p>The default options are <code>silhouette</code> for partioning around medioids (<code>pam</code>)
and <code>fixed_cut</code> otherwise.</p>
</td></tr>
<tr><td><code id=".parse_preprocessing_settings_+3A_cluster_similarity_metric">cluster_similarity_metric</code></td>
<td>
<p>(<em>optional</em>) Clusters are formed based on
feature similarity. All features are compared in a pair-wise fashion to
compute similarity, for example correlation. The resulting similarity grid
is converted into a distance matrix that is subsequently used for
clustering. The following metrics are supported to compute pairwise
similarities:
</p>

<ul>
<li> <p><code>mutual_information</code> (default): normalised mutual information.
</p>
</li>
<li> <p><code>mcfadden_r2</code>: McFadden's pseudo R-squared (McFadden, 1974).
</p>
</li>
<li> <p><code>cox_snell_r2</code>: Cox and Snell's pseudo R-squared (Cox and Snell, 1989).
</p>
</li>
<li> <p><code>nagelkerke_r2</code>: Nagelkerke's pseudo R-squared (Nagelkerke, 1991).
</p>
</li>
<li> <p><code>spearman</code>: Spearman's rank order correlation.
</p>
</li>
<li> <p><code>kendall</code>: Kendall rank correlation.
</p>
</li>
<li> <p><code>pearson</code>: Pearson product-moment correlation.
</p>
</li></ul>

<p>The pseudo R-squared metrics can be used to assess similarity between mixed
pairs of numeric and categorical features, as these are based on the
log-likelihood of regression models. In <code>familiar</code>, the more informative
feature is used as the predictor and the other feature as the reponse
variable. In numeric-categorical pairs, the numeric feature is considered
to be more informative and is thus used as the predictor. In
categorical-categorical pairs, the feature with most levels is used as the
predictor.
</p>
<p>In case any of the classical correlation coefficients (<code>pearson</code>,
<code>spearman</code> and <code>kendall</code>) are used with (mixed) categorical features, the
categorical features are one-hot encoded and the mean correlation over all
resulting pairs is used as similarity.</p>
</td></tr>
<tr><td><code id=".parse_preprocessing_settings_+3A_cluster_similarity_threshold">cluster_similarity_threshold</code></td>
<td>
<p>(<em>optional</em>) The threshold level for
pair-wise similarity that is required to form clusters using <code>fixed_cut</code>.
This should be a numerical value between 0.0 and 1.0. Note however, that a
reasonable threshold value depends strongly on the similarity metric. The
following are the default values used:
</p>

<ul>
<li> <p><code>mcfadden_r2</code> and <code>mutual_information</code>: <code>0.30</code>
</p>
</li>
<li> <p><code>cox_snell_r2</code> and <code>nagelkerke_r2</code>: <code>0.75</code>
</p>
</li>
<li> <p><code>spearman</code>, <code>kendall</code> and <code>pearson</code>: <code>0.90</code>
</p>
</li></ul>

<p>Alternatively, if the <code style="white-space: pre;">&#8288;fixed cut&#8288;</code> method is not used, this value determines
whether any clustering should be performed, because the data may not
contain highly similar features. The default values in this situation are:
</p>

<ul>
<li> <p><code>mcfadden_r2</code>  and <code>mutual_information</code>: <code>0.25</code>
</p>
</li>
<li> <p><code>cox_snell_r2</code> and <code>nagelkerke_r2</code>: <code>0.40</code>
</p>
</li>
<li> <p><code>spearman</code>, <code>kendall</code> and <code>pearson</code>: <code>0.70</code>
</p>
</li></ul>

<p>The threshold value is converted to a distance (1-similarity) prior to
cutting hierarchical trees.</p>
</td></tr>
<tr><td><code id=".parse_preprocessing_settings_+3A_cluster_representation_method">cluster_representation_method</code></td>
<td>
<p>(<em>optional</em>) Method used to determine
how the information of co-clustered features is summarised and used to
represent the cluster. The following methods can be selected:
</p>

<ul>
<li> <p><code>best_predictor</code> (default): The feature with the highest importance
according to univariate regression with the outcome is used to represent
the cluster.
</p>
</li>
<li> <p><code>medioid</code>: The feature closest to the cluster center, i.e. the feature
that is most similar to the remaining features in the cluster, is used to
represent the feature.
</p>
</li>
<li> <p><code>mean</code>: A meta-feature is generated by averaging the feature values for
all features in a cluster. This method aligns all features so that all
features will be positively correlated prior to averaging. Should a cluster
contain one or more categorical features, the <code>medioid</code> method will be used
instead, as averaging is not possible. Note that if this method is chosen,
the <code>normalisation_method</code> parameter should be one of <code>standardisation</code>,
<code>standardisation_trim</code>, <code>standardisation_winsor</code> or <code>quantile</code>.'
</p>
</li></ul>

<p>If the <code>pam</code> cluster method is selected, only the <code>medioid</code> method can be
used. In that case 1 medioid is used by default.</p>
</td></tr>
<tr><td><code id=".parse_preprocessing_settings_+3A_parallel_preprocessing">parallel_preprocessing</code></td>
<td>
<p>(<em>optional</em>) Enable parallel processing for the
preprocessing workflow. Defaults to <code>TRUE</code>. When set to <code>FALSE</code>, this will
disable the use of parallel processing while preprocessing, regardless of
the settings of the <code>parallel</code> parameter. <code>parallel_preprocessing</code> is
ignored if <code>parallel=FALSE</code>.</p>
</td></tr>
<tr><td><code id=".parse_preprocessing_settings_+3A_...">...</code></td>
<td>
<p>Unused arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List of parameters related to preprocessing.
</p>


<h3>References</h3>


<ol>
<li><p> Storey, J. D. A direct approach to false discovery rates. J.
R. Stat. Soc. Series B Stat. Methodol. 64, 479–498 (2002).
</p>
</li>
<li><p> Shrout, P. E. &amp; Fleiss, J. L. Intraclass correlations: uses in assessing
rater reliability. Psychol. Bull. 86, 420–428 (1979).
</p>
</li>
<li><p> Koo, T. K. &amp; Li, M. Y. A guideline of selecting and reporting intraclass
correlation coefficients for reliability research. J. Chiropr. Med. 15,
155–163 (2016).
</p>
</li>
<li><p> Yeo, I. &amp; Johnson, R. A. A new family of power transformations to
improve normality or symmetry. Biometrika 87, 954–959 (2000).
</p>
</li>
<li><p> Box, G. E. P. &amp; Cox, D. R. An analysis of transformations. J. R. Stat.
Soc. Series B Stat. Methodol. 26, 211–252 (1964).
</p>
</li>
<li><p> Raymaekers, J., Rousseeuw,  P. J. Transforming variables to central
normality. Mach Learn. (2021).
</p>
</li>
<li><p> Park, M. Y., Hastie, T. &amp; Tibshirani, R. Averaged gene expressions for
regression. Biostatistics 8, 212–227 (2007).
</p>
</li>
<li><p> Tolosi, L. &amp; Lengauer, T. Classification with correlated features:
unreliability of feature ranking and solutions. Bioinformatics 27,
1986–1994 (2011).
</p>
</li>
<li><p> Johnson, W. E., Li, C. &amp; Rabinovic, A. Adjusting batch effects in
microarray expression data using empirical Bayes methods. Biostatistics 8,
118–127 (2007)
</p>
</li>
<li><p> Kaufman, L. &amp; Rousseeuw, P. J. Finding groups in data: an introduction
to cluster analysis. (John Wiley &amp; Sons, 2009).
</p>
</li>
<li><p> Muellner, D. fastcluster: fast hierarchical, agglomerative clustering
routines for R and Python. J. Stat. Softw. 53, 1–18 (2013).
</p>
</li>
<li><p> Rousseeuw, P. J. Silhouettes: A graphical aid to the interpretation and
validation of cluster analysis. J. Comput. Appl. Math. 20, 53–65 (1987).
</p>
</li>
<li><p> Langfelder, P., Zhang, B. &amp; Horvath, S. Defining clusters from a
hierarchical cluster tree: the Dynamic Tree Cut package for R.
Bioinformatics 24, 719–720 (2008).
</p>
</li>
<li><p> McFadden, D. Conditional logit analysis of qualitative choice behavior.
in Frontiers in Econometrics (ed. Zarembka, P.) 105–142 (Academic Press,
1974).
</p>
</li>
<li><p> Cox, D. R. &amp; Snell, E. J. Analysis of binary data. (Chapman and Hall,
1989).
</p>
</li>
<li><p> Nagelkerke, N. J. D. A note on a general definition of the coefficient
of determination. Biometrika 78, 691–692 (1991).
</p>
</li></ol>


<hr>
<h2 id='.parse_setup_settings'>Internal function for parsing settings related to the computational setup</h2><span id='topic+.parse_setup_settings'></span>

<h3>Description</h3>

<p>Internal function for parsing settings related to the computational setup
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.parse_setup_settings(
  config = NULL,
  parallel = waiver(),
  parallel_nr_cores = waiver(),
  restart_cluster = waiver(),
  cluster_type = waiver(),
  backend_type = waiver(),
  server_port = waiver(),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".parse_setup_settings_+3A_config">config</code></td>
<td>
<p>A list of settings, e.g. from an xml file.</p>
</td></tr>
<tr><td><code id=".parse_setup_settings_+3A_parallel">parallel</code></td>
<td>
<p>(<em>optional</em>) Enable parallel processing. Defaults to <code>TRUE</code>.
When set to <code>FALSE</code>, this disables all parallel processing, regardless of
specific parameters such as <code>parallel_preprocessing</code>. However, when
<code>parallel</code> is <code>TRUE</code>, parallel processing of different parts of the
workflow can be disabled by setting respective flags to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id=".parse_setup_settings_+3A_parallel_nr_cores">parallel_nr_cores</code></td>
<td>
<p>(<em>optional</em>) Number of cores available for
parallelisation. Defaults to 2. This setting does nothing if
parallelisation is disabled.</p>
</td></tr>
<tr><td><code id=".parse_setup_settings_+3A_restart_cluster">restart_cluster</code></td>
<td>
<p>(<em>optional</em>) Restart nodes used for parallel computing
to free up memory prior to starting a parallel process. Note that it does
take time to set up the clusters. Therefore setting this argument to <code>TRUE</code>
may impact processing speed. This argument is ignored if <code>parallel</code> is
<code>FALSE</code> or the cluster was initialised outside of familiar. Default is
<code>FALSE</code>, which causes the clusters to be initialised only once.</p>
</td></tr>
<tr><td><code id=".parse_setup_settings_+3A_cluster_type">cluster_type</code></td>
<td>
<p>(<em>optional</em>) Selection of the cluster type for parallel
processing. Available types are the ones supported by the parallel package
that is part of the base R distribution: <code>psock</code> (default), <code>fork</code>, <code>mpi</code>,
<code>nws</code>, <code>sock</code>. In addition, <code>none</code> is available, which also disables
parallel processing.</p>
</td></tr>
<tr><td><code id=".parse_setup_settings_+3A_backend_type">backend_type</code></td>
<td>
<p>(<em>optional</em>) Selection of the backend for distributing
copies of the data. This backend ensures that only a single master copy is
kept in memory. This limits memory usage during parallel processing.
</p>
<p>Several backend options are available, notably <code>socket_server</code>, and <code>none</code>
(default). <code>socket_server</code> is based on the callr package and R sockets,
comes with <code>familiar</code> and is available for any OS. <code>none</code> uses the package
environment of familiar to store data, and is available for any OS.
However, <code>none</code> requires copying of data to any parallel process, and has a
larger memory footprint.</p>
</td></tr>
<tr><td><code id=".parse_setup_settings_+3A_server_port">server_port</code></td>
<td>
<p>(<em>optional</em>) Integer indicating the port on which the
socket server or RServe process should communicate. Defaults to port 6311.
Note that ports 0 to 1024 and 49152 to 65535 cannot be used.</p>
</td></tr>
<tr><td><code id=".parse_setup_settings_+3A_...">...</code></td>
<td>
<p>Unused arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List of parameters related to the computational setup.
</p>

<hr>
<h2 id='.plot_permutation_variable_importance'>Internal plotting function for permutation variable importance plots</h2><span id='topic+.plot_permutation_variable_importance'></span>

<h3>Description</h3>

<p>Internal plotting function for permutation variable importance plots
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.plot_permutation_variable_importance(
  x,
  color_by,
  facet_by,
  facet_wrap_cols,
  ggtheme,
  discrete_palette,
  x_label,
  y_label,
  legend_label,
  plot_title,
  plot_sub_title,
  caption,
  conf_int_style,
  conf_int_alpha,
  x_range,
  x_breaks
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".plot_permutation_variable_importance_+3A_color_by">color_by</code></td>
<td>
<p>(<em>optional</em>) Variables used to determine fill colour of plot
objects. The variables cannot overlap with those provided to the <code>split_by</code>
argument, but may overlap with other arguments. See details for available
variables.</p>
</td></tr>
<tr><td><code id=".plot_permutation_variable_importance_+3A_facet_by">facet_by</code></td>
<td>
<p>(<em>optional</em>) Variables used to determine how and if facets of
each figure appear. In case the <code>facet_wrap_cols</code> argument is <code>NULL</code>, the
first variable is used to define columns, and the remaing variables are
used to define rows of facets. The variables cannot overlap with those
provided to the <code>split_by</code> argument, but may overlap with other arguments.
See details for available variables.</p>
</td></tr>
<tr><td><code id=".plot_permutation_variable_importance_+3A_facet_wrap_cols">facet_wrap_cols</code></td>
<td>
<p>(<em>optional</em>) Number of columns to generate when facet
wrapping. If NULL, a facet grid is produced instead.</p>
</td></tr>
<tr><td><code id=".plot_permutation_variable_importance_+3A_ggtheme">ggtheme</code></td>
<td>
<p>(<em>optional</em>) <code>ggplot</code> theme to use for plotting.</p>
</td></tr>
<tr><td><code id=".plot_permutation_variable_importance_+3A_discrete_palette">discrete_palette</code></td>
<td>
<p>(<em>optional</em>) Palette used to fill the bars in case a
non-singular variable was provided to the <code>color_by</code> argument.</p>
</td></tr>
<tr><td><code id=".plot_permutation_variable_importance_+3A_x_label">x_label</code></td>
<td>
<p>(<em>optional</em>) Label to provide to the x-axis. If NULL, no label
is shown.</p>
</td></tr>
<tr><td><code id=".plot_permutation_variable_importance_+3A_y_label">y_label</code></td>
<td>
<p>(<em>optional</em>) Label to provide to the y-axis. If NULL, no label
is shown.</p>
</td></tr>
<tr><td><code id=".plot_permutation_variable_importance_+3A_legend_label">legend_label</code></td>
<td>
<p>(<em>optional</em>) Label to provide to the legend. If NULL, the
legend will not have a name.</p>
</td></tr>
<tr><td><code id=".plot_permutation_variable_importance_+3A_plot_title">plot_title</code></td>
<td>
<p>(<em>optional</em>) Label to provide as figure title. If NULL, no
title is shown.</p>
</td></tr>
<tr><td><code id=".plot_permutation_variable_importance_+3A_plot_sub_title">plot_sub_title</code></td>
<td>
<p>(<em>optional</em>) Label to provide as figure subtitle. If
NULL, no subtitle is shown.</p>
</td></tr>
<tr><td><code id=".plot_permutation_variable_importance_+3A_caption">caption</code></td>
<td>
<p>(<em>optional</em>) Label to provide as figure caption. If NULL, no
caption is shown.</p>
</td></tr>
<tr><td><code id=".plot_permutation_variable_importance_+3A_conf_int_style">conf_int_style</code></td>
<td>
<p>(<em>optional</em>) Confidence interval style. See details for
allowed styles.</p>
</td></tr>
<tr><td><code id=".plot_permutation_variable_importance_+3A_conf_int_alpha">conf_int_alpha</code></td>
<td>
<p>(<em>optional</em>) Alpha value to determine transparency of
confidence intervals or, alternatively, other plot elements with which the
confidence interval overlaps. Only values between 0.0 (fully transparent)
and 1.0 (fully opaque) are allowed.</p>
</td></tr>
<tr><td><code id=".plot_permutation_variable_importance_+3A_x_range">x_range</code></td>
<td>
<p>(<em>optional</em>) Value range for the x-axis.</p>
</td></tr>
<tr><td><code id=".plot_permutation_variable_importance_+3A_x_breaks">x_breaks</code></td>
<td>
<p>(<em>optional</em>) Break points on the x-axis of the plot.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>ggplot plot object.
</p>

<hr>
<h2 id='.plot_univariate_importance'>Internal plotting function for univariate plots</h2><span id='topic+.plot_univariate_importance'></span>

<h3>Description</h3>

<p>Internal plotting function for univariate plots
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.plot_univariate_importance(
  x,
  color_by,
  facet_by,
  facet_wrap_cols,
  ggtheme,
  show_cluster,
  discrete_palette,
  gradient_palette,
  x_label,
  y_label,
  legend_label,
  plot_title,
  plot_sub_title,
  caption,
  x_range,
  x_breaks,
  significance_level_shown
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".plot_univariate_importance_+3A_color_by">color_by</code></td>
<td>
<p>(<em>optional</em>) Variables used to determine fill colour of plot
objects. The variables cannot overlap with those provided to the <code>split_by</code>
argument, but may overlap with other arguments. See details for available
variables.</p>
</td></tr>
<tr><td><code id=".plot_univariate_importance_+3A_facet_by">facet_by</code></td>
<td>
<p>(<em>optional</em>) Variables used to determine how and if facets of
each figure appear. In case the <code>facet_wrap_cols</code> argument is <code>NULL</code>, the
first variable is used to define columns, and the remaing variables are
used to define rows of facets. The variables cannot overlap with those
provided to the <code>split_by</code> argument, but may overlap with other arguments.
See details for available variables.</p>
</td></tr>
<tr><td><code id=".plot_univariate_importance_+3A_facet_wrap_cols">facet_wrap_cols</code></td>
<td>
<p>(<em>optional</em>) Number of columns to generate when facet
wrapping. If NULL, a facet grid is produced instead.</p>
</td></tr>
<tr><td><code id=".plot_univariate_importance_+3A_ggtheme">ggtheme</code></td>
<td>
<p>(<em>optional</em>) <code>ggplot</code> theme to use for plotting.</p>
</td></tr>
<tr><td><code id=".plot_univariate_importance_+3A_show_cluster">show_cluster</code></td>
<td>
<p>(<em>optional</em>) Show which features were clustered together.</p>
</td></tr>
<tr><td><code id=".plot_univariate_importance_+3A_discrete_palette">discrete_palette</code></td>
<td>
<p>(<em>optional</em>) Palette used to fill the bars in case a
non-singular variable was provided to the <code>color_by</code> argument.</p>
</td></tr>
<tr><td><code id=".plot_univariate_importance_+3A_gradient_palette">gradient_palette</code></td>
<td>
<p>(<em>optional</em>) Palette to use for filling the bars in
case the <code>color_by</code> argument is not set. The bars are then coloured
according to their importance. By default, no gradient is used, and the bars
are not filled according to importance. Use <code>NULL</code> to fill the bars using
the default palette in <code>familiar</code>.</p>
</td></tr>
<tr><td><code id=".plot_univariate_importance_+3A_x_label">x_label</code></td>
<td>
<p>(<em>optional</em>) Label to provide to the x-axis. If NULL, no label
is shown.</p>
</td></tr>
<tr><td><code id=".plot_univariate_importance_+3A_y_label">y_label</code></td>
<td>
<p>(<em>optional</em>) Label to provide to the y-axis. If NULL, no label
is shown.</p>
</td></tr>
<tr><td><code id=".plot_univariate_importance_+3A_legend_label">legend_label</code></td>
<td>
<p>(<em>optional</em>) Label to provide to the legend. If NULL, the
legend will not have a name.</p>
</td></tr>
<tr><td><code id=".plot_univariate_importance_+3A_plot_title">plot_title</code></td>
<td>
<p>(<em>optional</em>) Label to provide as figure title. If NULL, no
title is shown.</p>
</td></tr>
<tr><td><code id=".plot_univariate_importance_+3A_plot_sub_title">plot_sub_title</code></td>
<td>
<p>(<em>optional</em>) Label to provide as figure subtitle. If
NULL, no subtitle is shown.</p>
</td></tr>
<tr><td><code id=".plot_univariate_importance_+3A_caption">caption</code></td>
<td>
<p>(<em>optional</em>) Label to provide as figure caption. If NULL, no
caption is shown.</p>
</td></tr>
<tr><td><code id=".plot_univariate_importance_+3A_x_range">x_range</code></td>
<td>
<p>(<em>optional</em>) Value range for the x-axis.</p>
</td></tr>
<tr><td><code id=".plot_univariate_importance_+3A_x_breaks">x_breaks</code></td>
<td>
<p>(<em>optional</em>) Break points on the x-axis of the plot.</p>
</td></tr>
<tr><td><code id=".plot_univariate_importance_+3A_significance_level_shown">significance_level_shown</code></td>
<td>
<p>Position(s) to draw vertical lines indicating
a significance level, e.g. 0.05. Can be NULL to not draw anything.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>ggplot plot object.
</p>


<h3>See Also</h3>


<ul>
<li> <p><code><a href="#topic+plot_univariate_importance">plot_univariate_importance</a></code> for the user interface.
</p>
</li></ul>


<hr>
<h2 id='.prepare_familiar_data_sets'>Prepare familiarData objects for evaluation at runtime.</h2><span id='topic+.prepare_familiar_data_sets'></span>

<h3>Description</h3>

<p>Information concerning models, features and the experiment is
processed and stored in familiarData objects. Information can be extracted
from these objects as csv files, or by plotting, or multiple objects can be
combined into familiarCollection objects, which allows aggregated exports.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.prepare_familiar_data_sets(
  cl = NULL,
  only_pooling = FALSE,
  message_indent = 0L,
  verbose = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".prepare_familiar_data_sets_+3A_cl">cl</code></td>
<td>
<p>Cluster for parallel processing.</p>
</td></tr>
<tr><td><code id=".prepare_familiar_data_sets_+3A_only_pooling">only_pooling</code></td>
<td>
<p>Flag that, if set, forces evaluation of only the
top-level data, and not e.g. ensembles.</p>
</td></tr>
<tr><td><code id=".prepare_familiar_data_sets_+3A_message_indent">message_indent</code></td>
<td>
<p>indent that messages should have.</p>
</td></tr>
<tr><td><code id=".prepare_familiar_data_sets_+3A_verbose">verbose</code></td>
<td>
<p>Sets verbosity</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function generates the names of familiarData object files, and
their corresponding generating ensemble, which allows the familiarData
objects to be created.
</p>


<h3>Value</h3>

<p>A data.table with created links to created data objects.
</p>

<hr>
<h2 id='.update_experimental_design_settings'>Internal function to check batch assignment to development and validation</h2><span id='topic+.update_experimental_design_settings'></span>

<h3>Description</h3>

<p>This function checks which batches in the data set are assigned to model
development and external validation. Several errors may be raised if there
are inconsistencies such as an overlapping assignment, name mismatches etc.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.update_experimental_design_settings(section_table, data, settings)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".update_experimental_design_settings_+3A_section_table">section_table</code></td>
<td>
<p>data.table generated by the <code>extract_experimental_setup</code>
function. Contains information regarding the experiment.</p>
</td></tr>
<tr><td><code id=".update_experimental_design_settings_+3A_data">data</code></td>
<td>
<p>Data set as loaded using the <code>.load_data</code> function.</p>
</td></tr>
<tr><td><code id=".update_experimental_design_settings_+3A_settings">settings</code></td>
<td>
<p>List of parameter settings for data set parsing and setting
up the experiment.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A verified and updated list of parameter settings.
</p>

<hr>
<h2 id='.update_initial_settings'>Internal check and update of settings related to data set parsing</h2><span id='topic+.update_initial_settings'></span>

<h3>Description</h3>

<p>This function updates and checks parameters related to data set parsing based
on the available data set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>.update_initial_settings(
  formula = NULL,
  data,
  settings,
  check_stringency = "strict"
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id=".update_initial_settings_+3A_formula">formula</code></td>
<td>
<p>User-provided formula, may be absent (<code>NULL</code>).</p>
</td></tr>
<tr><td><code id=".update_initial_settings_+3A_data">data</code></td>
<td>
<p>Data set as loaded using the <code>.load_data</code> function.</p>
</td></tr>
<tr><td><code id=".update_initial_settings_+3A_settings">settings</code></td>
<td>
<p>List of parameter settings for data set parsing.</p>
</td></tr>
<tr><td><code id=".update_initial_settings_+3A_check_stringency">check_stringency</code></td>
<td>
<p>Specifies stringency of various checks. This is mostly:
</p>

<ul>
<li> <p><code>strict</code>: default value used for <code>summon_familiar</code>. Thoroughly checks
input data. Used internally for checking development data.
</p>
</li>
<li> <p><code>external_warn</code>: value used for <code>extract_data</code> and related methods. Less
stringent checks, but will warn for possible issues. Used internally for
checking data for evaluation and explanation.
</p>
</li>
<li> <p><code>external</code>: value used for external methods such as <code>predict</code>. Less
stringent checks, particularly for identifier and outcome columns, which may
be completely absent. Used internally for <code>predict</code>.
</p>
</li></ul>
</td></tr>
</table>


<h3>Value</h3>

<p>A verified and updated list of parameter settings.
</p>

<hr>
<h2 id='aggregate_vimp_table'>Aggregate variable importance from multiple variable importance
objects.</h2><span id='topic+aggregate_vimp_table'></span><span id='topic+aggregate_vimp_table+2Clist-method'></span><span id='topic+aggregate_vimp_table+2Ccharacter-method'></span><span id='topic+aggregate_vimp_table+2CvimpTable-method'></span><span id='topic+aggregate_vimp_table+2CNULL-method'></span><span id='topic+aggregate_vimp_table+2CexperimentData-method'></span>

<h3>Description</h3>

<p>This methods aggregates variable importance from one or more
<code>vimpTable</code> objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>aggregate_vimp_table(x, aggregation_method, rank_threshold = NULL, ...)

## S4 method for signature 'list'
aggregate_vimp_table(x, aggregation_method, rank_threshold = NULL, ...)

## S4 method for signature 'character'
aggregate_vimp_table(x, aggregation_method, rank_threshold = NULL, ...)

## S4 method for signature 'vimpTable'
aggregate_vimp_table(x, aggregation_method, rank_threshold = NULL, ...)

## S4 method for signature ''NULL''
aggregate_vimp_table(x, aggregation_method, rank_threshold = NULL, ...)

## S4 method for signature 'experimentData'
aggregate_vimp_table(x, aggregation_method, rank_threshold = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="aggregate_vimp_table_+3A_x">x</code></td>
<td>
<p>Variable importance (<code>vimpTable</code>) object, a list thereof, or one or
more paths to these objects.</p>
</td></tr>
<tr><td><code id="aggregate_vimp_table_+3A_aggregation_method">aggregation_method</code></td>
<td>
<p>Method used to aggregate variable importance. The
available methods are described in the <em>feature selection methods</em> vignette.</p>
</td></tr>
<tr><td><code id="aggregate_vimp_table_+3A_rank_threshold">rank_threshold</code></td>
<td>
<p>Rank threshold used within several aggregation methods.
See the <em>feature selection methods</em> vignette for more details.</p>
</td></tr>
<tr><td><code id="aggregate_vimp_table_+3A_...">...</code></td>
<td>
<p>unused parameters.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>vimpTable</code> object with aggregated variable importance data.
</p>

<hr>
<h2 id='as_data_object'>Creates a valid data object from input data.</h2><span id='topic+as_data_object'></span><span id='topic+as_data_object+2CdataObject-method'></span><span id='topic+as_data_object+2Cdata.table-method'></span><span id='topic+as_data_object+2CANY-method'></span>

<h3>Description</h3>

<p>Creates <code>dataObject</code> a object from input data. Input data can be
a <code>data.frame</code> or <code>data.table</code>, a path to such tables on a local or network
drive, or a path to tabular data that may be converted to these formats.
</p>
<p>In addition, a <code>familiarEnsemble</code> or <code>familiarModel</code> object can be passed
along to check whether the data are formatted correctly, e.g. by checking
the levels of categorical features, whether all expected columns are
present, etc.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as_data_object(data, ...)

## S4 method for signature 'dataObject'
as_data_object(data, object = NULL, ...)

## S4 method for signature 'data.table'
as_data_object(
  data,
  object = NULL,
  sample_id_column = waiver(),
  batch_id_column = waiver(),
  series_id_column = waiver(),
  development_batch_id = waiver(),
  validation_batch_id = waiver(),
  outcome_name = waiver(),
  outcome_column = waiver(),
  outcome_type = waiver(),
  event_indicator = waiver(),
  censoring_indicator = waiver(),
  competing_risk_indicator = waiver(),
  class_levels = waiver(),
  exclude_features = waiver(),
  include_features = waiver(),
  reference_method = waiver(),
  check_stringency = "strict",
  ...
)

## S4 method for signature 'ANY'
as_data_object(
  data,
  object = NULL,
  sample_id_column = waiver(),
  batch_id_column = waiver(),
  series_id_column = waiver(),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="as_data_object_+3A_data">data</code></td>
<td>
<p>A <code>data.frame</code> or <code>data.table</code>, a path to such tables on a local
or network drive, or a path to tabular data that may be converted to these
formats.</p>
</td></tr>
<tr><td><code id="as_data_object_+3A_...">...</code></td>
<td>
<p>Unused arguments.</p>
</td></tr>
<tr><td><code id="as_data_object_+3A_object">object</code></td>
<td>
<p>A <code>familiarEnsemble</code> or <code>familiarModel</code> object that is used to
check consistency of these objects.</p>
</td></tr>
<tr><td><code id="as_data_object_+3A_sample_id_column">sample_id_column</code></td>
<td>
<p>(<strong>recommended</strong>) Name of the column containing
sample or subject identifiers. See <code>batch_id_column</code> above for more
details.
</p>
<p>If unset, every row will be identified as a single sample.</p>
</td></tr>
<tr><td><code id="as_data_object_+3A_batch_id_column">batch_id_column</code></td>
<td>
<p>(<strong>recommended</strong>) Name of the column containing batch
or cohort identifiers. This parameter is required if more than one dataset
is provided, or if external validation is performed.
</p>
<p>In familiar any row of data is organised by four identifiers:
</p>

<ul>
<li><p> The batch identifier <code>batch_id_column</code>: This denotes the group to which a
set of samples belongs, e.g. patients from a single study, samples measured
in a batch, etc. The batch identifier is used for batch normalisation, as
well as selection of development and validation datasets.
</p>
</li>
<li><p> The sample identifier <code>sample_id_column</code>: This denotes the sample level,
e.g. data from a single individual. Subsets of data, e.g. bootstraps or
cross-validation folds, are created at this level.
</p>
</li>
<li><p> The series identifier <code>series_id_column</code>: Indicates measurements on a
single sample that may not share the same outcome value, e.g. a time
series, or the number of cells in a view.
</p>
</li>
<li><p> The repetition identifier: Indicates repeated measurements in a single
series where any feature values may differ, but the outcome does not.
Repetition identifiers are always implicitly set when multiple entries for
the same series of the same sample in the same batch that share the same
outcome are encountered.
</p>
</li></ul>
</td></tr>
<tr><td><code id="as_data_object_+3A_series_id_column">series_id_column</code></td>
<td>
<p>(<strong>optional</strong>) Name of the column containing series
identifiers, which distinguish between measurements that are part of a
series for a single sample. See <code>batch_id_column</code> above for more details.
</p>
<p>If unset, rows which share the same batch and sample identifiers but have a
different outcome are assigned unique series identifiers.</p>
</td></tr>
<tr><td><code id="as_data_object_+3A_development_batch_id">development_batch_id</code></td>
<td>
<p>(<em>optional</em>) One or more batch or cohort
identifiers to constitute data sets for development. Defaults to all, or
all minus the identifiers in <code>validation_batch_id</code> for external validation.
Required if external validation is performed and <code>validation_batch_id</code> is
not provided.</p>
</td></tr>
<tr><td><code id="as_data_object_+3A_validation_batch_id">validation_batch_id</code></td>
<td>
<p>(<em>optional</em>) One or more batch or cohort
identifiers to constitute data sets for external validation. Defaults to
all data sets except those in <code>development_batch_id</code> for external
validation, or none if not. Required if <code>development_batch_id</code> is not
provided.</p>
</td></tr>
<tr><td><code id="as_data_object_+3A_outcome_name">outcome_name</code></td>
<td>
<p>(<em>optional</em>) Name of the modelled outcome. This name will
be used in figures created by <code>familiar</code>.
</p>
<p>If not set, the column name in <code>outcome_column</code> will be used for
<code>binomial</code>, <code>multinomial</code>, <code>count</code> and <code>continuous</code> outcomes. For other
outcomes (<code>survival</code> and <code>competing_risk</code>) no default is used.</p>
</td></tr>
<tr><td><code id="as_data_object_+3A_outcome_column">outcome_column</code></td>
<td>
<p>(<strong>recommended</strong>) Name of the column containing the
outcome of interest. May be identified from a formula, if a formula is
provided as an argument. Otherwise an error is raised. Note that <code>survival</code>
and <code>competing_risk</code> outcome type outcomes require two columns that
indicate the time-to-event or the time of last follow-up and the event
status.</p>
</td></tr>
<tr><td><code id="as_data_object_+3A_outcome_type">outcome_type</code></td>
<td>
<p>(<strong>recommended</strong>) Type of outcome found in the outcome
column. The outcome type determines many aspects of the overall process,
e.g. the available feature selection methods and learners, but also the
type of assessments that can be conducted to evaluate the resulting models.
Implemented outcome types are:
</p>

<ul>
<li> <p><code>binomial</code>: categorical outcome with 2 levels.
</p>
</li>
<li> <p><code>multinomial</code>: categorical outcome with 2 or more levels.
</p>
</li>
<li> <p><code>count</code>: Poisson-distributed numeric outcomes.
</p>
</li>
<li> <p><code>continuous</code>: general continuous numeric outcomes.
</p>
</li>
<li> <p><code>survival</code>: survival outcome for time-to-event data.
</p>
</li></ul>

<p>If not provided, the algorithm will attempt to obtain outcome_type from
contents of the outcome column. This may lead to unexpected results, and we
therefore advise to provide this information manually.
</p>
<p>Note that <code>competing_risk</code> survival analysis are not fully supported, and
is currently not a valid choice for <code>outcome_type</code>.</p>
</td></tr>
<tr><td><code id="as_data_object_+3A_event_indicator">event_indicator</code></td>
<td>
<p>(<strong>recommended</strong>) Indicator for events in <code>survival</code>
and <code>competing_risk</code> analyses. <code>familiar</code> will automatically recognise <code>1</code>,
<code>true</code>, <code>t</code>, <code>y</code> and <code>yes</code> as event indicators, including different
capitalisations. If this parameter is set, it replaces the default values.</p>
</td></tr>
<tr><td><code id="as_data_object_+3A_censoring_indicator">censoring_indicator</code></td>
<td>
<p>(<strong>recommended</strong>) Indicator for right-censoring in
<code>survival</code> and <code>competing_risk</code> analyses. <code>familiar</code> will automatically
recognise <code>0</code>, <code>false</code>, <code>f</code>, <code>n</code>, <code>no</code> as censoring indicators, including
different capitalisations. If this parameter is set, it replaces the
default values.</p>
</td></tr>
<tr><td><code id="as_data_object_+3A_competing_risk_indicator">competing_risk_indicator</code></td>
<td>
<p>(<strong>recommended</strong>) Indicator for competing
risks in <code>competing_risk</code> analyses. There are no default values, and if
unset, all values other than those specified by the <code>event_indicator</code> and
<code>censoring_indicator</code> parameters are considered to indicate competing
risks.</p>
</td></tr>
<tr><td><code id="as_data_object_+3A_class_levels">class_levels</code></td>
<td>
<p>(<em>optional</em>) Class levels for <code>binomial</code> or <code>multinomial</code>
outcomes. This argument can be used to specify the ordering of levels for
categorical outcomes. These class levels must exactly match the levels
present in the outcome column.</p>
</td></tr>
<tr><td><code id="as_data_object_+3A_exclude_features">exclude_features</code></td>
<td>
<p>(<em>optional</em>) Feature columns that will be removed
from the data set. Cannot overlap with features in <code>signature</code>,
<code>novelty_features</code> or <code>include_features</code>.</p>
</td></tr>
<tr><td><code id="as_data_object_+3A_include_features">include_features</code></td>
<td>
<p>(<em>optional</em>) Feature columns that are specifically
included in the data set. By default all features are included. Cannot
overlap with <code>exclude_features</code>, but may overlap <code>signature</code>. Features in
<code>signature</code> and <code>novelty_features</code> are always included. If both
<code>exclude_features</code> and <code>include_features</code> are provided, <code>include_features</code>
takes precedence, provided that there is no overlap between the two.</p>
</td></tr>
<tr><td><code id="as_data_object_+3A_reference_method">reference_method</code></td>
<td>
<p>(<em>optional</em>) Method used to set reference levels for
categorical features. There are several options:
</p>

<ul>
<li> <p><code>auto</code> (default): Categorical features that are not explicitly set by the
user, i.e. columns containing boolean values or characters, use the most
frequent level as reference. Categorical features that are explicitly set,
i.e. as factors, are used as is.
</p>
</li>
<li> <p><code>always</code>: Both automatically detected and user-specified categorical
features have the reference level set to the most frequent level. Ordinal
features are not altered, but are used as is.
</p>
</li>
<li> <p><code>never</code>: User-specified categorical features are used as is.
Automatically detected categorical features are simply sorted, and the
first level is then used as the reference level. This was the behaviour
prior to familiar version 1.3.0.
</p>
</li></ul>
</td></tr>
<tr><td><code id="as_data_object_+3A_check_stringency">check_stringency</code></td>
<td>
<p>Specifies stringency of various checks. This is mostly:
</p>

<ul>
<li> <p><code>strict</code>: default value used for <code>summon_familiar</code>. Thoroughly checks
input data. Used internally for checking development data.
</p>
</li>
<li> <p><code>external_warn</code>: value used for <code>extract_data</code> and related methods. Less
stringent checks, but will warn for possible issues. Used internally for
checking data for evaluation and explanation.
</p>
</li>
<li> <p><code>external</code>: value used for external methods such as <code>predict</code>. Less
stringent checks, particularly for identifier and outcome columns, which may
be completely absent. Used internally for <code>predict</code>.
</p>
</li></ul>
</td></tr>
</table>


<h3>Details</h3>

<p>You can specify settings for your data manually, e.g. the column for
sample identifiers (<code>sample_id_column</code>). This prevents you from having to
change the column name externally. In the case you provide a <code>familiarModel</code>
or <code>familiarEnsemble</code> for the <code>object</code> argument, any parameters you provide
take precedence over parameters specified by the object.
</p>


<h3>Value</h3>

<p>A <code>dataObject</code> object.
</p>

<hr>
<h2 id='as_familiar_collection'>Conversion to familiarCollection object.</h2><span id='topic+as_familiar_collection'></span><span id='topic+as_familiar_collection+2CfamiliarCollection-method'></span><span id='topic+as_familiar_collection+2CfamiliarData-method'></span><span id='topic+as_familiar_collection+2CfamiliarEnsemble-method'></span><span id='topic+as_familiar_collection+2CfamiliarModel-method'></span><span id='topic+as_familiar_collection+2Clist-method'></span><span id='topic+as_familiar_collection+2Ccharacter-method'></span><span id='topic+as_familiar_collection+2CANY-method'></span>

<h3>Description</h3>

<p>Creates a <code>familiarCollection</code> objects from <code>familiarData</code>,
<code>familiarEnsemble</code> or <code>familiarModel</code> objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as_familiar_collection(
  object,
  familiar_data_names = NULL,
  collection_name = NULL,
  ...
)

## S4 method for signature 'familiarCollection'
as_familiar_collection(
  object,
  familiar_data_names = NULL,
  collection_name = NULL,
  ...
)

## S4 method for signature 'familiarData'
as_familiar_collection(
  object,
  familiar_data_names = NULL,
  collection_name = NULL,
  ...
)

## S4 method for signature 'familiarEnsemble'
as_familiar_collection(
  object,
  familiar_data_names = NULL,
  collection_name = NULL,
  ...
)

## S4 method for signature 'familiarModel'
as_familiar_collection(
  object,
  familiar_data_names = NULL,
  collection_name = NULL,
  ...
)

## S4 method for signature 'list'
as_familiar_collection(
  object,
  familiar_data_names = NULL,
  collection_name = NULL,
  ...
)

## S4 method for signature 'character'
as_familiar_collection(
  object,
  familiar_data_names = NULL,
  collection_name = NULL,
  ...
)

## S4 method for signature 'ANY'
as_familiar_collection(
  object,
  familiar_data_names = NULL,
  collection_name = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="as_familiar_collection_+3A_object">object</code></td>
<td>
<p><code>familiarCollection</code> object, or one or more <code>familiarData</code>
objects, that will be internally converted to a <code>familiarCollection</code> object.
It is also possible to provide a <code>familiarEnsemble</code> or one or more
<code>familiarModel</code> objects together with the data from which data is computed
prior to export. Paths to such files can also be provided.</p>
</td></tr>
<tr><td><code id="as_familiar_collection_+3A_familiar_data_names">familiar_data_names</code></td>
<td>
<p>Names of the dataset(s). Only used if the <code>object</code> parameter
is one or more <code>familiarData</code> objects.</p>
</td></tr>
<tr><td><code id="as_familiar_collection_+3A_collection_name">collection_name</code></td>
<td>
<p>Name of the collection.</p>
</td></tr>
<tr><td><code id="as_familiar_collection_+3A_...">...</code></td>
<td>

<p>Arguments passed on to <code><a href="#topic+extract_data">extract_data</a></code>
</p>

<dl>
<dt><code>data</code></dt><dd><p>A <code>dataObject</code> object, <code>data.table</code> or <code>data.frame</code> that
constitutes the data that are assessed.</p>
</dd>
<dt><code>is_pre_processed</code></dt><dd><p>Flag that indicates whether the data was already
pre-processed externally, e.g. normalised and clustered. Only used if the
<code>data</code> argument is a <code>data.table</code> or <code>data.frame</code>.</p>
</dd>
<dt><code>cl</code></dt><dd><p>Cluster created using the <code>parallel</code> package. This cluster is then
used to speed up computation through parallellisation.</p>
</dd>
<dt><code>time_max</code></dt><dd><p>Time point which is used as the benchmark for e.g. cumulative
risks generated by random forest, or the cut-off value for Uno's concordance
index. If not provided explicitly, this parameter is read from settings used
at creation of the underlying <code>familiarModel</code> objects. Only used for
<code>survival</code> outcomes.</p>
</dd>
<dt><code>evaluation_times</code></dt><dd><p>One or more time points that are used for in analysis of
survival problems when data has to be assessed at a set time, e.g.
calibration. If not provided explicitly, this parameter is read from
settings used at creation of the underlying <code>familiarModel</code> objects. Only
used for <code>survival</code> outcomes.</p>
</dd>
<dt><code>aggregation_method</code></dt><dd><p>Method for aggregating variable importances for the
purpose of evaluation. Variable importances are determined during feature
selection steps and after training the model. Both types are evaluated, but
feature selection variable importance is only evaluated at run-time.
</p>
<p>See the documentation for the <code>vimp_aggregation_method</code> argument in
<code>summon_familiar</code> for information concerning the different available
methods.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</dd>
<dt><code>rank_threshold</code></dt><dd><p>The threshold used to  define the subset of highly
important features during evaluation.
</p>
<p>See the documentation for the <code>vimp_aggregation_rank_threshold</code> argument in
<code>summon_familiar</code> for more information.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</dd>
<dt><code>ensemble_method</code></dt><dd><p>Method for ensembling predictions from models for the
same sample. Available methods are:
</p>

<ul>
<li> <p><code>median</code> (default): Use the median of the predicted values as the ensemble
value for a sample.
</p>
</li>
<li> <p><code>mean</code>: Use the mean of the predicted values as the ensemble value for a
sample.
</p>
</li></ul>
</dd>
<dt><code>metric</code></dt><dd><p>One or more metrics for assessing model performance. See the
vignette on performance metrics for the available metrics. If not provided
explicitly, this parameter is read from settings used at creation of the
underlying <code>familiarModel</code> objects.</p>
</dd>
<dt><code>feature_cluster_method</code></dt><dd><p>The method used to perform clustering. These are
the same methods as for the <code>cluster_method</code> configuration parameter:
<code>none</code>, <code>hclust</code>, <code>agnes</code>, <code>diana</code> and <code>pam</code>.
</p>
<p><code>none</code> cannot be used when extracting data regarding mutual correlation or
feature expressions.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</dd>
<dt><code>feature_linkage_method</code></dt><dd><p>The method used for agglomerative clustering in
<code>hclust</code> and <code>agnes</code>. These are the same methods as for the
<code>cluster_linkage_method</code> configuration parameter: <code>average</code>, <code>single</code>,
<code>complete</code>, <code>weighted</code>, and <code>ward</code>.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</dd>
<dt><code>feature_cluster_cut_method</code></dt><dd><p>The method used to divide features into
separate clusters. The available methods are the same as for the
<code>cluster_cut_method</code> configuration parameter: <code>silhouette</code>, <code>fixed_cut</code> and
<code>dynamic_cut</code>.
</p>
<p><code>silhouette</code> is available for all cluster methods, but <code>fixed_cut</code> only
applies to methods that create hierarchical trees (<code>hclust</code>, <code>agnes</code> and
<code>diana</code>). <code>dynamic_cut</code> requires the <code>dynamicTreeCut</code> package and can only
be used with <code>agnes</code> and <code>hclust</code>.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</dd>
<dt><code>feature_similarity_threshold</code></dt><dd><p>The threshold level for pair-wise
similarity that is required to form feature clusters with the <code>fixed_cut</code>
method.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</dd>
<dt><code>feature_similarity_metric</code></dt><dd><p>Metric to determine pairwise similarity
between features. Similarity is computed in the same manner as for
clustering, and <code>feature_similarity_metric</code> therefore has the same options
as <code>cluster_similarity_metric</code>: <code>mcfadden_r2</code>, <code>cox_snell_r2</code>,
<code>nagelkerke_r2</code>, <code>spearman</code>, <code>kendall</code> and <code>pearson</code>.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</dd>
<dt><code>sample_cluster_method</code></dt><dd><p>The method used to perform clustering based on
distance between samples. These are the same methods as for the
<code>cluster_method</code> configuration parameter: <code>hclust</code>, <code>agnes</code>, <code>diana</code> and
<code>pam</code>.
</p>
<p><code>none</code> cannot be used when extracting data for feature expressions.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</dd>
<dt><code>sample_linkage_method</code></dt><dd><p>The method used for agglomerative clustering in
<code>hclust</code> and <code>agnes</code>. These are the same methods as for the
<code>cluster_linkage_method</code> configuration parameter: <code>average</code>, <code>single</code>,
<code>complete</code>, <code>weighted</code>, and <code>ward</code>.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</dd>
<dt><code>sample_similarity_metric</code></dt><dd><p>Metric to determine pairwise similarity
between samples. Similarity is computed in the same manner as for
clustering, but <code>sample_similarity_metric</code> has different options that are
better suited to computing distance between samples instead of between
features: <code>gower</code>, <code>euclidean</code>.
</p>
<p>The underlying feature data is scaled to the <code class="reqn">[0, 1]</code> range (for
numerical features) using the feature values across the samples. The
normalisation parameters required can optionally be computed from feature
data with the outer 5% (on both sides) of feature values trimmed or
winsorised. To do so append <code style="white-space: pre;">&#8288;_trim&#8288;</code> (trimming) or <code style="white-space: pre;">&#8288;_winsor&#8288;</code> (winsorising) to
the metric name. This reduces the effect of outliers somewhat.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</dd>
<dt><code>icc_type</code></dt><dd><p>String indicating the type of intraclass correlation
coefficient (<code>1</code>, <code>2</code> or <code>3</code>) that should be used to compute robustness for
features in repeated measurements during the evaluation of univariate
importance. These types correspond to the types in Shrout and Fleiss (1979).
If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</dd>
<dt><code>verbose</code></dt><dd><p>Flag to indicate whether feedback should be provided on the
computation and extraction of various data elements.</p>
</dd>
<dt><code>message_indent</code></dt><dd><p>Number of indentation steps for messages shown during
computation and extraction of various data elements.</p>
</dd>
<dt><code>data_element</code></dt><dd><p>String indicating which data elements are to be extracted.
Default is <code>all</code>, but specific elements can be specified to speed up
computations if not all elements are to be computed. This is an internal
parameter that is set by, e.g. the <code>export_model_vimp</code> method.</p>
</dd>
<dt><code>sample_limit</code></dt><dd><p>(<em>optional</em>) Set the upper limit of the number of samples
that are used during evaluation steps. Cannot be less than 20.
</p>
<p>This setting can be specified per data element by providing a parameter
value in a named list with data elements, e.g.
<code>list("sample_similarity"=100, "permutation_vimp"=1000)</code>.
</p>
<p>This parameter can be set for the following data elements:
<code>sample_similarity</code> and <code>ice_data</code>.</p>
</dd>
<dt><code>detail_level</code></dt><dd><p>(<em>optional</em>) Sets the level at which results are computed
and aggregated.
</p>

<ul>
<li> <p><code>ensemble</code>: Results are computed at the ensemble level, i.e. over all
models in the ensemble. This means that, for example, bias-corrected
estimates of model performance are assessed by creating (at least) 20
bootstraps and computing the model performance of the ensemble model for
each bootstrap.
</p>
</li>
<li> <p><code>hybrid</code> (default): Results are computed at the level of models in an
ensemble. This means that, for example, bias-corrected estimates of model
performance are directly computed using the models in the ensemble. If there
are at least 20 trained models in the ensemble, performance is computed for
each model, in contrast to <code>ensemble</code> where performance is computed for the
ensemble of models. If there are less than 20 trained models in the
ensemble, bootstraps are created so that at least 20 point estimates can be
made.
</p>
</li>
<li> <p><code>model</code>: Results are computed at the model level. This means that, for
example, bias-corrected estimates of model performance are assessed by
creating (at least) 20 bootstraps and computing the performance of the model
for each bootstrap.
</p>
</li></ul>

<p>Note that each level of detail has a different interpretation for bootstrap
confidence intervals. For <code>ensemble</code> and <code>model</code> these are the confidence
intervals for the ensemble and an individual model, respectively. That is,
the confidence interval describes the range where an estimate produced by a
respective ensemble or model trained on a repeat of the experiment may be
found with the probability of the confidence level. For <code>hybrid</code>, it
represents the range where any single model trained on a repeat of the
experiment may be found with the probability of the confidence level. By
definition, confidence intervals obtained using <code>hybrid</code> are at least as
wide as those for <code>ensemble</code>. <code>hybrid</code> offers the correct interpretation if
the goal of the analysis is to assess the result of a single, unspecified,
model.
</p>
<p><code>hybrid</code> is generally computationally less expensive then <code>ensemble</code>, which
in turn is somewhat less expensive than <code>model</code>.
</p>
<p>A non-default <code>detail_level</code> parameter can be specified for separate
evaluation steps by providing a parameter value in a named list with data
elements, e.g. <code>list("auc_data"="ensemble", "model_performance"="hybrid")</code>.
This parameter can be set for the following data elements: <code>auc_data</code>,
<code>decision_curve_analyis</code>, <code>model_performance</code>, <code>permutation_vimp</code>,
<code>ice_data</code>, <code>prediction_data</code> and <code>confusion_matrix</code>.</p>
</dd>
<dt><code>estimation_type</code></dt><dd><p>(<em>optional</em>) Sets the type of estimation that should be
possible. This has the following options:
</p>

<ul>
<li> <p><code>point</code>: Point estimates.
</p>
</li>
<li> <p><code>bias_correction</code> or <code>bc</code>: Bias-corrected estimates. A bias-corrected
estimate is computed from (at least) 20 point estimates, and <code>familiar</code> may
bootstrap the data to create them.
</p>
</li>
<li> <p><code>bootstrap_confidence_interval</code> or <code>bci</code> (default): Bias-corrected
estimates with bootstrap confidence intervals (Efron and Hastie, 2016). The
number of point estimates required depends on the <code>confidence_level</code>
parameter, and <code>familiar</code> may bootstrap the data to create them.
</p>
</li></ul>

<p>As with <code>detail_level</code>, a non-default <code>estimation_type</code> parameter can be
specified for separate evaluation steps by providing a parameter value in a
named list with data elements, e.g. <code>list("auc_data"="bci", "model_performance"="point")</code>. This parameter can be set for the following
data elements: <code>auc_data</code>, <code>decision_curve_analyis</code>, <code>model_performance</code>,
<code>permutation_vimp</code>, <code>ice_data</code>, and <code>prediction_data</code>.</p>
</dd>
<dt><code>aggregate_results</code></dt><dd><p>(<em>optional</em>) Flag that signifies whether results
should be aggregated during evaluation. If <code>estimation_type</code> is
<code>bias_correction</code> or <code>bc</code>, aggregation leads to a single bias-corrected
estimate. If <code>estimation_type</code> is <code>bootstrap_confidence_interval</code> or <code>bci</code>,
aggregation leads to a single bias-corrected estimate with lower and upper
boundaries of the confidence interval. This has no effect if
<code>estimation_type</code> is <code>point</code>.
</p>
<p>The default value is equal to <code>TRUE</code> except when assessing metrics to assess
model performance, as the default violin plot requires underlying data.
</p>
<p>As with <code>detail_level</code> and <code>estimation_type</code>, a non-default
<code>aggregate_results</code> parameter can be specified for separate evaluation steps
by providing a parameter value in a named list with data elements, e.g.
<code>list("auc_data"=TRUE, , "model_performance"=FALSE)</code>. This parameter exists
for the same elements as <code>estimation_type</code>.</p>
</dd>
<dt><code>confidence_level</code></dt><dd><p>(<em>optional</em>) Numeric value for the level at which
confidence intervals are determined. In the case bootstraps are used to
determine the confidence intervals bootstrap estimation, <code>familiar</code> uses the
rule of thumb <code class="reqn">n = 20 / ci.level</code> to determine the number of required
bootstraps.
</p>
<p>The default value is <code>0.95</code>.</p>
</dd>
<dt><code>bootstrap_ci_method</code></dt><dd><p>(<em>optional</em>) Method used to determine bootstrap
confidence intervals (Efron and Hastie, 2016). The following methods are
implemented:
</p>

<ul>
<li> <p><code>percentile</code> (default): Confidence intervals obtained using the percentile
method.
</p>
</li>
<li> <p><code>bc</code>: Bias-corrected confidence intervals.
</p>
</li></ul>

<p>Note that the standard method is not implemented because this method is
often not suitable due to non-normal distributions. The bias-corrected and
accelerated (BCa) method is not implemented yet.</p>
</dd>
<dt><code>stratification_method</code></dt><dd><p>(<em>optional</em>) Method for determining the
stratification threshold for creating survival groups. The actual,
model-dependent, threshold value is obtained from the development data, and
can afterwards be used to perform stratification on validation data.
</p>
<p>The following stratification methods are available:
</p>

<ul>
<li> <p><code>median</code> (default): The median predicted value in the development cohort
is used to stratify the samples into two risk groups. For predicted outcome
values that build a continuous spectrum, the two risk groups in the
development cohort will be roughly equal in size.
</p>
</li>
<li> <p><code>mean</code>: The mean predicted value in the development cohort is used to
stratify the samples into two risk groups.
</p>
</li>
<li> <p><code>mean_trim</code>: As <code>mean</code>, but based on the set of predicted values
where the 5% lowest and 5% highest values are discarded. This reduces the
effect of outliers.
</p>
</li>
<li> <p><code>mean_winsor</code>: As <code>mean</code>, but based on the set of predicted values where
the 5% lowest and 5% highest values are winsorised. This reduces the effect
of outliers.
</p>
</li>
<li> <p><code>fixed</code>: Samples are stratified based on the sample quantiles of the
predicted values. These quantiles are defined using the
<code>stratification_threshold</code> parameter.
</p>
</li>
<li> <p><code>optimised</code>: Use maximally selected rank statistics to determine the
optimal threshold (Lausen and Schumacher, 1992; Hothorn et al., 2003) to
stratify samples into two optimally separated risk groups.
</p>
</li></ul>

<p>One or more stratification methods can be selected simultaneously.
</p>
<p>This parameter is only relevant for <code>survival</code> outcomes.</p>
</dd>
<dt><code>dynamic_model_loading</code></dt><dd><p>(<em>optional</em>) Enables dynamic loading of models
during the evaluation process, if <code>TRUE</code>. Defaults to <code>FALSE</code>. Dynamic
loading of models may reduce the overall memory footprint, at the cost of
increased disk or network IO. Models can only be dynamically loaded if they
are found at an accessible disk or network location. Setting this parameter
to <code>TRUE</code> may help if parallel processing causes out-of-memory issues during
evaluation.</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Details</h3>

<p>A <code>data</code> argument is expected if the <code>object</code> argument is a
<code>familiarEnsemble</code> object or one or more <code>familiarModel</code> objects.
</p>


<h3>Value</h3>

<p>A <code>familiarCollection</code> object.
</p>

<hr>
<h2 id='as_familiar_data'>Conversion to familiarData object.</h2><span id='topic+as_familiar_data'></span><span id='topic+as_familiar_data+2CfamiliarData-method'></span><span id='topic+as_familiar_data+2CfamiliarEnsemble-method'></span><span id='topic+as_familiar_data+2CfamiliarModel-method'></span><span id='topic+as_familiar_data+2Clist-method'></span><span id='topic+as_familiar_data+2Ccharacter-method'></span><span id='topic+as_familiar_data+2CANY-method'></span>

<h3>Description</h3>

<p>Creates <code>familiarData</code> a object from <code>familiarEnsemble</code> or
<code>familiarModel</code> objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as_familiar_data(object, ...)

## S4 method for signature 'familiarData'
as_familiar_data(object, ...)

## S4 method for signature 'familiarEnsemble'
as_familiar_data(object, name = NULL, ...)

## S4 method for signature 'familiarModel'
as_familiar_data(object, ...)

## S4 method for signature 'list'
as_familiar_data(object, ...)

## S4 method for signature 'character'
as_familiar_data(object, ...)

## S4 method for signature 'ANY'
as_familiar_data(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="as_familiar_data_+3A_object">object</code></td>
<td>
<p>A <code>familiarData</code> object, or a <code>familiarEnsemble</code> or
<code>familiarModel</code> objects that will be internally converted to a
<code>familiarData</code> object. Paths to such objects can also be provided.</p>
</td></tr>
<tr><td><code id="as_familiar_data_+3A_...">...</code></td>
<td>

<p>Arguments passed on to <code><a href="#topic+extract_data">extract_data</a></code>
</p>

<dl>
<dt><code>data</code></dt><dd><p>A <code>dataObject</code> object, <code>data.table</code> or <code>data.frame</code> that
constitutes the data that are assessed.</p>
</dd>
<dt><code>is_pre_processed</code></dt><dd><p>Flag that indicates whether the data was already
pre-processed externally, e.g. normalised and clustered. Only used if the
<code>data</code> argument is a <code>data.table</code> or <code>data.frame</code>.</p>
</dd>
<dt><code>cl</code></dt><dd><p>Cluster created using the <code>parallel</code> package. This cluster is then
used to speed up computation through parallellisation.</p>
</dd>
<dt><code>time_max</code></dt><dd><p>Time point which is used as the benchmark for e.g. cumulative
risks generated by random forest, or the cut-off value for Uno's concordance
index. If not provided explicitly, this parameter is read from settings used
at creation of the underlying <code>familiarModel</code> objects. Only used for
<code>survival</code> outcomes.</p>
</dd>
<dt><code>evaluation_times</code></dt><dd><p>One or more time points that are used for in analysis of
survival problems when data has to be assessed at a set time, e.g.
calibration. If not provided explicitly, this parameter is read from
settings used at creation of the underlying <code>familiarModel</code> objects. Only
used for <code>survival</code> outcomes.</p>
</dd>
<dt><code>aggregation_method</code></dt><dd><p>Method for aggregating variable importances for the
purpose of evaluation. Variable importances are determined during feature
selection steps and after training the model. Both types are evaluated, but
feature selection variable importance is only evaluated at run-time.
</p>
<p>See the documentation for the <code>vimp_aggregation_method</code> argument in
<code>summon_familiar</code> for information concerning the different available
methods.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</dd>
<dt><code>rank_threshold</code></dt><dd><p>The threshold used to  define the subset of highly
important features during evaluation.
</p>
<p>See the documentation for the <code>vimp_aggregation_rank_threshold</code> argument in
<code>summon_familiar</code> for more information.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</dd>
<dt><code>ensemble_method</code></dt><dd><p>Method for ensembling predictions from models for the
same sample. Available methods are:
</p>

<ul>
<li> <p><code>median</code> (default): Use the median of the predicted values as the ensemble
value for a sample.
</p>
</li>
<li> <p><code>mean</code>: Use the mean of the predicted values as the ensemble value for a
sample.
</p>
</li></ul>
</dd>
<dt><code>metric</code></dt><dd><p>One or more metrics for assessing model performance. See the
vignette on performance metrics for the available metrics. If not provided
explicitly, this parameter is read from settings used at creation of the
underlying <code>familiarModel</code> objects.</p>
</dd>
<dt><code>feature_cluster_method</code></dt><dd><p>The method used to perform clustering. These are
the same methods as for the <code>cluster_method</code> configuration parameter:
<code>none</code>, <code>hclust</code>, <code>agnes</code>, <code>diana</code> and <code>pam</code>.
</p>
<p><code>none</code> cannot be used when extracting data regarding mutual correlation or
feature expressions.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</dd>
<dt><code>feature_linkage_method</code></dt><dd><p>The method used for agglomerative clustering in
<code>hclust</code> and <code>agnes</code>. These are the same methods as for the
<code>cluster_linkage_method</code> configuration parameter: <code>average</code>, <code>single</code>,
<code>complete</code>, <code>weighted</code>, and <code>ward</code>.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</dd>
<dt><code>feature_cluster_cut_method</code></dt><dd><p>The method used to divide features into
separate clusters. The available methods are the same as for the
<code>cluster_cut_method</code> configuration parameter: <code>silhouette</code>, <code>fixed_cut</code> and
<code>dynamic_cut</code>.
</p>
<p><code>silhouette</code> is available for all cluster methods, but <code>fixed_cut</code> only
applies to methods that create hierarchical trees (<code>hclust</code>, <code>agnes</code> and
<code>diana</code>). <code>dynamic_cut</code> requires the <code>dynamicTreeCut</code> package and can only
be used with <code>agnes</code> and <code>hclust</code>.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</dd>
<dt><code>feature_similarity_threshold</code></dt><dd><p>The threshold level for pair-wise
similarity that is required to form feature clusters with the <code>fixed_cut</code>
method.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</dd>
<dt><code>feature_similarity_metric</code></dt><dd><p>Metric to determine pairwise similarity
between features. Similarity is computed in the same manner as for
clustering, and <code>feature_similarity_metric</code> therefore has the same options
as <code>cluster_similarity_metric</code>: <code>mcfadden_r2</code>, <code>cox_snell_r2</code>,
<code>nagelkerke_r2</code>, <code>spearman</code>, <code>kendall</code> and <code>pearson</code>.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</dd>
<dt><code>sample_cluster_method</code></dt><dd><p>The method used to perform clustering based on
distance between samples. These are the same methods as for the
<code>cluster_method</code> configuration parameter: <code>hclust</code>, <code>agnes</code>, <code>diana</code> and
<code>pam</code>.
</p>
<p><code>none</code> cannot be used when extracting data for feature expressions.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</dd>
<dt><code>sample_linkage_method</code></dt><dd><p>The method used for agglomerative clustering in
<code>hclust</code> and <code>agnes</code>. These are the same methods as for the
<code>cluster_linkage_method</code> configuration parameter: <code>average</code>, <code>single</code>,
<code>complete</code>, <code>weighted</code>, and <code>ward</code>.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</dd>
<dt><code>sample_similarity_metric</code></dt><dd><p>Metric to determine pairwise similarity
between samples. Similarity is computed in the same manner as for
clustering, but <code>sample_similarity_metric</code> has different options that are
better suited to computing distance between samples instead of between
features: <code>gower</code>, <code>euclidean</code>.
</p>
<p>The underlying feature data is scaled to the <code class="reqn">[0, 1]</code> range (for
numerical features) using the feature values across the samples. The
normalisation parameters required can optionally be computed from feature
data with the outer 5% (on both sides) of feature values trimmed or
winsorised. To do so append <code style="white-space: pre;">&#8288;_trim&#8288;</code> (trimming) or <code style="white-space: pre;">&#8288;_winsor&#8288;</code> (winsorising) to
the metric name. This reduces the effect of outliers somewhat.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</dd>
<dt><code>icc_type</code></dt><dd><p>String indicating the type of intraclass correlation
coefficient (<code>1</code>, <code>2</code> or <code>3</code>) that should be used to compute robustness for
features in repeated measurements during the evaluation of univariate
importance. These types correspond to the types in Shrout and Fleiss (1979).
If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</dd>
<dt><code>verbose</code></dt><dd><p>Flag to indicate whether feedback should be provided on the
computation and extraction of various data elements.</p>
</dd>
<dt><code>message_indent</code></dt><dd><p>Number of indentation steps for messages shown during
computation and extraction of various data elements.</p>
</dd>
<dt><code>data_element</code></dt><dd><p>String indicating which data elements are to be extracted.
Default is <code>all</code>, but specific elements can be specified to speed up
computations if not all elements are to be computed. This is an internal
parameter that is set by, e.g. the <code>export_model_vimp</code> method.</p>
</dd>
<dt><code>sample_limit</code></dt><dd><p>(<em>optional</em>) Set the upper limit of the number of samples
that are used during evaluation steps. Cannot be less than 20.
</p>
<p>This setting can be specified per data element by providing a parameter
value in a named list with data elements, e.g.
<code>list("sample_similarity"=100, "permutation_vimp"=1000)</code>.
</p>
<p>This parameter can be set for the following data elements:
<code>sample_similarity</code> and <code>ice_data</code>.</p>
</dd>
<dt><code>detail_level</code></dt><dd><p>(<em>optional</em>) Sets the level at which results are computed
and aggregated.
</p>

<ul>
<li> <p><code>ensemble</code>: Results are computed at the ensemble level, i.e. over all
models in the ensemble. This means that, for example, bias-corrected
estimates of model performance are assessed by creating (at least) 20
bootstraps and computing the model performance of the ensemble model for
each bootstrap.
</p>
</li>
<li> <p><code>hybrid</code> (default): Results are computed at the level of models in an
ensemble. This means that, for example, bias-corrected estimates of model
performance are directly computed using the models in the ensemble. If there
are at least 20 trained models in the ensemble, performance is computed for
each model, in contrast to <code>ensemble</code> where performance is computed for the
ensemble of models. If there are less than 20 trained models in the
ensemble, bootstraps are created so that at least 20 point estimates can be
made.
</p>
</li>
<li> <p><code>model</code>: Results are computed at the model level. This means that, for
example, bias-corrected estimates of model performance are assessed by
creating (at least) 20 bootstraps and computing the performance of the model
for each bootstrap.
</p>
</li></ul>

<p>Note that each level of detail has a different interpretation for bootstrap
confidence intervals. For <code>ensemble</code> and <code>model</code> these are the confidence
intervals for the ensemble and an individual model, respectively. That is,
the confidence interval describes the range where an estimate produced by a
respective ensemble or model trained on a repeat of the experiment may be
found with the probability of the confidence level. For <code>hybrid</code>, it
represents the range where any single model trained on a repeat of the
experiment may be found with the probability of the confidence level. By
definition, confidence intervals obtained using <code>hybrid</code> are at least as
wide as those for <code>ensemble</code>. <code>hybrid</code> offers the correct interpretation if
the goal of the analysis is to assess the result of a single, unspecified,
model.
</p>
<p><code>hybrid</code> is generally computationally less expensive then <code>ensemble</code>, which
in turn is somewhat less expensive than <code>model</code>.
</p>
<p>A non-default <code>detail_level</code> parameter can be specified for separate
evaluation steps by providing a parameter value in a named list with data
elements, e.g. <code>list("auc_data"="ensemble", "model_performance"="hybrid")</code>.
This parameter can be set for the following data elements: <code>auc_data</code>,
<code>decision_curve_analyis</code>, <code>model_performance</code>, <code>permutation_vimp</code>,
<code>ice_data</code>, <code>prediction_data</code> and <code>confusion_matrix</code>.</p>
</dd>
<dt><code>estimation_type</code></dt><dd><p>(<em>optional</em>) Sets the type of estimation that should be
possible. This has the following options:
</p>

<ul>
<li> <p><code>point</code>: Point estimates.
</p>
</li>
<li> <p><code>bias_correction</code> or <code>bc</code>: Bias-corrected estimates. A bias-corrected
estimate is computed from (at least) 20 point estimates, and <code>familiar</code> may
bootstrap the data to create them.
</p>
</li>
<li> <p><code>bootstrap_confidence_interval</code> or <code>bci</code> (default): Bias-corrected
estimates with bootstrap confidence intervals (Efron and Hastie, 2016). The
number of point estimates required depends on the <code>confidence_level</code>
parameter, and <code>familiar</code> may bootstrap the data to create them.
</p>
</li></ul>

<p>As with <code>detail_level</code>, a non-default <code>estimation_type</code> parameter can be
specified for separate evaluation steps by providing a parameter value in a
named list with data elements, e.g. <code>list("auc_data"="bci", "model_performance"="point")</code>. This parameter can be set for the following
data elements: <code>auc_data</code>, <code>decision_curve_analyis</code>, <code>model_performance</code>,
<code>permutation_vimp</code>, <code>ice_data</code>, and <code>prediction_data</code>.</p>
</dd>
<dt><code>aggregate_results</code></dt><dd><p>(<em>optional</em>) Flag that signifies whether results
should be aggregated during evaluation. If <code>estimation_type</code> is
<code>bias_correction</code> or <code>bc</code>, aggregation leads to a single bias-corrected
estimate. If <code>estimation_type</code> is <code>bootstrap_confidence_interval</code> or <code>bci</code>,
aggregation leads to a single bias-corrected estimate with lower and upper
boundaries of the confidence interval. This has no effect if
<code>estimation_type</code> is <code>point</code>.
</p>
<p>The default value is equal to <code>TRUE</code> except when assessing metrics to assess
model performance, as the default violin plot requires underlying data.
</p>
<p>As with <code>detail_level</code> and <code>estimation_type</code>, a non-default
<code>aggregate_results</code> parameter can be specified for separate evaluation steps
by providing a parameter value in a named list with data elements, e.g.
<code>list("auc_data"=TRUE, , "model_performance"=FALSE)</code>. This parameter exists
for the same elements as <code>estimation_type</code>.</p>
</dd>
<dt><code>confidence_level</code></dt><dd><p>(<em>optional</em>) Numeric value for the level at which
confidence intervals are determined. In the case bootstraps are used to
determine the confidence intervals bootstrap estimation, <code>familiar</code> uses the
rule of thumb <code class="reqn">n = 20 / ci.level</code> to determine the number of required
bootstraps.
</p>
<p>The default value is <code>0.95</code>.</p>
</dd>
<dt><code>bootstrap_ci_method</code></dt><dd><p>(<em>optional</em>) Method used to determine bootstrap
confidence intervals (Efron and Hastie, 2016). The following methods are
implemented:
</p>

<ul>
<li> <p><code>percentile</code> (default): Confidence intervals obtained using the percentile
method.
</p>
</li>
<li> <p><code>bc</code>: Bias-corrected confidence intervals.
</p>
</li></ul>

<p>Note that the standard method is not implemented because this method is
often not suitable due to non-normal distributions. The bias-corrected and
accelerated (BCa) method is not implemented yet.</p>
</dd>
<dt><code>stratification_method</code></dt><dd><p>(<em>optional</em>) Method for determining the
stratification threshold for creating survival groups. The actual,
model-dependent, threshold value is obtained from the development data, and
can afterwards be used to perform stratification on validation data.
</p>
<p>The following stratification methods are available:
</p>

<ul>
<li> <p><code>median</code> (default): The median predicted value in the development cohort
is used to stratify the samples into two risk groups. For predicted outcome
values that build a continuous spectrum, the two risk groups in the
development cohort will be roughly equal in size.
</p>
</li>
<li> <p><code>mean</code>: The mean predicted value in the development cohort is used to
stratify the samples into two risk groups.
</p>
</li>
<li> <p><code>mean_trim</code>: As <code>mean</code>, but based on the set of predicted values
where the 5% lowest and 5% highest values are discarded. This reduces the
effect of outliers.
</p>
</li>
<li> <p><code>mean_winsor</code>: As <code>mean</code>, but based on the set of predicted values where
the 5% lowest and 5% highest values are winsorised. This reduces the effect
of outliers.
</p>
</li>
<li> <p><code>fixed</code>: Samples are stratified based on the sample quantiles of the
predicted values. These quantiles are defined using the
<code>stratification_threshold</code> parameter.
</p>
</li>
<li> <p><code>optimised</code>: Use maximally selected rank statistics to determine the
optimal threshold (Lausen and Schumacher, 1992; Hothorn et al., 2003) to
stratify samples into two optimally separated risk groups.
</p>
</li></ul>

<p>One or more stratification methods can be selected simultaneously.
</p>
<p>This parameter is only relevant for <code>survival</code> outcomes.</p>
</dd>
<dt><code>dynamic_model_loading</code></dt><dd><p>(<em>optional</em>) Enables dynamic loading of models
during the evaluation process, if <code>TRUE</code>. Defaults to <code>FALSE</code>. Dynamic
loading of models may reduce the overall memory footprint, at the cost of
increased disk or network IO. Models can only be dynamically loaded if they
are found at an accessible disk or network location. Setting this parameter
to <code>TRUE</code> may help if parallel processing causes out-of-memory issues during
evaluation.</p>
</dd>
</dl>
</td></tr>
<tr><td><code id="as_familiar_data_+3A_name">name</code></td>
<td>
<p>Name of the <code>familiarData</code> object. If not set, a name is
automatically generated.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The <code>data</code> argument is required if <code>familiarEnsemble</code> or
<code>familiarModel</code> objects are provided.
</p>


<h3>Value</h3>

<p>A <code>familiarData</code> object.
</p>

<hr>
<h2 id='as_familiar_ensemble'>Conversion to familiarEnsemble object.</h2><span id='topic+as_familiar_ensemble'></span><span id='topic+as_familiar_ensemble+2CfamiliarEnsemble-method'></span><span id='topic+as_familiar_ensemble+2CfamiliarModel-method'></span><span id='topic+as_familiar_ensemble+2Clist-method'></span><span id='topic+as_familiar_ensemble+2Ccharacter-method'></span><span id='topic+as_familiar_ensemble+2CANY-method'></span>

<h3>Description</h3>

<p>Creates <code>familiarEnsemble</code> a object from <code>familiarModel</code> objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>as_familiar_ensemble(object, ...)

## S4 method for signature 'familiarEnsemble'
as_familiar_ensemble(object, ...)

## S4 method for signature 'familiarModel'
as_familiar_ensemble(object, ...)

## S4 method for signature 'list'
as_familiar_ensemble(object, ...)

## S4 method for signature 'character'
as_familiar_ensemble(object, ...)

## S4 method for signature 'ANY'
as_familiar_ensemble(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="as_familiar_ensemble_+3A_object">object</code></td>
<td>
<p>A <code>familiarEnsemble</code> object, or one or more
<code>familiarModel</code> objects that will be internally converted to a
<code>familiarEnsemble</code> object. Paths to such objects can also be provided.</p>
</td></tr>
<tr><td><code id="as_familiar_ensemble_+3A_...">...</code></td>
<td>
<p>Unused arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>familiarEnsemble</code> object.
</p>

<hr>
<h2 id='coef'>Extract model coefficients</h2><span id='topic+coef'></span><span id='topic+coef+2CfamiliarModel-method'></span>

<h3>Description</h3>

<p>Extract model coefficients
</p>


<h3>Usage</h3>

<pre><code class='language-R'>coef(object, ...)

## S4 method for signature 'familiarModel'
coef(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="coef_+3A_object">object</code></td>
<td>
<p>a familiarModel object</p>
</td></tr>
<tr><td><code id="coef_+3A_...">...</code></td>
<td>
<p>additional arguments passed to <code>coef</code> methods for the underlying
model, when available.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This method extends the <code>coef</code> S3 method. For some models <code>coef</code>
requires information that is trimmed from the model. In this case a copy of
the model coefficient is stored with the model, and returned.
</p>


<h3>Value</h3>

<p>Coefficients extracted from the model in the familiarModel object, if
any.
</p>

<hr>
<h2 id='create_randomised_groups'>Create randomised groups
Creates randomised groups, e.g. for tests that depend on splitting (continuous) data into groups, such as the Hosmer-Lemeshow test</h2><span id='topic+create_randomised_groups'></span>

<h3>Description</h3>

<p>The default fast mode is based on random sampling, whereas the slow mode is based on probabilistic joining of adjacent groups. As
the name suggests, fast mode operates considerably more efficient.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_randomised_groups(
  x,
  y = NULL,
  sample_identifiers,
  n_max_groups = NULL,
  n_min_groups = NULL,
  n_min_y_in_group = NULL,
  n_groups_init = 30,
  fast_mode = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="create_randomised_groups_+3A_x">x</code></td>
<td>
<p>Vector with data used for sorting. Groups are formed based on adjacent values.</p>
</td></tr>
<tr><td><code id="create_randomised_groups_+3A_y">y</code></td>
<td>
<p>Vector with markers, e.g. the events. Should be 0 or 1 (for an event).</p>
</td></tr>
<tr><td><code id="create_randomised_groups_+3A_sample_identifiers">sample_identifiers</code></td>
<td>
<p>data.table with sample_identifiers. If provide, a list of grouped sample_identifiers will be returned, and integers otherwise.</p>
</td></tr>
<tr><td><code id="create_randomised_groups_+3A_n_max_groups">n_max_groups</code></td>
<td>
<p>Maximum number of groups that need to be formed.</p>
</td></tr>
<tr><td><code id="create_randomised_groups_+3A_n_min_groups">n_min_groups</code></td>
<td>
<p>Minimum number of groups that need to be formed.</p>
</td></tr>
<tr><td><code id="create_randomised_groups_+3A_n_min_y_in_group">n_min_y_in_group</code></td>
<td>
<p>Minimum number of y=1 in each group for a valid group.</p>
</td></tr>
<tr><td><code id="create_randomised_groups_+3A_n_groups_init">n_groups_init</code></td>
<td>
<p>Number of initial groups (default: 30)</p>
</td></tr>
<tr><td><code id="create_randomised_groups_+3A_fast_mode">fast_mode</code></td>
<td>
<p>Enables fast randomised grouping mode (default: TRUE)</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List of group sample ids or indices.
</p>

<hr>
<h2 id='dataObject-class'>Data object</h2><span id='topic+dataObject-class'></span>

<h3>Description</h3>

<p>The dataObject class is used to resolve the issue of keeping track of
pre-processing status and data loading inside complex workflows, e.g. nested
predict functions inside a calibration function.
</p>


<h3>Slots</h3>


<dl>
<dt><code>data</code></dt><dd><p>NULL or data table containing the data. This is the data which
will be read and used.</p>
</dd>
<dt><code>preprocessing_level</code></dt><dd><p>character indicating the level of pre-processing
already conducted.</p>
</dd>
<dt><code>outcome_type</code></dt><dd><p>character, determines the outcome type.</p>
</dd>
<dt><code>data_column_info</code></dt><dd><p>Object containing column information.</p>
</dd>
<dt><code>delay_loading</code></dt><dd><p>logical. Allows delayed loading data, which enables data
parsing downstream without additional workflow complexity or memory
utilisation.</p>
</dd>
<dt><code>perturb_level</code></dt><dd><p>numeric. This is the perturbation level for data which
has not been loaded. Used for data retrieval by interacting with the run
table of the accompanying model.</p>
</dd>
<dt><code>load_validation</code></dt><dd><p>logical. This determines which internal data set will
be loaded. If TRUE, the validation data will be loaded, whereas FALSE loads
the development data.</p>
</dd>
<dt><code>aggregate_on_load</code></dt><dd><p>logical. Determines whether data is aggregated after
loading.</p>
</dd>
<dt><code>sample_set_on_load</code></dt><dd><p>NULL or vector of sample identifiers to be loaded.</p>
</dd>
</dl>

<hr>
<h2 id='encapsulate_path'>Encapsulate path</h2><span id='topic+encapsulate_path'></span>

<h3>Description</h3>

<p>This function is used to encapsulate paths to allow for behaviour switches.
One use is for example when plotting. The plot_all method will encapsulate a
path so that plots may be saved to a directory structure. Other plot methods,
e.g. plot_model_performance do not encapsulate a path, and if the user calls
these functions directly, the plot may be written to the provided path
instead of a directory structure.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>encapsulate_path(path)
</code></pre>


<h3>Value</h3>

<p>encapsulated_path object
</p>

<hr>
<h2 id='experimentData-class'>Experiment data</h2><span id='topic+experimentData-class'></span>

<h3>Description</h3>

<p>An experimentData object contains information concerning the experiment.
These objects can be used to instantiate multiple experiments using the same
iterations, feature information and variable importance.
</p>


<h3>Details</h3>

<p>experimentData objects are primarily used to improve
reproducibility, since these allow for training models on a shared
foundation.
</p>


<h3>Slots</h3>


<dl>
<dt><code>experiment_setup</code></dt><dd><p>Contains regarding the experimental setup that is used
to generate the iteration list.</p>
</dd>
<dt><code>iteration_list</code></dt><dd><p>List of iteration data that determines which instances
are assigned to training, validation and test sets.</p>
</dd>
<dt><code>feature_info</code></dt><dd><p>Feature information objects. Only available if the
experimentData object was generated using the <code>precompute_feature_info</code> or
<code>precompute_vimp</code> functions.</p>
</dd>
<dt><code>vimp_table_list</code></dt><dd><p>List of variable importance table objects. Only
available if the experimentData object was created using the
<code>precompute_vimp</code> function.</p>
</dd>
<dt><code>project_id</code></dt><dd><p>Identifier of the project that generated the experimentData
object.</p>
</dd>
<dt><code>familiar_version</code></dt><dd><p>Version of the familiar package used to create this
experimentData.</p>
</dd>
</dl>


<h3>See Also</h3>

<p><code><a href="#topic+precompute_data_assignment">precompute_data_assignment</a></code>
<code><a href="#topic+precompute_feature_info">precompute_feature_info</a></code>, <code><a href="#topic+precompute_vimp">precompute_vimp</a></code>
</p>

<hr>
<h2 id='export_all'>Extract and export all data.</h2><span id='topic+export_all'></span><span id='topic+export_all+2CfamiliarCollection-method'></span><span id='topic+export_all+2CANY-method'></span>

<h3>Description</h3>

<p>Extract and export all data from a familiarCollection.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>export_all(object, dir_path = NULL, aggregate_results = waiver(), ...)

## S4 method for signature 'familiarCollection'
export_all(object, dir_path = NULL, aggregate_results = waiver(), ...)

## S4 method for signature 'ANY'
export_all(object, dir_path = NULL, aggregate_results = waiver(), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="export_all_+3A_object">object</code></td>
<td>
<p>A <code>familiarCollection</code> object, or other other objects from which
a <code>familiarCollection</code> can be extracted. See details for more information.</p>
</td></tr>
<tr><td><code id="export_all_+3A_dir_path">dir_path</code></td>
<td>
<p>Path to folder where extracted data should be saved. <code>NULL</code>
will allow export as a structured list of data.tables.</p>
</td></tr>
<tr><td><code id="export_all_+3A_aggregate_results">aggregate_results</code></td>
<td>
<p>Flag that signifies whether results should be
aggregated for export.</p>
</td></tr>
<tr><td><code id="export_all_+3A_...">...</code></td>
<td>

<p>Arguments passed on to <code><a href="#topic+extract_data">extract_data</a></code>, <code><a href="#topic+as_familiar_collection">as_familiar_collection</a></code>
</p>

<dl>
<dt><code>data</code></dt><dd><p>A <code>dataObject</code> object, <code>data.table</code> or <code>data.frame</code> that
constitutes the data that are assessed.</p>
</dd>
<dt><code>is_pre_processed</code></dt><dd><p>Flag that indicates whether the data was already
pre-processed externally, e.g. normalised and clustered. Only used if the
<code>data</code> argument is a <code>data.table</code> or <code>data.frame</code>.</p>
</dd>
<dt><code>cl</code></dt><dd><p>Cluster created using the <code>parallel</code> package. This cluster is then
used to speed up computation through parallellisation.</p>
</dd>
<dt><code>time_max</code></dt><dd><p>Time point which is used as the benchmark for e.g. cumulative
risks generated by random forest, or the cut-off value for Uno's concordance
index. If not provided explicitly, this parameter is read from settings used
at creation of the underlying <code>familiarModel</code> objects. Only used for
<code>survival</code> outcomes.</p>
</dd>
<dt><code>evaluation_times</code></dt><dd><p>One or more time points that are used for in analysis of
survival problems when data has to be assessed at a set time, e.g.
calibration. If not provided explicitly, this parameter is read from
settings used at creation of the underlying <code>familiarModel</code> objects. Only
used for <code>survival</code> outcomes.</p>
</dd>
<dt><code>aggregation_method</code></dt><dd><p>Method for aggregating variable importances for the
purpose of evaluation. Variable importances are determined during feature
selection steps and after training the model. Both types are evaluated, but
feature selection variable importance is only evaluated at run-time.
</p>
<p>See the documentation for the <code>vimp_aggregation_method</code> argument in
<code>summon_familiar</code> for information concerning the different available
methods.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</dd>
<dt><code>rank_threshold</code></dt><dd><p>The threshold used to  define the subset of highly
important features during evaluation.
</p>
<p>See the documentation for the <code>vimp_aggregation_rank_threshold</code> argument in
<code>summon_familiar</code> for more information.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</dd>
<dt><code>ensemble_method</code></dt><dd><p>Method for ensembling predictions from models for the
same sample. Available methods are:
</p>

<ul>
<li> <p><code>median</code> (default): Use the median of the predicted values as the ensemble
value for a sample.
</p>
</li>
<li> <p><code>mean</code>: Use the mean of the predicted values as the ensemble value for a
sample.
</p>
</li></ul>
</dd>
<dt><code>metric</code></dt><dd><p>One or more metrics for assessing model performance. See the
vignette on performance metrics for the available metrics. If not provided
explicitly, this parameter is read from settings used at creation of the
underlying <code>familiarModel</code> objects.</p>
</dd>
<dt><code>feature_cluster_method</code></dt><dd><p>The method used to perform clustering. These are
the same methods as for the <code>cluster_method</code> configuration parameter:
<code>none</code>, <code>hclust</code>, <code>agnes</code>, <code>diana</code> and <code>pam</code>.
</p>
<p><code>none</code> cannot be used when extracting data regarding mutual correlation or
feature expressions.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</dd>
<dt><code>feature_linkage_method</code></dt><dd><p>The method used for agglomerative clustering in
<code>hclust</code> and <code>agnes</code>. These are the same methods as for the
<code>cluster_linkage_method</code> configuration parameter: <code>average</code>, <code>single</code>,
<code>complete</code>, <code>weighted</code>, and <code>ward</code>.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</dd>
<dt><code>feature_cluster_cut_method</code></dt><dd><p>The method used to divide features into
separate clusters. The available methods are the same as for the
<code>cluster_cut_method</code> configuration parameter: <code>silhouette</code>, <code>fixed_cut</code> and
<code>dynamic_cut</code>.
</p>
<p><code>silhouette</code> is available for all cluster methods, but <code>fixed_cut</code> only
applies to methods that create hierarchical trees (<code>hclust</code>, <code>agnes</code> and
<code>diana</code>). <code>dynamic_cut</code> requires the <code>dynamicTreeCut</code> package and can only
be used with <code>agnes</code> and <code>hclust</code>.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</dd>
<dt><code>feature_similarity_threshold</code></dt><dd><p>The threshold level for pair-wise
similarity that is required to form feature clusters with the <code>fixed_cut</code>
method.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</dd>
<dt><code>feature_similarity_metric</code></dt><dd><p>Metric to determine pairwise similarity
between features. Similarity is computed in the same manner as for
clustering, and <code>feature_similarity_metric</code> therefore has the same options
as <code>cluster_similarity_metric</code>: <code>mcfadden_r2</code>, <code>cox_snell_r2</code>,
<code>nagelkerke_r2</code>, <code>spearman</code>, <code>kendall</code> and <code>pearson</code>.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</dd>
<dt><code>sample_cluster_method</code></dt><dd><p>The method used to perform clustering based on
distance between samples. These are the same methods as for the
<code>cluster_method</code> configuration parameter: <code>hclust</code>, <code>agnes</code>, <code>diana</code> and
<code>pam</code>.
</p>
<p><code>none</code> cannot be used when extracting data for feature expressions.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</dd>
<dt><code>sample_linkage_method</code></dt><dd><p>The method used for agglomerative clustering in
<code>hclust</code> and <code>agnes</code>. These are the same methods as for the
<code>cluster_linkage_method</code> configuration parameter: <code>average</code>, <code>single</code>,
<code>complete</code>, <code>weighted</code>, and <code>ward</code>.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</dd>
<dt><code>sample_similarity_metric</code></dt><dd><p>Metric to determine pairwise similarity
between samples. Similarity is computed in the same manner as for
clustering, but <code>sample_similarity_metric</code> has different options that are
better suited to computing distance between samples instead of between
features: <code>gower</code>, <code>euclidean</code>.
</p>
<p>The underlying feature data is scaled to the <code class="reqn">[0, 1]</code> range (for
numerical features) using the feature values across the samples. The
normalisation parameters required can optionally be computed from feature
data with the outer 5% (on both sides) of feature values trimmed or
winsorised. To do so append <code style="white-space: pre;">&#8288;_trim&#8288;</code> (trimming) or <code style="white-space: pre;">&#8288;_winsor&#8288;</code> (winsorising) to
the metric name. This reduces the effect of outliers somewhat.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</dd>
<dt><code>icc_type</code></dt><dd><p>String indicating the type of intraclass correlation
coefficient (<code>1</code>, <code>2</code> or <code>3</code>) that should be used to compute robustness for
features in repeated measurements during the evaluation of univariate
importance. These types correspond to the types in Shrout and Fleiss (1979).
If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</dd>
<dt><code>verbose</code></dt><dd><p>Flag to indicate whether feedback should be provided on the
computation and extraction of various data elements.</p>
</dd>
<dt><code>message_indent</code></dt><dd><p>Number of indentation steps for messages shown during
computation and extraction of various data elements.</p>
</dd>
<dt><code>data_element</code></dt><dd><p>String indicating which data elements are to be extracted.
Default is <code>all</code>, but specific elements can be specified to speed up
computations if not all elements are to be computed. This is an internal
parameter that is set by, e.g. the <code>export_model_vimp</code> method.</p>
</dd>
<dt><code>sample_limit</code></dt><dd><p>(<em>optional</em>) Set the upper limit of the number of samples
that are used during evaluation steps. Cannot be less than 20.
</p>
<p>This setting can be specified per data element by providing a parameter
value in a named list with data elements, e.g.
<code>list("sample_similarity"=100, "permutation_vimp"=1000)</code>.
</p>
<p>This parameter can be set for the following data elements:
<code>sample_similarity</code> and <code>ice_data</code>.</p>
</dd>
<dt><code>detail_level</code></dt><dd><p>(<em>optional</em>) Sets the level at which results are computed
and aggregated.
</p>

<ul>
<li> <p><code>ensemble</code>: Results are computed at the ensemble level, i.e. over all
models in the ensemble. This means that, for example, bias-corrected
estimates of model performance are assessed by creating (at least) 20
bootstraps and computing the model performance of the ensemble model for
each bootstrap.
</p>
</li>
<li> <p><code>hybrid</code> (default): Results are computed at the level of models in an
ensemble. This means that, for example, bias-corrected estimates of model
performance are directly computed using the models in the ensemble. If there
are at least 20 trained models in the ensemble, performance is computed for
each model, in contrast to <code>ensemble</code> where performance is computed for the
ensemble of models. If there are less than 20 trained models in the
ensemble, bootstraps are created so that at least 20 point estimates can be
made.
</p>
</li>
<li> <p><code>model</code>: Results are computed at the model level. This means that, for
example, bias-corrected estimates of model performance are assessed by
creating (at least) 20 bootstraps and computing the performance of the model
for each bootstrap.
</p>
</li></ul>

<p>Note that each level of detail has a different interpretation for bootstrap
confidence intervals. For <code>ensemble</code> and <code>model</code> these are the confidence
intervals for the ensemble and an individual model, respectively. That is,
the confidence interval describes the range where an estimate produced by a
respective ensemble or model trained on a repeat of the experiment may be
found with the probability of the confidence level. For <code>hybrid</code>, it
represents the range where any single model trained on a repeat of the
experiment may be found with the probability of the confidence level. By
definition, confidence intervals obtained using <code>hybrid</code> are at least as
wide as those for <code>ensemble</code>. <code>hybrid</code> offers the correct interpretation if
the goal of the analysis is to assess the result of a single, unspecified,
model.
</p>
<p><code>hybrid</code> is generally computationally less expensive then <code>ensemble</code>, which
in turn is somewhat less expensive than <code>model</code>.
</p>
<p>A non-default <code>detail_level</code> parameter can be specified for separate
evaluation steps by providing a parameter value in a named list with data
elements, e.g. <code>list("auc_data"="ensemble", "model_performance"="hybrid")</code>.
This parameter can be set for the following data elements: <code>auc_data</code>,
<code>decision_curve_analyis</code>, <code>model_performance</code>, <code>permutation_vimp</code>,
<code>ice_data</code>, <code>prediction_data</code> and <code>confusion_matrix</code>.</p>
</dd>
<dt><code>estimation_type</code></dt><dd><p>(<em>optional</em>) Sets the type of estimation that should be
possible. This has the following options:
</p>

<ul>
<li> <p><code>point</code>: Point estimates.
</p>
</li>
<li> <p><code>bias_correction</code> or <code>bc</code>: Bias-corrected estimates. A bias-corrected
estimate is computed from (at least) 20 point estimates, and <code>familiar</code> may
bootstrap the data to create them.
</p>
</li>
<li> <p><code>bootstrap_confidence_interval</code> or <code>bci</code> (default): Bias-corrected
estimates with bootstrap confidence intervals (Efron and Hastie, 2016). The
number of point estimates required depends on the <code>confidence_level</code>
parameter, and <code>familiar</code> may bootstrap the data to create them.
</p>
</li></ul>

<p>As with <code>detail_level</code>, a non-default <code>estimation_type</code> parameter can be
specified for separate evaluation steps by providing a parameter value in a
named list with data elements, e.g. <code>list("auc_data"="bci", "model_performance"="point")</code>. This parameter can be set for the following
data elements: <code>auc_data</code>, <code>decision_curve_analyis</code>, <code>model_performance</code>,
<code>permutation_vimp</code>, <code>ice_data</code>, and <code>prediction_data</code>.</p>
</dd>
<dt><code>confidence_level</code></dt><dd><p>(<em>optional</em>) Numeric value for the level at which
confidence intervals are determined. In the case bootstraps are used to
determine the confidence intervals bootstrap estimation, <code>familiar</code> uses the
rule of thumb <code class="reqn">n = 20 / ci.level</code> to determine the number of required
bootstraps.
</p>
<p>The default value is <code>0.95</code>.</p>
</dd>
<dt><code>bootstrap_ci_method</code></dt><dd><p>(<em>optional</em>) Method used to determine bootstrap
confidence intervals (Efron and Hastie, 2016). The following methods are
implemented:
</p>

<ul>
<li> <p><code>percentile</code> (default): Confidence intervals obtained using the percentile
method.
</p>
</li>
<li> <p><code>bc</code>: Bias-corrected confidence intervals.
</p>
</li></ul>

<p>Note that the standard method is not implemented because this method is
often not suitable due to non-normal distributions. The bias-corrected and
accelerated (BCa) method is not implemented yet.</p>
</dd>
<dt><code>stratification_method</code></dt><dd><p>(<em>optional</em>) Method for determining the
stratification threshold for creating survival groups. The actual,
model-dependent, threshold value is obtained from the development data, and
can afterwards be used to perform stratification on validation data.
</p>
<p>The following stratification methods are available:
</p>

<ul>
<li> <p><code>median</code> (default): The median predicted value in the development cohort
is used to stratify the samples into two risk groups. For predicted outcome
values that build a continuous spectrum, the two risk groups in the
development cohort will be roughly equal in size.
</p>
</li>
<li> <p><code>mean</code>: The mean predicted value in the development cohort is used to
stratify the samples into two risk groups.
</p>
</li>
<li> <p><code>mean_trim</code>: As <code>mean</code>, but based on the set of predicted values
where the 5% lowest and 5% highest values are discarded. This reduces the
effect of outliers.
</p>
</li>
<li> <p><code>mean_winsor</code>: As <code>mean</code>, but based on the set of predicted values where
the 5% lowest and 5% highest values are winsorised. This reduces the effect
of outliers.
</p>
</li>
<li> <p><code>fixed</code>: Samples are stratified based on the sample quantiles of the
predicted values. These quantiles are defined using the
<code>stratification_threshold</code> parameter.
</p>
</li>
<li> <p><code>optimised</code>: Use maximally selected rank statistics to determine the
optimal threshold (Lausen and Schumacher, 1992; Hothorn et al., 2003) to
stratify samples into two optimally separated risk groups.
</p>
</li></ul>

<p>One or more stratification methods can be selected simultaneously.
</p>
<p>This parameter is only relevant for <code>survival</code> outcomes.</p>
</dd>
<dt><code>dynamic_model_loading</code></dt><dd><p>(<em>optional</em>) Enables dynamic loading of models
during the evaluation process, if <code>TRUE</code>. Defaults to <code>FALSE</code>. Dynamic
loading of models may reduce the overall memory footprint, at the cost of
increased disk or network IO. Models can only be dynamically loaded if they
are found at an accessible disk or network location. Setting this parameter
to <code>TRUE</code> may help if parallel processing causes out-of-memory issues during
evaluation.</p>
</dd>
<dt><code>familiar_data_names</code></dt><dd><p>Names of the dataset(s). Only used if the <code>object</code> parameter
is one or more <code>familiarData</code> objects.</p>
</dd>
<dt><code>collection_name</code></dt><dd><p>Name of the collection.</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Details</h3>

<p>Data, such as model performance and calibration information, is
usually collected from a <code>familiarCollection</code> object. However, you can also
provide one or more <code>familiarData</code> objects, that will be internally
converted to a <code>familiarCollection</code> object. It is also possible to provide a
<code>familiarEnsemble</code> or one or more <code>familiarModel</code> objects together with the
data from which data is computed prior to export. Paths to the previous
files can also be provided.
</p>
<p>All parameters aside from <code>object</code> and <code>dir_path</code> are only used if <code>object</code>
is not a <code>familiarCollection</code> object, or a path to one.
</p>


<h3>Value</h3>

<p>A list of data.tables (if <code>dir_path</code> is not provided), or nothing, as
all data is exported to <code>csv</code> files.
</p>

<hr>
<h2 id='export_auc_data'>Extract and export ROC and Precision-Recall curves.</h2><span id='topic+export_auc_data'></span><span id='topic+export_auc_data+2CfamiliarCollection-method'></span><span id='topic+export_auc_data+2CANY-method'></span>

<h3>Description</h3>

<p>Extract and export ROC and Precision-Recall curves for models in
a familiarCollection.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>export_auc_data(
  object,
  dir_path = NULL,
  aggregate_results = TRUE,
  export_collection = FALSE,
  ...
)

## S4 method for signature 'familiarCollection'
export_auc_data(
  object,
  dir_path = NULL,
  aggregate_results = TRUE,
  export_collection = FALSE,
  ...
)

## S4 method for signature 'ANY'
export_auc_data(
  object,
  dir_path = NULL,
  aggregate_results = TRUE,
  export_collection = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="export_auc_data_+3A_object">object</code></td>
<td>
<p>A <code>familiarCollection</code> object, or other other objects from which
a <code>familiarCollection</code> can be extracted. See details for more information.</p>
</td></tr>
<tr><td><code id="export_auc_data_+3A_dir_path">dir_path</code></td>
<td>
<p>Path to folder where extracted data should be saved. <code>NULL</code>
will allow export as a structured list of data.tables.</p>
</td></tr>
<tr><td><code id="export_auc_data_+3A_aggregate_results">aggregate_results</code></td>
<td>
<p>Flag that signifies whether results should be
aggregated for export.</p>
</td></tr>
<tr><td><code id="export_auc_data_+3A_export_collection">export_collection</code></td>
<td>
<p>(<em>optional</em>) Exports the collection if TRUE.</p>
</td></tr>
<tr><td><code id="export_auc_data_+3A_...">...</code></td>
<td>

<p>Arguments passed on to <code><a href="#topic+extract_auc_data">extract_auc_data</a></code>, <code><a href="#topic+as_familiar_collection">as_familiar_collection</a></code>
</p>

<dl>
<dt><code>data</code></dt><dd><p>A <code>dataObject</code> object, <code>data.table</code> or <code>data.frame</code> that
constitutes the data that are assessed.</p>
</dd>
<dt><code>is_pre_processed</code></dt><dd><p>Flag that indicates whether the data was already
pre-processed externally, e.g. normalised and clustered. Only used if the
<code>data</code> argument is a <code>data.table</code> or <code>data.frame</code>.</p>
</dd>
<dt><code>cl</code></dt><dd><p>Cluster created using the <code>parallel</code> package. This cluster is then
used to speed up computation through parallellisation.</p>
</dd>
<dt><code>ensemble_method</code></dt><dd><p>Method for ensembling predictions from models for the
same sample. Available methods are:
</p>

<ul>
<li> <p><code>median</code> (default): Use the median of the predicted values as the ensemble
value for a sample.
</p>
</li>
<li> <p><code>mean</code>: Use the mean of the predicted values as the ensemble value for a
sample.
</p>
</li></ul>
</dd>
<dt><code>verbose</code></dt><dd><p>Flag to indicate whether feedback should be provided on the
computation and extraction of various data elements.</p>
</dd>
<dt><code>message_indent</code></dt><dd><p>Number of indentation steps for messages shown during
computation and extraction of various data elements.</p>
</dd>
<dt><code>detail_level</code></dt><dd><p>(<em>optional</em>) Sets the level at which results are computed
and aggregated.
</p>

<ul>
<li> <p><code>ensemble</code>: Results are computed at the ensemble level, i.e. over all
models in the ensemble. This means that, for example, bias-corrected
estimates of model performance are assessed by creating (at least) 20
bootstraps and computing the model performance of the ensemble model for
each bootstrap.
</p>
</li>
<li> <p><code>hybrid</code> (default): Results are computed at the level of models in an
ensemble. This means that, for example, bias-corrected estimates of model
performance are directly computed using the models in the ensemble. If there
are at least 20 trained models in the ensemble, performance is computed for
each model, in contrast to <code>ensemble</code> where performance is computed for the
ensemble of models. If there are less than 20 trained models in the
ensemble, bootstraps are created so that at least 20 point estimates can be
made.
</p>
</li>
<li> <p><code>model</code>: Results are computed at the model level. This means that, for
example, bias-corrected estimates of model performance are assessed by
creating (at least) 20 bootstraps and computing the performance of the model
for each bootstrap.
</p>
</li></ul>

<p>Note that each level of detail has a different interpretation for bootstrap
confidence intervals. For <code>ensemble</code> and <code>model</code> these are the confidence
intervals for the ensemble and an individual model, respectively. That is,
the confidence interval describes the range where an estimate produced by a
respective ensemble or model trained on a repeat of the experiment may be
found with the probability of the confidence level. For <code>hybrid</code>, it
represents the range where any single model trained on a repeat of the
experiment may be found with the probability of the confidence level. By
definition, confidence intervals obtained using <code>hybrid</code> are at least as
wide as those for <code>ensemble</code>. <code>hybrid</code> offers the correct interpretation if
the goal of the analysis is to assess the result of a single, unspecified,
model.
</p>
<p><code>hybrid</code> is generally computationally less expensive then <code>ensemble</code>, which
in turn is somewhat less expensive than <code>model</code>.
</p>
<p>A non-default <code>detail_level</code> parameter can be specified for separate
evaluation steps by providing a parameter value in a named list with data
elements, e.g. <code>list("auc_data"="ensemble", "model_performance"="hybrid")</code>.
This parameter can be set for the following data elements: <code>auc_data</code>,
<code>decision_curve_analyis</code>, <code>model_performance</code>, <code>permutation_vimp</code>,
<code>ice_data</code>, <code>prediction_data</code> and <code>confusion_matrix</code>.</p>
</dd>
<dt><code>estimation_type</code></dt><dd><p>(<em>optional</em>) Sets the type of estimation that should be
possible. This has the following options:
</p>

<ul>
<li> <p><code>point</code>: Point estimates.
</p>
</li>
<li> <p><code>bias_correction</code> or <code>bc</code>: Bias-corrected estimates. A bias-corrected
estimate is computed from (at least) 20 point estimates, and <code>familiar</code> may
bootstrap the data to create them.
</p>
</li>
<li> <p><code>bootstrap_confidence_interval</code> or <code>bci</code> (default): Bias-corrected
estimates with bootstrap confidence intervals (Efron and Hastie, 2016). The
number of point estimates required depends on the <code>confidence_level</code>
parameter, and <code>familiar</code> may bootstrap the data to create them.
</p>
</li></ul>

<p>As with <code>detail_level</code>, a non-default <code>estimation_type</code> parameter can be
specified for separate evaluation steps by providing a parameter value in a
named list with data elements, e.g. <code>list("auc_data"="bci", "model_performance"="point")</code>. This parameter can be set for the following
data elements: <code>auc_data</code>, <code>decision_curve_analyis</code>, <code>model_performance</code>,
<code>permutation_vimp</code>, <code>ice_data</code>, and <code>prediction_data</code>.</p>
</dd>
<dt><code>confidence_level</code></dt><dd><p>(<em>optional</em>) Numeric value for the level at which
confidence intervals are determined. In the case bootstraps are used to
determine the confidence intervals bootstrap estimation, <code>familiar</code> uses the
rule of thumb <code class="reqn">n = 20 / ci.level</code> to determine the number of required
bootstraps.
</p>
<p>The default value is <code>0.95</code>.</p>
</dd>
<dt><code>bootstrap_ci_method</code></dt><dd><p>(<em>optional</em>) Method used to determine bootstrap
confidence intervals (Efron and Hastie, 2016). The following methods are
implemented:
</p>

<ul>
<li> <p><code>percentile</code> (default): Confidence intervals obtained using the percentile
method.
</p>
</li>
<li> <p><code>bc</code>: Bias-corrected confidence intervals.
</p>
</li></ul>

<p>Note that the standard method is not implemented because this method is
often not suitable due to non-normal distributions. The bias-corrected and
accelerated (BCa) method is not implemented yet.</p>
</dd>
<dt><code>familiar_data_names</code></dt><dd><p>Names of the dataset(s). Only used if the <code>object</code> parameter
is one or more <code>familiarData</code> objects.</p>
</dd>
<dt><code>collection_name</code></dt><dd><p>Name of the collection.</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Details</h3>

<p>Data is usually collected from a <code>familiarCollection</code> object.
However, you can also provide one or more <code>familiarData</code> objects, that will
be internally converted to a <code>familiarCollection</code> object. It is also
possible to provide a <code>familiarEnsemble</code> or one or more <code>familiarModel</code>
objects together with the data from which data is computed prior to export.
Paths to the previous files can also be provided.
</p>
<p>All parameters aside from <code>object</code> and <code>dir_path</code> are only used if <code>object</code>
is not a <code>familiarCollection</code> object, or a path to one.
</p>
<p>ROC curve data are exported for individual and ensemble models. For ensemble
models, a credibility interval for the ROC curve is determined using
bootstrapping for each metric. In case of multinomial outcomes, ROC-curves
are computed for each class, using a one-against-all approach.
</p>


<h3>Value</h3>

<p>A list of data.tables (if <code>dir_path</code> is not provided), or nothing, as
all data is exported to <code>csv</code> files.
</p>

<hr>
<h2 id='export_calibration_data'>Extract and export calibration and goodness-of-fit tests.</h2><span id='topic+export_calibration_data'></span><span id='topic+export_calibration_data+2CfamiliarCollection-method'></span><span id='topic+export_calibration_data+2CANY-method'></span>

<h3>Description</h3>

<p>Extract and export calibration and goodness-of-fit tests for data
in a familiarCollection.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>export_calibration_data(
  object,
  dir_path = NULL,
  aggregate_results = TRUE,
  export_collection = FALSE,
  ...
)

## S4 method for signature 'familiarCollection'
export_calibration_data(
  object,
  dir_path = NULL,
  aggregate_results = TRUE,
  export_collection = FALSE,
  ...
)

## S4 method for signature 'ANY'
export_calibration_data(
  object,
  dir_path = NULL,
  aggregate_results = TRUE,
  export_collection = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="export_calibration_data_+3A_object">object</code></td>
<td>
<p>A <code>familiarCollection</code> object, or other other objects from which
a <code>familiarCollection</code> can be extracted. See details for more information.</p>
</td></tr>
<tr><td><code id="export_calibration_data_+3A_dir_path">dir_path</code></td>
<td>
<p>Path to folder where extracted data should be saved. <code>NULL</code>
will allow export as a structured list of data.tables.</p>
</td></tr>
<tr><td><code id="export_calibration_data_+3A_aggregate_results">aggregate_results</code></td>
<td>
<p>Flag that signifies whether results should be
aggregated for export.</p>
</td></tr>
<tr><td><code id="export_calibration_data_+3A_export_collection">export_collection</code></td>
<td>
<p>(<em>optional</em>) Exports the collection if TRUE.</p>
</td></tr>
<tr><td><code id="export_calibration_data_+3A_...">...</code></td>
<td>

<p>Arguments passed on to <code><a href="#topic+extract_calibration_data">extract_calibration_data</a></code>, <code><a href="#topic+as_familiar_collection">as_familiar_collection</a></code>
</p>

<dl>
<dt><code>data</code></dt><dd><p>A <code>dataObject</code> object, <code>data.table</code> or <code>data.frame</code> that
constitutes the data that are assessed.</p>
</dd>
<dt><code>is_pre_processed</code></dt><dd><p>Flag that indicates whether the data was already
pre-processed externally, e.g. normalised and clustered. Only used if the
<code>data</code> argument is a <code>data.table</code> or <code>data.frame</code>.</p>
</dd>
<dt><code>cl</code></dt><dd><p>Cluster created using the <code>parallel</code> package. This cluster is then
used to speed up computation through parallellisation.</p>
</dd>
<dt><code>evaluation_times</code></dt><dd><p>One or more time points that are used for in analysis of
survival problems when data has to be assessed at a set time, e.g.
calibration. If not provided explicitly, this parameter is read from
settings used at creation of the underlying <code>familiarModel</code> objects. Only
used for <code>survival</code> outcomes.</p>
</dd>
<dt><code>ensemble_method</code></dt><dd><p>Method for ensembling predictions from models for the
same sample. Available methods are:
</p>

<ul>
<li> <p><code>median</code> (default): Use the median of the predicted values as the ensemble
value for a sample.
</p>
</li>
<li> <p><code>mean</code>: Use the mean of the predicted values as the ensemble value for a
sample.
</p>
</li></ul>
</dd>
<dt><code>verbose</code></dt><dd><p>Flag to indicate whether feedback should be provided on the
computation and extraction of various data elements.</p>
</dd>
<dt><code>message_indent</code></dt><dd><p>Number of indentation steps for messages shown during
computation and extraction of various data elements.</p>
</dd>
<dt><code>detail_level</code></dt><dd><p>(<em>optional</em>) Sets the level at which results are computed
and aggregated.
</p>

<ul>
<li> <p><code>ensemble</code>: Results are computed at the ensemble level, i.e. over all
models in the ensemble. This means that, for example, bias-corrected
estimates of model performance are assessed by creating (at least) 20
bootstraps and computing the model performance of the ensemble model for
each bootstrap.
</p>
</li>
<li> <p><code>hybrid</code> (default): Results are computed at the level of models in an
ensemble. This means that, for example, bias-corrected estimates of model
performance are directly computed using the models in the ensemble. If there
are at least 20 trained models in the ensemble, performance is computed for
each model, in contrast to <code>ensemble</code> where performance is computed for the
ensemble of models. If there are less than 20 trained models in the
ensemble, bootstraps are created so that at least 20 point estimates can be
made.
</p>
</li>
<li> <p><code>model</code>: Results are computed at the model level. This means that, for
example, bias-corrected estimates of model performance are assessed by
creating (at least) 20 bootstraps and computing the performance of the model
for each bootstrap.
</p>
</li></ul>

<p>Note that each level of detail has a different interpretation for bootstrap
confidence intervals. For <code>ensemble</code> and <code>model</code> these are the confidence
intervals for the ensemble and an individual model, respectively. That is,
the confidence interval describes the range where an estimate produced by a
respective ensemble or model trained on a repeat of the experiment may be
found with the probability of the confidence level. For <code>hybrid</code>, it
represents the range where any single model trained on a repeat of the
experiment may be found with the probability of the confidence level. By
definition, confidence intervals obtained using <code>hybrid</code> are at least as
wide as those for <code>ensemble</code>. <code>hybrid</code> offers the correct interpretation if
the goal of the analysis is to assess the result of a single, unspecified,
model.
</p>
<p><code>hybrid</code> is generally computationally less expensive then <code>ensemble</code>, which
in turn is somewhat less expensive than <code>model</code>.
</p>
<p>A non-default <code>detail_level</code> parameter can be specified for separate
evaluation steps by providing a parameter value in a named list with data
elements, e.g. <code>list("auc_data"="ensemble", "model_performance"="hybrid")</code>.
This parameter can be set for the following data elements: <code>auc_data</code>,
<code>decision_curve_analyis</code>, <code>model_performance</code>, <code>permutation_vimp</code>,
<code>ice_data</code>, <code>prediction_data</code> and <code>confusion_matrix</code>.</p>
</dd>
<dt><code>estimation_type</code></dt><dd><p>(<em>optional</em>) Sets the type of estimation that should be
possible. This has the following options:
</p>

<ul>
<li> <p><code>point</code>: Point estimates.
</p>
</li>
<li> <p><code>bias_correction</code> or <code>bc</code>: Bias-corrected estimates. A bias-corrected
estimate is computed from (at least) 20 point estimates, and <code>familiar</code> may
bootstrap the data to create them.
</p>
</li>
<li> <p><code>bootstrap_confidence_interval</code> or <code>bci</code> (default): Bias-corrected
estimates with bootstrap confidence intervals (Efron and Hastie, 2016). The
number of point estimates required depends on the <code>confidence_level</code>
parameter, and <code>familiar</code> may bootstrap the data to create them.
</p>
</li></ul>

<p>As with <code>detail_level</code>, a non-default <code>estimation_type</code> parameter can be
specified for separate evaluation steps by providing a parameter value in a
named list with data elements, e.g. <code>list("auc_data"="bci", "model_performance"="point")</code>. This parameter can be set for the following
data elements: <code>auc_data</code>, <code>decision_curve_analyis</code>, <code>model_performance</code>,
<code>permutation_vimp</code>, <code>ice_data</code>, and <code>prediction_data</code>.</p>
</dd>
<dt><code>confidence_level</code></dt><dd><p>(<em>optional</em>) Numeric value for the level at which
confidence intervals are determined. In the case bootstraps are used to
determine the confidence intervals bootstrap estimation, <code>familiar</code> uses the
rule of thumb <code class="reqn">n = 20 / ci.level</code> to determine the number of required
bootstraps.
</p>
<p>The default value is <code>0.95</code>.</p>
</dd>
<dt><code>bootstrap_ci_method</code></dt><dd><p>(<em>optional</em>) Method used to determine bootstrap
confidence intervals (Efron and Hastie, 2016). The following methods are
implemented:
</p>

<ul>
<li> <p><code>percentile</code> (default): Confidence intervals obtained using the percentile
method.
</p>
</li>
<li> <p><code>bc</code>: Bias-corrected confidence intervals.
</p>
</li></ul>

<p>Note that the standard method is not implemented because this method is
often not suitable due to non-normal distributions. The bias-corrected and
accelerated (BCa) method is not implemented yet.</p>
</dd>
<dt><code>familiar_data_names</code></dt><dd><p>Names of the dataset(s). Only used if the <code>object</code> parameter
is one or more <code>familiarData</code> objects.</p>
</dd>
<dt><code>collection_name</code></dt><dd><p>Name of the collection.</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Details</h3>

<p>Data is usually collected from a <code>familiarCollection</code> object.
However, you can also provide one or more <code>familiarData</code> objects, that will
be internally converted to a <code>familiarCollection</code> object. It is also
possible to provide a <code>familiarEnsemble</code> or one or more <code>familiarModel</code>
objects together with the data from which data is computed prior to export.
Paths to the previous files can also be provided.
</p>
<p>All parameters aside from <code>object</code> and <code>dir_path</code> are only used if <code>object</code>
is not a <code>familiarCollection</code> object, or a path to one.
</p>
<p>Calibration tests are performed based on expected (predicted) and observed
outcomes. For all outcomes, calibration-at-the-large and calibration slopes
are determined. Furthermore, for all but survival outcomes, a repeated,
randomised grouping Hosmer-Lemeshow test is performed. For survival
outcomes, the Nam-D'Agostino and Greenwood-Nam-D'Agostino tests are
performed.
</p>


<h3>Value</h3>

<p>A list of data.tables (if <code>dir_path</code> is not provided), or nothing, as
all data is exported to <code>csv</code> files.
</p>

<hr>
<h2 id='export_calibration_info'>Extract and export calibration information.</h2><span id='topic+export_calibration_info'></span><span id='topic+export_calibration_info+2CfamiliarCollection-method'></span><span id='topic+export_calibration_info+2CANY-method'></span>

<h3>Description</h3>

<p>Extract and export calibration information (e.g. baseline
survival) for data in a familiarCollection.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>export_calibration_info(
  object,
  dir_path = NULL,
  aggregate_results = TRUE,
  export_collection = FALSE,
  ...
)

## S4 method for signature 'familiarCollection'
export_calibration_info(
  object,
  dir_path = NULL,
  aggregate_results = TRUE,
  export_collection = FALSE,
  ...
)

## S4 method for signature 'ANY'
export_calibration_info(
  object,
  dir_path = NULL,
  aggregate_results = TRUE,
  export_collection = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="export_calibration_info_+3A_object">object</code></td>
<td>
<p>A <code>familiarCollection</code> object, or other other objects from which
a <code>familiarCollection</code> can be extracted. See details for more information.</p>
</td></tr>
<tr><td><code id="export_calibration_info_+3A_dir_path">dir_path</code></td>
<td>
<p>Path to folder where extracted data should be saved. <code>NULL</code>
will allow export as a structured list of data.tables.</p>
</td></tr>
<tr><td><code id="export_calibration_info_+3A_aggregate_results">aggregate_results</code></td>
<td>
<p>Flag that signifies whether results should be
aggregated for export.</p>
</td></tr>
<tr><td><code id="export_calibration_info_+3A_export_collection">export_collection</code></td>
<td>
<p>(<em>optional</em>) Exports the collection if TRUE.</p>
</td></tr>
<tr><td><code id="export_calibration_info_+3A_...">...</code></td>
<td>

<p>Arguments passed on to <code><a href="#topic+as_familiar_collection">as_familiar_collection</a></code>
</p>

<dl>
<dt><code>familiar_data_names</code></dt><dd><p>Names of the dataset(s). Only used if the <code>object</code> parameter
is one or more <code>familiarData</code> objects.</p>
</dd>
<dt><code>collection_name</code></dt><dd><p>Name of the collection.</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Details</h3>

<p>Data is usually collected from a <code>familiarCollection</code> object.
However, you can also provide one or more <code>familiarData</code> objects, that will
be internally converted to a <code>familiarCollection</code> object. It is also
possible to provide a <code>familiarEnsemble</code> or one or more <code>familiarModel</code>
objects together with the data from which data is computed prior to export.
Paths to the previous files can also be provided.
</p>
<p>All parameters aside from <code>object</code> and <code>dir_path</code> are only used if <code>object</code>
is not a <code>familiarCollection</code> object, or a path to one.
</p>
<p>Currently only baseline survival is exported as supporting calibration
information. See <code>export_calibration_data</code> for export of direct assessment
of calibration, including calibration and goodness-of-fit tests.
</p>


<h3>Value</h3>

<p>A data.table (if <code>dir_path</code> is not provided), or nothing, as all data
is exported to <code>csv</code> files.
</p>

<hr>
<h2 id='export_confusion_matrix_data'>Extract and export confusion matrices.</h2><span id='topic+export_confusion_matrix_data'></span><span id='topic+export_confusion_matrix_data+2CfamiliarCollection-method'></span><span id='topic+export_confusion_matrix_data+2CANY-method'></span>

<h3>Description</h3>

<p>Extract and export confusion matrics for models in a
familiarCollection.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>export_confusion_matrix_data(
  object,
  dir_path = NULL,
  export_collection = FALSE,
  ...
)

## S4 method for signature 'familiarCollection'
export_confusion_matrix_data(
  object,
  dir_path = NULL,
  export_collection = FALSE,
  ...
)

## S4 method for signature 'ANY'
export_confusion_matrix_data(
  object,
  dir_path = NULL,
  export_collection = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="export_confusion_matrix_data_+3A_object">object</code></td>
<td>
<p>A <code>familiarCollection</code> object, or other other objects from which
a <code>familiarCollection</code> can be extracted. See details for more information.</p>
</td></tr>
<tr><td><code id="export_confusion_matrix_data_+3A_dir_path">dir_path</code></td>
<td>
<p>Path to folder where extracted data should be saved. <code>NULL</code>
will allow export as a structured list of data.tables.</p>
</td></tr>
<tr><td><code id="export_confusion_matrix_data_+3A_export_collection">export_collection</code></td>
<td>
<p>(<em>optional</em>) Exports the collection if TRUE.</p>
</td></tr>
<tr><td><code id="export_confusion_matrix_data_+3A_...">...</code></td>
<td>

<p>Arguments passed on to <code><a href="#topic+extract_confusion_matrix">extract_confusion_matrix</a></code>, <code><a href="#topic+as_familiar_collection">as_familiar_collection</a></code>
</p>

<dl>
<dt><code>data</code></dt><dd><p>A <code>dataObject</code> object, <code>data.table</code> or <code>data.frame</code> that
constitutes the data that are assessed.</p>
</dd>
<dt><code>is_pre_processed</code></dt><dd><p>Flag that indicates whether the data was already
pre-processed externally, e.g. normalised and clustered. Only used if the
<code>data</code> argument is a <code>data.table</code> or <code>data.frame</code>.</p>
</dd>
<dt><code>cl</code></dt><dd><p>Cluster created using the <code>parallel</code> package. This cluster is then
used to speed up computation through parallellisation.</p>
</dd>
<dt><code>ensemble_method</code></dt><dd><p>Method for ensembling predictions from models for the
same sample. Available methods are:
</p>

<ul>
<li> <p><code>median</code> (default): Use the median of the predicted values as the ensemble
value for a sample.
</p>
</li>
<li> <p><code>mean</code>: Use the mean of the predicted values as the ensemble value for a
sample.
</p>
</li></ul>
</dd>
<dt><code>verbose</code></dt><dd><p>Flag to indicate whether feedback should be provided on the
computation and extraction of various data elements.</p>
</dd>
<dt><code>message_indent</code></dt><dd><p>Number of indentation steps for messages shown during
computation and extraction of various data elements.</p>
</dd>
<dt><code>detail_level</code></dt><dd><p>(<em>optional</em>) Sets the level at which results are computed
and aggregated.
</p>

<ul>
<li> <p><code>ensemble</code>: Results are computed at the ensemble level, i.e. over all
models in the ensemble. This means that, for example, bias-corrected
estimates of model performance are assessed by creating (at least) 20
bootstraps and computing the model performance of the ensemble model for
each bootstrap.
</p>
</li>
<li> <p><code>hybrid</code> (default): Results are computed at the level of models in an
ensemble. This means that, for example, bias-corrected estimates of model
performance are directly computed using the models in the ensemble. If there
are at least 20 trained models in the ensemble, performance is computed for
each model, in contrast to <code>ensemble</code> where performance is computed for the
ensemble of models. If there are less than 20 trained models in the
ensemble, bootstraps are created so that at least 20 point estimates can be
made.
</p>
</li>
<li> <p><code>model</code>: Results are computed at the model level. This means that, for
example, bias-corrected estimates of model performance are assessed by
creating (at least) 20 bootstraps and computing the performance of the model
for each bootstrap.
</p>
</li></ul>

<p>Note that each level of detail has a different interpretation for bootstrap
confidence intervals. For <code>ensemble</code> and <code>model</code> these are the confidence
intervals for the ensemble and an individual model, respectively. That is,
the confidence interval describes the range where an estimate produced by a
respective ensemble or model trained on a repeat of the experiment may be
found with the probability of the confidence level. For <code>hybrid</code>, it
represents the range where any single model trained on a repeat of the
experiment may be found with the probability of the confidence level. By
definition, confidence intervals obtained using <code>hybrid</code> are at least as
wide as those for <code>ensemble</code>. <code>hybrid</code> offers the correct interpretation if
the goal of the analysis is to assess the result of a single, unspecified,
model.
</p>
<p><code>hybrid</code> is generally computationally less expensive then <code>ensemble</code>, which
in turn is somewhat less expensive than <code>model</code>.
</p>
<p>A non-default <code>detail_level</code> parameter can be specified for separate
evaluation steps by providing a parameter value in a named list with data
elements, e.g. <code>list("auc_data"="ensemble", "model_performance"="hybrid")</code>.
This parameter can be set for the following data elements: <code>auc_data</code>,
<code>decision_curve_analyis</code>, <code>model_performance</code>, <code>permutation_vimp</code>,
<code>ice_data</code>, <code>prediction_data</code> and <code>confusion_matrix</code>.</p>
</dd>
<dt><code>familiar_data_names</code></dt><dd><p>Names of the dataset(s). Only used if the <code>object</code> parameter
is one or more <code>familiarData</code> objects.</p>
</dd>
<dt><code>collection_name</code></dt><dd><p>Name of the collection.</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Details</h3>

<p>Data is usually collected from a <code>familiarCollection</code> object.
However, you can also provide one or more <code>familiarData</code> objects, that will
be internally converted to a <code>familiarCollection</code> object. It is also
possible to provide a <code>familiarEnsemble</code> or one or more <code>familiarModel</code>
objects together with the data from which data is computed prior to export.
Paths to the previous files can also be provided.
</p>
<p>All parameters aside from <code>object</code> and <code>dir_path</code> are only used if <code>object</code>
is not a <code>familiarCollection</code> object, or a path to one.
</p>
<p>Confusion matrices are exported for individual and ensemble models.
</p>


<h3>Value</h3>

<p>A list of data.tables (if <code>dir_path</code> is not provided), or nothing, as
all data is exported to <code>csv</code> files.
</p>

<hr>
<h2 id='export_decision_curve_analysis_data'>Extract and export decision curve analysis data.</h2><span id='topic+export_decision_curve_analysis_data'></span><span id='topic+export_decision_curve_analysis_data+2CfamiliarCollection-method'></span><span id='topic+export_decision_curve_analysis_data+2CANY-method'></span>

<h3>Description</h3>

<p>Extract and export decision curve analysis data in a
familiarCollection.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>export_decision_curve_analysis_data(
  object,
  dir_path = NULL,
  aggregate_results = TRUE,
  ...
)

## S4 method for signature 'familiarCollection'
export_decision_curve_analysis_data(
  object,
  dir_path = NULL,
  aggregate_results = TRUE,
  ...
)

## S4 method for signature 'ANY'
export_decision_curve_analysis_data(
  object,
  dir_path = NULL,
  aggregate_results = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="export_decision_curve_analysis_data_+3A_object">object</code></td>
<td>
<p>A <code>familiarCollection</code> object, or other other objects from which
a <code>familiarCollection</code> can be extracted. See details for more information.</p>
</td></tr>
<tr><td><code id="export_decision_curve_analysis_data_+3A_dir_path">dir_path</code></td>
<td>
<p>Path to folder where extracted data should be saved. <code>NULL</code>
will allow export as a structured list of data.tables.</p>
</td></tr>
<tr><td><code id="export_decision_curve_analysis_data_+3A_aggregate_results">aggregate_results</code></td>
<td>
<p>Flag that signifies whether results should be
aggregated for export.</p>
</td></tr>
<tr><td><code id="export_decision_curve_analysis_data_+3A_...">...</code></td>
<td>

<p>Arguments passed on to <code><a href="#topic+as_familiar_collection">as_familiar_collection</a></code>
</p>

<dl>
<dt><code>familiar_data_names</code></dt><dd><p>Names of the dataset(s). Only used if the <code>object</code> parameter
is one or more <code>familiarData</code> objects.</p>
</dd>
<dt><code>collection_name</code></dt><dd><p>Name of the collection.</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Details</h3>

<p>Data is usually collected from a <code>familiarCollection</code> object.
However, you can also provide one or more <code>familiarData</code> objects, that will
be internally converted to a <code>familiarCollection</code> object. It is also
possible to provide a <code>familiarEnsemble</code> or one or more <code>familiarModel</code>
objects together with the data from which data is computed prior to export.
Paths to the previous files can also be provided.
</p>
<p>All parameters aside from <code>object</code> and <code>dir_path</code> are only used if <code>object</code>
is not a <code>familiarCollection</code> object, or a path to one.
</p>
<p>Decision curve analysis data is computed for categorical outcomes, i.e.
binomial and multinomial, as well as survival outcomes.
</p>


<h3>Value</h3>

<p>A list of data.table (if <code>dir_path</code> is not provided), or nothing, as
all data is exported to <code>csv</code> files.
</p>

<hr>
<h2 id='export_feature_expressions'>Extract and export feature expressions.</h2><span id='topic+export_feature_expressions'></span><span id='topic+export_feature_expressions+2CfamiliarCollection-method'></span><span id='topic+export_feature_expressions+2CANY-method'></span>

<h3>Description</h3>

<p>Extract and export feature expressions for the features in a
familiarCollection.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>export_feature_expressions(
  object,
  dir_path = NULL,
  evaluation_time = waiver(),
  export_collection = FALSE,
  ...
)

## S4 method for signature 'familiarCollection'
export_feature_expressions(
  object,
  dir_path = NULL,
  evaluation_time = waiver(),
  export_collection = FALSE,
  ...
)

## S4 method for signature 'ANY'
export_feature_expressions(
  object,
  dir_path = NULL,
  evaluation_time = waiver(),
  export_collection = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="export_feature_expressions_+3A_object">object</code></td>
<td>
<p>A <code>familiarCollection</code> object, or other other objects from which
a <code>familiarCollection</code> can be extracted. See details for more information.</p>
</td></tr>
<tr><td><code id="export_feature_expressions_+3A_dir_path">dir_path</code></td>
<td>
<p>Path to folder where extracted data should be saved. <code>NULL</code>
will allow export as a structured list of data.tables.</p>
</td></tr>
<tr><td><code id="export_feature_expressions_+3A_evaluation_time">evaluation_time</code></td>
<td>
<p>One or more time points that are used to create the
outcome columns in expression plots. If not provided explicitly, this
parameter is read from settings used at creation of the underlying
<code>familiarData</code> objects. Only used for <code>survival</code> outcomes.</p>
</td></tr>
<tr><td><code id="export_feature_expressions_+3A_export_collection">export_collection</code></td>
<td>
<p>(<em>optional</em>) Exports the collection if TRUE.</p>
</td></tr>
<tr><td><code id="export_feature_expressions_+3A_...">...</code></td>
<td>

<p>Arguments passed on to <code><a href="#topic+extract_feature_expression">extract_feature_expression</a></code>, <code><a href="#topic+as_familiar_collection">as_familiar_collection</a></code>
</p>

<dl>
<dt><code>feature_similarity</code></dt><dd><p>Table containing pairwise distance between
sample. This is used to determine cluster information, and indicate which
samples are similar. The table is created by the
<code>extract_sample_similarity</code> method.</p>
</dd>
<dt><code>data</code></dt><dd><p>A <code>dataObject</code> object, <code>data.table</code> or <code>data.frame</code> that
constitutes the data that are assessed.</p>
</dd>
<dt><code>evaluation_times</code></dt><dd><p>One or more time points that are used for in analysis of
survival problems when data has to be assessed at a set time, e.g.
calibration. If not provided explicitly, this parameter is read from
settings used at creation of the underlying <code>familiarModel</code> objects. Only
used for <code>survival</code> outcomes.</p>
</dd>
<dt><code>feature_cluster_method</code></dt><dd><p>The method used to perform clustering. These are
the same methods as for the <code>cluster_method</code> configuration parameter:
<code>none</code>, <code>hclust</code>, <code>agnes</code>, <code>diana</code> and <code>pam</code>.
</p>
<p><code>none</code> cannot be used when extracting data regarding mutual correlation or
feature expressions.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</dd>
<dt><code>feature_linkage_method</code></dt><dd><p>The method used for agglomerative clustering in
<code>hclust</code> and <code>agnes</code>. These are the same methods as for the
<code>cluster_linkage_method</code> configuration parameter: <code>average</code>, <code>single</code>,
<code>complete</code>, <code>weighted</code>, and <code>ward</code>.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</dd>
<dt><code>feature_similarity_metric</code></dt><dd><p>Metric to determine pairwise similarity
between features. Similarity is computed in the same manner as for
clustering, and <code>feature_similarity_metric</code> therefore has the same options
as <code>cluster_similarity_metric</code>: <code>mcfadden_r2</code>, <code>cox_snell_r2</code>,
<code>nagelkerke_r2</code>, <code>spearman</code>, <code>kendall</code> and <code>pearson</code>.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</dd>
<dt><code>sample_cluster_method</code></dt><dd><p>The method used to perform clustering based on
distance between samples. These are the same methods as for the
<code>cluster_method</code> configuration parameter: <code>hclust</code>, <code>agnes</code>, <code>diana</code> and
<code>pam</code>.
</p>
<p><code>none</code> cannot be used when extracting data for feature expressions.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</dd>
<dt><code>sample_linkage_method</code></dt><dd><p>The method used for agglomerative clustering in
<code>hclust</code> and <code>agnes</code>. These are the same methods as for the
<code>cluster_linkage_method</code> configuration parameter: <code>average</code>, <code>single</code>,
<code>complete</code>, <code>weighted</code>, and <code>ward</code>.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</dd>
<dt><code>sample_similarity_metric</code></dt><dd><p>Metric to determine pairwise similarity
between samples. Similarity is computed in the same manner as for
clustering, but <code>sample_similarity_metric</code> has different options that are
better suited to computing distance between samples instead of between
features: <code>gower</code>, <code>euclidean</code>.
</p>
<p>The underlying feature data is scaled to the <code class="reqn">[0, 1]</code> range (for
numerical features) using the feature values across the samples. The
normalisation parameters required can optionally be computed from feature
data with the outer 5% (on both sides) of feature values trimmed or
winsorised. To do so append <code style="white-space: pre;">&#8288;_trim&#8288;</code> (trimming) or <code style="white-space: pre;">&#8288;_winsor&#8288;</code> (winsorising) to
the metric name. This reduces the effect of outliers somewhat.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</dd>
<dt><code>verbose</code></dt><dd><p>Flag to indicate whether feedback should be provided on the
computation and extraction of various data elements.</p>
</dd>
<dt><code>message_indent</code></dt><dd><p>Number of indentation steps for messages shown during
computation and extraction of various data elements.</p>
</dd>
<dt><code>familiar_data_names</code></dt><dd><p>Names of the dataset(s). Only used if the <code>object</code> parameter
is one or more <code>familiarData</code> objects.</p>
</dd>
<dt><code>collection_name</code></dt><dd><p>Name of the collection.</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Details</h3>

<p>Data is usually collected from a <code>familiarCollection</code> object.
However, you can also provide one or more <code>familiarData</code> objects, that will
be internally converted to a <code>familiarCollection</code> object. It is also
possible to provide a <code>familiarEnsemble</code> or one or more <code>familiarModel</code>
objects together with the data from which data is computed prior to export.
Paths to the previous files can also be provided.
</p>
<p>All parameters aside from <code>object</code> and <code>dir_path</code> are only used if <code>object</code>
is not a <code>familiarCollection</code> object, or a path to one.
</p>
<p>Feature expressions are computed by standardising each feature, i.e. sample
mean is 0 and standard deviation is 1.
</p>


<h3>Value</h3>

<p>A data.table (if <code>dir_path</code> is not provided), or nothing, as all data
is exported to <code>csv</code> files.
</p>

<hr>
<h2 id='export_feature_similarity'>Extract and export mutual correlation between features.</h2><span id='topic+export_feature_similarity'></span><span id='topic+export_feature_similarity+2CfamiliarCollection-method'></span><span id='topic+export_feature_similarity+2CANY-method'></span>

<h3>Description</h3>

<p>Extract and export mutual correlation between features in a
familiarCollection.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>export_feature_similarity(
  object,
  dir_path = NULL,
  aggregate_results = TRUE,
  feature_cluster_method = waiver(),
  feature_linkage_method = waiver(),
  feature_cluster_cut_method = waiver(),
  feature_similarity_threshold = waiver(),
  export_dendrogram = FALSE,
  export_ordered_data = FALSE,
  export_clustering = FALSE,
  export_collection = FALSE,
  ...
)

## S4 method for signature 'familiarCollection'
export_feature_similarity(
  object,
  dir_path = NULL,
  aggregate_results = TRUE,
  feature_cluster_method = waiver(),
  feature_linkage_method = waiver(),
  feature_cluster_cut_method = waiver(),
  feature_similarity_threshold = waiver(),
  export_dendrogram = FALSE,
  export_ordered_data = FALSE,
  export_clustering = FALSE,
  export_collection = FALSE,
  ...
)

## S4 method for signature 'ANY'
export_feature_similarity(
  object,
  dir_path = NULL,
  aggregate_results = TRUE,
  feature_cluster_method = waiver(),
  feature_linkage_method = waiver(),
  feature_cluster_cut_method = waiver(),
  feature_similarity_threshold = waiver(),
  export_dendrogram = FALSE,
  export_ordered_data = FALSE,
  export_clustering = FALSE,
  export_collection = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="export_feature_similarity_+3A_object">object</code></td>
<td>
<p>A <code>familiarCollection</code> object, or other other objects from which
a <code>familiarCollection</code> can be extracted. See details for more information.</p>
</td></tr>
<tr><td><code id="export_feature_similarity_+3A_dir_path">dir_path</code></td>
<td>
<p>Path to folder where extracted data should be saved. <code>NULL</code>
will allow export as a structured list of data.tables.</p>
</td></tr>
<tr><td><code id="export_feature_similarity_+3A_aggregate_results">aggregate_results</code></td>
<td>
<p>Flag that signifies whether results should be
aggregated for export.</p>
</td></tr>
<tr><td><code id="export_feature_similarity_+3A_feature_cluster_method">feature_cluster_method</code></td>
<td>
<p>The method used to perform clustering. These are
the same methods as for the <code>cluster_method</code> configuration parameter:
<code>none</code>, <code>hclust</code>, <code>agnes</code>, <code>diana</code> and <code>pam</code>.
</p>
<p><code>none</code> cannot be used when extracting data regarding mutual correlation or
feature expressions.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="export_feature_similarity_+3A_feature_linkage_method">feature_linkage_method</code></td>
<td>
<p>The method used for agglomerative clustering in
<code>hclust</code> and <code>agnes</code>. These are the same methods as for the
<code>cluster_linkage_method</code> configuration parameter: <code>average</code>, <code>single</code>,
<code>complete</code>, <code>weighted</code>, and <code>ward</code>.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="export_feature_similarity_+3A_feature_cluster_cut_method">feature_cluster_cut_method</code></td>
<td>
<p>The method used to divide features into
separate clusters. The available methods are the same as for the
<code>cluster_cut_method</code> configuration parameter: <code>silhouette</code>, <code>fixed_cut</code> and
<code>dynamic_cut</code>.
</p>
<p><code>silhouette</code> is available for all cluster methods, but <code>fixed_cut</code> only
applies to methods that create hierarchical trees (<code>hclust</code>, <code>agnes</code> and
<code>diana</code>). <code>dynamic_cut</code> requires the <code>dynamicTreeCut</code> package and can only
be used with <code>agnes</code> and <code>hclust</code>.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="export_feature_similarity_+3A_feature_similarity_threshold">feature_similarity_threshold</code></td>
<td>
<p>The threshold level for pair-wise
similarity that is required to form feature clusters with the <code>fixed_cut</code>
method.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="export_feature_similarity_+3A_export_dendrogram">export_dendrogram</code></td>
<td>
<p>Add dendrogram in the data element objects.</p>
</td></tr>
<tr><td><code id="export_feature_similarity_+3A_export_ordered_data">export_ordered_data</code></td>
<td>
<p>Add feature label ordering to data in the data
element objects.</p>
</td></tr>
<tr><td><code id="export_feature_similarity_+3A_export_clustering">export_clustering</code></td>
<td>
<p>Add clustering information to data.</p>
</td></tr>
<tr><td><code id="export_feature_similarity_+3A_export_collection">export_collection</code></td>
<td>
<p>(<em>optional</em>) Exports the collection if TRUE.</p>
</td></tr>
<tr><td><code id="export_feature_similarity_+3A_...">...</code></td>
<td>

<p>Arguments passed on to <code><a href="#topic+as_familiar_collection">as_familiar_collection</a></code>
</p>

<dl>
<dt><code>familiar_data_names</code></dt><dd><p>Names of the dataset(s). Only used if the <code>object</code> parameter
is one or more <code>familiarData</code> objects.</p>
</dd>
<dt><code>collection_name</code></dt><dd><p>Name of the collection.</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Details</h3>

<p>Data is usually collected from a <code>familiarCollection</code> object.
However, you can also provide one or more <code>familiarData</code> objects, that will
be internally converted to a <code>familiarCollection</code> object. It is also
possible to provide a <code>familiarEnsemble</code> or one or more <code>familiarModel</code>
objects together with the data from which data is computed prior to export.
Paths to the previous files can also be provided.
</p>
<p>All parameters aside from <code>object</code> and <code>dir_path</code> are only used if <code>object</code>
is not a <code>familiarCollection</code> object, or a path to one.
</p>


<h3>Value</h3>

<p>A list containing a data.table (if <code>dir_path</code> is not provided), or
nothing, as all data is exported to <code>csv</code> files.
</p>

<hr>
<h2 id='export_fs_vimp'>Extract and export feature selection variable importance.</h2><span id='topic+export_fs_vimp'></span><span id='topic+export_fs_vimp+2CfamiliarCollection-method'></span><span id='topic+export_fs_vimp+2CANY-method'></span>

<h3>Description</h3>

<p>Extract and export feature selection variable importance from a
familiarCollection.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>export_fs_vimp(
  object,
  dir_path = NULL,
  aggregate_results = TRUE,
  aggregation_method = waiver(),
  rank_threshold = waiver(),
  export_collection = FALSE,
  ...
)

## S4 method for signature 'familiarCollection'
export_fs_vimp(
  object,
  dir_path = NULL,
  aggregate_results = TRUE,
  aggregation_method = waiver(),
  rank_threshold = waiver(),
  export_collection = FALSE,
  ...
)

## S4 method for signature 'ANY'
export_fs_vimp(
  object,
  dir_path = NULL,
  aggregate_results = TRUE,
  aggregation_method = waiver(),
  rank_threshold = waiver(),
  export_collection = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="export_fs_vimp_+3A_object">object</code></td>
<td>
<p>A <code>familiarCollection</code> object, or other other objects from which
a <code>familiarCollection</code> can be extracted. See details for more information.</p>
</td></tr>
<tr><td><code id="export_fs_vimp_+3A_dir_path">dir_path</code></td>
<td>
<p>Path to folder where extracted data should be saved. <code>NULL</code>
will allow export as a structured list of data.tables.</p>
</td></tr>
<tr><td><code id="export_fs_vimp_+3A_aggregate_results">aggregate_results</code></td>
<td>
<p>Flag that signifies whether results should be
aggregated for export.</p>
</td></tr>
<tr><td><code id="export_fs_vimp_+3A_aggregation_method">aggregation_method</code></td>
<td>
<p>(<em>optional</em>) The method used to aggregate variable
importances over different data subsets, e.g. bootstraps. The following
methods can be selected:
</p>

<ul>
<li> <p><code>mean</code> (default): Use the mean rank of a feature over the subsets to
determine the aggregated feature rank.
</p>
</li>
<li> <p><code>median</code>: Use the median rank of a feature over the subsets to determine
the aggregated feature rank.
</p>
</li>
<li> <p><code>best</code>: Use the best rank the feature obtained in any subset to determine
the aggregated feature rank.
</p>
</li>
<li> <p><code>worst</code>: Use the worst rank the feature obtained in any subset to
determine the aggregated feature rank.
</p>
</li>
<li> <p><code>stability</code>: Use the frequency of the feature being in the subset of
highly ranked features as measure for the aggregated feature rank
(Meinshausen and Buehlmann, 2010).
</p>
</li>
<li> <p><code>exponential</code>: Use a rank-weighted frequence of occurrence in the subset
of highly ranked features as measure for the aggregated feature rank (Haury
et al., 2011).
</p>
</li>
<li> <p><code>borda</code>: Use the borda count as measure for the aggregated feature rank
(Wald et al., 2012).
</p>
</li>
<li> <p><code>enhanced_borda</code>: Use an occurrence frequency-weighted borda count as
measure for the aggregated feature rank (Wald et al., 2012).
</p>
</li>
<li> <p><code>truncated_borda</code>: Use borda count computed only on features within the
subset of highly ranked features.
</p>
</li>
<li> <p><code>enhanced_truncated_borda</code>: Apply both the enhanced borda method and the
truncated borda method and use the resulting borda count as the aggregated
feature rank.
</p>
</li></ul>
</td></tr>
<tr><td><code id="export_fs_vimp_+3A_rank_threshold">rank_threshold</code></td>
<td>
<p>(<em>optional</em>) The threshold used to define the subset of
highly important features. If not set, this threshold is determined by
maximising the variance in the occurrence value over all features over the
subset size.
</p>
<p>This parameter is only relevant for <code>stability</code>, <code>exponential</code>,
<code>enhanced_borda</code>, <code>truncated_borda</code> and <code>enhanced_truncated_borda</code> methods.</p>
</td></tr>
<tr><td><code id="export_fs_vimp_+3A_export_collection">export_collection</code></td>
<td>
<p>(<em>optional</em>) Exports the collection if TRUE.</p>
</td></tr>
<tr><td><code id="export_fs_vimp_+3A_...">...</code></td>
<td>

<p>Arguments passed on to <code><a href="#topic+as_familiar_collection">as_familiar_collection</a></code>
</p>

<dl>
<dt><code>familiar_data_names</code></dt><dd><p>Names of the dataset(s). Only used if the <code>object</code> parameter
is one or more <code>familiarData</code> objects.</p>
</dd>
<dt><code>collection_name</code></dt><dd><p>Name of the collection.</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Details</h3>

<p>Data, such as model performance and calibration information, is
usually collected from a <code>familiarCollection</code> object. However, you can also
provide one or more <code>familiarData</code> objects, that will be internally
converted to a <code>familiarCollection</code> object. Paths to the previous files can
also be provided.
</p>
<p>Unlike other export function, export using <code>familiarEnsemble</code> or
<code>familiarModel</code> objects is not possible. This is because feature selection
variable importance is not stored within <code>familiarModel</code> objects.
</p>
<p>All parameters aside from <code>object</code> and <code>dir_path</code> are only used if <code>object</code>
is not a <code>familiarCollection</code> object, or a path to one.
</p>
<p>Variable importance is based on the ranking produced by feature selection
routines. In case feature selection was performed repeatedly, e.g. using
bootstraps, feature ranks are first aggregated using the method defined by
the <code>aggregation_method</code>, some of which require a <code>rank_threshold</code> to
indicate a subset of most important features.
</p>
<p>Information concerning highly similar features that form clusters is
provided as well. This information is based on consensus clustering of the
features. This clustering information is also used during aggregation to
ensure that co-clustered features are only taken into account once.
</p>


<h3>Value</h3>

<p>A data.table (if <code>dir_path</code> is not provided), or nothing, as all data
is exported to <code>csv</code> files.
</p>

<hr>
<h2 id='export_hyperparameters'>Extract and export model hyperparameters.</h2><span id='topic+export_hyperparameters'></span><span id='topic+export_hyperparameters+2CfamiliarCollection-method'></span><span id='topic+export_hyperparameters+2CANY-method'></span>

<h3>Description</h3>

<p>Extract and export model hyperparameters from models in a
familiarCollection.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>export_hyperparameters(
  object,
  dir_path = NULL,
  aggregate_results = TRUE,
  export_collection = FALSE,
  ...
)

## S4 method for signature 'familiarCollection'
export_hyperparameters(
  object,
  dir_path = NULL,
  aggregate_results = TRUE,
  export_collection = FALSE,
  ...
)

## S4 method for signature 'ANY'
export_hyperparameters(
  object,
  dir_path = NULL,
  aggregate_results = TRUE,
  export_collection = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="export_hyperparameters_+3A_object">object</code></td>
<td>
<p>A <code>familiarCollection</code> object, or other other objects from which
a <code>familiarCollection</code> can be extracted. See details for more information.</p>
</td></tr>
<tr><td><code id="export_hyperparameters_+3A_dir_path">dir_path</code></td>
<td>
<p>Path to folder where extracted data should be saved. <code>NULL</code>
will allow export as a structured list of data.tables.</p>
</td></tr>
<tr><td><code id="export_hyperparameters_+3A_aggregate_results">aggregate_results</code></td>
<td>
<p>Flag that signifies whether results should be
aggregated for export.</p>
</td></tr>
<tr><td><code id="export_hyperparameters_+3A_export_collection">export_collection</code></td>
<td>
<p>(<em>optional</em>) Exports the collection if TRUE.</p>
</td></tr>
<tr><td><code id="export_hyperparameters_+3A_...">...</code></td>
<td>

<p>Arguments passed on to <code><a href="#topic+as_familiar_collection">as_familiar_collection</a></code>
</p>

<dl>
<dt><code>familiar_data_names</code></dt><dd><p>Names of the dataset(s). Only used if the <code>object</code> parameter
is one or more <code>familiarData</code> objects.</p>
</dd>
<dt><code>collection_name</code></dt><dd><p>Name of the collection.</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Details</h3>

<p>Data, such as model performance and calibration information, is
usually collected from a <code>familiarCollection</code> object. However, you can also
provide one or more <code>familiarData</code> objects, that will be internally
converted to a <code>familiarCollection</code> object. It is also possible to provide a
<code>familiarEnsemble</code> or one or more <code>familiarModel</code> objects together with the
data from which data is computed prior to export. Paths to the previous
files can also be provided.
</p>
<p>All parameters aside from <code>object</code> and <code>dir_path</code> are only used if <code>object</code>
is not a <code>familiarCollection</code> object, or a path to one.
</p>
<p>Many model hyperparameters are optimised using sequential model-based
optimisation. The extracted hyperparameters are those that were selected to
construct the underlying models (<code>familiarModel</code> objects).
</p>


<h3>Value</h3>

<p>A data.table (if <code>dir_path</code> is not provided), or nothing, as all data
is exported to <code>csv</code> files. In case of the latter, hyperparameters are
summarised.
</p>

<hr>
<h2 id='export_ice_data'>Extract and export individual conditional expectation data.</h2><span id='topic+export_ice_data'></span><span id='topic+export_ice_data+2CfamiliarCollection-method'></span><span id='topic+export_ice_data+2CANY-method'></span>

<h3>Description</h3>

<p>Extract and export individual conditional expectation data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>export_ice_data(
  object,
  dir_path = NULL,
  aggregate_results = TRUE,
  export_collection = FALSE,
  ...
)

## S4 method for signature 'familiarCollection'
export_ice_data(
  object,
  dir_path = NULL,
  aggregate_results = TRUE,
  export_collection = FALSE,
  ...
)

## S4 method for signature 'ANY'
export_ice_data(
  object,
  dir_path = NULL,
  aggregate_results = TRUE,
  export_collection = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="export_ice_data_+3A_object">object</code></td>
<td>
<p>A <code>familiarCollection</code> object, or other other objects from which
a <code>familiarCollection</code> can be extracted. See details for more information.</p>
</td></tr>
<tr><td><code id="export_ice_data_+3A_dir_path">dir_path</code></td>
<td>
<p>Path to folder where extracted data should be saved. <code>NULL</code>
will allow export as a structured list of data.tables.</p>
</td></tr>
<tr><td><code id="export_ice_data_+3A_aggregate_results">aggregate_results</code></td>
<td>
<p>Flag that signifies whether results should be
aggregated for export.</p>
</td></tr>
<tr><td><code id="export_ice_data_+3A_export_collection">export_collection</code></td>
<td>
<p>(<em>optional</em>) Exports the collection if TRUE.</p>
</td></tr>
<tr><td><code id="export_ice_data_+3A_...">...</code></td>
<td>

<p>Arguments passed on to <code><a href="#topic+extract_ice">extract_ice</a></code>, <code><a href="#topic+as_familiar_collection">as_familiar_collection</a></code>
</p>

<dl>
<dt><code>features</code></dt><dd><p>Names of the feature or features (2) assessed simultaneously.
By default <code>NULL</code>, which means that all features are assessed one-by-one.</p>
</dd>
<dt><code>feature_x_range</code></dt><dd><p>When one or two features are defined using <code>features</code>,
<code>feature_x_range</code> can be used to set the range of values for the first
feature. For numeric features, a vector of two values is assumed to indicate
a range from which <code>n_sample_points</code> are uniformly sampled. A vector of more
than two values is interpreted as is, i.e. these represent the values to be
sampled. For categorical features, values should represent a (sub)set of
available levels.</p>
</dd>
<dt><code>feature_y_range</code></dt><dd><p>As <code>feature_x_range</code>, but for the second feature in
case two features are defined.</p>
</dd>
<dt><code>n_sample_points</code></dt><dd><p>Number of points used to sample continuous features.</p>
</dd>
<dt><code>data</code></dt><dd><p>A <code>dataObject</code> object, <code>data.table</code> or <code>data.frame</code> that
constitutes the data that are assessed.</p>
</dd>
<dt><code>is_pre_processed</code></dt><dd><p>Flag that indicates whether the data was already
pre-processed externally, e.g. normalised and clustered. Only used if the
<code>data</code> argument is a <code>data.table</code> or <code>data.frame</code>.</p>
</dd>
<dt><code>cl</code></dt><dd><p>Cluster created using the <code>parallel</code> package. This cluster is then
used to speed up computation through parallellisation.</p>
</dd>
<dt><code>evaluation_times</code></dt><dd><p>One or more time points that are used for in analysis of
survival problems when data has to be assessed at a set time, e.g.
calibration. If not provided explicitly, this parameter is read from
settings used at creation of the underlying <code>familiarModel</code> objects. Only
used for <code>survival</code> outcomes.</p>
</dd>
<dt><code>ensemble_method</code></dt><dd><p>Method for ensembling predictions from models for the
same sample. Available methods are:
</p>

<ul>
<li> <p><code>median</code> (default): Use the median of the predicted values as the ensemble
value for a sample.
</p>
</li>
<li> <p><code>mean</code>: Use the mean of the predicted values as the ensemble value for a
sample.
</p>
</li></ul>
</dd>
<dt><code>verbose</code></dt><dd><p>Flag to indicate whether feedback should be provided on the
computation and extraction of various data elements.</p>
</dd>
<dt><code>message_indent</code></dt><dd><p>Number of indentation steps for messages shown during
computation and extraction of various data elements.</p>
</dd>
<dt><code>sample_limit</code></dt><dd><p>(<em>optional</em>) Set the upper limit of the number of samples
that are used during evaluation steps. Cannot be less than 20.
</p>
<p>This setting can be specified per data element by providing a parameter
value in a named list with data elements, e.g.
<code>list("sample_similarity"=100, "permutation_vimp"=1000)</code>.
</p>
<p>This parameter can be set for the following data elements:
<code>sample_similarity</code> and <code>ice_data</code>.</p>
</dd>
<dt><code>detail_level</code></dt><dd><p>(<em>optional</em>) Sets the level at which results are computed
and aggregated.
</p>

<ul>
<li> <p><code>ensemble</code>: Results are computed at the ensemble level, i.e. over all
models in the ensemble. This means that, for example, bias-corrected
estimates of model performance are assessed by creating (at least) 20
bootstraps and computing the model performance of the ensemble model for
each bootstrap.
</p>
</li>
<li> <p><code>hybrid</code> (default): Results are computed at the level of models in an
ensemble. This means that, for example, bias-corrected estimates of model
performance are directly computed using the models in the ensemble. If there
are at least 20 trained models in the ensemble, performance is computed for
each model, in contrast to <code>ensemble</code> where performance is computed for the
ensemble of models. If there are less than 20 trained models in the
ensemble, bootstraps are created so that at least 20 point estimates can be
made.
</p>
</li>
<li> <p><code>model</code>: Results are computed at the model level. This means that, for
example, bias-corrected estimates of model performance are assessed by
creating (at least) 20 bootstraps and computing the performance of the model
for each bootstrap.
</p>
</li></ul>

<p>Note that each level of detail has a different interpretation for bootstrap
confidence intervals. For <code>ensemble</code> and <code>model</code> these are the confidence
intervals for the ensemble and an individual model, respectively. That is,
the confidence interval describes the range where an estimate produced by a
respective ensemble or model trained on a repeat of the experiment may be
found with the probability of the confidence level. For <code>hybrid</code>, it
represents the range where any single model trained on a repeat of the
experiment may be found with the probability of the confidence level. By
definition, confidence intervals obtained using <code>hybrid</code> are at least as
wide as those for <code>ensemble</code>. <code>hybrid</code> offers the correct interpretation if
the goal of the analysis is to assess the result of a single, unspecified,
model.
</p>
<p><code>hybrid</code> is generally computationally less expensive then <code>ensemble</code>, which
in turn is somewhat less expensive than <code>model</code>.
</p>
<p>A non-default <code>detail_level</code> parameter can be specified for separate
evaluation steps by providing a parameter value in a named list with data
elements, e.g. <code>list("auc_data"="ensemble", "model_performance"="hybrid")</code>.
This parameter can be set for the following data elements: <code>auc_data</code>,
<code>decision_curve_analyis</code>, <code>model_performance</code>, <code>permutation_vimp</code>,
<code>ice_data</code>, <code>prediction_data</code> and <code>confusion_matrix</code>.</p>
</dd>
<dt><code>estimation_type</code></dt><dd><p>(<em>optional</em>) Sets the type of estimation that should be
possible. This has the following options:
</p>

<ul>
<li> <p><code>point</code>: Point estimates.
</p>
</li>
<li> <p><code>bias_correction</code> or <code>bc</code>: Bias-corrected estimates. A bias-corrected
estimate is computed from (at least) 20 point estimates, and <code>familiar</code> may
bootstrap the data to create them.
</p>
</li>
<li> <p><code>bootstrap_confidence_interval</code> or <code>bci</code> (default): Bias-corrected
estimates with bootstrap confidence intervals (Efron and Hastie, 2016). The
number of point estimates required depends on the <code>confidence_level</code>
parameter, and <code>familiar</code> may bootstrap the data to create them.
</p>
</li></ul>

<p>As with <code>detail_level</code>, a non-default <code>estimation_type</code> parameter can be
specified for separate evaluation steps by providing a parameter value in a
named list with data elements, e.g. <code>list("auc_data"="bci", "model_performance"="point")</code>. This parameter can be set for the following
data elements: <code>auc_data</code>, <code>decision_curve_analyis</code>, <code>model_performance</code>,
<code>permutation_vimp</code>, <code>ice_data</code>, and <code>prediction_data</code>.</p>
</dd>
<dt><code>confidence_level</code></dt><dd><p>(<em>optional</em>) Numeric value for the level at which
confidence intervals are determined. In the case bootstraps are used to
determine the confidence intervals bootstrap estimation, <code>familiar</code> uses the
rule of thumb <code class="reqn">n = 20 / ci.level</code> to determine the number of required
bootstraps.
</p>
<p>The default value is <code>0.95</code>.</p>
</dd>
<dt><code>bootstrap_ci_method</code></dt><dd><p>(<em>optional</em>) Method used to determine bootstrap
confidence intervals (Efron and Hastie, 2016). The following methods are
implemented:
</p>

<ul>
<li> <p><code>percentile</code> (default): Confidence intervals obtained using the percentile
method.
</p>
</li>
<li> <p><code>bc</code>: Bias-corrected confidence intervals.
</p>
</li></ul>

<p>Note that the standard method is not implemented because this method is
often not suitable due to non-normal distributions. The bias-corrected and
accelerated (BCa) method is not implemented yet.</p>
</dd>
<dt><code>familiar_data_names</code></dt><dd><p>Names of the dataset(s). Only used if the <code>object</code> parameter
is one or more <code>familiarData</code> objects.</p>
</dd>
<dt><code>collection_name</code></dt><dd><p>Name of the collection.</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Details</h3>

<p>Data is usually collected from a <code>familiarCollection</code> object.
However, you can also provide one or more <code>familiarData</code> objects, that will
be internally converted to a <code>familiarCollection</code> object. It is also
possible to provide a <code>familiarEnsemble</code> or one or more <code>familiarModel</code>
objects together with the data from which data is computed prior to export.
Paths to the previous files can also be provided.
</p>
<p>All parameters aside from <code>object</code> and <code>dir_path</code> are only used if <code>object</code>
is not a <code>familiarCollection</code> object, or a path to one.
</p>


<h3>Value</h3>

<p>A list of data.tables (if <code>dir_path</code> is not provided), or nothing, as
all data is exported to <code>csv</code> files.
</p>

<hr>
<h2 id='export_model_performance'>Extract and export metrics for model performance.</h2><span id='topic+export_model_performance'></span><span id='topic+export_model_performance+2CfamiliarCollection-method'></span><span id='topic+export_model_performance+2CANY-method'></span>

<h3>Description</h3>

<p>Extract and export metrics for model performance of models in a
familiarCollection.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>export_model_performance(
  object,
  dir_path = NULL,
  aggregate_results = FALSE,
  export_collection = FALSE,
  ...
)

## S4 method for signature 'familiarCollection'
export_model_performance(
  object,
  dir_path = NULL,
  aggregate_results = FALSE,
  export_collection = FALSE,
  ...
)

## S4 method for signature 'ANY'
export_model_performance(
  object,
  dir_path = NULL,
  aggregate_results = FALSE,
  export_collection = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="export_model_performance_+3A_object">object</code></td>
<td>
<p>A <code>familiarCollection</code> object, or other other objects from which
a <code>familiarCollection</code> can be extracted. See details for more information.</p>
</td></tr>
<tr><td><code id="export_model_performance_+3A_dir_path">dir_path</code></td>
<td>
<p>Path to folder where extracted data should be saved. <code>NULL</code>
will allow export as a structured list of data.tables.</p>
</td></tr>
<tr><td><code id="export_model_performance_+3A_aggregate_results">aggregate_results</code></td>
<td>
<p>Flag that signifies whether results should be
aggregated for export.</p>
</td></tr>
<tr><td><code id="export_model_performance_+3A_export_collection">export_collection</code></td>
<td>
<p>(<em>optional</em>) Exports the collection if TRUE.</p>
</td></tr>
<tr><td><code id="export_model_performance_+3A_...">...</code></td>
<td>

<p>Arguments passed on to <code><a href="#topic+extract_performance">extract_performance</a></code>, <code><a href="#topic+as_familiar_collection">as_familiar_collection</a></code>
</p>

<dl>
<dt><code>data</code></dt><dd><p>A <code>dataObject</code> object, <code>data.table</code> or <code>data.frame</code> that
constitutes the data that are assessed.</p>
</dd>
<dt><code>is_pre_processed</code></dt><dd><p>Flag that indicates whether the data was already
pre-processed externally, e.g. normalised and clustered. Only used if the
<code>data</code> argument is a <code>data.table</code> or <code>data.frame</code>.</p>
</dd>
<dt><code>cl</code></dt><dd><p>Cluster created using the <code>parallel</code> package. This cluster is then
used to speed up computation through parallellisation.</p>
</dd>
<dt><code>evaluation_times</code></dt><dd><p>One or more time points that are used for in analysis of
survival problems when data has to be assessed at a set time, e.g.
calibration. If not provided explicitly, this parameter is read from
settings used at creation of the underlying <code>familiarModel</code> objects. Only
used for <code>survival</code> outcomes.</p>
</dd>
<dt><code>ensemble_method</code></dt><dd><p>Method for ensembling predictions from models for the
same sample. Available methods are:
</p>

<ul>
<li> <p><code>median</code> (default): Use the median of the predicted values as the ensemble
value for a sample.
</p>
</li>
<li> <p><code>mean</code>: Use the mean of the predicted values as the ensemble value for a
sample.
</p>
</li></ul>
</dd>
<dt><code>metric</code></dt><dd><p>One or more metrics for assessing model performance. See the
vignette on performance metrics for the available metrics. If not provided
explicitly, this parameter is read from settings used at creation of the
underlying <code>familiarModel</code> objects.</p>
</dd>
<dt><code>verbose</code></dt><dd><p>Flag to indicate whether feedback should be provided on the
computation and extraction of various data elements.</p>
</dd>
<dt><code>message_indent</code></dt><dd><p>Number of indentation steps for messages shown during
computation and extraction of various data elements.</p>
</dd>
<dt><code>detail_level</code></dt><dd><p>(<em>optional</em>) Sets the level at which results are computed
and aggregated.
</p>

<ul>
<li> <p><code>ensemble</code>: Results are computed at the ensemble level, i.e. over all
models in the ensemble. This means that, for example, bias-corrected
estimates of model performance are assessed by creating (at least) 20
bootstraps and computing the model performance of the ensemble model for
each bootstrap.
</p>
</li>
<li> <p><code>hybrid</code> (default): Results are computed at the level of models in an
ensemble. This means that, for example, bias-corrected estimates of model
performance are directly computed using the models in the ensemble. If there
are at least 20 trained models in the ensemble, performance is computed for
each model, in contrast to <code>ensemble</code> where performance is computed for the
ensemble of models. If there are less than 20 trained models in the
ensemble, bootstraps are created so that at least 20 point estimates can be
made.
</p>
</li>
<li> <p><code>model</code>: Results are computed at the model level. This means that, for
example, bias-corrected estimates of model performance are assessed by
creating (at least) 20 bootstraps and computing the performance of the model
for each bootstrap.
</p>
</li></ul>

<p>Note that each level of detail has a different interpretation for bootstrap
confidence intervals. For <code>ensemble</code> and <code>model</code> these are the confidence
intervals for the ensemble and an individual model, respectively. That is,
the confidence interval describes the range where an estimate produced by a
respective ensemble or model trained on a repeat of the experiment may be
found with the probability of the confidence level. For <code>hybrid</code>, it
represents the range where any single model trained on a repeat of the
experiment may be found with the probability of the confidence level. By
definition, confidence intervals obtained using <code>hybrid</code> are at least as
wide as those for <code>ensemble</code>. <code>hybrid</code> offers the correct interpretation if
the goal of the analysis is to assess the result of a single, unspecified,
model.
</p>
<p><code>hybrid</code> is generally computationally less expensive then <code>ensemble</code>, which
in turn is somewhat less expensive than <code>model</code>.
</p>
<p>A non-default <code>detail_level</code> parameter can be specified for separate
evaluation steps by providing a parameter value in a named list with data
elements, e.g. <code>list("auc_data"="ensemble", "model_performance"="hybrid")</code>.
This parameter can be set for the following data elements: <code>auc_data</code>,
<code>decision_curve_analyis</code>, <code>model_performance</code>, <code>permutation_vimp</code>,
<code>ice_data</code>, <code>prediction_data</code> and <code>confusion_matrix</code>.</p>
</dd>
<dt><code>estimation_type</code></dt><dd><p>(<em>optional</em>) Sets the type of estimation that should be
possible. This has the following options:
</p>

<ul>
<li> <p><code>point</code>: Point estimates.
</p>
</li>
<li> <p><code>bias_correction</code> or <code>bc</code>: Bias-corrected estimates. A bias-corrected
estimate is computed from (at least) 20 point estimates, and <code>familiar</code> may
bootstrap the data to create them.
</p>
</li>
<li> <p><code>bootstrap_confidence_interval</code> or <code>bci</code> (default): Bias-corrected
estimates with bootstrap confidence intervals (Efron and Hastie, 2016). The
number of point estimates required depends on the <code>confidence_level</code>
parameter, and <code>familiar</code> may bootstrap the data to create them.
</p>
</li></ul>

<p>As with <code>detail_level</code>, a non-default <code>estimation_type</code> parameter can be
specified for separate evaluation steps by providing a parameter value in a
named list with data elements, e.g. <code>list("auc_data"="bci", "model_performance"="point")</code>. This parameter can be set for the following
data elements: <code>auc_data</code>, <code>decision_curve_analyis</code>, <code>model_performance</code>,
<code>permutation_vimp</code>, <code>ice_data</code>, and <code>prediction_data</code>.</p>
</dd>
<dt><code>confidence_level</code></dt><dd><p>(<em>optional</em>) Numeric value for the level at which
confidence intervals are determined. In the case bootstraps are used to
determine the confidence intervals bootstrap estimation, <code>familiar</code> uses the
rule of thumb <code class="reqn">n = 20 / ci.level</code> to determine the number of required
bootstraps.
</p>
<p>The default value is <code>0.95</code>.</p>
</dd>
<dt><code>bootstrap_ci_method</code></dt><dd><p>(<em>optional</em>) Method used to determine bootstrap
confidence intervals (Efron and Hastie, 2016). The following methods are
implemented:
</p>

<ul>
<li> <p><code>percentile</code> (default): Confidence intervals obtained using the percentile
method.
</p>
</li>
<li> <p><code>bc</code>: Bias-corrected confidence intervals.
</p>
</li></ul>

<p>Note that the standard method is not implemented because this method is
often not suitable due to non-normal distributions. The bias-corrected and
accelerated (BCa) method is not implemented yet.</p>
</dd>
<dt><code>familiar_data_names</code></dt><dd><p>Names of the dataset(s). Only used if the <code>object</code> parameter
is one or more <code>familiarData</code> objects.</p>
</dd>
<dt><code>collection_name</code></dt><dd><p>Name of the collection.</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Details</h3>

<p>Data is usually collected from a <code>familiarCollection</code> object.
However, you can also provide one or more <code>familiarData</code> objects, that will
be internally converted to a <code>familiarCollection</code> object. It is also
possible to provide a <code>familiarEnsemble</code> or one or more <code>familiarModel</code>
objects together with the data from which data is computed prior to export.
Paths to the previous files can also be provided.
</p>
<p>All parameters aside from <code>object</code> and <code>dir_path</code> are only used if <code>object</code>
is not a <code>familiarCollection</code> object, or a path to one.
</p>
<p>Performance of individual and ensemble models is exported. For ensemble
models, a credibility interval is determined using bootstrapping for each
metric.
</p>


<h3>Value</h3>

<p>A list of data.tables (if <code>dir_path</code> is not provided), or nothing, as
all data is exported to <code>csv</code> files.
</p>

<hr>
<h2 id='export_model_vimp'>Extract and export model-based variable importance.</h2><span id='topic+export_model_vimp'></span><span id='topic+export_model_vimp+2CfamiliarCollection-method'></span><span id='topic+export_model_vimp+2CANY-method'></span>

<h3>Description</h3>

<p>Extract and export model-based variable importance from a
familiarCollection.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>export_model_vimp(
  object,
  dir_path = NULL,
  aggregate_results = TRUE,
  aggregation_method = waiver(),
  rank_threshold = waiver(),
  export_collection = FALSE,
  ...
)

## S4 method for signature 'familiarCollection'
export_model_vimp(
  object,
  dir_path = NULL,
  aggregate_results = TRUE,
  aggregation_method = waiver(),
  rank_threshold = waiver(),
  export_collection = FALSE,
  ...
)

## S4 method for signature 'ANY'
export_model_vimp(
  object,
  dir_path = NULL,
  aggregate_results = TRUE,
  aggregation_method = waiver(),
  rank_threshold = waiver(),
  export_collection = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="export_model_vimp_+3A_object">object</code></td>
<td>
<p>A <code>familiarCollection</code> object, or other other objects from which
a <code>familiarCollection</code> can be extracted. See details for more information.</p>
</td></tr>
<tr><td><code id="export_model_vimp_+3A_dir_path">dir_path</code></td>
<td>
<p>Path to folder where extracted data should be saved. <code>NULL</code>
will allow export as a structured list of data.tables.</p>
</td></tr>
<tr><td><code id="export_model_vimp_+3A_aggregate_results">aggregate_results</code></td>
<td>
<p>Flag that signifies whether results should be
aggregated for export.</p>
</td></tr>
<tr><td><code id="export_model_vimp_+3A_aggregation_method">aggregation_method</code></td>
<td>
<p>(<em>optional</em>) The method used to aggregate variable
importances over different data subsets, e.g. bootstraps. The following
methods can be selected:
</p>

<ul>
<li> <p><code>mean</code> (default): Use the mean rank of a feature over the subsets to
determine the aggregated feature rank.
</p>
</li>
<li> <p><code>median</code>: Use the median rank of a feature over the subsets to determine
the aggregated feature rank.
</p>
</li>
<li> <p><code>best</code>: Use the best rank the feature obtained in any subset to determine
the aggregated feature rank.
</p>
</li>
<li> <p><code>worst</code>: Use the worst rank the feature obtained in any subset to
determine the aggregated feature rank.
</p>
</li>
<li> <p><code>stability</code>: Use the frequency of the feature being in the subset of
highly ranked features as measure for the aggregated feature rank
(Meinshausen and Buehlmann, 2010).
</p>
</li>
<li> <p><code>exponential</code>: Use a rank-weighted frequence of occurrence in the subset
of highly ranked features as measure for the aggregated feature rank (Haury
et al., 2011).
</p>
</li>
<li> <p><code>borda</code>: Use the borda count as measure for the aggregated feature rank
(Wald et al., 2012).
</p>
</li>
<li> <p><code>enhanced_borda</code>: Use an occurrence frequency-weighted borda count as
measure for the aggregated feature rank (Wald et al., 2012).
</p>
</li>
<li> <p><code>truncated_borda</code>: Use borda count computed only on features within the
subset of highly ranked features.
</p>
</li>
<li> <p><code>enhanced_truncated_borda</code>: Apply both the enhanced borda method and the
truncated borda method and use the resulting borda count as the aggregated
feature rank.
</p>
</li></ul>
</td></tr>
<tr><td><code id="export_model_vimp_+3A_rank_threshold">rank_threshold</code></td>
<td>
<p>(<em>optional</em>) The threshold used to define the subset of
highly important features. If not set, this threshold is determined by
maximising the variance in the occurrence value over all features over the
subset size.
</p>
<p>This parameter is only relevant for <code>stability</code>, <code>exponential</code>,
<code>enhanced_borda</code>, <code>truncated_borda</code> and <code>enhanced_truncated_borda</code> methods.</p>
</td></tr>
<tr><td><code id="export_model_vimp_+3A_export_collection">export_collection</code></td>
<td>
<p>(<em>optional</em>) Exports the collection if TRUE.</p>
</td></tr>
<tr><td><code id="export_model_vimp_+3A_...">...</code></td>
<td>

<p>Arguments passed on to <code><a href="#topic+as_familiar_collection">as_familiar_collection</a></code>
</p>

<dl>
<dt><code>familiar_data_names</code></dt><dd><p>Names of the dataset(s). Only used if the <code>object</code> parameter
is one or more <code>familiarData</code> objects.</p>
</dd>
<dt><code>collection_name</code></dt><dd><p>Name of the collection.</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Details</h3>

<p>Data, such as model performance and calibration information, is
usually collected from a <code>familiarCollection</code> object. However, you can also
provide one or more <code>familiarData</code> objects, that will be internally
converted to a <code>familiarCollection</code> object. It is also possible to provide a
<code>familiarEnsemble</code> or one or more <code>familiarModel</code> objects together with the
data from which data is computed prior to export. Paths to the previous
files can also be provided.
</p>
<p>All parameters aside from <code>object</code> and <code>dir_path</code> are only used if <code>object</code>
is not a <code>familiarCollection</code> object, or a path to one.
</p>
<p>Variable importance is based on the ranking produced by model-specific
variable importance routines, e.g. permutation for random forests. If such a
routine is absent, variable importance is based on the feature selection
method that led to the features included in the model. In case multiple
models (<code>familiarModel</code> objects) are combined, feature ranks are first
aggregated using the method defined by the <code>aggregation_method</code>, some of
which require a <code>rank_threshold</code> to indicate a subset of most important
features.
</p>
<p>Information concerning highly similar features that form clusters is
provided as well. This information is based on consensus clustering of the
features that were used in the signatures of the underlying models. This
clustering information is also used during aggregation to ensure that
co-clustered features are only taken into account once.
</p>


<h3>Value</h3>

<p>A data.table (if <code>dir_path</code> is not provided), or nothing, as all data
is exported to <code>csv</code> files.
</p>

<hr>
<h2 id='export_partial_dependence_data'>Extract and export partial dependence data.</h2><span id='topic+export_partial_dependence_data'></span><span id='topic+export_partial_dependence_data+2CfamiliarCollection-method'></span><span id='topic+export_partial_dependence_data+2CANY-method'></span>

<h3>Description</h3>

<p>Extract and export partial dependence data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>export_partial_dependence_data(
  object,
  dir_path = NULL,
  aggregate_results = TRUE,
  export_collection = FALSE,
  ...
)

## S4 method for signature 'familiarCollection'
export_partial_dependence_data(
  object,
  dir_path = NULL,
  aggregate_results = TRUE,
  export_collection = FALSE,
  ...
)

## S4 method for signature 'ANY'
export_partial_dependence_data(
  object,
  dir_path = NULL,
  aggregate_results = TRUE,
  export_collection = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="export_partial_dependence_data_+3A_object">object</code></td>
<td>
<p>A <code>familiarCollection</code> object, or other other objects from which
a <code>familiarCollection</code> can be extracted. See details for more information.</p>
</td></tr>
<tr><td><code id="export_partial_dependence_data_+3A_dir_path">dir_path</code></td>
<td>
<p>Path to folder where extracted data should be saved. <code>NULL</code>
will allow export as a structured list of data.tables.</p>
</td></tr>
<tr><td><code id="export_partial_dependence_data_+3A_aggregate_results">aggregate_results</code></td>
<td>
<p>Flag that signifies whether results should be
aggregated for export.</p>
</td></tr>
<tr><td><code id="export_partial_dependence_data_+3A_export_collection">export_collection</code></td>
<td>
<p>(<em>optional</em>) Exports the collection if TRUE.</p>
</td></tr>
<tr><td><code id="export_partial_dependence_data_+3A_...">...</code></td>
<td>

<p>Arguments passed on to <code><a href="#topic+extract_ice">extract_ice</a></code>, <code><a href="#topic+as_familiar_collection">as_familiar_collection</a></code>
</p>

<dl>
<dt><code>features</code></dt><dd><p>Names of the feature or features (2) assessed simultaneously.
By default <code>NULL</code>, which means that all features are assessed one-by-one.</p>
</dd>
<dt><code>feature_x_range</code></dt><dd><p>When one or two features are defined using <code>features</code>,
<code>feature_x_range</code> can be used to set the range of values for the first
feature. For numeric features, a vector of two values is assumed to indicate
a range from which <code>n_sample_points</code> are uniformly sampled. A vector of more
than two values is interpreted as is, i.e. these represent the values to be
sampled. For categorical features, values should represent a (sub)set of
available levels.</p>
</dd>
<dt><code>feature_y_range</code></dt><dd><p>As <code>feature_x_range</code>, but for the second feature in
case two features are defined.</p>
</dd>
<dt><code>n_sample_points</code></dt><dd><p>Number of points used to sample continuous features.</p>
</dd>
<dt><code>data</code></dt><dd><p>A <code>dataObject</code> object, <code>data.table</code> or <code>data.frame</code> that
constitutes the data that are assessed.</p>
</dd>
<dt><code>is_pre_processed</code></dt><dd><p>Flag that indicates whether the data was already
pre-processed externally, e.g. normalised and clustered. Only used if the
<code>data</code> argument is a <code>data.table</code> or <code>data.frame</code>.</p>
</dd>
<dt><code>cl</code></dt><dd><p>Cluster created using the <code>parallel</code> package. This cluster is then
used to speed up computation through parallellisation.</p>
</dd>
<dt><code>evaluation_times</code></dt><dd><p>One or more time points that are used for in analysis of
survival problems when data has to be assessed at a set time, e.g.
calibration. If not provided explicitly, this parameter is read from
settings used at creation of the underlying <code>familiarModel</code> objects. Only
used for <code>survival</code> outcomes.</p>
</dd>
<dt><code>ensemble_method</code></dt><dd><p>Method for ensembling predictions from models for the
same sample. Available methods are:
</p>

<ul>
<li> <p><code>median</code> (default): Use the median of the predicted values as the ensemble
value for a sample.
</p>
</li>
<li> <p><code>mean</code>: Use the mean of the predicted values as the ensemble value for a
sample.
</p>
</li></ul>
</dd>
<dt><code>verbose</code></dt><dd><p>Flag to indicate whether feedback should be provided on the
computation and extraction of various data elements.</p>
</dd>
<dt><code>message_indent</code></dt><dd><p>Number of indentation steps for messages shown during
computation and extraction of various data elements.</p>
</dd>
<dt><code>sample_limit</code></dt><dd><p>(<em>optional</em>) Set the upper limit of the number of samples
that are used during evaluation steps. Cannot be less than 20.
</p>
<p>This setting can be specified per data element by providing a parameter
value in a named list with data elements, e.g.
<code>list("sample_similarity"=100, "permutation_vimp"=1000)</code>.
</p>
<p>This parameter can be set for the following data elements:
<code>sample_similarity</code> and <code>ice_data</code>.</p>
</dd>
<dt><code>detail_level</code></dt><dd><p>(<em>optional</em>) Sets the level at which results are computed
and aggregated.
</p>

<ul>
<li> <p><code>ensemble</code>: Results are computed at the ensemble level, i.e. over all
models in the ensemble. This means that, for example, bias-corrected
estimates of model performance are assessed by creating (at least) 20
bootstraps and computing the model performance of the ensemble model for
each bootstrap.
</p>
</li>
<li> <p><code>hybrid</code> (default): Results are computed at the level of models in an
ensemble. This means that, for example, bias-corrected estimates of model
performance are directly computed using the models in the ensemble. If there
are at least 20 trained models in the ensemble, performance is computed for
each model, in contrast to <code>ensemble</code> where performance is computed for the
ensemble of models. If there are less than 20 trained models in the
ensemble, bootstraps are created so that at least 20 point estimates can be
made.
</p>
</li>
<li> <p><code>model</code>: Results are computed at the model level. This means that, for
example, bias-corrected estimates of model performance are assessed by
creating (at least) 20 bootstraps and computing the performance of the model
for each bootstrap.
</p>
</li></ul>

<p>Note that each level of detail has a different interpretation for bootstrap
confidence intervals. For <code>ensemble</code> and <code>model</code> these are the confidence
intervals for the ensemble and an individual model, respectively. That is,
the confidence interval describes the range where an estimate produced by a
respective ensemble or model trained on a repeat of the experiment may be
found with the probability of the confidence level. For <code>hybrid</code>, it
represents the range where any single model trained on a repeat of the
experiment may be found with the probability of the confidence level. By
definition, confidence intervals obtained using <code>hybrid</code> are at least as
wide as those for <code>ensemble</code>. <code>hybrid</code> offers the correct interpretation if
the goal of the analysis is to assess the result of a single, unspecified,
model.
</p>
<p><code>hybrid</code> is generally computationally less expensive then <code>ensemble</code>, which
in turn is somewhat less expensive than <code>model</code>.
</p>
<p>A non-default <code>detail_level</code> parameter can be specified for separate
evaluation steps by providing a parameter value in a named list with data
elements, e.g. <code>list("auc_data"="ensemble", "model_performance"="hybrid")</code>.
This parameter can be set for the following data elements: <code>auc_data</code>,
<code>decision_curve_analyis</code>, <code>model_performance</code>, <code>permutation_vimp</code>,
<code>ice_data</code>, <code>prediction_data</code> and <code>confusion_matrix</code>.</p>
</dd>
<dt><code>estimation_type</code></dt><dd><p>(<em>optional</em>) Sets the type of estimation that should be
possible. This has the following options:
</p>

<ul>
<li> <p><code>point</code>: Point estimates.
</p>
</li>
<li> <p><code>bias_correction</code> or <code>bc</code>: Bias-corrected estimates. A bias-corrected
estimate is computed from (at least) 20 point estimates, and <code>familiar</code> may
bootstrap the data to create them.
</p>
</li>
<li> <p><code>bootstrap_confidence_interval</code> or <code>bci</code> (default): Bias-corrected
estimates with bootstrap confidence intervals (Efron and Hastie, 2016). The
number of point estimates required depends on the <code>confidence_level</code>
parameter, and <code>familiar</code> may bootstrap the data to create them.
</p>
</li></ul>

<p>As with <code>detail_level</code>, a non-default <code>estimation_type</code> parameter can be
specified for separate evaluation steps by providing a parameter value in a
named list with data elements, e.g. <code>list("auc_data"="bci", "model_performance"="point")</code>. This parameter can be set for the following
data elements: <code>auc_data</code>, <code>decision_curve_analyis</code>, <code>model_performance</code>,
<code>permutation_vimp</code>, <code>ice_data</code>, and <code>prediction_data</code>.</p>
</dd>
<dt><code>confidence_level</code></dt><dd><p>(<em>optional</em>) Numeric value for the level at which
confidence intervals are determined. In the case bootstraps are used to
determine the confidence intervals bootstrap estimation, <code>familiar</code> uses the
rule of thumb <code class="reqn">n = 20 / ci.level</code> to determine the number of required
bootstraps.
</p>
<p>The default value is <code>0.95</code>.</p>
</dd>
<dt><code>bootstrap_ci_method</code></dt><dd><p>(<em>optional</em>) Method used to determine bootstrap
confidence intervals (Efron and Hastie, 2016). The following methods are
implemented:
</p>

<ul>
<li> <p><code>percentile</code> (default): Confidence intervals obtained using the percentile
method.
</p>
</li>
<li> <p><code>bc</code>: Bias-corrected confidence intervals.
</p>
</li></ul>

<p>Note that the standard method is not implemented because this method is
often not suitable due to non-normal distributions. The bias-corrected and
accelerated (BCa) method is not implemented yet.</p>
</dd>
<dt><code>familiar_data_names</code></dt><dd><p>Names of the dataset(s). Only used if the <code>object</code> parameter
is one or more <code>familiarData</code> objects.</p>
</dd>
<dt><code>collection_name</code></dt><dd><p>Name of the collection.</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Details</h3>

<p>Data is usually collected from a <code>familiarCollection</code> object.
However, you can also provide one or more <code>familiarData</code> objects, that will
be internally converted to a <code>familiarCollection</code> object. It is also
possible to provide a <code>familiarEnsemble</code> or one or more <code>familiarModel</code>
objects together with the data from which data is computed prior to export.
Paths to the previous files can also be provided.
</p>
<p>All parameters aside from <code>object</code> and <code>dir_path</code> are only used if <code>object</code>
is not a <code>familiarCollection</code> object, or a path to one.
</p>


<h3>Value</h3>

<p>A list of data.tables (if <code>dir_path</code> is not provided), or nothing, as
all data is exported to <code>csv</code> files.
</p>

<hr>
<h2 id='export_permutation_vimp'>Extract and export permutation variable importance.</h2><span id='topic+export_permutation_vimp'></span><span id='topic+export_permutation_vimp+2CfamiliarCollection-method'></span><span id='topic+export_permutation_vimp+2CANY-method'></span>

<h3>Description</h3>

<p>Extract and export model-based variable importance from a
familiarCollection.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>export_permutation_vimp(
  object,
  dir_path = NULL,
  aggregate_results = TRUE,
  export_collection = FALSE,
  ...
)

## S4 method for signature 'familiarCollection'
export_permutation_vimp(
  object,
  dir_path = NULL,
  aggregate_results = TRUE,
  export_collection = FALSE,
  ...
)

## S4 method for signature 'ANY'
export_permutation_vimp(
  object,
  dir_path = NULL,
  aggregate_results = TRUE,
  export_collection = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="export_permutation_vimp_+3A_object">object</code></td>
<td>
<p>A <code>familiarCollection</code> object, or other other objects from which
a <code>familiarCollection</code> can be extracted. See details for more information.</p>
</td></tr>
<tr><td><code id="export_permutation_vimp_+3A_dir_path">dir_path</code></td>
<td>
<p>Path to folder where extracted data should be saved. <code>NULL</code>
will allow export as a structured list of data.tables.</p>
</td></tr>
<tr><td><code id="export_permutation_vimp_+3A_aggregate_results">aggregate_results</code></td>
<td>
<p>Flag that signifies whether results should be
aggregated for export.</p>
</td></tr>
<tr><td><code id="export_permutation_vimp_+3A_export_collection">export_collection</code></td>
<td>
<p>(<em>optional</em>) Exports the collection if TRUE.</p>
</td></tr>
<tr><td><code id="export_permutation_vimp_+3A_...">...</code></td>
<td>

<p>Arguments passed on to <code><a href="#topic+extract_permutation_vimp">extract_permutation_vimp</a></code>, <code><a href="#topic+as_familiar_collection">as_familiar_collection</a></code>
</p>

<dl>
<dt><code>data</code></dt><dd><p>A <code>dataObject</code> object, <code>data.table</code> or <code>data.frame</code> that
constitutes the data that are assessed.</p>
</dd>
<dt><code>is_pre_processed</code></dt><dd><p>Flag that indicates whether the data was already
pre-processed externally, e.g. normalised and clustered. Only used if the
<code>data</code> argument is a <code>data.table</code> or <code>data.frame</code>.</p>
</dd>
<dt><code>cl</code></dt><dd><p>Cluster created using the <code>parallel</code> package. This cluster is then
used to speed up computation through parallellisation.</p>
</dd>
<dt><code>evaluation_times</code></dt><dd><p>One or more time points that are used for in analysis of
survival problems when data has to be assessed at a set time, e.g.
calibration. If not provided explicitly, this parameter is read from
settings used at creation of the underlying <code>familiarModel</code> objects. Only
used for <code>survival</code> outcomes.</p>
</dd>
<dt><code>ensemble_method</code></dt><dd><p>Method for ensembling predictions from models for the
same sample. Available methods are:
</p>

<ul>
<li> <p><code>median</code> (default): Use the median of the predicted values as the ensemble
value for a sample.
</p>
</li>
<li> <p><code>mean</code>: Use the mean of the predicted values as the ensemble value for a
sample.
</p>
</li></ul>
</dd>
<dt><code>metric</code></dt><dd><p>One or more metrics for assessing model performance. See the
vignette on performance metrics for the available metrics. If not provided
explicitly, this parameter is read from settings used at creation of the
underlying <code>familiarModel</code> objects.</p>
</dd>
<dt><code>feature_cluster_method</code></dt><dd><p>The method used to perform clustering. These are
the same methods as for the <code>cluster_method</code> configuration parameter:
<code>none</code>, <code>hclust</code>, <code>agnes</code>, <code>diana</code> and <code>pam</code>.
</p>
<p><code>none</code> cannot be used when extracting data regarding mutual correlation or
feature expressions.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</dd>
<dt><code>feature_linkage_method</code></dt><dd><p>The method used for agglomerative clustering in
<code>hclust</code> and <code>agnes</code>. These are the same methods as for the
<code>cluster_linkage_method</code> configuration parameter: <code>average</code>, <code>single</code>,
<code>complete</code>, <code>weighted</code>, and <code>ward</code>.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</dd>
<dt><code>feature_cluster_cut_method</code></dt><dd><p>The method used to divide features into
separate clusters. The available methods are the same as for the
<code>cluster_cut_method</code> configuration parameter: <code>silhouette</code>, <code>fixed_cut</code> and
<code>dynamic_cut</code>.
</p>
<p><code>silhouette</code> is available for all cluster methods, but <code>fixed_cut</code> only
applies to methods that create hierarchical trees (<code>hclust</code>, <code>agnes</code> and
<code>diana</code>). <code>dynamic_cut</code> requires the <code>dynamicTreeCut</code> package and can only
be used with <code>agnes</code> and <code>hclust</code>.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</dd>
<dt><code>feature_similarity_threshold</code></dt><dd><p>The threshold level for pair-wise
similarity that is required to form feature clusters with the <code>fixed_cut</code>
method.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</dd>
<dt><code>feature_similarity_metric</code></dt><dd><p>Metric to determine pairwise similarity
between features. Similarity is computed in the same manner as for
clustering, and <code>feature_similarity_metric</code> therefore has the same options
as <code>cluster_similarity_metric</code>: <code>mcfadden_r2</code>, <code>cox_snell_r2</code>,
<code>nagelkerke_r2</code>, <code>spearman</code>, <code>kendall</code> and <code>pearson</code>.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</dd>
<dt><code>verbose</code></dt><dd><p>Flag to indicate whether feedback should be provided on the
computation and extraction of various data elements.</p>
</dd>
<dt><code>message_indent</code></dt><dd><p>Number of indentation steps for messages shown during
computation and extraction of various data elements.</p>
</dd>
<dt><code>detail_level</code></dt><dd><p>(<em>optional</em>) Sets the level at which results are computed
and aggregated.
</p>

<ul>
<li> <p><code>ensemble</code>: Results are computed at the ensemble level, i.e. over all
models in the ensemble. This means that, for example, bias-corrected
estimates of model performance are assessed by creating (at least) 20
bootstraps and computing the model performance of the ensemble model for
each bootstrap.
</p>
</li>
<li> <p><code>hybrid</code> (default): Results are computed at the level of models in an
ensemble. This means that, for example, bias-corrected estimates of model
performance are directly computed using the models in the ensemble. If there
are at least 20 trained models in the ensemble, performance is computed for
each model, in contrast to <code>ensemble</code> where performance is computed for the
ensemble of models. If there are less than 20 trained models in the
ensemble, bootstraps are created so that at least 20 point estimates can be
made.
</p>
</li>
<li> <p><code>model</code>: Results are computed at the model level. This means that, for
example, bias-corrected estimates of model performance are assessed by
creating (at least) 20 bootstraps and computing the performance of the model
for each bootstrap.
</p>
</li></ul>

<p>Note that each level of detail has a different interpretation for bootstrap
confidence intervals. For <code>ensemble</code> and <code>model</code> these are the confidence
intervals for the ensemble and an individual model, respectively. That is,
the confidence interval describes the range where an estimate produced by a
respective ensemble or model trained on a repeat of the experiment may be
found with the probability of the confidence level. For <code>hybrid</code>, it
represents the range where any single model trained on a repeat of the
experiment may be found with the probability of the confidence level. By
definition, confidence intervals obtained using <code>hybrid</code> are at least as
wide as those for <code>ensemble</code>. <code>hybrid</code> offers the correct interpretation if
the goal of the analysis is to assess the result of a single, unspecified,
model.
</p>
<p><code>hybrid</code> is generally computationally less expensive then <code>ensemble</code>, which
in turn is somewhat less expensive than <code>model</code>.
</p>
<p>A non-default <code>detail_level</code> parameter can be specified for separate
evaluation steps by providing a parameter value in a named list with data
elements, e.g. <code>list("auc_data"="ensemble", "model_performance"="hybrid")</code>.
This parameter can be set for the following data elements: <code>auc_data</code>,
<code>decision_curve_analyis</code>, <code>model_performance</code>, <code>permutation_vimp</code>,
<code>ice_data</code>, <code>prediction_data</code> and <code>confusion_matrix</code>.</p>
</dd>
<dt><code>estimation_type</code></dt><dd><p>(<em>optional</em>) Sets the type of estimation that should be
possible. This has the following options:
</p>

<ul>
<li> <p><code>point</code>: Point estimates.
</p>
</li>
<li> <p><code>bias_correction</code> or <code>bc</code>: Bias-corrected estimates. A bias-corrected
estimate is computed from (at least) 20 point estimates, and <code>familiar</code> may
bootstrap the data to create them.
</p>
</li>
<li> <p><code>bootstrap_confidence_interval</code> or <code>bci</code> (default): Bias-corrected
estimates with bootstrap confidence intervals (Efron and Hastie, 2016). The
number of point estimates required depends on the <code>confidence_level</code>
parameter, and <code>familiar</code> may bootstrap the data to create them.
</p>
</li></ul>

<p>As with <code>detail_level</code>, a non-default <code>estimation_type</code> parameter can be
specified for separate evaluation steps by providing a parameter value in a
named list with data elements, e.g. <code>list("auc_data"="bci", "model_performance"="point")</code>. This parameter can be set for the following
data elements: <code>auc_data</code>, <code>decision_curve_analyis</code>, <code>model_performance</code>,
<code>permutation_vimp</code>, <code>ice_data</code>, and <code>prediction_data</code>.</p>
</dd>
<dt><code>confidence_level</code></dt><dd><p>(<em>optional</em>) Numeric value for the level at which
confidence intervals are determined. In the case bootstraps are used to
determine the confidence intervals bootstrap estimation, <code>familiar</code> uses the
rule of thumb <code class="reqn">n = 20 / ci.level</code> to determine the number of required
bootstraps.
</p>
<p>The default value is <code>0.95</code>.</p>
</dd>
<dt><code>bootstrap_ci_method</code></dt><dd><p>(<em>optional</em>) Method used to determine bootstrap
confidence intervals (Efron and Hastie, 2016). The following methods are
implemented:
</p>

<ul>
<li> <p><code>percentile</code> (default): Confidence intervals obtained using the percentile
method.
</p>
</li>
<li> <p><code>bc</code>: Bias-corrected confidence intervals.
</p>
</li></ul>

<p>Note that the standard method is not implemented because this method is
often not suitable due to non-normal distributions. The bias-corrected and
accelerated (BCa) method is not implemented yet.</p>
</dd>
<dt><code>familiar_data_names</code></dt><dd><p>Names of the dataset(s). Only used if the <code>object</code> parameter
is one or more <code>familiarData</code> objects.</p>
</dd>
<dt><code>collection_name</code></dt><dd><p>Name of the collection.</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Details</h3>

<p>Data, such as permutation variable importance and calibration
information, is usually collected from a <code>familiarCollection</code> object.
However, you can also provide one or more <code>familiarData</code> objects, that will
be internally converted to a <code>familiarCollection</code> object. It is also
possible to provide a <code>familiarEnsemble</code> or one or more <code>familiarModel</code>
objects together with the data from which data is computed prior to export.
Paths to the previously mentioned files can also be provided.
</p>
<p>All parameters aside from <code>object</code> and <code>dir_path</code> are only used if <code>object</code>
is not a <code>familiarCollection</code> object, or a path to one.
</p>
<p>Permutation Variable importance assesses the improvement in model
performance due to a feature. For this purpose, the performance of the model
is measured as normal, and is measured again with a dataset where the values
of the feature in question have been randomly permuted. The difference
between both performance measurements is the permutation variable
importance.
</p>
<p>In familiar, this basic concept is extended in several ways:
</p>

<ul>
<li><p> Point estimates of variable importance are based on multiple (21) random
permutations. The difference between model performance on the normal dataset
and the median performance measurement of the randomly permuted datasets is
used as permutation variable importance.
</p>
</li>
<li><p> Confidence intervals for the ensemble model are determined using bootstrap
methods.
</p>
</li>
<li><p> Permutation variable importance is assessed for any metric specified using
the <code>metric</code> argument.
</p>
</li>
<li><p> Permutation variable importance can take into account similarity between
features and permute similar features simultaneously.
</p>
</li></ul>



<h3>Value</h3>

<p>A data.table (if <code>dir_path</code> is not provided), or nothing, as all data
is exported to <code>csv</code> files.
</p>

<hr>
<h2 id='export_prediction_data'>Extract and export predicted values.</h2><span id='topic+export_prediction_data'></span><span id='topic+export_prediction_data+2CfamiliarCollection-method'></span><span id='topic+export_prediction_data+2CANY-method'></span>

<h3>Description</h3>

<p>Extract and export the values predicted by single and ensemble
models in a familiarCollection.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>export_prediction_data(object, dir_path = NULL, export_collection = FALSE, ...)

## S4 method for signature 'familiarCollection'
export_prediction_data(object, dir_path = NULL, export_collection = FALSE, ...)

## S4 method for signature 'ANY'
export_prediction_data(object, dir_path = NULL, export_collection = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="export_prediction_data_+3A_object">object</code></td>
<td>
<p>A <code>familiarCollection</code> object, or other other objects from which
a <code>familiarCollection</code> can be extracted. See details for more information.</p>
</td></tr>
<tr><td><code id="export_prediction_data_+3A_dir_path">dir_path</code></td>
<td>
<p>Path to folder where extracted data should be saved. <code>NULL</code>
will allow export as a structured list of data.tables.</p>
</td></tr>
<tr><td><code id="export_prediction_data_+3A_export_collection">export_collection</code></td>
<td>
<p>(<em>optional</em>) Exports the collection if TRUE.</p>
</td></tr>
<tr><td><code id="export_prediction_data_+3A_...">...</code></td>
<td>

<p>Arguments passed on to <code><a href="#topic+extract_predictions">extract_predictions</a></code>, <code><a href="#topic+as_familiar_collection">as_familiar_collection</a></code>
</p>

<dl>
<dt><code>data</code></dt><dd><p>A <code>dataObject</code> object, <code>data.table</code> or <code>data.frame</code> that
constitutes the data that are assessed.</p>
</dd>
<dt><code>is_pre_processed</code></dt><dd><p>Flag that indicates whether the data was already
pre-processed externally, e.g. normalised and clustered. Only used if the
<code>data</code> argument is a <code>data.table</code> or <code>data.frame</code>.</p>
</dd>
<dt><code>cl</code></dt><dd><p>Cluster created using the <code>parallel</code> package. This cluster is then
used to speed up computation through parallellisation.</p>
</dd>
<dt><code>evaluation_times</code></dt><dd><p>One or more time points that are used for in analysis of
survival problems when data has to be assessed at a set time, e.g.
calibration. If not provided explicitly, this parameter is read from
settings used at creation of the underlying <code>familiarModel</code> objects. Only
used for <code>survival</code> outcomes.</p>
</dd>
<dt><code>ensemble_method</code></dt><dd><p>Method for ensembling predictions from models for the
same sample. Available methods are:
</p>

<ul>
<li> <p><code>median</code> (default): Use the median of the predicted values as the ensemble
value for a sample.
</p>
</li>
<li> <p><code>mean</code>: Use the mean of the predicted values as the ensemble value for a
sample.
</p>
</li></ul>
</dd>
<dt><code>verbose</code></dt><dd><p>Flag to indicate whether feedback should be provided on the
computation and extraction of various data elements.</p>
</dd>
<dt><code>message_indent</code></dt><dd><p>Number of indentation steps for messages shown during
computation and extraction of various data elements.</p>
</dd>
<dt><code>detail_level</code></dt><dd><p>(<em>optional</em>) Sets the level at which results are computed
and aggregated.
</p>

<ul>
<li> <p><code>ensemble</code>: Results are computed at the ensemble level, i.e. over all
models in the ensemble. This means that, for example, bias-corrected
estimates of model performance are assessed by creating (at least) 20
bootstraps and computing the model performance of the ensemble model for
each bootstrap.
</p>
</li>
<li> <p><code>hybrid</code> (default): Results are computed at the level of models in an
ensemble. This means that, for example, bias-corrected estimates of model
performance are directly computed using the models in the ensemble. If there
are at least 20 trained models in the ensemble, performance is computed for
each model, in contrast to <code>ensemble</code> where performance is computed for the
ensemble of models. If there are less than 20 trained models in the
ensemble, bootstraps are created so that at least 20 point estimates can be
made.
</p>
</li>
<li> <p><code>model</code>: Results are computed at the model level. This means that, for
example, bias-corrected estimates of model performance are assessed by
creating (at least) 20 bootstraps and computing the performance of the model
for each bootstrap.
</p>
</li></ul>

<p>Note that each level of detail has a different interpretation for bootstrap
confidence intervals. For <code>ensemble</code> and <code>model</code> these are the confidence
intervals for the ensemble and an individual model, respectively. That is,
the confidence interval describes the range where an estimate produced by a
respective ensemble or model trained on a repeat of the experiment may be
found with the probability of the confidence level. For <code>hybrid</code>, it
represents the range where any single model trained on a repeat of the
experiment may be found with the probability of the confidence level. By
definition, confidence intervals obtained using <code>hybrid</code> are at least as
wide as those for <code>ensemble</code>. <code>hybrid</code> offers the correct interpretation if
the goal of the analysis is to assess the result of a single, unspecified,
model.
</p>
<p><code>hybrid</code> is generally computationally less expensive then <code>ensemble</code>, which
in turn is somewhat less expensive than <code>model</code>.
</p>
<p>A non-default <code>detail_level</code> parameter can be specified for separate
evaluation steps by providing a parameter value in a named list with data
elements, e.g. <code>list("auc_data"="ensemble", "model_performance"="hybrid")</code>.
This parameter can be set for the following data elements: <code>auc_data</code>,
<code>decision_curve_analyis</code>, <code>model_performance</code>, <code>permutation_vimp</code>,
<code>ice_data</code>, <code>prediction_data</code> and <code>confusion_matrix</code>.</p>
</dd>
<dt><code>estimation_type</code></dt><dd><p>(<em>optional</em>) Sets the type of estimation that should be
possible. This has the following options:
</p>

<ul>
<li> <p><code>point</code>: Point estimates.
</p>
</li>
<li> <p><code>bias_correction</code> or <code>bc</code>: Bias-corrected estimates. A bias-corrected
estimate is computed from (at least) 20 point estimates, and <code>familiar</code> may
bootstrap the data to create them.
</p>
</li>
<li> <p><code>bootstrap_confidence_interval</code> or <code>bci</code> (default): Bias-corrected
estimates with bootstrap confidence intervals (Efron and Hastie, 2016). The
number of point estimates required depends on the <code>confidence_level</code>
parameter, and <code>familiar</code> may bootstrap the data to create them.
</p>
</li></ul>

<p>As with <code>detail_level</code>, a non-default <code>estimation_type</code> parameter can be
specified for separate evaluation steps by providing a parameter value in a
named list with data elements, e.g. <code>list("auc_data"="bci", "model_performance"="point")</code>. This parameter can be set for the following
data elements: <code>auc_data</code>, <code>decision_curve_analyis</code>, <code>model_performance</code>,
<code>permutation_vimp</code>, <code>ice_data</code>, and <code>prediction_data</code>.</p>
</dd>
<dt><code>aggregate_results</code></dt><dd><p>(<em>optional</em>) Flag that signifies whether results
should be aggregated during evaluation. If <code>estimation_type</code> is
<code>bias_correction</code> or <code>bc</code>, aggregation leads to a single bias-corrected
estimate. If <code>estimation_type</code> is <code>bootstrap_confidence_interval</code> or <code>bci</code>,
aggregation leads to a single bias-corrected estimate with lower and upper
boundaries of the confidence interval. This has no effect if
<code>estimation_type</code> is <code>point</code>.
</p>
<p>The default value is equal to <code>TRUE</code> except when assessing metrics to assess
model performance, as the default violin plot requires underlying data.
</p>
<p>As with <code>detail_level</code> and <code>estimation_type</code>, a non-default
<code>aggregate_results</code> parameter can be specified for separate evaluation steps
by providing a parameter value in a named list with data elements, e.g.
<code>list("auc_data"=TRUE, , "model_performance"=FALSE)</code>. This parameter exists
for the same elements as <code>estimation_type</code>.</p>
</dd>
<dt><code>confidence_level</code></dt><dd><p>(<em>optional</em>) Numeric value for the level at which
confidence intervals are determined. In the case bootstraps are used to
determine the confidence intervals bootstrap estimation, <code>familiar</code> uses the
rule of thumb <code class="reqn">n = 20 / ci.level</code> to determine the number of required
bootstraps.
</p>
<p>The default value is <code>0.95</code>.</p>
</dd>
<dt><code>familiar_data_names</code></dt><dd><p>Names of the dataset(s). Only used if the <code>object</code> parameter
is one or more <code>familiarData</code> objects.</p>
</dd>
<dt><code>collection_name</code></dt><dd><p>Name of the collection.</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Details</h3>

<p>Data, such as model performance and calibration information, is
usually collected from a <code>familiarCollection</code> object. However, you can also
provide one or more <code>familiarData</code> objects, that will be internally
converted to a <code>familiarCollection</code> object. It is also possible to provide a
<code>familiarEnsemble</code> or one or more <code>familiarModel</code> objects together with the
data from which data is computed prior to export. Paths to the previous
files can also be provided.
</p>
<p>All parameters aside from <code>object</code> and <code>dir_path</code> are only used if <code>object</code>
is not a <code>familiarCollection</code> object, or a path to one.
</p>
<p>Both single and ensemble predictions are exported.
</p>


<h3>Value</h3>

<p>A list of data.tables (if <code>dir_path</code> is not provided), or nothing, as
all data is exported to <code>csv</code> files.
</p>

<hr>
<h2 id='export_risk_stratification_data'>Extract and export sample risk group stratification and associated
tests.</h2><span id='topic+export_risk_stratification_data'></span><span id='topic+export_risk_stratification_data+2CfamiliarCollection-method'></span><span id='topic+export_risk_stratification_data+2CANY-method'></span>

<h3>Description</h3>

<p>Extract and export sample risk group stratification and
associated tests for data in a familiarCollection.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>export_risk_stratification_data(
  object,
  dir_path = NULL,
  export_strata = TRUE,
  time_range = NULL,
  export_collection = FALSE,
  ...
)

## S4 method for signature 'familiarCollection'
export_risk_stratification_data(
  object,
  dir_path = NULL,
  export_strata = TRUE,
  time_range = NULL,
  export_collection = FALSE,
  ...
)

## S4 method for signature 'ANY'
export_risk_stratification_data(
  object,
  dir_path = NULL,
  export_strata = TRUE,
  time_range = NULL,
  export_collection = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="export_risk_stratification_data_+3A_object">object</code></td>
<td>
<p>A <code>familiarCollection</code> object, or other other objects from which
a <code>familiarCollection</code> can be extracted. See details for more information.</p>
</td></tr>
<tr><td><code id="export_risk_stratification_data_+3A_dir_path">dir_path</code></td>
<td>
<p>Path to folder where extracted data should be saved. <code>NULL</code>
will allow export as a structured list of data.tables.</p>
</td></tr>
<tr><td><code id="export_risk_stratification_data_+3A_export_strata">export_strata</code></td>
<td>
<p>Flag that determines whether the raw data or strata are
exported.</p>
</td></tr>
<tr><td><code id="export_risk_stratification_data_+3A_time_range">time_range</code></td>
<td>
<p>Time range for which strata should be created. If <code>NULL</code>,
the full time range is used.</p>
</td></tr>
<tr><td><code id="export_risk_stratification_data_+3A_export_collection">export_collection</code></td>
<td>
<p>(<em>optional</em>) Exports the collection if TRUE.</p>
</td></tr>
<tr><td><code id="export_risk_stratification_data_+3A_...">...</code></td>
<td>

<p>Arguments passed on to <code><a href="#topic+extract_risk_stratification_data">extract_risk_stratification_data</a></code>, <code><a href="#topic+as_familiar_collection">as_familiar_collection</a></code>
</p>

<dl>
<dt><code>data</code></dt><dd><p>A <code>dataObject</code> object, <code>data.table</code> or <code>data.frame</code> that
constitutes the data that are assessed.</p>
</dd>
<dt><code>is_pre_processed</code></dt><dd><p>Flag that indicates whether the data was already
pre-processed externally, e.g. normalised and clustered. Only used if the
<code>data</code> argument is a <code>data.table</code> or <code>data.frame</code>.</p>
</dd>
<dt><code>cl</code></dt><dd><p>Cluster created using the <code>parallel</code> package. This cluster is then
used to speed up computation through parallellisation.</p>
</dd>
<dt><code>ensemble_method</code></dt><dd><p>Method for ensembling predictions from models for the
same sample. Available methods are:
</p>

<ul>
<li> <p><code>median</code> (default): Use the median of the predicted values as the ensemble
value for a sample.
</p>
</li>
<li> <p><code>mean</code>: Use the mean of the predicted values as the ensemble value for a
sample.
</p>
</li></ul>
</dd>
<dt><code>verbose</code></dt><dd><p>Flag to indicate whether feedback should be provided on the
computation and extraction of various data elements.</p>
</dd>
<dt><code>message_indent</code></dt><dd><p>Number of indentation steps for messages shown during
computation and extraction of various data elements.</p>
</dd>
<dt><code>detail_level</code></dt><dd><p>(<em>optional</em>) Sets the level at which results are computed
and aggregated.
</p>

<ul>
<li> <p><code>ensemble</code>: Results are computed at the ensemble level, i.e. over all
models in the ensemble. This means that, for example, bias-corrected
estimates of model performance are assessed by creating (at least) 20
bootstraps and computing the model performance of the ensemble model for
each bootstrap.
</p>
</li>
<li> <p><code>hybrid</code> (default): Results are computed at the level of models in an
ensemble. This means that, for example, bias-corrected estimates of model
performance are directly computed using the models in the ensemble. If there
are at least 20 trained models in the ensemble, performance is computed for
each model, in contrast to <code>ensemble</code> where performance is computed for the
ensemble of models. If there are less than 20 trained models in the
ensemble, bootstraps are created so that at least 20 point estimates can be
made.
</p>
</li>
<li> <p><code>model</code>: Results are computed at the model level. This means that, for
example, bias-corrected estimates of model performance are assessed by
creating (at least) 20 bootstraps and computing the performance of the model
for each bootstrap.
</p>
</li></ul>

<p>Note that each level of detail has a different interpretation for bootstrap
confidence intervals. For <code>ensemble</code> and <code>model</code> these are the confidence
intervals for the ensemble and an individual model, respectively. That is,
the confidence interval describes the range where an estimate produced by a
respective ensemble or model trained on a repeat of the experiment may be
found with the probability of the confidence level. For <code>hybrid</code>, it
represents the range where any single model trained on a repeat of the
experiment may be found with the probability of the confidence level. By
definition, confidence intervals obtained using <code>hybrid</code> are at least as
wide as those for <code>ensemble</code>. <code>hybrid</code> offers the correct interpretation if
the goal of the analysis is to assess the result of a single, unspecified,
model.
</p>
<p><code>hybrid</code> is generally computationally less expensive then <code>ensemble</code>, which
in turn is somewhat less expensive than <code>model</code>.
</p>
<p>A non-default <code>detail_level</code> parameter can be specified for separate
evaluation steps by providing a parameter value in a named list with data
elements, e.g. <code>list("auc_data"="ensemble", "model_performance"="hybrid")</code>.
This parameter can be set for the following data elements: <code>auc_data</code>,
<code>decision_curve_analyis</code>, <code>model_performance</code>, <code>permutation_vimp</code>,
<code>ice_data</code>, <code>prediction_data</code> and <code>confusion_matrix</code>.</p>
</dd>
<dt><code>confidence_level</code></dt><dd><p>(<em>optional</em>) Numeric value for the level at which
confidence intervals are determined. In the case bootstraps are used to
determine the confidence intervals bootstrap estimation, <code>familiar</code> uses the
rule of thumb <code class="reqn">n = 20 / ci.level</code> to determine the number of required
bootstraps.
</p>
<p>The default value is <code>0.95</code>.</p>
</dd>
<dt><code>familiar_data_names</code></dt><dd><p>Names of the dataset(s). Only used if the <code>object</code> parameter
is one or more <code>familiarData</code> objects.</p>
</dd>
<dt><code>collection_name</code></dt><dd><p>Name of the collection.</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Details</h3>

<p>Data is usually collected from a <code>familiarCollection</code> object.
However, you can also provide one or more <code>familiarData</code> objects, that will
be internally converted to a <code>familiarCollection</code> object. It is also
possible to provide a <code>familiarEnsemble</code> or one or more <code>familiarModel</code>
objects together with the data from which data is computed prior to export.
Paths to the previous files can also be provided.
</p>
<p>All parameters aside from <code>object</code> and <code>dir_path</code> are only used if <code>object</code>
is not a <code>familiarCollection</code> object, or a path to one.
</p>
<p>Three tables are exported in a list:
</p>

<ul>
<li> <p><code>data</code>: Contains the assigned risk group for a given sample, along with
its reported survival time and censoring status.
</p>
</li>
<li> <p><code>hr_ratio</code>: Contains the hazard ratio between different risk groups.
</p>
</li>
<li> <p><code>logrank</code>: Contains the results from the logrank test between different
risk groups.
</p>
</li></ul>



<h3>Value</h3>

<p>A list of data.tables (if <code>dir_path</code> is not provided), or nothing, as
all data is exported to <code>csv</code> files.
</p>

<hr>
<h2 id='export_risk_stratification_info'>Extract and export cut-off values for risk group stratification.</h2><span id='topic+export_risk_stratification_info'></span><span id='topic+export_risk_stratification_info+2CfamiliarCollection-method'></span><span id='topic+export_risk_stratification_info+2CANY-method'></span>

<h3>Description</h3>

<p>Extract and export cut-off values for risk group stratification
by models in a familiarCollection.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>export_risk_stratification_info(
  object,
  dir_path = NULL,
  aggregate_results = TRUE,
  export_collection = FALSE,
  ...
)

## S4 method for signature 'familiarCollection'
export_risk_stratification_info(
  object,
  dir_path = NULL,
  aggregate_results = TRUE,
  export_collection = FALSE,
  ...
)

## S4 method for signature 'ANY'
export_risk_stratification_info(
  object,
  dir_path = NULL,
  aggregate_results = TRUE,
  export_collection = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="export_risk_stratification_info_+3A_object">object</code></td>
<td>
<p>A <code>familiarCollection</code> object, or other other objects from which
a <code>familiarCollection</code> can be extracted. See details for more information.</p>
</td></tr>
<tr><td><code id="export_risk_stratification_info_+3A_dir_path">dir_path</code></td>
<td>
<p>Path to folder where extracted data should be saved. <code>NULL</code>
will allow export as a structured list of data.tables.</p>
</td></tr>
<tr><td><code id="export_risk_stratification_info_+3A_aggregate_results">aggregate_results</code></td>
<td>
<p>Flag that signifies whether results should be
aggregated for export.</p>
</td></tr>
<tr><td><code id="export_risk_stratification_info_+3A_export_collection">export_collection</code></td>
<td>
<p>(<em>optional</em>) Exports the collection if TRUE.</p>
</td></tr>
<tr><td><code id="export_risk_stratification_info_+3A_...">...</code></td>
<td>

<p>Arguments passed on to <code><a href="#topic+as_familiar_collection">as_familiar_collection</a></code>
</p>

<dl>
<dt><code>familiar_data_names</code></dt><dd><p>Names of the dataset(s). Only used if the <code>object</code> parameter
is one or more <code>familiarData</code> objects.</p>
</dd>
<dt><code>collection_name</code></dt><dd><p>Name of the collection.</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Details</h3>

<p>Data is usually collected from a <code>familiarCollection</code> object.
However, you can also provide one or more <code>familiarData</code> objects, that will
be internally converted to a <code>familiarCollection</code> object. It is also
possible to provide a <code>familiarEnsemble</code> or one or more <code>familiarModel</code>
objects together with the data from which data is computed prior to export.
Paths to the previous files can also be provided.
</p>
<p>All parameters aside from <code>object</code> and <code>dir_path</code> are only used if <code>object</code>
is not a <code>familiarCollection</code> object, or a path to one.
</p>
<p>Stratification cut-off values are determined when creating a model, using
one of several methods set by the <code>stratification_method</code> parameter. These
values are then used to stratify samples in any new dataset. The available
methods are:
</p>

<ul>
<li> <p><code>median</code> (default): The median predicted value in the development cohort
is used to stratify the samples into two risk groups.
</p>
</li>
<li> <p><code>fixed</code>: Samples are stratified based on the sample quantiles of the
predicted values. These quantiles are defined using the
<code>stratification_threshold</code> parameter.
</p>
</li>
<li> <p><code>optimised</code>: Use maximally selected rank statistics to determine the
optimal threshold (Lausen and Schumacher, 1992; Hothorn et al., 2003) to
stratify samples into two optimally separated risk groups.
</p>
</li></ul>



<h3>Value</h3>

<p>A data.table (if <code>dir_path</code> is not provided), or nothing, as all data
is exported to <code>csv</code> files.
</p>


<h3>References</h3>


<ol>
<li><p> Lausen, B. &amp; Schumacher, M. Maximally Selected Rank Statistics.
Biometrics 48, 73 (1992).
</p>
</li>
<li><p> Hothorn, T. &amp; Lausen, B. On the exact distribution of maximally selected
rank statistics. Comput. Stat. Data Anal. 43, 121–137 (2003).
</p>
</li></ol>


<hr>
<h2 id='export_sample_similarity'>Extract and export mutual correlation between features.</h2><span id='topic+export_sample_similarity'></span><span id='topic+export_sample_similarity+2CfamiliarCollection-method'></span><span id='topic+export_sample_similarity+2CANY-method'></span>

<h3>Description</h3>

<p>Extract and export mutual correlation between features in a
familiarCollection.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>export_sample_similarity(
  object,
  dir_path = NULL,
  aggregate_results = TRUE,
  sample_limit = waiver(),
  sample_cluster_method = waiver(),
  sample_linkage_method = waiver(),
  export_dendrogram = FALSE,
  export_collection = FALSE,
  ...
)

## S4 method for signature 'familiarCollection'
export_sample_similarity(
  object,
  dir_path = NULL,
  aggregate_results = TRUE,
  sample_limit = waiver(),
  sample_cluster_method = waiver(),
  sample_linkage_method = waiver(),
  export_dendrogram = FALSE,
  export_collection = FALSE,
  ...
)

## S4 method for signature 'ANY'
export_sample_similarity(
  object,
  dir_path = NULL,
  aggregate_results = TRUE,
  sample_limit = waiver(),
  sample_cluster_method = waiver(),
  sample_linkage_method = waiver(),
  export_dendrogram = FALSE,
  export_collection = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="export_sample_similarity_+3A_object">object</code></td>
<td>
<p>A <code>familiarCollection</code> object, or other other objects from which
a <code>familiarCollection</code> can be extracted. See details for more information.</p>
</td></tr>
<tr><td><code id="export_sample_similarity_+3A_dir_path">dir_path</code></td>
<td>
<p>Path to folder where extracted data should be saved. <code>NULL</code>
will allow export as a structured list of data.tables.</p>
</td></tr>
<tr><td><code id="export_sample_similarity_+3A_aggregate_results">aggregate_results</code></td>
<td>
<p>Flag that signifies whether results should be
aggregated for export.</p>
</td></tr>
<tr><td><code id="export_sample_similarity_+3A_sample_limit">sample_limit</code></td>
<td>
<p>(<em>optional</em>) Set the upper limit of the number of samples
that are used during evaluation steps. Cannot be less than 20.
</p>
<p>This setting can be specified per data element by providing a parameter
value in a named list with data elements, e.g.
<code>list("sample_similarity"=100, "permutation_vimp"=1000)</code>.
</p>
<p>This parameter can be set for the following data elements:
<code>sample_similarity</code> and <code>ice_data</code>.</p>
</td></tr>
<tr><td><code id="export_sample_similarity_+3A_sample_cluster_method">sample_cluster_method</code></td>
<td>
<p>The method used to perform clustering based on
distance between samples. These are the same methods as for the
<code>cluster_method</code> configuration parameter: <code>hclust</code>, <code>agnes</code>, <code>diana</code> and
<code>pam</code>.
</p>
<p><code>none</code> cannot be used when extracting data for feature expressions.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="export_sample_similarity_+3A_sample_linkage_method">sample_linkage_method</code></td>
<td>
<p>The method used for agglomerative clustering in
<code>hclust</code> and <code>agnes</code>. These are the same methods as for the
<code>cluster_linkage_method</code> configuration parameter: <code>average</code>, <code>single</code>,
<code>complete</code>, <code>weighted</code>, and <code>ward</code>.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="export_sample_similarity_+3A_export_dendrogram">export_dendrogram</code></td>
<td>
<p>Add dendrogram in the data element objects.</p>
</td></tr>
<tr><td><code id="export_sample_similarity_+3A_export_collection">export_collection</code></td>
<td>
<p>(<em>optional</em>) Exports the collection if TRUE.</p>
</td></tr>
<tr><td><code id="export_sample_similarity_+3A_...">...</code></td>
<td>

<p>Arguments passed on to <code><a href="#topic+as_familiar_collection">as_familiar_collection</a></code>
</p>

<dl>
<dt><code>familiar_data_names</code></dt><dd><p>Names of the dataset(s). Only used if the <code>object</code> parameter
is one or more <code>familiarData</code> objects.</p>
</dd>
<dt><code>collection_name</code></dt><dd><p>Name of the collection.</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Details</h3>

<p>Data is usually collected from a <code>familiarCollection</code> object.
However, you can also provide one or more <code>familiarData</code> objects, that will
be internally converted to a <code>familiarCollection</code> object. It is also
possible to provide a <code>familiarEnsemble</code> or one or more <code>familiarModel</code>
objects together with the data from which data is computed prior to export.
Paths to the previous files can also be provided.
</p>
<p>All parameters aside from <code>object</code> and <code>dir_path</code> are only used if <code>object</code>
is not a <code>familiarCollection</code> object, or a path to one.
</p>


<h3>Value</h3>

<p>A list containing a data.table (if <code>dir_path</code> is not provided), or
nothing, as all data is exported to <code>csv</code> files.
</p>

<hr>
<h2 id='export_univariate_analysis_data'>Extract and export univariate analysis data of features.</h2><span id='topic+export_univariate_analysis_data'></span><span id='topic+export_univariate_analysis_data+2CfamiliarCollection-method'></span><span id='topic+export_univariate_analysis_data+2CANY-method'></span>

<h3>Description</h3>

<p>Extract and export univariate analysis data of features for data
in a familiarCollection.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>export_univariate_analysis_data(
  object,
  dir_path = NULL,
  p_adjustment_method = waiver(),
  export_collection = FALSE,
  ...
)

## S4 method for signature 'familiarCollection'
export_univariate_analysis_data(
  object,
  dir_path = NULL,
  p_adjustment_method = waiver(),
  export_collection = FALSE,
  ...
)

## S4 method for signature 'ANY'
export_univariate_analysis_data(
  object,
  dir_path = NULL,
  p_adjustment_method = waiver(),
  export_collection = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="export_univariate_analysis_data_+3A_object">object</code></td>
<td>
<p>A <code>familiarCollection</code> object, or other other objects from which
a <code>familiarCollection</code> can be extracted. See details for more information.</p>
</td></tr>
<tr><td><code id="export_univariate_analysis_data_+3A_dir_path">dir_path</code></td>
<td>
<p>Path to folder where extracted data should be saved. <code>NULL</code>
will allow export as a structured list of data.tables.</p>
</td></tr>
<tr><td><code id="export_univariate_analysis_data_+3A_p_adjustment_method">p_adjustment_method</code></td>
<td>
<p>(<em>optional</em>) Indicates type of p-value that is
shown. One of <code>holm</code>, <code>hochberg</code>, <code>hommel</code>, <code>bonferroni</code>, <code>BH</code>, <code>BY</code>, <code>fdr</code>,
<code>none</code>, <code>p_value</code> or <code>q_value</code> for adjusted p-values, uncorrected p-values
and q-values. q-values may not be available.</p>
</td></tr>
<tr><td><code id="export_univariate_analysis_data_+3A_export_collection">export_collection</code></td>
<td>
<p>(<em>optional</em>) Exports the collection if TRUE.</p>
</td></tr>
<tr><td><code id="export_univariate_analysis_data_+3A_...">...</code></td>
<td>

<p>Arguments passed on to <code><a href="#topic+extract_univariate_analysis">extract_univariate_analysis</a></code>, <code><a href="#topic+as_familiar_collection">as_familiar_collection</a></code>
</p>

<dl>
<dt><code>data</code></dt><dd><p>A <code>dataObject</code> object, <code>data.table</code> or <code>data.frame</code> that
constitutes the data that are assessed.</p>
</dd>
<dt><code>cl</code></dt><dd><p>Cluster created using the <code>parallel</code> package. This cluster is then
used to speed up computation through parallellisation.</p>
</dd>
<dt><code>feature_cluster_method</code></dt><dd><p>The method used to perform clustering. These are
the same methods as for the <code>cluster_method</code> configuration parameter:
<code>none</code>, <code>hclust</code>, <code>agnes</code>, <code>diana</code> and <code>pam</code>.
</p>
<p><code>none</code> cannot be used when extracting data regarding mutual correlation or
feature expressions.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</dd>
<dt><code>feature_linkage_method</code></dt><dd><p>The method used for agglomerative clustering in
<code>hclust</code> and <code>agnes</code>. These are the same methods as for the
<code>cluster_linkage_method</code> configuration parameter: <code>average</code>, <code>single</code>,
<code>complete</code>, <code>weighted</code>, and <code>ward</code>.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</dd>
<dt><code>feature_cluster_cut_method</code></dt><dd><p>The method used to divide features into
separate clusters. The available methods are the same as for the
<code>cluster_cut_method</code> configuration parameter: <code>silhouette</code>, <code>fixed_cut</code> and
<code>dynamic_cut</code>.
</p>
<p><code>silhouette</code> is available for all cluster methods, but <code>fixed_cut</code> only
applies to methods that create hierarchical trees (<code>hclust</code>, <code>agnes</code> and
<code>diana</code>). <code>dynamic_cut</code> requires the <code>dynamicTreeCut</code> package and can only
be used with <code>agnes</code> and <code>hclust</code>.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</dd>
<dt><code>feature_similarity_threshold</code></dt><dd><p>The threshold level for pair-wise
similarity that is required to form feature clusters with the <code>fixed_cut</code>
method.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</dd>
<dt><code>feature_similarity_metric</code></dt><dd><p>Metric to determine pairwise similarity
between features. Similarity is computed in the same manner as for
clustering, and <code>feature_similarity_metric</code> therefore has the same options
as <code>cluster_similarity_metric</code>: <code>mcfadden_r2</code>, <code>cox_snell_r2</code>,
<code>nagelkerke_r2</code>, <code>spearman</code>, <code>kendall</code> and <code>pearson</code>.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</dd>
<dt><code>icc_type</code></dt><dd><p>String indicating the type of intraclass correlation
coefficient (<code>1</code>, <code>2</code> or <code>3</code>) that should be used to compute robustness for
features in repeated measurements during the evaluation of univariate
importance. These types correspond to the types in Shrout and Fleiss (1979).
If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</dd>
<dt><code>verbose</code></dt><dd><p>Flag to indicate whether feedback should be provided on the
computation and extraction of various data elements.</p>
</dd>
<dt><code>message_indent</code></dt><dd><p>Number of indentation steps for messages shown during
computation and extraction of various data elements.</p>
</dd>
<dt><code>familiar_data_names</code></dt><dd><p>Names of the dataset(s). Only used if the <code>object</code> parameter
is one or more <code>familiarData</code> objects.</p>
</dd>
<dt><code>collection_name</code></dt><dd><p>Name of the collection.</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Details</h3>

<p>Data is usually collected from a <code>familiarCollection</code> object.
However, you can also provide one or more <code>familiarData</code> objects, that will
be internally converted to a <code>familiarCollection</code> object. It is also
possible to provide a <code>familiarEnsemble</code> or one or more <code>familiarModel</code>
objects together with the data from which data is computed prior to export.
Paths to the previous files can also be provided.
</p>
<p>All parameters aside from <code>object</code> and <code>dir_path</code> are only used if <code>object</code>
is not a <code>familiarCollection</code> object, or a path to one.
</p>
<p>Univariate analysis includes the computation of p and q-values, as well as
robustness (in case of repeated measurements). p-values are derived from
Wald's test.
</p>


<h3>Value</h3>

<p>A data.table (if <code>dir_path</code> is not provided), or nothing, as
all data is exported to <code>csv</code> files.
</p>

<hr>
<h2 id='extract_auc_data'>Internal function to extract area under the ROC curve information.</h2><span id='topic+extract_auc_data'></span>

<h3>Description</h3>

<p>Computes the ROC curve from a <code>familiarEnsemble</code>.
'
</p>


<h3>Usage</h3>

<pre><code class='language-R'>extract_auc_data(
  object,
  data,
  cl = NULL,
  ensemble_method = waiver(),
  detail_level = waiver(),
  estimation_type = waiver(),
  aggregate_results = waiver(),
  confidence_level = waiver(),
  bootstrap_ci_method = waiver(),
  is_pre_processed = FALSE,
  message_indent = 0L,
  verbose = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="extract_auc_data_+3A_object">object</code></td>
<td>
<p>A <code>familiarEnsemble</code> object, which is an ensemble of one or more
<code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="extract_auc_data_+3A_data">data</code></td>
<td>
<p>A <code>dataObject</code> object, <code>data.table</code> or <code>data.frame</code> that
constitutes the data that are assessed.</p>
</td></tr>
<tr><td><code id="extract_auc_data_+3A_cl">cl</code></td>
<td>
<p>Cluster created using the <code>parallel</code> package. This cluster is then
used to speed up computation through parallellisation.</p>
</td></tr>
<tr><td><code id="extract_auc_data_+3A_ensemble_method">ensemble_method</code></td>
<td>
<p>Method for ensembling predictions from models for the
same sample. Available methods are:
</p>

<ul>
<li> <p><code>median</code> (default): Use the median of the predicted values as the ensemble
value for a sample.
</p>
</li>
<li> <p><code>mean</code>: Use the mean of the predicted values as the ensemble value for a
sample.
</p>
</li></ul>
</td></tr>
<tr><td><code id="extract_auc_data_+3A_detail_level">detail_level</code></td>
<td>
<p>(<em>optional</em>) Sets the level at which results are computed
and aggregated.
</p>

<ul>
<li> <p><code>ensemble</code>: Results are computed at the ensemble level, i.e. over all
models in the ensemble. This means that, for example, bias-corrected
estimates of model performance are assessed by creating (at least) 20
bootstraps and computing the model performance of the ensemble model for
each bootstrap.
</p>
</li>
<li> <p><code>hybrid</code> (default): Results are computed at the level of models in an
ensemble. This means that, for example, bias-corrected estimates of model
performance are directly computed using the models in the ensemble. If there
are at least 20 trained models in the ensemble, performance is computed for
each model, in contrast to <code>ensemble</code> where performance is computed for the
ensemble of models. If there are less than 20 trained models in the
ensemble, bootstraps are created so that at least 20 point estimates can be
made.
</p>
</li>
<li> <p><code>model</code>: Results are computed at the model level. This means that, for
example, bias-corrected estimates of model performance are assessed by
creating (at least) 20 bootstraps and computing the performance of the model
for each bootstrap.
</p>
</li></ul>

<p>Note that each level of detail has a different interpretation for bootstrap
confidence intervals. For <code>ensemble</code> and <code>model</code> these are the confidence
intervals for the ensemble and an individual model, respectively. That is,
the confidence interval describes the range where an estimate produced by a
respective ensemble or model trained on a repeat of the experiment may be
found with the probability of the confidence level. For <code>hybrid</code>, it
represents the range where any single model trained on a repeat of the
experiment may be found with the probability of the confidence level. By
definition, confidence intervals obtained using <code>hybrid</code> are at least as
wide as those for <code>ensemble</code>. <code>hybrid</code> offers the correct interpretation if
the goal of the analysis is to assess the result of a single, unspecified,
model.
</p>
<p><code>hybrid</code> is generally computationally less expensive then <code>ensemble</code>, which
in turn is somewhat less expensive than <code>model</code>.
</p>
<p>A non-default <code>detail_level</code> parameter can be specified for separate
evaluation steps by providing a parameter value in a named list with data
elements, e.g. <code>list("auc_data"="ensemble", "model_performance"="hybrid")</code>.
This parameter can be set for the following data elements: <code>auc_data</code>,
<code>decision_curve_analyis</code>, <code>model_performance</code>, <code>permutation_vimp</code>,
<code>ice_data</code>, <code>prediction_data</code> and <code>confusion_matrix</code>.</p>
</td></tr>
<tr><td><code id="extract_auc_data_+3A_estimation_type">estimation_type</code></td>
<td>
<p>(<em>optional</em>) Sets the type of estimation that should be
possible. This has the following options:
</p>

<ul>
<li> <p><code>point</code>: Point estimates.
</p>
</li>
<li> <p><code>bias_correction</code> or <code>bc</code>: Bias-corrected estimates. A bias-corrected
estimate is computed from (at least) 20 point estimates, and <code>familiar</code> may
bootstrap the data to create them.
</p>
</li>
<li> <p><code>bootstrap_confidence_interval</code> or <code>bci</code> (default): Bias-corrected
estimates with bootstrap confidence intervals (Efron and Hastie, 2016). The
number of point estimates required depends on the <code>confidence_level</code>
parameter, and <code>familiar</code> may bootstrap the data to create them.
</p>
</li></ul>

<p>As with <code>detail_level</code>, a non-default <code>estimation_type</code> parameter can be
specified for separate evaluation steps by providing a parameter value in a
named list with data elements, e.g. <code>list("auc_data"="bci", "model_performance"="point")</code>. This parameter can be set for the following
data elements: <code>auc_data</code>, <code>decision_curve_analyis</code>, <code>model_performance</code>,
<code>permutation_vimp</code>, <code>ice_data</code>, and <code>prediction_data</code>.</p>
</td></tr>
<tr><td><code id="extract_auc_data_+3A_aggregate_results">aggregate_results</code></td>
<td>
<p>(<em>optional</em>) Flag that signifies whether results
should be aggregated during evaluation. If <code>estimation_type</code> is
<code>bias_correction</code> or <code>bc</code>, aggregation leads to a single bias-corrected
estimate. If <code>estimation_type</code> is <code>bootstrap_confidence_interval</code> or <code>bci</code>,
aggregation leads to a single bias-corrected estimate with lower and upper
boundaries of the confidence interval. This has no effect if
<code>estimation_type</code> is <code>point</code>.
</p>
<p>The default value is equal to <code>TRUE</code> except when assessing metrics to assess
model performance, as the default violin plot requires underlying data.
</p>
<p>As with <code>detail_level</code> and <code>estimation_type</code>, a non-default
<code>aggregate_results</code> parameter can be specified for separate evaluation steps
by providing a parameter value in a named list with data elements, e.g.
<code>list("auc_data"=TRUE, , "model_performance"=FALSE)</code>. This parameter exists
for the same elements as <code>estimation_type</code>.</p>
</td></tr>
<tr><td><code id="extract_auc_data_+3A_confidence_level">confidence_level</code></td>
<td>
<p>(<em>optional</em>) Numeric value for the level at which
confidence intervals are determined. In the case bootstraps are used to
determine the confidence intervals bootstrap estimation, <code>familiar</code> uses the
rule of thumb <code class="reqn">n = 20 / ci.level</code> to determine the number of required
bootstraps.
</p>
<p>The default value is <code>0.95</code>.</p>
</td></tr>
<tr><td><code id="extract_auc_data_+3A_bootstrap_ci_method">bootstrap_ci_method</code></td>
<td>
<p>(<em>optional</em>) Method used to determine bootstrap
confidence intervals (Efron and Hastie, 2016). The following methods are
implemented:
</p>

<ul>
<li> <p><code>percentile</code> (default): Confidence intervals obtained using the percentile
method.
</p>
</li>
<li> <p><code>bc</code>: Bias-corrected confidence intervals.
</p>
</li></ul>

<p>Note that the standard method is not implemented because this method is
often not suitable due to non-normal distributions. The bias-corrected and
accelerated (BCa) method is not implemented yet.</p>
</td></tr>
<tr><td><code id="extract_auc_data_+3A_is_pre_processed">is_pre_processed</code></td>
<td>
<p>Flag that indicates whether the data was already
pre-processed externally, e.g. normalised and clustered. Only used if the
<code>data</code> argument is a <code>data.table</code> or <code>data.frame</code>.</p>
</td></tr>
<tr><td><code id="extract_auc_data_+3A_message_indent">message_indent</code></td>
<td>
<p>Number of indentation steps for messages shown during
computation and extraction of various data elements.</p>
</td></tr>
<tr><td><code id="extract_auc_data_+3A_verbose">verbose</code></td>
<td>
<p>Flag to indicate whether feedback should be provided on the
computation and extraction of various data elements.</p>
</td></tr>
<tr><td><code id="extract_auc_data_+3A_...">...</code></td>
<td>
<p>Unused arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function also computes credibility intervals for the ROC curve
for the ensemble model, at the level of <code>confidence_level</code>. In the case of
multinomial outcomes, an AUC curve is computed per class in a
one-against-all fashion.
</p>
<p>To allow plotting of multiple AUC curves in the same plot and the use of
ensemble models, the AUC curve is evaluated at 0.01 (1-specificity) intervals.
</p>


<h3>Value</h3>

<p>A list with data.tables for single and ensemble model ROC curve data.
</p>

<hr>
<h2 id='extract_calibration_data'>Internal function to extract calibration data.</h2><span id='topic+extract_calibration_data'></span>

<h3>Description</h3>

<p>Computes calibration data from a <code>familiarEnsemble</code> object.
Calibration tests are performed based on expected (predicted) and observed
outcomes. For all outcomes, calibration-at-the-large and calibration slopes
are determined. Furthermore, for all but survival outcomes, a repeated,
randomised grouping Hosmer-Lemeshow test is performed. For survival
outcomes, the Nam-D'Agostino and Greenwood-Nam-D'Agostino tests are
performed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>extract_calibration_data(
  object,
  data,
  cl = NULL,
  ensemble_method = waiver(),
  evaluation_times = waiver(),
  detail_level = waiver(),
  estimation_type = waiver(),
  aggregate_results = waiver(),
  confidence_level = waiver(),
  bootstrap_ci_method = waiver(),
  is_pre_processed = FALSE,
  message_indent = 0L,
  verbose = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="extract_calibration_data_+3A_object">object</code></td>
<td>
<p>A <code>familiarEnsemble</code> object, which is an ensemble of one or more
<code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="extract_calibration_data_+3A_data">data</code></td>
<td>
<p>A <code>dataObject</code> object, <code>data.table</code> or <code>data.frame</code> that
constitutes the data that are assessed.</p>
</td></tr>
<tr><td><code id="extract_calibration_data_+3A_cl">cl</code></td>
<td>
<p>Cluster created using the <code>parallel</code> package. This cluster is then
used to speed up computation through parallellisation.</p>
</td></tr>
<tr><td><code id="extract_calibration_data_+3A_ensemble_method">ensemble_method</code></td>
<td>
<p>Method for ensembling predictions from models for the
same sample. Available methods are:
</p>

<ul>
<li> <p><code>median</code> (default): Use the median of the predicted values as the ensemble
value for a sample.
</p>
</li>
<li> <p><code>mean</code>: Use the mean of the predicted values as the ensemble value for a
sample.
</p>
</li></ul>
</td></tr>
<tr><td><code id="extract_calibration_data_+3A_evaluation_times">evaluation_times</code></td>
<td>
<p>One or more time points that are used for in analysis of
survival problems when data has to be assessed at a set time, e.g.
calibration. If not provided explicitly, this parameter is read from
settings used at creation of the underlying <code>familiarModel</code> objects. Only
used for <code>survival</code> outcomes.</p>
</td></tr>
<tr><td><code id="extract_calibration_data_+3A_detail_level">detail_level</code></td>
<td>
<p>(<em>optional</em>) Sets the level at which results are computed
and aggregated.
</p>

<ul>
<li> <p><code>ensemble</code>: Results are computed at the ensemble level, i.e. over all
models in the ensemble. This means that, for example, bias-corrected
estimates of model performance are assessed by creating (at least) 20
bootstraps and computing the model performance of the ensemble model for
each bootstrap.
</p>
</li>
<li> <p><code>hybrid</code> (default): Results are computed at the level of models in an
ensemble. This means that, for example, bias-corrected estimates of model
performance are directly computed using the models in the ensemble. If there
are at least 20 trained models in the ensemble, performance is computed for
each model, in contrast to <code>ensemble</code> where performance is computed for the
ensemble of models. If there are less than 20 trained models in the
ensemble, bootstraps are created so that at least 20 point estimates can be
made.
</p>
</li>
<li> <p><code>model</code>: Results are computed at the model level. This means that, for
example, bias-corrected estimates of model performance are assessed by
creating (at least) 20 bootstraps and computing the performance of the model
for each bootstrap.
</p>
</li></ul>

<p>Note that each level of detail has a different interpretation for bootstrap
confidence intervals. For <code>ensemble</code> and <code>model</code> these are the confidence
intervals for the ensemble and an individual model, respectively. That is,
the confidence interval describes the range where an estimate produced by a
respective ensemble or model trained on a repeat of the experiment may be
found with the probability of the confidence level. For <code>hybrid</code>, it
represents the range where any single model trained on a repeat of the
experiment may be found with the probability of the confidence level. By
definition, confidence intervals obtained using <code>hybrid</code> are at least as
wide as those for <code>ensemble</code>. <code>hybrid</code> offers the correct interpretation if
the goal of the analysis is to assess the result of a single, unspecified,
model.
</p>
<p><code>hybrid</code> is generally computationally less expensive then <code>ensemble</code>, which
in turn is somewhat less expensive than <code>model</code>.
</p>
<p>A non-default <code>detail_level</code> parameter can be specified for separate
evaluation steps by providing a parameter value in a named list with data
elements, e.g. <code>list("auc_data"="ensemble", "model_performance"="hybrid")</code>.
This parameter can be set for the following data elements: <code>auc_data</code>,
<code>decision_curve_analyis</code>, <code>model_performance</code>, <code>permutation_vimp</code>,
<code>ice_data</code>, <code>prediction_data</code> and <code>confusion_matrix</code>.</p>
</td></tr>
<tr><td><code id="extract_calibration_data_+3A_estimation_type">estimation_type</code></td>
<td>
<p>(<em>optional</em>) Sets the type of estimation that should be
possible. This has the following options:
</p>

<ul>
<li> <p><code>point</code>: Point estimates.
</p>
</li>
<li> <p><code>bias_correction</code> or <code>bc</code>: Bias-corrected estimates. A bias-corrected
estimate is computed from (at least) 20 point estimates, and <code>familiar</code> may
bootstrap the data to create them.
</p>
</li>
<li> <p><code>bootstrap_confidence_interval</code> or <code>bci</code> (default): Bias-corrected
estimates with bootstrap confidence intervals (Efron and Hastie, 2016). The
number of point estimates required depends on the <code>confidence_level</code>
parameter, and <code>familiar</code> may bootstrap the data to create them.
</p>
</li></ul>

<p>As with <code>detail_level</code>, a non-default <code>estimation_type</code> parameter can be
specified for separate evaluation steps by providing a parameter value in a
named list with data elements, e.g. <code>list("auc_data"="bci", "model_performance"="point")</code>. This parameter can be set for the following
data elements: <code>auc_data</code>, <code>decision_curve_analyis</code>, <code>model_performance</code>,
<code>permutation_vimp</code>, <code>ice_data</code>, and <code>prediction_data</code>.</p>
</td></tr>
<tr><td><code id="extract_calibration_data_+3A_aggregate_results">aggregate_results</code></td>
<td>
<p>(<em>optional</em>) Flag that signifies whether results
should be aggregated during evaluation. If <code>estimation_type</code> is
<code>bias_correction</code> or <code>bc</code>, aggregation leads to a single bias-corrected
estimate. If <code>estimation_type</code> is <code>bootstrap_confidence_interval</code> or <code>bci</code>,
aggregation leads to a single bias-corrected estimate with lower and upper
boundaries of the confidence interval. This has no effect if
<code>estimation_type</code> is <code>point</code>.
</p>
<p>The default value is equal to <code>TRUE</code> except when assessing metrics to assess
model performance, as the default violin plot requires underlying data.
</p>
<p>As with <code>detail_level</code> and <code>estimation_type</code>, a non-default
<code>aggregate_results</code> parameter can be specified for separate evaluation steps
by providing a parameter value in a named list with data elements, e.g.
<code>list("auc_data"=TRUE, , "model_performance"=FALSE)</code>. This parameter exists
for the same elements as <code>estimation_type</code>.</p>
</td></tr>
<tr><td><code id="extract_calibration_data_+3A_confidence_level">confidence_level</code></td>
<td>
<p>(<em>optional</em>) Numeric value for the level at which
confidence intervals are determined. In the case bootstraps are used to
determine the confidence intervals bootstrap estimation, <code>familiar</code> uses the
rule of thumb <code class="reqn">n = 20 / ci.level</code> to determine the number of required
bootstraps.
</p>
<p>The default value is <code>0.95</code>.</p>
</td></tr>
<tr><td><code id="extract_calibration_data_+3A_bootstrap_ci_method">bootstrap_ci_method</code></td>
<td>
<p>(<em>optional</em>) Method used to determine bootstrap
confidence intervals (Efron and Hastie, 2016). The following methods are
implemented:
</p>

<ul>
<li> <p><code>percentile</code> (default): Confidence intervals obtained using the percentile
method.
</p>
</li>
<li> <p><code>bc</code>: Bias-corrected confidence intervals.
</p>
</li></ul>

<p>Note that the standard method is not implemented because this method is
often not suitable due to non-normal distributions. The bias-corrected and
accelerated (BCa) method is not implemented yet.</p>
</td></tr>
<tr><td><code id="extract_calibration_data_+3A_is_pre_processed">is_pre_processed</code></td>
<td>
<p>Flag that indicates whether the data was already
pre-processed externally, e.g. normalised and clustered. Only used if the
<code>data</code> argument is a <code>data.table</code> or <code>data.frame</code>.</p>
</td></tr>
<tr><td><code id="extract_calibration_data_+3A_message_indent">message_indent</code></td>
<td>
<p>Number of indentation steps for messages shown during
computation and extraction of various data elements.</p>
</td></tr>
<tr><td><code id="extract_calibration_data_+3A_verbose">verbose</code></td>
<td>
<p>Flag to indicate whether feedback should be provided on the
computation and extraction of various data elements.</p>
</td></tr>
<tr><td><code id="extract_calibration_data_+3A_...">...</code></td>
<td>
<p>Unused arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with data.tables containing calibration test information for
the ensemble model.
</p>

<hr>
<h2 id='extract_calibration_info'>Internal function to extract calibration info from data.</h2><span id='topic+extract_calibration_info'></span>

<h3>Description</h3>

<p>Collects .
</p>


<h3>Usage</h3>

<pre><code class='language-R'>extract_calibration_info(
  object,
  detail_level = waiver(),
  message_indent = 0L,
  verbose = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="extract_calibration_info_+3A_object">object</code></td>
<td>
<p>A <code>familiarEnsemble</code> object, which is an ensemble of one or more
<code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="extract_calibration_info_+3A_detail_level">detail_level</code></td>
<td>
<p>(<em>optional</em>) Sets the level at which results are computed
and aggregated.
</p>

<ul>
<li> <p><code>ensemble</code>: Results are computed at the ensemble level, i.e. over all
models in the ensemble. This means that, for example, bias-corrected
estimates of model performance are assessed by creating (at least) 20
bootstraps and computing the model performance of the ensemble model for
each bootstrap.
</p>
</li>
<li> <p><code>hybrid</code> (default): Results are computed at the level of models in an
ensemble. This means that, for example, bias-corrected estimates of model
performance are directly computed using the models in the ensemble. If there
are at least 20 trained models in the ensemble, performance is computed for
each model, in contrast to <code>ensemble</code> where performance is computed for the
ensemble of models. If there are less than 20 trained models in the
ensemble, bootstraps are created so that at least 20 point estimates can be
made.
</p>
</li>
<li> <p><code>model</code>: Results are computed at the model level. This means that, for
example, bias-corrected estimates of model performance are assessed by
creating (at least) 20 bootstraps and computing the performance of the model
for each bootstrap.
</p>
</li></ul>

<p>Note that each level of detail has a different interpretation for bootstrap
confidence intervals. For <code>ensemble</code> and <code>model</code> these are the confidence
intervals for the ensemble and an individual model, respectively. That is,
the confidence interval describes the range where an estimate produced by a
respective ensemble or model trained on a repeat of the experiment may be
found with the probability of the confidence level. For <code>hybrid</code>, it
represents the range where any single model trained on a repeat of the
experiment may be found with the probability of the confidence level. By
definition, confidence intervals obtained using <code>hybrid</code> are at least as
wide as those for <code>ensemble</code>. <code>hybrid</code> offers the correct interpretation if
the goal of the analysis is to assess the result of a single, unspecified,
model.
</p>
<p><code>hybrid</code> is generally computationally less expensive then <code>ensemble</code>, which
in turn is somewhat less expensive than <code>model</code>.
</p>
<p>A non-default <code>detail_level</code> parameter can be specified for separate
evaluation steps by providing a parameter value in a named list with data
elements, e.g. <code>list("auc_data"="ensemble", "model_performance"="hybrid")</code>.
This parameter can be set for the following data elements: <code>auc_data</code>,
<code>decision_curve_analyis</code>, <code>model_performance</code>, <code>permutation_vimp</code>,
<code>ice_data</code>, <code>prediction_data</code> and <code>confusion_matrix</code>.</p>
</td></tr>
<tr><td><code id="extract_calibration_info_+3A_message_indent">message_indent</code></td>
<td>
<p>Number of indentation steps for messages shown during
computation and extraction of various data elements.</p>
</td></tr>
<tr><td><code id="extract_calibration_info_+3A_verbose">verbose</code></td>
<td>
<p>Flag to indicate whether feedback should be provided on the
computation and extraction of various data elements.</p>
</td></tr>
<tr><td><code id="extract_calibration_info_+3A_...">...</code></td>
<td>
<p>Unused arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of familiarDataElements with hyperparameters.
</p>

<hr>
<h2 id='extract_confusion_matrix'>Internal function to extract the confusion matrix.</h2><span id='topic+extract_confusion_matrix'></span>

<h3>Description</h3>

<p>Computes and extracts the confusion matrix for predicted and
observed categorical outcomes used in a <code>familiarEnsemble</code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>extract_confusion_matrix(
  object,
  data,
  cl = NULL,
  ensemble_method = waiver(),
  detail_level = waiver(),
  is_pre_processed = FALSE,
  message_indent = 0L,
  verbose = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="extract_confusion_matrix_+3A_object">object</code></td>
<td>
<p>A <code>familiarEnsemble</code> object, which is an ensemble of one or more
<code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="extract_confusion_matrix_+3A_data">data</code></td>
<td>
<p>A <code>dataObject</code> object, <code>data.table</code> or <code>data.frame</code> that
constitutes the data that are assessed.</p>
</td></tr>
<tr><td><code id="extract_confusion_matrix_+3A_cl">cl</code></td>
<td>
<p>Cluster created using the <code>parallel</code> package. This cluster is then
used to speed up computation through parallellisation.</p>
</td></tr>
<tr><td><code id="extract_confusion_matrix_+3A_ensemble_method">ensemble_method</code></td>
<td>
<p>Method for ensembling predictions from models for the
same sample. Available methods are:
</p>

<ul>
<li> <p><code>median</code> (default): Use the median of the predicted values as the ensemble
value for a sample.
</p>
</li>
<li> <p><code>mean</code>: Use the mean of the predicted values as the ensemble value for a
sample.
</p>
</li></ul>
</td></tr>
<tr><td><code id="extract_confusion_matrix_+3A_detail_level">detail_level</code></td>
<td>
<p>(<em>optional</em>) Sets the level at which results are computed
and aggregated.
</p>

<ul>
<li> <p><code>ensemble</code>: Results are computed at the ensemble level, i.e. over all
models in the ensemble. This means that, for example, bias-corrected
estimates of model performance are assessed by creating (at least) 20
bootstraps and computing the model performance of the ensemble model for
each bootstrap.
</p>
</li>
<li> <p><code>hybrid</code> (default): Results are computed at the level of models in an
ensemble. This means that, for example, bias-corrected estimates of model
performance are directly computed using the models in the ensemble. If there
are at least 20 trained models in the ensemble, performance is computed for
each model, in contrast to <code>ensemble</code> where performance is computed for the
ensemble of models. If there are less than 20 trained models in the
ensemble, bootstraps are created so that at least 20 point estimates can be
made.
</p>
</li>
<li> <p><code>model</code>: Results are computed at the model level. This means that, for
example, bias-corrected estimates of model performance are assessed by
creating (at least) 20 bootstraps and computing the performance of the model
for each bootstrap.
</p>
</li></ul>

<p>Note that each level of detail has a different interpretation for bootstrap
confidence intervals. For <code>ensemble</code> and <code>model</code> these are the confidence
intervals for the ensemble and an individual model, respectively. That is,
the confidence interval describes the range where an estimate produced by a
respective ensemble or model trained on a repeat of the experiment may be
found with the probability of the confidence level. For <code>hybrid</code>, it
represents the range where any single model trained on a repeat of the
experiment may be found with the probability of the confidence level. By
definition, confidence intervals obtained using <code>hybrid</code> are at least as
wide as those for <code>ensemble</code>. <code>hybrid</code> offers the correct interpretation if
the goal of the analysis is to assess the result of a single, unspecified,
model.
</p>
<p><code>hybrid</code> is generally computationally less expensive then <code>ensemble</code>, which
in turn is somewhat less expensive than <code>model</code>.
</p>
<p>A non-default <code>detail_level</code> parameter can be specified for separate
evaluation steps by providing a parameter value in a named list with data
elements, e.g. <code>list("auc_data"="ensemble", "model_performance"="hybrid")</code>.
This parameter can be set for the following data elements: <code>auc_data</code>,
<code>decision_curve_analyis</code>, <code>model_performance</code>, <code>permutation_vimp</code>,
<code>ice_data</code>, <code>prediction_data</code> and <code>confusion_matrix</code>.</p>
</td></tr>
<tr><td><code id="extract_confusion_matrix_+3A_is_pre_processed">is_pre_processed</code></td>
<td>
<p>Flag that indicates whether the data was already
pre-processed externally, e.g. normalised and clustered. Only used if the
<code>data</code> argument is a <code>data.table</code> or <code>data.frame</code>.</p>
</td></tr>
<tr><td><code id="extract_confusion_matrix_+3A_message_indent">message_indent</code></td>
<td>
<p>Number of indentation steps for messages shown during
computation and extraction of various data elements.</p>
</td></tr>
<tr><td><code id="extract_confusion_matrix_+3A_verbose">verbose</code></td>
<td>
<p>Flag to indicate whether feedback should be provided on the
computation and extraction of various data elements.</p>
</td></tr>
<tr><td><code id="extract_confusion_matrix_+3A_...">...</code></td>
<td>
<p>Unused arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data.table containing predicted and observed outcome data together
with a co-occurence count.
</p>

<hr>
<h2 id='extract_data'>Internal function to create a familiarData object.</h2><span id='topic+extract_data'></span>

<h3>Description</h3>

<p>Compute various data related to model performance and calibration
from the provided dataset and <code>familiarEnsemble</code> object and store it as a
<code>familiarData</code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>extract_data(
  object,
  data,
  data_element = waiver(),
  is_pre_processed = FALSE,
  cl = NULL,
  time_max = waiver(),
  aggregation_method = waiver(),
  rank_threshold = waiver(),
  ensemble_method = waiver(),
  stratification_method = waiver(),
  evaluation_times = waiver(),
  metric = waiver(),
  feature_cluster_method = waiver(),
  feature_cluster_cut_method = waiver(),
  feature_linkage_method = waiver(),
  feature_similarity_metric = waiver(),
  feature_similarity_threshold = waiver(),
  sample_cluster_method = waiver(),
  sample_linkage_method = waiver(),
  sample_similarity_metric = waiver(),
  sample_limit = waiver(),
  detail_level = waiver(),
  estimation_type = waiver(),
  aggregate_results = waiver(),
  confidence_level = waiver(),
  bootstrap_ci_method = waiver(),
  icc_type = waiver(),
  dynamic_model_loading = FALSE,
  message_indent = 0L,
  verbose = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="extract_data_+3A_object">object</code></td>
<td>
<p>A <code>familiarEnsemble</code> object, which is an ensemble of one or more
<code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="extract_data_+3A_data">data</code></td>
<td>
<p>A <code>dataObject</code> object, <code>data.table</code> or <code>data.frame</code> that
constitutes the data that are assessed.</p>
</td></tr>
<tr><td><code id="extract_data_+3A_data_element">data_element</code></td>
<td>
<p>String indicating which data elements are to be extracted.
Default is <code>all</code>, but specific elements can be specified to speed up
computations if not all elements are to be computed. This is an internal
parameter that is set by, e.g. the <code>export_model_vimp</code> method.</p>
</td></tr>
<tr><td><code id="extract_data_+3A_is_pre_processed">is_pre_processed</code></td>
<td>
<p>Flag that indicates whether the data was already
pre-processed externally, e.g. normalised and clustered. Only used if the
<code>data</code> argument is a <code>data.table</code> or <code>data.frame</code>.</p>
</td></tr>
<tr><td><code id="extract_data_+3A_cl">cl</code></td>
<td>
<p>Cluster created using the <code>parallel</code> package. This cluster is then
used to speed up computation through parallellisation.</p>
</td></tr>
<tr><td><code id="extract_data_+3A_time_max">time_max</code></td>
<td>
<p>Time point which is used as the benchmark for e.g. cumulative
risks generated by random forest, or the cut-off value for Uno's concordance
index. If not provided explicitly, this parameter is read from settings used
at creation of the underlying <code>familiarModel</code> objects. Only used for
<code>survival</code> outcomes.</p>
</td></tr>
<tr><td><code id="extract_data_+3A_aggregation_method">aggregation_method</code></td>
<td>
<p>Method for aggregating variable importances for the
purpose of evaluation. Variable importances are determined during feature
selection steps and after training the model. Both types are evaluated, but
feature selection variable importance is only evaluated at run-time.
</p>
<p>See the documentation for the <code>vimp_aggregation_method</code> argument in
<code>summon_familiar</code> for information concerning the different available
methods.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="extract_data_+3A_rank_threshold">rank_threshold</code></td>
<td>
<p>The threshold used to  define the subset of highly
important features during evaluation.
</p>
<p>See the documentation for the <code>vimp_aggregation_rank_threshold</code> argument in
<code>summon_familiar</code> for more information.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="extract_data_+3A_ensemble_method">ensemble_method</code></td>
<td>
<p>Method for ensembling predictions from models for the
same sample. Available methods are:
</p>

<ul>
<li> <p><code>median</code> (default): Use the median of the predicted values as the ensemble
value for a sample.
</p>
</li>
<li> <p><code>mean</code>: Use the mean of the predicted values as the ensemble value for a
sample.
</p>
</li></ul>
</td></tr>
<tr><td><code id="extract_data_+3A_stratification_method">stratification_method</code></td>
<td>
<p>(<em>optional</em>) Method for determining the
stratification threshold for creating survival groups. The actual,
model-dependent, threshold value is obtained from the development data, and
can afterwards be used to perform stratification on validation data.
</p>
<p>The following stratification methods are available:
</p>

<ul>
<li> <p><code>median</code> (default): The median predicted value in the development cohort
is used to stratify the samples into two risk groups. For predicted outcome
values that build a continuous spectrum, the two risk groups in the
development cohort will be roughly equal in size.
</p>
</li>
<li> <p><code>mean</code>: The mean predicted value in the development cohort is used to
stratify the samples into two risk groups.
</p>
</li>
<li> <p><code>mean_trim</code>: As <code>mean</code>, but based on the set of predicted values
where the 5% lowest and 5% highest values are discarded. This reduces the
effect of outliers.
</p>
</li>
<li> <p><code>mean_winsor</code>: As <code>mean</code>, but based on the set of predicted values where
the 5% lowest and 5% highest values are winsorised. This reduces the effect
of outliers.
</p>
</li>
<li> <p><code>fixed</code>: Samples are stratified based on the sample quantiles of the
predicted values. These quantiles are defined using the
<code>stratification_threshold</code> parameter.
</p>
</li>
<li> <p><code>optimised</code>: Use maximally selected rank statistics to determine the
optimal threshold (Lausen and Schumacher, 1992; Hothorn et al., 2003) to
stratify samples into two optimally separated risk groups.
</p>
</li></ul>

<p>One or more stratification methods can be selected simultaneously.
</p>
<p>This parameter is only relevant for <code>survival</code> outcomes.</p>
</td></tr>
<tr><td><code id="extract_data_+3A_evaluation_times">evaluation_times</code></td>
<td>
<p>One or more time points that are used for in analysis of
survival problems when data has to be assessed at a set time, e.g.
calibration. If not provided explicitly, this parameter is read from
settings used at creation of the underlying <code>familiarModel</code> objects. Only
used for <code>survival</code> outcomes.</p>
</td></tr>
<tr><td><code id="extract_data_+3A_metric">metric</code></td>
<td>
<p>One or more metrics for assessing model performance. See the
vignette on performance metrics for the available metrics. If not provided
explicitly, this parameter is read from settings used at creation of the
underlying <code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="extract_data_+3A_feature_cluster_method">feature_cluster_method</code></td>
<td>
<p>The method used to perform clustering. These are
the same methods as for the <code>cluster_method</code> configuration parameter:
<code>none</code>, <code>hclust</code>, <code>agnes</code>, <code>diana</code> and <code>pam</code>.
</p>
<p><code>none</code> cannot be used when extracting data regarding mutual correlation or
feature expressions.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="extract_data_+3A_feature_cluster_cut_method">feature_cluster_cut_method</code></td>
<td>
<p>The method used to divide features into
separate clusters. The available methods are the same as for the
<code>cluster_cut_method</code> configuration parameter: <code>silhouette</code>, <code>fixed_cut</code> and
<code>dynamic_cut</code>.
</p>
<p><code>silhouette</code> is available for all cluster methods, but <code>fixed_cut</code> only
applies to methods that create hierarchical trees (<code>hclust</code>, <code>agnes</code> and
<code>diana</code>). <code>dynamic_cut</code> requires the <code>dynamicTreeCut</code> package and can only
be used with <code>agnes</code> and <code>hclust</code>.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="extract_data_+3A_feature_linkage_method">feature_linkage_method</code></td>
<td>
<p>The method used for agglomerative clustering in
<code>hclust</code> and <code>agnes</code>. These are the same methods as for the
<code>cluster_linkage_method</code> configuration parameter: <code>average</code>, <code>single</code>,
<code>complete</code>, <code>weighted</code>, and <code>ward</code>.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="extract_data_+3A_feature_similarity_metric">feature_similarity_metric</code></td>
<td>
<p>Metric to determine pairwise similarity
between features. Similarity is computed in the same manner as for
clustering, and <code>feature_similarity_metric</code> therefore has the same options
as <code>cluster_similarity_metric</code>: <code>mcfadden_r2</code>, <code>cox_snell_r2</code>,
<code>nagelkerke_r2</code>, <code>spearman</code>, <code>kendall</code> and <code>pearson</code>.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="extract_data_+3A_feature_similarity_threshold">feature_similarity_threshold</code></td>
<td>
<p>The threshold level for pair-wise
similarity that is required to form feature clusters with the <code>fixed_cut</code>
method.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="extract_data_+3A_sample_cluster_method">sample_cluster_method</code></td>
<td>
<p>The method used to perform clustering based on
distance between samples. These are the same methods as for the
<code>cluster_method</code> configuration parameter: <code>hclust</code>, <code>agnes</code>, <code>diana</code> and
<code>pam</code>.
</p>
<p><code>none</code> cannot be used when extracting data for feature expressions.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="extract_data_+3A_sample_linkage_method">sample_linkage_method</code></td>
<td>
<p>The method used for agglomerative clustering in
<code>hclust</code> and <code>agnes</code>. These are the same methods as for the
<code>cluster_linkage_method</code> configuration parameter: <code>average</code>, <code>single</code>,
<code>complete</code>, <code>weighted</code>, and <code>ward</code>.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="extract_data_+3A_sample_similarity_metric">sample_similarity_metric</code></td>
<td>
<p>Metric to determine pairwise similarity
between samples. Similarity is computed in the same manner as for
clustering, but <code>sample_similarity_metric</code> has different options that are
better suited to computing distance between samples instead of between
features: <code>gower</code>, <code>euclidean</code>.
</p>
<p>The underlying feature data is scaled to the <code class="reqn">[0, 1]</code> range (for
numerical features) using the feature values across the samples. The
normalisation parameters required can optionally be computed from feature
data with the outer 5% (on both sides) of feature values trimmed or
winsorised. To do so append <code style="white-space: pre;">&#8288;_trim&#8288;</code> (trimming) or <code style="white-space: pre;">&#8288;_winsor&#8288;</code> (winsorising) to
the metric name. This reduces the effect of outliers somewhat.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="extract_data_+3A_sample_limit">sample_limit</code></td>
<td>
<p>(<em>optional</em>) Set the upper limit of the number of samples
that are used during evaluation steps. Cannot be less than 20.
</p>
<p>This setting can be specified per data element by providing a parameter
value in a named list with data elements, e.g.
<code>list("sample_similarity"=100, "permutation_vimp"=1000)</code>.
</p>
<p>This parameter can be set for the following data elements:
<code>sample_similarity</code> and <code>ice_data</code>.</p>
</td></tr>
<tr><td><code id="extract_data_+3A_detail_level">detail_level</code></td>
<td>
<p>(<em>optional</em>) Sets the level at which results are computed
and aggregated.
</p>

<ul>
<li> <p><code>ensemble</code>: Results are computed at the ensemble level, i.e. over all
models in the ensemble. This means that, for example, bias-corrected
estimates of model performance are assessed by creating (at least) 20
bootstraps and computing the model performance of the ensemble model for
each bootstrap.
</p>
</li>
<li> <p><code>hybrid</code> (default): Results are computed at the level of models in an
ensemble. This means that, for example, bias-corrected estimates of model
performance are directly computed using the models in the ensemble. If there
are at least 20 trained models in the ensemble, performance is computed for
each model, in contrast to <code>ensemble</code> where performance is computed for the
ensemble of models. If there are less than 20 trained models in the
ensemble, bootstraps are created so that at least 20 point estimates can be
made.
</p>
</li>
<li> <p><code>model</code>: Results are computed at the model level. This means that, for
example, bias-corrected estimates of model performance are assessed by
creating (at least) 20 bootstraps and computing the performance of the model
for each bootstrap.
</p>
</li></ul>

<p>Note that each level of detail has a different interpretation for bootstrap
confidence intervals. For <code>ensemble</code> and <code>model</code> these are the confidence
intervals for the ensemble and an individual model, respectively. That is,
the confidence interval describes the range where an estimate produced by a
respective ensemble or model trained on a repeat of the experiment may be
found with the probability of the confidence level. For <code>hybrid</code>, it
represents the range where any single model trained on a repeat of the
experiment may be found with the probability of the confidence level. By
definition, confidence intervals obtained using <code>hybrid</code> are at least as
wide as those for <code>ensemble</code>. <code>hybrid</code> offers the correct interpretation if
the goal of the analysis is to assess the result of a single, unspecified,
model.
</p>
<p><code>hybrid</code> is generally computationally less expensive then <code>ensemble</code>, which
in turn is somewhat less expensive than <code>model</code>.
</p>
<p>A non-default <code>detail_level</code> parameter can be specified for separate
evaluation steps by providing a parameter value in a named list with data
elements, e.g. <code>list("auc_data"="ensemble", "model_performance"="hybrid")</code>.
This parameter can be set for the following data elements: <code>auc_data</code>,
<code>decision_curve_analyis</code>, <code>model_performance</code>, <code>permutation_vimp</code>,
<code>ice_data</code>, <code>prediction_data</code> and <code>confusion_matrix</code>.</p>
</td></tr>
<tr><td><code id="extract_data_+3A_estimation_type">estimation_type</code></td>
<td>
<p>(<em>optional</em>) Sets the type of estimation that should be
possible. This has the following options:
</p>

<ul>
<li> <p><code>point</code>: Point estimates.
</p>
</li>
<li> <p><code>bias_correction</code> or <code>bc</code>: Bias-corrected estimates. A bias-corrected
estimate is computed from (at least) 20 point estimates, and <code>familiar</code> may
bootstrap the data to create them.
</p>
</li>
<li> <p><code>bootstrap_confidence_interval</code> or <code>bci</code> (default): Bias-corrected
estimates with bootstrap confidence intervals (Efron and Hastie, 2016). The
number of point estimates required depends on the <code>confidence_level</code>
parameter, and <code>familiar</code> may bootstrap the data to create them.
</p>
</li></ul>

<p>As with <code>detail_level</code>, a non-default <code>estimation_type</code> parameter can be
specified for separate evaluation steps by providing a parameter value in a
named list with data elements, e.g. <code>list("auc_data"="bci", "model_performance"="point")</code>. This parameter can be set for the following
data elements: <code>auc_data</code>, <code>decision_curve_analyis</code>, <code>model_performance</code>,
<code>permutation_vimp</code>, <code>ice_data</code>, and <code>prediction_data</code>.</p>
</td></tr>
<tr><td><code id="extract_data_+3A_aggregate_results">aggregate_results</code></td>
<td>
<p>(<em>optional</em>) Flag that signifies whether results
should be aggregated during evaluation. If <code>estimation_type</code> is
<code>bias_correction</code> or <code>bc</code>, aggregation leads to a single bias-corrected
estimate. If <code>estimation_type</code> is <code>bootstrap_confidence_interval</code> or <code>bci</code>,
aggregation leads to a single bias-corrected estimate with lower and upper
boundaries of the confidence interval. This has no effect if
<code>estimation_type</code> is <code>point</code>.
</p>
<p>The default value is equal to <code>TRUE</code> except when assessing metrics to assess
model performance, as the default violin plot requires underlying data.
</p>
<p>As with <code>detail_level</code> and <code>estimation_type</code>, a non-default
<code>aggregate_results</code> parameter can be specified for separate evaluation steps
by providing a parameter value in a named list with data elements, e.g.
<code>list("auc_data"=TRUE, , "model_performance"=FALSE)</code>. This parameter exists
for the same elements as <code>estimation_type</code>.</p>
</td></tr>
<tr><td><code id="extract_data_+3A_confidence_level">confidence_level</code></td>
<td>
<p>(<em>optional</em>) Numeric value for the level at which
confidence intervals are determined. In the case bootstraps are used to
determine the confidence intervals bootstrap estimation, <code>familiar</code> uses the
rule of thumb <code class="reqn">n = 20 / ci.level</code> to determine the number of required
bootstraps.
</p>
<p>The default value is <code>0.95</code>.</p>
</td></tr>
<tr><td><code id="extract_data_+3A_bootstrap_ci_method">bootstrap_ci_method</code></td>
<td>
<p>(<em>optional</em>) Method used to determine bootstrap
confidence intervals (Efron and Hastie, 2016). The following methods are
implemented:
</p>

<ul>
<li> <p><code>percentile</code> (default): Confidence intervals obtained using the percentile
method.
</p>
</li>
<li> <p><code>bc</code>: Bias-corrected confidence intervals.
</p>
</li></ul>

<p>Note that the standard method is not implemented because this method is
often not suitable due to non-normal distributions. The bias-corrected and
accelerated (BCa) method is not implemented yet.</p>
</td></tr>
<tr><td><code id="extract_data_+3A_icc_type">icc_type</code></td>
<td>
<p>String indicating the type of intraclass correlation
coefficient (<code>1</code>, <code>2</code> or <code>3</code>) that should be used to compute robustness for
features in repeated measurements during the evaluation of univariate
importance. These types correspond to the types in Shrout and Fleiss (1979).
If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="extract_data_+3A_dynamic_model_loading">dynamic_model_loading</code></td>
<td>
<p>(<em>optional</em>) Enables dynamic loading of models
during the evaluation process, if <code>TRUE</code>. Defaults to <code>FALSE</code>. Dynamic
loading of models may reduce the overall memory footprint, at the cost of
increased disk or network IO. Models can only be dynamically loaded if they
are found at an accessible disk or network location. Setting this parameter
to <code>TRUE</code> may help if parallel processing causes out-of-memory issues during
evaluation.</p>
</td></tr>
<tr><td><code id="extract_data_+3A_message_indent">message_indent</code></td>
<td>
<p>Number of indentation steps for messages shown during
computation and extraction of various data elements.</p>
</td></tr>
<tr><td><code id="extract_data_+3A_verbose">verbose</code></td>
<td>
<p>Flag to indicate whether feedback should be provided on the
computation and extraction of various data elements.</p>
</td></tr>
<tr><td><code id="extract_data_+3A_...">...</code></td>
<td>
<p>Unused arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>familiarData</code> object.
</p>


<h3>References</h3>


<ol>
<li><p> Shrout, P. E. &amp; Fleiss, J. L. Intraclass correlations: uses in
assessing rater reliability. Psychol. Bull. 86, 420–428 (1979).
</p>
</li></ol>


<hr>
<h2 id='extract_decision_curve_data'>Internal function to extract decision curve analysis data.</h2><span id='topic+extract_decision_curve_data'></span>

<h3>Description</h3>

<p>Computes decision curve analysis data from a <code>familiarEnsemble</code> object.
Calibration tests are performed based on expected (predicted) and observed
outcomes. For all outcomes, calibration-at-the-large and calibration slopes
are determined. Furthermore, for all but survival outcomes, a repeated,
randomised grouping Hosmer-Lemeshow test is performed. For survival
outcomes, the Nam-D'Agostino and Greenwood-Nam-D'Agostino tests are
performed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>extract_decision_curve_data(
  object,
  data,
  cl = NULL,
  ensemble_method = waiver(),
  evaluation_times = waiver(),
  detail_level = waiver(),
  estimation_type = waiver(),
  aggregate_results = waiver(),
  confidence_level = waiver(),
  bootstrap_ci_method = waiver(),
  is_pre_processed = FALSE,
  message_indent = 0L,
  verbose = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="extract_decision_curve_data_+3A_object">object</code></td>
<td>
<p>A <code>familiarEnsemble</code> object, which is an ensemble of one or more
<code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="extract_decision_curve_data_+3A_data">data</code></td>
<td>
<p>A <code>dataObject</code> object, <code>data.table</code> or <code>data.frame</code> that
constitutes the data that are assessed.</p>
</td></tr>
<tr><td><code id="extract_decision_curve_data_+3A_cl">cl</code></td>
<td>
<p>Cluster created using the <code>parallel</code> package. This cluster is then
used to speed up computation through parallellisation.</p>
</td></tr>
<tr><td><code id="extract_decision_curve_data_+3A_ensemble_method">ensemble_method</code></td>
<td>
<p>Method for ensembling predictions from models for the
same sample. Available methods are:
</p>

<ul>
<li> <p><code>median</code> (default): Use the median of the predicted values as the ensemble
value for a sample.
</p>
</li>
<li> <p><code>mean</code>: Use the mean of the predicted values as the ensemble value for a
sample.
</p>
</li></ul>
</td></tr>
<tr><td><code id="extract_decision_curve_data_+3A_evaluation_times">evaluation_times</code></td>
<td>
<p>One or more time points that are used for in analysis of
survival problems when data has to be assessed at a set time, e.g.
calibration. If not provided explicitly, this parameter is read from
settings used at creation of the underlying <code>familiarModel</code> objects. Only
used for <code>survival</code> outcomes.</p>
</td></tr>
<tr><td><code id="extract_decision_curve_data_+3A_detail_level">detail_level</code></td>
<td>
<p>(<em>optional</em>) Sets the level at which results are computed
and aggregated.
</p>

<ul>
<li> <p><code>ensemble</code>: Results are computed at the ensemble level, i.e. over all
models in the ensemble. This means that, for example, bias-corrected
estimates of model performance are assessed by creating (at least) 20
bootstraps and computing the model performance of the ensemble model for
each bootstrap.
</p>
</li>
<li> <p><code>hybrid</code> (default): Results are computed at the level of models in an
ensemble. This means that, for example, bias-corrected estimates of model
performance are directly computed using the models in the ensemble. If there
are at least 20 trained models in the ensemble, performance is computed for
each model, in contrast to <code>ensemble</code> where performance is computed for the
ensemble of models. If there are less than 20 trained models in the
ensemble, bootstraps are created so that at least 20 point estimates can be
made.
</p>
</li>
<li> <p><code>model</code>: Results are computed at the model level. This means that, for
example, bias-corrected estimates of model performance are assessed by
creating (at least) 20 bootstraps and computing the performance of the model
for each bootstrap.
</p>
</li></ul>

<p>Note that each level of detail has a different interpretation for bootstrap
confidence intervals. For <code>ensemble</code> and <code>model</code> these are the confidence
intervals for the ensemble and an individual model, respectively. That is,
the confidence interval describes the range where an estimate produced by a
respective ensemble or model trained on a repeat of the experiment may be
found with the probability of the confidence level. For <code>hybrid</code>, it
represents the range where any single model trained on a repeat of the
experiment may be found with the probability of the confidence level. By
definition, confidence intervals obtained using <code>hybrid</code> are at least as
wide as those for <code>ensemble</code>. <code>hybrid</code> offers the correct interpretation if
the goal of the analysis is to assess the result of a single, unspecified,
model.
</p>
<p><code>hybrid</code> is generally computationally less expensive then <code>ensemble</code>, which
in turn is somewhat less expensive than <code>model</code>.
</p>
<p>A non-default <code>detail_level</code> parameter can be specified for separate
evaluation steps by providing a parameter value in a named list with data
elements, e.g. <code>list("auc_data"="ensemble", "model_performance"="hybrid")</code>.
This parameter can be set for the following data elements: <code>auc_data</code>,
<code>decision_curve_analyis</code>, <code>model_performance</code>, <code>permutation_vimp</code>,
<code>ice_data</code>, <code>prediction_data</code> and <code>confusion_matrix</code>.</p>
</td></tr>
<tr><td><code id="extract_decision_curve_data_+3A_estimation_type">estimation_type</code></td>
<td>
<p>(<em>optional</em>) Sets the type of estimation that should be
possible. This has the following options:
</p>

<ul>
<li> <p><code>point</code>: Point estimates.
</p>
</li>
<li> <p><code>bias_correction</code> or <code>bc</code>: Bias-corrected estimates. A bias-corrected
estimate is computed from (at least) 20 point estimates, and <code>familiar</code> may
bootstrap the data to create them.
</p>
</li>
<li> <p><code>bootstrap_confidence_interval</code> or <code>bci</code> (default): Bias-corrected
estimates with bootstrap confidence intervals (Efron and Hastie, 2016). The
number of point estimates required depends on the <code>confidence_level</code>
parameter, and <code>familiar</code> may bootstrap the data to create them.
</p>
</li></ul>

<p>As with <code>detail_level</code>, a non-default <code>estimation_type</code> parameter can be
specified for separate evaluation steps by providing a parameter value in a
named list with data elements, e.g. <code>list("auc_data"="bci", "model_performance"="point")</code>. This parameter can be set for the following
data elements: <code>auc_data</code>, <code>decision_curve_analyis</code>, <code>model_performance</code>,
<code>permutation_vimp</code>, <code>ice_data</code>, and <code>prediction_data</code>.</p>
</td></tr>
<tr><td><code id="extract_decision_curve_data_+3A_aggregate_results">aggregate_results</code></td>
<td>
<p>(<em>optional</em>) Flag that signifies whether results
should be aggregated during evaluation. If <code>estimation_type</code> is
<code>bias_correction</code> or <code>bc</code>, aggregation leads to a single bias-corrected
estimate. If <code>estimation_type</code> is <code>bootstrap_confidence_interval</code> or <code>bci</code>,
aggregation leads to a single bias-corrected estimate with lower and upper
boundaries of the confidence interval. This has no effect if
<code>estimation_type</code> is <code>point</code>.
</p>
<p>The default value is equal to <code>TRUE</code> except when assessing metrics to assess
model performance, as the default violin plot requires underlying data.
</p>
<p>As with <code>detail_level</code> and <code>estimation_type</code>, a non-default
<code>aggregate_results</code> parameter can be specified for separate evaluation steps
by providing a parameter value in a named list with data elements, e.g.
<code>list("auc_data"=TRUE, , "model_performance"=FALSE)</code>. This parameter exists
for the same elements as <code>estimation_type</code>.</p>
</td></tr>
<tr><td><code id="extract_decision_curve_data_+3A_confidence_level">confidence_level</code></td>
<td>
<p>(<em>optional</em>) Numeric value for the level at which
confidence intervals are determined. In the case bootstraps are used to
determine the confidence intervals bootstrap estimation, <code>familiar</code> uses the
rule of thumb <code class="reqn">n = 20 / ci.level</code> to determine the number of required
bootstraps.
</p>
<p>The default value is <code>0.95</code>.</p>
</td></tr>
<tr><td><code id="extract_decision_curve_data_+3A_bootstrap_ci_method">bootstrap_ci_method</code></td>
<td>
<p>(<em>optional</em>) Method used to determine bootstrap
confidence intervals (Efron and Hastie, 2016). The following methods are
implemented:
</p>

<ul>
<li> <p><code>percentile</code> (default): Confidence intervals obtained using the percentile
method.
</p>
</li>
<li> <p><code>bc</code>: Bias-corrected confidence intervals.
</p>
</li></ul>

<p>Note that the standard method is not implemented because this method is
often not suitable due to non-normal distributions. The bias-corrected and
accelerated (BCa) method is not implemented yet.</p>
</td></tr>
<tr><td><code id="extract_decision_curve_data_+3A_is_pre_processed">is_pre_processed</code></td>
<td>
<p>Flag that indicates whether the data was already
pre-processed externally, e.g. normalised and clustered. Only used if the
<code>data</code> argument is a <code>data.table</code> or <code>data.frame</code>.</p>
</td></tr>
<tr><td><code id="extract_decision_curve_data_+3A_message_indent">message_indent</code></td>
<td>
<p>Number of indentation steps for messages shown during
computation and extraction of various data elements.</p>
</td></tr>
<tr><td><code id="extract_decision_curve_data_+3A_verbose">verbose</code></td>
<td>
<p>Flag to indicate whether feedback should be provided on the
computation and extraction of various data elements.</p>
</td></tr>
<tr><td><code id="extract_decision_curve_data_+3A_...">...</code></td>
<td>
<p>Unused arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with data.tables containing calibration test information for
the ensemble model.
</p>

<hr>
<h2 id='extract_dispatcher+2CfamiliarEnsemble+2CfamiliarDataElement-method'>Internal function to dispatch extraction functions.</h2><span id='topic+extract_dispatcher+2CfamiliarEnsemble+2CfamiliarDataElement-method'></span>

<h3>Description</h3>

<p>This function provides a unified access point to extraction
functions. Some of these functions require bootstrapping and result
aggregation, which are handled here.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'familiarEnsemble,familiarDataElement'
extract_dispatcher(
  cl = NULL,
  FUN,
  object,
  proto_data_element,
  aggregate_results,
  has_internal_bootstrap,
  ...,
  message_indent = 0L,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="extract_dispatcher+2B2CfamiliarEnsemble+2B2CfamiliarDataElement-method_+3A_cl">cl</code></td>
<td>
<p>Cluster created using the <code>parallel</code> package. This cluster is then
used to speed up computation through parallellisation.</p>
</td></tr>
<tr><td><code id="extract_dispatcher+2B2CfamiliarEnsemble+2B2CfamiliarDataElement-method_+3A_fun">FUN</code></td>
<td>
<p>Extraction function or method to which data and parameters are
dispatched.</p>
</td></tr>
<tr><td><code id="extract_dispatcher+2B2CfamiliarEnsemble+2B2CfamiliarDataElement-method_+3A_object">object</code></td>
<td>
<p>A <code>familiarEnsemble</code> object.</p>
</td></tr>
<tr><td><code id="extract_dispatcher+2B2CfamiliarEnsemble+2B2CfamiliarDataElement-method_+3A_proto_data_element">proto_data_element</code></td>
<td>
<p>A <code>familiarDataElement</code> object, or an object that
inherits from it.</p>
</td></tr>
<tr><td><code id="extract_dispatcher+2B2CfamiliarEnsemble+2B2CfamiliarDataElement-method_+3A_aggregate_results">aggregate_results</code></td>
<td>
<p>A logical flag indicating whether results should be
aggregated.</p>
</td></tr>
<tr><td><code id="extract_dispatcher+2B2CfamiliarEnsemble+2B2CfamiliarDataElement-method_+3A_has_internal_bootstrap">has_internal_bootstrap</code></td>
<td>
<p>A logical flag that indicates whether <code>FUN</code> has
internal bootstrapping capabilities.</p>
</td></tr>
<tr><td><code id="extract_dispatcher+2B2CfamiliarEnsemble+2B2CfamiliarDataElement-method_+3A_...">...</code></td>
<td>
<p>Unused arguments.</p>
</td></tr>
<tr><td><code id="extract_dispatcher+2B2CfamiliarEnsemble+2B2CfamiliarDataElement-method_+3A_message_indent">message_indent</code></td>
<td>
<p>Number of indentation steps for messages shown during
computation and extraction of various data elements.</p>
</td></tr>
<tr><td><code id="extract_dispatcher+2B2CfamiliarEnsemble+2B2CfamiliarDataElement-method_+3A_verbose">verbose</code></td>
<td>
<p>Flag to indicate whether feedback should be provided on the
computation and extraction of various data elements.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function first determines how many data points need to be
evaluated to complete the desired estimation, i.e. 1 for point estimates, 20
for bias-corrected estimates, and 20 / (1 - confidence level) for bootstrap
confidence intervals.
</p>
<p>Subsequently, we determine the number of models. This number is used to set
external or internal clusters, the number of bootstraps, and to evaluate
whether the estimation can be done in case <code>FUN</code> does not support
bootstrapping.
</p>


<h3>Value</h3>

<p>A list of <code>familiarDataElement</code> objects.
</p>

<hr>
<h2 id='extract_experimental_setup'>Parse experimental design</h2><span id='topic+extract_experimental_setup'></span>

<h3>Description</h3>

<p>Parse experimental design
</p>


<h3>Usage</h3>

<pre><code class='language-R'>extract_experimental_setup(
  experimental_design,
  file_dir,
  message_indent = 0L,
  verbose = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="extract_experimental_setup_+3A_experimental_design">experimental_design</code></td>
<td>
<p>(<strong>required</strong>) Defines what the experiment looks
like, e.g. <code>cv(bt(fs,20)+mb,3,2)+ev</code> for 2 times repeated 3-fold
cross-validation with nested feature selection on 20 bootstraps and
model-building, and external validation. The basic workflow components are:
</p>

<ul>
<li> <p><code>fs</code>: (required) feature selection step.
</p>
</li>
<li> <p><code>mb</code>: (required) model building step.
</p>
</li>
<li> <p><code>ev</code>: (optional) external validation. Note that internal validation due
to subsampling will always be conducted if the subsampling methods create
any validation data sets.
</p>
</li></ul>

<p>The different components are linked using <code>+</code>.
</p>
<p>Different subsampling methods can be used in conjunction with the basic
workflow components:
</p>

<ul>
<li> <p><code>bs(x,n)</code>: (stratified) .632 bootstrap, with <code>n</code> the number of
bootstraps. In contrast to <code>bt</code>, feature pre-processing parameters and
hyperparameter optimisation are conducted on individual bootstraps.
</p>
</li>
<li> <p><code>bt(x,n)</code>: (stratified) .632 bootstrap, with <code>n</code> the number of
bootstraps. Unlike <code>bs</code> and other subsampling methods, no separate
pre-processing parameters or optimised hyperparameters will be determined
for each bootstrap.
</p>
</li>
<li> <p><code>cv(x,n,p)</code>: (stratified) <code>n</code>-fold cross-validation, repeated <code>p</code> times.
Pre-processing parameters are determined for each iteration.
</p>
</li>
<li> <p><code>lv(x)</code>: leave-one-out-cross-validation. Pre-processing parameters are
determined for each iteration.
</p>
</li>
<li> <p><code>ip(x)</code>: imbalance partitioning for addressing class imbalances on the
data set. Pre-processing parameters are determined for each partition. The
number of partitions generated depends on the imbalance correction method
(see the <code>imbalance_correction_method</code> parameter). Imbalance partitioning
does not generate validation sets.
</p>
</li></ul>

<p>As shown in the example above, sampling algorithms can be nested.
</p>
<p>The simplest valid experimental design is <code>fs+mb</code>, which corresponds to a
TRIPOD type 1a analysis. Type 1b analyses are only possible using
bootstraps, e.g. <code>bt(fs+mb,100)</code>. Type 2a analyses can be conducted using
cross-validation, e.g. <code>cv(bt(fs,100)+mb,10,1)</code>. Depending on the origin of
the external validation data, designs such as <code>fs+mb+ev</code> or
<code>cv(bt(fs,100)+mb,10,1)+ev</code> constitute type 2b or type 3 analyses. Type 4
analyses can be done by obtaining one or more <code>familiarModel</code> objects from
others and applying them to your own data set.
</p>
<p>Alternatively, the <code>experimental_design</code> parameter may be used to provide a
path to a file containing iterations, which is named <code style="white-space: pre;">&#8288;####_iterations.RDS&#8288;</code>
by convention. This path can be relative to the directory of the current
experiment (<code>experiment_dir</code>), or an absolute path. The absolute path may
thus also point to a file from a different experiment.</p>
</td></tr>
<tr><td><code id="extract_experimental_setup_+3A_message_indent">message_indent</code></td>
<td>
<p>Spacing inserted before messages.</p>
</td></tr>
<tr><td><code id="extract_experimental_setup_+3A_verbose">verbose</code></td>
<td>
<p>Sets verbosity.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function converts the experimental_design string
</p>


<h3>Value</h3>

<p>data.table with subsampler information at different levels of the
experimental design.
</p>

<hr>
<h2 id='extract_feature_expression'>Internal function to extract feature expressions.</h2><span id='topic+extract_feature_expression'></span>

<h3>Description</h3>

<p>Computes and extracts feature expressions for features
used in a <code>familiarEnsemble</code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>extract_feature_expression(
  object,
  data,
  feature_similarity,
  sample_similarity,
  feature_cluster_method = waiver(),
  feature_linkage_method = waiver(),
  feature_similarity_metric = waiver(),
  sample_cluster_method = waiver(),
  sample_linkage_method = waiver(),
  sample_similarity_metric = waiver(),
  evaluation_times = waiver(),
  message_indent = 0L,
  verbose = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="extract_feature_expression_+3A_object">object</code></td>
<td>
<p>A <code>familiarEnsemble</code> object, which is an ensemble of one or more
<code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="extract_feature_expression_+3A_data">data</code></td>
<td>
<p>A <code>dataObject</code> object, <code>data.table</code> or <code>data.frame</code> that
constitutes the data that are assessed.</p>
</td></tr>
<tr><td><code id="extract_feature_expression_+3A_feature_similarity">feature_similarity</code></td>
<td>
<p>Table containing pairwise distance between
sample. This is used to determine cluster information, and indicate which
samples are similar. The table is created by the
<code>extract_sample_similarity</code> method.</p>
</td></tr>
<tr><td><code id="extract_feature_expression_+3A_feature_cluster_method">feature_cluster_method</code></td>
<td>
<p>The method used to perform clustering. These are
the same methods as for the <code>cluster_method</code> configuration parameter:
<code>none</code>, <code>hclust</code>, <code>agnes</code>, <code>diana</code> and <code>pam</code>.
</p>
<p><code>none</code> cannot be used when extracting data regarding mutual correlation or
feature expressions.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="extract_feature_expression_+3A_feature_linkage_method">feature_linkage_method</code></td>
<td>
<p>The method used for agglomerative clustering in
<code>hclust</code> and <code>agnes</code>. These are the same methods as for the
<code>cluster_linkage_method</code> configuration parameter: <code>average</code>, <code>single</code>,
<code>complete</code>, <code>weighted</code>, and <code>ward</code>.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="extract_feature_expression_+3A_feature_similarity_metric">feature_similarity_metric</code></td>
<td>
<p>Metric to determine pairwise similarity
between features. Similarity is computed in the same manner as for
clustering, and <code>feature_similarity_metric</code> therefore has the same options
as <code>cluster_similarity_metric</code>: <code>mcfadden_r2</code>, <code>cox_snell_r2</code>,
<code>nagelkerke_r2</code>, <code>spearman</code>, <code>kendall</code> and <code>pearson</code>.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="extract_feature_expression_+3A_sample_cluster_method">sample_cluster_method</code></td>
<td>
<p>The method used to perform clustering based on
distance between samples. These are the same methods as for the
<code>cluster_method</code> configuration parameter: <code>hclust</code>, <code>agnes</code>, <code>diana</code> and
<code>pam</code>.
</p>
<p><code>none</code> cannot be used when extracting data for feature expressions.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="extract_feature_expression_+3A_sample_linkage_method">sample_linkage_method</code></td>
<td>
<p>The method used for agglomerative clustering in
<code>hclust</code> and <code>agnes</code>. These are the same methods as for the
<code>cluster_linkage_method</code> configuration parameter: <code>average</code>, <code>single</code>,
<code>complete</code>, <code>weighted</code>, and <code>ward</code>.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="extract_feature_expression_+3A_sample_similarity_metric">sample_similarity_metric</code></td>
<td>
<p>Metric to determine pairwise similarity
between samples. Similarity is computed in the same manner as for
clustering, but <code>sample_similarity_metric</code> has different options that are
better suited to computing distance between samples instead of between
features: <code>gower</code>, <code>euclidean</code>.
</p>
<p>The underlying feature data is scaled to the <code class="reqn">[0, 1]</code> range (for
numerical features) using the feature values across the samples. The
normalisation parameters required can optionally be computed from feature
data with the outer 5% (on both sides) of feature values trimmed or
winsorised. To do so append <code style="white-space: pre;">&#8288;_trim&#8288;</code> (trimming) or <code style="white-space: pre;">&#8288;_winsor&#8288;</code> (winsorising) to
the metric name. This reduces the effect of outliers somewhat.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="extract_feature_expression_+3A_evaluation_times">evaluation_times</code></td>
<td>
<p>One or more time points that are used for in analysis of
survival problems when data has to be assessed at a set time, e.g.
calibration. If not provided explicitly, this parameter is read from
settings used at creation of the underlying <code>familiarModel</code> objects. Only
used for <code>survival</code> outcomes.</p>
</td></tr>
<tr><td><code id="extract_feature_expression_+3A_message_indent">message_indent</code></td>
<td>
<p>Number of indentation steps for messages shown during
computation and extraction of various data elements.</p>
</td></tr>
<tr><td><code id="extract_feature_expression_+3A_verbose">verbose</code></td>
<td>
<p>Flag to indicate whether feedback should be provided on the
computation and extraction of various data elements.</p>
</td></tr>
<tr><td><code id="extract_feature_expression_+3A_...">...</code></td>
<td>
<p>Unused arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with a data.table containing feature expressions.
</p>

<hr>
<h2 id='extract_feature_similarity'>Internal function to extract the feature distance table.</h2><span id='topic+extract_feature_similarity'></span>

<h3>Description</h3>

<p>Computes and extracts the feature distance table for features
used in a <code>familiarEnsemble</code> object. This table can be used to cluster
features, and is exported directly by <code>export_feature_similarity</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>extract_feature_similarity(
  object,
  data,
  cl = NULL,
  estimation_type = waiver(),
  aggregate_results = waiver(),
  confidence_level = waiver(),
  bootstrap_ci_method = waiver(),
  is_pre_processed = FALSE,
  feature_cluster_method = waiver(),
  feature_linkage_method = waiver(),
  feature_cluster_cut_method = waiver(),
  feature_similarity_threshold = waiver(),
  feature_similarity_metric = waiver(),
  verbose = FALSE,
  message_indent = 0L,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="extract_feature_similarity_+3A_object">object</code></td>
<td>
<p>A <code>familiarEnsemble</code> object, which is an ensemble of one or more
<code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="extract_feature_similarity_+3A_data">data</code></td>
<td>
<p>A <code>dataObject</code> object, <code>data.table</code> or <code>data.frame</code> that
constitutes the data that are assessed.</p>
</td></tr>
<tr><td><code id="extract_feature_similarity_+3A_cl">cl</code></td>
<td>
<p>Cluster created using the <code>parallel</code> package. This cluster is then
used to speed up computation through parallellisation.</p>
</td></tr>
<tr><td><code id="extract_feature_similarity_+3A_estimation_type">estimation_type</code></td>
<td>
<p>(<em>optional</em>) Sets the type of estimation that should be
possible. This has the following options:
</p>

<ul>
<li> <p><code>point</code>: Point estimates.
</p>
</li>
<li> <p><code>bias_correction</code> or <code>bc</code>: Bias-corrected estimates. A bias-corrected
estimate is computed from (at least) 20 point estimates, and <code>familiar</code> may
bootstrap the data to create them.
</p>
</li>
<li> <p><code>bootstrap_confidence_interval</code> or <code>bci</code> (default): Bias-corrected
estimates with bootstrap confidence intervals (Efron and Hastie, 2016). The
number of point estimates required depends on the <code>confidence_level</code>
parameter, and <code>familiar</code> may bootstrap the data to create them.
</p>
</li></ul>

<p>As with <code>detail_level</code>, a non-default <code>estimation_type</code> parameter can be
specified for separate evaluation steps by providing a parameter value in a
named list with data elements, e.g. <code>list("auc_data"="bci", "model_performance"="point")</code>. This parameter can be set for the following
data elements: <code>auc_data</code>, <code>decision_curve_analyis</code>, <code>model_performance</code>,
<code>permutation_vimp</code>, <code>ice_data</code>, and <code>prediction_data</code>.</p>
</td></tr>
<tr><td><code id="extract_feature_similarity_+3A_aggregate_results">aggregate_results</code></td>
<td>
<p>(<em>optional</em>) Flag that signifies whether results
should be aggregated during evaluation. If <code>estimation_type</code> is
<code>bias_correction</code> or <code>bc</code>, aggregation leads to a single bias-corrected
estimate. If <code>estimation_type</code> is <code>bootstrap_confidence_interval</code> or <code>bci</code>,
aggregation leads to a single bias-corrected estimate with lower and upper
boundaries of the confidence interval. This has no effect if
<code>estimation_type</code> is <code>point</code>.
</p>
<p>The default value is equal to <code>TRUE</code> except when assessing metrics to assess
model performance, as the default violin plot requires underlying data.
</p>
<p>As with <code>detail_level</code> and <code>estimation_type</code>, a non-default
<code>aggregate_results</code> parameter can be specified for separate evaluation steps
by providing a parameter value in a named list with data elements, e.g.
<code>list("auc_data"=TRUE, , "model_performance"=FALSE)</code>. This parameter exists
for the same elements as <code>estimation_type</code>.</p>
</td></tr>
<tr><td><code id="extract_feature_similarity_+3A_confidence_level">confidence_level</code></td>
<td>
<p>(<em>optional</em>) Numeric value for the level at which
confidence intervals are determined. In the case bootstraps are used to
determine the confidence intervals bootstrap estimation, <code>familiar</code> uses the
rule of thumb <code class="reqn">n = 20 / ci.level</code> to determine the number of required
bootstraps.
</p>
<p>The default value is <code>0.95</code>.</p>
</td></tr>
<tr><td><code id="extract_feature_similarity_+3A_bootstrap_ci_method">bootstrap_ci_method</code></td>
<td>
<p>(<em>optional</em>) Method used to determine bootstrap
confidence intervals (Efron and Hastie, 2016). The following methods are
implemented:
</p>

<ul>
<li> <p><code>percentile</code> (default): Confidence intervals obtained using the percentile
method.
</p>
</li>
<li> <p><code>bc</code>: Bias-corrected confidence intervals.
</p>
</li></ul>

<p>Note that the standard method is not implemented because this method is
often not suitable due to non-normal distributions. The bias-corrected and
accelerated (BCa) method is not implemented yet.</p>
</td></tr>
<tr><td><code id="extract_feature_similarity_+3A_is_pre_processed">is_pre_processed</code></td>
<td>
<p>Flag that indicates whether the data was already
pre-processed externally, e.g. normalised and clustered. Only used if the
<code>data</code> argument is a <code>data.table</code> or <code>data.frame</code>.</p>
</td></tr>
<tr><td><code id="extract_feature_similarity_+3A_feature_cluster_method">feature_cluster_method</code></td>
<td>
<p>The method used to perform clustering. These are
the same methods as for the <code>cluster_method</code> configuration parameter:
<code>none</code>, <code>hclust</code>, <code>agnes</code>, <code>diana</code> and <code>pam</code>.
</p>
<p><code>none</code> cannot be used when extracting data regarding mutual correlation or
feature expressions.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="extract_feature_similarity_+3A_feature_linkage_method">feature_linkage_method</code></td>
<td>
<p>The method used for agglomerative clustering in
<code>hclust</code> and <code>agnes</code>. These are the same methods as for the
<code>cluster_linkage_method</code> configuration parameter: <code>average</code>, <code>single</code>,
<code>complete</code>, <code>weighted</code>, and <code>ward</code>.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="extract_feature_similarity_+3A_feature_cluster_cut_method">feature_cluster_cut_method</code></td>
<td>
<p>The method used to divide features into
separate clusters. The available methods are the same as for the
<code>cluster_cut_method</code> configuration parameter: <code>silhouette</code>, <code>fixed_cut</code> and
<code>dynamic_cut</code>.
</p>
<p><code>silhouette</code> is available for all cluster methods, but <code>fixed_cut</code> only
applies to methods that create hierarchical trees (<code>hclust</code>, <code>agnes</code> and
<code>diana</code>). <code>dynamic_cut</code> requires the <code>dynamicTreeCut</code> package and can only
be used with <code>agnes</code> and <code>hclust</code>.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="extract_feature_similarity_+3A_feature_similarity_threshold">feature_similarity_threshold</code></td>
<td>
<p>The threshold level for pair-wise
similarity that is required to form feature clusters with the <code>fixed_cut</code>
method.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="extract_feature_similarity_+3A_feature_similarity_metric">feature_similarity_metric</code></td>
<td>
<p>Metric to determine pairwise similarity
between features. Similarity is computed in the same manner as for
clustering, and <code>feature_similarity_metric</code> therefore has the same options
as <code>cluster_similarity_metric</code>: <code>mcfadden_r2</code>, <code>cox_snell_r2</code>,
<code>nagelkerke_r2</code>, <code>spearman</code>, <code>kendall</code> and <code>pearson</code>.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="extract_feature_similarity_+3A_verbose">verbose</code></td>
<td>
<p>Flag to indicate whether feedback should be provided on the
computation and extraction of various data elements.</p>
</td></tr>
<tr><td><code id="extract_feature_similarity_+3A_message_indent">message_indent</code></td>
<td>
<p>Number of indentation steps for messages shown during
computation and extraction of various data elements.</p>
</td></tr>
<tr><td><code id="extract_feature_similarity_+3A_...">...</code></td>
<td>
<p>Unused arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data.table containing pairwise distance between features. This data
is only the upper triangular of the complete matrix (i.e. the sparse
unitriangular representation). Diagonals will always be 0.0 and the lower
triangular is mirrored.
</p>

<hr>
<h2 id='extract_fs_vimp'>Internal function to extract feature selection variable importance.</h2><span id='topic+extract_fs_vimp'></span>

<h3>Description</h3>

<p>Aggregate variable importance obtained during feature selection.
This information can only be obtained as part of the main <code>summon_familiar</code> process.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>extract_fs_vimp(
  object,
  aggregation_method = waiver(),
  rank_threshold = waiver(),
  message_indent = 0L,
  verbose = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="extract_fs_vimp_+3A_object">object</code></td>
<td>
<p>A <code>familiarEnsemble</code> object, which is an ensemble of one or more
<code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="extract_fs_vimp_+3A_aggregation_method">aggregation_method</code></td>
<td>
<p>Method for aggregating variable importances for the
purpose of evaluation. Variable importances are determined during feature
selection steps and after training the model. Both types are evaluated, but
feature selection variable importance is only evaluated at run-time.
</p>
<p>See the documentation for the <code>vimp_aggregation_method</code> argument in
<code>summon_familiar</code> for information concerning the different available
methods.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="extract_fs_vimp_+3A_rank_threshold">rank_threshold</code></td>
<td>
<p>The threshold used to  define the subset of highly
important features during evaluation.
</p>
<p>See the documentation for the <code>vimp_aggregation_rank_threshold</code> argument in
<code>summon_familiar</code> for more information.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="extract_fs_vimp_+3A_message_indent">message_indent</code></td>
<td>
<p>Number of indentation steps for messages shown during
computation and extraction of various data elements.</p>
</td></tr>
<tr><td><code id="extract_fs_vimp_+3A_verbose">verbose</code></td>
<td>
<p>Flag to indicate whether feedback should be provided on the
computation and extraction of various data elements.</p>
</td></tr>
<tr><td><code id="extract_fs_vimp_+3A_...">...</code></td>
<td>
<p>Unused arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing feature selection variable importance information.
</p>

<hr>
<h2 id='extract_hyperparameters'>Internal function to extract hyperparameters from models.</h2><span id='topic+extract_hyperparameters'></span>

<h3>Description</h3>

<p>Collects hyperparameters from models in a <code>familiarEnsemble</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>extract_hyperparameters(object, message_indent = 0L, verbose = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="extract_hyperparameters_+3A_object">object</code></td>
<td>
<p>A <code>familiarEnsemble</code> object, which is an ensemble of one or more
<code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="extract_hyperparameters_+3A_message_indent">message_indent</code></td>
<td>
<p>Number of indentation steps for messages shown during
computation and extraction of various data elements.</p>
</td></tr>
<tr><td><code id="extract_hyperparameters_+3A_verbose">verbose</code></td>
<td>
<p>Flag to indicate whether feedback should be provided on the
computation and extraction of various data elements.</p>
</td></tr>
<tr><td><code id="extract_hyperparameters_+3A_...">...</code></td>
<td>
<p>Unused arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of familiarDataElements with hyperparameters.
</p>

<hr>
<h2 id='extract_ice'>Internal function to extract data for individual conditional
expectation plots.</h2><span id='topic+extract_ice'></span>

<h3>Description</h3>

<p>Computes data for individual conditional expectation plots and
partial dependence plots for the model(s) in a <code>familiarEnsemble</code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>extract_ice(
  object,
  data,
  cl = NULL,
  features = NULL,
  feature_x_range = NULL,
  feature_y_range = NULL,
  n_sample_points = 50L,
  ensemble_method = waiver(),
  evaluation_times = waiver(),
  sample_limit = waiver(),
  detail_level = waiver(),
  estimation_type = waiver(),
  aggregate_results = waiver(),
  confidence_level = waiver(),
  bootstrap_ci_method = waiver(),
  is_pre_processed = FALSE,
  message_indent = 0L,
  verbose = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="extract_ice_+3A_object">object</code></td>
<td>
<p>A <code>familiarEnsemble</code> object, which is an ensemble of one or more
<code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="extract_ice_+3A_data">data</code></td>
<td>
<p>A <code>dataObject</code> object, <code>data.table</code> or <code>data.frame</code> that
constitutes the data that are assessed.</p>
</td></tr>
<tr><td><code id="extract_ice_+3A_cl">cl</code></td>
<td>
<p>Cluster created using the <code>parallel</code> package. This cluster is then
used to speed up computation through parallellisation.</p>
</td></tr>
<tr><td><code id="extract_ice_+3A_features">features</code></td>
<td>
<p>Names of the feature or features (2) assessed simultaneously.
By default <code>NULL</code>, which means that all features are assessed one-by-one.</p>
</td></tr>
<tr><td><code id="extract_ice_+3A_feature_x_range">feature_x_range</code></td>
<td>
<p>When one or two features are defined using <code>features</code>,
<code>feature_x_range</code> can be used to set the range of values for the first
feature. For numeric features, a vector of two values is assumed to indicate
a range from which <code>n_sample_points</code> are uniformly sampled. A vector of more
than two values is interpreted as is, i.e. these represent the values to be
sampled. For categorical features, values should represent a (sub)set of
available levels.</p>
</td></tr>
<tr><td><code id="extract_ice_+3A_feature_y_range">feature_y_range</code></td>
<td>
<p>As <code>feature_x_range</code>, but for the second feature in
case two features are defined.</p>
</td></tr>
<tr><td><code id="extract_ice_+3A_n_sample_points">n_sample_points</code></td>
<td>
<p>Number of points used to sample continuous features.</p>
</td></tr>
<tr><td><code id="extract_ice_+3A_ensemble_method">ensemble_method</code></td>
<td>
<p>Method for ensembling predictions from models for the
same sample. Available methods are:
</p>

<ul>
<li> <p><code>median</code> (default): Use the median of the predicted values as the ensemble
value for a sample.
</p>
</li>
<li> <p><code>mean</code>: Use the mean of the predicted values as the ensemble value for a
sample.
</p>
</li></ul>
</td></tr>
<tr><td><code id="extract_ice_+3A_evaluation_times">evaluation_times</code></td>
<td>
<p>One or more time points that are used for in analysis of
survival problems when data has to be assessed at a set time, e.g.
calibration. If not provided explicitly, this parameter is read from
settings used at creation of the underlying <code>familiarModel</code> objects. Only
used for <code>survival</code> outcomes.</p>
</td></tr>
<tr><td><code id="extract_ice_+3A_sample_limit">sample_limit</code></td>
<td>
<p>(<em>optional</em>) Set the upper limit of the number of samples
that are used during evaluation steps. Cannot be less than 20.
</p>
<p>This setting can be specified per data element by providing a parameter
value in a named list with data elements, e.g.
<code>list("sample_similarity"=100, "permutation_vimp"=1000)</code>.
</p>
<p>This parameter can be set for the following data elements:
<code>sample_similarity</code> and <code>ice_data</code>.</p>
</td></tr>
<tr><td><code id="extract_ice_+3A_detail_level">detail_level</code></td>
<td>
<p>(<em>optional</em>) Sets the level at which results are computed
and aggregated.
</p>

<ul>
<li> <p><code>ensemble</code>: Results are computed at the ensemble level, i.e. over all
models in the ensemble. This means that, for example, bias-corrected
estimates of model performance are assessed by creating (at least) 20
bootstraps and computing the model performance of the ensemble model for
each bootstrap.
</p>
</li>
<li> <p><code>hybrid</code> (default): Results are computed at the level of models in an
ensemble. This means that, for example, bias-corrected estimates of model
performance are directly computed using the models in the ensemble. If there
are at least 20 trained models in the ensemble, performance is computed for
each model, in contrast to <code>ensemble</code> where performance is computed for the
ensemble of models. If there are less than 20 trained models in the
ensemble, bootstraps are created so that at least 20 point estimates can be
made.
</p>
</li>
<li> <p><code>model</code>: Results are computed at the model level. This means that, for
example, bias-corrected estimates of model performance are assessed by
creating (at least) 20 bootstraps and computing the performance of the model
for each bootstrap.
</p>
</li></ul>

<p>Note that each level of detail has a different interpretation for bootstrap
confidence intervals. For <code>ensemble</code> and <code>model</code> these are the confidence
intervals for the ensemble and an individual model, respectively. That is,
the confidence interval describes the range where an estimate produced by a
respective ensemble or model trained on a repeat of the experiment may be
found with the probability of the confidence level. For <code>hybrid</code>, it
represents the range where any single model trained on a repeat of the
experiment may be found with the probability of the confidence level. By
definition, confidence intervals obtained using <code>hybrid</code> are at least as
wide as those for <code>ensemble</code>. <code>hybrid</code> offers the correct interpretation if
the goal of the analysis is to assess the result of a single, unspecified,
model.
</p>
<p><code>hybrid</code> is generally computationally less expensive then <code>ensemble</code>, which
in turn is somewhat less expensive than <code>model</code>.
</p>
<p>A non-default <code>detail_level</code> parameter can be specified for separate
evaluation steps by providing a parameter value in a named list with data
elements, e.g. <code>list("auc_data"="ensemble", "model_performance"="hybrid")</code>.
This parameter can be set for the following data elements: <code>auc_data</code>,
<code>decision_curve_analyis</code>, <code>model_performance</code>, <code>permutation_vimp</code>,
<code>ice_data</code>, <code>prediction_data</code> and <code>confusion_matrix</code>.</p>
</td></tr>
<tr><td><code id="extract_ice_+3A_estimation_type">estimation_type</code></td>
<td>
<p>(<em>optional</em>) Sets the type of estimation that should be
possible. This has the following options:
</p>

<ul>
<li> <p><code>point</code>: Point estimates.
</p>
</li>
<li> <p><code>bias_correction</code> or <code>bc</code>: Bias-corrected estimates. A bias-corrected
estimate is computed from (at least) 20 point estimates, and <code>familiar</code> may
bootstrap the data to create them.
</p>
</li>
<li> <p><code>bootstrap_confidence_interval</code> or <code>bci</code> (default): Bias-corrected
estimates with bootstrap confidence intervals (Efron and Hastie, 2016). The
number of point estimates required depends on the <code>confidence_level</code>
parameter, and <code>familiar</code> may bootstrap the data to create them.
</p>
</li></ul>

<p>As with <code>detail_level</code>, a non-default <code>estimation_type</code> parameter can be
specified for separate evaluation steps by providing a parameter value in a
named list with data elements, e.g. <code>list("auc_data"="bci", "model_performance"="point")</code>. This parameter can be set for the following
data elements: <code>auc_data</code>, <code>decision_curve_analyis</code>, <code>model_performance</code>,
<code>permutation_vimp</code>, <code>ice_data</code>, and <code>prediction_data</code>.</p>
</td></tr>
<tr><td><code id="extract_ice_+3A_aggregate_results">aggregate_results</code></td>
<td>
<p>(<em>optional</em>) Flag that signifies whether results
should be aggregated during evaluation. If <code>estimation_type</code> is
<code>bias_correction</code> or <code>bc</code>, aggregation leads to a single bias-corrected
estimate. If <code>estimation_type</code> is <code>bootstrap_confidence_interval</code> or <code>bci</code>,
aggregation leads to a single bias-corrected estimate with lower and upper
boundaries of the confidence interval. This has no effect if
<code>estimation_type</code> is <code>point</code>.
</p>
<p>The default value is equal to <code>TRUE</code> except when assessing metrics to assess
model performance, as the default violin plot requires underlying data.
</p>
<p>As with <code>detail_level</code> and <code>estimation_type</code>, a non-default
<code>aggregate_results</code> parameter can be specified for separate evaluation steps
by providing a parameter value in a named list with data elements, e.g.
<code>list("auc_data"=TRUE, , "model_performance"=FALSE)</code>. This parameter exists
for the same elements as <code>estimation_type</code>.</p>
</td></tr>
<tr><td><code id="extract_ice_+3A_confidence_level">confidence_level</code></td>
<td>
<p>(<em>optional</em>) Numeric value for the level at which
confidence intervals are determined. In the case bootstraps are used to
determine the confidence intervals bootstrap estimation, <code>familiar</code> uses the
rule of thumb <code class="reqn">n = 20 / ci.level</code> to determine the number of required
bootstraps.
</p>
<p>The default value is <code>0.95</code>.</p>
</td></tr>
<tr><td><code id="extract_ice_+3A_bootstrap_ci_method">bootstrap_ci_method</code></td>
<td>
<p>(<em>optional</em>) Method used to determine bootstrap
confidence intervals (Efron and Hastie, 2016). The following methods are
implemented:
</p>

<ul>
<li> <p><code>percentile</code> (default): Confidence intervals obtained using the percentile
method.
</p>
</li>
<li> <p><code>bc</code>: Bias-corrected confidence intervals.
</p>
</li></ul>

<p>Note that the standard method is not implemented because this method is
often not suitable due to non-normal distributions. The bias-corrected and
accelerated (BCa) method is not implemented yet.</p>
</td></tr>
<tr><td><code id="extract_ice_+3A_is_pre_processed">is_pre_processed</code></td>
<td>
<p>Flag that indicates whether the data was already
pre-processed externally, e.g. normalised and clustered. Only used if the
<code>data</code> argument is a <code>data.table</code> or <code>data.frame</code>.</p>
</td></tr>
<tr><td><code id="extract_ice_+3A_message_indent">message_indent</code></td>
<td>
<p>Number of indentation steps for messages shown during
computation and extraction of various data elements.</p>
</td></tr>
<tr><td><code id="extract_ice_+3A_verbose">verbose</code></td>
<td>
<p>Flag to indicate whether feedback should be provided on the
computation and extraction of various data elements.</p>
</td></tr>
<tr><td><code id="extract_ice_+3A_...">...</code></td>
<td>
<p>Unused arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data.table containing predicted and observed outcome data together
with a co-occurence count.
</p>

<hr>
<h2 id='extract_model_vimp'>Internal function to extract variable importance from models.</h2><span id='topic+extract_model_vimp'></span>

<h3>Description</h3>

<p>Aggregate variable importance from models in a
<code>familiarEnsemble</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>extract_model_vimp(
  object,
  data,
  aggregation_method = waiver(),
  rank_threshold = waiver(),
  message_indent = 0L,
  verbose = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="extract_model_vimp_+3A_object">object</code></td>
<td>
<p>A <code>familiarEnsemble</code> object, which is an ensemble of one or more
<code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="extract_model_vimp_+3A_data">data</code></td>
<td>
<p>A <code>dataObject</code> object, <code>data.table</code> or <code>data.frame</code> that
constitutes the data that are assessed.</p>
</td></tr>
<tr><td><code id="extract_model_vimp_+3A_aggregation_method">aggregation_method</code></td>
<td>
<p>Method for aggregating variable importances for the
purpose of evaluation. Variable importances are determined during feature
selection steps and after training the model. Both types are evaluated, but
feature selection variable importance is only evaluated at run-time.
</p>
<p>See the documentation for the <code>vimp_aggregation_method</code> argument in
<code>summon_familiar</code> for information concerning the different available
methods.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="extract_model_vimp_+3A_rank_threshold">rank_threshold</code></td>
<td>
<p>The threshold used to  define the subset of highly
important features during evaluation.
</p>
<p>See the documentation for the <code>vimp_aggregation_rank_threshold</code> argument in
<code>summon_familiar</code> for more information.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="extract_model_vimp_+3A_message_indent">message_indent</code></td>
<td>
<p>Number of indentation steps for messages shown during
computation and extraction of various data elements.</p>
</td></tr>
<tr><td><code id="extract_model_vimp_+3A_verbose">verbose</code></td>
<td>
<p>Flag to indicate whether feedback should be provided on the
computation and extraction of various data elements.</p>
</td></tr>
<tr><td><code id="extract_model_vimp_+3A_...">...</code></td>
<td>
<p>Unused arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing variable importance information.
</p>

<hr>
<h2 id='extract_performance'>Internal function to extract performance metrics.</h2><span id='topic+extract_performance'></span>

<h3>Description</h3>

<p>Computes and collects discriminative performance metrics from a
<code>familiarEnsemble</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>extract_performance(
  object,
  data,
  cl = NULL,
  metric = waiver(),
  ensemble_method = waiver(),
  evaluation_times = waiver(),
  detail_level = waiver(),
  estimation_type = waiver(),
  aggregate_results = waiver(),
  confidence_level = waiver(),
  bootstrap_ci_method = waiver(),
  is_pre_processed = FALSE,
  message_indent = 0L,
  verbose = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="extract_performance_+3A_object">object</code></td>
<td>
<p>A <code>familiarEnsemble</code> object, which is an ensemble of one or more
<code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="extract_performance_+3A_data">data</code></td>
<td>
<p>A <code>dataObject</code> object, <code>data.table</code> or <code>data.frame</code> that
constitutes the data that are assessed.</p>
</td></tr>
<tr><td><code id="extract_performance_+3A_cl">cl</code></td>
<td>
<p>Cluster created using the <code>parallel</code> package. This cluster is then
used to speed up computation through parallellisation.</p>
</td></tr>
<tr><td><code id="extract_performance_+3A_metric">metric</code></td>
<td>
<p>One or more metrics for assessing model performance. See the
vignette on performance metrics for the available metrics. If not provided
explicitly, this parameter is read from settings used at creation of the
underlying <code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="extract_performance_+3A_ensemble_method">ensemble_method</code></td>
<td>
<p>Method for ensembling predictions from models for the
same sample. Available methods are:
</p>

<ul>
<li> <p><code>median</code> (default): Use the median of the predicted values as the ensemble
value for a sample.
</p>
</li>
<li> <p><code>mean</code>: Use the mean of the predicted values as the ensemble value for a
sample.
</p>
</li></ul>
</td></tr>
<tr><td><code id="extract_performance_+3A_evaluation_times">evaluation_times</code></td>
<td>
<p>One or more time points that are used for in analysis of
survival problems when data has to be assessed at a set time, e.g.
calibration. If not provided explicitly, this parameter is read from
settings used at creation of the underlying <code>familiarModel</code> objects. Only
used for <code>survival</code> outcomes.</p>
</td></tr>
<tr><td><code id="extract_performance_+3A_detail_level">detail_level</code></td>
<td>
<p>(<em>optional</em>) Sets the level at which results are computed
and aggregated.
</p>

<ul>
<li> <p><code>ensemble</code>: Results are computed at the ensemble level, i.e. over all
models in the ensemble. This means that, for example, bias-corrected
estimates of model performance are assessed by creating (at least) 20
bootstraps and computing the model performance of the ensemble model for
each bootstrap.
</p>
</li>
<li> <p><code>hybrid</code> (default): Results are computed at the level of models in an
ensemble. This means that, for example, bias-corrected estimates of model
performance are directly computed using the models in the ensemble. If there
are at least 20 trained models in the ensemble, performance is computed for
each model, in contrast to <code>ensemble</code> where performance is computed for the
ensemble of models. If there are less than 20 trained models in the
ensemble, bootstraps are created so that at least 20 point estimates can be
made.
</p>
</li>
<li> <p><code>model</code>: Results are computed at the model level. This means that, for
example, bias-corrected estimates of model performance are assessed by
creating (at least) 20 bootstraps and computing the performance of the model
for each bootstrap.
</p>
</li></ul>

<p>Note that each level of detail has a different interpretation for bootstrap
confidence intervals. For <code>ensemble</code> and <code>model</code> these are the confidence
intervals for the ensemble and an individual model, respectively. That is,
the confidence interval describes the range where an estimate produced by a
respective ensemble or model trained on a repeat of the experiment may be
found with the probability of the confidence level. For <code>hybrid</code>, it
represents the range where any single model trained on a repeat of the
experiment may be found with the probability of the confidence level. By
definition, confidence intervals obtained using <code>hybrid</code> are at least as
wide as those for <code>ensemble</code>. <code>hybrid</code> offers the correct interpretation if
the goal of the analysis is to assess the result of a single, unspecified,
model.
</p>
<p><code>hybrid</code> is generally computationally less expensive then <code>ensemble</code>, which
in turn is somewhat less expensive than <code>model</code>.
</p>
<p>A non-default <code>detail_level</code> parameter can be specified for separate
evaluation steps by providing a parameter value in a named list with data
elements, e.g. <code>list("auc_data"="ensemble", "model_performance"="hybrid")</code>.
This parameter can be set for the following data elements: <code>auc_data</code>,
<code>decision_curve_analyis</code>, <code>model_performance</code>, <code>permutation_vimp</code>,
<code>ice_data</code>, <code>prediction_data</code> and <code>confusion_matrix</code>.</p>
</td></tr>
<tr><td><code id="extract_performance_+3A_estimation_type">estimation_type</code></td>
<td>
<p>(<em>optional</em>) Sets the type of estimation that should be
possible. This has the following options:
</p>

<ul>
<li> <p><code>point</code>: Point estimates.
</p>
</li>
<li> <p><code>bias_correction</code> or <code>bc</code>: Bias-corrected estimates. A bias-corrected
estimate is computed from (at least) 20 point estimates, and <code>familiar</code> may
bootstrap the data to create them.
</p>
</li>
<li> <p><code>bootstrap_confidence_interval</code> or <code>bci</code> (default): Bias-corrected
estimates with bootstrap confidence intervals (Efron and Hastie, 2016). The
number of point estimates required depends on the <code>confidence_level</code>
parameter, and <code>familiar</code> may bootstrap the data to create them.
</p>
</li></ul>

<p>As with <code>detail_level</code>, a non-default <code>estimation_type</code> parameter can be
specified for separate evaluation steps by providing a parameter value in a
named list with data elements, e.g. <code>list("auc_data"="bci", "model_performance"="point")</code>. This parameter can be set for the following
data elements: <code>auc_data</code>, <code>decision_curve_analyis</code>, <code>model_performance</code>,
<code>permutation_vimp</code>, <code>ice_data</code>, and <code>prediction_data</code>.</p>
</td></tr>
<tr><td><code id="extract_performance_+3A_aggregate_results">aggregate_results</code></td>
<td>
<p>(<em>optional</em>) Flag that signifies whether results
should be aggregated during evaluation. If <code>estimation_type</code> is
<code>bias_correction</code> or <code>bc</code>, aggregation leads to a single bias-corrected
estimate. If <code>estimation_type</code> is <code>bootstrap_confidence_interval</code> or <code>bci</code>,
aggregation leads to a single bias-corrected estimate with lower and upper
boundaries of the confidence interval. This has no effect if
<code>estimation_type</code> is <code>point</code>.
</p>
<p>The default value is equal to <code>TRUE</code> except when assessing metrics to assess
model performance, as the default violin plot requires underlying data.
</p>
<p>As with <code>detail_level</code> and <code>estimation_type</code>, a non-default
<code>aggregate_results</code> parameter can be specified for separate evaluation steps
by providing a parameter value in a named list with data elements, e.g.
<code>list("auc_data"=TRUE, , "model_performance"=FALSE)</code>. This parameter exists
for the same elements as <code>estimation_type</code>.</p>
</td></tr>
<tr><td><code id="extract_performance_+3A_confidence_level">confidence_level</code></td>
<td>
<p>(<em>optional</em>) Numeric value for the level at which
confidence intervals are determined. In the case bootstraps are used to
determine the confidence intervals bootstrap estimation, <code>familiar</code> uses the
rule of thumb <code class="reqn">n = 20 / ci.level</code> to determine the number of required
bootstraps.
</p>
<p>The default value is <code>0.95</code>.</p>
</td></tr>
<tr><td><code id="extract_performance_+3A_bootstrap_ci_method">bootstrap_ci_method</code></td>
<td>
<p>(<em>optional</em>) Method used to determine bootstrap
confidence intervals (Efron and Hastie, 2016). The following methods are
implemented:
</p>

<ul>
<li> <p><code>percentile</code> (default): Confidence intervals obtained using the percentile
method.
</p>
</li>
<li> <p><code>bc</code>: Bias-corrected confidence intervals.
</p>
</li></ul>

<p>Note that the standard method is not implemented because this method is
often not suitable due to non-normal distributions. The bias-corrected and
accelerated (BCa) method is not implemented yet.</p>
</td></tr>
<tr><td><code id="extract_performance_+3A_is_pre_processed">is_pre_processed</code></td>
<td>
<p>Flag that indicates whether the data was already
pre-processed externally, e.g. normalised and clustered. Only used if the
<code>data</code> argument is a <code>data.table</code> or <code>data.frame</code>.</p>
</td></tr>
<tr><td><code id="extract_performance_+3A_message_indent">message_indent</code></td>
<td>
<p>Number of indentation steps for messages shown during
computation and extraction of various data elements.</p>
</td></tr>
<tr><td><code id="extract_performance_+3A_verbose">verbose</code></td>
<td>
<p>Flag to indicate whether feedback should be provided on the
computation and extraction of various data elements.</p>
</td></tr>
<tr><td><code id="extract_performance_+3A_...">...</code></td>
<td>
<p>Unused arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This method computes credibility intervals for the ensemble model, at
the level of <code>confidence_level</code>. This is a general method. Metrics with
known, theoretically derived confidence intervals, nevertheless have a
credibility interval computed.
</p>


<h3>Value</h3>

<p>A list with data.tables for single and ensemble model assessments.
</p>

<hr>
<h2 id='extract_permutation_vimp'>Internal function to extract permutation variable importance.</h2><span id='topic+extract_permutation_vimp'></span>

<h3>Description</h3>

<p>Computes and collects permutation variable importance from a
<code>familiarEnsemble</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>extract_permutation_vimp(
  object,
  data,
  cl = NULL,
  ensemble_method = waiver(),
  feature_similarity,
  feature_cluster_method = waiver(),
  feature_linkage_method = waiver(),
  feature_cluster_cut_method = waiver(),
  feature_similarity_metric = waiver(),
  feature_similarity_threshold = waiver(),
  metric = waiver(),
  evaluation_times = waiver(),
  detail_level = waiver(),
  estimation_type = waiver(),
  aggregate_results = waiver(),
  confidence_level = waiver(),
  bootstrap_ci_method = waiver(),
  is_pre_processed = FALSE,
  message_indent = 0L,
  verbose = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="extract_permutation_vimp_+3A_object">object</code></td>
<td>
<p>A <code>familiarEnsemble</code> object, which is an ensemble of one or more
<code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="extract_permutation_vimp_+3A_data">data</code></td>
<td>
<p>A <code>dataObject</code> object, <code>data.table</code> or <code>data.frame</code> that
constitutes the data that are assessed.</p>
</td></tr>
<tr><td><code id="extract_permutation_vimp_+3A_cl">cl</code></td>
<td>
<p>Cluster created using the <code>parallel</code> package. This cluster is then
used to speed up computation through parallellisation.</p>
</td></tr>
<tr><td><code id="extract_permutation_vimp_+3A_ensemble_method">ensemble_method</code></td>
<td>
<p>Method for ensembling predictions from models for the
same sample. Available methods are:
</p>

<ul>
<li> <p><code>median</code> (default): Use the median of the predicted values as the ensemble
value for a sample.
</p>
</li>
<li> <p><code>mean</code>: Use the mean of the predicted values as the ensemble value for a
sample.
</p>
</li></ul>
</td></tr>
<tr><td><code id="extract_permutation_vimp_+3A_feature_cluster_method">feature_cluster_method</code></td>
<td>
<p>The method used to perform clustering. These are
the same methods as for the <code>cluster_method</code> configuration parameter:
<code>none</code>, <code>hclust</code>, <code>agnes</code>, <code>diana</code> and <code>pam</code>.
</p>
<p><code>none</code> cannot be used when extracting data regarding mutual correlation or
feature expressions.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="extract_permutation_vimp_+3A_feature_linkage_method">feature_linkage_method</code></td>
<td>
<p>The method used for agglomerative clustering in
<code>hclust</code> and <code>agnes</code>. These are the same methods as for the
<code>cluster_linkage_method</code> configuration parameter: <code>average</code>, <code>single</code>,
<code>complete</code>, <code>weighted</code>, and <code>ward</code>.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="extract_permutation_vimp_+3A_feature_cluster_cut_method">feature_cluster_cut_method</code></td>
<td>
<p>The method used to divide features into
separate clusters. The available methods are the same as for the
<code>cluster_cut_method</code> configuration parameter: <code>silhouette</code>, <code>fixed_cut</code> and
<code>dynamic_cut</code>.
</p>
<p><code>silhouette</code> is available for all cluster methods, but <code>fixed_cut</code> only
applies to methods that create hierarchical trees (<code>hclust</code>, <code>agnes</code> and
<code>diana</code>). <code>dynamic_cut</code> requires the <code>dynamicTreeCut</code> package and can only
be used with <code>agnes</code> and <code>hclust</code>.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="extract_permutation_vimp_+3A_feature_similarity_metric">feature_similarity_metric</code></td>
<td>
<p>Metric to determine pairwise similarity
between features. Similarity is computed in the same manner as for
clustering, and <code>feature_similarity_metric</code> therefore has the same options
as <code>cluster_similarity_metric</code>: <code>mcfadden_r2</code>, <code>cox_snell_r2</code>,
<code>nagelkerke_r2</code>, <code>spearman</code>, <code>kendall</code> and <code>pearson</code>.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="extract_permutation_vimp_+3A_feature_similarity_threshold">feature_similarity_threshold</code></td>
<td>
<p>The threshold level for pair-wise
similarity that is required to form feature clusters with the <code>fixed_cut</code>
method.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="extract_permutation_vimp_+3A_metric">metric</code></td>
<td>
<p>One or more metrics for assessing model performance. See the
vignette on performance metrics for the available metrics. If not provided
explicitly, this parameter is read from settings used at creation of the
underlying <code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="extract_permutation_vimp_+3A_evaluation_times">evaluation_times</code></td>
<td>
<p>One or more time points that are used for in analysis of
survival problems when data has to be assessed at a set time, e.g.
calibration. If not provided explicitly, this parameter is read from
settings used at creation of the underlying <code>familiarModel</code> objects. Only
used for <code>survival</code> outcomes.</p>
</td></tr>
<tr><td><code id="extract_permutation_vimp_+3A_detail_level">detail_level</code></td>
<td>
<p>(<em>optional</em>) Sets the level at which results are computed
and aggregated.
</p>

<ul>
<li> <p><code>ensemble</code>: Results are computed at the ensemble level, i.e. over all
models in the ensemble. This means that, for example, bias-corrected
estimates of model performance are assessed by creating (at least) 20
bootstraps and computing the model performance of the ensemble model for
each bootstrap.
</p>
</li>
<li> <p><code>hybrid</code> (default): Results are computed at the level of models in an
ensemble. This means that, for example, bias-corrected estimates of model
performance are directly computed using the models in the ensemble. If there
are at least 20 trained models in the ensemble, performance is computed for
each model, in contrast to <code>ensemble</code> where performance is computed for the
ensemble of models. If there are less than 20 trained models in the
ensemble, bootstraps are created so that at least 20 point estimates can be
made.
</p>
</li>
<li> <p><code>model</code>: Results are computed at the model level. This means that, for
example, bias-corrected estimates of model performance are assessed by
creating (at least) 20 bootstraps and computing the performance of the model
for each bootstrap.
</p>
</li></ul>

<p>Note that each level of detail has a different interpretation for bootstrap
confidence intervals. For <code>ensemble</code> and <code>model</code> these are the confidence
intervals for the ensemble and an individual model, respectively. That is,
the confidence interval describes the range where an estimate produced by a
respective ensemble or model trained on a repeat of the experiment may be
found with the probability of the confidence level. For <code>hybrid</code>, it
represents the range where any single model trained on a repeat of the
experiment may be found with the probability of the confidence level. By
definition, confidence intervals obtained using <code>hybrid</code> are at least as
wide as those for <code>ensemble</code>. <code>hybrid</code> offers the correct interpretation if
the goal of the analysis is to assess the result of a single, unspecified,
model.
</p>
<p><code>hybrid</code> is generally computationally less expensive then <code>ensemble</code>, which
in turn is somewhat less expensive than <code>model</code>.
</p>
<p>A non-default <code>detail_level</code> parameter can be specified for separate
evaluation steps by providing a parameter value in a named list with data
elements, e.g. <code>list("auc_data"="ensemble", "model_performance"="hybrid")</code>.
This parameter can be set for the following data elements: <code>auc_data</code>,
<code>decision_curve_analyis</code>, <code>model_performance</code>, <code>permutation_vimp</code>,
<code>ice_data</code>, <code>prediction_data</code> and <code>confusion_matrix</code>.</p>
</td></tr>
<tr><td><code id="extract_permutation_vimp_+3A_estimation_type">estimation_type</code></td>
<td>
<p>(<em>optional</em>) Sets the type of estimation that should be
possible. This has the following options:
</p>

<ul>
<li> <p><code>point</code>: Point estimates.
</p>
</li>
<li> <p><code>bias_correction</code> or <code>bc</code>: Bias-corrected estimates. A bias-corrected
estimate is computed from (at least) 20 point estimates, and <code>familiar</code> may
bootstrap the data to create them.
</p>
</li>
<li> <p><code>bootstrap_confidence_interval</code> or <code>bci</code> (default): Bias-corrected
estimates with bootstrap confidence intervals (Efron and Hastie, 2016). The
number of point estimates required depends on the <code>confidence_level</code>
parameter, and <code>familiar</code> may bootstrap the data to create them.
</p>
</li></ul>

<p>As with <code>detail_level</code>, a non-default <code>estimation_type</code> parameter can be
specified for separate evaluation steps by providing a parameter value in a
named list with data elements, e.g. <code>list("auc_data"="bci", "model_performance"="point")</code>. This parameter can be set for the following
data elements: <code>auc_data</code>, <code>decision_curve_analyis</code>, <code>model_performance</code>,
<code>permutation_vimp</code>, <code>ice_data</code>, and <code>prediction_data</code>.</p>
</td></tr>
<tr><td><code id="extract_permutation_vimp_+3A_aggregate_results">aggregate_results</code></td>
<td>
<p>(<em>optional</em>) Flag that signifies whether results
should be aggregated during evaluation. If <code>estimation_type</code> is
<code>bias_correction</code> or <code>bc</code>, aggregation leads to a single bias-corrected
estimate. If <code>estimation_type</code> is <code>bootstrap_confidence_interval</code> or <code>bci</code>,
aggregation leads to a single bias-corrected estimate with lower and upper
boundaries of the confidence interval. This has no effect if
<code>estimation_type</code> is <code>point</code>.
</p>
<p>The default value is equal to <code>TRUE</code> except when assessing metrics to assess
model performance, as the default violin plot requires underlying data.
</p>
<p>As with <code>detail_level</code> and <code>estimation_type</code>, a non-default
<code>aggregate_results</code> parameter can be specified for separate evaluation steps
by providing a parameter value in a named list with data elements, e.g.
<code>list("auc_data"=TRUE, , "model_performance"=FALSE)</code>. This parameter exists
for the same elements as <code>estimation_type</code>.</p>
</td></tr>
<tr><td><code id="extract_permutation_vimp_+3A_confidence_level">confidence_level</code></td>
<td>
<p>(<em>optional</em>) Numeric value for the level at which
confidence intervals are determined. In the case bootstraps are used to
determine the confidence intervals bootstrap estimation, <code>familiar</code> uses the
rule of thumb <code class="reqn">n = 20 / ci.level</code> to determine the number of required
bootstraps.
</p>
<p>The default value is <code>0.95</code>.</p>
</td></tr>
<tr><td><code id="extract_permutation_vimp_+3A_bootstrap_ci_method">bootstrap_ci_method</code></td>
<td>
<p>(<em>optional</em>) Method used to determine bootstrap
confidence intervals (Efron and Hastie, 2016). The following methods are
implemented:
</p>

<ul>
<li> <p><code>percentile</code> (default): Confidence intervals obtained using the percentile
method.
</p>
</li>
<li> <p><code>bc</code>: Bias-corrected confidence intervals.
</p>
</li></ul>

<p>Note that the standard method is not implemented because this method is
often not suitable due to non-normal distributions. The bias-corrected and
accelerated (BCa) method is not implemented yet.</p>
</td></tr>
<tr><td><code id="extract_permutation_vimp_+3A_is_pre_processed">is_pre_processed</code></td>
<td>
<p>Flag that indicates whether the data was already
pre-processed externally, e.g. normalised and clustered. Only used if the
<code>data</code> argument is a <code>data.table</code> or <code>data.frame</code>.</p>
</td></tr>
<tr><td><code id="extract_permutation_vimp_+3A_message_indent">message_indent</code></td>
<td>
<p>Number of indentation steps for messages shown during
computation and extraction of various data elements.</p>
</td></tr>
<tr><td><code id="extract_permutation_vimp_+3A_verbose">verbose</code></td>
<td>
<p>Flag to indicate whether feedback should be provided on the
computation and extraction of various data elements.</p>
</td></tr>
<tr><td><code id="extract_permutation_vimp_+3A_...">...</code></td>
<td>
<p>Unused arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function also computes credibility intervals for the ensemble
model, at the level of <code>confidence_level</code>.
</p>


<h3>Value</h3>

<p>A list with data.tables for single and ensemble model assessments.
</p>

<hr>
<h2 id='extract_predictions'>Internal function to extract predicted values from models.</h2><span id='topic+extract_predictions'></span>

<h3>Description</h3>

<p>Collects predicted values from models in a <code>familiarEnsemble</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>extract_predictions(
  object,
  data,
  cl = NULL,
  is_pre_processed = FALSE,
  ensemble_method = waiver(),
  evaluation_times = waiver(),
  detail_level = waiver(),
  estimation_type = waiver(),
  aggregate_results = waiver(),
  confidence_level = waiver(),
  message_indent = 0L,
  verbose = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="extract_predictions_+3A_object">object</code></td>
<td>
<p>A <code>familiarEnsemble</code> object, which is an ensemble of one or more
<code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="extract_predictions_+3A_data">data</code></td>
<td>
<p>A <code>dataObject</code> object, <code>data.table</code> or <code>data.frame</code> that
constitutes the data that are assessed.</p>
</td></tr>
<tr><td><code id="extract_predictions_+3A_cl">cl</code></td>
<td>
<p>Cluster created using the <code>parallel</code> package. This cluster is then
used to speed up computation through parallellisation.</p>
</td></tr>
<tr><td><code id="extract_predictions_+3A_is_pre_processed">is_pre_processed</code></td>
<td>
<p>Flag that indicates whether the data was already
pre-processed externally, e.g. normalised and clustered. Only used if the
<code>data</code> argument is a <code>data.table</code> or <code>data.frame</code>.</p>
</td></tr>
<tr><td><code id="extract_predictions_+3A_ensemble_method">ensemble_method</code></td>
<td>
<p>Method for ensembling predictions from models for the
same sample. Available methods are:
</p>

<ul>
<li> <p><code>median</code> (default): Use the median of the predicted values as the ensemble
value for a sample.
</p>
</li>
<li> <p><code>mean</code>: Use the mean of the predicted values as the ensemble value for a
sample.
</p>
</li></ul>
</td></tr>
<tr><td><code id="extract_predictions_+3A_evaluation_times">evaluation_times</code></td>
<td>
<p>One or more time points that are used for in analysis of
survival problems when data has to be assessed at a set time, e.g.
calibration. If not provided explicitly, this parameter is read from
settings used at creation of the underlying <code>familiarModel</code> objects. Only
used for <code>survival</code> outcomes.</p>
</td></tr>
<tr><td><code id="extract_predictions_+3A_detail_level">detail_level</code></td>
<td>
<p>(<em>optional</em>) Sets the level at which results are computed
and aggregated.
</p>

<ul>
<li> <p><code>ensemble</code>: Results are computed at the ensemble level, i.e. over all
models in the ensemble. This means that, for example, bias-corrected
estimates of model performance are assessed by creating (at least) 20
bootstraps and computing the model performance of the ensemble model for
each bootstrap.
</p>
</li>
<li> <p><code>hybrid</code> (default): Results are computed at the level of models in an
ensemble. This means that, for example, bias-corrected estimates of model
performance are directly computed using the models in the ensemble. If there
are at least 20 trained models in the ensemble, performance is computed for
each model, in contrast to <code>ensemble</code> where performance is computed for the
ensemble of models. If there are less than 20 trained models in the
ensemble, bootstraps are created so that at least 20 point estimates can be
made.
</p>
</li>
<li> <p><code>model</code>: Results are computed at the model level. This means that, for
example, bias-corrected estimates of model performance are assessed by
creating (at least) 20 bootstraps and computing the performance of the model
for each bootstrap.
</p>
</li></ul>

<p>Note that each level of detail has a different interpretation for bootstrap
confidence intervals. For <code>ensemble</code> and <code>model</code> these are the confidence
intervals for the ensemble and an individual model, respectively. That is,
the confidence interval describes the range where an estimate produced by a
respective ensemble or model trained on a repeat of the experiment may be
found with the probability of the confidence level. For <code>hybrid</code>, it
represents the range where any single model trained on a repeat of the
experiment may be found with the probability of the confidence level. By
definition, confidence intervals obtained using <code>hybrid</code> are at least as
wide as those for <code>ensemble</code>. <code>hybrid</code> offers the correct interpretation if
the goal of the analysis is to assess the result of a single, unspecified,
model.
</p>
<p><code>hybrid</code> is generally computationally less expensive then <code>ensemble</code>, which
in turn is somewhat less expensive than <code>model</code>.
</p>
<p>A non-default <code>detail_level</code> parameter can be specified for separate
evaluation steps by providing a parameter value in a named list with data
elements, e.g. <code>list("auc_data"="ensemble", "model_performance"="hybrid")</code>.
This parameter can be set for the following data elements: <code>auc_data</code>,
<code>decision_curve_analyis</code>, <code>model_performance</code>, <code>permutation_vimp</code>,
<code>ice_data</code>, <code>prediction_data</code> and <code>confusion_matrix</code>.</p>
</td></tr>
<tr><td><code id="extract_predictions_+3A_estimation_type">estimation_type</code></td>
<td>
<p>(<em>optional</em>) Sets the type of estimation that should be
possible. This has the following options:
</p>

<ul>
<li> <p><code>point</code>: Point estimates.
</p>
</li>
<li> <p><code>bias_correction</code> or <code>bc</code>: Bias-corrected estimates. A bias-corrected
estimate is computed from (at least) 20 point estimates, and <code>familiar</code> may
bootstrap the data to create them.
</p>
</li>
<li> <p><code>bootstrap_confidence_interval</code> or <code>bci</code> (default): Bias-corrected
estimates with bootstrap confidence intervals (Efron and Hastie, 2016). The
number of point estimates required depends on the <code>confidence_level</code>
parameter, and <code>familiar</code> may bootstrap the data to create them.
</p>
</li></ul>

<p>As with <code>detail_level</code>, a non-default <code>estimation_type</code> parameter can be
specified for separate evaluation steps by providing a parameter value in a
named list with data elements, e.g. <code>list("auc_data"="bci", "model_performance"="point")</code>. This parameter can be set for the following
data elements: <code>auc_data</code>, <code>decision_curve_analyis</code>, <code>model_performance</code>,
<code>permutation_vimp</code>, <code>ice_data</code>, and <code>prediction_data</code>.</p>
</td></tr>
<tr><td><code id="extract_predictions_+3A_aggregate_results">aggregate_results</code></td>
<td>
<p>(<em>optional</em>) Flag that signifies whether results
should be aggregated during evaluation. If <code>estimation_type</code> is
<code>bias_correction</code> or <code>bc</code>, aggregation leads to a single bias-corrected
estimate. If <code>estimation_type</code> is <code>bootstrap_confidence_interval</code> or <code>bci</code>,
aggregation leads to a single bias-corrected estimate with lower and upper
boundaries of the confidence interval. This has no effect if
<code>estimation_type</code> is <code>point</code>.
</p>
<p>The default value is equal to <code>TRUE</code> except when assessing metrics to assess
model performance, as the default violin plot requires underlying data.
</p>
<p>As with <code>detail_level</code> and <code>estimation_type</code>, a non-default
<code>aggregate_results</code> parameter can be specified for separate evaluation steps
by providing a parameter value in a named list with data elements, e.g.
<code>list("auc_data"=TRUE, , "model_performance"=FALSE)</code>. This parameter exists
for the same elements as <code>estimation_type</code>.</p>
</td></tr>
<tr><td><code id="extract_predictions_+3A_confidence_level">confidence_level</code></td>
<td>
<p>(<em>optional</em>) Numeric value for the level at which
confidence intervals are determined. In the case bootstraps are used to
determine the confidence intervals bootstrap estimation, <code>familiar</code> uses the
rule of thumb <code class="reqn">n = 20 / ci.level</code> to determine the number of required
bootstraps.
</p>
<p>The default value is <code>0.95</code>.</p>
</td></tr>
<tr><td><code id="extract_predictions_+3A_message_indent">message_indent</code></td>
<td>
<p>Number of indentation steps for messages shown during
computation and extraction of various data elements.</p>
</td></tr>
<tr><td><code id="extract_predictions_+3A_verbose">verbose</code></td>
<td>
<p>Flag to indicate whether feedback should be provided on the
computation and extraction of various data elements.</p>
</td></tr>
<tr><td><code id="extract_predictions_+3A_...">...</code></td>
<td>
<p>Unused arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with single-model and ensemble predictions.
</p>

<hr>
<h2 id='extract_risk_stratification_data'>Internal function to extract stratification data.</h2><span id='topic+extract_risk_stratification_data'></span>

<h3>Description</h3>

<p>Computes and extracts stratification data from a
<code>familiarEnsemble</code> object. This includes the data required to draw
Kaplan-Meier plots, as well as logrank and hazard-ratio tests between the
respective risk groups.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>extract_risk_stratification_data(
  object,
  data,
  cl = NULL,
  is_pre_processed = FALSE,
  ensemble_method = waiver(),
  detail_level = waiver(),
  confidence_level = waiver(),
  message_indent = 0L,
  verbose = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="extract_risk_stratification_data_+3A_object">object</code></td>
<td>
<p>A <code>familiarEnsemble</code> object, which is an ensemble of one or more
<code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="extract_risk_stratification_data_+3A_data">data</code></td>
<td>
<p>A <code>dataObject</code> object, <code>data.table</code> or <code>data.frame</code> that
constitutes the data that are assessed.</p>
</td></tr>
<tr><td><code id="extract_risk_stratification_data_+3A_cl">cl</code></td>
<td>
<p>Cluster created using the <code>parallel</code> package. This cluster is then
used to speed up computation through parallellisation.</p>
</td></tr>
<tr><td><code id="extract_risk_stratification_data_+3A_is_pre_processed">is_pre_processed</code></td>
<td>
<p>Flag that indicates whether the data was already
pre-processed externally, e.g. normalised and clustered. Only used if the
<code>data</code> argument is a <code>data.table</code> or <code>data.frame</code>.</p>
</td></tr>
<tr><td><code id="extract_risk_stratification_data_+3A_ensemble_method">ensemble_method</code></td>
<td>
<p>Method for ensembling predictions from models for the
same sample. Available methods are:
</p>

<ul>
<li> <p><code>median</code> (default): Use the median of the predicted values as the ensemble
value for a sample.
</p>
</li>
<li> <p><code>mean</code>: Use the mean of the predicted values as the ensemble value for a
sample.
</p>
</li></ul>
</td></tr>
<tr><td><code id="extract_risk_stratification_data_+3A_detail_level">detail_level</code></td>
<td>
<p>(<em>optional</em>) Sets the level at which results are computed
and aggregated.
</p>

<ul>
<li> <p><code>ensemble</code>: Results are computed at the ensemble level, i.e. over all
models in the ensemble. This means that, for example, bias-corrected
estimates of model performance are assessed by creating (at least) 20
bootstraps and computing the model performance of the ensemble model for
each bootstrap.
</p>
</li>
<li> <p><code>hybrid</code> (default): Results are computed at the level of models in an
ensemble. This means that, for example, bias-corrected estimates of model
performance are directly computed using the models in the ensemble. If there
are at least 20 trained models in the ensemble, performance is computed for
each model, in contrast to <code>ensemble</code> where performance is computed for the
ensemble of models. If there are less than 20 trained models in the
ensemble, bootstraps are created so that at least 20 point estimates can be
made.
</p>
</li>
<li> <p><code>model</code>: Results are computed at the model level. This means that, for
example, bias-corrected estimates of model performance are assessed by
creating (at least) 20 bootstraps and computing the performance of the model
for each bootstrap.
</p>
</li></ul>

<p>Note that each level of detail has a different interpretation for bootstrap
confidence intervals. For <code>ensemble</code> and <code>model</code> these are the confidence
intervals for the ensemble and an individual model, respectively. That is,
the confidence interval describes the range where an estimate produced by a
respective ensemble or model trained on a repeat of the experiment may be
found with the probability of the confidence level. For <code>hybrid</code>, it
represents the range where any single model trained on a repeat of the
experiment may be found with the probability of the confidence level. By
definition, confidence intervals obtained using <code>hybrid</code> are at least as
wide as those for <code>ensemble</code>. <code>hybrid</code> offers the correct interpretation if
the goal of the analysis is to assess the result of a single, unspecified,
model.
</p>
<p><code>hybrid</code> is generally computationally less expensive then <code>ensemble</code>, which
in turn is somewhat less expensive than <code>model</code>.
</p>
<p>A non-default <code>detail_level</code> parameter can be specified for separate
evaluation steps by providing a parameter value in a named list with data
elements, e.g. <code>list("auc_data"="ensemble", "model_performance"="hybrid")</code>.
This parameter can be set for the following data elements: <code>auc_data</code>,
<code>decision_curve_analyis</code>, <code>model_performance</code>, <code>permutation_vimp</code>,
<code>ice_data</code>, <code>prediction_data</code> and <code>confusion_matrix</code>.</p>
</td></tr>
<tr><td><code id="extract_risk_stratification_data_+3A_confidence_level">confidence_level</code></td>
<td>
<p>(<em>optional</em>) Numeric value for the level at which
confidence intervals are determined. In the case bootstraps are used to
determine the confidence intervals bootstrap estimation, <code>familiar</code> uses the
rule of thumb <code class="reqn">n = 20 / ci.level</code> to determine the number of required
bootstraps.
</p>
<p>The default value is <code>0.95</code>.</p>
</td></tr>
<tr><td><code id="extract_risk_stratification_data_+3A_message_indent">message_indent</code></td>
<td>
<p>Number of indentation steps for messages shown during
computation and extraction of various data elements.</p>
</td></tr>
<tr><td><code id="extract_risk_stratification_data_+3A_verbose">verbose</code></td>
<td>
<p>Flag to indicate whether feedback should be provided on the
computation and extraction of various data elements.</p>
</td></tr>
<tr><td><code id="extract_risk_stratification_data_+3A_...">...</code></td>
<td>
<p>Unused arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with data.tables containing information concerning risk group
stratification.
</p>

<hr>
<h2 id='extract_risk_stratification_info'>Internal function to extract risk stratification info from data.</h2><span id='topic+extract_risk_stratification_info'></span>

<h3>Description</h3>

<p>Collects risk stratification information.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>extract_risk_stratification_info(
  object,
  detail_level = waiver(),
  message_indent = 0L,
  verbose = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="extract_risk_stratification_info_+3A_object">object</code></td>
<td>
<p>A <code>familiarEnsemble</code> object, which is an ensemble of one or more
<code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="extract_risk_stratification_info_+3A_detail_level">detail_level</code></td>
<td>
<p>(<em>optional</em>) Sets the level at which results are computed
and aggregated.
</p>

<ul>
<li> <p><code>ensemble</code>: Results are computed at the ensemble level, i.e. over all
models in the ensemble. This means that, for example, bias-corrected
estimates of model performance are assessed by creating (at least) 20
bootstraps and computing the model performance of the ensemble model for
each bootstrap.
</p>
</li>
<li> <p><code>hybrid</code> (default): Results are computed at the level of models in an
ensemble. This means that, for example, bias-corrected estimates of model
performance are directly computed using the models in the ensemble. If there
are at least 20 trained models in the ensemble, performance is computed for
each model, in contrast to <code>ensemble</code> where performance is computed for the
ensemble of models. If there are less than 20 trained models in the
ensemble, bootstraps are created so that at least 20 point estimates can be
made.
</p>
</li>
<li> <p><code>model</code>: Results are computed at the model level. This means that, for
example, bias-corrected estimates of model performance are assessed by
creating (at least) 20 bootstraps and computing the performance of the model
for each bootstrap.
</p>
</li></ul>

<p>Note that each level of detail has a different interpretation for bootstrap
confidence intervals. For <code>ensemble</code> and <code>model</code> these are the confidence
intervals for the ensemble and an individual model, respectively. That is,
the confidence interval describes the range where an estimate produced by a
respective ensemble or model trained on a repeat of the experiment may be
found with the probability of the confidence level. For <code>hybrid</code>, it
represents the range where any single model trained on a repeat of the
experiment may be found with the probability of the confidence level. By
definition, confidence intervals obtained using <code>hybrid</code> are at least as
wide as those for <code>ensemble</code>. <code>hybrid</code> offers the correct interpretation if
the goal of the analysis is to assess the result of a single, unspecified,
model.
</p>
<p><code>hybrid</code> is generally computationally less expensive then <code>ensemble</code>, which
in turn is somewhat less expensive than <code>model</code>.
</p>
<p>A non-default <code>detail_level</code> parameter can be specified for separate
evaluation steps by providing a parameter value in a named list with data
elements, e.g. <code>list("auc_data"="ensemble", "model_performance"="hybrid")</code>.
This parameter can be set for the following data elements: <code>auc_data</code>,
<code>decision_curve_analyis</code>, <code>model_performance</code>, <code>permutation_vimp</code>,
<code>ice_data</code>, <code>prediction_data</code> and <code>confusion_matrix</code>.</p>
</td></tr>
<tr><td><code id="extract_risk_stratification_info_+3A_message_indent">message_indent</code></td>
<td>
<p>Number of indentation steps for messages shown during
computation and extraction of various data elements.</p>
</td></tr>
<tr><td><code id="extract_risk_stratification_info_+3A_verbose">verbose</code></td>
<td>
<p>Flag to indicate whether feedback should be provided on the
computation and extraction of various data elements.</p>
</td></tr>
<tr><td><code id="extract_risk_stratification_info_+3A_...">...</code></td>
<td>
<p>Unused arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of familiarDataElements with risk stratification information.
</p>

<hr>
<h2 id='extract_sample_similarity'>Internal function to extract the sample distance table.</h2><span id='topic+extract_sample_similarity'></span>

<h3>Description</h3>

<p>Computes and extracts the sample distance table for samples
analysed using a <code>familiarEnsemble</code> object to form a <code>familiarData</code> object. This table can be used to cluster
samples, and is exported directly by <code>extract_feature_expression</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>extract_sample_similarity(
  object,
  data,
  cl = NULL,
  is_pre_processed = FALSE,
  sample_limit = waiver(),
  sample_cluster_method = waiver(),
  sample_linkage_method = waiver(),
  sample_similarity_metric = waiver(),
  verbose = FALSE,
  message_indent = 0L,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="extract_sample_similarity_+3A_object">object</code></td>
<td>
<p>A <code>familiarEnsemble</code> object, which is an ensemble of one or more
<code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="extract_sample_similarity_+3A_data">data</code></td>
<td>
<p>A <code>dataObject</code> object, <code>data.table</code> or <code>data.frame</code> that
constitutes the data that are assessed.</p>
</td></tr>
<tr><td><code id="extract_sample_similarity_+3A_cl">cl</code></td>
<td>
<p>Cluster created using the <code>parallel</code> package. This cluster is then
used to speed up computation through parallellisation.</p>
</td></tr>
<tr><td><code id="extract_sample_similarity_+3A_is_pre_processed">is_pre_processed</code></td>
<td>
<p>Flag that indicates whether the data was already
pre-processed externally, e.g. normalised and clustered. Only used if the
<code>data</code> argument is a <code>data.table</code> or <code>data.frame</code>.</p>
</td></tr>
<tr><td><code id="extract_sample_similarity_+3A_sample_limit">sample_limit</code></td>
<td>
<p>(<em>optional</em>) Set the upper limit of the number of samples
that are used during evaluation steps. Cannot be less than 20.
</p>
<p>This setting can be specified per data element by providing a parameter
value in a named list with data elements, e.g.
<code>list("sample_similarity"=100, "permutation_vimp"=1000)</code>.
</p>
<p>This parameter can be set for the following data elements:
<code>sample_similarity</code> and <code>ice_data</code>.</p>
</td></tr>
<tr><td><code id="extract_sample_similarity_+3A_sample_cluster_method">sample_cluster_method</code></td>
<td>
<p>The method used to perform clustering based on
distance between samples. These are the same methods as for the
<code>cluster_method</code> configuration parameter: <code>hclust</code>, <code>agnes</code>, <code>diana</code> and
<code>pam</code>.
</p>
<p><code>none</code> cannot be used when extracting data for feature expressions.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="extract_sample_similarity_+3A_sample_linkage_method">sample_linkage_method</code></td>
<td>
<p>The method used for agglomerative clustering in
<code>hclust</code> and <code>agnes</code>. These are the same methods as for the
<code>cluster_linkage_method</code> configuration parameter: <code>average</code>, <code>single</code>,
<code>complete</code>, <code>weighted</code>, and <code>ward</code>.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="extract_sample_similarity_+3A_sample_similarity_metric">sample_similarity_metric</code></td>
<td>
<p>Metric to determine pairwise similarity
between samples. Similarity is computed in the same manner as for
clustering, but <code>sample_similarity_metric</code> has different options that are
better suited to computing distance between samples instead of between
features: <code>gower</code>, <code>euclidean</code>.
</p>
<p>The underlying feature data is scaled to the <code class="reqn">[0, 1]</code> range (for
numerical features) using the feature values across the samples. The
normalisation parameters required can optionally be computed from feature
data with the outer 5% (on both sides) of feature values trimmed or
winsorised. To do so append <code style="white-space: pre;">&#8288;_trim&#8288;</code> (trimming) or <code style="white-space: pre;">&#8288;_winsor&#8288;</code> (winsorising) to
the metric name. This reduces the effect of outliers somewhat.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="extract_sample_similarity_+3A_verbose">verbose</code></td>
<td>
<p>Flag to indicate whether feedback should be provided on the
computation and extraction of various data elements.</p>
</td></tr>
<tr><td><code id="extract_sample_similarity_+3A_message_indent">message_indent</code></td>
<td>
<p>Number of indentation steps for messages shown during
computation and extraction of various data elements.</p>
</td></tr>
<tr><td><code id="extract_sample_similarity_+3A_...">...</code></td>
<td>
<p>Unused arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data.table containing pairwise distance between samples. This data
is only the upper triangular of the complete matrix (i.e. the sparse
unitriangular representation). Diagonals will always be 0.0 and the lower
triangular is mirrored.
</p>

<hr>
<h2 id='extract_univariate_analysis'>Internal function to extract data from a univariate analysis.</h2><span id='topic+extract_univariate_analysis'></span>

<h3>Description</h3>

<p>Computes and extracts univariate analysis for the features used
in a <code>familiarEnsemble</code> object. This assessment includes the computation of
p and q-values, as well as robustness (in case of repeated measurements).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>extract_univariate_analysis(
  object,
  data,
  cl = NULL,
  icc_type = waiver(),
  feature_similarity = NULL,
  feature_cluster_method = waiver(),
  feature_cluster_cut_method = waiver(),
  feature_linkage_method = waiver(),
  feature_similarity_threshold = waiver(),
  feature_similarity_metric = waiver(),
  message_indent = 0L,
  verbose = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="extract_univariate_analysis_+3A_object">object</code></td>
<td>
<p>A <code>familiarEnsemble</code> object, which is an ensemble of one or more
<code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="extract_univariate_analysis_+3A_data">data</code></td>
<td>
<p>A <code>dataObject</code> object, <code>data.table</code> or <code>data.frame</code> that
constitutes the data that are assessed.</p>
</td></tr>
<tr><td><code id="extract_univariate_analysis_+3A_cl">cl</code></td>
<td>
<p>Cluster created using the <code>parallel</code> package. This cluster is then
used to speed up computation through parallellisation.</p>
</td></tr>
<tr><td><code id="extract_univariate_analysis_+3A_icc_type">icc_type</code></td>
<td>
<p>String indicating the type of intraclass correlation
coefficient (<code>1</code>, <code>2</code> or <code>3</code>) that should be used to compute robustness for
features in repeated measurements during the evaluation of univariate
importance. These types correspond to the types in Shrout and Fleiss (1979).
If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="extract_univariate_analysis_+3A_feature_cluster_method">feature_cluster_method</code></td>
<td>
<p>The method used to perform clustering. These are
the same methods as for the <code>cluster_method</code> configuration parameter:
<code>none</code>, <code>hclust</code>, <code>agnes</code>, <code>diana</code> and <code>pam</code>.
</p>
<p><code>none</code> cannot be used when extracting data regarding mutual correlation or
feature expressions.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="extract_univariate_analysis_+3A_feature_cluster_cut_method">feature_cluster_cut_method</code></td>
<td>
<p>The method used to divide features into
separate clusters. The available methods are the same as for the
<code>cluster_cut_method</code> configuration parameter: <code>silhouette</code>, <code>fixed_cut</code> and
<code>dynamic_cut</code>.
</p>
<p><code>silhouette</code> is available for all cluster methods, but <code>fixed_cut</code> only
applies to methods that create hierarchical trees (<code>hclust</code>, <code>agnes</code> and
<code>diana</code>). <code>dynamic_cut</code> requires the <code>dynamicTreeCut</code> package and can only
be used with <code>agnes</code> and <code>hclust</code>.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="extract_univariate_analysis_+3A_feature_linkage_method">feature_linkage_method</code></td>
<td>
<p>The method used for agglomerative clustering in
<code>hclust</code> and <code>agnes</code>. These are the same methods as for the
<code>cluster_linkage_method</code> configuration parameter: <code>average</code>, <code>single</code>,
<code>complete</code>, <code>weighted</code>, and <code>ward</code>.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="extract_univariate_analysis_+3A_feature_similarity_threshold">feature_similarity_threshold</code></td>
<td>
<p>The threshold level for pair-wise
similarity that is required to form feature clusters with the <code>fixed_cut</code>
method.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="extract_univariate_analysis_+3A_feature_similarity_metric">feature_similarity_metric</code></td>
<td>
<p>Metric to determine pairwise similarity
between features. Similarity is computed in the same manner as for
clustering, and <code>feature_similarity_metric</code> therefore has the same options
as <code>cluster_similarity_metric</code>: <code>mcfadden_r2</code>, <code>cox_snell_r2</code>,
<code>nagelkerke_r2</code>, <code>spearman</code>, <code>kendall</code> and <code>pearson</code>.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="extract_univariate_analysis_+3A_message_indent">message_indent</code></td>
<td>
<p>Number of indentation steps for messages shown during
computation and extraction of various data elements.</p>
</td></tr>
<tr><td><code id="extract_univariate_analysis_+3A_verbose">verbose</code></td>
<td>
<p>Flag to indicate whether feedback should be provided on the
computation and extraction of various data elements.</p>
</td></tr>
<tr><td><code id="extract_univariate_analysis_+3A_...">...</code></td>
<td>
<p>Unused arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with a data.table containing information concerning the
univariate analysis of important features.
</p>

<hr>
<h2 id='familiar'>familiar: Fully Automated Machine Learning with Interpretable Analysis of Results</h2><span id='topic+familiar'></span><span id='topic+familiar-package'></span>

<h3>Description</h3>

<p>End-to-end, automated machine learning package for creating
trustworthy and interpretable machine learning models. Familiar supports
modelling of regression, categorical and time-to-event (survival) outcomes.
Models created using familiar are self-containing, and their use does not
require additional information such as baseline survival, feature
clustering, or feature transformation and normalisation parameters. In
addition, an novelty or out-of-distribution detector is trained
simultaneously and contained with every model. Model performance,
calibration, risk group stratification, (permutation) variable importance,
individual conditional expectation, partial dependence, and more, are
assessed automatically as part of the evaluation process and exported in
tabular format and plotted, and may also be computed manually using export
and plot functions. Where possible, metrics and values obtained during the
evaluation process come with confidence intervals.
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Alex Zwanenburg <a href="mailto:alexander.zwanenburg@nct-dresden.de">alexander.zwanenburg@nct-dresden.de</a> (<a href="https://orcid.org/0000-0002-0342-9545">ORCID</a>)
</p>
<p>Authors:
</p>

<ul>
<li><p> Steffen Löck
</p>
</li></ul>

<p>Other contributors:
</p>

<ul>
<li><p> Stefan Leger [contributor]
</p>
</li>
<li><p> Iram Shahzadi [contributor]
</p>
</li>
<li><p> Asier Rabasco Meneghetti [contributor]
</p>
</li>
<li><p> Sebastian Starke [contributor]
</p>
</li>
<li><p> Technische Universität Dresden [copyright holder]
</p>
</li>
<li><p> German Cancer Research Center (DKFZ) [copyright holder]
</p>
</li></ul>



<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://github.com/alexzwanenburg/familiar">https://github.com/alexzwanenburg/familiar</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/alexzwanenburg/familiar/issues">https://github.com/alexzwanenburg/familiar/issues</a>
</p>
</li></ul>


<hr>
<h2 id='familiarCollection-class'>Collection of familiar data.</h2><span id='topic+familiarCollection-class'></span>

<h3>Description</h3>

<p>A familiarCollection object aggregates data from one or more familiarData
objects.
</p>


<h3>Slots</h3>


<dl>
<dt><code>name</code></dt><dd><p>Name of the collection.</p>
</dd>
<dt><code>data_sets</code></dt><dd><p>Name of the individual underlying datasets.</p>
</dd>
<dt><code>outcome_type</code></dt><dd><p>Outcome type for which the collection was created.</p>
</dd>
<dt><code>outcome_info</code></dt><dd><p>Outcome information object, which contains information
concerning the outcome, such as class levels.</p>
</dd>
<dt><code>fs_vimp</code></dt><dd><p>Variable importance data collected by feature selection
methods.</p>
</dd>
<dt><code>model_vimp</code></dt><dd><p>Variable importance data collected from model-specific
algorithms implemented by models created by familiar.</p>
</dd>
<dt><code>permutation_vimp</code></dt><dd><p>Data collected for permutation variable importance.</p>
</dd>
<dt><code>hyperparameters</code></dt><dd><p>Hyperparameters collected from created models.</p>
</dd>
<dt><code>hyperparameter_data</code></dt><dd><p>Additional data concerning hyperparameters. This is
currently not used yet.</p>
</dd>
<dt><code>required_features</code></dt><dd><p>The set of features required for complete
reproduction, i.e. with imputation.</p>
</dd>
<dt><code>model_features</code></dt><dd><p>The set of features that are required for using the
model, but without imputation.</p>
</dd>
<dt><code>learner</code></dt><dd><p>Learning algorithm(s) used for data in the collection.</p>
</dd>
<dt><code>fs_method</code></dt><dd><p>Feature selection method(s) used for data in the collection.</p>
</dd>
<dt><code>prediction_data</code></dt><dd><p>Model predictions for the data in the collection.</p>
</dd>
<dt><code>confusion_matrix</code></dt><dd><p>Confusion matrix information for the data in the
collection.</p>
</dd>
<dt><code>decision_curve_data</code></dt><dd><p>Decision curve analysis data for the data in the
collection.</p>
</dd>
<dt><code>calibration_info</code></dt><dd><p>Calibration information, e.g. baseline survival in the
development cohort.</p>
</dd>
<dt><code>calibration_data</code></dt><dd><p>Model calibration data collected from data in the
collection.</p>
</dd>
<dt><code>model_performance</code></dt><dd><p>Collection of model performance data for data in the
collection.</p>
</dd>
<dt><code>km_info</code></dt><dd><p>Information concerning risk-stratification cut-off values for
data in the collection.</p>
</dd>
<dt><code>km_data</code></dt><dd><p>Kaplan-Meier survival data for data in the collection.</p>
</dd>
<dt><code>auc_data</code></dt><dd><p>AUC-ROC and AUC-PR data for data in the collection.</p>
</dd>
<dt><code>ice_data</code></dt><dd><p>Individual conditional expectation data for data in the
collection. Partial dependence data are computed on the fly from these
data.</p>
</dd>
<dt><code>univariate_analysis</code></dt><dd><p>Univariate analysis results of data in the
collection.</p>
</dd>
<dt><code>feature_expressions</code></dt><dd><p>Feature expression values for data in the
collection.</p>
</dd>
<dt><code>feature_similarity</code></dt><dd><p>Feature similarity information for data in the
collection.</p>
</dd>
<dt><code>sample_similarity</code></dt><dd><p>Sample similarity information for data in the
collection.</p>
</dd>
<dt><code>data_set_labels</code></dt><dd><p>Labels for the different datasets in the collection.
See <code>get_data_set_names</code> and <code>set_data_set_names</code>.</p>
</dd>
<dt><code>learner_labels</code></dt><dd><p>Labels for the different learning algorithms used to
create the collection. See <code>get_learner_names</code> and <code>set_learner_names</code>.</p>
</dd>
<dt><code>fs_method_labels</code></dt><dd><p>Labels for the different feature selection methods
used to create the collection. See <code>get_fs_method_names</code> and
<code>set_fs_method_names</code>.</p>
</dd>
<dt><code>feature_labels</code></dt><dd><p>Labels for the features in this collection. See
<code>get_feature_names</code> and <code>set_feature_names</code>.</p>
</dd>
<dt><code>km_group_labels</code></dt><dd><p>Labels for the risk strata in this collection. See
<code>get_risk_group_names</code> and <code>set_risk_group_names</code>.</p>
</dd>
<dt><code>class_labels</code></dt><dd><p>Labels of the response variable. See <code>get_class_names</code> and
<code>set_class_names</code>.</p>
</dd>
<dt><code>project_id</code></dt><dd><p>Identifier of the project that generated this collection.</p>
</dd>
<dt><code>familiar_version</code></dt><dd><p>Version of the familiar package.
</p>
<p>familiarCollection objects collect data from one or more familiarData
objects. This objects are important, as all plotting and export functions use
it. The fact that one can supply familiarModel, familiarEnsemble and
familiarData objects as arguments for these methods, is because familiar
internally converts these into familiarCollection objects prior to executing
the method.</p>
</dd>
</dl>

<hr>
<h2 id='familiarData-class'>Dataset obtained after evaluating models on a dataset.</h2><span id='topic+familiarData-class'></span>

<h3>Description</h3>

<p>A familiarData object is created by evaluating familiarEnsemble or
familiarModel objects on a dataset. Multiple familiarData objects are
aggregated in a familiarCollection object.
</p>


<h3>Slots</h3>


<dl>
<dt><code>name</code></dt><dd><p>Name of the dataset, e.g. training or internal validation.</p>
</dd>
<dt><code>outcome_type</code></dt><dd><p>Outcome type of the data used to create the object.</p>
</dd>
<dt><code>outcome_info</code></dt><dd><p>Outcome information object, which contains additional
information concerning the outcome, such as class levels.</p>
</dd>
<dt><code>fs_vimp</code></dt><dd><p>Variable importance data collected from feature selection
methods.</p>
</dd>
<dt><code>model_vimp</code></dt><dd><p>Variable importance data collected from model-specific
algorithms implemented by models created by familiar.</p>
</dd>
<dt><code>permutation_vimp</code></dt><dd><p>Data collected for permutation variable importance.</p>
</dd>
<dt><code>hyperparameters</code></dt><dd><p>Hyperparameters collected from created models.</p>
</dd>
<dt><code>hyperparameter_data</code></dt><dd><p>Additional data concerning hyperparameters. This is
currently not used yet.</p>
</dd>
<dt><code>required_features</code></dt><dd><p>The set of features required for complete
reproduction, i.e. with imputation.</p>
</dd>
<dt><code>model_features</code></dt><dd><p>The set of features that are required for using the
model or ensemble of models, but without imputation.</p>
</dd>
<dt><code>learner</code></dt><dd><p>Learning algorithm used to create the model or ensemble of
models.</p>
</dd>
<dt><code>fs_method</code></dt><dd><p>Feature selection method used to determine variable
importance for the model or ensemble of models.</p>
</dd>
<dt><code>pooling_table</code></dt><dd><p>Run table for the data underlying the familiarData
object. Used internally.</p>
</dd>
<dt><code>prediction_data</code></dt><dd><p>Model predictions for a model or ensemble of models for
the underlying dataset.</p>
</dd>
<dt><code>confusion_matrix</code></dt><dd><p>Confusion matrix for a model or ensemble of models,
based on the underlying dataset.</p>
</dd>
<dt><code>decision_curve_data</code></dt><dd><p>Decision curve analysis data for a model or
ensemble of models, based on the underlying dataset.</p>
</dd>
<dt><code>calibration_info</code></dt><dd><p>Calibration information, e.g. baseline survival in the
development cohort.</p>
</dd>
<dt><code>calibration_data</code></dt><dd><p>Calibration data for a model or ensemble of models,
based on the underlying dataset.</p>
</dd>
<dt><code>model_performance</code></dt><dd><p>Model performance data for a model or ensemble of
models, based on the underlying dataset.</p>
</dd>
<dt><code>km_info</code></dt><dd><p>Information concerning risk-stratification cut-off values..</p>
</dd>
<dt><code>km_data</code></dt><dd><p>Kaplan-Meier survival data for a model or ensemble of models,
based on the underlying dataset.</p>
</dd>
<dt><code>auc_data</code></dt><dd><p>AUC-ROC and AUC-PR data for a model or ensemble of models,
based on the underlying dataset.</p>
</dd>
<dt><code>ice_data</code></dt><dd><p>Individual conditional expectation data for features included
in a model or ensemble of models, based on the underlying dataset. Partial
dependence data are computed on the fly from these data.</p>
</dd>
<dt><code>univariate_analysis</code></dt><dd><p>Univariate analysis of the underlying dataset.</p>
</dd>
<dt><code>feature_expressions</code></dt><dd><p>Feature expression values of the underlying
dataset.</p>
</dd>
<dt><code>feature_similarity</code></dt><dd><p>Feature similarity information of the underlying
dataset.</p>
</dd>
<dt><code>sample_similarity</code></dt><dd><p>Sample similarity information of the underlying
dataset.</p>
</dd>
<dt><code>is_validation</code></dt><dd><p>Signifies whether the underlying data forms a validation
dataset. Used internally.</p>
</dd>
<dt><code>generating_ensemble</code></dt><dd><p>Name of the ensemble that was used to generate the
familiarData object.</p>
</dd>
<dt><code>project_id</code></dt><dd><p>Identifier of the project that generated the familiarData
object.</p>
</dd>
<dt><code>familiar_version</code></dt><dd><p>Version of the familiar package.
</p>
<p>familiarData objects contain information obtained by evaluating a single
model or single ensemble of models on a dataset.</p>
</dd>
</dl>

<hr>
<h2 id='familiarDataElement-class'>Data container for evaluation data.</h2><span id='topic+familiarDataElement-class'></span>

<h3>Description</h3>

<p>Most attributes of the familiarData object are objects of the
familiarDataElement class. This (super-)class is used to allow for
standardised aggregation and processing of evaluation data.
</p>


<h3>Slots</h3>


<dl>
<dt><code>data</code></dt><dd><p>Evaluation data, typically a data.table or list.</p>
</dd>
<dt><code>identifiers</code></dt><dd><p>Identifiers of the data, e.g. the generating model name,
learner, etc.</p>
</dd>
<dt><code>detail_level</code></dt><dd><p>Sets the level at which results are computed and
aggregated.
</p>

<ul>
<li> <p><code>ensemble</code>: Results are computed at the ensemble level, i.e. over all
models in the ensemble. This means that, for example, bias-corrected
estimates of model performance are assessed by creating (at least) 20
bootstraps and computing the model performance of the ensemble model for
each bootstrap.
</p>
</li>
<li> <p><code>hybrid</code> (default): Results are computed at the level of models in an
ensemble. This means that, for example, bias-corrected estimates of model
performance are directly computed using the models in the ensemble. If there
are at least 20 trained models in the ensemble, performance is computed for
each model, in contrast to <code>ensemble</code> where performance is computed for the
ensemble of models. If there are less than 20 trained models in the
ensemble, bootstraps are created so that at least 20 point estimates can be
made.
</p>
</li>
<li> <p><code>model</code>: Results are computed at the model level. This means that, for
example, bias-corrected estimates of model performance are assessed by
creating (at least) 20 bootstraps and computing the performance of the model
for each bootstrap.
</p>
</li></ul>

<p>Note that each level of detail has a different interpretation for bootstrap
confidence intervals. For <code>ensemble</code> and <code>model</code> these are the confidence
intervals for the ensemble and an individual model, respectively. That is,
the confidence interval describes the range where an estimate produced by a
respective ensemble or model trained on a repeat of the experiment may be
found with the probability of the confidence level. For <code>hybrid</code>, it
represents the range where any single model trained on a repeat of the
experiment may be found with the probability of the confidence level. By
definition, confidence intervals obtained using <code>hybrid</code> are at least as
wide as those for <code>ensemble</code>. <code>hybrid</code> offers the correct interpretation if
the goal of the analysis is to assess the result of a single, unspecified,
model.
</p>
<p>Some child classes do not use this parameter.</p>
</dd>
<dt><code>estimation_type</code></dt><dd><p>Sets the type of estimation that should be possible.
This has the following options:
</p>

<ul>
<li> <p><code>point</code>: Point estimates.
</p>
</li>
<li> <p><code>bias_correction</code> or <code>bc</code>: Bias-corrected estimates. A bias-corrected
estimate is computed from (at least) 20 point estimates, and <code>familiar</code> may
bootstrap the data to create them.
</p>
</li>
<li> <p><code>bootstrap_confidence_interval</code> or <code>bci</code> (default): Bias-corrected
estimates with bootstrap confidence intervals (Efron and Hastie, 2016). The
number of point estimates required depends on the <code>confidence_level</code>
parameter, and <code>familiar</code> may bootstrap the data to create them.
</p>
</li></ul>

<p>Some child classes do not use this parameter.</p>
</dd>
<dt><code>confidence_level</code></dt><dd><p>(<em>optional</em>) Numeric value for the level at which
confidence intervals are determined. In the case bootstraps are used to
determine the confidence intervals bootstrap estimation, <code>familiar</code> uses the
rule of thumb <code class="reqn">n = 20 / ci.level</code> to determine the number of required
bootstraps.</p>
</dd>
<dt><code>bootstrap_ci_method</code></dt><dd><p>Method used to determine bootstrap confidence
intervals (Efron and Hastie, 2016). The following methods are implemented:
</p>

<ul>
<li> <p><code>percentile</code> (default): Confidence intervals obtained using the percentile
method.
</p>
</li>
<li> <p><code>bc</code>: Bias-corrected confidence intervals.
</p>
</li></ul>

<p>Note that the standard method is not implemented because this method is
often not suitable due to non-normal distributions. The bias-corrected and
accelerated (BCa) method is not implemented yet.</p>
</dd>
<dt><code>value_column</code></dt><dd><p>Identifies column(s) in the <code>data</code> attribute presenting
values.</p>
</dd>
<dt><code>grouping_column</code></dt><dd><p>Identifies column(s) in the <code>data</code> attribute presenting
identifier columns for grouping during aggregation. Familiar will
automatically assign items from the <code>identifiers</code> attribute to the data and
this attribute when combining multiple familiarDataElements of the same
(child) class.</p>
</dd>
<dt><code>is_aggregated</code></dt><dd><p>Defines whether the object was aggregated.</p>
</dd>
</dl>


<h3>References</h3>


<ol>
<li><p> Efron, B. &amp; Hastie, T. Computer Age Statistical Inference.
(Cambridge University Press, 2016).
</p>
</li></ol>


<hr>
<h2 id='familiarEnsemble-class'>Ensemble of familiar models.</h2><span id='topic+familiarEnsemble-class'></span>

<h3>Description</h3>

<p>A familiarEnsemble object contains one or more familiarModel objects.
</p>


<h3>Slots</h3>


<dl>
<dt><code>name</code></dt><dd><p>Name of the familiarEnsemble object.</p>
</dd>
<dt><code>model_list</code></dt><dd><p>List of attached familiarModel objects, or paths to these
objects. Familiar attaches familiarModel objects when required.</p>
</dd>
<dt><code>outcome_type</code></dt><dd><p>Outcome type of the data used to create the object.</p>
</dd>
<dt><code>outcome_info</code></dt><dd><p>Outcome information object, which contains additional
information concerning the outcome, such as class levels.</p>
</dd>
<dt><code>data_column_info</code></dt><dd><p>Data information object containing information
regarding identifier column names and outcome column names.</p>
</dd>
<dt><code>learner</code></dt><dd><p>Learning algorithm used to create the models in the ensemble.</p>
</dd>
<dt><code>fs_method</code></dt><dd><p>Feature selection method used to determine variable
importance for the models in the ensemble.</p>
</dd>
<dt><code>feature_info</code></dt><dd><p>List of objects containing feature information, e.g.,
name, class levels, transformation, normalisation and clustering
parameters.</p>
</dd>
<dt><code>required_features</code></dt><dd><p>The set of features required for complete
reproduction, i.e. with imputation.</p>
</dd>
<dt><code>model_features</code></dt><dd><p>The combined set of features that is used to train the
models in the ensemble,</p>
</dd>
<dt><code>novelty_features</code></dt><dd><p>The combined set of features that is used to train
all novelty detectors in the ensemble.</p>
</dd>
<dt><code>run_table</code></dt><dd><p>Run table for the data used to train the ensemble. Used
internally.</p>
</dd>
<dt><code>calibration_info</code></dt><dd><p>Calibration information, e.g. baseline survival in the
development cohort.</p>
</dd>
<dt><code>model_dir_path</code></dt><dd><p>Path to folder containing the familiarModel objects. Can
be updated using the <code>update_model_dir_path</code> method.</p>
</dd>
<dt><code>auto_detach</code></dt><dd><p>Flag used to determine whether models should be detached
from the model after use, or not. Used internally.</p>
</dd>
<dt><code>settings</code></dt><dd><p>A copy of the evaluation configuration parameters used at
model creation. These are used as default parameters when evaluating the
ensemble to create a familiarData object.</p>
</dd>
<dt><code>project_id</code></dt><dd><p>Identifier of the project that generated the
underlying familiarModel object(s).</p>
</dd>
<dt><code>familiar_version</code></dt><dd><p>Version of the familiar package.</p>
</dd>
</dl>

<hr>
<h2 id='familiarHyperparameterLearner-class'>Hyperparameter learner.</h2><span id='topic+familiarHyperparameterLearner-class'></span>

<h3>Description</h3>

<p>A familiarHyperparameterLearner object is a self-contained model that can be
applied to predict optimisation scores for a set of hyperparameters.
</p>


<h3>Details</h3>

<p>Hyperparameter learners are used to infer the optimisation score for
sets of hyperparameters. These are then used to either infer utility using
acquisition functions or to generate summary scores to identify the optimal
model.
</p>


<h3>Slots</h3>


<dl>
<dt><code>name</code></dt><dd><p>Name of the familiarHyperparameterLearner object.</p>
</dd>
<dt><code>learner</code></dt><dd><p>Algorithm used to create the hyperparameter learner.</p>
</dd>
<dt><code>target_learner</code></dt><dd><p>Algorithm for which the hyperparameters are being
learned.</p>
</dd>
<dt><code>target_outcome_type</code></dt><dd><p>Outcome type of the learner for which
hyperparameters are being modeled. Used to determine the target
hyperparameters.</p>
</dd>
<dt><code>optimisation_metric</code></dt><dd><p>One or metrics used to generate the optimisation
score.</p>
</dd>
<dt><code>optimisation_function</code></dt><dd><p>Function used to generate the optimisation score.</p>
</dd>
<dt><code>model</code></dt><dd><p>The actual model trained using the specific algorithm, e.g. a
isolation forest from the <code>isotree</code> package.</p>
</dd>
<dt><code>target_hyperparameters</code></dt><dd><p>The names of the hyperparameters that are used
to train the hyperparameter learner.</p>
</dd>
<dt><code>project_id</code></dt><dd><p>Identifier of the project that generated the
familiarHyperparameterLearner object.</p>
</dd>
<dt><code>familiar_version</code></dt><dd><p>Version of the familiar package.</p>
</dd>
<dt><code>package</code></dt><dd><p>Name of package(s) required to executed the hyperparameter
learner itself, e.g. <code>laGP</code>.</p>
</dd>
<dt><code>package_version</code></dt><dd><p>Version of the packages mentioned in the <code>package</code>
attribute.</p>
</dd>
</dl>

<hr>
<h2 id='familiarMetric-class'>Model performance metric.</h2><span id='topic+familiarMetric-class'></span>

<h3>Description</h3>

<p>Superclass for model performance objects.
</p>


<h3>Slots</h3>


<dl>
<dt><code>metric</code></dt><dd><p>Performance metric.</p>
</dd>
<dt><code>outcome_type</code></dt><dd><p>Type of outcome being predicted.</p>
</dd>
<dt><code>name</code></dt><dd><p>Name of the performance metric.</p>
</dd>
<dt><code>value_range</code></dt><dd><p>Range of the performance metric. Can be half-open.</p>
</dd>
<dt><code>baseline_value</code></dt><dd><p>Value of the metric for trivial models, e.g. models that
always predict the median value, the majority class, or the mean hazard,
etc.</p>
</dd>
<dt><code>higher_better</code></dt><dd><p>States whether higher metric values correspond to better
predictive model performance (e.g. accuracy) or not (e.g. root mean squared
error).</p>
</dd>
</dl>

<hr>
<h2 id='familiarModel-class'>Familiar model.</h2><span id='topic+familiarModel-class'></span>

<h3>Description</h3>

<p>A familiarModel object is a self-contained model that can be applied to
generate predictions for a dataset. familiarModel objects form the parent
class of learner-specific child classes.
</p>


<h3>Slots</h3>


<dl>
<dt><code>name</code></dt><dd><p>Name of the familiarModel object.</p>
</dd>
<dt><code>model</code></dt><dd><p>The actual model trained using a specific algorithm, e.g. a
random forest from the <code>ranger</code> package, or a LASSO model from <code>glmnet</code>.</p>
</dd>
<dt><code>outcome_type</code></dt><dd><p>Outcome type of the data used to create the object.</p>
</dd>
<dt><code>outcome_info</code></dt><dd><p>Outcome information object, which contains additional
information concerning the outcome, such as class levels.</p>
</dd>
<dt><code>feature_info</code></dt><dd><p>List of objects containing feature information, e.g.,
name, class levels, transformation, normalisation and clustering
parameters.</p>
</dd>
<dt><code>data_column_info</code></dt><dd><p>Data information object containing information
regarding identifier column names and outcome column names.</p>
</dd>
<dt><code>hyperparameters</code></dt><dd><p>Set of hyperparameters used to train the model.</p>
</dd>
<dt><code>hyperparameter_data</code></dt><dd><p>Information generated during hyperparameter
optimisation.</p>
</dd>
<dt><code>calibration_model</code></dt><dd><p>One or more models used to recalibrate the model
output. Currently only used by some models.</p>
</dd>
<dt><code>novelty_detector</code></dt><dd><p>A familiarNoveltyDetector object that can be used to
detect out-of-distribution samples.</p>
</dd>
<dt><code>learner</code></dt><dd><p>Learning algorithm used to create the model.</p>
</dd>
<dt><code>fs_method</code></dt><dd><p>Feature selection method used to determine variable
importance for the model.</p>
</dd>
<dt><code>required_features</code></dt><dd><p>The set of features required for complete
reproduction, i.e. with imputation.</p>
</dd>
<dt><code>model_features</code></dt><dd><p>The set of features that is used to train the model,</p>
</dd>
<dt><code>novelty_features</code></dt><dd><p>The set of features that is used to train all novelty
detectors in the ensemble.</p>
</dd>
<dt><code>calibration_info</code></dt><dd><p>Calibration information, e.g. baseline survival in the
development cohort.</p>
</dd>
<dt><code>km_info</code></dt><dd><p>Data concerning stratification into risk groups.</p>
</dd>
<dt><code>run_table</code></dt><dd><p>Run table for the data used to train the model. Used
internally.</p>
</dd>
<dt><code>settings</code></dt><dd><p>A copy of the evaluation configuration parameters used at
model creation. These are used as default parameters when evaluating the
model (technically, familiarEnsemble) to create a familiarData object.</p>
</dd>
<dt><code>is_trimmed</code></dt><dd><p>Flag that indicates whether the model, stored in the <code>model</code>
slot, has been trimmed.</p>
</dd>
<dt><code>trimmed_function</code></dt><dd><p>List of functions whose output has been captured prior
to trimming the model.</p>
</dd>
<dt><code>messages</code></dt><dd><p>List of warning and error messages generated during training.</p>
</dd>
<dt><code>project_id</code></dt><dd><p>Identifier of the project that generated the familiarModel
object.</p>
</dd>
<dt><code>familiar_version</code></dt><dd><p>Version of the familiar package.</p>
</dd>
<dt><code>package</code></dt><dd><p>Name of package(s) required to executed the model itself, e.g.
<code>ranger</code> or <code>glmnet</code>.</p>
</dd>
<dt><code>package_version</code></dt><dd><p>Version of the packages mentioned in the <code>package</code>
attribute.</p>
</dd>
</dl>

<hr>
<h2 id='familiarNoveltyDetector-class'>Novelty detector.</h2><span id='topic+familiarNoveltyDetector-class'></span>

<h3>Description</h3>

<p>A familiarNoveltyDetector object is a self-contained model that can be
applied to generate out-of-distribution predictions for instances in a
dataset.
</p>


<h3>Slots</h3>


<dl>
<dt><code>name</code></dt><dd><p>Name of the familiarNoveltyDetector object.</p>
</dd>
<dt><code>learner</code></dt><dd><p>Learning algorithm used to create the novelty detector.</p>
</dd>
<dt><code>model</code></dt><dd><p>The actual novelty detector trained using a specific algorithm,
e.g. a isolation forest from the <code>isotree</code> package.</p>
</dd>
<dt><code>feature_info</code></dt><dd><p>List of objects containing feature information, e.g.,
name, class levels, transformation, normalisation and clustering
parameters.</p>
</dd>
<dt><code>data_column_info</code></dt><dd><p>Data information object containing information
regarding identifier column names.</p>
</dd>
<dt><code>conversion_parameters</code></dt><dd><p>Parameters used to convert raw output to
statistical probability of being out-of-distribution. Currently unused.</p>
</dd>
<dt><code>hyperparameters</code></dt><dd><p>Set of hyperparameters used to train the detector.</p>
</dd>
<dt><code>required_features</code></dt><dd><p>The set of features required for complete
reproduction, i.e. with imputation.</p>
</dd>
<dt><code>model_features</code></dt><dd><p>The set of features that is used to train the detector.</p>
</dd>
<dt><code>run_table</code></dt><dd><p>Run table for the data used to train the detector. Used
internally.</p>
</dd>
<dt><code>is_trimmed</code></dt><dd><p>Flag that indicates whether the detector, stored in the
<code>model</code> slot, has been trimmed.</p>
</dd>
<dt><code>trimmed_function</code></dt><dd><p>List of functions whose output has been captured prior
to trimming the model.</p>
</dd>
<dt><code>project_id</code></dt><dd><p>Identifier of the project that generated the
familiarNoveltyDetector object.</p>
</dd>
<dt><code>familiar_version</code></dt><dd><p>Version of the familiar package.</p>
</dd>
<dt><code>package</code></dt><dd><p>Name of package(s) required to executed the detector itself,
e.g. <code>isotree</code>.</p>
</dd>
<dt><code>package_version</code></dt><dd><p>Version of the packages mentioned in the <code>package</code>
attribute.
</p>
<p>Note that these objects do not contain any data concerning outcome, as this
not relevant for (prospective) out-of-distribution detection.</p>
</dd>
</dl>

<hr>
<h2 id='familiarVimpMethod-class'>Variable importance method object.</h2><span id='topic+familiarVimpMethod-class'></span>

<h3>Description</h3>

<p>The familiarVimpMethod class is the parent class for all variable importance
methods in familiar.
</p>


<h3>Slots</h3>


<dl>
<dt><code>outcome_type</code></dt><dd><p>Outcome type of the data to be evaluated using the object.</p>
</dd>
<dt><code>hyperparameters</code></dt><dd><p>Set of hyperparameters for the variable importance
method.</p>
</dd>
<dt><code>vimp_method</code></dt><dd><p>The character string indicating the variable importance
method.</p>
</dd>
<dt><code>multivariate</code></dt><dd><p>Flags whether the variable importance method is
multivariate vs. univariate.</p>
</dd>
<dt><code>outcome_info</code></dt><dd><p>Outcome information object, which contains additional
information concerning the outcome, such as class levels.</p>
</dd>
<dt><code>feature_info</code></dt><dd><p>List of objects containing feature information, e.g.,
name, class levels, transformation, normalisation and clustering
parameters.</p>
</dd>
<dt><code>required_features</code></dt><dd><p>The set of features to be assessed by the variable
importance method.</p>
</dd>
<dt><code>package</code></dt><dd><p>Name of the package(s) required to execute the variable
importance method.</p>
</dd>
<dt><code>run_table</code></dt><dd><p>Run table for the data to be assessed by the variable
importance method. Used internally.</p>
</dd>
<dt><code>project_id</code></dt><dd><p>Identifier of the project that generated the
familiarVimpMethod object.</p>
</dd>
</dl>

<hr>
<h2 id='featureInfo-class'>Feature information object.</h2><span id='topic+featureInfo-class'></span>

<h3>Description</h3>

<p>A featureInfo object contains information for a single feature. This
information is used to check data prospectively for consistency and for data
preparation. These objects are, for instance, attached to a familiarModel
object so that data can be pre-processed in the same way as the development
data.
</p>


<h3>Slots</h3>


<dl>
<dt><code>name</code></dt><dd><p>Name of the feature, which by default is the column name of the
feature.</p>
</dd>
<dt><code>set_descriptor</code></dt><dd><p>Character string describing the set to which the feature
belongs. Currently not used.</p>
</dd>
<dt><code>feature_type</code></dt><dd><p>Describes the feature type, i.e. <code>factor</code> or <code>numeric</code>.</p>
</dd>
<dt><code>levels</code></dt><dd><p>The class levels of categorical features. This is used to check
prospective datasets.</p>
</dd>
<dt><code>ordered</code></dt><dd><p>Specifies whether the</p>
</dd>
<dt><code>distribution</code></dt><dd><p>Five-number summary (numeric) or class frequency
(categorical).</p>
</dd>
<dt><code>data_id</code></dt><dd><p>Internal identifier for the dataset used to derive the feature
information.</p>
</dd>
<dt><code>run_id</code></dt><dd><p>Internal identifier for the specific subset of the dataset used
to derive the feature information.</p>
</dd>
<dt><code>in_signature</code></dt><dd><p>Specifies whether the feature is included in the model
signature.</p>
</dd>
<dt><code>in_novelty</code></dt><dd><p>Specifies whether the feature is included in the novelty
detector.</p>
</dd>
<dt><code>removed</code></dt><dd><p>Specifies whether the feature was removed during
pre-processing.</p>
</dd>
<dt><code>removed_unknown_type</code></dt><dd><p>Specifies whether the feature was removed during
pre-processing because the type was neither factor nor numeric..</p>
</dd>
<dt><code>removed_missing_values</code></dt><dd><p>Specifies whether the feature was removed during
pre-processing because it contained too many missing values.</p>
</dd>
<dt><code>removed_no_variance</code></dt><dd><p>Specifies whether the feature was removed during
pre-processing because it did not contain more than 1 unique value.</p>
</dd>
<dt><code>removed_low_variance</code></dt><dd><p>Specifies whether the feature was removed during
pre-processing because the variance was too low. Requires applying
<code>low_variance</code> as a <code>filter_method</code>.</p>
</dd>
<dt><code>removed_low_robustness</code></dt><dd><p>Specifies whether the feature was removed during
pre-processing because it lacks robustness. Requires applying
<code>robustness</code> as a <code>filter_method</code>, as well as repeated measurement.</p>
</dd>
<dt><code>removed_low_importance</code></dt><dd><p>Specifies whether the feature was removed during
pre-processing because it lacks relevance. Requires applying
<code>univariate_test</code> as a <code>filter_method</code>.</p>
</dd>
<dt><code>fraction_missing</code></dt><dd><p>Specifies the fraction of missing values.</p>
</dd>
<dt><code>robustness</code></dt><dd><p>Specifies robustness of the feature, if measured.</p>
</dd>
<dt><code>univariate_importance</code></dt><dd><p>Specifies the univariate p-value of the feature, if measured.</p>
</dd>
<dt><code>transformation_parameters</code></dt><dd><p>Details parameters for power transformation of numeric features.</p>
</dd>
<dt><code>normalisation_parameters</code></dt><dd><p>Details parameters for (global) normalisation of numeric features.</p>
</dd>
<dt><code>batch_normalisation_parameters</code></dt><dd><p>Details parameters for batch normalisation of numeric features.</p>
</dd>
<dt><code>imputation_parameters</code></dt><dd><p>Details parameters or models for imputation of missing values.</p>
</dd>
<dt><code>cluster_parameters</code></dt><dd><p>Details parameters for forming clusters with other features.</p>
</dd>
<dt><code>required_features</code></dt><dd><p>Details features required for clustering or imputation.</p>
</dd>
<dt><code>familiar_version</code></dt><dd><p>Version of the familiar package.</p>
</dd>
</dl>

<hr>
<h2 id='featureInfoParameters-class'>Feature information parameters object.</h2><span id='topic+featureInfoParameters-class'></span>

<h3>Description</h3>

<p>A featureInfo object contains information for a single feature. Some
information, for example concerning clustering and transformation contains
various parameters that allow for applying the data transformation correctly.
These are stored in featureInfoParameters objects.
</p>


<h3>Details</h3>

<p>featureInfoParameters is normally a parent class for specific
classes, such as featureInfoParametersTransformation.
</p>


<h3>Slots</h3>


<dl>
<dt><code>name</code></dt><dd><p>Name of the feature, which by default is the column name of the
feature. Typically used to correctly assign the data.</p>
</dd>
<dt><code>complete</code></dt><dd><p>Flags whether the parameters have been completely set.</p>
</dd>
<dt><code>familiar_version</code></dt><dd><p>Version of the familiar package.</p>
</dd>
</dl>

<hr>
<h2 id='get_class_names+2CfamiliarCollection-method'>Get outcome class labels</h2><span id='topic+get_class_names+2CfamiliarCollection-method'></span><span id='topic+get_class_names'></span>

<h3>Description</h3>

<p>Outcome classes in familiarCollection objects can have custom names for export and plotting. This function retrieves the currently assigned names.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'familiarCollection'
get_class_names(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_class_names+2B2CfamiliarCollection-method_+3A_x">x</code></td>
<td>
<p>A familiarCollection object.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Labels convert internal class names to the requested label at export or when plotting. Labels can be changed using the <code>set_class_names</code> method.
</p>


<h3>Value</h3>

<p>An ordered array of class labels.
</p>


<h3>See Also</h3>


<ul>
<li> <p><a href="#topic+familiarCollection-class">familiarCollection</a> for information concerning the familiarCollection class.
</p>
</li>
<li> <p><code><a href="#topic+set_class_names">set_class_names</a></code> for updating the name and ordering of classes.
</p>
</li></ul>


<hr>
<h2 id='get_data_set_names+2CfamiliarCollection-method'>Get current name of datasets</h2><span id='topic+get_data_set_names+2CfamiliarCollection-method'></span><span id='topic+get_data_set_names'></span>

<h3>Description</h3>

<p>Datasets in familiarCollection objects can have custom names for export and plotting. This function retrieves the currently assigned names.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'familiarCollection'
get_data_set_names(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_data_set_names+2B2CfamiliarCollection-method_+3A_x">x</code></td>
<td>
<p>A familiarCollection object.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Labels convert internal naming of data sets to the requested label at export or when plotting. Labels can be changed using the <code>set_data_set_names</code> method.
</p>


<h3>Value</h3>

<p>An ordered array of dataset name labels.
</p>


<h3>See Also</h3>


<ul>
<li> <p><a href="#topic+familiarCollection-class">familiarCollection</a> for information concerning the familiarCollection class.
</p>
</li>
<li> <p><code><a href="#topic+set_data_set_names">set_data_set_names</a></code> for updating the name of datasets and their ordering.
</p>
</li></ul>


<hr>
<h2 id='get_feature_names+2CfamiliarCollection-method'>Get current feature labels</h2><span id='topic+get_feature_names+2CfamiliarCollection-method'></span><span id='topic+get_feature_names'></span>

<h3>Description</h3>

<p>Features in familiarCollection objects can have custom names for export and plotting. This function retrieves the currently assigned names.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'familiarCollection'
get_feature_names(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_feature_names+2B2CfamiliarCollection-method_+3A_x">x</code></td>
<td>
<p>A familiarCollection object.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Labels convert internal naming of features to the requested label at export or when plotting. Labels can be changed using the <code>set_feature_names</code> method.
</p>


<h3>Value</h3>

<p>An ordered array of feature labels.
</p>


<h3>See Also</h3>


<ul>
<li> <p><a href="#topic+familiarCollection-class">familiarCollection</a> for information concerning the familiarCollection class.
</p>
</li>
<li> <p><code><a href="#topic+set_feature_names">set_feature_names</a></code> for updating the name and ordering of features.
</p>
</li></ul>


<hr>
<h2 id='get_fs_method_names+2CfamiliarCollection-method'>Get current feature selection method name labels</h2><span id='topic+get_fs_method_names+2CfamiliarCollection-method'></span><span id='topic+get_fs_method_names'></span>

<h3>Description</h3>

<p>Feature selection methods in familiarCollection objects can have custom names for export and plotting. This function retrieves the currently assigned names.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'familiarCollection'
get_fs_method_names(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_fs_method_names+2B2CfamiliarCollection-method_+3A_x">x</code></td>
<td>
<p>A familiarCollection object.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Labels convert internal naming of feature selection methods to the requested label at export or when plotting. Labels can be changed using the <code>set_fs_method_names</code> method.
</p>


<h3>Value</h3>

<p>An ordered array of feature selection method name labels.
</p>


<h3>See Also</h3>


<ul>
<li> <p><a href="#topic+familiarCollection-class">familiarCollection</a> for information concerning the familiarCollection class.
</p>
</li>
<li> <p><code><a href="#topic+set_fs_method_names">set_fs_method_names</a></code> for updating the name of feature selection methods and their ordering.
</p>
</li></ul>


<hr>
<h2 id='get_learner_names+2CfamiliarCollection-method'>Get current learner name labels</h2><span id='topic+get_learner_names+2CfamiliarCollection-method'></span><span id='topic+get_learner_names'></span>

<h3>Description</h3>

<p>Learners in familiarCollection objects can have custom names for export and plotting. This function retrieves the currently assigned names.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'familiarCollection'
get_learner_names(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_learner_names+2B2CfamiliarCollection-method_+3A_x">x</code></td>
<td>
<p>A familiarCollection object.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Labels convert internal naming of learners to the requested label at export or when plotting. Labels can be changed using the <code>set_learner_names</code> method.
</p>


<h3>Value</h3>

<p>An ordered array of learner name labels.
</p>


<h3>See Also</h3>


<ul>
<li> <p><a href="#topic+familiarCollection-class">familiarCollection</a> for information concerning the familiarCollection class.
</p>
</li>
<li> <p><code><a href="#topic+set_learner_names">set_learner_names</a></code> for updating the name of learners and their ordering.
</p>
</li></ul>


<hr>
<h2 id='get_risk_group_names+2CfamiliarCollection-method'>Get current risk group labels</h2><span id='topic+get_risk_group_names+2CfamiliarCollection-method'></span><span id='topic+get_risk_group_names'></span>

<h3>Description</h3>

<p>Risk groups in familiarCollection objects can have custom names for export and plotting. This function retrieves the currently assigned names.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'familiarCollection'
get_risk_group_names(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_risk_group_names+2B2CfamiliarCollection-method_+3A_x">x</code></td>
<td>
<p>A familiarCollection object.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Labels convert internal naming of risk groups to the requested label at export or when plotting. Labels can be changed using the <code>set_risk_group_names</code> method.
</p>


<h3>Value</h3>

<p>An ordered array of risk group labels.
</p>


<h3>See Also</h3>


<ul>
<li> <p><a href="#topic+familiarCollection-class">familiarCollection</a> for information concerning the familiarCollection class.
</p>
</li>
<li> <p><code><a href="#topic+set_risk_group_names">set_risk_group_names</a></code> for updating the name and ordering of risk groups.
</p>
</li></ul>


<hr>
<h2 id='get_vimp_table'>Extract variable importance table.</h2><span id='topic+get_vimp_table'></span><span id='topic+get_vimp_table+2Clist-method'></span><span id='topic+get_vimp_table+2Ccharacter-method'></span><span id='topic+get_vimp_table+2CvimpTable-method'></span><span id='topic+get_vimp_table+2CNULL-method'></span><span id='topic+get_vimp_table+2CexperimentData-method'></span><span id='topic+get_vimp_table+2CfamiliarModel-method'></span>

<h3>Description</h3>

<p>This method retrieves and parses variable importance tables from
their respective <code>vimpTable</code> objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_vimp_table(x, state = "ranked", ...)

## S4 method for signature 'list'
get_vimp_table(x, state = "ranked", ...)

## S4 method for signature 'character'
get_vimp_table(x, state = "ranked", ...)

## S4 method for signature 'vimpTable'
get_vimp_table(x, state = "ranked", ...)

## S4 method for signature ''NULL''
get_vimp_table(x, state = "ranked", ...)

## S4 method for signature 'experimentData'
get_vimp_table(x, state = "ranked", ...)

## S4 method for signature 'familiarModel'
get_vimp_table(x, state = "ranked", data = NULL, as_object = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_vimp_table_+3A_x">x</code></td>
<td>
<p>Variable importance (<code>vimpTable</code>) object, a list thereof, or one or
more paths to these objects. This method extracts the variable importance
table from such objects.</p>
</td></tr>
<tr><td><code id="get_vimp_table_+3A_state">state</code></td>
<td>
<p>State of the returned variable importance table. This affects
what contents are shown, and in which format. The variable importance table
can be returned with the following states:
</p>

<ul>
<li> <p><code>initial</code>: initial state, directly after the variable importance table is
filled. The returned variable importance table shows the raw, un-processed
data.
</p>
</li>
<li> <p><code>decoded</code>: depending on the variable importance method, the initial
variable importance table may contain the scores of individual contrasts for
categorical variables. When decoded, scores from all contrasts are
aggregated to a single score for each feature.
</p>
</li>
<li> <p><code>declustered</code>: variable importance is determined from fully processed
features, which includes clustering. This means that a single feature in the
variable importance table may represent multiple original features. When a
variable importance table has been declustered, all clusters have been
turned into their constituent features.
</p>
</li>
<li> <p><code>ranked</code> (default): The scores have been used to create ranks, with lower
ranks indicating better features.
</p>
</li></ul>

<p>Internally, the variable importance table will go through each state, i.e.
an variable importance table in the initial state will be decoded,
declustered and then ranked prior to returning the variable importance
table.</p>
</td></tr>
<tr><td><code id="get_vimp_table_+3A_...">...</code></td>
<td>
<p>Unused arguments.</p>
</td></tr>
<tr><td><code id="get_vimp_table_+3A_data">data</code></td>
<td>
<p>Internally used argument for use with <code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="get_vimp_table_+3A_as_object">as_object</code></td>
<td>
<p>Internally used argument for use with <code>familiarModel</code>
objects.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>data.table</code> with variable importance scores and, with
<code>state="ranked"</code>, the respective ranks.
</p>

<hr>
<h2 id='get_xml_config'>Create an empty xml configuration file</h2><span id='topic+get_xml_config'></span>

<h3>Description</h3>

<p>This function creates an empty configuration xml file in the directory
specified by <code>dir_path</code>. This provides an alternative to the use of input
arguments for familiar.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_xml_config(dir_path)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="get_xml_config_+3A_dir_path">dir_path</code></td>
<td>
<p>Path to the directory where the configuration file should be
created. The directory should exist, and no file named <code>config.xml</code> should
be present.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Nothing. A file named <code>config.xml</code> is created in the directory
indicated by <code>dir_path</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Creates a config.xml file in the working directory
get_xml_config(dir_path=getwd())

## End(Not run)
</code></pre>

<hr>
<h2 id='is.encapsulated_path'>Internal test for encapsulated_path</h2><span id='topic+is.encapsulated_path'></span>

<h3>Description</h3>

<p>This function tests if the object is an <code>encapsulated_path</code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.encapsulated_path(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="is.encapsulated_path_+3A_x">x</code></td>
<td>
<p>Object to be tested.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>TRUE</code> for objects that are <code>encapsulated_path</code>, <code>FALSE</code> otherwise.
</p>

<hr>
<h2 id='is.waive'>Internal test to see if an object is a waiver</h2><span id='topic+is.waive'></span>

<h3>Description</h3>

<p>This function tests if the object was created by the <code>waiver</code> function. This
function is functionally identical to <code>ggplot2:::is.waive()</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.waive(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="is.waive_+3A_x">x</code></td>
<td>
<p>Object to be tested.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>TRUE</code> for objects that are waivers, <code>FALSE</code> otherwise.
</p>

<hr>
<h2 id='outcomeInfo-class'>Outcome information object.</h2><span id='topic+outcomeInfo-class'></span>

<h3>Description</h3>

<p>An outcome information object stores data concerning an outcome. This is used
to prospectively check data.
</p>


<h3>Slots</h3>


<dl>
<dt><code>name</code></dt><dd><p>Name of the outcome, inherited from the original column name by
default.</p>
</dd>
<dt><code>outcome_type</code></dt><dd><p>Type of outcome.</p>
</dd>
<dt><code>outcome_column</code></dt><dd><p>Name of the outcome column in data.</p>
</dd>
<dt><code>levels</code></dt><dd><p>Specifies class levels of categorical outcomes.</p>
</dd>
<dt><code>ordered</code></dt><dd><p>Specifies whether categorical outcomes are ordered.</p>
</dd>
<dt><code>reference</code></dt><dd><p>Class level used as reference.</p>
</dd>
<dt><code>time</code></dt><dd><p>Maximum time, as set by the <code>time_max</code> configuration parameter.</p>
</dd>
<dt><code>censored</code></dt><dd><p>Censoring indicators for survival outcomes.</p>
</dd>
<dt><code>event</code></dt><dd><p>Event indicators for survival outcomes.</p>
</dd>
<dt><code>competing_risk</code></dt><dd><p>Indicators for competing risks in survival outcomes.</p>
</dd>
<dt><code>distribution</code></dt><dd><p>Five-number summary (numeric outcomes), class frequency
(categorical outcomes), or survival distributions.</p>
</dd>
<dt><code>data_id</code></dt><dd><p>Internal identifier for the dataset used to derive the outcome
information.</p>
</dd>
<dt><code>run_id</code></dt><dd><p>Internal identifier for the specific subset of the dataset used
to derive the outcome information.</p>
</dd>
<dt><code>transformation_parameters</code></dt><dd><p>Parameters used for transforming a numeric
outcomes. Currently unused.</p>
</dd>
<dt><code>normalisation_parameters</code></dt><dd><p>Parameters used for normalising numeric
outcomes. Currently unused.</p>
</dd>
</dl>

<hr>
<h2 id='plot_auc_precision_recall_curve'>Plot the precision-recall curve.</h2><span id='topic+plot_auc_precision_recall_curve'></span><span id='topic+plot_auc_precision_recall_curve+2CANY-method'></span><span id='topic+plot_auc_precision_recall_curve+2CfamiliarCollection-method'></span>

<h3>Description</h3>

<p>This method creates precision-recall curves based on data in a
familiarCollection object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_auc_precision_recall_curve(
  object,
  draw = FALSE,
  dir_path = NULL,
  split_by = NULL,
  color_by = NULL,
  facet_by = NULL,
  facet_wrap_cols = NULL,
  ggtheme = NULL,
  discrete_palette = NULL,
  x_label = waiver(),
  y_label = waiver(),
  legend_label = waiver(),
  plot_title = waiver(),
  plot_sub_title = waiver(),
  caption = NULL,
  x_n_breaks = 5,
  x_breaks = NULL,
  y_n_breaks = 5,
  y_breaks = NULL,
  conf_int_style = c("ribbon", "step", "none"),
  conf_int_alpha = 0.4,
  width = waiver(),
  height = waiver(),
  units = waiver(),
  export_collection = FALSE,
  ...
)

## S4 method for signature 'ANY'
plot_auc_precision_recall_curve(
  object,
  draw = FALSE,
  dir_path = NULL,
  split_by = NULL,
  color_by = NULL,
  facet_by = NULL,
  facet_wrap_cols = NULL,
  ggtheme = NULL,
  discrete_palette = NULL,
  x_label = waiver(),
  y_label = waiver(),
  legend_label = waiver(),
  plot_title = waiver(),
  plot_sub_title = waiver(),
  caption = NULL,
  x_n_breaks = 5,
  x_breaks = NULL,
  y_n_breaks = 5,
  y_breaks = NULL,
  conf_int_style = c("ribbon", "step", "none"),
  conf_int_alpha = 0.4,
  width = waiver(),
  height = waiver(),
  units = waiver(),
  export_collection = FALSE,
  ...
)

## S4 method for signature 'familiarCollection'
plot_auc_precision_recall_curve(
  object,
  draw = FALSE,
  dir_path = NULL,
  split_by = NULL,
  color_by = NULL,
  facet_by = NULL,
  facet_wrap_cols = NULL,
  ggtheme = NULL,
  discrete_palette = NULL,
  x_label = waiver(),
  y_label = waiver(),
  legend_label = waiver(),
  plot_title = waiver(),
  plot_sub_title = waiver(),
  caption = NULL,
  x_n_breaks = 5,
  x_breaks = NULL,
  y_n_breaks = 5,
  y_breaks = NULL,
  conf_int_style = c("ribbon", "step", "none"),
  conf_int_alpha = 0.4,
  width = waiver(),
  height = waiver(),
  units = waiver(),
  export_collection = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_auc_precision_recall_curve_+3A_object">object</code></td>
<td>
<p><code>familiarCollection</code> object, or one or more <code>familiarData</code>
objects, that will be internally converted to a <code>familiarCollection</code> object.
It is also possible to provide a <code>familiarEnsemble</code> or one or more
<code>familiarModel</code> objects together with the data from which data is computed
prior to export. Paths to such files can also be provided.</p>
</td></tr>
<tr><td><code id="plot_auc_precision_recall_curve_+3A_draw">draw</code></td>
<td>
<p>(<em>optional</em>) Draws the plot if TRUE.</p>
</td></tr>
<tr><td><code id="plot_auc_precision_recall_curve_+3A_dir_path">dir_path</code></td>
<td>
<p>(<em>optional</em>) Path to the directory where the plots of receiver
operating characteristic curves are saved to. Output is saved in the
<code>performance</code> subdirectory. If <code>NULL</code> no figures are saved, but are returned
instead.</p>
</td></tr>
<tr><td><code id="plot_auc_precision_recall_curve_+3A_split_by">split_by</code></td>
<td>
<p>(<em>optional</em>) Splitting variables. This refers to column names
on which datasets are split. A separate figure is created for each split.
See details for available variables.</p>
</td></tr>
<tr><td><code id="plot_auc_precision_recall_curve_+3A_color_by">color_by</code></td>
<td>
<p>(<em>optional</em>) Variables used to determine fill colour of plot
objects. The variables cannot overlap with those provided to the <code>split_by</code>
argument, but may overlap with other arguments. See details for available
variables.</p>
</td></tr>
<tr><td><code id="plot_auc_precision_recall_curve_+3A_facet_by">facet_by</code></td>
<td>
<p>(<em>optional</em>) Variables used to determine how and if facets of
each figure appear. In case the <code>facet_wrap_cols</code> argument is <code>NULL</code>, the
first variable is used to define columns, and the remaing variables are
used to define rows of facets. The variables cannot overlap with those
provided to the <code>split_by</code> argument, but may overlap with other arguments.
See details for available variables.</p>
</td></tr>
<tr><td><code id="plot_auc_precision_recall_curve_+3A_facet_wrap_cols">facet_wrap_cols</code></td>
<td>
<p>(<em>optional</em>) Number of columns to generate when facet
wrapping. If NULL, a facet grid is produced instead.</p>
</td></tr>
<tr><td><code id="plot_auc_precision_recall_curve_+3A_ggtheme">ggtheme</code></td>
<td>
<p>(<em>optional</em>) <code>ggplot</code> theme to use for plotting.</p>
</td></tr>
<tr><td><code id="plot_auc_precision_recall_curve_+3A_discrete_palette">discrete_palette</code></td>
<td>
<p>(<em>optional</em>) Palette to use to color the different
plot elements in case a value was provided to the <code>color_by</code> argument.</p>
</td></tr>
<tr><td><code id="plot_auc_precision_recall_curve_+3A_x_label">x_label</code></td>
<td>
<p>(<em>optional</em>) Label to provide to the x-axis. If NULL, no label
is shown.</p>
</td></tr>
<tr><td><code id="plot_auc_precision_recall_curve_+3A_y_label">y_label</code></td>
<td>
<p>(<em>optional</em>) Label to provide to the y-axis. If NULL, no label
is shown.</p>
</td></tr>
<tr><td><code id="plot_auc_precision_recall_curve_+3A_legend_label">legend_label</code></td>
<td>
<p>(<em>optional</em>) Label to provide to the legend. If NULL, the
legend will not have a name.</p>
</td></tr>
<tr><td><code id="plot_auc_precision_recall_curve_+3A_plot_title">plot_title</code></td>
<td>
<p>(<em>optional</em>) Label to provide as figure title. If NULL, no
title is shown.</p>
</td></tr>
<tr><td><code id="plot_auc_precision_recall_curve_+3A_plot_sub_title">plot_sub_title</code></td>
<td>
<p>(<em>optional</em>) Label to provide as figure subtitle. If
NULL, no subtitle is shown.</p>
</td></tr>
<tr><td><code id="plot_auc_precision_recall_curve_+3A_caption">caption</code></td>
<td>
<p>(<em>optional</em>) Label to provide as figure caption. If NULL, no
caption is shown.</p>
</td></tr>
<tr><td><code id="plot_auc_precision_recall_curve_+3A_x_n_breaks">x_n_breaks</code></td>
<td>
<p>(<em>optional</em>) Number of breaks to show on the x-axis of the
plot. <code>x_n_breaks</code> is used to determine the <code>x_breaks</code> argument in case it
is unset.</p>
</td></tr>
<tr><td><code id="plot_auc_precision_recall_curve_+3A_x_breaks">x_breaks</code></td>
<td>
<p>(<em>optional</em>) Break points on the x-axis of the plot.</p>
</td></tr>
<tr><td><code id="plot_auc_precision_recall_curve_+3A_y_n_breaks">y_n_breaks</code></td>
<td>
<p>(<em>optional</em>) Number of breaks to show on the y-axis of the
plot. <code>y_n_breaks</code> is used to determine the <code>y_breaks</code> argument in case it
is unset.</p>
</td></tr>
<tr><td><code id="plot_auc_precision_recall_curve_+3A_y_breaks">y_breaks</code></td>
<td>
<p>(<em>optional</em>) Break points on the y-axis of the plot.</p>
</td></tr>
<tr><td><code id="plot_auc_precision_recall_curve_+3A_conf_int_style">conf_int_style</code></td>
<td>
<p>(<em>optional</em>) Confidence interval style. See details for
allowed styles.</p>
</td></tr>
<tr><td><code id="plot_auc_precision_recall_curve_+3A_conf_int_alpha">conf_int_alpha</code></td>
<td>
<p>(<em>optional</em>) Alpha value to determine transparency of
confidence intervals or, alternatively, other plot elements with which the
confidence interval overlaps. Only values between 0.0 (fully transparent)
and 1.0 (fully opaque) are allowed.</p>
</td></tr>
<tr><td><code id="plot_auc_precision_recall_curve_+3A_width">width</code></td>
<td>
<p>(<em>optional</em>) Width of the plot. A default value is derived from
the number of facets.</p>
</td></tr>
<tr><td><code id="plot_auc_precision_recall_curve_+3A_height">height</code></td>
<td>
<p>(<em>optional</em>) Height of the plot. A default value is derived from
the number of features and the number of facets.</p>
</td></tr>
<tr><td><code id="plot_auc_precision_recall_curve_+3A_units">units</code></td>
<td>
<p>(<em>optional</em>) Plot size unit. Either <code>cm</code> (default), <code>mm</code> or <code style="white-space: pre;">&#8288;in&#8288;</code>.</p>
</td></tr>
<tr><td><code id="plot_auc_precision_recall_curve_+3A_export_collection">export_collection</code></td>
<td>
<p>(<em>optional</em>) Exports the collection if TRUE.</p>
</td></tr>
<tr><td><code id="plot_auc_precision_recall_curve_+3A_...">...</code></td>
<td>

<p>Arguments passed on to <code><a href="#topic+as_familiar_collection">as_familiar_collection</a></code>, <code><a href="ggplot2.html#topic+ggsave">ggplot2::ggsave</a></code>
</p>

<dl>
<dt><code>familiar_data_names</code></dt><dd><p>Names of the dataset(s). Only used if the <code>object</code> parameter
is one or more <code>familiarData</code> objects.</p>
</dd>
<dt><code>collection_name</code></dt><dd><p>Name of the collection.</p>
</dd>
<dt><code>filename</code></dt><dd><p>File name to create on disk.</p>
</dd>
<dt><code>plot</code></dt><dd><p>Plot to save, defaults to last plot displayed.</p>
</dd>
<dt><code>device</code></dt><dd><p>Device to use. Can either be a device function
(e.g. <a href="grDevices.html#topic+png">png</a>), or one of &quot;eps&quot;, &quot;ps&quot;, &quot;tex&quot; (pictex),
&quot;pdf&quot;, &quot;jpeg&quot;, &quot;tiff&quot;, &quot;png&quot;, &quot;bmp&quot;, &quot;svg&quot; or &quot;wmf&quot; (windows only).</p>
</dd>
<dt><code>path</code></dt><dd><p>Path of the directory to save plot to: <code>path</code> and <code>filename</code>
are combined to create the fully qualified file name. Defaults to the
working directory.</p>
</dd>
<dt><code>scale</code></dt><dd><p>Multiplicative scaling factor.</p>
</dd>
<dt><code>dpi</code></dt><dd><p>Plot resolution. Also accepts a string input: &quot;retina&quot; (320),
&quot;print&quot; (300), or &quot;screen&quot; (72). Applies only to raster output types.</p>
</dd>
<dt><code>limitsize</code></dt><dd><p>When <code>TRUE</code> (the default), <code>ggsave()</code> will not
save images larger than 50x50 inches, to prevent the common error of
specifying dimensions in pixels.</p>
</dd>
<dt><code>bg</code></dt><dd><p>Background colour. If <code>NULL</code>, uses the <code>plot.background</code> fill value
from the plot theme.</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Details</h3>

<p>This function generates area under the precision-recall curve plots.
</p>
<p>Available splitting variables are: <code>fs_method</code>, <code>learner</code>, <code>data_set</code> and
<code>positive_class</code>. By default, the data is split by <code>fs_method</code> and <code>learner</code>,
with faceting by <code>data_set</code> and colouring by <code>positive_class</code>.
</p>
<p>Available palettes for <code>discrete_palette</code> are those listed by
<code>grDevices::palette.pals()</code> (requires R &gt;= 4.0.0), <code>grDevices::hcl.pals()</code>
(requires R &gt;= 3.6.0) and <code>rainbow</code>, <code>heat.colors</code>, <code>terrain.colors</code>,
<code>topo.colors</code> and <code>cm.colors</code>, which correspond to the palettes of the same
name in <code>grDevices</code>. If not specified, a default palette based on palettes
in Tableau are used. You may also specify your own palette by using colour
names listed by <code>grDevices::colors()</code> or through hexadecimal RGB strings.
</p>
<p>Bootstrap confidence intervals of the ROC curve (if present) can be shown
using various styles set by <code>conf_int_style</code>:
</p>

<ul>
<li> <p><code>ribbon</code> (default): confidence intervals are shown as a ribbon with an
opacity of <code>conf_int_alpha</code> around the point estimate of the ROC curve.
</p>
</li>
<li> <p><code>step</code> (default): confidence intervals are shown as a step function around
the point estimate of the ROC curve.
</p>
</li>
<li> <p><code>none</code>: confidence intervals are not shown. The point estimate of the ROC
curve is shown as usual.
</p>
</li></ul>

<p>Labelling methods such as <code>set_fs_method_names</code> or <code>set_data_set_names</code> can
be applied to the <code>familiarCollection</code> object to update labels, and order
the output in the figure.
</p>


<h3>Value</h3>

<p><code>NULL</code> or list of plot objects, if <code>dir_path</code> is <code>NULL</code>.
</p>

<hr>
<h2 id='plot_auc_roc_curve'>Plot the receiver operating characteristic curve.</h2><span id='topic+plot_auc_roc_curve'></span><span id='topic+plot_auc_roc_curve+2CANY-method'></span><span id='topic+plot_auc_roc_curve+2CfamiliarCollection-method'></span>

<h3>Description</h3>

<p>This method creates receiver operating characteristic curves
based on data in a familiarCollection object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_auc_roc_curve(
  object,
  draw = FALSE,
  dir_path = NULL,
  split_by = NULL,
  color_by = NULL,
  facet_by = NULL,
  facet_wrap_cols = NULL,
  ggtheme = NULL,
  discrete_palette = NULL,
  x_label = waiver(),
  y_label = waiver(),
  legend_label = waiver(),
  plot_title = waiver(),
  plot_sub_title = waiver(),
  caption = NULL,
  x_n_breaks = 5,
  x_breaks = NULL,
  y_n_breaks = 5,
  y_breaks = NULL,
  conf_int_style = c("ribbon", "step", "none"),
  conf_int_alpha = 0.4,
  width = waiver(),
  height = waiver(),
  units = waiver(),
  export_collection = FALSE,
  ...
)

## S4 method for signature 'ANY'
plot_auc_roc_curve(
  object,
  draw = FALSE,
  dir_path = NULL,
  split_by = NULL,
  color_by = NULL,
  facet_by = NULL,
  facet_wrap_cols = NULL,
  ggtheme = NULL,
  discrete_palette = NULL,
  x_label = waiver(),
  y_label = waiver(),
  legend_label = waiver(),
  plot_title = waiver(),
  plot_sub_title = waiver(),
  caption = NULL,
  x_n_breaks = 5,
  x_breaks = NULL,
  y_n_breaks = 5,
  y_breaks = NULL,
  conf_int_style = c("ribbon", "step", "none"),
  conf_int_alpha = 0.4,
  width = waiver(),
  height = waiver(),
  units = waiver(),
  export_collection = FALSE,
  ...
)

## S4 method for signature 'familiarCollection'
plot_auc_roc_curve(
  object,
  draw = FALSE,
  dir_path = NULL,
  split_by = NULL,
  color_by = NULL,
  facet_by = NULL,
  facet_wrap_cols = NULL,
  ggtheme = NULL,
  discrete_palette = NULL,
  x_label = waiver(),
  y_label = waiver(),
  legend_label = waiver(),
  plot_title = waiver(),
  plot_sub_title = waiver(),
  caption = NULL,
  x_n_breaks = 5,
  x_breaks = NULL,
  y_n_breaks = 5,
  y_breaks = NULL,
  conf_int_style = c("ribbon", "step", "none"),
  conf_int_alpha = 0.4,
  width = waiver(),
  height = waiver(),
  units = waiver(),
  export_collection = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_auc_roc_curve_+3A_object">object</code></td>
<td>
<p><code>familiarCollection</code> object, or one or more <code>familiarData</code>
objects, that will be internally converted to a <code>familiarCollection</code> object.
It is also possible to provide a <code>familiarEnsemble</code> or one or more
<code>familiarModel</code> objects together with the data from which data is computed
prior to export. Paths to such files can also be provided.</p>
</td></tr>
<tr><td><code id="plot_auc_roc_curve_+3A_draw">draw</code></td>
<td>
<p>(<em>optional</em>) Draws the plot if TRUE.</p>
</td></tr>
<tr><td><code id="plot_auc_roc_curve_+3A_dir_path">dir_path</code></td>
<td>
<p>(<em>optional</em>) Path to the directory where the plots of receiver
operating characteristic curves are saved to. Output is saved in the
<code>performance</code> subdirectory. If <code>NULL</code> no figures are saved, but are returned
instead.</p>
</td></tr>
<tr><td><code id="plot_auc_roc_curve_+3A_split_by">split_by</code></td>
<td>
<p>(<em>optional</em>) Splitting variables. This refers to column names
on which datasets are split. A separate figure is created for each split.
See details for available variables.</p>
</td></tr>
<tr><td><code id="plot_auc_roc_curve_+3A_color_by">color_by</code></td>
<td>
<p>(<em>optional</em>) Variables used to determine fill colour of plot
objects. The variables cannot overlap with those provided to the <code>split_by</code>
argument, but may overlap with other arguments. See details for available
variables.</p>
</td></tr>
<tr><td><code id="plot_auc_roc_curve_+3A_facet_by">facet_by</code></td>
<td>
<p>(<em>optional</em>) Variables used to determine how and if facets of
each figure appear. In case the <code>facet_wrap_cols</code> argument is <code>NULL</code>, the
first variable is used to define columns, and the remaing variables are
used to define rows of facets. The variables cannot overlap with those
provided to the <code>split_by</code> argument, but may overlap with other arguments.
See details for available variables.</p>
</td></tr>
<tr><td><code id="plot_auc_roc_curve_+3A_facet_wrap_cols">facet_wrap_cols</code></td>
<td>
<p>(<em>optional</em>) Number of columns to generate when facet
wrapping. If NULL, a facet grid is produced instead.</p>
</td></tr>
<tr><td><code id="plot_auc_roc_curve_+3A_ggtheme">ggtheme</code></td>
<td>
<p>(<em>optional</em>) <code>ggplot</code> theme to use for plotting.</p>
</td></tr>
<tr><td><code id="plot_auc_roc_curve_+3A_discrete_palette">discrete_palette</code></td>
<td>
<p>(<em>optional</em>) Palette to use to color the different
plot elements in case a value was provided to the <code>color_by</code> argument.</p>
</td></tr>
<tr><td><code id="plot_auc_roc_curve_+3A_x_label">x_label</code></td>
<td>
<p>(<em>optional</em>) Label to provide to the x-axis. If NULL, no label
is shown.</p>
</td></tr>
<tr><td><code id="plot_auc_roc_curve_+3A_y_label">y_label</code></td>
<td>
<p>(<em>optional</em>) Label to provide to the y-axis. If NULL, no label
is shown.</p>
</td></tr>
<tr><td><code id="plot_auc_roc_curve_+3A_legend_label">legend_label</code></td>
<td>
<p>(<em>optional</em>) Label to provide to the legend. If NULL, the
legend will not have a name.</p>
</td></tr>
<tr><td><code id="plot_auc_roc_curve_+3A_plot_title">plot_title</code></td>
<td>
<p>(<em>optional</em>) Label to provide as figure title. If NULL, no
title is shown.</p>
</td></tr>
<tr><td><code id="plot_auc_roc_curve_+3A_plot_sub_title">plot_sub_title</code></td>
<td>
<p>(<em>optional</em>) Label to provide as figure subtitle. If
NULL, no subtitle is shown.</p>
</td></tr>
<tr><td><code id="plot_auc_roc_curve_+3A_caption">caption</code></td>
<td>
<p>(<em>optional</em>) Label to provide as figure caption. If NULL, no
caption is shown.</p>
</td></tr>
<tr><td><code id="plot_auc_roc_curve_+3A_x_n_breaks">x_n_breaks</code></td>
<td>
<p>(<em>optional</em>) Number of breaks to show on the x-axis of the
plot. <code>x_n_breaks</code> is used to determine the <code>x_breaks</code> argument in case it
is unset.</p>
</td></tr>
<tr><td><code id="plot_auc_roc_curve_+3A_x_breaks">x_breaks</code></td>
<td>
<p>(<em>optional</em>) Break points on the x-axis of the plot.</p>
</td></tr>
<tr><td><code id="plot_auc_roc_curve_+3A_y_n_breaks">y_n_breaks</code></td>
<td>
<p>(<em>optional</em>) Number of breaks to show on the y-axis of the
plot. <code>y_n_breaks</code> is used to determine the <code>y_breaks</code> argument in case it
is unset.</p>
</td></tr>
<tr><td><code id="plot_auc_roc_curve_+3A_y_breaks">y_breaks</code></td>
<td>
<p>(<em>optional</em>) Break points on the y-axis of the plot.</p>
</td></tr>
<tr><td><code id="plot_auc_roc_curve_+3A_conf_int_style">conf_int_style</code></td>
<td>
<p>(<em>optional</em>) Confidence interval style. See details for
allowed styles.</p>
</td></tr>
<tr><td><code id="plot_auc_roc_curve_+3A_conf_int_alpha">conf_int_alpha</code></td>
<td>
<p>(<em>optional</em>) Alpha value to determine transparency of
confidence intervals or, alternatively, other plot elements with which the
confidence interval overlaps. Only values between 0.0 (fully transparent)
and 1.0 (fully opaque) are allowed.</p>
</td></tr>
<tr><td><code id="plot_auc_roc_curve_+3A_width">width</code></td>
<td>
<p>(<em>optional</em>) Width of the plot. A default value is derived from
the number of facets.</p>
</td></tr>
<tr><td><code id="plot_auc_roc_curve_+3A_height">height</code></td>
<td>
<p>(<em>optional</em>) Height of the plot. A default value is derived from
the number of features and the number of facets.</p>
</td></tr>
<tr><td><code id="plot_auc_roc_curve_+3A_units">units</code></td>
<td>
<p>(<em>optional</em>) Plot size unit. Either <code>cm</code> (default), <code>mm</code> or <code style="white-space: pre;">&#8288;in&#8288;</code>.</p>
</td></tr>
<tr><td><code id="plot_auc_roc_curve_+3A_export_collection">export_collection</code></td>
<td>
<p>(<em>optional</em>) Exports the collection if TRUE.</p>
</td></tr>
<tr><td><code id="plot_auc_roc_curve_+3A_...">...</code></td>
<td>

<p>Arguments passed on to <code><a href="#topic+as_familiar_collection">as_familiar_collection</a></code>, <code><a href="ggplot2.html#topic+ggsave">ggplot2::ggsave</a></code>, <code><a href="#topic+extract_auc_data">extract_auc_data</a></code>
</p>

<dl>
<dt><code>familiar_data_names</code></dt><dd><p>Names of the dataset(s). Only used if the <code>object</code> parameter
is one or more <code>familiarData</code> objects.</p>
</dd>
<dt><code>collection_name</code></dt><dd><p>Name of the collection.</p>
</dd>
<dt><code>filename</code></dt><dd><p>File name to create on disk.</p>
</dd>
<dt><code>plot</code></dt><dd><p>Plot to save, defaults to last plot displayed.</p>
</dd>
<dt><code>device</code></dt><dd><p>Device to use. Can either be a device function
(e.g. <a href="grDevices.html#topic+png">png</a>), or one of &quot;eps&quot;, &quot;ps&quot;, &quot;tex&quot; (pictex),
&quot;pdf&quot;, &quot;jpeg&quot;, &quot;tiff&quot;, &quot;png&quot;, &quot;bmp&quot;, &quot;svg&quot; or &quot;wmf&quot; (windows only).</p>
</dd>
<dt><code>path</code></dt><dd><p>Path of the directory to save plot to: <code>path</code> and <code>filename</code>
are combined to create the fully qualified file name. Defaults to the
working directory.</p>
</dd>
<dt><code>scale</code></dt><dd><p>Multiplicative scaling factor.</p>
</dd>
<dt><code>dpi</code></dt><dd><p>Plot resolution. Also accepts a string input: &quot;retina&quot; (320),
&quot;print&quot; (300), or &quot;screen&quot; (72). Applies only to raster output types.</p>
</dd>
<dt><code>limitsize</code></dt><dd><p>When <code>TRUE</code> (the default), <code>ggsave()</code> will not
save images larger than 50x50 inches, to prevent the common error of
specifying dimensions in pixels.</p>
</dd>
<dt><code>bg</code></dt><dd><p>Background colour. If <code>NULL</code>, uses the <code>plot.background</code> fill value
from the plot theme.</p>
</dd>
<dt><code>data</code></dt><dd><p>A <code>dataObject</code> object, <code>data.table</code> or <code>data.frame</code> that
constitutes the data that are assessed.</p>
</dd>
<dt><code>is_pre_processed</code></dt><dd><p>Flag that indicates whether the data was already
pre-processed externally, e.g. normalised and clustered. Only used if the
<code>data</code> argument is a <code>data.table</code> or <code>data.frame</code>.</p>
</dd>
<dt><code>cl</code></dt><dd><p>Cluster created using the <code>parallel</code> package. This cluster is then
used to speed up computation through parallellisation.</p>
</dd>
<dt><code>ensemble_method</code></dt><dd><p>Method for ensembling predictions from models for the
same sample. Available methods are:
</p>

<ul>
<li> <p><code>median</code> (default): Use the median of the predicted values as the ensemble
value for a sample.
</p>
</li>
<li> <p><code>mean</code>: Use the mean of the predicted values as the ensemble value for a
sample.
</p>
</li></ul>
</dd>
<dt><code>verbose</code></dt><dd><p>Flag to indicate whether feedback should be provided on the
computation and extraction of various data elements.</p>
</dd>
<dt><code>message_indent</code></dt><dd><p>Number of indentation steps for messages shown during
computation and extraction of various data elements.</p>
</dd>
<dt><code>detail_level</code></dt><dd><p>(<em>optional</em>) Sets the level at which results are computed
and aggregated.
</p>

<ul>
<li> <p><code>ensemble</code>: Results are computed at the ensemble level, i.e. over all
models in the ensemble. This means that, for example, bias-corrected
estimates of model performance are assessed by creating (at least) 20
bootstraps and computing the model performance of the ensemble model for
each bootstrap.
</p>
</li>
<li> <p><code>hybrid</code> (default): Results are computed at the level of models in an
ensemble. This means that, for example, bias-corrected estimates of model
performance are directly computed using the models in the ensemble. If there
are at least 20 trained models in the ensemble, performance is computed for
each model, in contrast to <code>ensemble</code> where performance is computed for the
ensemble of models. If there are less than 20 trained models in the
ensemble, bootstraps are created so that at least 20 point estimates can be
made.
</p>
</li>
<li> <p><code>model</code>: Results are computed at the model level. This means that, for
example, bias-corrected estimates of model performance are assessed by
creating (at least) 20 bootstraps and computing the performance of the model
for each bootstrap.
</p>
</li></ul>

<p>Note that each level of detail has a different interpretation for bootstrap
confidence intervals. For <code>ensemble</code> and <code>model</code> these are the confidence
intervals for the ensemble and an individual model, respectively. That is,
the confidence interval describes the range where an estimate produced by a
respective ensemble or model trained on a repeat of the experiment may be
found with the probability of the confidence level. For <code>hybrid</code>, it
represents the range where any single model trained on a repeat of the
experiment may be found with the probability of the confidence level. By
definition, confidence intervals obtained using <code>hybrid</code> are at least as
wide as those for <code>ensemble</code>. <code>hybrid</code> offers the correct interpretation if
the goal of the analysis is to assess the result of a single, unspecified,
model.
</p>
<p><code>hybrid</code> is generally computationally less expensive then <code>ensemble</code>, which
in turn is somewhat less expensive than <code>model</code>.
</p>
<p>A non-default <code>detail_level</code> parameter can be specified for separate
evaluation steps by providing a parameter value in a named list with data
elements, e.g. <code>list("auc_data"="ensemble", "model_performance"="hybrid")</code>.
This parameter can be set for the following data elements: <code>auc_data</code>,
<code>decision_curve_analyis</code>, <code>model_performance</code>, <code>permutation_vimp</code>,
<code>ice_data</code>, <code>prediction_data</code> and <code>confusion_matrix</code>.</p>
</dd>
<dt><code>estimation_type</code></dt><dd><p>(<em>optional</em>) Sets the type of estimation that should be
possible. This has the following options:
</p>

<ul>
<li> <p><code>point</code>: Point estimates.
</p>
</li>
<li> <p><code>bias_correction</code> or <code>bc</code>: Bias-corrected estimates. A bias-corrected
estimate is computed from (at least) 20 point estimates, and <code>familiar</code> may
bootstrap the data to create them.
</p>
</li>
<li> <p><code>bootstrap_confidence_interval</code> or <code>bci</code> (default): Bias-corrected
estimates with bootstrap confidence intervals (Efron and Hastie, 2016). The
number of point estimates required depends on the <code>confidence_level</code>
parameter, and <code>familiar</code> may bootstrap the data to create them.
</p>
</li></ul>

<p>As with <code>detail_level</code>, a non-default <code>estimation_type</code> parameter can be
specified for separate evaluation steps by providing a parameter value in a
named list with data elements, e.g. <code>list("auc_data"="bci", "model_performance"="point")</code>. This parameter can be set for the following
data elements: <code>auc_data</code>, <code>decision_curve_analyis</code>, <code>model_performance</code>,
<code>permutation_vimp</code>, <code>ice_data</code>, and <code>prediction_data</code>.</p>
</dd>
<dt><code>aggregate_results</code></dt><dd><p>(<em>optional</em>) Flag that signifies whether results
should be aggregated during evaluation. If <code>estimation_type</code> is
<code>bias_correction</code> or <code>bc</code>, aggregation leads to a single bias-corrected
estimate. If <code>estimation_type</code> is <code>bootstrap_confidence_interval</code> or <code>bci</code>,
aggregation leads to a single bias-corrected estimate with lower and upper
boundaries of the confidence interval. This has no effect if
<code>estimation_type</code> is <code>point</code>.
</p>
<p>The default value is equal to <code>TRUE</code> except when assessing metrics to assess
model performance, as the default violin plot requires underlying data.
</p>
<p>As with <code>detail_level</code> and <code>estimation_type</code>, a non-default
<code>aggregate_results</code> parameter can be specified for separate evaluation steps
by providing a parameter value in a named list with data elements, e.g.
<code>list("auc_data"=TRUE, , "model_performance"=FALSE)</code>. This parameter exists
for the same elements as <code>estimation_type</code>.</p>
</dd>
<dt><code>confidence_level</code></dt><dd><p>(<em>optional</em>) Numeric value for the level at which
confidence intervals are determined. In the case bootstraps are used to
determine the confidence intervals bootstrap estimation, <code>familiar</code> uses the
rule of thumb <code class="reqn">n = 20 / ci.level</code> to determine the number of required
bootstraps.
</p>
<p>The default value is <code>0.95</code>.</p>
</dd>
<dt><code>bootstrap_ci_method</code></dt><dd><p>(<em>optional</em>) Method used to determine bootstrap
confidence intervals (Efron and Hastie, 2016). The following methods are
implemented:
</p>

<ul>
<li> <p><code>percentile</code> (default): Confidence intervals obtained using the percentile
method.
</p>
</li>
<li> <p><code>bc</code>: Bias-corrected confidence intervals.
</p>
</li></ul>

<p>Note that the standard method is not implemented because this method is
often not suitable due to non-normal distributions. The bias-corrected and
accelerated (BCa) method is not implemented yet.</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Details</h3>

<p>This function generates area under the ROC curve plots.
</p>
<p>Available splitting variables are: <code>fs_method</code>, <code>learner</code>, <code>data_set</code> and
<code>positive_class</code>. By default, the data is split by <code>fs_method</code> and <code>learner</code>,
with faceting by <code>data_set</code> and colouring by <code>positive_class</code>.
</p>
<p>Available palettes for <code>discrete_palette</code> are those listed by
<code>grDevices::palette.pals()</code> (requires R &gt;= 4.0.0), <code>grDevices::hcl.pals()</code>
(requires R &gt;= 3.6.0) and <code>rainbow</code>, <code>heat.colors</code>, <code>terrain.colors</code>,
<code>topo.colors</code> and <code>cm.colors</code>, which correspond to the palettes of the same
name in <code>grDevices</code>. If not specified, a default palette based on palettes
in Tableau are used. You may also specify your own palette by using colour
names listed by <code>grDevices::colors()</code> or through hexadecimal RGB strings.
</p>
<p>Bootstrap confidence intervals of the ROC curve (if present) can be shown
using various styles set by <code>conf_int_style</code>:
</p>

<ul>
<li> <p><code>ribbon</code> (default): confidence intervals are shown as a ribbon with an
opacity of <code>conf_int_alpha</code> around the point estimate of the ROC curve.
</p>
</li>
<li> <p><code>step</code> (default): confidence intervals are shown as a step function around
the point estimate of the ROC curve.
</p>
</li>
<li> <p><code>none</code>: confidence intervals are not shown. The point estimate of the ROC
curve is shown as usual.
</p>
</li></ul>

<p>Labelling methods such as <code>set_fs_method_names</code> or <code>set_data_set_names</code> can
be applied to the <code>familiarCollection</code> object to update labels, and order
the output in the figure.
</p>


<h3>Value</h3>

<p><code>NULL</code> or list of plot objects, if <code>dir_path</code> is <code>NULL</code>.
</p>

<hr>
<h2 id='plot_calibration_data'>Plot calibration figures.</h2><span id='topic+plot_calibration_data'></span><span id='topic+plot_calibration_data+2CANY-method'></span><span id='topic+plot_calibration_data+2CfamiliarCollection-method'></span>

<h3>Description</h3>

<p>This method creates calibration plots from calibration data
stored in a familiarCollection object. For this figures, the expected
(predicted) values are plotted against the observed values. A
well-calibrated model should be close to the identity line.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_calibration_data(
  object,
  draw = FALSE,
  dir_path = NULL,
  split_by = NULL,
  color_by = NULL,
  facet_by = NULL,
  facet_wrap_cols = NULL,
  ggtheme = NULL,
  discrete_palette = NULL,
  x_label = waiver(),
  x_label_shared = "column",
  y_label = waiver(),
  y_label_shared = "row",
  legend_label = waiver(),
  plot_title = waiver(),
  plot_sub_title = waiver(),
  caption = NULL,
  x_range = NULL,
  x_n_breaks = 5,
  x_breaks = NULL,
  y_range = NULL,
  y_n_breaks = 5,
  y_breaks = NULL,
  conf_int_style = c("ribbon", "step", "none"),
  conf_int_alpha = 0.4,
  show_density = TRUE,
  show_calibration_fit = TRUE,
  show_goodness_of_fit = TRUE,
  density_plot_height = grid::unit(1, "cm"),
  width = waiver(),
  height = waiver(),
  units = waiver(),
  export_collection = FALSE,
  ...
)

## S4 method for signature 'ANY'
plot_calibration_data(
  object,
  draw = FALSE,
  dir_path = NULL,
  split_by = NULL,
  color_by = NULL,
  facet_by = NULL,
  facet_wrap_cols = NULL,
  ggtheme = NULL,
  discrete_palette = NULL,
  x_label = waiver(),
  x_label_shared = "column",
  y_label = waiver(),
  y_label_shared = "row",
  legend_label = waiver(),
  plot_title = waiver(),
  plot_sub_title = waiver(),
  caption = NULL,
  x_range = NULL,
  x_n_breaks = 5,
  x_breaks = NULL,
  y_range = NULL,
  y_n_breaks = 5,
  y_breaks = NULL,
  conf_int_style = c("ribbon", "step", "none"),
  conf_int_alpha = 0.4,
  show_density = TRUE,
  show_calibration_fit = TRUE,
  show_goodness_of_fit = TRUE,
  density_plot_height = grid::unit(1, "cm"),
  width = waiver(),
  height = waiver(),
  units = waiver(),
  export_collection = FALSE,
  ...
)

## S4 method for signature 'familiarCollection'
plot_calibration_data(
  object,
  draw = FALSE,
  dir_path = NULL,
  split_by = NULL,
  color_by = NULL,
  facet_by = NULL,
  facet_wrap_cols = NULL,
  ggtheme = NULL,
  discrete_palette = NULL,
  x_label = waiver(),
  x_label_shared = "column",
  y_label = waiver(),
  y_label_shared = "row",
  legend_label = waiver(),
  plot_title = waiver(),
  plot_sub_title = waiver(),
  caption = NULL,
  x_range = NULL,
  x_n_breaks = 5,
  x_breaks = NULL,
  y_range = NULL,
  y_n_breaks = 5,
  y_breaks = NULL,
  conf_int_style = c("ribbon", "step", "none"),
  conf_int_alpha = 0.4,
  show_density = TRUE,
  show_calibration_fit = TRUE,
  show_goodness_of_fit = TRUE,
  density_plot_height = grid::unit(1, "cm"),
  width = waiver(),
  height = waiver(),
  units = waiver(),
  export_collection = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_calibration_data_+3A_object">object</code></td>
<td>
<p><code>familiarCollection</code> object, or one or more <code>familiarData</code>
objects, that will be internally converted to a <code>familiarCollection</code> object.
It is also possible to provide a <code>familiarEnsemble</code> or one or more
<code>familiarModel</code> objects together with the data from which data is computed
prior to export. Paths to such files can also be provided.</p>
</td></tr>
<tr><td><code id="plot_calibration_data_+3A_draw">draw</code></td>
<td>
<p>(<em>optional</em>) Draws the plot if TRUE.</p>
</td></tr>
<tr><td><code id="plot_calibration_data_+3A_dir_path">dir_path</code></td>
<td>
<p>(<em>optional</em>) Path to the directory where created calibration
plots are saved to. Output is saved in the <code>calibration</code> subdirectory. If
<code>NULL</code> no figures are saved, but are returned instead.</p>
</td></tr>
<tr><td><code id="plot_calibration_data_+3A_split_by">split_by</code></td>
<td>
<p>(<em>optional</em>) Splitting variables. This refers to column names
on which datasets are split. A separate figure is created for each split.
See details for available variables.</p>
</td></tr>
<tr><td><code id="plot_calibration_data_+3A_color_by">color_by</code></td>
<td>
<p>(<em>optional</em>) Variables used to determine fill colour of plot
objects. The variables cannot overlap with those provided to the <code>split_by</code>
argument, but may overlap with other arguments. See details for available
variables.</p>
</td></tr>
<tr><td><code id="plot_calibration_data_+3A_facet_by">facet_by</code></td>
<td>
<p>(<em>optional</em>) Variables used to determine how and if facets of
each figure appear. In case the <code>facet_wrap_cols</code> argument is <code>NULL</code>, the
first variable is used to define columns, and the remaing variables are
used to define rows of facets. The variables cannot overlap with those
provided to the <code>split_by</code> argument, but may overlap with other arguments.
See details for available variables.</p>
</td></tr>
<tr><td><code id="plot_calibration_data_+3A_facet_wrap_cols">facet_wrap_cols</code></td>
<td>
<p>(<em>optional</em>) Number of columns to generate when facet
wrapping. If NULL, a facet grid is produced instead.</p>
</td></tr>
<tr><td><code id="plot_calibration_data_+3A_ggtheme">ggtheme</code></td>
<td>
<p>(<em>optional</em>) <code>ggplot</code> theme to use for plotting.</p>
</td></tr>
<tr><td><code id="plot_calibration_data_+3A_discrete_palette">discrete_palette</code></td>
<td>
<p>(<em>optional</em>) Palette to use to color the different
data points and fit lines in case a non-singular variable was provided to
the <code>color_by</code> argument.</p>
</td></tr>
<tr><td><code id="plot_calibration_data_+3A_x_label">x_label</code></td>
<td>
<p>(<em>optional</em>) Label to provide to the x-axis. If NULL, no label
is shown.</p>
</td></tr>
<tr><td><code id="plot_calibration_data_+3A_x_label_shared">x_label_shared</code></td>
<td>
<p>(<em>optional</em>) Sharing of x-axis labels between facets.
One of three values:
</p>

<ul>
<li> <p><code>overall</code>: A single label is placed at the bottom of the figure. Tick
text (but not the ticks themselves) is removed for all but the bottom facet
plot(s).
</p>
</li>
<li> <p><code>column</code>: A label is placed at the bottom of each column. Tick text (but
not the ticks themselves) is removed for all but the bottom facet plot(s).
</p>
</li>
<li> <p><code>individual</code>: A label is placed below each facet plot. Tick text is kept.
</p>
</li></ul>
</td></tr>
<tr><td><code id="plot_calibration_data_+3A_y_label">y_label</code></td>
<td>
<p>(<em>optional</em>) Label to provide to the y-axis. If NULL, no label
is shown.</p>
</td></tr>
<tr><td><code id="plot_calibration_data_+3A_y_label_shared">y_label_shared</code></td>
<td>
<p>(<em>optional</em>) Sharing of y-axis labels between facets.
One of three values:
</p>

<ul>
<li> <p><code>overall</code>: A single label is placed to the left of the figure. Tick text
(but not the ticks themselves) is removed for all but the left-most facet
plot(s).
</p>
</li>
<li> <p><code>row</code>: A label is placed to the left of each row. Tick text (but not the
ticks themselves) is removed for all but the left-most facet plot(s).
</p>
</li>
<li> <p><code>individual</code>: A label is placed below each facet plot. Tick text is kept.
</p>
</li></ul>
</td></tr>
<tr><td><code id="plot_calibration_data_+3A_legend_label">legend_label</code></td>
<td>
<p>(<em>optional</em>) Label to provide to the legend. If NULL, the
legend will not have a name.</p>
</td></tr>
<tr><td><code id="plot_calibration_data_+3A_plot_title">plot_title</code></td>
<td>
<p>(<em>optional</em>) Label to provide as figure title. If NULL, no
title is shown.</p>
</td></tr>
<tr><td><code id="plot_calibration_data_+3A_plot_sub_title">plot_sub_title</code></td>
<td>
<p>(<em>optional</em>) Label to provide as figure subtitle. If
NULL, no subtitle is shown.</p>
</td></tr>
<tr><td><code id="plot_calibration_data_+3A_caption">caption</code></td>
<td>
<p>(<em>optional</em>) Label to provide as figure caption. If NULL, no
caption is shown.</p>
</td></tr>
<tr><td><code id="plot_calibration_data_+3A_x_range">x_range</code></td>
<td>
<p>(<em>optional</em>) Value range for the x-axis.</p>
</td></tr>
<tr><td><code id="plot_calibration_data_+3A_x_n_breaks">x_n_breaks</code></td>
<td>
<p>(<em>optional</em>) Number of breaks to show on the x-axis of the
plot. <code>x_n_breaks</code> is used to determine the <code>x_breaks</code> argument in case it
is unset.</p>
</td></tr>
<tr><td><code id="plot_calibration_data_+3A_x_breaks">x_breaks</code></td>
<td>
<p>(<em>optional</em>) Break points on the x-axis of the plot.</p>
</td></tr>
<tr><td><code id="plot_calibration_data_+3A_y_range">y_range</code></td>
<td>
<p>(<em>optional</em>) Value range for the y-axis.</p>
</td></tr>
<tr><td><code id="plot_calibration_data_+3A_y_n_breaks">y_n_breaks</code></td>
<td>
<p>(<em>optional</em>) Number of breaks to show on the y-axis of the
plot. <code>y_n_breaks</code> is used to determine the <code>y_breaks</code> argument in case it
is unset.</p>
</td></tr>
<tr><td><code id="plot_calibration_data_+3A_y_breaks">y_breaks</code></td>
<td>
<p>(<em>optional</em>) Break points on the y-axis of the plot.</p>
</td></tr>
<tr><td><code id="plot_calibration_data_+3A_conf_int_style">conf_int_style</code></td>
<td>
<p>(<em>optional</em>) Confidence interval style. See details for
allowed styles.</p>
</td></tr>
<tr><td><code id="plot_calibration_data_+3A_conf_int_alpha">conf_int_alpha</code></td>
<td>
<p>(<em>optional</em>) Alpha value to determine transparency of
confidence intervals or, alternatively, other plot elements with which the
confidence interval overlaps. Only values between 0.0 (fully transparent)
and 1.0 (fully opaque) are allowed.</p>
</td></tr>
<tr><td><code id="plot_calibration_data_+3A_show_density">show_density</code></td>
<td>
<p>(<em>optional</em>) Show point density in top margin of the
figure. If <code>color_by</code> is set, this information will not be shown.</p>
</td></tr>
<tr><td><code id="plot_calibration_data_+3A_show_calibration_fit">show_calibration_fit</code></td>
<td>
<p>(<em>optional</em>) Specifies whether the calibration in
the large and calibration slope are annotated in the plot. If <code>color_by</code> is
set, this information will not be shown.</p>
</td></tr>
<tr><td><code id="plot_calibration_data_+3A_show_goodness_of_fit">show_goodness_of_fit</code></td>
<td>
<p>(<em>optional</em>) Specifies whether a the results of
goodness of fit tests are annotated in the plot. If <code>color_by</code> is set, this
information will not be shown.</p>
</td></tr>
<tr><td><code id="plot_calibration_data_+3A_density_plot_height">density_plot_height</code></td>
<td>
<p>(<em>optional</em>) Height of the density plot. The height
is 1.5 cm by default. Height is expected to be grid unit (see <code>grid::unit</code>),
which also allows for specifying relative heights. Will be ignored if
<code>show_density</code> is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="plot_calibration_data_+3A_width">width</code></td>
<td>
<p>(<em>optional</em>) Width of the plot. A default value is derived from
the number of facets.</p>
</td></tr>
<tr><td><code id="plot_calibration_data_+3A_height">height</code></td>
<td>
<p>(<em>optional</em>) Height of the plot. A default value is derived from
the number of features and the number of facets.</p>
</td></tr>
<tr><td><code id="plot_calibration_data_+3A_units">units</code></td>
<td>
<p>(<em>optional</em>) Plot size unit. Either <code>cm</code> (default), <code>mm</code> or <code style="white-space: pre;">&#8288;in&#8288;</code>.</p>
</td></tr>
<tr><td><code id="plot_calibration_data_+3A_export_collection">export_collection</code></td>
<td>
<p>(<em>optional</em>) Exports the collection if TRUE.</p>
</td></tr>
<tr><td><code id="plot_calibration_data_+3A_...">...</code></td>
<td>

<p>Arguments passed on to <code><a href="#topic+as_familiar_collection">as_familiar_collection</a></code>, <code><a href="ggplot2.html#topic+ggsave">ggplot2::ggsave</a></code>, <code><a href="#topic+extract_calibration_data">extract_calibration_data</a></code>
</p>

<dl>
<dt><code>familiar_data_names</code></dt><dd><p>Names of the dataset(s). Only used if the <code>object</code> parameter
is one or more <code>familiarData</code> objects.</p>
</dd>
<dt><code>collection_name</code></dt><dd><p>Name of the collection.</p>
</dd>
<dt><code>filename</code></dt><dd><p>File name to create on disk.</p>
</dd>
<dt><code>plot</code></dt><dd><p>Plot to save, defaults to last plot displayed.</p>
</dd>
<dt><code>device</code></dt><dd><p>Device to use. Can either be a device function
(e.g. <a href="grDevices.html#topic+png">png</a>), or one of &quot;eps&quot;, &quot;ps&quot;, &quot;tex&quot; (pictex),
&quot;pdf&quot;, &quot;jpeg&quot;, &quot;tiff&quot;, &quot;png&quot;, &quot;bmp&quot;, &quot;svg&quot; or &quot;wmf&quot; (windows only).</p>
</dd>
<dt><code>path</code></dt><dd><p>Path of the directory to save plot to: <code>path</code> and <code>filename</code>
are combined to create the fully qualified file name. Defaults to the
working directory.</p>
</dd>
<dt><code>scale</code></dt><dd><p>Multiplicative scaling factor.</p>
</dd>
<dt><code>dpi</code></dt><dd><p>Plot resolution. Also accepts a string input: &quot;retina&quot; (320),
&quot;print&quot; (300), or &quot;screen&quot; (72). Applies only to raster output types.</p>
</dd>
<dt><code>limitsize</code></dt><dd><p>When <code>TRUE</code> (the default), <code>ggsave()</code> will not
save images larger than 50x50 inches, to prevent the common error of
specifying dimensions in pixels.</p>
</dd>
<dt><code>bg</code></dt><dd><p>Background colour. If <code>NULL</code>, uses the <code>plot.background</code> fill value
from the plot theme.</p>
</dd>
<dt><code>data</code></dt><dd><p>A <code>dataObject</code> object, <code>data.table</code> or <code>data.frame</code> that
constitutes the data that are assessed.</p>
</dd>
<dt><code>is_pre_processed</code></dt><dd><p>Flag that indicates whether the data was already
pre-processed externally, e.g. normalised and clustered. Only used if the
<code>data</code> argument is a <code>data.table</code> or <code>data.frame</code>.</p>
</dd>
<dt><code>cl</code></dt><dd><p>Cluster created using the <code>parallel</code> package. This cluster is then
used to speed up computation through parallellisation.</p>
</dd>
<dt><code>evaluation_times</code></dt><dd><p>One or more time points that are used for in analysis of
survival problems when data has to be assessed at a set time, e.g.
calibration. If not provided explicitly, this parameter is read from
settings used at creation of the underlying <code>familiarModel</code> objects. Only
used for <code>survival</code> outcomes.</p>
</dd>
<dt><code>ensemble_method</code></dt><dd><p>Method for ensembling predictions from models for the
same sample. Available methods are:
</p>

<ul>
<li> <p><code>median</code> (default): Use the median of the predicted values as the ensemble
value for a sample.
</p>
</li>
<li> <p><code>mean</code>: Use the mean of the predicted values as the ensemble value for a
sample.
</p>
</li></ul>
</dd>
<dt><code>verbose</code></dt><dd><p>Flag to indicate whether feedback should be provided on the
computation and extraction of various data elements.</p>
</dd>
<dt><code>message_indent</code></dt><dd><p>Number of indentation steps for messages shown during
computation and extraction of various data elements.</p>
</dd>
<dt><code>detail_level</code></dt><dd><p>(<em>optional</em>) Sets the level at which results are computed
and aggregated.
</p>

<ul>
<li> <p><code>ensemble</code>: Results are computed at the ensemble level, i.e. over all
models in the ensemble. This means that, for example, bias-corrected
estimates of model performance are assessed by creating (at least) 20
bootstraps and computing the model performance of the ensemble model for
each bootstrap.
</p>
</li>
<li> <p><code>hybrid</code> (default): Results are computed at the level of models in an
ensemble. This means that, for example, bias-corrected estimates of model
performance are directly computed using the models in the ensemble. If there
are at least 20 trained models in the ensemble, performance is computed for
each model, in contrast to <code>ensemble</code> where performance is computed for the
ensemble of models. If there are less than 20 trained models in the
ensemble, bootstraps are created so that at least 20 point estimates can be
made.
</p>
</li>
<li> <p><code>model</code>: Results are computed at the model level. This means that, for
example, bias-corrected estimates of model performance are assessed by
creating (at least) 20 bootstraps and computing the performance of the model
for each bootstrap.
</p>
</li></ul>

<p>Note that each level of detail has a different interpretation for bootstrap
confidence intervals. For <code>ensemble</code> and <code>model</code> these are the confidence
intervals for the ensemble and an individual model, respectively. That is,
the confidence interval describes the range where an estimate produced by a
respective ensemble or model trained on a repeat of the experiment may be
found with the probability of the confidence level. For <code>hybrid</code>, it
represents the range where any single model trained on a repeat of the
experiment may be found with the probability of the confidence level. By
definition, confidence intervals obtained using <code>hybrid</code> are at least as
wide as those for <code>ensemble</code>. <code>hybrid</code> offers the correct interpretation if
the goal of the analysis is to assess the result of a single, unspecified,
model.
</p>
<p><code>hybrid</code> is generally computationally less expensive then <code>ensemble</code>, which
in turn is somewhat less expensive than <code>model</code>.
</p>
<p>A non-default <code>detail_level</code> parameter can be specified for separate
evaluation steps by providing a parameter value in a named list with data
elements, e.g. <code>list("auc_data"="ensemble", "model_performance"="hybrid")</code>.
This parameter can be set for the following data elements: <code>auc_data</code>,
<code>decision_curve_analyis</code>, <code>model_performance</code>, <code>permutation_vimp</code>,
<code>ice_data</code>, <code>prediction_data</code> and <code>confusion_matrix</code>.</p>
</dd>
<dt><code>estimation_type</code></dt><dd><p>(<em>optional</em>) Sets the type of estimation that should be
possible. This has the following options:
</p>

<ul>
<li> <p><code>point</code>: Point estimates.
</p>
</li>
<li> <p><code>bias_correction</code> or <code>bc</code>: Bias-corrected estimates. A bias-corrected
estimate is computed from (at least) 20 point estimates, and <code>familiar</code> may
bootstrap the data to create them.
</p>
</li>
<li> <p><code>bootstrap_confidence_interval</code> or <code>bci</code> (default): Bias-corrected
estimates with bootstrap confidence intervals (Efron and Hastie, 2016). The
number of point estimates required depends on the <code>confidence_level</code>
parameter, and <code>familiar</code> may bootstrap the data to create them.
</p>
</li></ul>

<p>As with <code>detail_level</code>, a non-default <code>estimation_type</code> parameter can be
specified for separate evaluation steps by providing a parameter value in a
named list with data elements, e.g. <code>list("auc_data"="bci", "model_performance"="point")</code>. This parameter can be set for the following
data elements: <code>auc_data</code>, <code>decision_curve_analyis</code>, <code>model_performance</code>,
<code>permutation_vimp</code>, <code>ice_data</code>, and <code>prediction_data</code>.</p>
</dd>
<dt><code>aggregate_results</code></dt><dd><p>(<em>optional</em>) Flag that signifies whether results
should be aggregated during evaluation. If <code>estimation_type</code> is
<code>bias_correction</code> or <code>bc</code>, aggregation leads to a single bias-corrected
estimate. If <code>estimation_type</code> is <code>bootstrap_confidence_interval</code> or <code>bci</code>,
aggregation leads to a single bias-corrected estimate with lower and upper
boundaries of the confidence interval. This has no effect if
<code>estimation_type</code> is <code>point</code>.
</p>
<p>The default value is equal to <code>TRUE</code> except when assessing metrics to assess
model performance, as the default violin plot requires underlying data.
</p>
<p>As with <code>detail_level</code> and <code>estimation_type</code>, a non-default
<code>aggregate_results</code> parameter can be specified for separate evaluation steps
by providing a parameter value in a named list with data elements, e.g.
<code>list("auc_data"=TRUE, , "model_performance"=FALSE)</code>. This parameter exists
for the same elements as <code>estimation_type</code>.</p>
</dd>
<dt><code>confidence_level</code></dt><dd><p>(<em>optional</em>) Numeric value for the level at which
confidence intervals are determined. In the case bootstraps are used to
determine the confidence intervals bootstrap estimation, <code>familiar</code> uses the
rule of thumb <code class="reqn">n = 20 / ci.level</code> to determine the number of required
bootstraps.
</p>
<p>The default value is <code>0.95</code>.</p>
</dd>
<dt><code>bootstrap_ci_method</code></dt><dd><p>(<em>optional</em>) Method used to determine bootstrap
confidence intervals (Efron and Hastie, 2016). The following methods are
implemented:
</p>

<ul>
<li> <p><code>percentile</code> (default): Confidence intervals obtained using the percentile
method.
</p>
</li>
<li> <p><code>bc</code>: Bias-corrected confidence intervals.
</p>
</li></ul>

<p>Note that the standard method is not implemented because this method is
often not suitable due to non-normal distributions. The bias-corrected and
accelerated (BCa) method is not implemented yet.</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Details</h3>

<p>This function generates a calibration plot for each model in each
dataset. Any data used for calibration (e.g. baseline survival) is obtained
during model creation.
</p>
<p>Available splitting variables are: <code>fs_method</code>, <code>learner</code>, <code>data_set</code> and
<code>evaluation_time</code> (survival analysis only) and <code>positive_class</code> (multinomial
endpoints only). By default, separate figures are created for each
combination of <code>fs_method</code> and <code>learner</code>, with facetting by <code>data_set</code>.
</p>
<p>Calibration in survival analysis is performed at set time points so that
survival probabilities can be computed from the model, and compared with
observed survival probabilities. This is done differently depending on the
underlying model. For Cox partial hazards regression models, the base
survival (of the development samples) are used, whereas accelerated failure
time models (e.g. Weibull) and survival random forests can be used to
directly predict survival probabilities at a given time point. For survival
analysis, <code>evaluation_time</code> is an additional facet variable (by default).
</p>
<p>Calibration for multinomial endpoints is performed in a one-against-all
manner. This yields calibration information for each individual class of the
endpoint. For such endpoints, <code>positive_class</code> is an additional facet variable
(by default).
</p>
<p>Calibration plots have a density plot in the margin, which shows the density
of the plotted points, ordered by the expected probability or value. For
binomial and multinomial outcomes, the density for positive and negative
classes are shown separately. Note that this information is only provided in
when <code>color_by</code> is not used as a splitting variable (i.e. one calibration
plot per facet).
</p>
<p>Calibration plots are annotated with the intercept and the slope of a linear
model fitted to the sample points. A well-calibrated model has an intercept
close to 0.0 and a slope of 1.0. Intercept and slope are shown with their
respective 95% confidence intervals. In addition, goodness-of-fit tests may
be shown. For most endpoints these are based on the Hosmer-Lemeshow (HL)
test, but for survival endpoints both the Nam-D'Agostino (ND) and the
Greenwood-Nam-D'Agostino (GND) tests are shown. Note that this information
is only annotated when <code>color_by</code> is not used as a splitting variable (i.e.
one calibration plot per facet).
</p>
<p>Available palettes for <code>discrete_palette</code> are those listed by
<code>grDevices::palette.pals()</code> (requires R &gt;= 4.0.0), <code>grDevices::hcl.pals()</code>
(requires R &gt;= 3.6.0) and <code>rainbow</code>, <code>heat.colors</code>, <code>terrain.colors</code>,
<code>topo.colors</code> and <code>cm.colors</code>, which correspond to the palettes of the same
name in <code>grDevices</code>. If not specified, a default palette based on palettes
in Tableau are used. You may also specify your own palette by using colour
names listed by <code>grDevices::colors()</code> or through hexadecimal RGB strings.
</p>
<p>Labeling methods such as <code>set_risk_group_names</code> or <code>set_data_set_names</code> can
be applied to the <code>familiarCollection</code> object to update labels, and order
the output in the figure.
</p>


<h3>Value</h3>

<p><code>NULL</code> or list of plot objects, if <code>dir_path</code> is <code>NULL</code>.
</p>


<h3>References</h3>


<ol>
<li><p> Hosmer, D. W., Hosmer, T., Le Cessie, S. &amp; Lemeshow, S. A
comparison of goodness-of-fit tests for the logistic regression model. Stat.
Med. 16, 965–980 (1997).
</p>
</li>
<li><p> D’Agostino, R. B. &amp; Nam, B.-H. Evaluation of the Performance of Survival
Analysis Models: Discrimination and Calibration Measures. in Handbook of
Statistics vol. 23 1–25 (Elsevier, 2003).
</p>
</li>
<li><p> Demler, O. V., Paynter, N. P. &amp; Cook, N. R. Tests of calibration and
goodness-of-fit in the survival setting. Stat. Med. 34, 1659–1680 (2015).
</p>
</li></ol>


<hr>
<h2 id='plot_confusion_matrix'>Plot confusion matrix.</h2><span id='topic+plot_confusion_matrix'></span><span id='topic+plot_confusion_matrix+2CANY-method'></span><span id='topic+plot_confusion_matrix+2CfamiliarCollection-method'></span>

<h3>Description</h3>

<p>This method creates confusion matrices based on data in a
familiarCollection object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_confusion_matrix(
  object,
  draw = FALSE,
  dir_path = NULL,
  split_by = NULL,
  facet_by = NULL,
  facet_wrap_cols = NULL,
  ggtheme = NULL,
  discrete_palette = NULL,
  x_label = waiver(),
  y_label = waiver(),
  legend_label = waiver(),
  plot_title = waiver(),
  plot_sub_title = waiver(),
  caption = NULL,
  rotate_x_tick_labels = waiver(),
  show_alpha = TRUE,
  width = waiver(),
  height = waiver(),
  units = waiver(),
  export_collection = FALSE,
  ...
)

## S4 method for signature 'ANY'
plot_confusion_matrix(
  object,
  draw = FALSE,
  dir_path = NULL,
  split_by = NULL,
  facet_by = NULL,
  facet_wrap_cols = NULL,
  ggtheme = NULL,
  discrete_palette = NULL,
  x_label = waiver(),
  y_label = waiver(),
  legend_label = waiver(),
  plot_title = waiver(),
  plot_sub_title = waiver(),
  caption = NULL,
  rotate_x_tick_labels = waiver(),
  show_alpha = TRUE,
  width = waiver(),
  height = waiver(),
  units = waiver(),
  export_collection = FALSE,
  ...
)

## S4 method for signature 'familiarCollection'
plot_confusion_matrix(
  object,
  draw = FALSE,
  dir_path = NULL,
  split_by = NULL,
  facet_by = NULL,
  facet_wrap_cols = NULL,
  ggtheme = NULL,
  discrete_palette = NULL,
  x_label = waiver(),
  y_label = waiver(),
  legend_label = waiver(),
  plot_title = waiver(),
  plot_sub_title = waiver(),
  caption = NULL,
  rotate_x_tick_labels = waiver(),
  show_alpha = TRUE,
  width = waiver(),
  height = waiver(),
  units = waiver(),
  export_collection = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_confusion_matrix_+3A_object">object</code></td>
<td>
<p><code>familiarCollection</code> object, or one or more <code>familiarData</code>
objects, that will be internally converted to a <code>familiarCollection</code> object.
It is also possible to provide a <code>familiarEnsemble</code> or one or more
<code>familiarModel</code> objects together with the data from which data is computed
prior to export. Paths to such files can also be provided.</p>
</td></tr>
<tr><td><code id="plot_confusion_matrix_+3A_draw">draw</code></td>
<td>
<p>(<em>optional</em>) Draws the plot if TRUE.</p>
</td></tr>
<tr><td><code id="plot_confusion_matrix_+3A_dir_path">dir_path</code></td>
<td>
<p>(<em>optional</em>) Path to the directory where created confusion
matrixes are saved to. Output is saved in the <code>performance</code> subdirectory. If
<code>NULL</code> no figures are saved, but are returned instead.</p>
</td></tr>
<tr><td><code id="plot_confusion_matrix_+3A_split_by">split_by</code></td>
<td>
<p>(<em>optional</em>) Splitting variables. This refers to column names
on which datasets are split. A separate figure is created for each split.
See details for available variables.</p>
</td></tr>
<tr><td><code id="plot_confusion_matrix_+3A_facet_by">facet_by</code></td>
<td>
<p>(<em>optional</em>) Variables used to determine how and if facets of
each figure appear. In case the <code>facet_wrap_cols</code> argument is <code>NULL</code>, the
first variable is used to define columns, and the remaing variables are
used to define rows of facets. The variables cannot overlap with those
provided to the <code>split_by</code> argument, but may overlap with other arguments.
See details for available variables.</p>
</td></tr>
<tr><td><code id="plot_confusion_matrix_+3A_facet_wrap_cols">facet_wrap_cols</code></td>
<td>
<p>(<em>optional</em>) Number of columns to generate when facet
wrapping. If NULL, a facet grid is produced instead.</p>
</td></tr>
<tr><td><code id="plot_confusion_matrix_+3A_ggtheme">ggtheme</code></td>
<td>
<p>(<em>optional</em>) <code>ggplot</code> theme to use for plotting.</p>
</td></tr>
<tr><td><code id="plot_confusion_matrix_+3A_discrete_palette">discrete_palette</code></td>
<td>
<p>(<em>optional</em>) Palette used to colour the confusion
matrix. The colour depends on whether each cell of the confusion matrix is
on the diagonal (observed outcome matched expected outcome) or not.</p>
</td></tr>
<tr><td><code id="plot_confusion_matrix_+3A_x_label">x_label</code></td>
<td>
<p>(<em>optional</em>) Label to provide to the x-axis. If NULL, no label
is shown.</p>
</td></tr>
<tr><td><code id="plot_confusion_matrix_+3A_y_label">y_label</code></td>
<td>
<p>(<em>optional</em>) Label to provide to the y-axis. If NULL, no label
is shown.</p>
</td></tr>
<tr><td><code id="plot_confusion_matrix_+3A_legend_label">legend_label</code></td>
<td>
<p>(<em>optional</em>) Label to provide to the legend. If NULL, the
legend will not have a name.</p>
</td></tr>
<tr><td><code id="plot_confusion_matrix_+3A_plot_title">plot_title</code></td>
<td>
<p>(<em>optional</em>) Label to provide as figure title. If NULL, no
title is shown.</p>
</td></tr>
<tr><td><code id="plot_confusion_matrix_+3A_plot_sub_title">plot_sub_title</code></td>
<td>
<p>(<em>optional</em>) Label to provide as figure subtitle. If
NULL, no subtitle is shown.</p>
</td></tr>
<tr><td><code id="plot_confusion_matrix_+3A_caption">caption</code></td>
<td>
<p>(<em>optional</em>) Label to provide as figure caption. If NULL, no
caption is shown.</p>
</td></tr>
<tr><td><code id="plot_confusion_matrix_+3A_rotate_x_tick_labels">rotate_x_tick_labels</code></td>
<td>
<p>(<em>optional</em>) Rotate tick labels on the x-axis by
90 degrees. Defaults to <code>TRUE</code>. Rotation of x-axis tick labels may also be
controlled through the <code>ggtheme</code>. In this case, <code>FALSE</code> should be provided
explicitly.</p>
</td></tr>
<tr><td><code id="plot_confusion_matrix_+3A_show_alpha">show_alpha</code></td>
<td>
<p>(<em>optional</em>) Interpreting confusion matrices is made easier
by setting the opacity of the cells. <code>show_alpha</code> takes the following
values:
</p>

<ul>
<li> <p><code>none</code>: Cell opacity is not altered. Diagonal and off-diagonal cells are
completely opaque and transparent, respectively. Same as <code>show_alpha=FALSE</code>.
</p>
</li>
<li> <p><code>by_class</code>: Cell opacity is normalised by the number of instances for each
observed outcome class in each confusion matrix.
</p>
</li>
<li> <p><code>by_matrix</code> (default): Cell opacity is normalised by the number of
instances in the largest observed outcome class in each confusion matrix.
Same as <code>show_alpha=TRUE</code>
</p>
</li>
<li> <p><code>by_figure</code>: Cell opacity is normalised by the number of instances in the
largest observed outcome class across confusion matrices in different
facets.
</p>
</li>
<li> <p><code>by_all</code>: Cell opacity is normalised by the number of instances in the
largest observed outcome class across all confusion matrices.
</p>
</li></ul>
</td></tr>
<tr><td><code id="plot_confusion_matrix_+3A_width">width</code></td>
<td>
<p>(<em>optional</em>) Width of the plot. A default value is derived from
the number of facets.</p>
</td></tr>
<tr><td><code id="plot_confusion_matrix_+3A_height">height</code></td>
<td>
<p>(<em>optional</em>) Height of the plot. A default value is derived from
the number of features and the number of facets.</p>
</td></tr>
<tr><td><code id="plot_confusion_matrix_+3A_units">units</code></td>
<td>
<p>(<em>optional</em>) Plot size unit. Either <code>cm</code> (default), <code>mm</code> or <code style="white-space: pre;">&#8288;in&#8288;</code>.</p>
</td></tr>
<tr><td><code id="plot_confusion_matrix_+3A_export_collection">export_collection</code></td>
<td>
<p>(<em>optional</em>) Exports the collection if TRUE.</p>
</td></tr>
<tr><td><code id="plot_confusion_matrix_+3A_...">...</code></td>
<td>

<p>Arguments passed on to <code><a href="#topic+as_familiar_collection">as_familiar_collection</a></code>, <code><a href="ggplot2.html#topic+ggsave">ggplot2::ggsave</a></code>, <code><a href="#topic+extract_confusion_matrix">extract_confusion_matrix</a></code>
</p>

<dl>
<dt><code>familiar_data_names</code></dt><dd><p>Names of the dataset(s). Only used if the <code>object</code> parameter
is one or more <code>familiarData</code> objects.</p>
</dd>
<dt><code>collection_name</code></dt><dd><p>Name of the collection.</p>
</dd>
<dt><code>filename</code></dt><dd><p>File name to create on disk.</p>
</dd>
<dt><code>plot</code></dt><dd><p>Plot to save, defaults to last plot displayed.</p>
</dd>
<dt><code>device</code></dt><dd><p>Device to use. Can either be a device function
(e.g. <a href="grDevices.html#topic+png">png</a>), or one of &quot;eps&quot;, &quot;ps&quot;, &quot;tex&quot; (pictex),
&quot;pdf&quot;, &quot;jpeg&quot;, &quot;tiff&quot;, &quot;png&quot;, &quot;bmp&quot;, &quot;svg&quot; or &quot;wmf&quot; (windows only).</p>
</dd>
<dt><code>path</code></dt><dd><p>Path of the directory to save plot to: <code>path</code> and <code>filename</code>
are combined to create the fully qualified file name. Defaults to the
working directory.</p>
</dd>
<dt><code>scale</code></dt><dd><p>Multiplicative scaling factor.</p>
</dd>
<dt><code>dpi</code></dt><dd><p>Plot resolution. Also accepts a string input: &quot;retina&quot; (320),
&quot;print&quot; (300), or &quot;screen&quot; (72). Applies only to raster output types.</p>
</dd>
<dt><code>limitsize</code></dt><dd><p>When <code>TRUE</code> (the default), <code>ggsave()</code> will not
save images larger than 50x50 inches, to prevent the common error of
specifying dimensions in pixels.</p>
</dd>
<dt><code>bg</code></dt><dd><p>Background colour. If <code>NULL</code>, uses the <code>plot.background</code> fill value
from the plot theme.</p>
</dd>
<dt><code>data</code></dt><dd><p>A <code>dataObject</code> object, <code>data.table</code> or <code>data.frame</code> that
constitutes the data that are assessed.</p>
</dd>
<dt><code>is_pre_processed</code></dt><dd><p>Flag that indicates whether the data was already
pre-processed externally, e.g. normalised and clustered. Only used if the
<code>data</code> argument is a <code>data.table</code> or <code>data.frame</code>.</p>
</dd>
<dt><code>cl</code></dt><dd><p>Cluster created using the <code>parallel</code> package. This cluster is then
used to speed up computation through parallellisation.</p>
</dd>
<dt><code>ensemble_method</code></dt><dd><p>Method for ensembling predictions from models for the
same sample. Available methods are:
</p>

<ul>
<li> <p><code>median</code> (default): Use the median of the predicted values as the ensemble
value for a sample.
</p>
</li>
<li> <p><code>mean</code>: Use the mean of the predicted values as the ensemble value for a
sample.
</p>
</li></ul>
</dd>
<dt><code>verbose</code></dt><dd><p>Flag to indicate whether feedback should be provided on the
computation and extraction of various data elements.</p>
</dd>
<dt><code>message_indent</code></dt><dd><p>Number of indentation steps for messages shown during
computation and extraction of various data elements.</p>
</dd>
<dt><code>detail_level</code></dt><dd><p>(<em>optional</em>) Sets the level at which results are computed
and aggregated.
</p>

<ul>
<li> <p><code>ensemble</code>: Results are computed at the ensemble level, i.e. over all
models in the ensemble. This means that, for example, bias-corrected
estimates of model performance are assessed by creating (at least) 20
bootstraps and computing the model performance of the ensemble model for
each bootstrap.
</p>
</li>
<li> <p><code>hybrid</code> (default): Results are computed at the level of models in an
ensemble. This means that, for example, bias-corrected estimates of model
performance are directly computed using the models in the ensemble. If there
are at least 20 trained models in the ensemble, performance is computed for
each model, in contrast to <code>ensemble</code> where performance is computed for the
ensemble of models. If there are less than 20 trained models in the
ensemble, bootstraps are created so that at least 20 point estimates can be
made.
</p>
</li>
<li> <p><code>model</code>: Results are computed at the model level. This means that, for
example, bias-corrected estimates of model performance are assessed by
creating (at least) 20 bootstraps and computing the performance of the model
for each bootstrap.
</p>
</li></ul>

<p>Note that each level of detail has a different interpretation for bootstrap
confidence intervals. For <code>ensemble</code> and <code>model</code> these are the confidence
intervals for the ensemble and an individual model, respectively. That is,
the confidence interval describes the range where an estimate produced by a
respective ensemble or model trained on a repeat of the experiment may be
found with the probability of the confidence level. For <code>hybrid</code>, it
represents the range where any single model trained on a repeat of the
experiment may be found with the probability of the confidence level. By
definition, confidence intervals obtained using <code>hybrid</code> are at least as
wide as those for <code>ensemble</code>. <code>hybrid</code> offers the correct interpretation if
the goal of the analysis is to assess the result of a single, unspecified,
model.
</p>
<p><code>hybrid</code> is generally computationally less expensive then <code>ensemble</code>, which
in turn is somewhat less expensive than <code>model</code>.
</p>
<p>A non-default <code>detail_level</code> parameter can be specified for separate
evaluation steps by providing a parameter value in a named list with data
elements, e.g. <code>list("auc_data"="ensemble", "model_performance"="hybrid")</code>.
This parameter can be set for the following data elements: <code>auc_data</code>,
<code>decision_curve_analyis</code>, <code>model_performance</code>, <code>permutation_vimp</code>,
<code>ice_data</code>, <code>prediction_data</code> and <code>confusion_matrix</code>.</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Details</h3>

<p>This function generates area under the ROC curve plots.
</p>
<p>Available splitting variables are: <code>fs_method</code>, <code>learner</code> and <code>data_set</code>. By
default, the data is split by <code>fs_method</code> and <code>learner</code>, with facetting by
<code>data_set</code>.
</p>
<p>Available palettes for <code>discrete_palette</code> are those listed by
<code>grDevices::palette.pals()</code> (requires R &gt;= 4.0.0), <code>grDevices::hcl.pals()</code>
(requires R &gt;= 3.6.0) and <code>rainbow</code>, <code>heat.colors</code>, <code>terrain.colors</code>,
<code>topo.colors</code> and <code>cm.colors</code>, which correspond to the palettes of the same
name in <code>grDevices</code>. If not specified, a default palette based on palettes
in Tableau are used. You may also specify your own palette by using colour
names listed by <code>grDevices::colors()</code> or through hexadecimal RGB strings.
</p>
<p>Labeling methods such as <code>set_fs_method_names</code> or <code>set_data_set_names</code> can
be applied to the <code>familiarCollection</code> object to update labels, and order
the output in the figure.
</p>


<h3>Value</h3>

<p><code>NULL</code> or list of plot objects, if <code>dir_path</code> is <code>NULL</code>.
</p>

<hr>
<h2 id='plot_decision_curve'>Plot decision curves.</h2><span id='topic+plot_decision_curve'></span><span id='topic+plot_decision_curve+2CANY-method'></span><span id='topic+plot_decision_curve+2CfamiliarCollection-method'></span>

<h3>Description</h3>

<p>This method creates decision curves based on data in a
familiarCollection object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_decision_curve(
  object,
  draw = FALSE,
  dir_path = NULL,
  split_by = NULL,
  color_by = NULL,
  facet_by = NULL,
  facet_wrap_cols = NULL,
  ggtheme = NULL,
  discrete_palette = NULL,
  x_label = waiver(),
  y_label = waiver(),
  legend_label = waiver(),
  plot_title = waiver(),
  plot_sub_title = waiver(),
  caption = NULL,
  x_range = NULL,
  x_n_breaks = 5,
  x_breaks = NULL,
  y_range = NULL,
  y_n_breaks = 5,
  y_breaks = NULL,
  conf_int_style = c("ribbon", "step", "none"),
  conf_int_alpha = 0.4,
  width = waiver(),
  height = waiver(),
  units = waiver(),
  export_collection = FALSE,
  ...
)

## S4 method for signature 'ANY'
plot_decision_curve(
  object,
  draw = FALSE,
  dir_path = NULL,
  split_by = NULL,
  color_by = NULL,
  facet_by = NULL,
  facet_wrap_cols = NULL,
  ggtheme = NULL,
  discrete_palette = NULL,
  x_label = waiver(),
  y_label = waiver(),
  legend_label = waiver(),
  plot_title = waiver(),
  plot_sub_title = waiver(),
  caption = NULL,
  x_range = NULL,
  x_n_breaks = 5,
  x_breaks = NULL,
  y_range = NULL,
  y_n_breaks = 5,
  y_breaks = NULL,
  conf_int_style = c("ribbon", "step", "none"),
  conf_int_alpha = 0.4,
  width = waiver(),
  height = waiver(),
  units = waiver(),
  export_collection = FALSE,
  ...
)

## S4 method for signature 'familiarCollection'
plot_decision_curve(
  object,
  draw = FALSE,
  dir_path = NULL,
  split_by = NULL,
  color_by = NULL,
  facet_by = NULL,
  facet_wrap_cols = NULL,
  ggtheme = NULL,
  discrete_palette = NULL,
  x_label = waiver(),
  y_label = waiver(),
  legend_label = waiver(),
  plot_title = waiver(),
  plot_sub_title = waiver(),
  caption = NULL,
  x_range = NULL,
  x_n_breaks = 5,
  x_breaks = NULL,
  y_range = NULL,
  y_n_breaks = 5,
  y_breaks = NULL,
  conf_int_style = c("ribbon", "step", "none"),
  conf_int_alpha = 0.4,
  width = waiver(),
  height = waiver(),
  units = waiver(),
  export_collection = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_decision_curve_+3A_object">object</code></td>
<td>
<p><code>familiarCollection</code> object, or one or more <code>familiarData</code>
objects, that will be internally converted to a <code>familiarCollection</code> object.
It is also possible to provide a <code>familiarEnsemble</code> or one or more
<code>familiarModel</code> objects together with the data from which data is computed
prior to export. Paths to such files can also be provided.</p>
</td></tr>
<tr><td><code id="plot_decision_curve_+3A_draw">draw</code></td>
<td>
<p>(<em>optional</em>) Draws the plot if TRUE.</p>
</td></tr>
<tr><td><code id="plot_decision_curve_+3A_dir_path">dir_path</code></td>
<td>
<p>(<em>optional</em>) Path to the directory where created decision
curve plots are saved to. Output is saved in the <code>decision_curve_analysis</code>
subdirectory. If <code>NULL</code>, figures are written to the folder, but are returned
instead.</p>
</td></tr>
<tr><td><code id="plot_decision_curve_+3A_split_by">split_by</code></td>
<td>
<p>(<em>optional</em>) Splitting variables. This refers to column names
on which datasets are split. A separate figure is created for each split.
See details for available variables.</p>
</td></tr>
<tr><td><code id="plot_decision_curve_+3A_color_by">color_by</code></td>
<td>
<p>(<em>optional</em>) Variables used to determine fill colour of plot
objects. The variables cannot overlap with those provided to the <code>split_by</code>
argument, but may overlap with other arguments. See details for available
variables.</p>
</td></tr>
<tr><td><code id="plot_decision_curve_+3A_facet_by">facet_by</code></td>
<td>
<p>(<em>optional</em>) Variables used to determine how and if facets of
each figure appear. In case the <code>facet_wrap_cols</code> argument is <code>NULL</code>, the
first variable is used to define columns, and the remaing variables are
used to define rows of facets. The variables cannot overlap with those
provided to the <code>split_by</code> argument, but may overlap with other arguments.
See details for available variables.</p>
</td></tr>
<tr><td><code id="plot_decision_curve_+3A_facet_wrap_cols">facet_wrap_cols</code></td>
<td>
<p>(<em>optional</em>) Number of columns to generate when facet
wrapping. If NULL, a facet grid is produced instead.</p>
</td></tr>
<tr><td><code id="plot_decision_curve_+3A_ggtheme">ggtheme</code></td>
<td>
<p>(<em>optional</em>) <code>ggplot</code> theme to use for plotting.</p>
</td></tr>
<tr><td><code id="plot_decision_curve_+3A_discrete_palette">discrete_palette</code></td>
<td>
<p>(<em>optional</em>) Palette to use to color the different
plot elements in case a value was provided to the <code>color_by</code> argument.</p>
</td></tr>
<tr><td><code id="plot_decision_curve_+3A_x_label">x_label</code></td>
<td>
<p>(<em>optional</em>) Label to provide to the x-axis. If NULL, no label
is shown.</p>
</td></tr>
<tr><td><code id="plot_decision_curve_+3A_y_label">y_label</code></td>
<td>
<p>(<em>optional</em>) Label to provide to the y-axis. If NULL, no label
is shown.</p>
</td></tr>
<tr><td><code id="plot_decision_curve_+3A_legend_label">legend_label</code></td>
<td>
<p>(<em>optional</em>) Label to provide to the legend. If NULL, the
legend will not have a name.</p>
</td></tr>
<tr><td><code id="plot_decision_curve_+3A_plot_title">plot_title</code></td>
<td>
<p>(<em>optional</em>) Label to provide as figure title. If NULL, no
title is shown.</p>
</td></tr>
<tr><td><code id="plot_decision_curve_+3A_plot_sub_title">plot_sub_title</code></td>
<td>
<p>(<em>optional</em>) Label to provide as figure subtitle. If
NULL, no subtitle is shown.</p>
</td></tr>
<tr><td><code id="plot_decision_curve_+3A_caption">caption</code></td>
<td>
<p>(<em>optional</em>) Label to provide as figure caption. If NULL, no
caption is shown.</p>
</td></tr>
<tr><td><code id="plot_decision_curve_+3A_x_range">x_range</code></td>
<td>
<p>(<em>optional</em>) Value range for the x-axis.</p>
</td></tr>
<tr><td><code id="plot_decision_curve_+3A_x_n_breaks">x_n_breaks</code></td>
<td>
<p>(<em>optional</em>) Number of breaks to show on the x-axis of the
plot. <code>x_n_breaks</code> is used to determine the <code>x_breaks</code> argument in case it
is unset.</p>
</td></tr>
<tr><td><code id="plot_decision_curve_+3A_x_breaks">x_breaks</code></td>
<td>
<p>(<em>optional</em>) Break points on the x-axis of the plot.</p>
</td></tr>
<tr><td><code id="plot_decision_curve_+3A_y_range">y_range</code></td>
<td>
<p>(<em>optional</em>) Value range for the y-axis.</p>
</td></tr>
<tr><td><code id="plot_decision_curve_+3A_y_n_breaks">y_n_breaks</code></td>
<td>
<p>(<em>optional</em>) Number of breaks to show on the y-axis of the
plot. <code>y_n_breaks</code> is used to determine the <code>y_breaks</code> argument in case it
is unset.</p>
</td></tr>
<tr><td><code id="plot_decision_curve_+3A_y_breaks">y_breaks</code></td>
<td>
<p>(<em>optional</em>) Break points on the y-axis of the plot.</p>
</td></tr>
<tr><td><code id="plot_decision_curve_+3A_conf_int_style">conf_int_style</code></td>
<td>
<p>(<em>optional</em>) Confidence interval style. See details for
allowed styles.</p>
</td></tr>
<tr><td><code id="plot_decision_curve_+3A_conf_int_alpha">conf_int_alpha</code></td>
<td>
<p>(<em>optional</em>) Alpha value to determine transparency of
confidence intervals or, alternatively, other plot elements with which the
confidence interval overlaps. Only values between 0.0 (fully transparent)
and 1.0 (fully opaque) are allowed.</p>
</td></tr>
<tr><td><code id="plot_decision_curve_+3A_width">width</code></td>
<td>
<p>(<em>optional</em>) Width of the plot. A default value is derived from
the number of facets.</p>
</td></tr>
<tr><td><code id="plot_decision_curve_+3A_height">height</code></td>
<td>
<p>(<em>optional</em>) Height of the plot. A default value is derived from
the number of features and the number of facets.</p>
</td></tr>
<tr><td><code id="plot_decision_curve_+3A_units">units</code></td>
<td>
<p>(<em>optional</em>) Plot size unit. Either <code>cm</code> (default), <code>mm</code> or <code style="white-space: pre;">&#8288;in&#8288;</code>.</p>
</td></tr>
<tr><td><code id="plot_decision_curve_+3A_export_collection">export_collection</code></td>
<td>
<p>(<em>optional</em>) Exports the collection if TRUE.</p>
</td></tr>
<tr><td><code id="plot_decision_curve_+3A_...">...</code></td>
<td>

<p>Arguments passed on to <code><a href="#topic+as_familiar_collection">as_familiar_collection</a></code>, <code><a href="ggplot2.html#topic+ggsave">ggplot2::ggsave</a></code>, <code><a href="#topic+extract_decision_curve_data">extract_decision_curve_data</a></code>
</p>

<dl>
<dt><code>familiar_data_names</code></dt><dd><p>Names of the dataset(s). Only used if the <code>object</code> parameter
is one or more <code>familiarData</code> objects.</p>
</dd>
<dt><code>collection_name</code></dt><dd><p>Name of the collection.</p>
</dd>
<dt><code>filename</code></dt><dd><p>File name to create on disk.</p>
</dd>
<dt><code>plot</code></dt><dd><p>Plot to save, defaults to last plot displayed.</p>
</dd>
<dt><code>device</code></dt><dd><p>Device to use. Can either be a device function
(e.g. <a href="grDevices.html#topic+png">png</a>), or one of &quot;eps&quot;, &quot;ps&quot;, &quot;tex&quot; (pictex),
&quot;pdf&quot;, &quot;jpeg&quot;, &quot;tiff&quot;, &quot;png&quot;, &quot;bmp&quot;, &quot;svg&quot; or &quot;wmf&quot; (windows only).</p>
</dd>
<dt><code>path</code></dt><dd><p>Path of the directory to save plot to: <code>path</code> and <code>filename</code>
are combined to create the fully qualified file name. Defaults to the
working directory.</p>
</dd>
<dt><code>scale</code></dt><dd><p>Multiplicative scaling factor.</p>
</dd>
<dt><code>dpi</code></dt><dd><p>Plot resolution. Also accepts a string input: &quot;retina&quot; (320),
&quot;print&quot; (300), or &quot;screen&quot; (72). Applies only to raster output types.</p>
</dd>
<dt><code>limitsize</code></dt><dd><p>When <code>TRUE</code> (the default), <code>ggsave()</code> will not
save images larger than 50x50 inches, to prevent the common error of
specifying dimensions in pixels.</p>
</dd>
<dt><code>bg</code></dt><dd><p>Background colour. If <code>NULL</code>, uses the <code>plot.background</code> fill value
from the plot theme.</p>
</dd>
<dt><code>data</code></dt><dd><p>A <code>dataObject</code> object, <code>data.table</code> or <code>data.frame</code> that
constitutes the data that are assessed.</p>
</dd>
<dt><code>is_pre_processed</code></dt><dd><p>Flag that indicates whether the data was already
pre-processed externally, e.g. normalised and clustered. Only used if the
<code>data</code> argument is a <code>data.table</code> or <code>data.frame</code>.</p>
</dd>
<dt><code>cl</code></dt><dd><p>Cluster created using the <code>parallel</code> package. This cluster is then
used to speed up computation through parallellisation.</p>
</dd>
<dt><code>evaluation_times</code></dt><dd><p>One or more time points that are used for in analysis of
survival problems when data has to be assessed at a set time, e.g.
calibration. If not provided explicitly, this parameter is read from
settings used at creation of the underlying <code>familiarModel</code> objects. Only
used for <code>survival</code> outcomes.</p>
</dd>
<dt><code>ensemble_method</code></dt><dd><p>Method for ensembling predictions from models for the
same sample. Available methods are:
</p>

<ul>
<li> <p><code>median</code> (default): Use the median of the predicted values as the ensemble
value for a sample.
</p>
</li>
<li> <p><code>mean</code>: Use the mean of the predicted values as the ensemble value for a
sample.
</p>
</li></ul>
</dd>
<dt><code>verbose</code></dt><dd><p>Flag to indicate whether feedback should be provided on the
computation and extraction of various data elements.</p>
</dd>
<dt><code>message_indent</code></dt><dd><p>Number of indentation steps for messages shown during
computation and extraction of various data elements.</p>
</dd>
<dt><code>detail_level</code></dt><dd><p>(<em>optional</em>) Sets the level at which results are computed
and aggregated.
</p>

<ul>
<li> <p><code>ensemble</code>: Results are computed at the ensemble level, i.e. over all
models in the ensemble. This means that, for example, bias-corrected
estimates of model performance are assessed by creating (at least) 20
bootstraps and computing the model performance of the ensemble model for
each bootstrap.
</p>
</li>
<li> <p><code>hybrid</code> (default): Results are computed at the level of models in an
ensemble. This means that, for example, bias-corrected estimates of model
performance are directly computed using the models in the ensemble. If there
are at least 20 trained models in the ensemble, performance is computed for
each model, in contrast to <code>ensemble</code> where performance is computed for the
ensemble of models. If there are less than 20 trained models in the
ensemble, bootstraps are created so that at least 20 point estimates can be
made.
</p>
</li>
<li> <p><code>model</code>: Results are computed at the model level. This means that, for
example, bias-corrected estimates of model performance are assessed by
creating (at least) 20 bootstraps and computing the performance of the model
for each bootstrap.
</p>
</li></ul>

<p>Note that each level of detail has a different interpretation for bootstrap
confidence intervals. For <code>ensemble</code> and <code>model</code> these are the confidence
intervals for the ensemble and an individual model, respectively. That is,
the confidence interval describes the range where an estimate produced by a
respective ensemble or model trained on a repeat of the experiment may be
found with the probability of the confidence level. For <code>hybrid</code>, it
represents the range where any single model trained on a repeat of the
experiment may be found with the probability of the confidence level. By
definition, confidence intervals obtained using <code>hybrid</code> are at least as
wide as those for <code>ensemble</code>. <code>hybrid</code> offers the correct interpretation if
the goal of the analysis is to assess the result of a single, unspecified,
model.
</p>
<p><code>hybrid</code> is generally computationally less expensive then <code>ensemble</code>, which
in turn is somewhat less expensive than <code>model</code>.
</p>
<p>A non-default <code>detail_level</code> parameter can be specified for separate
evaluation steps by providing a parameter value in a named list with data
elements, e.g. <code>list("auc_data"="ensemble", "model_performance"="hybrid")</code>.
This parameter can be set for the following data elements: <code>auc_data</code>,
<code>decision_curve_analyis</code>, <code>model_performance</code>, <code>permutation_vimp</code>,
<code>ice_data</code>, <code>prediction_data</code> and <code>confusion_matrix</code>.</p>
</dd>
<dt><code>estimation_type</code></dt><dd><p>(<em>optional</em>) Sets the type of estimation that should be
possible. This has the following options:
</p>

<ul>
<li> <p><code>point</code>: Point estimates.
</p>
</li>
<li> <p><code>bias_correction</code> or <code>bc</code>: Bias-corrected estimates. A bias-corrected
estimate is computed from (at least) 20 point estimates, and <code>familiar</code> may
bootstrap the data to create them.
</p>
</li>
<li> <p><code>bootstrap_confidence_interval</code> or <code>bci</code> (default): Bias-corrected
estimates with bootstrap confidence intervals (Efron and Hastie, 2016). The
number of point estimates required depends on the <code>confidence_level</code>
parameter, and <code>familiar</code> may bootstrap the data to create them.
</p>
</li></ul>

<p>As with <code>detail_level</code>, a non-default <code>estimation_type</code> parameter can be
specified for separate evaluation steps by providing a parameter value in a
named list with data elements, e.g. <code>list("auc_data"="bci", "model_performance"="point")</code>. This parameter can be set for the following
data elements: <code>auc_data</code>, <code>decision_curve_analyis</code>, <code>model_performance</code>,
<code>permutation_vimp</code>, <code>ice_data</code>, and <code>prediction_data</code>.</p>
</dd>
<dt><code>aggregate_results</code></dt><dd><p>(<em>optional</em>) Flag that signifies whether results
should be aggregated during evaluation. If <code>estimation_type</code> is
<code>bias_correction</code> or <code>bc</code>, aggregation leads to a single bias-corrected
estimate. If <code>estimation_type</code> is <code>bootstrap_confidence_interval</code> or <code>bci</code>,
aggregation leads to a single bias-corrected estimate with lower and upper
boundaries of the confidence interval. This has no effect if
<code>estimation_type</code> is <code>point</code>.
</p>
<p>The default value is equal to <code>TRUE</code> except when assessing metrics to assess
model performance, as the default violin plot requires underlying data.
</p>
<p>As with <code>detail_level</code> and <code>estimation_type</code>, a non-default
<code>aggregate_results</code> parameter can be specified for separate evaluation steps
by providing a parameter value in a named list with data elements, e.g.
<code>list("auc_data"=TRUE, , "model_performance"=FALSE)</code>. This parameter exists
for the same elements as <code>estimation_type</code>.</p>
</dd>
<dt><code>confidence_level</code></dt><dd><p>(<em>optional</em>) Numeric value for the level at which
confidence intervals are determined. In the case bootstraps are used to
determine the confidence intervals bootstrap estimation, <code>familiar</code> uses the
rule of thumb <code class="reqn">n = 20 / ci.level</code> to determine the number of required
bootstraps.
</p>
<p>The default value is <code>0.95</code>.</p>
</dd>
<dt><code>bootstrap_ci_method</code></dt><dd><p>(<em>optional</em>) Method used to determine bootstrap
confidence intervals (Efron and Hastie, 2016). The following methods are
implemented:
</p>

<ul>
<li> <p><code>percentile</code> (default): Confidence intervals obtained using the percentile
method.
</p>
</li>
<li> <p><code>bc</code>: Bias-corrected confidence intervals.
</p>
</li></ul>

<p>Note that the standard method is not implemented because this method is
often not suitable due to non-normal distributions. The bias-corrected and
accelerated (BCa) method is not implemented yet.</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Details</h3>

<p>This function generates plots for decision curves.
</p>
<p>Available splitting variables are: <code>fs_method</code>, <code>learner</code>, <code>data_set</code> and
<code>positive_class</code> (categorical outcomes) or <code>evaluation_time</code> (survival outcomes).
By default, the data is split by <code>fs_method</code> and <code>learner</code>, with faceting by
<code>data_set</code> and colouring by <code>positive_class</code> or <code>evaluation_time</code>.
</p>
<p>Available palettes for <code>discrete_palette</code> are those listed by
<code>grDevices::palette.pals()</code> (requires R &gt;= 4.0.0), <code>grDevices::hcl.pals()</code>
(requires R &gt;= 3.6.0) and <code>rainbow</code>, <code>heat.colors</code>, <code>terrain.colors</code>,
<code>topo.colors</code> and <code>cm.colors</code>, which correspond to the palettes of the same
name in <code>grDevices</code>. If not specified, a default palette based on palettes
in Tableau are used. You may also specify your own palette by using colour
names listed by <code>grDevices::colors()</code> or through hexadecimal RGB strings.
</p>
<p>Bootstrap confidence intervals of the decision curve (if present) can be
shown using various styles set by <code>conf_int_style</code>:
</p>

<ul>
<li> <p><code>ribbon</code> (default): confidence intervals are shown as a ribbon with an
opacity of <code>conf_int_alpha</code> around the point estimate of the decision curve.
</p>
</li>
<li> <p><code>step</code> (default): confidence intervals are shown as a step function around
the point estimate of the decision curve.
</p>
</li>
<li> <p><code>none</code>: confidence intervals are not shown. The point estimate of the
decision curve is shown as usual.
</p>
</li></ul>

<p>Labelling methods such as <code>set_fs_method_names</code> or <code>set_data_set_names</code> can
be applied to the <code>familiarCollection</code> object to update labels, and order
the output in the figure.
</p>


<h3>Value</h3>

<p><code>NULL</code> or list of plot objects, if <code>dir_path</code> is <code>NULL</code>.
</p>


<h3>References</h3>


<ol>
<li><p> Vickers, A. J. &amp; Elkin, E. B. Decision curve analysis: a novel
method for evaluating prediction models. Med. Decis. Making 26, 565–574
(2006).
</p>
</li>
<li><p> Vickers, A. J., Cronin, A. M., Elkin, E. B. &amp; Gonen, M. Extensions to
decision curve analysis, a novel method for evaluating diagnostic tests,
prediction models and molecular markers. BMC Med. Inform. Decis. Mak. 8, 53
(2008).
</p>
</li>
<li><p> Vickers, A. J., van Calster, B. &amp; Steyerberg, E. W. A simple,
step-by-step guide to interpreting decision curve analysis. Diagn Progn Res
3, 18 (2019).
</p>
</li></ol>


<hr>
<h2 id='plot_feature_similarity'>Plot heatmaps for pairwise similarity between features.</h2><span id='topic+plot_feature_similarity'></span><span id='topic+plot_feature_similarity+2CANY-method'></span><span id='topic+plot_feature_similarity+2CfamiliarCollection-method'></span>

<h3>Description</h3>

<p>This method creates a heatmap based on data stored in a
<code>familiarCollection</code> object. Features in the heatmap are ordered so that
more similar features appear together.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_feature_similarity(
  object,
  feature_cluster_method = waiver(),
  feature_linkage_method = waiver(),
  feature_cluster_cut_method = waiver(),
  feature_similarity_threshold = waiver(),
  draw = FALSE,
  dir_path = NULL,
  split_by = NULL,
  facet_by = NULL,
  facet_wrap_cols = NULL,
  ggtheme = NULL,
  gradient_palette = NULL,
  gradient_palette_range = NULL,
  x_label = waiver(),
  x_label_shared = "column",
  y_label = waiver(),
  y_label_shared = "row",
  legend_label = waiver(),
  plot_title = waiver(),
  plot_sub_title = waiver(),
  caption = NULL,
  y_range = NULL,
  y_n_breaks = 3,
  y_breaks = NULL,
  rotate_x_tick_labels = waiver(),
  show_dendrogram = c("top", "right"),
  dendrogram_height = grid::unit(1.5, "cm"),
  width = waiver(),
  height = waiver(),
  units = waiver(),
  export_collection = FALSE,
  ...
)

## S4 method for signature 'ANY'
plot_feature_similarity(
  object,
  feature_cluster_method = waiver(),
  feature_linkage_method = waiver(),
  feature_cluster_cut_method = waiver(),
  feature_similarity_threshold = waiver(),
  draw = FALSE,
  dir_path = NULL,
  split_by = NULL,
  facet_by = NULL,
  facet_wrap_cols = NULL,
  ggtheme = NULL,
  gradient_palette = NULL,
  gradient_palette_range = NULL,
  x_label = waiver(),
  x_label_shared = "column",
  y_label = waiver(),
  y_label_shared = "row",
  legend_label = waiver(),
  plot_title = waiver(),
  plot_sub_title = waiver(),
  caption = NULL,
  y_range = NULL,
  y_n_breaks = 3,
  y_breaks = NULL,
  rotate_x_tick_labels = waiver(),
  show_dendrogram = c("top", "right"),
  dendrogram_height = grid::unit(1.5, "cm"),
  width = waiver(),
  height = waiver(),
  units = waiver(),
  export_collection = FALSE,
  ...
)

## S4 method for signature 'familiarCollection'
plot_feature_similarity(
  object,
  feature_cluster_method = waiver(),
  feature_linkage_method = waiver(),
  feature_cluster_cut_method = waiver(),
  feature_similarity_threshold = waiver(),
  draw = FALSE,
  dir_path = NULL,
  split_by = NULL,
  facet_by = NULL,
  facet_wrap_cols = NULL,
  ggtheme = NULL,
  gradient_palette = NULL,
  gradient_palette_range = NULL,
  x_label = waiver(),
  x_label_shared = "column",
  y_label = waiver(),
  y_label_shared = "row",
  legend_label = waiver(),
  plot_title = waiver(),
  plot_sub_title = waiver(),
  caption = NULL,
  y_range = NULL,
  y_n_breaks = 3,
  y_breaks = NULL,
  rotate_x_tick_labels = waiver(),
  show_dendrogram = c("top", "right"),
  dendrogram_height = grid::unit(1.5, "cm"),
  width = waiver(),
  height = waiver(),
  units = waiver(),
  export_collection = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_feature_similarity_+3A_object">object</code></td>
<td>
<p>A <code>familiarCollection</code> object, or other other objects from which
a <code>familiarCollection</code> can be extracted. See details for more information.</p>
</td></tr>
<tr><td><code id="plot_feature_similarity_+3A_feature_cluster_method">feature_cluster_method</code></td>
<td>
<p>The method used to perform clustering. These are
the same methods as for the <code>cluster_method</code> configuration parameter:
<code>none</code>, <code>hclust</code>, <code>agnes</code>, <code>diana</code> and <code>pam</code>.
</p>
<p><code>none</code> cannot be used when extracting data regarding mutual correlation or
feature expressions.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="plot_feature_similarity_+3A_feature_linkage_method">feature_linkage_method</code></td>
<td>
<p>The method used for agglomerative clustering in
<code>hclust</code> and <code>agnes</code>. These are the same methods as for the
<code>cluster_linkage_method</code> configuration parameter: <code>average</code>, <code>single</code>,
<code>complete</code>, <code>weighted</code>, and <code>ward</code>.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="plot_feature_similarity_+3A_feature_cluster_cut_method">feature_cluster_cut_method</code></td>
<td>
<p>The method used to divide features into
separate clusters. The available methods are the same as for the
<code>cluster_cut_method</code> configuration parameter: <code>silhouette</code>, <code>fixed_cut</code> and
<code>dynamic_cut</code>.
</p>
<p><code>silhouette</code> is available for all cluster methods, but <code>fixed_cut</code> only
applies to methods that create hierarchical trees (<code>hclust</code>, <code>agnes</code> and
<code>diana</code>). <code>dynamic_cut</code> requires the <code>dynamicTreeCut</code> package and can only
be used with <code>agnes</code> and <code>hclust</code>.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="plot_feature_similarity_+3A_feature_similarity_threshold">feature_similarity_threshold</code></td>
<td>
<p>The threshold level for pair-wise
similarity that is required to form feature clusters with the <code>fixed_cut</code>
method.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="plot_feature_similarity_+3A_draw">draw</code></td>
<td>
<p>(<em>optional</em>) Draws the plot if TRUE.</p>
</td></tr>
<tr><td><code id="plot_feature_similarity_+3A_dir_path">dir_path</code></td>
<td>
<p>(<em>optional</em>) Path to the directory where created performance
plots are saved to. Output is saved in the <code>feature_similarity</code>
subdirectory. If <code>NULL</code> no figures are saved, but are returned instead.</p>
</td></tr>
<tr><td><code id="plot_feature_similarity_+3A_split_by">split_by</code></td>
<td>
<p>(<em>optional</em>) Splitting variables. This refers to column names
on which datasets are split. A separate figure is created for each split.
See details for available variables.</p>
</td></tr>
<tr><td><code id="plot_feature_similarity_+3A_facet_by">facet_by</code></td>
<td>
<p>(<em>optional</em>) Variables used to determine how and if facets of
each figure appear. In case the <code>facet_wrap_cols</code> argument is <code>NULL</code>, the
first variable is used to define columns, and the remaing variables are
used to define rows of facets. The variables cannot overlap with those
provided to the <code>split_by</code> argument, but may overlap with other arguments.
See details for available variables.</p>
</td></tr>
<tr><td><code id="plot_feature_similarity_+3A_facet_wrap_cols">facet_wrap_cols</code></td>
<td>
<p>(<em>optional</em>) Number of columns to generate when facet
wrapping. If NULL, a facet grid is produced instead.</p>
</td></tr>
<tr><td><code id="plot_feature_similarity_+3A_ggtheme">ggtheme</code></td>
<td>
<p>(<em>optional</em>) <code>ggplot</code> theme to use for plotting.</p>
</td></tr>
<tr><td><code id="plot_feature_similarity_+3A_gradient_palette">gradient_palette</code></td>
<td>
<p>(<em>optional</em>) Sequential or divergent palette used to
colour the similarity or distance between features in a heatmap.</p>
</td></tr>
<tr><td><code id="plot_feature_similarity_+3A_gradient_palette_range">gradient_palette_range</code></td>
<td>
<p>(<em>optional</em>) Numerical range used to span the
gradient. This should be a range of two values, e.g. <code>c(0, 1)</code>. Lower or
upper boundary can be unset by using <code>NA</code>. If not set, the full
metric-specific range is used.</p>
</td></tr>
<tr><td><code id="plot_feature_similarity_+3A_x_label">x_label</code></td>
<td>
<p>(<em>optional</em>) Label to provide to the x-axis. If NULL, no label
is shown.</p>
</td></tr>
<tr><td><code id="plot_feature_similarity_+3A_x_label_shared">x_label_shared</code></td>
<td>
<p>(<em>optional</em>) Sharing of x-axis labels between facets.
One of three values:
</p>

<ul>
<li> <p><code>overall</code>: A single label is placed at the bottom of the figure. Tick
text (but not the ticks themselves) is removed for all but the bottom facet
plot(s).
</p>
</li>
<li> <p><code>column</code>: A label is placed at the bottom of each column. Tick text (but
not the ticks themselves) is removed for all but the bottom facet plot(s).
</p>
</li>
<li> <p><code>individual</code>: A label is placed below each facet plot. Tick text is kept.
</p>
</li></ul>
</td></tr>
<tr><td><code id="plot_feature_similarity_+3A_y_label">y_label</code></td>
<td>
<p>(<em>optional</em>) Label to provide to the y-axis. If NULL, no label
is shown.</p>
</td></tr>
<tr><td><code id="plot_feature_similarity_+3A_y_label_shared">y_label_shared</code></td>
<td>
<p>(<em>optional</em>) Sharing of y-axis labels between facets.
One of three values:
</p>

<ul>
<li> <p><code>overall</code>: A single label is placed to the left of the figure. Tick text
(but not the ticks themselves) is removed for all but the left-most facet
plot(s).
</p>
</li>
<li> <p><code>row</code>: A label is placed to the left of each row. Tick text (but not the
ticks themselves) is removed for all but the left-most facet plot(s).
</p>
</li>
<li> <p><code>individual</code>: A label is placed below each facet plot. Tick text is kept.
</p>
</li></ul>
</td></tr>
<tr><td><code id="plot_feature_similarity_+3A_legend_label">legend_label</code></td>
<td>
<p>(<em>optional</em>) Label to provide to the legend. If NULL, the
legend will not have a name.</p>
</td></tr>
<tr><td><code id="plot_feature_similarity_+3A_plot_title">plot_title</code></td>
<td>
<p>(<em>optional</em>) Label to provide as figure title. If NULL, no
title is shown.</p>
</td></tr>
<tr><td><code id="plot_feature_similarity_+3A_plot_sub_title">plot_sub_title</code></td>
<td>
<p>(<em>optional</em>) Label to provide as figure subtitle. If
NULL, no subtitle is shown.</p>
</td></tr>
<tr><td><code id="plot_feature_similarity_+3A_caption">caption</code></td>
<td>
<p>(<em>optional</em>) Label to provide as figure caption. If NULL, no
caption is shown.</p>
</td></tr>
<tr><td><code id="plot_feature_similarity_+3A_y_range">y_range</code></td>
<td>
<p>(<em>optional</em>) Value range for the y-axis.</p>
</td></tr>
<tr><td><code id="plot_feature_similarity_+3A_y_n_breaks">y_n_breaks</code></td>
<td>
<p>(<em>optional</em>) Number of breaks to show on the y-axis of the
plot. <code>y_n_breaks</code> is used to determine the <code>y_breaks</code> argument in case it
is unset.</p>
</td></tr>
<tr><td><code id="plot_feature_similarity_+3A_y_breaks">y_breaks</code></td>
<td>
<p>(<em>optional</em>) Break points on the y-axis of the plot.</p>
</td></tr>
<tr><td><code id="plot_feature_similarity_+3A_rotate_x_tick_labels">rotate_x_tick_labels</code></td>
<td>
<p>(<em>optional</em>) Rotate tick labels on the x-axis by
90 degrees. Defaults to <code>TRUE</code>. Rotation of x-axis tick labels may also be
controlled through the <code>ggtheme</code>. In this case, <code>FALSE</code> should be provided
explicitly.</p>
</td></tr>
<tr><td><code id="plot_feature_similarity_+3A_show_dendrogram">show_dendrogram</code></td>
<td>
<p>(<em>optional</em>) Show dendrogram around the main panel. Can
be <code>TRUE</code>, <code>FALSE</code>, <code>NULL</code>, or a position, i.e. <code>top</code>, <code>bottom</code>, <code>left</code> and
<code>right</code>. Up to two positions may be provided, but only as long as the
dendrograms are not on opposite sides of the heatmap: <code>top</code> and <code>bottom</code>,
and <code>left</code> and <code>right</code> cannot be used together.
</p>
<p>A dendrogram can only be drawn from cluster methods that produce
dendrograms, such as <code>hclust</code>. A dendrogram can for example not be
constructed using the partitioning around medioids method (<code>pam</code>).
</p>
<p>By default, a dendrogram is drawn to the top and right of the panel.</p>
</td></tr>
<tr><td><code id="plot_feature_similarity_+3A_dendrogram_height">dendrogram_height</code></td>
<td>
<p>(<em>optional</em>) Height of the dendrogram. The height is
1.5 cm by default. Height is expected to be grid unit (see <code>grid::unit</code>),
which also allows for specifying relative heights.</p>
</td></tr>
<tr><td><code id="plot_feature_similarity_+3A_width">width</code></td>
<td>
<p>(<em>optional</em>) Width of the plot. A default value is derived from
the number of facets.</p>
</td></tr>
<tr><td><code id="plot_feature_similarity_+3A_height">height</code></td>
<td>
<p>(<em>optional</em>) Height of the plot. A default value is derived from
the number of features and the number of facets.</p>
</td></tr>
<tr><td><code id="plot_feature_similarity_+3A_units">units</code></td>
<td>
<p>(<em>optional</em>) Plot size unit. Either <code>cm</code> (default), <code>mm</code> or <code style="white-space: pre;">&#8288;in&#8288;</code>.</p>
</td></tr>
<tr><td><code id="plot_feature_similarity_+3A_export_collection">export_collection</code></td>
<td>
<p>(<em>optional</em>) Exports the collection if TRUE.</p>
</td></tr>
<tr><td><code id="plot_feature_similarity_+3A_...">...</code></td>
<td>

<p>Arguments passed on to <code><a href="#topic+as_familiar_collection">as_familiar_collection</a></code>, <code><a href="ggplot2.html#topic+ggsave">ggplot2::ggsave</a></code>, <code><a href="#topic+extract_feature_similarity">extract_feature_similarity</a></code>
</p>

<dl>
<dt><code>familiar_data_names</code></dt><dd><p>Names of the dataset(s). Only used if the <code>object</code> parameter
is one or more <code>familiarData</code> objects.</p>
</dd>
<dt><code>collection_name</code></dt><dd><p>Name of the collection.</p>
</dd>
<dt><code>filename</code></dt><dd><p>File name to create on disk.</p>
</dd>
<dt><code>plot</code></dt><dd><p>Plot to save, defaults to last plot displayed.</p>
</dd>
<dt><code>device</code></dt><dd><p>Device to use. Can either be a device function
(e.g. <a href="grDevices.html#topic+png">png</a>), or one of &quot;eps&quot;, &quot;ps&quot;, &quot;tex&quot; (pictex),
&quot;pdf&quot;, &quot;jpeg&quot;, &quot;tiff&quot;, &quot;png&quot;, &quot;bmp&quot;, &quot;svg&quot; or &quot;wmf&quot; (windows only).</p>
</dd>
<dt><code>path</code></dt><dd><p>Path of the directory to save plot to: <code>path</code> and <code>filename</code>
are combined to create the fully qualified file name. Defaults to the
working directory.</p>
</dd>
<dt><code>scale</code></dt><dd><p>Multiplicative scaling factor.</p>
</dd>
<dt><code>dpi</code></dt><dd><p>Plot resolution. Also accepts a string input: &quot;retina&quot; (320),
&quot;print&quot; (300), or &quot;screen&quot; (72). Applies only to raster output types.</p>
</dd>
<dt><code>limitsize</code></dt><dd><p>When <code>TRUE</code> (the default), <code>ggsave()</code> will not
save images larger than 50x50 inches, to prevent the common error of
specifying dimensions in pixels.</p>
</dd>
<dt><code>bg</code></dt><dd><p>Background colour. If <code>NULL</code>, uses the <code>plot.background</code> fill value
from the plot theme.</p>
</dd>
<dt><code>data</code></dt><dd><p>A <code>dataObject</code> object, <code>data.table</code> or <code>data.frame</code> that
constitutes the data that are assessed.</p>
</dd>
<dt><code>is_pre_processed</code></dt><dd><p>Flag that indicates whether the data was already
pre-processed externally, e.g. normalised and clustered. Only used if the
<code>data</code> argument is a <code>data.table</code> or <code>data.frame</code>.</p>
</dd>
<dt><code>cl</code></dt><dd><p>Cluster created using the <code>parallel</code> package. This cluster is then
used to speed up computation through parallellisation.</p>
</dd>
<dt><code>feature_similarity_metric</code></dt><dd><p>Metric to determine pairwise similarity
between features. Similarity is computed in the same manner as for
clustering, and <code>feature_similarity_metric</code> therefore has the same options
as <code>cluster_similarity_metric</code>: <code>mcfadden_r2</code>, <code>cox_snell_r2</code>,
<code>nagelkerke_r2</code>, <code>spearman</code>, <code>kendall</code> and <code>pearson</code>.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</dd>
<dt><code>verbose</code></dt><dd><p>Flag to indicate whether feedback should be provided on the
computation and extraction of various data elements.</p>
</dd>
<dt><code>message_indent</code></dt><dd><p>Number of indentation steps for messages shown during
computation and extraction of various data elements.</p>
</dd>
<dt><code>estimation_type</code></dt><dd><p>(<em>optional</em>) Sets the type of estimation that should be
possible. This has the following options:
</p>

<ul>
<li> <p><code>point</code>: Point estimates.
</p>
</li>
<li> <p><code>bias_correction</code> or <code>bc</code>: Bias-corrected estimates. A bias-corrected
estimate is computed from (at least) 20 point estimates, and <code>familiar</code> may
bootstrap the data to create them.
</p>
</li>
<li> <p><code>bootstrap_confidence_interval</code> or <code>bci</code> (default): Bias-corrected
estimates with bootstrap confidence intervals (Efron and Hastie, 2016). The
number of point estimates required depends on the <code>confidence_level</code>
parameter, and <code>familiar</code> may bootstrap the data to create them.
</p>
</li></ul>

<p>As with <code>detail_level</code>, a non-default <code>estimation_type</code> parameter can be
specified for separate evaluation steps by providing a parameter value in a
named list with data elements, e.g. <code>list("auc_data"="bci", "model_performance"="point")</code>. This parameter can be set for the following
data elements: <code>auc_data</code>, <code>decision_curve_analyis</code>, <code>model_performance</code>,
<code>permutation_vimp</code>, <code>ice_data</code>, and <code>prediction_data</code>.</p>
</dd>
<dt><code>aggregate_results</code></dt><dd><p>(<em>optional</em>) Flag that signifies whether results
should be aggregated during evaluation. If <code>estimation_type</code> is
<code>bias_correction</code> or <code>bc</code>, aggregation leads to a single bias-corrected
estimate. If <code>estimation_type</code> is <code>bootstrap_confidence_interval</code> or <code>bci</code>,
aggregation leads to a single bias-corrected estimate with lower and upper
boundaries of the confidence interval. This has no effect if
<code>estimation_type</code> is <code>point</code>.
</p>
<p>The default value is equal to <code>TRUE</code> except when assessing metrics to assess
model performance, as the default violin plot requires underlying data.
</p>
<p>As with <code>detail_level</code> and <code>estimation_type</code>, a non-default
<code>aggregate_results</code> parameter can be specified for separate evaluation steps
by providing a parameter value in a named list with data elements, e.g.
<code>list("auc_data"=TRUE, , "model_performance"=FALSE)</code>. This parameter exists
for the same elements as <code>estimation_type</code>.</p>
</dd>
<dt><code>confidence_level</code></dt><dd><p>(<em>optional</em>) Numeric value for the level at which
confidence intervals are determined. In the case bootstraps are used to
determine the confidence intervals bootstrap estimation, <code>familiar</code> uses the
rule of thumb <code class="reqn">n = 20 / ci.level</code> to determine the number of required
bootstraps.
</p>
<p>The default value is <code>0.95</code>.</p>
</dd>
<dt><code>bootstrap_ci_method</code></dt><dd><p>(<em>optional</em>) Method used to determine bootstrap
confidence intervals (Efron and Hastie, 2016). The following methods are
implemented:
</p>

<ul>
<li> <p><code>percentile</code> (default): Confidence intervals obtained using the percentile
method.
</p>
</li>
<li> <p><code>bc</code>: Bias-corrected confidence intervals.
</p>
</li></ul>

<p>Note that the standard method is not implemented because this method is
often not suitable due to non-normal distributions. The bias-corrected and
accelerated (BCa) method is not implemented yet.</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Details</h3>

<p>This function generates area under the ROC curve plots.
</p>
<p>Available splitting variables are: <code>fs_method</code>, <code>learner</code>, and <code>data_set</code>.
By default, the data is split by <code>fs_method</code> and <code>learner</code>, with facetting
by <code>data_set</code>.
</p>
<p>Note that similarity is determined based on the underlying data. Hence the
ordering of features may differ between facets, and tick labels are
maintained for each panel.
</p>
<p>Available palettes for <code>gradient_palette</code> are those listed by
<code>grDevices::palette.pals()</code> (requires R &gt;= 4.0.0), <code>grDevices::hcl.pals()</code>
(requires R &gt;= 3.6.0) and <code>rainbow</code>, <code>heat.colors</code>, <code>terrain.colors</code>,
<code>topo.colors</code> and <code>cm.colors</code>, which correspond to the palettes of the same
name in <code>grDevices</code>. If not specified, a default palette based on palettes
in Tableau are used. You may also specify your own palette by using colour
names listed by <code>grDevices::colors()</code> or through hexadecimal RGB strings.
</p>
<p>Labeling methods such as <code>set_fs_method_names</code> or <code>set_data_set_names</code> can
be applied to the <code>familiarCollection</code> object to update labels, and order
the output in the figure.
</p>


<h3>Value</h3>

<p><code>NULL</code> or list of plot objects, if <code>dir_path</code> is <code>NULL</code>.
</p>

<hr>
<h2 id='plot_ice'>Plot individual conditional expectation plots.</h2><span id='topic+plot_ice'></span><span id='topic+plot_ice+2CANY-method'></span><span id='topic+plot_ice+2CfamiliarCollection-method'></span>

<h3>Description</h3>

<p>This method creates individual conditional expectation plots
based on data in a familiarCollection object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_ice(
  object,
  draw = FALSE,
  dir_path = NULL,
  split_by = NULL,
  color_by = NULL,
  facet_by = NULL,
  facet_wrap_cols = NULL,
  ggtheme = NULL,
  discrete_palette = NULL,
  gradient_palette = NULL,
  gradient_palette_range = NULL,
  x_label = waiver(),
  y_label = waiver(),
  legend_label = waiver(),
  plot_title = NULL,
  plot_sub_title = NULL,
  caption = NULL,
  x_range = NULL,
  x_n_breaks = 5,
  x_breaks = NULL,
  y_range = NULL,
  y_n_breaks = 5,
  y_breaks = NULL,
  novelty_range = NULL,
  value_scales = waiver(),
  novelty_scales = waiver(),
  conf_int_style = c("ribbon", "step", "none"),
  conf_int_alpha = 0.4,
  ice_default_alpha = 0.6,
  n_max_samples_shown = 50L,
  show_ice = TRUE,
  show_pd = TRUE,
  show_novelty = TRUE,
  anchor_values = NULL,
  width = waiver(),
  height = waiver(),
  units = waiver(),
  export_collection = FALSE,
  ...
)

## S4 method for signature 'ANY'
plot_ice(
  object,
  draw = FALSE,
  dir_path = NULL,
  split_by = NULL,
  color_by = NULL,
  facet_by = NULL,
  facet_wrap_cols = NULL,
  ggtheme = NULL,
  discrete_palette = NULL,
  gradient_palette = NULL,
  gradient_palette_range = NULL,
  x_label = waiver(),
  y_label = waiver(),
  legend_label = waiver(),
  plot_title = NULL,
  plot_sub_title = NULL,
  caption = NULL,
  x_range = NULL,
  x_n_breaks = 5,
  x_breaks = NULL,
  y_range = NULL,
  y_n_breaks = 5,
  y_breaks = NULL,
  novelty_range = NULL,
  value_scales = waiver(),
  novelty_scales = waiver(),
  conf_int_style = c("ribbon", "step", "none"),
  conf_int_alpha = 0.4,
  ice_default_alpha = 0.6,
  n_max_samples_shown = 50L,
  show_ice = TRUE,
  show_pd = TRUE,
  show_novelty = TRUE,
  anchor_values = NULL,
  width = waiver(),
  height = waiver(),
  units = waiver(),
  export_collection = FALSE,
  ...
)

## S4 method for signature 'familiarCollection'
plot_ice(
  object,
  draw = FALSE,
  dir_path = NULL,
  split_by = NULL,
  color_by = NULL,
  facet_by = NULL,
  facet_wrap_cols = NULL,
  ggtheme = NULL,
  discrete_palette = NULL,
  gradient_palette = NULL,
  gradient_palette_range = NULL,
  x_label = waiver(),
  y_label = waiver(),
  legend_label = waiver(),
  plot_title = waiver(),
  plot_sub_title = waiver(),
  caption = NULL,
  x_range = NULL,
  x_n_breaks = 5,
  x_breaks = NULL,
  y_range = NULL,
  y_n_breaks = 5,
  y_breaks = NULL,
  novelty_range = NULL,
  value_scales = waiver(),
  novelty_scales = waiver(),
  conf_int_style = c("ribbon", "step", "none"),
  conf_int_alpha = 0.4,
  ice_default_alpha = 0.6,
  n_max_samples_shown = 50L,
  show_ice = TRUE,
  show_pd = TRUE,
  show_novelty = TRUE,
  anchor_values = NULL,
  width = waiver(),
  height = waiver(),
  units = waiver(),
  export_collection = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_ice_+3A_object">object</code></td>
<td>
<p><code>familiarCollection</code> object, or one or more <code>familiarData</code>
objects, that will be internally converted to a <code>familiarCollection</code> object.
It is also possible to provide a <code>familiarEnsemble</code> or one or more
<code>familiarModel</code> objects together with the data from which data is computed
prior to export. Paths to such files can also be provided.</p>
</td></tr>
<tr><td><code id="plot_ice_+3A_draw">draw</code></td>
<td>
<p>(<em>optional</em>) Draws the plot if TRUE.</p>
</td></tr>
<tr><td><code id="plot_ice_+3A_dir_path">dir_path</code></td>
<td>
<p>(<em>optional</em>) Path to the directory where created individual
conditional expectation plots are saved to. Output is saved in the
<code>explanation</code> subdirectory. If <code>NULL</code>, figures are written to the folder,
but are returned instead.</p>
</td></tr>
<tr><td><code id="plot_ice_+3A_split_by">split_by</code></td>
<td>
<p>(<em>optional</em>) Splitting variables. This refers to column names
on which datasets are split. A separate figure is created for each split.
See details for available variables.</p>
</td></tr>
<tr><td><code id="plot_ice_+3A_color_by">color_by</code></td>
<td>
<p>(<em>optional</em>) Variables used to determine fill colour of plot
objects. The variables cannot overlap with those provided to the <code>split_by</code>
argument, but may overlap with other arguments. See details for available
variables.</p>
</td></tr>
<tr><td><code id="plot_ice_+3A_facet_by">facet_by</code></td>
<td>
<p>(<em>optional</em>) Variables used to determine how and if facets of
each figure appear. In case the <code>facet_wrap_cols</code> argument is <code>NULL</code>, the
first variable is used to define columns, and the remaing variables are
used to define rows of facets. The variables cannot overlap with those
provided to the <code>split_by</code> argument, but may overlap with other arguments.
See details for available variables.</p>
</td></tr>
<tr><td><code id="plot_ice_+3A_facet_wrap_cols">facet_wrap_cols</code></td>
<td>
<p>(<em>optional</em>) Number of columns to generate when facet
wrapping. If NULL, a facet grid is produced instead.</p>
</td></tr>
<tr><td><code id="plot_ice_+3A_ggtheme">ggtheme</code></td>
<td>
<p>(<em>optional</em>) <code>ggplot</code> theme to use for plotting.</p>
</td></tr>
<tr><td><code id="plot_ice_+3A_discrete_palette">discrete_palette</code></td>
<td>
<p>(<em>optional</em>) Palette to use to colour the different
plot elements in case a value was provided to the <code>color_by</code> argument. For
2D individual conditional expectation plots without novelty, the initial
colour determines the colour of the points indicating sample values.</p>
</td></tr>
<tr><td><code id="plot_ice_+3A_gradient_palette">gradient_palette</code></td>
<td>
<p>(<em>optional</em>) Sequential or divergent palette used to
colour the raster in 2D individual conditional expectation or partial
dependence plots. This argument is not used for 1D plots.</p>
</td></tr>
<tr><td><code id="plot_ice_+3A_gradient_palette_range">gradient_palette_range</code></td>
<td>
<p>(<em>optional</em>) Numerical range used to span the
gradient for 2D plots. This should be a range of two values, e.g. <code>c(0, 1)</code>.
By default, values are determined from the data, dependent on the
<code>value_scales</code> parameter. This parameter is ignored for 1D plots.</p>
</td></tr>
<tr><td><code id="plot_ice_+3A_x_label">x_label</code></td>
<td>
<p>(<em>optional</em>) Label to provide to the x-axis. If NULL, no label
is shown.</p>
</td></tr>
<tr><td><code id="plot_ice_+3A_y_label">y_label</code></td>
<td>
<p>(<em>optional</em>) Label to provide to the y-axis. If NULL, no label
is shown.</p>
</td></tr>
<tr><td><code id="plot_ice_+3A_legend_label">legend_label</code></td>
<td>
<p>(<em>optional</em>) Label to provide to the legend. If NULL, the
legend will not have a name.</p>
</td></tr>
<tr><td><code id="plot_ice_+3A_plot_title">plot_title</code></td>
<td>
<p>(<em>optional</em>) Label to provide as figure title. If NULL, no
title is shown.</p>
</td></tr>
<tr><td><code id="plot_ice_+3A_plot_sub_title">plot_sub_title</code></td>
<td>
<p>(<em>optional</em>) Label to provide as figure subtitle. If
NULL, no subtitle is shown.</p>
</td></tr>
<tr><td><code id="plot_ice_+3A_caption">caption</code></td>
<td>
<p>(<em>optional</em>) Label to provide as figure caption. If NULL, no
caption is shown.</p>
</td></tr>
<tr><td><code id="plot_ice_+3A_x_range">x_range</code></td>
<td>
<p>(<em>optional</em>) Value range for the x-axis.</p>
</td></tr>
<tr><td><code id="plot_ice_+3A_x_n_breaks">x_n_breaks</code></td>
<td>
<p>(<em>optional</em>) Number of breaks to show on the x-axis of the
plot. <code>x_n_breaks</code> is used to determine the <code>x_breaks</code> argument in case it
is unset.</p>
</td></tr>
<tr><td><code id="plot_ice_+3A_x_breaks">x_breaks</code></td>
<td>
<p>(<em>optional</em>) Break points on the x-axis of the plot.</p>
</td></tr>
<tr><td><code id="plot_ice_+3A_y_range">y_range</code></td>
<td>
<p>(<em>optional</em>) Value range for the y-axis.</p>
</td></tr>
<tr><td><code id="plot_ice_+3A_y_n_breaks">y_n_breaks</code></td>
<td>
<p>(<em>optional</em>) Number of breaks to show on the y-axis of the
plot. <code>y_n_breaks</code> is used to determine the <code>y_breaks</code> argument in case it
is unset.</p>
</td></tr>
<tr><td><code id="plot_ice_+3A_y_breaks">y_breaks</code></td>
<td>
<p>(<em>optional</em>) Break points on the y-axis of the plot.</p>
</td></tr>
<tr><td><code id="plot_ice_+3A_novelty_range">novelty_range</code></td>
<td>
<p>(<em>optional</em>) Numerical range used to span the range of
novelty values. This determines the size of the bubbles in 2D, and
transparency of lines in 1D. This should be a range of two values, e.g.
<code>c(0, 1)</code>. By default, values are determined from the data, dependent on the
<code>value_scales</code> parameter. This parameter is ignored if <code>show_novelty=FALSE</code>.</p>
</td></tr>
<tr><td><code id="plot_ice_+3A_value_scales">value_scales</code></td>
<td>
<p>(<em>optional</em>) Sets scaling of predicted values. This
parameter has several options:
</p>

<ul>
<li> <p><code>fixed</code> (default): The value axis for all features will have the same
range.
</p>
</li>
<li> <p><code>feature</code>: The value axis for each feature will have the same range. This
option is unavailable for 2D plots.
</p>
</li>
<li> <p><code>figure</code>: The value axis for all facets in a figure will have the same
range.
</p>
</li>
<li> <p><code>facet</code>: Each facet has its own range. This option is unavailable for 2D
plots.
</p>
</li></ul>

<p>For 1D plots, this option is ignored if the <code>y_range</code> is provided, whereas
for 2D it is ignored if the <code>gradient_palette_range</code> is provided.</p>
</td></tr>
<tr><td><code id="plot_ice_+3A_novelty_scales">novelty_scales</code></td>
<td>
<p>(<em>optional</em>) Sets scaling of novelty values, similar to
the <code>value_scales</code> parameter, but with more limited options:
</p>

<ul>
<li> <p><code>fixed</code> (default): The novelty will have the same range for all features.
</p>
</li>
<li> <p><code>figure</code>: The novelty will have the same range for all facets in a figure.
</p>
</li></ul>
</td></tr>
<tr><td><code id="plot_ice_+3A_conf_int_style">conf_int_style</code></td>
<td>
<p>(<em>optional</em>) Confidence interval style. See details for
allowed styles.</p>
</td></tr>
<tr><td><code id="plot_ice_+3A_conf_int_alpha">conf_int_alpha</code></td>
<td>
<p>(<em>optional</em>) Alpha value to determine transparency of
confidence intervals or, alternatively, other plot elements with which the
confidence interval overlaps. Only values between 0.0 (fully transparent)
and 1.0 (fully opaque) are allowed.</p>
</td></tr>
<tr><td><code id="plot_ice_+3A_ice_default_alpha">ice_default_alpha</code></td>
<td>
<p>(<em>optional</em>) Default transparency (value) of sample
lines in an 1D plot. When novelty is shown, this is the transparency
corresponding to the least novel points. The confidence interval alpha
values is scaled by this value.</p>
</td></tr>
<tr><td><code id="plot_ice_+3A_n_max_samples_shown">n_max_samples_shown</code></td>
<td>
<p>(<em>optional</em>) Maximum number of samples shown in an
individual conditional expectation plot. Defaults to 50. These samples are
randomly picked from the samples present in the ICE data, but the same
samples are consistently picked. Partial dependence is nonetheless computed
from all available samples.</p>
</td></tr>
<tr><td><code id="plot_ice_+3A_show_ice">show_ice</code></td>
<td>
<p>(<em>optional</em>) Sets whether individual conditional expectation
plots should be created.</p>
</td></tr>
<tr><td><code id="plot_ice_+3A_show_pd">show_pd</code></td>
<td>
<p>(<em>optional</em>) Sets whether partial dependence plots should be
created. Note that if an anchor is set for a particular feature, its partial
dependence cannot be shown.</p>
</td></tr>
<tr><td><code id="plot_ice_+3A_show_novelty">show_novelty</code></td>
<td>
<p>(<em>optional</em>) Sets whether novelty is shown in plots.</p>
</td></tr>
<tr><td><code id="plot_ice_+3A_anchor_values">anchor_values</code></td>
<td>
<p>(<em>optional</em>) A single value or a named list or array of
values that are used to centre the individual conditional expectation plot.
A single value is valid if and only if only a single feature is assessed.
Otherwise, values Has no effect if the plot is not shown, i.e.
<code>show_ice=FALSE</code>. A partial dependence plot cannot be shown for those
features.</p>
</td></tr>
<tr><td><code id="plot_ice_+3A_width">width</code></td>
<td>
<p>(<em>optional</em>) Width of the plot. A default value is derived from
the number of facets.</p>
</td></tr>
<tr><td><code id="plot_ice_+3A_height">height</code></td>
<td>
<p>(<em>optional</em>) Height of the plot. A default value is derived from
the number of features and the number of facets.</p>
</td></tr>
<tr><td><code id="plot_ice_+3A_units">units</code></td>
<td>
<p>(<em>optional</em>) Plot size unit. Either <code>cm</code> (default), <code>mm</code> or <code style="white-space: pre;">&#8288;in&#8288;</code>.</p>
</td></tr>
<tr><td><code id="plot_ice_+3A_export_collection">export_collection</code></td>
<td>
<p>(<em>optional</em>) Exports the collection if TRUE.</p>
</td></tr>
<tr><td><code id="plot_ice_+3A_...">...</code></td>
<td>

<p>Arguments passed on to <code><a href="#topic+export_ice_data">export_ice_data</a></code>, <code><a href="ggplot2.html#topic+ggsave">ggplot2::ggsave</a></code>, <code><a href="#topic+extract_ice">extract_ice</a></code>
</p>

<dl>
<dt><code>aggregate_results</code></dt><dd><p>Flag that signifies whether results should be
aggregated for export.</p>
</dd>
<dt><code>filename</code></dt><dd><p>File name to create on disk.</p>
</dd>
<dt><code>plot</code></dt><dd><p>Plot to save, defaults to last plot displayed.</p>
</dd>
<dt><code>device</code></dt><dd><p>Device to use. Can either be a device function
(e.g. <a href="grDevices.html#topic+png">png</a>), or one of &quot;eps&quot;, &quot;ps&quot;, &quot;tex&quot; (pictex),
&quot;pdf&quot;, &quot;jpeg&quot;, &quot;tiff&quot;, &quot;png&quot;, &quot;bmp&quot;, &quot;svg&quot; or &quot;wmf&quot; (windows only).</p>
</dd>
<dt><code>path</code></dt><dd><p>Path of the directory to save plot to: <code>path</code> and <code>filename</code>
are combined to create the fully qualified file name. Defaults to the
working directory.</p>
</dd>
<dt><code>scale</code></dt><dd><p>Multiplicative scaling factor.</p>
</dd>
<dt><code>dpi</code></dt><dd><p>Plot resolution. Also accepts a string input: &quot;retina&quot; (320),
&quot;print&quot; (300), or &quot;screen&quot; (72). Applies only to raster output types.</p>
</dd>
<dt><code>limitsize</code></dt><dd><p>When <code>TRUE</code> (the default), <code>ggsave()</code> will not
save images larger than 50x50 inches, to prevent the common error of
specifying dimensions in pixels.</p>
</dd>
<dt><code>bg</code></dt><dd><p>Background colour. If <code>NULL</code>, uses the <code>plot.background</code> fill value
from the plot theme.</p>
</dd>
<dt><code>features</code></dt><dd><p>Names of the feature or features (2) assessed simultaneously.
By default <code>NULL</code>, which means that all features are assessed one-by-one.</p>
</dd>
<dt><code>feature_x_range</code></dt><dd><p>When one or two features are defined using <code>features</code>,
<code>feature_x_range</code> can be used to set the range of values for the first
feature. For numeric features, a vector of two values is assumed to indicate
a range from which <code>n_sample_points</code> are uniformly sampled. A vector of more
than two values is interpreted as is, i.e. these represent the values to be
sampled. For categorical features, values should represent a (sub)set of
available levels.</p>
</dd>
<dt><code>feature_y_range</code></dt><dd><p>As <code>feature_x_range</code>, but for the second feature in
case two features are defined.</p>
</dd>
<dt><code>n_sample_points</code></dt><dd><p>Number of points used to sample continuous features.</p>
</dd>
<dt><code>data</code></dt><dd><p>A <code>dataObject</code> object, <code>data.table</code> or <code>data.frame</code> that
constitutes the data that are assessed.</p>
</dd>
<dt><code>is_pre_processed</code></dt><dd><p>Flag that indicates whether the data was already
pre-processed externally, e.g. normalised and clustered. Only used if the
<code>data</code> argument is a <code>data.table</code> or <code>data.frame</code>.</p>
</dd>
<dt><code>cl</code></dt><dd><p>Cluster created using the <code>parallel</code> package. This cluster is then
used to speed up computation through parallellisation.</p>
</dd>
<dt><code>evaluation_times</code></dt><dd><p>One or more time points that are used for in analysis of
survival problems when data has to be assessed at a set time, e.g.
calibration. If not provided explicitly, this parameter is read from
settings used at creation of the underlying <code>familiarModel</code> objects. Only
used for <code>survival</code> outcomes.</p>
</dd>
<dt><code>ensemble_method</code></dt><dd><p>Method for ensembling predictions from models for the
same sample. Available methods are:
</p>

<ul>
<li> <p><code>median</code> (default): Use the median of the predicted values as the ensemble
value for a sample.
</p>
</li>
<li> <p><code>mean</code>: Use the mean of the predicted values as the ensemble value for a
sample.
</p>
</li></ul>
</dd>
<dt><code>verbose</code></dt><dd><p>Flag to indicate whether feedback should be provided on the
computation and extraction of various data elements.</p>
</dd>
<dt><code>message_indent</code></dt><dd><p>Number of indentation steps for messages shown during
computation and extraction of various data elements.</p>
</dd>
<dt><code>sample_limit</code></dt><dd><p>(<em>optional</em>) Set the upper limit of the number of samples
that are used during evaluation steps. Cannot be less than 20.
</p>
<p>This setting can be specified per data element by providing a parameter
value in a named list with data elements, e.g.
<code>list("sample_similarity"=100, "permutation_vimp"=1000)</code>.
</p>
<p>This parameter can be set for the following data elements:
<code>sample_similarity</code> and <code>ice_data</code>.</p>
</dd>
<dt><code>detail_level</code></dt><dd><p>(<em>optional</em>) Sets the level at which results are computed
and aggregated.
</p>

<ul>
<li> <p><code>ensemble</code>: Results are computed at the ensemble level, i.e. over all
models in the ensemble. This means that, for example, bias-corrected
estimates of model performance are assessed by creating (at least) 20
bootstraps and computing the model performance of the ensemble model for
each bootstrap.
</p>
</li>
<li> <p><code>hybrid</code> (default): Results are computed at the level of models in an
ensemble. This means that, for example, bias-corrected estimates of model
performance are directly computed using the models in the ensemble. If there
are at least 20 trained models in the ensemble, performance is computed for
each model, in contrast to <code>ensemble</code> where performance is computed for the
ensemble of models. If there are less than 20 trained models in the
ensemble, bootstraps are created so that at least 20 point estimates can be
made.
</p>
</li>
<li> <p><code>model</code>: Results are computed at the model level. This means that, for
example, bias-corrected estimates of model performance are assessed by
creating (at least) 20 bootstraps and computing the performance of the model
for each bootstrap.
</p>
</li></ul>

<p>Note that each level of detail has a different interpretation for bootstrap
confidence intervals. For <code>ensemble</code> and <code>model</code> these are the confidence
intervals for the ensemble and an individual model, respectively. That is,
the confidence interval describes the range where an estimate produced by a
respective ensemble or model trained on a repeat of the experiment may be
found with the probability of the confidence level. For <code>hybrid</code>, it
represents the range where any single model trained on a repeat of the
experiment may be found with the probability of the confidence level. By
definition, confidence intervals obtained using <code>hybrid</code> are at least as
wide as those for <code>ensemble</code>. <code>hybrid</code> offers the correct interpretation if
the goal of the analysis is to assess the result of a single, unspecified,
model.
</p>
<p><code>hybrid</code> is generally computationally less expensive then <code>ensemble</code>, which
in turn is somewhat less expensive than <code>model</code>.
</p>
<p>A non-default <code>detail_level</code> parameter can be specified for separate
evaluation steps by providing a parameter value in a named list with data
elements, e.g. <code>list("auc_data"="ensemble", "model_performance"="hybrid")</code>.
This parameter can be set for the following data elements: <code>auc_data</code>,
<code>decision_curve_analyis</code>, <code>model_performance</code>, <code>permutation_vimp</code>,
<code>ice_data</code>, <code>prediction_data</code> and <code>confusion_matrix</code>.</p>
</dd>
<dt><code>estimation_type</code></dt><dd><p>(<em>optional</em>) Sets the type of estimation that should be
possible. This has the following options:
</p>

<ul>
<li> <p><code>point</code>: Point estimates.
</p>
</li>
<li> <p><code>bias_correction</code> or <code>bc</code>: Bias-corrected estimates. A bias-corrected
estimate is computed from (at least) 20 point estimates, and <code>familiar</code> may
bootstrap the data to create them.
</p>
</li>
<li> <p><code>bootstrap_confidence_interval</code> or <code>bci</code> (default): Bias-corrected
estimates with bootstrap confidence intervals (Efron and Hastie, 2016). The
number of point estimates required depends on the <code>confidence_level</code>
parameter, and <code>familiar</code> may bootstrap the data to create them.
</p>
</li></ul>

<p>As with <code>detail_level</code>, a non-default <code>estimation_type</code> parameter can be
specified for separate evaluation steps by providing a parameter value in a
named list with data elements, e.g. <code>list("auc_data"="bci", "model_performance"="point")</code>. This parameter can be set for the following
data elements: <code>auc_data</code>, <code>decision_curve_analyis</code>, <code>model_performance</code>,
<code>permutation_vimp</code>, <code>ice_data</code>, and <code>prediction_data</code>.</p>
</dd>
<dt><code>confidence_level</code></dt><dd><p>(<em>optional</em>) Numeric value for the level at which
confidence intervals are determined. In the case bootstraps are used to
determine the confidence intervals bootstrap estimation, <code>familiar</code> uses the
rule of thumb <code class="reqn">n = 20 / ci.level</code> to determine the number of required
bootstraps.
</p>
<p>The default value is <code>0.95</code>.</p>
</dd>
<dt><code>bootstrap_ci_method</code></dt><dd><p>(<em>optional</em>) Method used to determine bootstrap
confidence intervals (Efron and Hastie, 2016). The following methods are
implemented:
</p>

<ul>
<li> <p><code>percentile</code> (default): Confidence intervals obtained using the percentile
method.
</p>
</li>
<li> <p><code>bc</code>: Bias-corrected confidence intervals.
</p>
</li></ul>

<p>Note that the standard method is not implemented because this method is
often not suitable due to non-normal distributions. The bias-corrected and
accelerated (BCa) method is not implemented yet.</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Details</h3>

<p>This function generates individual conditional expectation plots.
These plots come in two varieties, namely 1D and 2D. 1D plots show the
predicted value as function of a single feature, whereas 2D plots show the
predicted value as a function of two features.
</p>
<p>Available splitting variables are: <code>feature_x</code>, <code>feature_y</code> (2D only),
<code>fs_method</code>, <code>learner</code>, <code>data_set</code> and <code>positive_class</code> (categorical
outcomes) or <code>evaluation_time</code> (survival outcomes). By default, for 1D ICE
plots the data are split by <code>feature_x</code>, <code>fs_method</code> and <code>learner</code>, with
faceting by <code>data_set</code>, <code>positive_class</code> or <code>evaluation_time</code>. If only
partial dependence is shown, <code>positive_class</code> and <code>evaluation_time</code> are used
to set colours instead. For 2D plots, by default the data are split by
<code>feature_x</code>, <code>fs_method</code> and <code>learner</code>, with faceting by <code>data_set</code>,
<code>positive_class</code> or <code>evaluation_time</code>. The <code>color_by</code> argument cannot be
used with 2D plots, and attempting to do so causes an error. Attempting to
specify <code>feature_x</code> or <code>feature_y</code> for <code>color_by</code> will likewise result in an
error, as multiple features cannot be shown in the same facet.
</p>
<p>The splitting variables indicated by <code>color_by</code> are coloured according to
the <code>discrete_palette</code> parameter. This parameter is therefore only used for
1D plots. Available palettes for <code>discrete_palette</code> and <code>gradient_palette</code>
are those listed by <code>grDevices::palette.pals()</code> (requires R &gt;= 4.0.0),
<code>grDevices::hcl.pals()</code> (requires R &gt;= 3.6.0) and <code>rainbow</code>, <code>heat.colors</code>,
<code>terrain.colors</code>, <code>topo.colors</code> and <code>cm.colors</code>, which correspond to the
palettes of the same name in <code>grDevices</code>. If not specified, a default
palette based on palettes in Tableau are used. You may also specify your own
palette by using colour names listed by <code>grDevices::colors()</code> or through
hexadecimal RGB strings.
</p>
<p>Bootstrap confidence intervals of the partial dependence plots can be shown
using various styles set by <code>conf_int_style</code>:
</p>

<ul>
<li> <p><code>ribbon</code> (default): confidence intervals are shown as a ribbon with an
opacity of <code>conf_int_alpha</code> around the point estimate of the partial
dependence.
</p>
</li>
<li> <p><code>step</code> (default): confidence intervals are shown as a step function around
the point estimate of the partial dependence.
</p>
</li>
<li> <p><code>none</code>: confidence intervals are not shown. The point estimate of the
partial dependence is shown as usual.
</p>
</li></ul>

<p>Note that when bootstrap confidence intervals were computed, they were also
computed for individual samples in individual conditional expectation plots.
To avoid clutter, only point estimates for individual samples are shown.
</p>
<p>Labelling methods such as <code>set_fs_method_names</code> or <code>set_data_set_names</code> can
be applied to the <code>familiarCollection</code> object to update labels, and order
the output in the figure.
</p>


<h3>Value</h3>

<p><code>NULL</code> or list of plot objects, if <code>dir_path</code> is <code>NULL</code>.
</p>

<hr>
<h2 id='plot_kaplan_meier'>Plot Kaplan-Meier survival curves.</h2><span id='topic+plot_kaplan_meier'></span><span id='topic+plot_kaplan_meier+2CANY-method'></span><span id='topic+plot_kaplan_meier+2CfamiliarCollection-method'></span>

<h3>Description</h3>

<p>This function creates Kaplan-Meier survival curves from
stratification data stored in a familiarCollection object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_kaplan_meier(
  object,
  draw = FALSE,
  dir_path = NULL,
  split_by = NULL,
  color_by = NULL,
  linetype_by = NULL,
  facet_by = NULL,
  facet_wrap_cols = NULL,
  combine_legend = TRUE,
  ggtheme = NULL,
  discrete_palette = NULL,
  x_label = "time",
  x_label_shared = "column",
  y_label = "survival probability",
  y_label_shared = "row",
  legend_label = waiver(),
  plot_title = waiver(),
  plot_sub_title = waiver(),
  caption = NULL,
  x_range = NULL,
  x_n_breaks = 5,
  x_breaks = NULL,
  y_range = c(0, 1),
  y_n_breaks = 5,
  y_breaks = NULL,
  confidence_level = NULL,
  conf_int_style = c("ribbon", "step", "none"),
  conf_int_alpha = 0.4,
  censoring = TRUE,
  censor_shape = "plus",
  show_logrank = TRUE,
  show_survival_table = TRUE,
  width = waiver(),
  height = waiver(),
  units = waiver(),
  export_collection = FALSE,
  ...
)

## S4 method for signature 'ANY'
plot_kaplan_meier(
  object,
  draw = FALSE,
  dir_path = NULL,
  split_by = NULL,
  color_by = NULL,
  linetype_by = NULL,
  facet_by = NULL,
  facet_wrap_cols = NULL,
  combine_legend = TRUE,
  ggtheme = NULL,
  discrete_palette = NULL,
  x_label = "time",
  x_label_shared = "column",
  y_label = "survival probability",
  y_label_shared = "row",
  legend_label = waiver(),
  plot_title = waiver(),
  plot_sub_title = waiver(),
  caption = NULL,
  x_range = NULL,
  x_n_breaks = 5,
  x_breaks = NULL,
  y_range = c(0, 1),
  y_n_breaks = 5,
  y_breaks = NULL,
  confidence_level = NULL,
  conf_int_style = c("ribbon", "step", "none"),
  conf_int_alpha = 0.4,
  censoring = TRUE,
  censor_shape = "plus",
  show_logrank = TRUE,
  show_survival_table = TRUE,
  width = waiver(),
  height = waiver(),
  units = waiver(),
  export_collection = FALSE,
  ...
)

## S4 method for signature 'familiarCollection'
plot_kaplan_meier(
  object,
  draw = FALSE,
  dir_path = NULL,
  split_by = NULL,
  color_by = NULL,
  linetype_by = NULL,
  facet_by = NULL,
  facet_wrap_cols = NULL,
  combine_legend = TRUE,
  ggtheme = NULL,
  discrete_palette = NULL,
  x_label = "time",
  x_label_shared = "column",
  y_label = "survival probability",
  y_label_shared = "row",
  legend_label = waiver(),
  plot_title = waiver(),
  plot_sub_title = waiver(),
  caption = NULL,
  x_range = NULL,
  x_n_breaks = 5,
  x_breaks = NULL,
  y_range = c(0, 1),
  y_n_breaks = 5,
  y_breaks = NULL,
  confidence_level = NULL,
  conf_int_style = c("ribbon", "step", "none"),
  conf_int_alpha = 0.4,
  censoring = TRUE,
  censor_shape = "plus",
  show_logrank = TRUE,
  show_survival_table = TRUE,
  width = waiver(),
  height = waiver(),
  units = waiver(),
  export_collection = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_kaplan_meier_+3A_object">object</code></td>
<td>
<p><code>familiarCollection</code> object, or one or more <code>familiarData</code>
objects, that will be internally converted to a <code>familiarCollection</code> object.
It is also possible to provide a <code>familiarEnsemble</code> or one or more
<code>familiarModel</code> objects together with the data from which data is computed
prior to export. Paths to such files can also be provided.</p>
</td></tr>
<tr><td><code id="plot_kaplan_meier_+3A_draw">draw</code></td>
<td>
<p>(<em>optional</em>) Draws the plot if TRUE.</p>
</td></tr>
<tr><td><code id="plot_kaplan_meier_+3A_dir_path">dir_path</code></td>
<td>
<p>(<em>optional</em>) Path to the directory where created figures are
saved to. Output is saved in the <code>stratification</code> subdirectory. If <code>NULL</code> no
figures are saved, but are returned instead.</p>
</td></tr>
<tr><td><code id="plot_kaplan_meier_+3A_split_by">split_by</code></td>
<td>
<p>(<em>optional</em>) Splitting variables. This refers to column names
on which datasets are split. A separate figure is created for each split.
See details for available variables.</p>
</td></tr>
<tr><td><code id="plot_kaplan_meier_+3A_color_by">color_by</code></td>
<td>
<p>(<em>optional</em>) Variables used to determine fill colour of plot
objects. The variables cannot overlap with those provided to the <code>split_by</code>
argument, but may overlap with other arguments. See details for available
variables.</p>
</td></tr>
<tr><td><code id="plot_kaplan_meier_+3A_linetype_by">linetype_by</code></td>
<td>
<p>(<em>optional</em>) Variables that are used to determine the
linetype of lines in a plot. The variables cannot overlap with those
provided to the <code>split_by</code> argument, but may overlap with other arguments.
Sett details for available variables.</p>
</td></tr>
<tr><td><code id="plot_kaplan_meier_+3A_facet_by">facet_by</code></td>
<td>
<p>(<em>optional</em>) Variables used to determine how and if facets of
each figure appear. In case the <code>facet_wrap_cols</code> argument is <code>NULL</code>, the
first variable is used to define columns, and the remaing variables are
used to define rows of facets. The variables cannot overlap with those
provided to the <code>split_by</code> argument, but may overlap with other arguments.
See details for available variables.</p>
</td></tr>
<tr><td><code id="plot_kaplan_meier_+3A_facet_wrap_cols">facet_wrap_cols</code></td>
<td>
<p>(<em>optional</em>) Number of columns to generate when facet
wrapping. If NULL, a facet grid is produced instead.</p>
</td></tr>
<tr><td><code id="plot_kaplan_meier_+3A_combine_legend">combine_legend</code></td>
<td>
<p>(<em>optional</em>) Flag to indicate whether the same legend
is to be shared by multiple aesthetics, such as those specified by
<code>color_by</code> and <code>linetype_by</code> arguments.</p>
</td></tr>
<tr><td><code id="plot_kaplan_meier_+3A_ggtheme">ggtheme</code></td>
<td>
<p>(<em>optional</em>) <code>ggplot</code> theme to use for plotting.</p>
</td></tr>
<tr><td><code id="plot_kaplan_meier_+3A_discrete_palette">discrete_palette</code></td>
<td>
<p>(<em>optional</em>) Palette to use to color the different
risk strata in case a non-singular variable was provided to the <code>color_by</code>
argument.</p>
</td></tr>
<tr><td><code id="plot_kaplan_meier_+3A_x_label">x_label</code></td>
<td>
<p>(<em>optional</em>) Label to provide to the x-axis. If NULL, no label
is shown.</p>
</td></tr>
<tr><td><code id="plot_kaplan_meier_+3A_x_label_shared">x_label_shared</code></td>
<td>
<p>(<em>optional</em>) Sharing of x-axis labels between facets.
One of three values:
</p>

<ul>
<li> <p><code>overall</code>: A single label is placed at the bottom of the figure. Tick
text (but not the ticks themselves) is removed for all but the bottom facet
plot(s).
</p>
</li>
<li> <p><code>column</code>: A label is placed at the bottom of each column. Tick text (but
not the ticks themselves) is removed for all but the bottom facet plot(s).
</p>
</li>
<li> <p><code>individual</code>: A label is placed below each facet plot. Tick text is kept.
</p>
</li></ul>
</td></tr>
<tr><td><code id="plot_kaplan_meier_+3A_y_label">y_label</code></td>
<td>
<p>(<em>optional</em>) Label to provide to the y-axis. If NULL, no label
is shown.</p>
</td></tr>
<tr><td><code id="plot_kaplan_meier_+3A_y_label_shared">y_label_shared</code></td>
<td>
<p>(<em>optional</em>) Sharing of y-axis labels between facets.
One of three values:
</p>

<ul>
<li> <p><code>overall</code>: A single label is placed to the left of the figure. Tick text
(but not the ticks themselves) is removed for all but the left-most facet
plot(s).
</p>
</li>
<li> <p><code>row</code>: A label is placed to the left of each row. Tick text (but not the
ticks themselves) is removed for all but the left-most facet plot(s).
</p>
</li>
<li> <p><code>individual</code>: A label is placed below each facet plot. Tick text is kept.
</p>
</li></ul>
</td></tr>
<tr><td><code id="plot_kaplan_meier_+3A_legend_label">legend_label</code></td>
<td>
<p>(<em>optional</em>) Label to provide to the legend. If NULL, the
legend will not have a name.</p>
</td></tr>
<tr><td><code id="plot_kaplan_meier_+3A_plot_title">plot_title</code></td>
<td>
<p>(<em>optional</em>) Label to provide as figure title. If NULL, no
title is shown.</p>
</td></tr>
<tr><td><code id="plot_kaplan_meier_+3A_plot_sub_title">plot_sub_title</code></td>
<td>
<p>(<em>optional</em>) Label to provide as figure subtitle. If
NULL, no subtitle is shown.</p>
</td></tr>
<tr><td><code id="plot_kaplan_meier_+3A_caption">caption</code></td>
<td>
<p>(<em>optional</em>) Label to provide as figure caption. If NULL, no
caption is shown.</p>
</td></tr>
<tr><td><code id="plot_kaplan_meier_+3A_x_range">x_range</code></td>
<td>
<p>(<em>optional</em>) Value range for the x-axis.</p>
</td></tr>
<tr><td><code id="plot_kaplan_meier_+3A_x_n_breaks">x_n_breaks</code></td>
<td>
<p>(<em>optional</em>) Number of breaks to show on the x-axis of the
plot. <code>x_n_breaks</code> is used to determine the <code>x_breaks</code> argument in case it
is unset.</p>
</td></tr>
<tr><td><code id="plot_kaplan_meier_+3A_x_breaks">x_breaks</code></td>
<td>
<p>(<em>optional</em>) Break points on the x-axis of the plot.</p>
</td></tr>
<tr><td><code id="plot_kaplan_meier_+3A_y_range">y_range</code></td>
<td>
<p>(<em>optional</em>) Value range for the y-axis.</p>
</td></tr>
<tr><td><code id="plot_kaplan_meier_+3A_y_n_breaks">y_n_breaks</code></td>
<td>
<p>(<em>optional</em>) Number of breaks to show on the y-axis of the
plot. <code>y_n_breaks</code> is used to determine the <code>y_breaks</code> argument in case it
is unset.</p>
</td></tr>
<tr><td><code id="plot_kaplan_meier_+3A_y_breaks">y_breaks</code></td>
<td>
<p>(<em>optional</em>) Break points on the y-axis of the plot.</p>
</td></tr>
<tr><td><code id="plot_kaplan_meier_+3A_confidence_level">confidence_level</code></td>
<td>
<p>(<em>optional</em>) Confidence level for the strata in the
plot.</p>
</td></tr>
<tr><td><code id="plot_kaplan_meier_+3A_conf_int_style">conf_int_style</code></td>
<td>
<p>(<em>optional</em>) Confidence interval style. See details for
allowed styles.</p>
</td></tr>
<tr><td><code id="plot_kaplan_meier_+3A_conf_int_alpha">conf_int_alpha</code></td>
<td>
<p>(<em>optional</em>) Alpha value to determine transparency of
confidence intervals or, alternatively, other plot elements with which the
confidence interval overlaps. Only values between 0.0 (fully transparent)
and 1.0 (fully opaque) are allowed.</p>
</td></tr>
<tr><td><code id="plot_kaplan_meier_+3A_censoring">censoring</code></td>
<td>
<p>(<em>optional</em>) Flag to indicate whether censored samples should
be indicated on the survival curve.</p>
</td></tr>
<tr><td><code id="plot_kaplan_meier_+3A_censor_shape">censor_shape</code></td>
<td>
<p>(<em>optional</em>) Shape used to indicate censored samples on
the survival curve. Available shapes are documented in the <code>ggplot2</code>
vignette <em>Aesthetic specifications</em>. By default a plus shape is used.</p>
</td></tr>
<tr><td><code id="plot_kaplan_meier_+3A_show_logrank">show_logrank</code></td>
<td>
<p>(<em>optional</em>) Specifies whether the results of a logrank
test to assess differences between the risk strata is annotated in the plot.
A log-rank test can only be shown when <code>color_by</code> and <code>linestyle_by</code> are
either unset, or only contain <code>risk_group</code>.</p>
</td></tr>
<tr><td><code id="plot_kaplan_meier_+3A_show_survival_table">show_survival_table</code></td>
<td>
<p>(<em>optional</em>) Specifies whether a survival table is
shown below the Kaplan-Meier survival curves. Survival in the risk strata is
assessed for each of the breaks in <code>x_breaks</code>.</p>
</td></tr>
<tr><td><code id="plot_kaplan_meier_+3A_width">width</code></td>
<td>
<p>(<em>optional</em>) Width of the plot. A default value is derived from
the number of facets.</p>
</td></tr>
<tr><td><code id="plot_kaplan_meier_+3A_height">height</code></td>
<td>
<p>(<em>optional</em>) Height of the plot. A default value is derived from
number of facets and the inclusion of survival tables.</p>
</td></tr>
<tr><td><code id="plot_kaplan_meier_+3A_units">units</code></td>
<td>
<p>(<em>optional</em>) Plot size unit. Either <code>cm</code> (default), <code>mm</code> or <code style="white-space: pre;">&#8288;in&#8288;</code>.</p>
</td></tr>
<tr><td><code id="plot_kaplan_meier_+3A_export_collection">export_collection</code></td>
<td>
<p>(<em>optional</em>) Exports the collection if TRUE.</p>
</td></tr>
<tr><td><code id="plot_kaplan_meier_+3A_...">...</code></td>
<td>

<p>Arguments passed on to <code><a href="#topic+as_familiar_collection">as_familiar_collection</a></code>, <code><a href="ggplot2.html#topic+ggsave">ggplot2::ggsave</a></code>, <code><a href="#topic+extract_risk_stratification_data">extract_risk_stratification_data</a></code>
</p>

<dl>
<dt><code>familiar_data_names</code></dt><dd><p>Names of the dataset(s). Only used if the <code>object</code> parameter
is one or more <code>familiarData</code> objects.</p>
</dd>
<dt><code>collection_name</code></dt><dd><p>Name of the collection.</p>
</dd>
<dt><code>filename</code></dt><dd><p>File name to create on disk.</p>
</dd>
<dt><code>plot</code></dt><dd><p>Plot to save, defaults to last plot displayed.</p>
</dd>
<dt><code>device</code></dt><dd><p>Device to use. Can either be a device function
(e.g. <a href="grDevices.html#topic+png">png</a>), or one of &quot;eps&quot;, &quot;ps&quot;, &quot;tex&quot; (pictex),
&quot;pdf&quot;, &quot;jpeg&quot;, &quot;tiff&quot;, &quot;png&quot;, &quot;bmp&quot;, &quot;svg&quot; or &quot;wmf&quot; (windows only).</p>
</dd>
<dt><code>path</code></dt><dd><p>Path of the directory to save plot to: <code>path</code> and <code>filename</code>
are combined to create the fully qualified file name. Defaults to the
working directory.</p>
</dd>
<dt><code>scale</code></dt><dd><p>Multiplicative scaling factor.</p>
</dd>
<dt><code>dpi</code></dt><dd><p>Plot resolution. Also accepts a string input: &quot;retina&quot; (320),
&quot;print&quot; (300), or &quot;screen&quot; (72). Applies only to raster output types.</p>
</dd>
<dt><code>limitsize</code></dt><dd><p>When <code>TRUE</code> (the default), <code>ggsave()</code> will not
save images larger than 50x50 inches, to prevent the common error of
specifying dimensions in pixels.</p>
</dd>
<dt><code>bg</code></dt><dd><p>Background colour. If <code>NULL</code>, uses the <code>plot.background</code> fill value
from the plot theme.</p>
</dd>
<dt><code>data</code></dt><dd><p>A <code>dataObject</code> object, <code>data.table</code> or <code>data.frame</code> that
constitutes the data that are assessed.</p>
</dd>
<dt><code>is_pre_processed</code></dt><dd><p>Flag that indicates whether the data was already
pre-processed externally, e.g. normalised and clustered. Only used if the
<code>data</code> argument is a <code>data.table</code> or <code>data.frame</code>.</p>
</dd>
<dt><code>cl</code></dt><dd><p>Cluster created using the <code>parallel</code> package. This cluster is then
used to speed up computation through parallellisation.</p>
</dd>
<dt><code>ensemble_method</code></dt><dd><p>Method for ensembling predictions from models for the
same sample. Available methods are:
</p>

<ul>
<li> <p><code>median</code> (default): Use the median of the predicted values as the ensemble
value for a sample.
</p>
</li>
<li> <p><code>mean</code>: Use the mean of the predicted values as the ensemble value for a
sample.
</p>
</li></ul>
</dd>
<dt><code>verbose</code></dt><dd><p>Flag to indicate whether feedback should be provided on the
computation and extraction of various data elements.</p>
</dd>
<dt><code>message_indent</code></dt><dd><p>Number of indentation steps for messages shown during
computation and extraction of various data elements.</p>
</dd>
<dt><code>detail_level</code></dt><dd><p>(<em>optional</em>) Sets the level at which results are computed
and aggregated.
</p>

<ul>
<li> <p><code>ensemble</code>: Results are computed at the ensemble level, i.e. over all
models in the ensemble. This means that, for example, bias-corrected
estimates of model performance are assessed by creating (at least) 20
bootstraps and computing the model performance of the ensemble model for
each bootstrap.
</p>
</li>
<li> <p><code>hybrid</code> (default): Results are computed at the level of models in an
ensemble. This means that, for example, bias-corrected estimates of model
performance are directly computed using the models in the ensemble. If there
are at least 20 trained models in the ensemble, performance is computed for
each model, in contrast to <code>ensemble</code> where performance is computed for the
ensemble of models. If there are less than 20 trained models in the
ensemble, bootstraps are created so that at least 20 point estimates can be
made.
</p>
</li>
<li> <p><code>model</code>: Results are computed at the model level. This means that, for
example, bias-corrected estimates of model performance are assessed by
creating (at least) 20 bootstraps and computing the performance of the model
for each bootstrap.
</p>
</li></ul>

<p>Note that each level of detail has a different interpretation for bootstrap
confidence intervals. For <code>ensemble</code> and <code>model</code> these are the confidence
intervals for the ensemble and an individual model, respectively. That is,
the confidence interval describes the range where an estimate produced by a
respective ensemble or model trained on a repeat of the experiment may be
found with the probability of the confidence level. For <code>hybrid</code>, it
represents the range where any single model trained on a repeat of the
experiment may be found with the probability of the confidence level. By
definition, confidence intervals obtained using <code>hybrid</code> are at least as
wide as those for <code>ensemble</code>. <code>hybrid</code> offers the correct interpretation if
the goal of the analysis is to assess the result of a single, unspecified,
model.
</p>
<p><code>hybrid</code> is generally computationally less expensive then <code>ensemble</code>, which
in turn is somewhat less expensive than <code>model</code>.
</p>
<p>A non-default <code>detail_level</code> parameter can be specified for separate
evaluation steps by providing a parameter value in a named list with data
elements, e.g. <code>list("auc_data"="ensemble", "model_performance"="hybrid")</code>.
This parameter can be set for the following data elements: <code>auc_data</code>,
<code>decision_curve_analyis</code>, <code>model_performance</code>, <code>permutation_vimp</code>,
<code>ice_data</code>, <code>prediction_data</code> and <code>confusion_matrix</code>.</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Details</h3>

<p>This function generates a Kaplan-Meier survival plot based on risk
group stratification by the learners.
</p>
<p><code>familiar</code> does not determine what units the x-axis has or what kind of
survival the y-axis represents. It is therefore recommended to provide
<code>x_label</code> and <code>y_label</code> arguments.
</p>
<p>Available splitting variables are: <code>fs_method</code>, <code>learner</code>, <code>data_set</code>,
<code>risk_group</code> and <code>stratification_method</code>. By default, separate figures are
created for each combination of <code>fs_method</code> and <code>learner</code>, with faceting by
<code>data_set</code>, colouring of the strata in each individual plot by <code>risk_group</code>.
</p>
<p>Available palettes for <code>discrete_palette</code> are those listed by
<code>grDevices::palette.pals()</code> (requires R &gt;= 4.0.0), <code>grDevices::hcl.pals()</code>
(requires R &gt;= 3.6.0) and <code>rainbow</code>, <code>heat.colors</code>, <code>terrain.colors</code>,
<code>topo.colors</code> and <code>cm.colors</code>, which correspond to the palettes of the same
name in <code>grDevices</code>. If not specified, a default palette based on palettes
in Tableau are used. You may also specify your own palette by using colour
names listed by <code>grDevices::colors()</code> or through hexadecimal RGB strings.
</p>
<p>Greenwood confidence intervals of the Kaplan-Meier curve can be shown using
various styles set by <code>conf_int_style</code>:
</p>

<ul>
<li> <p><code>ribbon</code> (default): confidence intervals are shown as a ribbon with an
opacity of <code>conf_int_alpha</code> around the point estimate of the Kaplan-Meier
curve.
</p>
</li>
<li> <p><code>step</code> (default): confidence intervals are shown as a step function around
the point estimate of the Kaplan-Meier curve.
</p>
</li>
<li> <p><code>none</code>: confidence intervals are not shown. The point estimate of the ROC
curve is shown as usual.
</p>
</li></ul>

<p>Labelling methods such as <code>set_risk_group_names</code> or <code>set_data_set_names</code> can
be applied to the <code>familiarCollection</code> object to update labels, and order
the output in the figure.
</p>


<h3>Value</h3>

<p><code>NULL</code> or list of plot objects, if <code>dir_path</code> is <code>NULL</code>.
</p>

<hr>
<h2 id='plot_model_performance'>Plot model performance.</h2><span id='topic+plot_model_performance'></span><span id='topic+plot_model_performance+2CANY-method'></span><span id='topic+plot_model_performance+2CfamiliarCollection-method'></span>

<h3>Description</h3>

<p>This method creates plots that show model performance from the
data stored in a familiarCollection object. This method may create several
types of plots, as determined by <code>plot_type</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_model_performance(
  object,
  draw = FALSE,
  dir_path = NULL,
  split_by = NULL,
  x_axis_by = NULL,
  y_axis_by = NULL,
  color_by = NULL,
  facet_by = NULL,
  facet_wrap_cols = NULL,
  plot_type = NULL,
  ggtheme = NULL,
  discrete_palette = NULL,
  gradient_palette = NULL,
  gradient_palette_range = waiver(),
  x_label = waiver(),
  y_label = waiver(),
  legend_label = waiver(),
  plot_title = waiver(),
  plot_sub_title = waiver(),
  caption = NULL,
  rotate_x_tick_labels = waiver(),
  y_range = NULL,
  y_n_breaks = 5,
  y_breaks = NULL,
  width = waiver(),
  height = waiver(),
  units = waiver(),
  annotate_performance = NULL,
  export_collection = FALSE,
  ...
)

## S4 method for signature 'ANY'
plot_model_performance(
  object,
  draw = FALSE,
  dir_path = NULL,
  split_by = NULL,
  x_axis_by = NULL,
  y_axis_by = NULL,
  color_by = NULL,
  facet_by = NULL,
  facet_wrap_cols = NULL,
  plot_type = NULL,
  ggtheme = NULL,
  discrete_palette = NULL,
  gradient_palette = NULL,
  gradient_palette_range = waiver(),
  x_label = waiver(),
  y_label = waiver(),
  legend_label = waiver(),
  plot_title = waiver(),
  plot_sub_title = waiver(),
  caption = NULL,
  rotate_x_tick_labels = waiver(),
  y_range = NULL,
  y_n_breaks = 5,
  y_breaks = NULL,
  width = waiver(),
  height = waiver(),
  units = waiver(),
  annotate_performance = NULL,
  export_collection = FALSE,
  ...
)

## S4 method for signature 'familiarCollection'
plot_model_performance(
  object,
  draw = FALSE,
  dir_path = NULL,
  split_by = NULL,
  x_axis_by = NULL,
  y_axis_by = NULL,
  color_by = NULL,
  facet_by = NULL,
  facet_wrap_cols = NULL,
  plot_type = NULL,
  ggtheme = NULL,
  discrete_palette = NULL,
  gradient_palette = NULL,
  gradient_palette_range = waiver(),
  x_label = waiver(),
  y_label = waiver(),
  legend_label = waiver(),
  plot_title = waiver(),
  plot_sub_title = waiver(),
  caption = NULL,
  rotate_x_tick_labels = waiver(),
  y_range = NULL,
  y_n_breaks = 5,
  y_breaks = NULL,
  width = waiver(),
  height = waiver(),
  units = waiver(),
  annotate_performance = NULL,
  export_collection = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_model_performance_+3A_object">object</code></td>
<td>
<p><code>familiarCollection</code> object, or one or more <code>familiarData</code>
objects, that will be internally converted to a <code>familiarCollection</code> object.
It is also possible to provide a <code>familiarEnsemble</code> or one or more
<code>familiarModel</code> objects together with the data from which data is computed
prior to export. Paths to such files can also be provided.</p>
</td></tr>
<tr><td><code id="plot_model_performance_+3A_draw">draw</code></td>
<td>
<p>(<em>optional</em>) Draws the plot if TRUE.</p>
</td></tr>
<tr><td><code id="plot_model_performance_+3A_dir_path">dir_path</code></td>
<td>
<p>(<em>optional</em>) Path to the directory where created performance
plots are saved to. Output is saved in the <code>performance</code> subdirectory. If
<code>NULL</code> no figures are saved, but are returned instead.</p>
</td></tr>
<tr><td><code id="plot_model_performance_+3A_split_by">split_by</code></td>
<td>
<p>(<em>optional</em>) Splitting variables. This refers to column names
on which datasets are split. A separate figure is created for each split.
See details for available variables.</p>
</td></tr>
<tr><td><code id="plot_model_performance_+3A_x_axis_by">x_axis_by</code></td>
<td>
<p>(<em>optional</em>) Variable plotted along the x-axis of a plot.
The variable cannot overlap with variables provided to the <code>split_by</code> and
<code>y_axis_by</code> arguments (if used), but may overlap with other arguments. Only
one variable is allowed for this argument. See details for available
variables.</p>
</td></tr>
<tr><td><code id="plot_model_performance_+3A_y_axis_by">y_axis_by</code></td>
<td>
<p>(<em>optional</em>) Variable plotted along the y-axis of a plot.
The variable cannot overlap with variables provided to the <code>split_by</code> and
<code>x_axis_by</code> arguments (if used), but may overlap with other arguments. Only
one variable is allowed for this argument. See details for available
variables.</p>
</td></tr>
<tr><td><code id="plot_model_performance_+3A_color_by">color_by</code></td>
<td>
<p>(<em>optional</em>) Variables used to determine fill colour of plot
objects. The variables cannot overlap with those provided to the <code>split_by</code>
argument, but may overlap with other arguments. See details for available
variables.</p>
</td></tr>
<tr><td><code id="plot_model_performance_+3A_facet_by">facet_by</code></td>
<td>
<p>(<em>optional</em>) Variables used to determine how and if facets of
each figure appear. In case the <code>facet_wrap_cols</code> argument is <code>NULL</code>, the
first variable is used to define columns, and the remaing variables are
used to define rows of facets. The variables cannot overlap with those
provided to the <code>split_by</code> argument, but may overlap with other arguments.
See details for available variables.</p>
</td></tr>
<tr><td><code id="plot_model_performance_+3A_facet_wrap_cols">facet_wrap_cols</code></td>
<td>
<p>(<em>optional</em>) Number of columns to generate when facet
wrapping. If NULL, a facet grid is produced instead.</p>
</td></tr>
<tr><td><code id="plot_model_performance_+3A_plot_type">plot_type</code></td>
<td>
<p>(<em>optional</em>) Type of plot to draw. This is one of <code>heatmap</code>
(draws a heatmap), <code>barplot</code> (draws a barplot with confidence intervals),
<code>boxplot</code> (draws a boxplot) and <code>violinplot</code> (draws a violin plot). Defaults
to <code>violinplot</code>.
</p>
<p>The choice for <code>plot_type</code> affects several other arguments, e.g. <code>color_by</code>
is not used for <code>heatmap</code> and <code>y_axis_by</code> is only used by <code>heatmap</code>.</p>
</td></tr>
<tr><td><code id="plot_model_performance_+3A_ggtheme">ggtheme</code></td>
<td>
<p>(<em>optional</em>) <code>ggplot</code> theme to use for plotting.</p>
</td></tr>
<tr><td><code id="plot_model_performance_+3A_discrete_palette">discrete_palette</code></td>
<td>
<p>(<em>optional</em>) Palette to use to color the different
plot elements in case a value was provided to the <code>color_by</code> argument. Only
used when <code>plot_type</code> is not <code>heatmap</code>.</p>
</td></tr>
<tr><td><code id="plot_model_performance_+3A_gradient_palette">gradient_palette</code></td>
<td>
<p>(<em>optional</em>) Sequential or divergent palette used to
color the raster in <code>heatmap</code> plots. This argument is not used for other
<code>plot_type</code> value.</p>
</td></tr>
<tr><td><code id="plot_model_performance_+3A_gradient_palette_range">gradient_palette_range</code></td>
<td>
<p>(<em>optional</em>) Numerical range used to span the
gradient. This should be a range of two values, e.g. <code>c(0, 1)</code>. Lower or
upper boundary can be unset by using <code>NA</code>. If not set, the full
metric-specific range is used.</p>
</td></tr>
<tr><td><code id="plot_model_performance_+3A_x_label">x_label</code></td>
<td>
<p>(<em>optional</em>) Label to provide to the x-axis. If NULL, no label
is shown.</p>
</td></tr>
<tr><td><code id="plot_model_performance_+3A_y_label">y_label</code></td>
<td>
<p>(<em>optional</em>) Label to provide to the y-axis. If NULL, no label
is shown.</p>
</td></tr>
<tr><td><code id="plot_model_performance_+3A_legend_label">legend_label</code></td>
<td>
<p>(<em>optional</em>) Label to provide to the legend. If NULL, the
legend will not have a name.</p>
</td></tr>
<tr><td><code id="plot_model_performance_+3A_plot_title">plot_title</code></td>
<td>
<p>(<em>optional</em>) Label to provide as figure title. If NULL, no
title is shown.</p>
</td></tr>
<tr><td><code id="plot_model_performance_+3A_plot_sub_title">plot_sub_title</code></td>
<td>
<p>(<em>optional</em>) Label to provide as figure subtitle. If
NULL, no subtitle is shown.</p>
</td></tr>
<tr><td><code id="plot_model_performance_+3A_caption">caption</code></td>
<td>
<p>(<em>optional</em>) Label to provide as figure caption. If NULL, no
caption is shown.</p>
</td></tr>
<tr><td><code id="plot_model_performance_+3A_rotate_x_tick_labels">rotate_x_tick_labels</code></td>
<td>
<p>(<em>optional</em>) Rotate tick labels on the x-axis by
90 degrees. Defaults to <code>TRUE</code>. Rotation of x-axis tick labels may also be
controlled through the <code>ggtheme</code>. In this case, <code>FALSE</code> should be provided
explicitly.</p>
</td></tr>
<tr><td><code id="plot_model_performance_+3A_y_range">y_range</code></td>
<td>
<p>(<em>optional</em>) Value range for the y-axis.</p>
</td></tr>
<tr><td><code id="plot_model_performance_+3A_y_n_breaks">y_n_breaks</code></td>
<td>
<p>(<em>optional</em>) Number of breaks to show on the y-axis of the
plot. <code>y_n_breaks</code> is used to determine the <code>y_breaks</code> argument in case it
is unset.</p>
</td></tr>
<tr><td><code id="plot_model_performance_+3A_y_breaks">y_breaks</code></td>
<td>
<p>(<em>optional</em>) Break points on the y-axis of the plot.</p>
</td></tr>
<tr><td><code id="plot_model_performance_+3A_width">width</code></td>
<td>
<p>(<em>optional</em>) Width of the plot. A default value is derived from
the number of facets.</p>
</td></tr>
<tr><td><code id="plot_model_performance_+3A_height">height</code></td>
<td>
<p>(<em>optional</em>) Height of the plot. A default value is derived from
the number of features and the number of facets.</p>
</td></tr>
<tr><td><code id="plot_model_performance_+3A_units">units</code></td>
<td>
<p>(<em>optional</em>) Plot size unit. Either <code>cm</code> (default), <code>mm</code> or <code style="white-space: pre;">&#8288;in&#8288;</code>.</p>
</td></tr>
<tr><td><code id="plot_model_performance_+3A_annotate_performance">annotate_performance</code></td>
<td>
<p>(<em>optional</em>) Indicates whether performance in
heatmaps should be annotated with text. Can be <code>none</code>, <code>value</code> (default), or
<code>value_ci</code> (median value plus 95% credibility intervals).</p>
</td></tr>
<tr><td><code id="plot_model_performance_+3A_export_collection">export_collection</code></td>
<td>
<p>(<em>optional</em>) Exports the collection if TRUE.</p>
</td></tr>
<tr><td><code id="plot_model_performance_+3A_...">...</code></td>
<td>

<p>Arguments passed on to <code><a href="#topic+extract_performance">extract_performance</a></code>, <code><a href="#topic+as_familiar_collection">as_familiar_collection</a></code>, <code><a href="ggplot2.html#topic+ggsave">ggplot2::ggsave</a></code>
</p>

<dl>
<dt><code>data</code></dt><dd><p>A <code>dataObject</code> object, <code>data.table</code> or <code>data.frame</code> that
constitutes the data that are assessed.</p>
</dd>
<dt><code>is_pre_processed</code></dt><dd><p>Flag that indicates whether the data was already
pre-processed externally, e.g. normalised and clustered. Only used if the
<code>data</code> argument is a <code>data.table</code> or <code>data.frame</code>.</p>
</dd>
<dt><code>cl</code></dt><dd><p>Cluster created using the <code>parallel</code> package. This cluster is then
used to speed up computation through parallellisation.</p>
</dd>
<dt><code>evaluation_times</code></dt><dd><p>One or more time points that are used for in analysis of
survival problems when data has to be assessed at a set time, e.g.
calibration. If not provided explicitly, this parameter is read from
settings used at creation of the underlying <code>familiarModel</code> objects. Only
used for <code>survival</code> outcomes.</p>
</dd>
<dt><code>ensemble_method</code></dt><dd><p>Method for ensembling predictions from models for the
same sample. Available methods are:
</p>

<ul>
<li> <p><code>median</code> (default): Use the median of the predicted values as the ensemble
value for a sample.
</p>
</li>
<li> <p><code>mean</code>: Use the mean of the predicted values as the ensemble value for a
sample.
</p>
</li></ul>
</dd>
<dt><code>metric</code></dt><dd><p>One or more metrics for assessing model performance. See the
vignette on performance metrics for the available metrics. If not provided
explicitly, this parameter is read from settings used at creation of the
underlying <code>familiarModel</code> objects.</p>
</dd>
<dt><code>verbose</code></dt><dd><p>Flag to indicate whether feedback should be provided on the
computation and extraction of various data elements.</p>
</dd>
<dt><code>message_indent</code></dt><dd><p>Number of indentation steps for messages shown during
computation and extraction of various data elements.</p>
</dd>
<dt><code>detail_level</code></dt><dd><p>(<em>optional</em>) Sets the level at which results are computed
and aggregated.
</p>

<ul>
<li> <p><code>ensemble</code>: Results are computed at the ensemble level, i.e. over all
models in the ensemble. This means that, for example, bias-corrected
estimates of model performance are assessed by creating (at least) 20
bootstraps and computing the model performance of the ensemble model for
each bootstrap.
</p>
</li>
<li> <p><code>hybrid</code> (default): Results are computed at the level of models in an
ensemble. This means that, for example, bias-corrected estimates of model
performance are directly computed using the models in the ensemble. If there
are at least 20 trained models in the ensemble, performance is computed for
each model, in contrast to <code>ensemble</code> where performance is computed for the
ensemble of models. If there are less than 20 trained models in the
ensemble, bootstraps are created so that at least 20 point estimates can be
made.
</p>
</li>
<li> <p><code>model</code>: Results are computed at the model level. This means that, for
example, bias-corrected estimates of model performance are assessed by
creating (at least) 20 bootstraps and computing the performance of the model
for each bootstrap.
</p>
</li></ul>

<p>Note that each level of detail has a different interpretation for bootstrap
confidence intervals. For <code>ensemble</code> and <code>model</code> these are the confidence
intervals for the ensemble and an individual model, respectively. That is,
the confidence interval describes the range where an estimate produced by a
respective ensemble or model trained on a repeat of the experiment may be
found with the probability of the confidence level. For <code>hybrid</code>, it
represents the range where any single model trained on a repeat of the
experiment may be found with the probability of the confidence level. By
definition, confidence intervals obtained using <code>hybrid</code> are at least as
wide as those for <code>ensemble</code>. <code>hybrid</code> offers the correct interpretation if
the goal of the analysis is to assess the result of a single, unspecified,
model.
</p>
<p><code>hybrid</code> is generally computationally less expensive then <code>ensemble</code>, which
in turn is somewhat less expensive than <code>model</code>.
</p>
<p>A non-default <code>detail_level</code> parameter can be specified for separate
evaluation steps by providing a parameter value in a named list with data
elements, e.g. <code>list("auc_data"="ensemble", "model_performance"="hybrid")</code>.
This parameter can be set for the following data elements: <code>auc_data</code>,
<code>decision_curve_analyis</code>, <code>model_performance</code>, <code>permutation_vimp</code>,
<code>ice_data</code>, <code>prediction_data</code> and <code>confusion_matrix</code>.</p>
</dd>
<dt><code>estimation_type</code></dt><dd><p>(<em>optional</em>) Sets the type of estimation that should be
possible. This has the following options:
</p>

<ul>
<li> <p><code>point</code>: Point estimates.
</p>
</li>
<li> <p><code>bias_correction</code> or <code>bc</code>: Bias-corrected estimates. A bias-corrected
estimate is computed from (at least) 20 point estimates, and <code>familiar</code> may
bootstrap the data to create them.
</p>
</li>
<li> <p><code>bootstrap_confidence_interval</code> or <code>bci</code> (default): Bias-corrected
estimates with bootstrap confidence intervals (Efron and Hastie, 2016). The
number of point estimates required depends on the <code>confidence_level</code>
parameter, and <code>familiar</code> may bootstrap the data to create them.
</p>
</li></ul>

<p>As with <code>detail_level</code>, a non-default <code>estimation_type</code> parameter can be
specified for separate evaluation steps by providing a parameter value in a
named list with data elements, e.g. <code>list("auc_data"="bci", "model_performance"="point")</code>. This parameter can be set for the following
data elements: <code>auc_data</code>, <code>decision_curve_analyis</code>, <code>model_performance</code>,
<code>permutation_vimp</code>, <code>ice_data</code>, and <code>prediction_data</code>.</p>
</dd>
<dt><code>aggregate_results</code></dt><dd><p>(<em>optional</em>) Flag that signifies whether results
should be aggregated during evaluation. If <code>estimation_type</code> is
<code>bias_correction</code> or <code>bc</code>, aggregation leads to a single bias-corrected
estimate. If <code>estimation_type</code> is <code>bootstrap_confidence_interval</code> or <code>bci</code>,
aggregation leads to a single bias-corrected estimate with lower and upper
boundaries of the confidence interval. This has no effect if
<code>estimation_type</code> is <code>point</code>.
</p>
<p>The default value is equal to <code>TRUE</code> except when assessing metrics to assess
model performance, as the default violin plot requires underlying data.
</p>
<p>As with <code>detail_level</code> and <code>estimation_type</code>, a non-default
<code>aggregate_results</code> parameter can be specified for separate evaluation steps
by providing a parameter value in a named list with data elements, e.g.
<code>list("auc_data"=TRUE, , "model_performance"=FALSE)</code>. This parameter exists
for the same elements as <code>estimation_type</code>.</p>
</dd>
<dt><code>confidence_level</code></dt><dd><p>(<em>optional</em>) Numeric value for the level at which
confidence intervals are determined. In the case bootstraps are used to
determine the confidence intervals bootstrap estimation, <code>familiar</code> uses the
rule of thumb <code class="reqn">n = 20 / ci.level</code> to determine the number of required
bootstraps.
</p>
<p>The default value is <code>0.95</code>.</p>
</dd>
<dt><code>bootstrap_ci_method</code></dt><dd><p>(<em>optional</em>) Method used to determine bootstrap
confidence intervals (Efron and Hastie, 2016). The following methods are
implemented:
</p>

<ul>
<li> <p><code>percentile</code> (default): Confidence intervals obtained using the percentile
method.
</p>
</li>
<li> <p><code>bc</code>: Bias-corrected confidence intervals.
</p>
</li></ul>

<p>Note that the standard method is not implemented because this method is
often not suitable due to non-normal distributions. The bias-corrected and
accelerated (BCa) method is not implemented yet.</p>
</dd>
<dt><code>familiar_data_names</code></dt><dd><p>Names of the dataset(s). Only used if the <code>object</code> parameter
is one or more <code>familiarData</code> objects.</p>
</dd>
<dt><code>collection_name</code></dt><dd><p>Name of the collection.</p>
</dd>
<dt><code>filename</code></dt><dd><p>File name to create on disk.</p>
</dd>
<dt><code>plot</code></dt><dd><p>Plot to save, defaults to last plot displayed.</p>
</dd>
<dt><code>device</code></dt><dd><p>Device to use. Can either be a device function
(e.g. <a href="grDevices.html#topic+png">png</a>), or one of &quot;eps&quot;, &quot;ps&quot;, &quot;tex&quot; (pictex),
&quot;pdf&quot;, &quot;jpeg&quot;, &quot;tiff&quot;, &quot;png&quot;, &quot;bmp&quot;, &quot;svg&quot; or &quot;wmf&quot; (windows only).</p>
</dd>
<dt><code>path</code></dt><dd><p>Path of the directory to save plot to: <code>path</code> and <code>filename</code>
are combined to create the fully qualified file name. Defaults to the
working directory.</p>
</dd>
<dt><code>scale</code></dt><dd><p>Multiplicative scaling factor.</p>
</dd>
<dt><code>dpi</code></dt><dd><p>Plot resolution. Also accepts a string input: &quot;retina&quot; (320),
&quot;print&quot; (300), or &quot;screen&quot; (72). Applies only to raster output types.</p>
</dd>
<dt><code>limitsize</code></dt><dd><p>When <code>TRUE</code> (the default), <code>ggsave()</code> will not
save images larger than 50x50 inches, to prevent the common error of
specifying dimensions in pixels.</p>
</dd>
<dt><code>bg</code></dt><dd><p>Background colour. If <code>NULL</code>, uses the <code>plot.background</code> fill value
from the plot theme.</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Details</h3>

<p>This function plots model performance based on empirical bootstraps,
using various plot representations.
</p>
<p>Available splitting variables are: <code>fs_method</code>, <code>learner</code>, <code>data_set</code>,
<code>evaluation_time</code> (survival outcome only) and <code>metric</code>. The default for
<code>heatmap</code> is to split by <code>metric</code>, facet by <code>data_set</code> and
<code>evaluation_time</code>, position <code>learner</code> along the x-axis and <code>fs_method</code> along
the y-axis. The <code>color_by</code> argument is not used. The only valid options for
<code>x_axis_by</code> and <code>y_axis_by</code> are <code>learner</code> and <code>fs_method</code>.
</p>
<p>For other plot types (<code>barplot</code>, <code>boxplot</code> and <code>violinplot</code>), depends on the
number of learners and feature selection methods:
</p>

<ul>
<li> <p><em>one feature selection method and one learner</em>: the default is to split by
<code>metric</code>, and have <code>data_set</code> along the x-axis.
</p>
</li>
<li> <p><em>one feature selection and multiple learners</em>: the default is to split by
<code>metric</code>, facet by <code>data_set</code> and have <code>learner</code> along the x-axis.
</p>
</li>
<li> <p><em>multiple feature selection methods and one learner</em>: the default is to
split by <code>metric</code>, facet by <code>data_set</code> and have <code>fs_method</code> along the
x-axis.
</p>
</li>
<li> <p><em>multiple feature selection methods and learners</em>: the default is to split
by <code>metric</code>, facet by <code>data_set</code>, colour by <code>fs_method</code> and have <code>learner</code>
along the x-axis.
</p>
</li></ul>

<p>If applicable, additional faceting is performed for <code>evaluation_time</code>.
</p>
<p>Available palettes for <code>discrete_palette</code> and <code>gradient_palette</code> are those
listed by <code>grDevices::palette.pals()</code> (requires R &gt;= 4.0.0),
<code>grDevices::hcl.pals()</code> (requires R &gt;= 3.6.0) and <code>rainbow</code>, <code>heat.colors</code>,
<code>terrain.colors</code>, <code>topo.colors</code> and <code>cm.colors</code>, which correspond to the
palettes of the same name in <code>grDevices</code>. If not specified, a default
palette based on palettes in Tableau are used. You may also specify your own
palette by using colour names listed by <code>grDevices::colors()</code> or through
hexadecimal RGB strings.
</p>
<p>Labeling methods such as <code>set_fs_method_names</code> or <code>set_data_set_names</code> can
be applied to the <code>familiarCollection</code> object to update labels, and order
the output in the figure.
</p>


<h3>Value</h3>

<p><code>NULL</code> or list of plot objects, if <code>dir_path</code> is <code>NULL</code>.
</p>

<hr>
<h2 id='plot_pd'>Plot partial dependence.</h2><span id='topic+plot_pd'></span><span id='topic+plot_pd+2CANY-method'></span>

<h3>Description</h3>

<p>This method creates partial dependence plots
based on data in a familiarCollection object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_pd(
  object,
  draw = FALSE,
  dir_path = NULL,
  split_by = NULL,
  color_by = NULL,
  facet_by = NULL,
  facet_wrap_cols = NULL,
  ggtheme = NULL,
  discrete_palette = NULL,
  gradient_palette = NULL,
  gradient_palette_range = NULL,
  x_label = waiver(),
  y_label = waiver(),
  legend_label = waiver(),
  plot_title = waiver(),
  plot_sub_title = waiver(),
  caption = NULL,
  x_range = NULL,
  x_n_breaks = 5,
  x_breaks = NULL,
  y_range = NULL,
  y_n_breaks = 5,
  y_breaks = NULL,
  novelty_range = NULL,
  value_scales = waiver(),
  novelty_scales = waiver(),
  conf_int_style = c("ribbon", "step", "none"),
  conf_int_alpha = 0.4,
  show_novelty = TRUE,
  anchor_values = NULL,
  width = waiver(),
  height = waiver(),
  units = waiver(),
  export_collection = FALSE,
  ...
)

## S4 method for signature 'ANY'
plot_pd(
  object,
  draw = FALSE,
  dir_path = NULL,
  split_by = NULL,
  color_by = NULL,
  facet_by = NULL,
  facet_wrap_cols = NULL,
  ggtheme = NULL,
  discrete_palette = NULL,
  gradient_palette = NULL,
  gradient_palette_range = NULL,
  x_label = waiver(),
  y_label = waiver(),
  legend_label = waiver(),
  plot_title = waiver(),
  plot_sub_title = waiver(),
  caption = NULL,
  x_range = NULL,
  x_n_breaks = 5,
  x_breaks = NULL,
  y_range = NULL,
  y_n_breaks = 5,
  y_breaks = NULL,
  novelty_range = NULL,
  value_scales = waiver(),
  novelty_scales = waiver(),
  conf_int_style = c("ribbon", "step", "none"),
  conf_int_alpha = 0.4,
  show_novelty = TRUE,
  anchor_values = NULL,
  width = waiver(),
  height = waiver(),
  units = waiver(),
  export_collection = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_pd_+3A_object">object</code></td>
<td>
<p><code>familiarCollection</code> object, or one or more <code>familiarData</code>
objects, that will be internally converted to a <code>familiarCollection</code> object.
It is also possible to provide a <code>familiarEnsemble</code> or one or more
<code>familiarModel</code> objects together with the data from which data is computed
prior to export. Paths to such files can also be provided.</p>
</td></tr>
<tr><td><code id="plot_pd_+3A_draw">draw</code></td>
<td>
<p>(<em>optional</em>) Draws the plot if TRUE.</p>
</td></tr>
<tr><td><code id="plot_pd_+3A_dir_path">dir_path</code></td>
<td>
<p>(<em>optional</em>) Path to the directory where created individual
conditional expectation plots are saved to. Output is saved in the
<code>explanation</code> subdirectory. If <code>NULL</code>, figures are written to the folder,
but are returned instead.</p>
</td></tr>
<tr><td><code id="plot_pd_+3A_split_by">split_by</code></td>
<td>
<p>(<em>optional</em>) Splitting variables. This refers to column names
on which datasets are split. A separate figure is created for each split.
See details for available variables.</p>
</td></tr>
<tr><td><code id="plot_pd_+3A_color_by">color_by</code></td>
<td>
<p>(<em>optional</em>) Variables used to determine fill colour of plot
objects. The variables cannot overlap with those provided to the <code>split_by</code>
argument, but may overlap with other arguments. See details for available
variables.</p>
</td></tr>
<tr><td><code id="plot_pd_+3A_facet_by">facet_by</code></td>
<td>
<p>(<em>optional</em>) Variables used to determine how and if facets of
each figure appear. In case the <code>facet_wrap_cols</code> argument is <code>NULL</code>, the
first variable is used to define columns, and the remaing variables are
used to define rows of facets. The variables cannot overlap with those
provided to the <code>split_by</code> argument, but may overlap with other arguments.
See details for available variables.</p>
</td></tr>
<tr><td><code id="plot_pd_+3A_facet_wrap_cols">facet_wrap_cols</code></td>
<td>
<p>(<em>optional</em>) Number of columns to generate when facet
wrapping. If NULL, a facet grid is produced instead.</p>
</td></tr>
<tr><td><code id="plot_pd_+3A_ggtheme">ggtheme</code></td>
<td>
<p>(<em>optional</em>) <code>ggplot</code> theme to use for plotting.</p>
</td></tr>
<tr><td><code id="plot_pd_+3A_discrete_palette">discrete_palette</code></td>
<td>
<p>(<em>optional</em>) Palette to use to colour the different
plot elements in case a value was provided to the <code>color_by</code> argument. For
2D individual conditional expectation plots without novelty, the initial
colour determines the colour of the points indicating sample values.</p>
</td></tr>
<tr><td><code id="plot_pd_+3A_gradient_palette">gradient_palette</code></td>
<td>
<p>(<em>optional</em>) Sequential or divergent palette used to
colour the raster in 2D individual conditional expectation or partial
dependence plots. This argument is not used for 1D plots.</p>
</td></tr>
<tr><td><code id="plot_pd_+3A_gradient_palette_range">gradient_palette_range</code></td>
<td>
<p>(<em>optional</em>) Numerical range used to span the
gradient for 2D plots. This should be a range of two values, e.g. <code>c(0, 1)</code>.
By default, values are determined from the data, dependent on the
<code>value_scales</code> parameter. This parameter is ignored for 1D plots.</p>
</td></tr>
<tr><td><code id="plot_pd_+3A_x_label">x_label</code></td>
<td>
<p>(<em>optional</em>) Label to provide to the x-axis. If NULL, no label
is shown.</p>
</td></tr>
<tr><td><code id="plot_pd_+3A_y_label">y_label</code></td>
<td>
<p>(<em>optional</em>) Label to provide to the y-axis. If NULL, no label
is shown.</p>
</td></tr>
<tr><td><code id="plot_pd_+3A_legend_label">legend_label</code></td>
<td>
<p>(<em>optional</em>) Label to provide to the legend. If NULL, the
legend will not have a name.</p>
</td></tr>
<tr><td><code id="plot_pd_+3A_plot_title">plot_title</code></td>
<td>
<p>(<em>optional</em>) Label to provide as figure title. If NULL, no
title is shown.</p>
</td></tr>
<tr><td><code id="plot_pd_+3A_plot_sub_title">plot_sub_title</code></td>
<td>
<p>(<em>optional</em>) Label to provide as figure subtitle. If
NULL, no subtitle is shown.</p>
</td></tr>
<tr><td><code id="plot_pd_+3A_caption">caption</code></td>
<td>
<p>(<em>optional</em>) Label to provide as figure caption. If NULL, no
caption is shown.</p>
</td></tr>
<tr><td><code id="plot_pd_+3A_x_range">x_range</code></td>
<td>
<p>(<em>optional</em>) Value range for the x-axis.</p>
</td></tr>
<tr><td><code id="plot_pd_+3A_x_n_breaks">x_n_breaks</code></td>
<td>
<p>(<em>optional</em>) Number of breaks to show on the x-axis of the
plot. <code>x_n_breaks</code> is used to determine the <code>x_breaks</code> argument in case it
is unset.</p>
</td></tr>
<tr><td><code id="plot_pd_+3A_x_breaks">x_breaks</code></td>
<td>
<p>(<em>optional</em>) Break points on the x-axis of the plot.</p>
</td></tr>
<tr><td><code id="plot_pd_+3A_y_range">y_range</code></td>
<td>
<p>(<em>optional</em>) Value range for the y-axis.</p>
</td></tr>
<tr><td><code id="plot_pd_+3A_y_n_breaks">y_n_breaks</code></td>
<td>
<p>(<em>optional</em>) Number of breaks to show on the y-axis of the
plot. <code>y_n_breaks</code> is used to determine the <code>y_breaks</code> argument in case it
is unset.</p>
</td></tr>
<tr><td><code id="plot_pd_+3A_y_breaks">y_breaks</code></td>
<td>
<p>(<em>optional</em>) Break points on the y-axis of the plot.</p>
</td></tr>
<tr><td><code id="plot_pd_+3A_novelty_range">novelty_range</code></td>
<td>
<p>(<em>optional</em>) Numerical range used to span the range of
novelty values. This determines the size of the bubbles in 2D, and
transparency of lines in 1D. This should be a range of two values, e.g.
<code>c(0, 1)</code>. By default, values are determined from the data, dependent on the
<code>value_scales</code> parameter. This parameter is ignored if <code>show_novelty=FALSE</code>.</p>
</td></tr>
<tr><td><code id="plot_pd_+3A_value_scales">value_scales</code></td>
<td>
<p>(<em>optional</em>) Sets scaling of predicted values. This
parameter has several options:
</p>

<ul>
<li> <p><code>fixed</code> (default): The value axis for all features will have the same
range.
</p>
</li>
<li> <p><code>feature</code>: The value axis for each feature will have the same range. This
option is unavailable for 2D plots.
</p>
</li>
<li> <p><code>figure</code>: The value axis for all facets in a figure will have the same
range.
</p>
</li>
<li> <p><code>facet</code>: Each facet has its own range. This option is unavailable for 2D
plots.
</p>
</li></ul>

<p>For 1D plots, this option is ignored if the <code>y_range</code> is provided, whereas
for 2D it is ignored if the <code>gradient_palette_range</code> is provided.</p>
</td></tr>
<tr><td><code id="plot_pd_+3A_novelty_scales">novelty_scales</code></td>
<td>
<p>(<em>optional</em>) Sets scaling of novelty values, similar to
the <code>value_scales</code> parameter, but with more limited options:
</p>

<ul>
<li> <p><code>fixed</code> (default): The novelty will have the same range for all features.
</p>
</li>
<li> <p><code>figure</code>: The novelty will have the same range for all facets in a figure.
</p>
</li></ul>
</td></tr>
<tr><td><code id="plot_pd_+3A_conf_int_style">conf_int_style</code></td>
<td>
<p>(<em>optional</em>) Confidence interval style. See details for
allowed styles.</p>
</td></tr>
<tr><td><code id="plot_pd_+3A_conf_int_alpha">conf_int_alpha</code></td>
<td>
<p>(<em>optional</em>) Alpha value to determine transparency of
confidence intervals or, alternatively, other plot elements with which the
confidence interval overlaps. Only values between 0.0 (fully transparent)
and 1.0 (fully opaque) are allowed.</p>
</td></tr>
<tr><td><code id="plot_pd_+3A_show_novelty">show_novelty</code></td>
<td>
<p>(<em>optional</em>) Sets whether novelty is shown in plots.</p>
</td></tr>
<tr><td><code id="plot_pd_+3A_anchor_values">anchor_values</code></td>
<td>
<p>(<em>optional</em>) A single value or a named list or array of
values that are used to centre the individual conditional expectation plot.
A single value is valid if and only if only a single feature is assessed.
Otherwise, values Has no effect if the plot is not shown, i.e.
<code>show_ice=FALSE</code>. A partial dependence plot cannot be shown for those
features.</p>
</td></tr>
<tr><td><code id="plot_pd_+3A_width">width</code></td>
<td>
<p>(<em>optional</em>) Width of the plot. A default value is derived from
the number of facets.</p>
</td></tr>
<tr><td><code id="plot_pd_+3A_height">height</code></td>
<td>
<p>(<em>optional</em>) Height of the plot. A default value is derived from
the number of features and the number of facets.</p>
</td></tr>
<tr><td><code id="plot_pd_+3A_units">units</code></td>
<td>
<p>(<em>optional</em>) Plot size unit. Either <code>cm</code> (default), <code>mm</code> or <code style="white-space: pre;">&#8288;in&#8288;</code>.</p>
</td></tr>
<tr><td><code id="plot_pd_+3A_export_collection">export_collection</code></td>
<td>
<p>(<em>optional</em>) Exports the collection if TRUE.</p>
</td></tr>
<tr><td><code id="plot_pd_+3A_...">...</code></td>
<td>

<p>Arguments passed on to <code><a href="#topic+export_ice_data">export_ice_data</a></code>, <code><a href="ggplot2.html#topic+ggsave">ggplot2::ggsave</a></code>, <code><a href="#topic+extract_ice">extract_ice</a></code>
</p>

<dl>
<dt><code>aggregate_results</code></dt><dd><p>Flag that signifies whether results should be
aggregated for export.</p>
</dd>
<dt><code>filename</code></dt><dd><p>File name to create on disk.</p>
</dd>
<dt><code>plot</code></dt><dd><p>Plot to save, defaults to last plot displayed.</p>
</dd>
<dt><code>device</code></dt><dd><p>Device to use. Can either be a device function
(e.g. <a href="grDevices.html#topic+png">png</a>), or one of &quot;eps&quot;, &quot;ps&quot;, &quot;tex&quot; (pictex),
&quot;pdf&quot;, &quot;jpeg&quot;, &quot;tiff&quot;, &quot;png&quot;, &quot;bmp&quot;, &quot;svg&quot; or &quot;wmf&quot; (windows only).</p>
</dd>
<dt><code>path</code></dt><dd><p>Path of the directory to save plot to: <code>path</code> and <code>filename</code>
are combined to create the fully qualified file name. Defaults to the
working directory.</p>
</dd>
<dt><code>scale</code></dt><dd><p>Multiplicative scaling factor.</p>
</dd>
<dt><code>dpi</code></dt><dd><p>Plot resolution. Also accepts a string input: &quot;retina&quot; (320),
&quot;print&quot; (300), or &quot;screen&quot; (72). Applies only to raster output types.</p>
</dd>
<dt><code>limitsize</code></dt><dd><p>When <code>TRUE</code> (the default), <code>ggsave()</code> will not
save images larger than 50x50 inches, to prevent the common error of
specifying dimensions in pixels.</p>
</dd>
<dt><code>bg</code></dt><dd><p>Background colour. If <code>NULL</code>, uses the <code>plot.background</code> fill value
from the plot theme.</p>
</dd>
<dt><code>features</code></dt><dd><p>Names of the feature or features (2) assessed simultaneously.
By default <code>NULL</code>, which means that all features are assessed one-by-one.</p>
</dd>
<dt><code>feature_x_range</code></dt><dd><p>When one or two features are defined using <code>features</code>,
<code>feature_x_range</code> can be used to set the range of values for the first
feature. For numeric features, a vector of two values is assumed to indicate
a range from which <code>n_sample_points</code> are uniformly sampled. A vector of more
than two values is interpreted as is, i.e. these represent the values to be
sampled. For categorical features, values should represent a (sub)set of
available levels.</p>
</dd>
<dt><code>feature_y_range</code></dt><dd><p>As <code>feature_x_range</code>, but for the second feature in
case two features are defined.</p>
</dd>
<dt><code>n_sample_points</code></dt><dd><p>Number of points used to sample continuous features.</p>
</dd>
<dt><code>data</code></dt><dd><p>A <code>dataObject</code> object, <code>data.table</code> or <code>data.frame</code> that
constitutes the data that are assessed.</p>
</dd>
<dt><code>is_pre_processed</code></dt><dd><p>Flag that indicates whether the data was already
pre-processed externally, e.g. normalised and clustered. Only used if the
<code>data</code> argument is a <code>data.table</code> or <code>data.frame</code>.</p>
</dd>
<dt><code>cl</code></dt><dd><p>Cluster created using the <code>parallel</code> package. This cluster is then
used to speed up computation through parallellisation.</p>
</dd>
<dt><code>evaluation_times</code></dt><dd><p>One or more time points that are used for in analysis of
survival problems when data has to be assessed at a set time, e.g.
calibration. If not provided explicitly, this parameter is read from
settings used at creation of the underlying <code>familiarModel</code> objects. Only
used for <code>survival</code> outcomes.</p>
</dd>
<dt><code>ensemble_method</code></dt><dd><p>Method for ensembling predictions from models for the
same sample. Available methods are:
</p>

<ul>
<li> <p><code>median</code> (default): Use the median of the predicted values as the ensemble
value for a sample.
</p>
</li>
<li> <p><code>mean</code>: Use the mean of the predicted values as the ensemble value for a
sample.
</p>
</li></ul>
</dd>
<dt><code>verbose</code></dt><dd><p>Flag to indicate whether feedback should be provided on the
computation and extraction of various data elements.</p>
</dd>
<dt><code>message_indent</code></dt><dd><p>Number of indentation steps for messages shown during
computation and extraction of various data elements.</p>
</dd>
<dt><code>sample_limit</code></dt><dd><p>(<em>optional</em>) Set the upper limit of the number of samples
that are used during evaluation steps. Cannot be less than 20.
</p>
<p>This setting can be specified per data element by providing a parameter
value in a named list with data elements, e.g.
<code>list("sample_similarity"=100, "permutation_vimp"=1000)</code>.
</p>
<p>This parameter can be set for the following data elements:
<code>sample_similarity</code> and <code>ice_data</code>.</p>
</dd>
<dt><code>detail_level</code></dt><dd><p>(<em>optional</em>) Sets the level at which results are computed
and aggregated.
</p>

<ul>
<li> <p><code>ensemble</code>: Results are computed at the ensemble level, i.e. over all
models in the ensemble. This means that, for example, bias-corrected
estimates of model performance are assessed by creating (at least) 20
bootstraps and computing the model performance of the ensemble model for
each bootstrap.
</p>
</li>
<li> <p><code>hybrid</code> (default): Results are computed at the level of models in an
ensemble. This means that, for example, bias-corrected estimates of model
performance are directly computed using the models in the ensemble. If there
are at least 20 trained models in the ensemble, performance is computed for
each model, in contrast to <code>ensemble</code> where performance is computed for the
ensemble of models. If there are less than 20 trained models in the
ensemble, bootstraps are created so that at least 20 point estimates can be
made.
</p>
</li>
<li> <p><code>model</code>: Results are computed at the model level. This means that, for
example, bias-corrected estimates of model performance are assessed by
creating (at least) 20 bootstraps and computing the performance of the model
for each bootstrap.
</p>
</li></ul>

<p>Note that each level of detail has a different interpretation for bootstrap
confidence intervals. For <code>ensemble</code> and <code>model</code> these are the confidence
intervals for the ensemble and an individual model, respectively. That is,
the confidence interval describes the range where an estimate produced by a
respective ensemble or model trained on a repeat of the experiment may be
found with the probability of the confidence level. For <code>hybrid</code>, it
represents the range where any single model trained on a repeat of the
experiment may be found with the probability of the confidence level. By
definition, confidence intervals obtained using <code>hybrid</code> are at least as
wide as those for <code>ensemble</code>. <code>hybrid</code> offers the correct interpretation if
the goal of the analysis is to assess the result of a single, unspecified,
model.
</p>
<p><code>hybrid</code> is generally computationally less expensive then <code>ensemble</code>, which
in turn is somewhat less expensive than <code>model</code>.
</p>
<p>A non-default <code>detail_level</code> parameter can be specified for separate
evaluation steps by providing a parameter value in a named list with data
elements, e.g. <code>list("auc_data"="ensemble", "model_performance"="hybrid")</code>.
This parameter can be set for the following data elements: <code>auc_data</code>,
<code>decision_curve_analyis</code>, <code>model_performance</code>, <code>permutation_vimp</code>,
<code>ice_data</code>, <code>prediction_data</code> and <code>confusion_matrix</code>.</p>
</dd>
<dt><code>estimation_type</code></dt><dd><p>(<em>optional</em>) Sets the type of estimation that should be
possible. This has the following options:
</p>

<ul>
<li> <p><code>point</code>: Point estimates.
</p>
</li>
<li> <p><code>bias_correction</code> or <code>bc</code>: Bias-corrected estimates. A bias-corrected
estimate is computed from (at least) 20 point estimates, and <code>familiar</code> may
bootstrap the data to create them.
</p>
</li>
<li> <p><code>bootstrap_confidence_interval</code> or <code>bci</code> (default): Bias-corrected
estimates with bootstrap confidence intervals (Efron and Hastie, 2016). The
number of point estimates required depends on the <code>confidence_level</code>
parameter, and <code>familiar</code> may bootstrap the data to create them.
</p>
</li></ul>

<p>As with <code>detail_level</code>, a non-default <code>estimation_type</code> parameter can be
specified for separate evaluation steps by providing a parameter value in a
named list with data elements, e.g. <code>list("auc_data"="bci", "model_performance"="point")</code>. This parameter can be set for the following
data elements: <code>auc_data</code>, <code>decision_curve_analyis</code>, <code>model_performance</code>,
<code>permutation_vimp</code>, <code>ice_data</code>, and <code>prediction_data</code>.</p>
</dd>
<dt><code>confidence_level</code></dt><dd><p>(<em>optional</em>) Numeric value for the level at which
confidence intervals are determined. In the case bootstraps are used to
determine the confidence intervals bootstrap estimation, <code>familiar</code> uses the
rule of thumb <code class="reqn">n = 20 / ci.level</code> to determine the number of required
bootstraps.
</p>
<p>The default value is <code>0.95</code>.</p>
</dd>
<dt><code>bootstrap_ci_method</code></dt><dd><p>(<em>optional</em>) Method used to determine bootstrap
confidence intervals (Efron and Hastie, 2016). The following methods are
implemented:
</p>

<ul>
<li> <p><code>percentile</code> (default): Confidence intervals obtained using the percentile
method.
</p>
</li>
<li> <p><code>bc</code>: Bias-corrected confidence intervals.
</p>
</li></ul>

<p>Note that the standard method is not implemented because this method is
often not suitable due to non-normal distributions. The bias-corrected and
accelerated (BCa) method is not implemented yet.</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Details</h3>

<p>This function generates partial dependence plots.
These plots come in two varieties, namely 1D and 2D. 1D plots show the
predicted value as function of a single feature, whereas 2D plots show the
predicted value as a function of two features.
</p>
<p>Available splitting variables are: <code>feature_x</code>, <code>feature_y</code> (2D only),
<code>fs_method</code>, <code>learner</code>, <code>data_set</code> and <code>positive_class</code> (categorical
outcomes) or <code>evaluation_time</code> (survival outcomes). By default, for 1D ICE
plots the data are split by <code>feature_x</code>, <code>fs_method</code> and <code>learner</code>, with
faceting by <code>data_set</code>, <code>positive_class</code> or <code>evaluation_time</code>. If only
partial dependence is shown, <code>positive_class</code> and <code>evaluation_time</code> are used
to set colours instead. For 2D plots, by default the data are split by
<code>feature_x</code>, <code>fs_method</code> and <code>learner</code>, with faceting by <code>data_set</code>,
<code>positive_class</code> or <code>evaluation_time</code>. The <code>color_by</code> argument cannot be
used with 2D plots, and attempting to do so causes an error. Attempting to
specify <code>feature_x</code> or <code>feature_y</code> for <code>color_by</code> will likewise result in an
error, as multiple features cannot be shown in the same facet.
</p>
<p>The splitting variables indicated by <code>color_by</code> are coloured according to
the <code>discrete_palette</code> parameter. This parameter is therefore only used for
1D plots. Available palettes for <code>discrete_palette</code> and <code>gradient_palette</code>
are those listed by <code>grDevices::palette.pals()</code> (requires R &gt;= 4.0.0),
<code>grDevices::hcl.pals()</code> (requires R &gt;= 3.6.0) and <code>rainbow</code>, <code>heat.colors</code>,
<code>terrain.colors</code>, <code>topo.colors</code> and <code>cm.colors</code>, which correspond to the
palettes of the same name in <code>grDevices</code>. If not specified, a default
palette based on palettes in Tableau are used. You may also specify your own
palette by using colour names listed by <code>grDevices::colors()</code> or through
hexadecimal RGB strings.
</p>
<p>Bootstrap confidence intervals of the partial dependence plots can be shown
using various styles set by <code>conf_int_style</code>:
</p>

<ul>
<li> <p><code>ribbon</code> (default): confidence intervals are shown as a ribbon with an
opacity of <code>conf_int_alpha</code> around the point estimate of the partial
dependence.
</p>
</li>
<li> <p><code>step</code> (default): confidence intervals are shown as a step function around
the point estimate of the partial dependence.
</p>
</li>
<li> <p><code>none</code>: confidence intervals are not shown. The point estimate of the
partial dependence is shown as usual.
</p>
</li></ul>

<p>Labelling methods such as <code>set_fs_method_names</code> or <code>set_data_set_names</code> can
be applied to the <code>familiarCollection</code> object to update labels, and order
the output in the figure.
</p>


<h3>Value</h3>

<p><code>NULL</code> or list of plot objects, if <code>dir_path</code> is <code>NULL</code>.
</p>

<hr>
<h2 id='plot_permutation_variable_importance'>Plot permutation variable importance.</h2><span id='topic+plot_permutation_variable_importance'></span><span id='topic+plot_permutation_variable_importance+2CANY-method'></span><span id='topic+plot_permutation_variable_importance+2CfamiliarCollection-method'></span>

<h3>Description</h3>

<p>This function plots the data on permutation variable importance
stored in a familiarCollection object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_permutation_variable_importance(
  object,
  draw = FALSE,
  dir_path = NULL,
  split_by = NULL,
  color_by = NULL,
  facet_by = NULL,
  facet_wrap_cols = NULL,
  ggtheme = NULL,
  discrete_palette = NULL,
  x_label = waiver(),
  y_label = "feature",
  legend_label = waiver(),
  plot_title = waiver(),
  plot_sub_title = waiver(),
  caption = NULL,
  x_range = NULL,
  x_n_breaks = 5,
  x_breaks = NULL,
  conf_int_style = c("point_line", "line", "bar_line", "none"),
  conf_int_alpha = 0.4,
  width = waiver(),
  height = waiver(),
  units = waiver(),
  export_collection = FALSE,
  ...
)

## S4 method for signature 'ANY'
plot_permutation_variable_importance(
  object,
  draw = FALSE,
  dir_path = NULL,
  split_by = NULL,
  color_by = NULL,
  facet_by = NULL,
  facet_wrap_cols = NULL,
  ggtheme = NULL,
  discrete_palette = NULL,
  x_label = waiver(),
  y_label = "feature",
  legend_label = waiver(),
  plot_title = waiver(),
  plot_sub_title = waiver(),
  caption = NULL,
  x_range = NULL,
  x_n_breaks = 5,
  x_breaks = NULL,
  conf_int_style = c("point_line", "line", "bar_line", "none"),
  conf_int_alpha = 0.4,
  width = waiver(),
  height = waiver(),
  units = waiver(),
  export_collection = FALSE,
  ...
)

## S4 method for signature 'familiarCollection'
plot_permutation_variable_importance(
  object,
  draw = FALSE,
  dir_path = NULL,
  split_by = NULL,
  color_by = NULL,
  facet_by = NULL,
  facet_wrap_cols = NULL,
  ggtheme = NULL,
  discrete_palette = NULL,
  x_label = waiver(),
  y_label = "feature",
  legend_label = waiver(),
  plot_title = waiver(),
  plot_sub_title = waiver(),
  caption = NULL,
  x_range = NULL,
  x_n_breaks = 5,
  x_breaks = NULL,
  conf_int_style = c("point_line", "line", "bar_line", "none"),
  conf_int_alpha = 0.4,
  width = waiver(),
  height = waiver(),
  units = waiver(),
  export_collection = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_permutation_variable_importance_+3A_object">object</code></td>
<td>
<p><code>familiarCollection</code> object, or one or more <code>familiarData</code>
objects, that will be internally converted to a <code>familiarCollection</code> object.
It is also possible to provide a <code>familiarEnsemble</code> or one or more
<code>familiarModel</code> objects together with the data from which data is computed
prior to export. Paths to such files can also be provided.</p>
</td></tr>
<tr><td><code id="plot_permutation_variable_importance_+3A_draw">draw</code></td>
<td>
<p>(<em>optional</em>) Draws the plot if TRUE.</p>
</td></tr>
<tr><td><code id="plot_permutation_variable_importance_+3A_dir_path">dir_path</code></td>
<td>
<p>(<em>optional</em>) Path to the directory where created figures are
saved to. Output is saved in the <code>variable_importance</code> subdirectory. If NULL
no figures are saved, but are returned instead.</p>
</td></tr>
<tr><td><code id="plot_permutation_variable_importance_+3A_split_by">split_by</code></td>
<td>
<p>(<em>optional</em>) Splitting variables. This refers to column names
on which datasets are split. A separate figure is created for each split.
See details for available variables.</p>
</td></tr>
<tr><td><code id="plot_permutation_variable_importance_+3A_color_by">color_by</code></td>
<td>
<p>(<em>optional</em>) Variables used to determine fill colour of plot
objects. The variables cannot overlap with those provided to the <code>split_by</code>
argument, but may overlap with other arguments. See details for available
variables.</p>
</td></tr>
<tr><td><code id="plot_permutation_variable_importance_+3A_facet_by">facet_by</code></td>
<td>
<p>(<em>optional</em>) Variables used to determine how and if facets of
each figure appear. In case the <code>facet_wrap_cols</code> argument is <code>NULL</code>, the
first variable is used to define columns, and the remaing variables are
used to define rows of facets. The variables cannot overlap with those
provided to the <code>split_by</code> argument, but may overlap with other arguments.
See details for available variables.</p>
</td></tr>
<tr><td><code id="plot_permutation_variable_importance_+3A_facet_wrap_cols">facet_wrap_cols</code></td>
<td>
<p>(<em>optional</em>) Number of columns to generate when facet
wrapping. If NULL, a facet grid is produced instead.</p>
</td></tr>
<tr><td><code id="plot_permutation_variable_importance_+3A_ggtheme">ggtheme</code></td>
<td>
<p>(<em>optional</em>) <code>ggplot</code> theme to use for plotting.</p>
</td></tr>
<tr><td><code id="plot_permutation_variable_importance_+3A_discrete_palette">discrete_palette</code></td>
<td>
<p>(<em>optional</em>) Palette used to fill the bars in case a
non-singular variable was provided to the <code>color_by</code> argument.</p>
</td></tr>
<tr><td><code id="plot_permutation_variable_importance_+3A_x_label">x_label</code></td>
<td>
<p>(<em>optional</em>) Label to provide to the x-axis. If NULL, no label
is shown.</p>
</td></tr>
<tr><td><code id="plot_permutation_variable_importance_+3A_y_label">y_label</code></td>
<td>
<p>(<em>optional</em>) Label to provide to the y-axis. If NULL, no label
is shown.</p>
</td></tr>
<tr><td><code id="plot_permutation_variable_importance_+3A_legend_label">legend_label</code></td>
<td>
<p>(<em>optional</em>) Label to provide to the legend. If NULL, the
legend will not have a name.</p>
</td></tr>
<tr><td><code id="plot_permutation_variable_importance_+3A_plot_title">plot_title</code></td>
<td>
<p>(<em>optional</em>) Label to provide as figure title. If NULL, no
title is shown.</p>
</td></tr>
<tr><td><code id="plot_permutation_variable_importance_+3A_plot_sub_title">plot_sub_title</code></td>
<td>
<p>(<em>optional</em>) Label to provide as figure subtitle. If
NULL, no subtitle is shown.</p>
</td></tr>
<tr><td><code id="plot_permutation_variable_importance_+3A_caption">caption</code></td>
<td>
<p>(<em>optional</em>) Label to provide as figure caption. If NULL, no
caption is shown.</p>
</td></tr>
<tr><td><code id="plot_permutation_variable_importance_+3A_x_range">x_range</code></td>
<td>
<p>(<em>optional</em>) Value range for the x-axis.</p>
</td></tr>
<tr><td><code id="plot_permutation_variable_importance_+3A_x_n_breaks">x_n_breaks</code></td>
<td>
<p>(<em>optional</em>) Number of breaks to show on the x-axis of the
plot. <code>x_n_breaks</code> is used to determine the <code>x_breaks</code> argument in case it
is unset.</p>
</td></tr>
<tr><td><code id="plot_permutation_variable_importance_+3A_x_breaks">x_breaks</code></td>
<td>
<p>(<em>optional</em>) Break points on the x-axis of the plot.</p>
</td></tr>
<tr><td><code id="plot_permutation_variable_importance_+3A_conf_int_style">conf_int_style</code></td>
<td>
<p>(<em>optional</em>) Confidence interval style. See details for
allowed styles.</p>
</td></tr>
<tr><td><code id="plot_permutation_variable_importance_+3A_conf_int_alpha">conf_int_alpha</code></td>
<td>
<p>(<em>optional</em>) Alpha value to determine transparency of
confidence intervals or, alternatively, other plot elements with which the
confidence interval overlaps. Only values between 0.0 (fully transparent)
and 1.0 (fully opaque) are allowed.</p>
</td></tr>
<tr><td><code id="plot_permutation_variable_importance_+3A_width">width</code></td>
<td>
<p>(<em>optional</em>) Width of the plot. A default value is derived from
the number of facets.</p>
</td></tr>
<tr><td><code id="plot_permutation_variable_importance_+3A_height">height</code></td>
<td>
<p>(<em>optional</em>) Height of the plot. A default value is derived from
the number of features and the number of facets.</p>
</td></tr>
<tr><td><code id="plot_permutation_variable_importance_+3A_units">units</code></td>
<td>
<p>(<em>optional</em>) Plot size unit. Either <code>cm</code> (default), <code>mm</code> or <code style="white-space: pre;">&#8288;in&#8288;</code>.</p>
</td></tr>
<tr><td><code id="plot_permutation_variable_importance_+3A_export_collection">export_collection</code></td>
<td>
<p>(<em>optional</em>) Exports the collection if TRUE.</p>
</td></tr>
<tr><td><code id="plot_permutation_variable_importance_+3A_...">...</code></td>
<td>

<p>Arguments passed on to <code><a href="#topic+as_familiar_collection">as_familiar_collection</a></code>, <code><a href="ggplot2.html#topic+ggsave">ggplot2::ggsave</a></code>, <code><a href="#topic+extract_permutation_vimp">extract_permutation_vimp</a></code>
</p>

<dl>
<dt><code>familiar_data_names</code></dt><dd><p>Names of the dataset(s). Only used if the <code>object</code> parameter
is one or more <code>familiarData</code> objects.</p>
</dd>
<dt><code>collection_name</code></dt><dd><p>Name of the collection.</p>
</dd>
<dt><code>filename</code></dt><dd><p>File name to create on disk.</p>
</dd>
<dt><code>plot</code></dt><dd><p>Plot to save, defaults to last plot displayed.</p>
</dd>
<dt><code>device</code></dt><dd><p>Device to use. Can either be a device function
(e.g. <a href="grDevices.html#topic+png">png</a>), or one of &quot;eps&quot;, &quot;ps&quot;, &quot;tex&quot; (pictex),
&quot;pdf&quot;, &quot;jpeg&quot;, &quot;tiff&quot;, &quot;png&quot;, &quot;bmp&quot;, &quot;svg&quot; or &quot;wmf&quot; (windows only).</p>
</dd>
<dt><code>path</code></dt><dd><p>Path of the directory to save plot to: <code>path</code> and <code>filename</code>
are combined to create the fully qualified file name. Defaults to the
working directory.</p>
</dd>
<dt><code>scale</code></dt><dd><p>Multiplicative scaling factor.</p>
</dd>
<dt><code>dpi</code></dt><dd><p>Plot resolution. Also accepts a string input: &quot;retina&quot; (320),
&quot;print&quot; (300), or &quot;screen&quot; (72). Applies only to raster output types.</p>
</dd>
<dt><code>limitsize</code></dt><dd><p>When <code>TRUE</code> (the default), <code>ggsave()</code> will not
save images larger than 50x50 inches, to prevent the common error of
specifying dimensions in pixels.</p>
</dd>
<dt><code>bg</code></dt><dd><p>Background colour. If <code>NULL</code>, uses the <code>plot.background</code> fill value
from the plot theme.</p>
</dd>
<dt><code>data</code></dt><dd><p>A <code>dataObject</code> object, <code>data.table</code> or <code>data.frame</code> that
constitutes the data that are assessed.</p>
</dd>
<dt><code>is_pre_processed</code></dt><dd><p>Flag that indicates whether the data was already
pre-processed externally, e.g. normalised and clustered. Only used if the
<code>data</code> argument is a <code>data.table</code> or <code>data.frame</code>.</p>
</dd>
<dt><code>cl</code></dt><dd><p>Cluster created using the <code>parallel</code> package. This cluster is then
used to speed up computation through parallellisation.</p>
</dd>
<dt><code>evaluation_times</code></dt><dd><p>One or more time points that are used for in analysis of
survival problems when data has to be assessed at a set time, e.g.
calibration. If not provided explicitly, this parameter is read from
settings used at creation of the underlying <code>familiarModel</code> objects. Only
used for <code>survival</code> outcomes.</p>
</dd>
<dt><code>ensemble_method</code></dt><dd><p>Method for ensembling predictions from models for the
same sample. Available methods are:
</p>

<ul>
<li> <p><code>median</code> (default): Use the median of the predicted values as the ensemble
value for a sample.
</p>
</li>
<li> <p><code>mean</code>: Use the mean of the predicted values as the ensemble value for a
sample.
</p>
</li></ul>
</dd>
<dt><code>metric</code></dt><dd><p>One or more metrics for assessing model performance. See the
vignette on performance metrics for the available metrics. If not provided
explicitly, this parameter is read from settings used at creation of the
underlying <code>familiarModel</code> objects.</p>
</dd>
<dt><code>feature_cluster_method</code></dt><dd><p>The method used to perform clustering. These are
the same methods as for the <code>cluster_method</code> configuration parameter:
<code>none</code>, <code>hclust</code>, <code>agnes</code>, <code>diana</code> and <code>pam</code>.
</p>
<p><code>none</code> cannot be used when extracting data regarding mutual correlation or
feature expressions.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</dd>
<dt><code>feature_linkage_method</code></dt><dd><p>The method used for agglomerative clustering in
<code>hclust</code> and <code>agnes</code>. These are the same methods as for the
<code>cluster_linkage_method</code> configuration parameter: <code>average</code>, <code>single</code>,
<code>complete</code>, <code>weighted</code>, and <code>ward</code>.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</dd>
<dt><code>feature_cluster_cut_method</code></dt><dd><p>The method used to divide features into
separate clusters. The available methods are the same as for the
<code>cluster_cut_method</code> configuration parameter: <code>silhouette</code>, <code>fixed_cut</code> and
<code>dynamic_cut</code>.
</p>
<p><code>silhouette</code> is available for all cluster methods, but <code>fixed_cut</code> only
applies to methods that create hierarchical trees (<code>hclust</code>, <code>agnes</code> and
<code>diana</code>). <code>dynamic_cut</code> requires the <code>dynamicTreeCut</code> package and can only
be used with <code>agnes</code> and <code>hclust</code>.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</dd>
<dt><code>feature_similarity_threshold</code></dt><dd><p>The threshold level for pair-wise
similarity that is required to form feature clusters with the <code>fixed_cut</code>
method.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</dd>
<dt><code>feature_similarity_metric</code></dt><dd><p>Metric to determine pairwise similarity
between features. Similarity is computed in the same manner as for
clustering, and <code>feature_similarity_metric</code> therefore has the same options
as <code>cluster_similarity_metric</code>: <code>mcfadden_r2</code>, <code>cox_snell_r2</code>,
<code>nagelkerke_r2</code>, <code>spearman</code>, <code>kendall</code> and <code>pearson</code>.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</dd>
<dt><code>verbose</code></dt><dd><p>Flag to indicate whether feedback should be provided on the
computation and extraction of various data elements.</p>
</dd>
<dt><code>message_indent</code></dt><dd><p>Number of indentation steps for messages shown during
computation and extraction of various data elements.</p>
</dd>
<dt><code>detail_level</code></dt><dd><p>(<em>optional</em>) Sets the level at which results are computed
and aggregated.
</p>

<ul>
<li> <p><code>ensemble</code>: Results are computed at the ensemble level, i.e. over all
models in the ensemble. This means that, for example, bias-corrected
estimates of model performance are assessed by creating (at least) 20
bootstraps and computing the model performance of the ensemble model for
each bootstrap.
</p>
</li>
<li> <p><code>hybrid</code> (default): Results are computed at the level of models in an
ensemble. This means that, for example, bias-corrected estimates of model
performance are directly computed using the models in the ensemble. If there
are at least 20 trained models in the ensemble, performance is computed for
each model, in contrast to <code>ensemble</code> where performance is computed for the
ensemble of models. If there are less than 20 trained models in the
ensemble, bootstraps are created so that at least 20 point estimates can be
made.
</p>
</li>
<li> <p><code>model</code>: Results are computed at the model level. This means that, for
example, bias-corrected estimates of model performance are assessed by
creating (at least) 20 bootstraps and computing the performance of the model
for each bootstrap.
</p>
</li></ul>

<p>Note that each level of detail has a different interpretation for bootstrap
confidence intervals. For <code>ensemble</code> and <code>model</code> these are the confidence
intervals for the ensemble and an individual model, respectively. That is,
the confidence interval describes the range where an estimate produced by a
respective ensemble or model trained on a repeat of the experiment may be
found with the probability of the confidence level. For <code>hybrid</code>, it
represents the range where any single model trained on a repeat of the
experiment may be found with the probability of the confidence level. By
definition, confidence intervals obtained using <code>hybrid</code> are at least as
wide as those for <code>ensemble</code>. <code>hybrid</code> offers the correct interpretation if
the goal of the analysis is to assess the result of a single, unspecified,
model.
</p>
<p><code>hybrid</code> is generally computationally less expensive then <code>ensemble</code>, which
in turn is somewhat less expensive than <code>model</code>.
</p>
<p>A non-default <code>detail_level</code> parameter can be specified for separate
evaluation steps by providing a parameter value in a named list with data
elements, e.g. <code>list("auc_data"="ensemble", "model_performance"="hybrid")</code>.
This parameter can be set for the following data elements: <code>auc_data</code>,
<code>decision_curve_analyis</code>, <code>model_performance</code>, <code>permutation_vimp</code>,
<code>ice_data</code>, <code>prediction_data</code> and <code>confusion_matrix</code>.</p>
</dd>
<dt><code>estimation_type</code></dt><dd><p>(<em>optional</em>) Sets the type of estimation that should be
possible. This has the following options:
</p>

<ul>
<li> <p><code>point</code>: Point estimates.
</p>
</li>
<li> <p><code>bias_correction</code> or <code>bc</code>: Bias-corrected estimates. A bias-corrected
estimate is computed from (at least) 20 point estimates, and <code>familiar</code> may
bootstrap the data to create them.
</p>
</li>
<li> <p><code>bootstrap_confidence_interval</code> or <code>bci</code> (default): Bias-corrected
estimates with bootstrap confidence intervals (Efron and Hastie, 2016). The
number of point estimates required depends on the <code>confidence_level</code>
parameter, and <code>familiar</code> may bootstrap the data to create them.
</p>
</li></ul>

<p>As with <code>detail_level</code>, a non-default <code>estimation_type</code> parameter can be
specified for separate evaluation steps by providing a parameter value in a
named list with data elements, e.g. <code>list("auc_data"="bci", "model_performance"="point")</code>. This parameter can be set for the following
data elements: <code>auc_data</code>, <code>decision_curve_analyis</code>, <code>model_performance</code>,
<code>permutation_vimp</code>, <code>ice_data</code>, and <code>prediction_data</code>.</p>
</dd>
<dt><code>aggregate_results</code></dt><dd><p>(<em>optional</em>) Flag that signifies whether results
should be aggregated during evaluation. If <code>estimation_type</code> is
<code>bias_correction</code> or <code>bc</code>, aggregation leads to a single bias-corrected
estimate. If <code>estimation_type</code> is <code>bootstrap_confidence_interval</code> or <code>bci</code>,
aggregation leads to a single bias-corrected estimate with lower and upper
boundaries of the confidence interval. This has no effect if
<code>estimation_type</code> is <code>point</code>.
</p>
<p>The default value is equal to <code>TRUE</code> except when assessing metrics to assess
model performance, as the default violin plot requires underlying data.
</p>
<p>As with <code>detail_level</code> and <code>estimation_type</code>, a non-default
<code>aggregate_results</code> parameter can be specified for separate evaluation steps
by providing a parameter value in a named list with data elements, e.g.
<code>list("auc_data"=TRUE, , "model_performance"=FALSE)</code>. This parameter exists
for the same elements as <code>estimation_type</code>.</p>
</dd>
<dt><code>confidence_level</code></dt><dd><p>(<em>optional</em>) Numeric value for the level at which
confidence intervals are determined. In the case bootstraps are used to
determine the confidence intervals bootstrap estimation, <code>familiar</code> uses the
rule of thumb <code class="reqn">n = 20 / ci.level</code> to determine the number of required
bootstraps.
</p>
<p>The default value is <code>0.95</code>.</p>
</dd>
<dt><code>bootstrap_ci_method</code></dt><dd><p>(<em>optional</em>) Method used to determine bootstrap
confidence intervals (Efron and Hastie, 2016). The following methods are
implemented:
</p>

<ul>
<li> <p><code>percentile</code> (default): Confidence intervals obtained using the percentile
method.
</p>
</li>
<li> <p><code>bc</code>: Bias-corrected confidence intervals.
</p>
</li></ul>

<p>Note that the standard method is not implemented because this method is
often not suitable due to non-normal distributions. The bias-corrected and
accelerated (BCa) method is not implemented yet.</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Details</h3>

<p>This function generates a horizontal barplot that lists features by
the estimated model improvement over that of a dataset where the respective
feature is randomly permuted.
</p>
<p>The following splitting variables are available for <code>split_by</code>, <code>color_by</code>
and <code>facet_by</code>:
</p>

<ul>
<li> <p><code>fs_method</code>: feature selection methods.
</p>
</li>
<li> <p><code>learner</code>: learners.
</p>
</li>
<li> <p><code>data_set</code>: data sets.
</p>
</li>
<li> <p><code>metric</code>: the model performance metrics.
</p>
</li>
<li> <p><code>evaluation_time</code>: the evaluation times (survival outcomes only).
</p>
</li>
<li> <p><code>similarity_threshold</code>: the similarity threshold used to identify groups
of features to permute simultaneously.
</p>
</li></ul>

<p>By default, the data is split by <code>fs_method</code>, <code>learner</code> and <code>metric</code>,
faceted by <code>data_set</code> and <code>evaluation_time</code>, and coloured by
<code>similarity_threshold</code>.
</p>
<p>Available palettes for <code>discrete_palette</code> are those listed by
<code>grDevices::palette.pals()</code> (requires R &gt;= 4.0.0), <code>grDevices::hcl.pals()</code>
(requires R &gt;= 3.6.0) and <code>rainbow</code>, <code>heat.colors</code>, <code>terrain.colors</code>,
<code>topo.colors</code> and <code>cm.colors</code>, which correspond to the palettes of the same
name in <code>grDevices</code>. If not specified, a default palette based on palettes
in Tableau are used. You may also specify your own palette by using colour
names listed by <code>grDevices::colors()</code> or through hexadecimal RGB strings.
</p>
<p>Labelling methods such as <code>set_fs_method_names</code> or <code>set_feature_names</code> can
be applied to the <code>familiarCollection</code> object to update labels, and order
the output in the figure.
</p>
<p>Bootstrap confidence intervals (if present) can be shown using various
styles set by <code>conf_int_style</code>:
</p>

<ul>
<li> <p><code>point_line</code> (default): confidence intervals are shown as lines, on which
the point estimate is likewise shown.
</p>
</li>
<li> <p><code>line</code> (default): confidence intervals are shown as lines, but the point
estimate is not shown.
</p>
</li>
<li> <p><code>bar_line</code>: confidence intervals are shown as lines, with the point
estimate shown as a bar plot with the opacity of <code>conf_int_alpha</code>.
</p>
</li>
<li> <p><code>none</code>: confidence intervals are not shown. The point estimate is shown as
a bar plot.
</p>
</li></ul>

<p>For metrics where lower values indicate better model performance, more
negative permutation variable importance values indicate features that are
more important. Because this may cause confusion, values obtained for these
metrics are mirrored around 0.0 for plotting (but not any tabular data
export).
</p>


<h3>Value</h3>

<p><code>NULL</code> or list of plot objects, if <code>dir_path</code> is <code>NULL</code>.
</p>

<hr>
<h2 id='plot_sample_clustering'>Plot heatmaps for pairwise similarity between features.</h2><span id='topic+plot_sample_clustering'></span><span id='topic+plot_sample_clustering+2CANY-method'></span><span id='topic+plot_sample_clustering+2CfamiliarCollection-method'></span>

<h3>Description</h3>

<p>This method creates a heatmap based on data stored in a
<code>familiarCollection</code> object. Features in the heatmap are ordered so that
more similar features appear together.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_sample_clustering(
  object,
  feature_cluster_method = waiver(),
  feature_linkage_method = waiver(),
  sample_cluster_method = waiver(),
  sample_linkage_method = waiver(),
  sample_limit = waiver(),
  draw = FALSE,
  dir_path = NULL,
  split_by = NULL,
  x_axis_by = NULL,
  y_axis_by = NULL,
  facet_by = NULL,
  facet_wrap_cols = NULL,
  ggtheme = NULL,
  gradient_palette = NULL,
  gradient_palette_range = waiver(),
  outcome_palette = NULL,
  outcome_palette_range = waiver(),
  x_label = waiver(),
  x_label_shared = "column",
  y_label = waiver(),
  y_label_shared = "row",
  legend_label = waiver(),
  outcome_legend_label = waiver(),
  plot_title = waiver(),
  plot_sub_title = waiver(),
  caption = NULL,
  x_range = NULL,
  x_n_breaks = 3,
  x_breaks = NULL,
  y_range = NULL,
  y_n_breaks = 3,
  y_breaks = NULL,
  rotate_x_tick_labels = waiver(),
  show_feature_dendrogram = TRUE,
  show_sample_dendrogram = TRUE,
  show_normalised_data = TRUE,
  show_outcome = TRUE,
  dendrogram_height = grid::unit(1.5, "cm"),
  outcome_height = grid::unit(0.3, "cm"),
  evaluation_times = waiver(),
  width = waiver(),
  height = waiver(),
  units = waiver(),
  export_collection = FALSE,
  verbose = TRUE,
  ...
)

## S4 method for signature 'ANY'
plot_sample_clustering(
  object,
  feature_cluster_method = waiver(),
  feature_linkage_method = waiver(),
  sample_cluster_method = waiver(),
  sample_linkage_method = waiver(),
  sample_limit = waiver(),
  draw = FALSE,
  dir_path = NULL,
  split_by = NULL,
  x_axis_by = NULL,
  y_axis_by = NULL,
  facet_by = NULL,
  facet_wrap_cols = NULL,
  ggtheme = NULL,
  gradient_palette = NULL,
  gradient_palette_range = waiver(),
  outcome_palette = NULL,
  outcome_palette_range = waiver(),
  x_label = waiver(),
  x_label_shared = "column",
  y_label = waiver(),
  y_label_shared = "row",
  legend_label = waiver(),
  outcome_legend_label = waiver(),
  plot_title = waiver(),
  plot_sub_title = waiver(),
  caption = NULL,
  x_range = NULL,
  x_n_breaks = 3,
  x_breaks = NULL,
  y_range = NULL,
  y_n_breaks = 3,
  y_breaks = NULL,
  rotate_x_tick_labels = waiver(),
  show_feature_dendrogram = TRUE,
  show_sample_dendrogram = TRUE,
  show_normalised_data = TRUE,
  show_outcome = TRUE,
  dendrogram_height = grid::unit(1.5, "cm"),
  outcome_height = grid::unit(0.3, "cm"),
  evaluation_times = waiver(),
  width = waiver(),
  height = waiver(),
  units = waiver(),
  export_collection = FALSE,
  verbose = TRUE,
  ...
)

## S4 method for signature 'familiarCollection'
plot_sample_clustering(
  object,
  feature_cluster_method = waiver(),
  feature_linkage_method = waiver(),
  sample_cluster_method = waiver(),
  sample_linkage_method = waiver(),
  sample_limit = waiver(),
  draw = FALSE,
  dir_path = NULL,
  split_by = NULL,
  x_axis_by = NULL,
  y_axis_by = NULL,
  facet_by = NULL,
  facet_wrap_cols = NULL,
  ggtheme = NULL,
  gradient_palette = NULL,
  gradient_palette_range = waiver(),
  outcome_palette = NULL,
  outcome_palette_range = waiver(),
  x_label = waiver(),
  x_label_shared = "column",
  y_label = waiver(),
  y_label_shared = "row",
  legend_label = waiver(),
  outcome_legend_label = waiver(),
  plot_title = waiver(),
  plot_sub_title = waiver(),
  caption = NULL,
  x_range = NULL,
  x_n_breaks = 3,
  x_breaks = NULL,
  y_range = NULL,
  y_n_breaks = 3,
  y_breaks = NULL,
  rotate_x_tick_labels = waiver(),
  show_feature_dendrogram = TRUE,
  show_sample_dendrogram = TRUE,
  show_normalised_data = TRUE,
  show_outcome = TRUE,
  dendrogram_height = grid::unit(1.5, "cm"),
  outcome_height = grid::unit(0.3, "cm"),
  evaluation_times = waiver(),
  width = waiver(),
  height = waiver(),
  units = waiver(),
  export_collection = FALSE,
  verbose = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_sample_clustering_+3A_object">object</code></td>
<td>
<p>A <code>familiarCollection</code> object, or other other objects from which
a <code>familiarCollection</code> can be extracted. See details for more information.</p>
</td></tr>
<tr><td><code id="plot_sample_clustering_+3A_feature_cluster_method">feature_cluster_method</code></td>
<td>
<p>The method used to perform clustering. These are
the same methods as for the <code>cluster_method</code> configuration parameter:
<code>none</code>, <code>hclust</code>, <code>agnes</code>, <code>diana</code> and <code>pam</code>.
</p>
<p><code>none</code> cannot be used when extracting data regarding mutual correlation or
feature expressions.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="plot_sample_clustering_+3A_feature_linkage_method">feature_linkage_method</code></td>
<td>
<p>The method used for agglomerative clustering in
<code>hclust</code> and <code>agnes</code>. These are the same methods as for the
<code>cluster_linkage_method</code> configuration parameter: <code>average</code>, <code>single</code>,
<code>complete</code>, <code>weighted</code>, and <code>ward</code>.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="plot_sample_clustering_+3A_sample_cluster_method">sample_cluster_method</code></td>
<td>
<p>The method used to perform clustering based on
distance between samples. These are the same methods as for the
<code>cluster_method</code> configuration parameter: <code>hclust</code>, <code>agnes</code>, <code>diana</code> and
<code>pam</code>.
</p>
<p><code>none</code> cannot be used when extracting data for feature expressions.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="plot_sample_clustering_+3A_sample_linkage_method">sample_linkage_method</code></td>
<td>
<p>The method used for agglomerative clustering in
<code>hclust</code> and <code>agnes</code>. These are the same methods as for the
<code>cluster_linkage_method</code> configuration parameter: <code>average</code>, <code>single</code>,
<code>complete</code>, <code>weighted</code>, and <code>ward</code>.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="plot_sample_clustering_+3A_sample_limit">sample_limit</code></td>
<td>
<p>(<em>optional</em>) Set the upper limit of the number of samples
that are used during evaluation steps. Cannot be less than 20.
</p>
<p>This setting can be specified per data element by providing a parameter
value in a named list with data elements, e.g.
<code>list("sample_similarity"=100, "permutation_vimp"=1000)</code>.
</p>
<p>This parameter can be set for the following data elements:
<code>sample_similarity</code> and <code>ice_data</code>.</p>
</td></tr>
<tr><td><code id="plot_sample_clustering_+3A_draw">draw</code></td>
<td>
<p>(<em>optional</em>) Draws the plot if TRUE.</p>
</td></tr>
<tr><td><code id="plot_sample_clustering_+3A_dir_path">dir_path</code></td>
<td>
<p>(<em>optional</em>) Path to the directory where created performance
plots are saved to. Output is saved in the <code>feature_similarity</code>
subdirectory. If <code>NULL</code> no figures are saved, but are returned instead.</p>
</td></tr>
<tr><td><code id="plot_sample_clustering_+3A_split_by">split_by</code></td>
<td>
<p>(<em>optional</em>) Splitting variables. This refers to column names
on which datasets are split. A separate figure is created for each split.
See details for available variables.</p>
</td></tr>
<tr><td><code id="plot_sample_clustering_+3A_x_axis_by">x_axis_by</code></td>
<td>
<p>(<em>optional</em>) Variable plotted along the x-axis of a plot.
The variable cannot overlap with variables provided to the <code>split_by</code> and
<code>y_axis_by</code> arguments (if used), but may overlap with other arguments. Only
one variable is allowed for this argument. See details for available
variables.</p>
</td></tr>
<tr><td><code id="plot_sample_clustering_+3A_y_axis_by">y_axis_by</code></td>
<td>
<p>(<em>optional</em>) Variable plotted along the y-axis of a plot.
The variable cannot overlap with variables provided to the <code>split_by</code> and
<code>x_axis_by</code> arguments (if used), but may overlap with other arguments. Only
one variable is allowed for this argument. See details for available
variables.</p>
</td></tr>
<tr><td><code id="plot_sample_clustering_+3A_facet_by">facet_by</code></td>
<td>
<p>(<em>optional</em>) Variables used to determine how and if facets of
each figure appear. In case the <code>facet_wrap_cols</code> argument is <code>NULL</code>, the
first variable is used to define columns, and the remaing variables are
used to define rows of facets. The variables cannot overlap with those
provided to the <code>split_by</code> argument, but may overlap with other arguments.
See details for available variables.</p>
</td></tr>
<tr><td><code id="plot_sample_clustering_+3A_facet_wrap_cols">facet_wrap_cols</code></td>
<td>
<p>(<em>optional</em>) Number of columns to generate when facet
wrapping. If NULL, a facet grid is produced instead.</p>
</td></tr>
<tr><td><code id="plot_sample_clustering_+3A_ggtheme">ggtheme</code></td>
<td>
<p>(<em>optional</em>) <code>ggplot</code> theme to use for plotting.</p>
</td></tr>
<tr><td><code id="plot_sample_clustering_+3A_gradient_palette">gradient_palette</code></td>
<td>
<p>(<em>optional</em>) Sequential or divergent palette used to
colour the similarity or distance between features in a heatmap.</p>
</td></tr>
<tr><td><code id="plot_sample_clustering_+3A_gradient_palette_range">gradient_palette_range</code></td>
<td>
<p>(<em>optional</em>) Numerical range used to span the
gradient. This should be a range of two values, e.g. <code>c(0, 1)</code>. Lower or
upper boundary can be unset by using <code>NA</code>. If not set, the full
metric-specific range is used.</p>
</td></tr>
<tr><td><code id="plot_sample_clustering_+3A_outcome_palette">outcome_palette</code></td>
<td>
<p>(<em>optional</em>) Sequential (<code>continuous</code>, <code>count</code>
outcomes) or qualitative (other outcome types) palette used to show outcome
values. This argument is ignored if the outcome is not shown.</p>
</td></tr>
<tr><td><code id="plot_sample_clustering_+3A_outcome_palette_range">outcome_palette_range</code></td>
<td>
<p>(<em>optional</em>) Numerical range used to span the
gradient of numeric (<code>continuous</code>, <code>count</code>) outcome values. This argument is
ignored for other outcome types or if the outcome is not shown.</p>
</td></tr>
<tr><td><code id="plot_sample_clustering_+3A_x_label">x_label</code></td>
<td>
<p>(<em>optional</em>) Label to provide to the x-axis. If NULL, no label
is shown.</p>
</td></tr>
<tr><td><code id="plot_sample_clustering_+3A_x_label_shared">x_label_shared</code></td>
<td>
<p>(<em>optional</em>) Sharing of x-axis labels between facets.
One of three values:
</p>

<ul>
<li> <p><code>overall</code>: A single label is placed at the bottom of the figure. Tick
text (but not the ticks themselves) is removed for all but the bottom facet
plot(s).
</p>
</li>
<li> <p><code>column</code>: A label is placed at the bottom of each column. Tick text (but
not the ticks themselves) is removed for all but the bottom facet plot(s).
</p>
</li>
<li> <p><code>individual</code>: A label is placed below each facet plot. Tick text is kept.
</p>
</li></ul>
</td></tr>
<tr><td><code id="plot_sample_clustering_+3A_y_label">y_label</code></td>
<td>
<p>(<em>optional</em>) Label to provide to the y-axis. If NULL, no label
is shown.</p>
</td></tr>
<tr><td><code id="plot_sample_clustering_+3A_y_label_shared">y_label_shared</code></td>
<td>
<p>(<em>optional</em>) Sharing of y-axis labels between facets.
One of three values:
</p>

<ul>
<li> <p><code>overall</code>: A single label is placed to the left of the figure. Tick text
(but not the ticks themselves) is removed for all but the left-most facet
plot(s).
</p>
</li>
<li> <p><code>row</code>: A label is placed to the left of each row. Tick text (but not the
ticks themselves) is removed for all but the left-most facet plot(s).
</p>
</li>
<li> <p><code>individual</code>: A label is placed below each facet plot. Tick text is kept.
</p>
</li></ul>
</td></tr>
<tr><td><code id="plot_sample_clustering_+3A_legend_label">legend_label</code></td>
<td>
<p>(<em>optional</em>) Label to provide to the legend. If NULL, the
legend will not have a name.</p>
</td></tr>
<tr><td><code id="plot_sample_clustering_+3A_outcome_legend_label">outcome_legend_label</code></td>
<td>
<p>(<em>optional</em>) Label to provide to the legend for
outcome data. If NULL, the legend will not have a name. By default, <code>class</code>,
<code>value</code> and <code>event</code> are used for <code>binomial</code> and <code>multinomial</code>, <code>continuous</code>
and <code>count</code>, and <code>survival</code> outcome types, respectively.</p>
</td></tr>
<tr><td><code id="plot_sample_clustering_+3A_plot_title">plot_title</code></td>
<td>
<p>(<em>optional</em>) Label to provide as figure title. If NULL, no
title is shown.</p>
</td></tr>
<tr><td><code id="plot_sample_clustering_+3A_plot_sub_title">plot_sub_title</code></td>
<td>
<p>(<em>optional</em>) Label to provide as figure subtitle. If
NULL, no subtitle is shown.</p>
</td></tr>
<tr><td><code id="plot_sample_clustering_+3A_caption">caption</code></td>
<td>
<p>(<em>optional</em>) Label to provide as figure caption. If NULL, no
caption is shown.</p>
</td></tr>
<tr><td><code id="plot_sample_clustering_+3A_x_range">x_range</code></td>
<td>
<p>(<em>optional</em>) Value range for the x-axis.</p>
</td></tr>
<tr><td><code id="plot_sample_clustering_+3A_x_n_breaks">x_n_breaks</code></td>
<td>
<p>(<em>optional</em>) Number of breaks to show on the x-axis of the
plot. <code>x_n_breaks</code> is used to determine the <code>x_breaks</code> argument in case it
is unset.</p>
</td></tr>
<tr><td><code id="plot_sample_clustering_+3A_x_breaks">x_breaks</code></td>
<td>
<p>(<em>optional</em>) Break points on the x-axis of the plot.</p>
</td></tr>
<tr><td><code id="plot_sample_clustering_+3A_y_range">y_range</code></td>
<td>
<p>(<em>optional</em>) Value range for the y-axis.</p>
</td></tr>
<tr><td><code id="plot_sample_clustering_+3A_y_n_breaks">y_n_breaks</code></td>
<td>
<p>(<em>optional</em>) Number of breaks to show on the y-axis of the
plot. <code>y_n_breaks</code> is used to determine the <code>y_breaks</code> argument in case it
is unset.</p>
</td></tr>
<tr><td><code id="plot_sample_clustering_+3A_y_breaks">y_breaks</code></td>
<td>
<p>(<em>optional</em>) Break points on the y-axis of the plot.</p>
</td></tr>
<tr><td><code id="plot_sample_clustering_+3A_rotate_x_tick_labels">rotate_x_tick_labels</code></td>
<td>
<p>(<em>optional</em>) Rotate tick labels on the x-axis by
90 degrees. Defaults to <code>TRUE</code>. Rotation of x-axis tick labels may also be
controlled through the <code>ggtheme</code>. In this case, <code>FALSE</code> should be provided
explicitly.</p>
</td></tr>
<tr><td><code id="plot_sample_clustering_+3A_show_feature_dendrogram">show_feature_dendrogram</code></td>
<td>
<p>(<em>optional</em>) Show feature dendrogram around the
main panel. Can be <code>TRUE</code>, <code>FALSE</code>, <code>NULL</code>, or a position, i.e. <code>top</code>,
<code>bottom</code>, <code>left</code> and <code>right</code>.
</p>
<p>If a position is specified, it should be appropriate with regard to the
<code>x_axis_by</code> or <code>y_axis_by</code> argument. If <code>x_axis_by</code> is <code>sample</code> (default),
the only valid positions are <code>top</code> (default) and <code>bottom</code>. Alternatively, if
<code>y_axis_by</code> is <code>feature</code>, the only valid positions are <code>right</code> (default) and
<code>left</code>.
</p>
<p>A dendrogram can only be drawn from cluster methods that produce dendograms,
such as <code>hclust</code>. A dendogram can for example not be constructed using the
partioning around medioids method (<code>pam</code>).</p>
</td></tr>
<tr><td><code id="plot_sample_clustering_+3A_show_sample_dendrogram">show_sample_dendrogram</code></td>
<td>
<p>(<em>optional</em>) Show sample dendrogram around the
main panel. Can be <code>TRUE</code>, <code>FALSE</code>, <code>NULL</code>, or a position, i.e. <code>top</code>,
<code>bottom</code>, <code>left</code> and <code>right</code>.
</p>
<p>If a position is specified, it should be appropriate with regard to the
<code>x_axis_by</code> or <code>y_axis_by</code> argument. If <code>y_axis_by</code> is <code>sample</code> (default),
the only valid positions are <code>right</code> (default) and <code>left</code>. Alternatively, if
<code>x_axis_by</code> is <code>sample</code>, the only valid positions are <code>top</code> (default) and
<code>bottom</code>.
</p>
<p>A dendrogram can only be drawn from cluster methods that produce dendograms,
such as <code>hclust</code>. A dendogram can for example not be constructed using the
partioning around medioids method (<code>pam</code>).</p>
</td></tr>
<tr><td><code id="plot_sample_clustering_+3A_show_normalised_data">show_normalised_data</code></td>
<td>
<p>(<em>optional</em>) Flag that determines whether the data
shown in the main heatmap is normalised using the same settings as within
the analysis (<code>fixed</code>; default), using a standardisation method
(<code>set_normalisation</code>) that is applied separately to each dataset, or not at
all (<code>none</code>), which shows the data at the original scale, albeit with
batch-corrections.
</p>
<p>Categorial variables are plotted to span 90% of the entire numerical value
range, i.e. the levels of categorical variables with 2 levels are
represented at 5% and 95% of the range, with 3 levels at 5%, 50%, and 95%,
etc.</p>
</td></tr>
<tr><td><code id="plot_sample_clustering_+3A_show_outcome">show_outcome</code></td>
<td>
<p>(<em>optional</em>) Show outcome column(s) or row(s) in the
graph. Can be <code>TRUE</code>, <code>FALSE</code>, <code>NULL</code> or a poistion, i.e. <code>top</code>, <code>bottom</code>,
<code>left</code> and <code>right</code>.
</p>
<p>If a position is specified, it should be appropriate with regard to the
<code>x_axis_by</code> or <code>y_axis_by</code> argument. If <code>y_axis_by</code> is <code>sample</code> (default),
the only valid positions are <code>left</code> (default) and <code>right</code>. Alternatively, if
<code>x_axis_by</code> is <code>sample</code>, the only valid positions are <code>top</code> (default) and
<code>bottom</code>.
</p>
<p>The outcome data will be drawn between the main panel and the sample
dendrogram (if any).</p>
</td></tr>
<tr><td><code id="plot_sample_clustering_+3A_dendrogram_height">dendrogram_height</code></td>
<td>
<p>(<em>optional</em>) Height of the dendrogram. The height is
1.5 cm by default. Height is expected to be grid unit (see <code>grid::unit</code>),
which also allows for specifying relative heights.</p>
</td></tr>
<tr><td><code id="plot_sample_clustering_+3A_outcome_height">outcome_height</code></td>
<td>
<p>(<em>optional</em>) Height of an outcome data column/row. The
height is 0.3 cm by default. Height is expected to be a grid unit (see
<code>grid::unit</code>), which also allows for specifying relative heights. In case of
<code>survival</code> outcome data with multipe <code>evaluation_times</code>, this height is
multiplied by the number of time points.</p>
</td></tr>
<tr><td><code id="plot_sample_clustering_+3A_evaluation_times">evaluation_times</code></td>
<td>
<p>(<em>optional</em>) Times at which the event status of
time-to-event survival outcomes are determined. Only used for <code>survival</code>
outcome. If not specified, the values used when creating the underlying
<code>familiarData</code> objects are used.</p>
</td></tr>
<tr><td><code id="plot_sample_clustering_+3A_width">width</code></td>
<td>
<p>(<em>optional</em>) Width of the plot. A default value is derived from
the number of facets.</p>
</td></tr>
<tr><td><code id="plot_sample_clustering_+3A_height">height</code></td>
<td>
<p>(<em>optional</em>) Height of the plot. A default value is derived from
the number of features and the number of facets.</p>
</td></tr>
<tr><td><code id="plot_sample_clustering_+3A_units">units</code></td>
<td>
<p>(<em>optional</em>) Plot size unit. Either <code>cm</code> (default), <code>mm</code> or <code style="white-space: pre;">&#8288;in&#8288;</code>.</p>
</td></tr>
<tr><td><code id="plot_sample_clustering_+3A_export_collection">export_collection</code></td>
<td>
<p>(<em>optional</em>) Exports the collection if TRUE.</p>
</td></tr>
<tr><td><code id="plot_sample_clustering_+3A_verbose">verbose</code></td>
<td>
<p>Flag to indicate whether feedback should be provided for the
plotting.</p>
</td></tr>
<tr><td><code id="plot_sample_clustering_+3A_...">...</code></td>
<td>

<p>Arguments passed on to <code><a href="#topic+as_familiar_collection">as_familiar_collection</a></code>, <code><a href="ggplot2.html#topic+ggsave">ggplot2::ggsave</a></code>, <code><a href="#topic+extract_feature_expression">extract_feature_expression</a></code>
</p>

<dl>
<dt><code>familiar_data_names</code></dt><dd><p>Names of the dataset(s). Only used if the <code>object</code> parameter
is one or more <code>familiarData</code> objects.</p>
</dd>
<dt><code>collection_name</code></dt><dd><p>Name of the collection.</p>
</dd>
<dt><code>filename</code></dt><dd><p>File name to create on disk.</p>
</dd>
<dt><code>plot</code></dt><dd><p>Plot to save, defaults to last plot displayed.</p>
</dd>
<dt><code>device</code></dt><dd><p>Device to use. Can either be a device function
(e.g. <a href="grDevices.html#topic+png">png</a>), or one of &quot;eps&quot;, &quot;ps&quot;, &quot;tex&quot; (pictex),
&quot;pdf&quot;, &quot;jpeg&quot;, &quot;tiff&quot;, &quot;png&quot;, &quot;bmp&quot;, &quot;svg&quot; or &quot;wmf&quot; (windows only).</p>
</dd>
<dt><code>path</code></dt><dd><p>Path of the directory to save plot to: <code>path</code> and <code>filename</code>
are combined to create the fully qualified file name. Defaults to the
working directory.</p>
</dd>
<dt><code>scale</code></dt><dd><p>Multiplicative scaling factor.</p>
</dd>
<dt><code>dpi</code></dt><dd><p>Plot resolution. Also accepts a string input: &quot;retina&quot; (320),
&quot;print&quot; (300), or &quot;screen&quot; (72). Applies only to raster output types.</p>
</dd>
<dt><code>limitsize</code></dt><dd><p>When <code>TRUE</code> (the default), <code>ggsave()</code> will not
save images larger than 50x50 inches, to prevent the common error of
specifying dimensions in pixels.</p>
</dd>
<dt><code>bg</code></dt><dd><p>Background colour. If <code>NULL</code>, uses the <code>plot.background</code> fill value
from the plot theme.</p>
</dd>
<dt><code>feature_similarity</code></dt><dd><p>Table containing pairwise distance between
sample. This is used to determine cluster information, and indicate which
samples are similar. The table is created by the
<code>extract_sample_similarity</code> method.</p>
</dd>
<dt><code>data</code></dt><dd><p>A <code>dataObject</code> object, <code>data.table</code> or <code>data.frame</code> that
constitutes the data that are assessed.</p>
</dd>
<dt><code>feature_similarity_metric</code></dt><dd><p>Metric to determine pairwise similarity
between features. Similarity is computed in the same manner as for
clustering, and <code>feature_similarity_metric</code> therefore has the same options
as <code>cluster_similarity_metric</code>: <code>mcfadden_r2</code>, <code>cox_snell_r2</code>,
<code>nagelkerke_r2</code>, <code>spearman</code>, <code>kendall</code> and <code>pearson</code>.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</dd>
<dt><code>sample_similarity_metric</code></dt><dd><p>Metric to determine pairwise similarity
between samples. Similarity is computed in the same manner as for
clustering, but <code>sample_similarity_metric</code> has different options that are
better suited to computing distance between samples instead of between
features: <code>gower</code>, <code>euclidean</code>.
</p>
<p>The underlying feature data is scaled to the <code class="reqn">[0, 1]</code> range (for
numerical features) using the feature values across the samples. The
normalisation parameters required can optionally be computed from feature
data with the outer 5% (on both sides) of feature values trimmed or
winsorised. To do so append <code style="white-space: pre;">&#8288;_trim&#8288;</code> (trimming) or <code style="white-space: pre;">&#8288;_winsor&#8288;</code> (winsorising) to
the metric name. This reduces the effect of outliers somewhat.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</dd>
<dt><code>message_indent</code></dt><dd><p>Number of indentation steps for messages shown during
computation and extraction of various data elements.</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Details</h3>

<p>This function generates area under the ROC curve plots.
</p>
<p>Available splitting variables are: <code>fs_method</code>, <code>learner</code>, and <code>data_set</code>.
By default, the data is split by <code>fs_method</code> and <code>learner</code> and <code>data_set</code>,
since the number of samples will typically differ between data sets, even
for the same feature selection method and learner.
</p>
<p>The <code>x_axis_by</code> and <code>y_axis_by</code> arguments determine what data are shown
along which axis. Each argument takes one of <code>feature</code> and <code>sample</code>, and
both arguments should be unique. By default, features are shown along the
x-axis and samples along the y-axis.
</p>
<p>Note that similarity is determined based on the underlying data. Hence the
ordering of features may differ between facets, and tick labels are
maintained for each panel.
</p>
<p>Available palettes for <code>gradient_palette</code> are those listed by
<code>grDevices::palette.pals()</code> (requires R &gt;= 4.0.0), <code>grDevices::hcl.pals()</code>
(requires R &gt;= 3.6.0) and <code>rainbow</code>, <code>heat.colors</code>, <code>terrain.colors</code>,
<code>topo.colors</code> and <code>cm.colors</code>, which correspond to the palettes of the same
name in <code>grDevices</code>. If not specified, a default palette based on palettes
in Tableau are used. You may also specify your own palette by using colour
names listed by <code>grDevices::colors()</code> or through hexadecimal RGB strings.
</p>
<p>Labeling methods such as <code>set_fs_method_names</code> or <code>set_data_set_names</code> can
be applied to the <code>familiarCollection</code> object to update labels, and order
the output in the figure.
</p>


<h3>Value</h3>

<p><code>NULL</code> or list of plot objects, if <code>dir_path</code> is <code>NULL</code>.
</p>

<hr>
<h2 id='plot_univariate_importance'>Plot univariate importance.</h2><span id='topic+plot_univariate_importance'></span><span id='topic+plot_univariate_importance+2CANY-method'></span><span id='topic+plot_univariate_importance+2CfamiliarCollection-method'></span>

<h3>Description</h3>

<p>This function plots the univariate analysis data stored in a
familiarCollection object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_univariate_importance(
  object,
  feature_cluster_method = waiver(),
  feature_linkage_method = waiver(),
  feature_cluster_cut_method = waiver(),
  feature_similarity_threshold = waiver(),
  draw = FALSE,
  dir_path = NULL,
  p_adjustment_method = waiver(),
  split_by = NULL,
  color_by = NULL,
  facet_by = NULL,
  facet_wrap_cols = NULL,
  show_cluster = TRUE,
  ggtheme = NULL,
  discrete_palette = NULL,
  gradient_palette = waiver(),
  x_label = waiver(),
  y_label = "feature",
  legend_label = waiver(),
  plot_title = waiver(),
  plot_sub_title = waiver(),
  caption = NULL,
  x_range = NULL,
  x_n_breaks = 5,
  x_breaks = NULL,
  significance_level_shown = 0.05,
  width = waiver(),
  height = waiver(),
  units = waiver(),
  verbose = TRUE,
  export_collection = FALSE,
  ...
)

## S4 method for signature 'ANY'
plot_univariate_importance(
  object,
  feature_cluster_method = waiver(),
  feature_linkage_method = waiver(),
  feature_cluster_cut_method = waiver(),
  feature_similarity_threshold = waiver(),
  draw = FALSE,
  dir_path = NULL,
  p_adjustment_method = waiver(),
  split_by = NULL,
  color_by = NULL,
  facet_by = NULL,
  facet_wrap_cols = NULL,
  show_cluster = TRUE,
  ggtheme = NULL,
  discrete_palette = NULL,
  gradient_palette = waiver(),
  x_label = waiver(),
  y_label = "feature",
  legend_label = waiver(),
  plot_title = waiver(),
  plot_sub_title = waiver(),
  caption = NULL,
  x_range = NULL,
  x_n_breaks = 5,
  x_breaks = NULL,
  significance_level_shown = 0.05,
  width = waiver(),
  height = waiver(),
  units = waiver(),
  verbose = TRUE,
  export_collection = FALSE,
  ...
)

## S4 method for signature 'familiarCollection'
plot_univariate_importance(
  object,
  feature_cluster_method = waiver(),
  feature_linkage_method = waiver(),
  feature_cluster_cut_method = waiver(),
  feature_similarity_threshold = waiver(),
  draw = FALSE,
  dir_path = NULL,
  p_adjustment_method = waiver(),
  split_by = NULL,
  color_by = NULL,
  facet_by = NULL,
  facet_wrap_cols = NULL,
  show_cluster = TRUE,
  ggtheme = NULL,
  discrete_palette = NULL,
  gradient_palette = waiver(),
  x_label = waiver(),
  y_label = "feature",
  legend_label = waiver(),
  plot_title = waiver(),
  plot_sub_title = waiver(),
  caption = NULL,
  x_range = NULL,
  x_n_breaks = 5,
  x_breaks = NULL,
  significance_level_shown = 0.05,
  width = waiver(),
  height = waiver(),
  units = waiver(),
  verbose = TRUE,
  export_collection = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_univariate_importance_+3A_object">object</code></td>
<td>
<p>A <code>familiarCollection</code> object, or other other objects from which
a <code>familiarCollection</code> can be extracted. See details for more information.</p>
</td></tr>
<tr><td><code id="plot_univariate_importance_+3A_feature_cluster_method">feature_cluster_method</code></td>
<td>
<p>The method used to perform clustering. These are
the same methods as for the <code>cluster_method</code> configuration parameter:
<code>none</code>, <code>hclust</code>, <code>agnes</code>, <code>diana</code> and <code>pam</code>.
</p>
<p><code>none</code> cannot be used when extracting data regarding mutual correlation or
feature expressions.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="plot_univariate_importance_+3A_feature_linkage_method">feature_linkage_method</code></td>
<td>
<p>The method used for agglomerative clustering in
<code>hclust</code> and <code>agnes</code>. These are the same methods as for the
<code>cluster_linkage_method</code> configuration parameter: <code>average</code>, <code>single</code>,
<code>complete</code>, <code>weighted</code>, and <code>ward</code>.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="plot_univariate_importance_+3A_feature_cluster_cut_method">feature_cluster_cut_method</code></td>
<td>
<p>The method used to divide features into
separate clusters. The available methods are the same as for the
<code>cluster_cut_method</code> configuration parameter: <code>silhouette</code>, <code>fixed_cut</code> and
<code>dynamic_cut</code>.
</p>
<p><code>silhouette</code> is available for all cluster methods, but <code>fixed_cut</code> only
applies to methods that create hierarchical trees (<code>hclust</code>, <code>agnes</code> and
<code>diana</code>). <code>dynamic_cut</code> requires the <code>dynamicTreeCut</code> package and can only
be used with <code>agnes</code> and <code>hclust</code>.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="plot_univariate_importance_+3A_feature_similarity_threshold">feature_similarity_threshold</code></td>
<td>
<p>The threshold level for pair-wise
similarity that is required to form feature clusters with the <code>fixed_cut</code>
method.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="plot_univariate_importance_+3A_draw">draw</code></td>
<td>
<p>(<em>optional</em>) Draws the plot if TRUE.</p>
</td></tr>
<tr><td><code id="plot_univariate_importance_+3A_dir_path">dir_path</code></td>
<td>
<p>(<em>optional</em>) Path to the directory where created figures are
saved to. Output is saved in the <code>variable_importance</code> subdirectory. If NULL
no figures are saved, but are returned instead.</p>
</td></tr>
<tr><td><code id="plot_univariate_importance_+3A_p_adjustment_method">p_adjustment_method</code></td>
<td>
<p>(<em>optional</em>) Indicates type of p-value that is
shown. One of <code>holm</code>, <code>hochberg</code>, <code>hommel</code>, <code>bonferroni</code>, <code>BH</code>, <code>BY</code>, <code>fdr</code>,
<code>none</code>, <code>p_value</code> or <code>q_value</code> for adjusted p-values, uncorrected p-values
and q-values. q-values may not be available.</p>
</td></tr>
<tr><td><code id="plot_univariate_importance_+3A_split_by">split_by</code></td>
<td>
<p>(<em>optional</em>) Splitting variables. This refers to column names
on which datasets are split. A separate figure is created for each split.
See details for available variables.</p>
</td></tr>
<tr><td><code id="plot_univariate_importance_+3A_color_by">color_by</code></td>
<td>
<p>(<em>optional</em>) Variables used to determine fill colour of plot
objects. The variables cannot overlap with those provided to the <code>split_by</code>
argument, but may overlap with other arguments. See details for available
variables.</p>
</td></tr>
<tr><td><code id="plot_univariate_importance_+3A_facet_by">facet_by</code></td>
<td>
<p>(<em>optional</em>) Variables used to determine how and if facets of
each figure appear. In case the <code>facet_wrap_cols</code> argument is <code>NULL</code>, the
first variable is used to define columns, and the remaing variables are
used to define rows of facets. The variables cannot overlap with those
provided to the <code>split_by</code> argument, but may overlap with other arguments.
See details for available variables.</p>
</td></tr>
<tr><td><code id="plot_univariate_importance_+3A_facet_wrap_cols">facet_wrap_cols</code></td>
<td>
<p>(<em>optional</em>) Number of columns to generate when facet
wrapping. If NULL, a facet grid is produced instead.</p>
</td></tr>
<tr><td><code id="plot_univariate_importance_+3A_show_cluster">show_cluster</code></td>
<td>
<p>(<em>optional</em>) Show which features were clustered together.</p>
</td></tr>
<tr><td><code id="plot_univariate_importance_+3A_ggtheme">ggtheme</code></td>
<td>
<p>(<em>optional</em>) <code>ggplot</code> theme to use for plotting.</p>
</td></tr>
<tr><td><code id="plot_univariate_importance_+3A_discrete_palette">discrete_palette</code></td>
<td>
<p>(<em>optional</em>) Palette used to fill the bars in case a
non-singular variable was provided to the <code>color_by</code> argument.</p>
</td></tr>
<tr><td><code id="plot_univariate_importance_+3A_gradient_palette">gradient_palette</code></td>
<td>
<p>(<em>optional</em>) Palette to use for filling the bars in
case the <code>color_by</code> argument is not set. The bars are then coloured
according to their importance. By default, no gradient is used, and the bars
are not filled according to importance. Use <code>NULL</code> to fill the bars using
the default palette in <code>familiar</code>.</p>
</td></tr>
<tr><td><code id="plot_univariate_importance_+3A_x_label">x_label</code></td>
<td>
<p>(<em>optional</em>) Label to provide to the x-axis. If NULL, no label
is shown.</p>
</td></tr>
<tr><td><code id="plot_univariate_importance_+3A_y_label">y_label</code></td>
<td>
<p>(<em>optional</em>) Label to provide to the y-axis. If NULL, no label
is shown.</p>
</td></tr>
<tr><td><code id="plot_univariate_importance_+3A_legend_label">legend_label</code></td>
<td>
<p>(<em>optional</em>) Label to provide to the legend. If NULL, the
legend will not have a name.</p>
</td></tr>
<tr><td><code id="plot_univariate_importance_+3A_plot_title">plot_title</code></td>
<td>
<p>(<em>optional</em>) Label to provide as figure title. If NULL, no
title is shown.</p>
</td></tr>
<tr><td><code id="plot_univariate_importance_+3A_plot_sub_title">plot_sub_title</code></td>
<td>
<p>(<em>optional</em>) Label to provide as figure subtitle. If
NULL, no subtitle is shown.</p>
</td></tr>
<tr><td><code id="plot_univariate_importance_+3A_caption">caption</code></td>
<td>
<p>(<em>optional</em>) Label to provide as figure caption. If NULL, no
caption is shown.</p>
</td></tr>
<tr><td><code id="plot_univariate_importance_+3A_x_range">x_range</code></td>
<td>
<p>(<em>optional</em>) Value range for the x-axis.</p>
</td></tr>
<tr><td><code id="plot_univariate_importance_+3A_x_n_breaks">x_n_breaks</code></td>
<td>
<p>(<em>optional</em>) Number of breaks to show on the x-axis of the
plot. <code>x_n_breaks</code> is used to determine the <code>x_breaks</code> argument in case it
is unset.</p>
</td></tr>
<tr><td><code id="plot_univariate_importance_+3A_x_breaks">x_breaks</code></td>
<td>
<p>(<em>optional</em>) Break points on the x-axis of the plot.</p>
</td></tr>
<tr><td><code id="plot_univariate_importance_+3A_significance_level_shown">significance_level_shown</code></td>
<td>
<p>Position(s) to draw vertical lines indicating
a significance level, e.g. 0.05. Can be NULL to not draw anything.</p>
</td></tr>
<tr><td><code id="plot_univariate_importance_+3A_width">width</code></td>
<td>
<p>(<em>optional</em>) Width of the plot. A default value is derived from
the number of facets.</p>
</td></tr>
<tr><td><code id="plot_univariate_importance_+3A_height">height</code></td>
<td>
<p>(<em>optional</em>) Height of the plot. A default value is derived from
the number of features and the number of facets.</p>
</td></tr>
<tr><td><code id="plot_univariate_importance_+3A_units">units</code></td>
<td>
<p>(<em>optional</em>) Plot size unit. Either <code>cm</code> (default), <code>mm</code> or <code style="white-space: pre;">&#8288;in&#8288;</code>.</p>
</td></tr>
<tr><td><code id="plot_univariate_importance_+3A_verbose">verbose</code></td>
<td>
<p>Flag to indicate whether feedback should be provided for the
plotting.</p>
</td></tr>
<tr><td><code id="plot_univariate_importance_+3A_export_collection">export_collection</code></td>
<td>
<p>(<em>optional</em>) Exports the collection if TRUE.</p>
</td></tr>
<tr><td><code id="plot_univariate_importance_+3A_...">...</code></td>
<td>

<p>Arguments passed on to <code><a href="#topic+as_familiar_collection">as_familiar_collection</a></code>, <code><a href="ggplot2.html#topic+ggsave">ggplot2::ggsave</a></code>, <code><a href="#topic+extract_univariate_analysis">extract_univariate_analysis</a></code>
</p>

<dl>
<dt><code>familiar_data_names</code></dt><dd><p>Names of the dataset(s). Only used if the <code>object</code> parameter
is one or more <code>familiarData</code> objects.</p>
</dd>
<dt><code>collection_name</code></dt><dd><p>Name of the collection.</p>
</dd>
<dt><code>filename</code></dt><dd><p>File name to create on disk.</p>
</dd>
<dt><code>plot</code></dt><dd><p>Plot to save, defaults to last plot displayed.</p>
</dd>
<dt><code>device</code></dt><dd><p>Device to use. Can either be a device function
(e.g. <a href="grDevices.html#topic+png">png</a>), or one of &quot;eps&quot;, &quot;ps&quot;, &quot;tex&quot; (pictex),
&quot;pdf&quot;, &quot;jpeg&quot;, &quot;tiff&quot;, &quot;png&quot;, &quot;bmp&quot;, &quot;svg&quot; or &quot;wmf&quot; (windows only).</p>
</dd>
<dt><code>path</code></dt><dd><p>Path of the directory to save plot to: <code>path</code> and <code>filename</code>
are combined to create the fully qualified file name. Defaults to the
working directory.</p>
</dd>
<dt><code>scale</code></dt><dd><p>Multiplicative scaling factor.</p>
</dd>
<dt><code>dpi</code></dt><dd><p>Plot resolution. Also accepts a string input: &quot;retina&quot; (320),
&quot;print&quot; (300), or &quot;screen&quot; (72). Applies only to raster output types.</p>
</dd>
<dt><code>limitsize</code></dt><dd><p>When <code>TRUE</code> (the default), <code>ggsave()</code> will not
save images larger than 50x50 inches, to prevent the common error of
specifying dimensions in pixels.</p>
</dd>
<dt><code>bg</code></dt><dd><p>Background colour. If <code>NULL</code>, uses the <code>plot.background</code> fill value
from the plot theme.</p>
</dd>
<dt><code>data</code></dt><dd><p>A <code>dataObject</code> object, <code>data.table</code> or <code>data.frame</code> that
constitutes the data that are assessed.</p>
</dd>
<dt><code>cl</code></dt><dd><p>Cluster created using the <code>parallel</code> package. This cluster is then
used to speed up computation through parallellisation.</p>
</dd>
<dt><code>feature_similarity_metric</code></dt><dd><p>Metric to determine pairwise similarity
between features. Similarity is computed in the same manner as for
clustering, and <code>feature_similarity_metric</code> therefore has the same options
as <code>cluster_similarity_metric</code>: <code>mcfadden_r2</code>, <code>cox_snell_r2</code>,
<code>nagelkerke_r2</code>, <code>spearman</code>, <code>kendall</code> and <code>pearson</code>.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</dd>
<dt><code>icc_type</code></dt><dd><p>String indicating the type of intraclass correlation
coefficient (<code>1</code>, <code>2</code> or <code>3</code>) that should be used to compute robustness for
features in repeated measurements during the evaluation of univariate
importance. These types correspond to the types in Shrout and Fleiss (1979).
If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</dd>
<dt><code>message_indent</code></dt><dd><p>Number of indentation steps for messages shown during
computation and extraction of various data elements.</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Details</h3>

<p>This function generates a horizontal barplot with the length of the
bars corresponding to the 10-logarithm of the (multiple-testing corrected)
p-value or q-value.
</p>
<p>Features are assessed univariately using one-sample location t-tests after
fitting a suitable regression model. The fitted model coefficient and the
covariance matrix are then used to compute a p-value.
</p>
<p>The following splitting variables are available for <code>split_by</code>, <code>color_by</code>
and <code>facet_by</code>:
</p>

<ul>
<li> <p><code>fs_method</code>: feature selection methods
</p>
</li>
<li> <p><code>learner</code>: learners
</p>
</li>
<li> <p><code>data_set</code>: data sets
</p>
</li></ul>

<p>Unlike for plots of feature ranking in feature selection and after modelling
(as assessed by model-specific routines), clusters of features are now found
during creation of underlying <code>familiarData</code> objects, instead of through
consensus clustering. Hence, clustering results may differ due to
differences in the underlying datasets.
</p>
<p>Available palettes for <code>discrete_palette</code> and <code>gradient_palette</code> are those
listed by <code>grDevices::palette.pals()</code> (requires R &gt;= 4.0.0),
<code>grDevices::hcl.pals()</code> (requires R &gt;= 3.6.0) and <code>rainbow</code>, <code>heat.colors</code>,
<code>terrain.colors</code>, <code>topo.colors</code> and <code>cm.colors</code>, which correspond to the
palettes of the same name in <code>grDevices</code>. If not specified, a default
palette based on palettes in Tableau are used. You may also specify your own
palette by using colour names listed by <code>grDevices::colors()</code> or through
hexadecimal RGB strings.
</p>
<p>Labelling methods such as <code>set_fs_method_names</code> or <code>set_feature_names</code> can
be applied to the <code>familiarCollection</code> object to update labels, and order
the output in the figure.
</p>


<h3>Value</h3>

<p><code>NULL</code> or list of plot objects, if <code>dir_path</code> is <code>NULL</code>.
</p>

<hr>
<h2 id='plot_variable_importance'>Plot variable importance scores of features during feature selection or
after training a model.</h2><span id='topic+plot_variable_importance'></span><span id='topic+plot_variable_importance+2CANY-method'></span><span id='topic+plot_variable_importance+2CfamiliarCollection-method'></span><span id='topic+plot_feature_selection_occurrence'></span><span id='topic+plot_feature_selection_variable_importance'></span><span id='topic+plot_model_signature_occurrence'></span><span id='topic+plot_model_signature_variable_importance'></span>

<h3>Description</h3>

<p>This function plots variable importance based data obtained
during feature selection or after training a model, which are stored in a
<code>familiarCollection</code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_variable_importance(
  object,
  type,
  feature_cluster_method = waiver(),
  feature_linkage_method = waiver(),
  feature_cluster_cut_method = waiver(),
  feature_similarity_threshold = waiver(),
  aggregation_method = waiver(),
  rank_threshold = waiver(),
  draw = FALSE,
  dir_path = NULL,
  split_by = NULL,
  color_by = NULL,
  facet_by = NULL,
  facet_wrap_cols = NULL,
  show_cluster = TRUE,
  ggtheme = NULL,
  discrete_palette = NULL,
  gradient_palette = waiver(),
  x_label = "feature",
  rotate_x_tick_labels = waiver(),
  y_label = waiver(),
  legend_label = waiver(),
  plot_title = waiver(),
  plot_sub_title = waiver(),
  caption = NULL,
  y_range = NULL,
  y_n_breaks = 5,
  y_breaks = NULL,
  width = waiver(),
  height = waiver(),
  units = waiver(),
  export_collection = FALSE,
  ...
)

## S4 method for signature 'ANY'
plot_variable_importance(
  object,
  type,
  feature_cluster_method = waiver(),
  feature_linkage_method = waiver(),
  feature_cluster_cut_method = waiver(),
  feature_similarity_threshold = waiver(),
  aggregation_method = waiver(),
  rank_threshold = waiver(),
  draw = FALSE,
  dir_path = NULL,
  split_by = NULL,
  color_by = NULL,
  facet_by = NULL,
  facet_wrap_cols = NULL,
  show_cluster = TRUE,
  ggtheme = NULL,
  discrete_palette = NULL,
  gradient_palette = waiver(),
  x_label = "feature",
  rotate_x_tick_labels = waiver(),
  y_label = waiver(),
  legend_label = waiver(),
  plot_title = waiver(),
  plot_sub_title = waiver(),
  caption = NULL,
  y_range = NULL,
  y_n_breaks = 5,
  y_breaks = NULL,
  width = waiver(),
  height = waiver(),
  units = waiver(),
  export_collection = FALSE,
  ...
)

## S4 method for signature 'familiarCollection'
plot_variable_importance(
  object,
  type,
  feature_cluster_method = waiver(),
  feature_linkage_method = waiver(),
  feature_cluster_cut_method = waiver(),
  feature_similarity_threshold = waiver(),
  aggregation_method = waiver(),
  rank_threshold = waiver(),
  draw = FALSE,
  dir_path = NULL,
  split_by = NULL,
  color_by = NULL,
  facet_by = NULL,
  facet_wrap_cols = NULL,
  show_cluster = TRUE,
  ggtheme = NULL,
  discrete_palette = NULL,
  gradient_palette = waiver(),
  x_label = "feature",
  rotate_x_tick_labels = waiver(),
  y_label = waiver(),
  legend_label = waiver(),
  plot_title = waiver(),
  plot_sub_title = waiver(),
  caption = NULL,
  y_range = NULL,
  y_n_breaks = 5,
  y_breaks = NULL,
  width = waiver(),
  height = waiver(),
  units = waiver(),
  export_collection = FALSE,
  ...
)

plot_feature_selection_occurrence(...)

plot_feature_selection_variable_importance(...)

plot_model_signature_occurrence(...)

plot_model_signature_variable_importance(...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_variable_importance_+3A_object">object</code></td>
<td>
<p>A <code>familiarCollection</code> object, or other other objects from which
a <code>familiarCollection</code> can be extracted. See details for more information.</p>
</td></tr>
<tr><td><code id="plot_variable_importance_+3A_type">type</code></td>
<td>
<p>Determine what variable importance should be shown. Can be
<code>feature_selection</code> or <code>model</code> for the variable importance after the
feature selection step and after the model training step, respectively.</p>
</td></tr>
<tr><td><code id="plot_variable_importance_+3A_feature_cluster_method">feature_cluster_method</code></td>
<td>
<p>The method used to perform clustering. These are
the same methods as for the <code>cluster_method</code> configuration parameter:
<code>none</code>, <code>hclust</code>, <code>agnes</code>, <code>diana</code> and <code>pam</code>.
</p>
<p><code>none</code> cannot be used when extracting data regarding mutual correlation or
feature expressions.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="plot_variable_importance_+3A_feature_linkage_method">feature_linkage_method</code></td>
<td>
<p>The method used for agglomerative clustering in
<code>hclust</code> and <code>agnes</code>. These are the same methods as for the
<code>cluster_linkage_method</code> configuration parameter: <code>average</code>, <code>single</code>,
<code>complete</code>, <code>weighted</code>, and <code>ward</code>.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="plot_variable_importance_+3A_feature_cluster_cut_method">feature_cluster_cut_method</code></td>
<td>
<p>The method used to divide features into
separate clusters. The available methods are the same as for the
<code>cluster_cut_method</code> configuration parameter: <code>silhouette</code>, <code>fixed_cut</code> and
<code>dynamic_cut</code>.
</p>
<p><code>silhouette</code> is available for all cluster methods, but <code>fixed_cut</code> only
applies to methods that create hierarchical trees (<code>hclust</code>, <code>agnes</code> and
<code>diana</code>). <code>dynamic_cut</code> requires the <code>dynamicTreeCut</code> package and can only
be used with <code>agnes</code> and <code>hclust</code>.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="plot_variable_importance_+3A_feature_similarity_threshold">feature_similarity_threshold</code></td>
<td>
<p>The threshold level for pair-wise
similarity that is required to form feature clusters with the <code>fixed_cut</code>
method.
</p>
<p>If not provided explicitly, this parameter is read from settings used at
creation of the underlying <code>familiarModel</code> objects.</p>
</td></tr>
<tr><td><code id="plot_variable_importance_+3A_aggregation_method">aggregation_method</code></td>
<td>
<p>(<em>optional</em>) The method used to aggregate variable
importances over different data subsets, e.g. bootstraps. The following
methods can be selected:
</p>

<ul>
<li> <p><code>mean</code> (default): Use the mean rank of a feature over the subsets to
determine the aggregated feature rank.
</p>
</li>
<li> <p><code>median</code>: Use the median rank of a feature over the subsets to determine
the aggregated feature rank.
</p>
</li>
<li> <p><code>best</code>: Use the best rank the feature obtained in any subset to determine
the aggregated feature rank.
</p>
</li>
<li> <p><code>worst</code>: Use the worst rank the feature obtained in any subset to
determine the aggregated feature rank.
</p>
</li>
<li> <p><code>stability</code>: Use the frequency of the feature being in the subset of
highly ranked features as measure for the aggregated feature rank
(Meinshausen and Buehlmann, 2010).
</p>
</li>
<li> <p><code>exponential</code>: Use a rank-weighted frequence of occurrence in the subset
of highly ranked features as measure for the aggregated feature rank (Haury
et al., 2011).
</p>
</li>
<li> <p><code>borda</code>: Use the borda count as measure for the aggregated feature rank
(Wald et al., 2012).
</p>
</li>
<li> <p><code>enhanced_borda</code>: Use an occurrence frequency-weighted borda count as
measure for the aggregated feature rank (Wald et al., 2012).
</p>
</li>
<li> <p><code>truncated_borda</code>: Use borda count computed only on features within the
subset of highly ranked features.
</p>
</li>
<li> <p><code>enhanced_truncated_borda</code>: Apply both the enhanced borda method and the
truncated borda method and use the resulting borda count as the aggregated
feature rank.
</p>
</li></ul>
</td></tr>
<tr><td><code id="plot_variable_importance_+3A_rank_threshold">rank_threshold</code></td>
<td>
<p>(<em>optional</em>) The threshold used to define the subset of
highly important features. If not set, this threshold is determined by
maximising the variance in the occurrence value over all features over the
subset size.
</p>
<p>This parameter is only relevant for <code>stability</code>, <code>exponential</code>,
<code>enhanced_borda</code>, <code>truncated_borda</code> and <code>enhanced_truncated_borda</code> methods.</p>
</td></tr>
<tr><td><code id="plot_variable_importance_+3A_draw">draw</code></td>
<td>
<p>(<em>optional</em>) Draws the plot if TRUE.</p>
</td></tr>
<tr><td><code id="plot_variable_importance_+3A_dir_path">dir_path</code></td>
<td>
<p>(<em>optional</em>) Path to the directory where created figures are
saved to. Output is saved in the <code>variable_importance</code> subdirectory. If
<code>NULL</code> no figures are saved, but are returned instead.</p>
</td></tr>
<tr><td><code id="plot_variable_importance_+3A_split_by">split_by</code></td>
<td>
<p>(<em>optional</em>) Splitting variables. This refers to column names
on which datasets are split. A separate figure is created for each split.
See details for available variables.</p>
</td></tr>
<tr><td><code id="plot_variable_importance_+3A_color_by">color_by</code></td>
<td>
<p>(<em>optional</em>) Variables used to determine fill colour of plot
objects. The variables cannot overlap with those provided to the <code>split_by</code>
argument, but may overlap with other arguments. See details for available
variables.</p>
</td></tr>
<tr><td><code id="plot_variable_importance_+3A_facet_by">facet_by</code></td>
<td>
<p>(<em>optional</em>) Variables used to determine how and if facets of
each figure appear. In case the <code>facet_wrap_cols</code> argument is <code>NULL</code>, the
first variable is used to define columns, and the remaing variables are
used to define rows of facets. The variables cannot overlap with those
provided to the <code>split_by</code> argument, but may overlap with other arguments.
See details for available variables.</p>
</td></tr>
<tr><td><code id="plot_variable_importance_+3A_facet_wrap_cols">facet_wrap_cols</code></td>
<td>
<p>(<em>optional</em>) Number of columns to generate when facet
wrapping. If NULL, a facet grid is produced instead.</p>
</td></tr>
<tr><td><code id="plot_variable_importance_+3A_show_cluster">show_cluster</code></td>
<td>
<p>(<em>optional</em>) Show which features were clustered together.
Currently not available in combination with variable importance obtained
during feature selection.</p>
</td></tr>
<tr><td><code id="plot_variable_importance_+3A_ggtheme">ggtheme</code></td>
<td>
<p>(<em>optional</em>) <code>ggplot</code> theme to use for plotting.</p>
</td></tr>
<tr><td><code id="plot_variable_importance_+3A_discrete_palette">discrete_palette</code></td>
<td>
<p>(<em>optional</em>) Palette to use for coloring bar plots, in
case a non-singular variable was provided to the <code>color_by</code> argument.</p>
</td></tr>
<tr><td><code id="plot_variable_importance_+3A_gradient_palette">gradient_palette</code></td>
<td>
<p>(<em>optional</em>) Palette to use for filling the bars in
case the <code>color_by</code> argument is not set. The bars are then coloured
according to the occurrence of features. By default, no gradient is used,
and the bars are not filled according to occurrence. Use <code>NULL</code> to fill the
bars using the default palette in <code>familiar</code>.</p>
</td></tr>
<tr><td><code id="plot_variable_importance_+3A_x_label">x_label</code></td>
<td>
<p>(<em>optional</em>) Label to provide to the x-axis. If NULL, no label
is shown.</p>
</td></tr>
<tr><td><code id="plot_variable_importance_+3A_rotate_x_tick_labels">rotate_x_tick_labels</code></td>
<td>
<p>(<em>optional</em>) Rotate tick labels on the x-axis by
90 degrees. Defaults to <code>TRUE</code>. Rotation of x-axis tick labels may also be
controlled through the <code>ggtheme</code>. In this case, <code>FALSE</code> should be provided
explicitly.</p>
</td></tr>
<tr><td><code id="plot_variable_importance_+3A_y_label">y_label</code></td>
<td>
<p>(<em>optional</em>) Label to provide to the y-axis. If NULL, no label
is shown.</p>
</td></tr>
<tr><td><code id="plot_variable_importance_+3A_legend_label">legend_label</code></td>
<td>
<p>(<em>optional</em>) Label to provide to the legend. If NULL, the
legend will not have a name.</p>
</td></tr>
<tr><td><code id="plot_variable_importance_+3A_plot_title">plot_title</code></td>
<td>
<p>(<em>optional</em>) Label to provide as figure title. If NULL, no
title is shown.</p>
</td></tr>
<tr><td><code id="plot_variable_importance_+3A_plot_sub_title">plot_sub_title</code></td>
<td>
<p>(<em>optional</em>) Label to provide as figure subtitle. If
NULL, no subtitle is shown.</p>
</td></tr>
<tr><td><code id="plot_variable_importance_+3A_caption">caption</code></td>
<td>
<p>(<em>optional</em>) Label to provide as figure caption. If NULL, no
caption is shown.</p>
</td></tr>
<tr><td><code id="plot_variable_importance_+3A_y_range">y_range</code></td>
<td>
<p>(<em>optional</em>) Value range for the y-axis.</p>
</td></tr>
<tr><td><code id="plot_variable_importance_+3A_y_n_breaks">y_n_breaks</code></td>
<td>
<p>(<em>optional</em>) Number of breaks to show on the y-axis of the
plot. <code>y_n_breaks</code> is used to determine the <code>y_breaks</code> argument in case it
is unset.</p>
</td></tr>
<tr><td><code id="plot_variable_importance_+3A_y_breaks">y_breaks</code></td>
<td>
<p>(<em>optional</em>) Break points on the y-axis of the plot.</p>
</td></tr>
<tr><td><code id="plot_variable_importance_+3A_width">width</code></td>
<td>
<p>(<em>optional</em>) Width of the plot. A default value is derived from
the number of facets and the number of features.</p>
</td></tr>
<tr><td><code id="plot_variable_importance_+3A_height">height</code></td>
<td>
<p>(<em>optional</em>) Height of the plot. A default value is derived from
number of facets, and the length of the longest feature name (if
<code>rotate_x_tick_labels</code> is <code>TRUE</code>).</p>
</td></tr>
<tr><td><code id="plot_variable_importance_+3A_units">units</code></td>
<td>
<p>(<em>optional</em>) Plot size unit. Either <code>cm</code> (default), <code>mm</code> or <code style="white-space: pre;">&#8288;in&#8288;</code>.</p>
</td></tr>
<tr><td><code id="plot_variable_importance_+3A_export_collection">export_collection</code></td>
<td>
<p>(<em>optional</em>) Exports the collection if TRUE.</p>
</td></tr>
<tr><td><code id="plot_variable_importance_+3A_...">...</code></td>
<td>

<p>Arguments passed on to <code><a href="#topic+as_familiar_collection">as_familiar_collection</a></code>, <code><a href="ggplot2.html#topic+ggsave">ggplot2::ggsave</a></code>, <code><a href="#topic+extract_fs_vimp">extract_fs_vimp</a></code>
</p>

<dl>
<dt><code>familiar_data_names</code></dt><dd><p>Names of the dataset(s). Only used if the <code>object</code> parameter
is one or more <code>familiarData</code> objects.</p>
</dd>
<dt><code>collection_name</code></dt><dd><p>Name of the collection.</p>
</dd>
<dt><code>filename</code></dt><dd><p>File name to create on disk.</p>
</dd>
<dt><code>plot</code></dt><dd><p>Plot to save, defaults to last plot displayed.</p>
</dd>
<dt><code>device</code></dt><dd><p>Device to use. Can either be a device function
(e.g. <a href="grDevices.html#topic+png">png</a>), or one of &quot;eps&quot;, &quot;ps&quot;, &quot;tex&quot; (pictex),
&quot;pdf&quot;, &quot;jpeg&quot;, &quot;tiff&quot;, &quot;png&quot;, &quot;bmp&quot;, &quot;svg&quot; or &quot;wmf&quot; (windows only).</p>
</dd>
<dt><code>path</code></dt><dd><p>Path of the directory to save plot to: <code>path</code> and <code>filename</code>
are combined to create the fully qualified file name. Defaults to the
working directory.</p>
</dd>
<dt><code>scale</code></dt><dd><p>Multiplicative scaling factor.</p>
</dd>
<dt><code>dpi</code></dt><dd><p>Plot resolution. Also accepts a string input: &quot;retina&quot; (320),
&quot;print&quot; (300), or &quot;screen&quot; (72). Applies only to raster output types.</p>
</dd>
<dt><code>limitsize</code></dt><dd><p>When <code>TRUE</code> (the default), <code>ggsave()</code> will not
save images larger than 50x50 inches, to prevent the common error of
specifying dimensions in pixels.</p>
</dd>
<dt><code>bg</code></dt><dd><p>Background colour. If <code>NULL</code>, uses the <code>plot.background</code> fill value
from the plot theme.</p>
</dd>
<dt><code>verbose</code></dt><dd><p>Flag to indicate whether feedback should be provided on the
computation and extraction of various data elements.</p>
</dd>
<dt><code>message_indent</code></dt><dd><p>Number of indentation steps for messages shown during
computation and extraction of various data elements.</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Details</h3>

<p>This function generates a barplot based on variable importance of
features.
</p>
<p>The only allowed values for <code>split_by</code>, <code>color_by</code> or <code>facet_by</code> are
<code>fs_method</code> and <code>learner</code>, but note that <code>learner</code> has no effect when
plotting variable importance of features acquired during feature selection.
</p>
<p>Available palettes for <code>discrete_palette</code> and <code>gradient_palette</code> are those
listed by <code>grDevices::palette.pals()</code> (requires R &gt;= 4.0.0),
<code>grDevices::hcl.pals()</code> (requires R &gt;= 3.6.0) and <code>rainbow</code>, <code>heat.colors</code>,
<code>terrain.colors</code>, <code>topo.colors</code> and <code>cm.colors</code>, which correspond to the
palettes of the same name in <code>grDevices</code>. If not specified, a default
palette based on palettes in Tableau are used. You may also specify your own
palette by using colour names listed by <code>grDevices::colors()</code> or through
hexadecimal RGB strings.
</p>
<p>Labeling methods such as <code>set_feature_names</code> or <code>set_fs_method_names</code> can be
applied to the <code>familiarCollection</code> object to update labels, and order the
output in the figure.
</p>


<h3>Value</h3>

<p><code>NULL</code> or list of plot objects, if <code>dir_path</code> is <code>NULL</code>.
</p>

<hr>
<h2 id='plotting.check_data_handling'>Checks and sanitizes splitting variables for plotting.</h2><span id='topic+plotting.check_data_handling'></span>

<h3>Description</h3>

<p>Checks and sanitizes splitting variables for plotting.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotting.check_data_handling(
  x,
  split_by = NULL,
  color_by = NULL,
  linetype_by = NULL,
  facet_by = NULL,
  x_axis_by = NULL,
  y_axis_by = NULL,
  available = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plotting.check_data_handling_+3A_x">x</code></td>
<td>
<p>data.table or data.frame containing the data used for splitting.</p>
</td></tr>
<tr><td><code id="plotting.check_data_handling_+3A_split_by">split_by</code></td>
<td>
<p>(<em>optional</em>) Splitting variables. This refers to column names
on which datasets are split. A separate figure is created for each split.
See details for available variables.</p>
</td></tr>
<tr><td><code id="plotting.check_data_handling_+3A_color_by">color_by</code></td>
<td>
<p>(<em>optional</em>) Variables used to determine fill colour of plot
objects. The variables cannot overlap with those provided to the <code>split_by</code>
argument, but may overlap with other arguments. See details for available
variables.</p>
</td></tr>
<tr><td><code id="plotting.check_data_handling_+3A_linetype_by">linetype_by</code></td>
<td>
<p>(<em>optional</em>) Variables that are used to determine the
linetype of lines in a plot. The variables cannot overlap with those
provided to the <code>split_by</code> argument, but may overlap with other arguments.
Sett details for available variables.</p>
</td></tr>
<tr><td><code id="plotting.check_data_handling_+3A_facet_by">facet_by</code></td>
<td>
<p>(<em>optional</em>) Variables used to determine how and if facets of
each figure appear. In case the <code>facet_wrap_cols</code> argument is <code>NULL</code>, the
first variable is used to define columns, and the remaing variables are
used to define rows of facets. The variables cannot overlap with those
provided to the <code>split_by</code> argument, but may overlap with other arguments.
See details for available variables.</p>
</td></tr>
<tr><td><code id="plotting.check_data_handling_+3A_x_axis_by">x_axis_by</code></td>
<td>
<p>(<em>optional</em>) Variable plotted along the x-axis of a plot.
The variable cannot overlap with variables provided to the <code>split_by</code> and
<code>y_axis_by</code> arguments (if used), but may overlap with other arguments. Only
one variable is allowed for this argument. See details for available
variables.</p>
</td></tr>
<tr><td><code id="plotting.check_data_handling_+3A_y_axis_by">y_axis_by</code></td>
<td>
<p>(<em>optional</em>) Variable plotted along the y-axis of a plot.
The variable cannot overlap with variables provided to the <code>split_by</code> and
<code>x_axis_by</code> arguments (if used), but may overlap with other arguments. Only
one variable is allowed for this argument. See details for available
variables.</p>
</td></tr>
<tr><td><code id="plotting.check_data_handling_+3A_available">available</code></td>
<td>
<p>Names of columns available for splitting.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This internal function allows some flexibility regarding the exact
input. Allowed splitting variables should be defined by the available
argument.
</p>


<h3>Value</h3>

<p>A sanitized list of splitting variables.
</p>

<hr>
<h2 id='plotting.check_input_args'>Internal checks on common plot input arguments</h2><span id='topic+plotting.check_input_args'></span>

<h3>Description</h3>

<p>Internal checks on common plot input arguments
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotting.check_input_args(
  x_range = waiver(),
  y_range = waiver(),
  x_n_breaks = waiver(),
  y_n_breaks = waiver(),
  x_breaks = waiver(),
  y_breaks = waiver(),
  conf_int = waiver(),
  conf_int_alpha = waiver(),
  conf_int_style = waiver(),
  conf_int_default = c("step", "ribbon", "none"),
  facet_wrap_cols = waiver(),
  x_label = waiver(),
  y_label = waiver(),
  x_label_shared = waiver(),
  y_label_shared = waiver(),
  rotate_x_tick_labels = waiver(),
  rotate_y_tick_labels = waiver(),
  legend_label = waiver(),
  combine_legend = waiver(),
  plot_title = waiver(),
  plot_sub_title = waiver(),
  caption = waiver()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plotting.check_input_args_+3A_x_range">x_range</code></td>
<td>
<p>(<em>optional</em>) Value range for the x-axis.</p>
</td></tr>
<tr><td><code id="plotting.check_input_args_+3A_y_range">y_range</code></td>
<td>
<p>(<em>optional</em>) Value range for the y-axis.</p>
</td></tr>
<tr><td><code id="plotting.check_input_args_+3A_x_n_breaks">x_n_breaks</code></td>
<td>
<p>(<em>optional</em>) Number of breaks to show on the x-axis of the
plot. <code>x_n_breaks</code> is used to determine the <code>x_breaks</code> argument in case it
is unset.</p>
</td></tr>
<tr><td><code id="plotting.check_input_args_+3A_y_n_breaks">y_n_breaks</code></td>
<td>
<p>(<em>optional</em>) Number of breaks to show on the y-axis of the
plot. <code>y_n_breaks</code> is used to determine the <code>y_breaks</code> argument in case it
is unset.</p>
</td></tr>
<tr><td><code id="plotting.check_input_args_+3A_x_breaks">x_breaks</code></td>
<td>
<p>(<em>optional</em>) Break points on the x-axis of the plot.</p>
</td></tr>
<tr><td><code id="plotting.check_input_args_+3A_y_breaks">y_breaks</code></td>
<td>
<p>(<em>optional</em>) Break points on the y-axis of the plot.</p>
</td></tr>
<tr><td><code id="plotting.check_input_args_+3A_conf_int">conf_int</code></td>
<td>
<p>(<em>optional</em>)</p>
</td></tr>
<tr><td><code id="plotting.check_input_args_+3A_conf_int_alpha">conf_int_alpha</code></td>
<td>
<p>(<em>optional</em>) Alpha value to determine transparency of
confidence intervals or, alternatively, other plot elements with which the
confidence interval overlaps. Only values between 0.0 (fully transparent)
and 1.0 (fully opaque) are allowed.</p>
</td></tr>
<tr><td><code id="plotting.check_input_args_+3A_conf_int_style">conf_int_style</code></td>
<td>
<p>(<em>optional</em>) Confidence interval style. See details for
allowed styles.</p>
</td></tr>
<tr><td><code id="plotting.check_input_args_+3A_conf_int_default">conf_int_default</code></td>
<td>
<p>Sets the default options for the confidence interval.</p>
</td></tr>
<tr><td><code id="plotting.check_input_args_+3A_facet_wrap_cols">facet_wrap_cols</code></td>
<td>
<p>(<em>optional</em>) Number of columns to generate when facet
wrapping. If NULL, a facet grid is produced instead.</p>
</td></tr>
<tr><td><code id="plotting.check_input_args_+3A_x_label">x_label</code></td>
<td>
<p>(<em>optional</em>) Label to provide to the x-axis. If NULL, no label
is shown.</p>
</td></tr>
<tr><td><code id="plotting.check_input_args_+3A_y_label">y_label</code></td>
<td>
<p>(<em>optional</em>) Label to provide to the y-axis. If NULL, no label
is shown.</p>
</td></tr>
<tr><td><code id="plotting.check_input_args_+3A_x_label_shared">x_label_shared</code></td>
<td>
<p>(<em>optional</em>) Sharing of x-axis labels between facets.
One of three values:
</p>

<ul>
<li> <p><code>overall</code>: A single label is placed at the bottom of the figure. Tick
text (but not the ticks themselves) is removed for all but the bottom facet
plot(s).
</p>
</li>
<li> <p><code>column</code>: A label is placed at the bottom of each column. Tick text (but
not the ticks themselves) is removed for all but the bottom facet plot(s).
</p>
</li>
<li> <p><code>individual</code>: A label is placed below each facet plot. Tick text is kept.
</p>
</li></ul>
</td></tr>
<tr><td><code id="plotting.check_input_args_+3A_y_label_shared">y_label_shared</code></td>
<td>
<p>(<em>optional</em>) Sharing of y-axis labels between facets.
One of three values:
</p>

<ul>
<li> <p><code>overall</code>: A single label is placed to the left of the figure. Tick text
(but not the ticks themselves) is removed for all but the left-most facet
plot(s).
</p>
</li>
<li> <p><code>row</code>: A label is placed to the left of each row. Tick text (but not the
ticks themselves) is removed for all but the left-most facet plot(s).
</p>
</li>
<li> <p><code>individual</code>: A label is placed below each facet plot. Tick text is kept.
</p>
</li></ul>
</td></tr>
<tr><td><code id="plotting.check_input_args_+3A_rotate_x_tick_labels">rotate_x_tick_labels</code></td>
<td>
<p>(<em>optional</em>) Rotate tick labels on the x-axis by
90 degrees. Defaults to <code>TRUE</code>. Rotation of x-axis tick labels may also be
controlled through the <code>ggtheme</code>. In this case, <code>FALSE</code> should be provided
explicitly.</p>
</td></tr>
<tr><td><code id="plotting.check_input_args_+3A_rotate_y_tick_labels">rotate_y_tick_labels</code></td>
<td>
<p>(<em>optional</em>) Rotate tick labels on the y-axis by
45 degrees.</p>
</td></tr>
<tr><td><code id="plotting.check_input_args_+3A_legend_label">legend_label</code></td>
<td>
<p>(<em>optional</em>) Label to provide to the legend. If NULL, the
legend will not have a name.</p>
</td></tr>
<tr><td><code id="plotting.check_input_args_+3A_combine_legend">combine_legend</code></td>
<td>
<p>(<em>optional</em>) Flag to indicate whether the same legend
is to be shared by multiple aesthetics, such as those specified by
<code>color_by</code> and <code>linetype_by</code> arguments.</p>
</td></tr>
<tr><td><code id="plotting.check_input_args_+3A_plot_title">plot_title</code></td>
<td>
<p>(<em>optional</em>) Label to provide as figure title. If NULL, no
title is shown.</p>
</td></tr>
<tr><td><code id="plotting.check_input_args_+3A_plot_sub_title">plot_sub_title</code></td>
<td>
<p>(<em>optional</em>) Label to provide as figure subtitle. If
NULL, no subtitle is shown.</p>
</td></tr>
<tr><td><code id="plotting.check_input_args_+3A_caption">caption</code></td>
<td>
<p>(<em>optional</em>) Label to provide as figure caption. If NULL, no
caption is shown.</p>
</td></tr>
</table>

<hr>
<h2 id='precompute_data_assignment'>Pre-compute data assignment</h2><span id='topic+precompute_data_assignment'></span>

<h3>Description</h3>

<p>Creates data assignment.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>precompute_data_assignment(
  formula = NULL,
  data = NULL,
  experiment_data = NULL,
  cl = NULL,
  experimental_design = "fs+mb",
  verbose = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="precompute_data_assignment_+3A_formula">formula</code></td>
<td>
<p>An R formula. The formula can only contain feature names and
dot (<code>.</code>). The <code>*</code> and <code>+1</code> operators are not supported as these refer to
columns that are not present in the data set.
</p>
<p>Use of the formula interface is optional.</p>
</td></tr>
<tr><td><code id="precompute_data_assignment_+3A_data">data</code></td>
<td>
<p>A <code>data.table</code> object, a <code>data.frame</code> object, list containing
multiple <code>data.table</code> or <code>data.frame</code> objects, or paths to data files.
</p>
<p><code>data</code> should be provided if no file paths are provided to the <code>data_files</code>
argument. If both are provided, only <code>data</code> will be used.
</p>
<p>All data is expected to be in wide format, and ideally has a sample
identifier (see <code>sample_id_column</code>), batch identifier (see <code>cohort_column</code>)
and outcome columns (see <code>outcome_column</code>).
</p>
<p>In case paths are provided, the data should be stored as <code>csv</code>, <code>rds</code> or
<code>RData</code> files. See documentation for the <code>data_files</code> argument for more
information.</p>
</td></tr>
<tr><td><code id="precompute_data_assignment_+3A_experiment_data">experiment_data</code></td>
<td>
<p>Experimental data may provided in the form of</p>
</td></tr>
<tr><td><code id="precompute_data_assignment_+3A_cl">cl</code></td>
<td>
<p>Cluster created using the <code>parallel</code> package. This cluster is then
used to speed up computation through parallelisation. When a cluster is not
provided, parallelisation is performed by setting up a cluster on the local
machine.
</p>
<p>This parameter has no effect if the <code>parallel</code> argument is set to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="precompute_data_assignment_+3A_experimental_design">experimental_design</code></td>
<td>
<p>(<strong>required</strong>) Defines what the experiment looks
like, e.g. <code>cv(bt(fs,20)+mb,3,2)</code> for 2 times repeated 3-fold
cross-validation with nested feature selection on 20 bootstraps and
model-building. The basic workflow components are:
</p>

<ul>
<li> <p><code>fs</code>: (required) feature selection step.
</p>
</li>
<li> <p><code>mb</code>: (required) model building step.
</p>
</li>
<li> <p><code>ev</code>: (optional) external validation. If validation batches or cohorts
are present in the dataset (<code>data</code>), these should be indicated in the
<code>validation_batch_id</code> argument.
</p>
</li></ul>

<p>The different components are linked using <code>+</code>.
</p>
<p>Different subsampling methods can be used in conjunction with the basic
workflow components:
</p>

<ul>
<li> <p><code>bs(x,n)</code>: (stratified) .632 bootstrap, with <code>n</code> the number of
bootstraps. In contrast to <code>bt</code>, feature pre-processing parameters and
hyperparameter optimisation are conducted on individual bootstraps.
</p>
</li>
<li> <p><code>bt(x,n)</code>: (stratified) .632 bootstrap, with <code>n</code> the number of
bootstraps. Unlike <code>bs</code> and other subsampling methods, no separate
pre-processing parameters or optimised hyperparameters will be determined
for each bootstrap.
</p>
</li>
<li> <p><code>cv(x,n,p)</code>: (stratified) <code>n</code>-fold cross-validation, repeated <code>p</code> times.
Pre-processing parameters are determined for each iteration.
</p>
</li>
<li> <p><code>lv(x)</code>: leave-one-out-cross-validation. Pre-processing parameters are
determined for each iteration.
</p>
</li>
<li> <p><code>ip(x)</code>: imbalance partitioning for addressing class imbalances on the
data set. Pre-processing parameters are determined for each partition. The
number of partitions generated depends on the imbalance correction method
(see the <code>imbalance_correction_method</code> parameter).
</p>
</li></ul>

<p>As shown in the example above, sampling algorithms can be nested.
</p>
<p>Though neither variable importance is determined nor models are learned
within <code>precompute_data_assignment</code>, the corresponding elements are still
required to prevent issues when using the resulting <code>experimentData</code> object
to warm-start the experiments.
</p>
<p>The simplest valid experimental design is <code>fs+mb</code>. This is the default in
<code>precompute_data_assignment</code>, and will simply assign all instances to the
training set.</p>
</td></tr>
<tr><td><code id="precompute_data_assignment_+3A_verbose">verbose</code></td>
<td>
<p>Indicates verbosity of the results. Default is TRUE, and all
messages and warnings are returned.</p>
</td></tr>
<tr><td><code id="precompute_data_assignment_+3A_...">...</code></td>
<td>

<p>Arguments passed on to <code><a href="#topic+.parse_experiment_settings">.parse_experiment_settings</a></code>, <code><a href="#topic+.parse_setup_settings">.parse_setup_settings</a></code>, <code><a href="#topic+.parse_preprocessing_settings">.parse_preprocessing_settings</a></code>
</p>

<dl>
<dt><code>batch_id_column</code></dt><dd><p>(<strong>recommended</strong>) Name of the column containing batch
or cohort identifiers. This parameter is required if more than one dataset
is provided, or if external validation is performed.
</p>
<p>In familiar any row of data is organised by four identifiers:
</p>

<ul>
<li><p> The batch identifier <code>batch_id_column</code>: This denotes the group to which a
set of samples belongs, e.g. patients from a single study, samples measured
in a batch, etc. The batch identifier is used for batch normalisation, as
well as selection of development and validation datasets.
</p>
</li>
<li><p> The sample identifier <code>sample_id_column</code>: This denotes the sample level,
e.g. data from a single individual. Subsets of data, e.g. bootstraps or
cross-validation folds, are created at this level.
</p>
</li>
<li><p> The series identifier <code>series_id_column</code>: Indicates measurements on a
single sample that may not share the same outcome value, e.g. a time
series, or the number of cells in a view.
</p>
</li>
<li><p> The repetition identifier: Indicates repeated measurements in a single
series where any feature values may differ, but the outcome does not.
Repetition identifiers are always implicitly set when multiple entries for
the same series of the same sample in the same batch that share the same
outcome are encountered.
</p>
</li></ul>
</dd>
<dt><code>sample_id_column</code></dt><dd><p>(<strong>recommended</strong>) Name of the column containing
sample or subject identifiers. See <code>batch_id_column</code> above for more
details.
</p>
<p>If unset, every row will be identified as a single sample.</p>
</dd>
<dt><code>series_id_column</code></dt><dd><p>(<strong>optional</strong>) Name of the column containing series
identifiers, which distinguish between measurements that are part of a
series for a single sample. See <code>batch_id_column</code> above for more details.
</p>
<p>If unset, rows which share the same batch and sample identifiers but have a
different outcome are assigned unique series identifiers.</p>
</dd>
<dt><code>development_batch_id</code></dt><dd><p>(<em>optional</em>) One or more batch or cohort
identifiers to constitute data sets for development. Defaults to all, or
all minus the identifiers in <code>validation_batch_id</code> for external validation.
Required if external validation is performed and <code>validation_batch_id</code> is
not provided.</p>
</dd>
<dt><code>validation_batch_id</code></dt><dd><p>(<em>optional</em>) One or more batch or cohort
identifiers to constitute data sets for external validation. Defaults to
all data sets except those in <code>development_batch_id</code> for external
validation, or none if not. Required if <code>development_batch_id</code> is not
provided.</p>
</dd>
<dt><code>outcome_name</code></dt><dd><p>(<em>optional</em>) Name of the modelled outcome. This name will
be used in figures created by <code>familiar</code>.
</p>
<p>If not set, the column name in <code>outcome_column</code> will be used for
<code>binomial</code>, <code>multinomial</code>, <code>count</code> and <code>continuous</code> outcomes. For other
outcomes (<code>survival</code> and <code>competing_risk</code>) no default is used.</p>
</dd>
<dt><code>outcome_column</code></dt><dd><p>(<strong>recommended</strong>) Name of the column containing the
outcome of interest. May be identified from a formula, if a formula is
provided as an argument. Otherwise an error is raised. Note that <code>survival</code>
and <code>competing_risk</code> outcome type outcomes require two columns that
indicate the time-to-event or the time of last follow-up and the event
status.</p>
</dd>
<dt><code>outcome_type</code></dt><dd><p>(<strong>recommended</strong>) Type of outcome found in the outcome
column. The outcome type determines many aspects of the overall process,
e.g. the available feature selection methods and learners, but also the
type of assessments that can be conducted to evaluate the resulting models.
Implemented outcome types are:
</p>

<ul>
<li> <p><code>binomial</code>: categorical outcome with 2 levels.
</p>
</li>
<li> <p><code>multinomial</code>: categorical outcome with 2 or more levels.
</p>
</li>
<li> <p><code>count</code>: Poisson-distributed numeric outcomes.
</p>
</li>
<li> <p><code>continuous</code>: general continuous numeric outcomes.
</p>
</li>
<li> <p><code>survival</code>: survival outcome for time-to-event data.
</p>
</li></ul>

<p>If not provided, the algorithm will attempt to obtain outcome_type from
contents of the outcome column. This may lead to unexpected results, and we
therefore advise to provide this information manually.
</p>
<p>Note that <code>competing_risk</code> survival analysis are not fully supported, and
is currently not a valid choice for <code>outcome_type</code>.</p>
</dd>
<dt><code>class_levels</code></dt><dd><p>(<em>optional</em>) Class levels for <code>binomial</code> or <code>multinomial</code>
outcomes. This argument can be used to specify the ordering of levels for
categorical outcomes. These class levels must exactly match the levels
present in the outcome column.</p>
</dd>
<dt><code>event_indicator</code></dt><dd><p>(<strong>recommended</strong>) Indicator for events in <code>survival</code>
and <code>competing_risk</code> analyses. <code>familiar</code> will automatically recognise <code>1</code>,
<code>true</code>, <code>t</code>, <code>y</code> and <code>yes</code> as event indicators, including different
capitalisations. If this parameter is set, it replaces the default values.</p>
</dd>
<dt><code>censoring_indicator</code></dt><dd><p>(<strong>recommended</strong>) Indicator for right-censoring in
<code>survival</code> and <code>competing_risk</code> analyses. <code>familiar</code> will automatically
recognise <code>0</code>, <code>false</code>, <code>f</code>, <code>n</code>, <code>no</code> as censoring indicators, including
different capitalisations. If this parameter is set, it replaces the
default values.</p>
</dd>
<dt><code>competing_risk_indicator</code></dt><dd><p>(<strong>recommended</strong>) Indicator for competing
risks in <code>competing_risk</code> analyses. There are no default values, and if
unset, all values other than those specified by the <code>event_indicator</code> and
<code>censoring_indicator</code> parameters are considered to indicate competing
risks.</p>
</dd>
<dt><code>signature</code></dt><dd><p>(<em>optional</em>) One or more names of feature columns that are
considered part of a specific signature. Features specified here will
always be used for modelling. Ranking from feature selection has no effect
for these features.</p>
</dd>
<dt><code>novelty_features</code></dt><dd><p>(<em>optional</em>) One or more names of feature columns
that should be included for the purpose of novelty detection.</p>
</dd>
<dt><code>exclude_features</code></dt><dd><p>(<em>optional</em>) Feature columns that will be removed
from the data set. Cannot overlap with features in <code>signature</code>,
<code>novelty_features</code> or <code>include_features</code>.</p>
</dd>
<dt><code>include_features</code></dt><dd><p>(<em>optional</em>) Feature columns that are specifically
included in the data set. By default all features are included. Cannot
overlap with <code>exclude_features</code>, but may overlap <code>signature</code>. Features in
<code>signature</code> and <code>novelty_features</code> are always included. If both
<code>exclude_features</code> and <code>include_features</code> are provided, <code>include_features</code>
takes precedence, provided that there is no overlap between the two.</p>
</dd>
<dt><code>reference_method</code></dt><dd><p>(<em>optional</em>) Method used to set reference levels for
categorical features. There are several options:
</p>

<ul>
<li> <p><code>auto</code> (default): Categorical features that are not explicitly set by the
user, i.e. columns containing boolean values or characters, use the most
frequent level as reference. Categorical features that are explicitly set,
i.e. as factors, are used as is.
</p>
</li>
<li> <p><code>always</code>: Both automatically detected and user-specified categorical
features have the reference level set to the most frequent level. Ordinal
features are not altered, but are used as is.
</p>
</li>
<li> <p><code>never</code>: User-specified categorical features are used as is.
Automatically detected categorical features are simply sorted, and the
first level is then used as the reference level. This was the behaviour
prior to familiar version 1.3.0.
</p>
</li></ul>
</dd>
<dt><code>imbalance_correction_method</code></dt><dd><p>(<em>optional</em>) Type of method used to
address class imbalances. Available options are:
</p>

<ul>
<li> <p><code>full_undersampling</code> (default): All data will be used in an ensemble
fashion. The full minority class will appear in each partition, but
majority classes are undersampled until all data have been used.
</p>
</li>
<li> <p><code>random_undersampling</code>: Randomly undersamples majority classes. This is
useful in cases where full undersampling would lead to the formation of
many models due major overrepresentation of the largest class.
</p>
</li></ul>

<p>This parameter is only used in combination with imbalance partitioning in
the experimental design, and <code>ip</code> should therefore appear in the string
that defines the design.</p>
</dd>
<dt><code>imbalance_n_partitions</code></dt><dd><p>(<em>optional</em>) Number of times random
undersampling should be repeated. 10 undersampled subsets with balanced
classes are formed by default.</p>
</dd>
<dt><code>parallel</code></dt><dd><p>(<em>optional</em>) Enable parallel processing. Defaults to <code>TRUE</code>.
When set to <code>FALSE</code>, this disables all parallel processing, regardless of
specific parameters such as <code>parallel_preprocessing</code>. However, when
<code>parallel</code> is <code>TRUE</code>, parallel processing of different parts of the
workflow can be disabled by setting respective flags to <code>FALSE</code>.</p>
</dd>
<dt><code>parallel_nr_cores</code></dt><dd><p>(<em>optional</em>) Number of cores available for
parallelisation. Defaults to 2. This setting does nothing if
parallelisation is disabled.</p>
</dd>
<dt><code>restart_cluster</code></dt><dd><p>(<em>optional</em>) Restart nodes used for parallel computing
to free up memory prior to starting a parallel process. Note that it does
take time to set up the clusters. Therefore setting this argument to <code>TRUE</code>
may impact processing speed. This argument is ignored if <code>parallel</code> is
<code>FALSE</code> or the cluster was initialised outside of familiar. Default is
<code>FALSE</code>, which causes the clusters to be initialised only once.</p>
</dd>
<dt><code>cluster_type</code></dt><dd><p>(<em>optional</em>) Selection of the cluster type for parallel
processing. Available types are the ones supported by the parallel package
that is part of the base R distribution: <code>psock</code> (default), <code>fork</code>, <code>mpi</code>,
<code>nws</code>, <code>sock</code>. In addition, <code>none</code> is available, which also disables
parallel processing.</p>
</dd>
<dt><code>backend_type</code></dt><dd><p>(<em>optional</em>) Selection of the backend for distributing
copies of the data. This backend ensures that only a single master copy is
kept in memory. This limits memory usage during parallel processing.
</p>
<p>Several backend options are available, notably <code>socket_server</code>, and <code>none</code>
(default). <code>socket_server</code> is based on the callr package and R sockets,
comes with <code>familiar</code> and is available for any OS. <code>none</code> uses the package
environment of familiar to store data, and is available for any OS.
However, <code>none</code> requires copying of data to any parallel process, and has a
larger memory footprint.</p>
</dd>
<dt><code>server_port</code></dt><dd><p>(<em>optional</em>) Integer indicating the port on which the
socket server or RServe process should communicate. Defaults to port 6311.
Note that ports 0 to 1024 and 49152 to 65535 cannot be used.</p>
</dd>
<dt><code>feature_max_fraction_missing</code></dt><dd><p>(<em>optional</em>) Numeric value between <code>0.0</code>
and <code>0.95</code> that determines the meximum fraction of missing values that
still allows a feature to be included in the data set. All features with a
missing value fraction over this threshold are not processed further. The
default value is <code>0.30</code>.</p>
</dd>
<dt><code>sample_max_fraction_missing</code></dt><dd><p>(<em>optional</em>) Numeric value between <code>0.0</code>
and <code>0.95</code> that determines the maximum fraction of missing values that
still allows a sample to be included in the data set. All samples with a
missing value fraction over this threshold are excluded and not processed
further. The default value is <code>0.30</code>.</p>
</dd>
<dt><code>filter_method</code></dt><dd><p>(<em>optional</em>) One or methods used to reduce
dimensionality of the data set by removing irrelevant or poorly
reproducible features.
</p>
<p>Several method are available:
</p>

<ul>
<li> <p><code>none</code> (default): None of the features will be filtered.
</p>
</li>
<li> <p><code>low_variance</code>: Features with a variance below the
<code>low_var_minimum_variance_threshold</code> are filtered. This can be useful to
filter, for example, genes that are not differentially expressed.
</p>
</li>
<li> <p><code>univariate_test</code>: Features undergo a univariate regression using an
outcome-appropriate regression model. The p-value of the model coefficient
is collected. Features with coefficient p or q-value above the
<code>univariate_test_threshold</code> are subsequently filtered.
</p>
</li>
<li> <p><code>robustness</code>: Features that are not sufficiently robust according to the
intraclass correlation coefficient are filtered. Use of this method
requires that repeated measurements are present in the data set, i.e. there
should be entries for which the sample and cohort identifiers are the same.
</p>
</li></ul>

<p>More than one method can be used simultaneously. Features with singular
values are always filtered, as these do not contain information.</p>
</dd>
<dt><code>univariate_test_threshold</code></dt><dd><p>(<em>optional</em>) Numeric value between <code>1.0</code> and
<code>0.0</code> that determines which features are irrelevant and will be filtered by
the <code>univariate_test</code>. The p or q-values are compared to this threshold.
All features with values above the threshold are filtered. The default
value is <code>0.20</code>.</p>
</dd>
<dt><code>univariate_test_threshold_metric</code></dt><dd><p>(<em>optional</em>) Metric used with the to
compare the <code>univariate_test_threshold</code> against. The following metrics can
be chosen:
</p>

<ul>
<li> <p><code>p_value</code> (default): The unadjusted p-value of each feature is used for
to filter features.
</p>
</li>
<li> <p><code>q_value</code>: The q-value (Story, 2002), is used to filter features. Some
data sets may have insufficient samples to compute the q-value. The
<code>qvalue</code> package must be installed from Bioconductor to use this method.
</p>
</li></ul>
</dd>
<dt><code>univariate_test_max_feature_set_size</code></dt><dd><p>(<em>optional</em>) Maximum size of the
feature set after the univariate test. P or q values of features are
compared against the threshold, but if the resulting data set would be
larger than this setting, only the most relevant features up to the desired
feature set size are selected.
</p>
<p>The default value is <code>NULL</code>, which causes features to be filtered based on
their relevance only.</p>
</dd>
<dt><code>low_var_minimum_variance_threshold</code></dt><dd><p>(required, if used) Numeric value
that determines which features will be filtered by the <code>low_variance</code>
method. The variance of each feature is computed and compared to the
threshold. If it is below the threshold, the feature is removed.
</p>
<p>This parameter has no default value and should be set if <code>low_variance</code> is
used.</p>
</dd>
<dt><code>low_var_max_feature_set_size</code></dt><dd><p>(<em>optional</em>) Maximum size of the feature
set after filtering features with a low variance. All features are first
compared against <code>low_var_minimum_variance_threshold</code>. If the resulting
feature set would be larger than specified, only the most strongly varying
features will be selected, up to the desired size of the feature set.
</p>
<p>The default value is <code>NULL</code>, which causes features to be filtered based on
their variance only.</p>
</dd>
<dt><code>robustness_icc_type</code></dt><dd><p>(<em>optional</em>) String indicating the type of
intraclass correlation coefficient (<code>1</code>, <code>2</code> or <code>3</code>) that should be used to
compute robustness for features in repeated measurements. These types
correspond to the types in Shrout and Fleiss (1979). The default value is
<code>1</code>.</p>
</dd>
<dt><code>robustness_threshold_metric</code></dt><dd><p>(<em>optional</em>) String indicating which
specific intraclass correlation coefficient (ICC) metric should be used to
filter features. This should be one of:
</p>

<ul>
<li> <p><code>icc</code>: The estimated ICC value itself.
</p>
</li>
<li> <p><code>icc_low</code> (default): The estimated lower limit of the 95% confidence
interval of the ICC, as suggested by Koo and Li (2016).
</p>
</li>
<li> <p><code>icc_panel</code>: The estimated ICC value over the panel average, i.e. the ICC
that would be obtained if all repeated measurements were averaged.
</p>
</li>
<li> <p><code>icc_panel_low</code>: The estimated lower limit of the 95% confidence interval
of the panel ICC.
</p>
</li></ul>
</dd>
<dt><code>robustness_threshold_value</code></dt><dd><p>(<em>optional</em>) The intraclass correlation
coefficient value that is as threshold. The default value is <code>0.70</code>.</p>
</dd>
<dt><code>transformation_method</code></dt><dd><p>(<em>optional</em>) The transformation method used to
change the distribution of the data to be more normal-like. The following
methods are available:
</p>

<ul>
<li> <p><code>none</code>: This disables transformation of features.
</p>
</li>
<li> <p><code>yeo_johnson</code> (default): Transformation using the Yeo-Johnson
transformation (Yeo and Johnson, 2000). The algorithm tests various lambda
values and selects the lambda that maximises the log-likelihood.
</p>
</li>
<li> <p><code>yeo_johnson_trim</code>: As <code>yeo_johnson</code>, but based on the set of feature
values where the 5% lowest and 5% highest values are discarded. This
reduces the effect of outliers.
</p>
</li>
<li> <p><code>yeo_johnson_winsor</code>: As <code>yeo_johnson</code>, but based on the set of feature
values where the 5% lowest and 5% highest values are winsorised. This
reduces the effect of outliers.
</p>
</li>
<li> <p><code>yeo_johnson_robust</code>: A robust version of <code>yeo_johnson</code> after Raymaekers
and Rousseeuw (2021). This method is less sensitive to outliers.
</p>
</li>
<li> <p><code>box_cox</code>: Transformation using the Box-Cox transformation (Box and Cox,
1964). Unlike the Yeo-Johnson transformation, the Box-Cox transformation
requires that all data are positive. Features that contain zero or negative
values cannot be transformed using this transformation. The algorithm tests
various lambda values and selects the lambda that maximises the
log-likelihood.
</p>
</li>
<li> <p><code>box_cox_trim</code>: As <code>box_cox</code>, but based on the set of feature values
where the 5% lowest and 5% highest values are discarded. This reduces the
effect of outliers.
</p>
</li>
<li> <p><code>box_cox_winsor</code>: As <code>box_cox</code>, but based on the set of feature values
where the 5% lowest and 5% highest values are winsorised. This reduces the
effect of outliers.
</p>
</li>
<li> <p><code>box_cox_robust</code>: A robust verson of <code>box_cox</code> after Raymaekers and
Rousseew (2021). This method is less sensitive to outliers.
</p>
</li></ul>

<p>Only features that contain numerical data are transformed. Transformation
parameters obtained in development data are stored within <code>featureInfo</code>
objects for later use with validation data sets.</p>
</dd>
<dt><code>normalisation_method</code></dt><dd><p>(<em>optional</em>) The normalisation method used to
improve the comparability between numerical features that may have very
different scales. The following normalisation methods can be chosen:
</p>

<ul>
<li> <p><code>none</code>: This disables feature normalisation.
</p>
</li>
<li> <p><code>standardisation</code>: Features are normalised by subtraction of their mean
values and division by their standard deviations. This causes every feature
to be have a center value of 0.0 and standard deviation of 1.0.
</p>
</li>
<li> <p><code>standardisation_trim</code>: As <code>standardisation</code>, but based on the set of
feature values where the 5% lowest and 5% highest values are discarded.
This reduces the effect of outliers.
</p>
</li>
<li> <p><code>standardisation_winsor</code>: As <code>standardisation</code>, but based on the set of
feature values where the 5% lowest and 5% highest values are winsorised.
This reduces the effect of outliers.
</p>
</li>
<li> <p><code>standardisation_robust</code> (default): A robust version of <code>standardisation</code>
that relies on computing Huber's M-estimators for location and scale.
</p>
</li>
<li> <p><code>normalisation</code>: Features are normalised by subtraction of their minimum
values and division by their ranges. This maps all feature values to a
<code class="reqn">[0, 1]</code> interval.
</p>
</li>
<li> <p><code>normalisation_trim</code>: As <code>normalisation</code>, but based on the set of feature
values where the 5% lowest and 5% highest values are discarded. This
reduces the effect of outliers.
</p>
</li>
<li> <p><code>normalisation_winsor</code>: As <code>normalisation</code>, but based on the set of
feature values where the 5% lowest and 5% highest values are winsorised.
This reduces the effect of outliers.
</p>
</li>
<li> <p><code>quantile</code>: Features are normalised by subtraction of their median values
and division by their interquartile range.
</p>
</li>
<li> <p><code>mean_centering</code>: Features are centered by substracting the mean, but do
not undergo rescaling.
</p>
</li></ul>

<p>Only features that contain numerical data are normalised. Normalisation
parameters obtained in development data are stored within <code>featureInfo</code>
objects for later use with validation data sets.</p>
</dd>
<dt><code>batch_normalisation_method</code></dt><dd><p>(<em>optional</em>) The method used for batch
normalisation. Available methods are:
</p>

<ul>
<li> <p><code>none</code> (default): This disables batch normalisation of features.
</p>
</li>
<li> <p><code>standardisation</code>: Features within each batch are normalised by
subtraction of the mean value and division by the standard deviation in
each batch.
</p>
</li>
<li> <p><code>standardisation_trim</code>: As <code>standardisation</code>, but based on the set of
feature values where the 5% lowest and 5% highest values are discarded.
This reduces the effect of outliers.
</p>
</li>
<li> <p><code>standardisation_winsor</code>: As <code>standardisation</code>, but based on the set of
feature values where the 5% lowest and 5% highest values are winsorised.
This reduces the effect of outliers.
</p>
</li>
<li> <p><code>standardisation_robust</code>: A robust version of <code>standardisation</code> that
relies on computing Huber's M-estimators for location and scale within each
batch.
</p>
</li>
<li> <p><code>normalisation</code>: Features within each batch are normalised by subtraction
of their minimum values and division by their range in each batch. This
maps all feature values in each batch to a <code class="reqn">[0, 1]</code> interval.
</p>
</li>
<li> <p><code>normalisation_trim</code>: As <code>normalisation</code>, but based on the set of feature
values where the 5% lowest and 5% highest values are discarded. This
reduces the effect of outliers.
</p>
</li>
<li> <p><code>normalisation_winsor</code>: As <code>normalisation</code>, but based on the set of
feature values where the 5% lowest and 5% highest values are winsorised.
This reduces the effect of outliers.
</p>
</li>
<li> <p><code>quantile</code>: Features in each batch are normalised by subtraction of the
median value and division by the interquartile range of each batch.
</p>
</li>
<li> <p><code>mean_centering</code>: Features in each batch are centered on 0.0 by
substracting the mean value in each batch, but are not rescaled.
</p>
</li>
<li> <p><code>combat_parametric</code>: Batch adjustments using parametric empirical Bayes
(Johnson et al, 2007). <code>combat_p</code> leads to the same method.
</p>
</li>
<li> <p><code>combat_non_parametric</code>: Batch adjustments using non-parametric empirical
Bayes (Johnson et al, 2007). <code>combat_np</code> and <code>combat</code> lead to the same
method. Note that we reduced complexity from O(<code class="reqn">n^2</code>) to O(<code class="reqn">n</code>) by
only computing batch adjustment parameters for each feature on a subset of
50 randomly selected features, instead of all features.
</p>
</li></ul>

<p>Only features that contain numerical data are normalised using batch
normalisation. Batch normalisation parameters obtained in development data
are stored within <code>featureInfo</code> objects for later use with validation data
sets, in case the validation data is from the same batch.
</p>
<p>If validation data contains data from unknown batches, normalisation
parameters are separately determined for these batches.
</p>
<p>Note that for both empirical Bayes methods, the batch effect is assumed to
produce results across the features. This is often true for things such as
gene expressions, but the assumption may not hold generally.
</p>
<p>When performing batch normalisation, it is moreover important to check that
differences between batches or cohorts are not related to the studied
endpoint.</p>
</dd>
<dt><code>imputation_method</code></dt><dd><p>(<em>optional</em>) Method used for imputing missing
feature values. Two methods are implemented:
</p>

<ul>
<li> <p><code>simple</code>: Simple replacement of a missing value by the median value (for
numeric features) or the modal value (for categorical features).
</p>
</li>
<li> <p><code>lasso</code>: Imputation of missing value by lasso regression (using <code>glmnet</code>)
based on information contained in other features.
</p>
</li></ul>

<p><code>simple</code> imputation precedes <code>lasso</code> imputation to ensure that any missing
values in predictors required for <code>lasso</code> regression are resolved. The
<code>lasso</code> estimate is then used to replace the missing value.
</p>
<p>The default value depends on the number of features in the dataset. If the
number is lower than 100, <code>lasso</code> is used by default, and <code>simple</code>
otherwise.
</p>
<p>Only single imputation is performed. Imputation models and parameters are
stored within <code>featureInfo</code> objects for later use with validation data
sets.</p>
</dd>
<dt><code>cluster_method</code></dt><dd><p>(<em>optional</em>) Clustering is performed to identify and
replace redundant features, for example those that are highly correlated.
Such features do not carry much additional information and may be removed
or replaced instead (Park et al., 2007; Tolosi and Lengauer, 2011).
</p>
<p>The cluster method determines the algorithm used to form the clusters. The
following cluster methods are implemented:
</p>

<ul>
<li> <p><code>none</code>: No clustering is performed.
</p>
</li>
<li> <p><code>hclust</code> (default): Hierarchical agglomerative clustering. If the
<code>fastcluster</code> package is installed, <code>fastcluster::hclust</code> is used (Muellner
2013), otherwise <code>stats::hclust</code> is used.
</p>
</li>
<li> <p><code>agnes</code>: Hierarchical clustering using agglomerative nesting (Kaufman and
Rousseeuw, 1990). This algorithm is similar to <code>hclust</code>, but uses the
<code>cluster::agnes</code> implementation.
</p>
</li>
<li> <p><code>diana</code>: Divisive analysis hierarchical clustering. This method uses
divisive instead of agglomerative clustering (Kaufman and Rousseeuw, 1990).
<code>cluster::diana</code> is used.
</p>
</li>
<li> <p><code>pam</code>: Partioning around medioids. This partitions the data into $k$
clusters around medioids (Kaufman and Rousseeuw, 1990). $k$ is selected
using the <code>silhouette</code> metric. <code>pam</code> is implemented using the
<code>cluster::pam</code> function.
</p>
</li></ul>

<p>Clusters and cluster information is stored within <code>featureInfo</code> objects for
later use with validation data sets. This enables reproduction of the same
clusters as formed in the development data set.</p>
</dd>
<dt><code>cluster_linkage_method</code></dt><dd><p>(<em>optional</em>) Linkage method used for
agglomerative clustering in <code>hclust</code> and <code>agnes</code>. The following linkage
methods can be used:
</p>

<ul>
<li> <p><code>average</code> (default): Average linkage.
</p>
</li>
<li> <p><code>single</code>: Single linkage.
</p>
</li>
<li> <p><code>complete</code>: Complete linkage.
</p>
</li>
<li> <p><code>weighted</code>: Weighted linkage, also known as McQuitty linkage.
</p>
</li>
<li> <p><code>ward</code>: Linkage using Ward's minimum variance method.
</p>
</li></ul>

<p><code>diana</code> and <code>pam</code> do not require a linkage method.</p>
</dd>
<dt><code>cluster_cut_method</code></dt><dd><p>(<em>optional</em>) The method used to define the actual
clusters. The following methods can be used:
</p>

<ul>
<li> <p><code>silhouette</code>: Clusters are formed based on the silhouette score
(Rousseeuw, 1987). The average silhouette score is computed from 2 to
<code class="reqn">n</code> clusters, with <code class="reqn">n</code> the number of features. Clusters are only
formed if the average silhouette exceeds 0.50, which indicates reasonable
evidence for structure. This procedure may be slow if the number of
features is large (&gt;100s).
</p>
</li>
<li> <p><code>fixed_cut</code>: Clusters are formed by cutting the hierarchical tree at the
point indicated by the <code>cluster_similarity_threshold</code>, e.g. where features
in a cluster have an average Spearman correlation of 0.90. <code>fixed_cut</code> is
only available for <code>agnes</code>, <code>diana</code> and <code>hclust</code>.
</p>
</li>
<li> <p><code>dynamic_cut</code>: Dynamic cluster formation using the cutting algorithm in
the <code>dynamicTreeCut</code> package. This package should be installed to select
this option. <code>dynamic_cut</code> can only be used with <code>agnes</code> and <code>hclust</code>.
</p>
</li></ul>

<p>The default options are <code>silhouette</code> for partioning around medioids (<code>pam</code>)
and <code>fixed_cut</code> otherwise.</p>
</dd>
<dt><code>cluster_similarity_metric</code></dt><dd><p>(<em>optional</em>) Clusters are formed based on
feature similarity. All features are compared in a pair-wise fashion to
compute similarity, for example correlation. The resulting similarity grid
is converted into a distance matrix that is subsequently used for
clustering. The following metrics are supported to compute pairwise
similarities:
</p>

<ul>
<li> <p><code>mutual_information</code> (default): normalised mutual information.
</p>
</li>
<li> <p><code>mcfadden_r2</code>: McFadden's pseudo R-squared (McFadden, 1974).
</p>
</li>
<li> <p><code>cox_snell_r2</code>: Cox and Snell's pseudo R-squared (Cox and Snell, 1989).
</p>
</li>
<li> <p><code>nagelkerke_r2</code>: Nagelkerke's pseudo R-squared (Nagelkerke, 1991).
</p>
</li>
<li> <p><code>spearman</code>: Spearman's rank order correlation.
</p>
</li>
<li> <p><code>kendall</code>: Kendall rank correlation.
</p>
</li>
<li> <p><code>pearson</code>: Pearson product-moment correlation.
</p>
</li></ul>

<p>The pseudo R-squared metrics can be used to assess similarity between mixed
pairs of numeric and categorical features, as these are based on the
log-likelihood of regression models. In <code>familiar</code>, the more informative
feature is used as the predictor and the other feature as the reponse
variable. In numeric-categorical pairs, the numeric feature is considered
to be more informative and is thus used as the predictor. In
categorical-categorical pairs, the feature with most levels is used as the
predictor.
</p>
<p>In case any of the classical correlation coefficients (<code>pearson</code>,
<code>spearman</code> and <code>kendall</code>) are used with (mixed) categorical features, the
categorical features are one-hot encoded and the mean correlation over all
resulting pairs is used as similarity.</p>
</dd>
<dt><code>cluster_similarity_threshold</code></dt><dd><p>(<em>optional</em>) The threshold level for
pair-wise similarity that is required to form clusters using <code>fixed_cut</code>.
This should be a numerical value between 0.0 and 1.0. Note however, that a
reasonable threshold value depends strongly on the similarity metric. The
following are the default values used:
</p>

<ul>
<li> <p><code>mcfadden_r2</code> and <code>mutual_information</code>: <code>0.30</code>
</p>
</li>
<li> <p><code>cox_snell_r2</code> and <code>nagelkerke_r2</code>: <code>0.75</code>
</p>
</li>
<li> <p><code>spearman</code>, <code>kendall</code> and <code>pearson</code>: <code>0.90</code>
</p>
</li></ul>

<p>Alternatively, if the <code style="white-space: pre;">&#8288;fixed cut&#8288;</code> method is not used, this value determines
whether any clustering should be performed, because the data may not
contain highly similar features. The default values in this situation are:
</p>

<ul>
<li> <p><code>mcfadden_r2</code>  and <code>mutual_information</code>: <code>0.25</code>
</p>
</li>
<li> <p><code>cox_snell_r2</code> and <code>nagelkerke_r2</code>: <code>0.40</code>
</p>
</li>
<li> <p><code>spearman</code>, <code>kendall</code> and <code>pearson</code>: <code>0.70</code>
</p>
</li></ul>

<p>The threshold value is converted to a distance (1-similarity) prior to
cutting hierarchical trees.</p>
</dd>
<dt><code>cluster_representation_method</code></dt><dd><p>(<em>optional</em>) Method used to determine
how the information of co-clustered features is summarised and used to
represent the cluster. The following methods can be selected:
</p>

<ul>
<li> <p><code>best_predictor</code> (default): The feature with the highest importance
according to univariate regression with the outcome is used to represent
the cluster.
</p>
</li>
<li> <p><code>medioid</code>: The feature closest to the cluster center, i.e. the feature
that is most similar to the remaining features in the cluster, is used to
represent the feature.
</p>
</li>
<li> <p><code>mean</code>: A meta-feature is generated by averaging the feature values for
all features in a cluster. This method aligns all features so that all
features will be positively correlated prior to averaging. Should a cluster
contain one or more categorical features, the <code>medioid</code> method will be used
instead, as averaging is not possible. Note that if this method is chosen,
the <code>normalisation_method</code> parameter should be one of <code>standardisation</code>,
<code>standardisation_trim</code>, <code>standardisation_winsor</code> or <code>quantile</code>.'
</p>
</li></ul>

<p>If the <code>pam</code> cluster method is selected, only the <code>medioid</code> method can be
used. In that case 1 medioid is used by default.</p>
</dd>
<dt><code>parallel_preprocessing</code></dt><dd><p>(<em>optional</em>) Enable parallel processing for the
preprocessing workflow. Defaults to <code>TRUE</code>. When set to <code>FALSE</code>, this will
disable the use of parallel processing while preprocessing, regardless of
the settings of the <code>parallel</code> parameter. <code>parallel_preprocessing</code> is
ignored if <code>parallel=FALSE</code>.</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a thin wrapper around <code>summon_familiar</code>, and functions like
it, but automatically skips computation of variable importance, learning
and subsequent evaluation steps.
</p>
<p>The function returns an <code>experimentData</code> object, which can be used to
warm-start other experiments by providing it to the <code>experiment_data</code>
argument.
</p>


<h3>Value</h3>

<p>An <code>experimentData</code> object.
</p>

<hr>
<h2 id='precompute_feature_info'>Pre-compute feature information</h2><span id='topic+precompute_feature_info'></span>

<h3>Description</h3>

<p>Creates data assignment and subsequently extracts feature
information such as normalisation and clustering parameters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>precompute_feature_info(
  formula = NULL,
  data = NULL,
  experiment_data = NULL,
  cl = NULL,
  experimental_design = "fs+mb",
  verbose = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="precompute_feature_info_+3A_formula">formula</code></td>
<td>
<p>An R formula. The formula can only contain feature names and
dot (<code>.</code>). The <code>*</code> and <code>+1</code> operators are not supported as these refer to
columns that are not present in the data set.
</p>
<p>Use of the formula interface is optional.</p>
</td></tr>
<tr><td><code id="precompute_feature_info_+3A_data">data</code></td>
<td>
<p>A <code>data.table</code> object, a <code>data.frame</code> object, list containing
multiple <code>data.table</code> or <code>data.frame</code> objects, or paths to data files.
</p>
<p><code>data</code> should be provided if no file paths are provided to the <code>data_files</code>
argument. If both are provided, only <code>data</code> will be used.
</p>
<p>All data is expected to be in wide format, and ideally has a sample
identifier (see <code>sample_id_column</code>), batch identifier (see <code>cohort_column</code>)
and outcome columns (see <code>outcome_column</code>).
</p>
<p>In case paths are provided, the data should be stored as <code>csv</code>, <code>rds</code> or
<code>RData</code> files. See documentation for the <code>data_files</code> argument for more
information.</p>
</td></tr>
<tr><td><code id="precompute_feature_info_+3A_experiment_data">experiment_data</code></td>
<td>
<p>Experimental data may provided in the form of</p>
</td></tr>
<tr><td><code id="precompute_feature_info_+3A_cl">cl</code></td>
<td>
<p>Cluster created using the <code>parallel</code> package. This cluster is then
used to speed up computation through parallelisation. When a cluster is not
provided, parallelisation is performed by setting up a cluster on the local
machine.
</p>
<p>This parameter has no effect if the <code>parallel</code> argument is set to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="precompute_feature_info_+3A_experimental_design">experimental_design</code></td>
<td>
<p>(<strong>required</strong>) Defines what the experiment looks
like, e.g. <code>cv(bt(fs,20)+mb,3,2)</code> for 2 times repeated 3-fold
cross-validation with nested feature selection on 20 bootstraps and
model-building. The basic workflow components are:
</p>

<ul>
<li> <p><code>fs</code>: (required) feature selection step.
</p>
</li>
<li> <p><code>mb</code>: (required) model building step.
</p>
</li>
<li> <p><code>ev</code>: (optional) external validation. If validation batches or cohorts
are present in the dataset (<code>data</code>), these should be indicated in the
<code>validation_batch_id</code> argument.
</p>
</li></ul>

<p>The different components are linked using <code>+</code>.
</p>
<p>Different subsampling methods can be used in conjunction with the basic
workflow components:
</p>

<ul>
<li> <p><code>bs(x,n)</code>: (stratified) .632 bootstrap, with <code>n</code> the number of
bootstraps. In contrast to <code>bt</code>, feature pre-processing parameters and
hyperparameter optimisation are conducted on individual bootstraps.
</p>
</li>
<li> <p><code>bt(x,n)</code>: (stratified) .632 bootstrap, with <code>n</code> the number of
bootstraps. Unlike <code>bs</code> and other subsampling methods, no separate
pre-processing parameters or optimised hyperparameters will be determined
for each bootstrap.
</p>
</li>
<li> <p><code>cv(x,n,p)</code>: (stratified) <code>n</code>-fold cross-validation, repeated <code>p</code> times.
Pre-processing parameters are determined for each iteration.
</p>
</li>
<li> <p><code>lv(x)</code>: leave-one-out-cross-validation. Pre-processing parameters are
determined for each iteration.
</p>
</li>
<li> <p><code>ip(x)</code>: imbalance partitioning for addressing class imbalances on the
data set. Pre-processing parameters are determined for each partition. The
number of partitions generated depends on the imbalance correction method
(see the <code>imbalance_correction_method</code> parameter).
</p>
</li></ul>

<p>As shown in the example above, sampling algorithms can be nested.
</p>
<p>Though neither variable importance is determined nor models are learned
within <code>precompute_feature_info</code>, the corresponding elements are still
required to prevent issues when using the resulting <code>experimentData</code> object
to warm-start the experiments.
</p>
<p>The simplest valid experimental design is <code>fs+mb</code>. This is the default in
<code>precompute_feature_info</code>, and will determine feature parameters over the
entire dataset.
</p>
<p>This argument is ignored if the <code>experiment_data</code> argument is set.</p>
</td></tr>
<tr><td><code id="precompute_feature_info_+3A_verbose">verbose</code></td>
<td>
<p>Indicates verbosity of the results. Default is TRUE, and all
messages and warnings are returned.</p>
</td></tr>
<tr><td><code id="precompute_feature_info_+3A_...">...</code></td>
<td>

<p>Arguments passed on to <code><a href="#topic+.parse_experiment_settings">.parse_experiment_settings</a></code>, <code><a href="#topic+.parse_setup_settings">.parse_setup_settings</a></code>, <code><a href="#topic+.parse_preprocessing_settings">.parse_preprocessing_settings</a></code>
</p>

<dl>
<dt><code>batch_id_column</code></dt><dd><p>(<strong>recommended</strong>) Name of the column containing batch
or cohort identifiers. This parameter is required if more than one dataset
is provided, or if external validation is performed.
</p>
<p>In familiar any row of data is organised by four identifiers:
</p>

<ul>
<li><p> The batch identifier <code>batch_id_column</code>: This denotes the group to which a
set of samples belongs, e.g. patients from a single study, samples measured
in a batch, etc. The batch identifier is used for batch normalisation, as
well as selection of development and validation datasets.
</p>
</li>
<li><p> The sample identifier <code>sample_id_column</code>: This denotes the sample level,
e.g. data from a single individual. Subsets of data, e.g. bootstraps or
cross-validation folds, are created at this level.
</p>
</li>
<li><p> The series identifier <code>series_id_column</code>: Indicates measurements on a
single sample that may not share the same outcome value, e.g. a time
series, or the number of cells in a view.
</p>
</li>
<li><p> The repetition identifier: Indicates repeated measurements in a single
series where any feature values may differ, but the outcome does not.
Repetition identifiers are always implicitly set when multiple entries for
the same series of the same sample in the same batch that share the same
outcome are encountered.
</p>
</li></ul>
</dd>
<dt><code>sample_id_column</code></dt><dd><p>(<strong>recommended</strong>) Name of the column containing
sample or subject identifiers. See <code>batch_id_column</code> above for more
details.
</p>
<p>If unset, every row will be identified as a single sample.</p>
</dd>
<dt><code>series_id_column</code></dt><dd><p>(<strong>optional</strong>) Name of the column containing series
identifiers, which distinguish between measurements that are part of a
series for a single sample. See <code>batch_id_column</code> above for more details.
</p>
<p>If unset, rows which share the same batch and sample identifiers but have a
different outcome are assigned unique series identifiers.</p>
</dd>
<dt><code>development_batch_id</code></dt><dd><p>(<em>optional</em>) One or more batch or cohort
identifiers to constitute data sets for development. Defaults to all, or
all minus the identifiers in <code>validation_batch_id</code> for external validation.
Required if external validation is performed and <code>validation_batch_id</code> is
not provided.</p>
</dd>
<dt><code>validation_batch_id</code></dt><dd><p>(<em>optional</em>) One or more batch or cohort
identifiers to constitute data sets for external validation. Defaults to
all data sets except those in <code>development_batch_id</code> for external
validation, or none if not. Required if <code>development_batch_id</code> is not
provided.</p>
</dd>
<dt><code>outcome_name</code></dt><dd><p>(<em>optional</em>) Name of the modelled outcome. This name will
be used in figures created by <code>familiar</code>.
</p>
<p>If not set, the column name in <code>outcome_column</code> will be used for
<code>binomial</code>, <code>multinomial</code>, <code>count</code> and <code>continuous</code> outcomes. For other
outcomes (<code>survival</code> and <code>competing_risk</code>) no default is used.</p>
</dd>
<dt><code>outcome_column</code></dt><dd><p>(<strong>recommended</strong>) Name of the column containing the
outcome of interest. May be identified from a formula, if a formula is
provided as an argument. Otherwise an error is raised. Note that <code>survival</code>
and <code>competing_risk</code> outcome type outcomes require two columns that
indicate the time-to-event or the time of last follow-up and the event
status.</p>
</dd>
<dt><code>outcome_type</code></dt><dd><p>(<strong>recommended</strong>) Type of outcome found in the outcome
column. The outcome type determines many aspects of the overall process,
e.g. the available feature selection methods and learners, but also the
type of assessments that can be conducted to evaluate the resulting models.
Implemented outcome types are:
</p>

<ul>
<li> <p><code>binomial</code>: categorical outcome with 2 levels.
</p>
</li>
<li> <p><code>multinomial</code>: categorical outcome with 2 or more levels.
</p>
</li>
<li> <p><code>count</code>: Poisson-distributed numeric outcomes.
</p>
</li>
<li> <p><code>continuous</code>: general continuous numeric outcomes.
</p>
</li>
<li> <p><code>survival</code>: survival outcome for time-to-event data.
</p>
</li></ul>

<p>If not provided, the algorithm will attempt to obtain outcome_type from
contents of the outcome column. This may lead to unexpected results, and we
therefore advise to provide this information manually.
</p>
<p>Note that <code>competing_risk</code> survival analysis are not fully supported, and
is currently not a valid choice for <code>outcome_type</code>.</p>
</dd>
<dt><code>class_levels</code></dt><dd><p>(<em>optional</em>) Class levels for <code>binomial</code> or <code>multinomial</code>
outcomes. This argument can be used to specify the ordering of levels for
categorical outcomes. These class levels must exactly match the levels
present in the outcome column.</p>
</dd>
<dt><code>event_indicator</code></dt><dd><p>(<strong>recommended</strong>) Indicator for events in <code>survival</code>
and <code>competing_risk</code> analyses. <code>familiar</code> will automatically recognise <code>1</code>,
<code>true</code>, <code>t</code>, <code>y</code> and <code>yes</code> as event indicators, including different
capitalisations. If this parameter is set, it replaces the default values.</p>
</dd>
<dt><code>censoring_indicator</code></dt><dd><p>(<strong>recommended</strong>) Indicator for right-censoring in
<code>survival</code> and <code>competing_risk</code> analyses. <code>familiar</code> will automatically
recognise <code>0</code>, <code>false</code>, <code>f</code>, <code>n</code>, <code>no</code> as censoring indicators, including
different capitalisations. If this parameter is set, it replaces the
default values.</p>
</dd>
<dt><code>competing_risk_indicator</code></dt><dd><p>(<strong>recommended</strong>) Indicator for competing
risks in <code>competing_risk</code> analyses. There are no default values, and if
unset, all values other than those specified by the <code>event_indicator</code> and
<code>censoring_indicator</code> parameters are considered to indicate competing
risks.</p>
</dd>
<dt><code>signature</code></dt><dd><p>(<em>optional</em>) One or more names of feature columns that are
considered part of a specific signature. Features specified here will
always be used for modelling. Ranking from feature selection has no effect
for these features.</p>
</dd>
<dt><code>novelty_features</code></dt><dd><p>(<em>optional</em>) One or more names of feature columns
that should be included for the purpose of novelty detection.</p>
</dd>
<dt><code>exclude_features</code></dt><dd><p>(<em>optional</em>) Feature columns that will be removed
from the data set. Cannot overlap with features in <code>signature</code>,
<code>novelty_features</code> or <code>include_features</code>.</p>
</dd>
<dt><code>include_features</code></dt><dd><p>(<em>optional</em>) Feature columns that are specifically
included in the data set. By default all features are included. Cannot
overlap with <code>exclude_features</code>, but may overlap <code>signature</code>. Features in
<code>signature</code> and <code>novelty_features</code> are always included. If both
<code>exclude_features</code> and <code>include_features</code> are provided, <code>include_features</code>
takes precedence, provided that there is no overlap between the two.</p>
</dd>
<dt><code>reference_method</code></dt><dd><p>(<em>optional</em>) Method used to set reference levels for
categorical features. There are several options:
</p>

<ul>
<li> <p><code>auto</code> (default): Categorical features that are not explicitly set by the
user, i.e. columns containing boolean values or characters, use the most
frequent level as reference. Categorical features that are explicitly set,
i.e. as factors, are used as is.
</p>
</li>
<li> <p><code>always</code>: Both automatically detected and user-specified categorical
features have the reference level set to the most frequent level. Ordinal
features are not altered, but are used as is.
</p>
</li>
<li> <p><code>never</code>: User-specified categorical features are used as is.
Automatically detected categorical features are simply sorted, and the
first level is then used as the reference level. This was the behaviour
prior to familiar version 1.3.0.
</p>
</li></ul>
</dd>
<dt><code>imbalance_correction_method</code></dt><dd><p>(<em>optional</em>) Type of method used to
address class imbalances. Available options are:
</p>

<ul>
<li> <p><code>full_undersampling</code> (default): All data will be used in an ensemble
fashion. The full minority class will appear in each partition, but
majority classes are undersampled until all data have been used.
</p>
</li>
<li> <p><code>random_undersampling</code>: Randomly undersamples majority classes. This is
useful in cases where full undersampling would lead to the formation of
many models due major overrepresentation of the largest class.
</p>
</li></ul>

<p>This parameter is only used in combination with imbalance partitioning in
the experimental design, and <code>ip</code> should therefore appear in the string
that defines the design.</p>
</dd>
<dt><code>imbalance_n_partitions</code></dt><dd><p>(<em>optional</em>) Number of times random
undersampling should be repeated. 10 undersampled subsets with balanced
classes are formed by default.</p>
</dd>
<dt><code>parallel</code></dt><dd><p>(<em>optional</em>) Enable parallel processing. Defaults to <code>TRUE</code>.
When set to <code>FALSE</code>, this disables all parallel processing, regardless of
specific parameters such as <code>parallel_preprocessing</code>. However, when
<code>parallel</code> is <code>TRUE</code>, parallel processing of different parts of the
workflow can be disabled by setting respective flags to <code>FALSE</code>.</p>
</dd>
<dt><code>parallel_nr_cores</code></dt><dd><p>(<em>optional</em>) Number of cores available for
parallelisation. Defaults to 2. This setting does nothing if
parallelisation is disabled.</p>
</dd>
<dt><code>restart_cluster</code></dt><dd><p>(<em>optional</em>) Restart nodes used for parallel computing
to free up memory prior to starting a parallel process. Note that it does
take time to set up the clusters. Therefore setting this argument to <code>TRUE</code>
may impact processing speed. This argument is ignored if <code>parallel</code> is
<code>FALSE</code> or the cluster was initialised outside of familiar. Default is
<code>FALSE</code>, which causes the clusters to be initialised only once.</p>
</dd>
<dt><code>cluster_type</code></dt><dd><p>(<em>optional</em>) Selection of the cluster type for parallel
processing. Available types are the ones supported by the parallel package
that is part of the base R distribution: <code>psock</code> (default), <code>fork</code>, <code>mpi</code>,
<code>nws</code>, <code>sock</code>. In addition, <code>none</code> is available, which also disables
parallel processing.</p>
</dd>
<dt><code>backend_type</code></dt><dd><p>(<em>optional</em>) Selection of the backend for distributing
copies of the data. This backend ensures that only a single master copy is
kept in memory. This limits memory usage during parallel processing.
</p>
<p>Several backend options are available, notably <code>socket_server</code>, and <code>none</code>
(default). <code>socket_server</code> is based on the callr package and R sockets,
comes with <code>familiar</code> and is available for any OS. <code>none</code> uses the package
environment of familiar to store data, and is available for any OS.
However, <code>none</code> requires copying of data to any parallel process, and has a
larger memory footprint.</p>
</dd>
<dt><code>server_port</code></dt><dd><p>(<em>optional</em>) Integer indicating the port on which the
socket server or RServe process should communicate. Defaults to port 6311.
Note that ports 0 to 1024 and 49152 to 65535 cannot be used.</p>
</dd>
<dt><code>feature_max_fraction_missing</code></dt><dd><p>(<em>optional</em>) Numeric value between <code>0.0</code>
and <code>0.95</code> that determines the meximum fraction of missing values that
still allows a feature to be included in the data set. All features with a
missing value fraction over this threshold are not processed further. The
default value is <code>0.30</code>.</p>
</dd>
<dt><code>sample_max_fraction_missing</code></dt><dd><p>(<em>optional</em>) Numeric value between <code>0.0</code>
and <code>0.95</code> that determines the maximum fraction of missing values that
still allows a sample to be included in the data set. All samples with a
missing value fraction over this threshold are excluded and not processed
further. The default value is <code>0.30</code>.</p>
</dd>
<dt><code>filter_method</code></dt><dd><p>(<em>optional</em>) One or methods used to reduce
dimensionality of the data set by removing irrelevant or poorly
reproducible features.
</p>
<p>Several method are available:
</p>

<ul>
<li> <p><code>none</code> (default): None of the features will be filtered.
</p>
</li>
<li> <p><code>low_variance</code>: Features with a variance below the
<code>low_var_minimum_variance_threshold</code> are filtered. This can be useful to
filter, for example, genes that are not differentially expressed.
</p>
</li>
<li> <p><code>univariate_test</code>: Features undergo a univariate regression using an
outcome-appropriate regression model. The p-value of the model coefficient
is collected. Features with coefficient p or q-value above the
<code>univariate_test_threshold</code> are subsequently filtered.
</p>
</li>
<li> <p><code>robustness</code>: Features that are not sufficiently robust according to the
intraclass correlation coefficient are filtered. Use of this method
requires that repeated measurements are present in the data set, i.e. there
should be entries for which the sample and cohort identifiers are the same.
</p>
</li></ul>

<p>More than one method can be used simultaneously. Features with singular
values are always filtered, as these do not contain information.</p>
</dd>
<dt><code>univariate_test_threshold</code></dt><dd><p>(<em>optional</em>) Numeric value between <code>1.0</code> and
<code>0.0</code> that determines which features are irrelevant and will be filtered by
the <code>univariate_test</code>. The p or q-values are compared to this threshold.
All features with values above the threshold are filtered. The default
value is <code>0.20</code>.</p>
</dd>
<dt><code>univariate_test_threshold_metric</code></dt><dd><p>(<em>optional</em>) Metric used with the to
compare the <code>univariate_test_threshold</code> against. The following metrics can
be chosen:
</p>

<ul>
<li> <p><code>p_value</code> (default): The unadjusted p-value of each feature is used for
to filter features.
</p>
</li>
<li> <p><code>q_value</code>: The q-value (Story, 2002), is used to filter features. Some
data sets may have insufficient samples to compute the q-value. The
<code>qvalue</code> package must be installed from Bioconductor to use this method.
</p>
</li></ul>
</dd>
<dt><code>univariate_test_max_feature_set_size</code></dt><dd><p>(<em>optional</em>) Maximum size of the
feature set after the univariate test. P or q values of features are
compared against the threshold, but if the resulting data set would be
larger than this setting, only the most relevant features up to the desired
feature set size are selected.
</p>
<p>The default value is <code>NULL</code>, which causes features to be filtered based on
their relevance only.</p>
</dd>
<dt><code>low_var_minimum_variance_threshold</code></dt><dd><p>(required, if used) Numeric value
that determines which features will be filtered by the <code>low_variance</code>
method. The variance of each feature is computed and compared to the
threshold. If it is below the threshold, the feature is removed.
</p>
<p>This parameter has no default value and should be set if <code>low_variance</code> is
used.</p>
</dd>
<dt><code>low_var_max_feature_set_size</code></dt><dd><p>(<em>optional</em>) Maximum size of the feature
set after filtering features with a low variance. All features are first
compared against <code>low_var_minimum_variance_threshold</code>. If the resulting
feature set would be larger than specified, only the most strongly varying
features will be selected, up to the desired size of the feature set.
</p>
<p>The default value is <code>NULL</code>, which causes features to be filtered based on
their variance only.</p>
</dd>
<dt><code>robustness_icc_type</code></dt><dd><p>(<em>optional</em>) String indicating the type of
intraclass correlation coefficient (<code>1</code>, <code>2</code> or <code>3</code>) that should be used to
compute robustness for features in repeated measurements. These types
correspond to the types in Shrout and Fleiss (1979). The default value is
<code>1</code>.</p>
</dd>
<dt><code>robustness_threshold_metric</code></dt><dd><p>(<em>optional</em>) String indicating which
specific intraclass correlation coefficient (ICC) metric should be used to
filter features. This should be one of:
</p>

<ul>
<li> <p><code>icc</code>: The estimated ICC value itself.
</p>
</li>
<li> <p><code>icc_low</code> (default): The estimated lower limit of the 95% confidence
interval of the ICC, as suggested by Koo and Li (2016).
</p>
</li>
<li> <p><code>icc_panel</code>: The estimated ICC value over the panel average, i.e. the ICC
that would be obtained if all repeated measurements were averaged.
</p>
</li>
<li> <p><code>icc_panel_low</code>: The estimated lower limit of the 95% confidence interval
of the panel ICC.
</p>
</li></ul>
</dd>
<dt><code>robustness_threshold_value</code></dt><dd><p>(<em>optional</em>) The intraclass correlation
coefficient value that is as threshold. The default value is <code>0.70</code>.</p>
</dd>
<dt><code>transformation_method</code></dt><dd><p>(<em>optional</em>) The transformation method used to
change the distribution of the data to be more normal-like. The following
methods are available:
</p>

<ul>
<li> <p><code>none</code>: This disables transformation of features.
</p>
</li>
<li> <p><code>yeo_johnson</code> (default): Transformation using the Yeo-Johnson
transformation (Yeo and Johnson, 2000). The algorithm tests various lambda
values and selects the lambda that maximises the log-likelihood.
</p>
</li>
<li> <p><code>yeo_johnson_trim</code>: As <code>yeo_johnson</code>, but based on the set of feature
values where the 5% lowest and 5% highest values are discarded. This
reduces the effect of outliers.
</p>
</li>
<li> <p><code>yeo_johnson_winsor</code>: As <code>yeo_johnson</code>, but based on the set of feature
values where the 5% lowest and 5% highest values are winsorised. This
reduces the effect of outliers.
</p>
</li>
<li> <p><code>yeo_johnson_robust</code>: A robust version of <code>yeo_johnson</code> after Raymaekers
and Rousseeuw (2021). This method is less sensitive to outliers.
</p>
</li>
<li> <p><code>box_cox</code>: Transformation using the Box-Cox transformation (Box and Cox,
1964). Unlike the Yeo-Johnson transformation, the Box-Cox transformation
requires that all data are positive. Features that contain zero or negative
values cannot be transformed using this transformation. The algorithm tests
various lambda values and selects the lambda that maximises the
log-likelihood.
</p>
</li>
<li> <p><code>box_cox_trim</code>: As <code>box_cox</code>, but based on the set of feature values
where the 5% lowest and 5% highest values are discarded. This reduces the
effect of outliers.
</p>
</li>
<li> <p><code>box_cox_winsor</code>: As <code>box_cox</code>, but based on the set of feature values
where the 5% lowest and 5% highest values are winsorised. This reduces the
effect of outliers.
</p>
</li>
<li> <p><code>box_cox_robust</code>: A robust verson of <code>box_cox</code> after Raymaekers and
Rousseew (2021). This method is less sensitive to outliers.
</p>
</li></ul>

<p>Only features that contain numerical data are transformed. Transformation
parameters obtained in development data are stored within <code>featureInfo</code>
objects for later use with validation data sets.</p>
</dd>
<dt><code>normalisation_method</code></dt><dd><p>(<em>optional</em>) The normalisation method used to
improve the comparability between numerical features that may have very
different scales. The following normalisation methods can be chosen:
</p>

<ul>
<li> <p><code>none</code>: This disables feature normalisation.
</p>
</li>
<li> <p><code>standardisation</code>: Features are normalised by subtraction of their mean
values and division by their standard deviations. This causes every feature
to be have a center value of 0.0 and standard deviation of 1.0.
</p>
</li>
<li> <p><code>standardisation_trim</code>: As <code>standardisation</code>, but based on the set of
feature values where the 5% lowest and 5% highest values are discarded.
This reduces the effect of outliers.
</p>
</li>
<li> <p><code>standardisation_winsor</code>: As <code>standardisation</code>, but based on the set of
feature values where the 5% lowest and 5% highest values are winsorised.
This reduces the effect of outliers.
</p>
</li>
<li> <p><code>standardisation_robust</code> (default): A robust version of <code>standardisation</code>
that relies on computing Huber's M-estimators for location and scale.
</p>
</li>
<li> <p><code>normalisation</code>: Features are normalised by subtraction of their minimum
values and division by their ranges. This maps all feature values to a
<code class="reqn">[0, 1]</code> interval.
</p>
</li>
<li> <p><code>normalisation_trim</code>: As <code>normalisation</code>, but based on the set of feature
values where the 5% lowest and 5% highest values are discarded. This
reduces the effect of outliers.
</p>
</li>
<li> <p><code>normalisation_winsor</code>: As <code>normalisation</code>, but based on the set of
feature values where the 5% lowest and 5% highest values are winsorised.
This reduces the effect of outliers.
</p>
</li>
<li> <p><code>quantile</code>: Features are normalised by subtraction of their median values
and division by their interquartile range.
</p>
</li>
<li> <p><code>mean_centering</code>: Features are centered by substracting the mean, but do
not undergo rescaling.
</p>
</li></ul>

<p>Only features that contain numerical data are normalised. Normalisation
parameters obtained in development data are stored within <code>featureInfo</code>
objects for later use with validation data sets.</p>
</dd>
<dt><code>batch_normalisation_method</code></dt><dd><p>(<em>optional</em>) The method used for batch
normalisation. Available methods are:
</p>

<ul>
<li> <p><code>none</code> (default): This disables batch normalisation of features.
</p>
</li>
<li> <p><code>standardisation</code>: Features within each batch are normalised by
subtraction of the mean value and division by the standard deviation in
each batch.
</p>
</li>
<li> <p><code>standardisation_trim</code>: As <code>standardisation</code>, but based on the set of
feature values where the 5% lowest and 5% highest values are discarded.
This reduces the effect of outliers.
</p>
</li>
<li> <p><code>standardisation_winsor</code>: As <code>standardisation</code>, but based on the set of
feature values where the 5% lowest and 5% highest values are winsorised.
This reduces the effect of outliers.
</p>
</li>
<li> <p><code>standardisation_robust</code>: A robust version of <code>standardisation</code> that
relies on computing Huber's M-estimators for location and scale within each
batch.
</p>
</li>
<li> <p><code>normalisation</code>: Features within each batch are normalised by subtraction
of their minimum values and division by their range in each batch. This
maps all feature values in each batch to a <code class="reqn">[0, 1]</code> interval.
</p>
</li>
<li> <p><code>normalisation_trim</code>: As <code>normalisation</code>, but based on the set of feature
values where the 5% lowest and 5% highest values are discarded. This
reduces the effect of outliers.
</p>
</li>
<li> <p><code>normalisation_winsor</code>: As <code>normalisation</code>, but based on the set of
feature values where the 5% lowest and 5% highest values are winsorised.
This reduces the effect of outliers.
</p>
</li>
<li> <p><code>quantile</code>: Features in each batch are normalised by subtraction of the
median value and division by the interquartile range of each batch.
</p>
</li>
<li> <p><code>mean_centering</code>: Features in each batch are centered on 0.0 by
substracting the mean value in each batch, but are not rescaled.
</p>
</li>
<li> <p><code>combat_parametric</code>: Batch adjustments using parametric empirical Bayes
(Johnson et al, 2007). <code>combat_p</code> leads to the same method.
</p>
</li>
<li> <p><code>combat_non_parametric</code>: Batch adjustments using non-parametric empirical
Bayes (Johnson et al, 2007). <code>combat_np</code> and <code>combat</code> lead to the same
method. Note that we reduced complexity from O(<code class="reqn">n^2</code>) to O(<code class="reqn">n</code>) by
only computing batch adjustment parameters for each feature on a subset of
50 randomly selected features, instead of all features.
</p>
</li></ul>

<p>Only features that contain numerical data are normalised using batch
normalisation. Batch normalisation parameters obtained in development data
are stored within <code>featureInfo</code> objects for later use with validation data
sets, in case the validation data is from the same batch.
</p>
<p>If validation data contains data from unknown batches, normalisation
parameters are separately determined for these batches.
</p>
<p>Note that for both empirical Bayes methods, the batch effect is assumed to
produce results across the features. This is often true for things such as
gene expressions, but the assumption may not hold generally.
</p>
<p>When performing batch normalisation, it is moreover important to check that
differences between batches or cohorts are not related to the studied
endpoint.</p>
</dd>
<dt><code>imputation_method</code></dt><dd><p>(<em>optional</em>) Method used for imputing missing
feature values. Two methods are implemented:
</p>

<ul>
<li> <p><code>simple</code>: Simple replacement of a missing value by the median value (for
numeric features) or the modal value (for categorical features).
</p>
</li>
<li> <p><code>lasso</code>: Imputation of missing value by lasso regression (using <code>glmnet</code>)
based on information contained in other features.
</p>
</li></ul>

<p><code>simple</code> imputation precedes <code>lasso</code> imputation to ensure that any missing
values in predictors required for <code>lasso</code> regression are resolved. The
<code>lasso</code> estimate is then used to replace the missing value.
</p>
<p>The default value depends on the number of features in the dataset. If the
number is lower than 100, <code>lasso</code> is used by default, and <code>simple</code>
otherwise.
</p>
<p>Only single imputation is performed. Imputation models and parameters are
stored within <code>featureInfo</code> objects for later use with validation data
sets.</p>
</dd>
<dt><code>cluster_method</code></dt><dd><p>(<em>optional</em>) Clustering is performed to identify and
replace redundant features, for example those that are highly correlated.
Such features do not carry much additional information and may be removed
or replaced instead (Park et al., 2007; Tolosi and Lengauer, 2011).
</p>
<p>The cluster method determines the algorithm used to form the clusters. The
following cluster methods are implemented:
</p>

<ul>
<li> <p><code>none</code>: No clustering is performed.
</p>
</li>
<li> <p><code>hclust</code> (default): Hierarchical agglomerative clustering. If the
<code>fastcluster</code> package is installed, <code>fastcluster::hclust</code> is used (Muellner
2013), otherwise <code>stats::hclust</code> is used.
</p>
</li>
<li> <p><code>agnes</code>: Hierarchical clustering using agglomerative nesting (Kaufman and
Rousseeuw, 1990). This algorithm is similar to <code>hclust</code>, but uses the
<code>cluster::agnes</code> implementation.
</p>
</li>
<li> <p><code>diana</code>: Divisive analysis hierarchical clustering. This method uses
divisive instead of agglomerative clustering (Kaufman and Rousseeuw, 1990).
<code>cluster::diana</code> is used.
</p>
</li>
<li> <p><code>pam</code>: Partioning around medioids. This partitions the data into $k$
clusters around medioids (Kaufman and Rousseeuw, 1990). $k$ is selected
using the <code>silhouette</code> metric. <code>pam</code> is implemented using the
<code>cluster::pam</code> function.
</p>
</li></ul>

<p>Clusters and cluster information is stored within <code>featureInfo</code> objects for
later use with validation data sets. This enables reproduction of the same
clusters as formed in the development data set.</p>
</dd>
<dt><code>cluster_linkage_method</code></dt><dd><p>(<em>optional</em>) Linkage method used for
agglomerative clustering in <code>hclust</code> and <code>agnes</code>. The following linkage
methods can be used:
</p>

<ul>
<li> <p><code>average</code> (default): Average linkage.
</p>
</li>
<li> <p><code>single</code>: Single linkage.
</p>
</li>
<li> <p><code>complete</code>: Complete linkage.
</p>
</li>
<li> <p><code>weighted</code>: Weighted linkage, also known as McQuitty linkage.
</p>
</li>
<li> <p><code>ward</code>: Linkage using Ward's minimum variance method.
</p>
</li></ul>

<p><code>diana</code> and <code>pam</code> do not require a linkage method.</p>
</dd>
<dt><code>cluster_cut_method</code></dt><dd><p>(<em>optional</em>) The method used to define the actual
clusters. The following methods can be used:
</p>

<ul>
<li> <p><code>silhouette</code>: Clusters are formed based on the silhouette score
(Rousseeuw, 1987). The average silhouette score is computed from 2 to
<code class="reqn">n</code> clusters, with <code class="reqn">n</code> the number of features. Clusters are only
formed if the average silhouette exceeds 0.50, which indicates reasonable
evidence for structure. This procedure may be slow if the number of
features is large (&gt;100s).
</p>
</li>
<li> <p><code>fixed_cut</code>: Clusters are formed by cutting the hierarchical tree at the
point indicated by the <code>cluster_similarity_threshold</code>, e.g. where features
in a cluster have an average Spearman correlation of 0.90. <code>fixed_cut</code> is
only available for <code>agnes</code>, <code>diana</code> and <code>hclust</code>.
</p>
</li>
<li> <p><code>dynamic_cut</code>: Dynamic cluster formation using the cutting algorithm in
the <code>dynamicTreeCut</code> package. This package should be installed to select
this option. <code>dynamic_cut</code> can only be used with <code>agnes</code> and <code>hclust</code>.
</p>
</li></ul>

<p>The default options are <code>silhouette</code> for partioning around medioids (<code>pam</code>)
and <code>fixed_cut</code> otherwise.</p>
</dd>
<dt><code>cluster_similarity_metric</code></dt><dd><p>(<em>optional</em>) Clusters are formed based on
feature similarity. All features are compared in a pair-wise fashion to
compute similarity, for example correlation. The resulting similarity grid
is converted into a distance matrix that is subsequently used for
clustering. The following metrics are supported to compute pairwise
similarities:
</p>

<ul>
<li> <p><code>mutual_information</code> (default): normalised mutual information.
</p>
</li>
<li> <p><code>mcfadden_r2</code>: McFadden's pseudo R-squared (McFadden, 1974).
</p>
</li>
<li> <p><code>cox_snell_r2</code>: Cox and Snell's pseudo R-squared (Cox and Snell, 1989).
</p>
</li>
<li> <p><code>nagelkerke_r2</code>: Nagelkerke's pseudo R-squared (Nagelkerke, 1991).
</p>
</li>
<li> <p><code>spearman</code>: Spearman's rank order correlation.
</p>
</li>
<li> <p><code>kendall</code>: Kendall rank correlation.
</p>
</li>
<li> <p><code>pearson</code>: Pearson product-moment correlation.
</p>
</li></ul>

<p>The pseudo R-squared metrics can be used to assess similarity between mixed
pairs of numeric and categorical features, as these are based on the
log-likelihood of regression models. In <code>familiar</code>, the more informative
feature is used as the predictor and the other feature as the reponse
variable. In numeric-categorical pairs, the numeric feature is considered
to be more informative and is thus used as the predictor. In
categorical-categorical pairs, the feature with most levels is used as the
predictor.
</p>
<p>In case any of the classical correlation coefficients (<code>pearson</code>,
<code>spearman</code> and <code>kendall</code>) are used with (mixed) categorical features, the
categorical features are one-hot encoded and the mean correlation over all
resulting pairs is used as similarity.</p>
</dd>
<dt><code>cluster_similarity_threshold</code></dt><dd><p>(<em>optional</em>) The threshold level for
pair-wise similarity that is required to form clusters using <code>fixed_cut</code>.
This should be a numerical value between 0.0 and 1.0. Note however, that a
reasonable threshold value depends strongly on the similarity metric. The
following are the default values used:
</p>

<ul>
<li> <p><code>mcfadden_r2</code> and <code>mutual_information</code>: <code>0.30</code>
</p>
</li>
<li> <p><code>cox_snell_r2</code> and <code>nagelkerke_r2</code>: <code>0.75</code>
</p>
</li>
<li> <p><code>spearman</code>, <code>kendall</code> and <code>pearson</code>: <code>0.90</code>
</p>
</li></ul>

<p>Alternatively, if the <code style="white-space: pre;">&#8288;fixed cut&#8288;</code> method is not used, this value determines
whether any clustering should be performed, because the data may not
contain highly similar features. The default values in this situation are:
</p>

<ul>
<li> <p><code>mcfadden_r2</code>  and <code>mutual_information</code>: <code>0.25</code>
</p>
</li>
<li> <p><code>cox_snell_r2</code> and <code>nagelkerke_r2</code>: <code>0.40</code>
</p>
</li>
<li> <p><code>spearman</code>, <code>kendall</code> and <code>pearson</code>: <code>0.70</code>
</p>
</li></ul>

<p>The threshold value is converted to a distance (1-similarity) prior to
cutting hierarchical trees.</p>
</dd>
<dt><code>cluster_representation_method</code></dt><dd><p>(<em>optional</em>) Method used to determine
how the information of co-clustered features is summarised and used to
represent the cluster. The following methods can be selected:
</p>

<ul>
<li> <p><code>best_predictor</code> (default): The feature with the highest importance
according to univariate regression with the outcome is used to represent
the cluster.
</p>
</li>
<li> <p><code>medioid</code>: The feature closest to the cluster center, i.e. the feature
that is most similar to the remaining features in the cluster, is used to
represent the feature.
</p>
</li>
<li> <p><code>mean</code>: A meta-feature is generated by averaging the feature values for
all features in a cluster. This method aligns all features so that all
features will be positively correlated prior to averaging. Should a cluster
contain one or more categorical features, the <code>medioid</code> method will be used
instead, as averaging is not possible. Note that if this method is chosen,
the <code>normalisation_method</code> parameter should be one of <code>standardisation</code>,
<code>standardisation_trim</code>, <code>standardisation_winsor</code> or <code>quantile</code>.'
</p>
</li></ul>

<p>If the <code>pam</code> cluster method is selected, only the <code>medioid</code> method can be
used. In that case 1 medioid is used by default.</p>
</dd>
<dt><code>parallel_preprocessing</code></dt><dd><p>(<em>optional</em>) Enable parallel processing for the
preprocessing workflow. Defaults to <code>TRUE</code>. When set to <code>FALSE</code>, this will
disable the use of parallel processing while preprocessing, regardless of
the settings of the <code>parallel</code> parameter. <code>parallel_preprocessing</code> is
ignored if <code>parallel=FALSE</code>.</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a thin wrapper around <code>summon_familiar</code>, and functions like
it, but automatically skips computation of variable importance, learning
and subsequent evaluation steps.
</p>
<p>The function returns an <code>experimentData</code> object, which can be used to
warm-start other experiments by providing it to the <code>experiment_data</code>
argument.
</p>


<h3>Value</h3>

<p>An <code>experimentData</code> object.
</p>

<hr>
<h2 id='precompute_vimp'>Pre-compute variable importance</h2><span id='topic+precompute_vimp'></span>

<h3>Description</h3>

<p>Creates data assignment, extracts feature information and
subsequently computes variable importance.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>precompute_vimp(
  formula = NULL,
  data = NULL,
  experiment_data = NULL,
  cl = NULL,
  experimental_design = "fs+mb",
  fs_method = NULL,
  fs_method_parameter = NULL,
  verbose = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="precompute_vimp_+3A_formula">formula</code></td>
<td>
<p>An R formula. The formula can only contain feature names and
dot (<code>.</code>). The <code>*</code> and <code>+1</code> operators are not supported as these refer to
columns that are not present in the data set.
</p>
<p>Use of the formula interface is optional.</p>
</td></tr>
<tr><td><code id="precompute_vimp_+3A_data">data</code></td>
<td>
<p>A <code>data.table</code> object, a <code>data.frame</code> object, list containing
multiple <code>data.table</code> or <code>data.frame</code> objects, or paths to data files.
</p>
<p><code>data</code> should be provided if no file paths are provided to the <code>data_files</code>
argument. If both are provided, only <code>data</code> will be used.
</p>
<p>All data is expected to be in wide format, and ideally has a sample
identifier (see <code>sample_id_column</code>), batch identifier (see <code>cohort_column</code>)
and outcome columns (see <code>outcome_column</code>).
</p>
<p>In case paths are provided, the data should be stored as <code>csv</code>, <code>rds</code> or
<code>RData</code> files. See documentation for the <code>data_files</code> argument for more
information.</p>
</td></tr>
<tr><td><code id="precompute_vimp_+3A_experiment_data">experiment_data</code></td>
<td>
<p>Experimental data may provided in the form of</p>
</td></tr>
<tr><td><code id="precompute_vimp_+3A_cl">cl</code></td>
<td>
<p>Cluster created using the <code>parallel</code> package. This cluster is then
used to speed up computation through parallelisation. When a cluster is not
provided, parallelisation is performed by setting up a cluster on the local
machine.
</p>
<p>This parameter has no effect if the <code>parallel</code> argument is set to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="precompute_vimp_+3A_experimental_design">experimental_design</code></td>
<td>
<p>(<strong>required</strong>) Defines what the experiment looks
like, e.g. <code>cv(bt(fs,20)+mb,3,2)</code> for 2 times repeated 3-fold
cross-validation with nested feature selection on 20 bootstraps and
model-building. The basic workflow components are:
</p>

<ul>
<li> <p><code>fs</code>: (required) feature selection step.
</p>
</li>
<li> <p><code>mb</code>: (required) model building step. Though models are not learned by
<code>precompute_vimp</code>, this element is still required to prevent issues when
using the resulting <code>experimentData</code> object to warm-start the experiments.
</p>
</li>
<li> <p><code>ev</code>: (optional) external validation. If validation batches or cohorts
are present in the dataset (<code>data</code>), these should be indicated in the
<code>validation_batch_id</code> argument.
</p>
</li></ul>

<p>The different components are linked using <code>+</code>.
</p>
<p>Different subsampling methods can be used in conjunction with the basic
workflow components:
</p>

<ul>
<li> <p><code>bs(x,n)</code>: (stratified) .632 bootstrap, with <code>n</code> the number of
bootstraps. In contrast to <code>bt</code>, feature pre-processing parameters and
hyperparameter optimisation are conducted on individual bootstraps.
</p>
</li>
<li> <p><code>bt(x,n)</code>: (stratified) .632 bootstrap, with <code>n</code> the number of
bootstraps. Unlike <code>bs</code> and other subsampling methods, no separate
pre-processing parameters or optimised hyperparameters will be determined
for each bootstrap.
</p>
</li>
<li> <p><code>cv(x,n,p)</code>: (stratified) <code>n</code>-fold cross-validation, repeated <code>p</code> times.
Pre-processing parameters are determined for each iteration.
</p>
</li>
<li> <p><code>lv(x)</code>: leave-one-out-cross-validation. Pre-processing parameters are
determined for each iteration.
</p>
</li>
<li> <p><code>ip(x)</code>: imbalance partitioning for addressing class imbalances on the
data set. Pre-processing parameters are determined for each partition. The
number of partitions generated depends on the imbalance correction method
(see the <code>imbalance_correction_method</code> parameter).
</p>
</li></ul>

<p>As shown in the example above, sampling algorithms can be nested.
</p>
<p>The simplest valid experimental design is <code>fs+mb</code>. This is the default in
<code>precompute_vimp</code>, and will compute variable importance over the entire
dataset.
</p>
<p>This argument is ignored if the <code>experiment_data</code> argument is set.</p>
</td></tr>
<tr><td><code id="precompute_vimp_+3A_fs_method">fs_method</code></td>
<td>
<p>(<strong>required</strong>) Feature selection method to be used for
determining variable importance. <code>familiar</code> implements various feature
selection methods. Please refer to the vignette on feature selection
methods for more details.
</p>
<p>More than one feature selection method can be chosen. The experiment will
then repeated for each feature selection method.
</p>
<p>Feature selection methods determines the ranking of features. Actual
selection of features is done by optimising the signature size model
hyperparameter during the hyperparameter optimisation step.</p>
</td></tr>
<tr><td><code id="precompute_vimp_+3A_fs_method_parameter">fs_method_parameter</code></td>
<td>
<p>(<em>optional</em>) List of lists containing parameters
for feature selection methods. Each sublist should have the name of the
feature selection method it corresponds to.
</p>
<p>Most feature selection methods do not have parameters that can be set.
Please refer to the vignette on feature selection methods for more details.
Note that if the feature selection method is based on a learner (e.g. lasso
regression), hyperparameter optimisation may be performed prior to
assessing variable importance.</p>
</td></tr>
<tr><td><code id="precompute_vimp_+3A_verbose">verbose</code></td>
<td>
<p>Indicates verbosity of the results. Default is TRUE, and all
messages and warnings are returned.</p>
</td></tr>
<tr><td><code id="precompute_vimp_+3A_...">...</code></td>
<td>

<p>Arguments passed on to <code><a href="#topic+.parse_experiment_settings">.parse_experiment_settings</a></code>, <code><a href="#topic+.parse_setup_settings">.parse_setup_settings</a></code>, <code><a href="#topic+.parse_preprocessing_settings">.parse_preprocessing_settings</a></code>, <code><a href="#topic+.parse_feature_selection_settings">.parse_feature_selection_settings</a></code>
</p>

<dl>
<dt><code>batch_id_column</code></dt><dd><p>(<strong>recommended</strong>) Name of the column containing batch
or cohort identifiers. This parameter is required if more than one dataset
is provided, or if external validation is performed.
</p>
<p>In familiar any row of data is organised by four identifiers:
</p>

<ul>
<li><p> The batch identifier <code>batch_id_column</code>: This denotes the group to which a
set of samples belongs, e.g. patients from a single study, samples measured
in a batch, etc. The batch identifier is used for batch normalisation, as
well as selection of development and validation datasets.
</p>
</li>
<li><p> The sample identifier <code>sample_id_column</code>: This denotes the sample level,
e.g. data from a single individual. Subsets of data, e.g. bootstraps or
cross-validation folds, are created at this level.
</p>
</li>
<li><p> The series identifier <code>series_id_column</code>: Indicates measurements on a
single sample that may not share the same outcome value, e.g. a time
series, or the number of cells in a view.
</p>
</li>
<li><p> The repetition identifier: Indicates repeated measurements in a single
series where any feature values may differ, but the outcome does not.
Repetition identifiers are always implicitly set when multiple entries for
the same series of the same sample in the same batch that share the same
outcome are encountered.
</p>
</li></ul>
</dd>
<dt><code>sample_id_column</code></dt><dd><p>(<strong>recommended</strong>) Name of the column containing
sample or subject identifiers. See <code>batch_id_column</code> above for more
details.
</p>
<p>If unset, every row will be identified as a single sample.</p>
</dd>
<dt><code>series_id_column</code></dt><dd><p>(<strong>optional</strong>) Name of the column containing series
identifiers, which distinguish between measurements that are part of a
series for a single sample. See <code>batch_id_column</code> above for more details.
</p>
<p>If unset, rows which share the same batch and sample identifiers but have a
different outcome are assigned unique series identifiers.</p>
</dd>
<dt><code>development_batch_id</code></dt><dd><p>(<em>optional</em>) One or more batch or cohort
identifiers to constitute data sets for development. Defaults to all, or
all minus the identifiers in <code>validation_batch_id</code> for external validation.
Required if external validation is performed and <code>validation_batch_id</code> is
not provided.</p>
</dd>
<dt><code>validation_batch_id</code></dt><dd><p>(<em>optional</em>) One or more batch or cohort
identifiers to constitute data sets for external validation. Defaults to
all data sets except those in <code>development_batch_id</code> for external
validation, or none if not. Required if <code>development_batch_id</code> is not
provided.</p>
</dd>
<dt><code>outcome_name</code></dt><dd><p>(<em>optional</em>) Name of the modelled outcome. This name will
be used in figures created by <code>familiar</code>.
</p>
<p>If not set, the column name in <code>outcome_column</code> will be used for
<code>binomial</code>, <code>multinomial</code>, <code>count</code> and <code>continuous</code> outcomes. For other
outcomes (<code>survival</code> and <code>competing_risk</code>) no default is used.</p>
</dd>
<dt><code>outcome_column</code></dt><dd><p>(<strong>recommended</strong>) Name of the column containing the
outcome of interest. May be identified from a formula, if a formula is
provided as an argument. Otherwise an error is raised. Note that <code>survival</code>
and <code>competing_risk</code> outcome type outcomes require two columns that
indicate the time-to-event or the time of last follow-up and the event
status.</p>
</dd>
<dt><code>outcome_type</code></dt><dd><p>(<strong>recommended</strong>) Type of outcome found in the outcome
column. The outcome type determines many aspects of the overall process,
e.g. the available feature selection methods and learners, but also the
type of assessments that can be conducted to evaluate the resulting models.
Implemented outcome types are:
</p>

<ul>
<li> <p><code>binomial</code>: categorical outcome with 2 levels.
</p>
</li>
<li> <p><code>multinomial</code>: categorical outcome with 2 or more levels.
</p>
</li>
<li> <p><code>count</code>: Poisson-distributed numeric outcomes.
</p>
</li>
<li> <p><code>continuous</code>: general continuous numeric outcomes.
</p>
</li>
<li> <p><code>survival</code>: survival outcome for time-to-event data.
</p>
</li></ul>

<p>If not provided, the algorithm will attempt to obtain outcome_type from
contents of the outcome column. This may lead to unexpected results, and we
therefore advise to provide this information manually.
</p>
<p>Note that <code>competing_risk</code> survival analysis are not fully supported, and
is currently not a valid choice for <code>outcome_type</code>.</p>
</dd>
<dt><code>class_levels</code></dt><dd><p>(<em>optional</em>) Class levels for <code>binomial</code> or <code>multinomial</code>
outcomes. This argument can be used to specify the ordering of levels for
categorical outcomes. These class levels must exactly match the levels
present in the outcome column.</p>
</dd>
<dt><code>event_indicator</code></dt><dd><p>(<strong>recommended</strong>) Indicator for events in <code>survival</code>
and <code>competing_risk</code> analyses. <code>familiar</code> will automatically recognise <code>1</code>,
<code>true</code>, <code>t</code>, <code>y</code> and <code>yes</code> as event indicators, including different
capitalisations. If this parameter is set, it replaces the default values.</p>
</dd>
<dt><code>censoring_indicator</code></dt><dd><p>(<strong>recommended</strong>) Indicator for right-censoring in
<code>survival</code> and <code>competing_risk</code> analyses. <code>familiar</code> will automatically
recognise <code>0</code>, <code>false</code>, <code>f</code>, <code>n</code>, <code>no</code> as censoring indicators, including
different capitalisations. If this parameter is set, it replaces the
default values.</p>
</dd>
<dt><code>competing_risk_indicator</code></dt><dd><p>(<strong>recommended</strong>) Indicator for competing
risks in <code>competing_risk</code> analyses. There are no default values, and if
unset, all values other than those specified by the <code>event_indicator</code> and
<code>censoring_indicator</code> parameters are considered to indicate competing
risks.</p>
</dd>
<dt><code>signature</code></dt><dd><p>(<em>optional</em>) One or more names of feature columns that are
considered part of a specific signature. Features specified here will
always be used for modelling. Ranking from feature selection has no effect
for these features.</p>
</dd>
<dt><code>novelty_features</code></dt><dd><p>(<em>optional</em>) One or more names of feature columns
that should be included for the purpose of novelty detection.</p>
</dd>
<dt><code>exclude_features</code></dt><dd><p>(<em>optional</em>) Feature columns that will be removed
from the data set. Cannot overlap with features in <code>signature</code>,
<code>novelty_features</code> or <code>include_features</code>.</p>
</dd>
<dt><code>include_features</code></dt><dd><p>(<em>optional</em>) Feature columns that are specifically
included in the data set. By default all features are included. Cannot
overlap with <code>exclude_features</code>, but may overlap <code>signature</code>. Features in
<code>signature</code> and <code>novelty_features</code> are always included. If both
<code>exclude_features</code> and <code>include_features</code> are provided, <code>include_features</code>
takes precedence, provided that there is no overlap between the two.</p>
</dd>
<dt><code>reference_method</code></dt><dd><p>(<em>optional</em>) Method used to set reference levels for
categorical features. There are several options:
</p>

<ul>
<li> <p><code>auto</code> (default): Categorical features that are not explicitly set by the
user, i.e. columns containing boolean values or characters, use the most
frequent level as reference. Categorical features that are explicitly set,
i.e. as factors, are used as is.
</p>
</li>
<li> <p><code>always</code>: Both automatically detected and user-specified categorical
features have the reference level set to the most frequent level. Ordinal
features are not altered, but are used as is.
</p>
</li>
<li> <p><code>never</code>: User-specified categorical features are used as is.
Automatically detected categorical features are simply sorted, and the
first level is then used as the reference level. This was the behaviour
prior to familiar version 1.3.0.
</p>
</li></ul>
</dd>
<dt><code>imbalance_correction_method</code></dt><dd><p>(<em>optional</em>) Type of method used to
address class imbalances. Available options are:
</p>

<ul>
<li> <p><code>full_undersampling</code> (default): All data will be used in an ensemble
fashion. The full minority class will appear in each partition, but
majority classes are undersampled until all data have been used.
</p>
</li>
<li> <p><code>random_undersampling</code>: Randomly undersamples majority classes. This is
useful in cases where full undersampling would lead to the formation of
many models due major overrepresentation of the largest class.
</p>
</li></ul>

<p>This parameter is only used in combination with imbalance partitioning in
the experimental design, and <code>ip</code> should therefore appear in the string
that defines the design.</p>
</dd>
<dt><code>imbalance_n_partitions</code></dt><dd><p>(<em>optional</em>) Number of times random
undersampling should be repeated. 10 undersampled subsets with balanced
classes are formed by default.</p>
</dd>
<dt><code>parallel</code></dt><dd><p>(<em>optional</em>) Enable parallel processing. Defaults to <code>TRUE</code>.
When set to <code>FALSE</code>, this disables all parallel processing, regardless of
specific parameters such as <code>parallel_preprocessing</code>. However, when
<code>parallel</code> is <code>TRUE</code>, parallel processing of different parts of the
workflow can be disabled by setting respective flags to <code>FALSE</code>.</p>
</dd>
<dt><code>parallel_nr_cores</code></dt><dd><p>(<em>optional</em>) Number of cores available for
parallelisation. Defaults to 2. This setting does nothing if
parallelisation is disabled.</p>
</dd>
<dt><code>restart_cluster</code></dt><dd><p>(<em>optional</em>) Restart nodes used for parallel computing
to free up memory prior to starting a parallel process. Note that it does
take time to set up the clusters. Therefore setting this argument to <code>TRUE</code>
may impact processing speed. This argument is ignored if <code>parallel</code> is
<code>FALSE</code> or the cluster was initialised outside of familiar. Default is
<code>FALSE</code>, which causes the clusters to be initialised only once.</p>
</dd>
<dt><code>cluster_type</code></dt><dd><p>(<em>optional</em>) Selection of the cluster type for parallel
processing. Available types are the ones supported by the parallel package
that is part of the base R distribution: <code>psock</code> (default), <code>fork</code>, <code>mpi</code>,
<code>nws</code>, <code>sock</code>. In addition, <code>none</code> is available, which also disables
parallel processing.</p>
</dd>
<dt><code>backend_type</code></dt><dd><p>(<em>optional</em>) Selection of the backend for distributing
copies of the data. This backend ensures that only a single master copy is
kept in memory. This limits memory usage during parallel processing.
</p>
<p>Several backend options are available, notably <code>socket_server</code>, and <code>none</code>
(default). <code>socket_server</code> is based on the callr package and R sockets,
comes with <code>familiar</code> and is available for any OS. <code>none</code> uses the package
environment of familiar to store data, and is available for any OS.
However, <code>none</code> requires copying of data to any parallel process, and has a
larger memory footprint.</p>
</dd>
<dt><code>server_port</code></dt><dd><p>(<em>optional</em>) Integer indicating the port on which the
socket server or RServe process should communicate. Defaults to port 6311.
Note that ports 0 to 1024 and 49152 to 65535 cannot be used.</p>
</dd>
<dt><code>feature_max_fraction_missing</code></dt><dd><p>(<em>optional</em>) Numeric value between <code>0.0</code>
and <code>0.95</code> that determines the meximum fraction of missing values that
still allows a feature to be included in the data set. All features with a
missing value fraction over this threshold are not processed further. The
default value is <code>0.30</code>.</p>
</dd>
<dt><code>sample_max_fraction_missing</code></dt><dd><p>(<em>optional</em>) Numeric value between <code>0.0</code>
and <code>0.95</code> that determines the maximum fraction of missing values that
still allows a sample to be included in the data set. All samples with a
missing value fraction over this threshold are excluded and not processed
further. The default value is <code>0.30</code>.</p>
</dd>
<dt><code>filter_method</code></dt><dd><p>(<em>optional</em>) One or methods used to reduce
dimensionality of the data set by removing irrelevant or poorly
reproducible features.
</p>
<p>Several method are available:
</p>

<ul>
<li> <p><code>none</code> (default): None of the features will be filtered.
</p>
</li>
<li> <p><code>low_variance</code>: Features with a variance below the
<code>low_var_minimum_variance_threshold</code> are filtered. This can be useful to
filter, for example, genes that are not differentially expressed.
</p>
</li>
<li> <p><code>univariate_test</code>: Features undergo a univariate regression using an
outcome-appropriate regression model. The p-value of the model coefficient
is collected. Features with coefficient p or q-value above the
<code>univariate_test_threshold</code> are subsequently filtered.
</p>
</li>
<li> <p><code>robustness</code>: Features that are not sufficiently robust according to the
intraclass correlation coefficient are filtered. Use of this method
requires that repeated measurements are present in the data set, i.e. there
should be entries for which the sample and cohort identifiers are the same.
</p>
</li></ul>

<p>More than one method can be used simultaneously. Features with singular
values are always filtered, as these do not contain information.</p>
</dd>
<dt><code>univariate_test_threshold</code></dt><dd><p>(<em>optional</em>) Numeric value between <code>1.0</code> and
<code>0.0</code> that determines which features are irrelevant and will be filtered by
the <code>univariate_test</code>. The p or q-values are compared to this threshold.
All features with values above the threshold are filtered. The default
value is <code>0.20</code>.</p>
</dd>
<dt><code>univariate_test_threshold_metric</code></dt><dd><p>(<em>optional</em>) Metric used with the to
compare the <code>univariate_test_threshold</code> against. The following metrics can
be chosen:
</p>

<ul>
<li> <p><code>p_value</code> (default): The unadjusted p-value of each feature is used for
to filter features.
</p>
</li>
<li> <p><code>q_value</code>: The q-value (Story, 2002), is used to filter features. Some
data sets may have insufficient samples to compute the q-value. The
<code>qvalue</code> package must be installed from Bioconductor to use this method.
</p>
</li></ul>
</dd>
<dt><code>univariate_test_max_feature_set_size</code></dt><dd><p>(<em>optional</em>) Maximum size of the
feature set after the univariate test. P or q values of features are
compared against the threshold, but if the resulting data set would be
larger than this setting, only the most relevant features up to the desired
feature set size are selected.
</p>
<p>The default value is <code>NULL</code>, which causes features to be filtered based on
their relevance only.</p>
</dd>
<dt><code>low_var_minimum_variance_threshold</code></dt><dd><p>(required, if used) Numeric value
that determines which features will be filtered by the <code>low_variance</code>
method. The variance of each feature is computed and compared to the
threshold. If it is below the threshold, the feature is removed.
</p>
<p>This parameter has no default value and should be set if <code>low_variance</code> is
used.</p>
</dd>
<dt><code>low_var_max_feature_set_size</code></dt><dd><p>(<em>optional</em>) Maximum size of the feature
set after filtering features with a low variance. All features are first
compared against <code>low_var_minimum_variance_threshold</code>. If the resulting
feature set would be larger than specified, only the most strongly varying
features will be selected, up to the desired size of the feature set.
</p>
<p>The default value is <code>NULL</code>, which causes features to be filtered based on
their variance only.</p>
</dd>
<dt><code>robustness_icc_type</code></dt><dd><p>(<em>optional</em>) String indicating the type of
intraclass correlation coefficient (<code>1</code>, <code>2</code> or <code>3</code>) that should be used to
compute robustness for features in repeated measurements. These types
correspond to the types in Shrout and Fleiss (1979). The default value is
<code>1</code>.</p>
</dd>
<dt><code>robustness_threshold_metric</code></dt><dd><p>(<em>optional</em>) String indicating which
specific intraclass correlation coefficient (ICC) metric should be used to
filter features. This should be one of:
</p>

<ul>
<li> <p><code>icc</code>: The estimated ICC value itself.
</p>
</li>
<li> <p><code>icc_low</code> (default): The estimated lower limit of the 95% confidence
interval of the ICC, as suggested by Koo and Li (2016).
</p>
</li>
<li> <p><code>icc_panel</code>: The estimated ICC value over the panel average, i.e. the ICC
that would be obtained if all repeated measurements were averaged.
</p>
</li>
<li> <p><code>icc_panel_low</code>: The estimated lower limit of the 95% confidence interval
of the panel ICC.
</p>
</li></ul>
</dd>
<dt><code>robustness_threshold_value</code></dt><dd><p>(<em>optional</em>) The intraclass correlation
coefficient value that is as threshold. The default value is <code>0.70</code>.</p>
</dd>
<dt><code>transformation_method</code></dt><dd><p>(<em>optional</em>) The transformation method used to
change the distribution of the data to be more normal-like. The following
methods are available:
</p>

<ul>
<li> <p><code>none</code>: This disables transformation of features.
</p>
</li>
<li> <p><code>yeo_johnson</code> (default): Transformation using the Yeo-Johnson
transformation (Yeo and Johnson, 2000). The algorithm tests various lambda
values and selects the lambda that maximises the log-likelihood.
</p>
</li>
<li> <p><code>yeo_johnson_trim</code>: As <code>yeo_johnson</code>, but based on the set of feature
values where the 5% lowest and 5% highest values are discarded. This
reduces the effect of outliers.
</p>
</li>
<li> <p><code>yeo_johnson_winsor</code>: As <code>yeo_johnson</code>, but based on the set of feature
values where the 5% lowest and 5% highest values are winsorised. This
reduces the effect of outliers.
</p>
</li>
<li> <p><code>yeo_johnson_robust</code>: A robust version of <code>yeo_johnson</code> after Raymaekers
and Rousseeuw (2021). This method is less sensitive to outliers.
</p>
</li>
<li> <p><code>box_cox</code>: Transformation using the Box-Cox transformation (Box and Cox,
1964). Unlike the Yeo-Johnson transformation, the Box-Cox transformation
requires that all data are positive. Features that contain zero or negative
values cannot be transformed using this transformation. The algorithm tests
various lambda values and selects the lambda that maximises the
log-likelihood.
</p>
</li>
<li> <p><code>box_cox_trim</code>: As <code>box_cox</code>, but based on the set of feature values
where the 5% lowest and 5% highest values are discarded. This reduces the
effect of outliers.
</p>
</li>
<li> <p><code>box_cox_winsor</code>: As <code>box_cox</code>, but based on the set of feature values
where the 5% lowest and 5% highest values are winsorised. This reduces the
effect of outliers.
</p>
</li>
<li> <p><code>box_cox_robust</code>: A robust verson of <code>box_cox</code> after Raymaekers and
Rousseew (2021). This method is less sensitive to outliers.
</p>
</li></ul>

<p>Only features that contain numerical data are transformed. Transformation
parameters obtained in development data are stored within <code>featureInfo</code>
objects for later use with validation data sets.</p>
</dd>
<dt><code>normalisation_method</code></dt><dd><p>(<em>optional</em>) The normalisation method used to
improve the comparability between numerical features that may have very
different scales. The following normalisation methods can be chosen:
</p>

<ul>
<li> <p><code>none</code>: This disables feature normalisation.
</p>
</li>
<li> <p><code>standardisation</code>: Features are normalised by subtraction of their mean
values and division by their standard deviations. This causes every feature
to be have a center value of 0.0 and standard deviation of 1.0.
</p>
</li>
<li> <p><code>standardisation_trim</code>: As <code>standardisation</code>, but based on the set of
feature values where the 5% lowest and 5% highest values are discarded.
This reduces the effect of outliers.
</p>
</li>
<li> <p><code>standardisation_winsor</code>: As <code>standardisation</code>, but based on the set of
feature values where the 5% lowest and 5% highest values are winsorised.
This reduces the effect of outliers.
</p>
</li>
<li> <p><code>standardisation_robust</code> (default): A robust version of <code>standardisation</code>
that relies on computing Huber's M-estimators for location and scale.
</p>
</li>
<li> <p><code>normalisation</code>: Features are normalised by subtraction of their minimum
values and division by their ranges. This maps all feature values to a
<code class="reqn">[0, 1]</code> interval.
</p>
</li>
<li> <p><code>normalisation_trim</code>: As <code>normalisation</code>, but based on the set of feature
values where the 5% lowest and 5% highest values are discarded. This
reduces the effect of outliers.
</p>
</li>
<li> <p><code>normalisation_winsor</code>: As <code>normalisation</code>, but based on the set of
feature values where the 5% lowest and 5% highest values are winsorised.
This reduces the effect of outliers.
</p>
</li>
<li> <p><code>quantile</code>: Features are normalised by subtraction of their median values
and division by their interquartile range.
</p>
</li>
<li> <p><code>mean_centering</code>: Features are centered by substracting the mean, but do
not undergo rescaling.
</p>
</li></ul>

<p>Only features that contain numerical data are normalised. Normalisation
parameters obtained in development data are stored within <code>featureInfo</code>
objects for later use with validation data sets.</p>
</dd>
<dt><code>batch_normalisation_method</code></dt><dd><p>(<em>optional</em>) The method used for batch
normalisation. Available methods are:
</p>

<ul>
<li> <p><code>none</code> (default): This disables batch normalisation of features.
</p>
</li>
<li> <p><code>standardisation</code>: Features within each batch are normalised by
subtraction of the mean value and division by the standard deviation in
each batch.
</p>
</li>
<li> <p><code>standardisation_trim</code>: As <code>standardisation</code>, but based on the set of
feature values where the 5% lowest and 5% highest values are discarded.
This reduces the effect of outliers.
</p>
</li>
<li> <p><code>standardisation_winsor</code>: As <code>standardisation</code>, but based on the set of
feature values where the 5% lowest and 5% highest values are winsorised.
This reduces the effect of outliers.
</p>
</li>
<li> <p><code>standardisation_robust</code>: A robust version of <code>standardisation</code> that
relies on computing Huber's M-estimators for location and scale within each
batch.
</p>
</li>
<li> <p><code>normalisation</code>: Features within each batch are normalised by subtraction
of their minimum values and division by their range in each batch. This
maps all feature values in each batch to a <code class="reqn">[0, 1]</code> interval.
</p>
</li>
<li> <p><code>normalisation_trim</code>: As <code>normalisation</code>, but based on the set of feature
values where the 5% lowest and 5% highest values are discarded. This
reduces the effect of outliers.
</p>
</li>
<li> <p><code>normalisation_winsor</code>: As <code>normalisation</code>, but based on the set of
feature values where the 5% lowest and 5% highest values are winsorised.
This reduces the effect of outliers.
</p>
</li>
<li> <p><code>quantile</code>: Features in each batch are normalised by subtraction of the
median value and division by the interquartile range of each batch.
</p>
</li>
<li> <p><code>mean_centering</code>: Features in each batch are centered on 0.0 by
substracting the mean value in each batch, but are not rescaled.
</p>
</li>
<li> <p><code>combat_parametric</code>: Batch adjustments using parametric empirical Bayes
(Johnson et al, 2007). <code>combat_p</code> leads to the same method.
</p>
</li>
<li> <p><code>combat_non_parametric</code>: Batch adjustments using non-parametric empirical
Bayes (Johnson et al, 2007). <code>combat_np</code> and <code>combat</code> lead to the same
method. Note that we reduced complexity from O(<code class="reqn">n^2</code>) to O(<code class="reqn">n</code>) by
only computing batch adjustment parameters for each feature on a subset of
50 randomly selected features, instead of all features.
</p>
</li></ul>

<p>Only features that contain numerical data are normalised using batch
normalisation. Batch normalisation parameters obtained in development data
are stored within <code>featureInfo</code> objects for later use with validation data
sets, in case the validation data is from the same batch.
</p>
<p>If validation data contains data from unknown batches, normalisation
parameters are separately determined for these batches.
</p>
<p>Note that for both empirical Bayes methods, the batch effect is assumed to
produce results across the features. This is often true for things such as
gene expressions, but the assumption may not hold generally.
</p>
<p>When performing batch normalisation, it is moreover important to check that
differences between batches or cohorts are not related to the studied
endpoint.</p>
</dd>
<dt><code>imputation_method</code></dt><dd><p>(<em>optional</em>) Method used for imputing missing
feature values. Two methods are implemented:
</p>

<ul>
<li> <p><code>simple</code>: Simple replacement of a missing value by the median value (for
numeric features) or the modal value (for categorical features).
</p>
</li>
<li> <p><code>lasso</code>: Imputation of missing value by lasso regression (using <code>glmnet</code>)
based on information contained in other features.
</p>
</li></ul>

<p><code>simple</code> imputation precedes <code>lasso</code> imputation to ensure that any missing
values in predictors required for <code>lasso</code> regression are resolved. The
<code>lasso</code> estimate is then used to replace the missing value.
</p>
<p>The default value depends on the number of features in the dataset. If the
number is lower than 100, <code>lasso</code> is used by default, and <code>simple</code>
otherwise.
</p>
<p>Only single imputation is performed. Imputation models and parameters are
stored within <code>featureInfo</code> objects for later use with validation data
sets.</p>
</dd>
<dt><code>cluster_method</code></dt><dd><p>(<em>optional</em>) Clustering is performed to identify and
replace redundant features, for example those that are highly correlated.
Such features do not carry much additional information and may be removed
or replaced instead (Park et al., 2007; Tolosi and Lengauer, 2011).
</p>
<p>The cluster method determines the algorithm used to form the clusters. The
following cluster methods are implemented:
</p>

<ul>
<li> <p><code>none</code>: No clustering is performed.
</p>
</li>
<li> <p><code>hclust</code> (default): Hierarchical agglomerative clustering. If the
<code>fastcluster</code> package is installed, <code>fastcluster::hclust</code> is used (Muellner
2013), otherwise <code>stats::hclust</code> is used.
</p>
</li>
<li> <p><code>agnes</code>: Hierarchical clustering using agglomerative nesting (Kaufman and
Rousseeuw, 1990). This algorithm is similar to <code>hclust</code>, but uses the
<code>cluster::agnes</code> implementation.
</p>
</li>
<li> <p><code>diana</code>: Divisive analysis hierarchical clustering. This method uses
divisive instead of agglomerative clustering (Kaufman and Rousseeuw, 1990).
<code>cluster::diana</code> is used.
</p>
</li>
<li> <p><code>pam</code>: Partioning around medioids. This partitions the data into $k$
clusters around medioids (Kaufman and Rousseeuw, 1990). $k$ is selected
using the <code>silhouette</code> metric. <code>pam</code> is implemented using the
<code>cluster::pam</code> function.
</p>
</li></ul>

<p>Clusters and cluster information is stored within <code>featureInfo</code> objects for
later use with validation data sets. This enables reproduction of the same
clusters as formed in the development data set.</p>
</dd>
<dt><code>cluster_linkage_method</code></dt><dd><p>(<em>optional</em>) Linkage method used for
agglomerative clustering in <code>hclust</code> and <code>agnes</code>. The following linkage
methods can be used:
</p>

<ul>
<li> <p><code>average</code> (default): Average linkage.
</p>
</li>
<li> <p><code>single</code>: Single linkage.
</p>
</li>
<li> <p><code>complete</code>: Complete linkage.
</p>
</li>
<li> <p><code>weighted</code>: Weighted linkage, also known as McQuitty linkage.
</p>
</li>
<li> <p><code>ward</code>: Linkage using Ward's minimum variance method.
</p>
</li></ul>

<p><code>diana</code> and <code>pam</code> do not require a linkage method.</p>
</dd>
<dt><code>cluster_cut_method</code></dt><dd><p>(<em>optional</em>) The method used to define the actual
clusters. The following methods can be used:
</p>

<ul>
<li> <p><code>silhouette</code>: Clusters are formed based on the silhouette score
(Rousseeuw, 1987). The average silhouette score is computed from 2 to
<code class="reqn">n</code> clusters, with <code class="reqn">n</code> the number of features. Clusters are only
formed if the average silhouette exceeds 0.50, which indicates reasonable
evidence for structure. This procedure may be slow if the number of
features is large (&gt;100s).
</p>
</li>
<li> <p><code>fixed_cut</code>: Clusters are formed by cutting the hierarchical tree at the
point indicated by the <code>cluster_similarity_threshold</code>, e.g. where features
in a cluster have an average Spearman correlation of 0.90. <code>fixed_cut</code> is
only available for <code>agnes</code>, <code>diana</code> and <code>hclust</code>.
</p>
</li>
<li> <p><code>dynamic_cut</code>: Dynamic cluster formation using the cutting algorithm in
the <code>dynamicTreeCut</code> package. This package should be installed to select
this option. <code>dynamic_cut</code> can only be used with <code>agnes</code> and <code>hclust</code>.
</p>
</li></ul>

<p>The default options are <code>silhouette</code> for partioning around medioids (<code>pam</code>)
and <code>fixed_cut</code> otherwise.</p>
</dd>
<dt><code>cluster_similarity_metric</code></dt><dd><p>(<em>optional</em>) Clusters are formed based on
feature similarity. All features are compared in a pair-wise fashion to
compute similarity, for example correlation. The resulting similarity grid
is converted into a distance matrix that is subsequently used for
clustering. The following metrics are supported to compute pairwise
similarities:
</p>

<ul>
<li> <p><code>mutual_information</code> (default): normalised mutual information.
</p>
</li>
<li> <p><code>mcfadden_r2</code>: McFadden's pseudo R-squared (McFadden, 1974).
</p>
</li>
<li> <p><code>cox_snell_r2</code>: Cox and Snell's pseudo R-squared (Cox and Snell, 1989).
</p>
</li>
<li> <p><code>nagelkerke_r2</code>: Nagelkerke's pseudo R-squared (Nagelkerke, 1991).
</p>
</li>
<li> <p><code>spearman</code>: Spearman's rank order correlation.
</p>
</li>
<li> <p><code>kendall</code>: Kendall rank correlation.
</p>
</li>
<li> <p><code>pearson</code>: Pearson product-moment correlation.
</p>
</li></ul>

<p>The pseudo R-squared metrics can be used to assess similarity between mixed
pairs of numeric and categorical features, as these are based on the
log-likelihood of regression models. In <code>familiar</code>, the more informative
feature is used as the predictor and the other feature as the reponse
variable. In numeric-categorical pairs, the numeric feature is considered
to be more informative and is thus used as the predictor. In
categorical-categorical pairs, the feature with most levels is used as the
predictor.
</p>
<p>In case any of the classical correlation coefficients (<code>pearson</code>,
<code>spearman</code> and <code>kendall</code>) are used with (mixed) categorical features, the
categorical features are one-hot encoded and the mean correlation over all
resulting pairs is used as similarity.</p>
</dd>
<dt><code>cluster_similarity_threshold</code></dt><dd><p>(<em>optional</em>) The threshold level for
pair-wise similarity that is required to form clusters using <code>fixed_cut</code>.
This should be a numerical value between 0.0 and 1.0. Note however, that a
reasonable threshold value depends strongly on the similarity metric. The
following are the default values used:
</p>

<ul>
<li> <p><code>mcfadden_r2</code> and <code>mutual_information</code>: <code>0.30</code>
</p>
</li>
<li> <p><code>cox_snell_r2</code> and <code>nagelkerke_r2</code>: <code>0.75</code>
</p>
</li>
<li> <p><code>spearman</code>, <code>kendall</code> and <code>pearson</code>: <code>0.90</code>
</p>
</li></ul>

<p>Alternatively, if the <code style="white-space: pre;">&#8288;fixed cut&#8288;</code> method is not used, this value determines
whether any clustering should be performed, because the data may not
contain highly similar features. The default values in this situation are:
</p>

<ul>
<li> <p><code>mcfadden_r2</code>  and <code>mutual_information</code>: <code>0.25</code>
</p>
</li>
<li> <p><code>cox_snell_r2</code> and <code>nagelkerke_r2</code>: <code>0.40</code>
</p>
</li>
<li> <p><code>spearman</code>, <code>kendall</code> and <code>pearson</code>: <code>0.70</code>
</p>
</li></ul>

<p>The threshold value is converted to a distance (1-similarity) prior to
cutting hierarchical trees.</p>
</dd>
<dt><code>cluster_representation_method</code></dt><dd><p>(<em>optional</em>) Method used to determine
how the information of co-clustered features is summarised and used to
represent the cluster. The following methods can be selected:
</p>

<ul>
<li> <p><code>best_predictor</code> (default): The feature with the highest importance
according to univariate regression with the outcome is used to represent
the cluster.
</p>
</li>
<li> <p><code>medioid</code>: The feature closest to the cluster center, i.e. the feature
that is most similar to the remaining features in the cluster, is used to
represent the feature.
</p>
</li>
<li> <p><code>mean</code>: A meta-feature is generated by averaging the feature values for
all features in a cluster. This method aligns all features so that all
features will be positively correlated prior to averaging. Should a cluster
contain one or more categorical features, the <code>medioid</code> method will be used
instead, as averaging is not possible. Note that if this method is chosen,
the <code>normalisation_method</code> parameter should be one of <code>standardisation</code>,
<code>standardisation_trim</code>, <code>standardisation_winsor</code> or <code>quantile</code>.'
</p>
</li></ul>

<p>If the <code>pam</code> cluster method is selected, only the <code>medioid</code> method can be
used. In that case 1 medioid is used by default.</p>
</dd>
<dt><code>parallel_preprocessing</code></dt><dd><p>(<em>optional</em>) Enable parallel processing for the
preprocessing workflow. Defaults to <code>TRUE</code>. When set to <code>FALSE</code>, this will
disable the use of parallel processing while preprocessing, regardless of
the settings of the <code>parallel</code> parameter. <code>parallel_preprocessing</code> is
ignored if <code>parallel=FALSE</code>.</p>
</dd>
<dt><code>parallel_feature_selection</code></dt><dd><p>(<em>optional</em>) Enable parallel processing for
the feature selection workflow. Defaults to <code>TRUE</code>. When set to <code>FALSE</code>,
this will disable the use of parallel processing while performing feature
selection, regardless of the settings of the <code>parallel</code> parameter.
<code>parallel_feature_selection</code> is ignored if <code>parallel=FALSE</code>.</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a thin wrapper around <code>summon_familiar</code>, and functions like
it, but automatically skips learning and subsequent evaluation steps.
</p>
<p>The function returns an <code>experimentData</code> object, which can be used to
warm-start other experiments by providing it to the <code>experiment_data</code>
argument. Variable importance may be retrieved from this object using the
<code>get_vimp_table</code> and <code>aggregate_vimp_table</code> methods.
</p>


<h3>Value</h3>

<p>An <code>experimentData</code> object.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+get_vimp_table">get_vimp_table</a></code>, <code><a href="#topic+aggregate_vimp_table">aggregate_vimp_table</a></code>
</p>

<hr>
<h2 id='predict'>Model predictions for familiar models and model ensembles</h2><span id='topic+predict'></span><span id='topic+predict+2CfamiliarModel-method'></span><span id='topic+predict+2CfamiliarEnsemble-method'></span><span id='topic+predict+2CfamiliarNoveltyDetector-method'></span><span id='topic+predict+2Clist-method'></span><span id='topic+predict+2Ccharacter-method'></span>

<h3>Description</h3>

<p>Fits the model or ensemble of models to the data and shows the
result.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>predict(object, ...)

## S4 method for signature 'familiarModel'
predict(
  object,
  newdata,
  type = "default",
  time = NULL,
  dir_path = NULL,
  ensemble_method = "median",
  stratification_threshold = NULL,
  stratification_method = NULL,
  percentiles = NULL,
  ...
)

## S4 method for signature 'familiarEnsemble'
predict(
  object,
  newdata,
  type = "default",
  time = NULL,
  dir_path = NULL,
  ensemble_method = "median",
  stratification_threshold = NULL,
  stratification_method = NULL,
  percentiles = NULL,
  ...
)

## S4 method for signature 'familiarNoveltyDetector'
predict(object, newdata, type = "novelty", ...)

## S4 method for signature 'list'
predict(
  object,
  newdata,
  type = "default",
  time = NULL,
  dir_path = NULL,
  ensemble_method = "median",
  stratification_threshold = NULL,
  stratification_method = NULL,
  percentiles = NULL,
  ...
)

## S4 method for signature 'character'
predict(
  object,
  newdata,
  type = "default",
  time = NULL,
  dir_path = NULL,
  ensemble_method = "median",
  stratification_threshold = NULL,
  stratification_method = NULL,
  percentiles = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict_+3A_object">object</code></td>
<td>
<p>A familiar model or ensemble of models that should be used for
prediction. This can also be a path to the ensemble model, one or more paths
to models, or a list of models.</p>
</td></tr>
<tr><td><code id="predict_+3A_...">...</code></td>
<td>
<p>to be documented.</p>
</td></tr>
<tr><td><code id="predict_+3A_newdata">newdata</code></td>
<td>
<p>Data to which the models are fitted. <code>familiar</code> performs checks
on the data to ensure that all features required for fitting the model are
present, and no additional levels are present in categorical features.
Unlike other <code>predict</code> methods, <code>newdata</code> cannot be missing in <code>familiar</code>,
as training data are not stored with the models.</p>
</td></tr>
<tr><td><code id="predict_+3A_type">type</code></td>
<td>
<p>Type of prediction made. The following values are directly
supported:
</p>

<ul>
<li> <p><code>default</code>: Default prediction, i.e. value estimates for <code>count</code> and
<code>continuous</code> outcomes, predicted class probabilities and class for
<code>binomial</code> and <code>multinomial</code> and the model response for <code>survival</code> outcomes.
</p>
</li>
<li> <p><code>survival_probability</code>: Predicts survival probabilities at the time
specified by <code>time</code>. Only applicable to <code>survival</code> outcomes. Some models may
not allow for predicting survival probabilities based on their response.
</p>
</li>
<li> <p><code>novelty</code>: Predicts novelty of each sample, which can be used for
out-of-distribution detection.
</p>
</li>
<li> <p><code>risk_stratification</code>: Predicts the strata to which the data belongs. Only
for <code>survival</code> outcomes.
</p>
</li></ul>

<p>Other values for type are passed to the fitting method of the actual
underlying model. For example for generalised linear models (<code>glm</code>) <code>type</code>
can be <code>link</code>, <code>response</code> or <code>terms</code> as well. Some of these model-specific
prediction types may fail to return results if the model has been trimmed.</p>
</td></tr>
<tr><td><code id="predict_+3A_time">time</code></td>
<td>
<p>Time at which the response (<code>default</code>) or survival probability
(<code>survival_probability</code>) should be predicted for <code>survival</code> outcomes. Some
models have a response that does not depend on <code>time</code>, e.g. <code>cox</code>, whereas
others do, e.g. <code>random_forest</code>.</p>
</td></tr>
<tr><td><code id="predict_+3A_dir_path">dir_path</code></td>
<td>
<p>Path to the folder containing the models. Ensemble objects are
stored with the models detached. In case the models were moved since
creation, <code>dir_path</code> can be used to specify the current folder.
Alternatively the <code>update_model_dir_path</code> method can be used to update the
path.</p>
</td></tr>
<tr><td><code id="predict_+3A_ensemble_method">ensemble_method</code></td>
<td>
<p>Method for ensembling predictions from models for the
same sample. Available methods are:
</p>

<ul>
<li> <p><code>median</code> (default): Use the median of the predicted values as the ensemble
value for a sample.
</p>
</li>
<li> <p><code>mean</code>: Use the mean of the predicted values as the ensemble value for a
sample.
</p>
</li></ul>
</td></tr>
<tr><td><code id="predict_+3A_stratification_threshold">stratification_threshold</code></td>
<td>
<p>Threshold value(s) used for stratifying
instances into risk groups. If this parameter is specified,
<code>stratification_method</code> and any threshold values that come with the model
are ignored, and <code>stratification_threshold</code> is used instead.</p>
</td></tr>
<tr><td><code id="predict_+3A_stratification_method">stratification_method</code></td>
<td>
<p>Selects the stratification method from which the
threshold values should be selected. If the model or ensemble of models does
not contain thresholds for the indicated method, an error is returned. In
addition this argument is ignored if a <code>stratification_threshold</code> is set.</p>
</td></tr>
<tr><td><code id="predict_+3A_percentiles">percentiles</code></td>
<td>
<p>Currently unused.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This method is used to predict values for instances specified by the
<code>newdata</code> using the model or ensemble of models specified by the <code>object</code>
argument.
</p>


<h3>Value</h3>

<p>A <code>data.table</code> with predicted values.
</p>

<hr>
<h2 id='set_class_names+2CfamiliarCollection-method'>Rename outcome classes for plotting and export</h2><span id='topic+set_class_names+2CfamiliarCollection-method'></span><span id='topic+set_class_names'></span>

<h3>Description</h3>

<p>Tabular exports and figures created from a familiarCollection
object can be customised by providing names for outcome classes.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'familiarCollection'
set_class_names(x, old = NULL, new = NULL, order = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="set_class_names+2B2CfamiliarCollection-method_+3A_x">x</code></td>
<td>
<p>A familiarCollection object.</p>
</td></tr>
<tr><td><code id="set_class_names+2B2CfamiliarCollection-method_+3A_old">old</code></td>
<td>
<p>(optional) Set of old labels to replace.</p>
</td></tr>
<tr><td><code id="set_class_names+2B2CfamiliarCollection-method_+3A_new">new</code></td>
<td>
<p>Set of replacement labels. The number of replacement labels should
be equal to the number of provided old labels or the full number of labels.
If a subset of labels is to be replaced, both <code>old</code> and <code>new</code>
should be provided.</p>
</td></tr>
<tr><td><code id="set_class_names+2B2CfamiliarCollection-method_+3A_order">order</code></td>
<td>
<p>(optional) Ordered set of replacement labels. This is used to
provide the order in which the labels should be placed, which affects e.g.
levels in a plot. If the ordering is not explicitly provided, the old
ordering is used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Labels convert the internal naming for class levels to the requested
label at export or when plotting. This enables customisation of class
names. Currently assigned labels can be found using the
<code>get_class_names</code> method.
</p>


<h3>Value</h3>

<p>A familiarCollection object with updated labels.
</p>


<h3>See Also</h3>


<ul>
<li> <p><a href="#topic+familiarCollection-class">familiarCollection</a> for information concerning the
familiarCollection class. * <code><a href="#topic+get_class_names">get_class_names</a></code> for obtaining
currently assigned class names.
</p>
</li></ul>


<hr>
<h2 id='set_data_set_names+2CfamiliarCollection-method'>Name datasets for plotting and export</h2><span id='topic+set_data_set_names+2CfamiliarCollection-method'></span><span id='topic+set_data_set_names'></span>

<h3>Description</h3>

<p>Tabular exports and figures created from a familiarCollection
object can be customised by setting data labels.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'familiarCollection'
set_data_set_names(x, old = NULL, new = NULL, order = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="set_data_set_names+2B2CfamiliarCollection-method_+3A_x">x</code></td>
<td>
<p>A familiarCollection object.</p>
</td></tr>
<tr><td><code id="set_data_set_names+2B2CfamiliarCollection-method_+3A_old">old</code></td>
<td>
<p>(optional) Set of old labels to replace.</p>
</td></tr>
<tr><td><code id="set_data_set_names+2B2CfamiliarCollection-method_+3A_new">new</code></td>
<td>
<p>Set of replacement labels. The number of replacement labels should
be equal to the number of provided old labels or the full number of labels.
If a subset of labels is to be replaced, both <code>old</code> and <code>new</code>
should be provided.</p>
</td></tr>
<tr><td><code id="set_data_set_names+2B2CfamiliarCollection-method_+3A_order">order</code></td>
<td>
<p>(optional) Ordered set of replacement labels. This is used to
provide the order in which the labels should be placed, which affects e.g.
levels in a plot. If the ordering is not explicitly provided, the old
ordering is used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Labels convert internal naming of data sets to the requested label
at export or when plotting. Currently assigned labels can be found using
the <code>get_data_set_names</code> method.
</p>


<h3>Value</h3>

<p>A familiarCollection object with custom names for the data sets.
</p>


<h3>See Also</h3>


<ul>
<li> <p><a href="#topic+familiarCollection-class">familiarCollection</a> for information concerning the
familiarCollection class. * <code><a href="#topic+get_data_set_names">get_data_set_names</a></code> for obtaining
currently assigned labels.
</p>
</li></ul>


<hr>
<h2 id='set_feature_names+2CfamiliarCollection-method'>Rename features for plotting and export</h2><span id='topic+set_feature_names+2CfamiliarCollection-method'></span><span id='topic+set_feature_names'></span>

<h3>Description</h3>

<p>Tabular exports and figures created from a familiarCollection
object can be customised by providing names for features.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'familiarCollection'
set_feature_names(x, old = NULL, new = NULL, order = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="set_feature_names+2B2CfamiliarCollection-method_+3A_x">x</code></td>
<td>
<p>A familiarCollection object.</p>
</td></tr>
<tr><td><code id="set_feature_names+2B2CfamiliarCollection-method_+3A_old">old</code></td>
<td>
<p>(optional) Set of old labels to replace.</p>
</td></tr>
<tr><td><code id="set_feature_names+2B2CfamiliarCollection-method_+3A_new">new</code></td>
<td>
<p>Set of replacement labels. The number of replacement labels should
be equal to the number of provided old labels or the full number of labels.
If a subset of labels is to be replaced, both <code>old</code> and <code>new</code>
should be provided.</p>
</td></tr>
<tr><td><code id="set_feature_names+2B2CfamiliarCollection-method_+3A_order">order</code></td>
<td>
<p>(optional) Ordered set of replacement labels. This is used to
provide the order in which the labels should be placed, which affects e.g.
levels in a plot. If the ordering is not explicitly provided, the old
ordering is used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Labels convert the internal naming for features to the requested
label at export or when plotting. This enables customisation without
redoing the analysis with renamed input data. Currently assigned labels can
be found using the <code>get_feature_names</code> method.
</p>


<h3>Value</h3>

<p>A familiarCollection object with updated labels.
</p>


<h3>See Also</h3>


<ul>
<li> <p><a href="#topic+familiarCollection-class">familiarCollection</a> for information concerning the
familiarCollection class. * <code><a href="#topic+get_feature_names">get_feature_names</a></code> for obtaining
currently assigned feature names.
</p>
</li></ul>


<hr>
<h2 id='set_fs_method_names+2CfamiliarCollection-method'>Rename feature selection methods for plotting and export</h2><span id='topic+set_fs_method_names+2CfamiliarCollection-method'></span><span id='topic+set_fs_method_names'></span>

<h3>Description</h3>

<p>Tabular exports and figures created from a familiarCollection
object can be customised by providing names for the feature selection
methods.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'familiarCollection'
set_fs_method_names(x, old = NULL, new = NULL, order = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="set_fs_method_names+2B2CfamiliarCollection-method_+3A_x">x</code></td>
<td>
<p>A familiarCollection object.</p>
</td></tr>
<tr><td><code id="set_fs_method_names+2B2CfamiliarCollection-method_+3A_old">old</code></td>
<td>
<p>(optional) Set of old labels to replace.</p>
</td></tr>
<tr><td><code id="set_fs_method_names+2B2CfamiliarCollection-method_+3A_new">new</code></td>
<td>
<p>Set of replacement labels. The number of replacement labels should
be equal to the number of provided old labels or the full number of labels.
If a subset of labels is to be replaced, both <code>old</code> and <code>new</code>
should be provided.</p>
</td></tr>
<tr><td><code id="set_fs_method_names+2B2CfamiliarCollection-method_+3A_order">order</code></td>
<td>
<p>(optional) Ordered set of replacement labels. This is used to
provide the order in which the labels should be placed, which affects e.g.
levels in a plot. If the ordering is not explicitly provided, the old
ordering is used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Labels convert the internal naming for feature selection methods to
the requested label at export or when plotting. This enables the use of
more specific naming, e.g. changing <code>mim</code> to <code>Mutual Information
  Maximisation</code>. Currently assigned labels can be found using the
<code>get_fs_method_names</code> method.
</p>


<h3>Value</h3>

<p>A familiarCollection object with updated labels.
</p>


<h3>See Also</h3>


<ul>
<li> <p><a href="#topic+familiarCollection-class">familiarCollection</a> for information concerning the
familiarCollection class. * <code><a href="#topic+get_fs_method_names">get_fs_method_names</a></code> for obtaining
currently assigned labels.
</p>
</li></ul>


<hr>
<h2 id='set_learner_names+2CfamiliarCollection-method'>Rename learners for plotting and export</h2><span id='topic+set_learner_names+2CfamiliarCollection-method'></span><span id='topic+set_learner_names'></span>

<h3>Description</h3>

<p>Tabular exports and figures created from a familiarCollection
object can be customised by providing names for the learners.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'familiarCollection'
set_learner_names(x, old = NULL, new = NULL, order = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="set_learner_names+2B2CfamiliarCollection-method_+3A_x">x</code></td>
<td>
<p>A familiarCollection object.</p>
</td></tr>
<tr><td><code id="set_learner_names+2B2CfamiliarCollection-method_+3A_old">old</code></td>
<td>
<p>(optional) Set of old labels to replace.</p>
</td></tr>
<tr><td><code id="set_learner_names+2B2CfamiliarCollection-method_+3A_new">new</code></td>
<td>
<p>Set of replacement labels. The number of replacement labels should
be equal to the number of provided old labels or the full number of labels.
If a subset of labels is to be replaced, both <code>old</code> and <code>new</code>
should be provided.</p>
</td></tr>
<tr><td><code id="set_learner_names+2B2CfamiliarCollection-method_+3A_order">order</code></td>
<td>
<p>(optional) Ordered set of replacement labels. This is used to
provide the order in which the labels should be placed, which affects e.g.
levels in a plot. If the ordering is not explicitly provided, the old
ordering is used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Labels convert the internal naming for learners to the requested
label at export or when plotting. This enables the use of more specific
naming, e.g. changing <code>random_forest_rfsrc</code> to <code>Random Forest</code>.
Currently assigned labels can be found using the <code>get_learner_names</code>
method.
</p>


<h3>Value</h3>

<p>A familiarCollection object with custom labels for the learners.
</p>


<h3>See Also</h3>


<ul>
<li> <p><a href="#topic+familiarCollection-class">familiarCollection</a> for information concerning the
familiarCollection class. * <code><a href="#topic+get_learner_names">get_learner_names</a></code> for obtaining
currently assigned labels.
</p>
</li></ul>


<hr>
<h2 id='set_object_name+2CfamiliarData-method'>Set the name of a <code>familiarData</code> object.</h2><span id='topic+set_object_name+2CfamiliarData-method'></span>

<h3>Description</h3>

<p>Set the <code>name</code> slot using the object name.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'familiarData'
set_object_name(x, new = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="set_object_name+2B2CfamiliarData-method_+3A_x">x</code></td>
<td>
<p>A <code>familiarData</code> object.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>familiarData</code> object with a generated or a provided name.
</p>

<hr>
<h2 id='set_object_name+2CfamiliarEnsemble-method'>Set the name of a <code>familiarEnsemble</code> object.</h2><span id='topic+set_object_name+2CfamiliarEnsemble-method'></span>

<h3>Description</h3>

<p>Set the <code>name</code> slot using the object name.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'familiarEnsemble'
set_object_name(x, new = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="set_object_name+2B2CfamiliarEnsemble-method_+3A_x">x</code></td>
<td>
<p>A <code>familiarEnsemble</code> object.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>familiarEnsemble</code> object with a generated or a provided name.
</p>

<hr>
<h2 id='set_object_name+2CfamiliarModel-method'>Set the name of a <code>familiarModel</code> object.</h2><span id='topic+set_object_name+2CfamiliarModel-method'></span>

<h3>Description</h3>

<p>Set the <code>name</code> slot using the object name.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'familiarModel'
set_object_name(x, new = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="set_object_name+2B2CfamiliarModel-method_+3A_x">x</code></td>
<td>
<p>A <code>familiarModel</code> object.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code>familiarModel</code> object with a generated or a provided name.
</p>

<hr>
<h2 id='set_risk_group_names+2CfamiliarCollection-method'>Rename risk groups for plotting and export</h2><span id='topic+set_risk_group_names+2CfamiliarCollection-method'></span><span id='topic+set_risk_group_names'></span>

<h3>Description</h3>

<p>Tabular exports and figures created from a familiarCollection
object can be customised by providing names for risk groups in survival
analysis.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S4 method for signature 'familiarCollection'
set_risk_group_names(x, old = NULL, new = NULL, order = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="set_risk_group_names+2B2CfamiliarCollection-method_+3A_x">x</code></td>
<td>
<p>A familiarCollection object.</p>
</td></tr>
<tr><td><code id="set_risk_group_names+2B2CfamiliarCollection-method_+3A_old">old</code></td>
<td>
<p>(optional) Set of old labels to replace.</p>
</td></tr>
<tr><td><code id="set_risk_group_names+2B2CfamiliarCollection-method_+3A_new">new</code></td>
<td>
<p>Set of replacement labels. The number of replacement labels should
be equal to the number of provided old labels or the full number of labels.
If a subset of labels is to be replaced, both <code>old</code> and <code>new</code>
should be provided.</p>
</td></tr>
<tr><td><code id="set_risk_group_names+2B2CfamiliarCollection-method_+3A_order">order</code></td>
<td>
<p>(optional) Ordered set of replacement labels. This is used to
provide the order in which the labels should be placed, which affects e.g.
levels in a plot. If the ordering is not explicitly provided, the old
ordering is used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Labels convert the internal naming for risk groups to the requested
label at export or when plotting. This enables customisation of risk group
names. Currently assigned labels can be found using the
<code>get_risk_group_names</code> method.
</p>


<h3>Value</h3>

<p>A familiarCollection object with updated labels.
</p>


<h3>See Also</h3>


<ul>
<li> <p><a href="#topic+familiarCollection-class">familiarCollection</a> for information concerning the
familiarCollection class. * <code><a href="#topic+get_risk_group_names">get_risk_group_names</a></code> for obtaining
currently assigned risk group labels.
</p>
</li></ul>


<hr>
<h2 id='summary'>Model summaries</h2><span id='topic+summary'></span><span id='topic+summary+2CfamiliarModel-method'></span>

<h3>Description</h3>

<p><code>summary</code> produces model summaries.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>summary(object, ...)

## S4 method for signature 'familiarModel'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary_+3A_object">object</code></td>
<td>
<p>a familiarModel object</p>
</td></tr>
<tr><td><code id="summary_+3A_...">...</code></td>
<td>
<p>additional arguments passed to <code>summary</code> methods for the underlying
model, when available.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This method extends the <code>summary</code> S3 method. For some models
<code>summary</code> requires information that is trimmed from the model. In this case
a copy of summary data is stored with the model, and returned.
</p>


<h3>Value</h3>

<p>Depends on underlying model. See the documentation for the particular
models.
</p>

<hr>
<h2 id='summon_familiar'>Perform end-to-end machine learning and data analysis</h2><span id='topic+summon_familiar'></span>

<h3>Description</h3>

<p>Perform end-to-end machine learning and data analysis
</p>


<h3>Usage</h3>

<pre><code class='language-R'>summon_familiar(
  formula = NULL,
  data = NULL,
  experiment_data = NULL,
  cl = NULL,
  config = NULL,
  config_id = 1,
  verbose = TRUE,
  .stop_after = "evaluation",
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summon_familiar_+3A_formula">formula</code></td>
<td>
<p>An R formula. The formula can only contain feature names and
dot (<code>.</code>). The <code>*</code> and <code>+1</code> operators are not supported as these refer to
columns that are not present in the data set.
</p>
<p>Use of the formula interface is optional.</p>
</td></tr>
<tr><td><code id="summon_familiar_+3A_data">data</code></td>
<td>
<p>A <code>data.table</code> object, a <code>data.frame</code> object, list containing
multiple <code>data.table</code> or <code>data.frame</code> objects, or paths to data files.
</p>
<p><code>data</code> should be provided if no file paths are provided to the <code>data_files</code>
argument. If both are provided, only <code>data</code> will be used.
</p>
<p>All data is expected to be in wide format, and ideally has a sample
identifier (see <code>sample_id_column</code>), batch identifier (see <code>cohort_column</code>)
and outcome columns (see <code>outcome_column</code>).
</p>
<p>In case paths are provided, the data should be stored as <code>csv</code>, <code>rds</code> or
<code>RData</code> files. See documentation for the <code>data_files</code> argument for more
information.</p>
</td></tr>
<tr><td><code id="summon_familiar_+3A_experiment_data">experiment_data</code></td>
<td>
<p>Experimental data may provided in the form of</p>
</td></tr>
<tr><td><code id="summon_familiar_+3A_cl">cl</code></td>
<td>
<p>Cluster created using the <code>parallel</code> package. This cluster is then
used to speed up computation through parallelisation. When a cluster is not
provided, parallelisation is performed by setting up a cluster on the local
machine.
</p>
<p>This parameter has no effect if the <code>parallel</code> argument is set to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="summon_familiar_+3A_config">config</code></td>
<td>
<p>List containing configuration parameters, or path to an <code>xml</code>
file containing these parameters. An empty configuration file can obtained
using the <code>get_xml_config</code> function.
</p>
<p>All parameters can also be set programmatically. These supersede any
arguments derived from the configuration list.</p>
</td></tr>
<tr><td><code id="summon_familiar_+3A_config_id">config_id</code></td>
<td>
<p>Identifier for the configuration in case the list or <code>xml</code>
table indicated by <code>config</code> contains more than one set of configurations.</p>
</td></tr>
<tr><td><code id="summon_familiar_+3A_verbose">verbose</code></td>
<td>
<p>Indicates verbosity of the results. Default is TRUE, and all
messages and warnings are returned.</p>
</td></tr>
<tr><td><code id="summon_familiar_+3A_.stop_after">.stop_after</code></td>
<td>
<p>Variable for internal use.</p>
</td></tr>
<tr><td><code id="summon_familiar_+3A_...">...</code></td>
<td>

<p>Arguments passed on to <code><a href="#topic+.parse_file_paths">.parse_file_paths</a></code>, <code><a href="#topic+.parse_experiment_settings">.parse_experiment_settings</a></code>, <code><a href="#topic+.parse_setup_settings">.parse_setup_settings</a></code>, <code><a href="#topic+.parse_preprocessing_settings">.parse_preprocessing_settings</a></code>, <code><a href="#topic+.parse_feature_selection_settings">.parse_feature_selection_settings</a></code>, <code><a href="#topic+.parse_model_development_settings">.parse_model_development_settings</a></code>, <code><a href="#topic+.parse_hyperparameter_optimisation_settings">.parse_hyperparameter_optimisation_settings</a></code>, <code><a href="#topic+.parse_evaluation_settings">.parse_evaluation_settings</a></code>
</p>

<dl>
<dt><code>project_dir</code></dt><dd><p>(<em>optional</em>) Path to the project directory. <code>familiar</code>
checks if the directory indicated by <code>experiment_dir</code> and data files in
<code>data_file</code> are relative to the <code>project_dir</code>.</p>
</dd>
<dt><code>experiment_dir</code></dt><dd><p>(<strong>recommended</strong>) Path to the directory where all
intermediate and final results produced by <code>familiar</code> are written to.
</p>
<p>The <code>experiment_dir</code> can be a path relative to <code>project_dir</code> or an absolute
path.
</p>
<p>In case no project directory is provided and the experiment directory is
not on an absolute path, a directory will be created in the temporary R
directory indicated by <code>tempdir()</code>. This directory is deleted after closing
the R session or once data analysis has finished. All information will be
lost afterwards. Hence, it is recommended to provide either
<code>experiment_dir</code> as an absolute path, or provide both <code>project_dir</code> and
<code>experiment_dir</code>.</p>
</dd>
<dt><code>data_file</code></dt><dd><p>(<em>optional</em>) Path to files containing data that should be
analysed. The paths can be relative to <code>project_dir</code> or absolute paths. An
error will be raised if the file cannot be found.
</p>
<p>The following types of data are supported.
</p>

<ul>
<li> <p><code>csv</code> files containing column headers on the first row, and samples per
row. <code>csv</code> files are read using <code>data.table::fread</code>.
</p>
</li>
<li> <p><code>rds</code> files that contain a <code>data.table</code> or <code>data.frame</code> object. <code>rds</code>
files are imported using <code>base::readRDS</code>.
</p>
</li>
<li> <p><code>RData</code> files that contain a single <code>data.table</code> or <code>data.frame</code> object.
<code>RData</code> files are imported using <code>base::load</code>.
</p>
</li></ul>

<p>All data are expected in wide format, with sample information organised
row-wise.
</p>
<p>More than one data file can be provided. <code>familiar</code> will try to combine
data files based on column names and identifier columns.
</p>
<p>Alternatively, data can be provided using the <code>data</code> argument. These data
are expected to be <code>data.frame</code> or <code>data.table</code> objects or paths to data
files. The latter are handled in the same way as file paths provided to
<code>data_file</code>.</p>
</dd>
<dt><code>batch_id_column</code></dt><dd><p>(<strong>recommended</strong>) Name of the column containing batch
or cohort identifiers. This parameter is required if more than one dataset
is provided, or if external validation is performed.
</p>
<p>In familiar any row of data is organised by four identifiers:
</p>

<ul>
<li><p> The batch identifier <code>batch_id_column</code>: This denotes the group to which a
set of samples belongs, e.g. patients from a single study, samples measured
in a batch, etc. The batch identifier is used for batch normalisation, as
well as selection of development and validation datasets.
</p>
</li>
<li><p> The sample identifier <code>sample_id_column</code>: This denotes the sample level,
e.g. data from a single individual. Subsets of data, e.g. bootstraps or
cross-validation folds, are created at this level.
</p>
</li>
<li><p> The series identifier <code>series_id_column</code>: Indicates measurements on a
single sample that may not share the same outcome value, e.g. a time
series, or the number of cells in a view.
</p>
</li>
<li><p> The repetition identifier: Indicates repeated measurements in a single
series where any feature values may differ, but the outcome does not.
Repetition identifiers are always implicitly set when multiple entries for
the same series of the same sample in the same batch that share the same
outcome are encountered.
</p>
</li></ul>
</dd>
<dt><code>sample_id_column</code></dt><dd><p>(<strong>recommended</strong>) Name of the column containing
sample or subject identifiers. See <code>batch_id_column</code> above for more
details.
</p>
<p>If unset, every row will be identified as a single sample.</p>
</dd>
<dt><code>series_id_column</code></dt><dd><p>(<strong>optional</strong>) Name of the column containing series
identifiers, which distinguish between measurements that are part of a
series for a single sample. See <code>batch_id_column</code> above for more details.
</p>
<p>If unset, rows which share the same batch and sample identifiers but have a
different outcome are assigned unique series identifiers.</p>
</dd>
<dt><code>development_batch_id</code></dt><dd><p>(<em>optional</em>) One or more batch or cohort
identifiers to constitute data sets for development. Defaults to all, or
all minus the identifiers in <code>validation_batch_id</code> for external validation.
Required if external validation is performed and <code>validation_batch_id</code> is
not provided.</p>
</dd>
<dt><code>validation_batch_id</code></dt><dd><p>(<em>optional</em>) One or more batch or cohort
identifiers to constitute data sets for external validation. Defaults to
all data sets except those in <code>development_batch_id</code> for external
validation, or none if not. Required if <code>development_batch_id</code> is not
provided.</p>
</dd>
<dt><code>outcome_name</code></dt><dd><p>(<em>optional</em>) Name of the modelled outcome. This name will
be used in figures created by <code>familiar</code>.
</p>
<p>If not set, the column name in <code>outcome_column</code> will be used for
<code>binomial</code>, <code>multinomial</code>, <code>count</code> and <code>continuous</code> outcomes. For other
outcomes (<code>survival</code> and <code>competing_risk</code>) no default is used.</p>
</dd>
<dt><code>outcome_column</code></dt><dd><p>(<strong>recommended</strong>) Name of the column containing the
outcome of interest. May be identified from a formula, if a formula is
provided as an argument. Otherwise an error is raised. Note that <code>survival</code>
and <code>competing_risk</code> outcome type outcomes require two columns that
indicate the time-to-event or the time of last follow-up and the event
status.</p>
</dd>
<dt><code>outcome_type</code></dt><dd><p>(<strong>recommended</strong>) Type of outcome found in the outcome
column. The outcome type determines many aspects of the overall process,
e.g. the available feature selection methods and learners, but also the
type of assessments that can be conducted to evaluate the resulting models.
Implemented outcome types are:
</p>

<ul>
<li> <p><code>binomial</code>: categorical outcome with 2 levels.
</p>
</li>
<li> <p><code>multinomial</code>: categorical outcome with 2 or more levels.
</p>
</li>
<li> <p><code>count</code>: Poisson-distributed numeric outcomes.
</p>
</li>
<li> <p><code>continuous</code>: general continuous numeric outcomes.
</p>
</li>
<li> <p><code>survival</code>: survival outcome for time-to-event data.
</p>
</li></ul>

<p>If not provided, the algorithm will attempt to obtain outcome_type from
contents of the outcome column. This may lead to unexpected results, and we
therefore advise to provide this information manually.
</p>
<p>Note that <code>competing_risk</code> survival analysis are not fully supported, and
is currently not a valid choice for <code>outcome_type</code>.</p>
</dd>
<dt><code>class_levels</code></dt><dd><p>(<em>optional</em>) Class levels for <code>binomial</code> or <code>multinomial</code>
outcomes. This argument can be used to specify the ordering of levels for
categorical outcomes. These class levels must exactly match the levels
present in the outcome column.</p>
</dd>
<dt><code>event_indicator</code></dt><dd><p>(<strong>recommended</strong>) Indicator for events in <code>survival</code>
and <code>competing_risk</code> analyses. <code>familiar</code> will automatically recognise <code>1</code>,
<code>true</code>, <code>t</code>, <code>y</code> and <code>yes</code> as event indicators, including different
capitalisations. If this parameter is set, it replaces the default values.</p>
</dd>
<dt><code>censoring_indicator</code></dt><dd><p>(<strong>recommended</strong>) Indicator for right-censoring in
<code>survival</code> and <code>competing_risk</code> analyses. <code>familiar</code> will automatically
recognise <code>0</code>, <code>false</code>, <code>f</code>, <code>n</code>, <code>no</code> as censoring indicators, including
different capitalisations. If this parameter is set, it replaces the
default values.</p>
</dd>
<dt><code>competing_risk_indicator</code></dt><dd><p>(<strong>recommended</strong>) Indicator for competing
risks in <code>competing_risk</code> analyses. There are no default values, and if
unset, all values other than those specified by the <code>event_indicator</code> and
<code>censoring_indicator</code> parameters are considered to indicate competing
risks.</p>
</dd>
<dt><code>signature</code></dt><dd><p>(<em>optional</em>) One or more names of feature columns that are
considered part of a specific signature. Features specified here will
always be used for modelling. Ranking from feature selection has no effect
for these features.</p>
</dd>
<dt><code>novelty_features</code></dt><dd><p>(<em>optional</em>) One or more names of feature columns
that should be included for the purpose of novelty detection.</p>
</dd>
<dt><code>exclude_features</code></dt><dd><p>(<em>optional</em>) Feature columns that will be removed
from the data set. Cannot overlap with features in <code>signature</code>,
<code>novelty_features</code> or <code>include_features</code>.</p>
</dd>
<dt><code>include_features</code></dt><dd><p>(<em>optional</em>) Feature columns that are specifically
included in the data set. By default all features are included. Cannot
overlap with <code>exclude_features</code>, but may overlap <code>signature</code>. Features in
<code>signature</code> and <code>novelty_features</code> are always included. If both
<code>exclude_features</code> and <code>include_features</code> are provided, <code>include_features</code>
takes precedence, provided that there is no overlap between the two.</p>
</dd>
<dt><code>reference_method</code></dt><dd><p>(<em>optional</em>) Method used to set reference levels for
categorical features. There are several options:
</p>

<ul>
<li> <p><code>auto</code> (default): Categorical features that are not explicitly set by the
user, i.e. columns containing boolean values or characters, use the most
frequent level as reference. Categorical features that are explicitly set,
i.e. as factors, are used as is.
</p>
</li>
<li> <p><code>always</code>: Both automatically detected and user-specified categorical
features have the reference level set to the most frequent level. Ordinal
features are not altered, but are used as is.
</p>
</li>
<li> <p><code>never</code>: User-specified categorical features are used as is.
Automatically detected categorical features are simply sorted, and the
first level is then used as the reference level. This was the behaviour
prior to familiar version 1.3.0.
</p>
</li></ul>
</dd>
<dt><code>experimental_design</code></dt><dd><p>(<strong>required</strong>) Defines what the experiment looks
like, e.g. <code>cv(bt(fs,20)+mb,3,2)+ev</code> for 2 times repeated 3-fold
cross-validation with nested feature selection on 20 bootstraps and
model-building, and external validation. The basic workflow components are:
</p>

<ul>
<li> <p><code>fs</code>: (required) feature selection step.
</p>
</li>
<li> <p><code>mb</code>: (required) model building step.
</p>
</li>
<li> <p><code>ev</code>: (optional) external validation. Note that internal validation due
to subsampling will always be conducted if the subsampling methods create
any validation data sets.
</p>
</li></ul>

<p>The different components are linked using <code>+</code>.
</p>
<p>Different subsampling methods can be used in conjunction with the basic
workflow components:
</p>

<ul>
<li> <p><code>bs(x,n)</code>: (stratified) .632 bootstrap, with <code>n</code> the number of
bootstraps. In contrast to <code>bt</code>, feature pre-processing parameters and
hyperparameter optimisation are conducted on individual bootstraps.
</p>
</li>
<li> <p><code>bt(x,n)</code>: (stratified) .632 bootstrap, with <code>n</code> the number of
bootstraps. Unlike <code>bs</code> and other subsampling methods, no separate
pre-processing parameters or optimised hyperparameters will be determined
for each bootstrap.
</p>
</li>
<li> <p><code>cv(x,n,p)</code>: (stratified) <code>n</code>-fold cross-validation, repeated <code>p</code> times.
Pre-processing parameters are determined for each iteration.
</p>
</li>
<li> <p><code>lv(x)</code>: leave-one-out-cross-validation. Pre-processing parameters are
determined for each iteration.
</p>
</li>
<li> <p><code>ip(x)</code>: imbalance partitioning for addressing class imbalances on the
data set. Pre-processing parameters are determined for each partition. The
number of partitions generated depends on the imbalance correction method
(see the <code>imbalance_correction_method</code> parameter). Imbalance partitioning
does not generate validation sets.
</p>
</li></ul>

<p>As shown in the example above, sampling algorithms can be nested.
</p>
<p>The simplest valid experimental design is <code>fs+mb</code>, which corresponds to a
TRIPOD type 1a analysis. Type 1b analyses are only possible using
bootstraps, e.g. <code>bt(fs+mb,100)</code>. Type 2a analyses can be conducted using
cross-validation, e.g. <code>cv(bt(fs,100)+mb,10,1)</code>. Depending on the origin of
the external validation data, designs such as <code>fs+mb+ev</code> or
<code>cv(bt(fs,100)+mb,10,1)+ev</code> constitute type 2b or type 3 analyses. Type 4
analyses can be done by obtaining one or more <code>familiarModel</code> objects from
others and applying them to your own data set.
</p>
<p>Alternatively, the <code>experimental_design</code> parameter may be used to provide a
path to a file containing iterations, which is named <code style="white-space: pre;">&#8288;####_iterations.RDS&#8288;</code>
by convention. This path can be relative to the directory of the current
experiment (<code>experiment_dir</code>), or an absolute path. The absolute path may
thus also point to a file from a different experiment.</p>
</dd>
<dt><code>imbalance_correction_method</code></dt><dd><p>(<em>optional</em>) Type of method used to
address class imbalances. Available options are:
</p>

<ul>
<li> <p><code>full_undersampling</code> (default): All data will be used in an ensemble
fashion. The full minority class will appear in each partition, but
majority classes are undersampled until all data have been used.
</p>
</li>
<li> <p><code>random_undersampling</code>: Randomly undersamples majority classes. This is
useful in cases where full undersampling would lead to the formation of
many models due major overrepresentation of the largest class.
</p>
</li></ul>

<p>This parameter is only used in combination with imbalance partitioning in
the experimental design, and <code>ip</code> should therefore appear in the string
that defines the design.</p>
</dd>
<dt><code>imbalance_n_partitions</code></dt><dd><p>(<em>optional</em>) Number of times random
undersampling should be repeated. 10 undersampled subsets with balanced
classes are formed by default.</p>
</dd>
<dt><code>parallel</code></dt><dd><p>(<em>optional</em>) Enable parallel processing. Defaults to <code>TRUE</code>.
When set to <code>FALSE</code>, this disables all parallel processing, regardless of
specific parameters such as <code>parallel_preprocessing</code>. However, when
<code>parallel</code> is <code>TRUE</code>, parallel processing of different parts of the
workflow can be disabled by setting respective flags to <code>FALSE</code>.</p>
</dd>
<dt><code>parallel_nr_cores</code></dt><dd><p>(<em>optional</em>) Number of cores available for
parallelisation. Defaults to 2. This setting does nothing if
parallelisation is disabled.</p>
</dd>
<dt><code>restart_cluster</code></dt><dd><p>(<em>optional</em>) Restart nodes used for parallel computing
to free up memory prior to starting a parallel process. Note that it does
take time to set up the clusters. Therefore setting this argument to <code>TRUE</code>
may impact processing speed. This argument is ignored if <code>parallel</code> is
<code>FALSE</code> or the cluster was initialised outside of familiar. Default is
<code>FALSE</code>, which causes the clusters to be initialised only once.</p>
</dd>
<dt><code>cluster_type</code></dt><dd><p>(<em>optional</em>) Selection of the cluster type for parallel
processing. Available types are the ones supported by the parallel package
that is part of the base R distribution: <code>psock</code> (default), <code>fork</code>, <code>mpi</code>,
<code>nws</code>, <code>sock</code>. In addition, <code>none</code> is available, which also disables
parallel processing.</p>
</dd>
<dt><code>backend_type</code></dt><dd><p>(<em>optional</em>) Selection of the backend for distributing
copies of the data. This backend ensures that only a single master copy is
kept in memory. This limits memory usage during parallel processing.
</p>
<p>Several backend options are available, notably <code>socket_server</code>, and <code>none</code>
(default). <code>socket_server</code> is based on the callr package and R sockets,
comes with <code>familiar</code> and is available for any OS. <code>none</code> uses the package
environment of familiar to store data, and is available for any OS.
However, <code>none</code> requires copying of data to any parallel process, and has a
larger memory footprint.</p>
</dd>
<dt><code>server_port</code></dt><dd><p>(<em>optional</em>) Integer indicating the port on which the
socket server or RServe process should communicate. Defaults to port 6311.
Note that ports 0 to 1024 and 49152 to 65535 cannot be used.</p>
</dd>
<dt><code>feature_max_fraction_missing</code></dt><dd><p>(<em>optional</em>) Numeric value between <code>0.0</code>
and <code>0.95</code> that determines the meximum fraction of missing values that
still allows a feature to be included in the data set. All features with a
missing value fraction over this threshold are not processed further. The
default value is <code>0.30</code>.</p>
</dd>
<dt><code>sample_max_fraction_missing</code></dt><dd><p>(<em>optional</em>) Numeric value between <code>0.0</code>
and <code>0.95</code> that determines the maximum fraction of missing values that
still allows a sample to be included in the data set. All samples with a
missing value fraction over this threshold are excluded and not processed
further. The default value is <code>0.30</code>.</p>
</dd>
<dt><code>filter_method</code></dt><dd><p>(<em>optional</em>) One or methods used to reduce
dimensionality of the data set by removing irrelevant or poorly
reproducible features.
</p>
<p>Several method are available:
</p>

<ul>
<li> <p><code>none</code> (default): None of the features will be filtered.
</p>
</li>
<li> <p><code>low_variance</code>: Features with a variance below the
<code>low_var_minimum_variance_threshold</code> are filtered. This can be useful to
filter, for example, genes that are not differentially expressed.
</p>
</li>
<li> <p><code>univariate_test</code>: Features undergo a univariate regression using an
outcome-appropriate regression model. The p-value of the model coefficient
is collected. Features with coefficient p or q-value above the
<code>univariate_test_threshold</code> are subsequently filtered.
</p>
</li>
<li> <p><code>robustness</code>: Features that are not sufficiently robust according to the
intraclass correlation coefficient are filtered. Use of this method
requires that repeated measurements are present in the data set, i.e. there
should be entries for which the sample and cohort identifiers are the same.
</p>
</li></ul>

<p>More than one method can be used simultaneously. Features with singular
values are always filtered, as these do not contain information.</p>
</dd>
<dt><code>univariate_test_threshold</code></dt><dd><p>(<em>optional</em>) Numeric value between <code>1.0</code> and
<code>0.0</code> that determines which features are irrelevant and will be filtered by
the <code>univariate_test</code>. The p or q-values are compared to this threshold.
All features with values above the threshold are filtered. The default
value is <code>0.20</code>.</p>
</dd>
<dt><code>univariate_test_threshold_metric</code></dt><dd><p>(<em>optional</em>) Metric used with the to
compare the <code>univariate_test_threshold</code> against. The following metrics can
be chosen:
</p>

<ul>
<li> <p><code>p_value</code> (default): The unadjusted p-value of each feature is used for
to filter features.
</p>
</li>
<li> <p><code>q_value</code>: The q-value (Story, 2002), is used to filter features. Some
data sets may have insufficient samples to compute the q-value. The
<code>qvalue</code> package must be installed from Bioconductor to use this method.
</p>
</li></ul>
</dd>
<dt><code>univariate_test_max_feature_set_size</code></dt><dd><p>(<em>optional</em>) Maximum size of the
feature set after the univariate test. P or q values of features are
compared against the threshold, but if the resulting data set would be
larger than this setting, only the most relevant features up to the desired
feature set size are selected.
</p>
<p>The default value is <code>NULL</code>, which causes features to be filtered based on
their relevance only.</p>
</dd>
<dt><code>low_var_minimum_variance_threshold</code></dt><dd><p>(required, if used) Numeric value
that determines which features will be filtered by the <code>low_variance</code>
method. The variance of each feature is computed and compared to the
threshold. If it is below the threshold, the feature is removed.
</p>
<p>This parameter has no default value and should be set if <code>low_variance</code> is
used.</p>
</dd>
<dt><code>low_var_max_feature_set_size</code></dt><dd><p>(<em>optional</em>) Maximum size of the feature
set after filtering features with a low variance. All features are first
compared against <code>low_var_minimum_variance_threshold</code>. If the resulting
feature set would be larger than specified, only the most strongly varying
features will be selected, up to the desired size of the feature set.
</p>
<p>The default value is <code>NULL</code>, which causes features to be filtered based on
their variance only.</p>
</dd>
<dt><code>robustness_icc_type</code></dt><dd><p>(<em>optional</em>) String indicating the type of
intraclass correlation coefficient (<code>1</code>, <code>2</code> or <code>3</code>) that should be used to
compute robustness for features in repeated measurements. These types
correspond to the types in Shrout and Fleiss (1979). The default value is
<code>1</code>.</p>
</dd>
<dt><code>robustness_threshold_metric</code></dt><dd><p>(<em>optional</em>) String indicating which
specific intraclass correlation coefficient (ICC) metric should be used to
filter features. This should be one of:
</p>

<ul>
<li> <p><code>icc</code>: The estimated ICC value itself.
</p>
</li>
<li> <p><code>icc_low</code> (default): The estimated lower limit of the 95% confidence
interval of the ICC, as suggested by Koo and Li (2016).
</p>
</li>
<li> <p><code>icc_panel</code>: The estimated ICC value over the panel average, i.e. the ICC
that would be obtained if all repeated measurements were averaged.
</p>
</li>
<li> <p><code>icc_panel_low</code>: The estimated lower limit of the 95% confidence interval
of the panel ICC.
</p>
</li></ul>
</dd>
<dt><code>robustness_threshold_value</code></dt><dd><p>(<em>optional</em>) The intraclass correlation
coefficient value that is as threshold. The default value is <code>0.70</code>.</p>
</dd>
<dt><code>transformation_method</code></dt><dd><p>(<em>optional</em>) The transformation method used to
change the distribution of the data to be more normal-like. The following
methods are available:
</p>

<ul>
<li> <p><code>none</code>: This disables transformation of features.
</p>
</li>
<li> <p><code>yeo_johnson</code> (default): Transformation using the Yeo-Johnson
transformation (Yeo and Johnson, 2000). The algorithm tests various lambda
values and selects the lambda that maximises the log-likelihood.
</p>
</li>
<li> <p><code>yeo_johnson_trim</code>: As <code>yeo_johnson</code>, but based on the set of feature
values where the 5% lowest and 5% highest values are discarded. This
reduces the effect of outliers.
</p>
</li>
<li> <p><code>yeo_johnson_winsor</code>: As <code>yeo_johnson</code>, but based on the set of feature
values where the 5% lowest and 5% highest values are winsorised. This
reduces the effect of outliers.
</p>
</li>
<li> <p><code>yeo_johnson_robust</code>: A robust version of <code>yeo_johnson</code> after Raymaekers
and Rousseeuw (2021). This method is less sensitive to outliers.
</p>
</li>
<li> <p><code>box_cox</code>: Transformation using the Box-Cox transformation (Box and Cox,
1964). Unlike the Yeo-Johnson transformation, the Box-Cox transformation
requires that all data are positive. Features that contain zero or negative
values cannot be transformed using this transformation. The algorithm tests
various lambda values and selects the lambda that maximises the
log-likelihood.
</p>
</li>
<li> <p><code>box_cox_trim</code>: As <code>box_cox</code>, but based on the set of feature values
where the 5% lowest and 5% highest values are discarded. This reduces the
effect of outliers.
</p>
</li>
<li> <p><code>box_cox_winsor</code>: As <code>box_cox</code>, but based on the set of feature values
where the 5% lowest and 5% highest values are winsorised. This reduces the
effect of outliers.
</p>
</li>
<li> <p><code>box_cox_robust</code>: A robust verson of <code>box_cox</code> after Raymaekers and
Rousseew (2021). This method is less sensitive to outliers.
</p>
</li></ul>

<p>Only features that contain numerical data are transformed. Transformation
parameters obtained in development data are stored within <code>featureInfo</code>
objects for later use with validation data sets.</p>
</dd>
<dt><code>normalisation_method</code></dt><dd><p>(<em>optional</em>) The normalisation method used to
improve the comparability between numerical features that may have very
different scales. The following normalisation methods can be chosen:
</p>

<ul>
<li> <p><code>none</code>: This disables feature normalisation.
</p>
</li>
<li> <p><code>standardisation</code>: Features are normalised by subtraction of their mean
values and division by their standard deviations. This causes every feature
to be have a center value of 0.0 and standard deviation of 1.0.
</p>
</li>
<li> <p><code>standardisation_trim</code>: As <code>standardisation</code>, but based on the set of
feature values where the 5% lowest and 5% highest values are discarded.
This reduces the effect of outliers.
</p>
</li>
<li> <p><code>standardisation_winsor</code>: As <code>standardisation</code>, but based on the set of
feature values where the 5% lowest and 5% highest values are winsorised.
This reduces the effect of outliers.
</p>
</li>
<li> <p><code>standardisation_robust</code> (default): A robust version of <code>standardisation</code>
that relies on computing Huber's M-estimators for location and scale.
</p>
</li>
<li> <p><code>normalisation</code>: Features are normalised by subtraction of their minimum
values and division by their ranges. This maps all feature values to a
<code class="reqn">[0, 1]</code> interval.
</p>
</li>
<li> <p><code>normalisation_trim</code>: As <code>normalisation</code>, but based on the set of feature
values where the 5% lowest and 5% highest values are discarded. This
reduces the effect of outliers.
</p>
</li>
<li> <p><code>normalisation_winsor</code>: As <code>normalisation</code>, but based on the set of
feature values where the 5% lowest and 5% highest values are winsorised.
This reduces the effect of outliers.
</p>
</li>
<li> <p><code>quantile</code>: Features are normalised by subtraction of their median values
and division by their interquartile range.
</p>
</li>
<li> <p><code>mean_centering</code>: Features are centered by substracting the mean, but do
not undergo rescaling.
</p>
</li></ul>

<p>Only features that contain numerical data are normalised. Normalisation
parameters obtained in development data are stored within <code>featureInfo</code>
objects for later use with validation data sets.</p>
</dd>
<dt><code>batch_normalisation_method</code></dt><dd><p>(<em>optional</em>) The method used for batch
normalisation. Available methods are:
</p>

<ul>
<li> <p><code>none</code> (default): This disables batch normalisation of features.
</p>
</li>
<li> <p><code>standardisation</code>: Features within each batch are normalised by
subtraction of the mean value and division by the standard deviation in
each batch.
</p>
</li>
<li> <p><code>standardisation_trim</code>: As <code>standardisation</code>, but based on the set of
feature values where the 5% lowest and 5% highest values are discarded.
This reduces the effect of outliers.
</p>
</li>
<li> <p><code>standardisation_winsor</code>: As <code>standardisation</code>, but based on the set of
feature values where the 5% lowest and 5% highest values are winsorised.
This reduces the effect of outliers.
</p>
</li>
<li> <p><code>standardisation_robust</code>: A robust version of <code>standardisation</code> that
relies on computing Huber's M-estimators for location and scale within each
batch.
</p>
</li>
<li> <p><code>normalisation</code>: Features within each batch are normalised by subtraction
of their minimum values and division by their range in each batch. This
maps all feature values in each batch to a <code class="reqn">[0, 1]</code> interval.
</p>
</li>
<li> <p><code>normalisation_trim</code>: As <code>normalisation</code>, but based on the set of feature
values where the 5% lowest and 5% highest values are discarded. This
reduces the effect of outliers.
</p>
</li>
<li> <p><code>normalisation_winsor</code>: As <code>normalisation</code>, but based on the set of
feature values where the 5% lowest and 5% highest values are winsorised.
This reduces the effect of outliers.
</p>
</li>
<li> <p><code>quantile</code>: Features in each batch are normalised by subtraction of the
median value and division by the interquartile range of each batch.
</p>
</li>
<li> <p><code>mean_centering</code>: Features in each batch are centered on 0.0 by
substracting the mean value in each batch, but are not rescaled.
</p>
</li>
<li> <p><code>combat_parametric</code>: Batch adjustments using parametric empirical Bayes
(Johnson et al, 2007). <code>combat_p</code> leads to the same method.
</p>
</li>
<li> <p><code>combat_non_parametric</code>: Batch adjustments using non-parametric empirical
Bayes (Johnson et al, 2007). <code>combat_np</code> and <code>combat</code> lead to the same
method. Note that we reduced complexity from O(<code class="reqn">n^2</code>) to O(<code class="reqn">n</code>) by
only computing batch adjustment parameters for each feature on a subset of
50 randomly selected features, instead of all features.
</p>
</li></ul>

<p>Only features that contain numerical data are normalised using batch
normalisation. Batch normalisation parameters obtained in development data
are stored within <code>featureInfo</code> objects for later use with validation data
sets, in case the validation data is from the same batch.
</p>
<p>If validation data contains data from unknown batches, normalisation
parameters are separately determined for these batches.
</p>
<p>Note that for both empirical Bayes methods, the batch effect is assumed to
produce results across the features. This is often true for things such as
gene expressions, but the assumption may not hold generally.
</p>
<p>When performing batch normalisation, it is moreover important to check that
differences between batches or cohorts are not related to the studied
endpoint.</p>
</dd>
<dt><code>imputation_method</code></dt><dd><p>(<em>optional</em>) Method used for imputing missing
feature values. Two methods are implemented:
</p>

<ul>
<li> <p><code>simple</code>: Simple replacement of a missing value by the median value (for
numeric features) or the modal value (for categorical features).
</p>
</li>
<li> <p><code>lasso</code>: Imputation of missing value by lasso regression (using <code>glmnet</code>)
based on information contained in other features.
</p>
</li></ul>

<p><code>simple</code> imputation precedes <code>lasso</code> imputation to ensure that any missing
values in predictors required for <code>lasso</code> regression are resolved. The
<code>lasso</code> estimate is then used to replace the missing value.
</p>
<p>The default value depends on the number of features in the dataset. If the
number is lower than 100, <code>lasso</code> is used by default, and <code>simple</code>
otherwise.
</p>
<p>Only single imputation is performed. Imputation models and parameters are
stored within <code>featureInfo</code> objects for later use with validation data
sets.</p>
</dd>
<dt><code>cluster_method</code></dt><dd><p>(<em>optional</em>) Clustering is performed to identify and
replace redundant features, for example those that are highly correlated.
Such features do not carry much additional information and may be removed
or replaced instead (Park et al., 2007; Tolosi and Lengauer, 2011).
</p>
<p>The cluster method determines the algorithm used to form the clusters. The
following cluster methods are implemented:
</p>

<ul>
<li> <p><code>none</code>: No clustering is performed.
</p>
</li>
<li> <p><code>hclust</code> (default): Hierarchical agglomerative clustering. If the
<code>fastcluster</code> package is installed, <code>fastcluster::hclust</code> is used (Muellner
2013), otherwise <code>stats::hclust</code> is used.
</p>
</li>
<li> <p><code>agnes</code>: Hierarchical clustering using agglomerative nesting (Kaufman and
Rousseeuw, 1990). This algorithm is similar to <code>hclust</code>, but uses the
<code>cluster::agnes</code> implementation.
</p>
</li>
<li> <p><code>diana</code>: Divisive analysis hierarchical clustering. This method uses
divisive instead of agglomerative clustering (Kaufman and Rousseeuw, 1990).
<code>cluster::diana</code> is used.
</p>
</li>
<li> <p><code>pam</code>: Partioning around medioids. This partitions the data into $k$
clusters around medioids (Kaufman and Rousseeuw, 1990). $k$ is selected
using the <code>silhouette</code> metric. <code>pam</code> is implemented using the
<code>cluster::pam</code> function.
</p>
</li></ul>

<p>Clusters and cluster information is stored within <code>featureInfo</code> objects for
later use with validation data sets. This enables reproduction of the same
clusters as formed in the development data set.</p>
</dd>
<dt><code>cluster_linkage_method</code></dt><dd><p>(<em>optional</em>) Linkage method used for
agglomerative clustering in <code>hclust</code> and <code>agnes</code>. The following linkage
methods can be used:
</p>

<ul>
<li> <p><code>average</code> (default): Average linkage.
</p>
</li>
<li> <p><code>single</code>: Single linkage.
</p>
</li>
<li> <p><code>complete</code>: Complete linkage.
</p>
</li>
<li> <p><code>weighted</code>: Weighted linkage, also known as McQuitty linkage.
</p>
</li>
<li> <p><code>ward</code>: Linkage using Ward's minimum variance method.
</p>
</li></ul>

<p><code>diana</code> and <code>pam</code> do not require a linkage method.</p>
</dd>
<dt><code>cluster_cut_method</code></dt><dd><p>(<em>optional</em>) The method used to define the actual
clusters. The following methods can be used:
</p>

<ul>
<li> <p><code>silhouette</code>: Clusters are formed based on the silhouette score
(Rousseeuw, 1987). The average silhouette score is computed from 2 to
<code class="reqn">n</code> clusters, with <code class="reqn">n</code> the number of features. Clusters are only
formed if the average silhouette exceeds 0.50, which indicates reasonable
evidence for structure. This procedure may be slow if the number of
features is large (&gt;100s).
</p>
</li>
<li> <p><code>fixed_cut</code>: Clusters are formed by cutting the hierarchical tree at the
point indicated by the <code>cluster_similarity_threshold</code>, e.g. where features
in a cluster have an average Spearman correlation of 0.90. <code>fixed_cut</code> is
only available for <code>agnes</code>, <code>diana</code> and <code>hclust</code>.
</p>
</li>
<li> <p><code>dynamic_cut</code>: Dynamic cluster formation using the cutting algorithm in
the <code>dynamicTreeCut</code> package. This package should be installed to select
this option. <code>dynamic_cut</code> can only be used with <code>agnes</code> and <code>hclust</code>.
</p>
</li></ul>

<p>The default options are <code>silhouette</code> for partioning around medioids (<code>pam</code>)
and <code>fixed_cut</code> otherwise.</p>
</dd>
<dt><code>cluster_similarity_metric</code></dt><dd><p>(<em>optional</em>) Clusters are formed based on
feature similarity. All features are compared in a pair-wise fashion to
compute similarity, for example correlation. The resulting similarity grid
is converted into a distance matrix that is subsequently used for
clustering. The following metrics are supported to compute pairwise
similarities:
</p>

<ul>
<li> <p><code>mutual_information</code> (default): normalised mutual information.
</p>
</li>
<li> <p><code>mcfadden_r2</code>: McFadden's pseudo R-squared (McFadden, 1974).
</p>
</li>
<li> <p><code>cox_snell_r2</code>: Cox and Snell's pseudo R-squared (Cox and Snell, 1989).
</p>
</li>
<li> <p><code>nagelkerke_r2</code>: Nagelkerke's pseudo R-squared (Nagelkerke, 1991).
</p>
</li>
<li> <p><code>spearman</code>: Spearman's rank order correlation.
</p>
</li>
<li> <p><code>kendall</code>: Kendall rank correlation.
</p>
</li>
<li> <p><code>pearson</code>: Pearson product-moment correlation.
</p>
</li></ul>

<p>The pseudo R-squared metrics can be used to assess similarity between mixed
pairs of numeric and categorical features, as these are based on the
log-likelihood of regression models. In <code>familiar</code>, the more informative
feature is used as the predictor and the other feature as the reponse
variable. In numeric-categorical pairs, the numeric feature is considered
to be more informative and is thus used as the predictor. In
categorical-categorical pairs, the feature with most levels is used as the
predictor.
</p>
<p>In case any of the classical correlation coefficients (<code>pearson</code>,
<code>spearman</code> and <code>kendall</code>) are used with (mixed) categorical features, the
categorical features are one-hot encoded and the mean correlation over all
resulting pairs is used as similarity.</p>
</dd>
<dt><code>cluster_similarity_threshold</code></dt><dd><p>(<em>optional</em>) The threshold level for
pair-wise similarity that is required to form clusters using <code>fixed_cut</code>.
This should be a numerical value between 0.0 and 1.0. Note however, that a
reasonable threshold value depends strongly on the similarity metric. The
following are the default values used:
</p>

<ul>
<li> <p><code>mcfadden_r2</code> and <code>mutual_information</code>: <code>0.30</code>
</p>
</li>
<li> <p><code>cox_snell_r2</code> and <code>nagelkerke_r2</code>: <code>0.75</code>
</p>
</li>
<li> <p><code>spearman</code>, <code>kendall</code> and <code>pearson</code>: <code>0.90</code>
</p>
</li></ul>

<p>Alternatively, if the <code style="white-space: pre;">&#8288;fixed cut&#8288;</code> method is not used, this value determines
whether any clustering should be performed, because the data may not
contain highly similar features. The default values in this situation are:
</p>

<ul>
<li> <p><code>mcfadden_r2</code>  and <code>mutual_information</code>: <code>0.25</code>
</p>
</li>
<li> <p><code>cox_snell_r2</code> and <code>nagelkerke_r2</code>: <code>0.40</code>
</p>
</li>
<li> <p><code>spearman</code>, <code>kendall</code> and <code>pearson</code>: <code>0.70</code>
</p>
</li></ul>

<p>The threshold value is converted to a distance (1-similarity) prior to
cutting hierarchical trees.</p>
</dd>
<dt><code>cluster_representation_method</code></dt><dd><p>(<em>optional</em>) Method used to determine
how the information of co-clustered features is summarised and used to
represent the cluster. The following methods can be selected:
</p>

<ul>
<li> <p><code>best_predictor</code> (default): The feature with the highest importance
according to univariate regression with the outcome is used to represent
the cluster.
</p>
</li>
<li> <p><code>medioid</code>: The feature closest to the cluster center, i.e. the feature
that is most similar to the remaining features in the cluster, is used to
represent the feature.
</p>
</li>
<li> <p><code>mean</code>: A meta-feature is generated by averaging the feature values for
all features in a cluster. This method aligns all features so that all
features will be positively correlated prior to averaging. Should a cluster
contain one or more categorical features, the <code>medioid</code> method will be used
instead, as averaging is not possible. Note that if this method is chosen,
the <code>normalisation_method</code> parameter should be one of <code>standardisation</code>,
<code>standardisation_trim</code>, <code>standardisation_winsor</code> or <code>quantile</code>.'
</p>
</li></ul>

<p>If the <code>pam</code> cluster method is selected, only the <code>medioid</code> method can be
used. In that case 1 medioid is used by default.</p>
</dd>
<dt><code>parallel_preprocessing</code></dt><dd><p>(<em>optional</em>) Enable parallel processing for the
preprocessing workflow. Defaults to <code>TRUE</code>. When set to <code>FALSE</code>, this will
disable the use of parallel processing while preprocessing, regardless of
the settings of the <code>parallel</code> parameter. <code>parallel_preprocessing</code> is
ignored if <code>parallel=FALSE</code>.</p>
</dd>
<dt><code>fs_method</code></dt><dd><p>(<strong>required</strong>) Feature selection method to be used for
determining variable importance. <code>familiar</code> implements various feature
selection methods. Please refer to the vignette on feature selection
methods for more details.
</p>
<p>More than one feature selection method can be chosen. The experiment will
then repeated for each feature selection method.
</p>
<p>Feature selection methods determines the ranking of features. Actual
selection of features is done by optimising the signature size model
hyperparameter during the hyperparameter optimisation step.</p>
</dd>
<dt><code>fs_method_parameter</code></dt><dd><p>(<em>optional</em>) List of lists containing parameters
for feature selection methods. Each sublist should have the name of the
feature selection method it corresponds to.
</p>
<p>Most feature selection methods do not have parameters that can be set.
Please refer to the vignette on feature selection methods for more details.
Note that if the feature selection method is based on a learner (e.g. lasso
regression), hyperparameter optimisation may be performed prior to
assessing variable importance.</p>
</dd>
<dt><code>vimp_aggregation_method</code></dt><dd><p>(<em>optional</em>) The method used to aggregate
variable importances over different data subsets, e.g. bootstraps. The
following methods can be selected:
</p>

<ul>
<li> <p><code>none</code>: Don't aggregate ranks, but rather aggregate the variable
importance scores themselves.
</p>
</li>
<li> <p><code>mean</code>: Use the mean rank of a feature over the subsets to
determine the aggregated feature rank.
</p>
</li>
<li> <p><code>median</code>: Use the median rank of a feature over the subsets to determine
the aggregated feature rank.
</p>
</li>
<li> <p><code>best</code>: Use the best rank the feature obtained in any subset to determine
the aggregated feature rank.
</p>
</li>
<li> <p><code>worst</code>: Use the worst rank the feature obtained in any subset to
determine the aggregated feature rank.
</p>
</li>
<li> <p><code>stability</code>: Use the frequency of the feature being in the subset of
highly ranked features as measure for the aggregated feature rank
(Meinshausen and Buehlmann, 2010).
</p>
</li>
<li> <p><code>exponential</code>: Use a rank-weighted frequence of occurrence in the subset
of highly ranked features as measure for the aggregated feature rank (Haury
et al., 2011).
</p>
</li>
<li> <p><code>borda</code> (default): Use the borda count as measure for the aggregated
feature rank (Wald et al., 2012).
</p>
</li>
<li> <p><code>enhanced_borda</code>: Use an occurrence frequency-weighted borda count as
measure for the aggregated feature rank (Wald et al., 2012).
</p>
</li>
<li> <p><code>truncated_borda</code>: Use borda count computed only on features within the
subset of highly ranked features.
</p>
</li>
<li> <p><code>enhanced_truncated_borda</code>: Apply both the enhanced borda method and the
truncated borda method and use the resulting borda count as the aggregated
feature rank.
</p>
</li></ul>

<p>The <em>feature selection methods</em> vignette provides additional information.</p>
</dd>
<dt><code>vimp_aggregation_rank_threshold</code></dt><dd><p>(<em>optional</em>) The threshold used to
define the subset of highly important features. If not set, this threshold
is determined by maximising the variance in the occurrence value over all
features over the subset size.
</p>
<p>This parameter is only relevant for <code>stability</code>, <code>exponential</code>,
<code>enhanced_borda</code>, <code>truncated_borda</code> and <code>enhanced_truncated_borda</code> methods.</p>
</dd>
<dt><code>parallel_feature_selection</code></dt><dd><p>(<em>optional</em>) Enable parallel processing for
the feature selection workflow. Defaults to <code>TRUE</code>. When set to <code>FALSE</code>,
this will disable the use of parallel processing while performing feature
selection, regardless of the settings of the <code>parallel</code> parameter.
<code>parallel_feature_selection</code> is ignored if <code>parallel=FALSE</code>.</p>
</dd>
<dt><code>learner</code></dt><dd><p>(<strong>required</strong>) One or more algorithms used for model
development. A sizeable number learners is supported in <code>familiar</code>. Please
see the vignette on learners for more information concerning the available
learners.</p>
</dd>
<dt><code>hyperparameter</code></dt><dd><p>(<em>optional</em>) List of lists containing hyperparameters
for learners. Each sublist should have the name of the learner method it
corresponds to, with list elements being named after the intended
hyperparameter, e.g. <code>"glm_logistic"=list("sign_size"=3)</code>
</p>
<p>All learners have hyperparameters. Please refer to the vignette on learners
for more details. If no parameters are provided, sequential model-based
optimisation is used to determine optimal hyperparameters.
</p>
<p>Hyperparameters provided by the user are never optimised. However, if more
than one value is provided for a single hyperparameter, optimisation will
be conducted using these values.</p>
</dd>
<dt><code>novelty_detector</code></dt><dd><p>(<em>optional</em>) Specify the algorithm used for training
a novelty detector. This detector can be used to identify
out-of-distribution data prospectively.</p>
</dd>
<dt><code>detector_parameters</code></dt><dd><p>(<em>optional</em>) List lists containing hyperparameters
for novelty detectors. Currently not used.</p>
</dd>
<dt><code>parallel_model_development</code></dt><dd><p>(<em>optional</em>) Enable parallel processing for
the model development workflow. Defaults to <code>TRUE</code>. When set to <code>FALSE</code>,
this will disable the use of parallel processing while developing models,
regardless of the settings of the <code>parallel</code> parameter.
<code>parallel_model_development</code> is ignored if <code>parallel=FALSE</code>.</p>
</dd>
<dt><code>optimisation_bootstraps</code></dt><dd><p>(<em>optional</em>) Number of bootstraps that should
be generated from the development data set. During the optimisation
procedure one or more of these bootstraps (indicated by
<code>smbo_step_bootstraps</code>) are used for model development using different
combinations of hyperparameters. The effect of the hyperparameters is then
assessed by comparing in-bag and out-of-bag model performance.
</p>
<p>The default number of bootstraps is <code>50</code>. Hyperparameter optimisation may
finish before exhausting the set of bootstraps.</p>
</dd>
<dt><code>optimisation_determine_vimp</code></dt><dd><p>(<em>optional</em>) Logical value that indicates
whether variable importance is determined separately for each of the
bootstraps created during the optimisation process (<code>TRUE</code>) or the
applicable results from the feature selection step are used (<code>FALSE</code>).
</p>
<p>Determining variable importance increases the initial computational
overhead. However, it prevents positive biases for the out-of-bag data due
to overlap of these data with the development data set used for the feature
selection step. In this case, any hyperparameters of the variable
importance method are not determined separately for each bootstrap, but
those obtained during the feature selection step are used instead. In case
multiple of such hyperparameter sets could be applicable, the set that will
be used is randomly selected for each bootstrap.
</p>
<p>This parameter only affects hyperparameter optimisation of learners. The
default is <code>TRUE</code>.</p>
</dd>
<dt><code>smbo_random_initialisation</code></dt><dd><p>(<em>optional</em>) String indicating the
initialisation method for the hyperparameter space. Can be one of
<code>fixed_subsample</code> (default), <code>fixed</code>, or <code>random</code>. <code>fixed</code> and
<code>fixed_subsample</code> first create hyperparameter sets from a range of default
values set by familiar. <code>fixed_subsample</code> then randomly draws up to
<code>smbo_n_random_sets</code> from the grid. <code>random</code> does not rely upon a fixed
grid, and randomly draws up to <code>smbo_n_random_sets</code> hyperparameter sets
from the hyperparameter space.</p>
</dd>
<dt><code>smbo_n_random_sets</code></dt><dd><p>(<em>optional</em>) Number of random or subsampled
hyperparameters drawn during the initialisation process. Default: <code>100</code>.
Cannot be smaller than <code>10</code>. The parameter is not used when
<code>smbo_random_initialisation</code> is <code>fixed</code>, as the entire pre-defined grid
will be explored.</p>
</dd>
<dt><code>max_smbo_iterations</code></dt><dd><p>(<em>optional</em>) Maximum number of intensify
iterations of the SMBO algorithm. During an intensify iteration a run-off
occurs between the current <em>best</em> hyperparameter combination and either 10
challenger combination with the highest expected improvement or a set of 20
random combinations.
</p>
<p>Run-off with random combinations is used to force exploration of the
hyperparameter space, and is performed every second intensify iteration, or
if there is no expected improvement for any challenger combination.
</p>
<p>If a combination of hyperparameters leads to better performance on the same
data than the incumbent <em>best</em> set of hyperparameters, it replaces the
incumbent set at the end of the intensify iteration.
</p>
<p>The default number of intensify iteration is <code>20</code>. Iterations may be
stopped early if the incumbent set of hyperparameters remains the same for
<code>smbo_stop_convergent_iterations</code> iterations, or performance improvement is
minimal. This behaviour is suppressed during the first 4 iterations to
enable the algorithm to explore the hyperparameter space.</p>
</dd>
<dt><code>smbo_stop_convergent_iterations</code></dt><dd><p>(<em>optional</em>) The number of subsequent
convergent SMBO iterations required to stop hyperparameter optimisation
early. An iteration is convergent if the <em>best</em> parameter set has not
changed or the optimisation score over the 4 most recent iterations has not
changed beyond the tolerance level in <code>smbo_stop_tolerance</code>.
</p>
<p>The default value is <code>3</code>.</p>
</dd>
<dt><code>smbo_stop_tolerance</code></dt><dd><p>(<em>optional</em>) Tolerance for early stopping due to
convergent optimisation score.
</p>
<p>The default value depends on the square root of the number of samples (at
the series level), and is <code>0.01</code> for 100 samples. This value is computed as
<code>0.1 * 1 / sqrt(n_samples)</code>. The upper limit is <code>0.0001</code> for 1M or more
samples.</p>
</dd>
<dt><code>smbo_time_limit</code></dt><dd><p>(<em>optional</em>) Time limit (in minutes) for the
optimisation process. Optimisation is stopped after this limit is exceeded.
Time taken to determine variable importance for the optimisation process
(see the <code>optimisation_determine_vimp</code> parameter) does not count.
</p>
<p>The default is <code>NULL</code>, indicating that there is no time limit for the
optimisation process. The time limit cannot be less than 1 minute.</p>
</dd>
<dt><code>smbo_initial_bootstraps</code></dt><dd><p>(<em>optional</em>) The number of bootstraps taken
from the set of <code>optimisation_bootstraps</code> as the bootstraps assessed
initially.
</p>
<p>The default value is <code>1</code>. The value cannot be larger than
<code>optimisation_bootstraps</code>.</p>
</dd>
<dt><code>smbo_step_bootstraps</code></dt><dd><p>(<em>optional</em>) The number of bootstraps taken from
the set of <code>optimisation_bootstraps</code> bootstraps as the bootstraps assessed
during the steps of each intensify iteration.
</p>
<p>The default value is <code>3</code>. The value cannot be larger than
<code>optimisation_bootstraps</code>.</p>
</dd>
<dt><code>smbo_intensify_steps</code></dt><dd><p>(<em>optional</em>) The number of steps in each SMBO
intensify iteration. Each step a new set of <code>smbo_step_bootstraps</code>
bootstraps is drawn and used in the run-off between the incumbent <em>best</em>
hyperparameter combination and its challengers.
</p>
<p>The default value is <code>5</code>. Higher numbers allow for a more detailed
comparison, but this comes with added computational cost.</p>
</dd>
<dt><code>optimisation_metric</code></dt><dd><p>(<em>optional</em>) One or more metrics used to compute
performance scores. See the vignette on performance metrics for the
available metrics.
</p>
<p>If unset, the following metrics are used by default:
</p>

<ul>
<li> <p><code>auc_roc</code>: For <code>binomial</code> and <code>multinomial</code> models.
</p>
</li>
<li> <p><code>mse</code>: Mean squared error for <code>continuous</code> models.
</p>
</li>
<li> <p><code>msle</code>: Mean squared logarithmic error for <code>count</code> models.
</p>
</li>
<li> <p><code>concordance_index</code>: For <code>survival</code> models.
</p>
</li></ul>

<p>Multiple optimisation metrics can be specified. Actual metric values are
converted to an objective value by comparison with a baseline metric value
that derives from a trivial model, i.e. majority class for binomial and
multinomial outcomes, the median outcome for count and continuous outcomes
and a fixed risk or time for survival outcomes.</p>
</dd>
<dt><code>optimisation_function</code></dt><dd><p>(<em>optional</em>) Type of optimisation function used
to quantify the performance of a hyperparameter set. Model performance is
assessed using the metric(s) specified by <code>optimisation_metric</code> on the
in-bag (IB) and out-of-bag (OOB) samples of a bootstrap. These values are
converted to objective scores with a standardised interval of <code class="reqn">[-1.0,
  1.0]</code>. Each pair of objective is subsequently used to compute an
optimisation score. The optimisation score across different bootstraps is
than aggregated to a summary score. This summary score is used to rank
hyperparameter sets, and select the optimal set.
</p>
<p>The combination of optimisation score and summary score is determined by
the optimisation function indicated by this parameter:
</p>

<ul>
<li> <p><code>validation</code> or <code>max_validation</code> (default): seeks to maximise OOB score.
</p>
</li>
<li> <p><code>balanced</code>: seeks to balance IB and OOB score.
</p>
</li>
<li> <p><code>stronger_balance</code>: similar to <code>balanced</code>, but with stronger penalty for
differences between IB and OOB scores.
</p>
</li>
<li> <p><code>validation_minus_sd</code>: seeks to optimise the average OOB score minus its
standard deviation.
</p>
</li>
<li> <p><code>validation_25th_percentile</code>: seeks to optimise the 25th percentile of
OOB scores, and is conceptually similar to <code>validation_minus_sd</code>.
</p>
</li>
<li> <p><code>model_estimate</code>: seeks to maximise the OOB score estimate predicted by
the hyperparameter learner (not available for random search).
</p>
</li>
<li> <p><code>model_estimate_minus_sd</code>: seeks to maximise the OOB score estimate minus
its estimated standard deviation, as predicted by the hyperparameter
learner (not available for random search).
</p>
</li>
<li> <p><code>model_balanced_estimate</code>: seeks to maximise the estimate of the balanced
IB and OOB score. This is similar to the <code>balanced</code> score, and in fact uses
a hyperparameter learner to predict said score (not available for random
search).
</p>
</li>
<li> <p><code>model_balanced_estimate_minus_sd</code>: seeks to maximise the estimate of the
balanced IB and OOB score, minus its estimated standard deviation. This is
similar to the <code>balanced</code> score, but takes into account its estimated
spread.
</p>
</li></ul>

<p>Additional detail are provided in the <em>Learning algorithms and
hyperparameter optimisation</em> vignette.</p>
</dd>
<dt><code>hyperparameter_learner</code></dt><dd><p>(<em>optional</em>) Any point in the hyperparameter
space has a single, scalar, optimisation score value that is <em>a priori</em>
unknown. During the optimisation process, the algorithm samples from the
hyperparameter space by selecting hyperparameter sets and computing the
optimisation score value for one or more bootstraps. For each
hyperparameter set the resulting values are distributed around the actual
value. The learner indicated by <code>hyperparameter_learner</code> is then used to
infer optimisation score estimates for unsampled parts of the
hyperparameter space.
</p>
<p>The following models are available:
</p>

<ul>
<li> <p><code>bayesian_additive_regression_trees</code> or <code>bart</code>: Uses Bayesian Additive
Regression Trees (Sparapani et al., 2021) for inference. Unlike standard
random forests, BART allows for estimating posterior distributions directly
and can extrapolate.
</p>
</li>
<li> <p><code>gaussian_process</code> (default): Creates a localised approximate Gaussian
process for inference (Gramacy, 2016). This allows for better scaling than
deterministic Gaussian Processes.
</p>
</li>
<li> <p><code>random_forest</code>: Creates a random forest for inference. Originally
suggested by Hutter et al. (2011). A weakness of random forests is their
lack of extrapolation beyond observed values, which limits their usefulness
in exploiting promising areas of hyperparameter space.
</p>
</li>
<li> <p><code>random</code> or <code>random_search</code>: Forgoes the use of models to steer
optimisation. Instead, a random search is performed.
</p>
</li></ul>
</dd>
<dt><code>acquisition_function</code></dt><dd><p>(<em>optional</em>) The acquisition function influences
how new hyperparameter sets are selected. The algorithm uses the model
learned by the learner indicated by <code>hyperparameter_learner</code> to search the
hyperparameter space for hyperparameter sets that are either likely better
than the best known set (<em>exploitation</em>) or where there is considerable
uncertainty (<em>exploration</em>). The acquisition function quantifies this
(Shahriari et al., 2016).
</p>
<p>The following acquisition functions are available, and are described in
more detail in the <em>learner algorithms</em> vignette:
</p>

<ul>
<li> <p><code>improvement_probability</code>: The probability of improvement quantifies the
probability that the expected optimisation score for a set is better than
the best observed optimisation score
</p>
</li>
<li> <p><code>improvement_empirical_probability</code>: Similar to
<code>improvement_probability</code>, but based directly on optimisation scores
predicted by the individual decision trees.
</p>
</li>
<li> <p><code>expected_improvement</code> (default): Computes expected improvement.
</p>
</li>
<li> <p><code>upper_confidence_bound</code>: This acquisition function is based on the upper
confidence bound of the distribution (Srinivas et al., 2012).
</p>
</li>
<li> <p><code>bayes_upper_confidence_bound</code>: This acquisition function is based on the
upper confidence bound of the distribution (Kaufmann et al., 2012).
</p>
</li></ul>
</dd>
<dt><code>exploration_method</code></dt><dd><p>(<em>optional</em>) Method used to steer exploration in
post-initialisation intensive searching steps. As stated earlier, each SMBO
iteration step compares suggested alternative parameter sets with an
incumbent <strong>best</strong> set in a series of steps. The exploration method
controls how the set of alternative parameter sets is pruned after each
step in an iteration. Can be one of the following:
</p>

<ul>
<li> <p><code>single_shot</code> (default): The set of alternative parameter sets is not
pruned, and each intensification iteration contains only a single
intensification step that only uses a single bootstrap. This is the fastest
exploration method, but only superficially tests each parameter set.
</p>
</li>
<li> <p><code>successive_halving</code>: The set of alternative parameter sets is
pruned by removing the worst performing half of the sets after each step
(Jamieson and Talwalkar, 2016).
</p>
</li>
<li> <p><code>stochastic_reject</code>: The set of alternative parameter sets is pruned by
comparing the performance of each parameter set with that of the incumbent
<strong>best</strong> parameter set using a paired Wilcoxon test based on shared
bootstraps. Parameter sets that perform significantly worse, at an alpha
level indicated by <code>smbo_stochastic_reject_p_value</code>, are pruned.
</p>
</li>
<li> <p><code>none</code>: The set of alternative parameter sets is not pruned.
</p>
</li></ul>
</dd>
<dt><code>smbo_stochastic_reject_p_value</code></dt><dd><p>(<em>optional</em>) The p-value threshold used
for the <code>stochastic_reject</code> exploration method.
</p>
<p>The default value is <code>0.05</code>.</p>
</dd>
<dt><code>parallel_hyperparameter_optimisation</code></dt><dd><p>(<em>optional</em>) Enable parallel
processing for hyperparameter optimisation. Defaults to <code>TRUE</code>. When set to
<code>FALSE</code>, this will disable the use of parallel processing while performing
optimisation, regardless of the settings of the <code>parallel</code> parameter. The
parameter moreover specifies whether parallelisation takes place within the
optimisation algorithm (<code>inner</code>, default), or in an outer loop ( <code>outer</code>)
over learners, data subsamples, etc.
</p>
<p><code>parallel_hyperparameter_optimisation</code> is ignored if <code>parallel=FALSE</code>.</p>
</dd>
<dt><code>evaluate_top_level_only</code></dt><dd><p>(<em>optional</em>) Flag that signals that only
evaluation at the most global experiment level is required. Consider a
cross-validation experiment with additional external validation. The global
experiment level consists of data that are used for development, internal
validation and external validation. The next lower experiment level are the
individual cross-validation iterations.
</p>
<p>When the flag is <code>true</code>, evaluations take place on the global level only,
and no results are generated for the next lower experiment levels. In our
example, this means that results from individual cross-validation iterations
are not computed and shown. When the flag is <code>false</code>, results are computed
from both the global layer and the next lower level.
</p>
<p>Setting the flag to <code>true</code> saves computation time.</p>
</dd>
<dt><code>skip_evaluation_elements</code></dt><dd><p>(<em>optional</em>) Specifies which evaluation steps,
if any, should be skipped as part of the evaluation process. Defaults to
<code>none</code>, which means that all relevant evaluation steps are performed. It can
have one or more of the following values:
</p>

<ul>
<li> <p><code>none</code>, <code>false</code>: no steps are skipped.
</p>
</li>
<li> <p><code>all</code>, <code>true</code>: all steps are skipped.
</p>
</li>
<li> <p><code>auc_data</code>: data for assessing and plotting the area under the receiver
operating characteristic curve are not computed.
</p>
</li>
<li> <p><code>calibration_data</code>: data for assessing and plotting model calibration are
not computed.
</p>
</li>
<li> <p><code>calibration_info</code>: data required to assess calibration, such as baseline
survival curves, are not collected. These data will still be present in the
models.
</p>
</li>
<li> <p><code>confusion_matrix</code>: data for assessing and plotting a confusion matrix are
not collected.
</p>
</li>
<li> <p><code>decision_curve_analyis</code>: data for performing a decision curve analysis
are not computed.
</p>
</li>
<li> <p><code>feature_expressions</code>: data for assessing and plotting sample clustering
are not computed.
</p>
</li>
<li> <p><code>feature_similarity</code>: data for assessing and plotting feature clusters are
not computed.
</p>
</li>
<li> <p><code>fs_vimp</code>: data for assessing and plotting feature selection-based
variable importance are not collected.
</p>
</li>
<li> <p><code>hyperparameters</code>: data for assessing model hyperparameters are not
collected. These data will still be present in the models.
</p>
</li>
<li> <p><code>ice_data</code>: data for individual conditional expectation and partial
dependence plots are not created.
</p>
</li>
<li> <p><code>model_performance</code>: data for assessing and visualising model performance
are not created.
</p>
</li>
<li> <p><code>model_vimp</code>: data for assessing and plotting model-based variable
importance are not collected.
</p>
</li>
<li> <p><code>permutation_vimp</code>: data for assessing and plotting model-agnostic
permutation variable importance are not computed.
</p>
</li>
<li> <p><code>prediction_data</code>: predictions for each sample are not made and exported.
</p>
</li>
<li> <p><code>risk_stratification_data</code>: data for assessing and plotting Kaplan-Meier
survival curves are not collected.
</p>
</li>
<li> <p><code>risk_stratification_info</code>: data for assessing stratification into risk
groups are not computed.
</p>
</li>
<li> <p><code>univariate_analysis</code>: data for assessing and plotting univariate feature
importance are not computed.
</p>
</li></ul>
</dd>
<dt><code>ensemble_method</code></dt><dd><p>(<em>optional</em>) Method for ensembling predictions from
models for the same sample. Available methods are:
</p>

<ul>
<li> <p><code>median</code> (default): Use the median of the predicted values as the ensemble
value for a sample.
</p>
</li>
<li> <p><code>mean</code>: Use the mean of the predicted values as the ensemble value for a
sample.
</p>
</li></ul>

<p>This parameter is only used if <code>detail_level</code> is <code>ensemble</code>.</p>
</dd>
<dt><code>evaluation_metric</code></dt><dd><p>(<em>optional</em>) One or more metrics for assessing model
performance. See the vignette on performance metrics for the available
metrics.
</p>
<p>Confidence intervals (or rather credibility intervals) are computed for each
metric during evaluation. This is done using bootstraps, the number of which
depends on the value of <code>confidence_level</code> (Davison and Hinkley, 1997).
</p>
<p>If unset, the metric in the <code>optimisation_metric</code> variable is used.</p>
</dd>
<dt><code>sample_limit</code></dt><dd><p>(<em>optional</em>) Set the upper limit of the number of samples
that are used during evaluation steps. Cannot be less than 20.
</p>
<p>This setting can be specified per data element by providing a parameter
value in a named list with data elements, e.g.
<code>list("sample_similarity"=100, "permutation_vimp"=1000)</code>.
</p>
<p>This parameter can be set for the following data elements:
<code>sample_similarity</code> and <code>ice_data</code>.</p>
</dd>
<dt><code>detail_level</code></dt><dd><p>(<em>optional</em>) Sets the level at which results are computed
and aggregated.
</p>

<ul>
<li> <p><code>ensemble</code>: Results are computed at the ensemble level, i.e. over all
models in the ensemble. This means that, for example, bias-corrected
estimates of model performance are assessed by creating (at least) 20
bootstraps and computing the model performance of the ensemble model for
each bootstrap.
</p>
</li>
<li> <p><code>hybrid</code> (default): Results are computed at the level of models in an
ensemble. This means that, for example, bias-corrected estimates of model
performance are directly computed using the models in the ensemble. If there
are at least 20 trained models in the ensemble, performance is computed for
each model, in contrast to <code>ensemble</code> where performance is computed for the
ensemble of models. If there are less than 20 trained models in the
ensemble, bootstraps are created so that at least 20 point estimates can be
made.
</p>
</li>
<li> <p><code>model</code>: Results are computed at the model level. This means that, for
example, bias-corrected estimates of model performance are assessed by
creating (at least) 20 bootstraps and computing the performance of the model
for each bootstrap.
</p>
</li></ul>

<p>Note that each level of detail has a different interpretation for bootstrap
confidence intervals. For <code>ensemble</code> and <code>model</code> these are the confidence
intervals for the ensemble and an individual model, respectively. That is,
the confidence interval describes the range where an estimate produced by a
respective ensemble or model trained on a repeat of the experiment may be
found with the probability of the confidence level. For <code>hybrid</code>, it
represents the range where any single model trained on a repeat of the
experiment may be found with the probability of the confidence level. By
definition, confidence intervals obtained using <code>hybrid</code> are at least as
wide as those for <code>ensemble</code>. <code>hybrid</code> offers the correct interpretation if
the goal of the analysis is to assess the result of a single, unspecified,
model.
</p>
<p><code>hybrid</code> is generally computationally less expensive then <code>ensemble</code>, which
in turn is somewhat less expensive than <code>model</code>.
</p>
<p>A non-default <code>detail_level</code> parameter can be specified for separate
evaluation steps by providing a parameter value in a named list with data
elements, e.g. <code>list("auc_data"="ensemble", "model_performance"="hybrid")</code>.
This parameter can be set for the following data elements: <code>auc_data</code>,
<code>decision_curve_analyis</code>, <code>model_performance</code>, <code>permutation_vimp</code>,
<code>ice_data</code>, <code>prediction_data</code> and <code>confusion_matrix</code>.</p>
</dd>
<dt><code>estimation_type</code></dt><dd><p>(<em>optional</em>) Sets the type of estimation that should be
possible. This has the following options:
</p>

<ul>
<li> <p><code>point</code>: Point estimates.
</p>
</li>
<li> <p><code>bias_correction</code> or <code>bc</code>: Bias-corrected estimates. A bias-corrected
estimate is computed from (at least) 20 point estimates, and <code>familiar</code> may
bootstrap the data to create them.
</p>
</li>
<li> <p><code>bootstrap_confidence_interval</code> or <code>bci</code> (default): Bias-corrected
estimates with bootstrap confidence intervals (Efron and Hastie, 2016). The
number of point estimates required depends on the <code>confidence_level</code>
parameter, and <code>familiar</code> may bootstrap the data to create them.
</p>
</li></ul>

<p>As with <code>detail_level</code>, a non-default <code>estimation_type</code> parameter can be
specified for separate evaluation steps by providing a parameter value in a
named list with data elements, e.g. <code>list("auc_data"="bci", "model_performance"="point")</code>. This parameter can be set for the following
data elements: <code>auc_data</code>, <code>decision_curve_analyis</code>, <code>model_performance</code>,
<code>permutation_vimp</code>, <code>ice_data</code>, and <code>prediction_data</code>.</p>
</dd>
<dt><code>aggregate_results</code></dt><dd><p>(<em>optional</em>) Flag that signifies whether results
should be aggregated during evaluation. If <code>estimation_type</code> is
<code>bias_correction</code> or <code>bc</code>, aggregation leads to a single bias-corrected
estimate. If <code>estimation_type</code> is <code>bootstrap_confidence_interval</code> or <code>bci</code>,
aggregation leads to a single bias-corrected estimate with lower and upper
boundaries of the confidence interval. This has no effect if
<code>estimation_type</code> is <code>point</code>.
</p>
<p>The default value is equal to <code>TRUE</code> except when assessing metrics to assess
model performance, as the default violin plot requires underlying data.
</p>
<p>As with <code>detail_level</code> and <code>estimation_type</code>, a non-default
<code>aggregate_results</code> parameter can be specified for separate evaluation steps
by providing a parameter value in a named list with data elements, e.g.
<code>list("auc_data"=TRUE, , "model_performance"=FALSE)</code>. This parameter exists
for the same elements as <code>estimation_type</code>.</p>
</dd>
<dt><code>confidence_level</code></dt><dd><p>(<em>optional</em>) Numeric value for the level at which
confidence intervals are determined. In the case bootstraps are used to
determine the confidence intervals bootstrap estimation, <code>familiar</code> uses the
rule of thumb <code class="reqn">n = 20 / ci.level</code> to determine the number of required
bootstraps.
</p>
<p>The default value is <code>0.95</code>.</p>
</dd>
<dt><code>bootstrap_ci_method</code></dt><dd><p>(<em>optional</em>) Method used to determine bootstrap
confidence intervals (Efron and Hastie, 2016). The following methods are
implemented:
</p>

<ul>
<li> <p><code>percentile</code> (default): Confidence intervals obtained using the percentile
method.
</p>
</li>
<li> <p><code>bc</code>: Bias-corrected confidence intervals.
</p>
</li></ul>

<p>Note that the standard method is not implemented because this method is
often not suitable due to non-normal distributions. The bias-corrected and
accelerated (BCa) method is not implemented yet.</p>
</dd>
<dt><code>feature_cluster_method</code></dt><dd><p>(<em>optional</em>) Method used to perform clustering
of features. The same methods as for the <code>cluster_method</code> configuration
parameter are available: <code>none</code>, <code>hclust</code>, <code>agnes</code>, <code>diana</code> and <code>pam</code>.
</p>
<p>The value for the <code>cluster_method</code> configuration parameter is used by
default. When generating clusters for the purpose of determining mutual
correlation and ordering feature expressions, <code>none</code> is ignored and <code>hclust</code>
is used instead.</p>
</dd>
<dt><code>feature_linkage_method</code></dt><dd><p>(<em>optional</em>) Method used for agglomerative
clustering with <code>hclust</code> and <code>agnes</code>. Linkage determines how features are
sequentially combined into clusters based on distance. The methods are
shared with the <code>cluster_linkage_method</code> configuration parameter: <code>average</code>,
<code>single</code>, <code>complete</code>, <code>weighted</code>, and <code>ward</code>.
</p>
<p>The value for the <code>cluster_linkage_method</code> configuration parameters is used
by default.</p>
</dd>
<dt><code>feature_cluster_cut_method</code></dt><dd><p>(<em>optional</em>) Method used to divide features
into separate clusters. The available methods are the same as for the
<code>cluster_cut_method</code> configuration parameter: <code>silhouette</code>, <code>fixed_cut</code> and
<code>dynamic_cut</code>.
</p>
<p><code>silhouette</code> is available for all cluster methods, but <code>fixed_cut</code> only
applies to methods that create hierarchical trees (<code>hclust</code>, <code>agnes</code> and
<code>diana</code>). <code>dynamic_cut</code> requires the <code>dynamicTreeCut</code> package and can only
be used with <code>agnes</code> and <code>hclust</code>.
</p>
<p>The value for the <code>cluster_cut_method</code> configuration parameter is used by
default.</p>
</dd>
<dt><code>feature_similarity_metric</code></dt><dd><p>(<em>optional</em>) Metric to determine pairwise
similarity between features. Similarity is computed in the same manner as
for clustering, and <code>feature_similarity_metric</code> therefore has the same
options as <code>cluster_similarity_metric</code>: <code>mcfadden_r2</code>, <code>cox_snell_r2</code>,
<code>nagelkerke_r2</code>, <code>mutual_information</code>, <code>spearman</code>, <code>kendall</code> and <code>pearson</code>.
</p>
<p>The value used for the <code>cluster_similarity_metric</code> configuration parameter
is used by default.</p>
</dd>
<dt><code>feature_similarity_threshold</code></dt><dd><p>(<em>optional</em>) The threshold level for
pair-wise similarity that is required to form feature clusters with the
<code>fixed_cut</code> method. This threshold functions in the same manner as the one
defined using the <code>cluster_similarity_threshold</code> parameter.
</p>
<p>By default, the value for the <code>cluster_similarity_threshold</code> configuration
parameter is used.
</p>
<p>Unlike for <code>cluster_similarity_threshold</code>, more than one value can be
supplied here.</p>
</dd>
<dt><code>sample_cluster_method</code></dt><dd><p>(<em>optional</em>) The method used to perform
clustering based on distance between samples. These are the same methods as
for the <code>cluster_method</code> configuration parameter: <code>hclust</code>, <code>agnes</code>, <code>diana</code>
and <code>pam</code>.
</p>
<p>The value for the <code>cluster_method</code> configuration parameter is used by
default. When generating clusters for the purpose of ordering samples in
feature expressions, <code>none</code> is ignored and <code>hclust</code> is used instead.</p>
</dd>
<dt><code>sample_linkage_method</code></dt><dd><p>(<em>optional</em>) The method used for agglomerative
clustering in <code>hclust</code> and <code>agnes</code>. These are the same methods as for the
<code>cluster_linkage_method</code> configuration parameter: <code>average</code>, <code>single</code>,
<code>complete</code>, <code>weighted</code>, and <code>ward</code>.
</p>
<p>The value for the <code>cluster_linkage_method</code> configuration parameters is used
by default.</p>
</dd>
<dt><code>sample_similarity_metric</code></dt><dd><p>(<em>optional</em>) Metric to determine pairwise
similarity between samples. Similarity is computed in the same manner as for
clustering, but <code>sample_similarity_metric</code> has different options that are
better suited to computing distance between samples instead of between
features. The following metrics are available.
</p>

<ul>
<li> <p><code>gower</code> (default): compute Gower's distance between samples. By default,
Gower's distance is computed based on winsorised data to reduce the effect
of outliers (see below).
</p>
</li>
<li> <p><code>euclidean</code>: compute the Euclidean distance between samples.
</p>
</li></ul>

<p>The underlying feature data for numerical features is scaled to the
<code class="reqn">[0,1]</code> range using the feature values across the samples. The
normalisation parameters required can optionally be computed from feature
data with the outer 5% (on both sides) of feature values trimmed or
winsorised. To do so append <code style="white-space: pre;">&#8288;_trim&#8288;</code> (trimming) or <code style="white-space: pre;">&#8288;_winsor&#8288;</code> (winsorising) to
the metric name. This reduces the effect of outliers somewhat.
</p>
<p>Regardless of metric, all categorical features are handled as for the
Gower's distance: distance is 0 if the values in a pair of samples match,
and 1 if they do not.</p>
</dd>
<dt><code>eval_aggregation_method</code></dt><dd><p>(<em>optional</em>) Method for aggregating variable
importances for the purpose of evaluation. Variable importances are
determined during feature selection steps and after training the model. Both
types are evaluated, but feature selection variable importance is only
evaluated at run-time.
</p>
<p>See the documentation for the <code>vimp_aggregation_method</code> argument for
information concerning the different methods available.</p>
</dd>
<dt><code>eval_aggregation_rank_threshold</code></dt><dd><p>(<em>optional</em>) The threshold used to
define the subset of highly important features during evaluation.
</p>
<p>See the documentation for the <code>vimp_aggregation_rank_threshold</code> argument for
more information.</p>
</dd>
<dt><code>eval_icc_type</code></dt><dd><p>(<em>optional</em>) String indicating the type of intraclass
correlation coefficient (<code>1</code>, <code>2</code> or <code>3</code>) that should be used to compute
robustness for features in repeated measurements during the evaluation of
univariate importance. These types correspond to the types in Shrout and
Fleiss (1979). The default value is <code>1</code>.</p>
</dd>
<dt><code>stratification_method</code></dt><dd><p>(<em>optional</em>) Method for determining the
stratification threshold for creating survival groups. The actual,
model-dependent, threshold value is obtained from the development data, and
can afterwards be used to perform stratification on validation data.
</p>
<p>The following stratification methods are available:
</p>

<ul>
<li> <p><code>median</code> (default): The median predicted value in the development cohort
is used to stratify the samples into two risk groups. For predicted outcome
values that build a continuous spectrum, the two risk groups in the
development cohort will be roughly equal in size.
</p>
</li>
<li> <p><code>mean</code>: The mean predicted value in the development cohort is used to
stratify the samples into two risk groups.
</p>
</li>
<li> <p><code>mean_trim</code>: As <code>mean</code>, but based on the set of predicted values
where the 5% lowest and 5% highest values are discarded. This reduces the
effect of outliers.
</p>
</li>
<li> <p><code>mean_winsor</code>: As <code>mean</code>, but based on the set of predicted values where
the 5% lowest and 5% highest values are winsorised. This reduces the effect
of outliers.
</p>
</li>
<li> <p><code>fixed</code>: Samples are stratified based on the sample quantiles of the
predicted values. These quantiles are defined using the
<code>stratification_threshold</code> parameter.
</p>
</li>
<li> <p><code>optimised</code>: Use maximally selected rank statistics to determine the
optimal threshold (Lausen and Schumacher, 1992; Hothorn et al., 2003) to
stratify samples into two optimally separated risk groups.
</p>
</li></ul>

<p>One or more stratification methods can be selected simultaneously.
</p>
<p>This parameter is only relevant for <code>survival</code> outcomes.</p>
</dd>
<dt><code>stratification_threshold</code></dt><dd><p>(<em>optional</em>) Numeric value(s) signifying the
sample quantiles for stratification using the <code>fixed</code> method. The number of
risk groups will be the number of values +1.
</p>
<p>The default value is <code>c(1/3, 2/3)</code>, which will yield two thresholds that
divide samples into three equally sized groups. If <code>fixed</code> is not among the
selected stratification methods, this parameter is ignored.
</p>
<p>This parameter is only relevant for <code>survival</code> outcomes.</p>
</dd>
<dt><code>time_max</code></dt><dd><p>(<em>optional</em>) Time point which is used as the benchmark for
e.g. cumulative risks generated by random forest, or the cutoff for Uno's
concordance index.
</p>
<p>If <code>time_max</code> is not provided, but <code>evaluation_times</code> is, the largest value
of <code>evaluation_times</code> is used. If both are not provided, <code>time_max</code> is set
to the 98th percentile of the distribution of survival times for samples
with an event in the development data set.
</p>
<p>This parameter is only relevant for <code>survival</code> outcomes.</p>
</dd>
<dt><code>evaluation_times</code></dt><dd><p>(<em>optional</em>) One or more time points that are used for
assessing calibration in survival problems. This is done as expected and
observed survival probabilities depend on time.
</p>
<p>If unset, <code>evaluation_times</code> will be equal to <code>time_max</code>.
</p>
<p>This parameter is only relevant for <code>survival</code> outcomes.</p>
</dd>
<dt><code>dynamic_model_loading</code></dt><dd><p>(<em>optional</em>) Enables dynamic loading of models
during the evaluation process, if <code>TRUE</code>. Defaults to <code>FALSE</code>. Dynamic
loading of models may reduce the overall memory footprint, at the cost of
increased disk or network IO. Models can only be dynamically loaded if they
are found at an accessible disk or network location. Setting this parameter
to <code>TRUE</code> may help if parallel processing causes out-of-memory issues during
evaluation.</p>
</dd>
<dt><code>parallel_evaluation</code></dt><dd><p>(<em>optional</em>) Enable parallel processing for
hyperparameter optimisation. Defaults to <code>TRUE</code>. When set to <code>FALSE</code>, this
will disable the use of parallel processing while performing optimisation,
regardless of the settings of the <code>parallel</code> parameter. The parameter
moreover specifies whether parallelisation takes place within the evaluation
process steps (<code>inner</code>, default), or in an outer loop ( <code>outer</code>) over
learners, data subsamples, etc.
</p>
<p><code>parallel_evaluation</code> is ignored if <code>parallel=FALSE</code>.</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Value</h3>

<p>Nothing. All output is written to the experiment directory. If the
experiment directory is in a temporary location, a list with all
familiarModel, familiarEnsemble, familiarData and familiarCollection
objects will be returned.
</p>


<h3>References</h3>


<ol>
<li><p> Storey, J. D. A direct approach to false discovery rates. J.
R. Stat. Soc. Series B Stat. Methodol. 64, 479–498 (2002).
</p>
</li>
<li><p> Shrout, P. E. &amp; Fleiss, J. L. Intraclass correlations: uses in assessing
rater reliability. Psychol. Bull. 86, 420–428 (1979).
</p>
</li>
<li><p> Koo, T. K. &amp; Li, M. Y. A guideline of selecting and reporting intraclass
correlation coefficients for reliability research. J. Chiropr. Med. 15,
155–163 (2016).
</p>
</li>
<li><p> Yeo, I. &amp; Johnson, R. A. A new family of power transformations to
improve normality or symmetry. Biometrika 87, 954–959 (2000).
</p>
</li>
<li><p> Box, G. E. P. &amp; Cox, D. R. An analysis of transformations. J. R. Stat.
Soc. Series B Stat. Methodol. 26, 211–252 (1964).
</p>
</li>
<li><p> Raymaekers, J., Rousseeuw,  P. J. Transforming variables to central
normality. Mach Learn. (2021).
</p>
</li>
<li><p> Park, M. Y., Hastie, T. &amp; Tibshirani, R. Averaged gene expressions for
regression. Biostatistics 8, 212–227 (2007).
</p>
</li>
<li><p> Tolosi, L. &amp; Lengauer, T. Classification with correlated features:
unreliability of feature ranking and solutions. Bioinformatics 27,
1986–1994 (2011).
</p>
</li>
<li><p> Johnson, W. E., Li, C. &amp; Rabinovic, A. Adjusting batch effects in
microarray expression data using empirical Bayes methods. Biostatistics 8,
118–127 (2007)
</p>
</li>
<li><p> Kaufman, L. &amp; Rousseeuw, P. J. Finding groups in data: an introduction
to cluster analysis. (John Wiley &amp; Sons, 2009).
</p>
</li>
<li><p> Muellner, D. fastcluster: fast hierarchical, agglomerative clustering
routines for R and Python. J. Stat. Softw. 53, 1–18 (2013).
</p>
</li>
<li><p> Rousseeuw, P. J. Silhouettes: A graphical aid to the interpretation and
validation of cluster analysis. J. Comput. Appl. Math. 20, 53–65 (1987).
</p>
</li>
<li><p> Langfelder, P., Zhang, B. &amp; Horvath, S. Defining clusters from a
hierarchical cluster tree: the Dynamic Tree Cut package for R.
Bioinformatics 24, 719–720 (2008).
</p>
</li>
<li><p> McFadden, D. Conditional logit analysis of qualitative choice behavior.
in Frontiers in Econometrics (ed. Zarembka, P.) 105–142 (Academic Press,
1974).
</p>
</li>
<li><p> Cox, D. R. &amp; Snell, E. J. Analysis of binary data. (Chapman and Hall,
1989).
</p>
</li>
<li><p> Nagelkerke, N. J. D. A note on a general definition of the coefficient
of determination. Biometrika 78, 691–692 (1991).
</p>
</li>
<li><p> Meinshausen, N. &amp; Buehlmann, P. Stability selection. J. R. Stat. Soc.
Series B Stat. Methodol. 72, 417–473 (2010).
</p>
</li>
<li><p> Haury, A.-C., Gestraud, P. &amp; Vert, J.-P. The influence of feature
selection methods on accuracy, stability and interpretability of molecular
signatures. PLoS One 6, e28210 (2011).
</p>
</li>
<li><p> Wald, R., Khoshgoftaar, T. M., Dittman, D., Awada, W. &amp; Napolitano,A. An
extensive comparison of feature ranking aggregation techniques in
bioinformatics. in 2012 IEEE 13th International Conference on Information
Reuse Integration (IRI) 377–384 (2012).
</p>
</li>
<li><p> Hutter, F., Hoos, H. H. &amp; Leyton-Brown, K. Sequential model-based
optimization for general algorithm configuration. in Learning and
Intelligent Optimization (ed. Coello, C. A. C.) 6683, 507–523 (Springer
Berlin Heidelberg, 2011).
</p>
</li>
<li><p> Shahriari, B., Swersky, K., Wang, Z., Adams, R. P. &amp; de Freitas, N.
Taking the Human Out of the Loop: A Review of Bayesian Optimization. Proc.
IEEE 104, 148–175 (2016)
</p>
</li>
<li><p> Srinivas, N., Krause, A., Kakade, S. M. &amp; Seeger, M. W.
Information-Theoretic Regret Bounds for Gaussian Process Optimization in
the Bandit Setting. IEEE Trans. Inf. Theory 58, 3250–3265 (2012)
</p>
</li>
<li><p> Kaufmann, E., Cappé, O. &amp; Garivier, A. On Bayesian upper confidence
bounds for bandit problems. in Artificial intelligence and statistics
592–600 (2012).
</p>
</li>
<li><p> Jamieson, K. &amp; Talwalkar, A. Non-stochastic Best Arm Identification and
Hyperparameter Optimization. in Proceedings of the 19th International
Conference on Artificial Intelligence and Statistics (eds. Gretton, A. &amp;
Robert, C. C.) vol. 51 240–248 (PMLR, 2016).
</p>
</li>
<li><p> Gramacy, R. B. laGP: Large-Scale Spatial Modeling via Local Approximate
Gaussian Processes in R. Journal of Statistical Software 72, 1–46 (2016)
</p>
</li>
<li><p> Sparapani, R., Spanbauer, C. &amp; McCulloch, R. Nonparametric Machine
Learning and Efficient Computation with Bayesian Additive Regression Trees:
The BART R Package. Journal of Statistical Software 97, 1–66 (2021)
</p>
</li>
<li><p> Davison, A. C. &amp; Hinkley, D. V. Bootstrap methods and their application.
(Cambridge University Press, 1997).
</p>
</li>
<li><p> Efron, B. &amp; Hastie, T. Computer Age Statistical Inference. (Cambridge
University Press, 2016).
</p>
</li>
<li><p> Lausen, B. &amp; Schumacher, M. Maximally Selected Rank Statistics.
Biometrics 48, 73 (1992).
</p>
</li>
<li><p> Hothorn, T. &amp; Lausen, B. On the exact distribution of maximally selected
rank statistics. Comput. Stat. Data Anal. 43, 121–137 (2003).
</p>
</li></ol>


<hr>
<h2 id='theme_familiar'>Familiar ggplot2 theme</h2><span id='topic+theme_familiar'></span>

<h3>Description</h3>

<p>This is the default theme used for plots created by familiar. The theme uses
<code>ggplot2::theme_light</code> as the base template.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>theme_familiar(
  base_size = 11,
  base_family = "",
  base_line_size = 0.5,
  base_rect_size = 0.5
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="theme_familiar_+3A_base_size">base_size</code></td>
<td>
<p>Base font size in points. Size of other plot text elements
is based off this.</p>
</td></tr>
<tr><td><code id="theme_familiar_+3A_base_family">base_family</code></td>
<td>
<p>Font family used for text elements.</p>
</td></tr>
<tr><td><code id="theme_familiar_+3A_base_line_size">base_line_size</code></td>
<td>
<p>Base size for line elements, in points.</p>
</td></tr>
<tr><td><code id="theme_familiar_+3A_base_rect_size">base_rect_size</code></td>
<td>
<p>Base size for rectangular elements, in points.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A complete plotting theme.
</p>

<hr>
<h2 id='train_familiar'>Create models using end-to-end machine learning</h2><span id='topic+train_familiar'></span>

<h3>Description</h3>

<p>Train models using familiar. Evaluation is not performed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>train_familiar(
  formula = NULL,
  data = NULL,
  experiment_data = NULL,
  cl = NULL,
  experimental_design = "fs+mb",
  learner = NULL,
  hyperparameter = NULL,
  verbose = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="train_familiar_+3A_formula">formula</code></td>
<td>
<p>An R formula. The formula can only contain feature names and
dot (<code>.</code>). The <code>*</code> and <code>+1</code> operators are not supported as these refer to
columns that are not present in the data set.
</p>
<p>Use of the formula interface is optional.</p>
</td></tr>
<tr><td><code id="train_familiar_+3A_data">data</code></td>
<td>
<p>A <code>data.table</code> object, a <code>data.frame</code> object, list containing
multiple <code>data.table</code> or <code>data.frame</code> objects, or paths to data files.
</p>
<p><code>data</code> should be provided if no file paths are provided to the <code>data_files</code>
argument. If both are provided, only <code>data</code> will be used.
</p>
<p>All data is expected to be in wide format, and ideally has a sample
identifier (see <code>sample_id_column</code>), batch identifier (see <code>cohort_column</code>)
and outcome columns (see <code>outcome_column</code>).
</p>
<p>In case paths are provided, the data should be stored as <code>csv</code>, <code>rds</code> or
<code>RData</code> files. See documentation for the <code>data_files</code> argument for more
information.</p>
</td></tr>
<tr><td><code id="train_familiar_+3A_experiment_data">experiment_data</code></td>
<td>
<p>Experimental data may provided in the form of</p>
</td></tr>
<tr><td><code id="train_familiar_+3A_cl">cl</code></td>
<td>
<p>Cluster created using the <code>parallel</code> package. This cluster is then
used to speed up computation through parallelisation. When a cluster is not
provided, parallelisation is performed by setting up a cluster on the local
machine.
</p>
<p>This parameter has no effect if the <code>parallel</code> argument is set to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="train_familiar_+3A_experimental_design">experimental_design</code></td>
<td>
<p>(<strong>required</strong>) Defines what the experiment looks
like, e.g. <code>cv(bt(fs,20)+mb,3,2)</code> for 2 times repeated 3-fold
cross-validation with nested feature selection on 20 bootstraps and
model-building. The basic workflow components are:
</p>

<ul>
<li> <p><code>fs</code>: (required) feature selection step.
</p>
</li>
<li> <p><code>mb</code>: (required) model building step.
</p>
</li>
<li> <p><code>ev</code>: (optional) external validation. Setting this is not required for
<code>train_familiar</code>, but if validation batches or cohorts are present in the
dataset (<code>data</code>), these should be indicated in the <code>validation_batch_id</code>
argument.
</p>
</li></ul>

<p>The different components are linked using <code>+</code>.
</p>
<p>Different subsampling methods can be used in conjunction with the basic
workflow components:
</p>

<ul>
<li> <p><code>bs(x,n)</code>: (stratified) .632 bootstrap, with <code>n</code> the number of
bootstraps. In contrast to <code>bt</code>, feature pre-processing parameters and
hyperparameter optimisation are conducted on individual bootstraps.
</p>
</li>
<li> <p><code>bt(x,n)</code>: (stratified) .632 bootstrap, with <code>n</code> the number of
bootstraps. Unlike <code>bs</code> and other subsampling methods, no separate
pre-processing parameters or optimised hyperparameters will be determined
for each bootstrap.
</p>
</li>
<li> <p><code>cv(x,n,p)</code>: (stratified) <code>n</code>-fold cross-validation, repeated <code>p</code> times.
Pre-processing parameters are determined for each iteration.
</p>
</li>
<li> <p><code>lv(x)</code>: leave-one-out-cross-validation. Pre-processing parameters are
determined for each iteration.
</p>
</li>
<li> <p><code>ip(x)</code>: imbalance partitioning for addressing class imbalances on the
data set. Pre-processing parameters are determined for each partition. The
number of partitions generated depends on the imbalance correction method
(see the <code>imbalance_correction_method</code> parameter).
</p>
</li></ul>

<p>As shown in the example above, sampling algorithms can be nested.
</p>
<p>The simplest valid experimental design is <code>fs+mb</code>. This is the default in
<code>train_familiar</code>, and will create one model for each feature selection
method in <code>fs_method</code>. To create more models, a subsampling method should
be introduced, e.g. <code>bs(fs+mb,20)</code> to create 20 models based on bootstraps
of the data.
</p>
<p>This argument is ignored if the <code>experiment_data</code> argument is set.</p>
</td></tr>
<tr><td><code id="train_familiar_+3A_learner">learner</code></td>
<td>
<p>(<strong>required</strong>) Name of the learner used to develop a model. A
sizeable number learners is supported in <code>familiar</code>. Please see the
vignette on learners for more information concerning the available
learners. Unlike the <code>summon_familiar</code> function, <code>train_familiar</code> only
allows for a single learner.</p>
</td></tr>
<tr><td><code id="train_familiar_+3A_hyperparameter">hyperparameter</code></td>
<td>
<p>(<em>optional</em>) List, or nested list containing
hyperparameters for learners. If a nested list is provided, each sublist
should have the name of the learner method it corresponds to, with list
elements being named after the intended hyperparameter, e.g.
<code>"glm_logistic"=list("sign_size"=3)</code>
</p>
<p>All learners have hyperparameters. Please refer to the vignette on learners
for more details. If no parameters are provided, sequential model-based
optimisation is used to determine optimal hyperparameters.
</p>
<p>Hyperparameters provided by the user are never optimised. However, if more
than one value is provided for a single hyperparameter, optimisation will
be conducted using these values.</p>
</td></tr>
<tr><td><code id="train_familiar_+3A_verbose">verbose</code></td>
<td>
<p>Indicates verbosity of the results. Default is TRUE, and all
messages and warnings are returned.</p>
</td></tr>
<tr><td><code id="train_familiar_+3A_...">...</code></td>
<td>

<p>Arguments passed on to <code><a href="#topic+.parse_experiment_settings">.parse_experiment_settings</a></code>, <code><a href="#topic+.parse_setup_settings">.parse_setup_settings</a></code>, <code><a href="#topic+.parse_preprocessing_settings">.parse_preprocessing_settings</a></code>, <code><a href="#topic+.parse_feature_selection_settings">.parse_feature_selection_settings</a></code>, <code><a href="#topic+.parse_model_development_settings">.parse_model_development_settings</a></code>, <code><a href="#topic+.parse_hyperparameter_optimisation_settings">.parse_hyperparameter_optimisation_settings</a></code>
</p>

<dl>
<dt><code>batch_id_column</code></dt><dd><p>(<strong>recommended</strong>) Name of the column containing batch
or cohort identifiers. This parameter is required if more than one dataset
is provided, or if external validation is performed.
</p>
<p>In familiar any row of data is organised by four identifiers:
</p>

<ul>
<li><p> The batch identifier <code>batch_id_column</code>: This denotes the group to which a
set of samples belongs, e.g. patients from a single study, samples measured
in a batch, etc. The batch identifier is used for batch normalisation, as
well as selection of development and validation datasets.
</p>
</li>
<li><p> The sample identifier <code>sample_id_column</code>: This denotes the sample level,
e.g. data from a single individual. Subsets of data, e.g. bootstraps or
cross-validation folds, are created at this level.
</p>
</li>
<li><p> The series identifier <code>series_id_column</code>: Indicates measurements on a
single sample that may not share the same outcome value, e.g. a time
series, or the number of cells in a view.
</p>
</li>
<li><p> The repetition identifier: Indicates repeated measurements in a single
series where any feature values may differ, but the outcome does not.
Repetition identifiers are always implicitly set when multiple entries for
the same series of the same sample in the same batch that share the same
outcome are encountered.
</p>
</li></ul>
</dd>
<dt><code>sample_id_column</code></dt><dd><p>(<strong>recommended</strong>) Name of the column containing
sample or subject identifiers. See <code>batch_id_column</code> above for more
details.
</p>
<p>If unset, every row will be identified as a single sample.</p>
</dd>
<dt><code>series_id_column</code></dt><dd><p>(<strong>optional</strong>) Name of the column containing series
identifiers, which distinguish between measurements that are part of a
series for a single sample. See <code>batch_id_column</code> above for more details.
</p>
<p>If unset, rows which share the same batch and sample identifiers but have a
different outcome are assigned unique series identifiers.</p>
</dd>
<dt><code>development_batch_id</code></dt><dd><p>(<em>optional</em>) One or more batch or cohort
identifiers to constitute data sets for development. Defaults to all, or
all minus the identifiers in <code>validation_batch_id</code> for external validation.
Required if external validation is performed and <code>validation_batch_id</code> is
not provided.</p>
</dd>
<dt><code>validation_batch_id</code></dt><dd><p>(<em>optional</em>) One or more batch or cohort
identifiers to constitute data sets for external validation. Defaults to
all data sets except those in <code>development_batch_id</code> for external
validation, or none if not. Required if <code>development_batch_id</code> is not
provided.</p>
</dd>
<dt><code>outcome_name</code></dt><dd><p>(<em>optional</em>) Name of the modelled outcome. This name will
be used in figures created by <code>familiar</code>.
</p>
<p>If not set, the column name in <code>outcome_column</code> will be used for
<code>binomial</code>, <code>multinomial</code>, <code>count</code> and <code>continuous</code> outcomes. For other
outcomes (<code>survival</code> and <code>competing_risk</code>) no default is used.</p>
</dd>
<dt><code>outcome_column</code></dt><dd><p>(<strong>recommended</strong>) Name of the column containing the
outcome of interest. May be identified from a formula, if a formula is
provided as an argument. Otherwise an error is raised. Note that <code>survival</code>
and <code>competing_risk</code> outcome type outcomes require two columns that
indicate the time-to-event or the time of last follow-up and the event
status.</p>
</dd>
<dt><code>outcome_type</code></dt><dd><p>(<strong>recommended</strong>) Type of outcome found in the outcome
column. The outcome type determines many aspects of the overall process,
e.g. the available feature selection methods and learners, but also the
type of assessments that can be conducted to evaluate the resulting models.
Implemented outcome types are:
</p>

<ul>
<li> <p><code>binomial</code>: categorical outcome with 2 levels.
</p>
</li>
<li> <p><code>multinomial</code>: categorical outcome with 2 or more levels.
</p>
</li>
<li> <p><code>count</code>: Poisson-distributed numeric outcomes.
</p>
</li>
<li> <p><code>continuous</code>: general continuous numeric outcomes.
</p>
</li>
<li> <p><code>survival</code>: survival outcome for time-to-event data.
</p>
</li></ul>

<p>If not provided, the algorithm will attempt to obtain outcome_type from
contents of the outcome column. This may lead to unexpected results, and we
therefore advise to provide this information manually.
</p>
<p>Note that <code>competing_risk</code> survival analysis are not fully supported, and
is currently not a valid choice for <code>outcome_type</code>.</p>
</dd>
<dt><code>class_levels</code></dt><dd><p>(<em>optional</em>) Class levels for <code>binomial</code> or <code>multinomial</code>
outcomes. This argument can be used to specify the ordering of levels for
categorical outcomes. These class levels must exactly match the levels
present in the outcome column.</p>
</dd>
<dt><code>event_indicator</code></dt><dd><p>(<strong>recommended</strong>) Indicator for events in <code>survival</code>
and <code>competing_risk</code> analyses. <code>familiar</code> will automatically recognise <code>1</code>,
<code>true</code>, <code>t</code>, <code>y</code> and <code>yes</code> as event indicators, including different
capitalisations. If this parameter is set, it replaces the default values.</p>
</dd>
<dt><code>censoring_indicator</code></dt><dd><p>(<strong>recommended</strong>) Indicator for right-censoring in
<code>survival</code> and <code>competing_risk</code> analyses. <code>familiar</code> will automatically
recognise <code>0</code>, <code>false</code>, <code>f</code>, <code>n</code>, <code>no</code> as censoring indicators, including
different capitalisations. If this parameter is set, it replaces the
default values.</p>
</dd>
<dt><code>competing_risk_indicator</code></dt><dd><p>(<strong>recommended</strong>) Indicator for competing
risks in <code>competing_risk</code> analyses. There are no default values, and if
unset, all values other than those specified by the <code>event_indicator</code> and
<code>censoring_indicator</code> parameters are considered to indicate competing
risks.</p>
</dd>
<dt><code>signature</code></dt><dd><p>(<em>optional</em>) One or more names of feature columns that are
considered part of a specific signature. Features specified here will
always be used for modelling. Ranking from feature selection has no effect
for these features.</p>
</dd>
<dt><code>novelty_features</code></dt><dd><p>(<em>optional</em>) One or more names of feature columns
that should be included for the purpose of novelty detection.</p>
</dd>
<dt><code>exclude_features</code></dt><dd><p>(<em>optional</em>) Feature columns that will be removed
from the data set. Cannot overlap with features in <code>signature</code>,
<code>novelty_features</code> or <code>include_features</code>.</p>
</dd>
<dt><code>include_features</code></dt><dd><p>(<em>optional</em>) Feature columns that are specifically
included in the data set. By default all features are included. Cannot
overlap with <code>exclude_features</code>, but may overlap <code>signature</code>. Features in
<code>signature</code> and <code>novelty_features</code> are always included. If both
<code>exclude_features</code> and <code>include_features</code> are provided, <code>include_features</code>
takes precedence, provided that there is no overlap between the two.</p>
</dd>
<dt><code>reference_method</code></dt><dd><p>(<em>optional</em>) Method used to set reference levels for
categorical features. There are several options:
</p>

<ul>
<li> <p><code>auto</code> (default): Categorical features that are not explicitly set by the
user, i.e. columns containing boolean values or characters, use the most
frequent level as reference. Categorical features that are explicitly set,
i.e. as factors, are used as is.
</p>
</li>
<li> <p><code>always</code>: Both automatically detected and user-specified categorical
features have the reference level set to the most frequent level. Ordinal
features are not altered, but are used as is.
</p>
</li>
<li> <p><code>never</code>: User-specified categorical features are used as is.
Automatically detected categorical features are simply sorted, and the
first level is then used as the reference level. This was the behaviour
prior to familiar version 1.3.0.
</p>
</li></ul>
</dd>
<dt><code>imbalance_correction_method</code></dt><dd><p>(<em>optional</em>) Type of method used to
address class imbalances. Available options are:
</p>

<ul>
<li> <p><code>full_undersampling</code> (default): All data will be used in an ensemble
fashion. The full minority class will appear in each partition, but
majority classes are undersampled until all data have been used.
</p>
</li>
<li> <p><code>random_undersampling</code>: Randomly undersamples majority classes. This is
useful in cases where full undersampling would lead to the formation of
many models due major overrepresentation of the largest class.
</p>
</li></ul>

<p>This parameter is only used in combination with imbalance partitioning in
the experimental design, and <code>ip</code> should therefore appear in the string
that defines the design.</p>
</dd>
<dt><code>imbalance_n_partitions</code></dt><dd><p>(<em>optional</em>) Number of times random
undersampling should be repeated. 10 undersampled subsets with balanced
classes are formed by default.</p>
</dd>
<dt><code>parallel</code></dt><dd><p>(<em>optional</em>) Enable parallel processing. Defaults to <code>TRUE</code>.
When set to <code>FALSE</code>, this disables all parallel processing, regardless of
specific parameters such as <code>parallel_preprocessing</code>. However, when
<code>parallel</code> is <code>TRUE</code>, parallel processing of different parts of the
workflow can be disabled by setting respective flags to <code>FALSE</code>.</p>
</dd>
<dt><code>parallel_nr_cores</code></dt><dd><p>(<em>optional</em>) Number of cores available for
parallelisation. Defaults to 2. This setting does nothing if
parallelisation is disabled.</p>
</dd>
<dt><code>restart_cluster</code></dt><dd><p>(<em>optional</em>) Restart nodes used for parallel computing
to free up memory prior to starting a parallel process. Note that it does
take time to set up the clusters. Therefore setting this argument to <code>TRUE</code>
may impact processing speed. This argument is ignored if <code>parallel</code> is
<code>FALSE</code> or the cluster was initialised outside of familiar. Default is
<code>FALSE</code>, which causes the clusters to be initialised only once.</p>
</dd>
<dt><code>cluster_type</code></dt><dd><p>(<em>optional</em>) Selection of the cluster type for parallel
processing. Available types are the ones supported by the parallel package
that is part of the base R distribution: <code>psock</code> (default), <code>fork</code>, <code>mpi</code>,
<code>nws</code>, <code>sock</code>. In addition, <code>none</code> is available, which also disables
parallel processing.</p>
</dd>
<dt><code>backend_type</code></dt><dd><p>(<em>optional</em>) Selection of the backend for distributing
copies of the data. This backend ensures that only a single master copy is
kept in memory. This limits memory usage during parallel processing.
</p>
<p>Several backend options are available, notably <code>socket_server</code>, and <code>none</code>
(default). <code>socket_server</code> is based on the callr package and R sockets,
comes with <code>familiar</code> and is available for any OS. <code>none</code> uses the package
environment of familiar to store data, and is available for any OS.
However, <code>none</code> requires copying of data to any parallel process, and has a
larger memory footprint.</p>
</dd>
<dt><code>server_port</code></dt><dd><p>(<em>optional</em>) Integer indicating the port on which the
socket server or RServe process should communicate. Defaults to port 6311.
Note that ports 0 to 1024 and 49152 to 65535 cannot be used.</p>
</dd>
<dt><code>feature_max_fraction_missing</code></dt><dd><p>(<em>optional</em>) Numeric value between <code>0.0</code>
and <code>0.95</code> that determines the meximum fraction of missing values that
still allows a feature to be included in the data set. All features with a
missing value fraction over this threshold are not processed further. The
default value is <code>0.30</code>.</p>
</dd>
<dt><code>sample_max_fraction_missing</code></dt><dd><p>(<em>optional</em>) Numeric value between <code>0.0</code>
and <code>0.95</code> that determines the maximum fraction of missing values that
still allows a sample to be included in the data set. All samples with a
missing value fraction over this threshold are excluded and not processed
further. The default value is <code>0.30</code>.</p>
</dd>
<dt><code>filter_method</code></dt><dd><p>(<em>optional</em>) One or methods used to reduce
dimensionality of the data set by removing irrelevant or poorly
reproducible features.
</p>
<p>Several method are available:
</p>

<ul>
<li> <p><code>none</code> (default): None of the features will be filtered.
</p>
</li>
<li> <p><code>low_variance</code>: Features with a variance below the
<code>low_var_minimum_variance_threshold</code> are filtered. This can be useful to
filter, for example, genes that are not differentially expressed.
</p>
</li>
<li> <p><code>univariate_test</code>: Features undergo a univariate regression using an
outcome-appropriate regression model. The p-value of the model coefficient
is collected. Features with coefficient p or q-value above the
<code>univariate_test_threshold</code> are subsequently filtered.
</p>
</li>
<li> <p><code>robustness</code>: Features that are not sufficiently robust according to the
intraclass correlation coefficient are filtered. Use of this method
requires that repeated measurements are present in the data set, i.e. there
should be entries for which the sample and cohort identifiers are the same.
</p>
</li></ul>

<p>More than one method can be used simultaneously. Features with singular
values are always filtered, as these do not contain information.</p>
</dd>
<dt><code>univariate_test_threshold</code></dt><dd><p>(<em>optional</em>) Numeric value between <code>1.0</code> and
<code>0.0</code> that determines which features are irrelevant and will be filtered by
the <code>univariate_test</code>. The p or q-values are compared to this threshold.
All features with values above the threshold are filtered. The default
value is <code>0.20</code>.</p>
</dd>
<dt><code>univariate_test_threshold_metric</code></dt><dd><p>(<em>optional</em>) Metric used with the to
compare the <code>univariate_test_threshold</code> against. The following metrics can
be chosen:
</p>

<ul>
<li> <p><code>p_value</code> (default): The unadjusted p-value of each feature is used for
to filter features.
</p>
</li>
<li> <p><code>q_value</code>: The q-value (Story, 2002), is used to filter features. Some
data sets may have insufficient samples to compute the q-value. The
<code>qvalue</code> package must be installed from Bioconductor to use this method.
</p>
</li></ul>
</dd>
<dt><code>univariate_test_max_feature_set_size</code></dt><dd><p>(<em>optional</em>) Maximum size of the
feature set after the univariate test. P or q values of features are
compared against the threshold, but if the resulting data set would be
larger than this setting, only the most relevant features up to the desired
feature set size are selected.
</p>
<p>The default value is <code>NULL</code>, which causes features to be filtered based on
their relevance only.</p>
</dd>
<dt><code>low_var_minimum_variance_threshold</code></dt><dd><p>(required, if used) Numeric value
that determines which features will be filtered by the <code>low_variance</code>
method. The variance of each feature is computed and compared to the
threshold. If it is below the threshold, the feature is removed.
</p>
<p>This parameter has no default value and should be set if <code>low_variance</code> is
used.</p>
</dd>
<dt><code>low_var_max_feature_set_size</code></dt><dd><p>(<em>optional</em>) Maximum size of the feature
set after filtering features with a low variance. All features are first
compared against <code>low_var_minimum_variance_threshold</code>. If the resulting
feature set would be larger than specified, only the most strongly varying
features will be selected, up to the desired size of the feature set.
</p>
<p>The default value is <code>NULL</code>, which causes features to be filtered based on
their variance only.</p>
</dd>
<dt><code>robustness_icc_type</code></dt><dd><p>(<em>optional</em>) String indicating the type of
intraclass correlation coefficient (<code>1</code>, <code>2</code> or <code>3</code>) that should be used to
compute robustness for features in repeated measurements. These types
correspond to the types in Shrout and Fleiss (1979). The default value is
<code>1</code>.</p>
</dd>
<dt><code>robustness_threshold_metric</code></dt><dd><p>(<em>optional</em>) String indicating which
specific intraclass correlation coefficient (ICC) metric should be used to
filter features. This should be one of:
</p>

<ul>
<li> <p><code>icc</code>: The estimated ICC value itself.
</p>
</li>
<li> <p><code>icc_low</code> (default): The estimated lower limit of the 95% confidence
interval of the ICC, as suggested by Koo and Li (2016).
</p>
</li>
<li> <p><code>icc_panel</code>: The estimated ICC value over the panel average, i.e. the ICC
that would be obtained if all repeated measurements were averaged.
</p>
</li>
<li> <p><code>icc_panel_low</code>: The estimated lower limit of the 95% confidence interval
of the panel ICC.
</p>
</li></ul>
</dd>
<dt><code>robustness_threshold_value</code></dt><dd><p>(<em>optional</em>) The intraclass correlation
coefficient value that is as threshold. The default value is <code>0.70</code>.</p>
</dd>
<dt><code>transformation_method</code></dt><dd><p>(<em>optional</em>) The transformation method used to
change the distribution of the data to be more normal-like. The following
methods are available:
</p>

<ul>
<li> <p><code>none</code>: This disables transformation of features.
</p>
</li>
<li> <p><code>yeo_johnson</code> (default): Transformation using the Yeo-Johnson
transformation (Yeo and Johnson, 2000). The algorithm tests various lambda
values and selects the lambda that maximises the log-likelihood.
</p>
</li>
<li> <p><code>yeo_johnson_trim</code>: As <code>yeo_johnson</code>, but based on the set of feature
values where the 5% lowest and 5% highest values are discarded. This
reduces the effect of outliers.
</p>
</li>
<li> <p><code>yeo_johnson_winsor</code>: As <code>yeo_johnson</code>, but based on the set of feature
values where the 5% lowest and 5% highest values are winsorised. This
reduces the effect of outliers.
</p>
</li>
<li> <p><code>yeo_johnson_robust</code>: A robust version of <code>yeo_johnson</code> after Raymaekers
and Rousseeuw (2021). This method is less sensitive to outliers.
</p>
</li>
<li> <p><code>box_cox</code>: Transformation using the Box-Cox transformation (Box and Cox,
1964). Unlike the Yeo-Johnson transformation, the Box-Cox transformation
requires that all data are positive. Features that contain zero or negative
values cannot be transformed using this transformation. The algorithm tests
various lambda values and selects the lambda that maximises the
log-likelihood.
</p>
</li>
<li> <p><code>box_cox_trim</code>: As <code>box_cox</code>, but based on the set of feature values
where the 5% lowest and 5% highest values are discarded. This reduces the
effect of outliers.
</p>
</li>
<li> <p><code>box_cox_winsor</code>: As <code>box_cox</code>, but based on the set of feature values
where the 5% lowest and 5% highest values are winsorised. This reduces the
effect of outliers.
</p>
</li>
<li> <p><code>box_cox_robust</code>: A robust verson of <code>box_cox</code> after Raymaekers and
Rousseew (2021). This method is less sensitive to outliers.
</p>
</li></ul>

<p>Only features that contain numerical data are transformed. Transformation
parameters obtained in development data are stored within <code>featureInfo</code>
objects for later use with validation data sets.</p>
</dd>
<dt><code>normalisation_method</code></dt><dd><p>(<em>optional</em>) The normalisation method used to
improve the comparability between numerical features that may have very
different scales. The following normalisation methods can be chosen:
</p>

<ul>
<li> <p><code>none</code>: This disables feature normalisation.
</p>
</li>
<li> <p><code>standardisation</code>: Features are normalised by subtraction of their mean
values and division by their standard deviations. This causes every feature
to be have a center value of 0.0 and standard deviation of 1.0.
</p>
</li>
<li> <p><code>standardisation_trim</code>: As <code>standardisation</code>, but based on the set of
feature values where the 5% lowest and 5% highest values are discarded.
This reduces the effect of outliers.
</p>
</li>
<li> <p><code>standardisation_winsor</code>: As <code>standardisation</code>, but based on the set of
feature values where the 5% lowest and 5% highest values are winsorised.
This reduces the effect of outliers.
</p>
</li>
<li> <p><code>standardisation_robust</code> (default): A robust version of <code>standardisation</code>
that relies on computing Huber's M-estimators for location and scale.
</p>
</li>
<li> <p><code>normalisation</code>: Features are normalised by subtraction of their minimum
values and division by their ranges. This maps all feature values to a
<code class="reqn">[0, 1]</code> interval.
</p>
</li>
<li> <p><code>normalisation_trim</code>: As <code>normalisation</code>, but based on the set of feature
values where the 5% lowest and 5% highest values are discarded. This
reduces the effect of outliers.
</p>
</li>
<li> <p><code>normalisation_winsor</code>: As <code>normalisation</code>, but based on the set of
feature values where the 5% lowest and 5% highest values are winsorised.
This reduces the effect of outliers.
</p>
</li>
<li> <p><code>quantile</code>: Features are normalised by subtraction of their median values
and division by their interquartile range.
</p>
</li>
<li> <p><code>mean_centering</code>: Features are centered by substracting the mean, but do
not undergo rescaling.
</p>
</li></ul>

<p>Only features that contain numerical data are normalised. Normalisation
parameters obtained in development data are stored within <code>featureInfo</code>
objects for later use with validation data sets.</p>
</dd>
<dt><code>batch_normalisation_method</code></dt><dd><p>(<em>optional</em>) The method used for batch
normalisation. Available methods are:
</p>

<ul>
<li> <p><code>none</code> (default): This disables batch normalisation of features.
</p>
</li>
<li> <p><code>standardisation</code>: Features within each batch are normalised by
subtraction of the mean value and division by the standard deviation in
each batch.
</p>
</li>
<li> <p><code>standardisation_trim</code>: As <code>standardisation</code>, but based on the set of
feature values where the 5% lowest and 5% highest values are discarded.
This reduces the effect of outliers.
</p>
</li>
<li> <p><code>standardisation_winsor</code>: As <code>standardisation</code>, but based on the set of
feature values where the 5% lowest and 5% highest values are winsorised.
This reduces the effect of outliers.
</p>
</li>
<li> <p><code>standardisation_robust</code>: A robust version of <code>standardisation</code> that
relies on computing Huber's M-estimators for location and scale within each
batch.
</p>
</li>
<li> <p><code>normalisation</code>: Features within each batch are normalised by subtraction
of their minimum values and division by their range in each batch. This
maps all feature values in each batch to a <code class="reqn">[0, 1]</code> interval.
</p>
</li>
<li> <p><code>normalisation_trim</code>: As <code>normalisation</code>, but based on the set of feature
values where the 5% lowest and 5% highest values are discarded. This
reduces the effect of outliers.
</p>
</li>
<li> <p><code>normalisation_winsor</code>: As <code>normalisation</code>, but based on the set of
feature values where the 5% lowest and 5% highest values are winsorised.
This reduces the effect of outliers.
</p>
</li>
<li> <p><code>quantile</code>: Features in each batch are normalised by subtraction of the
median value and division by the interquartile range of each batch.
</p>
</li>
<li> <p><code>mean_centering</code>: Features in each batch are centered on 0.0 by
substracting the mean value in each batch, but are not rescaled.
</p>
</li>
<li> <p><code>combat_parametric</code>: Batch adjustments using parametric empirical Bayes
(Johnson et al, 2007). <code>combat_p</code> leads to the same method.
</p>
</li>
<li> <p><code>combat_non_parametric</code>: Batch adjustments using non-parametric empirical
Bayes (Johnson et al, 2007). <code>combat_np</code> and <code>combat</code> lead to the same
method. Note that we reduced complexity from O(<code class="reqn">n^2</code>) to O(<code class="reqn">n</code>) by
only computing batch adjustment parameters for each feature on a subset of
50 randomly selected features, instead of all features.
</p>
</li></ul>

<p>Only features that contain numerical data are normalised using batch
normalisation. Batch normalisation parameters obtained in development data
are stored within <code>featureInfo</code> objects for later use with validation data
sets, in case the validation data is from the same batch.
</p>
<p>If validation data contains data from unknown batches, normalisation
parameters are separately determined for these batches.
</p>
<p>Note that for both empirical Bayes methods, the batch effect is assumed to
produce results across the features. This is often true for things such as
gene expressions, but the assumption may not hold generally.
</p>
<p>When performing batch normalisation, it is moreover important to check that
differences between batches or cohorts are not related to the studied
endpoint.</p>
</dd>
<dt><code>imputation_method</code></dt><dd><p>(<em>optional</em>) Method used for imputing missing
feature values. Two methods are implemented:
</p>

<ul>
<li> <p><code>simple</code>: Simple replacement of a missing value by the median value (for
numeric features) or the modal value (for categorical features).
</p>
</li>
<li> <p><code>lasso</code>: Imputation of missing value by lasso regression (using <code>glmnet</code>)
based on information contained in other features.
</p>
</li></ul>

<p><code>simple</code> imputation precedes <code>lasso</code> imputation to ensure that any missing
values in predictors required for <code>lasso</code> regression are resolved. The
<code>lasso</code> estimate is then used to replace the missing value.
</p>
<p>The default value depends on the number of features in the dataset. If the
number is lower than 100, <code>lasso</code> is used by default, and <code>simple</code>
otherwise.
</p>
<p>Only single imputation is performed. Imputation models and parameters are
stored within <code>featureInfo</code> objects for later use with validation data
sets.</p>
</dd>
<dt><code>cluster_method</code></dt><dd><p>(<em>optional</em>) Clustering is performed to identify and
replace redundant features, for example those that are highly correlated.
Such features do not carry much additional information and may be removed
or replaced instead (Park et al., 2007; Tolosi and Lengauer, 2011).
</p>
<p>The cluster method determines the algorithm used to form the clusters. The
following cluster methods are implemented:
</p>

<ul>
<li> <p><code>none</code>: No clustering is performed.
</p>
</li>
<li> <p><code>hclust</code> (default): Hierarchical agglomerative clustering. If the
<code>fastcluster</code> package is installed, <code>fastcluster::hclust</code> is used (Muellner
2013), otherwise <code>stats::hclust</code> is used.
</p>
</li>
<li> <p><code>agnes</code>: Hierarchical clustering using agglomerative nesting (Kaufman and
Rousseeuw, 1990). This algorithm is similar to <code>hclust</code>, but uses the
<code>cluster::agnes</code> implementation.
</p>
</li>
<li> <p><code>diana</code>: Divisive analysis hierarchical clustering. This method uses
divisive instead of agglomerative clustering (Kaufman and Rousseeuw, 1990).
<code>cluster::diana</code> is used.
</p>
</li>
<li> <p><code>pam</code>: Partioning around medioids. This partitions the data into $k$
clusters around medioids (Kaufman and Rousseeuw, 1990). $k$ is selected
using the <code>silhouette</code> metric. <code>pam</code> is implemented using the
<code>cluster::pam</code> function.
</p>
</li></ul>

<p>Clusters and cluster information is stored within <code>featureInfo</code> objects for
later use with validation data sets. This enables reproduction of the same
clusters as formed in the development data set.</p>
</dd>
<dt><code>cluster_linkage_method</code></dt><dd><p>(<em>optional</em>) Linkage method used for
agglomerative clustering in <code>hclust</code> and <code>agnes</code>. The following linkage
methods can be used:
</p>

<ul>
<li> <p><code>average</code> (default): Average linkage.
</p>
</li>
<li> <p><code>single</code>: Single linkage.
</p>
</li>
<li> <p><code>complete</code>: Complete linkage.
</p>
</li>
<li> <p><code>weighted</code>: Weighted linkage, also known as McQuitty linkage.
</p>
</li>
<li> <p><code>ward</code>: Linkage using Ward's minimum variance method.
</p>
</li></ul>

<p><code>diana</code> and <code>pam</code> do not require a linkage method.</p>
</dd>
<dt><code>cluster_cut_method</code></dt><dd><p>(<em>optional</em>) The method used to define the actual
clusters. The following methods can be used:
</p>

<ul>
<li> <p><code>silhouette</code>: Clusters are formed based on the silhouette score
(Rousseeuw, 1987). The average silhouette score is computed from 2 to
<code class="reqn">n</code> clusters, with <code class="reqn">n</code> the number of features. Clusters are only
formed if the average silhouette exceeds 0.50, which indicates reasonable
evidence for structure. This procedure may be slow if the number of
features is large (&gt;100s).
</p>
</li>
<li> <p><code>fixed_cut</code>: Clusters are formed by cutting the hierarchical tree at the
point indicated by the <code>cluster_similarity_threshold</code>, e.g. where features
in a cluster have an average Spearman correlation of 0.90. <code>fixed_cut</code> is
only available for <code>agnes</code>, <code>diana</code> and <code>hclust</code>.
</p>
</li>
<li> <p><code>dynamic_cut</code>: Dynamic cluster formation using the cutting algorithm in
the <code>dynamicTreeCut</code> package. This package should be installed to select
this option. <code>dynamic_cut</code> can only be used with <code>agnes</code> and <code>hclust</code>.
</p>
</li></ul>

<p>The default options are <code>silhouette</code> for partioning around medioids (<code>pam</code>)
and <code>fixed_cut</code> otherwise.</p>
</dd>
<dt><code>cluster_similarity_metric</code></dt><dd><p>(<em>optional</em>) Clusters are formed based on
feature similarity. All features are compared in a pair-wise fashion to
compute similarity, for example correlation. The resulting similarity grid
is converted into a distance matrix that is subsequently used for
clustering. The following metrics are supported to compute pairwise
similarities:
</p>

<ul>
<li> <p><code>mutual_information</code> (default): normalised mutual information.
</p>
</li>
<li> <p><code>mcfadden_r2</code>: McFadden's pseudo R-squared (McFadden, 1974).
</p>
</li>
<li> <p><code>cox_snell_r2</code>: Cox and Snell's pseudo R-squared (Cox and Snell, 1989).
</p>
</li>
<li> <p><code>nagelkerke_r2</code>: Nagelkerke's pseudo R-squared (Nagelkerke, 1991).
</p>
</li>
<li> <p><code>spearman</code>: Spearman's rank order correlation.
</p>
</li>
<li> <p><code>kendall</code>: Kendall rank correlation.
</p>
</li>
<li> <p><code>pearson</code>: Pearson product-moment correlation.
</p>
</li></ul>

<p>The pseudo R-squared metrics can be used to assess similarity between mixed
pairs of numeric and categorical features, as these are based on the
log-likelihood of regression models. In <code>familiar</code>, the more informative
feature is used as the predictor and the other feature as the reponse
variable. In numeric-categorical pairs, the numeric feature is considered
to be more informative and is thus used as the predictor. In
categorical-categorical pairs, the feature with most levels is used as the
predictor.
</p>
<p>In case any of the classical correlation coefficients (<code>pearson</code>,
<code>spearman</code> and <code>kendall</code>) are used with (mixed) categorical features, the
categorical features are one-hot encoded and the mean correlation over all
resulting pairs is used as similarity.</p>
</dd>
<dt><code>cluster_similarity_threshold</code></dt><dd><p>(<em>optional</em>) The threshold level for
pair-wise similarity that is required to form clusters using <code>fixed_cut</code>.
This should be a numerical value between 0.0 and 1.0. Note however, that a
reasonable threshold value depends strongly on the similarity metric. The
following are the default values used:
</p>

<ul>
<li> <p><code>mcfadden_r2</code> and <code>mutual_information</code>: <code>0.30</code>
</p>
</li>
<li> <p><code>cox_snell_r2</code> and <code>nagelkerke_r2</code>: <code>0.75</code>
</p>
</li>
<li> <p><code>spearman</code>, <code>kendall</code> and <code>pearson</code>: <code>0.90</code>
</p>
</li></ul>

<p>Alternatively, if the <code style="white-space: pre;">&#8288;fixed cut&#8288;</code> method is not used, this value determines
whether any clustering should be performed, because the data may not
contain highly similar features. The default values in this situation are:
</p>

<ul>
<li> <p><code>mcfadden_r2</code>  and <code>mutual_information</code>: <code>0.25</code>
</p>
</li>
<li> <p><code>cox_snell_r2</code> and <code>nagelkerke_r2</code>: <code>0.40</code>
</p>
</li>
<li> <p><code>spearman</code>, <code>kendall</code> and <code>pearson</code>: <code>0.70</code>
</p>
</li></ul>

<p>The threshold value is converted to a distance (1-similarity) prior to
cutting hierarchical trees.</p>
</dd>
<dt><code>cluster_representation_method</code></dt><dd><p>(<em>optional</em>) Method used to determine
how the information of co-clustered features is summarised and used to
represent the cluster. The following methods can be selected:
</p>

<ul>
<li> <p><code>best_predictor</code> (default): The feature with the highest importance
according to univariate regression with the outcome is used to represent
the cluster.
</p>
</li>
<li> <p><code>medioid</code>: The feature closest to the cluster center, i.e. the feature
that is most similar to the remaining features in the cluster, is used to
represent the feature.
</p>
</li>
<li> <p><code>mean</code>: A meta-feature is generated by averaging the feature values for
all features in a cluster. This method aligns all features so that all
features will be positively correlated prior to averaging. Should a cluster
contain one or more categorical features, the <code>medioid</code> method will be used
instead, as averaging is not possible. Note that if this method is chosen,
the <code>normalisation_method</code> parameter should be one of <code>standardisation</code>,
<code>standardisation_trim</code>, <code>standardisation_winsor</code> or <code>quantile</code>.'
</p>
</li></ul>

<p>If the <code>pam</code> cluster method is selected, only the <code>medioid</code> method can be
used. In that case 1 medioid is used by default.</p>
</dd>
<dt><code>parallel_preprocessing</code></dt><dd><p>(<em>optional</em>) Enable parallel processing for the
preprocessing workflow. Defaults to <code>TRUE</code>. When set to <code>FALSE</code>, this will
disable the use of parallel processing while preprocessing, regardless of
the settings of the <code>parallel</code> parameter. <code>parallel_preprocessing</code> is
ignored if <code>parallel=FALSE</code>.</p>
</dd>
<dt><code>fs_method</code></dt><dd><p>(<strong>required</strong>) Feature selection method to be used for
determining variable importance. <code>familiar</code> implements various feature
selection methods. Please refer to the vignette on feature selection
methods for more details.
</p>
<p>More than one feature selection method can be chosen. The experiment will
then repeated for each feature selection method.
</p>
<p>Feature selection methods determines the ranking of features. Actual
selection of features is done by optimising the signature size model
hyperparameter during the hyperparameter optimisation step.</p>
</dd>
<dt><code>fs_method_parameter</code></dt><dd><p>(<em>optional</em>) List of lists containing parameters
for feature selection methods. Each sublist should have the name of the
feature selection method it corresponds to.
</p>
<p>Most feature selection methods do not have parameters that can be set.
Please refer to the vignette on feature selection methods for more details.
Note that if the feature selection method is based on a learner (e.g. lasso
regression), hyperparameter optimisation may be performed prior to
assessing variable importance.</p>
</dd>
<dt><code>vimp_aggregation_method</code></dt><dd><p>(<em>optional</em>) The method used to aggregate
variable importances over different data subsets, e.g. bootstraps. The
following methods can be selected:
</p>

<ul>
<li> <p><code>none</code>: Don't aggregate ranks, but rather aggregate the variable
importance scores themselves.
</p>
</li>
<li> <p><code>mean</code>: Use the mean rank of a feature over the subsets to
determine the aggregated feature rank.
</p>
</li>
<li> <p><code>median</code>: Use the median rank of a feature over the subsets to determine
the aggregated feature rank.
</p>
</li>
<li> <p><code>best</code>: Use the best rank the feature obtained in any subset to determine
the aggregated feature rank.
</p>
</li>
<li> <p><code>worst</code>: Use the worst rank the feature obtained in any subset to
determine the aggregated feature rank.
</p>
</li>
<li> <p><code>stability</code>: Use the frequency of the feature being in the subset of
highly ranked features as measure for the aggregated feature rank
(Meinshausen and Buehlmann, 2010).
</p>
</li>
<li> <p><code>exponential</code>: Use a rank-weighted frequence of occurrence in the subset
of highly ranked features as measure for the aggregated feature rank (Haury
et al., 2011).
</p>
</li>
<li> <p><code>borda</code> (default): Use the borda count as measure for the aggregated
feature rank (Wald et al., 2012).
</p>
</li>
<li> <p><code>enhanced_borda</code>: Use an occurrence frequency-weighted borda count as
measure for the aggregated feature rank (Wald et al., 2012).
</p>
</li>
<li> <p><code>truncated_borda</code>: Use borda count computed only on features within the
subset of highly ranked features.
</p>
</li>
<li> <p><code>enhanced_truncated_borda</code>: Apply both the enhanced borda method and the
truncated borda method and use the resulting borda count as the aggregated
feature rank.
</p>
</li></ul>

<p>The <em>feature selection methods</em> vignette provides additional information.</p>
</dd>
<dt><code>vimp_aggregation_rank_threshold</code></dt><dd><p>(<em>optional</em>) The threshold used to
define the subset of highly important features. If not set, this threshold
is determined by maximising the variance in the occurrence value over all
features over the subset size.
</p>
<p>This parameter is only relevant for <code>stability</code>, <code>exponential</code>,
<code>enhanced_borda</code>, <code>truncated_borda</code> and <code>enhanced_truncated_borda</code> methods.</p>
</dd>
<dt><code>parallel_feature_selection</code></dt><dd><p>(<em>optional</em>) Enable parallel processing for
the feature selection workflow. Defaults to <code>TRUE</code>. When set to <code>FALSE</code>,
this will disable the use of parallel processing while performing feature
selection, regardless of the settings of the <code>parallel</code> parameter.
<code>parallel_feature_selection</code> is ignored if <code>parallel=FALSE</code>.</p>
</dd>
<dt><code>novelty_detector</code></dt><dd><p>(<em>optional</em>) Specify the algorithm used for training
a novelty detector. This detector can be used to identify
out-of-distribution data prospectively.</p>
</dd>
<dt><code>detector_parameters</code></dt><dd><p>(<em>optional</em>) List lists containing hyperparameters
for novelty detectors. Currently not used.</p>
</dd>
<dt><code>parallel_model_development</code></dt><dd><p>(<em>optional</em>) Enable parallel processing for
the model development workflow. Defaults to <code>TRUE</code>. When set to <code>FALSE</code>,
this will disable the use of parallel processing while developing models,
regardless of the settings of the <code>parallel</code> parameter.
<code>parallel_model_development</code> is ignored if <code>parallel=FALSE</code>.</p>
</dd>
<dt><code>optimisation_bootstraps</code></dt><dd><p>(<em>optional</em>) Number of bootstraps that should
be generated from the development data set. During the optimisation
procedure one or more of these bootstraps (indicated by
<code>smbo_step_bootstraps</code>) are used for model development using different
combinations of hyperparameters. The effect of the hyperparameters is then
assessed by comparing in-bag and out-of-bag model performance.
</p>
<p>The default number of bootstraps is <code>50</code>. Hyperparameter optimisation may
finish before exhausting the set of bootstraps.</p>
</dd>
<dt><code>optimisation_determine_vimp</code></dt><dd><p>(<em>optional</em>) Logical value that indicates
whether variable importance is determined separately for each of the
bootstraps created during the optimisation process (<code>TRUE</code>) or the
applicable results from the feature selection step are used (<code>FALSE</code>).
</p>
<p>Determining variable importance increases the initial computational
overhead. However, it prevents positive biases for the out-of-bag data due
to overlap of these data with the development data set used for the feature
selection step. In this case, any hyperparameters of the variable
importance method are not determined separately for each bootstrap, but
those obtained during the feature selection step are used instead. In case
multiple of such hyperparameter sets could be applicable, the set that will
be used is randomly selected for each bootstrap.
</p>
<p>This parameter only affects hyperparameter optimisation of learners. The
default is <code>TRUE</code>.</p>
</dd>
<dt><code>smbo_random_initialisation</code></dt><dd><p>(<em>optional</em>) String indicating the
initialisation method for the hyperparameter space. Can be one of
<code>fixed_subsample</code> (default), <code>fixed</code>, or <code>random</code>. <code>fixed</code> and
<code>fixed_subsample</code> first create hyperparameter sets from a range of default
values set by familiar. <code>fixed_subsample</code> then randomly draws up to
<code>smbo_n_random_sets</code> from the grid. <code>random</code> does not rely upon a fixed
grid, and randomly draws up to <code>smbo_n_random_sets</code> hyperparameter sets
from the hyperparameter space.</p>
</dd>
<dt><code>smbo_n_random_sets</code></dt><dd><p>(<em>optional</em>) Number of random or subsampled
hyperparameters drawn during the initialisation process. Default: <code>100</code>.
Cannot be smaller than <code>10</code>. The parameter is not used when
<code>smbo_random_initialisation</code> is <code>fixed</code>, as the entire pre-defined grid
will be explored.</p>
</dd>
<dt><code>max_smbo_iterations</code></dt><dd><p>(<em>optional</em>) Maximum number of intensify
iterations of the SMBO algorithm. During an intensify iteration a run-off
occurs between the current <em>best</em> hyperparameter combination and either 10
challenger combination with the highest expected improvement or a set of 20
random combinations.
</p>
<p>Run-off with random combinations is used to force exploration of the
hyperparameter space, and is performed every second intensify iteration, or
if there is no expected improvement for any challenger combination.
</p>
<p>If a combination of hyperparameters leads to better performance on the same
data than the incumbent <em>best</em> set of hyperparameters, it replaces the
incumbent set at the end of the intensify iteration.
</p>
<p>The default number of intensify iteration is <code>20</code>. Iterations may be
stopped early if the incumbent set of hyperparameters remains the same for
<code>smbo_stop_convergent_iterations</code> iterations, or performance improvement is
minimal. This behaviour is suppressed during the first 4 iterations to
enable the algorithm to explore the hyperparameter space.</p>
</dd>
<dt><code>smbo_stop_convergent_iterations</code></dt><dd><p>(<em>optional</em>) The number of subsequent
convergent SMBO iterations required to stop hyperparameter optimisation
early. An iteration is convergent if the <em>best</em> parameter set has not
changed or the optimisation score over the 4 most recent iterations has not
changed beyond the tolerance level in <code>smbo_stop_tolerance</code>.
</p>
<p>The default value is <code>3</code>.</p>
</dd>
<dt><code>smbo_stop_tolerance</code></dt><dd><p>(<em>optional</em>) Tolerance for early stopping due to
convergent optimisation score.
</p>
<p>The default value depends on the square root of the number of samples (at
the series level), and is <code>0.01</code> for 100 samples. This value is computed as
<code>0.1 * 1 / sqrt(n_samples)</code>. The upper limit is <code>0.0001</code> for 1M or more
samples.</p>
</dd>
<dt><code>smbo_time_limit</code></dt><dd><p>(<em>optional</em>) Time limit (in minutes) for the
optimisation process. Optimisation is stopped after this limit is exceeded.
Time taken to determine variable importance for the optimisation process
(see the <code>optimisation_determine_vimp</code> parameter) does not count.
</p>
<p>The default is <code>NULL</code>, indicating that there is no time limit for the
optimisation process. The time limit cannot be less than 1 minute.</p>
</dd>
<dt><code>smbo_initial_bootstraps</code></dt><dd><p>(<em>optional</em>) The number of bootstraps taken
from the set of <code>optimisation_bootstraps</code> as the bootstraps assessed
initially.
</p>
<p>The default value is <code>1</code>. The value cannot be larger than
<code>optimisation_bootstraps</code>.</p>
</dd>
<dt><code>smbo_step_bootstraps</code></dt><dd><p>(<em>optional</em>) The number of bootstraps taken from
the set of <code>optimisation_bootstraps</code> bootstraps as the bootstraps assessed
during the steps of each intensify iteration.
</p>
<p>The default value is <code>3</code>. The value cannot be larger than
<code>optimisation_bootstraps</code>.</p>
</dd>
<dt><code>smbo_intensify_steps</code></dt><dd><p>(<em>optional</em>) The number of steps in each SMBO
intensify iteration. Each step a new set of <code>smbo_step_bootstraps</code>
bootstraps is drawn and used in the run-off between the incumbent <em>best</em>
hyperparameter combination and its challengers.
</p>
<p>The default value is <code>5</code>. Higher numbers allow for a more detailed
comparison, but this comes with added computational cost.</p>
</dd>
<dt><code>optimisation_metric</code></dt><dd><p>(<em>optional</em>) One or more metrics used to compute
performance scores. See the vignette on performance metrics for the
available metrics.
</p>
<p>If unset, the following metrics are used by default:
</p>

<ul>
<li> <p><code>auc_roc</code>: For <code>binomial</code> and <code>multinomial</code> models.
</p>
</li>
<li> <p><code>mse</code>: Mean squared error for <code>continuous</code> models.
</p>
</li>
<li> <p><code>msle</code>: Mean squared logarithmic error for <code>count</code> models.
</p>
</li>
<li> <p><code>concordance_index</code>: For <code>survival</code> models.
</p>
</li></ul>

<p>Multiple optimisation metrics can be specified. Actual metric values are
converted to an objective value by comparison with a baseline metric value
that derives from a trivial model, i.e. majority class for binomial and
multinomial outcomes, the median outcome for count and continuous outcomes
and a fixed risk or time for survival outcomes.</p>
</dd>
<dt><code>optimisation_function</code></dt><dd><p>(<em>optional</em>) Type of optimisation function used
to quantify the performance of a hyperparameter set. Model performance is
assessed using the metric(s) specified by <code>optimisation_metric</code> on the
in-bag (IB) and out-of-bag (OOB) samples of a bootstrap. These values are
converted to objective scores with a standardised interval of <code class="reqn">[-1.0,
  1.0]</code>. Each pair of objective is subsequently used to compute an
optimisation score. The optimisation score across different bootstraps is
than aggregated to a summary score. This summary score is used to rank
hyperparameter sets, and select the optimal set.
</p>
<p>The combination of optimisation score and summary score is determined by
the optimisation function indicated by this parameter:
</p>

<ul>
<li> <p><code>validation</code> or <code>max_validation</code> (default): seeks to maximise OOB score.
</p>
</li>
<li> <p><code>balanced</code>: seeks to balance IB and OOB score.
</p>
</li>
<li> <p><code>stronger_balance</code>: similar to <code>balanced</code>, but with stronger penalty for
differences between IB and OOB scores.
</p>
</li>
<li> <p><code>validation_minus_sd</code>: seeks to optimise the average OOB score minus its
standard deviation.
</p>
</li>
<li> <p><code>validation_25th_percentile</code>: seeks to optimise the 25th percentile of
OOB scores, and is conceptually similar to <code>validation_minus_sd</code>.
</p>
</li>
<li> <p><code>model_estimate</code>: seeks to maximise the OOB score estimate predicted by
the hyperparameter learner (not available for random search).
</p>
</li>
<li> <p><code>model_estimate_minus_sd</code>: seeks to maximise the OOB score estimate minus
its estimated standard deviation, as predicted by the hyperparameter
learner (not available for random search).
</p>
</li>
<li> <p><code>model_balanced_estimate</code>: seeks to maximise the estimate of the balanced
IB and OOB score. This is similar to the <code>balanced</code> score, and in fact uses
a hyperparameter learner to predict said score (not available for random
search).
</p>
</li>
<li> <p><code>model_balanced_estimate_minus_sd</code>: seeks to maximise the estimate of the
balanced IB and OOB score, minus its estimated standard deviation. This is
similar to the <code>balanced</code> score, but takes into account its estimated
spread.
</p>
</li></ul>

<p>Additional detail are provided in the <em>Learning algorithms and
hyperparameter optimisation</em> vignette.</p>
</dd>
<dt><code>hyperparameter_learner</code></dt><dd><p>(<em>optional</em>) Any point in the hyperparameter
space has a single, scalar, optimisation score value that is <em>a priori</em>
unknown. During the optimisation process, the algorithm samples from the
hyperparameter space by selecting hyperparameter sets and computing the
optimisation score value for one or more bootstraps. For each
hyperparameter set the resulting values are distributed around the actual
value. The learner indicated by <code>hyperparameter_learner</code> is then used to
infer optimisation score estimates for unsampled parts of the
hyperparameter space.
</p>
<p>The following models are available:
</p>

<ul>
<li> <p><code>bayesian_additive_regression_trees</code> or <code>bart</code>: Uses Bayesian Additive
Regression Trees (Sparapani et al., 2021) for inference. Unlike standard
random forests, BART allows for estimating posterior distributions directly
and can extrapolate.
</p>
</li>
<li> <p><code>gaussian_process</code> (default): Creates a localised approximate Gaussian
process for inference (Gramacy, 2016). This allows for better scaling than
deterministic Gaussian Processes.
</p>
</li>
<li> <p><code>random_forest</code>: Creates a random forest for inference. Originally
suggested by Hutter et al. (2011). A weakness of random forests is their
lack of extrapolation beyond observed values, which limits their usefulness
in exploiting promising areas of hyperparameter space.
</p>
</li>
<li> <p><code>random</code> or <code>random_search</code>: Forgoes the use of models to steer
optimisation. Instead, a random search is performed.
</p>
</li></ul>
</dd>
<dt><code>acquisition_function</code></dt><dd><p>(<em>optional</em>) The acquisition function influences
how new hyperparameter sets are selected. The algorithm uses the model
learned by the learner indicated by <code>hyperparameter_learner</code> to search the
hyperparameter space for hyperparameter sets that are either likely better
than the best known set (<em>exploitation</em>) or where there is considerable
uncertainty (<em>exploration</em>). The acquisition function quantifies this
(Shahriari et al., 2016).
</p>
<p>The following acquisition functions are available, and are described in
more detail in the <em>learner algorithms</em> vignette:
</p>

<ul>
<li> <p><code>improvement_probability</code>: The probability of improvement quantifies the
probability that the expected optimisation score for a set is better than
the best observed optimisation score
</p>
</li>
<li> <p><code>improvement_empirical_probability</code>: Similar to
<code>improvement_probability</code>, but based directly on optimisation scores
predicted by the individual decision trees.
</p>
</li>
<li> <p><code>expected_improvement</code> (default): Computes expected improvement.
</p>
</li>
<li> <p><code>upper_confidence_bound</code>: This acquisition function is based on the upper
confidence bound of the distribution (Srinivas et al., 2012).
</p>
</li>
<li> <p><code>bayes_upper_confidence_bound</code>: This acquisition function is based on the
upper confidence bound of the distribution (Kaufmann et al., 2012).
</p>
</li></ul>
</dd>
<dt><code>exploration_method</code></dt><dd><p>(<em>optional</em>) Method used to steer exploration in
post-initialisation intensive searching steps. As stated earlier, each SMBO
iteration step compares suggested alternative parameter sets with an
incumbent <strong>best</strong> set in a series of steps. The exploration method
controls how the set of alternative parameter sets is pruned after each
step in an iteration. Can be one of the following:
</p>

<ul>
<li> <p><code>single_shot</code> (default): The set of alternative parameter sets is not
pruned, and each intensification iteration contains only a single
intensification step that only uses a single bootstrap. This is the fastest
exploration method, but only superficially tests each parameter set.
</p>
</li>
<li> <p><code>successive_halving</code>: The set of alternative parameter sets is
pruned by removing the worst performing half of the sets after each step
(Jamieson and Talwalkar, 2016).
</p>
</li>
<li> <p><code>stochastic_reject</code>: The set of alternative parameter sets is pruned by
comparing the performance of each parameter set with that of the incumbent
<strong>best</strong> parameter set using a paired Wilcoxon test based on shared
bootstraps. Parameter sets that perform significantly worse, at an alpha
level indicated by <code>smbo_stochastic_reject_p_value</code>, are pruned.
</p>
</li>
<li> <p><code>none</code>: The set of alternative parameter sets is not pruned.
</p>
</li></ul>
</dd>
<dt><code>smbo_stochastic_reject_p_value</code></dt><dd><p>(<em>optional</em>) The p-value threshold used
for the <code>stochastic_reject</code> exploration method.
</p>
<p>The default value is <code>0.05</code>.</p>
</dd>
<dt><code>parallel_hyperparameter_optimisation</code></dt><dd><p>(<em>optional</em>) Enable parallel
processing for hyperparameter optimisation. Defaults to <code>TRUE</code>. When set to
<code>FALSE</code>, this will disable the use of parallel processing while performing
optimisation, regardless of the settings of the <code>parallel</code> parameter. The
parameter moreover specifies whether parallelisation takes place within the
optimisation algorithm (<code>inner</code>, default), or in an outer loop ( <code>outer</code>)
over learners, data subsamples, etc.
</p>
<p><code>parallel_hyperparameter_optimisation</code> is ignored if <code>parallel=FALSE</code>.</p>
</dd>
</dl>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a thin wrapper around <code>summon_familiar</code>, and functions like
it, but automatically skips all evaluation steps. Only a single learner is
allowed.
</p>


<h3>Value</h3>

<p>One or more familiarModel objects.
</p>

<hr>
<h2 id='update_model_dir_path'>Updates model directory path for ensemble objects.</h2><span id='topic+update_model_dir_path'></span><span id='topic+update_model_dir_path+2CfamiliarEnsemble-method'></span><span id='topic+update_model_dir_path+2CANY-method'></span>

<h3>Description</h3>

<p>Updates the model directory path of a <code>familiarEnsemble</code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>update_model_dir_path(object, dir_path, ...)

## S4 method for signature 'familiarEnsemble'
update_model_dir_path(object, dir_path)

## S4 method for signature 'ANY'
update_model_dir_path(object, dir_path)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="update_model_dir_path_+3A_object">object</code></td>
<td>
<p>A <code>familiarEnsemble</code> object, or one or more <code>familiarModel</code>
objects that will be internally converted to a <code>familiarEnsemble</code> object.
Paths to such objects can also be provided.</p>
</td></tr>
<tr><td><code id="update_model_dir_path_+3A_dir_path">dir_path</code></td>
<td>
<p>Path to the directory where models are stored.</p>
</td></tr>
<tr><td><code id="update_model_dir_path_+3A_...">...</code></td>
<td>
<p>Unused arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Ensemble models created by familiar are often written to a directory
on a local drive or network. In such cases, the actual models are detached,
and paths to the models are stored instead. When the models are moved from
their original location, they can no longer be found and attached to the
ensemble. This method allows for pointing to the new directory containing
the models.
</p>


<h3>Value</h3>

<p>A <code>familiarEnsemble</code> object.
</p>

<hr>
<h2 id='update_object'>Update familiar S4 objects to the most recent version.</h2><span id='topic+update_object'></span><span id='topic+update_object+2CfamiliarModel-method'></span><span id='topic+update_object+2CfamiliarEnsemble-method'></span><span id='topic+update_object+2CfamiliarData-method'></span><span id='topic+update_object+2CfamiliarCollection-method'></span><span id='topic+update_object+2CvimpTable-method'></span><span id='topic+update_object+2CfamiliarNoveltyDetector-method'></span><span id='topic+update_object+2CfeatureInfo-method'></span><span id='topic+update_object+2CexperimentData-method'></span><span id='topic+update_object+2Clist-method'></span><span id='topic+update_object+2CANY-method'></span>

<h3>Description</h3>

<p>Provides backward compatibility for familiar objects exported to
a file. This mitigates compatibility issues when working with files that
become outdated as new versions of familiar are released, e.g. because slots
have been removed.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>update_object(object, ...)

## S4 method for signature 'familiarModel'
update_object(object, ...)

## S4 method for signature 'familiarEnsemble'
update_object(object, ...)

## S4 method for signature 'familiarData'
update_object(object, ...)

## S4 method for signature 'familiarCollection'
update_object(object, ...)

## S4 method for signature 'vimpTable'
update_object(object, ...)

## S4 method for signature 'familiarNoveltyDetector'
update_object(object, ...)

## S4 method for signature 'featureInfo'
update_object(object, ...)

## S4 method for signature 'experimentData'
update_object(object, ...)

## S4 method for signature 'list'
update_object(object, ...)

## S4 method for signature 'ANY'
update_object(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="update_object_+3A_object">object</code></td>
<td>
<p>A <code>familiarModel</code>, a <code>familiarEnsemble</code>, a <code>familiarData</code> or
<code>familiarCollection</code> object.</p>
</td></tr>
<tr><td><code id="update_object_+3A_...">...</code></td>
<td>
<p>Unused arguments.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An up-to-date version of the respective S4 object.
</p>

<hr>
<h2 id='vcov'>Calculate variance-covariance matrix for a model</h2><span id='topic+vcov'></span><span id='topic+vcov+2CfamiliarModel-method'></span>

<h3>Description</h3>

<p>Calculate variance-covariance matrix for a model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vcov(object, ...)

## S4 method for signature 'familiarModel'
vcov(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vcov_+3A_object">object</code></td>
<td>
<p>a familiarModel object</p>
</td></tr>
<tr><td><code id="vcov_+3A_...">...</code></td>
<td>
<p>additional arguments passed to <code>vcob</code> methods for the underlying
model, when available.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This method extends the <code>vcov</code> S3 method. For some models <code>vcov</code>
requires information that is trimmed from the model. In this case a copy of
the variance-covariance matrix is stored with the model, and returned.
</p>


<h3>Value</h3>

<p>Variance-covariance matrix of the model in the familiarModel object,
if any.
</p>

<hr>
<h2 id='vimpTable-class'>Variable importance table</h2><span id='topic+vimpTable-class'></span>

<h3>Description</h3>

<p>A vimpTable object contains information concerning variable importance of one
or more features. These objects are created during feature selection.
</p>


<h3>Details</h3>

<p>vimpTable objects exists in various states. These states are
generally incremental, i.e. one cannot turn a declustered table into the
initial version. Some methods such as aggregation internally do some state
reshuffling.
</p>
<p>This object replaces the ad-hoc lists with information that were used in
versions prior to familiar 1.2.0.
</p>


<h3>Slots</h3>


<dl>
<dt><code>vimp_table</code></dt><dd><p>Table containing features with corresponding scores.</p>
</dd>
<dt><code>vimp_method</code></dt><dd><p>Method used to compute variable importance scores for each
feature.</p>
</dd>
<dt><code>run_table</code></dt><dd><p>Run table for the data used to compute variable importances
from. Used internally.</p>
</dd>
<dt><code>score_aggregation</code></dt><dd><p>Method used to aggregate the score of contrasts for
each categorical feature, if any,</p>
</dd>
<dt><code>encoding_table</code></dt><dd><p>Table used to relate categorical features to their
contrasts, if any. Not used for all variable importance methods.</p>
</dd>
<dt><code>cluster_table</code></dt><dd><p>Table used to relate original features with features
after clustering. Variable importance is determined after feature
processing, which includes clustering.</p>
</dd>
<dt><code>invert</code></dt><dd><p>Determines whether increasing score corresponds to increasing
(<code>FALSE</code>) or decreasing rank (<code>TRUE</code>). Used internally to determine how
ranks should be formed.</p>
</dd>
<dt><code>project_id</code></dt><dd><p>Identifier of the project that generated the vimpTable
object.</p>
</dd>
<dt><code>familiar_version</code></dt><dd><p>Version of the familiar package used to create this
table.</p>
</dd>
<dt><code>state</code></dt><dd><p>State of the variable importance table. The object can have the
following states:
</p>

<ul>
<li> <p><code>initial</code>: initial state, directly after the variable importance table is
filled.
</p>
</li>
<li> <p><code>decoded</code>: depending on the variable importance method, the initial
variable importance table may contain the scores of individual contrasts
for categorical variables. When decoded, data in the <code>encoding_table</code>
attribute has been used to aggregate scores from all contrasts into a
single score for each feature.
</p>
</li>
<li> <p><code>declustered</code>: variable importance is determined from fully processed
features, which includes clustering. This means that a single feature in
the variable importance table may represent multiple original features.
When a variable importance table has been declustered, all clusters have
been turned into their constituent features.
</p>
</li>
<li> <p><code>reclustered</code>: When the table is reclustered, features are replaced by
their respective clusters. This is actually used when updating the cluster
table to ensure it fits to a local context. This prevents issues when
attempting to aggregate or apply variable importance tables in data with
different feature preprocessing, and as a result, different clusters.
</p>
</li>
<li> <p><code>ranked</code>: The scores have been used to create ranks, with lower ranks
indicating better features.
</p>
</li>
<li> <p><code>aggregated</code>: Score and ranks from multiple variable importance tables
were aggregated.
</p>
</li></ul>
</dd>
</dl>


<h3>See Also</h3>

<p><code><a href="#topic+get_vimp_table">get_vimp_table</a></code>, <code><a href="#topic+aggregate_vimp_table">aggregate_vimp_table</a></code>
</p>

<hr>
<h2 id='waiver'>Create a waiver object</h2><span id='topic+waiver'></span>

<h3>Description</h3>

<p>This function is functionally identical to <code>ggplot2::waiver()</code> function and
creates a waiver object. A waiver object is an otherwise empty object that
serves the same purpose as <code>NULL</code>, i.e. as placeholder for a default value.
Because <code>NULL</code> can sometimes be a valid input argument, it can therefore not
be used to switch to an internal default value.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>waiver()
</code></pre>


<h3>Value</h3>

<p>waiver object
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
