<!DOCTYPE html><html lang="en"><head><title>Help for package CASMI</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {CASMI}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#AQI'><p>AQI Index</p></a></li>
<li><a href='#autoBin.binary'><p>Automatically Dichotomize Quantitative Variables</p></a></li>
<li><a href='#CASMI.mineCombination'><p>Discover Factor Combinations based on <span class="pkg">CASMI</span></p></a></li>
<li><a href='#CASMI.selectFeatures'><p><span class="pkg">CASMI</span> Feature Selection</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>'CASMI'-Based Functions</td>
</tr>
<tr>
<td>Version:</td>
<td>2.0.0</td>
</tr>
<tr>
<td>Description:</td>
<td>Contains Coverage Adjusted Standardized Mutual Information ('CASMI')-based functions. 'CASMI' is a fundamental concept of a series of methods. For more information about 'CASMI' and 'CASMI'-related methods, please refer to the corresponding publications (e.g., a feature selection method, Shi, J., Zhang, J., &amp; Ge, Y. (2019) &lt;<a href="https://doi.org/10.3390%2Fe21121179">doi:10.3390/e21121179</a>&gt;, and a dataset quality measurement method, Shi, J., Zhang, J., &amp; Ge, Y. (2019) &lt;<a href="https://doi.org/10.1109%2FICHI.2019.8904553">doi:10.1109/ICHI.2019.8904553</a>&gt;) or contact the package author for the latest updates.</td>
</tr>
<tr>
<td>Imports:</td>
<td>EntropyEstimation, entropy, stats</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.2</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat (&ge; 3.0.0)</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2025-02-13 22:28:39 UTC; js5328</td>
</tr>
<tr>
<td>Author:</td>
<td>Jingyi (Catherine) Shi [aut, cre, cph, ctb],
  Shirli Arndt [aut],
  Jialin Zhang [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Jingyi (Catherine) Shi &lt;jshi@math.msstate.edu&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2025-02-13 22:50:03 UTC</td>
</tr>
</table>
<hr>
<h2 id='AQI'>AQI Index</h2><span id='topic+AQI'></span>

<h3>Description</h3>

<p>A quantitative measure of dataset quality. The AQI Index score indicates the degree to which features are associated with the outcome in a dataset. (Synonyms of &quot;feature&quot; in this document: &quot;independent variable,&quot; &quot;factor,&quot; &quot;predictor.&quot;) <br />
For more information, please refer to the corresponding publication: Shi, J., Zhang, J. and Ge, Y. (2019), &quot;An Association-Based Intrinsic Quality Index for Healthcare Dataset Ranking&quot; &lt;doi:10.1109/ICHI.2019.8904553&gt;
</p>


<h3>Usage</h3>

<pre><code class='language-R'>AQI(data, alpha.ind = 0.2)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="AQI_+3A_data">data</code></td>
<td>
<p>data frame with variables as columns and observations as rows. The data must include at least one feature (a.k.a., independent variable, predictor, factor) and only one outcome variable (Y). The outcome variable MUST BE THE LAST COLUMN. Both the features and the outcome MUST be categorical or discrete. If variables are not naturally discrete, you may preprocess them using the 'autoBin.binary()' function in the same package.</p>
</td></tr>
<tr><td><code id="AQI_+3A_alpha.ind">alpha.ind</code></td>
<td>
<p>level of significance for the mutual information test of independence in step 2 (&lt;doi:10.1109/ICHI.2019.8904553&gt;). By default, 'alpha.ind = 0.2'.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The AQI Index score.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## ---- Generate a toy dataset: "data" ----
n &lt;- 10000
set.seed(1)
x1 &lt;- rbinom(n, 3, 0.5) + 0.2
set.seed(2)
x2 &lt;- rbinom(n, 2, 0.8) + 0.5
set.seed(3)
x3 &lt;- rbinom(n, 5, 0.3)
set.seed(4)
error &lt;- round(runif(n, min=-1, max=1))
y &lt;- x1 + x3 + error
data &lt;- data.frame(cbind(x1, x2, x3, y))
colnames(data) &lt;- c("feature1", "feature2", "feature3", "Y")

## ---- Calculate the AQI score of "data" ----
AQI(data)
</code></pre>

<hr>
<h2 id='autoBin.binary'>Automatically Dichotomize Quantitative Variables</h2><span id='topic+autoBin.binary'></span>

<h3>Description</h3>

<p>Automatically compute optimal cutting points (based on mutual information) to dichotomize quantitative variables. This function can be used as a pre-processing step before using the <span class="pkg">CASMI</span>-based functions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>autoBin.binary(data, index)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="autoBin.binary_+3A_data">data</code></td>
<td>
<p>data frame with variables as columns and observations as rows. The outcome variable (Y) MUST be categorical or discrete. The outcome variable (Y) MUST be the last column.</p>
</td></tr>
<tr><td><code id="autoBin.binary_+3A_index">index</code></td>
<td>
<p>index or a vector of indices of the quantitative features (a.k.a., predictors, factors, independent variables) that need to be automatically categorized.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>'autoBin.binary()' returns the entire data frame after automatically dichotomizing the selected quantitative variable(s).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Using the "iris" dataset embedded in R
data("iris")
head(iris) # The original data

# ---- Dichotomize One Single Feature ----
# Dichotomize the column with index 1.
newData1 &lt;- autoBin.binary(iris, 1)
head(newData1)

# ---- Dichotomize Multiple Features at a Time ----
# Dichotomize the columns with indices 1, 2, 3, and 4.
newData2 &lt;- autoBin.binary(iris, c(1,2,3,4))
head(newData2)

# ---- Dichotomize Features Using Column Names ----
# Dichotomize the columns with the names "Sepal.Length" and "Sepal.Width".
cols_of_interest &lt;- c("Sepal.Length", "Sepal.Width")
col_indices &lt;- which(names(iris) %in% cols_of_interest)
newData3 &lt;- autoBin.binary(iris, col_indices)
head(newData3)

</code></pre>

<hr>
<h2 id='CASMI.mineCombination'>Discover Factor Combinations based on <span class="pkg">CASMI</span></h2><span id='topic+CASMI.mineCombination'></span>

<h3>Description</h3>

<p>The 'CASMI.mineCombination()' function is designed to suggest combinations of factors that are most strongly associated with the outcome in a dataset. This function is partially developed based on the 'CASMI.selectFeatures()' function. (Synonyms for &quot;factor&quot; in this document: &quot;independent variable,&quot; &quot;feature,&quot; and &quot;predictor.&quot;)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CASMI.mineCombination(
  data,
  NumOfVar = NULL,
  NA.handle = "stepwise",
  alpha = 0.05,
  alpha.ind = 0.1,
  intermediate.steps = FALSE,
  kappa.star.cap = 1,
  NumOfComb = 3
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="CASMI.mineCombination_+3A_data">data</code></td>
<td>
<p>data frame with variables as columns and observations as rows. The data MUST include at least one feature (a.k.a., independent variable, predictor, factor) and only one outcome variable (Y). The outcome variable MUST BE THE LAST COLUMN. Both the features and the outcome MUST be categorical or discrete. If variables are not naturally discrete, you may preprocess them using the 'autoBin.binary()' function in the same package.</p>
</td></tr>
<tr><td><code id="CASMI.mineCombination_+3A_numofvar">NumOfVar</code></td>
<td>
<p>the number of variables in a combination (integer). This setting is optional. If NULL, an automatically suggested number of variables will be returned.</p>
</td></tr>
<tr><td><code id="CASMI.mineCombination_+3A_na.handle">NA.handle</code></td>
<td>
<p>method for handling missing values. This parameter is inherited from the 'CASMI.selectFeature()' function. There are three possible options: 'NA.handle = &quot;stepwise&quot;' (default), 'NA.handle = &quot;na.omit&quot;', or 'NA.handle = &quot;NA as a category&quot;'. Check the 'CASMI.selectFeature()' documentation for more details.</p>
</td></tr>
<tr><td><code id="CASMI.mineCombination_+3A_alpha">alpha</code></td>
<td>
<p>level of significance used for the confidence intervals in the results; the default is 0.05.</p>
</td></tr>
<tr><td><code id="CASMI.mineCombination_+3A_alpha.ind">alpha.ind</code></td>
<td>
<p>level of significance used for the initial screening of features based on a test of independence; the default is 0.1. This parameter is also used in the 'CASMI.selectFeature()' function; check the 'CASMI.selectFeature()' documentation for more details.</p>
</td></tr>
<tr><td><code id="CASMI.mineCombination_+3A_intermediate.steps">intermediate.steps</code></td>
<td>
<p>setting for outputting intermediate steps while awaiting the final results. There are two possible settings: 'intermediate.steps = TRUE' or 'intermediate.steps = FALSE'.</p>
</td></tr>
<tr><td><code id="CASMI.mineCombination_+3A_kappa.star.cap">kappa.star.cap</code></td>
<td>
<p>threshold of 'kappa*' for halting the feature selection process. This parameter is inherited from the 'CASMI.selectFeature()' function; check the 'CASMI.selectFeature()' documentation for more details. This setting is applicable only when 'NumOfVar' is set to NULL (default).</p>
</td></tr>
<tr><td><code id="CASMI.mineCombination_+3A_numofcomb">NumOfComb</code></td>
<td>
<p>the number of top combinations to be returned; the default is 3. This setting is used only when a 'NumOfVar' value is defined (not NULL); if 'NumOfVar == NULL', only the automatically suggested combination will be returned.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>'CASMI.mineCombination()' returns the following components:
</p>

<ul>
<li> <p><code>`Outcome`</code>: Name of the outcome variable (last column) in the input dataset.
</p>
</li>
<li> <p><code>`Conf.Level`</code>: Confidence level used for the results.
</p>
</li>
<li> <p><code>`NumOfVar`</code>: The number of variables in each combination.
</p>
</li>
<li> <p><code>`TopResults`</code>: A results data frame. The number of combinations (rows) returned depends on the 'NumOfComb' setting.
</p>
</li>
<li> <p><code>`Comb.Idx`</code>: Indices of the variables in the combination.
</p>
</li>
<li> <p><code>`n`</code>: Number of observations used in the analysis.
</p>
</li>
<li> <p><code>`kappa*`</code>: A comprehensive score reflecting the association between the factor combination and the outcome. A larger 'kappa*' indicates that the factor combination has a stronger association with the outcome. For more information about 'kappa*', please refer to the paper: Shi, J., Zhang, J. and Ge, Y. (2019), &quot;<span class="pkg">CASMI</span>—An Entropic Feature Selection Method in Turing’s Perspective&quot; &lt;doi:10.3390/e21121179&gt;
</p>
</li>
<li> <p><code>`kappa*.low`</code>: Lower bound of the confidence interval for 'kappa*'.
</p>
</li>
<li> <p><code>`kappa*.upr`</code>: Upper bound of the confidence interval for 'kappa*'.
</p>
</li>
<li> <p><code>`SMIz`</code>: Standardized Mutual Information (SMI) (using the z-estimator) between the factor combination and the outcomes.
</p>
</li>
<li> <p><code>`SMIz.low`</code>: Lower bound of the confidence interval for 'SMIz'.
</p>
</li>
<li> <p><code>`SMIz.upr`</code>: Upper bound of the confidence interval for 'SMIz'.
</p>
</li>
<li> <p><code>`p.MIz`</code>: P-value between the factor combination and the outcome using the mutual information test of independence based on the z-estimator.
</p>
</li>
<li> <p><code>`Var.Name`</code>: Names of the variables in the combination.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'># ---- Generate a toy dataset for usage examples: "data" ----
set.seed(123)
n &lt;- 200
x1 &lt;- sample(c("A", "B", "C", "D"), size = n, replace = TRUE, prob = c(0.1, 0.2, 0.3, 0.4))
x2 &lt;- sample(c("W", "X", "Y", "Z"), size = n, replace = TRUE, prob = c(0.4, 0.3, 0.2, 0.1))
x3 &lt;- sample(c("E", "F", "G", "H", "I"), size = n,
             replace = TRUE, prob = c(0.2, 0.3, 0.2, 0.2, 0.1))
x4 &lt;- sample(c("A", "B", "C", "D"), size = n, replace = TRUE)
x5 &lt;- sample(c("L", "M", "N"), size = n, replace = TRUE)
x6 &lt;- sample(c("E", "F", "G", "H", "I"), size = n, replace = TRUE)

# Generate y variable dependent on x1 to x3
x1_num &lt;- as.numeric(factor(x1, levels = c("A", "B", "C", "D")))
x2_num &lt;- as.numeric(factor(x2, levels = c("W", "X", "Y", "Z")))
x3_num &lt;- as.numeric(factor(x3, levels = c("E", "F", "G", "H", "I")))
# Calculate y with added noise
y_numeric &lt;- 3*x1_num + 2*x2_num - 2*x3_num + rnorm(n,mean=0,sd=2)
# Discretize y into categories
y &lt;- cut(y_numeric, breaks = 10, labels = paste0("Category", 1:10))

# Combine into a dataframe
data &lt;- data.frame(x1, x2, x3, x4, x5, x6, y)

# The outcome of the toy dataset is dependent on x1, x2, and x3
# but is independent of x4, x5, and x6.
head(data)


# ---- Usage Examples ----

## Return the suggested combination with the default settings:
CASMI.mineCombination(data)

## Return combinations when the number of variables to be included
## in each combination is specified (e.g., NumOfVar = 2):
CASMI.mineCombination(data, NumOfVar = 2)

## Return combinations when the number of variables to be included
## in each combination is specified (e.g., NumOfVar = 2),
## while the number of top combinations to return is specified
## (e.g., NumOfComb = 2):
CASMI.mineCombination(data,
                     NumOfVar = 2,
                     NumOfComb = 2)

</code></pre>

<hr>
<h2 id='CASMI.selectFeatures'><span class="pkg">CASMI</span> Feature Selection</h2><span id='topic+CASMI.selectFeatures'></span>

<h3>Description</h3>

<p>Selects features that are associated with an outcome while taking into account a sample coverage penalty and feature redundancy. It automatically determines the number of features to be selected, and the chosen features are ranked. (Synonyms for &quot;feature&quot; in this document: &quot;independent variable,&quot; &quot;factor,&quot; and &quot;predictor.&quot;) <br />
For additional information, please refer to the publication: Shi, J., Zhang, J. and Ge, Y. (2019), &quot;<span class="pkg">CASMI</span>—An Entropic Feature Selection Method in Turing’s Perspective&quot; &lt;doi:10.3390/e21121179&gt;
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CASMI.selectFeatures(
  data,
  NA.handle = "stepwise",
  alpha = 0.05,
  alpha.ind = 0.1,
  intermediate.steps = FALSE,
  kappa.star.cap = 1,
  feature.num.cap = ncol(data)
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="CASMI.selectFeatures_+3A_data">data</code></td>
<td>
<p>data frame with variables as columns and observations as rows. The data MUST include at least one feature (a.k.a., independent variable, predictor, factor) and only one outcome variable (Y). The outcome variable MUST BE THE LAST COLUMN. Both the features and the outcome MUST be categorical or discrete. If variables are not naturally discrete, you may preprocess them using the 'autoBin.binary()' function in the same package.</p>
</td></tr>
<tr><td><code id="CASMI.selectFeatures_+3A_na.handle">NA.handle</code></td>
<td>
<p>options for handling NA values in the data. There are three options: 'NA.handle = &quot;stepwise&quot;' (default), 'NA.handle = &quot;na.omit&quot;', and 'NA.handle = &quot;NA as a category&quot;'. (1) 'NA.handle = &quot;stepwise&quot;' excludes NA rows only when a particular variable is being used in a sub-step. For example, suppose we have data (Feature1: A, NA, B; Feature2: C, D, E; Feature3: F, G, H; Outcome: O, P, Q); the second observation will be excluded only when a particular step includes Feature1, but will not be excluded when a step is analyzing only Feature2, Feature3, and the Outcome. This option is designed to take advantage of the maximum possible number of observations. (2) 'NA.handle = &quot;na.omit&quot;' excludes observations with any NA values at the beginning of the analysis. (3) 'NA.handle = &quot;NA as a category&quot;' treats the NA value as a new category. This is designed to be used when NA values in the data have a consistent meaning instead of being missing values. For example, in survey data asking for comments, each NA value might consistently mean &quot;no opinion.&quot;</p>
</td></tr>
<tr><td><code id="CASMI.selectFeatures_+3A_alpha">alpha</code></td>
<td>
<p>level of significance for the confidence intervals in final results. By default, 'alpha = 0.05'.</p>
</td></tr>
<tr><td><code id="CASMI.selectFeatures_+3A_alpha.ind">alpha.ind</code></td>
<td>
<p>level of significance for the mutual information test of independence in step 1 of the features selection (for an initial screening). The smaller the 'alpha.ind', the fewer features are sent to step 2 (&lt;doi:10.3390/e21121179&gt;). By default, 'alpha.ind = 0.1'.</p>
</td></tr>
<tr><td><code id="CASMI.selectFeatures_+3A_intermediate.steps">intermediate.steps</code></td>
<td>
<p>setting for outputting intermediate steps while awaiting the final results. There are two possible settings: 'intermediate.steps = TRUE' or 'intermediate.steps = FALSE'.</p>
</td></tr>
<tr><td><code id="CASMI.selectFeatures_+3A_kappa.star.cap">kappa.star.cap</code></td>
<td>
<p>a threshold of 'kappa*' for halting the feature selection process. The program will automatically terminate at the first feature whose cumulative 'kappa*' value exceeds the 'kappa.star.cap' threshold. By default, 'kappa.star.cap = 1.0', which is the maximum possible value. A lower value may result in fewer final features but reduced computing time.</p>
</td></tr>
<tr><td><code id="CASMI.selectFeatures_+3A_feature.num.cap">feature.num.cap</code></td>
<td>
<p>the maximum number of features to be selected. A lower value may result in fewer final features but less computing time.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>'CASMI.selectFeatures()' returns the following components:
</p>

<ul>
<li> <p><code>`Outcome`</code>: Name of the outcome variable (last column) in the input dataset.
</p>
</li>
<li> <p><code>`Conf.Level`</code>: Confidence level used for the results.
</p>
</li>
<li> <p><code>`KappaStar`</code>: The estimated 'kappa*' of all selected features. A larger 'kappa*' indicates that the selected features have a stronger association with the outcome.
</p>
</li>
<li> <p><code>`KappaStarCI`</code>: The confidence interval of 'kappa*' for all selected features.
</p>
</li>
<li> <p><code>`Results`</code>: A results data frame. The selected features are ranked.
</p>
</li>
<li> <p><code>`Var.Idx`</code>: Column index of the selected feature.
</p>
</li>
<li> <p><code>`n`</code>: Number of observations used in the analysis.
</p>
</li>
<li> <p><code>`cml.kappa*`</code>: The estimated cumulative 'kappa*' score when this particular feature was added to the list. That is, the 'kappa*' score of all currently selected features.
</p>
</li>
<li> <p><code>`SMIz`</code>: The Standardized Mutual Information (SMI) (using the z-estimator) between this particular feature and the outcome.
</p>
</li>
<li> <p><code>`SMIz.Low`</code>: Lower bound of the confidence interval for 'SMIz'.
</p>
</li>
<li> <p><code>`SMIz.Upr`</code>: Upper bound of the confidence interval for 'SMIz'.
</p>
</li>
<li> <p><code>`p.MIz`</code>: P-value between this particular feature and the outcome using the mutual information test of independence based on the z-estimator.
</p>
</li>
<li> <p><code>`Var.Name`</code>: Column name of the selected feature.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'># ---- Generate a toy dataset for usage examples: "data" ----
set.seed(123)
n &lt;- 200
x1 &lt;- sample(c("A", "B", "C", "D"), size = n, replace = TRUE, prob = c(0.1, 0.2, 0.3, 0.4))
x2 &lt;- sample(c("W", "X", "Y", "Z"), size = n, replace = TRUE, prob = c(0.4, 0.3, 0.2, 0.1))
x3 &lt;- sample(c("E", "F", "G", "H", "I"), size = n,
             replace = TRUE, prob = c(0.2, 0.3, 0.2, 0.2, 0.1))
x4 &lt;- sample(c("A", "B", "C", "D"), size = n, replace = TRUE)
x5 &lt;- sample(c("L", "M", "N"), size = n, replace = TRUE)
x6 &lt;- sample(c("E", "F", "G", "H", "I"), size = n, replace = TRUE)

# Generate y variable dependent on x1 to x3
x1_num &lt;- as.numeric(factor(x1, levels = c("A", "B", "C", "D")))
x2_num &lt;- as.numeric(factor(x2, levels = c("W", "X", "Y", "Z")))
x3_num &lt;- as.numeric(factor(x3, levels = c("E", "F", "G", "H", "I")))
# Calculate y with added noise
y_numeric &lt;- 3*x1_num + 2*x2_num - 2*x3_num + rnorm(n,mean=0,sd=2)
# Discretize y into categories
y &lt;- cut(y_numeric, breaks = 10, labels = paste0("Category", 1:10))

# Combine into a dataframe
data &lt;- data.frame(x1, x2, x3, x4, x5, x6, y)

# The outcome of the toy dataset is dependent on x1, x2, and x3
# but is independent of x4, x5, and x6.
head(data)


# ---- Usage Examples ----

## Select features and provide relevant results:
CASMI.selectFeatures(data)

## Adjust 'feature.num.cap' for including fewer features:
## (Note: A lower 'feature.num.cap' value may result in fewer
## final features but less computing time.)
CASMI.selectFeatures(data, feature.num.cap = 2)


</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
