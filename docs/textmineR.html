<!DOCTYPE html><html><head><title>Help for package textmineR</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {textmineR}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#CalcGamma'><p>Calculate a matrix whose rows represent P(topic_i|tokens)</p></a></li>
<li><a href='#CalcHellingerDist'><p>Calculate Hellinger Distance</p></a></li>
<li><a href='#CalcJSDivergence'><p>Calculate Jensen-Shannon Divergence</p></a></li>
<li><a href='#CalcLikelihood'><p>Calculate the log likelihood of a document term matrix given a topic model</p></a></li>
<li><a href='#CalcProbCoherence'><p>Probabilistic coherence of topics</p></a></li>
<li><a href='#CalcTopicModelR2'><p>Calculate the R-squared of a topic model.</p></a></li>
<li><a href='#Cluster2TopicModel'><p>Represent a document clustering as a topic model</p></a></li>
<li><a href='#CreateDtm'><p>Convert a character vector to a document term matrix.</p></a></li>
<li><a href='#CreateTcm'><p>Convert a character vector to a term co-occurrence matrix.</p></a></li>
<li><a href='#Dtm2Docs'><p>Convert a DTM to a Character Vector of documents</p></a></li>
<li><a href='#Dtm2Lexicon'><p>Turn a document term matrix into a list for LDA Gibbs sampling</p></a></li>
<li><a href='#Dtm2Tcm'><p>Turn a document term matrix into a term co-occurrence matrix</p></a></li>
<li><a href='#FitCtmModel'><p>Fit a Correlated Topic Model</p></a></li>
<li><a href='#FitLdaModel'><p>Fit a Latent Dirichlet Allocation topic model</p></a></li>
<li><a href='#FitLsaModel'><p>Fit a topic model using Latent Semantic Analysis</p></a></li>
<li><a href='#GetProbableTerms'><p>Get cluster labels using a &quot;more probable&quot; method of terms</p></a></li>
<li><a href='#GetTopTerms'><p>Get Top Terms for each topic from a topic model</p></a></li>
<li><a href='#Internals'><p>Internal helper functions for <code>textmineR</code></p></a></li>
<li><a href='#LabelTopics'><p>Get some topic labels using a &quot;more probable&quot; method of terms</p></a></li>
<li><a href='#nih'><p>Abstracts and metadata from NIH research grants awarded in 2014</p></a></li>
<li><a href='#posterior'><p>Posterior methods for topic models</p></a></li>
<li><a href='#posterior.lda_topic_model'><p>Draw from the posterior of an LDA topic model</p></a></li>
<li><a href='#predict.ctm_topic_model'><p>Predict method for Correlated topic models (CTM)</p></a></li>
<li><a href='#predict.lda_topic_model'><p>Get predictions from a Latent Dirichlet Allocation model</p></a></li>
<li><a href='#predict.lsa_topic_model'><p>Predict method for LSA topic models</p></a></li>
<li><a href='#SummarizeTopics'><p>Summarize topics in a topic model</p></a></li>
<li><a href='#TermDocFreq'><p>Get term frequencies and document frequencies from a document term matrix.</p></a></li>
<li><a href='#textmineR'><p>textmineR</p></a></li>
<li><a href='#textmineR-deprecated'><p>Deprecated functions in package <span class="pkg">textmineR</span>.</p></a></li>
<li><a href='#TmParallelApply'><p>An OS-independent parallel version of <code>lapply</code></p></a></li>
<li><a href='#update'><p>Update methods for topic models</p></a></li>
<li><a href='#update.lda_topic_model'><p>Update a Latent Dirichlet Allocation topic model with new data</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Functions for Text Mining and Topic Modeling</td>
</tr>
<tr>
<td>Version:</td>
<td>3.0.5</td>
</tr>
<tr>
<td>Description:</td>
<td>An aid for text mining in R, with a syntax that
    should be familiar to experienced R users. Provides a wrapper for several 
    topic models that take similarly-formatted input and give similarly-formatted
    output. Has additional functionality for analyzing and diagnostics for
    topic models.</td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>GNU make, C++11</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.0.2), Matrix</td>
</tr>
<tr>
<td>Imports:</td>
<td>gtools, magrittr, methods, parallel, text2vec (&ge; 0.5),
stopwords, stringr, Rcpp, RcppProgress, RSpectra, utils</td>
</tr>
<tr>
<td>Suggests:</td>
<td>spelling, digest, dplyr, igraph, knitr, lda, MASS, rmarkdown,
SnowballC, stringi, testthat, tibble, tidyr, tidytext,
topicmodels, wordcloud</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://www.rtextminer.com/">https://www.rtextminer.com/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/TommyJones/textmineR/issues">https://github.com/TommyJones/textmineR/issues</a></td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo, RcppProgress</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.1.1</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Language:</td>
<td>en-US</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2021-06-27 23:50:12 UTC; tommy</td>
</tr>
<tr>
<td>Author:</td>
<td>Tommy Jones [aut, cre],
  William Doane [ctb],
  Mattias Attbom [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Tommy Jones &lt;jones.thos.w@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2021-06-28 05:00:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='CalcGamma'>Calculate a matrix whose rows represent P(topic_i|tokens)</h2><span id='topic+CalcGamma'></span>

<h3>Description</h3>

<p>This function takes a phi matrix (P(token|topic)) and a theta 
matrix (P(topic|document)) and returns the phi prime matrix (P(topic|token)). 
Phi prime can be used for classifying new documents and for alternative
topic labels.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CalcGamma(phi, theta, p_docs = NULL, correct = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CalcGamma_+3A_phi">phi</code></td>
<td>
<p>The phi matrix whose rows index topics and columns index words. 
The i, j entries are P(word_i | topic_j)</p>
</td></tr>
<tr><td><code id="CalcGamma_+3A_theta">theta</code></td>
<td>
<p>The theta matrix whose rows index documents and columns index 
topics. The i, j entries are P(topic_i | document_j)</p>
</td></tr>
<tr><td><code id="CalcGamma_+3A_p_docs">p_docs</code></td>
<td>
<p>A numeric vector of length <code>nrow(theta)</code> that is 
proportional to the number of terms in each document. This is
an optional argument. It defaults to NULL</p>
</td></tr>
<tr><td><code id="CalcGamma_+3A_correct">correct</code></td>
<td>
<p>Logical. Do you want to set NAs or NaNs in the final result to
zero? Useful when hitting computational underflow. Defaults to
<code>TRUE</code>. Set to <code>FALSE</code> for troubleshooting or diagnostics.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a <code>matrix</code> whose rows correspond to topics and whose columns
correspond to tokens. The i,j entry corresponds to P(topic_i|token_j)
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load a pre-formatted dtm and topic model
data(nih_sample_topic_model) 

# Make a gamma matrix, P(topic|words)
gamma &lt;- CalcGamma(phi = nih_sample_topic_model$phi, 
                   theta = nih_sample_topic_model$theta)

</code></pre>

<hr>
<h2 id='CalcHellingerDist'>Calculate Hellinger Distance</h2><span id='topic+CalcHellingerDist'></span>

<h3>Description</h3>

<p>Calculates the Hellinger distances or the rows or columns of a 
numeric matrix or for two numeric vectors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CalcHellingerDist(x, y = NULL, by_rows = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CalcHellingerDist_+3A_x">x</code></td>
<td>
<p>A numeric matrix or numeric vector</p>
</td></tr>
<tr><td><code id="CalcHellingerDist_+3A_y">y</code></td>
<td>
<p>A numeric vector. <code>y</code> must be specified if <code>x</code> is a numeric vector.</p>
</td></tr>
<tr><td><code id="CalcHellingerDist_+3A_by_rows">by_rows</code></td>
<td>
<p>Logical. If <code>x</code> is a matrix, should distances be calculated by rows?</p>
</td></tr>
</table>


<h3>Value</h3>

<p>If <code>x</code> is a matrix, this returns an square and symmetric matrix. 
The i,j entries correspond to the Hellinger Distance between the rows of <code>x</code> 
(or the columns of <code>x</code> if <code>by_rows = FALSE</code>). If <code>x</code> and <code>y</code>
are vectors, this returns a numeric scalar whose value is the Hellinger Distance
between <code>x</code> and <code>y</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- rchisq(n = 100, df = 8)
y &lt;- x^2
CalcHellingerDist(x = x, y = y)

mymat &lt;- rbind(x, y)
CalcHellingerDist(x = mymat)
</code></pre>

<hr>
<h2 id='CalcJSDivergence'>Calculate Jensen-Shannon Divergence</h2><span id='topic+CalcJSDivergence'></span>

<h3>Description</h3>

<p>This function calculates the Jensen Shannon Divergence for the 
rows or columns of a numeric matrix or for two numeric vectors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CalcJSDivergence(x, y = NULL, by_rows = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CalcJSDivergence_+3A_x">x</code></td>
<td>
<p>A numeric matrix or numeric vector</p>
</td></tr>
<tr><td><code id="CalcJSDivergence_+3A_y">y</code></td>
<td>
<p>A numeric vector. <code>y</code> must be specified if <code>x</code> is a numeric vector.</p>
</td></tr>
<tr><td><code id="CalcJSDivergence_+3A_by_rows">by_rows</code></td>
<td>
<p>Logical. If <code>x</code> is a matrix, should distances be calculated by rows?</p>
</td></tr>
</table>


<h3>Value</h3>

<p>If <code>x</code> is a matrix, this returns an square and symmetric matrix. 
The i,j entries correspond to the Hellinger Distance between the rows of <code>x</code> 
(or the columns of <code>x</code> if <code>by_rows = FALSE</code>). If <code>x</code> and <code>y</code>
are vectors, this returns a numeric scalar whose value is the Hellinger Distance
between <code>x</code> and <code>y</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- rchisq(n = 100, df = 8)
y &lt;- x^2
CalcJSDivergence(x = x, y = y)

mymat &lt;- rbind(x, y)
CalcJSDivergence(x = mymat)
</code></pre>

<hr>
<h2 id='CalcLikelihood'>Calculate the log likelihood of a document term matrix given a topic model</h2><span id='topic+CalcLikelihood'></span>

<h3>Description</h3>

<p>This function takes a DTM, phi matrix (P(word|topic)), and a theta matrix 
(P(topic|document)) and returns a single value for the likelihood of the 
data given the model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CalcLikelihood(dtm, phi, theta, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CalcLikelihood_+3A_dtm">dtm</code></td>
<td>
<p>The document term matrix of class <code>dgCMatrix</code>.</p>
</td></tr>
<tr><td><code id="CalcLikelihood_+3A_phi">phi</code></td>
<td>
<p>The phi matrix whose rows index topics and columns index words. 
The i, j entries are P(word_i | topic_j)</p>
</td></tr>
<tr><td><code id="CalcLikelihood_+3A_theta">theta</code></td>
<td>
<p>The theta matrix whose rows index documents and columns index topics. 
The i, j entries are P(topic_i | document_j)</p>
</td></tr>
<tr><td><code id="CalcLikelihood_+3A_...">...</code></td>
<td>
<p>Other arguments to pass to <code><a href="#topic+TmParallelApply">TmParallelApply</a></code>. See note, below.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns an object of class <code>numeric</code> corresponding to the log likelihood.
</p>


<h3>Note</h3>

<p>This function performs parallel computation if <code>dtm</code> has more than 3,000
rows. The default is to use all available cores according to <code><a href="parallel.html#topic+detectCores">detectCores</a></code>.
However, this can be modified by passing the <code>cpus</code> argument when calling
this function.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load a pre-formatted dtm and topic model
data(nih_sample_dtm) 
data(nih_sample_topic_model)

# Get the likelihood of the data given the fitted model parameters
ll &lt;- CalcLikelihood(dtm = nih_sample_dtm, 
                     phi = nih_sample_topic_model$phi, 
                     theta = nih_sample_topic_model$theta)

ll
</code></pre>

<hr>
<h2 id='CalcProbCoherence'>Probabilistic coherence of topics</h2><span id='topic+CalcProbCoherence'></span>

<h3>Description</h3>

<p>Calculates the probabilistic coherence of a topic or topics. 
This approximates semantic coherence or human understandability of a topic.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CalcProbCoherence(phi, dtm, M = 5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CalcProbCoherence_+3A_phi">phi</code></td>
<td>
<p>A numeric matrix or a numeric vector. The vector, or rows of the 
matrix represent the numeric relationship between topic(s) and terms. For
example, this relationship may be p(word|topic) or p(topic|word).</p>
</td></tr>
<tr><td><code id="CalcProbCoherence_+3A_dtm">dtm</code></td>
<td>
<p>A document term matrix  or co-occurrence matrix of class 
<code>matrix</code> or whose class inherits from the <code>Matrix</code> package. Columns
must index terms.</p>
</td></tr>
<tr><td><code id="CalcProbCoherence_+3A_m">M</code></td>
<td>
<p>An integer for the number of words to be used in the calculation. 
Defaults to 5</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns an object of class <code>numeric</code> corresponding to the 
probabilistic coherence of the input topic(s).
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load a pre-formatted dtm and topic model
data(nih_sample_topic_model)
data(nih_sample_dtm) 

CalcProbCoherence(phi = nih_sample_topic_model$phi, dtm = nih_sample_dtm, M = 5)
</code></pre>

<hr>
<h2 id='CalcTopicModelR2'>Calculate the R-squared of a topic model.</h2><span id='topic+CalcTopicModelR2'></span>

<h3>Description</h3>

<p>Function to calculate R-squared for a topic model. 
This uses a geometric interpretation of R-squared as the proportion of total distance 
each document is from the center of all the documents that is explained by the model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CalcTopicModelR2(dtm, phi, theta, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CalcTopicModelR2_+3A_dtm">dtm</code></td>
<td>
<p>A documents by terms dimensional document term matrix of class
<code>dgCMatrix</code> or of class <code>matrix</code>.</p>
</td></tr>
<tr><td><code id="CalcTopicModelR2_+3A_phi">phi</code></td>
<td>
<p>A topics by terms dimensional matrix where each entry is p(term_i |topic_j)</p>
</td></tr>
<tr><td><code id="CalcTopicModelR2_+3A_theta">theta</code></td>
<td>
<p>A documents by topics dimensional matrix where each entry is p(topic_j|document_d)</p>
</td></tr>
<tr><td><code id="CalcTopicModelR2_+3A_...">...</code></td>
<td>
<p>Other arguments to be passed to <code><a href="#topic+TmParallelApply">TmParallelApply</a></code>. See note, below.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns an object of class <code>numeric</code> representing the proportion of variability
in the data that is explained by the topic model.
</p>


<h3>Note</h3>

<p>This function performs parallel computation if <code>dtm</code> has more than 3,000
rows. The default is to use all available cores according to <code><a href="parallel.html#topic+detectCores">detectCores</a></code>.
However, this can be modified by passing the <code>cpus</code> argument when calling
this function.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load a pre-formatted dtm and topic model
data(nih_sample_dtm) 
data(nih_sample_topic_model)

# Get the R-squared of the model
r2 &lt;- CalcTopicModelR2(dtm = nih_sample_dtm, 
                     phi = nih_sample_topic_model$phi, 
                     theta = nih_sample_topic_model$theta)


r2
</code></pre>

<hr>
<h2 id='Cluster2TopicModel'>Represent a document clustering as a topic model</h2><span id='topic+Cluster2TopicModel'></span>

<h3>Description</h3>

<p>Represents a document clustering as a topic model of two matrices.
phi: P(term | cluster) theta: P(cluster | document)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Cluster2TopicModel(dtm, clustering, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Cluster2TopicModel_+3A_dtm">dtm</code></td>
<td>
<p>A document term matrix of class <code>dgCMatrix</code> or whose class 
inherits from the <code>Matrix</code> package. Columns must index terms, rows must 
index documents.</p>
</td></tr>
<tr><td><code id="Cluster2TopicModel_+3A_clustering">clustering</code></td>
<td>
<p>A vector of length <code>nrow(dtm)</code> whose entries form a
partitional clustering of the documents.</p>
</td></tr>
<tr><td><code id="Cluster2TopicModel_+3A_...">...</code></td>
<td>
<p>Other arguments to be passed to <code><a href="#topic+TmParallelApply">TmParallelApply</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a list with two elements, phi and theta. 'phi' is a matrix 
whose j-th row represents P(terms | cluster_j). 'theta' is a matrix whose
j-th row represents P(clusters | document_j). Each row of theta should only
have one non-zero element.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Load pre-formatted data for use
data(nih_sample_dtm)
data(nih_sample) 

result &lt;- Cluster2TopicModel(dtm = nih_sample_dtm, 
                             clustering = nih_sample$IC_NAME)

## End(Not run)
</code></pre>

<hr>
<h2 id='CreateDtm'>Convert a character vector to a document term matrix.</h2><span id='topic+CreateDtm'></span>

<h3>Description</h3>

<p>This is the main document term matrix creating function for <code>textmineR</code>.
In most cases, all you need to do is import documents as a character vector in R and then 
run this function to get a document term matrix that is compatible with the 
rest of <code>textmineR</code>'s functionality and many other libraries. <code>CreateDtm</code>
is built on top of the excellent <code><a href="text2vec.html#topic+text2vec">text2vec</a></code> library.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CreateDtm(
  doc_vec,
  doc_names = names(doc_vec),
  ngram_window = c(1, 1),
  stopword_vec = c(stopwords::stopwords("en"), stopwords::stopwords(source = "smart")),
  lower = TRUE,
  remove_punctuation = TRUE,
  remove_numbers = TRUE,
  stem_lemma_function = NULL,
  verbose = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CreateDtm_+3A_doc_vec">doc_vec</code></td>
<td>
<p>A character vector of documents.</p>
</td></tr>
<tr><td><code id="CreateDtm_+3A_doc_names">doc_names</code></td>
<td>
<p>A vector of names for your documents. Defaults to 
<code>names(doc_vec)</code>. If NULL, then doc_names is set to be 
<code>1:length(doc_vec)</code>.</p>
</td></tr>
<tr><td><code id="CreateDtm_+3A_ngram_window">ngram_window</code></td>
<td>
<p>A numeric vector of length 2. The first entry is the minimum
n-gram size; the second entry is the maximum n-gram size. Defaults to
<code>c(1, 1)</code>.</p>
</td></tr>
<tr><td><code id="CreateDtm_+3A_stopword_vec">stopword_vec</code></td>
<td>
<p>A character vector of stopwords you would like to remove.
Defaults to <code>c(stopwords::stopwords("en"), stopwords::stopwords(source = "smart"))</code>. 
If you do not want stopwords removed, specify <code>stopword_vec = c()</code>.</p>
</td></tr>
<tr><td><code id="CreateDtm_+3A_lower">lower</code></td>
<td>
<p>Do you want all words coerced to lower case? Defaults to <code>TRUE</code></p>
</td></tr>
<tr><td><code id="CreateDtm_+3A_remove_punctuation">remove_punctuation</code></td>
<td>
<p>Do you want to convert all non-alpha numeric 
characters to spaces? Defaults to <code>TRUE</code></p>
</td></tr>
<tr><td><code id="CreateDtm_+3A_remove_numbers">remove_numbers</code></td>
<td>
<p>Do you want to convert all numbers to spaces? Defaults 
to <code>TRUE</code></p>
</td></tr>
<tr><td><code id="CreateDtm_+3A_stem_lemma_function">stem_lemma_function</code></td>
<td>
<p>A function that you would like to apply to the 
documents for stemming, lemmatization, or similar. See examples for
usage.</p>
</td></tr>
<tr><td><code id="CreateDtm_+3A_verbose">verbose</code></td>
<td>
<p>Defaults to <code>TRUE</code>. Do you want to see status during 
vectorization?</p>
</td></tr>
<tr><td><code id="CreateDtm_+3A_...">...</code></td>
<td>
<p>Other arguments to be passed to <code><a href="#topic+TmParallelApply">TmParallelApply</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A document term matrix of class <code>dgCMatrix</code>. The rows index 
documents. The columns index terms. The i, j entries represent the count of 
term j appearing in document i.
</p>


<h3>Note</h3>

<p>The following transformations are applied to <code>stopword_vec</code> as 
well as <code>doc_vec</code>: 
<code>lower</code>, 
<code>remove_punctuation</code>, 
<code>remove_numbers</code>
</p>
<p>See <code><a href="tm.html#topic+stopwords">stopwords</a></code> for details on the default to the 
<code>stopword_vec</code> argument.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
data(nih_sample)

# DTM of unigrams and bigrams
dtm &lt;- CreateDtm(doc_vec = nih_sample$ABSTRACT_TEXT,
                 doc_names = nih_sample$APPLICATION_ID, 
                 ngram_window = c(1, 2))

# DTM of unigrams with Porter's stemmer applied
dtm &lt;- CreateDtm(doc_vec = nih_sample$ABSTRACT_TEXT,
                 doc_names = nih_sample$APPLICATION_ID,
                 stem_lemma_function = function(x) SnowballC::wordStem(x, "porter"))

## End(Not run)
</code></pre>

<hr>
<h2 id='CreateTcm'>Convert a character vector to a term co-occurrence matrix.</h2><span id='topic+CreateTcm'></span>

<h3>Description</h3>

<p>This is the main term co-occurrence matrix creating function for <code>textmineR</code>.
In most cases, all you need to do is import documents as a character vector in R and then 
run this function to get a term co-occurrence matrix that is compatible with the 
rest of <code>textmineR</code>'s functionality and many other libraries. <code>CreateTcm</code>
is built on top of the excellent <code><a href="text2vec.html#topic+text2vec">text2vec</a></code> library.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CreateTcm(
  doc_vec,
  skipgram_window = Inf,
  ngram_window = c(1, 1),
  stopword_vec = c(stopwords::stopwords("en"), stopwords::stopwords(source = "smart")),
  lower = TRUE,
  remove_punctuation = TRUE,
  remove_numbers = TRUE,
  stem_lemma_function = NULL,
  verbose = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="CreateTcm_+3A_doc_vec">doc_vec</code></td>
<td>
<p>A character vector of documents.</p>
</td></tr>
<tr><td><code id="CreateTcm_+3A_skipgram_window">skipgram_window</code></td>
<td>
<p>An integer window, from <code>0</code> to <code>Inf</code> for 
skip-grams. Defaults to <code>Inf</code>. See 'Details', below.</p>
</td></tr>
<tr><td><code id="CreateTcm_+3A_ngram_window">ngram_window</code></td>
<td>
<p>A numeric vector of length 2. The first entry is the minimum
n-gram size; the second entry is the maximum n-gram size. Defaults to
<code>c(1, 1)</code>. Must be <code>c(1, 1)</code> if <code>skipgram_window</code> is 
not <code>0</code> or <code>Inf</code>.</p>
</td></tr>
<tr><td><code id="CreateTcm_+3A_stopword_vec">stopword_vec</code></td>
<td>
<p>A character vector of stopwords you would like to remove.
Defaults to <code>c(stopwords::stopwords("en"), stopwords::stopwords(source = "smart"))</code>. 
If you do not want stopwords removed, specify <code>stopword_vec = c()</code>.</p>
</td></tr>
<tr><td><code id="CreateTcm_+3A_lower">lower</code></td>
<td>
<p>Do you want all words coerced to lower case? Defaults to <code>TRUE</code></p>
</td></tr>
<tr><td><code id="CreateTcm_+3A_remove_punctuation">remove_punctuation</code></td>
<td>
<p>Do you want to convert all non-alpha numeric 
characters to spaces? Defaults to <code>TRUE</code></p>
</td></tr>
<tr><td><code id="CreateTcm_+3A_remove_numbers">remove_numbers</code></td>
<td>
<p>Do you want to convert all numbers to spaces? Defaults 
to <code>TRUE</code></p>
</td></tr>
<tr><td><code id="CreateTcm_+3A_stem_lemma_function">stem_lemma_function</code></td>
<td>
<p>A function that you would like to apply to the 
documents for stemming, lemmatization, or similar. See examples for
usage.</p>
</td></tr>
<tr><td><code id="CreateTcm_+3A_verbose">verbose</code></td>
<td>
<p>Defaults to <code>TRUE</code>. Do you want to see status during 
vectorization?</p>
</td></tr>
<tr><td><code id="CreateTcm_+3A_...">...</code></td>
<td>
<p>Other arguments to be passed to <code><a href="#topic+TmParallelApply">TmParallelApply</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Setting <code>skipgram_window</code> counts the number of times that term
<code>j</code> appears within <code>skipgram_window</code> places of term <code>i</code>.
<code>Inf</code> and <code>0</code> create somewhat special TCMs. Setting <code>skipgram_window</code>
to <code>Inf</code> counts the number of documents in which term <code>j</code> 
and term <code>i</code> occur together. Setting <code>skipgram_window</code>
to <code>0</code> counts the number of terms shared by document <code>j</code> 
and document <code>i</code>. A TCM where <code>skipgram_window</code> 
is <code>0</code> is the only TCM that will be symmetric.
</p>


<h3>Value</h3>

<p>A document term matrix of class <code>dgCMatrix</code>. The rows index 
documents. The columns index terms. The i, j entries represent the count of 
term j appearing in document i.
</p>


<h3>Note</h3>

<p>The following transformations are applied to <code>stopword_vec</code> as 
well as <code>doc_vec</code>: 
<code>lower</code>, 
<code>remove_punctuation</code>, 
<code>remove_numbers</code>
</p>
<p>See <code><a href="tm.html#topic+stopwords">stopwords</a></code> for details on the default to the 
<code>stopword_vec</code> argument.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
data(nih_sample)

# TCM of unigrams and bigrams
tcm &lt;- CreateTcm(doc_vec = nih_sample$ABSTRACT_TEXT,
                 skipgram_window = Inf, 
                 ngram_window = c(1, 2))

# TCM of unigrams and a skip=gram window of 3, applying Porter's word stemmer
tcm &lt;- CreateTcm(doc_vec = nih_sample$ABSTRACT_TEXT,
                 skipgram_window = 3,
                 stem_lemma_function = function(x) SnowballC::wordStem(x, "porter"))

## End(Not run)
</code></pre>

<hr>
<h2 id='Dtm2Docs'>Convert a DTM to a Character Vector of documents</h2><span id='topic+Dtm2Docs'></span>

<h3>Description</h3>

<p>This function takes a sparse matrix (DTM) as input and returns a character vector
whose length is equal to the number of rows of the input DTM.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Dtm2Docs(dtm, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Dtm2Docs_+3A_dtm">dtm</code></td>
<td>
<p>A sparse Matrix from the matrix package whose rownames correspond 
to documents and colnames correspond to words</p>
</td></tr>
<tr><td><code id="Dtm2Docs_+3A_...">...</code></td>
<td>
<p>Other arguments to be passed to <code><a href="#topic+TmParallelApply">TmParallelApply</a></code>. See note, below.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a character vector. Each entry of this vector corresponds to the rows
of <code>dtm</code>.
</p>


<h3>Note</h3>

<p>This function performs parallel computation if <code>dtm</code> has more than 3,000
rows. The default is to use all available cores according to <code><a href="parallel.html#topic+detectCores">detectCores</a></code>.
However, this can be modified by passing the <code>cpus</code> argument when calling
this function.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load a pre-formatted dtm and topic model
data(nih_sample)
data(nih_sample_dtm) 

# see the original documents
nih_sample$ABSTRACT_TEXT[ 1:3 ]

# see the new documents re-structured from the DTM
new_docs &lt;- Dtm2Docs(dtm = nih_sample_dtm)

new_docs[ 1:3 ]

</code></pre>

<hr>
<h2 id='Dtm2Lexicon'>Turn a document term matrix into a list for LDA Gibbs sampling</h2><span id='topic+Dtm2Lexicon'></span>

<h3>Description</h3>

<p>Represents a document term matrix as a list.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Dtm2Lexicon(dtm, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Dtm2Lexicon_+3A_dtm">dtm</code></td>
<td>
<p>A document term matrix (or term co-occurrence matrix) of class 
<code>dgCMatrix</code>.</p>
</td></tr>
<tr><td><code id="Dtm2Lexicon_+3A_...">...</code></td>
<td>
<p>Other arguments to be passed to <code><a href="#topic+TmParallelApply">TmParallelApply</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a list. Each element of the list represents a row of the input
matrix. Each list element contains a numeric vector with as many entries as
tokens in the original document. The entries are the column index for that token, minus 1.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Load pre-formatted data for use
data(nih_sample_dtm)

result &lt;- Dtm2Lexicon(dtm = nih_sample_dtm, 
                      cpus = 2)

## End(Not run)
</code></pre>

<hr>
<h2 id='Dtm2Tcm'>Turn a document term matrix into a term co-occurrence matrix</h2><span id='topic+Dtm2Tcm'></span>

<h3>Description</h3>

<p>Turn a document term matrix, whose rows index documents and 
whose columns index terms, into a term co-occurrence matrix. A term co-occurrence
matrix's rows and columns both index terms. See <code>details</code>, below.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Dtm2Tcm(dtm)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Dtm2Tcm_+3A_dtm">dtm</code></td>
<td>
<p>A document term matrix, generally of class <code>dgCMatrix</code>, though
other classes, such as <code>dgTMatrix</code>, may also work without issue.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a square <code>dgCMatrix</code> whose rows and columns both index
terms. The i, j entries of this matrix represent the count of term j across
documents containing term i. Note that, while square, this matrix is not
symmetric.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(nih_sample_dtm)

tcm &lt;- Dtm2Tcm(nih_sample_dtm)
</code></pre>

<hr>
<h2 id='FitCtmModel'>Fit a Correlated Topic Model</h2><span id='topic+FitCtmModel'></span>

<h3>Description</h3>

<p>A wrapper for the <a href="topicmodels.html#topic+CTM">CTM</a> function based on 
Blei's original code that returns a nicely-formatted topic model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>FitCtmModel(
  dtm,
  k,
  calc_coherence = TRUE,
  calc_r2 = FALSE,
  return_all = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="FitCtmModel_+3A_dtm">dtm</code></td>
<td>
<p>A document term matrix of class <code>dgCMatrix</code></p>
</td></tr>
<tr><td><code id="FitCtmModel_+3A_k">k</code></td>
<td>
<p>Number of topics</p>
</td></tr>
<tr><td><code id="FitCtmModel_+3A_calc_coherence">calc_coherence</code></td>
<td>
<p>Do you want to calculate probabilistic coherence of topics
after the model is trained? Defaults to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="FitCtmModel_+3A_calc_r2">calc_r2</code></td>
<td>
<p>Do you want to calculate R-squared after the model is trained?
Defaults to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="FitCtmModel_+3A_return_all">return_all</code></td>
<td>
<p>Logical. Do you want the raw results of the underlying 
function returned along with the formatted results? Defaults to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="FitCtmModel_+3A_...">...</code></td>
<td>
<p>Other arguments to pass to <a href="topicmodels.html#topic+CTM">CTM</a> or <a href="#topic+TmParallelApply">TmParallelApply</a>.
See note below.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a list with a minimum of two objects, <code>phi</code> and 
<code>theta</code>. The rows of <code>phi</code> index topics and the columns index tokens.
The rows of <code>theta</code> index documents and the columns index topics.
</p>


<h3>Note</h3>

<p>When passing additional arguments to <a href="topicmodels.html#topic+CTM">CTM</a>, you must unlist the 
elements in the <code>control</code> argument and pass them one by one. See examples for
how to dot this correctly.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load a pre-formatted dtm 
data(nih_sample_dtm) 

# Fit a CTM model on a sample of documents
model &lt;- FitCtmModel(dtm = nih_sample_dtm[ sample(1:nrow(nih_sample_dtm) , 10) , ], 
                     k = 3, return_all = FALSE)
                     
# the correct way to pass control arguments to CTM
## Not run: 
topics_CTM &lt;- FitCtmModel(
    dtm = nih_sample_dtm[ sample(1:nrow(nih_sample_dtm) , 10) , ],
    k = 10,
    calc_coherence = TRUE,
    calc_r2 = TRUE,
    return_all = TRUE,
    estimate.beta = TRUE,
    verbose = 0,
    prefix = tempfile(),
    save = 0,
    keep = 0,
    seed = as.integer(Sys.time()),
    nstart = 1L,
    best = TRUE,
    var = list(iter.max = 500, tol = 10^-6),
    em = list(iter.max = 1000, tol = 10^-4),
    initialize = "random",
    cg = list(iter.max = 500, tol = 10^-5)
)

## End(Not run)
</code></pre>

<hr>
<h2 id='FitLdaModel'>Fit a Latent Dirichlet Allocation topic model</h2><span id='topic+FitLdaModel'></span>

<h3>Description</h3>

<p>Fit a Latent Dirichlet Allocation topic model using collapsed Gibbs sampling.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>FitLdaModel(
  dtm,
  k,
  iterations = NULL,
  burnin = -1,
  alpha = 0.1,
  beta = 0.05,
  optimize_alpha = FALSE,
  calc_likelihood = FALSE,
  calc_coherence = TRUE,
  calc_r2 = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="FitLdaModel_+3A_dtm">dtm</code></td>
<td>
<p>A document term matrix or term co-occurrence matrix of class dgCMatrix</p>
</td></tr>
<tr><td><code id="FitLdaModel_+3A_k">k</code></td>
<td>
<p>Integer number of topics</p>
</td></tr>
<tr><td><code id="FitLdaModel_+3A_iterations">iterations</code></td>
<td>
<p>Integer number of iterations for the Gibbs sampler to run. A
future version may include automatic stopping criteria.</p>
</td></tr>
<tr><td><code id="FitLdaModel_+3A_burnin">burnin</code></td>
<td>
<p>Integer number of burnin iterations. If <code>burnin</code> is greater than -1,
the resulting &quot;phi&quot; and &quot;theta&quot; matrices are an average over all iterations
greater than <code>burnin</code>.</p>
</td></tr>
<tr><td><code id="FitLdaModel_+3A_alpha">alpha</code></td>
<td>
<p>Vector of length <code>k</code> for asymmetric or a number for symmetric.
This is the prior for topics over documents</p>
</td></tr>
<tr><td><code id="FitLdaModel_+3A_beta">beta</code></td>
<td>
<p>Vector of length <code>ncol(dtm)</code> for asymmetric or a number for symmetric.
This is the prior for words over topics.</p>
</td></tr>
<tr><td><code id="FitLdaModel_+3A_optimize_alpha">optimize_alpha</code></td>
<td>
<p>Logical. Do you want to optimize alpha every 10 Gibbs iterations?
Defaults to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="FitLdaModel_+3A_calc_likelihood">calc_likelihood</code></td>
<td>
<p>Do you want to calculate the likelihood every 10 Gibbs iterations?
Useful for assessing convergence. Defaults to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="FitLdaModel_+3A_calc_coherence">calc_coherence</code></td>
<td>
<p>Do you want to calculate probabilistic coherence of topics
after the model is trained? Defaults to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="FitLdaModel_+3A_calc_r2">calc_r2</code></td>
<td>
<p>Do you want to calculate R-squared after the model is trained?
Defaults to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="FitLdaModel_+3A_...">...</code></td>
<td>
<p>Other arguments to be passed to <code><a href="#topic+TmParallelApply">TmParallelApply</a></code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>EXPLAIN IMPLEMENTATION DETAILS
</p>


<h3>Value</h3>

<p>Returns an S3 object of class c(&quot;LDA&quot;, &quot;TopicModel&quot;). DESCRIBE MORE
</p>


<h3>Examples</h3>

<pre><code class='language-R'># load some data
data(nih_sample_dtm)

# fit a model 
set.seed(12345)
m &lt;- FitLdaModel(dtm = nih_sample_dtm[1:20,], k = 5,
                 iterations = 200, burnin = 175)

str(m)

# predict on held-out documents using gibbs sampling "fold in"
p1 &lt;- predict(m, nih_sample_dtm[21:100,], method = "gibbs",
              iterations = 200, burnin = 175)

# predict on held-out documents using the dot product method
p2 &lt;- predict(m, nih_sample_dtm[21:100,], method = "dot")

# compare the methods
barplot(rbind(p1[1,],p2[1,]), beside = TRUE, col = c("red", "blue")) 
</code></pre>

<hr>
<h2 id='FitLsaModel'>Fit a topic model using Latent Semantic Analysis</h2><span id='topic+FitLsaModel'></span>

<h3>Description</h3>

<p>A wrapper for <code>RSpectra::svds</code> that returns 
a nicely-formatted latent semantic analysis topic model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>FitLsaModel(dtm, k, calc_coherence = TRUE, return_all = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="FitLsaModel_+3A_dtm">dtm</code></td>
<td>
<p>A document term matrix of class <code>Matrix::dgCMatrix</code></p>
</td></tr>
<tr><td><code id="FitLsaModel_+3A_k">k</code></td>
<td>
<p>Number of topics</p>
</td></tr>
<tr><td><code id="FitLsaModel_+3A_calc_coherence">calc_coherence</code></td>
<td>
<p>Do you want to calculate probabilistic coherence of topics
after the model is trained? Defaults to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="FitLsaModel_+3A_return_all">return_all</code></td>
<td>
<p>Should all objects returned from <code>RSpectra::svds</code> be
returned here? Defaults to <code>FALSE</code></p>
</td></tr>
<tr><td><code id="FitLsaModel_+3A_...">...</code></td>
<td>
<p>Other arguments to pass to <code><a href="RSpectra.html#topic+svds">svds</a></code> through 
its <code>opts</code> parameter.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Latent semantic analysis, LSA, uses single value decomposition to
factor the document term matrix. In many LSA applications, TF-IDF weights are
applied to the DTM before model fitting. However, this is not strictly 
necessary.
</p>


<h3>Value</h3>

<p>Returns a list with a minimum of three objects: <code>phi</code>,  
<code>theta</code>, and <code>sv</code>. The rows of <code>phi</code> index topics and the 
columns index tokens. The rows of <code>theta</code> index documents and the 
columns index topics. <code>sv</code> is a vector of singular values.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load a pre-formatted dtm 
data(nih_sample_dtm) 

# Convert raw word counts to TF-IDF frequency weights
idf &lt;- log(nrow(nih_sample_dtm) / Matrix::colSums(nih_sample_dtm &gt; 0))

dtm_tfidf &lt;- Matrix::t(nih_sample_dtm) * idf

dtm_tfidf &lt;- Matrix::t(dtm_tfidf)

# Fit an LSA model
model &lt;- FitLsaModel(dtm = dtm_tfidf, k = 5)

str(model)

</code></pre>

<hr>
<h2 id='GetProbableTerms'>Get cluster labels using a &quot;more probable&quot; method of terms</h2><span id='topic+GetProbableTerms'></span>

<h3>Description</h3>

<p>Function extracts probable terms from a set of documents. 
Probable here implies more probable than in a corpus overall.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>GetProbableTerms(docnames, dtm, p_terms = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="GetProbableTerms_+3A_docnames">docnames</code></td>
<td>
<p>A character vector of rownames of dtm for set of documents</p>
</td></tr>
<tr><td><code id="GetProbableTerms_+3A_dtm">dtm</code></td>
<td>
<p>A document term matrix of class <code>matrix</code> or <code>dgCMatrix</code>.</p>
</td></tr>
<tr><td><code id="GetProbableTerms_+3A_p_terms">p_terms</code></td>
<td>
<p>If not NULL (the default), a numeric vector representing the 
probability of each term in the corpus whose names correspond to colnames(dtm).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a numeric vector of the format p_terms. The entries of the vectors 
correspond to the difference in the probability of drawing a term from the 
set of documents given by docnames and the probability of drawing that term 
from the corpus overall (p_terms).
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load a pre-formatted dtm and topic model
data(nih_sample_topic_model)
data(nih_sample_dtm) 

# documents with a topic proportion of .25 or higher for topic 2
mydocs &lt;- rownames(nih_sample_topic_model$theta)[ nih_sample_topic_model$theta[ , 2 ] &gt;= 0.25 ] 

term_probs &lt;- Matrix::colSums(nih_sample_dtm) / sum(Matrix::colSums(nih_sample_dtm))

GetProbableTerms(docnames = mydocs, dtm = nih_sample_dtm, p_terms = term_probs)

</code></pre>

<hr>
<h2 id='GetTopTerms'>Get Top Terms for each topic from a topic model</h2><span id='topic+GetTopTerms'></span>

<h3>Description</h3>

<p>Takes topics by terms matrix and returns top M terms for each topic
</p>


<h3>Usage</h3>

<pre><code class='language-R'>GetTopTerms(phi, M, return_matrix = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="GetTopTerms_+3A_phi">phi</code></td>
<td>
<p>A matrix whose rows index topics and columns index words</p>
</td></tr>
<tr><td><code id="GetTopTerms_+3A_m">M</code></td>
<td>
<p>An integer for the number of terms to return</p>
</td></tr>
<tr><td><code id="GetTopTerms_+3A_return_matrix">return_matrix</code></td>
<td>
<p>Do you want a <code>matrix</code> or <code>data.frame</code>/<code>tibble</code>
returned? Defaults to <code>TRUE</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>If <code>return_matrix = TRUE</code> (the default) then a matrix. Otherwise,
returns a <code>data.frame</code> or <code>tibble</code> whose columns correspond to a topic and
whose m-th row correspond to the m-th top term from the input <code>phi</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load a pre-formatted dtm and topic model
data(nih_sample_topic_model) 

top_terms &lt;- GetTopTerms(phi = nih_sample_topic_model$phi, M = 5)

str(top_terms)
</code></pre>

<hr>
<h2 id='Internals'>Internal helper functions for <code>textmineR</code></h2><span id='topic+CalcLikelihoodC'></span><span id='topic+CalcSumSquares'></span><span id='topic+dtm_to_lexicon_c'></span><span id='topic+Dtm2DocsC'></span><span id='topic+fit_lda_c'></span><span id='topic+predict_lda_c'></span><span id='topic+Hellinger_cpp'></span><span id='topic+HellingerMat'></span><span id='topic+JSD_cpp'></span><span id='topic+JSDmat'></span>

<h3>Description</h3>

<p>These functions are internal helper functions for <code>textmineR</code>. They are not
designed to be called by users. Each of the functions here are C++ functions. 
There are corresponding R functions that call these that add additional functionality.
</p>

<hr>
<h2 id='LabelTopics'>Get some topic labels using a &quot;more probable&quot; method of terms</h2><span id='topic+LabelTopics'></span>

<h3>Description</h3>

<p>Function calls <code><a href="#topic+GetProbableTerms">GetProbableTerms</a></code> with some 
rules to get topic labels. This function is in &quot;super-ultra-mega alpha&quot;; use
at your own risk/discretion.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>LabelTopics(assignments, dtm, M = 2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="LabelTopics_+3A_assignments">assignments</code></td>
<td>
<p>A documents by topics matrix similar to <code>theta</code>. 
This will work best if this matrix is sparse, with only a few non-zero topics 
per document.</p>
</td></tr>
<tr><td><code id="LabelTopics_+3A_dtm">dtm</code></td>
<td>
<p>A document term matrix of class <code>matrix</code> or <code>dgCMatrix</code>.
The columns of <code>dtm</code> should be n-grams whose colnames have a &quot;_&quot; where
spaces would be between the words.</p>
</td></tr>
<tr><td><code id="LabelTopics_+3A_m">M</code></td>
<td>
<p>The number of n-gram labels you want to return. Defaults to 2</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a <code>matrix</code> whose rows correspond to topics and whose
j-th column corresponds to the j-th &quot;best&quot; label assignment.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># make a dtm with unigrams and bigrams
data(nih_sample_topic_model)

m &lt;- nih_sample_topic_model

assignments &lt;- t(apply(m$theta, 1, function(x){
  x[ x &lt; 0.05 ] &lt;- 0
  x / sum(x)
}))

assignments[is.na(assignments)] &lt;- 0

labels &lt;- LabelTopics(assignments = assignments, dtm = m$data, M = 2)

</code></pre>

<hr>
<h2 id='nih'>Abstracts and metadata from NIH research grants awarded in 2014</h2><span id='topic+nih'></span><span id='topic+nih_sample'></span><span id='topic+nih_sample_dtm'></span><span id='topic+nih_sample_topic_model'></span>

<h3>Description</h3>

<p>This dataset holds information on research grants awarded by the National
Institutes of Health (NIH) in 2014. The data set was downloaded in approximately
January of 2015 from <a href="https://exporter.nih.gov/ExPORTER_Catalog.aspx">https://exporter.nih.gov/ExPORTER_Catalog.aspx</a>. It
includes both 'projects' and 'abstracts' files.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  data("nih_sample")
  data("nih_sample_dtm")
  data("nih_sample_topic_model")
  </code></pre>


<h3>Format</h3>

<p>A <code>data.frame</code> of 100 randomly-sampled grants' abstracts and metadata.
A <code>dgCMatrix</code> representing the document term matrix of abstracts from
100 randomly-sampled grants.
A <code>list</code> containing a topic model of these 100 sampled grants.
</p>


<h3>Source</h3>

<p>National Institutes of Health ExPORTER <a href="https://exporter.nih.gov/ExPORTER_Catalog.aspx">https://exporter.nih.gov/ExPORTER_Catalog.aspx</a></p>

<hr>
<h2 id='posterior'>Posterior methods for topic models</h2><span id='topic+posterior'></span>

<h3>Description</h3>

<p><code>posterior</code> will draw from the posterior distribution of a
topic model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>posterior(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="posterior_+3A_object">object</code></td>
<td>
<p>An existing trained topic model</p>
</td></tr>
<tr><td><code id="posterior_+3A_...">...</code></td>
<td>
<p>Additional arguments to the call</p>
</td></tr>
</table>

<hr>
<h2 id='posterior.lda_topic_model'>Draw from the posterior of an LDA topic model</h2><span id='topic+posterior.lda_topic_model'></span>

<h3>Description</h3>

<p>This function takes an object of class <code>lda_topic_model</code> and
draws samples from the posterior of either <code>phi</code> or <code>theta</code>. This is 
useful for quantifying uncertainty around parametersof the final model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'lda_topic_model'
posterior(object, which = "theta", num_samples = 100, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="posterior.lda_topic_model_+3A_object">object</code></td>
<td>
<p>An object of class <code>lda_topic_model</code></p>
</td></tr>
<tr><td><code id="posterior.lda_topic_model_+3A_which">which</code></td>
<td>
<p>A character of either 'theta' or 'phi', indicating from which
matrix to draw posterior samples</p>
</td></tr>
<tr><td><code id="posterior.lda_topic_model_+3A_num_samples">num_samples</code></td>
<td>
<p>Integer number of samples to draw</p>
</td></tr>
<tr><td><code id="posterior.lda_topic_model_+3A_...">...</code></td>
<td>
<p>Other arguments to be passed to <code><a href="#topic+TmParallelApply">TmParallelApply</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a data frame where each row is a single sample from the posterior. 
Each column is the distribution over a single parameter. The variable <code>var</code>
is a facet for subsetting by document (for theta) or topic (for phi).
</p>


<h3>References</h3>

<p>Heinrich, G. (2005) Parameter estimation for text analysis. Technical report. 
<a href="http://www.arbylon.net/publications/text-est.pdf">http://www.arbylon.net/publications/text-est.pdf</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
a &lt;- posterior(object = nih_sample_topic_model, which = "theta", num_samples = 20)

plot(density(a$t1[a$var == "8693991"]))

b &lt;- posterior(object = nih_sample_topic_model, which = "phi", num_samples = 20)

plot(denisty(b$research[b$var == "t_5"]))

## End(Not run)
</code></pre>

<hr>
<h2 id='predict.ctm_topic_model'>Predict method for Correlated topic models (CTM)</h2><span id='topic+predict.ctm_topic_model'></span>

<h3>Description</h3>

<p>Obtains predictions of topics for new documents from a fitted CTM model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'ctm_topic_model'
predict(object, newdata, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.ctm_topic_model_+3A_object">object</code></td>
<td>
<p>a fitted object of class &quot;ctm_topic_model&quot;</p>
</td></tr>
<tr><td><code id="predict.ctm_topic_model_+3A_newdata">newdata</code></td>
<td>
<p>a DTM or TCM of class dgCMatrix or a numeric vector</p>
</td></tr>
<tr><td><code id="predict.ctm_topic_model_+3A_...">...</code></td>
<td>
<p>further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a &quot;theta&quot; matrix with one row per document and one column per topic
</p>


<h3>Note</h3>

<p>Predictions for this method are performed using the &quot;dot&quot; method as described
in the textmineR vignette &quot;c_topic_modeling&quot;.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load a pre-formatted dtm 
## Not run: 
data(nih_sample_dtm) 

model &lt;- FitCtmModel(dtm = nih_sample_dtm[1:20,], k = 3,
                     calc_coherence = FALSE, calc_r2 = FALSE)

# Get predictions on the next 50 documents
pred &lt;- predict(model, nih_sample_dtm[21:100,])

## End(Not run)
</code></pre>

<hr>
<h2 id='predict.lda_topic_model'>Get predictions from a Latent Dirichlet Allocation model</h2><span id='topic+predict.lda_topic_model'></span>

<h3>Description</h3>

<p>Obtains predictions of topics for new documents from a fitted LDA model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'lda_topic_model'
predict(
  object,
  newdata,
  method = c("gibbs", "dot"),
  iterations = NULL,
  burnin = -1,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.lda_topic_model_+3A_object">object</code></td>
<td>
<p>a fitted object of class <code>lda_topic_model</code></p>
</td></tr>
<tr><td><code id="predict.lda_topic_model_+3A_newdata">newdata</code></td>
<td>
<p>a DTM or TCM of class <code>dgCMatrix</code> or a numeric vector</p>
</td></tr>
<tr><td><code id="predict.lda_topic_model_+3A_method">method</code></td>
<td>
<p>one of either &quot;gibbs&quot; or &quot;dot&quot;. If &quot;gibbs&quot; Gibbs sampling is used
and <code>iterations</code> must be specified.</p>
</td></tr>
<tr><td><code id="predict.lda_topic_model_+3A_iterations">iterations</code></td>
<td>
<p>If <code>method = "gibbs"</code>, an integer number of iterations 
for the Gibbs sampler to run. A future version may include automatic stopping criteria.</p>
</td></tr>
<tr><td><code id="predict.lda_topic_model_+3A_burnin">burnin</code></td>
<td>
<p>If <code>method = "gibbs"</code>, an integer number of burnin iterations. 
If <code>burnin</code> is greater than -1, the entries of the resulting &quot;theta&quot; matrix 
are an average over all iterations greater than <code>burnin</code>.</p>
</td></tr>
<tr><td><code id="predict.lda_topic_model_+3A_...">...</code></td>
<td>
<p>Other arguments to be passed to <code><a href="#topic+TmParallelApply">TmParallelApply</a></code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>a &quot;theta&quot; matrix with one row per document and one column per topic
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# load some data
data(nih_sample_dtm)

# fit a model 
set.seed(12345)

m &lt;- FitLdaModel(dtm = nih_sample_dtm[1:20,], k = 5,
                 iterations = 200, burnin = 175)

str(m)

# predict on held-out documents using gibbs sampling "fold in"
p1 &lt;- predict(m, nih_sample_dtm[21:100,], method = "gibbs",
              iterations = 200, burnin = 175)

# predict on held-out documents using the dot product method
p2 &lt;- predict(m, nih_sample_dtm[21:100,], method = "dot")

# compare the methods
barplot(rbind(p1[1,],p2[1,]), beside = TRUE, col = c("red", "blue")) 

## End(Not run)
</code></pre>

<hr>
<h2 id='predict.lsa_topic_model'>Predict method for LSA topic models</h2><span id='topic+predict.lsa_topic_model'></span>

<h3>Description</h3>

<p>Obtains predictions of topics for new documents from a fitted LSA model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'lsa_topic_model'
predict(object, newdata, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.lsa_topic_model_+3A_object">object</code></td>
<td>
<p>a fitted object of class &quot;lsa_topic_model&quot;</p>
</td></tr>
<tr><td><code id="predict.lsa_topic_model_+3A_newdata">newdata</code></td>
<td>
<p>a DTM or TCM of class dgCMatrix or a numeric vector</p>
</td></tr>
<tr><td><code id="predict.lsa_topic_model_+3A_...">...</code></td>
<td>
<p>further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a &quot;theta&quot; matrix with one row per document and one column per topic
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load a pre-formatted dtm 
data(nih_sample_dtm) 

# Convert raw word counts to TF-IDF frequency weights
idf &lt;- log(nrow(nih_sample_dtm) / Matrix::colSums(nih_sample_dtm &gt; 0))

dtm_tfidf &lt;- Matrix::t(nih_sample_dtm) * idf

dtm_tfidf &lt;- Matrix::t(dtm_tfidf)

# Fit an LSA model on the first 50 documents
model &lt;- FitLsaModel(dtm = dtm_tfidf[1:50,], k = 5)

# Get predictions on the next 50 documents
pred &lt;- predict(model, dtm_tfidf[51:100,])
</code></pre>

<hr>
<h2 id='SummarizeTopics'>Summarize topics in a topic model</h2><span id='topic+SummarizeTopics'></span>

<h3>Description</h3>

<p>Create a data frame summarizing the contents of each topic in a model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SummarizeTopics(model)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SummarizeTopics_+3A_model">model</code></td>
<td>
<p>A list (or S3 object) with three named matrices: phi, theta, and gamma.
These conform to outputs of many of <a href="#topic+textmineR">textmineR</a>'s native
topic modeling functions such as <a href="#topic+FitLdaModel">FitLdaModel</a>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>'prevalence' is normalized to sum to 100. If your 'theta' matrix has
negative values (as may be the case with an LSA model), a constant is
added so that the least prevalent topic has a prevalence of 0.
</p>
<p>'coherence' is calculated using <a href="#topic+CalcProbCoherence">CalcProbCoherence</a>.
</p>
<p>'label' is assigned using the top label from <a href="#topic+LabelTopics">LabelTopics</a>.
This requires an &quot;assignment&quot; matrix. This matrix is like a &quot;theta&quot; matrix
except that it is binary. A topic is &quot;in&quot; a document or it is not.
The assignment is made by comparing each value of theta to the minimum
of the largest value for each row of theta (each document). This 
ensures that each document has at least one topic assigned to it.
</p>


<h3>Value</h3>

<p>An object of class <code>data.frame</code> or <code>tibble</code> with 6 columns: 'topic' is the 
name of the topic, 'prevalence' is the rough prevalence of the topic 
in all documents across the corpus, 'coherence' is the probabilistic
coherence of the topic, 'top_terms_phi' are the top 5 terms for each
topic according to P(word|topic), 'top_terms_gamma' are the top 5 terms
for each topic according to P(topic|word).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
SummarizeTopics(nih_sample_topic_model)

## End(Not run)
</code></pre>

<hr>
<h2 id='TermDocFreq'>Get term frequencies and document frequencies from a document term matrix.</h2><span id='topic+TermDocFreq'></span>

<h3>Description</h3>

<p>This function takes a document term matrix as input and 
returns a data frame with columns for term frequency, document frequency, 
and inverse-document frequency
</p>


<h3>Usage</h3>

<pre><code class='language-R'>TermDocFreq(dtm)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="TermDocFreq_+3A_dtm">dtm</code></td>
<td>
<p>A document term matrix of class <code>dgCMatrix</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a <code>data.frame</code> or <code>tibble</code> with 4 columns.
The first column, <code>term</code> is a vector of token labels.
The second column, <code>term_freq</code> is the count of times <code>term</code>
appears in the entire corpus. The third column <code>doc_freq</code> is the
count of the number of documents in which <code>term</code> appears.
The fourth column, <code>idf</code> is the log-weighted
inverse document frequency of <code>term</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load a pre-formatted dtm and topic model
data(nih_sample_dtm)
data(nih_sample_topic_model) 

# Get the term frequencies 
term_freq_mat &lt;- TermDocFreq(nih_sample_dtm)

str(term_freq_mat)
</code></pre>

<hr>
<h2 id='textmineR'>textmineR</h2><span id='topic+textmineR'></span>

<h3>Description</h3>

<p>Functions for Text Mining and Topic Modeling
</p>


<h3>Details</h3>

<p>An aid for text mining in R, with a syntax that
should be familiar to experienced R users. Provides a wrapper for several 
topic models that take similarly-formatted input and give similarly-formatted
output. Has additional functionality for analyzing and diagnostics for
topic models.
</p>

<hr>
<h2 id='textmineR-deprecated'>Deprecated functions in package <span class="pkg">textmineR</span>.</h2><span id='topic+textmineR-deprecated'></span>

<h3>Description</h3>

<p>The functions listed below are deprecated and will be defunct in
the near future. When possible, alternative functions with similar
functionality are also mentioned.
</p>


<h3>Details</h3>

<p>Below is a list of deprecated functions:
RecursiveRbind
Vec2Dtm
JSD
HellDist
GetPhiPrime
FormatRawLdaOutput
Files2Vec
DepluralizeDtm
CorrectS
CalcPhiPrime
</p>

<hr>
<h2 id='TmParallelApply'>An OS-independent parallel version of <code><a href="base.html#topic+lapply">lapply</a></code></h2><span id='topic+TmParallelApply'></span>

<h3>Description</h3>

<p>This function takes a vector or list and a function and applies in parallel.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>TmParallelApply(
  X,
  FUN,
  cpus = parallel::detectCores(),
  export = NULL,
  libraries = NULL,
  envir = parent.frame()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="TmParallelApply_+3A_x">X</code></td>
<td>
<p>A vector or list over which to apply <code>FUN</code></p>
</td></tr>
<tr><td><code id="TmParallelApply_+3A_fun">FUN</code></td>
<td>
<p>A function to apply over <code>X</code></p>
</td></tr>
<tr><td><code id="TmParallelApply_+3A_cpus">cpus</code></td>
<td>
<p>Number of CPU cores to use, defaults to the value returned by 
<code><a href="parallel.html#topic+detectCores">detectCores</a></code>.</p>
</td></tr>
<tr><td><code id="TmParallelApply_+3A_export">export</code></td>
<td>
<p>A character vector of objects in the workspace to export when 
using a Windows machine. Defaults to <code>NULL</code></p>
</td></tr>
<tr><td><code id="TmParallelApply_+3A_libraries">libraries</code></td>
<td>
<p>A character vector of library/package names to load on to
each cluster if using a Windows machine. Defaults to <code>NULL</code></p>
</td></tr>
<tr><td><code id="TmParallelApply_+3A_envir">envir</code></td>
<td>
<p>Environment from which to export variables in varlist</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is used to parallelize executions in <code>textmineR</code>. It is 
necessary because of differing capabilities between Windows and Unix.
Unix systems use <code><a href="parallel.html#topic+mclapply">mclapply</a></code>. Windows 
systems use <code><a href="parallel.html#topic+parLapply">parLapply</a></code>.
</p>


<h3>Value</h3>

<p>This function returns a <code>list</code> of length <code>length(X)</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
x &lt;- 1:10000
f &lt;- function(y) y * y + 12
result &lt;- TmParallelApply(x, f)

## End(Not run)
</code></pre>

<hr>
<h2 id='update'>Update methods for topic models</h2><span id='topic+update'></span>

<h3>Description</h3>

<p><code>update</code> will update a previously-trained topic model based
on new data. Useful for updates or transfer learning.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>update(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="update_+3A_object">object</code></td>
<td>
<p>An existing trained topic model</p>
</td></tr>
<tr><td><code id="update_+3A_...">...</code></td>
<td>
<p>Additional arguments to the call</p>
</td></tr>
</table>

<hr>
<h2 id='update.lda_topic_model'>Update a Latent Dirichlet Allocation topic model with new data</h2><span id='topic+update.lda_topic_model'></span>

<h3>Description</h3>

<p>Update an LDA model with new data using collapsed Gibbs sampling.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'lda_topic_model'
update(
  object,
  dtm,
  additional_k = 0,
  iterations = NULL,
  burnin = -1,
  new_alpha = NULL,
  new_beta = NULL,
  optimize_alpha = FALSE,
  calc_likelihood = FALSE,
  calc_coherence = TRUE,
  calc_r2 = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="update.lda_topic_model_+3A_object">object</code></td>
<td>
<p>a fitted object of class <code>lda_topic_model</code></p>
</td></tr>
<tr><td><code id="update.lda_topic_model_+3A_dtm">dtm</code></td>
<td>
<p>A document term matrix or term co-occurrence matrix of class dgCMatrix.</p>
</td></tr>
<tr><td><code id="update.lda_topic_model_+3A_additional_k">additional_k</code></td>
<td>
<p>Integer number of topics to add, defaults to 0.</p>
</td></tr>
<tr><td><code id="update.lda_topic_model_+3A_iterations">iterations</code></td>
<td>
<p>Integer number of iterations for the Gibbs sampler to run. A
future version may include automatic stopping criteria.</p>
</td></tr>
<tr><td><code id="update.lda_topic_model_+3A_burnin">burnin</code></td>
<td>
<p>Integer number of burnin iterations. If <code>burnin</code> is greater than -1,
the resulting &quot;phi&quot; and &quot;theta&quot; matrices are an average over all iterations
greater than <code>burnin</code>.</p>
</td></tr>
<tr><td><code id="update.lda_topic_model_+3A_new_alpha">new_alpha</code></td>
<td>
<p>For now not used. This is the prior for topics over documents
used when updating the model</p>
</td></tr>
<tr><td><code id="update.lda_topic_model_+3A_new_beta">new_beta</code></td>
<td>
<p>For now not used. This is the prior for words over topics
used when updating the model.</p>
</td></tr>
<tr><td><code id="update.lda_topic_model_+3A_optimize_alpha">optimize_alpha</code></td>
<td>
<p>Logical. Do you want to optimize alpha every 10 Gibbs iterations?
Defaults to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="update.lda_topic_model_+3A_calc_likelihood">calc_likelihood</code></td>
<td>
<p>Do you want to calculate the likelihood every 10 Gibbs iterations?
Useful for assessing convergence. Defaults to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="update.lda_topic_model_+3A_calc_coherence">calc_coherence</code></td>
<td>
<p>Do you want to calculate probabilistic coherence of topics
after the model is trained? Defaults to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="update.lda_topic_model_+3A_calc_r2">calc_r2</code></td>
<td>
<p>Do you want to calculate R-squared after the model is trained?
Defaults to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="update.lda_topic_model_+3A_...">...</code></td>
<td>
<p>Other arguments to be passed to <code><a href="#topic+TmParallelApply">TmParallelApply</a></code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns an S3 object of class c(&quot;LDA&quot;, &quot;TopicModel&quot;).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# load a document term matrix
d1 &lt;- nih_sample_dtm[1:50,]

d2 &lt;- nih_sample_dtm[51:100,]

# fit a model
m &lt;- FitLdaModel(d1, k = 10, 
                 iterations = 200, burnin = 175,
                 optimize_alpha = TRUE, 
                 calc_likelihood = FALSE,
                 calc_coherence = TRUE,
                 calc_r2 = FALSE)

# update an existing model by adding documents
m2 &lt;- update(object = m,
             dtm = rbind(d1, d2),
             iterations = 200,
             burnin = 175)
             
# use an old model as a prior for a new model
m3 &lt;- update(object = m,
             dtm = d2, # new documents only
             iterations = 200,
             burnin = 175)
             
# add topics while updating a model by adding documents
m4 &lt;- update(object = m,
             dtm = rbind(d1, d2),
             additional_k = 3,
             iterations = 200,
             burnin = 175)
             
# add topics to an existing model
m5 &lt;- update(object = m,
             dtm = d1, # this is the old data
             additional_k = 3,
             iterations = 200,
             burnin = 175)


## End(Not run)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
