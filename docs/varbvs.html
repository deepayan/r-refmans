<!DOCTYPE html><html lang="en"><head><title>Help for package varbvs</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {varbvs}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#varbvs-package'><p>Large-scale Bayesian variable selection using variational methods</p></a></li>
<li><a href='#cred'><p>Estimate credible interval.</p></a></li>
<li><a href='#cytokine'><p>Cytokine signaling genes SNP annotation.</p></a></li>
<li><a href='#leukemia'><p>Expression levels recorded in leukemia patients.</p></a></li>
<li><a href='#normalizelogweights'><p>Compute normalized probabilities.</p></a></li>
<li><a href='#plot.varbvs'><p>Summarize variable selection results in a single plot.</p></a></li>
<li><a href='#predict.varbvs'><p>Make predictions from a model fitted by varbvs.</p></a></li>
<li><a href='#predict.varbvsmix'><p>Make predictions from a model fitted by varbvsmix.</p></a></li>
<li><a href='#rand+2Crandn'><p>Return matrices of pseudorandom values.</p></a></li>
<li><a href='#subset.varbvs'><p>Select hyperparameter settings from varbvs analysis.</p></a></li>
<li><a href='#summary.varbvs+2Cprint.summary.varbvs'><p>Summarize a fitted variable selection model.</p></a></li>
<li><a href='#varbvs'><p>Fit variable selection model using variational approximation methods.</p></a></li>
<li><a href='#varbvs-internal'><p>Internal varbvs functions</p></a></li>
<li><a href='#varbvs.properties'><p>Accessing Properties of Fitted varbvs Models</p></a></li>
<li><a href='#varbvsbf'><p>Compute numerical estimate of Bayes factor.</p></a></li>
<li><a href='#varbvsindep'><p>Compute posterior statistics, ignoring correlations.</p></a></li>
<li><a href='#varbvsmix'><p>Fit linear regression with mixture-of-normals priors using</p>
variational approximation methods.</a></li>
<li><a href='#varbvsproxybf'><p>Compute Bayes factors measuring improvement-in-fit along 1</p>
dimension.</a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Version:</td>
<td>2.6-10</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-05-31</td>
</tr>
<tr>
<td>Title:</td>
<td>Large-Scale Bayesian Variable Selection Using Variational
Methods</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Peter Carbonetto &lt;peter.carbonetto@gmail.com&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Fast algorithms for fitting Bayesian variable selection
    models and computing Bayes factors, in which the outcome (or
    response variable) is modeled using a linear regression or a
    logistic regression. The algorithms are based on the variational
    approximations described in "Scalable variational inference for
    Bayesian variable selection in regression, and its accuracy in
    genetic association studies" (P. Carbonetto &amp; M. Stephens, 2012,
    &lt;<a href="https://doi.org/10.1214%2F12-BA703">doi:10.1214/12-BA703</a>&gt;). This software has been applied to large
    data sets with over a million variables and thousands of samples.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.1.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>methods, Matrix, stats, graphics, lattice, latticeExtra, Rcpp,
nor1mix</td>
</tr>
<tr>
<td>Suggests:</td>
<td>curl, glmnet, qtl, knitr, rmarkdown, testthat</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/pcarbo/varbvs">https://github.com/pcarbo/varbvs</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/pcarbo/varbvs/issues">https://github.com/pcarbo/varbvs/issues</a></td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-05-31 17:20:08 UTC; pcarbo</td>
</tr>
<tr>
<td>Author:</td>
<td>Peter Carbonetto [aut, cre],
  Matthew Stephens [aut],
  David Gerard [ctb]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-05-31 18:30:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='varbvs-package'>Large-scale Bayesian variable selection using variational methods</h2><span id='topic+varbvs-package'></span>

<h3>Description</h3>

<p>Fast algorithms for fitting Bayesian variable selection
models and computing Bayes factors, in which the outcome (or response
variable) is modeled using a linear regression or a logistic
regression. The algorithms are based on the variational approximations
described in &quot;Scalable variational inference for Bayesian variable
selection in regression, and its accuracy in genetic association
studies&quot; (P. Carbonetto and M. Stephens, Bayesian Analysis 7, 2012,
pages 73-108). This software has been applied to large data sets with
over a million variables and thousands of samples.</p>


<h3>Details</h3>

<p>The main functionality of this package is implemented in function
<code><a href="#topic+varbvs">varbvs</a></code>. This function selects the most appropriate
algorithm for the data set and selected model (linear or logistic
regression). See <code>help(varbvs)</code> for details. The varbvs interface
is intended to resemble interface for <span class="pkg">glmnet</span>, the popular package
for fitting genealized linear models.
</p>
<p>For more details about the this package, including the license and a
list of available functions, see <code>help(package=varbvs)</code>.
</p>


<h3>Author(s)</h3>

<p>Peter Carbonetto <a href="mailto:peter.carbonetto@gmail.com">peter.carbonetto@gmail.com</a></p>


<h3>References</h3>

<p>P. Carbonetto and M. Stephens (2012). Scalable variational
inference for Bayesian variable selection in regression, and its
accuracy in genetic association studies. <em>Bayesian Analysis</em> <b>7</b>, 
73&ndash;108.</p>

<hr>
<h2 id='cred'>Estimate credible interval.</h2><span id='topic+cred'></span>

<h3>Description</h3>

<p>Estimate credible interval from weighted samples.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  cred(x, x0, w = NULL, cred.int = 0.95)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cred_+3A_x">x</code></td>
<td>
<p>Vector of random samples of variable.</p>
</td></tr>
<tr><td><code id="cred_+3A_x0">x0</code></td>
<td>
<p>Mean of median of variable.</p>
</td></tr>
<tr><td><code id="cred_+3A_w">w</code></td>
<td>
<p>Weight &gt; 0 assigned to each sample. If w = NULL, all weights
are the same.</p>
</td></tr>
<tr><td><code id="cred_+3A_cred.int">cred.int</code></td>
<td>
<p>Credible interval must contain probability mass of at
least this amount. A number between 0 and 1.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Credible interval [a,b] is defined as smallest interval containing x0
that contains <code>cred.int</code> of the probability mass. Note that the credible
interval is not necessarily symmetric about x0. (Other definitions of the
credible interval are possible.)
</p>
<p>The algorithm is quadratic in the length of x (and w), so should not be
used for large numbers of samples.
</p>


<h3>Value</h3>

<p>list(a = a,b = b).</p>


<h3>Author(s)</h3>

<p>Peter Carbonetto <a href="mailto:peter.carbonetto@gmail.com">peter.carbonetto@gmail.com</a></p>


<h3>References</h3>

<p>P. Carbonetto and M. Stephens (2012). Scalable variational
inference for Bayesian variable selection in regression, and its
accuracy in genetic association studies. <em>Bayesian Analysis</em> <b>7</b>, 
73&ndash;108.</p>


<h3>Examples</h3>

<pre><code class='language-R'>  x   &lt;- rnorm(100)
  out &lt;- cred(x,mean(x),cred.int = 0.68)
</code></pre>

<hr>
<h2 id='cytokine'>Cytokine signaling genes SNP annotation.</h2><span id='topic+cytokine'></span>

<h3>Description</h3>

<p>This gene set was selected in Carbonetto and Stephens
(2013) from an interrogation of 3,158 derived from 8 publicly available
pathway databases.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(cytokine)</code></pre>


<h3>Format</h3>

<p><code>cytokine[i] = 1</code> if SNP i lies within 100 kb of a gene in the &quot;Cytokine
signaling in immune system&quot; gene set, and <code>cytokine[i] = 0</code> otherwise.
</p>


<h3>Source</h3>

<p>Pathway id 75790 from the Reactome database, or pathway id
366171 from the BioSystems database.</p>


<h3>References</h3>

<p>P. Carbonetto and M. Stephens (2013). Integrated enrichment analysis of
variants and pathways in genome-wide association studies indicates
central role for IL-2 signaling genes in type 1 diabetes, and cytokine
signaling genes in Crohn's disease. <em>PLoS Genetics</em> <b>9</b>,
e1003770.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  # See demo.cytokine.R vignette.
</code></pre>

<hr>
<h2 id='leukemia'>Expression levels recorded in leukemia patients.</h2><span id='topic+leukemia'></span>

<h3>Description</h3>

<p>Expression levels recorded for 3,571 genes in 72 patients with leukemia
(Golub et al, 1999). The binary outcome encodes the disease subtype:
acute lymphobastic leukemia (ALL) or acute myeloid leukemia (AML).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(leukemia)</code></pre>


<h3>Format</h3>

<p>Data are represented as a 72 x 3,571 matrix <code>x</code> of gene expression
values, and a vector <code>y</code> of 72 binary disease outcomes.
</p>


<h3>Source</h3>

<p>These are the preprocessed data of Dettling (2004) retrieved
from the supplementary materials accompanying Friedman et al (2010).</p>


<h3>References</h3>

<p>M. Dettling (2004). BagBoosting for tumor classification with gene
expression data. <em>Bioinformatics</em> <b>20</b>, 3583&ndash;3593.
</p>
<p>J. Friedman, T. Hastie and R. Tibshirani (2010). Regularization paths
for generalized linear models via coordinate descent. <em>Journal of 
Statistical Software</em> <b>33</b>, 1&ndash;22.
</p>
<p>T. R. Golub, et al. (1999). Molecular classification of cancer: class
discovery and class prediction by gene expression monitoring.
<em>Science</em> <b>286</b>, 531&ndash;537.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  # See demo.leukemia.R vignette.
</code></pre>

<hr>
<h2 id='normalizelogweights'>Compute normalized probabilities.</h2><span id='topic+normalizelogweights'></span>

<h3>Description</h3>

<p>Compute normalized probabilities from unnormalized
log-probabilities.</p>


<h3>Usage</h3>

<pre><code class='language-R'>  normalizelogweights(logw)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="normalizelogweights_+3A_logw">logw</code></td>
<td>
<p>Vector of unnormalized log-probabilities.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Guards against underflow or overflow by adjusting the
log-probabilities so that the largest probability is 1.
</p>


<h3>Value</h3>

<p>Normalized probabilities such that the sum is equal to 1.</p>


<h3>Author(s)</h3>

<p>Peter Carbonetto <a href="mailto:peter.carbonetto@gmail.com">peter.carbonetto@gmail.com</a></p>


<h3>References</h3>

<p>P. Carbonetto and M. Stephens (2012). Scalable variational
inference for Bayesian variable selection in regression, and its
accuracy in genetic association studies. <em>Bayesian Analysis</em> <b>7</b>, 
73&ndash;108.</p>


<h3>Examples</h3>

<pre><code class='language-R'>  logw &lt;- rnorm(6)
  w    &lt;- normalizelogweights(logw)
</code></pre>

<hr>
<h2 id='plot.varbvs'>Summarize variable selection results in a single plot.</h2><span id='topic+plot.varbvs'></span>

<h3>Description</h3>

<p>Generate a single plot that summarizes the results of fitting the
Bayesian variable selection model to the data. When the variables are
genetic markers, the groups are chromosomes, and the posterior
probabilities are plotted on the vertical axis (typically on the
logarithmic scale), the figure resembles a &quot;Manhattan plot&quot; typically
used to summarize the results of a genome-wide association study or
quantitative trait locus (QTL) mapping study.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  ## S3 method for class 'varbvs'
plot(x, score, groups, vars = NULL, var.labels,
  draw.threshold = NA, gap = 0,col = "midnightblue", pch = 20,
  scales = NULL, xlab = "variable", ylab = "posterior probability",
  main = "fitted varbvs model: variable selection results",
  abline.args = list(lty = "dotted",col = "orangered"),
  vars.xyplot.args = list(pch = 20,col = "magenta"),
  vars.ltext.args = list(col = "black",pos = 4,cex = 0.5), 
  par.settings = list(par.main.text = list(font = 1,cex = 0.8),
                      layout.heights = list(axis.top = 0, axis.bottom = 0)),
  ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.varbvs_+3A_x">x</code></td>
<td>
<p>Output of function <code><a href="#topic+varbvs">varbvs</a></code>.</p>
</td></tr>
<tr><td><code id="plot.varbvs_+3A_score">score</code></td>
<td>
<p>Value to plot on vertical axis. Must be a numeric vector
with one entry for each variable. If missing, the posterior inclusion
probability for each variable is plotted in the vertical axis, in
which this probability is averaged over hyperparameter settings,
treating <code>x$logw</code> as (unnormalized) log-marginal
probabilities. As alternative, set <code>score</code> to the posterior
inclusion probabilities that ignore correlations, using
<code><a href="#topic+varbvsindep">varbvsindep</a></code>.</p>
</td></tr>
<tr><td><code id="plot.varbvs_+3A_groups">groups</code></td>
<td>
<p>Group the variables in the plot according to this
argument. This must be a vector with one entry for each variable. If
missing, all variables are treated as a single group. This is useful
for grouping the genetic markers by chromosome in a genome-wide
association study.</p>
</td></tr>
<tr><td><code id="plot.varbvs_+3A_vars">vars</code></td>
<td>
<p>Indices (type <code>integer</code>) or names (type
<code>character</code>) of variables to highlight and label. By default,
<code>vars = NULL</code>, meaning no variables are highlighted.</p>
</td></tr>
<tr><td><code id="plot.varbvs_+3A_var.labels">var.labels</code></td>
<td>
<p>Labels to accompany the highlighted variables
only. If missing, labels are retrieved from <code>x</code>. If
<code>var.labels = NULL</code>, no labels are plotted.</p>
</td></tr>
<tr><td><code id="plot.varbvs_+3A_draw.threshold">draw.threshold</code></td>
<td>
<p>Plot a horizontal line at this location on the
vertical axis.</p>
</td></tr>
<tr><td><code id="plot.varbvs_+3A_gap">gap</code></td>
<td>
<p>Amount of space to leave between each group of variables in
the plot.</p>
</td></tr>
<tr><td><code id="plot.varbvs_+3A_col">col</code></td>
<td>
<p>Argument passed to <code>xyplot</code> specifying color of points.</p>
</td></tr>
<tr><td><code id="plot.varbvs_+3A_pch">pch</code></td>
<td>
<p>Argument passed to <code>xyplot</code> specifying symbol type.</p>
</td></tr>
<tr><td><code id="plot.varbvs_+3A_scales">scales</code></td>
<td>
<p>Argument passed to <code>xyplot</code> specifying how x- and
y-axes are drawn.</p>
</td></tr>
<tr><td><code id="plot.varbvs_+3A_xlab">xlab</code></td>
<td>
<p>Argument passed to <code>xyplot</code> specifying horizontal
axis title.</p>
</td></tr>
<tr><td><code id="plot.varbvs_+3A_ylab">ylab</code></td>
<td>
<p>Argument passed to <code>xyplot</code> specifying 
vertical axis title.</p>
</td></tr>
<tr><td><code id="plot.varbvs_+3A_main">main</code></td>
<td>
<p>Argument passed to <code>xyplot</code> specifying main plot title.</p>
</td></tr>
<tr><td><code id="plot.varbvs_+3A_abline.args">abline.args</code></td>
<td>
<p>Additional arguments passed to <code>panel.abline</code>
specifying how to draw the horizontal line at the location specified
by <code>draw.threshold</code>.</p>
</td></tr>
<tr><td><code id="plot.varbvs_+3A_vars.xyplot.args">vars.xyplot.args</code></td>
<td>
<p>Additional arguments passed to <code>xyplot</code>
for drawing the highlighted variables.</p>
</td></tr>
<tr><td><code id="plot.varbvs_+3A_vars.ltext.args">vars.ltext.args</code></td>
<td>
<p>Additional arguments passed to <code>ltext</code> for
specifying how the labels of the highlighted variables are drawn in
the plot.</p>
</td></tr>
<tr><td><code id="plot.varbvs_+3A_par.settings">par.settings</code></td>
<td>
<p>Argument passed to <code>xyplot</code> specifying
additional adjustments to the plot settings.</p>
</td></tr>
<tr><td><code id="plot.varbvs_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <code>xyplot</code> for drawing
the un-highlighted variables.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note that <code>plot.varbvs</code> uses function <code>xyplot</code> from the
<code>lattice</code> package, and <code>as.layer</code> from the
<code>latticeExtra</code> package. 
</p>


<h3>Value</h3>

<p>An object of class <code>"trellis"</code> generated by functions
<code>xyplot</code> and <code>as.layer</code>.
</p>


<h3>Author(s)</h3>

<p>Peter Carbonetto <a href="mailto:peter.carbonetto@gmail.com">peter.carbonetto@gmail.com</a></p>


<h3>References</h3>

<p>P. Carbonetto and M. Stephens (2012). Scalable variational
inference for Bayesian variable selection in regression, and its
accuracy in genetic association studies. <em>Bayesian Analysis</em>
<b>7</b>, 73&ndash;108.</p>


<h3>See Also</h3>

<p><code><a href="lattice.html#topic+xyplot">xyplot</a></code>, <code><a href="lattice.html#topic+ltext">ltext</a></code>,
<code><a href="lattice.html#topic+panel.abline">panel.abline</a></code>, <code><a href="#topic+varbvs">varbvs</a></code>,
<code><a href="#topic+summary.varbvs">summary.varbvs</a></code>, <code><a href="#topic+varbvsindep">varbvsindep</a></code>
</p>

<hr>
<h2 id='predict.varbvs'>Make predictions from a model fitted by varbvs.</h2><span id='topic+predict.varbvs'></span>

<h3>Description</h3>

<p>This function predicts outcomes (Y) given the observed
variables (X) and observed covariates (Z), and a model fitted using
<code><a href="#topic+varbvs">varbvs</a></code>.</p>


<h3>Usage</h3>

<pre><code class='language-R'>  ## S3 method for class 'varbvs'
predict(object, X, Z = NULL,
                           type = c("link","response","class"),
                           averaged = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.varbvs_+3A_object">object</code></td>
<td>
<p>Output of function <code><a href="#topic+varbvs">varbvs</a></code>.</p>
</td></tr>
<tr><td><code id="predict.varbvs_+3A_x">X</code></td>
<td>
<p>n x p input matrix, in which p is the number of variables,
and n is the number of samples for which predictions will be made
using the fitted model. X cannot be sparse, and cannot have any
missing values (NA).</p>
</td></tr>
<tr><td><code id="predict.varbvs_+3A_z">Z</code></td>
<td>
<p>n x m covariate data matrix, where m is the number of
covariates. Do not supply an intercept as a covariate (i.e., a
column of ones), because an intercept is automatically included in
the regression model. For no covariates, set <code>Z = NULL</code>.</p>
</td></tr>
<tr><td><code id="predict.varbvs_+3A_type">type</code></td>
<td>
<p>Type of prediction to output. The default, &quot;link&quot;, gives
the linear predictors for <code>family = "binomial"</code>, and gives the
fitted values for <code>family = "gaussian"</code>. For logistic
regression (<code>family = "binomial"</code>), there are two alternative
predictions: &quot;response&quot; givees the fitted probabilities, and &quot;class&quot;
produces the maximum-probability outcome (0 or 1).</p>
</td></tr>
<tr><td><code id="predict.varbvs_+3A_averaged">averaged</code></td>
<td>
<p>When <code>averaged = TRUE</code>, the predictions are
computed by averaging over the hyperparameter settings, treating
<code>object$logw</code> as (unnormalized) log-marginal
probabilities. (See <code><a href="#topic+varbvs">varbvs</a></code> for more details about
averaging.) When <code>averaged = FALSE</code>, the predictions are
returned as a matrix when one row for each data sample, and one
column for each hyperparameter setting.</p>
</td></tr>
<tr><td><code id="predict.varbvs_+3A_...">...</code></td>
<td>
<p>Other arguments to generic predict function. These
extra arguments are not used here.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note that the classification probabilities <code class="reqn">Pr(Y = 1 | X, Z,
  \theta)</code> are not guaranteed to be calibrated under the variational
approximation.
</p>


<h3>Value</h3>

<p>When <code>averaged = TRUE</code>, the output is a vector containing the
predicted outcomes for all samples. For <code>family = "binomial"</code>,
all vector entries are 0 or 1.
</p>
<p>When <code>averaged = FALSE</code>, the return value is a matrix with one
row for each sample, and one column for each hyperparameter setting.  
</p>


<h3>Author(s)</h3>

<p>Peter Carbonetto <a href="mailto:peter.carbonetto@gmail.com">peter.carbonetto@gmail.com</a></p>


<h3>References</h3>

<p>P. Carbonetto and M. Stephens (2012). Scalable variational
inference for Bayesian variable selection in regression, and its
accuracy in genetic association studies. <em>Bayesian Analysis</em>
<b>7</b>, 73&ndash;108.</p>


<h3>See Also</h3>

<p><code><a href="#topic+varbvs">varbvs</a></code>, <code><a href="#topic+summary.varbvs">summary.varbvs</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>  # See help(varbvs) for examples.
</code></pre>

<hr>
<h2 id='predict.varbvsmix'>Make predictions from a model fitted by varbvsmix.</h2><span id='topic+predict.varbvsmix'></span>

<h3>Description</h3>

<p>This function predicts outcomes (Y) given the observed
variables (X) and observed covariates (Z), and a model fitted using
<code><a href="#topic+varbvsmix">varbvsmix</a></code>.</p>


<h3>Usage</h3>

<pre><code class='language-R'>  ## S3 method for class 'varbvsmix'
predict(object, X, Z = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.varbvsmix_+3A_object">object</code></td>
<td>
<p>Output of function <code><a href="#topic+varbvsmix">varbvsmix</a></code>.</p>
</td></tr>
<tr><td><code id="predict.varbvsmix_+3A_x">X</code></td>
<td>
<p>n x p input matrix, in which p is the number of variables,
and n is the number of samples for which predictions will be made
using the fitted model. X cannot be sparse, and cannot have any
missing values (NA).</p>
</td></tr>
<tr><td><code id="predict.varbvsmix_+3A_z">Z</code></td>
<td>
<p>n x m covariate data matrix, where m is the number of
covariates. Do not supply an intercept as a covariate (i.e., a
column of ones), because an intercept is automatically included in
the regression model. For no covariates, set <code>Z = NULL</code>.</p>
</td></tr>
<tr><td><code id="predict.varbvsmix_+3A_...">...</code></td>
<td>
<p>Other arguments to generic predict function. These
extra arguments are not used here.</p>
</td></tr>
</table>


<h3>See Also</h3>

<p><code><a href="#topic+varbvsmix">varbvsmix</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>  # See help(varbvsmix) for examples.
</code></pre>

<hr>
<h2 id='rand+2Crandn'>Return matrices of pseudorandom values.</h2><span id='topic+rand'></span><span id='topic+randn'></span>

<h3>Description</h3>

<p>Generate matrices of pseudorandom values.</p>


<h3>Usage</h3>

<pre><code class='language-R'>  rand(m,n)
  randn(m,n)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="rand+2B2Crandn_+3A_m">m</code></td>
<td>
<p>Number of matrix rows.</p>
</td></tr>
<tr><td><code id="rand+2B2Crandn_+3A_n">n</code></td>
<td>
<p>Number of matrix columns.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Function <code>rand</code> returns a matrix containing pseudorandom values
drawn from the standard uniform distribution (using <code>runif</code>).
Function <code>randn</code> returns a matrix containing pseudorandom values
drawn from the standard normal distribution (using <code>rnorm</code>).
</p>


<h3>Value</h3>

<p>An m x n numeric matrix.</p>


<h3>Author(s)</h3>

<p>Peter Carbonetto <a href="mailto:peter.carbonetto@gmail.com">peter.carbonetto@gmail.com</a></p>


<h3>References</h3>

<p>P. Carbonetto and M. Stephens (2012). Scalable variational
inference for Bayesian variable selection in regression, and its
accuracy in genetic association studies. <em>Bayesian Analysis</em> <b>7</b>, 
73&ndash;108.</p>


<h3>Examples</h3>

<pre><code class='language-R'>  x &lt;- rand(10,5)
  y &lt;- randn(10,5)
</code></pre>

<hr>
<h2 id='subset.varbvs'>Select hyperparameter settings from varbvs analysis.</h2><span id='topic+subset.varbvs'></span>

<h3>Description</h3>

<p>Select a subset of the candidate hyperparameter settings,
and return a new varbvs analysis object with these hyperparameter
settings only.</p>


<h3>Usage</h3>

<pre><code class='language-R'>  ## S3 method for class 'varbvs'
subset(x, subset, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="subset.varbvs_+3A_x">x</code></td>
<td>
<p>Output of function <code><a href="#topic+varbvs">varbvs</a></code>.</p>
</td></tr>
<tr><td><code id="subset.varbvs_+3A_subset">subset</code></td>
<td>
<p>Expression indicating hyperparameter settings to select.
This expression should include one or more of <code>logodds</code>,
<code>sigma</code> and <code>sa</code>.</p>
</td></tr>
<tr><td><code id="subset.varbvs_+3A_...">...</code></td>
<td>
<p>Other arguments to generic subset function. These
extra arguments are not used here.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object with S3 class <code>c("varbvs","list")</code>.
</p>


<h3>Author(s)</h3>

<p>Peter Carbonetto <a href="mailto:peter.carbonetto@gmail.com">peter.carbonetto@gmail.com</a></p>


<h3>References</h3>

<p>P. Carbonetto and M. Stephens (2012). Scalable variational
inference for Bayesian variable selection in regression, and its
accuracy in genetic association studies. <em>Bayesian Analysis</em>
<b>7</b>, 73&ndash;108.</p>


<h3>See Also</h3>

<p><code><a href="#topic+varbvs">varbvs</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>
  # First run one of the examples in help(varbvs), then try running 
  # this.
  #
  #   fit.new &lt;- subset(fit,logodds &lt; (-2))
  # 
</code></pre>

<hr>
<h2 id='summary.varbvs+2Cprint.summary.varbvs'>Summarize a fitted variable selection model.</h2><span id='topic+summary.varbvs'></span><span id='topic+print.summary.varbvs'></span><span id='topic+print.varbvs'></span>

<h3>Description</h3>

<p>Generate a summary of the Bayesian variable selection model
fitted using variational approximation methods.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  ## S3 method for class 'varbvs'
summary(object, cred.int = 0.95, nv, pip.cutoff, ...)
  ## S3 method for class 'summary.varbvs'
print(x, digits = 3, ...)
  ## S3 method for class 'varbvs'
print(x, digits = 3, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="summary.varbvs+2B2Cprint.summary.varbvs_+3A_object">object</code></td>
<td>
<p>Output of function <code><a href="#topic+varbvs">varbvs</a></code>.</p>
</td></tr>
<tr><td><code id="summary.varbvs+2B2Cprint.summary.varbvs_+3A_cred.int">cred.int</code></td>
<td>
<p>Size of credible interval, a number between 0 and 1.</p>
</td></tr>
<tr><td><code id="summary.varbvs+2B2Cprint.summary.varbvs_+3A_nv">nv</code></td>
<td>
<p>Show detailed statistics for top nv variables, ranked
according to their posterior inclusion probabilities. Only one of
<code>nv</code> and <code>pip.cutoff</code> may be specified. If neither are
specified, the default is <code>nv = 5</code>.</p>
</td></tr>
<tr><td><code id="summary.varbvs+2B2Cprint.summary.varbvs_+3A_pip.cutoff">pip.cutoff</code></td>
<td>
<p>Show detailed statistics for all variables in which
the posterior inclusion probability (PIP) is at least
<code>pip.cutoff</code>. Only one of <code>nv</code> and <code>pip.cutoff</code> may
be specified.</p>
</td></tr>
<tr><td><code id="summary.varbvs+2B2Cprint.summary.varbvs_+3A_x">x</code></td>
<td>
<p>Output of function <code>summary.varbvs</code>.</p>
</td></tr>
<tr><td><code id="summary.varbvs+2B2Cprint.summary.varbvs_+3A_digits">digits</code></td>
<td>
<p>Number of digits shown when printing posterior
probabilities of top nv variables.</p>
</td></tr>
<tr><td><code id="summary.varbvs+2B2Cprint.summary.varbvs_+3A_...">...</code></td>
<td>
<p>Additional summary or print arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The printed summary is divided into three parts. The first part
summarizes the data and optimization settings. It also reports the
hyperparameter setting that yields the largest marginal
likelihood&mdash;more precisely, the approximate marginal likelihood
computed using the variational method. For the linear regression only
(<code>family = "gaussian"</code>) when no additional covariates (<code>Z</code>)
are included, it reports the estimated proportion of variance in the
outcome explained by the model (PVE), and the credible interval of the
PVE estimate brackets.
</p>
<p>The second part summarizes the approximate posterior distribution of
the hyperparameters (sigma, sa, logodds). The &quot;estimate&quot; column is the
value averaged over hyperparameter settings, treating
<code>objectlogw</code> as (unnormalized) log-marginal probabilities. The
next column, labeled &quot;Pr&gt;x&quot;, where <code>x = cred.int</code> gives the
credible interval based on these weights (computed using function
<code><a href="#topic+cred">cred</a></code>).
</p>
<p>The third part summarizes the variable selection results. This
includes the total number of variables included in the model at
different posterior probability thresholds, and a more detailed
summary of the variables included in the model with highest posterior
probability. For <code>family = "gaussian"</code>, the &quot;PVE&quot; column gives
the estimated proportion of variance in the outcome explained by the
variable (conditioned on being included in the model).
</p>


<h3>Value</h3>

<p>An object of class <code>summary.varbvs</code>, to be printed by
<code>print.summary.varbvs</code>.
</p>


<h3>Author(s)</h3>

<p>Peter Carbonetto <a href="mailto:peter.carbonetto@gmail.com">peter.carbonetto@gmail.com</a></p>


<h3>References</h3>

<p>P. Carbonetto and M. Stephens (2012). Scalable variational
inference for Bayesian variable selection in regression, and its
accuracy in genetic association studies. <em>Bayesian Analysis</em>
<b>7</b>, 73&ndash;108.</p>


<h3>See Also</h3>

<p><code><a href="#topic+varbvs">varbvs</a></code>, <a href="#topic+varbvs.properties">varbvs.properties</a></p>


<h3>Examples</h3>

<pre><code class='language-R'>  # See help(varbvs) for examples.
</code></pre>

<hr>
<h2 id='varbvs'>Fit variable selection model using variational approximation methods.</h2><span id='topic+varbvs'></span>

<h3>Description</h3>

<p>Compute fully-factorized variational approximation for
Bayesian variable selection in linear (family = gaussian) or logistic
regression (family = binomial). More precisely, find the &quot;best&quot;
fully-factorized approximation to the posterior distribution of the
coefficients, with spike-and-slab priors on the coefficients. By
&quot;best&quot;, we mean the approximating distribution that locally minimizes
the Kullback-Leibler divergence between the approximating distribution
and the exact posterior.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  varbvs(X, Z, y, family = c("gaussian","binomial"), sigma, sa,
         logodds, weights, resid.vcov, alpha, mu, eta, update.sigma,
         update.sa, optimize.eta, initialize.params, update.order,
         nr = 100, sa0 = 1, n0 = 10, tol = 1e-4, maxiter = 1e4,
         verbose = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="varbvs_+3A_x">X</code></td>
<td>
<p>n x p input matrix, where n is the number of samples,
and p is the number of variables. X cannot be sparse,
and cannot have any missing values (NA).</p>
</td></tr>
<tr><td><code id="varbvs_+3A_z">Z</code></td>
<td>
<p>n x m covariate data matrix, where m is the number of
covariates. Do not supply an intercept as a covariate (i.e.,
a column of ones), because an intercept is automatically
included in the regression model. For no covariates, set
<code>Z = NULL</code>. The covariates are assigned an improper,
uniform prior. Although improper priors are generally not
advisable because they can result in improper posteriors and
Bayes factors, this choice allows us to easily integrate out
these covariates.</p>
</td></tr>
<tr><td><code id="varbvs_+3A_y">y</code></td>
<td>
<p>Vector of length n containing observations of binary
(<code>family = "binomial"</code>) or continuous (<code>family =
          "gaussian"</code>) outcome. For a binary outcome, all entries
of y must be 0 or 1.</p>
</td></tr>
<tr><td><code id="varbvs_+3A_family">family</code></td>
<td>
<p>&quot;gaussian&quot; for linear regression model, or &quot;binomial&quot;
for logistic regression model.</p>
</td></tr>
<tr><td><code id="varbvs_+3A_sigma">sigma</code></td>
<td>
<p>Candidate settings for the residual variance
parameter. Must be of the same length as inputs sa and logodds (or
have length equal to the number of columns of logodds). Only used for
linear regression, and will generate an error if <code>family =
  "binomial"</code>. If missing, residual variance parameter is automatically
fitted to data by computing approximate maximum-likelihood (ML)
estimate.</p>
</td></tr>
<tr><td><code id="varbvs_+3A_sa">sa</code></td>
<td>
<p>Hyperparameter sa is the prior variance of regression
coefficients for variables that are included in the model. This
prior variance is always scaled by sigma (for logistic regression,
we take sigma = 1). Scaling the variance of the coefficients in this
way is necessary to ensure that this prior is invariant to
measurement scale (e.g., switching from grams to kilograms). This
input specifies the candidate settings for sa, of the same length as
inputs sigma and logodds (or have length equal to the number of
columns of logodds). If missing, prior variance is automatically
fitted to data by compute approximate maximum (ML) estimates, or
maximum a posteriori estimates when <code>n0 &gt; 0</code> and <code>sa0 &gt;
    0</code>.</p>
</td></tr>
<tr><td><code id="varbvs_+3A_logodds">logodds</code></td>
<td>
<p>Hyperparameter logodds is the prior log-odds that a
variable is included in the regression model; it is defined as
<code class="reqn">logodds = log10(q/(1-q)),</code> where q is the prior probability
that a variable is included in the regression model. Note that we
use the base-10 logarithm instead of the natural logarithm because
it is usually more natural to specify prior log-odds settings in
this way. (To obtain the prior probability from the log-odds, use
the following formula: <code class="reqn">q = 1/(1 + 10^(-logodds))</code>. The prior
log-odds may also be specified separately for each variable, which
is useful is there is prior information about which variables are
most relevant to the outcome. This is accomplished by setting
logodds to a p x ns matrix, where p is the number of variables, and
ns is the number of hyperparameter settings. Note it is not possible
to fit the logodds parameter; if logodds input is not provided as
input, then it is set to the default value when sa and sigma are
missing, and otherwise an error is generated.</p>
</td></tr>
<tr><td><code id="varbvs_+3A_weights">weights</code></td>
<td>
<p>Optional vector of weights for weighted linear
regression; larger weights correspond to smaller variances. It
should be <code>NULL</code>, or a numeric vector of the same length as
<code>y</code>. This is equivalent to the <code>weights</code> argument in
<code><a href="stats.html#topic+lm">lm</a></code>. This option is only available for linear
regression (<code>family = "gaussian"</code>), and cannot be combined with
<code>resid.vcov</code>.</p>
</td></tr>
<tr><td><code id="varbvs_+3A_resid.vcov">resid.vcov</code></td>
<td>
<p>Optional matrix specifying the covariance of the
residual, scaled by sigma. This is useful to allow for observations
with different variances (similar to <code>weights</code>), or for
observations that are not independent. It should either be
<code>NULL</code>, or a symmetric positive definite matrix in which the
number of rows and columns is equal to the length of <code>y</code>. This
option is only available for linear regression (<code>family =
    "gaussian"</code>), and cannot be combined with <code>weights</code>.</p>
</td></tr>
<tr><td><code id="varbvs_+3A_alpha">alpha</code></td>
<td>
<p>Good initial estimate for the variational parameter alpha
for each hyperparameter setting. Either missing, or a p x ns matrix,
where p is the number of variables, and ns is the number of
hyperparameter settings.</p>
</td></tr>
<tr><td><code id="varbvs_+3A_mu">mu</code></td>
<td>
<p>Good initial estimate for the variational parameter mu for
each hyperparameter setting. Either missing, or a p x ns matrix,
where p is the number of variables, and ns is the number of
hyperparameter settings.</p>
</td></tr>
<tr><td><code id="varbvs_+3A_eta">eta</code></td>
<td>
<p>Good initial estimate of the additional free parameters
specifying the variational approximation to the logistic regression
factors. Either missing, or an n x ns matrix, where n is the number
of samples, and ns is the number of hyperparameter settings.</p>
</td></tr>
<tr><td><code id="varbvs_+3A_update.sigma">update.sigma</code></td>
<td>
<p>Setting this to TRUE ensures that sigma is always
fitted to data, in which case input vector sigma is used to provide
initial estimates.</p>
</td></tr>
<tr><td><code id="varbvs_+3A_update.sa">update.sa</code></td>
<td>
<p>Setting this to TRUE ensures that sa is always fitted
to data, in which case input vector sa is used to provide initial
estimates.</p>
</td></tr>
<tr><td><code id="varbvs_+3A_optimize.eta">optimize.eta</code></td>
<td>
<p>When <code>optimize.eta = TRUE</code>, eta is fitted to
the data during the inner loop coordinate ascent updates, even when
good estimates of eta are provided as input.</p>
</td></tr>
<tr><td><code id="varbvs_+3A_initialize.params">initialize.params</code></td>
<td>
<p>If FALSE, the initialization stage of the
variational inference algorithm (see below) will be skipped, which
saves computation time for large data sets.</p>
</td></tr>
<tr><td><code id="varbvs_+3A_update.order">update.order</code></td>
<td>
<p>Order of the co-ordinate ascent updates for
fitting the variational approximation. The default is
<code>update.order = 1:p</code>, where <code>p</code> is the number of variables
(the number of columns of <code>X</code>).</p>
</td></tr>
<tr><td><code id="varbvs_+3A_nr">nr</code></td>
<td>
<p>Number of samples of &quot;model PVE&quot; to draw from posterior.</p>
</td></tr>
<tr><td><code id="varbvs_+3A_sa0">sa0</code></td>
<td>
<p>Scale parameter for a scaled inverse chi-square prior on
hyperparameter sa. Must be &gt;= 0.</p>
</td></tr>
<tr><td><code id="varbvs_+3A_n0">n0</code></td>
<td>
<p>Number of degrees of freedom for a scaled inverse chi-square
prior on hyperparameter sa. Must be &gt;= 0. Large settings of
<code>n0</code> provide greater stability of the parameter estimates for
cases when the model is &quot;sparse&quot;; that is, when few variables are
included in the model.</p>
</td></tr>
<tr><td><code id="varbvs_+3A_tol">tol</code></td>
<td>
<p>Convergence tolerance for inner loop.</p>
</td></tr>
<tr><td><code id="varbvs_+3A_maxiter">maxiter</code></td>
<td>
<p>Maximum number of inner loop iterations.</p>
</td></tr>
<tr><td><code id="varbvs_+3A_verbose">verbose</code></td>
<td>
<p>If <code>verbose = TRUE</code>, print progress of algorithm
to console.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object with S3 class <code>c("varbvs","list")</code>.
</p>
<table role = "presentation">
<tr><td><code>family</code></td>
<td>
<p>Either &quot;gaussian&quot; or &quot;binomial&quot;.</p>
</td></tr>
<tr><td><code>sigma</code></td>
<td>
<p>Settings for sigma (family = &quot;gaussian&quot; only).</p>
</td></tr>
<tr><td><code>sa</code></td>
<td>
<p>Settings for prior variance parameter.</p>
</td></tr>
<tr><td><code>logodds</code></td>
<td>
<p>Prior log-odds settings.</p>
</td></tr>
<tr><td><code>prior.same</code></td>
<td>
<p>TRUE if prior is identical for all variables. When
logodds is a p x ns matrix, <code>prior.same = FALSE</code>.</p>
</td></tr>
<tr><td><code>sa0</code></td>
<td>
<p>Scale parameter for prior on hyperparameter sa.</p>
</td></tr>
<tr><td><code>n0</code></td>
<td>
<p>Degrees of freedom for prior on hyperparameter sa.</p>
</td></tr>
<tr><td><code>update.sigma</code></td>
<td>
<p>If TRUE, sigma was fit to data for each setting of
prior logodds (<code>family = "gaussian"</code> only).</p>
</td></tr>
<tr><td><code>update.sa</code></td>
<td>
<p>If TRUE, sa was fit to data for each setting of prior
logodds.</p>
</td></tr>
<tr><td><code>logw</code></td>
<td>
<p>An array with ns elements, in which <code>logw[i]</code> is the
variational lower bound on the marginal log-likelihood for setting i
of the hyperparameters. These provide approximate values of the
marginal log-likelihood for each hyperparameter setting.</p>
</td></tr>
<tr><td><code>w</code></td>
<td>
<p>Normalized weights (posterior probabilities) for each of the
hyperparameter settings computed from <code>logw</code> using
<code><a href="#topic+normalizelogweights">normalizelogweights</a></code>.</p>
</td></tr>
<tr><td><code>alpha</code></td>
<td>
<p>Variational estimates of posterior inclusion
probabilities for each hyperparameter setting.</p>
</td></tr>
<tr><td><code>mu</code></td>
<td>
<p>Variational estimates of posterior mean coefficients for
each hyperparameter setting.</p>
</td></tr>
<tr><td><code>s</code></td>
<td>
<p>Variational estimates of posterior variances for each
hyperparameter setting.</p>
</td></tr>
<tr><td><code>pip</code></td>
<td>
<p>&quot;Averaged&quot; posterior inclusion probabilities computed as a
weighted average of the individual PIPs (<code>alpha</code>), with weights
given by <code>w</code>.</p>
</td></tr>
<tr><td><code>beta</code></td>
<td>
<p>&quot;Averaged&quot; posterior mean regression coefficients.</p>
</td></tr>
<tr><td><code>mu.cov</code></td>
<td>
<p>Posterior mean regression coefficients for covariates,
including intercept, for each hyperparameter setting.</p>
</td></tr>
<tr><td><code>beta.cov</code></td>
<td>
<p>&quot;Averaged&quot; posterior mean regression coefficients for
covariates, including intercept.</p>
</td></tr>
<tr><td><code>eta</code></td>
<td>
<p>Additional variational parameters for <code>family =
    "binomial"</code> only.</p>
</td></tr>
<tr><td><code>optimize.eta</code></td>
<td>
<p>If TRUE, eta was fit to data (<code>family =
    "binomial"</code> only).</p>
</td></tr>
<tr><td><code>fitted.values</code></td>
<td>
<p>Matrix containing the predicted (or &quot;fitted&quot;)
values of the outcome at each hyperparameter setting. For the
logistic regression model (<code>family = "binomial"</code>), each matrix
entry gives the probability that the binary outcome is equal to 1.</p>
</td></tr>
<tr><td><code>residuals</code></td>
<td>
<p>For linear regression, this is a matrix containing
the model residuals at each hyperparameter setting. For <code>family
    = "binomial"</code>, this is a list with two matrices of the same size;
<code>residuals$deviance</code> contains the deviance residuals based on
the likelihood ratio chi-squared statistic;
<code>residuals$response</code> contains the &quot;response&quot; residuals simply
defined as the difference between the actual binary values (0 or 1)
and the fitted binomial probabilities (<em>i.e.</em>,
<code>fitted.values</code>).</p>
</td></tr>
<tr><td><code>pve</code></td>
<td>
<p>For each hyperparameter setting, and for each variable,
mean estimate of the proportion of variance in outcome explained
conditioned on variable being included in the model (<code>family =
    "gaussian"</code> only).</p>
</td></tr>
<tr><td><code>model.pve</code></td>
<td>
<p>Samples drawn from posterior distribution giving
estimates of proportion of variance in outcome (y) explained by fitted
variable selection model. This is for <code>family = "gaussian"</code>
only. To obtain the posterior mean estimate of the proportion of
variance explained (PVE), for example, simply type
<code>mean(fit$model.pve)</code>.</p>
</td></tr>
</table>


<h3>Regression models</h3>

<p>Two types of outcomes (y) are modeled: (1) a continuous outcome,
also a &quot;quantitative trait&quot; in the genetics literature; or (2) a
binary outcome with possible values 0 and 1. For the former, set
<code>family = "gaussian"</code>, in which case, the outcome is
i.i.d. normal with mean <code class="reqn">u0 + Z*u + X*b</code> and variance sigma, in
which u and b are vectors of regresion coefficients, and u0 is the
intercept. In the second case, we use logistic regression to model
the outcome, in which the probability that y = 1 is equal to
<code class="reqn">sigmoid(u0 + Z*u + X*b).</code> See <code>help(sigmoid)</code> for a
description of the sigmoid function. Note that the regression always
includes an intercept term (u0).
</p>


<h3>Co-ordinate ascent optimization procedure</h3>

<p>For both regression models, the fitting procedure consists of an inner
loop and an outer loop. The outer loop iterates over each of the
hyperparameter settings (sa, sigma and logodds). Given a setting of
the hyperparameters, the inner loop cycles through coordinate ascent
updates to tighten the lower bound on the marginal likelihood,
<code class="reqn">Pr(Y | X, sigma, sa, logodds)</code>. The inner loop coordinate
ascent updates terminate when either (1) the maximum number of inner
loop iterations is reached, as specified by <code>maxiter</code>, or (2)
the maximum difference between the estimated posterior inclusion
probabilities is less than <code>tol</code>.
</p>
<p>To provide a more accurate variational approximation of the posterior
distribution, by default the fitting procedure has two stages. In the
first stage, the entire fitting procedure is run to completion, and the
variational parameters (alpha, mu, s, eta) corresponding to the maximum
lower bound are then used to initialize the coordinate ascent updates
in a second stage. Although this has the effect of doubling the
computation time (in the worst case), the final posterior estimates
tend to be more accurate with this two-stage fitting procedure. 
</p>


<h3>Variational approximation</h3>

<p>Outputs alpha, mu and s specify the approximate posterior distribution
of the regression coefficients. Each of these outputs is a p x ns
matrix. For the ith hyperparameter setting, alpha[,i] is the
variational estimate of the posterior inclusion probability (PIP)
for each variable; mu[,i] is the variational estimate of the
posterior mean coefficient given that it is included in the model;
and s[,i] is the estimated posterior variance of the coefficient
given that it is included in the model. These are also the
quantities that are optimized as part of the inner loop coordinate
ascent updates. An additional free parameter, eta, is needed for
fast computation with the logistic regression model <code>(family =
    "binomial")</code>. The fitted value of eta is returned as an n x ns
matrix.
</p>
<p>The variational estimates should be interpreted carefully,
especially when variables are strongly correlated. For example,
consider the simple scenario in which 2 candidate variables are
closely correlated, and at least one of them explains the outcome
with probability close to 1. Under the correct posterior
distribution, we would expect that each variable is included with
probability ~0.5. However, the variational approximation, due to the
conditional independence assumption, will typically get this wrong,
and concentrate most of the posterior weight on one variable (the
actual variable that is chosen will depend on the starting
conditions of the optimization). Although the individual PIPs are
incorrect, a statistic summarizing the variable selection for both
correlated variables (e.g., the total number of variables included
in the model) should be reasonably accurate.
</p>
<p>More generally, if variables can be reasonably grouped together
based on their correlations, we recommend interpreting the variable
selection results at a group level. For example, in genome-wide
association studies (see the vignettes) ,a SNP with a high PIP
indicates that this SNP is probably associated with the trait, and
one or more nearby SNPs within a chromosomal region, or &ldquo;locus,&rdquo;
may be associated as well. Therefore, we interpreted the GWAS
variable selection results at the level of loci, rather than at the
level of individual SNPs. 
</p>
<p>Also note that special care is required for interpreting the results
of the variational approximation with the logistic regression
model. In particular, interpretation of the individual estimates of
the regression coefficients (e.g., the posterior mean estimates
<code>fit$mu</code>) is not straightforward due to the additional
approximation introduced on the individual nonlinear factors in the
likelihood. As a general guideline, only the relative magnitudes of
the coefficients are meaningful.
</p>


<h3>Averaging over hyperparameter settings</h3>

<p>In many settings, it is good practice to account for uncertainty in
the hyperparameters when reporting final posterior quantities. For
example, hyperparameter sa is often estimated with a high degree of
uncertainty when only a few variables are included in the model.
Provided that (1) the hyperparameter settings sigma, sa and logodds
adequately represent the space of possible hyperparameter settings
with high posterior mass, (2) the hyperparameter settings are drawn
from the same distribution as the prior, and (3) the
fully-factorized variational approximation closely approximates the
true posterior distribution, then final posterior quantities can be
calculated by using logw as (unnormalized) log-marginal
probabilities.
</p>
<p>Even when conditions (1), (2) and/or (3) are not satisfied, this can
approach can still often yield reasonable estimates of averaged
posterior quantities. The examples below demonstrate how final
posterior quantities are reported by function
<code><a href="#topic+summary.varbvs">summary.varbvs</a></code> (see <code>help(summary.varbvs)</code> for
more details). To account for discrepancies between the prior on
(sigma,sa,logodds) and the sampling density used to draw candidate
settings of the hyperparameters, adjust the log-probabilities by
setting <code>fit$logw &lt;- fit$logw + logp/logq</code>, where logp is the
log-density of the prior distribution, and logq is the log-density
of the sampling distribution. (This is importance sampling; see, for
example, R. M. Neal, Annealed importance sampling, <em>Statistics
and Computing</em>, 2001.)
</p>


<h3>Prior on proportion of variance explained</h3>

<p>Specifying the prior variance of the regression coefficients (sa) can
be difficult, which is why we have included the option of fitting this
hyperparameter to the data (see input argument <code>update.sa</code>
above). However, in many settings, especially when a small number of
variables are included in the regression model, it is preferrable to
average over candidate settings of sa instead of fitting sa to the
data. To choose a set of candidate settings for sa, we have advocated
for setting sa indirectly through a prior estimate of the proportion
of variance in the outcome explained by the variables (abbreviated as
PVE), since it is often more natural to specify the PVE rather than
the prior variance (see references below). This is technically only
suitable or the linear regression model (<code>family = "gaussian"</code>),
but could potentially be used for the logistic regression model in an
approximate way.
</p>
<p>For example, one could approximate a uniform prior on the PVE by
drawing the PVE uniformly between 0 and 1, additionally specifying
candidate settings for the prior log-odds, then computing the prior
variance (sa) as follows:</p>
<pre>sx &lt;- sum(var1.cols(X))
sa &lt;- PVE/(1-PVE)/(sigmoid(log(10)*logodds)*sx)</pre>
<p>Note that this calculation will yield <code>sa = 0</code> when <code>PVE =
  0</code>, and <code>sa = Inf</code> when <code>PVE = 1</code>.
</p>
<p>Also, bear in mind that if there are additional covariates (Z)
included in the linear regression model that explain variance in Y,
then it will usually make more sense to first remove the linear
effects of these covariates before performing these calculations. The
PVE would then represent the prior proportion of variance in the
residuals of Y that are explained by the candidate variables.
Alternatively, one could include the matrix Z in the calculations
above, taking care to ensure that the covariates are included in the
model with probability 1.
</p>


<h3>Memory requirements</h3>

<p>Finally, we point out that the optimization procedures were
carefully designed so that they can be applied to very large data
sets; to date, this code has been tested on data sets with &gt;500,000
variables and &gt;10,000 samples. An important limiting factor is the
ability to store the data matrix X in memory. To reduce memory
requirements, in the MATLAB interface we require that X be single
precision, but this option is not available in R. Additionally, we
mostly avoid generating intermediate products that are of the same
size as X. Only one such intermediate product is generated when
<code>family = "gaussian"</code>, and none for <code>family = "binomial"</code>.
</p>


<h3>Author(s)</h3>

<p>Peter Carbonetto <a href="mailto:peter.carbonetto@gmail.com">peter.carbonetto@gmail.com</a></p>


<h3>References</h3>

<p>P. Carbonetto and M. Stephens (2012). Scalable variational inference
for Bayesian variable selection in regression, and its accuracy in
genetic association studies. <em>Bayesian Analysis</em> <b>7</b>,
73&ndash;108.
</p>
<p>Y. Guan and M. Stephens (2011). Bayesian variable selection regression
for genome-wide association studies and other large-scale
problems. <em>Annals of Applied Statistics</em> <b>5</b>, 1780&ndash;1815.
</p>
<p>X. Zhou, P. Carbonetto and M. Stephens (2013). Polygenic modeling with
Bayesian sparse linear mixed models. <em>PLoS Genetics</em> <b>9</b>,
e1003264.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+summary.varbvs">summary.varbvs</a></code>, <a href="#topic+varbvs.properties">varbvs.properties</a>,
<code><a href="#topic+varbvspve">varbvspve</a></code>, <code><a href="#topic+varbvsnorm">varbvsnorm</a></code>,
<code><a href="#topic+varbvsbin">varbvsbin</a></code>, <code><a href="#topic+varbvsbinz">varbvsbinz</a></code>,
<code><a href="#topic+normalizelogweights">normalizelogweights</a></code>, <a href="#topic+varbvs-internal">varbvs-internal</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# LINEAR REGRESSION EXAMPLE
# -------------------------
# Data are 200 uncorrelated ("unlinked") single nucleotide polymorphisms
# (SNPs) with simulated genotypes, in which the first 20 of them have an
# effect on the outcome. Also generate data for 3 covariates.
maf &lt;- 0.05 + 0.45*runif(200)
X   &lt;- (runif(400*200) &lt; maf) + (runif(400*200) &lt; maf)
X   &lt;- matrix(as.double(X),400,200,byrow = TRUE)
Z   &lt;- randn(400,3)

# Generate the ground-truth regression coefficients for the variables
# (X) and additional 3 covariates (Z). Adjust the QTL effects so that
# the variables (SNPs) explain 50 percent of the variance in the
# outcome.
u    &lt;- c(-1,2,1)
beta &lt;- c(rnorm(20),rep(0,180))
beta &lt;- 1/sd(c(X %*% beta)) * beta

# Generate the quantitative trait measurements.
y &lt;- c(-2 + Z %*% u + X %*% beta + rnorm(400))

# Fit the variable selection model.
fit &lt;- varbvs(X,Z,y,logodds = seq(-3,-1,0.1))
print(summary(fit))

# Compute the posterior mean estimate of hyperparameter sa.
sa &lt;- with(fit,sum(sa * w))

# Compare estimated outcomes against observed outcomes.
y.fit &lt;- predict(fit,X,Z)
print(cor(y,y.fit))

# LOGISTIC REGRESSION EXAMPLE
# ---------------------------
# Data are 100 uncorrelated ("unlinked") single nucleotide polymorphisms
# (SNPs) with simulated genotypes, in which the first 10 of them have an
# effect on the outcome. Also generate data for 2 covariates.
maf &lt;- 0.05 + 0.45*runif(100)
X   &lt;- (runif(750*100) &lt; maf) + (runif(750*100) &lt; maf)
X   &lt;- matrix(as.double(X),750,100,byrow = TRUE)
Z   &lt;- randn(750,2)

# Generate the ground-truth regression coefficients for the variables
# (X) and additional 2 covariates (Z).
u    &lt;- c(-1,1)
beta &lt;- c(0.5*rnorm(10),rep(0,90))

# Simulate the binary trait (case-control status) as a coin toss with
# success rates given by the logistic regression.
sigmoid &lt;- function (x)
  1/(1 + exp(-x))
y &lt;- as.double(runif(750) &lt; sigmoid(-1 + Z %*% u + X %*% beta))

# Fit the variable selection model.
fit &lt;- varbvs(X,Z,y,"binomial",logodds = seq(-2,-0.5,0.5))
print(summary(fit))
</code></pre>

<hr>
<h2 id='varbvs-internal'>Internal varbvs functions</h2><span id='topic+varbvs-internal'></span><span id='topic+varbvsnorm'></span><span id='topic+varbvsbin'></span><span id='topic+varbvsbinz'></span><span id='topic+varbvspve'></span><span id='topic+var1'></span><span id='topic+var1.cols'></span>

<h3>Description</h3>

<p>Internal varbvs functions</p>


<h3>Usage</h3>

<pre><code class='language-R'>var1(x)
var1.cols(X)
varbvspve(fit,X,nr = 1000)
varbvsnorm(X,y,sigma,sa,logodds,alpha,mu,update.order,tol = 1e-4,
           maxiter = 1e4,verbose = TRUE,outer.iter = NULL,
           update.sigma = TRUE,update.sa = TRUE,n0 = 10,sa0 = 1)
varbvsbin(X,y,sa,logodds,alpha,mu,eta,update.order,tol = 1e-4,
          maxiter = 1e4,verbose = TRUE,outer.iter = NULL,
          update.sa = TRUE,optimize.eta = TRUE,n0 = 10,sa0 = 1)
varbvsbinz(X,Z,y,sa,logodds,alpha,mu,eta,update.order,tol = 1e-4,
           maxiter = 1e4,verbose = TRUE,outer.iter = NULL,
           update.sa = TRUE,optimize.eta = TRUE,n0 = 10,sa0 = 1)
</code></pre>


<h3>Details</h3>

<p>These functions are only intended to be used by expert users. Here we
provide brief descriptions of some of these internal functions.
</p>
<p><code>var1(x)</code> returns the second moment of vector x about its mean.
</p>
<p><code>var1.cols(X)</code> computes the second moment of each column of X about
its mean.
</p>
<p><code>varbvspve</code> draws posterior estimates of the proportion of
variance in Y explained by the Bayesian variable selection model
fitted using a variational approximation. This function is only valid
for the linear regression model <code>(family = "gaussian")</code>.
</p>
<p>Functions <code>varbvsnorm</code>, <code>varbvsbin</code> and <code>varbvsbinz</code>
implement the co-ordinate ascent algorithm to fit the fully-factorized
variational approximation for Bayesian variable selection, conditioned
on settings of the hyperparameters. These functions implement the
algorithm for the linear regression, logistic regression with an
intercept, and logistic regression with arbitrary covariates,
respectively.
</p>


<h3>Author(s)</h3>

<p>Peter Carbonetto &lt;peter.carbonetto@gmail.com&gt;</p>

<hr>
<h2 id='varbvs.properties'>Accessing Properties of Fitted varbvs Models</h2><span id='topic+varbvs.properties'></span><span id='topic+varbvsmix.properties'></span><span id='topic+nobs.varbvs'></span><span id='topic+case.names.varbvs'></span><span id='topic+variable.names.varbvs'></span><span id='topic+labels.varbvs'></span><span id='topic+coef.varbvs'></span><span id='topic+coef.varbvsmix'></span><span id='topic+confint.varbvs'></span><span id='topic+fitted.varbvs'></span><span id='topic+resid.varbvs'></span><span id='topic+residuals.varbvs'></span><span id='topic+deviance.varbvs'></span>

<h3>Description</h3>

<p>All these functions are <code><a href="utils.html#topic+methods">methods</a></code> for class
<code>"varbvs"</code> objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'varbvs'
nobs(object, ...)
## S3 method for class 'varbvs'
case.names(object, ...)
## S3 method for class 'varbvs'
variable.names(object, full = FALSE,
                                include.threshold = 0.01, ...)
## S3 method for class 'varbvs'
labels(object, ...)
## S3 method for class 'varbvs'
coef(object, ...)
## S3 method for class 'varbvsmix'
coef(object, ...)
## S3 method for class 'varbvs'
confint(object, parm, level = 0.95, ...)
## S3 method for class 'varbvs'
fitted(object, ...)
## S3 method for class 'varbvs'
resid(object, type = c("deviance","response"), ...)
## S3 method for class 'varbvs'
residuals(object, type = c("deviance","response"), ...)
## S3 method for class 'varbvs'
deviance(object, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="varbvs.properties_+3A_object">object</code></td>
<td>
<p>An object inheriting from class <code>varbvs</code>, usually
the result of calling function <code><a href="#topic+varbvs">varbvs</a></code>.</p>
</td></tr>
<tr><td><code id="varbvs.properties_+3A_full">full</code></td>
<td>
<p>logical; if <code>TRUE</code>, names of all variables (columns
of <code>X</code>) are returned, including variables that have zero
probability of being included in the regression model.</p>
</td></tr>
<tr><td><code id="varbvs.properties_+3A_include.threshold">include.threshold</code></td>
<td>
<p>When <code>full = FALSE</code>, names of all
variables (columns of <code>X</code>) with &quot;averaged&quot; posterior inclusion
probability greater than <code>include.threshold</code> are returned.</p>
</td></tr>
<tr><td><code id="varbvs.properties_+3A_parm">parm</code></td>
<td>
<p>Confidence intervals are computed for these selected
variables. These may either be specified as numbers (column indices
of <code>varbvs</code> input matrix <code>X</code>) or names (column names of
<code>X</code>). If not specified, confidence intervals will be computed
for the top 5 variables by posterior inclusion probability.
Confidence intervals are not provided for covariates (columns of
<code>Z</code>); see below for details.</p>
</td></tr>
<tr><td><code id="varbvs.properties_+3A_level">level</code></td>
<td>
<p>Size of confidence level.</p>
</td></tr>
<tr><td><code id="varbvs.properties_+3A_type">type</code></td>
<td>
<p>Type of residuals to be returned. This argument is only
relevant for logistic regression models (<code>family =
    "binomial"</code>). See <code><a href="#topic+varbvs">varbvs</a></code> for more details about the
two available types of residuals for logistic regression.</p>
</td></tr>
<tr><td><code id="varbvs.properties_+3A_...">...</code></td>
<td>
<p>Further arguments passed to or from other methods.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The generic accessor functions <code>nobs</code>, <code>case.names</code>,
<code>variable.names</code> and <code>labels</code> can be used to extract various
useful properties of the fitted <code>varbvs</code> model. Method
<code>labels</code>, in particular, returns the names of the candidate
variables (columns of <code>X</code>) which may be used, for example, to
plot posterior inclusion probabilities or effect estimates.
</p>
<p><code>coef</code> returns a matrix containing the posterior estimates of the
regression coefficients at each hyperparameter setting, as well as an
additional column containing &quot;averaged&quot; coefficient estimates.
</p>
<p><code>confint</code> returns confidence intervals (also, equivalently in
this case, &quot;credible intervals&quot;) for all selected variables
<code>parm</code>. These are <em>conditional</em> confidence intervals; that
is, conditioned on each variable being included in the regression
model.
</p>
<p>The <code>confint</code> return value is different from the usual confidence
interval (e.g., for an <code><a href="stats.html#topic+lm">lm</a></code> result) because a
confidence interval is provided for each hyperparameter setting, as
well as an additional &quot;averaged&quot; confidence interval. The confidence
intervals are returned a list, with one list element per selected
variable, and each list element is a matrix with columns giving lower
and upper confidence limits for each hyperparameter setting, as well
as the averaged limits.
</p>
<p>Note that confidence intervals cannot currently be requested for
covariates (columns of <code>Z</code>).
</p>
<p><code>fitted</code> returns a matrix containing the predicted (or &quot;fitted&quot;)
values of the outcome at each hyperparameter setting. For the logistic
regression model (<code>family = "binomial"</code>), each matrix entry gives
the probability that the binary outcome is equal to 1.
</p>
<p>Likewise, <code>resid</code> and <code>residuals</code> each return a matrix
containing the model residuals at each hyperparameter setting.
</p>
<p><code>deviance</code> returns the deviance for the fitted model at each
hyperparameter setting.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+varbvs">varbvs</a></code>, <code><a href="stats.html#topic+nobs">nobs</a></code>,
<code><a href="stats.html#topic+case.names">case.names</a></code>, 
<code><a href="stats.html#topic+variable.names">variable.names</a></code>, <code><a href="base.html#topic+labels">labels</a></code>,
<code><a href="stats.html#topic+coef">coef</a></code>, <code><a href="stats.html#topic+coef">coef</a></code>,
<code><a href="stats.html#topic+fitted">fitted</a></code>, <code><a href="stats.html#topic+residuals">residuals</a></code>,
<code><a href="stats.html#topic+deviance">deviance</a></code>
</p>

<hr>
<h2 id='varbvsbf'>Compute numerical estimate of Bayes factor.</h2><span id='topic+varbvsbf'></span><span id='topic+bayesfactor'></span>

<h3>Description</h3>

<p>The Bayes factor is the ratio of the marginal likelihoods
under two different models (see Kass &amp; Raftery, 1995). Function
<code>varbvsbf</code> provides a convenient interface for computing the
Bayes factor comparing the fit of two different <code>varbvs</code> models.</p>


<h3>Usage</h3>

<pre><code class='language-R'>  varbvsbf (fit0, fit1)  

  bayesfactor (logw0, logw1)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="varbvsbf_+3A_fit0">fit0</code></td>
<td>
<p>An output returned from <code><a href="#topic+varbvs">varbvs</a></code>.</p>
</td></tr>
<tr><td><code id="varbvsbf_+3A_fit1">fit1</code></td>
<td>
<p>Another output returned from <code><a href="#topic+varbvs">varbvs</a></code>.</p>
</td></tr>
<tr><td><code id="varbvsbf_+3A_logw0">logw0</code></td>
<td>
<p>log-probabilities or log-importance weights under H0.</p>
</td></tr>
<tr><td><code id="varbvsbf_+3A_logw1">logw1</code></td>
<td>
<p>log-probabilities or log-importance weights under H1.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Computes numerical estimate of
</p>
<p style="text-align: center;"><code class="reqn">
  BF = Pr(data | H1) / Pr(data | H0),
</code>
</p>

<p>the probability of the data given the &quot;alternative&quot; hypothesis (H1) over
the probability of the data given the &quot;null&quot; hypothesis (H0). This is
also known as a Bayes factor (see Kass &amp; Raftery, 1995). Here we assume
that although these probabilities cannot be computed analytically
because they involve intractable integrals, we can obtain reasonable
estimates of these probabilities with a simple numerical approximation
over some latent variable assuming the prior over this latent variable
is uniform. The inputs are the log-probabilities
</p>
<p style="text-align: center;"><code class="reqn">
  Pr(data, Z0 | H0) = Pr(data | Z0, H0) x Pr(Z0 | H0),
  Pr(data, Z1 | H1) = Pr(data | Z1, H1) x Pr(Z1 | H1),
</code>
</p>

<p>where Pr(Z0 | H0) and Pr(Z1 | H1) are uniform over all Z0 and Z1.
</p>
<p>Alternatively, this function can be viewed as computing an importance
sampling estimate of the Bayes factor; see, for example, R. M. Neal,
&quot;Annealed importance sampling&quot;, Statistics and Computing, 2001. This
formulation described above is a special case of importance sampling
when the settings of the latent variable Z0 and A1 are drawn from the
same (uniform) distribution as the prior, Pr(Z0 | H0) and Pr(Z1 | H1),
respectively.
</p>


<h3>Value</h3>

<p>The estimated Bayes factor.
</p>


<h3>Author(s)</h3>

<p>Peter Carbonetto <a href="mailto:peter.carbonetto@gmail.com">peter.carbonetto@gmail.com</a></p>


<h3>References</h3>

<p>P. Carbonetto and M. Stephens (2012). Scalable variational inference
for Bayesian variable selection in regression, and its accuracy in
genetic association studies. <em>Bayesian Analysis</em> <b>7</b>,
73&ndash;108.
</p>
<p>R. E. Kass and A. E. Raftery (1995). Bayes Factors. <em>Journal of the
American Statistical Association</em> <b>90</b>, 773&ndash;795.
</p>
<p>R. M. Neal (2001). Annealed importance sampling. <em>Statistics and
Computing</em> <b>11</b>, 125&ndash;139.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+varbvs">varbvs</a></code>, <code><a href="#topic+normalizelogweights">normalizelogweights</a></code></p>

<hr>
<h2 id='varbvsindep'>Compute posterior statistics, ignoring correlations.</h2><span id='topic+varbvsindep'></span>

<h3>Description</h3>

<p>Compute the mean and variance of the coefficients, and the
posterior inclusion probabilities (PIPs), ignoring correlations between
variables. This is useful for inspecting or visualizing groups of
correlated variables (e.g., genetic markers in linkage disequilibrium).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  varbvsindep (fit, X, Z, y)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="varbvsindep_+3A_fit">fit</code></td>
<td>
<p>Output of function <code>varbvs</code>.</p>
</td></tr>
<tr><td><code id="varbvsindep_+3A_x">X</code></td>
<td>
<p>n x p input matrix, where n is the number of samples,
and p is the number of variables. X cannot be sparse,
and cannot have any missing values (NA).</p>
</td></tr>
<tr><td><code id="varbvsindep_+3A_z">Z</code></td>
<td>
<p>n x m covariate data matrix, where m is the number of
covariates. Do not supply an intercept as a covariate
(i.e., a column of ones), because an intercept is
automatically included in the regression model. For no
covariates, set <code>Z = NULL</code>.</p>
</td></tr>
<tr><td><code id="varbvsindep_+3A_y">y</code></td>
<td>
<p>Vector of length n containing observations of binary
(<code>family = "binomial"</code>) or continuous (<code>family =
          "gaussian"</code>) outcome. For a binary outcome, all entries
of y must be 0 or 1.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For the ith hyperparameter setting, <code>alpha[,i]</code> is the
variational estimate of the posterior inclusion probability (PIP) for
each variable; <code>mu[,i]</code> is the variational estimate of the
posterior mean coefficient given that it is included in the model; and
<code>s[,i]</code> is the estimated posterior variance of the coefficient
given that it is included in the model.
</p>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>alpha</code></td>
<td>
<p>Variational estimates of posterior inclusion
probabilities for each hyperparameter setting.</p>
</td></tr>
<tr><td><code>mu</code></td>
<td>
<p>Variational estimates of posterior mean coefficients for
each hyperparameter setting.</p>
</td></tr>
<tr><td><code>s</code></td>
<td>
<p>Variational estimates of posterior variances for each
hyperparameter setting.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Peter Carbonetto <a href="mailto:peter.carbonetto@gmail.com">peter.carbonetto@gmail.com</a></p>


<h3>References</h3>

<p>P. Carbonetto and M. Stephens (2012). Scalable variational
inference for Bayesian variable selection in regression, and its
accuracy in genetic association studies. <em>Bayesian Analysis</em> <b>7</b>, 
73&ndash;108.</p>


<h3>See Also</h3>

<p><code><a href="#topic+varbvs">varbvs</a></code></p>

<hr>
<h2 id='varbvsmix'>Fit linear regression with mixture-of-normals priors using
variational approximation methods.</h2><span id='topic+varbvsmix'></span>

<h3>Description</h3>

<p>Find the &quot;best&quot; fully-factorized approximation to the
posterior distribution of the coefficients, with linear regression
likelihood and mixture-of-normals priors on the coefficients. By
&quot;best&quot;, we mean the approximating distribution that locally minimizes
the Kullback-Leibler divergence between the approximating distribution
and the exact posterior. In the original formulation (see
<code><a href="#topic+varbvs">varbvs</a></code>), each regression coefficient was drawn
identically from a spike-and-slab prior. Here, we instead formulate
the &ldquo;slab&rdquo; as a mixture of normals.</p>


<h3>Usage</h3>

<pre><code class='language-R'>  varbvsmix(X, Z, y, sa, sigma, w, alpha, mu, update.sigma, update.sa,
            update.w, w.penalty, drop.threshold = 1e-8, tol = 1e-4,
            maxiter = 1e4, update.order = 1:ncol(X), verbose = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="varbvsmix_+3A_x">X</code></td>
<td>
<p>n x p input matrix, where n is the number of samples, and p
is the number of variables. X cannot be sparse, and cannot have any
missing values (NA).</p>
</td></tr>
<tr><td><code id="varbvsmix_+3A_z">Z</code></td>
<td>
<p>n x m covariate data matrix, where m is the number of
covariates. Do not supply an intercept as a covariate (i.e., a
column of ones), because an intercept is automatically included in
the regression model. For no covariates, set <code>Z = NULL</code>.</p>
</td></tr>
<tr><td><code id="varbvsmix_+3A_y">y</code></td>
<td>
<p>Vector of length n containing values of the continuous
outcome.</p>
</td></tr>
<tr><td><code id="varbvsmix_+3A_sa">sa</code></td>
<td>
<p>Vector specifying the prior variance of the regression
coefficients (scaled by <code>sigma</code>) for each mixture component. If
not specified, a &quot;reasonable&quot; set of prior variances is
automatically selected based on a simple regression analysis in
which all variables (columns of <code>X</code>) are treated as
independent. To override this default, set <code>sa</code> to vector of
variances in which the first mixture component is the &quot;spike&quot;, and
therefore should be zero. Alternatively, <code>sa</code> may be an integer
greater than 1, in which case this controls the number of mixture
components, and the variances of the individual mixture components
are automatically selected as described.</p>
</td></tr>
<tr><td><code id="varbvsmix_+3A_sigma">sigma</code></td>
<td>
<p>Residual variance parameter. If missing, it is
automatically fitted to the data by computing an approximate
maximum-likelihood estimate.</p>
</td></tr>
<tr><td><code id="varbvsmix_+3A_w">w</code></td>
<td>
<p>If missing, it is automatically fitted to the data by
computing an approximate maximum-likelihood estimate.</p>
</td></tr>
<tr><td><code id="varbvsmix_+3A_alpha">alpha</code></td>
<td>
<p>Initial estimates of the approximate posterior mixture
assignment probabilities. These should be specified as a p x K
matrix, where K is the number of mixture components. Each row must
add up to 1.</p>
</td></tr>
<tr><td><code id="varbvsmix_+3A_mu">mu</code></td>
<td>
<p>Initial estimates of the approximate regression coefficients
conditioned on being drawn from each of the K mixture
components. These estimates should be provided as a p x K matrix,
where K is the number of mixture components.</p>
</td></tr>
<tr><td><code id="varbvsmix_+3A_update.sigma">update.sigma</code></td>
<td>
<p>If <code>TRUE</code>, sigma is fitted to data using an
approximate EM algorithm, in which case argument <code>sigma</code>, if
provided, is the initial estimate.</p>
</td></tr>
<tr><td><code id="varbvsmix_+3A_update.sa">update.sa</code></td>
<td>
<p>Currently, estimate of mixture component variances is
not implemented, so this must be set to <code>TRUE</code>, otherwise an
error will be generated.</p>
</td></tr>
<tr><td><code id="varbvsmix_+3A_update.w">update.w</code></td>
<td>
<p>If <code>TRUE</code>, mixture weights are fitted using an
approximate EM algorithm, in which case argument <code>w</code>, if
provided, is the initial estimate.</p>
</td></tr>
<tr><td><code id="varbvsmix_+3A_w.penalty">w.penalty</code></td>
<td>
<p>Penalty term for the mixture weights. It is useful
for &quot;regularizing&quot; the estimate of <code>w</code> when we do not have a
lot of information. It should be a vector with one positive entry
for each mixture component. Larger values place more weight on the
corresponding mixture components. It is based on the Dirichlet
distribution with parameters <code>w.penalty</code>. The default is a
vector of ones, which reduces to a uniform prior on <code>w</code>.</p>
</td></tr>
<tr><td><code id="varbvsmix_+3A_drop.threshold">drop.threshold</code></td>
<td>
<p>Posterior probability threshold for dropping
mixture components. Should be a positive number close to zero. If,
at any point during the optimization, all posterior mixture
assignment probabilities for a given mixture component <code>k</code> are
less than <code>drop.threshold</code>, the mixture weight for component
<code>k</code> is automatically set to zero. Set <code>drop.threshold</code> to
zero to disable this behaviour. Setting larger values for
<code>drop.threshold</code> may improve computation speed at a small cost
to numerical accuracy of the final results.</p>
</td></tr>
<tr><td><code id="varbvsmix_+3A_tol">tol</code></td>
<td>
<p>Convergence tolerance for co-ordinate ascent updates.</p>
</td></tr>
<tr><td><code id="varbvsmix_+3A_maxiter">maxiter</code></td>
<td>
<p>Maximum number of co-ordinate ascent iterations.</p>
</td></tr>
<tr><td><code id="varbvsmix_+3A_update.order">update.order</code></td>
<td>
<p>Order of the co-ordinate ascent updates for
fitting the variational approximation. The default is
<code>update.order = 1:p</code>, where <code>p</code> is the number of variables
(the number of columns of <code>X</code>).</p>
</td></tr>
<tr><td><code id="varbvsmix_+3A_verbose">verbose</code></td>
<td>
<p>If <code>verbose = TRUE</code>, print progress of algorithm
to console.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>See <a href="https://www.overleaf.com/8954189vvpqnwpxhvhq">https://www.overleaf.com/8954189vvpqnwpxhvhq</a>.
</p>


<h3>Value</h3>

<p>An object with S3 class <code>c("varbvsmix","list")</code>.
</p>
<table role = "presentation">
<tr><td><code>n</code></td>
<td>
<p>Number of data samples used to fit model.</p>
</td></tr>
<tr><td><code>mu.cov</code></td>
<td>
<p>Posterior mean regression coefficients for covariates,
including intercept.</p>
</td></tr>
<tr><td><code>update.sigma</code></td>
<td>
<p>If <code>TRUE</code>, residual variance parameter
<code>sigma</code> was fit to data.</p>
</td></tr>
<tr><td><code>update.sa</code></td>
<td>
<p>If <code>TRUE</code>, mixture variances were fit to data.</p>
</td></tr>
<tr><td><code>update.w</code></td>
<td>
<p>If <code>TRUE</code>, mixture weights were fit to data.</p>
</td></tr>
<tr><td><code>w.penalty</code></td>
<td>
<p>Penalty used for updating mixture weights.</p>
</td></tr>
<tr><td><code>drop.threshold</code></td>
<td>
<p>Posterior probabiltiy threshold used in the
optimization procedure for setting mixture weights to zero.</p>
</td></tr>
<tr><td><code>sigma</code></td>
<td>
<p>Fitted or user-specified residual variance parameter.</p>
</td></tr>
<tr><td><code>sa</code></td>
<td>
<p>User-specified mixture variances.</p>
</td></tr>
<tr><td><code>w</code></td>
<td>
<p>Fitted or user-specified mixture weights.</p>
</td></tr>
<tr><td><code>alpha</code></td>
<td>
<p>Variational estimates of posterior mixture assignent
probabilities.</p>
</td></tr>
<tr><td><code>mu</code></td>
<td>
<p>Variational estimates of posterior mean coefficients.</p>
</td></tr>
<tr><td><code>s</code></td>
<td>
<p>Variational estimates of posterior variances.</p>
</td></tr>
<tr><td><code>lfsr</code></td>
<td>
<p>Local false sign rate (LFSR) for each variable computed
from variational estimates of posterior assignment probabilities and
posterior means and variances. See Stephens (2017) for a definition
of the LFSR.</p>
</td></tr>
<tr><td><code>logZ</code></td>
<td>
<p>Variational lower bound to marginal log-likelihood at each
iteration of the co-ordinate ascent algorithm.</p>
</td></tr>
<tr><td><code>err</code></td>
<td>
<p>Maximum difference in the variational posterior
probabilities at each iteration of the co-ordinate ascent
algorithm.</p>
</td></tr>
<tr><td><code>nzw</code></td>
<td>
<p>Number of nonzero mixture components (including the
&quot;spike&quot;) at each iteration of the co-ordinate ascent algorithm.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Peter Carbonetto <a href="mailto:peter.carbonetto@gmail.com">peter.carbonetto@gmail.com</a></p>


<h3>References</h3>

<p>M. Stephens (2017). False discovery rates: a new deal.
<em>Biostatistics</em> <b>18</b>, 275&ndash;294.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+varbvs">varbvs</a></code></p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Generate the data set.
set.seed(1)
n    &lt;- 200
p    &lt;- 500
X    &lt;- randn(n,p)
sd   &lt;- c(0,0.2,0.5)
w    &lt;- c(0.9,0.05,0.05)
k    &lt;- sample(length(w),p,replace = TRUE,prob = w)
beta &lt;- sd[k] * rnorm(p)
y    &lt;- c(X %*% beta + rnorm(n))

# Fit the model to the data, in which the variances of the mixture
# prior are automatically selected.
fit1 &lt;- varbvsmix(X,NULL,y)

# Fit the model, but use only 3 mixture components in the prior
# instead of the default of 20.
fit2 &lt;- varbvsmix(X,NULL,y,3)

# Use the "ground-truth" prior variances (the ones used to simulate
# the data).
fit3 &lt;- varbvsmix(X,NULL,y,sd^2)

# Compare predicted outcomes against observed outcomes.
y.fit1 &lt;- predict(fit1,X)
print(cor(y,y.fit1))

## Not run: 
library(lattice)
print(xyplot(beta.est ~ beta.true,
             data.frame(beta.true = beta,
                        beta.fitted = rowSums(fit$alpha * fit$mu)),
             pch = 20,col = "royalblue",cex = 1))

## End(Not run)
</code></pre>

<hr>
<h2 id='varbvsproxybf'>Compute Bayes factors measuring improvement-in-fit along 1
dimension.</h2><span id='topic+varbvsproxybf'></span>

<h3>Description</h3>

<p>For each candidate variable j, this function returns a
Bayes factor measuring the improvement in fit when variable j is
included in the model instead of variable i; that is, a larger Bayes
factor indicates a better model fit by swapping variables i and
j. From an optimization perspective, this could be viewed as
addressing the following question: if you had to update the
variational parameters for one variable so as to improve the &quot;fit&quot; of
the variational approximation after setting the posterior inclusion
probability for variable i to zero, which variable would you choose?
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  varbvsproxybf(X, Z, y, fit, i, vars)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="varbvsproxybf_+3A_x">X</code></td>
<td>
<p>n x p input matrix, where n is the number of samples, and p
is the number of variables. X cannot be sparse, and cannot have any
missing values (NA).</p>
</td></tr>
<tr><td><code id="varbvsproxybf_+3A_z">Z</code></td>
<td>
<p>n x m covariate data matrix, where m is the number of
covariates. Do not supply an intercept as a covariate (i.e., a
column of ones), because an intercept is automatically included in
the regression model. For no covariates, set <code>Z = NULL</code>.</p>
</td></tr>
<tr><td><code id="varbvsproxybf_+3A_y">y</code></td>
<td>
<p>Vector of length n containing values of the continuous
outcome.</p>
</td></tr>
<tr><td><code id="varbvsproxybf_+3A_fit">fit</code></td>
<td>
<p>An object inheriting from class <code>varbvs</code>, usually the
result of calling function <code><a href="#topic+varbvs">varbvs</a></code>. Currently, this is
only implemented for linear regression (<code>family =
    "gaussian"</code>); any other choice will produce an error.</p>
</td></tr>
<tr><td><code id="varbvsproxybf_+3A_i">i</code></td>
<td>
<p>Variable against will. Typically, will be a variable included
in the regression model with high probability, but not always.</p>
</td></tr>
<tr><td><code id="varbvsproxybf_+3A_vars">vars</code></td>
<td>
<p>Set of candidate &quot;proxy&quot; variables. This set may include
<code>i</code>, but not does not have to.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>varbvsproxybf</code> returns a list with the following components:
</p>
<table role = "presentation">
<tr><td><code>BF</code></td>
<td>
<p>Matrix containing Bayes factors for each candidate proxy
variable and for each hyperparameter setting.</p>
</td></tr>
<tr><td><code>mu</code></td>
<td>
<p>Matrix containing estimated posterior means for each
candidate proxy variable and for each hyperparameter setting.</p>
</td></tr>
<tr><td><code>s</code></td>
<td>
<p>Matrix containing estimated posterior variances for each
candidate proxy variance for each hyperparameter setting.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Peter Carbonetto <a href="mailto:peter.carbonetto@gmail.com">peter.carbonetto@gmail.com</a></p>


<h3>See Also</h3>

<p><code><a href="#topic+varbvs">varbvs</a></code></p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
