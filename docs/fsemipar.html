<!DOCTYPE html><html><head><title>Help for package fsemipar</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {fsemipar}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#fsemipar-package'>
<p>Estimation, Variable Selection and Prediction for Functional Semiparametric Models</p></a></li>
<li><a href='#FASSMR.kernel.fit'>
<p>FASSMR with kernel estimation</p></a></li>
<li><a href='#FASSMR.kNN.fit'>
<p>FASSMR with kNN estimation</p></a></li>
<li><a href='#fsemipar.internal'>
<p>Package fsemipar internal functions</p></a></li>
<li><a href='#fsim.kernel.fit'>
<p>Functional single-index model fit using kernel estimation</p></a></li>
<li><a href='#fsim.kernel.test'>
<p>Functional single-index kernel predictor</p></a></li>
<li><a href='#fsim.kNN.fit'><p>Functional single-index model fit using kNN estimation</p></a></li>
<li><a href='#fsim.kNN.test'>
<p>Functional single-index kNN predictor</p></a></li>
<li><a href='#IASSMR.kernel.fit'>
<p>IASSMR with kernel estimation</p></a></li>
<li><a href='#IASSMR.kNN.fit'>
<p>IASSMR with kNN estimation</p></a></li>
<li><a href='#lm.pels.fit'>
<p>Linear model fit</p></a></li>
<li><a href='#plot.classes'>
<p>Plot outputs from regression estimation methods</p></a></li>
<li><a href='#predict.fsim'>
<p>Prediction from functional single-index model estimates</p></a></li>
<li><a href='#predict.IASSMR'>
<p>Prediction from multi-functional partial linear single-index model</p></a></li>
<li><a href='#predict.lm'>
<p>Prediction from linear model estimates</p></a></li>
<li><a href='#predict.mfplm'>
<p>Prediction from multi-functional partial linear model</p></a></li>
<li><a href='#predict.sfpl'>
<p>Predictions from semi-functional partial linear regression</p></a></li>
<li><a href='#predict.sfplsim.FASSMR'>
<p>Prediction from functional semiparametric  model estimates</p></a></li>
<li><a href='#print.summary.fsim'>
<p>Summarize information from functional single-index model (FSIM) estimation methods</p></a></li>
<li><a href='#print.summary.lm'>
<p>Summarize information from  linear model estimation methods</p></a></li>
<li><a href='#print.summary.mfpl'>
<p>Summarize information from multi-functional partial linear  model (MFPLM) estimation methods</p></a></li>
<li><a href='#print.summary.mfplsim'>
<p>Summarize information from multi-functional partial linear single-index model (MFPLSIM) estimation methods</p></a></li>
<li><a href='#print.summary.sfpl'>
<p>Summarize information from semi-functional partial linear model (SFPLM) estimation methods</p></a></li>
<li><a href='#print.summary.sfplsim'>
<p>Summarize information from semi-functional partial linear single-index model (SFPLSIM) estimation methods</p></a></li>
<li><a href='#projec'><p>Inner product computation</p></a></li>
<li><a href='#PVS.fit'>
<p>PVS estimation</p></a></li>
<li><a href='#PVS.kernel.fit'>
<p>PVS with kernel estimation</p></a></li>
<li><a href='#PVS.kNN.fit'>
<p>PVS with kNN estimation</p></a></li>
<li><a href='#semimetric.projec'>
<p>Projection semi-metric computation</p></a></li>
<li><a href='#sfpl.kernel.fit'>
<p>Sparse semi-functional partial linear model fit using kernel estimation</p></a></li>
<li><a href='#sfpl.kNN.fit'>
<p>Sparse semi-functional partial linear model fit using kNN estimation</p></a></li>
<li><a href='#sfplsim.kernel.fit'>
<p>Sparse semi-functional partial linear single-index model fit using kernel estimation</p></a></li>
<li><a href='#sfplsim.kNN.fit'>
<p>Sparse semi-functional partial linear single-index model fit using kNN estimation</p></a></li>
<li><a href='#Sugar'>
<p>Sugar data</p></a></li>
<li><a href='#Tecator'>
<p>Tecator data</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Estimation, Variable Selection and Prediction for Functional
Semiparametric Models</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.1</td>
</tr>
<tr>
<td>Date:</td>
<td>2024-02-17</td>
</tr>
<tr>
<td>Author:</td>
<td>German Aneiros [aut],
  Silvia Novo [aut, cre]</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0), splines, gtools, grpreg, DiceKriging, graphics,
stats</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Silvia Novo &lt;snovo@est-econ.uc3m.es&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Routines for estimation or simultaneous estimation and variable selection of several functional semiparametric models with scalar response, such as the functional single-index model, the semi-functional partial linear model or the semi-functional partial linear single-index model. In addition, it includes algorithms for dealing with scalar covariates with linear effect coming from the discretization of a curve in the cases of the linear model, the multi-functional partial linear model and the multi-functional partial linear single-index model. </td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-02-17 15:56:10 UTC; Silvia</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-02-21 10:20:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='fsemipar-package'>
Estimation, Variable Selection and Prediction for Functional Semiparametric Models
</h2><span id='topic+fsemipar-package'></span><span id='topic+fsemipar'></span>

<h3>Description</h3>

<p> This package is devoted to estimation or simultaneous estimation and variable selection of several functional semiparametric models with scalar response, such as the functional single-index model, the semi-functional partial linear model or the semi-functional partial linear single-index model. It also contains algorithms for addressing estimation and variable selection in the linear model, the multi-functional partial linear model, and the multi-functional partial linear single-index model when the scalar covariates with linear effects come from the discretization of a curve. In addition, the package has routines for kernel- and kNN-based estimation with Nadaraya-Watson weights of models with a nonparametric component. It also contains functions to compute predictions from all the considered models and estimation procedures.
</p>


<h3>Details</h3>

<p>The package could be divided in several tematic sections:
</p>

<ol>
<li><p> Estimation and prediction of the functional single-index model.
</p>

<ul>
<li> <p><code><a href="#topic+projec">projec</a></code>.
</p>
</li>
<li> <p><code><a href="#topic+semimetric.projec">semimetric.projec</a></code>.
</p>
</li>
<li> <p><code><a href="#topic+fsim.kernel.fit">fsim.kernel.fit</a></code> and <code><a href="#topic+fsim.kNN.fit">fsim.kNN.fit</a></code>.
</p>
</li>
<li> <p><code><a href="#topic+fsim.kernel.test">fsim.kernel.test</a></code> and <code><a href="#topic+fsim.kNN.test">fsim.kNN.test</a></code>.
</p>
</li>
<li> <p><code>predict, summary</code> and <code>print</code> methods for <code>fsim.kernel</code> and <code>fsim.kNN</code> classes.
</p>
</li></ul>

</li>
<li><p> Variable selection, estimation and prediction of the semi-functional partial linear single-index model.
</p>

<ul>
<li> <p><code><a href="#topic+sfplsim.kernel.fit">sfplsim.kernel.fit</a></code> and <code><a href="#topic+sfplsim.kNN.fit">sfplsim.kNN.fit</a></code>.
</p>
</li>
<li> <p><code>predict, summary</code> and <code>print</code> methods for <code>sfplsim.kernel</code> and <code>sfplsim.kNN</code> classes.
</p>
</li></ul>

</li>
<li><p> Variable selection, estimation and prediction of the semi-functional partial linear model.
</p>

<ul>
<li> <p><code><a href="#topic+sfpl.kernel.fit">sfpl.kernel.fit</a></code> and <code><a href="#topic+sfpl.kNN.fit">sfpl.kNN.fit</a></code>.
</p>
</li>
<li> <p><code>predict, summary</code> and <code>print</code> methods for <code>sfpl.kernel</code> and <code>sfpl.kNN</code> classes.
</p>
</li></ul>

</li>
<li><p> Variable selection, estimation and prediction of the linear model.
</p>

<ul>
<li> <p><code><a href="#topic+lm.pels.fit">lm.pels.fit</a></code>.
</p>
</li>
<li> <p><code>predict, summary</code> and <code>print</code> methods for <code>lm.pels</code> class.
</p>
</li></ul>

</li>
<li><p> Variable selection, estimation and prediction of the linear model with covariates coming from the discretisation of a curve.
</p>

<ul>
<li> <p><code><a href="#topic+PVS.fit">PVS.fit</a></code>.
</p>
</li>
<li> <p><code>predict, summary</code> and <code>print</code> methods for <code>PVS</code> class.
</p>
</li></ul>

</li>
<li><p> Variable selection, estimation and prediction of the multi-functional partial linear model.
</p>

<ul>
<li> <p><code><a href="#topic+PVS.kernel.fit">PVS.kernel.fit</a></code> and <code><a href="#topic+PVS.kNN.fit">PVS.kNN.fit</a></code>.
</p>
</li>
<li> <p><code>predict, summary</code> and <code>print</code> methods for <code>PVS.kernel</code> and <code>PVS.kNN</code> classes.
</p>
</li></ul>

</li>
<li><p> Variable selection, estimation and prediction of the multi-functional partial linear single-index model.
</p>

<ul>
<li> <p><code><a href="#topic+FASSMR.kernel.fit">FASSMR.kernel.fit</a></code> and <code><a href="#topic+FASSMR.kNN.fit">FASSMR.kNN.fit</a></code>.
</p>
</li>
<li> <p><code><a href="#topic+IASSMR.kernel.fit">IASSMR.kernel.fit</a></code> and <code><a href="#topic+IASSMR.kNN.fit">IASSMR.kNN.fit</a></code>.
</p>
</li>
<li> <p><code>predict, summary</code> and <code>print</code> methods for <code>FASSMR.kernel</code>, <code>FASSMR.kNN</code>, <code>IASSMR.kernel</code> and <code>IASSMR.kNN</code> classes.
</p>
</li></ul>

</li>
<li><p> Two datasets: <code><a href="#topic+Tecator">Tecator</a></code> and <code><a href="#topic+Sugar">Sugar</a></code>.
</p>
</li></ol>



<h3>Author(s)</h3>

<p>German Aneiros [aut],
  Silvia Novo [aut, cre]
</p>
<p>Maintainer: Silvia Novo &lt;snovo@est-econ.uc3m.es&gt;
</p>


<h3>References</h3>

<p>Aneiros, G. and Vieu, P., (2014) Variable selection in infinite-dimensional problems, <em>Statistics and Probability Letters</em>, <b>94</b>, 12&ndash;20. <a href="https://doi.org/10.1016/j.spl.2014.06.025">doi:10.1016/j.spl.2014.06.025</a>.
</p>
<p>Aneiros, G., Ferraty, F., and Vieu, P., (2015) Variable selection in partial linear regression with functional
covariate, <em>Statistics</em>, <b>49</b> 1322&ndash;1347, <a href="https://doi.org/10.1080/02331888.2014.998675">doi:10.1080/02331888.2014.998675</a>.
</p>
<p>Aneiros, G., and Vieu, P., (2015) Partial linear modelling with multi-functional covariates. <em>Computational Statistics</em>, <b>30</b>, 647&ndash;671. <a href="https://doi.org/10.1007/s00180-015-0568-8">doi:10.1007/s00180-015-0568-8</a>.
</p>
<p>Novo S., Aneiros, G., and Vieu, P., (2019) Automatic and location-adaptive estimation in functional single-index regression, <em>Journal of Nonparametric Statistics</em>, <b>31(2)</b>, 364&ndash;392, <a href="https://doi.org/10.1080/10485252.2019.1567726">doi:10.1080/10485252.2019.1567726</a>.
</p>
<p>Novo, S., Aneiros, G., and Vieu, P., (2021) Sparse semiparametric regression
when predictors are mixture of functional and high-dimensional variables, <em>TEST</em>,
<b>30</b>, 481&ndash;504, <a href="https://doi.org/10.1007/s11749-020-00728-w">doi:10.1007/s11749-020-00728-w</a>.
</p>
<p>Novo, S., Aneiros, G., and Vieu, P., (2021) A kNN procedure in semiparametric
functional data analysis, <em>Statistics and Probability Letters</em>, <b>171</b>, 109028, <a href="https://doi.org/10.1016/j.spl.2020.109028">doi:10.1016/j.spl.2020.109028</a>.
</p>
<p>Novo, S., Vieu, P., and Aneiros, G., (2021) Fast and efficient algorithms for
sparse semiparametric bi-functional regression, <em>Australian and New Zealand
Journal of Statistics</em>, <b>63</b>, 606&ndash;638, <a href="https://doi.org/10.1111/anzs.12355">doi:10.1111/anzs.12355</a>.
</p>

<hr>
<h2 id='FASSMR.kernel.fit'>
FASSMR with kernel estimation
</h2><span id='topic+FASSMR.kernel.fit'></span>

<h3>Description</h3>

<p>This function computes the fast algorithm for sparse semiparametric multi-functional regression (FASSMR) with kernel estimation.
</p>
<p>This algorithm involves the penalised least-squares regularization procedure combined with kernel estimation with Nadaraya-Watson weights.
The procedure requires the  B-spline representation to estimate the functional index <code class="reqn">\theta_0</code> and an objective criterion (<code>criterion</code>) to select the initial number of covariates in the reduced model (<code>w.opt</code>), the bandwidth (<code>h.opt</code>) and the penalisation parameter (<code>lambda.opt</code>). 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>FASSMR.kernel.fit(x, z, y, seed.coeff = c(-1, 0, 1), order.Bspline = 3, 
  nknot.theta = 3, t0 = NULL, min.q.h = 0.05,max.q.h = 0.5, 
  h.seq = NULL, num.h = 10, range.grid = NULL, kind.of.kernel = "quad",
  nknot = NULL, lambda.min = NULL,lambda.min.h = NULL,
  lambda.min.l = NULL, factor.pn = 1, nlambda = 100, vn = ncol(z),
  nfolds = 10, seed = 123,wn = c(10, 15, 20), criterion = c("GCV", "BIC",
  "AIC", "k-fold-CV"), penalty = c("grLasso", "grMCP",
  "grSCAD", "gel", "cMCP", "gBridge", "gLasso", "gMCP"),
   max.iter = 1000)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="FASSMR.kernel.fit_+3A_x">x</code></td>
<td>

<p>Matrix containing the observations of the functional covariate collected by row (functional single-index component).
</p>
</td></tr>
<tr><td><code id="FASSMR.kernel.fit_+3A_z">z</code></td>
<td>

<p>Matrix containing the observations of the functional covariate that is discretised collected by row (linear component).
</p>
</td></tr>
<tr><td><code id="FASSMR.kernel.fit_+3A_y">y</code></td>
<td>

<p>Vector containing the scalar response.
</p>
</td></tr>
<tr><td><code id="FASSMR.kernel.fit_+3A_seed.coeff">seed.coeff</code></td>
<td>

<p>Vector of initial values used to  build the set <code class="reqn">\Theta_n</code> (see section <code>Details</code>). The coefficients for the B-spline representation of each eligible functional index <code class="reqn">\theta \in \Theta_n</code> are obtained from <code>seed.coeff</code>.  The default is <code>c(-1,0,1)</code>.
</p>
</td></tr>
<tr><td><code id="FASSMR.kernel.fit_+3A_order.bspline">order.Bspline</code></td>
<td>

<p>Positive integer giving the order of the B-spline basis functions. This is the number of coefficients in each piecewise polynomial segment. The default is 3.
</p>
</td></tr>
<tr><td><code id="FASSMR.kernel.fit_+3A_nknot.theta">nknot.theta</code></td>
<td>

<p>Positive integer indicating the number of uniform interior knots of the B-spline basis for the B-spline representation of <code class="reqn">\theta_0</code>. The default is 3.
</p>
</td></tr>
<tr><td><code id="FASSMR.kernel.fit_+3A_t0">t0</code></td>
<td>

<p>Value in the domain of the functional indexes at which we evaluate them to  build the set <code class="reqn">\Theta_n</code>. We assume <code class="reqn">\theta_0(t_0)&gt;0</code> for some arbitrary <code class="reqn">t_0</code> in the domain to ensure model identifiability. If <code>t0=NULL</code>, then <code>mean(range.grid)</code> is considered.
</p>
</td></tr>
<tr><td><code id="FASSMR.kernel.fit_+3A_min.q.h">min.q.h</code></td>
<td>
<p>Order of the quantile of the set of distances between curves (computed with the projection semi-metric) which gives the lower end of the sequence in which the bandwidth is selected. The default is 0.05.
</p>
</td></tr>
<tr><td><code id="FASSMR.kernel.fit_+3A_max.q.h">max.q.h</code></td>
<td>

<p>Order of the quantile of the set of distances between curves (computed with the projection semi-metric) which gives the upper end of the sequence in which the bandwidth is selected. The default is 0.5.
</p>
</td></tr>
<tr><td><code id="FASSMR.kernel.fit_+3A_h.seq">h.seq</code></td>
<td>
<p>Vector containing the sequence of bandwidths. The default is a sequence of <code>num.h</code> equispaced bandwidths in the range constructed using <code>min.q.h</code> and <code>max.q.h</code>.
</p>
</td></tr>
<tr><td><code id="FASSMR.kernel.fit_+3A_num.h">num.h</code></td>
<td>
<p>Positive integer indicating the number of bandwiths in the grid. The default is 10.
</p>
</td></tr>
<tr><td><code id="FASSMR.kernel.fit_+3A_range.grid">range.grid</code></td>
<td>

<p>Vector of length 2 containing the endpoints of the grid at which the observations of the functional covariate <code>x</code> are evaluated (i.e. the range of the discretization). If <code>range.grid=NULL</code>, then <code>range.grid=c(1,p)</code> is considered, where <code>p</code> is the size of the discretization size of <code>x</code> (i.e. <code>ncol(x))</code>.
</p>
</td></tr>
<tr><td><code id="FASSMR.kernel.fit_+3A_kind.of.kernel">kind.of.kernel</code></td>
<td>

<p>The type of kernel function used. Only Epanechnikov kernel (<code>"quad"</code>) is available.
</p>
</td></tr>
<tr><td><code id="FASSMR.kernel.fit_+3A_nknot">nknot</code></td>
<td>

<p>Positive integer indicating the number of interior knots for the B-spline representation of the functional covariate. The default value is <code>(p - order.Bspline - 1)%/%2</code>.
</p>
</td></tr>
<tr><td><code id="FASSMR.kernel.fit_+3A_lambda.min">lambda.min</code></td>
<td>

<p>The smallest value for lambda (i. e., the smallest value  of the sequence in which <code>lambda.opt</code> is selected), as fraction of <code>lambda.max</code>.
The defaults is <code>lambda.min.l</code> if the number of observations is larger than <code>factor.pn</code> times the number of covariates and <code>lambda.min.h</code> otherwise.
</p>
</td></tr>
<tr><td><code id="FASSMR.kernel.fit_+3A_lambda.min.h">lambda.min.h</code></td>
<td>

<p>The smallest value of the sequence in which <code>lambda.opt</code> is selected if the number of observations is smaller than <code>factor.pn</code> times the number of scalar covariates. The default is 0.05. 
</p>
</td></tr>
<tr><td><code id="FASSMR.kernel.fit_+3A_lambda.min.l">lambda.min.l</code></td>
<td>

<p>The smallest value of the sequence in which <code>lambda.opt</code> is selected if the number of observations is larger than <code>factor.pn</code> times the number of scalar covariates. The default is 0.0001.
</p>
</td></tr>
<tr><td><code id="FASSMR.kernel.fit_+3A_factor.pn">factor.pn</code></td>
<td>

<p>Positive integer used to set <code>lambda.min</code>. The default value is 1.
</p>
</td></tr>
<tr><td><code id="FASSMR.kernel.fit_+3A_nlambda">nlambda</code></td>
<td>

<p>Positive integer indicating the number of values of the sequence in which <code>lambda.opt</code> is selected. The default is 100.
</p>
</td></tr>
<tr><td><code id="FASSMR.kernel.fit_+3A_vn">vn</code></td>
<td>

<p>Positive integer or vector of positive integers indicating the number of groups of consecutive variables to be penalised together. The default value is <code>vn=ncol(z)</code>, which leads to the individual penalisation of each scalar covariate.
</p>
</td></tr>
<tr><td><code id="FASSMR.kernel.fit_+3A_nfolds">nfolds</code></td>
<td>

<p>Positive integer indicating the number of cross-validation folds (used if <code>criterion="k-fold-CV"</code>). Default is 10.
</p>
</td></tr>
<tr><td><code id="FASSMR.kernel.fit_+3A_seed">seed</code></td>
<td>

<p>You may set the seed of the random number generator to obtain reproducible results (used if <code>criterion="k-fold-CV"</code>). Default is 123.
</p>
</td></tr>
<tr><td><code id="FASSMR.kernel.fit_+3A_wn">wn</code></td>
<td>

<p>A vector of positive integers indicating the eligible number of covariates of the reduced model. See the section <code>Details</code>. The default is <code>c(10,15,20)</code>.
</p>
</td></tr>
<tr><td><code id="FASSMR.kernel.fit_+3A_criterion">criterion</code></td>
<td>

<p>The criterion by which to select the regularization parameter <code>lambda.opt</code> and <code>k.opt</code>. One of <code>"GCV", "BIC", "AIC"</code> or <code>"k-fold-CV"</code>. The default is <code>"GCV"</code>.
</p>
</td></tr>
<tr><td><code id="FASSMR.kernel.fit_+3A_penalty">penalty</code></td>
<td>

<p>The penalty function to be applied in the penalized least squares procedure. Only &quot;grLasso&quot; and &quot;grSCAD&quot; are implemented.
</p>
</td></tr>
<tr><td><code id="FASSMR.kernel.fit_+3A_max.iter">max.iter</code></td>
<td>

<p>Maximum number of iterations (total across entire path). Default is 1000.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The multi-functional partial linear single-index model (MFPLSIM) is given by the expression
</p>
<p style="text-align: center;"><code class="reqn">Y_i=\sum_{j=1}^{p_n}\beta_{0j}\zeta_i(t_j)+r\left(\left&lt;\theta_0,X_i\right&gt;\right)+\varepsilon_i,\ \ \ (i=1,\dots,n).</code>
</p>

<p>where 
</p>

<ul>
<li> <p><code class="reqn">Y_i</code> is a real random response and <code class="reqn">X_i</code> denotes a random element belonging to some separable Hilbert space <code class="reqn">\mathcal{H}</code> with inner product denoted by <code class="reqn">\left\langle\cdot,\cdot\right\rangle</code>. The second functional predictor <code class="reqn">\zeta_i</code> is supposed to be a random curve defined on some interval <code class="reqn">[a,b]</code> which  is observed at the points <code class="reqn">a\leq t_1&lt;\dots&lt;t_{p_n}\leq b</code>. 
</p>
</li>
<li>  <p><code class="reqn">\mathbf{\beta}_0=(\beta_{01},\dots,\beta_{0p_n})^{\top}</code> is a vector of unknown real coefficients and <code class="reqn">r(\cdot)</code> denotes a smooth unknown link function. In addition, <code class="reqn">\theta_0</code> is an unknown functional direction in <code class="reqn">\mathcal{H}</code>.  
</p>
</li>
<li> <p><code class="reqn">\varepsilon_i</code> denotes the random error.
</p>
</li></ul>

<p>In  the MFPLSIM, we assume that only a few scalar variables from the set <code class="reqn">\{\zeta(t_1),\dots,\zeta(t_{p_n})\}</code> form part of the model. Therefore, we must select the relevant variables in the linear component (the impact points of the curve <code class="reqn">\zeta</code> on the response) and estimate the model.
</p>
<p>In this function, the MFPLSIM is fitted using the FASSMR algorithm.  The main idea of this algorithm is to consider a reduced model, with only some (very few) linear covariates (but covering the entire discretization interval of <code class="reqn">\zeta</code>), and discarding directly the other linear covariates (since one expect that they contain very similar information about the response). 
</p>
<p>To explain the algorithm we assume, without lost of generality, that the number <code class="reqn">p_n</code> of linear covariates can be expressed as follows: <code class="reqn">p_n=q_nw_n</code> with <code class="reqn">q_n</code> and <code class="reqn">w_n</code> integers. 
The previous consideration allows to build a subset of the initial <code class="reqn">p_n</code> linear covariates, which contains only <code class="reqn">w_n</code> equally spaced discretized observations of  <code class="reqn">\zeta</code> covering the whole interval  <code class="reqn">[a,b]</code>. This subset is the following:
</p>
<p style="text-align: center;"><code class="reqn">
	\mathcal{R}_n^{\mathbf{1}}=\left\{\zeta\left(t_k^{\mathbf{1}}\right),\ \ k=1,\dots,w_n\right\},
</code>
</p>
 
<p>where  <code class="reqn">t_k^{\mathbf{1}}=t_{\left[(2k-1)q_n/2\right]}</code> and  <code class="reqn">\left[z\right]</code> denotes the smallest integer not less than the real number <code class="reqn">z</code>.
</p>
<p>We consider the following reduced model, which involves only the linear covariates belonging to <code class="reqn">\mathcal{R}_n^{\mathbf{1}}</code>:
</p>
<p style="text-align: center;"><code class="reqn">
	Y_i=\sum_{k=1}^{w_n}\beta_{0k}^{\mathbf{1}}\zeta_i(t_k^{\mathbf{1}})+r^{\mathbf{1}}\left(\left&lt;\theta_0^{\mathbf{1}},\mathcal{X}_i\right&gt;\right)+\varepsilon_i^{\mathbf{1}}.
</code>
</p>

<p>The program receives the eligible numbers of linear covariates for building the reduced model through the argument <code>wn</code>.
Then, the penalised least-squares variable selection procedure, with kernel estimation, is applied to the reduced model. This is done by means of the function <code><a href="#topic+sfplsim.kernel.fit">sfplsim.kernel.fit</a></code>, which requires remaining arguments (for details, see the documentation of the function <code><a href="#topic+sfplsim.kernel.fit">sfplsim.kernel.fit</a></code>). The estimates obtained after that are the outputs of the FASSMR algorithm. For further details on this algorithm, see Novo et al. (2021).
</p>
<p><b>Remark</b>: If the condition  <code class="reqn">p_n=w_n q_n</code> fails, the function considers not fixed  <code class="reqn">q_n=q_{n,k}</code> values <code class="reqn">k=1,\dots,w_n</code>,  when <code class="reqn">p_n/w_n</code> is not an integer number. Specifically:
</p>
<p style="text-align: center;"><code class="reqn">
	q_{n,k}= \left\{\begin{array}{ll}
	[p_n/w_n]+1 &amp;   k\in\{1,\dots,p_n-w_n[p_n/w_n]\},\\
	{[p_n/w_n]} &amp; k\in\{p_n-w_n[p_n/w_n]+1,\dots,w_n\},
	\end{array}
	\right.
</code>
</p>

<p>where <code class="reqn">[z]</code> denotes the integer part of the real number <code class="reqn">z</code>.
</p>


<h3>Value</h3>

<table>
<tr><td><code>call</code></td>
<td>
<p>The matched call.</p>
</td></tr>
<tr><td><code>fitted.values</code></td>
<td>
<p>Estimated scalar response.</p>
</td></tr>
<tr><td><code>residuals</code></td>
<td>
<p>Differences between <code>y</code> and the <code>fitted.values</code></p>
</td></tr>
<tr><td><code>beta.est</code></td>
<td>
<p><code class="reqn">\hat{\mathbf{\beta}}</code> (i. e. estimate of <code class="reqn">\mathbf{\beta}_0</code> when the optimal tuning parameters <code>w.opt</code>, <code>lambda.opt</code>, <code>h.opt</code> and <code>vn.opt</code> are used).</p>
</td></tr>
<tr><td><code>beta.red</code></td>
<td>
<p>Estimate of <code class="reqn">\beta_0^{\mathbf{1}}</code> in the reduced model when the optimal tuning parameters <code>w.opt</code>, <code>lambda.opt</code>, <code>h.opt</code> and <code>vn.opt</code> are used.</p>
</td></tr>
<tr><td><code>theta.est</code></td>
<td>
<p>Coefficients of <code class="reqn">\hat{\theta}</code> in the B-spline basis (i. e. estimate of <code class="reqn">\theta_0</code>when the optimal tuning parameters <code>w.opt</code>, <code>lambda.opt</code>, <code>h.opt</code> and <code>vn.opt</code> are used): a vector of <code>length(order.Bspline+nknot.theta)</code>.</p>
</td></tr>
<tr><td><code>indexes.beta.nonnull</code></td>
<td>
<p>Indexes of the non-zero <code class="reqn">\hat{\beta_{j}}</code>.</p>
</td></tr>
<tr><td><code>h.opt</code></td>
<td>
<p>Selected bandwidth (when <code>w.opt</code> is considered).</p>
</td></tr>
<tr><td><code>w.opt</code></td>
<td>
<p>Selected size for  <code class="reqn">\mathcal{R}_n^{\mathbf{1}}</code>.</p>
</td></tr>
<tr><td><code>lambda.opt</code></td>
<td>
<p>Selected value of the penalisation parameter (when <code>w.opt</code> is considered).</p>
</td></tr>
<tr><td><code>IC</code></td>
<td>
<p>Value of the criterion function considered to select <code>w.opt</code>, <code>lambda.opt</code>, <code>h.opt</code> and <code>vn.opt</code>.</p>
</td></tr>
<tr><td><code>vn.opt</code></td>
<td>
<p>Selected value of <code>vn</code> (when <code>w.opt</code> is considered).</p>
</td></tr>
<tr><td><code>beta.w</code></td>
<td>
<p>Estimate of <code class="reqn">\beta_0^{\mathbf{1}}</code>  for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>theta.w</code></td>
<td>
<p>Estimate of <code class="reqn">\theta_0^{\mathbf{1}}</code> for each value of the sequence <code>wn</code> (i.e. its coefficients in the B-spline basis).</p>
</td></tr> 
<tr><td><code>IC.w</code></td>
<td>
<p>Value of the criterion function for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>indexes.beta.nonnull.w</code></td>
<td>
<p>Indexes of the non-zero linear coefficients for each value of the sequence <code>wn</code>.</p>
</td></tr> 
<tr><td><code>lambda.w</code></td>
<td>
<p>Selected value of penalisation parameter for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>h.w</code></td>
<td>
<p>Selected bandwidth for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>index01</code></td>
<td>
<p>Indexes of the covariates (in the whole set of <code class="reqn">p_n</code>) used to build  <code class="reqn">\mathcal{R}_n^{\mathbf{1}}</code> for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>...</code></td>
<td>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>German Aneiros Perez <a href="mailto:german.aneiros@udc.es">german.aneiros@udc.es</a> 
</p>
<p>Silvia Novo Diaz  <a href="mailto:snovo@est-econ.uc3m.es">snovo@est-econ.uc3m.es</a>
</p>


<h3>References</h3>

<p>Novo, S., Vieu, P., and Aneiros, G., (2021) Fast and efficient algorithms for
sparse semiparametric bi-functional regression. <em>Australian and New Zealand
Journal of Statistics</em>, <b>63</b>, 606&ndash;638, <a href="https://doi.org/10.1111/anzs.12355">doi:10.1111/anzs.12355</a>.
</p>


<h3>See Also</h3>

<p>See also <code><a href="#topic+sfplsim.kernel.fit">sfplsim.kernel.fit</a></code>, <code><a href="#topic+predict.FASSMR.kernel">predict.FASSMR.kernel</a></code>,  <code><a href="#topic+plot.FASSMR.kernel">plot.FASSMR.kernel</a></code> and <code><a href="#topic+IASSMR.kernel.fit">IASSMR.kernel.fit</a></code>.
</p>
<p>Alternative method <code><a href="#topic+FASSMR.kNN.fit">FASSMR.kNN.fit</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

data(Sugar)

y&lt;-Sugar$ash
x&lt;-Sugar$wave.290
z&lt;-Sugar$wave.240

#Outliers
index.y.25 &lt;- y &gt; 25
index.atip &lt;- index.y.25
(1:268)[index.atip]


#Dataset to model
x.sug &lt;- x[!index.atip,]
z.sug&lt;- z[!index.atip,]
y.sug &lt;- y[!index.atip]

train&lt;-1:216

ptm=proc.time()
fit &lt;- FASSMR.kernel.fit(x=x.sug[train,],z=z.sug[train,], y=y.sug[train], 
        nknot.theta=2,lambda.min.h=0.03, lambda.min.l=0.03,
        max.q.h=0.35,num.h = 10, nknot=20,criterion="BIC", 
        penalty="grSCAD",max.iter=5000)
proc.time()-ptm

fit
names(fit)


   
</code></pre>

<hr>
<h2 id='FASSMR.kNN.fit'>
FASSMR with kNN estimation
</h2><span id='topic+FASSMR.kNN.fit'></span>

<h3>Description</h3>

<p>This function computes the fast algorithm for sparse semiparametric multi-functional regression (FASSMR) with kNN estimation.
</p>
<p>This algorithm involves the penalised least-squares regularization procedure combined with <code class="reqn">k</code>-nearest neighbours (kNN) estimation with Nadaraya-Watson weights.
The procedure requires the  B-spline representation to estimate the functional index <code class="reqn">\theta_0</code> and an objective criterion (<code>criterion</code>) to select the initial number of covariates in the reduced model (<code>w.opt</code>), the number of neighbours (<code>k.opt</code>) and the penalisation parameter (<code>lambda.opt</code>). 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>FASSMR.kNN.fit(x, z, y, seed.coeff = c(-1, 0, 1), order.Bspline = 3, 
  nknot.theta = 3, t0 = NULL, knearest = NULL, min.knn = 2, max.knn = NULL, 
  step = NULL, range.grid = NULL, kind.of.kernel = "quad", nknot = NULL, 
  lambda.min = NULL, lambda.min.h= NULL, lambda.min.l = NULL,
  factor.pn = 1, nlambda = 100, vn = ncol(z), nfolds = 10, seed = 123, 
  wn = c(10, 15, 20), criterion = c("GCV", "BIC", "AIC", "k-fold-CV"), 
  penalty = c("grLasso", "grMCP", "grSCAD", "gel", "cMCP", 
  "gBridge", "gLasso", "gMCP"),  max.iter = 1000)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="FASSMR.kNN.fit_+3A_x">x</code></td>
<td>

<p>Matrix containing the observations of the functional covariate collected by row (functional single-index component).
</p>
</td></tr>
<tr><td><code id="FASSMR.kNN.fit_+3A_z">z</code></td>
<td>

<p>Matrix containing the observations of the functional covariate that is discretised collected by row (linear component).
</p>
</td></tr>
<tr><td><code id="FASSMR.kNN.fit_+3A_y">y</code></td>
<td>

<p>Vector containing the scalar response.
</p>
</td></tr>
<tr><td><code id="FASSMR.kNN.fit_+3A_seed.coeff">seed.coeff</code></td>
<td>

<p>Vector of initial values used to  build the set <code class="reqn">\Theta_n</code> (see section <code>Details</code>). The coefficients for the B-spline representation of each eligible functional index <code class="reqn">\theta \in \Theta_n</code> are obtained from <code>seed.coeff</code>. The default is <code>c(-1,0,1)</code>.
</p>
</td></tr>
<tr><td><code id="FASSMR.kNN.fit_+3A_order.bspline">order.Bspline</code></td>
<td>

<p>Positive integer giving the order of the B-spline basis functions. This is the number of coefficients in each piecewise polynomial segment. The default is 3.
</p>
</td></tr>
<tr><td><code id="FASSMR.kNN.fit_+3A_nknot.theta">nknot.theta</code></td>
<td>

<p>Positive integer indicating the number of uniform interior knots of the B-spline basis for the B-spline representation of <code class="reqn">\theta_0</code>. The default is 3.
</p>
</td></tr>
<tr><td><code id="FASSMR.kNN.fit_+3A_t0">t0</code></td>
<td>

<p>Value in the domain of the functional indexes at which we evaluate them to  build the set <code class="reqn">\Theta_n</code>. We assume <code class="reqn">\theta_0(t_0)&gt;0</code> for some arbitrary <code class="reqn">t_0</code> in the domain to ensure model identifiability. If <code>t0=NULL</code>, then <code>mean(range.grid)</code> is considered.
</p>
</td></tr>
<tr><td><code id="FASSMR.kNN.fit_+3A_knearest">knearest</code></td>
<td>

<p>Vector of positive integers containing the sequence in which the  number of nearest neighbours <code>k.opt</code> is selected. If <code>knearest=NULL</code>, then <code>knearest &lt;- seq(from =min.knn, to = max.knn, by = step)</code>.
</p>
</td></tr>
<tr><td><code id="FASSMR.kNN.fit_+3A_min.knn">min.knn</code></td>
<td>

<p>Positive integer indicating the minumum value of the sequence in which the  number of nearest neighbours <code>k.opt</code> is selected (thus, this number must be smaller than the sample size). The default is 2.
</p>
</td></tr>
<tr><td><code id="FASSMR.kNN.fit_+3A_max.knn">max.knn</code></td>
<td>

<p>Positive integer indicating the maximum value of the sequence in which the number of nearest neighbours <code>k.opt</code> is selected (thus, this number must be larger than <code>min.kNN</code> and smaller than the sample size, <code>n</code>). The default is <code>max.knn &lt;- n%/%2</code>.
</p>
</td></tr>
<tr><td><code id="FASSMR.kNN.fit_+3A_step">step</code></td>
<td>

<p>Positive integer used to build the sequence of k-nearest neighbours in the following way: <code>min.knn, min.knn + step, min.knn + 2*step, min.knn + 3*step,...</code> The default is <code>step&lt;-ceiling(n/100)</code>.
</p>
</td></tr>
<tr><td><code id="FASSMR.kNN.fit_+3A_range.grid">range.grid</code></td>
<td>

<p>Vector of length 2 containing the endpoints of the grid at which the observations of the functional covariate <code>x</code> are evaluated (i.e. the range of the discretization). If <code>range.grid=NULL</code>, then <code>range.grid=c(1,p)</code> is considered, where <code>p</code> is the size of the discretization size of <code>x</code> (i.e. <code>ncol(x))</code>.
</p>
</td></tr>
<tr><td><code id="FASSMR.kNN.fit_+3A_kind.of.kernel">kind.of.kernel</code></td>
<td>

<p>The type of kernel function used. Only Epanechnikov kernel (<code>"quad"</code>) is available.
</p>
</td></tr>
<tr><td><code id="FASSMR.kNN.fit_+3A_nknot">nknot</code></td>
<td>

<p>Positive integer indicating the number of interior knots for the B-spline representation of the functional covariate. The default value is <code>(p - order.Bspline - 1)%/%2</code>.
</p>
</td></tr>
<tr><td><code id="FASSMR.kNN.fit_+3A_lambda.min">lambda.min</code></td>
<td>

<p>The smallest value for lambda (i. e., the smallest value  of the sequence in which <code>lambda.opt</code> is selected), as fraction of <code>lambda.max</code>.
The defaults is <code>lambda.min.l</code> if the number of observations is larger than <code>factor.pn</code> times the number of covariates and <code>lambda.min.h</code> otherwise.
</p>
</td></tr>
<tr><td><code id="FASSMR.kNN.fit_+3A_lambda.min.h">lambda.min.h</code></td>
<td>

<p>The smallest value of the sequence in which <code>lambda.opt</code> is selected if the number of observations is smaller than <code>factor.pn</code> times the number of scalar covariates. The default is 0.05. 
</p>
</td></tr>
<tr><td><code id="FASSMR.kNN.fit_+3A_lambda.min.l">lambda.min.l</code></td>
<td>

<p>The smallest value of the sequence in which <code>lambda.opt</code> is selected if the number of observations is larger than <code>factor.pn</code> times the number of scalar covariates. The default is 0.0001.
</p>
</td></tr>
<tr><td><code id="FASSMR.kNN.fit_+3A_factor.pn">factor.pn</code></td>
<td>

<p>Positive integer used to set <code>lambda.min</code>. The default value is 1.
</p>
</td></tr>
<tr><td><code id="FASSMR.kNN.fit_+3A_nlambda">nlambda</code></td>
<td>

<p>Positive integer indicating the number of values of the sequence in which <code>lambda.opt</code> is selected. The default is 100.
</p>
</td></tr>
<tr><td><code id="FASSMR.kNN.fit_+3A_vn">vn</code></td>
<td>

<p>Positive integer or vector of positive integers indicating the number of groups of consecutive variables to be penalised together. The default value is <code>vn=ncol(z)</code>, which leads to the individual penalisation of each scalar covariate.
</p>
</td></tr>
<tr><td><code id="FASSMR.kNN.fit_+3A_nfolds">nfolds</code></td>
<td>

<p>Positive integer indicating the number of cross-validation folds (used if <code>criterion="k-fold-CV"</code>). Default is 10.
</p>
</td></tr>
<tr><td><code id="FASSMR.kNN.fit_+3A_seed">seed</code></td>
<td>

<p>You may set the seed of the random number generator to obtain reproducible results (used if <code>criterion="k-fold-CV"</code>). Default is 123.
</p>
</td></tr>
<tr><td><code id="FASSMR.kNN.fit_+3A_wn">wn</code></td>
<td>

<p>A vector of positive integers indicating the eligible number of covariates of the reduced model. See the section <code>Details</code>. The default is <code>c(10,15,20)</code>.
</p>
</td></tr>
<tr><td><code id="FASSMR.kNN.fit_+3A_criterion">criterion</code></td>
<td>

<p>The criterion by which to select the regularization parameter <code>lambda.opt</code> and <code>k.opt</code>. One of <code>"GCV", "BIC", "AIC"</code> or <code>"k-fold-CV"</code>. The default is <code>"GCV"</code>.
</p>
</td></tr>
<tr><td><code id="FASSMR.kNN.fit_+3A_penalty">penalty</code></td>
<td>

<p>The penalty function to be applied in the penalized least squares procedure. Only &quot;grLasso&quot; and &quot;grSCAD&quot; are implemented.
</p>
</td></tr>
<tr><td><code id="FASSMR.kNN.fit_+3A_max.iter">max.iter</code></td>
<td>

<p>Maximum number of iterations (total across entire path). Default is 1000.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The multi-functional partial linear single-index model (MFPLSIM) is given by the expression
</p>
<p style="text-align: center;"><code class="reqn">Y_i=\sum_{j=1}^{p_n}\beta_{0j}\zeta_i(t_j)+r\left(\left&lt;\theta_0,X_i\right&gt;\right)+\varepsilon_i,\ \ \ (i=1,\dots,n).</code>
</p>

<p>where 
</p>

<ul>
<li> <p><code class="reqn">Y_i</code> is a real random response and <code class="reqn">X_i</code> denotes a random element belonging to some separable Hilbert space <code class="reqn">\mathcal{H}</code> with inner product denoted by <code class="reqn">\left\langle\cdot,\cdot\right\rangle</code>. The second functional predictor <code class="reqn">\zeta_i</code> is supposed to be a random curve defined on some interval <code class="reqn">[a,b]</code> which  is observed at the points <code class="reqn">a\leq t_1&lt;\dots&lt;t_{p_n}\leq b</code>. 
</p>
</li>
<li>  <p><code class="reqn">\mathbf{\beta}_0=(\beta_{01},\dots,\beta_{0p_n})^{\top}</code> is a vector of unknown real coefficients and <code class="reqn">r(\cdot)</code> denotes a smooth unknown link function. In addition, <code class="reqn">\theta_0</code> is an unknown functional direction in <code class="reqn">\mathcal{H}</code>.  
</p>
</li>
<li> <p><code class="reqn">\varepsilon_i</code> denotes the random error.
</p>
</li></ul>

<p>In  the MFPLSIM, we assume that only a few scalar variables from the set <code class="reqn">\{\zeta(t_1),\dots,\zeta(t_{p_n})\}</code> form part of the model. Therefore, we must select the relevant variables in the linear component (the impact points of the curve <code class="reqn">\zeta</code> on the response) and estimate the model.
</p>
<p>In this function, the MFPLSIM is fitted using the FASSMR algorithm.  The main idea of this algorithm is to consider a reduced model, with only some (very few) linear covariates (but covering the entire discretization interval of <code class="reqn">\zeta</code>), and discarding directly the other linear covariates (since one expect that they contain very similar information about the response). 
</p>
<p>To explain the algorithm we assume, without lost of generality, that the number <code class="reqn">p_n</code> of linear covariates can be expressed as follows: <code class="reqn">p_n=q_nw_n</code> with <code class="reqn">q_n</code> and <code class="reqn">w_n</code> integers. 
The previous consideration allows to build a subset of the initial <code class="reqn">p_n</code> linear covariates, which contains only <code class="reqn">w_n</code> equally spaced discretized observations of  <code class="reqn">\zeta</code> covering the whole interval  <code class="reqn">[a,b]</code>. This subset is the following:
</p>
<p style="text-align: center;"><code class="reqn">
	\mathcal{R}_n^{\mathbf{1}}=\left\{\zeta\left(t_k^{\mathbf{1}}\right),\ \ k=1,\dots,w_n\right\},
</code>
</p>
 
<p>where  <code class="reqn">t_k^{\mathbf{1}}=t_{\left[(2k-1)q_n/2\right]}</code> and  <code class="reqn">\left[z\right]</code> denotes the smallest integer not less than the real number <code class="reqn">z</code>.
</p>
<p>In this way, we consider the following reduced model, which involves only the linear covariates belonging to <code class="reqn">\mathcal{R}_n^{\mathbf{1}}</code>:
</p>
<p style="text-align: center;"><code class="reqn">
	Y_i=\sum_{k=1}^{w_n}\beta_{0k}^{\mathbf{1}}\zeta_i(t_k^{\mathbf{1}})+r^{\mathbf{1}}\left(\left&lt;\theta_0^{\mathbf{1}},\mathcal{X}_i\right&gt;\right)+\varepsilon_i^{\mathbf{1}}.
</code>
</p>

<p>The eligible numbers of linear covariates to build the reduced model are provided to the program in the argument <code>wn</code>.
Then, the penalised least-squares variable selection procedure, with kNN estimation, is applied to the reduced model. This is done by means of the function <code><a href="#topic+sfplsim.kNN.fit">sfplsim.kNN.fit</a></code>, which requires remaining arguments (for details, see the documentation of the function <code><a href="#topic+sfplsim.kNN.fit">sfplsim.kNN.fit</a></code>). The estimates obtained after that are the outputs of the FASSMR algorithm. For further details on this algorithm, see Novo et al. (2021).
</p>
<p><b>Remark</b>: If the condition  <code class="reqn">p_n=w_n q_n</code> fails, the function considers not fixed  <code class="reqn">q_n=q_{n,k}</code> values <code class="reqn">k=1,\dots,w_n</code>,  when <code class="reqn">p_n/w_n</code> is not an integer number. Specifically:
</p>
<p style="text-align: center;"><code class="reqn">
	q_{n,k}= \left\{\begin{array}{ll}
	[p_n/w_n]+1 &amp;   k\in\{1,\dots,p_n-w_n[p_n/w_n]\},\\
	{[p_n/w_n]} &amp; k\in\{p_n-w_n[p_n/w_n]+1,\dots,w_n\},
	\end{array}
	\right.
</code>
</p>

<p>where <code class="reqn">[z]</code> denotes the integer part of the real number <code class="reqn">z</code>.
</p>


<h3>Value</h3>

<table>
<tr><td><code>call</code></td>
<td>
<p>The matched call.</p>
</td></tr>
<tr><td><code>fitted.values</code></td>
<td>
<p>Estimated scalar response.</p>
</td></tr>
<tr><td><code>residuals</code></td>
<td>
<p>Differences between <code>y</code> and the <code>fitted.values</code></p>
</td></tr>
<tr><td><code>beta.est</code></td>
<td>
<p><code class="reqn">\hat{\mathbf{\beta}}</code> (i. e. estimate of <code class="reqn">\mathbf{\beta}_0</code> when the optimal tuning parameters <code>w.opt</code>, <code>lambda.opt</code>, <code>k.opt</code> and <code>vn.opt</code> are used).</p>
</td></tr>
<tr><td><code>beta.red</code></td>
<td>
<p>Estimate of <code class="reqn">\beta_0^{\mathbf{1}}</code> in the reduced model when the optimal tuning parameters <code>w.opt</code>, <code>lambda.opt</code>, <code>k.opt</code> and <code>vn.opt</code> are used.</p>
</td></tr>
<tr><td><code>theta.est</code></td>
<td>
<p>Coefficients of <code class="reqn">\hat{\theta}</code> in the B-spline basis (i. e. estimate of <code class="reqn">\theta_0</code> when the optimal tuning parameters <code>w.opt</code>, <code>lambda.opt</code>, <code>k.opt</code> and <code>vn.opt</code> are used): a vector of <code>length(order.Bspline+nknot.theta)</code>.</p>
</td></tr>
<tr><td><code>indexes.beta.nonnull</code></td>
<td>
<p>Indexes of the non-zero <code class="reqn">\hat{\beta_{j}}</code>.</p>
</td></tr>
<tr><td><code>k.opt</code></td>
<td>
<p>Selected number of nearest neighbours (when <code>w.opt</code> is considered).</p>
</td></tr>
<tr><td><code>w.opt</code></td>
<td>
<p>Selected size for  <code class="reqn">\mathcal{R}_n^{\mathbf{1}}</code>.</p>
</td></tr>
<tr><td><code>lambda.opt</code></td>
<td>
<p>Selected value of the penalisation parameter (when <code>w.opt</code> is considered).</p>
</td></tr>
<tr><td><code>IC</code></td>
<td>
<p>Value of the criterion function considered to select <code>w.opt</code>, <code>lambda.opt</code>, <code>k.opt</code> and <code>vn.opt</code>.</p>
</td></tr>
<tr><td><code>vn.opt</code></td>
<td>
<p>Selected value of <code>vn</code> (when <code>w.opt</code> is considered).</p>
</td></tr>
<tr><td><code>beta.w</code></td>
<td>
<p>Estimate of <code class="reqn">\beta_0^{\mathbf{1}}</code> for each value of the sequence <code>wn</code> (i.e. for each number of covariates in the reduced model).</p>
</td></tr>
<tr><td><code>theta.w</code></td>
<td>
<p>Estimate of <code class="reqn">\theta_0^{\mathbf{1}}</code> for each value of the sequence <code>wn</code> (i.e. its coefficients in the B-spline basis).</p>
</td></tr> 
<tr><td><code>IC.w</code></td>
<td>
<p>Value of the criterion function for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>indexes.beta.nonnull.w</code></td>
<td>
<p>Indexes of the non-zero linear coefficients for each value of the sequence <code>wn</code>.</p>
</td></tr> 
<tr><td><code>lambda.w</code></td>
<td>
<p>Selected value of penalisation parameter for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>k.w</code></td>
<td>
<p>Selected number of neighbours for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>index01</code></td>
<td>
<p>Indexes of the covariates (in the whole set of <code class="reqn">p_n</code>) used to build  <code class="reqn">\mathcal{R}_n^{\mathbf{1}}</code> for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>...</code></td>
<td>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>German Aneiros Perez <a href="mailto:german.aneiros@udc.es">german.aneiros@udc.es</a> 
</p>
<p>Silvia Novo Diaz  <a href="mailto:snovo@est-econ.uc3m.es">snovo@est-econ.uc3m.es</a>
</p>


<h3>References</h3>

<p>Novo, S., Vieu, P., and Aneiros, G., (2021) Fast and efficient algorithms for
sparse semiparametric bi-functional regression. <em>Australian and New Zealand
Journal of Statistics</em>, <b>63</b>, 606&ndash;638, <a href="https://doi.org/10.1111/anzs.12355">doi:10.1111/anzs.12355</a>.
</p>


<h3>See Also</h3>

<p>See also <code><a href="#topic+sfplsim.kNN.fit">sfplsim.kNN.fit</a>, <a href="#topic+predict.FASSMR.kNN">predict.FASSMR.kNN</a></code>, <code><a href="#topic+plot.FASSMR.kNN">plot.FASSMR.kNN</a></code> and <code><a href="#topic+IASSMR.kNN.fit">IASSMR.kNN.fit</a></code>.
</p>
<p>Alternative method <code><a href="#topic+FASSMR.kernel.fit">FASSMR.kernel.fit</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

data(Sugar)


y&lt;-Sugar$ash
x&lt;-Sugar$wave.290
z&lt;-Sugar$wave.240

#Outliers
index.y.25 &lt;- y &gt; 25
index.atip &lt;- index.y.25
(1:268)[index.atip]


#Dataset to model
x.sug &lt;- x[!index.atip,]
z.sug&lt;- z[!index.atip,]
y.sug &lt;- y[!index.atip]

train&lt;-1:216

ptm=proc.time()
fit&lt;- FASSMR.kNN.fit(x=x.sug[train,],z=z.sug[train,], y=y.sug[train], 
        nknot.theta=2,lambda.min.h=0.03, lambda.min.l=0.03, 
        max.knn=20,nknot=20,criterion="BIC", penalty="grSCAD",max.iter=5000)
proc.time()-ptm

fit
names(fit)

  
</code></pre>

<hr>
<h2 id='fsemipar.internal'>
Package fsemipar internal functions
</h2><span id='topic+fsemipar.internal'></span>

<h3>Description</h3>

<p>List of the internal functions. The construction of this code is based on that by F. Ferraty, which is available on his website <a href="https://www.math.univ-toulouse.fr/~ferraty/SOFTWARES/NPFDA/index.html">https://www.math.univ-toulouse.fr/~ferraty/SOFTWARES/NPFDA/index.html</a>. 
</p>


<h3>Details</h3>


<ul>
<li> <p><code>approx.spline.deriv</code>
</p>
</li>
<li><p><code>Bspline.ini</code>
</p>
</li>
<li><p><code>fnp.kernel.fit</code>
</p>
</li>
<li><p><code>fnp.kernel.fit.test</code>
</p>
</li>
<li><p><code>fnp.kernel.test</code>
</p>
</li>
<li><p><code>fnp.kNN.fit</code>
</p>
</li>
<li><p><code>fnp.kNN.fit.test</code>
</p>
</li>
<li><p><code>fnp.kNN.fit.test.loc</code>
</p>
</li>
<li><p><code>fnp.kNN.GCV</code>
</p>
</li>
<li><p><code>fnp.kNN.test</code>
</p>
</li>
<li><p><code>fsim.kernel.fit.fixedtheta</code>
</p>
</li>
<li><p><code>fsim.kNN.fit.fixedtheta</code>
</p>
</li>
<li><p><code>fun.kernel</code>
</p>
</li>
<li><p><code>fun.kernel.fixedtheta</code>
</p>
</li>
<li><p><code>fun.kNN</code>
</p>
</li>
<li><p><code>fun.kNN.fixedtheta</code>
</p>
</li>
<li><p><code>funopare.kNN</code>
</p>
</li>
<li><p><code>H.fnp.kernel</code>
</p>
</li>
<li><p><code>H.fnp.kNN</code>
</p>
</li>
<li><p><code>H.fsim.kernel</code>
</p>
</li>
<li><p><code>H.fsim.kNN</code>
</p>
</li>
<li><p><code>interp.spline.deriv</code>
</p>
</li>
<li><p><code>normaliza</code>
</p>
</li>
<li><p><code>quad</code>
</p>
</li>
<li><p><code>semimetric.deriv</code>
</p>
</li>
<li><p><code>semimetric.interv</code>
</p>
</li>
<li><p><code>semimetric.pca</code>
</p>
</li>
<li><p><code>sfplsim.kernel.fit.fixedtheta</code>
</p>
</li>
<li><p><code>sfplsim.kNN.fit.fixedtheta</code>
</p>
</li>
<li><p><code>Splinemlf</code>
</p>
</li>
<li><p><code>symsolve</code>
</p>
</li></ul>


<hr>
<h2 id='fsim.kernel.fit'>
Functional single-index model fit using kernel estimation
</h2><span id='topic+fsim.kernel.fit'></span>

<h3>Description</h3>

<p>This function fits a functional single-index model (FSIM) between a functional explanatory variable and
scalar response. 
The function uses kernel estimation with Nadaraya-Watson weights, a  B-spline representation to estimate the functional index <code class="reqn">\theta_0</code> and the cross-validation (CV) criterion to select the bandwidth (<code>h.opt</code>) and the coefficients of the functional index in the spline basis (<code>theta.est</code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fsim.kernel.fit(x, y, seed.coeff = c(-1, 0, 1), nknot.theta = 3, 
  order.Bspline = 3, t0 = NULL, min.q.h = 0.05, max.q.h = 0.5,
  h.seq = NULL, num.h = 10, kind.of.kernel = "quad", range.grid = NULL,
  nknot = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fsim.kernel.fit_+3A_x">x</code></td>
<td>
<p>Matrix containing the observations of the functional covariate (i.e. curves) collected by row.</p>
</td></tr>
<tr><td><code id="fsim.kernel.fit_+3A_y">y</code></td>
<td>
<p>Vector containing the scalar response.</p>
</td></tr>
<tr><td><code id="fsim.kernel.fit_+3A_seed.coeff">seed.coeff</code></td>
<td>

<p>Vector of initial values used to  build the set <code class="reqn">\Theta_n</code> (see section <code>Details</code>). The coefficients for the B-spline representation of each eligible functional index <code class="reqn">\theta \in \Theta_n</code> are obtained from <code>seed.coeff</code>.  The default is <code>c(-1,0,1)</code>.
</p>
</td></tr>
<tr><td><code id="fsim.kernel.fit_+3A_nknot.theta">nknot.theta</code></td>
<td>
<p>Positive integer indicating the number of uniform interior knots of the B-spline basis for the B-spline representation of <code class="reqn">\theta_0</code>. The default is 3.
</p>
</td></tr>
<tr><td><code id="fsim.kernel.fit_+3A_order.bspline">order.Bspline</code></td>
<td>

<p>Positive integer giving the order of the B-spline basis functions. This is the number of coefficients in each piecewise polynomial segment. The default is 3.
</p>
</td></tr>
<tr><td><code id="fsim.kernel.fit_+3A_t0">t0</code></td>
<td>

<p>Value in the domain of the functional indexes at which we evaluate them to build the set <code class="reqn">\Theta_n</code>. We assume <code class="reqn">\theta_0(t_0)&gt;0</code> for some arbitrary <code class="reqn">t_0</code> in the domain to ensure model identifiability. If <code>t0=NULL</code>, then <code>mean(range.grid)</code> is considered.
</p>
</td></tr>
<tr><td><code id="fsim.kernel.fit_+3A_min.q.h">min.q.h</code></td>
<td>
<p>Order of the quantile of the set of distances between curves (computed with  the projection semi-metric) which gives the lower end of the sequence in which the bandwidth is selected. The default is 0.05.
</p>
</td></tr>
<tr><td><code id="fsim.kernel.fit_+3A_max.q.h">max.q.h</code></td>
<td>

<p>Order of the quantile of the set of distances between curves (computed with the projection semi-metric) which gives the upper end of the sequence in which the bandwidth is selected. The default is 0.5.
</p>
</td></tr>
<tr><td><code id="fsim.kernel.fit_+3A_h.seq">h.seq</code></td>
<td>
<p>Vector containing the sequence of bandwidths. The default is a sequence of <code>num.h</code> equispaced bandwidths in the range constructed using <code>min.q.h</code> and <code>max.q.h</code>.
</p>
</td></tr>
<tr><td><code id="fsim.kernel.fit_+3A_num.h">num.h</code></td>
<td>
<p>Positive integer indicating the number of bandwiths in the grid. The default is 10.
</p>
</td></tr>
<tr><td><code id="fsim.kernel.fit_+3A_kind.of.kernel">kind.of.kernel</code></td>
<td>

<p>The type of kernel function used. Only Epanechnikov kernel (<code>"quad"</code>) is available.
</p>
</td></tr>
<tr><td><code id="fsim.kernel.fit_+3A_range.grid">range.grid</code></td>
<td>

<p>Vector of length 2 containing the endpoints of the grid at which the observations of the functional covariate <code>x</code> are evaluated (i.e. the range of the discretization). If <code>range.grid=NULL</code>, then <code>range.grid=c(1,p)</code> is considered, where <code>p</code> is the size of the discretization size of <code>x</code> (i.e. <code>ncol(x))</code>.
</p>
</td></tr>
<tr><td><code id="fsim.kernel.fit_+3A_nknot">nknot</code></td>
<td>

<p>Positive integer indicating the number of interior knots for the B-spline representation of the functional covariate. The default value is <code>(p - order.Bspline - 1)%/%2</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The functional single-index model (FSIM) is given by the expression:
</p>
<p style="text-align: center;"><code class="reqn">Y_i=r(\langle\theta_0,X_i\rangle)+\varepsilon_i, \quad i=1,\dots,n,</code>
</p>

<p>where <code class="reqn">Y_i</code> denotes a scalar response, 
<code class="reqn">X_i</code> is a functional covariate valued in a separable Hilbert space <code class="reqn">\mathcal{H}</code> with inner product <code class="reqn">\langle \cdot, \cdot\rangle</code>, <code class="reqn">\varepsilon</code> denotes the random error, <code class="reqn">\theta_0 \in \mathcal{H}</code> is the unknown functional index and <code class="reqn">r(\cdot)</code> denotes the unknown smooth link function.
</p>
<p>The FSIM is fitted using the kNN estimator
</p>
<p style="text-align: center;"><code class="reqn">
\widehat{r}_{h,\hat{\theta}}(x)=\sum_{i=1}^nw_{n,h,\hat{\theta}}(x,X_i)Y_i,  \quad   \forall x\in\mathcal{H},
</code>
</p>

<p>with Nadaraya-Watson weights
</p>
<p style="text-align: center;"><code class="reqn">
w_{n,h,\hat{\theta}}(x,X_i)=\frac{K\left(h^{-1}d_{\hat{\theta}}\left(X_i,x\right)\right)}{\sum_{i=1}^nK\left(h^{-1}d_{\hat{\theta}}\left(X_i,x\right)\right)},
</code>
</p>

<p>where
</p>

<ul>
<li><p> the real positive number <code class="reqn">h</code> is the bandwidth.
</p>
</li>
<li> <p><code class="reqn">K</code> is a kernel function (see the argument <code>kind.of.kernel</code>).
</p>
</li>
<li> <p><code class="reqn">d_{\hat{\theta}}(x_1,x_2)=|\langle\hat{\theta},x_1-x_2\rangle|</code> is the projection semi-metric, computed using <code><a href="#topic+semimetric.projec">semimetric.projec</a></code> and <code class="reqn">\hat{\theta}</code> is an estimate of <code class="reqn">\theta_0</code>. 
</p>
</li></ul>

<p>The procedure requires the estimation of the function-parameter <code class="reqn">\theta_0</code>. Therefore, we use B-spline representation to build a set  <code class="reqn">\Theta_n</code> of eligible functional indexes. The dimension of the B-spline basis is <code>order.Bspline</code>+<code>nknot.theta</code> and the set of eligible coefficients is obtained by calibrating (to ensure the identifiability of the model) the set of initial coefficients given in <code>seed.coeff</code>. The larger this set, the higher the size of <code class="reqn">\Theta_n</code>. Since our approach requires intensive computation, we need a trade-off between the size of <code class="reqn">\Theta_n</code> and the performance of the estimator. For that, Ait-Saidi et al. (2008) suggested considering <code>order.Bspline=3</code> and <code>seed.coeff=c(-1,0,1)</code>. For details on the construction of <code class="reqn">\Theta_n</code> see Novo et al. (2019).
</p>
<p>We obtain the estimated coefficients of <code class="reqn">\theta_0</code> in the spline basis (<code>theta.est</code>) and the selected bandwidth (<code>h.opt</code>) by minimising the CV criterion.
</p>


<h3>Value</h3>

<table>
<tr><td><code>call</code></td>
<td>
<p>The matched call.</p>
</td></tr>
<tr><td><code>fitted.values</code></td>
<td>
<p>Estimated scalar response.</p>
</td></tr>
<tr><td><code>residuals</code></td>
<td>
<p>Differences between <code>y</code> and the <code>fitted.values</code></p>
</td></tr>
<tr><td><code>theta.est</code></td>
<td>
<p>Coefficients of <code class="reqn">\hat{\theta}</code> in the B-spline basis: a vector of <code>length(order.Bspline+nknot.theta)</code>.</p>
</td></tr>
<tr><td><code>h.opt</code></td>
<td>
<p>Selected bandwidth.</p>
</td></tr>
<tr><td><code>r.squared</code></td>
<td>
<p>Coefficient of determination.</p>
</td></tr> 
<tr><td><code>var.res</code></td>
<td>
<p>Redidual variance.</p>
</td></tr>
<tr><td><code>df</code></td>
<td>
<p>Residual degrees of freedom.</p>
</td></tr>
<tr><td><code>yhat.cv</code></td>
<td>
<p>Predicted values for the scalar response using leave-one-out samples.</p>
</td></tr>
<tr><td><code>CV.opt</code></td>
<td>
<p>Minimum value of the CV function, i.e. the value of CV for <code>theta.est</code> and <code>h.opt</code>.</p>
</td></tr>
<tr><td><code>CV.values</code></td>
<td>
<p>Vector containing CV values for each functional index in <code class="reqn">\Theta_n</code> and the value of <code class="reqn">h</code> that minimises the CV for such index (i.e. <code>CV.values[j]</code> contains the value of the CV function corresponding to <code>theta.seq.norm[j,]</code> and the best value of the <code>h.seq</code> for this functional index according to the CV criterion).</p>
</td></tr>
<tr><td><code>H</code></td>
<td>
<p>Hat matrix.</p>
</td></tr>
<tr><td><code>m.opt</code></td>
<td>
<p>Index of <code class="reqn">\hat{\theta}</code> in the set <code class="reqn">\Theta_n</code>.</p>
</td></tr>
<tr><td><code>theta.seq.norm</code></td>
<td>
<p>The vector <code>theta.seq.norm[j,]</code> contains the coefficientes in the B-spline basis of the jth functional index in <code class="reqn">\Theta_n</code>.</p>
</td></tr>
<tr><td><code>h.seq</code></td>
<td>
<p>Sequence of eligible values for <code class="reqn">h</code>.</p>
</td></tr>
<tr><td><code>...</code></td>
<td>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>German Aneiros Perez <a href="mailto:german.aneiros@udc.es">german.aneiros@udc.es</a> 
</p>
<p>Silvia Novo Diaz  <a href="mailto:snovo@est-econ.uc3m.es">snovo@est-econ.uc3m.es</a>
</p>


<h3>References</h3>

<p>Ait-Saidi, A., Ferraty, F., Kassa, R., and Vieu, P. (2008) Cross-validated estimations in the single-functional index model. <em>Statistics</em>, <b>42(6)</b>, 475&ndash;494, <a href="https://doi.org/10.1080/02331880801980377">doi:10.1080/02331880801980377</a>.
</p>
<p>Novo S., Aneiros, G., and Vieu, P., (2019) Automatic and location-adaptive estimation in functional single&ndash;index regression. <em>Journal of Nonparametric Statistics</em>, <b>31(2)</b>, 364&ndash;392, <a href="https://doi.org/10.1080/10485252.2019.1567726">doi:10.1080/10485252.2019.1567726</a>.
</p>


<h3>See Also</h3>

<p>See also <code><a href="#topic+fsim.kernel.test">fsim.kernel.test</a></code>, <code><a href="#topic+predict.fsim.kernel">predict.fsim.kernel</a></code>, <code><a href="#topic+plot.fsim.kernel">plot.fsim.kernel</a></code>.
</p>
<p>Alternative procedure <code><a href="#topic+fsim.kNN.fit">fsim.kNN.fit</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(Tecator)
y&lt;-Tecator$fat
X&lt;-Tecator$absor.spectra2


#FSIM fit. With nknot.theta=2 and range.grid=c(850,1050), 
#Theta_n contains 108 thetas.
ptm&lt;-proc.time()
fit&lt;-fsim.kernel.fit(y[1:160],x=X[1:160,],max.q.h=0.35, nknot=20,
range.grid=c(850,1050),nknot.theta=2)
proc.time()-ptm
fit
names(fit)

</code></pre>

<hr>
<h2 id='fsim.kernel.test'>
Functional single-index kernel predictor
</h2><span id='topic+fsim.kernel.test'></span>

<h3>Description</h3>

<p>Provides predictions when we compute a functional single-index model (FSIM) using the nonparametric kernel procedure between a scalar response and a functional covariate given a functional index (<code class="reqn">\theta</code>), a global bandwidth (<code>h</code>) and new observations of the functional covariate (<code>x.test</code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fsim.kernel.test(x, y, x.test, y.test, theta = theta, nknot.theta = 3,
  order.Bspline = 3, h = 0.5, kind.of.kernel = "quad", range.grid = NULL,
  nknot = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fsim.kernel.test_+3A_x">x</code></td>
<td>

<p>Matrix containing the observations of the functional covariate  that correspond to the training sample collected by row.
</p>
</td></tr>
<tr><td><code id="fsim.kernel.test_+3A_y">y</code></td>
<td>

<p>Vector containing the scalar responses in the training sample.
</p>
</td></tr>
<tr><td><code id="fsim.kernel.test_+3A_x.test">x.test</code></td>
<td>

<p>Matrix containing the observations of the functional covariate  that correspond to the testing sample collected by row.
</p>
</td></tr>
<tr><td><code id="fsim.kernel.test_+3A_y.test">y.test</code></td>
<td>

<p>(optional) Vector/matrix containing the scalar responses in the testing sample.
</p>
</td></tr>
<tr><td><code id="fsim.kernel.test_+3A_theta">theta</code></td>
<td>

<p>Vector containing the coefficients of <code class="reqn">\theta</code> in a B-spline basis, so that <code>length(theta)=order.Bspline+nknot.theta</code>
</p>
</td></tr>
<tr><td><code id="fsim.kernel.test_+3A_nknot.theta">nknot.theta</code></td>
<td>

<p>Positive integer indicating the number of uniform interior knots of the B-spline basis for B-spline representation of <code class="reqn">\theta</code>. The default is 3.
</p>
</td></tr>
<tr><td><code id="fsim.kernel.test_+3A_order.bspline">order.Bspline</code></td>
<td>

<p>Positive integer giving the order of the B-spline basis functions for the B-spline representation of <code class="reqn">\theta</code>. This is the number of coefficients in each piecewise polynomial segment. The default is 3.
</p>
</td></tr>
<tr><td><code id="fsim.kernel.test_+3A_h">h</code></td>
<td>

<p>Positive real number indicating the global bandwidth.
</p>
</td></tr>
<tr><td><code id="fsim.kernel.test_+3A_kind.of.kernel">kind.of.kernel</code></td>
<td>

<p>The type of kernel function used. Only Epanechnikov kernel (<code>"quad"</code>) is available.
</p>
</td></tr>
<tr><td><code id="fsim.kernel.test_+3A_range.grid">range.grid</code></td>
<td>

<p>Vector of length 2 containing the endpoints of the grid at which the observations of the functional covariate <code>x</code> are evaluated (i.e. the range of the discretization). If <code>range.grid=NULL</code>, then <code>range.grid=c(1,p)</code> is considered, where <code>p</code> is the size of the discretization size of <code>x</code> (i.e. <code>ncol(x)</code>).
</p>
</td></tr>
<tr><td><code id="fsim.kernel.test_+3A_nknot">nknot</code></td>
<td>

<p>Positive integer indicating the number of interior knots for the B-spline representation of the functional covariate. The default value is <code>(p - order.Bspline - 1)%/%2</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The functional single-index model (FSIM) is given by the expression:
</p>
<p style="text-align: center;"><code class="reqn">Y_i=r(\langle\theta_0,X_i\rangle)+\varepsilon_i, \quad i=1,\dots,n,</code>
</p>

<p>where <code class="reqn">Y_i</code> denotes a scalar response, 
<code class="reqn">X_i</code> is a functional covariate valued in a separable Hilbert space <code class="reqn">\mathcal{H}</code> with inner product <code class="reqn">\langle \cdot, \cdot\rangle</code>, <code class="reqn">\varepsilon</code> denotes the random error, <code class="reqn">\theta_0 \in \mathcal{H}</code> is the unknown functional index, <code class="reqn">r(\cdot)</code> denotes the unknown smooth link function and <code class="reqn">n</code> is the training sample size.
</p>
<p>Given <code class="reqn">\theta \in \mathcal{H}</code>, <code class="reqn">h&gt;0</code> and a testing sample {<code class="reqn">X_j,\ j=1,\dots,n_{test}</code>}, the predicted responses (see the value <code>y.estimated.test</code>) can be computed using the kernel procedure by means of 
</p>
<p style="text-align: center;"><code class="reqn">
\widehat{r}_{h,\theta}(X_j)=\sum_{i=1}^nw_{n,h,\theta}(X_j,X_i)Y_i,\quad  j=1,\dots,n_{test}, 
</code>
</p>

<p>with Nadaraya-Watson weights
</p>
<p style="text-align: center;"><code class="reqn">
w_{n,h,\theta}(X_j,X_i)=\frac{K\left(h^{-1}d_{\theta}\left(X_i,X_j\right)\right)}{\sum_{i=1}^nK\left(h^{-1}d_{\theta}\left(X_i,X_j\right)\right)},
</code>
</p>

<p>where
</p>

<ul>
<li> <p><code class="reqn">K</code> is a kernel function (see the argument <code>kind.of.kernel</code>).
</p>
</li>
<li><p> for <code class="reqn">x_1,x_2 \in \mathcal{H}, </code>  <code class="reqn">d_{\theta}(x_1,x_2)=|\langle\theta,x_1-x_2\rangle|</code> is the projection semi-metric, computed using <code><a href="#topic+semimetric.projec">semimetric.projec</a></code>. 
</p>
</li></ul>

<p>If the argument <code>y.test</code> is given to the program (i. e. <code>if(!is.null(y.test))</code>), the function provides the mean squared error of prediction (see the value <code>MSE.test</code>) calculated as <code>mean((y.test-y.estimated.test)^2)</code>.
</p>


<h3>Value</h3>

<table>
<tr><td><code>y.estimated.test</code></td>
<td>
<p>Predicted responses.</p>
</td></tr>
<tr><td><code>MSE.test</code></td>
<td>
<p>Mean squared error between predicted and observed responses in the testing sample.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>German Aneiros Perez <a href="mailto:german.aneiros@udc.es">german.aneiros@udc.es</a> 
</p>
<p>Silvia Novo Diaz  <a href="mailto:snovo@est-econ.uc3m.es">snovo@est-econ.uc3m.es</a>
</p>


<h3>References</h3>

<p>Novo S., Aneiros, G., and Vieu, P., (2019) Automatic and location-adaptive estimation in functional single&ndash;index regression. <em>Journal of Nonparametric Statistics</em>, <b>31(2)</b>, 364&ndash;392, <a href="https://doi.org/10.1080/10485252.2019.1567726">doi:10.1080/10485252.2019.1567726</a>.
</p>


<h3>See Also</h3>

<p>See also <code><a href="#topic+fsim.kernel.fit">fsim.kernel.fit</a></code> and <code><a href="#topic+predict.fsim.kernel">predict.fsim.kernel</a></code>.
</p>
<p>Alternative procedure <code><a href="#topic+fsim.kNN.test">fsim.kNN.test</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(Tecator)
y&lt;-Tecator$fat
X&lt;-Tecator$absor.spectra2

train&lt;-1:160
test&lt;-161:215

#FSIM fit. With nknot.theta=2 and range.grid=c(850,1050), 
#Theta_n contains 108 thetas.
ptm&lt;-proc.time()
fit&lt;-fsim.kernel.fit(y=y[train],x=X[train,],max.q.h=0.35, nknot=20,
        range.grid=c(850,1050),nknot.theta=2)
proc.time()-ptm
fit

#FSIM prediction
test&lt;-fsim.kernel.test(y=y[train],x=X[train,],x.test=X[test,],y.test=y[test],
        theta=fit$theta.est,h=fit$h.opt,nknot.theta=2,nknot=20,
        range.grid=c(850,1050))

#MSEP
test$MSE.test
  
</code></pre>

<hr>
<h2 id='fsim.kNN.fit'>Functional single-index model fit using kNN estimation</h2><span id='topic+fsim.kNN.fit'></span>

<h3>Description</h3>

<p>This function fits a functional single-index model (FSIM) between a functional explanatory variable and
scalar response. 
The function uses <code class="reqn">k</code>-nearest neighbours (kNN) estimation with Nadaraya-Watson weights, a  B-spline representation to estimate the functional index <code class="reqn">\theta_0</code> and the cross-validation (CV) criterion to select the number of neighbours (<code>k.opt</code>) and the coefficients of the functional index in the spline basis (<code>theta.est</code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fsim.kNN.fit(x, y, seed.coeff = c(-1, 0, 1), order.Bspline = 3, 
  nknot.theta = 3, t0 = NULL, min.knn = 2, max.knn = NULL, knearest = NULL,
  step = NULL, kind.of.kernel = "quad", range.grid = NULL, nknot = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fsim.kNN.fit_+3A_x">x</code></td>
<td>
<p>Matrix containing the observations of the functional covariate collected by row.</p>
</td></tr>
<tr><td><code id="fsim.kNN.fit_+3A_y">y</code></td>
<td>
<p>Vector containing the scalar response.</p>
</td></tr>
<tr><td><code id="fsim.kNN.fit_+3A_seed.coeff">seed.coeff</code></td>
<td>
<p>Vector of initial values used to  build the set <code class="reqn">\Theta_n</code> (see section <b>Details</b>). The coefficients for the B-spline representation of each eligible functional index <code class="reqn">\theta \in \Theta_n</code> are obtained from <code>seed.coeff</code>.  The default is <code>c(-1,0,1)</code>.</p>
</td></tr>
<tr><td><code id="fsim.kNN.fit_+3A_order.bspline">order.Bspline</code></td>
<td>
<p>Positive integer giving the order of the B-spline basis functions. This is the number of coefficients in each piecewise polynomial segment. The default is 3.</p>
</td></tr>
<tr><td><code id="fsim.kNN.fit_+3A_nknot.theta">nknot.theta</code></td>
<td>
<p>Positive integer indicating the number of uniform interior knots of the B-spline basis for the B-spline representation of <code class="reqn">\theta_0</code>. The default is 3.</p>
</td></tr>
<tr><td><code id="fsim.kNN.fit_+3A_t0">t0</code></td>
<td>
<p>Value in the domain of the functional indexes at which we evaluate them to  build the set <code class="reqn">\Theta_n</code>. We assume <code class="reqn">\theta_0(t_0)&gt;0</code> for some arbitrary <code class="reqn">t_0</code> in the domain to ensure model identifiability. If <code>t0=NULL</code>, then <code>mean(range.grid)</code> is considered.</p>
</td></tr>
<tr><td><code id="fsim.kNN.fit_+3A_min.knn">min.knn</code></td>
<td>
<p>Positive integer indicating the smallest value of the sequence in which the  number of nearest neighbours <code>k.opt</code> is selected (thus, this number must be smaller than the sample size). The default is 2.</p>
</td></tr>
<tr><td><code id="fsim.kNN.fit_+3A_max.knn">max.knn</code></td>
<td>
<p>Positive integer indicating the largest value of the sequence in which the number of nearest neighbours <code>k.opt</code> is selected (thus, this number must be larger than <code>min.kNN</code> and smaller than the sample size, <code>n</code>). The default is <code>max.knn &lt;- n%/%2</code>.
</p>
</td></tr>
<tr><td><code id="fsim.kNN.fit_+3A_knearest">knearest</code></td>
<td>
<p>Vector of positive integers containing the sequence in which the  number of nearest neighbours <code>k.opt</code> is selected. If <code>knearest=NULL</code>, then <code>knearest &lt;- seq(from =min.knn, to = max.knn, by = step)</code>.</p>
</td></tr>
<tr><td><code id="fsim.kNN.fit_+3A_step">step</code></td>
<td>
<p>Positive integer used to build the sequence of k-nearest neighbours in the following way: <code>min.knn, min.knn + step, min.knn + 2*step, min.knn + 3*step,...</code> The default is <code>step&lt;-ceiling(n/100)</code>.</p>
</td></tr>
<tr><td><code id="fsim.kNN.fit_+3A_kind.of.kernel">kind.of.kernel</code></td>
<td>
<p>The type of kernel function used. Only Epanechnikov kernel (<code>"quad"</code>) is available.
</p>
</td></tr>
<tr><td><code id="fsim.kNN.fit_+3A_range.grid">range.grid</code></td>
<td>
<p>Vector of length 2 containing the endpoints of the grid at which the observations of the functional covariate <code>x</code> are evaluated (i.e. the range of the discretization). If <code>range.grid=NULL</code>, then <code>range.grid=c(1,p)</code> is considered, where <code>p</code> is the size of the discretization size of <code>x</code> (i.e. <code>ncol(x))</code>.</p>
</td></tr>
<tr><td><code id="fsim.kNN.fit_+3A_nknot">nknot</code></td>
<td>
<p>Positive integer indicating the number of interior knots for the B-spline representation of the functional covariate. The default value is <code>(p - order.Bspline - 1)%/%2</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The functional single-index model (FSIM) is given by the expression:
</p>
<p style="text-align: center;"><code class="reqn">Y_i=r(\langle\theta_0,X_i\rangle)+\varepsilon_i, \quad i=1,\dots,n,</code>
</p>

<p>where <code class="reqn">Y_i</code> denotes a scalar response, 
<code class="reqn">X_i</code> is a functional covariate valued in a separable Hilbert space <code class="reqn">\mathcal{H}</code> with inner product <code class="reqn">\langle \cdot, \cdot\rangle</code>, <code class="reqn">\varepsilon</code> denotes the random error, <code class="reqn">\theta_0 \in \mathcal{H}</code> is the unknown functional index and <code class="reqn">r(\cdot)</code> denotes the unknown smooth link function.
</p>
<p>The FSIM is fitted using the kNN estimator
</p>
<p style="text-align: center;"><code class="reqn">
\widehat{r}_{k,\hat{\theta}}(x)=\sum_{i=1}^nw_{n,k,\hat{\theta}}(x,X_i)Y_i,  \quad   \forall x\in\mathcal{H},
</code>
</p>

<p>with Nadaraya-Watson weights
</p>
<p style="text-align: center;"><code class="reqn">
w_{n,k,\hat{\theta}}(x,X_i)=\frac{K\left(H_{k,x,\hat{\theta}}^{-1}d_{\hat{\theta}}\left(X_i,x\right)\right)}{\sum_{i=1}^nK\left(H_{k,x,\hat{\theta}}^{-1}d_{\hat{\theta}}\left(X_i,x\right)\right)},
</code>
</p>

<p>where
</p>

<ul>
<li><p> the positive integer <code class="reqn">k</code> is a smoothing factor, representing the number of nearest neighbours.
</p>
</li>
<li> <p><code class="reqn">K</code> is a kernel function (see the argument <code>kind.of.kernel</code>).
</p>
</li>
<li> <p><code class="reqn">d_{\hat{\theta}}(x_1,x_2)=|\langle\hat{\theta},x_1-x_2\rangle|</code> is the projection semi-metric, computed using <code><a href="#topic+semimetric.projec">semimetric.projec</a></code> and <code class="reqn">\hat{\theta}</code> is an estimate of <code class="reqn">\theta_0</code>. 
</p>
</li>
<li> <p><code class="reqn">H_{k,x,\hat{\theta}}=\min\{h\in R^+ \text{ such that } \sum_{i=1}^n1_{B_{\hat{\theta}}(x,h)}(X_i)=k\}</code>, where <code class="reqn">1_{B_{\hat{\theta}}(x,h)}(\cdot)</code> is the indicator function of the open ball created with the projection semi-metric with centre <code class="reqn">x\in\mathcal{H}</code> and radius <code class="reqn">h</code>.
</p>
</li></ul>

<p>The procedure requires the estimation of the function-parameter <code class="reqn">\theta_0</code>. Therefore, we use B-spline representation to build a set  <code class="reqn">\Theta_n</code> of eligible functional indexes. The dimension of the B-spline basis is <code>order.Bspline</code>+<code>nknot.theta</code> and the set of eligible coefficients is obtained by calibrating (to ensure the identifiability of the model) the set of initial coefficients given in <code>seed.coeff</code>. The larger this set, the higher the size of <code class="reqn">\Theta_n</code>. Since our approach requires intensive computation, we need a trade-off between the size of <code class="reqn">\Theta_n</code> and the performance of the estimator. For that, Ait-Saidi et al. (2008) suggested considering <code>order.Bspline=3</code> and <code>seed.coeff=c(-1,0,1)</code>. For details on the construction of <code class="reqn">\Theta_n</code> see Novo et al. (2019).
</p>
<p>We obtain the estimated coefficients of <code class="reqn">\theta_0</code> in the spline basis (<code>theta.est</code>) and the selected number of neighbours (<code>k.opt</code>) by minimising the CV criterion.
</p>


<h3>Value</h3>

<table>
<tr><td><code>call</code></td>
<td>
<p>The matched call.</p>
</td></tr>
<tr><td><code>fitted.values</code></td>
<td>
<p>Estimated scalar response.</p>
</td></tr>
<tr><td><code>residuals</code></td>
<td>
<p>Differences between <code>y</code> and the <code>fitted.values</code></p>
</td></tr>
<tr><td><code>theta.est</code></td>
<td>
<p>Coefficients of <code class="reqn">\hat{\theta}</code> in the B-spline basis: a vector of <code>length(order.Bspline+nknot.theta)</code>.</p>
</td></tr>
<tr><td><code>k.opt</code></td>
<td>
<p>Selected number of nearest neighbours.</p>
</td></tr>
<tr><td><code>r.squared</code></td>
<td>
<p>Coefficient of determination.</p>
</td></tr> 
<tr><td><code>var.res</code></td>
<td>
<p>Redidual variance.</p>
</td></tr>
<tr><td><code>df</code></td>
<td>
<p>Residual degrees of freedom.</p>
</td></tr>
<tr><td><code>yhat.cv</code></td>
<td>
<p>Predicted values for the scalar response using leave-one-out samples.</p>
</td></tr>
<tr><td><code>CV.opt</code></td>
<td>
<p>Minimum value of the CV function, i.e. the value of CV for <code>theta.est</code> and <code>k.opt</code>.</p>
</td></tr>
<tr><td><code>CV.values</code></td>
<td>
<p>Vector containing CV values for each functional index in <code class="reqn">\Theta_n</code> and the value of <code class="reqn">k</code> that minimises the CV for such index (i.e. <code>CV.values[j]</code> contains the value of the CV function corresponding to <code>theta.seq.norm[j,]</code> and the best value of the <code>k.seq</code> for this functional index according to the CV criterion).</p>
</td></tr>
<tr><td><code>H</code></td>
<td>
<p>Hat matrix.</p>
</td></tr>
<tr><td><code>m.opt</code></td>
<td>
<p>Index of <code class="reqn">\hat{\theta}</code> in the set <code class="reqn">\Theta_n</code>.</p>
</td></tr>
<tr><td><code>theta.seq.norm</code></td>
<td>
<p>The vector <code>theta.seq.norm[j,]</code> contains the coefficientes in the B-spline basis of the jth functional index in <code class="reqn">\Theta_n</code>.</p>
</td></tr>
<tr><td><code>k.seq</code></td>
<td>
<p>Sequence of eligible values for <code class="reqn">k</code>.</p>
</td></tr>
<tr><td><code>...</code></td>
<td>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>German Aneiros Perez <a href="mailto:german.aneiros@udc.es">german.aneiros@udc.es</a> 
</p>
<p>Silvia Novo Diaz  <a href="mailto:snovo@est-econ.uc3m.es">snovo@est-econ.uc3m.es</a>
</p>


<h3>References</h3>

<p>Ait-Saidi, A., Ferraty, F., Kassa, R., and Vieu, P. (2008) Cross-validated estimations in the single-functional index model, <em>Statistics</em>, <b>42(6)</b>, 475&ndash;494, <a href="https://doi.org/10.1080/02331880801980377">doi:10.1080/02331880801980377</a>.
</p>
<p>Novo S., Aneiros, G., and Vieu, P., (2019) Automatic and location-adaptive estimation in functional single&ndash;index regression, <em>Journal of Nonparametric Statistics</em>, <b>31(2)</b>, 364&ndash;392, <a href="https://doi.org/10.1080/10485252.2019.1567726">doi:10.1080/10485252.2019.1567726</a>.
</p>


<h3>See Also</h3>

<p>See also <code><a href="#topic+fsim.kNN.test">fsim.kNN.test</a></code>, <code><a href="#topic+predict.fsim.kNN">predict.fsim.kNN</a></code>, <code><a href="#topic+plot.fsim.kNN">plot.fsim.kNN</a></code>.
</p>
<p>Alternative procedure <code><a href="#topic+fsim.kernel.fit">fsim.kernel.fit</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

data(Tecator)
y&lt;-Tecator$fat
X&lt;-Tecator$absor.spectra2

#FSIM fit. With nknot.theta=2 and range.grid=c(850,1050), 
#Theta_n contains 108 thetas.
ptm&lt;-proc.time()
fit&lt;-fsim.kNN.fit(y=y[1:160],x=X[1:160,],max.knn=20,nknot.theta=2,nknot=20,
range.grid=c(850,1050))
proc.time()-ptm
fit
names(fit)


</code></pre>

<hr>
<h2 id='fsim.kNN.test'>
Functional single-index kNN predictor
</h2><span id='topic+fsim.kNN.test'></span>

<h3>Description</h3>

<p>Provides predictions when we compute a functional single-index model (FSIM) using the <code class="reqn">k</code>NN procedure between a scalar response and a functional covariate given a functional index (<code class="reqn">\theta</code>), a global number of neighbours (<code>k</code>) and new observations of the functional covariate (<code>x.test</code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fsim.kNN.test(x, y, x.test, y.test = NULL, theta, order.Bspline = 3,
  nknot.theta = 3, k = 4, kind.of.kernel = "quad", range.grid = NULL, 
  nknot = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fsim.kNN.test_+3A_x">x</code></td>
<td>
<p>Matrix containing the observations of the functional covariate  that correspond to the training sample collected by row.
</p>
</td></tr>
<tr><td><code id="fsim.kNN.test_+3A_y">y</code></td>
<td>
<p>Vector containing the scalar responses in the training sample.
</p>
</td></tr>
<tr><td><code id="fsim.kNN.test_+3A_x.test">x.test</code></td>
<td>

<p>Matrix containing the observations of the functional covariate  that correspond to the testing sample collected by row.
</p>
</td></tr>
<tr><td><code id="fsim.kNN.test_+3A_y.test">y.test</code></td>
<td>
<p>(optional) Vector/matrix containing the scalar responses in the testing sample.
</p>
</td></tr>
<tr><td><code id="fsim.kNN.test_+3A_theta">theta</code></td>
<td>

<p>Vector containing the coefficients of <code class="reqn">\theta</code> in a B-spline basis, so that <code>length(theta)=order.Bspline+nknot.theta</code>.
</p>
</td></tr>
<tr><td><code id="fsim.kNN.test_+3A_order.bspline">order.Bspline</code></td>
<td>

<p>Positive integer giving the order of the B-spline basis functions for the B-spline representation of <code class="reqn">\theta</code>. This is the number of coefficients in each piecewise polynomial segment. The default is 3.
</p>
</td></tr>
<tr><td><code id="fsim.kNN.test_+3A_nknot.theta">nknot.theta</code></td>
<td>

<p>Positive integer indicating the number of uniform interior knots of the B-spline basis for B-spline representation of <code class="reqn">\theta</code>. The default is 3.
</p>
</td></tr>
<tr><td><code id="fsim.kNN.test_+3A_k">k</code></td>
<td>
<p>Positive integer indicating the global number of neighbours.</p>
</td></tr>
<tr><td><code id="fsim.kNN.test_+3A_kind.of.kernel">kind.of.kernel</code></td>
<td>

<p>The type of kernel function used. Only Epanechnikov kernel (<code>"quad"</code>) is available.
</p>
</td></tr>
<tr><td><code id="fsim.kNN.test_+3A_range.grid">range.grid</code></td>
<td>

<p>Vector of length 2 containing the endpoints of the grid at which the observations of the functional covariate <code>x</code> are evaluated (i.e. the range of the discretization). If <code>range.grid=NULL</code>, then <code>range.grid=c(1,p)</code> is considered, where <code>p</code> is the size of the discretization size of <code>x</code> (i.e. <code>ncol(x)</code>).
</p>
</td></tr>
<tr><td><code id="fsim.kNN.test_+3A_nknot">nknot</code></td>
<td>

<p>Positive integer indicating the number of interior knots for the B-spline representation of the functional covariate. The default value is <code>(p - order.Bspline - 1)%/%2</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The functional single-index model (FSIM) is given by the expression:
</p>
<p style="text-align: center;"><code class="reqn">Y_i=r(\langle\theta_0,X_i\rangle)+\varepsilon_i, \quad i=1,\dots,n,</code>
</p>

<p>where <code class="reqn">Y_i</code> denotes a scalar response, 
<code class="reqn">X_i</code> is a functional covariate valued in a separable Hilbert space <code class="reqn">\mathcal{H}</code> with inner product <code class="reqn">\langle \cdot, \cdot\rangle</code>, <code class="reqn">\varepsilon</code> denotes the random error, <code class="reqn">\theta_0 \in \mathcal{H}</code> is the unknown functional index, <code class="reqn">r(\cdot)</code> denotes the unknown smooth link function and <code class="reqn">n</code> is the training sample size.
</p>
<p>Given <code class="reqn">\theta \in \mathcal{H}</code>, <code class="reqn">1&lt;k&lt;n</code> and a testing sample {<code class="reqn">X_j,\ j=1,\dots,n_{test}</code>}, the predicted responses (see the value <code>y.estimated.test</code>) can be computed using the kNN procedure by means of 
</p>
<p style="text-align: center;"><code class="reqn">
\widehat{r}_{k,\theta}(X_j)=\sum_{i=1}^nw_{n,k,\theta}(X_j,X_i)Y_i,\quad  j=1,\dots,n_{test}, 
</code>
</p>

<p>with Nadaraya-Watson weights
</p>
<p style="text-align: center;"><code class="reqn">
w_{n,k,\theta}(X_j,X_i)=\frac{K\left(H_{k,X_j,{\theta}}^{-1}d_{\theta}\left(X_i,X_j\right)\right)}{\sum_{i=1}^nK\left(H_{k,X_j,\theta}^{-1}d_{\theta}\left(X_i,X_j\right)\right)},
</code>
</p>

<p>where
</p>

<ul>
<li> <p><code class="reqn">K</code> is a kernel function (see the argument <code>kind.of.kernel</code>).
</p>
</li>
<li><p> for <code class="reqn">x_1,x_2 \in \mathcal{H}, </code>  <code class="reqn">d_{\theta}(x_1,x_2)=|\langle\theta,x_1-x_2\rangle|</code> is the projection semi-metric, computed using <code><a href="#topic+semimetric.projec">semimetric.projec</a></code>. </p>
</li>
<li> <p><code class="reqn">H_{k,x,\theta}=\min\left\{h\in R^+ \text{ such that } \sum_{i=1}^n1_{B_{\theta}(x,h)}(X_i)=k\right\}</code>, where <code class="reqn">1_{B_{\theta}(x,h)}(\cdot)</code> is the indicator function of the open ball created with the projection semi-metric with centre <code class="reqn">x\in\mathcal{H}</code> and radius <code class="reqn">h</code>.
</p>
</li></ul>

<p>If the argument <code>y.test</code> is given to the program (i. e. <code>if(!is.null(y.test))</code>), the function provides the mean squared error of prediction (see the value <code>MSE.test</code>) calculated as <code>mean((y.test-y.estimated.test)^2)</code>.
</p>


<h3>Value</h3>

<table>
<tr><td><code>y.estimated.test</code></td>
<td>
<p>Predicted responses.</p>
</td></tr>
<tr><td><code>MSE.test</code></td>
<td>
<p>Mean squared error between predicted and observed responses in the testing sample.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>German Aneiros Perez <a href="mailto:german.aneiros@udc.es">german.aneiros@udc.es</a> 
</p>
<p>Silvia Novo Diaz  <a href="mailto:snovo@est-econ.uc3m.es">snovo@est-econ.uc3m.es</a>
</p>


<h3>References</h3>

<p>Novo S., Aneiros, G., and Vieu, P., (2019) Automatic and location-adaptive estimation in functional single&ndash;index regression. <em>Journal of Nonparametric Statistics</em>, <b>31(2)</b>, 364&ndash;392, <a href="https://doi.org/10.1080/10485252.2019.1567726">doi:10.1080/10485252.2019.1567726</a>.
</p>


<h3>See Also</h3>

<p>See also <code><a href="#topic+fsim.kNN.fit">fsim.kNN.fit</a></code> and <code><a href="#topic+predict.fsim.kNN">predict.fsim.kNN</a></code>.
</p>
<p>Alternative procedure <code><a href="#topic+fsim.kernel.test">fsim.kernel.test</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(Tecator)
y&lt;-Tecator$fat
X&lt;-Tecator$absor.spectra2


train&lt;-1:160
test&lt;-161:215

#FSIM fit. With nknot.theta=2 and range.grid=c(850,1050),
#Theta_n contains 108 thetas.
ptm&lt;-proc.time()
fit&lt;-fsim.kNN.fit(y=y[train],x=X[train,],max.knn=20,nknot.theta=2,nknot=20,
      range.grid=c(850,1050))
proc.time()-ptm
fit

#FSIM prediction
test&lt;-fsim.kNN.test(y=y[train],x=X[train,],x.test=X[test,],y.test=y[test],
        theta=fit$theta.est,k=fit$k.opt,nknot.theta=2,order.Bspline=3,nknot=20,
        range.grid=c(850,1050))

#MSEP
test$MSE.test

  
</code></pre>

<hr>
<h2 id='IASSMR.kernel.fit'>
IASSMR with kernel estimation
</h2><span id='topic+IASSMR.kernel.fit'></span>

<h3>Description</h3>

<p>This function computes the improved algorithm for sparse semiparametric multi-functional regression (IASSMR) with kernel estimation.
</p>
<p>This algorithm involves the penalised least-squares regularization procedure combined with kernel estimation with Nadaraya-Watson weights.
The procedure requires the  B-spline representation to estimate the functional index <code class="reqn">\theta_0</code> and an objective criterion (<code>criterion</code>) to select the number of covariates in the reduced model (<code>w.opt</code>), the bandwidth (<code>h.opt</code>) and the penalisation parameter (<code>lambda.opt</code>). 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>IASSMR.kernel.fit(x, z, y, train.1=NULL, train.2=NULL, seed.coeff = c(-1, 0, 1), 
  order.Bspline = 3, nknot.theta = 3, t0 = NULL,min.q.h = 0.05, 
  max.q.h = 0.5, h.seq = NULL, num.h = 10, range.grid = NULL,
  kind.of.kernel = "quad", nknot = NULL, lambda.min = NULL, 
  lambda.min.h = NULL, lambda.min.l = NULL, factor.pn = 1,
  nlambda = 100, vn = ncol(z), nfolds = 10, seed = 123, wn = c(10, 15, 20),
  criterion = c("GCV", "BIC", "AIC", "k-fold-CV"), 
  penalty = c("grLasso", "grMCP", "grSCAD", "gel", "cMCP", "gBridge", 
  "gLasso", "gMCP"),max.iter = 1000)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="IASSMR.kernel.fit_+3A_x">x</code></td>
<td>

<p>Matrix containing the observations of the functional covariate collected by row (functional single-index component).
</p>
</td></tr>
<tr><td><code id="IASSMR.kernel.fit_+3A_z">z</code></td>
<td>

<p>Matrix containing the observations of the functional covariate that is discretised collected by row (linear component).
</p>
</td></tr>
<tr><td><code id="IASSMR.kernel.fit_+3A_y">y</code></td>
<td>

<p>Vector containing the scalar response.
</p>
</td></tr>
<tr><td><code id="IASSMR.kernel.fit_+3A_train.1">train.1</code></td>
<td>

<p>Indexes of the data used as the training sample in the 1st step. The default is  <code>train.1&lt;-1:ceiling(n/2)</code>.
</p>
</td></tr>
<tr><td><code id="IASSMR.kernel.fit_+3A_train.2">train.2</code></td>
<td>

<p>Indexes of the data used as the training sample in the 2nd step. The default is <code>train.2&lt;-(ceiling(n/2)+1):n</code>.
</p>
</td></tr>
<tr><td><code id="IASSMR.kernel.fit_+3A_seed.coeff">seed.coeff</code></td>
<td>

<p>Vector of initial values used to  build the set <code class="reqn">\Theta_n</code> (see section <code>Details</code>). The coefficients for the B-spline representation of each eligible functional index <code class="reqn">\theta \in \Theta_n</code> are obtained from <code>seed.coeff</code>.  The default is <code>c(-1,0,1)</code>.
</p>
</td></tr>
<tr><td><code id="IASSMR.kernel.fit_+3A_order.bspline">order.Bspline</code></td>
<td>

<p>Positive integer giving the order of the B-spline basis functions. This is the number of coefficients in each piecewise polynomial segment. The default is 3.
</p>
</td></tr>
<tr><td><code id="IASSMR.kernel.fit_+3A_nknot.theta">nknot.theta</code></td>
<td>

<p>Positive integer indicating the number of uniform interior knots of the B-spline basis for the B-spline representation of <code class="reqn">\theta_0</code>. The default is 3.
</p>
</td></tr>
<tr><td><code id="IASSMR.kernel.fit_+3A_t0">t0</code></td>
<td>

<p>Value in the domain of the functional indexes at which we evaluate them to  build the set <code class="reqn">\Theta_n</code>. We assume <code class="reqn">\theta_0(t_0)&gt;0</code> for some arbitrary <code class="reqn">t_0</code> in the domain to ensure model identifiability. If <code>t0=NULL</code>, then <code>mean(range.grid)</code> is considered.
</p>
</td></tr>
<tr><td><code id="IASSMR.kernel.fit_+3A_min.q.h">min.q.h</code></td>
<td>
<p>Order of the quantile of the set of distances between curves (computed with the projection semi-metric) which gives the lower end of the sequence in which the bandwidth is selected. The default is 0.05.
</p>
</td></tr>
<tr><td><code id="IASSMR.kernel.fit_+3A_max.q.h">max.q.h</code></td>
<td>

<p>Order of the quantile of the set of distances between curves (computed with the projection semi-metric) which gives the upper end of the sequence in which the bandwidth is selected. The default is 0.5.
</p>
</td></tr>
<tr><td><code id="IASSMR.kernel.fit_+3A_h.seq">h.seq</code></td>
<td>
<p>Vector containing the sequence of bandwidths. The default is a sequence of <code>num.h</code> equispaced bandwidths in the range constructed using <code>min.q.h</code> and <code>max.q.h</code>.
</p>
</td></tr>
<tr><td><code id="IASSMR.kernel.fit_+3A_num.h">num.h</code></td>
<td>
<p>Positive integer indicating the number of bandwiths in the grid. The default is 10.
</p>
</td></tr>
<tr><td><code id="IASSMR.kernel.fit_+3A_range.grid">range.grid</code></td>
<td>

<p>Vector of length 2 containing the endpoints of the grid at which the observations of the functional covariate <code>x</code> are evaluated (i.e. the range of the discretization). If <code>range.grid=NULL</code>, then <code>range.grid=c(1,p)</code> is considered, where <code>p</code> is the size of the discretization size of <code>x</code> (i.e. <code>ncol(x))</code>.
</p>
</td></tr>
<tr><td><code id="IASSMR.kernel.fit_+3A_kind.of.kernel">kind.of.kernel</code></td>
<td>

<p>The type of kernel function used. Only Epanechnikov kernel (<code>"quad"</code>) is available.
</p>
</td></tr>
<tr><td><code id="IASSMR.kernel.fit_+3A_nknot">nknot</code></td>
<td>

<p>Positive integer indicating the number of interior knots for the B-spline representation of the functional covariate. The default value is <code>(p - order.Bspline - 1)%/%2</code>.
</p>
</td></tr>
<tr><td><code id="IASSMR.kernel.fit_+3A_lambda.min">lambda.min</code></td>
<td>

<p>The smallest value for lambda (i. e., the smallest value  of the sequence in which <code>lambda.opt</code> is selected), as fraction of <code>lambda.max</code>.
The defaults is <code>lambda.min.l</code> if the number of observations is larger than <code>factor.pn</code> times the number of covariates and <code>lambda.min.h</code> otherwise.
</p>
</td></tr>
<tr><td><code id="IASSMR.kernel.fit_+3A_lambda.min.h">lambda.min.h</code></td>
<td>

<p>The smallest value of the sequence in which <code>lambda.opt</code> is selected if the number of observations is smaller than <code>factor.pn</code> times the number of scalar covariates. The default is 0.05. 
</p>
</td></tr>
<tr><td><code id="IASSMR.kernel.fit_+3A_lambda.min.l">lambda.min.l</code></td>
<td>

<p>The smallest value of the sequence in which <code>lambda.opt</code> is selected if the number of observations is larger than <code>factor.pn</code> times the number of scalar covariates. The default is 0.0001.
</p>
</td></tr>
<tr><td><code id="IASSMR.kernel.fit_+3A_factor.pn">factor.pn</code></td>
<td>

<p>Positive integer used to set <code>lambda.min</code>. The default value is 1.
</p>
</td></tr>
<tr><td><code id="IASSMR.kernel.fit_+3A_nlambda">nlambda</code></td>
<td>

<p>Positive integer indicating the number of values of the sequence in which <code>lambda.opt</code> is selected. The default is 100.
</p>
</td></tr>
<tr><td><code id="IASSMR.kernel.fit_+3A_vn">vn</code></td>
<td>

<p>Positive integer or vector of positive integers indicating the number of groups of consecutive variables to be penalised together. The default value is <code>vn=ncol(z)</code>, which leads to the individual penalisation of each scalar covariate.
</p>
</td></tr>
<tr><td><code id="IASSMR.kernel.fit_+3A_nfolds">nfolds</code></td>
<td>

<p>Positive integer indicating the number of cross-validation folds (used if <code>criterion="k-fold-CV"</code>). The default is 10.
</p>
</td></tr>
<tr><td><code id="IASSMR.kernel.fit_+3A_seed">seed</code></td>
<td>

<p>You may set the seed of the random number generator to obtain reproducible results (used if <code>criterion="k-fold-CV"</code>). The default is 123.
</p>
</td></tr>
<tr><td><code id="IASSMR.kernel.fit_+3A_wn">wn</code></td>
<td>

<p>A vector of positive integers indicating the eligible number of covariates of the reduced model. See the section <code>Details</code>. The default is <code>c(10,15,20)</code>.
</p>
</td></tr>
<tr><td><code id="IASSMR.kernel.fit_+3A_criterion">criterion</code></td>
<td>

<p>The criterion by which to select the regularization parameter <code>lambda.opt</code> and <code>k.opt</code>. One of <code>"GCV", "BIC", "AIC"</code> or <code>"k-fold-CV"</code>. The default is <code>"GCV"</code>.
</p>
</td></tr>
<tr><td><code id="IASSMR.kernel.fit_+3A_penalty">penalty</code></td>
<td>

<p>The penalty function to be applied in the penalized least squares procedure. Only &quot;grLasso&quot; and &quot;grSCAD&quot; are implemented.
</p>
</td></tr>
<tr><td><code id="IASSMR.kernel.fit_+3A_max.iter">max.iter</code></td>
<td>

<p>Maximum number of iterations (total across entire path). Default is 1000.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The multi-functional partial linear single-index model (MFPLSIM) is given by the expression
</p>
<p style="text-align: center;"><code class="reqn">Y_i=\sum_{j=1}^{p_n}\beta_{0j}\zeta_i(t_j)+r\left(\left&lt;\theta_0,X_i\right&gt;\right)+\varepsilon_i,\ \ \ (i=1,\dots,n)</code>
</p>

<p>where 
</p>

<ul>
<li> <p><code class="reqn">Y_i</code> is a real random response and <code class="reqn">X_i</code> denotes a random element belonging to some separable Hilbert space <code class="reqn">\mathcal{H}</code> with inner product denoted by <code class="reqn">\left\langle\cdot,\cdot\right\rangle</code>. The second functional predictor <code class="reqn">\zeta_i</code> is supposed to be a random curve defined on some interval <code class="reqn">[a,b]</code> which  is observed at the points <code class="reqn">a\leq t_1&lt;\dots&lt;t_{p_n}\leq b</code>. 
</p>
</li>
<li>  <p><code class="reqn">\mathbf{\beta}_0=(\beta_{01},\dots,\beta_{0p_n})^{\top}</code> is a vector of unknown real coefficients and <code class="reqn">r(\cdot)</code> denotes a smooth unknown link function. In addition, <code class="reqn">\theta_0</code> is an unknown functional index in <code class="reqn">\mathcal{H}</code>.  
</p>
</li>
<li> <p><code class="reqn">\varepsilon_i</code> denotes the random error.
</p>
</li></ul>

<p>In  the MFPLSIM, we assume that only a few scalar variables from the set <code class="reqn">\{\zeta(t_1),\dots,\zeta(t_{p_n})\}</code>  form part of the model. Therefore, we must select the relevant variables in the linear component  (the impact points of the curve <code class="reqn">\zeta</code> on the response) and estimate the model.
</p>
<p>In this function, the MFPLSIM is fitted using the IASSMR. The IASSMR (version of the PVS algorithm for semiparametric regression) is an algorithm with two steps, so we split the sample into two independent subsamples (asymptotically of the same size <code class="reqn">n_1\sim n_2\sim n/2</code>), one of them to be used in the first stage of the method and the other in the second stage.
</p>
<p style="text-align: center;"><code class="reqn">
\mathcal{E}^{\mathbf{1}}=\{(\zeta_i,\mathcal{X}_i,Y_i),\quad i=1,\dots,n_1\},
</code>
</p>

<p style="text-align: center;"><code class="reqn">
\mathcal{E}^{\mathbf{2}}=\{(\zeta_i,\mathcal{X}_i,Y_i),\quad i=n_1+1,\dots,n_1+n_2=n\}.
</code>
</p>
 
<p>Note that these two subsamples are specified to the programme by means of the arguments <code>train.1</code> and <code>train.2</code>.
The superscript <code class="reqn">\mathbf{s}</code> with <code class="reqn">\mathbf{s}=\mathbf{1},\mathbf{2}</code> indicates the stage of the method in which the sample, function, variable or parameter is involved. 
</p>
<p>To explain the algorithm we assume, without lost of generality, that the number <code class="reqn">p_n</code> of linear covariates can be expressed as follows: <code class="reqn">p_n=q_nw_n</code> with <code class="reqn">q_n</code> and <code class="reqn">w_n</code> integers.
</p>

<ol>
<li> <p><b>First step</b>. The fast algorithm for sparse semiparametric multi-functional regression (FASSMR) combined with kernel estimation is applied using only the subsample <code class="reqn">\mathcal{E}^{\mathbf{1}}</code> (see the documentation of the function <code><a href="#topic+FASSMR.kernel.fit">FASSMR.kernel.fit</a></code>). Specifically:
</p>

<ul>
<li><p> Consider a subset of the initial <code class="reqn">p_n</code> linear covariates, which contains only <code class="reqn">w_n</code> equally spaced discretized observations of  <code class="reqn">\zeta</code> covering the whole interval  <code class="reqn">[a,b]</code>. This subset is the following:
</p>
<p style="text-align: center;"><code class="reqn">
	\mathcal{R}_n^{\mathbf{1}}=\left\{\zeta\left(t_k^{\mathbf{1}}\right),\ \ k=1,\dots,w_n\right\},
</code>
</p>
 
<p>where  <code class="reqn">t_k^{\mathbf{1}}=t_{\left[(2k-1)q_n/2\right]}</code> and  <code class="reqn">\left[z\right]</code> denotes the smallest integer not less than the real number <code class="reqn">z</code>.The size (cardinal) of this subset is provided to the program in the argument <code>wn</code> (which contains a sequence of eligible sizes).
</p>
</li>
<li><p> Consider the following reduced model, which involves only the <code class="reqn">w_n</code> linear covariates belonging to <code class="reqn">\mathcal{R}_n^{\mathbf{1}}</code>:
</p>
<p style="text-align: center;"><code class="reqn">
	Y_i=\sum_{k=1}^{w_n}\beta_{0k}^{\mathbf{1}}\zeta_i(t_k^{\mathbf{1}})+r^{\mathbf{1}}\left(\left&lt;\theta_0^{\mathbf{1}},X_i\right&gt;\right)+\varepsilon_i^{\mathbf{1}}.
</code>
</p>

<p>The penalised least-squares variable selection procedure, with kernel estimation, is applied to the reduced model. This is done by means of the function <code><a href="#topic+sfplsim.kernel.fit">sfplsim.kernel.fit</a></code>, which requires the remaining arguments (for details, see the documentation of the function <code><a href="#topic+sfplsim.kernel.fit">sfplsim.kernel.fit</a></code>). The estimates obtained after that are the outputs of the first step of the algorithm.
</p>
</li></ul>

</li>
<li> <p><b>Second step</b>. The variables selected in the first step and the variables in the neighbourhood of the ones selected are included. Then the penalised least-squares procedure, combined with kernel estimation, is carried out again. For that, we consider only the subsample <code class="reqn">\mathcal{E}^{\mathbf{2}}</code>. Specifically:
</p>

<ul>
<li><p> Consider a new set of variables :
</p>
<p style="text-align: center;"><code class="reqn">
		\mathcal{R}_n^{\mathbf{2}}=\bigcup_{\left\{k,\widehat{\beta}_{0k}^{\mathbf{1}}\not=0\right\}}\left\{\zeta(t_{(k-1)q_n+1}),\dots,\zeta(t_{kq_n})\right\}.
	</code>
</p>

<p>Denoting by <code class="reqn">r_n=\sharp(\mathcal{R}_n^{\mathbf{2}})</code>, we can rename the variables in <code class="reqn">\mathcal{R}_n^{\mathbf{2}}</code> as follows:
</p>
<p style="text-align: center;"><code class="reqn">
		\mathcal{R}_n^{\mathbf{2}}=\left\{\zeta(t_1^{\mathbf{2}}),\dots,\zeta(t_{r_n}^{\mathbf{2}})\right\},
		</code>
</p>

</li>
<li><p>  Consider the following model, which involves only the linear covariates belonging to <code class="reqn">\mathcal{R}_n^{\mathbf{2}}</code>
</p>
<p style="text-align: center;"><code class="reqn">
		Y_i=\sum_{k=1}^{r_n}\beta_{0k}^{\mathbf{2}}\zeta_i(t_k^{\mathbf{2}})+r^{\mathbf{2}}\left(\left&lt;\theta_0^{\mathbf{2}},X_i\right&gt;\right)+\varepsilon_i^{\mathbf{2}}.</code>
</p>

<p>The penalized least-squares variable selection procedure, with kernel estimation, is applied to this model by means of the function <code><a href="#topic+sfplsim.kernel.fit">sfplsim.kernel.fit</a></code>. 
</p>
</li></ul>

</li></ol>

<p>The outputs of the second step are the estimates of the MFPLSIM obtained with the IASSMR algorithm. For further details on this algorithm, see Novo et al. (2021).
</p>
<p><b>Remark</b>: If the condition  <code class="reqn">p_n=w_n q_n</code> fails, the function considers not fixed  <code class="reqn">q_n=q_{n,k}</code> values <code class="reqn">k=1,\dots,w_n</code>,  when <code class="reqn">p_n/w_n</code> is not an integer number. Specifically:
</p>
<p style="text-align: center;"><code class="reqn">
	q_{n,k}= \left\{\begin{array}{ll}
	[p_n/w_n]+1 &amp;   k\in\{1,\dots,p_n-w_n[p_n/w_n]\},\\
	{[p_n/w_n]} &amp; k\in\{p_n-w_n[p_n/w_n]+1,\dots,w_n\},
	\end{array}
	\right.
</code>
</p>

<p>where <code class="reqn">[z]</code> denotes the integer part of the real number <code class="reqn">z</code>.
</p>


<h3>Value</h3>

<table>
<tr><td><code>call</code></td>
<td>
<p>The matched call.</p>
</td></tr>
<tr><td><code>fitted.values</code></td>
<td>
<p>Estimated scalar response.</p>
</td></tr>
<tr><td><code>residuals</code></td>
<td>
<p>Differences between <code>y</code> and the <code>fitted.values</code></p>
</td></tr>
<tr><td><code>beta.est</code></td>
<td>
<p><code class="reqn">\hat{\mathbf{\beta}}</code> (i. e. estimate of <code class="reqn">\mathbf{\beta}_0</code> when the optimal tuning parameters <code>w.opt</code>, <code>lambda.opt</code>, <code>h.opt</code> and <code>vn.opt</code> are used).</p>
</td></tr>
<tr><td><code>theta.est</code></td>
<td>
<p>Coefficients of <code class="reqn">\hat{\theta}</code> in the B-spline basis (i. e. estimate of <code class="reqn">\theta_0</code>when the optimal tuning parameters <code>w.opt</code>, <code>lambda.opt</code>, <code>h.opt</code> and <code>vn.opt</code> are used): a vector of <code>length(order.Bspline+nknot.theta)</code>.</p>
</td></tr>
<tr><td><code>indexes.beta.nonnull</code></td>
<td>
<p>Indexes of the non-zero <code class="reqn">\hat{\beta_{j}}</code>.</p>
</td></tr>
<tr><td><code>h.opt</code></td>
<td>
<p>Selected bandwidth (when <code>w.opt</code> is considered).</p>
</td></tr>
<tr><td><code>w.opt</code></td>
<td>
<p>Selected size for  <code class="reqn">\mathcal{R}_n^{\mathbf{1}}</code>.</p>
</td></tr>
<tr><td><code>lambda.opt</code></td>
<td>
<p>Selected value of the penalisation parameter <code class="reqn">\lambda</code> (when <code>w.opt</code> is considered).</p>
</td></tr>
<tr><td><code>IC</code></td>
<td>
<p>Value of the criterion function considered to select <code>w.opt</code>, <code>lambda.opt</code>, <code>h.opt</code> and <code>vn.opt</code>.</p>
</td></tr>
<tr><td><code>vn.opt</code></td>
<td>
<p>Selected value of <code>vn</code> in the second step (when <code>w.opt</code> is considered).</p>
</td></tr>
<tr><td><code>beta2</code></td>
<td>
<p>Estimate of <code class="reqn">\mathbf{\beta}_0^{\mathbf{2}}</code> for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>theta2</code></td>
<td>
<p>Estimate of <code class="reqn">\theta_0^{\mathbf{2}}</code> for each value of the sequence <code>wn</code> (i.e. its coefficients in the B-spline basis).</p>
</td></tr> 
<tr><td><code>indexes.beta.nonnull2</code></td>
<td>
<p>Indexes of the non-zero linear coefficients after the step 2 of the method for each value of the sequence <code>wn</code>.</p>
</td></tr> 
<tr><td><code>h2</code></td>
<td>
<p>Selected bandwidth in the second step of the algorithm for each value of the sequence <code>wn</code>.</p>
</td></tr> 
<tr><td><code>IC2</code></td>
<td>
<p>Optimal value of the criterion function in the second step for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>lambda2</code></td>
<td>
<p>Selected value of penalisation parameter in the second step for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>index02</code></td>
<td>
<p>Indexes of the covariates (in the whole set of <code class="reqn">p_n</code>) used to build <code class="reqn">\mathcal{R}_n^{\mathbf{2}}</code> for each value of the sequence <code>wn</code>.</p>
</td></tr> 
<tr><td><code>beta1</code></td>
<td>
<p>Estimate of <code class="reqn">\mathbf{\beta}_0^{\mathbf{1}}</code> for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>theta1</code></td>
<td>
<p>Estimate of <code class="reqn">\theta_0^{\mathbf{1}}</code> for each value of the sequence <code>wn</code> (i.e. its coefficients in the B-spline basis).</p>
</td></tr>
<tr><td><code>h1</code></td>
<td>
<p>Selected bandwidth in the first step of the algorithm for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>IC1</code></td>
<td>
<p>Optimal value of the criterion function in the first step for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>lambda1</code></td>
<td>
<p>Selected value of penalisation parameter in the first step for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>index01</code></td>
<td>
<p>Indexes of the covariates (in the whole set of <code class="reqn">p_n</code>) used to build <code class="reqn">\mathcal{R}_n^{\mathbf{1}}</code> for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>index1</code></td>
<td>
<p>Indexes of the non-zero linear coefficients after the step 1 of the method for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>...</code></td>
<td>
<p>Further outputs to apply S3 methods.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>German Aneiros Perez <a href="mailto:german.aneiros@udc.es">german.aneiros@udc.es</a> 
</p>
<p>Silvia Novo Diaz  <a href="mailto:snovo@est-econ.uc3m.es">snovo@est-econ.uc3m.es</a>
</p>


<h3>References</h3>

<p>Novo, S., Vieu, P., and Aneiros, G., (2021) Fast and efficient algorithms for
sparse semiparametric bi-functional regression. <em>Australian and New Zealand
Journal of Statistics</em>, <b>63</b>, 606&ndash;638, <a href="https://doi.org/10.1111/anzs.12355">doi:10.1111/anzs.12355</a>.
</p>


<h3>See Also</h3>

<p>See also <code><a href="#topic+sfplsim.kernel.fit">sfplsim.kernel.fit</a></code>, <code><a href="#topic+predict.IASSMR.kernel">predict.IASSMR.kernel</a></code>, <code><a href="#topic+plot.IASSMR.kernel">plot.IASSMR.kernel</a></code> and <code><a href="#topic+FASSMR.kernel.fit">FASSMR.kernel.fit</a></code>.
</p>
<p>Alternative methods <code><a href="#topic+IASSMR.kNN.fit">IASSMR.kNN.fit</a></code>, <code><a href="#topic+FASSMR.kernel.fit">FASSMR.kernel.fit</a></code> and <code><a href="#topic+FASSMR.kNN.fit">FASSMR.kNN.fit</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

data(Sugar)


y&lt;-Sugar$ash
x&lt;-Sugar$wave.290
z&lt;-Sugar$wave.240

#Outliers
index.y.25 &lt;- y &gt; 25
index.atip &lt;- index.y.25
(1:268)[index.atip]


#Dataset to model
x.sug &lt;- x[!index.atip,]
z.sug&lt;- z[!index.atip,]
y.sug &lt;- y[!index.atip]

train&lt;-1:216

ptm=proc.time()
fit&lt;- IASSMR.kernel.fit(x=x.sug[train,],z=z.sug[train,], y=y.sug[train],
        train.1=1:108,train.2=109:216,nknot.theta=2,lambda.min.h=0.03,
        lambda.min.l=0.03,  max.q.h=0.35, num.h = 10, nknot=20,
        criterion="BIC", penalty="grSCAD", max.iter=5000)
proc.time()-ptm

fit 
names(fit)

   
</code></pre>

<hr>
<h2 id='IASSMR.kNN.fit'>
IASSMR with kNN estimation
</h2><span id='topic+IASSMR.kNN.fit'></span>

<h3>Description</h3>

<p>This function computes the improved algorithm for sparse semiparametric multi-functional regression (IASSMR) with kNN estimation.
</p>
<p>This algorithm involves the penalised least-squares regularization procedure combined with <code class="reqn">k</code>-nearest neighbours (kNN) estimation with Nadaraya-Watson weights.
The procedure requires the  B-spline representation to estimate the functional index <code class="reqn">\theta_0</code> and an objective criterion (<code>criterion</code>) to select the number of covariates in the reduced model (<code>w.opt</code>), the number of neighbours (<code>k.opt</code>) and the penalisation parameter (<code>lambda.opt</code>). 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>IASSMR.kNN.fit(x, z, y, train.1=NULL, train.2=NULL, seed.coeff = c(-1, 0, 1), 
  order.Bspline = 3, nknot.theta = 3, t0 = NULL, knearest = NULL, 
  min.knn = 2, max.knn = NULL, step = NULL, range.grid = NULL, 
  kind.of.kernel = "quad", nknot = NULL, lambda.min = NULL, 
  lambda.min.h = NULL, lambda.min.l = NULL, factor.pn = 1,
  nlambda = 100,vn = ncol(z), nfolds = 10, seed = 123, wn = c(10, 15, 20),
  criterion = c("GCV", "BIC", "AIC", "k-fold-CV"), 
  penalty = c("grLasso", "grMCP", "grSCAD", "gel", "cMCP",
  "gBridge", "gLasso", "gMCP"), max.iter = 1000)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="IASSMR.kNN.fit_+3A_x">x</code></td>
<td>

<p>Matrix containing the observations of the functional covariate collected by row (functional single-index component).
</p>
</td></tr>
<tr><td><code id="IASSMR.kNN.fit_+3A_z">z</code></td>
<td>

<p>Matrix containing the observations of the functional covariate that is discretised collected by row (linear component).
</p>
</td></tr>
<tr><td><code id="IASSMR.kNN.fit_+3A_y">y</code></td>
<td>

<p>Vector containing the scalar response.
</p>
</td></tr>
<tr><td><code id="IASSMR.kNN.fit_+3A_train.1">train.1</code></td>
<td>
<p>Indexes of the data used as the training sample in the 1st step. The default is  <code>train.1&lt;-1:ceiling(n/2)</code>.
</p>
</td></tr>
<tr><td><code id="IASSMR.kNN.fit_+3A_train.2">train.2</code></td>
<td>
<p>Indexes of the data used as the training sample in the 2nd step.  The default is <code>train.2&lt;-(ceiling(n/2)+1):n</code>.
</p>
</td></tr>
<tr><td><code id="IASSMR.kNN.fit_+3A_seed.coeff">seed.coeff</code></td>
<td>

<p>Vector of initial values used to  build the set <code class="reqn">\Theta_n</code> (see section <code>Details</code>). The coefficients for the B-spline representation of each eligible functional index <code class="reqn">\theta \in \Theta_n</code> are obtained from <code>seed.coeff</code>.  The default is <code>c(-1,0,1)</code>.
</p>
</td></tr>
<tr><td><code id="IASSMR.kNN.fit_+3A_order.bspline">order.Bspline</code></td>
<td>

<p>Positive integer giving the order of the B-spline basis functions. This is the number of coefficients in each piecewise polynomial segment. The default is 3.
</p>
</td></tr>
<tr><td><code id="IASSMR.kNN.fit_+3A_nknot.theta">nknot.theta</code></td>
<td>

<p>Positive integer indicating the number of uniform interior knots of the B-spline basis for the B-spline representation of <code class="reqn">\theta_0</code>. The default is 3.
</p>
</td></tr>
<tr><td><code id="IASSMR.kNN.fit_+3A_t0">t0</code></td>
<td>

<p>Value in the domain of the functional indexes at which we evaluate them to  build the set <code class="reqn">\Theta_n</code>. We assume <code class="reqn">\theta_0(t_0)&gt;0</code> for some arbitrary <code class="reqn">t_0</code> in the domain to ensure model identifiability. If <code>t0=NULL</code>, then <code>mean(range.grid)</code> is considered.
</p>
</td></tr>
<tr><td><code id="IASSMR.kNN.fit_+3A_knearest">knearest</code></td>
<td>

<p>Vector of positive integers containing the sequence in which the  number of nearest neighbours <code>k.opt</code> is selected. If <code>knearest=NULL</code>, then <code>knearest &lt;- seq(from =min.knn, to = max.knn, by = step)</code>.
</p>
</td></tr>
<tr><td><code id="IASSMR.kNN.fit_+3A_min.knn">min.knn</code></td>
<td>

<p>Positive integer indicating the minumum value of the sequence in which the  number of nearest neighbours <code>k.opt</code> is selected (thus, this number must be smaller than the sample size). The default is 2.
</p>
</td></tr>
<tr><td><code id="IASSMR.kNN.fit_+3A_max.knn">max.knn</code></td>
<td>

<p>Positive integer indicating the maximum value of the sequence in which the number of nearest neighbours <code>k.opt</code> is selected (thus, this number must be larger than <code>min.kNN</code> and smaller than the sample size). The default is <code>max.knn &lt;- n%/%2</code>, being <code class="reqn">n=n_1</code> in the 1st step and <code class="reqn">n=n_2</code> in the 2nd step of the method (see section <code>Details</code>).
</p>
</td></tr>
<tr><td><code id="IASSMR.kNN.fit_+3A_step">step</code></td>
<td>

<p>Positive integer used to build the sequence of k-nearest neighbours in the following way: <code>min.knn, min.knn + step, min.knn + 2*step, min.knn + 3*step,...</code> The default is <code>step&lt;-ceiling(n/100)</code>.
</p>
</td></tr>
<tr><td><code id="IASSMR.kNN.fit_+3A_range.grid">range.grid</code></td>
<td>

<p>Vector of length 2 containing the endpoints of the grid at which the observations of the functional covariate <code>x</code> are evaluated (i.e. the range of the discretization). If <code>range.grid=NULL</code>, then <code>range.grid=c(1,p)</code> is considered, where <code>p</code> is the size of the discretization size of <code>x</code> (i.e. <code>ncol(x))</code>.
</p>
</td></tr>
<tr><td><code id="IASSMR.kNN.fit_+3A_kind.of.kernel">kind.of.kernel</code></td>
<td>

<p>The type of kernel function used. Only Epanechnikov kernel (<code>"quad"</code>) is available.
</p>
</td></tr>
<tr><td><code id="IASSMR.kNN.fit_+3A_nknot">nknot</code></td>
<td>

<p>Positive integer indicating the number of interior knots for the B-spline representation of the functional covariate. The default value is <code>(p - order.Bspline - 1)%/%2</code>.
</p>
</td></tr>
<tr><td><code id="IASSMR.kNN.fit_+3A_lambda.min">lambda.min</code></td>
<td>

<p>The smallest value for lambda (i. e., the smallest value  of the sequence in which <code>lambda.opt</code> is selected), as fraction of <code>lambda.max</code>.
The defaults is <code>lambda.min.l</code> if the number of observations is larger than <code>factor.pn</code> times the number of covariates and <code>lambda.min.h</code> otherwise.
</p>
</td></tr>
<tr><td><code id="IASSMR.kNN.fit_+3A_lambda.min.h">lambda.min.h</code></td>
<td>

<p>The smallest value of the sequence in which <code>lambda.opt</code> is selected if the number of observations is smaller than <code>factor.pn</code> times the number of scalar covariates. The default is 0.05. 
</p>
</td></tr>
<tr><td><code id="IASSMR.kNN.fit_+3A_lambda.min.l">lambda.min.l</code></td>
<td>

<p>The smallest value of the sequence in which <code>lambda.opt</code> is selected if the number of observations is larger than <code>factor.pn</code> times the number of scalar covariates. The default is 0.0001.
</p>
</td></tr>
<tr><td><code id="IASSMR.kNN.fit_+3A_factor.pn">factor.pn</code></td>
<td>

<p>Positive integer used to set <code>lambda.min</code>. The default value is 1.
</p>
</td></tr>
<tr><td><code id="IASSMR.kNN.fit_+3A_nlambda">nlambda</code></td>
<td>

<p>Positive integer indicating the number of values of the sequence in which <code>lambda.opt</code> is selected. The default is 100.
</p>
</td></tr>
<tr><td><code id="IASSMR.kNN.fit_+3A_vn">vn</code></td>
<td>
<p>Positive integer or vector of positive integers indicating the number of groups of consecutive variables to be penalised together. The default value is <code>vn=ncol(z)</code>, which leads to the individual penalisation of each scalar covariate.
</p>
</td></tr>
<tr><td><code id="IASSMR.kNN.fit_+3A_nfolds">nfolds</code></td>
<td>

<p>Positive integer indicating the number of cross-validation folds (used if <code>criterion="k-fold-CV"</code>). Default is 10.
</p>
</td></tr>
<tr><td><code id="IASSMR.kNN.fit_+3A_seed">seed</code></td>
<td>

<p>You may set the seed of the random number generator to obtain reproducible results (used if <code>criterion="k-fold-CV"</code>). Default is 123.
</p>
</td></tr>
<tr><td><code id="IASSMR.kNN.fit_+3A_wn">wn</code></td>
<td>

<p>A vector of positive integers indicating the eligible number of covariates of the reduced model. See the section <code>Details</code>. The default is <code>c(10,15,20)</code>.
</p>
</td></tr>
<tr><td><code id="IASSMR.kNN.fit_+3A_criterion">criterion</code></td>
<td>

<p>The criterion by which to select the regularization parameter <code>lambda.opt</code> and <code>k.opt</code>. One of <code>"GCV", "BIC", "AIC"</code> or <code>"k-fold-CV"</code>. The default is <code>"GCV"</code>.
</p>
</td></tr>
<tr><td><code id="IASSMR.kNN.fit_+3A_penalty">penalty</code></td>
<td>

<p>The penalty function to be applied in the penalized least squares procedure. Only &quot;grLasso&quot; and &quot;grSCAD&quot; are implemented.
</p>
</td></tr>
<tr><td><code id="IASSMR.kNN.fit_+3A_max.iter">max.iter</code></td>
<td>

<p>Maximum number of iterations (total across entire path). Default is 1000.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The multi-functional partial linear single-index model (MFPLSIM) is given by the expression
</p>
<p style="text-align: center;"><code class="reqn">Y_i=\sum_{j=1}^{p_n}\beta_{0j}\zeta_i(t_j)+r\left(\left&lt;\theta_0,X_i\right&gt;\right)+\varepsilon_i,\ \ \ (i=1,\dots,n)</code>
</p>

<p>where 
</p>

<ul>
<li> <p><code class="reqn">Y_i</code> is a real random response and <code class="reqn">X_i</code> denotes a random element belonging to some separable Hilbert space <code class="reqn">\mathcal{H}</code> with inner product denoted by <code class="reqn">\left\langle\cdot,\cdot\right\rangle</code>. The second functional predictor <code class="reqn">\zeta_i</code> is supposed to be a random curve defined on some interval <code class="reqn">[a,b]</code> which  is observed at the points <code class="reqn">a\leq t_1&lt;\dots&lt;t_{p_n}\leq b</code>. 
</p>
</li>
<li>  <p><code class="reqn">\mathbf{\beta}_0=(\beta_{01},\dots,\beta_{0p_n})^{\top}</code> is a vector of unknown real coefficients and <code class="reqn">r(\cdot)</code> denotes a smooth unknown link function. In addition, <code class="reqn">\theta_0</code> is an unknown functional index in <code class="reqn">\mathcal{H}</code>.  
</p>
</li>
<li> <p><code class="reqn">\varepsilon_i</code> denotes the random error.
</p>
</li></ul>

<p>In  the MFPLSIM, we assume that only a few scalar variables from the set <code class="reqn">\{\zeta(t_1),\dots,\zeta(t_{p_n})\}</code>  form part of the model. Therefore, we must select the relevant variables in the linear component  (the impact points of the curve <code class="reqn">\zeta</code> on the response) and estimate the model.
</p>
<p>In this function, the MFPLSIM is fitted using the IASSMR. The IASSMR  (version of the PVS algorithm for semiparametric regression) is an algorithm with two steps, so we split the sample into two independent subsamples (asymptotically of the same size <code class="reqn">n_1\sim n_2\sim n/2</code>), one of them to be used in the first stage of the method and the other in the second stage.
</p>
<p style="text-align: center;"><code class="reqn">
\mathcal{E}^{\mathbf{1}}=\{(\zeta_i,\mathcal{X}_i,Y_i),\quad i=1,\dots,n_1\},
</code>
</p>

<p style="text-align: center;"><code class="reqn">
\mathcal{E}^{\mathbf{2}}=\{(\zeta_i,\mathcal{X}_i,Y_i),\quad i=n_1+1,\dots,n_1+n_2=n\}.
</code>
</p>
 
<p>Note that these two subsamples are specified to the programme by means of the arguments <code>train.1</code> and <code>train.2</code>.
The superscript <code class="reqn">\mathbf{s}</code> with <code class="reqn">\mathbf{s}=\mathbf{1},\mathbf{2}</code> indicates the stage of the method in which the sample, function, variable or parameter is involved. 
</p>
<p>To explain the algorithm we assume, without lost of generality, that the number <code class="reqn">p_n</code> of linear covariates can be expressed as follows: <code class="reqn">p_n=q_nw_n</code> with <code class="reqn">q_n</code> and <code class="reqn">w_n</code> integers.
</p>

<ol>
<li> <p><b>First step</b>. The fast algorithm for sparse semiparametric multi-functional regression (FASSMR) combined with kNN estimation is applied using only the subsample <code class="reqn">\mathcal{E}^{\mathbf{1}}</code> (see the documentation of the function <code><a href="#topic+FASSMR.kNN.fit">FASSMR.kNN.fit</a></code>). Specifically:
</p>

<ul>
<li><p> Consider a subset of the initial <code class="reqn">p_n</code> linear covariates, which contains only <code class="reqn">w_n</code> equally spaced discretized observations of  <code class="reqn">\zeta</code> covering the whole interval  <code class="reqn">[a,b]</code>. This subset is the following:
</p>
<p style="text-align: center;"><code class="reqn">
	\mathcal{R}_n^{\mathbf{1}}=\left\{\zeta\left(t_k^{\mathbf{1}}\right),\ \ k=1,\dots,w_n\right\},
</code>
</p>
 
<p>where  <code class="reqn">t_k^{\mathbf{1}}=t_{\left[(2k-1)q_n/2\right]}</code> and  <code class="reqn">\left[z\right]</code> denotes the smallest integer not less than the real number <code class="reqn">z</code>.The size (cardinal) of this subset is provided to the program in the argument <code>wn</code> (which contains a sequence of eligible sizes).
</p>
</li>
<li><p> Consider the following reduced model, which involves only the <code class="reqn">w_n</code> linear covariates belonging to <code class="reqn">\mathcal{R}_n^{\mathbf{1}}</code>:
</p>
<p style="text-align: center;"><code class="reqn">
	Y_i=\sum_{k=1}^{w_n}\beta_{0k}^{\mathbf{1}}\zeta_i(t_k^{\mathbf{1}})+r^{\mathbf{1}}\left(\left&lt;\theta_0^{\mathbf{1}},X_i\right&gt;\right)+\varepsilon_i^{\mathbf{1}}.
</code>
</p>

<p>The penalised least-squares variable selection procedure, with kNN estimation, is applied to the reduced model. This is done by means of the function <code><a href="#topic+sfplsim.kNN.fit">sfplsim.kNN.fit</a></code>, which requires the remaining arguments (for details, see the documentation of the function <code><a href="#topic+sfplsim.kNN.fit">sfplsim.kNN.fit</a></code>). The estimates obtained after that are the outputs of the first step of the algorithm.
</p>
</li></ul>

</li>
<li> <p><b>Second step</b>. The variables selected in the first step and the variables in the neighbourhood of the ones selected are included. Then the penalised least-squares procedure, combined with kNN estimation, is carried out again. For that, we consider only the subsample <code class="reqn">\mathcal{E}^{\mathbf{2}}</code>. Specifically:
</p>

<ul>
<li><p> Consider a new set of variables:
</p>
<p style="text-align: center;"><code class="reqn">
		\mathcal{R}_n^{\mathbf{2}}=\bigcup_{\left\{k,\widehat{\beta}_{0k}^{\mathbf{1}}\not=0\right\}}\left\{\zeta(t_{(k-1)q_n+1}),\dots,\zeta(t_{kq_n})\right\}.
	</code>
</p>

<p>Denoting by <code class="reqn">r_n=\sharp(\mathcal{R}_n^{\mathbf{2}})</code>, we can rename the variables in <code class="reqn">\mathcal{R}_n^{\mathbf{2}}</code> as follows:
</p>
<p style="text-align: center;"><code class="reqn">
		\mathcal{R}_n^{\mathbf{2}}=\left\{\zeta(t_1^{\mathbf{2}}),\dots,\zeta(t_{r_n}^{\mathbf{2}})\right\},
		</code>
</p>

</li>
<li><p>  Consider the following model, which involves only the linear covariates belonging to <code class="reqn">\mathcal{R}_n^{\mathbf{2}}</code>
</p>
<p style="text-align: center;"><code class="reqn">
		Y_i=\sum_{k=1}^{r_n}\beta_{0k}^{\mathbf{2}}\zeta_i(t_k^{\mathbf{2}})+r^{\mathbf{2}}\left(\left&lt;\theta_0^{\mathbf{2}},X_i\right&gt;\right)+\varepsilon_i^{\mathbf{2}}.</code>
</p>

<p>The penalized least-squares variable selection procedure, with kNN estimation, is applied to this model by means of the function <code><a href="#topic+sfplsim.kNN.fit">sfplsim.kNN.fit</a></code>. 
</p>
</li></ul>

</li></ol>

<p>The outputs of the second step are the estimates of the MFPLSIM obtained with the IASSMR algorithm. For further details on this algorithm, see Novo et al. (2021).
</p>
<p><b>Remark</b>: If the condition  <code class="reqn">p_n=w_n q_n</code> fails, the function considers not fixed  <code class="reqn">q_n=q_{n,k}</code> values <code class="reqn">k=1,\dots,w_n</code>,  when <code class="reqn">p_n/w_n</code> is not an integer number. Specifically:
</p>
<p style="text-align: center;"><code class="reqn">
	q_{n,k}= \left\{\begin{array}{ll}
	[p_n/w_n]+1 &amp;   k\in\{1,\dots,p_n-w_n[p_n/w_n]\},\\
	{[p_n/w_n]} &amp; k\in\{p_n-w_n[p_n/w_n]+1,\dots,w_n\},
	\end{array}
	\right.
</code>
</p>

<p>where <code class="reqn">[z]</code> denotes the integer part of the real number <code class="reqn">z</code>.
</p>


<h3>Value</h3>

<table>
<tr><td><code>call</code></td>
<td>
<p>The matched call.</p>
</td></tr>
<tr><td><code>fitted.values</code></td>
<td>
<p>Estimated scalar response.</p>
</td></tr>
<tr><td><code>residuals</code></td>
<td>
<p>Differences between <code>y</code> and the <code>fitted.values</code></p>
</td></tr>
<tr><td><code>beta.est</code></td>
<td>
<p><code class="reqn">\hat{\mathbf{\beta}}</code> (i.e. estimate of <code class="reqn">\mathbf{\beta}_0</code> when the optimal tuning parameters <code>w.opt</code>, <code>lambda.opt</code>, <code>vn.opt</code> and <code>k.opt</code> are used).</p>
</td></tr>
<tr><td><code>theta.est</code></td>
<td>
<p>Coefficients of <code class="reqn">\hat{\theta}</code> in the B-spline basis (i. e. estimate of <code class="reqn">\theta_0</code>when the optimal tuning parameters <code>w.opt</code>, <code>lambda.opt</code>, <code>vn.opt</code> and <code>k.opt</code> are used): a vector of <code>length(order.Bspline+nknot.theta)</code>.</p>
</td></tr>
<tr><td><code>indexes.beta.nonnull</code></td>
<td>
<p>Indexes of the non-zero <code class="reqn">\hat{\beta_{j}}</code>.</p>
</td></tr>
<tr><td><code>k.opt</code></td>
<td>
<p>Selected number of nearest neighbours (when <code>w.opt</code> is considered).</p>
</td></tr>
<tr><td><code>w.opt</code></td>
<td>
<p>Selected initial number of covariates in the reduced model.</p>
</td></tr>
<tr><td><code>lambda.opt</code></td>
<td>
<p>Selected value of the penalisation parameter <code class="reqn">\lambda</code> (when <code>w.opt</code> is considered).</p>
</td></tr>
<tr><td><code>IC</code></td>
<td>
<p>Value of the criterion function considered to select <code>w.opt</code>, <code>lambda.opt</code>, <code>vn.opt</code> and <code>k.opt</code>.</p>
</td></tr>
<tr><td><code>vn.opt</code></td>
<td>
<p>Selected value of <code>vn</code> in the second step (when <code>w.opt</code> is considered).</p>
</td></tr>
<tr><td><code>beta2</code></td>
<td>
<p>Estimate of <code class="reqn">\mathbf{\beta}_0^{\mathbf{2}}</code> for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>theta2</code></td>
<td>
<p>Estimate of <code class="reqn">\theta_0^{\mathbf{2}}</code> for each value of the sequence <code>wn</code> (i.e. its coefficients in the B-spline basis).</p>
</td></tr> 
<tr><td><code>indexes.beta.nonnull2</code></td>
<td>
<p>Indexes of the non-zero linear coefficients after the step 2 of the method for each value of the sequence <code>wn</code>.</p>
</td></tr> 
<tr><td><code>knn2</code></td>
<td>
<p>Selected number of neighbours in the second step of the algorithm for each value of the sequence <code>wn</code>.</p>
</td></tr> 
<tr><td><code>IC2</code></td>
<td>
<p>Optimal value of the criterion function in the second step for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>lambda2</code></td>
<td>
<p>Selected value of penalisation parameter in the second step for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>index02</code></td>
<td>
<p>Indexes of the covariates (in the whole set of <code class="reqn">p_n</code>) used to build <code class="reqn">\mathcal{R}_n^{\mathbf{2}}</code> for each value of the sequence <code>wn</code>.</p>
</td></tr> 
<tr><td><code>beta1</code></td>
<td>
<p>Estimate of <code class="reqn">\mathbf{\beta}_0^{\mathbf{1}}</code> for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>theta1</code></td>
<td>
<p>Estimate of <code class="reqn">\theta_0^{\mathbf{1}}</code> for each value of the sequence <code>wn</code> (i.e. its coefficients in the B-spline basis).</p>
</td></tr>
<tr><td><code>knn1</code></td>
<td>
<p>Selected number of neighbours in the first step of the algorithm for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>IC1</code></td>
<td>
<p>Optimal value of the criterion function in the first step for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>lambda1</code></td>
<td>
<p>Selected value of penalisation parameter in the first step for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>index01</code></td>
<td>
<p>Indexes of the covariates (in the whole set of <code class="reqn">p_n</code>) used to build <code class="reqn">\mathcal{R}_n^{\mathbf{1}}</code> for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>index1</code></td>
<td>
<p>Indexes of the non-zero linear coefficients after the step 1 of the method for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>...</code></td>
<td>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>German Aneiros Perez <a href="mailto:german.aneiros@udc.es">german.aneiros@udc.es</a> 
</p>
<p>Silvia Novo Diaz  <a href="mailto:snovo@est-econ.uc3m.es">snovo@est-econ.uc3m.es</a>
</p>


<h3>References</h3>

<p>Novo, S., Vieu, P., and Aneiros, G., (2021) Fast and efficient algorithms for
sparse semiparametric bi-functional regression. <em>Australian and New Zealand
Journal of Statistics</em>, <b>63</b>, 606&ndash;638, <a href="https://doi.org/10.1111/anzs.12355">doi:10.1111/anzs.12355</a>.
</p>


<h3>See Also</h3>

<p>See also <code><a href="#topic+sfplsim.kNN.fit">sfplsim.kNN.fit</a>, <a href="#topic+predict.IASSMR.kNN">predict.IASSMR.kNN</a></code>, <code><a href="#topic+plot.IASSMR.kNN">plot.IASSMR.kNN</a></code> and <code><a href="#topic+FASSMR.kNN.fit">FASSMR.kNN.fit</a></code>.
</p>
<p>Alternative method <code><a href="#topic+IASSMR.kernel.fit">IASSMR.kernel.fit</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(Sugar)


y&lt;-Sugar$ash
x&lt;-Sugar$wave.290
z&lt;-Sugar$wave.240

#Outliers
index.y.25 &lt;- y &gt; 25
index.atip &lt;- index.y.25
(1:268)[index.atip]


#Dataset to model
x.sug &lt;- x[!index.atip,]
z.sug&lt;- z[!index.atip,]
y.sug &lt;- y[!index.atip]

train&lt;-1:216

ptm=proc.time()
fit&lt;- IASSMR.kNN.fit(x=x.sug[train,],z=z.sug[train,], y=y.sug[train],
        train.1=1:108,train.2=109:216,nknot.theta=2,lambda.min.h=0.07, 
        lambda.min.l=0.07, max.knn=20, nknot=20,criterion="BIC", 
        penalty="grSCAD", max.iter=5000)
proc.time()-ptm

fit 
names(fit)

    
 
</code></pre>

<hr>
<h2 id='lm.pels.fit'>
Linear model fit
</h2><span id='topic+lm.pels.fit'></span>

<h3>Description</h3>

<p>This function fits a sparse  linear model between a scalar response and a vector of scalar covariates. 
The function uses the penalised least-squares regularization procedure.
The method requires an objective criterion (<code>criterion</code>) to select the regularization parameter (<code>lambda.opt</code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lm.pels.fit(z, y, lambda.min = NULL, lambda.min.h = NULL, 
  lambda.min.l = NULL, factor.pn =1, nlambda = 100,lambda.seq = NULL,
  vn = ncol(z), nfolds = 10, seed = 123, criterion = c("GCV", "BIC", 
  "AIC", "k-fold-CV"), penalty = c("grLasso", "grMCP",
  "grSCAD", "gel", "cMCP", "gBridge", "gLasso", "gMCP"), 
  max.iter = 1000)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lm.pels.fit_+3A_z">z</code></td>
<td>

<p>Matrix containing the observations of the covariates collected by row.
</p>
</td></tr>
<tr><td><code id="lm.pels.fit_+3A_y">y</code></td>
<td>

<p>Vector containing the scalar response.
</p>
</td></tr>
<tr><td><code id="lm.pels.fit_+3A_lambda.min">lambda.min</code></td>
<td>

<p>The smallest value for lambda (i. e., the smallest value  of the sequence in which <code>lambda.opt</code> is selected), as fraction of <code>lambda.max</code>.
The defaults is <code>lambda.min.l</code> if the number of observations is larger than <code>factor.pn</code> times the number of covariates and <code>lambda.min.h</code> otherwise.
</p>
</td></tr>
<tr><td><code id="lm.pels.fit_+3A_lambda.min.h">lambda.min.h</code></td>
<td>

<p>The smallest value of the sequence in which <code>lambda.opt</code> is selected if the number of observations is smaller than <code>factor.pn</code> times the number of scalar covariates. The default is 0.05. 
</p>
</td></tr>
<tr><td><code id="lm.pels.fit_+3A_lambda.min.l">lambda.min.l</code></td>
<td>

<p>The smallest value of the sequence in which <code>lambda.opt</code> is selected if the number of observations is larger than <code>factor.pn</code> times the number of scalar covariates. The default is 0.0001.
</p>
</td></tr>
<tr><td><code id="lm.pels.fit_+3A_factor.pn">factor.pn</code></td>
<td>

<p>Positive integer used to set <code>lambda.min</code>. The default is 1.
</p>
</td></tr>
<tr><td><code id="lm.pels.fit_+3A_nlambda">nlambda</code></td>
<td>

<p>Positive integer indicating the number of values of the sequence in which <code>lambda.opt</code> is selected. The default is 100.
</p>
</td></tr>
<tr><td><code id="lm.pels.fit_+3A_lambda.seq">lambda.seq</code></td>
<td>

<p>Sequence of values in which <code>lambda.opt</code> is selected. If <code>lambda.seq=NULL</code>, then the programme builds the sequence automatically using <code>lambda.min</code> and <code>nlambda</code>.
</p>
</td></tr>
<tr><td><code id="lm.pels.fit_+3A_vn">vn</code></td>
<td>

<p>Positive integer or vector of positive integers indicating the number of groups of consecutive variables to be penalised together. The default value is <code>vn=ncol(z)</code>, which leads to the individual penalisation of each scalar covariate.
</p>
</td></tr>
<tr><td><code id="lm.pels.fit_+3A_nfolds">nfolds</code></td>
<td>

<p>Positive integer indicating the number of cross-validation folds (used if <code>criterion="k-fold-CV"</code>). The default is 10.
</p>
</td></tr>
<tr><td><code id="lm.pels.fit_+3A_seed">seed</code></td>
<td>

<p>You may set the seed of the random number generator to obtain reproducible results (used if <code>criterion="k-fold-CV"</code>). The default is 123.
</p>
</td></tr>
<tr><td><code id="lm.pels.fit_+3A_criterion">criterion</code></td>
<td>

<p>The criterion by which to select the regularization parameter <code>lambda.opt</code> and <code>k.opt</code>. One of <code>"GCV", "BIC", "AIC"</code> or <code>"k-fold-CV"</code>. The default is <code>"GCV"</code>.
</p>
</td></tr>
<tr><td><code id="lm.pels.fit_+3A_penalty">penalty</code></td>
<td>

<p>The penalty function to be applied in the penalized least squares procedure. Only &quot;grLasso&quot; and &quot;grSCAD&quot; are implemented. 
</p>
</td></tr>
<tr><td><code id="lm.pels.fit_+3A_max.iter">max.iter</code></td>
<td>

<p>Maximum number of iterations (total across entire path). The default is 1000.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The sparse linear model (SLM) is given by the expression:
</p>
<p style="text-align: center;"><code class="reqn">
Y_i=Z_{i1}\beta_{01}+\dots+Z_{ip_n}\beta_{0p_n}+\varepsilon_i\ \ \ i=1,\dots,n,
</code>
</p>

<p>where <code class="reqn">Y_i</code> denotes a scalar response, <code class="reqn">Z_{i1},\dots,Z_{ip_n}</code> are real random covariates. In this equation, <code class="reqn">\mathbf{\beta}_0=(\beta_{01},\dots,\beta_{0p_n})^{\top}</code> is a vector of unknown real parameters and <code class="reqn">\varepsilon_i</code> is the random error.
</p>
<p>In this function, the SLM is fitted using the penalised least-squares approach by minimising 
</p>
<p style="text-align: center;"><code class="reqn">
\mathcal{Q}\left(\mathbf{\beta}\right)=\frac{1}{2}\left(\mathbf{Y}-\mathbf{Z}\mathbf{\beta}\right)^{\top}\left(\mathbf{Y}-\mathbf{Z}\mathbf{\beta}\right)+n\sum_{j=1}^{p_n}\mathcal{P}_{\lambda_{j_n}}\left(|\beta_j|\right), \quad (1)
</code>
</p>

<p>where <code class="reqn">\mathbf{\beta}=(\beta_1,\ldots,\beta_{p_n})^{\top}, \ \mathcal{P}_{\lambda_{j_n}}\left(\cdot\right)</code> is a penalty function (specified in the argument <code>penalty</code>) and <code class="reqn">\lambda_{j_n} &gt; 0</code> is a tuning parameter.
To reduce  the quantity of tuning parameters, <code class="reqn">\lambda_j</code>, to be selected for each sample, we consider <code class="reqn">\lambda_j = \lambda \widehat{\sigma}_{\beta_{0,j,OLS}}</code>, where <code class="reqn">\beta_{0,j,OLS}</code> denotes the OLS estimate of <code class="reqn">\beta_{0,j}</code> and <code class="reqn">\widehat{\sigma}_{\beta_{0,j,OLS}}</code> is the estimated standard deviation; <code class="reqn">\lambda</code> is selected using the objetive criterion specified in the argument <code>criterion</code>.
</p>
<p>For further details on the estimation procedure of the SLM, see, for instance, Fan and Li. (2001) or Fan and Lv (2011).
</p>
<p><b>Remark</b>: We should note that if we set <code>lambda.seq</code><code class="reqn">=0</code>, we can obtain the non-penalised estimation of the model, i.e. the OLS estimation. It is convenient to use <code>lambda.seq</code><code class="reqn">\not=0</code> when one suspects there are irrelevant variables.
</p>


<h3>Value</h3>

<table>
<tr><td><code>call</code></td>
<td>
<p>The matched call.</p>
</td></tr>
<tr><td><code>fitted.values</code></td>
<td>
<p>Estimated scalar response.</p>
</td></tr>
<tr><td><code>residuals</code></td>
<td>
<p>Differences between <code>y</code> and the <code>fitted.values</code></p>
</td></tr>
<tr><td><code>beta.est</code></td>
<td>
<p>Estimate of <code class="reqn">\beta_0</code> when the optimal penalisation parameter <code>lambda.opt</code> and <code>vn.opt</code> are used.</p>
</td></tr>
<tr><td><code>indexes.beta.nonnull</code></td>
<td>
<p>Indexes of the non-zero <code class="reqn">\hat{\beta_{j}}</code>.</p>
</td></tr>
<tr><td><code>lambda.opt</code></td>
<td>
<p>Selected value of lambda.</p>
</td></tr>
<tr><td><code>IC</code></td>
<td>
<p>Value of the criterion function considered to select <code>lambda.opt</code> and <code>vn.opt</code>.</p>
</td></tr>
<tr><td><code>vn.opt</code></td>
<td>
<p>Selected value of <code>vn</code>.</p>
</td></tr>
<tr><td><code>...</code></td>
<td>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>German Aneiros Perez <a href="mailto:german.aneiros@udc.es">german.aneiros@udc.es</a> 
</p>
<p>Silvia Novo Diaz  <a href="mailto:snovo@est-econ.uc3m.es">snovo@est-econ.uc3m.es</a>
</p>


<h3>References</h3>

<p>Fan, J., and Li, R. (2001) Variable selection via nonconcave penalized
likelihood and its oracle properties. <em>Journal of the American Statistical Association</em>, <b>96</b>, 1348&ndash;1360, <a href="https://doi.org/10.1198/016214501753382273">doi:10.1198/016214501753382273</a>.
</p>
<p>Fan, J., and  Lv, J. (2011) Nonconcave penalized likelihood with NP-dimensionality, <em>IEEE Transactions on Information Theory</em>, <b>57(8)</b>, 5467&ndash;5484, <a href="https://ieeexplore.ieee.org/document/5961830">https://ieeexplore.ieee.org/document/5961830</a>.
</p>


<h3>See Also</h3>

<p>See also <code><a href="#topic+PVS.fit">PVS.fit</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data("Tecator")
y&lt;-Tecator$fat
z1&lt;-Tecator$protein       
z2&lt;-Tecator$moisture

#Quadratic, cubic and interaction effects of the scalar covariates.
z.com&lt;-cbind(z1,z2,z1^2,z2^2,z1^3,z2^3,z1*z2)
train&lt;-1:160


#LM fit. 
ptm=proc.time()
fit&lt;-lm.pels.fit(z=z.com[train,], y=y[train],lambda.min.h=0.02,
      lambda.min.l=0.01,factor.pn=2, max.iter=5000, criterion="BIC",
      penalty="grSCAD")
proc.time()-ptm

#Results
fit
names(fit)


  
</code></pre>

<hr>
<h2 id='plot.classes'>
Plot outputs from regression estimation methods
</h2><span id='topic+plot.FASSMR.kernel'></span><span id='topic+plot.FASSMR.kNN'></span><span id='topic+plot.fsim.kernel'></span><span id='topic+plot.fsim.kNN'></span><span id='topic+plot.IASSMR.kernel'></span><span id='topic+plot.IASSMR.kNN'></span><span id='topic+plot.lm.pels'></span><span id='topic+plot.PVS'></span><span id='topic+plot.PVS.kernel'></span><span id='topic+plot.PVS.kNN'></span><span id='topic+plot.sfpl.kernel'></span><span id='topic+plot.sfpl.kNN'></span><span id='topic+plot.sfplsim.kernel'></span><span id='topic+plot.sfplsim.kNN'></span>

<h3>Description</h3>

<p><code>plot</code> function for <code>FASSMR.kernel.fit</code>, <code>FASSMR.kNN.fit</code>,  <code>fsim.kernel.fit</code>, <code>fsim.kNN.fit</code>, <code>IASSMR.kernel.fit</code>, <code>IASSMR.kNN.fit</code>, <code>lm.pels.fit</code>, <code>PVS.fit</code>, <code>PVS.kernel.fit</code>, <code>PVS.kNN.fit</code>, <code>sfpl.kernel.fit</code>, <code>sfpl.kNN.fit</code>,<code>sfplsim.kernel.fit</code> and <code>sfplsim.kNN.fit</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'FASSMR.kernel'
plot(x, cex.axis = 1.5, cex.lab = 1.5, cex = 2, col = 1, cex.main = 1.5, ...)

## S3 method for class 'FASSMR.kNN'
plot(x, cex.axis = 1.5, cex.lab = 1.5, cex = 2, col = 1, cex.main = 1.5, ...)

## S3 method for class 'fsim.kernel'
plot(x, cex.axis = 1.5, cex.lab = 1.5, cex = 2, col = 1, cex.main = 1.5, ...)

## S3 method for class 'fsim.kNN'
plot(x, cex.axis = 1.5, cex.lab = 1.5, cex = 2, col = 1, cex.main = 1.5, ...)

## S3 method for class 'IASSMR.kernel'
plot(x, cex.axis = 1.5, cex.lab = 1.5, cex = 2, col = 1, cex.main = 1.5, ...)

## S3 method for class 'IASSMR.kNN'
plot(x, cex.axis = 1.5, cex.lab = 1.5, cex = 2, col = 1, cex.main = 1.5, ...)

## S3 method for class 'lm.pels'
plot(x, cex.axis = 1.5, cex.lab = 1.5, cex = 2, col = 1, cex.main = 1.5, ...)

## S3 method for class 'PVS'
plot(x, cex.axis = 1.5, cex.lab = 1.5, cex = 2, col = 1, cex.main = 1.5, ...)

## S3 method for class 'PVS.kernel'
plot(x, cex.axis = 1.5, cex.lab = 1.5, cex = 2, col = 1, cex.main = 1.5, ...)

## S3 method for class 'PVS.kNN'
plot(x, cex.axis = 1.5, cex.lab = 1.5, cex = 2, col = 1, cex.main = 1.5, ...)

## S3 method for class 'sfpl.kernel'
plot(x, cex.axis = 1.5, cex.lab = 1.5, cex = 2, col = 1, cex.main = 1.5, ...)

## S3 method for class 'sfpl.kNN'
plot(x, cex.axis = 1.5, cex.lab = 1.5, cex = 2, col = 1, cex.main = 1.5, ...)

## S3 method for class 'sfplsim.kernel'
plot(x, cex.axis = 1.5, cex.lab = 1.5, cex = 2, col = 1, cex.main = 1.5, ...)

## S3 method for class 'sfplsim.kNN'
plot(x, cex.axis = 1.5, cex.lab = 1.5, cex = 2, col = 1, cex.main = 1.5, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.classes_+3A_x">x</code></td>
<td>

<p>Output of the functions mentioned in the <code>Description</code> (i.e. an object of the class <code>FASSMR.kernel</code>, <code>FASSMR.kNN</code>, <code>fsim.kernel</code>,<code>fsim.kNN</code>, <code>IASSMR.kernel</code>, <code>IASSMR.kNN</code>, <code>lm.pels</code>, <code>PVS</code>, <code>PVS.kernel</code>, <code>PVS.kNN</code>, <code>sfpl.kernel</code>,<code>sfpl.kNN</code>, <code>sfplsim.kernel</code> or <code>sfplsim.kNN</code>).
</p>
</td></tr>
<tr><td><code id="plot.classes_+3A_cex.axis">cex.axis</code></td>
<td>

<p>The magnification to be used for axis annotation relative to the current setting of <code>cex</code>. The default is 1.5.
</p>
</td></tr>
<tr><td><code id="plot.classes_+3A_cex.lab">cex.lab</code></td>
<td>

<p>The magnification to be used for x and y labels relative to the current setting of <code>cex</code>. The default is 1.5.
</p>
</td></tr>
<tr><td><code id="plot.classes_+3A_cex">cex</code></td>
<td>

<p>A numerical value giving the amount by which plotting text and symbols should be magnified. The default is 2.
</p>
</td></tr>
<tr><td><code id="plot.classes_+3A_col">col</code></td>
<td>

<p>A specification for the default plotting color. The default is <code>color=1</code>.
</p>
</td></tr>
<tr><td><code id="plot.classes_+3A_cex.main">cex.main</code></td>
<td>

<p>The magnification to be used for main titles relative to the current setting of <code>cex</code>. The default is 1.5.
</p>
</td></tr>
<tr><td><code id="plot.classes_+3A_...">...</code></td>
<td>
<p>Further arguments passed to or from other methods.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The functions return different graphical representations.
</p>

<ul>
<li><p> For the classes <code>fsim.kNN</code> and <code>fsim.kernel</code>:
</p>

<ol>
<li><p> The estimated functional index: <code class="reqn">\hat{\theta}</code>.
</p>
</li>
<li><p> The regression fit.
</p>
</li></ol>

</li>
<li><p> For the classes <code>FASSMR.kernel</code>, <code>FASSMR.kNN</code>, <code>IASSMR.kernel</code>, <code>IASSMR.kNN</code>, <code>sfplsim.kernel</code> and <code>sfplsim.kNN</code>.
</p>

<ol>
<li><p> The response over the <code>fitted.values</code>.
</p>
</li>
<li><p> The <code>residuals</code> over the <code>fitted.values</code>.
</p>
</li>
<li><p> The estimated functional index: <code class="reqn">\hat{\theta}</code>.
</p>
</li></ol>

</li>
<li><p> For the classes <code>lm.pels</code>, <code>PVS</code>, <code>PVS.kernel</code>, <code>PVS.kNN</code>, <code>sfpl.kernel</code> and <code>sfpl.kNN</code>.
</p>

<ol>
<li><p> The response over the <code>fitted.values</code>.
</p>
</li>
<li><p> The <code>residuals</code> over the <code>fitted.values</code>.
</p>
</li></ol>

</li></ul>



<h3>Author(s)</h3>

<p>German Aneiros Perez <a href="mailto:german.aneiros@udc.es">german.aneiros@udc.es</a> 
</p>
<p>Silvia Novo Diaz  <a href="mailto:snovo@est-econ.uc3m.es">snovo@est-econ.uc3m.es</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+FASSMR.kernel.fit">FASSMR.kernel.fit</a></code>, <code><a href="#topic+FASSMR.kNN.fit">FASSMR.kNN.fit</a></code>,  <code><a href="#topic+fsim.kernel.fit">fsim.kernel.fit</a></code>, <code><a href="#topic+fsim.kNN.fit">fsim.kNN.fit</a></code>, <code><a href="#topic+IASSMR.kernel.fit">IASSMR.kernel.fit</a></code>, <code><a href="#topic+IASSMR.kNN.fit">IASSMR.kNN.fit</a></code>, <code><a href="#topic+lm.pels.fit">lm.pels.fit</a></code>, <code><a href="#topic+PVS.fit">PVS.fit</a></code>, <code><a href="#topic+PVS.kernel.fit">PVS.kernel.fit</a></code>, <code><a href="#topic+PVS.kNN.fit">PVS.kNN.fit</a></code>, <code><a href="#topic+sfpl.kernel.fit">sfpl.kernel.fit</a></code>, <code><a href="#topic+sfpl.kNN.fit">sfpl.kNN.fit</a></code>, <code><a href="#topic+sfplsim.kernel.fit">sfplsim.kernel.fit</a></code> and <code><a href="#topic+sfplsim.kNN.fit">sfplsim.kNN.fit</a></code>.
</p>

<hr>
<h2 id='predict.fsim'>
Prediction from functional single-index model estimates
</h2><span id='topic+predict.fsim.kernel'></span><span id='topic+predict.fsim.kNN'></span>

<h3>Description</h3>

<p><code>predict</code> method for functional single-index regression fitted using <code>fsim.kernel.fit</code> or <code>fsim.kNN.fit</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'fsim.kernel'
predict(object, newdata = NULL, y.test = NULL, ...)
## S3 method for class 'fsim.kNN'
predict(object, newdata = NULL, y.test = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.fsim_+3A_object">object</code></td>
<td>

<p>Output of the <code>fsim.kernel.fit</code> or <code>fsim.kNN.fit</code> functions (i.e. an object of the class <code>fsim.kernel</code> or <code>fsim.kNN</code>).
</p>
</td></tr>
<tr><td><code id="predict.fsim_+3A_newdata">newdata</code></td>
<td>

<p>A matrix containing new observations of the functional covariate collected by row.
</p>
</td></tr>
<tr><td><code id="predict.fsim_+3A_y.test">y.test</code></td>
<td>

<p>(optional) A vector containing the new observations of the response.
</p>
</td></tr>
<tr><td><code id="predict.fsim_+3A_...">...</code></td>
<td>
<p>Further arguments passed to or from other methods.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The prediction is computed using the functions <code>fsim.kernel.test</code> and <code>fsim.kernel.fit</code>, respectively.
</p>


<h3>Value</h3>

<p>The function returns the predicted values of the response (<code>y</code>) for <code>newdata</code>. If <code>!is.null(y.test)</code>, it also provides the mean squared error of prediction (<code>MSEP</code>) computed as <code>mean((y-y.test)^2)</code>.
If <code>is.null(newdata)</code> the function returns the fitted values.
</p>


<h3>Author(s)</h3>

<p>German Aneiros Perez <a href="mailto:german.aneiros@udc.es">german.aneiros@udc.es</a> 
</p>
<p>Silvia Novo Diaz  <a href="mailto:snovo@est-econ.uc3m.es">snovo@est-econ.uc3m.es</a>
</p>


<h3>See Also</h3>

<p><code>fsim.kernel.fit</code> and  <code>fsim.kernel.test</code>  or <code>fsim.kNN.fit</code> and <code>fsim.kNN.test</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(Tecator)
y&lt;-Tecator$fat
X&lt;-Tecator$absor.spectra2

train&lt;-1:160
test&lt;-161:215


#FSIM fit. 
fit.kernel&lt;-fsim.kernel.fit(y[train],x=X[train,],max.q.h=0.35, nknot=20,
range.grid=c(850,1050),nknot.theta=4)
fit.kNN&lt;-fsim.kNN.fit(y=y[train],x=X[train,],max.knn=20,nknot.theta=4,
nknot=20,range.grid=c(850,1050))

test&lt;-161:215

pred.kernel&lt;-predict(fit.kernel,newdata=X[test,],y.test=y[test])
pred.kernel$MSEP
pred.kNN&lt;-predict(fit.kNN,newdata=X[test,],y.test=y[test])
pred.kNN$MSEP

</code></pre>

<hr>
<h2 id='predict.IASSMR'>
Prediction from multi-functional partial linear single-index model
</h2><span id='topic+predict.IASSMR.kernel'></span><span id='topic+predict.IASSMR.kNN'></span>

<h3>Description</h3>

<p><code>predict</code> method for the multi-functional partial linear single-index model fitted using <code>IASSMR.kernel.fit</code> or <code>IASSMR.kNN.fit</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
## S3 method for class 'IASSMR.kernel'
predict(object, newdata.x = NULL, newdata.z = NULL,
  y.test = NULL, option = NULL, ...)
## S3 method for class 'IASSMR.kNN'
predict(object, newdata.x = NULL, newdata.z = NULL,
  y.test = NULL, option = NULL, knearest.n = object$knearest, 
  min.knn.n = object$min.knn, max.knn.n = object$max.knn.n, 
  step.n = object$step, ...)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.IASSMR_+3A_object">object</code></td>
<td>

<p>Output of the functions mentioned in the <code>Description</code> (i.e. an object of the class <code>IASSMR.kernel</code> or <code>IASSMR.kNN</code>).
</p>
</td></tr>
<tr><td><code id="predict.IASSMR_+3A_newdata.x">newdata.x</code></td>
<td>

<p>A matrix containing new observations of the functional covariate in the functional single-index component collected by row.
</p>
</td></tr>
<tr><td><code id="predict.IASSMR_+3A_newdata.z">newdata.z</code></td>
<td>

<p>Matrix containing the new observations of the scalar covariates coming from the discretisation  of a curve  collected by row. 
</p>
</td></tr>
<tr><td><code id="predict.IASSMR_+3A_y.test">y.test</code></td>
<td>

<p>(optional) A vector containing the new observations of the response.
</p>
</td></tr>
<tr><td><code id="predict.IASSMR_+3A_option">option</code></td>
<td>

<p>Allows the choice between 1, 2 and 3. The default is 1. See the section <code>Details</code>.
</p>
</td></tr>
<tr><td><code id="predict.IASSMR_+3A_...">...</code></td>
<td>

<p>Further arguments.
</p>
</td></tr>
<tr><td><code id="predict.IASSMR_+3A_knearest.n">knearest.n</code></td>
<td>

<p>Only used for objects <code>IASSMR.kNN</code> if <code>option=2</code> or <code>option=3</code>: vector of positive integers containing the sequence in which the  number of nearest neighbours <code>k.opt</code> is selected. The default is <code>object$knearest</code>.
</p>
</td></tr>
<tr><td><code id="predict.IASSMR_+3A_min.knn.n">min.knn.n</code></td>
<td>

<p>Only used for objects <code>IASSMR.kNN</code> if <code>option=2</code> or <code>option=3</code>: positive integer indicating the minumum value of the sequence in which the  number of nearest neighbours <code>k.opt</code> is selected (thus, this number must be smaller than the sample size). The default is <code>object$min.knn</code>.
</p>
</td></tr>
<tr><td><code id="predict.IASSMR_+3A_max.knn.n">max.knn.n</code></td>
<td>

<p>Only used for objects <code>IASSMR.kNN</code> if <code>option=2</code> or <code>option=3</code>: positive integer indicating the maximum value of the sequence in which the number of nearest neighbours <code>k.opt</code> is selected (thus, this number must be larger than <code>min.kNN</code> and smaller than the sample size). The default is <code>object$max.knn</code>.
</p>
</td></tr>
<tr><td><code id="predict.IASSMR_+3A_step.n">step.n</code></td>
<td>

<p>Only used for objects <code>IASSMR.kNN</code> if <code>option=2</code> or <code>option=3</code>: positive integer used to build the sequence of k-nearest neighbours in the following way: <code>min.knn, min.knn + step, min.knn + 2*step, min.knn + 3*step,...</code>. The default is  <code>object$step</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>To obtain the predictions of the response for <code>newdata.x</code> and <code>newdata.z</code>, three options are provided:
</p>

<ul>
<li><p> If <code>option=1</code>, we mantain all the estimates (<code>k.opt</code> or <code>h.opt</code>, <code>theta.est</code> and <code>beta.est</code>) to predict the functional single-index component of the model. As we use the estimates of the second step of the algorithm, only the <code>train.2</code> is used as training sample to predict.
Then, it should be noted that <code>k.opt</code> or <code>h.opt</code> could not be suitable to predict the functional single-index component of the model.
</p>
</li>
<li><p> If <code>option=2</code>, we mantain <code>theta.est</code> and <code>beta.est</code>, while the tunning parameter (<code class="reqn">h</code> or <code class="reqn">k</code>) is seleted again to predict the functional single-index component of the model. This selection is performed using the cross-validation criterion in the functional single-index model associated and the complete training sample (i.e. <code>train=c(train.1,train.2)</code>). As we use the whole training sample (not just a subsample of it), the sample size is modified and, as a consequence,  the parameters <code>knearest</code>, <code>min.knn</code>, <code>max.knn</code>, <code>step</code> given to the function <code>IASSMR.kNN.fit</code> may need to be provided again to compute predictions. For that, we add the arguments <code>knearest.n</code>, <code>min.knn.n</code>, <code>max.knn.n</code>, <code>step.mn</code>.
</p>
</li>
<li><p>  If <code>option=3</code>, we mantain only the indexes of the relevant variables selected by the IASSMR. We estimate again the linear coefficients and the functional index  by means of <code>sfplsim.kernel.fit</code> or <code>sfplsim.kNN.fit</code>, respectively, without penalisation (setting <code>lambda.seq=0</code>) and using the whole training sample (<code>train=c(train.1,train.2)</code>). The method provides two predictions (and MSEPs):
</p>

<ul>
<li><p> a) The prediction associated to <code>option=1</code> for <code>sfplsim.kernel</code> or <code>sfplsim.kNN</code> class.
</p>
</li>
<li><p> b) The prediction associated to <code>option=2</code> for <code>sfplsim.kernel</code> or <code>sfplsim.kNN</code> class.
</p>
</li></ul>

<p>(see the documentation of the functions <code>predict.sfplsim.kernel</code> and <code>predict.sfplsim.kNN</code>)
</p>
</li></ul>



<h3>Value</h3>

<p>The function returns the predicted values of the response (<code>y</code>) for <code>newdata.x</code> and <code>newdata.z</code>. If <code>!is.null(y.test)</code>, it also provides the mean squared error of prediction (<code>MSEP</code>) computed as <code>mean((y-y.test)^2)</code>.
If <code>option=3</code> two sets of predictions are provided (and two MSEP), in correspondence with the items a) and b) mentioned in the section <code>Details.</code>
If <code>is.null(newdata.x)</code> or <code>is.null(newdata.z)</code>,  the function returns the fitted values.
</p>


<h3>Author(s)</h3>

<p>German Aneiros Perez <a href="mailto:german.aneiros@udc.es">german.aneiros@udc.es</a> 
</p>
<p>Silvia Novo Diaz  <a href="mailto:snovo@est-econ.uc3m.es">snovo@est-econ.uc3m.es</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+sfplsim.kernel.fit">sfplsim.kernel.fit</a></code>, <code><a href="#topic+sfplsim.kNN.fit">sfplsim.kNN.fit</a></code>, <code><a href="#topic+IASSMR.kernel.fit">IASSMR.kernel.fit</a></code>, <code><a href="#topic+IASSMR.kNN.fit">IASSMR.kNN.fit</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(Sugar)


y&lt;-Sugar$ash
x&lt;-Sugar$wave.290
z&lt;-Sugar$wave.240

#Outliers
index.y.25 &lt;- y &gt; 25
index.atip &lt;- index.y.25
(1:268)[index.atip]


#Dataset to model
x.sug &lt;- x[!index.atip,]
z.sug&lt;- z[!index.atip,]
y.sug &lt;- y[!index.atip]

train&lt;-1:216
test&lt;-217:266

#Fit
fit.kernel&lt;-IASSMR.kernel.fit(x=x.sug[train,],z=z.sug[train,], y=y.sug[train],
            train.1=1:108,train.2=109:216,nknot.theta=2,lambda.min.h=0.03,
            lambda.min.l=0.03,  max.q.h=0.35, num.h = 10, nknot=20,
            criterion="BIC", penalty="grSCAD", max.iter=5000)

fit.kNN&lt;- IASSMR.kNN.fit(x=x.sug[train,],z=z.sug[train,], y=y.sug[train],
          train.1=1:108,train.2=109:216,nknot.theta=2,lambda.min.h=0.07,
          lambda.min.l=0.07, max.knn=20, nknot=20,criterion="BIC",
          penalty="grSCAD", max.iter=5000)


#Predictions
predict(fit.kernel,newdata.x=x.sug[test,],newdata.z=z.sug[test,],y.test=y.sug[test],option=2)
predict(fit.kNN,newdata.x=x.sug[test,],newdata.z=z.sug[test,],y.test=y.sug[test],option=2)

</code></pre>

<hr>
<h2 id='predict.lm'>
Prediction from linear model estimates
</h2><span id='topic+predict.PVS'></span><span id='topic+predict.lm.pels'></span>

<h3>Description</h3>

<p><code>predict</code> method for: 
</p>

<ul>
<li><p> Linear model (LM) fitted using <code>lm.pels.fit</code>. 
</p>
</li>
<li><p> Linear model with covariates coming from the discretization of a curve fitted using <code>PVS.fit</code>.
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'lm.pels'
predict(object, newdata = NULL, y.test = NULL, ...)
## S3 method for class 'PVS'
predict(object, newdata = NULL, y.test = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.lm_+3A_object">object</code></td>
<td>

<p>Output of the <code>lm.pels.fit</code> or <code>PVS.fit</code> functions (i.e. an object of the class <code>lm.pels</code> or <code>PVS</code>)
</p>
</td></tr>
<tr><td><code id="predict.lm_+3A_newdata">newdata</code></td>
<td>

<p>Matrix containing the new observations of the scalar covariates (LM) or of the scalar covariates coming from the discretisation  of a curve, collected by row.
</p>
</td></tr>
<tr><td><code id="predict.lm_+3A_y.test">y.test</code></td>
<td>

<p>(optional) A vector containing the new observations of the response.
</p>
</td></tr>
<tr><td><code id="predict.lm_+3A_...">...</code></td>
<td>
<p>Further arguments passed to or from other methods.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The function returns the predicted values of the response (<code>y</code>) for <code>newdata</code>. If <code>!is.null(y.test)</code>, it also provides the mean squared error of prediction (<code>MSEP</code>) computed as <code>mean((y-y.test)^2)</code>.
If <code>is.null(newdata)</code> the function returns the fitted values.
</p>


<h3>Author(s)</h3>

<p>German Aneiros Perez <a href="mailto:german.aneiros@udc.es">german.aneiros@udc.es</a> 
</p>
<p>Silvia Novo Diaz  <a href="mailto:snovo@est-econ.uc3m.es">snovo@est-econ.uc3m.es</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+lm.pels.fit">lm.pels.fit</a></code> and <code><a href="#topic+PVS.fit">PVS.fit</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data("Tecator")
y&lt;-Tecator$fat
z1&lt;-Tecator$protein       
z2&lt;-Tecator$moisture

#Quadratic, cubic and interaction effects of the scalar covariates.
z.com&lt;-cbind(z1,z2,z1^2,z2^2,z1^3,z2^3,z1*z2)
train&lt;-1:160
test&lt;-161:215

#LM fit. 
fit&lt;-lm.pels.fit(z=z.com[train,], y=y[train],lambda.min.h=0.02,lambda.min.l=0.01,
      factor.pn=2, max.iter=5000, criterion="BIC", penalty="grSCAD")

#Predictions
predict(fit,newdata=z.com[test,],y.test=y[test])


data(Sugar)

y&lt;-Sugar$ash
z&lt;-Sugar$wave.240

#Outliers
index.y.25 &lt;- y &gt; 25
index.atip &lt;- index.y.25
(1:268)[index.atip]


#Dataset to model
z.sug&lt;- z[!index.atip,]
y.sug &lt;- y[!index.atip]

train&lt;-1:216
test&lt;-217:266

#Fit
fit.pvs&lt;-PVS.fit(z=z.sug[train,], y=y.sug[train],train.1=1:108,train.2=109:216,
          lambda.min.h=0.2,criterion="BIC", penalty="grSCAD", max.iter=5000)


#Predictions
predict(fit.pvs,newdata=z.sug[test,],y.test=y.sug[test])


</code></pre>

<hr>
<h2 id='predict.mfplm'>
Prediction from multi-functional partial linear model
</h2><span id='topic+predict.PVS.kernel'></span><span id='topic+predict.PVS.kNN'></span>

<h3>Description</h3>

<p><code>predict</code> method for the multi-functional partial linear model fitted using <code>PVS.kernel.fit</code> or <code>PVS.kNN.fit</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'PVS.kernel'
predict(object, newdata.x = NULL, newdata.z = NULL,
  y.test = NULL, option = NULL, ...)
## S3 method for class 'PVS.kNN'
predict(object, newdata.x = NULL, newdata.z = NULL, 
  y.test = NULL, option = NULL, knearest.n = object$knearest, 
  min.knn.n = object$min.knn, max.knn.n = object$max.knn.n, 
  step.n = object$step, ...)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.mfplm_+3A_object">object</code></td>
<td>

<p>Output of the functions mentioned in the <code>Description</code> (i.e. an object of the class <code>PVS.kernel</code> or <code>PVS.kNN</code>).
</p>
</td></tr>
<tr><td><code id="predict.mfplm_+3A_newdata.x">newdata.x</code></td>
<td>

<p>A matrix containing new observations of the functional covariate in the functional nonparametric component collected by row.
</p>
</td></tr>
<tr><td><code id="predict.mfplm_+3A_newdata.z">newdata.z</code></td>
<td>

<p>Matrix containing the new observations of the scalar covariates coming from the discretisation  of a curve  collected by row. 
</p>
</td></tr>
<tr><td><code id="predict.mfplm_+3A_y.test">y.test</code></td>
<td>

<p>(optional) A vector containing the new observations of the response.
</p>
</td></tr>
<tr><td><code id="predict.mfplm_+3A_option">option</code></td>
<td>

<p>Allows the choice between 1, 2 and 3 in the case of <code>PVS.kernel</code> objects and between 1, 2, 3, and 4 in <code>PVS.kNN</code> objects. The default is 1. See the section <code>Details</code>.
</p>
</td></tr>
<tr><td><code id="predict.mfplm_+3A_...">...</code></td>
<td>

<p>Further arguments.
</p>
</td></tr>
<tr><td><code id="predict.mfplm_+3A_knearest.n">knearest.n</code></td>
<td>

<p>Only used for objects <code>PVS.kNN</code> if <code>option=2</code>, <code>option=3</code> or <code>option=4</code>: vector of positive integers containing the sequence in which the  number of nearest neighbours <code>k.opt</code> is selected. The default is <code>object$knearest</code>.
</p>
</td></tr>
<tr><td><code id="predict.mfplm_+3A_min.knn.n">min.knn.n</code></td>
<td>

<p>Only used for objects <code>PVS.kNN</code> if <code>option=2</code>, <code>option=3</code> or <code>option=4</code>: positive integer indicating the minumum value of the sequence in which the  number of nearest neighbours <code>k.opt</code> is selected (thus, this number must be smaller than the sample size). The default is <code>object$min.knn</code>.
</p>
</td></tr>
<tr><td><code id="predict.mfplm_+3A_max.knn.n">max.knn.n</code></td>
<td>

<p>Only used for objects <code>PVS.kNN</code> if <code>option=2</code>, <code>option=3</code> or <code>option=4</code>: positive integer indicating the maximum value of the sequence in which the number of nearest neighbours <code>k.opt</code> is selected (thus, this number must be larger than <code>min.kNN</code> and smaller than the sample size). The default is <code>object$max.knn</code>.
</p>
</td></tr>
<tr><td><code id="predict.mfplm_+3A_step.n">step.n</code></td>
<td>

<p>Only used for objects <code>PVS.kNN</code> if <code>option=2</code>, <code>option=3</code> or <code>option=4</code>: positive integer used to build the sequence of k-nearest neighbours in the following way: <code>min.knn, min.knn + step, min.knn + 2*step, min.knn + 3*step,...</code>. The default is  <code>object$step</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>To obtain the predictions of the response for <code>newdata.x</code> and <code>newdata.z</code>, the following options are provided:
</p>

<ul>
<li><p> If <code>option=1</code>, we mantain all the estimates (<code>k.opt</code> or <code>h.opt</code> and <code>beta.est</code>) to predict the functional nonparametric component of the model. As we use the estimates of the second step of the algorithm, only the <code>train.2</code> is used as training sample to predict.
Then, it should be noted that <code>k.opt</code> or <code>h.opt</code> could not be suitable to predict the functional nonparametric component of the model.
</p>
</li>
<li><p> If <code>option=2</code>, we mantain <code>beta.est</code>, while the tunning parameter (<code class="reqn">h</code> or <code class="reqn">k</code>) is seleted again to predict the functional nonparametric component of the model. This selection is performed using the cross-validation criterion in the functional nonparametric model associated and the complete training sample (i.e. <code>train=c(train.1,train.2)</code>), obtaining a global selection for <code class="reqn">h</code> or <code class="reqn">k</code>. As we use the whole training sample (not just a subsample of it), the sample size is modified and, as a consequence,  the parameters <code>knearest</code>, <code>min.knn</code>, <code>max.knn</code>, <code>step</code> given to the function <code>IASSMR.kNN.fit</code> may need to be provided again to compute predictions. For that, we add the arguments <code>knearest.n</code>, <code>min.knn.n</code>, <code>max.knn.n</code>, <code>step.mn</code>. 
</p>
</li>
<li><p>  If <code>option=3</code>, we mantain only the indexes of the relevant variables selected by the IASSMR. We estimate again the linear coefficients  by means of <code>sfpl.kernel.fit</code> or <code>sfpl.kNN.fit</code>, respectively, without penalisation (setting <code>lambda.seq=0</code>) and using the whole training sample (<code>train=c(train.1,train.2)</code>). The method provides two predictions (and MSEPs):
</p>

<ul>
<li><p> a) The prediction associated to <code>option=1</code> for <code>sfpl.kernel</code> or <code>sfpl.kNN</code> class.
</p>
</li>
<li><p> b) The prediction associated to <code>option=2</code> for <code>sfpl.kernel</code> or <code>sfpl.kNN</code> class.
</p>
</li></ul>

<p>(see the documentation of the functions <code>predict.sfpl.kernel</code> and <code>predict.sfpl.kNN</code>)
</p>
</li>
<li><p> If <code>option=4</code> (option only available for the class <code>PVS.kNN</code>) we mantain <code>beta.est</code>, while the tunning parameter <code class="reqn">k</code> is seleted again to predict the functional nonparametric component of the model. This selection is performed using the cross-validation criterion in the functional nonparametric model associated and the complete training sample (i.e. <code>train=c(train.1,train.2)</code>), obtaining a local selection for <code class="reqn">k</code>.
</p>
</li></ul>



<h3>Value</h3>

<p>The function returns the predicted values of the response (<code>y</code>) for <code>newdata.x</code> and <code>newdata.z</code>. If <code>!is.null(y.test)</code>, it also provides the mean squared error of prediction (<code>MSEP</code>) computed as <code>mean((y-y.test)^2)</code>.
If <code>option=3</code>, two sets of predictions are provided (and two MSEP), in correspondence with the items a) and b) mentioned in the section <code>Details.</code>
If <code>is.null(newdata.x)</code> or <code>is.null(newdata.z)</code>,  the function returns the fitted values.
</p>


<h3>Author(s)</h3>

<p>German Aneiros Perez <a href="mailto:german.aneiros@udc.es">german.aneiros@udc.es</a> 
</p>
<p>Silvia Novo Diaz  <a href="mailto:snovo@est-econ.uc3m.es">snovo@est-econ.uc3m.es</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+PVS.kernel.fit">PVS.kernel.fit</a></code>, <code><a href="#topic+sfpl.kernel.fit">sfpl.kernel.fit</a></code> and <code><a href="#topic+predict.sfpl.kernel">predict.sfpl.kernel</a></code> or <code><a href="#topic+PVS.kNN.fit">PVS.kNN.fit</a></code>,
<code><a href="#topic+sfpl.kNN.fit">sfpl.kNN.fit</a></code> and <code><a href="#topic+predict.sfpl.kNN">predict.sfpl.kNN</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(Sugar)


y&lt;-Sugar$ash
x&lt;-Sugar$wave.290
z&lt;-Sugar$wave.240

#Outliers
index.y.25 &lt;- y &gt; 25
index.atip &lt;- index.y.25
(1:268)[index.atip]


#Dataset to model
x.sug &lt;- x[!index.atip,]
z.sug&lt;- z[!index.atip,]
y.sug &lt;- y[!index.atip]

train&lt;-1:216
test&lt;-217:266

#Fit
fit.kernel&lt;- PVS.kernel.fit(x=x.sug[train,],z=z.sug[train,], 
              y=y.sug[train],train.1=1:108,train.2=109:216,
              lambda.min.h=0.03,lambda.min.l=0.03,
              max.q.h=0.35,num.h = 10, nknot=20,criterion="BIC",
              penalty="grSCAD",max.iter=5000)
fit.kNN&lt;- PVS.kNN.fit(x=x.sug[train,],z=z.sug[train,], y=y.sug[train],
            train.1=1:108,train.2=109:216,lambda.min.h=0.07, 
            lambda.min.l=0.07, nknot=20,criterion="BIC", penalty="grSCAD",
            max.iter=5000)

#Preditions
predict(fit.kernel,newdata.x=x.sug[test,],newdata.z=z.sug[test,],y.test=y.sug[test],option=2)
predict(fit.kNN,newdata.x=x.sug[test,],newdata.z=z.sug[test,],y.test=y.sug[test],option=2)

</code></pre>

<hr>
<h2 id='predict.sfpl'>
Predictions from semi-functional partial linear regression
</h2><span id='topic+predict.sfpl.kernel'></span><span id='topic+predict.sfpl.kNN'></span>

<h3>Description</h3>

<p><code>predict</code> method for the semi-functional partial linear  model (SFPLM) fitted using <code>sfpl.kernel.fit</code> or <code>sfpl.kNN.fit</code>. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'sfpl.kernel'
predict(object, newdata.x = NULL, newdata.z = NULL,
  y.test = NULL, option = NULL, ...)
## S3 method for class 'sfpl.kNN'
predict(object, newdata.x = NULL, newdata.z = NULL, 
  y.test = NULL, option = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.sfpl_+3A_object">object</code></td>
<td>

<p>Output of the functions mentioned in the <code>Description</code> (i.e. an object of the class <code>sfpl.kernel</code> or <code>sfpl.kNN</code>.
</p>
</td></tr>
<tr><td><code id="predict.sfpl_+3A_newdata.x">newdata.x</code></td>
<td>

<p>A matrix containing new observations of the functional covariate in the functional-single index component collected by row.
</p>
</td></tr>
<tr><td><code id="predict.sfpl_+3A_newdata.z">newdata.z</code></td>
<td>

<p>Matrix containing the new observations of the scalar covariate collected by row.
</p>
</td></tr>
<tr><td><code id="predict.sfpl_+3A_y.test">y.test</code></td>
<td>

<p>(optional) A vector containing the new observations of the response.
</p>
</td></tr>
<tr><td><code id="predict.sfpl_+3A_option">option</code></td>
<td>

<p>Allows the choice between 1 and 2 in <code>sfpl.kernel</code> objects, and 1, 2 and 3 in <code>sfpl.kNN</code> objects. The default is 1. See the section <code>Details</code>.
</p>
</td></tr>
<tr><td><code id="predict.sfpl_+3A_...">...</code></td>
<td>
<p>Further arguments passed to or from other methods.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>To obtain the predictions of the response for <code>newdata.x</code> and <code>newdata.z</code>, the following options are provided:
</p>

<ul>
<li><p> If <code>option=1</code>, we mantain all the estimations (<code>k.opt</code> or <code>h.opt</code> and <code>beta.est</code>) to predict the functional nonparametric component of the model. 
</p>
</li>
<li><p> If <code>option=2</code>, we mantain <code>beta.est</code>, while the tunning parameter (<code class="reqn">h</code> or <code class="reqn">k</code>) is seleted again to predict the functional nonparametric component of the model. This selection is performed using the cross-validation criterion in the functional nonparametric model associated, obtaining a global selection for <code class="reqn">h</code> or <code class="reqn">k</code>.
</p>
</li></ul>

<p>In the case of <code>sfpl.kNN</code> objects if <code>option=3</code>, we mantain <code>beta.est</code>, while the tunning parameter <code class="reqn">k</code> is seleted again to predict the functional nonparametric component of the model. This selection is performed using the cross-validation criterion in the functional nonparametric model associated, performing a local selection for <code class="reqn">k</code>.
</p>


<h3>Value</h3>

<p>The function returns the predicted values of the response (<code>y</code>) for <code>newdata.x</code> and <code>newdata.z</code>. If <code>!is.null(y.test)</code>, it also provides the mean squared error of prediction (<code>MSEP</code>) computed as <code>mean((y-y.test)^2)</code>.
If <code>is.null(newdata.x)</code> or <code>is.null(newdata.z)</code>,  the function returns the fitted values.
</p>


<h3>Author(s)</h3>

<p>German Aneiros Perez <a href="mailto:german.aneiros@udc.es">german.aneiros@udc.es</a> 
</p>
<p>Silvia Novo Diaz  <a href="mailto:snovo@est-econ.uc3m.es">snovo@est-econ.uc3m.es</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+sfpl.kernel.fit">sfpl.kernel.fit</a></code> and <code><a href="#topic+sfpl.kNN.fit">sfpl.kNN.fit</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data("Tecator")
y&lt;-Tecator$fat
X&lt;-Tecator$absor.spectra
z1&lt;-Tecator$protein       
z2&lt;-Tecator$moisture

#Quadratic, cubic and interaction effects of the scalar covariates.
z.com&lt;-cbind(z1,z2,z1^2,z2^2,z1^3,z2^3,z1*z2)
train&lt;-1:160
test&lt;-161:215

 
#Fit
fit.kernel&lt;-sfpl.kernel.fit(x=X[train,], z=z.com[train,], y=y[train],q=2,
  max.q.h=0.35,lambda.min.h=0.02,lambda.min.l=0.01, 
  factor.pn=2, max.iter=5000, criterion="BIC", penalty="grSCAD",nknot=20)
fit.kNN&lt;-sfpl.kNN.fit(y=y[train],x=X[train,], z=z.com[train,],q=2, 
  max.knn=20,lambda.min.h=0.02,lambda.min.l=0.01, factor.pn=2, 
  criterion="BIC",range.grid=c(850,1050), penalty="grSCAD",
  nknot=20, max.iter=5000)

#Predictions
predict(fit.kernel,newdata.x=X[test,],newdata.z=z.com[test,],y.test=y[test],
  option=2)
predict(fit.kNN,newdata.x=X[test,],newdata.z=z.com[test,],y.test=y[test],
  option=2)

</code></pre>

<hr>
<h2 id='predict.sfplsim.FASSMR'>
Prediction from functional semiparametric  model estimates
</h2><span id='topic+predict.FASSMR.kernel'></span><span id='topic+predict.FASSMR.kNN'></span><span id='topic+predict.sfplsim.kernel'></span><span id='topic+predict.sfplsim.kNN'></span>

<h3>Description</h3>

<p><code>predict</code> method for: 
</p>

<ul>
<li><p> Semi-functional partial linear single-index model (SFPLSIM) fitted using <code>sfplsim.kernel.fit</code> or <code>sfplsim.kNN.fit</code>. 
</p>
</li>
<li><p> Multi-functional partial linear single-index regression (MFPLSIM) fitted using <code>FASSMR.kernel.fit</code> or <code>FASSMR.kNN.fit</code>.
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'sfplsim.kernel'
predict(object, newdata.x = NULL, newdata.z = NULL,
  y.test = NULL, option = NULL, ...)
## S3 method for class 'sfplsim.kNN'
predict(object, newdata.x = NULL, newdata.z = NULL, 
  y.test = NULL, option = NULL, ...)
## S3 method for class 'FASSMR.kernel'
predict(object, newdata.x = NULL, newdata.z = NULL,
  y.test = NULL, option = NULL, ...)
## S3 method for class 'FASSMR.kNN'
predict(object, newdata.x = NULL, newdata.z = NULL,
  y.test = NULL, option = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.sfplsim.FASSMR_+3A_object">object</code></td>
<td>

<p>Output of the functions mentioned in the <code>Description</code> (i.e. an object of the class <code>sfplsim.kernel</code>, <code>sfplsim.kNN</code>, <code>FASSMR.kernel</code> or <code>FASSMR.kNN</code>).
</p>
</td></tr>
<tr><td><code id="predict.sfplsim.FASSMR_+3A_newdata.x">newdata.x</code></td>
<td>

<p>A matrix containing new observations of the functional covariate in the functional-single index component collected by row.
</p>
</td></tr>
<tr><td><code id="predict.sfplsim.FASSMR_+3A_newdata.z">newdata.z</code></td>
<td>

<p>Matrix containing the new observations of the scalar covariates (SFPLSIM) or of the scalar covariates coming from the discretisation  of a curve (MFPLSIM), collected by row. 
</p>
</td></tr>
<tr><td><code id="predict.sfplsim.FASSMR_+3A_y.test">y.test</code></td>
<td>

<p>(optional) A vector containing the new observations of the response.
</p>
</td></tr>
<tr><td><code id="predict.sfplsim.FASSMR_+3A_option">option</code></td>
<td>

<p>Allows the choice between 1 and 2. The default is 1. See the section <code>Details</code>.
</p>
</td></tr>
<tr><td><code id="predict.sfplsim.FASSMR_+3A_...">...</code></td>
<td>
<p>Further arguments passed to or from other methods.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>To obtain the predictions of the response for <code>newdata.x</code> and <code>newdata.z</code>, two options are provided:
</p>

<ul>
<li><p> If <code>option=1</code>, we mantain all the estimations (<code>k.opt</code> or <code>h.opt</code>, <code>theta.est</code> and <code>beta.est</code>) to predict the functional single-index component of the model. 
</p>
</li>
<li><p> If <code>option=2</code>, we mantain <code>theta.est</code> and <code>beta.est</code>, while the tuning parameter (<code class="reqn">h</code> or <code class="reqn">k</code>) is selected again to predict the functional single-index component of the model. This selection is performed using the cross-validation criterion in the functional single-index model associated.
</p>
</li></ul>



<h3>Value</h3>

<p>The function returns the predicted values of the response (<code>y</code>) for <code>newdata.x</code> and <code>newdata.z</code>. If <code>!is.null(y.test)</code>, it also provides the mean squared error of prediction (<code>MSEP</code>) computed as <code>mean((y-y.test)^2)</code>.
If <code>is.null(newdata.x)</code> or <code>is.null(newdata.z)</code>,  the function returns the fitted values.
</p>


<h3>Author(s)</h3>

<p>German Aneiros Perez <a href="mailto:german.aneiros@udc.es">german.aneiros@udc.es</a> 
</p>
<p>Silvia Novo Diaz  <a href="mailto:snovo@est-econ.uc3m.es">snovo@est-econ.uc3m.es</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+sfplsim.kernel.fit">sfplsim.kernel.fit</a></code>, <code><a href="#topic+sfplsim.kNN.fit">sfplsim.kNN.fit</a></code>, <code><a href="#topic+FASSMR.kernel.fit">FASSMR.kernel.fit</a></code>  or <code><a href="#topic+FASSMR.kNN.fit">FASSMR.kNN.fit</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data("Tecator")
y&lt;-Tecator$fat
X&lt;-Tecator$absor.spectra2
z1&lt;-Tecator$protein       
z2&lt;-Tecator$moisture

#Quadratic, cubic and interaction effects of the scalar covariates.
z.com&lt;-cbind(z1,z2,z1^2,z2^2,z1^3,z2^3,z1*z2)
train&lt;-1:160
test&lt;-161:215

#SFPLSIM fit. Convergence errors for some theta are obtained.
s.fit.kernel&lt;-sfplsim.kernel.fit(x=X[train,], z=z.com[train,], y=y[train],
            max.q.h=0.35,lambda.min.h=0.02,lambda.min.l=0.01, 
            factor.pn=2, max.iter=5000, nknot.theta=4,criterion="BIC", 
            penalty="grSCAD",nknot=20)
s.fit.kNN&lt;-sfplsim.kNN.fit(y=y[train],x=X[train,], z=z.com[train,], 
        max.knn=20,lambda.min.h=0.02,lambda.min.l=0.01, factor.pn=2, 
        nknot.theta=4,criterion="BIC",range.grid=c(850,1050), penalty="grSCAD",
        nknot=20, max.iter=5000)


predict(s.fit.kernel,newdata.x=X[test,],newdata.z=z.com[test,],
  y.test=y[test],option=2)
predict(s.fit.kNN,newdata.x=X[test,],newdata.z=z.com[test,],
  y.test=y[test],option=2)



data(Sugar)
y&lt;-Sugar$ash
x&lt;-Sugar$wave.290
z&lt;-Sugar$wave.240

#Outliers
index.y.25 &lt;- y &gt; 25
index.atip &lt;- index.y.25
(1:268)[index.atip]


#Dataset to model
x.sug &lt;- x[!index.atip,]
z.sug&lt;- z[!index.atip,]
y.sug &lt;- y[!index.atip]

train&lt;-1:216
test&lt;-217:266

m.fit.kernel &lt;- FASSMR.kernel.fit(x=x.sug[train,],z=z.sug[train,], 
                  y=y.sug[train],  nknot.theta=2,lambda.min.h=0.03, 
                  lambda.min.l=0.03, max.q.h=0.35,num.h = 10, 
                  nknot=20,criterion="BIC", penalty="grSCAD",max.iter=5000)


m.fit.kNN&lt;- FASSMR.kNN.fit(x=x.sug[train,],z=z.sug[train,], y=y.sug[train], 
            nknot.theta=2,lambda.min.h=0.03, lambda.min.l=0.03, 
            max.knn=20,nknot=20,criterion="BIC", penalty="grSCAD",max.iter=5000)


predict(m.fit.kernel,newdata.x=x.sug[test,],newdata.z=z.sug[test,],
  y.test=y.sug[test],option=2)
predict(m.fit.kNN,newdata.x=x.sug[test,],newdata.z=z.sug[test,],
  y.test=y.sug[test],option=2)



</code></pre>

<hr>
<h2 id='print.summary.fsim'>
Summarize information from functional single-index model (FSIM) estimation methods
</h2><span id='topic+print.fsim.kernel'></span><span id='topic+print.fsim.kNN'></span><span id='topic+summary.fsim.kernel'></span><span id='topic+summary.fsim.kNN'></span>

<h3>Description</h3>

<p><code>summary</code> and <code>print</code> functions for <code>fsim.kNN.fit</code> and <code>fsim.kernel.fit</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
## S3 method for class 'fsim.kernel'
print(x, ...)
## S3 method for class 'fsim.kNN'
print(x, ...)
## S3 method for class 'fsim.kernel'
summary(object, ...)
## S3 method for class 'fsim.kNN'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.summary.fsim_+3A_x">x</code></td>
<td>

<p>Output of the <code>fsim.kernel.fit</code> or <code>fsim.kNN.fit</code> functions (i.e. an object of the class <code>fsim.kernel</code> or <code>fsim.kNN</code>).
</p>
</td></tr>
<tr><td><code id="print.summary.fsim_+3A_...">...</code></td>
<td>
<p> Further arguments.</p>
</td></tr>
<tr><td><code id="print.summary.fsim_+3A_object">object</code></td>
<td>

<p>Output of the <code>fsim.kernel.fit</code> or <code>fsim.kNN.fit</code> functions (i.e. an object of the class <code>fsim.kernel</code> or <code>fsim.kNN</code>).
</p>
</td></tr>
</table>


<h3>Value</h3>


<ul>
<li><p>The matched call.
</p>
</li>
<li><p>The optimal value of the tunning parameter (<code>h.opt</code> or <code>k.opt</code>).
</p>
</li>
<li><p>Coefficients of <code class="reqn">\hat{\theta}</code> in the B-spline basis (<code>theta.est</code>: a vector of <code>length(order.Bspline+nknot.theta).</code>
</p>
</li>
<li><p>Minimum value of the CV function, i.e. the value of CV for <code>theta.est</code> and <code>h.opt</code>/<code>k.opt</code>.
</p>
</li>
<li><p>R squared.
</p>
</li>
<li><p>Residual variance.
</p>
</li>
<li><p>Residual degrees of freedom.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>German Aneiros Perez <a href="mailto:german.aneiros@udc.es">german.aneiros@udc.es</a> 
</p>
<p>Silvia Novo Diaz  <a href="mailto:snovo@est-econ.uc3m.es">snovo@est-econ.uc3m.es</a>
</p>


<h3>See Also</h3>

<p><code>fsim.kernel.fit</code> and <code>fsim.kNN.fit</code>.
</p>

<hr>
<h2 id='print.summary.lm'>
Summarize information from  linear model estimation methods
</h2><span id='topic+print.lm.pels'></span><span id='topic+print.PVS'></span><span id='topic+summary.lm.pels'></span><span id='topic+summary.PVS'></span>

<h3>Description</h3>

<p><code>summary</code> and <code>print</code> functions for <code>lm.pels.fit</code> and <code>PVS.fit</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'lm.pels'
print(x, ...)
## S3 method for class 'PVS'
print(x, ...)
## S3 method for class 'lm.pels'
summary(object, ...)
## S3 method for class 'PVS'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.summary.lm_+3A_x">x</code></td>
<td>

<p>Output of the <code>lm.pels.fit</code> or <code>PVS.fit</code> functions (i.e. an object of the class <code>lm.pels</code> or <code>PVS</code>).
</p>
</td></tr>
<tr><td><code id="print.summary.lm_+3A_...">...</code></td>
<td>
<p>Further arguments.
</p>
</td></tr>
<tr><td><code id="print.summary.lm_+3A_object">object</code></td>
<td>

<p>Output of the <code>lm.pels.fit</code> or <code>PVS.fit</code> functions (i.e. an object of the class <code>lm.pels</code> or <code>PVS</code>).
</p>
</td></tr>
</table>


<h3>Value</h3>


<ul>
<li><p>The matched call.
</p>
</li>
<li><p>The estimated intercept of the model.
</p>
</li>
<li><p>The estimated vector of linear coefficients (<code>beta.est</code>).
</p>
</li>
<li><p>The number of non-zero components in <code>beta.est</code>.
</p>
</li>
<li><p>The indexes of the non-zero components in <code>beta.est</code>.
</p>
</li>
<li><p>The optimal value of the penalisation parameter (<code>lambda.opt</code>).
</p>
</li>
<li><p>The optimal value of the criterion function, i. e. the value  obtained with <code>lambda.opt</code> and <code>vn.opt</code> (and <code>w.opt</code> in the case of the PVS).
</p>
</li>
<li><p>Minimum value of the penalized profile least-squares function. That is, the value obtained using <code>beta.est</code>.
</p>
</li>
<li><p>The penalty function used.
</p>
</li>
<li><p>The criterion used to select the penalisation parameter and <code>vn</code>.
</p>
</li>
<li><p>The optimal value of <code>vn</code> in the case of the <code>lm.pels</code> object.
</p>
</li></ul>

<p>In the case of the <code>PVS</code> objects, these functions also return
the optimal initial number of covariates to build the reduced model in the first step of the algorithm (<code>w.opt</code>). This value is also selected by means of the criterion used to select the penalisation parameter.
</p>


<h3>Author(s)</h3>

<p>German Aneiros Perez <a href="mailto:german.aneiros@udc.es">german.aneiros@udc.es</a> 
</p>
<p>Silvia Novo Diaz  <a href="mailto:snovo@est-econ.uc3m.es">snovo@est-econ.uc3m.es</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+lm.pels.fit">lm.pels.fit</a></code> and <code><a href="#topic+PVS.fit">PVS.fit</a></code>.
</p>

<hr>
<h2 id='print.summary.mfpl'>
Summarize information from multi-functional partial linear  model (MFPLM) estimation methods
</h2><span id='topic+print.PVS.kernel'></span><span id='topic+print.PVS.kNN'></span><span id='topic+summary.PVS.kernel'></span><span id='topic+summary.PVS.kNN'></span>

<h3>Description</h3>

<p><code>summary</code> and <code>print</code> functions for <code>PVS.kernel.fit</code> and <code>PVS.kNN.fit</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'PVS.kernel'
print(x, ...)
## S3 method for class 'PVS.kNN'
print(x, ...)
## S3 method for class 'PVS.kernel'
summary(object, ...)
## S3 method for class 'PVS.kNN'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.summary.mfpl_+3A_x">x</code></td>
<td>

<p>Output of the <code>PVS.kernel.fit</code> or <code>PVS.kNN.fit</code> functions (i.e. an object of the class <code>PVS.kernel</code> or <code>PVS.kNN</code>).
</p>
</td></tr>
<tr><td><code id="print.summary.mfpl_+3A_...">...</code></td>
<td>
<p>Further arguments.
</p>
</td></tr>
<tr><td><code id="print.summary.mfpl_+3A_object">object</code></td>
<td>

<p>Output of the <code>PVS.kernel.fit</code> or <code>PVS.kNN.fit</code> functions (i.e. an object of the class <code>PVS.kernel</code> or <code>PVS.kNN</code>).
</p>
</td></tr>
</table>


<h3>Value</h3>


<ul>
<li><p>The matched call.
</p>
</li>
<li><p>The optimal value of the tunning parameter (<code>h.opt</code> or <code>k.opt</code>).
</p>
</li>
<li><p>The optimal initial number of covariates to build the reduced model (<code>w.opt</code>).
</p>
</li>
<li><p>The estimated vector of linear coefficients (<code>beta.est</code>).
</p>
</li>
<li><p>The number of non-zero components in <code>beta.est</code>.
</p>
</li>
<li><p>The indexes of the non-zero components in <code>beta.est</code>.
</p>
</li>
<li><p>The optimal value of the penalisation parameter (<code>lambda.opt</code>).
</p>
</li>
<li><p>The optimal value of the criterion function, i. e. the value  obtained with <code>w.opt</code>, <code>lambda.opt</code>, <code>vn.opt</code>  and <code>h.opt</code>/<code>k.opt</code>
</p>
</li>
<li><p>Minimum value of the penalized profile least-squares function. That is, the value obtained using <code>beta.est</code>.
</p>
</li>
<li><p>The penalty function used.
</p>
</li>
<li><p>The criterion used to select the initial number of covariates used to build the reduced model, the tunning parameter, the penalisation parameter and <code>vn</code>.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>German Aneiros Perez <a href="mailto:german.aneiros@udc.es">german.aneiros@udc.es</a> 
</p>
<p>Silvia Novo Diaz  <a href="mailto:snovo@est-econ.uc3m.es">snovo@est-econ.uc3m.es</a>
</p>


<h3>See Also</h3>

<p><code>PVS.kernel.fit</code> and <code>PVS.kNN.fit</code>.
</p>

<hr>
<h2 id='print.summary.mfplsim'>
Summarize information from multi-functional partial linear single-index model (MFPLSIM) estimation methods
</h2><span id='topic+print.FASSMR.kernel'></span><span id='topic+print.FASSMR.kNN'></span><span id='topic+print.IASSMR.kernel'></span><span id='topic+print.IASSMR.kNN'></span><span id='topic+summary.FASSMR.kernel'></span><span id='topic+summary.FASSMR.kNN'></span><span id='topic+summary.IASSMR.kernel'></span><span id='topic+summary.IASSMR.kNN'></span>

<h3>Description</h3>

<p><code>summary</code> and <code>print</code> functions for <code>FASSMR.kernel.fit</code>, <code>FASSMR.kNN.fit</code>, <code>IASSMR.kernel.fit</code> and <code>IASSMR.kNN.fit</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'FASSMR.kernel'
print(x, ...)
## S3 method for class 'FASSMR.kNN'
print(x, ...)
## S3 method for class 'IASSMR.kernel'
print(x, ...)
## S3 method for class 'IASSMR.kNN'
print(x, ...)
## S3 method for class 'FASSMR.kernel'
summary(object, ...)
## S3 method for class 'FASSMR.kNN'
summary(object, ...)
## S3 method for class 'IASSMR.kernel'
summary(object, ...)
## S3 method for class 'IASSMR.kNN'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.summary.mfplsim_+3A_x">x</code></td>
<td>

<p>Output of the <code>FASSMR.kernel.fit</code>, <code>FASSMR.kNN.fit</code>, <code>IASSMR.kernel.fit</code> or <code>IASSMR.kNN.fit</code> functions (i.e. an object of the class <code>FASSMR.kernel</code>,  <code>FASSMR.kNN</code>, <code>IASSMR.kernel</code> or <code>IASSMR.kNN</code>).
</p>
</td></tr>
<tr><td><code id="print.summary.mfplsim_+3A_...">...</code></td>
<td>
<p>Further arguments passed to or from other methods.
</p>
</td></tr>
<tr><td><code id="print.summary.mfplsim_+3A_object">object</code></td>
<td>

<p>Output of the <code>FASSMR.kernel.fit</code>, <code>FASSMR.kNN.fit</code>, <code>IASSMR.kernel.fit</code> or <code>IASSMR.kNN.fit</code> functions (i.e. an object of the class <code>FASSMR.kernel</code>,  <code>FASSMR.kNN</code>, <code>IASSMR.kernel</code> or <code>IASSMR.kNN</code>).
</p>
</td></tr>
</table>


<h3>Value</h3>


<ul>
<li><p>The matched call.
</p>
</li>
<li><p>The optimal value of the tunning parameter (<code>h.opt</code> or <code>k.opt</code>).
</p>
</li>
<li><p>The optimal initial number of covariates to build the reduced model (<code>w.opt</code>).
</p>
</li>
<li><p>Coefficients of <code class="reqn">\hat{\theta}</code> in the B-spline basis (<code>theta.est</code>): a vector of <code>length(order.Bspline+nknot.theta).</code>
</p>
</li>
<li><p>The estimated vector of linear coefficients (<code>beta.est</code>).
</p>
</li>
<li><p>The number of non-zero components in <code>beta.est</code>.
</p>
</li>
<li><p>The indexes of the non-zero components in <code>beta.est</code>.
</p>
</li>
<li><p>The optimal value of the penalisation parameter (<code>lambda.opt</code>).
</p>
</li>
<li><p>The optimal value of the criterion function, i. e. the value  obtained with <code>w.opt</code>, <code>lambda.opt</code>, <code>vn.opt</code>  and <code>h.opt</code>/<code>k.opt</code>
</p>
</li>
<li><p>Minimum value of the penalized profile least-squares function. That is, the value obtained using <code>theta.est</code> and <code>beta.est</code>.
</p>
</li>
<li><p>The penalty function used.
</p>
</li>
<li><p>The criterion used to select the initial number of covariates used to build the reduced model, the tunning parameter, the penalisation parameter and <code>Vn</code>.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>German Aneiros Perez <a href="mailto:german.aneiros@udc.es">german.aneiros@udc.es</a> 
</p>
<p>Silvia Novo Diaz  <a href="mailto:snovo@est-econ.uc3m.es">snovo@est-econ.uc3m.es</a>
</p>


<h3>See Also</h3>

<p><code>FASSMR.kernel.fit</code>, <code>FASSMR.kNN.fit</code>, <code>IASSMR.kernel.fit</code> and <code>IASSMR.kNN.fit</code>.
</p>

<hr>
<h2 id='print.summary.sfpl'>
Summarize information from semi-functional partial linear model (SFPLM) estimation methods
</h2><span id='topic+print.sfpl.kernel'></span><span id='topic+print.sfpl.kNN'></span><span id='topic+summary.sfpl.kernel'></span><span id='topic+summary.sfpl.kNN'></span>

<h3>Description</h3>

<p><code>summary</code> and <code>print</code> functions for <code>sfpl.kNN.fit</code> and <code>sfpl.kernel.fit</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'sfpl.kernel'
print(x, ...)
## S3 method for class 'sfpl.kNN'
print(x, ...)
## S3 method for class 'sfpl.kernel'
summary(object, ...)
## S3 method for class 'sfpl.kNN'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.summary.sfpl_+3A_x">x</code></td>
<td>

<p>Output of the <code>sfpl.kernel.fit</code> or <code>sfpl.kNN.fit</code> functions (i.e. an object of the class <code>sfpl.kernel</code> or <code>sfpl.kNN</code>).
</p>
</td></tr>
<tr><td><code id="print.summary.sfpl_+3A_...">...</code></td>
<td>

<p>Further arguments.
</p>
</td></tr>
<tr><td><code id="print.summary.sfpl_+3A_object">object</code></td>
<td>

<p>Output of the <code>sfpl.kernel.fit</code> or <code>sfpl.kNN.fit</code> functions (i.e. an object of the class <code>sfpl.kernel</code> or <code>sfpl.kNN</code>).
</p>
</td></tr>
</table>


<h3>Value</h3>


<ul>
<li><p>The matched call.
</p>
</li>
<li><p>The optimal value of the tunning parameter (<code>h.opt</code> or <code>k.opt</code>).
</p>
</li>
<li><p>The estimated vector of linear coefficients (<code>beta.est</code>).
</p>
</li>
<li><p>The number of non-zero components in <code>beta.est</code>.
</p>
</li>
<li><p>The indexes of the non-zero components in <code>beta.est</code>.
</p>
</li>
<li><p>The optimal value of the penalisation parameter (<code>lambda.opt</code>).
</p>
</li>
<li><p>The optimal value of the criterion function, i. e. the value  obtained with <code>lambda.opt</code>, <code>vn.opt</code>  and <code>h.opt</code>/<code>k.opt</code>
</p>
</li>
<li><p>Minimum value of the penalized profile least-squares function. That is, the value obtained using <code>beta.est</code>.
</p>
</li>
<li><p>The penalty function used.
</p>
</li>
<li><p>The criterion used to select the tunning parameter, the penalisation parameter and <code>vn</code>.
</p>
</li>
<li><p>The optimal value of <code>vn</code>.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>German Aneiros Perez <a href="mailto:german.aneiros@udc.es">german.aneiros@udc.es</a> 
</p>
<p>Silvia Novo Diaz  <a href="mailto:snovo@est-econ.uc3m.es">snovo@est-econ.uc3m.es</a>
</p>


<h3>See Also</h3>

<p><code>sfpl.kernel.fit</code> and <code>sfpl.kNN.fit</code>.
</p>

<hr>
<h2 id='print.summary.sfplsim'>
Summarize information from semi-functional partial linear single-index model (SFPLSIM) estimation methods
</h2><span id='topic+print.sfplsim.kernel'></span><span id='topic+print.sfplsim.kNN'></span><span id='topic+summary.sfplsim.kernel'></span><span id='topic+summary.sfplsim.kNN'></span>

<h3>Description</h3>

<p><code>summary</code> and <code>print</code> functions for <code>sfplsim.kNN.fit</code> and <code>sfplsim.kernel.fit</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'sfplsim.kernel'
print(x, ...)
## S3 method for class 'sfplsim.kNN'
print(x, ...)
## S3 method for class 'sfplsim.kernel'
summary(object, ...)
## S3 method for class 'sfplsim.kNN'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.summary.sfplsim_+3A_x">x</code></td>
<td>

<p>Output of the <code>sfplsim.kernel.fit</code> or <code>sfplsim.kNN.fit</code> functions (i.e. an object of the class <code>sfplsim.kernel</code> or <code>sfplsim.kNN</code>).
</p>
</td></tr>
<tr><td><code id="print.summary.sfplsim_+3A_...">...</code></td>
<td>

<p>Further arguments.
</p>
</td></tr>
<tr><td><code id="print.summary.sfplsim_+3A_object">object</code></td>
<td>

<p>Output of the <code>sfplsim.kernel.fit</code> or <code>sfplsim.kNN.fit</code> functions (i.e. an object of the class <code>sfplsim.kernel</code> or <code>sfplsim.kNN</code>).
</p>
</td></tr>
</table>


<h3>Value</h3>


<ul>
<li><p>The matched call.
</p>
</li>
<li><p>The optimal value of the tunning parameter (<code>h.opt</code> or <code>k.opt</code>).
</p>
</li>
<li><p>Coefficients of <code class="reqn">\hat{\theta}</code> in the B-spline basis (<code>theta.est</code>): a vector of <code>length(order.Bspline+nknot.theta).</code>
</p>
</li>
<li><p>The estimated vector of linear coefficients (<code>beta.est</code>).
</p>
</li>
<li><p>The number of non-zero components in <code>beta.est</code>.
</p>
</li>
<li><p>The indexes of the non-zero components in <code>beta.est</code>.
</p>
</li>
<li><p>The optimal value of the penalisation parameter (<code>lambda.opt</code>).
</p>
</li>
<li><p>The optimal value of the criterion function, i. e. the value  obtained with <code>lambda.opt</code>, <code>vn.opt</code>  and <code>h.opt</code>/<code>k.opt</code>
</p>
</li>
<li><p>Minimum value of the penalized profile least-squares function. That is, the value obtained using <code>theta.est</code> and <code>beta.est</code>.
</p>
</li>
<li><p>The penalty function used.
</p>
</li>
<li><p>The criterion used to select the tunning parameter, the penalisation parameter and <code>vn</code>.
</p>
</li>
<li><p>The optimal value of <code>vn</code>.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>German Aneiros Perez <a href="mailto:german.aneiros@udc.es">german.aneiros@udc.es</a> 
</p>
<p>Silvia Novo Diaz  <a href="mailto:snovo@est-econ.uc3m.es">snovo@est-econ.uc3m.es</a>
</p>


<h3>See Also</h3>

<p><code>sfplsim.kernel.fit</code> and <code>sfplsim.kNN.fit</code>.
</p>

<hr>
<h2 id='projec'>Inner product computation</h2><span id='topic+projec'></span>

<h3>Description</h3>

<p>Computes the inner product between each curve collected in <code>data</code> and a particular curve  <code class="reqn">\theta</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>projec(data, theta, order.Bspline = 3, nknot.theta = 3, range.grid = NULL, nknot = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="projec_+3A_data">data</code></td>
<td>

<p>Matrix containing functional data collected by row
</p>
</td></tr>
<tr><td><code id="projec_+3A_theta">theta</code></td>
<td>

<p>Vector containing the coefficients of <code class="reqn">\theta</code> in a B-spline basis, so that <code>length(theta)=order.Bspline+nknot.theta</code>
</p>
</td></tr>
<tr><td><code id="projec_+3A_order.bspline">order.Bspline</code></td>
<td>

<p>Positive integer giving the order of the B-spline basis functions for the B-spline representation of <code class="reqn">\theta</code>. This is the number of coefficients in each piecewise polynomial segment. The default is 3.
</p>
</td></tr>
<tr><td><code id="projec_+3A_nknot.theta">nknot.theta</code></td>
<td>

<p>Positive integer indicating the number of uniform interior knots of the B-spline basis. The default is 3.
</p>
</td></tr>
<tr><td><code id="projec_+3A_range.grid">range.grid</code></td>
<td>

<p>Vector of length 2 containing  the range of the discretization of the functional data. If <code>range.grid=NULL</code>, then <code>range.grid=c(1,p)</code> is considered, where <code>p</code> is the size of the discretization size of <code>data</code> (i.e. <code>ncol(data)</code>).
</p>
</td></tr>
<tr><td><code id="projec_+3A_nknot">nknot</code></td>
<td>

<p>Positive integer indicating the number of interior knots for the B-spline representation of the functional data. The default value is <code>(p - order.Bspline - 1)%/%2</code>.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A matrix with the inner products.
</p>


<h3>Note</h3>

<p>The construction of this code is based on that by Frederic Ferraty, which is available on his website <a href="https://www.math.univ-toulouse.fr/~ferraty/SOFTWARES/NPFDA/index.html">https://www.math.univ-toulouse.fr/~ferraty/SOFTWARES/NPFDA/index.html</a>.</p>


<h3>Author(s)</h3>

<p>German Aneiros Perez <a href="mailto:german.aneiros@udc.es">german.aneiros@udc.es</a> 
</p>
<p>Silvia Novo Diaz  <a href="mailto:snovo@est-econ.uc3m.es">snovo@est-econ.uc3m.es</a>
</p>


<h3>References</h3>

<p>Novo S., Aneiros, G., and Vieu, P., (2019) Automatic and location-adaptive estimation in functional single&ndash;index regression. <em>Journal of Nonparametric Statistics</em>, <b>31(2)</b>, 364&ndash;392, <a href="https://doi.org/10.1080/10485252.2019.1567726">doi:10.1080/10485252.2019.1567726</a>.
</p>


<h3>See Also</h3>

<p>See also <code><a href="#topic+semimetric.projec">semimetric.projec</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data("Tecator")
names(Tecator)
y&lt;-Tecator$fat
X&lt;-Tecator$absor.spectra

#length(theta)=6=order.Bspline+nknot.theta 
projec(X,theta=c(1,0,0,1,1,-1),nknot.theta=3,nknot=20,range.grid=c(850,1050))
</code></pre>

<hr>
<h2 id='PVS.fit'>
PVS estimation
</h2><span id='topic+PVS.fit'></span>

<h3>Description</h3>

<p>This function computes the partitioning variable selection algorithm (PVS) for sparse linear regression with covariates with functional origin.
</p>
<p>This algorithm involves the penalised least-squares regularization procedure, which 
requires an objective criterion (<code>criterion</code>) to select the number of covariates in the reduced model <code>w.opt</code> and the penalisation parameter <code>lambda.opt</code>. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>PVS.fit(z, y, train.1, train.2, lambda.min = NULL, lambda.min.h = NULL,
  lambda.min.l = NULL, factor.pn = 1, nlambda = 100, vn = ncol(z),
  nfolds = 10, seed = 123, wn = c(10,15,20),range.grid=NULL, criterion =
  c("GCV", "BIC", "AIC","k-fold-CV"), penalty = c("grLasso", "grMCP", 
  "grSCAD", "gel", "cMCP", "gBridge", "gLasso", "gMCP"), max.iter = 1000)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="PVS.fit_+3A_z">z</code></td>
<td>

<p>Matrix containing the observations of the functional covariate that is discretised collected by row.
</p>
</td></tr>
<tr><td><code id="PVS.fit_+3A_y">y</code></td>
<td>

<p>Vector containing the scalar response.
</p>
</td></tr>
<tr><td><code id="PVS.fit_+3A_train.1">train.1</code></td>
<td>

<p>Indexes of the data used as the training sample in the 1st step. The default is <code>train.1&lt;-1:ceiling(n/2)</code>.
</p>
</td></tr>
<tr><td><code id="PVS.fit_+3A_train.2">train.2</code></td>
<td>

<p>Indexes of the data used as the training sample in the 2nd step. The default is <code>train.2&lt;-(ceiling(n/2)+1):n</code>.
</p>
</td></tr>
<tr><td><code id="PVS.fit_+3A_lambda.min">lambda.min</code></td>
<td>

<p>The smallest value for lambda (i. e., the smallest value  of the sequence in which <code>lambda.opt</code> is selected), as fraction of <code>lambda.max</code>.
The defaults is <code>lambda.min.l</code> if the number of observations is larger than <code>factor.pn</code> times the number of covariates and <code>lambda.min.h</code> otherwise.
</p>
</td></tr>
<tr><td><code id="PVS.fit_+3A_lambda.min.h">lambda.min.h</code></td>
<td>

<p>The smallest value of the sequence in which <code>lambda.opt</code> is selected if the number of observations is smaller than <code>factor.pn</code> times the number of scalar covariates. The default is 0.05. 
</p>
</td></tr>
<tr><td><code id="PVS.fit_+3A_lambda.min.l">lambda.min.l</code></td>
<td>

<p>The smallest value of the sequence in which <code>lambda.opt</code> is selected if the number of observations is larger than <code>factor.pn</code> times the number of scalar covariates. The default is 0.0001.
</p>
</td></tr>
<tr><td><code id="PVS.fit_+3A_factor.pn">factor.pn</code></td>
<td>

<p>Positive integer used to set <code>lambda.min</code>. The default value is 1.
</p>
</td></tr>
<tr><td><code id="PVS.fit_+3A_nlambda">nlambda</code></td>
<td>

<p>Positive integer indicating the number of values of the sequence in which <code>lambda.opt</code> is selected. The default is 100.
</p>
</td></tr>
<tr><td><code id="PVS.fit_+3A_vn">vn</code></td>
<td>
<p>Positive integer or vector of positive integers indicating the number of groups of consecutive variables to be penalised together. The default value is <code>vn=ncol(z)</code>, which leads to the individual penalisation of each scalar covariate.
</p>
</td></tr>
<tr><td><code id="PVS.fit_+3A_nfolds">nfolds</code></td>
<td>

<p>Positive integer indicating the number of cross-validation folds (used if <code>criterion="k-fold-CV"</code>). Default is 10.
</p>
</td></tr>
<tr><td><code id="PVS.fit_+3A_seed">seed</code></td>
<td>

<p>You may set the seed of the random number generator to obtain reproducible results (used if <code>criterion="k-fold-CV"</code>). Default is 123.
</p>
</td></tr>
<tr><td><code id="PVS.fit_+3A_wn">wn</code></td>
<td>

<p>A vector of positive integers indicating the eligible number of covariates of the reduced model. See the section <code>Details</code>. The default is <code>c(10,15,20)</code>.
</p>
</td></tr>
<tr><td><code id="PVS.fit_+3A_range.grid">range.grid</code></td>
<td>

<p>Vector of length 2 containing the endpoints of the grid at which the observations of the functional covariate <code>x</code> are evaluated (i.e. the range of the discretization). If <code>range.grid=NULL</code>, then <code>range.grid=c(1,p)</code> is considered, where <code>p</code> is the size of the discretization size of <code>x</code> (i.e. <code>ncol(x))</code>.
</p>
</td></tr>
<tr><td><code id="PVS.fit_+3A_criterion">criterion</code></td>
<td>

<p>The criterion by which to select the regularization parameter <code>lambda.opt</code> and <code>k.opt</code>. One of <code>"GCV", "BIC", "AIC"</code> or <code>"k-fold-CV"</code>. The default is <code>"GCV"</code>.
</p>
</td></tr>
<tr><td><code id="PVS.fit_+3A_penalty">penalty</code></td>
<td>

<p>The penalty function to be applied in the penalized least squares procedure. Only &quot;grLasso&quot; and &quot;grSCAD&quot; are implemented.
</p>
</td></tr>
<tr><td><code id="PVS.fit_+3A_max.iter">max.iter</code></td>
<td>

<p>Maximum number of iterations (total across entire path). Default is 1000.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The sparse linear model with covariates coming from the discretization of a curve is given by the expression
</p>
<p style="text-align: center;"><code class="reqn">Y_i=\sum_{j=1}^{p_n}\beta_{0j}\zeta_i(t_j)+\varepsilon_i,\ \ \ (i=1,\dots,n)</code>
</p>

<p>where 
</p>

<ul>
<li> <p><code class="reqn">Y_i</code> is a real random response and  <code class="reqn">\zeta_i</code> is supposed to be a random curve defined on some interval <code class="reqn">[a,b]</code> which  is observed at the points <code class="reqn">a\leq t_1&lt;\dots&lt;t_{p_n}\leq b</code>. 
</p>
</li>
<li>  <p><code class="reqn">\mathbf{\beta}_0=(\beta_{01},\dots,\beta_{0p_n})^{\top}</code> is a vector of unknown real coefficients.
</p>
</li>
<li> <p><code class="reqn">\varepsilon_i</code> denotes the random error.
</p>
</li></ul>

<p>In  this model, we assume that only a few scalar variables from the set <code class="reqn">\{\zeta(t_1),\dots,\zeta(t_{p_n})\}</code>  form part of the model. Therefore, we must select the relevant variables (the impact points of the curve <code class="reqn">\zeta</code> on the response) and estimate the model.
</p>
<p>In this function, this model is fitted using the PVS. The PVS is an algorithm with two steps, so we split the sample into two independent subsamples (asymptotically of the same size <code class="reqn">n_1\sim n_2\sim n/2</code>), one of them to be used in the first stage of the method and the other in the second stage.
</p>
<p style="text-align: center;"><code class="reqn">
\mathcal{E}^{\mathbf{1}}=\{(\zeta_i,\mathcal{X}_i,Y_i),\quad i=1,\dots,n_1\},
</code>
</p>

<p style="text-align: center;"><code class="reqn">
\mathcal{E}^{\mathbf{2}}=\{(\zeta_i,\mathcal{X}_i,Y_i),\quad i=n_1+1,\dots,n_1+n_2=n\}.
</code>
</p>
 
<p>Note that these two subsamples are specified to the programme by means of the arguments <code>train.1</code> and <code>train.2</code>.
The superscript <code class="reqn">\mathbf{s}</code> with <code class="reqn">\mathbf{s}=\mathbf{1},\mathbf{2}</code> indicates the stage of the method in which the sample, function, variable or parameter is involved. 
</p>
<p>To explain the algorithm we assume, without lost of generality, that the number <code class="reqn">p_n</code> of linear covariates can be expressed as follows: <code class="reqn">p_n=q_nw_n</code> with <code class="reqn">q_n</code> and <code class="reqn">w_n</code> integers.
</p>

<ol>
<li> <p><b>First step</b>. A reduced model is consider, discarding many linear covariates. The penalised least-squares procedure is applied to the reduced model using only the subsample <code class="reqn">\mathcal{E}^{\mathbf{1}}</code>. Specifically:
</p>

<ul>
<li><p> Consider a subset of the initial <code class="reqn">p_n</code> linear covariates, which contains only <code class="reqn">w_n</code> equally spaced discretized observations of  <code class="reqn">\zeta</code> covering the whole interval  <code class="reqn">[a,b]</code>. This subset is the following:
</p>
<p style="text-align: center;"><code class="reqn">
	\mathcal{R}_n^{\mathbf{1}}=\left\{\zeta\left(t_k^{\mathbf{1}}\right),\ \ k=1,\dots,w_n\right\},
</code>
</p>
 
<p>where  <code class="reqn">t_k^{\mathbf{1}}=t_{\left[(2k-1)q_n/2\right]}</code> and  <code class="reqn">\left[z\right]</code> denotes the smallest integer not less than the real number <code class="reqn">z</code>.The size (cardinal) of this subset is provided to the program in the argument <code>wn</code> (which contains a sequence of eligible sizes).
</p>
</li>
<li><p> Consider the following reduced model, which involves only the <code class="reqn">w_n</code> linear covariates belonging to <code class="reqn">\mathcal{R}_n^{\mathbf{1}}</code>:
</p>
<p style="text-align: center;"><code class="reqn">
	Y_i=\sum_{k=1}^{w_n}\beta_{0k}^{\mathbf{1}}\zeta_i(t_k^{\mathbf{1}})+\varepsilon_i^{\mathbf{1}}.
</code>
</p>

<p>The penalised least-squares variable selection procedure is applied to the reduced model. This is done by means of the function <code><a href="#topic+lm.pels.fit">lm.pels.fit</a></code>, which requires the remaining arguments (for details, see the documentation of the function <code><a href="#topic+lm.pels.fit">lm.pels.fit</a></code>). The estimates obtained after that are the outputs of the first step of the algorithm.
</p>
</li></ul>

</li>
<li> <p><b>Second step</b>. The variables selected in the first step and the variables in the neighbourhood of the ones selected are included. Then the penalised least-squares procedure is performed again. For that, we consider only the subsample <code class="reqn">\mathcal{E}^{\mathbf{2}}</code>. Specifically:
</p>

<ul>
<li><p> Consider a new set of variables :
</p>
<p style="text-align: center;"><code class="reqn">
		\mathcal{R}_n^{\mathbf{2}}=\bigcup_{\left\{k,\widehat{\beta}_{0k}^{\mathbf{1}}\not=0\right\}}\left\{\zeta(t_{(k-1)q_n+1}),\dots,\zeta(t_{kq_n})\right\}.
	</code>
</p>

<p>Denoting by <code class="reqn">r_n=\sharp(\mathcal{R}_n^{\mathbf{2}})</code>, we can rename the variables in <code class="reqn">\mathcal{R}_n^{\mathbf{2}}</code> as follows:
</p>
<p style="text-align: center;"><code class="reqn">
		\mathcal{R}_n^{\mathbf{2}}=\left\{\zeta(t_1^{\mathbf{2}}),\dots,\zeta(t_{r_n}^{\mathbf{2}})\right\},
		</code>
</p>

</li>
<li><p>  Consider the following model, which involves only the linear covariates belonging to <code class="reqn">\mathcal{R}_n^{\mathbf{2}}</code>
</p>
<p style="text-align: center;"><code class="reqn">
		Y_i=\sum_{k=1}^{r_n}\beta_{0k}^{\mathbf{2}}\zeta_i(t_k^{\mathbf{2}})+\varepsilon_i^{\mathbf{2}}.</code>
</p>

<p>The penalized least-squares variable selection procedure, with kernel estimation, is applied to this model by means of the function <code><a href="#topic+lm.pels.fit">lm.pels.fit</a></code>. 
</p>
</li></ul>

</li></ol>

<p>The outputs of the second step are the estimates of the model obtained with the PVS algorithm. For further details on this algorithm, see Aneiros and Vieu (2014).
</p>
<p><b>Remark</b>: If the condition  <code class="reqn">p_n=w_n q_n</code> fails, the function considers not fixed  <code class="reqn">q_n=q_{n,k}</code> values <code class="reqn">k=1,\dots,w_n</code>,  when <code class="reqn">p_n/w_n</code> is not an integer number. Specifically:
</p>
<p style="text-align: center;"><code class="reqn">
	q_{n,k}= \left\{\begin{array}{ll}
	[p_n/w_n]+1 &amp;   k\in\{1,\dots,p_n-w_n[p_n/w_n]\},\\
	{[p_n/w_n]} &amp; k\in\{p_n-w_n[p_n/w_n]+1,\dots,w_n\},
	\end{array}
	\right.
</code>
</p>

<p>where <code class="reqn">[z]</code> denotes the integer part of the real number <code class="reqn">z</code>.
</p>


<h3>Value</h3>

<table>
<tr><td><code>call</code></td>
<td>
<p>The matched call.</p>
</td></tr>
<tr><td><code>fitted.values</code></td>
<td>
<p>Estimated scalar response.</p>
</td></tr>
<tr><td><code>residuals</code></td>
<td>
<p>Differences between <code>y</code> and the <code>fitted.values</code></p>
</td></tr>
<tr><td><code>beta.est</code></td>
<td>
<p><code class="reqn">\hat{\mathbf{\beta}}</code> (i. e. estimate of <code class="reqn">\mathbf{\beta}_0</code> when the optimal tuning parameters <code>w.opt</code> and <code>lambda.opt</code> are used).</p>
</td></tr>
<tr><td><code>indexes.beta.nonnull</code></td>
<td>
<p>Indexes of the non-zero <code class="reqn">\hat{\beta_{j}}</code>.</p>
</td></tr>
<tr><td><code>w.opt</code></td>
<td>
<p>Selected size for  <code class="reqn">\mathcal{R}_n^{\mathbf{1}}</code>.</p>
</td></tr>
<tr><td><code>lambda.opt</code></td>
<td>
<p>Selected value of the penalisation parameter <code class="reqn">\lambda</code> (when <code>w.opt</code> is considered).</p>
</td></tr>
<tr><td><code>IC</code></td>
<td>
<p>Value of the criterion function considered to select <code>w.opt</code> and <code>lambda.opt</code>.</p>
</td></tr>
<tr><td><code>beta2</code></td>
<td>
<p>Estimate of <code class="reqn">\mathbf{\beta}_0^{\mathbf{2}}</code> for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>indexes.beta.nonnull2</code></td>
<td>
<p>Indexes of the non-zero linear coefficients after the step 2 of the method for each value of the sequence <code>wn</code>.</p>
</td></tr> 
<tr><td><code>IC2</code></td>
<td>
<p>Optimal value of the criterion function in the second step for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>lambda2</code></td>
<td>
<p>Selected value of penalisation parameter in the second step for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>index02</code></td>
<td>
<p>Indexes of the covariates (in the whole set of <code class="reqn">p_n</code>) used to build <code class="reqn">\mathcal{R}_n^{\mathbf{2}}</code> for each value of the sequence <code>wn</code>.</p>
</td></tr> 
<tr><td><code>beta1</code></td>
<td>
<p>Estimate of <code class="reqn">\mathbf{\beta}_0^{\mathbf{1}}</code> for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>IC1</code></td>
<td>
<p>Optimal value of the criterion function in the first step for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>lambda1</code></td>
<td>
<p>Selected value of penalisation parameter in the first step for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>index01</code></td>
<td>
<p>Indexes of the covariates (in the whole set of <code class="reqn">p_n</code>) used to build <code class="reqn">\mathcal{R}_n^{\mathbf{1}}</code> for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>index1</code></td>
<td>
<p>Indexes of the non-zero linear coefficients after the step 1 of the method for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>...</code></td>
<td>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>German Aneiros Perez <a href="mailto:german.aneiros@udc.es">german.aneiros@udc.es</a> 
</p>
<p>Silvia Novo Diaz  <a href="mailto:snovo@est-econ.uc3m.es">snovo@est-econ.uc3m.es</a>
</p>


<h3>References</h3>

<p>Aneiros, G. and Vieu, P. (2014) Variable selection in infinite-dimensional problems. <em>Statistics  &amp; Probability Letters</em>, <b>94</b>, 12&ndash;20, <a href="https://doi.org/10.1016/j.spl.2014.06.025">doi:10.1016/j.spl.2014.06.025</a>.
</p>


<h3>See Also</h3>

<p>See also <code><a href="#topic+lm.pels.fit">lm.pels.fit</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Sugar)

y&lt;-Sugar$ash
z&lt;-Sugar$wave.240

#Outliers
index.y.25 &lt;- y &gt; 25
index.atip &lt;- index.y.25
(1:268)[index.atip]


#Dataset to model
z.sug&lt;- z[!index.atip,]
y.sug &lt;- y[!index.atip]

train&lt;-1:216

ptm=proc.time()
fit&lt;- PVS.fit(z=z.sug[train,], y=y.sug[train],train.1=1:108,train.2=109:216,
        lambda.min.h=0.2,criterion="BIC", penalty="grSCAD", max.iter=5000)
proc.time()-ptm

fit 
names(fit)

 
</code></pre>

<hr>
<h2 id='PVS.kernel.fit'>
PVS with kernel estimation
</h2><span id='topic+PVS.kernel.fit'></span>

<h3>Description</h3>

<p>This function computes the partitioning variable selection algorithm (PVS) for sparse  multi-functional partial linear regression.
</p>
<p>This algorithm involves the penalised least-squares regularization procedure combined with kernel estimation with Nadaraya-Watson weights.
The procedure requires an objective criterion (<code>criterion</code>) to select the number of covariates in the reduced model (<code>w.opt</code>), the bandwidth (<code>h.opt</code>) and the penalisation parameter (<code>lambda.opt</code>). 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>PVS.kernel.fit(x, z, y, train.1=NULL, train.2=NULL, semimetric = "deriv", q = NULL,
  min.q.h = 0.05, max.q.h = 0.5, h.seq = NULL, num.h = 10,
  range.grid = NULL, kind.of.kernel = "quad", nknot = NULL, lambda.min = NULL, 
  lambda.min.h = NULL, lambda.min.l = NULL, factor.pn = 1, 
  nlambda = 100, vn = ncol(z), nfolds = 10, seed = 123, wn = c(10, 15, 20),
  criterion = c("GCV", "BIC", "AIC", "k-fold-CV"), 
  penalty = c("grLasso", "grMCP", "grSCAD", "gel", "cMCP", "gBridge",
  "gLasso", "gMCP"), max.iter = 1000)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="PVS.kernel.fit_+3A_x">x</code></td>
<td>

<p>Matrix containing the observations of the functional covariate collected by row (functional nonparametric component).
</p>
</td></tr>
<tr><td><code id="PVS.kernel.fit_+3A_z">z</code></td>
<td>

<p>Matrix containing the observations of the functional covariate that is discretised collected by row (linear component).
</p>
</td></tr>
<tr><td><code id="PVS.kernel.fit_+3A_y">y</code></td>
<td>

<p>Vector containing the scalar response.
</p>
</td></tr>
<tr><td><code id="PVS.kernel.fit_+3A_train.1">train.1</code></td>
<td>

<p>Indexes of the data used as the training sample in the 1st step. The default is  <code>train.1&lt;-1:ceiling(n/2)</code>.
</p>
</td></tr>
<tr><td><code id="PVS.kernel.fit_+3A_train.2">train.2</code></td>
<td>

<p>Indexes of the data used as the training sample in the 2nd step. The default is <code>train.2&lt;-(ceiling(n/2)+1):n</code>.
</p>
</td></tr>
<tr><td><code id="PVS.kernel.fit_+3A_semimetric">semimetric</code></td>
<td>

<p>Semi-metric function.  Only <code>"deriv"</code> and <code>"pca"</code> are implemented.  By default <code>semimetric="deriv"</code>.
</p>
</td></tr>
<tr><td><code id="PVS.kernel.fit_+3A_q">q</code></td>
<td>

<p>Order of the derivative (if <code>semimetric="deriv"</code>) or number of principal components (if <code>semimetric="pca"</code>). The default values are 0 and 2, respectively.
</p>
</td></tr>
<tr><td><code id="PVS.kernel.fit_+3A_min.q.h">min.q.h</code></td>
<td>
<p>Order of the quantile of the set of distances between curves (computed with the provided <code>semimetric</code>) which gives the lower end of the sequence in which the bandwidth is selected. The default is 0.05.
</p>
</td></tr>
<tr><td><code id="PVS.kernel.fit_+3A_max.q.h">max.q.h</code></td>
<td>

<p>Order of the quantile of the set of distances between curves (computed with the provided <code>semimetric</code>) which gives the upper end of the sequence in which the bandwidth is selected. The default is 0.5.
</p>
</td></tr>
<tr><td><code id="PVS.kernel.fit_+3A_h.seq">h.seq</code></td>
<td>
<p>Vector containing the sequence of bandwidths. The default is a sequence of <code>num.h</code> equispaced bandwidths in the range constructed using <code>min.q.h</code> and <code>max.q.h</code>.
</p>
</td></tr>
<tr><td><code id="PVS.kernel.fit_+3A_num.h">num.h</code></td>
<td>
<p>Positive integer indicating the number of bandwiths in the grid. The default is 10.
</p>
</td></tr>
<tr><td><code id="PVS.kernel.fit_+3A_range.grid">range.grid</code></td>
<td>

<p>Vector of length 2 containing the endpoints of the grid at which the observations of the functional covariate <code>x</code> are evaluated (i.e. the range of the discretization). If <code>range.grid=NULL</code>, then <code>range.grid=c(1,p)</code> is considered, where <code>p</code> is the size of the discretization size of <code>x</code> (i.e. <code>ncol(x))</code>.
</p>
</td></tr>
<tr><td><code id="PVS.kernel.fit_+3A_kind.of.kernel">kind.of.kernel</code></td>
<td>

<p>The type of kernel function used. Only Epanechnikov kernel (<code>"quad"</code>) is available.
</p>
</td></tr>
<tr><td><code id="PVS.kernel.fit_+3A_nknot">nknot</code></td>
<td>

<p>Positive integer indicating the number of interior knots for the B-spline representation of the functional covariate. The default value is <code>(p - order.Bspline - 1)%/%2</code>.
</p>
</td></tr>
<tr><td><code id="PVS.kernel.fit_+3A_lambda.min">lambda.min</code></td>
<td>

<p>The smallest value for lambda (i. e., the smallest value  of the sequence in which <code>lambda.opt</code> is selected), as fraction of <code>lambda.max</code>.
The defaults is <code>lambda.min.l</code> if the number of observations is larger than <code>factor.pn</code> times the number of covariates and <code>lambda.min.h</code> otherwise.
</p>
</td></tr>
<tr><td><code id="PVS.kernel.fit_+3A_lambda.min.h">lambda.min.h</code></td>
<td>

<p>The smallest value of the sequence in which <code>lambda.opt</code> is selected if the number of observations is smaller than <code>factor.pn</code> times the number of scalar covariates. The default is 0.05. 
</p>
</td></tr>
<tr><td><code id="PVS.kernel.fit_+3A_lambda.min.l">lambda.min.l</code></td>
<td>

<p>The smallest value of the sequence in which <code>lambda.opt</code> is selected if the number of observations is larger than <code>factor.pn</code> times the number of scalar covariates. The default is 0.0001.
</p>
</td></tr>
<tr><td><code id="PVS.kernel.fit_+3A_factor.pn">factor.pn</code></td>
<td>

<p>Positive integer used to set <code>lambda.min</code>. The default value is 1.
</p>
</td></tr>
<tr><td><code id="PVS.kernel.fit_+3A_nlambda">nlambda</code></td>
<td>

<p>Positive integer indicating the number of values of the sequence in which <code>lambda.opt</code> is selected. The default is 100.
</p>
</td></tr>
<tr><td><code id="PVS.kernel.fit_+3A_vn">vn</code></td>
<td>
<p>Positive integer or vector of positive integers indicating the number of groups of consecutive variables to be penalised together. The default value is <code>vn=ncol(z)</code>, which leads to the individual penalisation of each scalar covariate.
</p>
</td></tr>
<tr><td><code id="PVS.kernel.fit_+3A_nfolds">nfolds</code></td>
<td>

<p>Positive integer indicating the number of cross-validation folds (used if <code>criterion="k-fold-CV"</code>). The default is 10.
</p>
</td></tr>
<tr><td><code id="PVS.kernel.fit_+3A_seed">seed</code></td>
<td>

<p>You may set the seed of the random number generator to obtain reproducible results (used if <code>criterion="k-fold-CV"</code>). The default is 123.
</p>
</td></tr>
<tr><td><code id="PVS.kernel.fit_+3A_wn">wn</code></td>
<td>

<p>A vector of positive integers indicating the eligible number of covariates of the reduced model. See the section <code>Details</code>. The default is <code>c(10,15,20)</code>.
</p>
</td></tr>
<tr><td><code id="PVS.kernel.fit_+3A_criterion">criterion</code></td>
<td>

<p>The criterion by which to select the regularization parameter <code>lambda.opt</code> and <code>k.opt</code>. One of <code>"GCV", "BIC", "AIC"</code> or <code>"k-fold-CV"</code>. The default is <code>"GCV"</code>.
</p>
</td></tr>
<tr><td><code id="PVS.kernel.fit_+3A_penalty">penalty</code></td>
<td>

<p>The penalty function to be applied in the penalized least squares procedure. Only &quot;grLasso&quot; and &quot;grSCAD&quot; are implemented.
</p>
</td></tr>
<tr><td><code id="PVS.kernel.fit_+3A_max.iter">max.iter</code></td>
<td>

<p>Maximum number of iterations (total across entire path). Default is 1000.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The multi-functional partial linear model (MFPLM) is given by the expression
</p>
<p style="text-align: center;"><code class="reqn">Y_i=\sum_{j=1}^{p_n}\beta_{0j}\zeta_i(t_j)+m\left(X_i\right)+\varepsilon_i,\ \ \ (i=1,\dots,n)</code>
</p>

<p>where 
</p>

<ul>
<li> <p><code class="reqn">Y_i</code> is a real random response and <code class="reqn">X_i</code> denotes a random element belonging to some semi-metric space <code class="reqn">\mathcal{H}</code>. The second functional predictor <code class="reqn">\zeta_i</code> is supposed to be a random curve defined on some interval <code class="reqn">[a,b]</code> which  is observed at the points <code class="reqn">a\leq t_1&lt;\dots&lt;t_{p_n}\leq b</code>. 
</p>
</li>
<li>  <p><code class="reqn">\mathbf{\beta}_0=(\beta_{01},\dots,\beta_{0p_n})^{\top}</code> is a vector of unknown real coefficients and <code class="reqn">m(\cdot)</code> denotes a smooth unknown real-valued link function. 
</p>
</li>
<li> <p><code class="reqn">\varepsilon_i</code> denotes the random error.
</p>
</li></ul>

<p>In  the MFPLM, we assume that only a few scalar variables from the set <code class="reqn">\{\zeta(t_1),\dots,\zeta(t_{p_n})\}</code>  form part of the model. Therefore, we must select the relevant variables in the linear component  (the impact points of the curve <code class="reqn">\zeta</code> on the response) and estimate the model.
</p>
<p>In this function, the MFPLM is fitted using the PVS. The PVS is an algorithm with two steps, so we split the sample into two independent subsamples (asymptotically of the same size <code class="reqn">n_1\sim n_2\sim n/2</code>), one of them to be used in the first stage of the method and the other in the second stage.
</p>
<p style="text-align: center;"><code class="reqn">
\mathcal{E}^{\mathbf{1}}=\{(\zeta_i,\mathcal{X}_i,Y_i),\quad i=1,\dots,n_1\},
</code>
</p>

<p style="text-align: center;"><code class="reqn">
\mathcal{E}^{\mathbf{2}}=\{(\zeta_i,\mathcal{X}_i,Y_i),\quad i=n_1+1,\dots,n_1+n_2=n\}.
</code>
</p>
 
<p>Note that these two subsamples are specified to the programme by means of the arguments <code>train.1</code> and <code>train.2</code>.
The superscript <code class="reqn">\mathbf{s}</code> with <code class="reqn">\mathbf{s}=\mathbf{1},\mathbf{2}</code> indicates the stage of the method in which the sample, function, variable or parameter is involved. 
</p>
<p>To explain the algorithm we assume, without lost of generality, that the number <code class="reqn">p_n</code> of linear covariates can be expressed as follows: <code class="reqn">p_n=q_nw_n</code> with <code class="reqn">q_n</code> and <code class="reqn">w_n</code> integers.
</p>

<ol>
<li> <p><b>First step</b>. A reduced model is consider, discarding many linear covariates. The penalised least-squares procedure is applied to the reduced model using only the subsample <code class="reqn">\mathcal{E}^{\mathbf{1}}</code>. Specifically:
</p>

<ul>
<li><p> Consider a subset of the initial <code class="reqn">p_n</code> linear covariates, which contains only <code class="reqn">w_n</code> equally spaced discretized observations of  <code class="reqn">\zeta</code> covering the whole interval  <code class="reqn">[a,b]</code>. This subset is the following:
</p>
<p style="text-align: center;"><code class="reqn">
	\mathcal{R}_n^{\mathbf{1}}=\left\{\zeta\left(t_k^{\mathbf{1}}\right),\ \ k=1,\dots,w_n\right\},
</code>
</p>
 
<p>where  <code class="reqn">t_k^{\mathbf{1}}=t_{\left[(2k-1)q_n/2\right]}</code> and  <code class="reqn">\left[z\right]</code> denotes the smallest integer not less than the real number  <code class="reqn">z</code>.The size (cardinal) of this subset is provided to the program in the argument <code>wn</code> (which contains a sequence of eligible sizes).
</p>
</li>
<li><p> Consider the following reduced model, which involves only the <code class="reqn">w_n</code> linear covariates belonging to <code class="reqn">\mathcal{R}_n^{\mathbf{1}}</code>:
</p>
<p style="text-align: center;"><code class="reqn">
	Y_i=\sum_{k=1}^{w_n}\beta_{0k}^{\mathbf{1}}\zeta_i(t_k^{\mathbf{1}})+m^{\mathbf{1}}\left(X_i\right)+\varepsilon_i^{\mathbf{1}}.
</code>
</p>

<p>The penalised least-squares variable selection procedure, with kernel estimation, is applied to the reduced model. This is done by means of the function <code><a href="#topic+sfpl.kernel.fit">sfpl.kernel.fit</a></code>, which requires the remaining arguments (for details, see the documentation of the function <code><a href="#topic+sfpl.kernel.fit">sfpl.kernel.fit</a></code>). The estimates obtained after that are the outputs of the first step of the algorithm.
</p>
</li></ul>

</li>
<li> <p><b>Second step</b>. The variables selected in the first step and the variables in the neighbourhood of the ones selected are included. Then the penalised least-squares procedure, combined with kernel estimation, is carried out again. For that, we consider only the subsample <code class="reqn">\mathcal{E}^{\mathbf{2}}</code>. Specifically:
</p>

<ul>
<li><p> Consider a new set of variables :
</p>
<p style="text-align: center;"><code class="reqn">
		\mathcal{R}_n^{\mathbf{2}}=\bigcup_{\left\{k,\widehat{\beta}_{0k}^{\mathbf{1}}\not=0\right\}}\left\{\zeta(t_{(k-1)q_n+1}),\dots,\zeta(t_{kq_n})\right\}.
	</code>
</p>

<p>Denoting by <code class="reqn">r_n=\sharp(\mathcal{R}_n^{\mathbf{2}})</code>, we can rename the variables in <code class="reqn">\mathcal{R}_n^{\mathbf{2}}</code> as follows:
</p>
<p style="text-align: center;"><code class="reqn">
		\mathcal{R}_n^{\mathbf{2}}=\left\{\zeta(t_1^{\mathbf{2}}),\dots,\zeta(t_{r_n}^{\mathbf{2}})\right\},
		</code>
</p>

</li>
<li><p>  Consider the following model, which involves only the linear covariates belonging to <code class="reqn">\mathcal{R}_n^{\mathbf{2}}</code>
</p>
<p style="text-align: center;"><code class="reqn">
		Y_i=\sum_{k=1}^{r_n}\beta_{0k}^{\mathbf{2}}\zeta_i(t_k^{\mathbf{2}})+m^{\mathbf{2}}\left(X_i\right)+\varepsilon_i^{\mathbf{2}}.</code>
</p>

<p>The penalized least-squares variable selection procedure, with kernel estimation, is applied to this model by means of the function <code><a href="#topic+sfpl.kernel.fit">sfpl.kernel.fit</a></code>. 
</p>
</li></ul>

</li></ol>

<p>The outputs of the second step are the estimates of the MFPLM obtained with the PVS algorithm. For further details on this algorithm, see Aneiros and Vieu (2015).
</p>
<p><b>Remark</b>: If the condition  <code class="reqn">p_n=w_n q_n</code> fails, the function considers not fixed  <code class="reqn">q_n=q_{n,k}</code> values <code class="reqn">k=1,\dots,w_n</code>,  when <code class="reqn">p_n/w_n</code> is not an integer number. Specifically:
</p>
<p style="text-align: center;"><code class="reqn">
	q_{n,k}= \left\{\begin{array}{ll}
	[p_n/w_n]+1 &amp;   k\in\{1,\dots,p_n-w_n[p_n/w_n]\},\\
	{[p_n/w_n]} &amp; k\in\{p_n-w_n[p_n/w_n]+1,\dots,w_n\},
	\end{array}
	\right.
</code>
</p>

<p>where <code class="reqn">[z]</code> denotes the integer part of the real number <code class="reqn">z</code>.
</p>


<h3>Value</h3>

<table>
<tr><td><code>call</code></td>
<td>
<p>The matched call.</p>
</td></tr>
<tr><td><code>fitted.values</code></td>
<td>
<p>Estimated scalar response.</p>
</td></tr>
<tr><td><code>residuals</code></td>
<td>
<p>Differences between <code>y</code> and the <code>fitted.values</code></p>
</td></tr>
<tr><td><code>beta.est</code></td>
<td>
<p><code class="reqn">\hat{\mathbf{\beta}}</code> (i. e. estimate of <code class="reqn">\mathbf{\beta}_0</code> when the optimal tuning parameters <code>w.opt</code>, <code>lambda.opt</code>, <code>vn.opt</code> and <code>h.opt</code> are used).</p>
</td></tr>
<tr><td><code>indexes.beta.nonnull</code></td>
<td>
<p>Indexes of the non-zero <code class="reqn">\hat{\beta_{j}}</code>.</p>
</td></tr>
<tr><td><code>h.opt</code></td>
<td>
<p>Selected bandwidth (when <code>w.opt</code> is considered).</p>
</td></tr>
<tr><td><code>w.opt</code></td>
<td>
<p>Selected size for  <code class="reqn">\mathcal{R}_n^{\mathbf{1}}</code>.</p>
</td></tr>
<tr><td><code>lambda.opt</code></td>
<td>
<p>Selected value of the penalisation parameter <code class="reqn">\lambda</code> (when <code>w.opt</code> is considered).</p>
</td></tr>
<tr><td><code>IC</code></td>
<td>
<p>Value of the criterion function considered to select <code>w.opt</code>, <code>lambda.opt</code>, <code>vn.opt</code> and <code>h.opt</code>.</p>
</td></tr>
<tr><td><code>vn.opt</code></td>
<td>
<p>Selected value of <code>vn</code> in the second step (when <code>w.opt</code> is considered).</p>
</td></tr>
<tr><td><code>beta2</code></td>
<td>
<p>Estimate of <code class="reqn">\mathbf{\beta}_0^{\mathbf{2}}</code> for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>indexes.beta.nonnull2</code></td>
<td>
<p>Indexes of the non-zero linear coefficients after the step 2 of the method for each value of the sequence <code>wn</code>.</p>
</td></tr> 
<tr><td><code>h2</code></td>
<td>
<p>Selected bandwidth in the second step of the algorithm for each value of the sequence <code>wn</code>.</p>
</td></tr> 
<tr><td><code>IC2</code></td>
<td>
<p>Optimal value of the criterion function in the second step for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>lambda2</code></td>
<td>
<p>Selected value of penalisation parameter in the second step for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>index02</code></td>
<td>
<p>Indexes of the covariates (in the whole set of <code class="reqn">p_n</code>) used to build <code class="reqn">\mathcal{R}_n^{\mathbf{2}}</code> for each value of the sequence <code>wn</code>.</p>
</td></tr> 
<tr><td><code>beta1</code></td>
<td>
<p>Estimate of <code class="reqn">\mathbf{\beta}_0^{\mathbf{1}}</code> for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>h1</code></td>
<td>
<p>Selected bandwidth in the first step of the algorithm for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>IC1</code></td>
<td>
<p>Optimal value of the criterion function in the first step for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>lambda1</code></td>
<td>
<p>Selected value of penalisation parameter in the first step for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>index01</code></td>
<td>
<p>Indexes of the covariates (in the whole set of <code class="reqn">p_n</code>) used to build <code class="reqn">\mathcal{R}_n^{\mathbf{1}}</code> for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>index1</code></td>
<td>
<p>Indexes of the non-zero linear coefficients after the step 1 of the method for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>...</code></td>
<td>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>German Aneiros Perez <a href="mailto:german.aneiros@udc.es">german.aneiros@udc.es</a> 
</p>
<p>Silvia Novo Diaz  <a href="mailto:snovo@est-econ.uc3m.es">snovo@est-econ.uc3m.es</a>
</p>


<h3>References</h3>

<p>Aneiros, G., and Vieu, P. (2015) Partial linear modelling with multi-functional covariates. <em>Computational Statistics</em>, <b>30</b>, 647&ndash;671, <a href="https://doi.org/10.1007/s00180-015-0568-8">doi:10.1007/s00180-015-0568-8</a>.
</p>


<h3>See Also</h3>

<p>See also <code><a href="#topic+sfpl.kernel.fit">sfpl.kernel.fit</a>, <a href="#topic+predict.PVS.kernel">predict.PVS.kernel</a></code> and <code><a href="#topic+plot.PVS.kernel">plot.PVS.kernel</a></code>.
</p>
<p>Alternative method <code><a href="#topic+PVS.kNN.fit">PVS.kNN.fit</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(Sugar)


y&lt;-Sugar$ash
x&lt;-Sugar$wave.290
z&lt;-Sugar$wave.240

#Outliers
index.y.25 &lt;- y &gt; 25
index.atip &lt;- index.y.25
(1:268)[index.atip]


#Dataset to model
x.sug &lt;- x[!index.atip,]
z.sug&lt;- z[!index.atip,]
y.sug &lt;- y[!index.atip]

train&lt;-1:216

ptm=proc.time()
fit&lt;- PVS.kernel.fit(x=x.sug[train,],z=z.sug[train,], y=y.sug[train],
        train.1=1:108,train.2=109:216,lambda.min.h=0.03, 
        lambda.min.l=0.03,  max.q.h=0.35, num.h = 10, nknot=20,
        criterion="BIC", penalty="grSCAD", max.iter=5000)
proc.time()-ptm

fit 
names(fit)

</code></pre>

<hr>
<h2 id='PVS.kNN.fit'>
PVS with kNN estimation
</h2><span id='topic+PVS.kNN.fit'></span>

<h3>Description</h3>

<p>This function computes the partitioning variable selection algorithm (PVS) for sparse  multi-functional partial linear regression.
</p>
<p>This algorithm involves the penalised least-squares regularization procedure combined with k-nearest neighbours (kNN) estimation with Nadaraya-Watson weights.
The procedure requires an objective criterion (<code>criterion</code>) to select the number of covariates in the reduced model (<code>w.opt</code>), the bandwidth (<code>k.opt</code>) and the penalisation parameter (<code>lambda.opt</code>). 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>PVS.kNN.fit(x, z, y, train.1=NULL, train.2=NULL, semimetric = "deriv", q = NULL, 
  knearest = NULL, min.knn = 2, max.knn = NULL, step = NULL, 
  range.grid = NULL, kind.of.kernel = "quad", nknot = NULL, lambda.min = NULL,
  lambda.min.h = NULL,lambda.min.l = NULL, factor.pn = 1,
  nlambda = 100, vn = ncol(z), nfolds = 10, seed = 123, wn = c(10, 15, 20),
  criterion = c("GCV", "BIC", "AIC", "k-fold-CV"),  
  penalty = c("grLasso", "grMCP", "grSCAD", "gel", "cMCP", "gBridge", 
  "gLasso", "gMCP"), max.iter = 1000)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="PVS.kNN.fit_+3A_x">x</code></td>
<td>

<p>Matrix containing the observations of the functional covariate collected by row (functional nonparametric component).
</p>
</td></tr>
<tr><td><code id="PVS.kNN.fit_+3A_z">z</code></td>
<td>

<p>Matrix containing the observations of the functional covariate that is discretised collected by row (linear component).
</p>
</td></tr>
<tr><td><code id="PVS.kNN.fit_+3A_y">y</code></td>
<td>

<p>Vector containing the scalar response.
</p>
</td></tr>
<tr><td><code id="PVS.kNN.fit_+3A_train.1">train.1</code></td>
<td>

<p>Indexes of the data used as the training sample in the 1st step. The default is <code>train.1&lt;-1:ceiling(n/2)</code>.
</p>
</td></tr>
<tr><td><code id="PVS.kNN.fit_+3A_train.2">train.2</code></td>
<td>

<p>Indexes of the data used as the training sample in the 2nd step. The default is <code>train.2&lt;-(ceiling(n/2)+1):n</code>.
</p>
</td></tr>
<tr><td><code id="PVS.kNN.fit_+3A_semimetric">semimetric</code></td>
<td>

<p>Semi-metric function. Only <code>"deriv"</code> and <code>"pca"</code> are implemented. By default <code>semimetric="deriv"</code>.
</p>
</td></tr>
<tr><td><code id="PVS.kNN.fit_+3A_q">q</code></td>
<td>

<p>Order of the derivative (if <code>semimetric="deriv"</code>) or number of principal components (if <code>semimetric="pca"</code>). The default values are 0 and 2, respectively.
</p>
</td></tr>
<tr><td><code id="PVS.kNN.fit_+3A_knearest">knearest</code></td>
<td>

<p>Sequence of eligible values for <code class="reqn">k</code> considered to seek for <code>k.opt</code>. If <code>knearest=NULL</code>, then <code>knearest &lt;- seq(from =min.knn, to = max.knn, by = step)</code>.
</p>
</td></tr>
<tr><td><code id="PVS.kNN.fit_+3A_min.knn">min.knn</code></td>
<td>

<p>Positive integer indicating the minumum value of the sequence in which the  number of nearest neighbours <code>k.opt</code> is selected (thus, this number must be smaller than the sample size). The default is 2.
</p>
</td></tr>
<tr><td><code id="PVS.kNN.fit_+3A_max.knn">max.knn</code></td>
<td>

<p>Positive integer indicating the maximum value of the sequence in which the number of nearest neighbours <code>k.opt</code> is selected (thus, this number must be larger than <code>min.kNN</code> and smaller than the sample size). The default is <code>max.knn &lt;- n%/%2</code>, being <code class="reqn">n=n_1</code> in the first step and <code class="reqn">n=n_2</code> in the second step of the method (see section <code>Details</code>).</p>
</td></tr>
<tr><td><code id="PVS.kNN.fit_+3A_step">step</code></td>
<td>

<p>Positive integer used to build the sequence of k-nearest neighbours in the following way: <code>min.knn, min.knn + step, min.knn + 2*step, min.knn + 3*step,...</code> The default is <code>step&lt;-ceiling(n/100)</code>, being <code class="reqn">n=n_1</code> in the 1st step and <code class="reqn">n=n_2</code> in the 2nd step of the method.
</p>
</td></tr>
<tr><td><code id="PVS.kNN.fit_+3A_range.grid">range.grid</code></td>
<td>

<p>Vector of length 2 containing the endpoints of the grid at which the observations of the functional covariate <code>x</code> are evaluated (i.e. the range of the discretization). If <code>range.grid=NULL</code>, then <code>range.grid=c(1,p)</code> is considered, where <code>p</code> is the size of the discretization size of <code>x</code> (i.e. <code>ncol(x))</code>.
</p>
</td></tr>
<tr><td><code id="PVS.kNN.fit_+3A_kind.of.kernel">kind.of.kernel</code></td>
<td>

<p>The type of kernel function used. Only Epanechnikov kernel (<code>"quad"</code>) is available.
</p>
</td></tr>
<tr><td><code id="PVS.kNN.fit_+3A_nknot">nknot</code></td>
<td>

<p>Positive integer indicating the number of interior knots for the B-spline representation of the functional covariate. The default value is <code>(p - order.Bspline - 1)%/%2</code>.
</p>
</td></tr>
<tr><td><code id="PVS.kNN.fit_+3A_lambda.min">lambda.min</code></td>
<td>

<p>The smallest value for lambda (i. e., the smallest value  of the sequence in which <code>lambda.opt</code> is selected), as fraction of <code>lambda.max</code>.
The defaults is <code>lambda.min.l</code> if the number of observations is larger than <code>factor.pn</code> times the number of covariates and <code>lambda.min.h</code> otherwise.
</p>
</td></tr>
<tr><td><code id="PVS.kNN.fit_+3A_lambda.min.h">lambda.min.h</code></td>
<td>

<p>The smallest value of the sequence in which <code>lambda.opt</code> is selected if the number of observations is smaller than <code>factor.pn</code> times the number of scalar covariates. The default is 0.05. 
</p>
</td></tr>
<tr><td><code id="PVS.kNN.fit_+3A_lambda.min.l">lambda.min.l</code></td>
<td>

<p>The smallest value of the sequence in which <code>lambda.opt</code> is selected if the number of observations is larger than <code>factor.pn</code> times the number of scalar covariates. The default is 0.0001.
</p>
</td></tr>
<tr><td><code id="PVS.kNN.fit_+3A_factor.pn">factor.pn</code></td>
<td>

<p>Positive integer used to set <code>lambda.min</code>. The default value is 1.
</p>
</td></tr>
<tr><td><code id="PVS.kNN.fit_+3A_nlambda">nlambda</code></td>
<td>

<p>Positive integer indicating the number of values of the sequence in which <code>lambda.opt</code> is selected. The default is 100.
</p>
</td></tr>
<tr><td><code id="PVS.kNN.fit_+3A_vn">vn</code></td>
<td>

<p>Positive integer or vector of positive integers indicating the number of groups of consecutive variables to be penalised together. The default value is <code>vn=ncol(z)</code>, which leads to the individual penalisation of each scalar covariate.
</p>
</td></tr>
<tr><td><code id="PVS.kNN.fit_+3A_nfolds">nfolds</code></td>
<td>

<p>Positive integer indicating the number of cross-validation folds (used if <code>criterion="k-fold-CV"</code>). The default is 10.
</p>
</td></tr>
<tr><td><code id="PVS.kNN.fit_+3A_seed">seed</code></td>
<td>

<p>You may set the seed of the random number generator to obtain reproducible results (used if <code>criterion="k-fold-CV"</code>). Default is 123.
</p>
</td></tr>
<tr><td><code id="PVS.kNN.fit_+3A_wn">wn</code></td>
<td>

<p>A vector of positive integers indicating the eligible number of covariates of the reduced model. See the section <code>Details</code>. The default is <code>c(10,15,20)</code>.
</p>
</td></tr>
<tr><td><code id="PVS.kNN.fit_+3A_criterion">criterion</code></td>
<td>

<p>The criterion by which to select the regularization parameter <code>lambda.opt</code> and <code>k.opt</code>. One of <code>"GCV", "BIC", "AIC"</code> or <code>"k-fold-CV"</code>. The default is <code>"GCV"</code>.
</p>
</td></tr>
<tr><td><code id="PVS.kNN.fit_+3A_penalty">penalty</code></td>
<td>

<p>The penalty function to be applied in the penalized least squares procedure. Only &quot;grLasso&quot; and &quot;grSCAD&quot; are implemented.
</p>
</td></tr>
<tr><td><code id="PVS.kNN.fit_+3A_max.iter">max.iter</code></td>
<td>

<p>Maximum number of iterations (total across entire path). The default is 1000.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The multi-functional partial linear model (MFPLM) is given by the expression
</p>
<p style="text-align: center;"><code class="reqn">Y_i=\sum_{j=1}^{p_n}\beta_{0j}\zeta_i(t_j)+m\left(X_i\right)+\varepsilon_i,\ \ \ (i=1,\dots,n)</code>
</p>

<p>where 
</p>

<ul>
<li> <p><code class="reqn">Y_i</code> is a real random response and <code class="reqn">X_i</code> denotes a random element belonging to some semi-metric space <code class="reqn">\mathcal{H}</code>. The second functional predictor <code class="reqn">\zeta_i</code> is supposed to be a random curve defined on some interval <code class="reqn">[a,b]</code> which  is observed at the points <code class="reqn">a\leq t_1&lt;\dots&lt;t_{p_n}\leq b</code>. 
</p>
</li>
<li>  <p><code class="reqn">\mathbf{\beta}_0=(\beta_{01},\dots,\beta_{0p_n})^{\top}</code> is a vector of unknown real coefficients and <code class="reqn">m(\cdot)</code> denotes a smooth unknown real-valued link function. 
</p>
</li>
<li> <p><code class="reqn">\varepsilon_i</code> denotes the random error.
</p>
</li></ul>

<p>In  the MFPLM, we assume that only a few scalar variables from the set <code class="reqn">\{\zeta(t_1),\dots,\zeta(t_{p_n})\}</code>  form part of the model. Therefore, we must select the relevant variables in the linear component  (the impact points of the curve <code class="reqn">\zeta</code> on the response) and estimate the model.
</p>
<p>In this function, the MFPLM is fitted using the PVS. The PVS is an algorithm with two steps, so we split the sample into two independent subsamples (asymptotically of the same size <code class="reqn">n_1\sim n_2\sim n/2</code>), one of them to be used in the first stage of the method and the other in the second stage.
</p>
<p style="text-align: center;"><code class="reqn">
\mathcal{E}^{\mathbf{1}}=\{(\zeta_i,\mathcal{X}_i,Y_i),\quad i=1,\dots,n_1\},
</code>
</p>

<p style="text-align: center;"><code class="reqn">
\mathcal{E}^{\mathbf{2}}=\{(\zeta_i,\mathcal{X}_i,Y_i),\quad i=n_1+1,\dots,n_1+n_2=n\}.
</code>
</p>
 
<p>Note that these two subsamples are specified to the programme by means of the arguments <code>train.1</code> and <code>train.2</code>.
The superscript <code class="reqn">\mathbf{s}</code> with <code class="reqn">\mathbf{s}=\mathbf{1},\mathbf{2}</code> indicates the stage of the method in which the sample, function, variable or parameter is involved. 
</p>
<p>To explain the algorithm we assume, without lost of generality, that the number <code class="reqn">p_n</code> of linear covariates can be expressed as follows: <code class="reqn">p_n=q_nw_n</code> with <code class="reqn">q_n</code> and <code class="reqn">w_n</code> integers.
</p>

<ol>
<li> <p><b>First step</b>. A reduced model is consider, discarding many linear covariates. The penalised least-squares procedure is applied to the reduced model using only the subsample <code class="reqn">\mathcal{E}^{\mathbf{1}}</code>. Specifically:
</p>

<ul>
<li><p> Consider a subset of the initial <code class="reqn">p_n</code> linear covariates, which contains only <code class="reqn">w_n</code> equally spaced discretized observations of  <code class="reqn">\zeta</code> covering the whole interval  <code class="reqn">[a,b]</code>. This subset is the following:
</p>
<p style="text-align: center;"><code class="reqn">
	\mathcal{R}_n^{\mathbf{1}}=\left\{\zeta\left(t_k^{\mathbf{1}}\right),\ \ k=1,\dots,w_n\right\},
</code>
</p>
 
<p>where  <code class="reqn">t_k^{\mathbf{1}}=t_{\left[(2k-1)q_n/2\right]}</code> and  <code class="reqn">\left[z\right]</code> denotes the smallest integer not less than the real number  <code class="reqn">z</code>.The size (cardinal) of this subset is provided to the program in the argument <code>wn</code> (which contains a sequence of eligible sizes).
</p>
</li>
<li><p> Consider the following reduced model, which involves only the <code class="reqn">w_n</code> linear covariates belonging to <code class="reqn">\mathcal{R}_n^{\mathbf{1}}</code>:
</p>
<p style="text-align: center;"><code class="reqn">
	Y_i=\sum_{k=1}^{w_n}\beta_{0k}^{\mathbf{1}}\zeta_i(t_k^{\mathbf{1}})+m^{\mathbf{1}}\left(X_i\right)+\varepsilon_i^{\mathbf{1}}.
</code>
</p>

<p>The penalised least-squares variable selection procedure, with kNN estimation, is applied to the reduced model. This is done by means of the function <code><a href="#topic+sfpl.kNN.fit">sfpl.kNN.fit</a></code>, which requires the remaining arguments (for details, see the documentation of the function <code><a href="#topic+sfpl.kNN.fit">sfpl.kNN.fit</a></code>). The estimates obtained after that are the outputs of the first step of the algorithm.
</p>
</li></ul>

</li>
<li> <p><b>Second step</b>. The variables selected in the first step and the variables in the neighbourhood of the ones selected are included. Then the penalised least-squares procedure, combined with kernel estimation, is carried out again. For that, we consider only the subsample <code class="reqn">\mathcal{E}^{\mathbf{2}}</code>. Specifically:
</p>

<ul>
<li><p> Consider a new set of variables :
</p>
<p style="text-align: center;"><code class="reqn">
		\mathcal{R}_n^{\mathbf{2}}=\bigcup_{\left\{k,\widehat{\beta}_{0k}^{\mathbf{1}}\not=0\right\}}\left\{\zeta(t_{(k-1)q_n+1}),\dots,\zeta(t_{kq_n})\right\}.
	</code>
</p>

<p>Denoting by <code class="reqn">r_n=\sharp(\mathcal{R}_n^{\mathbf{2}})</code>, we can rename the variables in <code class="reqn">\mathcal{R}_n^{\mathbf{2}}</code> as follows:
</p>
<p style="text-align: center;"><code class="reqn">
		\mathcal{R}_n^{\mathbf{2}}=\left\{\zeta(t_1^{\mathbf{2}}),\dots,\zeta(t_{r_n}^{\mathbf{2}})\right\},
		</code>
</p>

</li>
<li><p>  Consider the following model, which involves only the linear covariates belonging to <code class="reqn">\mathcal{R}_n^{\mathbf{2}}</code>
</p>
<p style="text-align: center;"><code class="reqn">
		Y_i=\sum_{k=1}^{r_n}\beta_{0k}^{\mathbf{2}}\zeta_i(t_k^{\mathbf{2}})+m^{\mathbf{2}}\left(X_i\right)+\varepsilon_i^{\mathbf{2}}.</code>
</p>

<p>The penalized least-squares variable selection procedure, with kNN estimation, is applied to this model by means of the function <code><a href="#topic+sfpl.kNN.fit">sfpl.kNN.fit</a></code>. 
</p>
</li></ul>

</li></ol>

<p>The outputs of the second step are the estimates of the MFPLM obtained with the PVS algorithm. For further details on this algorithm, see Aneiros and Vieu (2015).
</p>
<p><b>Remark</b>: If the condition  <code class="reqn">p_n=w_n q_n</code> fails, the function considers not fixed  <code class="reqn">q_n=q_{n,k}</code> values <code class="reqn">k=1,\dots,w_n</code>,  when <code class="reqn">p_n/w_n</code> is not an integer number. Specifically:
</p>
<p style="text-align: center;"><code class="reqn">
	q_{n,k}= \left\{\begin{array}{ll}
	[p_n/w_n]+1 &amp;   k\in\{1,\dots,p_n-w_n[p_n/w_n]\},\\
	{[p_n/w_n]} &amp; k\in\{p_n-w_n[p_n/w_n]+1,\dots,w_n\},
	\end{array}
	\right.
</code>
</p>

<p>where <code class="reqn">[z]</code> denotes the integer part of the real number <code class="reqn">z</code>.
</p>


<h3>Value</h3>

<table>
<tr><td><code>call</code></td>
<td>
<p>The matched call.</p>
</td></tr>
<tr><td><code>fitted.values</code></td>
<td>
<p>Estimated scalar response.</p>
</td></tr>
<tr><td><code>residuals</code></td>
<td>
<p>Differences between <code>y</code> and the <code>fitted.values</code></p>
</td></tr>
<tr><td><code>beta.est</code></td>
<td>
<p><code class="reqn">\hat{\mathbf{\beta}}</code> (i.e. estimate of <code class="reqn">\mathbf{\beta}_0</code> when the optimal tuning parameters <code>w.opt</code>, <code>lambda.opt</code>, <code>vn.opt</code> and <code>k.opt</code> are used).</p>
</td></tr>
<tr><td><code>indexes.beta.nonnull</code></td>
<td>
<p>Indexes of the non-zero <code class="reqn">\hat{\beta_{j}}</code>.</p>
</td></tr>
<tr><td><code>k.opt</code></td>
<td>
<p>Selected number of nearest neighbours (when <code>w.opt</code> is considered).</p>
</td></tr>
<tr><td><code>w.opt</code></td>
<td>
<p>Selected initial number of covariates in the reduced model.</p>
</td></tr>
<tr><td><code>lambda.opt</code></td>
<td>
<p>Selected value of the penalisation parameter <code class="reqn">\lambda</code> (when <code>w.opt</code> is considered).</p>
</td></tr>
<tr><td><code>IC</code></td>
<td>
<p>Value of the criterion function considered to select <code>w.opt</code>, <code>lambda.opt</code>, <code>vn.opt</code> and <code>k.opt</code>.</p>
</td></tr>
<tr><td><code>vn.opt</code></td>
<td>
<p>Selected value of <code>vn</code> in the second step (when <code>w.opt</code> is considered).</p>
</td></tr>
<tr><td><code>beta2</code></td>
<td>
<p>Estimate of <code class="reqn">\mathbf{\beta}_0^{\mathbf{2}}</code> for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>indexes.beta.nonnull2</code></td>
<td>
<p>Indexes of the non-zero linear coefficients after the step 2 of the method for each value of the sequence <code>wn</code>.</p>
</td></tr> 
<tr><td><code>knn2</code></td>
<td>
<p>Selected number of neighbours in the second step of the algorithm for each value of the sequence <code>wn</code>.</p>
</td></tr> 
<tr><td><code>IC2</code></td>
<td>
<p>Optimal value of the criterion function in the second step for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>lambda2</code></td>
<td>
<p>Selected value of penalisation parameter in the second step for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>index02</code></td>
<td>
<p>Indexes of the covariates (in the whole set of <code class="reqn">p_n</code>) used to build <code class="reqn">\mathcal{R}_n^{\mathbf{2}}</code> for each value of the sequence <code>wn</code>.</p>
</td></tr> 
<tr><td><code>beta1</code></td>
<td>
<p>Estimate of <code class="reqn">\mathbf{\beta}_0^{\mathbf{1}}</code> for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>knn1</code></td>
<td>
<p>Selected number of neighbours in the first step of the algorithm for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>IC1</code></td>
<td>
<p>Optimal value of the criterion function in the first step for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>lambda1</code></td>
<td>
<p>Selected value of penalisation parameter in the first step for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>index01</code></td>
<td>
<p>Indexes of the covariates (in the whole set of <code class="reqn">p_n</code>) used to build <code class="reqn">\mathcal{R}_n^{\mathbf{1}}</code> for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>index1</code></td>
<td>
<p>Indexes of the non-zero linear coefficients after the step 1 of the method for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>...</code></td>
<td>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>German Aneiros Perez <a href="mailto:german.aneiros@udc.es">german.aneiros@udc.es</a> 
</p>
<p>Silvia Novo Diaz  <a href="mailto:snovo@est-econ.uc3m.es">snovo@est-econ.uc3m.es</a>
</p>


<h3>References</h3>

<p>Aneiros, G., and Vieu, P. (2015) Partial linear modelling with multi-functional covariates. <em>Computational Statistics</em>, <b>30</b>, 647&ndash;671, <a href="https://doi.org/10.1007/s00180-015-0568-8">doi:10.1007/s00180-015-0568-8</a>.
</p>


<h3>See Also</h3>

<p>See also <code><a href="#topic+sfpl.kNN.fit">sfpl.kNN.fit</a>, <a href="#topic+predict.PVS.kNN">predict.PVS.kNN</a></code> and <code><a href="#topic+plot.PVS.kNN">plot.PVS.kNN</a></code>.
</p>
<p>Alternative method <code><a href="#topic+PVS.kernel.fit">PVS.kernel.fit</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(Sugar)


y&lt;-Sugar$ash
x&lt;-Sugar$wave.290
z&lt;-Sugar$wave.240

#Outliers
index.y.25 &lt;- y &gt; 25
index.atip &lt;- index.y.25
(1:268)[index.atip]


#Dataset to model
x.sug &lt;- x[!index.atip,]
z.sug&lt;- z[!index.atip,]
y.sug &lt;- y[!index.atip]

train&lt;-1:216

ptm=proc.time()
fit&lt;- PVS.kNN.fit(x=x.sug[train,],z=z.sug[train,], y=y.sug[train],
        train.1=1:108,train.2=109:216,lambda.min.h=0.07, 
        lambda.min.l=0.07, nknot=20,criterion="BIC", penalty="grSCAD", 
        max.iter=5000)
proc.time()-ptm

fit 
names(fit)

    
    
</code></pre>

<hr>
<h2 id='semimetric.projec'>
Projection semi-metric computation
</h2><span id='topic+semimetric.projec'></span>

<h3>Description</h3>

<p>Computes the projection semi-metric in a direction <code class="reqn">\theta</code> between each curve in <code>data1</code> and each curve in <code>data2</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>semimetric.projec(data1, data2, theta, order.Bspline = 3, nknot.theta = 3,
  range.grid = NULL, nknot = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="semimetric.projec_+3A_data1">data1</code></td>
<td>

<p>Matrix containing functional data collected by row.
</p>
</td></tr>
<tr><td><code id="semimetric.projec_+3A_data2">data2</code></td>
<td>

<p>Matrix containing functional data collected by row.
</p>
</td></tr>
<tr><td><code id="semimetric.projec_+3A_theta">theta</code></td>
<td>

<p>Vector containing the coefficients of <code class="reqn">\theta</code> in a B-spline basis, so that <code>length(theta)=order.Bspline+nknot.theta</code>.
</p>
</td></tr>
<tr><td><code id="semimetric.projec_+3A_order.bspline">order.Bspline</code></td>
<td>

<p>Positive integer giving the order of the B-spline basis functions for the B-spline representation of <code class="reqn">\theta</code>. This is the number of coefficients in each piecewise polynomial segment. The default is 3.
</p>
</td></tr>
<tr><td><code id="semimetric.projec_+3A_nknot.theta">nknot.theta</code></td>
<td>

<p>Positive integer indicating the number of uniform interior knots of the B-spline basis. The default is 3.
</p>
</td></tr>
<tr><td><code id="semimetric.projec_+3A_range.grid">range.grid</code></td>
<td>

<p>Vector of length 2 containing  the range of the discretization of the functional data. If <code>range.grid=NULL</code>, then <code>range.grid=c(1,p)</code> is considered, where <code>p</code> is the size of the discretization size of <code>data</code> (i.e. <code>ncol(data)</code>).
</p>
</td></tr>
<tr><td><code id="semimetric.projec_+3A_nknot">nknot</code></td>
<td>

<p>Positive integer indicating the number of interior knots for the B-spline representation of the functional data. The default value is <code>(p - order.Bspline - 1)%/%2</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For <code class="reqn">x_1,x_2 \in \mathcal{H}, </code> being <code class="reqn">\mathcal{H}</code> a separable Hilbert space, the projection semi-metric in the direction <code class="reqn">\theta\in \mathcal{H}</code> is defined as   </p>
<p style="text-align: center;"><code class="reqn">d_{\theta}(x_1,x_2)=|\langle\theta,x_1-x_2\rangle|.</code>
</p>

<p>The function <code>semimetric.projec</code> computes the projection semi-metric using the B-spline representation of the curves and <code class="reqn">\theta</code>. The dimension of the B-spline basis for <code class="reqn">\theta</code> is <code>order.Bspline</code>+<code>nknot.theta</code>.
</p>


<h3>Value</h3>

<p>A matrix with the projection semi-semimetrics of each pair of curves.
</p>


<h3>Author(s)</h3>

<p>German Aneiros Perez <a href="mailto:german.aneiros@udc.es">german.aneiros@udc.es</a> 
</p>
<p>Silvia Novo Diaz  <a href="mailto:snovo@est-econ.uc3m.es">snovo@est-econ.uc3m.es</a>
</p>


<h3>References</h3>

<p>Novo S., Aneiros, G., and Vieu, P., (2019) Automatic and location-adaptive estimation in functional single&ndash;index regression. <em>Journal of Nonparametric Statistics</em>, <b>31(2)</b>, 364&ndash;392, <a href="https://doi.org/10.1080/10485252.2019.1567726">doi:10.1080/10485252.2019.1567726</a>.
</p>


<h3>See Also</h3>

<p>See also <code><a href="#topic+projec">projec</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data("Tecator")
names(Tecator)
y&lt;-Tecator$fat
X&lt;-Tecator$absor.spectra

#length(theta)=6=order.Bspline+nknot.theta 
semimetric.projec(data1=X[1:5,], data2=X[5:10,],theta=c(1,0,0,1,1,-1),
  nknot.theta=3,nknot=20,range.grid=c(850,1050))

</code></pre>

<hr>
<h2 id='sfpl.kernel.fit'>
Sparse semi-functional partial linear model fit using kernel estimation
</h2><span id='topic+sfpl.kernel.fit'></span>

<h3>Description</h3>

<p>This function fits a sparse semi-functional partial linear model between a scalar response, a functional explanatory variable and
a vector of scalar covariates. 
The function uses the penalised least-squares regularization procedure combined with nonparametric kernel estimation with Nadaraya-Watson weights.
</p>
<p>The procedure requires an objective criterion (<code>criterion</code>) to select the bandwidth (<code>h.opt</code>) and the regularization parameter (<code>lambda.opt</code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sfpl.kernel.fit(x, z, y, semimetric = "deriv", q = NULL, min.q.h = 0.05,
  max.q.h = 0.5, h.seq = NULL,num.h = 10,range.grid = NULL, 
  kind.of.kernel = "quad", nknot = NULL, lambda.min = NULL, 
  lambda.min.h = NULL, lambda.min.l = NULL, factor.pn = 1, 
  nlambda = 100, lambda.seq = NULL, vn = ncol(z), nfolds = 10, seed = 123,
  criterion = c("GCV", "BIC", "AIC", "k-fold-CV"),
  penalty = c("grLasso", "grMCP", "grSCAD", "gel", "cMCP", "gBridge", "gLasso",
  "gMCP"),max.iter = 1000)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sfpl.kernel.fit_+3A_x">x</code></td>
<td>

<p>Matrix containing the observations of the functional covariate collected by row.
</p>
</td></tr>
<tr><td><code id="sfpl.kernel.fit_+3A_z">z</code></td>
<td>

<p>Matrix containing the observations of the scalar covariates collected by row.
</p>
</td></tr>
<tr><td><code id="sfpl.kernel.fit_+3A_y">y</code></td>
<td>

<p>Vector containing the scalar response.
</p>
</td></tr>
<tr><td><code id="sfpl.kernel.fit_+3A_semimetric">semimetric</code></td>
<td>

<p>Semi-metric function.  Only <code>"deriv"</code> and <code>"pca"</code> are implemented.  By default <code>semimetric="deriv"</code>.
</p>
</td></tr>
<tr><td><code id="sfpl.kernel.fit_+3A_q">q</code></td>
<td>

<p>Order of the derivative (if <code>semimetric="deriv"</code>) or number of principal components (if <code>semimetric="pca"</code>). The default values are 0 and 2, respectively.
</p>
</td></tr>
<tr><td><code id="sfpl.kernel.fit_+3A_min.q.h">min.q.h</code></td>
<td>

<p>Order of the quantile of the set of distances between curves (computed with the provided <code>semimetric</code>) which gives the lower end of the sequence in which the bandwidth is selected. The default is 0.05.
</p>
</td></tr>
<tr><td><code id="sfpl.kernel.fit_+3A_max.q.h">max.q.h</code></td>
<td>

<p>Order of the quantile of the set of distances between curves (computed with the provided <code>semimetric</code>) which gives the upper end of the sequence in which the bandwidth is selected. The default is 0.5.
</p>
</td></tr>
<tr><td><code id="sfpl.kernel.fit_+3A_h.seq">h.seq</code></td>
<td>

<p>Vector containing the sequence of bandwidths. The default is a sequence of <code>num.h</code> equispaced bandwidths in the range constructed using <code>min.q.h</code> and <code>max.q.h</code>.
</p>
</td></tr>
<tr><td><code id="sfpl.kernel.fit_+3A_num.h">num.h</code></td>
<td>

<p>Positive integer indicating the number of bandwiths in the grid. The default is 10.
</p>
</td></tr>
<tr><td><code id="sfpl.kernel.fit_+3A_range.grid">range.grid</code></td>
<td>

<p>Vector of length 2 containing the endpoints of the grid at which the observations of the functional covariate <code>x</code> are evaluated (i.e. the range of the discretization). If <code>range.grid=NULL</code>, then <code>range.grid=c(1,p)</code> is considered, where <code>p</code> is the size of the discretization size of <code>x</code> (i.e. <code>ncol(x))</code>.
</p>
</td></tr>
<tr><td><code id="sfpl.kernel.fit_+3A_kind.of.kernel">kind.of.kernel</code></td>
<td>

<p>The type of kernel function used. Only Epanechnikov kernel (<code>"quad"</code>) is available.
</p>
</td></tr>
<tr><td><code id="sfpl.kernel.fit_+3A_nknot">nknot</code></td>
<td>

<p>Positive integer indicating the number of interior knots for the B-spline representation of the functional covariate. The default value is <code>(p - order.Bspline - 1)%/%2</code>.
</p>
</td></tr>
<tr><td><code id="sfpl.kernel.fit_+3A_lambda.min">lambda.min</code></td>
<td>

<p>The smallest value for lambda (i. e., the smallest value  of the sequence in which <code>lambda.opt</code> is selected), as fraction of <code>lambda.max</code>.
The defaults is <code>lambda.min.l</code> if the number of observations is larger than <code>factor.pn</code> times the number of covariates and <code>lambda.min.h</code> otherwise.
</p>
</td></tr>
<tr><td><code id="sfpl.kernel.fit_+3A_lambda.min.h">lambda.min.h</code></td>
<td>

<p>The smallest value of the sequence in which <code>lambda.opt</code> is selected if the number of observations is smaller than <code>factor.pn</code> times the number of scalar covariates. The default is 0.05. 
</p>
</td></tr>
<tr><td><code id="sfpl.kernel.fit_+3A_lambda.min.l">lambda.min.l</code></td>
<td>

<p>The smallest value of the sequence in which <code>lambda.opt</code> is selected if the number of observations is larger than <code>factor.pn</code> times the number of scalar covariates. The default is 0.0001.
</p>
</td></tr>
<tr><td><code id="sfpl.kernel.fit_+3A_factor.pn">factor.pn</code></td>
<td>

<p>Positive integer used to set <code>lambda.min</code>. The default is 1.
</p>
</td></tr>
<tr><td><code id="sfpl.kernel.fit_+3A_nlambda">nlambda</code></td>
<td>

<p>Positive integer indicating the number of values of the sequence in which <code>lambda.opt</code> is selected. The default is 100.
</p>
</td></tr>
<tr><td><code id="sfpl.kernel.fit_+3A_lambda.seq">lambda.seq</code></td>
<td>

<p>Sequence of values in which <code>lambda.opt</code> is selected. If <code>lambda.seq=NULL</code>, then the programme builds the sequence automatically using <code>lambda.min</code> and <code>nlambda</code>.
</p>
</td></tr>
<tr><td><code id="sfpl.kernel.fit_+3A_vn">vn</code></td>
<td>

<p>Positive integer or vector of positive integers indicating the number of groups of consecutive variables to be penalised together. The default value is <code>vn=ncol(z)</code>, which leads to the individual penalisation of each scalar covariate.
</p>
</td></tr>
<tr><td><code id="sfpl.kernel.fit_+3A_nfolds">nfolds</code></td>
<td>

<p>Positive integer indicating the number of cross-validation folds (used if <code>criterion="k-fold-CV"</code>). The default is 10.
</p>
</td></tr>
<tr><td><code id="sfpl.kernel.fit_+3A_seed">seed</code></td>
<td>

<p>You may set the seed of the random number generator to obtain reproducible results (used if <code>criterion="k-fold-CV"</code>). The default is 123.
</p>
</td></tr>
<tr><td><code id="sfpl.kernel.fit_+3A_criterion">criterion</code></td>
<td>

<p>The criterion by which to select the regularization parameter <code>lambda.opt</code> and the bandwidth <code>h.opt</code>. One of <code>"GCV", "BIC", "AIC"</code> or <code>"k-fold-CV"</code>. The default is <code>"GCV"</code>.
</p>
</td></tr>
<tr><td><code id="sfpl.kernel.fit_+3A_penalty">penalty</code></td>
<td>

<p>The penalty function to be applied in the penalized least squares procedure. Only &quot;grLasso&quot; and &quot;grSCAD&quot; are implemented.
</p>
</td></tr>
<tr><td><code id="sfpl.kernel.fit_+3A_max.iter">max.iter</code></td>
<td>

<p>Maximum number of iterations (total across entire path). The default is 1000.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The sparse semi-functional partial linear model (SSFPLM) is given by the expression:
</p>
<p style="text-align: center;"><code class="reqn">
Y_i=Z_{i1}\beta_{01}+\dots+Z_{ip_n}\beta_{0p_n}+m(X_i)+\varepsilon_i\ \ \ i=1,\dots,n,
</code>
</p>

<p>where <code class="reqn">Y_i</code> denotes a scalar response, <code class="reqn">Z_{i1},\dots,Z_{ip_n}</code> are real random covariates and <code class="reqn">X_i</code> is a functional random covariate valued in some semi-metric space <code class="reqn">\mathcal{H}</code>. In this equation,
<code class="reqn">\mathbf{\beta}_0=(\beta_{01},\dots,\beta_{0p_n})^{\top}</code> and <code class="reqn">m(\cdot)</code> are a vector of unknown real parameters and an unknown smooth real-valued function, respectively. In addition, <code class="reqn">\varepsilon_i</code> is the random error.
</p>
<p>In this function the SSFPLM is fitted using the penalised least-squares approach. The first idea is to transform the SSFPLM into a linear model by extracting from <code class="reqn">Y_i</code> and <code class="reqn">Z_{ij}</code> (<code class="reqn">j=1,\ldots,p_n</code>) the effect of the functional covariate <code class="reqn">X_i</code> using functional nonparametric regression (see, for details, Ferraty and Vieu, 2006). This is made using  kernel estimation with Nadaraya-Watson weights.
</p>
<p>Then, an approximate linear model is obtained:
</p>
<p style="text-align: center;"><code class="reqn">\widetilde{\mathbf{Y}}\approx\widetilde{\mathbf{Z}}\mathbf{\beta}_0+\mathbf{\varepsilon},</code>
</p>

<p>and the penalised least-squares procedure is applied to this model by minimising 
</p>
<p style="text-align: center;"><code class="reqn">
\mathcal{Q}\left(\mathbf{\beta}\right)=\frac{1}{2}\left(\widetilde{\mathbf{Y}}-\widetilde{\mathbf{Z}}\mathbf{\beta}\right)^{\top}\left(\widetilde{\mathbf{Y}}-\widetilde{\mathbf{Z}}\mathbf{\beta}\right)+n\sum_{j=1}^{p_n}\mathcal{P}_{\lambda_{j_n}}\left(|\beta_j|\right), \quad (1)
</code>
</p>

<p>where <code class="reqn">\mathbf{\beta}=(\beta_1,\ldots,\beta_{p_n})^{\top}, \ \mathcal{P}_{\lambda_{j_n}}\left(\cdot\right)</code> is a penalty function (specified in the argument <code>penalty</code>) and <code class="reqn">\lambda_{j_n} &gt; 0</code> is a tuning parameter.
To reduce  the quantity of tuning parameters, <code class="reqn">\lambda_j</code>, to be selected for each sample, we consider <code class="reqn">\lambda_j = \lambda \widehat{\sigma}_{\beta_{0,j,OLS}}</code>, where <code class="reqn">\beta_{0,j,OLS}</code> denotes the OLS estimate of <code class="reqn">\beta_{0,j}</code> and <code class="reqn">\widehat{\sigma}_{\beta_{0,j,OLS}}</code> is the estimated standard deviation. Both <code class="reqn">\lambda</code> and <code class="reqn">h</code> (in the kernel estimation) are selected using the objetive criterion specified in the argument <code>criterion</code>.
</p>
<p>Finally, after estimating <code class="reqn">\mathbf{\beta}_0</code> by minimising (1), we deal with the estimation of the nonlinear function <code class="reqn">m(\cdot)</code>.
For that, we employ again the  kernel procedure with Nadaraya-Watson weights to smooth the partial residuals <code class="reqn">Y_i-\mathbf{Z}_i^{\top}\widehat{\mathbf{\beta}}</code>.
</p>
<p>For further details on the estimation procedure of the SSFPLM, see Aneiros et al. (2015).
</p>
<p><b>Remark</b>: We should note that if we set <code>lambda.seq</code><code class="reqn">=0</code>, we can obtain the non-penalised estimation of the model, i.e. the OLS estimation. It is convenient to use <code>lambda.seq</code><code class="reqn">\not=0</code> when one suspects there are irrelevant variables.
</p>


<h3>Value</h3>

<table>
<tr><td><code>call</code></td>
<td>
<p>The matched call.</p>
</td></tr>
<tr><td><code>fitted.values</code></td>
<td>
<p>Estimated scalar response.</p>
</td></tr>
<tr><td><code>residuals</code></td>
<td>
<p>Differences between <code>y</code> and the <code>fitted.values</code></p>
</td></tr>
<tr><td><code>beta.est</code></td>
<td>
<p>Estimate of <code class="reqn">\beta_0</code> when the optimal tuning parameters <code>lambda.opt</code>, <code>h.opt</code> and <code>vn.opt</code> are used.</p>
</td></tr>
<tr><td><code>indexes.beta.nonnull</code></td>
<td>
<p>Indexes of the non-zero <code class="reqn">\hat{\beta_{j}}</code>.</p>
</td></tr>
<tr><td><code>h.opt</code></td>
<td>
<p>Selected bandwidth.</p>
</td></tr>
<tr><td><code>lambda.opt</code></td>
<td>
<p>Selected value of lambda.</p>
</td></tr>
<tr><td><code>IC</code></td>
<td>
<p>Value of the criterion function considered to select <code>lambda.opt</code>, <code>h.opt</code> and <code>vn.opt</code>.</p>
</td></tr>
<tr><td><code>h.min.opt.max.mopt</code></td>
<td>
<p><code>h.opt=h.min.opt.max.mopt[2]</code> (used by <code>beta.est</code>) was seeked between <code>h.min.opt.max.mopt[1]</code> and <code>h.min.opt.max.mopt[3]</code>.</p>
</td></tr>
<tr><td><code>vn.opt</code></td>
<td>
<p>Selected value of <code>vn</code>.</p>
</td></tr>
<tr><td><code>...</code></td>
<td>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>German Aneiros Perez <a href="mailto:german.aneiros@udc.es">german.aneiros@udc.es</a> 
</p>
<p>Silvia Novo Diaz  <a href="mailto:snovo@est-econ.uc3m.es">snovo@est-econ.uc3m.es</a>
</p>


<h3>References</h3>

<p>Aneiros, G., Ferraty, F., Vieu, P. (2015) Variable selection in partial linear regression with functional
covariate. <em>Statistics</em>, <b>49</b>, 1322&ndash;1347, <a href="https://doi.org/10.1080/02331888.2014.998675">doi:10.1080/02331888.2014.998675</a>.
</p>
<p>Ferraty, F. and Vieu, P. (2006) <em>Nonparametric Functional Data Analysis</em>. Springer Series in Statistics, New York.
</p>


<h3>See Also</h3>

<p>See also <code><a href="#topic+predict.sfpl.kernel">predict.sfpl.kernel</a></code> and  <code><a href="#topic+plot.sfpl.kernel">plot.sfpl.kernel</a></code>.
</p>
<p>Alternative method <code><a href="#topic+sfpl.kNN.fit">sfpl.kNN.fit</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("Tecator")
y&lt;-Tecator$fat
X&lt;-Tecator$absor.spectra
z1&lt;-Tecator$protein       
z2&lt;-Tecator$moisture

#Quadratic, cubic and interaction effects of the scalar covariates.
z.com&lt;-cbind(z1,z2,z1^2,z2^2,z1^3,z2^3,z1*z2)
train&lt;-1:160


#SFPLM fit. 
ptm=proc.time()
fit&lt;-sfpl.kernel.fit(x=X[train,], z=z.com[train,], y=y[train],q=2, 
      max.q.h=0.35,lambda.min.h=0.02,lambda.min.l=0.01,
      factor.pn=2, max.iter=5000, criterion="BIC", penalty="grSCAD",nknot=20)
proc.time()-ptm

#Results
fit
names(fit)
</code></pre>

<hr>
<h2 id='sfpl.kNN.fit'>
Sparse semi-functional partial linear model fit using kNN estimation
</h2><span id='topic+sfpl.kNN.fit'></span>

<h3>Description</h3>

<p>This function fits a sparse semi-functional partial linear model between a scalar response, a functional explanatory variable and
a vector of scalar covariates. 
The function uses the penalised least-squares regularization procedure combined with k-nearest neighbours (kNN) estimation with Nadaraya-Watson weights.
</p>
<p>The procedure requires an objective criterion (<code>criterion</code>) to select the number of nearest neighbours (<code>k.opt</code>) and the regularization parameter (<code>lambda.opt</code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sfpl.kNN.fit(x, z, y, semimetric = "deriv", q = NULL, knearest = NULL, min.knn = 2,
  max.knn = NULL, step = NULL,range.grid = NULL, kind.of.kernel = "quad", 
  nknot = NULL, lambda.min = NULL, lambda.min.h = NULL, 
  lambda.min.l = NULL, factor.pn = 1, nlambda = 100, lambda.seq = NULL,
  vn = ncol(z), nfolds = 10, seed = 123,criterion = c("GCV", "BIC", 
  "AIC", "k-fold-CV"),penalty = c("grLasso", "grMCP", 
  "grSCAD", "gel", "cMCP", "gBridge", "gLasso", "gMCP"), 
  max.iter = 1000)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sfpl.kNN.fit_+3A_x">x</code></td>
<td>

<p>Matrix containing the observations of the functional covariate collected by row.
</p>
</td></tr>
<tr><td><code id="sfpl.kNN.fit_+3A_z">z</code></td>
<td>

<p>Matrix containing the observations of the scalar covariates collected by row.
</p>
</td></tr>
<tr><td><code id="sfpl.kNN.fit_+3A_y">y</code></td>
<td>

<p>Vector containing the scalar response.
</p>
</td></tr>
<tr><td><code id="sfpl.kNN.fit_+3A_semimetric">semimetric</code></td>
<td>

<p>Semi-metric function.  Only <code>"deriv"</code> and <code>"pca"</code> are implemented.  By default <code>semimetric="deriv"</code>.
</p>
</td></tr>
<tr><td><code id="sfpl.kNN.fit_+3A_q">q</code></td>
<td>

<p>Order of the derivative (if <code>semimetric="deriv"</code>) or number of principal components (if <code>semimetric="pca"</code>). The default values are 0 and 2, respectively.
</p>
</td></tr>
<tr><td><code id="sfpl.kNN.fit_+3A_knearest">knearest</code></td>
<td>
<p>Sequence of eligible values for <code class="reqn">k</code> considered to seek for <code>k.opt</code>. If <code>knearest=NULL</code>, then <code>knearest &lt;- seq(from = min.knn, to = max.knn, by = step)</code>.</p>
</td></tr>
<tr><td><code id="sfpl.kNN.fit_+3A_min.knn">min.knn</code></td>
<td>

<p>Positive integer indicating the minumum value of the sequence in which the  number of nearest neighbours <code>k.opt</code> is selected (thus, this number must be smaller than the sample size). The default is 2.
</p>
</td></tr>
<tr><td><code id="sfpl.kNN.fit_+3A_max.knn">max.knn</code></td>
<td>

<p>Positive integer indicating the maximum value of the sequence in which the number of nearest neighbours <code>k.opt</code> is selected (thus, this number must be larger than <code>min.kNN</code> and smaller than the sample size, <code>n</code>). The default is <code>max.knn &lt;- n%/%2</code>.
</p>
</td></tr>
<tr><td><code id="sfpl.kNN.fit_+3A_step">step</code></td>
<td>

<p>Positive integer used to build the sequence of k-nearest neighbours in the following way: <code>min.knn, min.knn + step, min.knn + 2*step, min.knn + 3*step,...</code> The default is <code>step&lt;-ceiling(n/100)</code>.
</p>
</td></tr>
<tr><td><code id="sfpl.kNN.fit_+3A_range.grid">range.grid</code></td>
<td>

<p>Vector of length 2 containing the endpoints of the grid at which the observations of the functional covariate <code>x</code> are evaluated (i.e. the range of the discretization). If <code>range.grid=NULL</code>, then <code>range.grid=c(1,p)</code> is considered, where <code>p</code> is the size of the discretization size of <code>x</code> (i.e. <code>ncol(x))</code>.
</p>
</td></tr>
<tr><td><code id="sfpl.kNN.fit_+3A_kind.of.kernel">kind.of.kernel</code></td>
<td>

<p>The type of kernel function used. Only Epanechnikov kernel (<code>"quad"</code>) is available.
</p>
</td></tr>
<tr><td><code id="sfpl.kNN.fit_+3A_nknot">nknot</code></td>
<td>

<p>Positive integer indicating the number of interior knots for the B-spline representation of the functional covariate. The default value is <code>(p - order.Bspline - 1)%/%2</code>.
</p>
</td></tr>
<tr><td><code id="sfpl.kNN.fit_+3A_lambda.min">lambda.min</code></td>
<td>

<p>The smallest value for lambda (i. e., the smallest value  of the sequence in which <code>lambda.opt</code> is selected), as fraction of <code>lambda.max</code>.
The defaults is <code>lambda.min.l</code> if the number of observations is larger than <code>factor.pn</code> times the number of covariates and <code>lambda.min.h</code> otherwise.
</p>
</td></tr>
<tr><td><code id="sfpl.kNN.fit_+3A_lambda.min.h">lambda.min.h</code></td>
<td>

<p>The smallest value of the sequence in which <code>lambda.opt</code> is selected if the number of observations is smaller than <code>factor.pn</code> times the number of scalar covariates. The default is 0.05. 
</p>
</td></tr>
<tr><td><code id="sfpl.kNN.fit_+3A_lambda.min.l">lambda.min.l</code></td>
<td>

<p>The smallest value of the sequence in which <code>lambda.opt</code> is selected if the number of observations is larger than <code>factor.pn</code> times the number of scalar covariates. The default is 0.0001.
</p>
</td></tr>
<tr><td><code id="sfpl.kNN.fit_+3A_factor.pn">factor.pn</code></td>
<td>

<p>Positive integer used to set <code>lambda.min</code>. The default value is 1.
</p>
</td></tr>
<tr><td><code id="sfpl.kNN.fit_+3A_nlambda">nlambda</code></td>
<td>

<p>Positive integer indicating the number of values of the sequence in which <code>lambda.opt</code> is selected. The default is 100.
</p>
</td></tr>
<tr><td><code id="sfpl.kNN.fit_+3A_lambda.seq">lambda.seq</code></td>
<td>

<p>Sequence of values in which <code>lambda.opt</code> is selected. If <code>lambda.seq=NULL</code>, then the programme builds the sequence automatically using <code>lambda.min</code> and <code>nlambda</code>.
</p>
</td></tr>
<tr><td><code id="sfpl.kNN.fit_+3A_vn">vn</code></td>
<td>
<p>Positive integer or vector of positive integers indicating the number of groups of consecutive variables to be penalised together. The default value is <code>vn=ncol(z)</code>, which leads to the individual penalisation of each scalar covariate.
</p>
</td></tr>
<tr><td><code id="sfpl.kNN.fit_+3A_nfolds">nfolds</code></td>
<td>

<p>Positive integer indicating the number of cross-validation folds (used if <code>criterion="k-fold-CV"</code>). The default is 10.
</p>
</td></tr>
<tr><td><code id="sfpl.kNN.fit_+3A_seed">seed</code></td>
<td>

<p>You may set the seed of the random number generator to obtain reproducible results (used if <code>criterion="k-fold-CV"</code>). The default is 123.
</p>
</td></tr>
<tr><td><code id="sfpl.kNN.fit_+3A_criterion">criterion</code></td>
<td>

<p>The criterion by which to select the regularization parameter <code>lambda.opt</code> and <code>k.opt</code>. One of <code>"GCV", "BIC", "AIC"</code> or <code>"k-fold-CV"</code>. The default is <code>"GCV"</code>.
</p>
</td></tr>
<tr><td><code id="sfpl.kNN.fit_+3A_penalty">penalty</code></td>
<td>

<p>The penalty function to be applied in the penalized least squares procedure. Only &quot;grLasso&quot; and &quot;grSCAD&quot; are implemented.
</p>
</td></tr>
<tr><td><code id="sfpl.kNN.fit_+3A_max.iter">max.iter</code></td>
<td>

<p>Maximum number of iterations (total across entire path). The default is 1000.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The sparse semi-functional partial linear model (SSFPLM) is given by the expression:
</p>
<p style="text-align: center;"><code class="reqn">
Y_i=Z_{i1}\beta_{01}+\dots+Z_{ip_n}\beta_{0p_n}+m(X_i)+\varepsilon_i\ \ \ i=1,\dots,n,
</code>
</p>

<p>where <code class="reqn">Y_i</code> denotes a scalar response, <code class="reqn">Z_{i1},\dots,Z_{ip_n}</code> are real random covariates and <code class="reqn">X_i</code> is a functional random covariate valued in some semi-metric space <code class="reqn">\mathcal{H}</code>. In this equation,
<code class="reqn">\mathbf{\beta}_0=(\beta_{01},\dots,\beta_{0p_n})^{\top}</code> and <code class="reqn">m(\cdot)</code> are a vector of unknown real parameters and an unknown smooth real-valued function, respectively. In addition, <code class="reqn">\varepsilon_i</code> is the random error.
</p>
<p>In this function the SSFPLM is fitted using the penalised least-squares approach. The first idea is to transform the SSFPLM into a linear model by extracting from <code class="reqn">Y_i</code> and <code class="reqn">Z_{ij}</code> (<code class="reqn">j=1,\ldots,p_n</code>) the effect of the functional covariate <code class="reqn">X_i</code> using functional nonparametric regression (see, for details, Ferraty and Vieu, 2006). This is made using  kNN estimation with Nadaraya-Watson weights.
</p>
<p>Then, an approximate linear model is obtained:
</p>
<p style="text-align: center;"><code class="reqn">\widetilde{\mathbf{Y}}\approx\widetilde{\mathbf{Z}}\mathbf{\beta}_0+\mathbf{\varepsilon},</code>
</p>

<p>and the penalised least-squares procedure is applied to this model by minimising 
</p>
<p style="text-align: center;"><code class="reqn">
\mathcal{Q}\left(\mathbf{\beta}\right)=\frac{1}{2}\left(\widetilde{\mathbf{Y}}-\widetilde{\mathbf{Z}}\mathbf{\beta}\right)^{\top}\left(\widetilde{\mathbf{Y}}-\widetilde{\mathbf{Z}}\mathbf{\beta}\right)+n\sum_{j=1}^{p_n}\mathcal{P}_{\lambda_{j_n}}\left(|\beta_j|\right), \quad (1)
</code>
</p>

<p>where <code class="reqn">\mathbf{\beta}=(\beta_1,\ldots,\beta_{p_n})^{\top}, \ \mathcal{P}_{\lambda_{j_n}}\left(\cdot\right)</code> is a penalty function (specified in the argument <code>penalty</code>) and <code class="reqn">\lambda_{j_n} &gt; 0</code> is a tuning parameter.
To reduce  the quantity of tuning parameters, <code class="reqn">\lambda_j</code>, to be selected for each sample, we consider <code class="reqn">\lambda_j = \lambda \widehat{\sigma}_{\beta_{0,j,OLS}}</code>, where <code class="reqn">\beta_{0,j,OLS}</code> denotes the OLS estimate of <code class="reqn">\beta_{0,j}</code> and <code class="reqn">\widehat{\sigma}_{\beta_{0,j,OLS}}</code> is the estimated standard deviation. Both <code class="reqn">\lambda</code> and <code class="reqn">k</code> (in the kNN estimation) are selected using the objetive criterion specified in the argument <code>criterion</code>.
</p>
<p>Finally, after estimating <code class="reqn">\mathbf{\beta}_0</code> by minimising (1), we deal with the estimation of the nonlinear function <code class="reqn">m(\cdot)</code>.
For that, we employ again the  kNN procedure with Nadaraya-Watson weights to smooth the partial residuals <code class="reqn">Y_i-\mathbf{Z}_i^{\top}\widehat{\mathbf{\beta}}</code>.
</p>
<p>For further details on the estimation procedure of the SSFPLM, see Aneiros et al. (2015).
</p>
<p><b>Remark</b>: We should note that if we set <code>lambda.seq</code><code class="reqn">=0</code>, we can obtain the non-penalised estimation of the model, i.e. the OLS estimation. It is convenient to use <code>lambda.seq</code><code class="reqn">\not=0</code> when one suspects there are irrelevant variables.
</p>


<h3>Value</h3>

<table>
<tr><td><code>call</code></td>
<td>
<p>The matched call.</p>
</td></tr>
<tr><td><code>fitted.values</code></td>
<td>
<p>Estimated scalar response.</p>
</td></tr>
<tr><td><code>residuals</code></td>
<td>
<p>Differences between <code>y</code> and the <code>fitted.values</code></p>
</td></tr>
<tr><td><code>beta.est</code></td>
<td>
<p>Estimate of <code class="reqn">\beta_0</code> when the optimal tuning parameters <code>lambda.opt</code>, <code>k.opt</code> and <code>vn.opt</code> are used.</p>
</td></tr>
<tr><td><code>indexes.beta.nonnull</code></td>
<td>
<p>Indexes of the non-zero <code class="reqn">\hat{\beta_{j}}</code>.</p>
</td></tr>
<tr><td><code>k.opt</code></td>
<td>
<p>Selected number of nearest neighbours.</p>
</td></tr>
<tr><td><code>lambda.opt</code></td>
<td>
<p>Selected value of lambda.</p>
</td></tr>
<tr><td><code>IC</code></td>
<td>
<p>Value of the criterion function considered to select both <code>lambda.opt</code>, <code>h.opt</code> and <code>vn.opt</code>.</p>
</td></tr>
<tr><td><code>vn.opt</code></td>
<td>
<p>Selected value of <code>vn</code>.</p>
</td></tr>
<tr><td><code>...</code></td>
<td>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>German Aneiros Perez <a href="mailto:german.aneiros@udc.es">german.aneiros@udc.es</a> 
</p>
<p>Silvia Novo Diaz  <a href="mailto:snovo@est-econ.uc3m.es">snovo@est-econ.uc3m.es</a>
</p>


<h3>References</h3>

<p>Aneiros, G., Ferraty, F., Vieu, P. (2015) Variable selection in partial linear regression with functional
covariate. <em>Statistics</em>, <b>49</b>, 1322&ndash;1347, <a href="https://doi.org/10.1080/02331888.2014.998675">doi:10.1080/02331888.2014.998675</a>.
</p>


<h3>See Also</h3>

<p>See also <code><a href="#topic+predict.sfpl.kNN">predict.sfpl.kNN</a></code> and  <code><a href="#topic+plot.sfpl.kNN">plot.sfpl.kNN</a></code>.
</p>
<p>Alternative method <code><a href="#topic+sfpl.kernel.fit">sfpl.kernel.fit</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("Tecator")
y&lt;-Tecator$fat
X&lt;-Tecator$absor.spectra
z1&lt;-Tecator$protein       
z2&lt;-Tecator$moisture

#Quadratic, cubic and interaction effects of the scalar covariates.
z.com&lt;-cbind(z1,z2,z1^2,z2^2,z1^3,z2^3,z1*z2)
train&lt;-1:160


#SFPLM fit. 
ptm=proc.time()
fit&lt;-sfpl.kNN.fit(y=y[train],x=X[train,], z=z.com[train,],q=2, max.knn=20,
  lambda.min.h=0.02,lambda.min.l=0.01, factor.pn=2, criterion="BIC",
  range.grid=c(850,1050), penalty="grSCAD",nknot=20, max.iter=5000)
proc.time()-ptm

#Results
fit
names(fit)
</code></pre>

<hr>
<h2 id='sfplsim.kernel.fit'>
Sparse semi-functional partial linear single-index model fit using kernel estimation
</h2><span id='topic+sfplsim.kernel.fit'></span>

<h3>Description</h3>

<p>This function fits a sparse semi-functional partial linear single-index model between a scalar response, a functional explanatory variable and
a vector of scalar covariates. 
The function uses the penalised least-squares regularization procedure combined with nonparametric kernel estimation with Nadaraya-Watson weights.
</p>
<p>The procedure requires the  B-spline representation to estimate the functional index <code class="reqn">\theta_0</code> and an objective criterion (<code>criterion</code>) to select the bandwidth (<code>h.opt</code>) and the regularization parameter (<code>lambda.opt</code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sfplsim.kernel.fit(x, z, y, seed.coeff = c(-1, 0, 1), order.Bspline = 3, 
  nknot.theta = 3,t0 = NULL, min.q.h = 0.05, max.q.h = 0.5,
  h.seq = NULL, num.h = 10, range.grid = NULL, kind.of.kernel = "quad",
  nknot = NULL, lambda.min = NULL, lambda.min.h = NULL, 
  lambda.min.l = NULL, factor.pn = 1, nlambda = 100, lambda.seq = NULL,
  vn = ncol(z), nfolds = 10, seed = 123, criterion = c("GCV", "BIC", "AIC",
  "k-fold-CV"), penalty = c("grLasso", "grMCP", 
  "grSCAD", "gel", "cMCP", "gBridge", "gLasso", "gMCP"), 
  max.iter = 1000)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sfplsim.kernel.fit_+3A_x">x</code></td>
<td>

<p>Matrix containing the observations of the functional covariate collected by row.
</p>
</td></tr>
<tr><td><code id="sfplsim.kernel.fit_+3A_z">z</code></td>
<td>

<p>Matrix containing the observations of the scalar covariates collected by row.
</p>
</td></tr>
<tr><td><code id="sfplsim.kernel.fit_+3A_y">y</code></td>
<td>

<p>Vector containing the scalar response.
</p>
</td></tr>
<tr><td><code id="sfplsim.kernel.fit_+3A_seed.coeff">seed.coeff</code></td>
<td>

<p>Vector of initial values used to  build the set <code class="reqn">\Theta_n</code> (see section <code>Details</code>). The coefficients for the B-spline representation of each eligible functional index <code class="reqn">\theta \in \Theta_n</code> are obtained from <code>seed.coeff</code>.  The default is <code>c(-1,0,1)</code>.
</p>
</td></tr>
<tr><td><code id="sfplsim.kernel.fit_+3A_order.bspline">order.Bspline</code></td>
<td>

<p>Positive integer giving the order of the B-spline basis functions. This is the number of coefficients in each piecewise polynomial segment. The default is 3.
</p>
</td></tr>
<tr><td><code id="sfplsim.kernel.fit_+3A_nknot.theta">nknot.theta</code></td>
<td>

<p>Positive integer indicating the number of uniform interior knots of the B-spline basis for the B-spline representation of <code class="reqn">\theta_0</code>. The default is 3.
</p>
</td></tr>
<tr><td><code id="sfplsim.kernel.fit_+3A_t0">t0</code></td>
<td>

<p>Value in the domain of the functional indexes at which we evaluate them to  build the set <code class="reqn">\Theta_n</code>. We assume <code class="reqn">\theta_0(t_0)&gt;0</code> for some arbitrary <code class="reqn">t_0</code> in the domain to ensure model identifiability. If <code>t0=NULL</code>, then <code>mean(range.grid)</code> is considered.
</p>
</td></tr>
<tr><td><code id="sfplsim.kernel.fit_+3A_min.q.h">min.q.h</code></td>
<td>
<p>Order of the quantile of the set of distances between curves (computed with the projection semi-metric) which gives the lower end of the sequence in which the bandwidth is selected. The default is 0.05.
</p>
</td></tr>
<tr><td><code id="sfplsim.kernel.fit_+3A_max.q.h">max.q.h</code></td>
<td>

<p>Order of the quantile of the set of distances between curves (computed with the projection semi-metric) which gives the upper end of the sequence in which the bandwidth is selected. The default is 0.5.
</p>
</td></tr>
<tr><td><code id="sfplsim.kernel.fit_+3A_h.seq">h.seq</code></td>
<td>
<p>Vector containing the sequence of bandwidths. The default is a sequence of <code>num.h</code> equispaced bandwidths in the range constructed using <code>min.q.h</code> and <code>max.q.h</code>.
</p>
</td></tr>
<tr><td><code id="sfplsim.kernel.fit_+3A_num.h">num.h</code></td>
<td>
<p>Positive integer indicating the number of bandwiths in the grid. The default is 10.
</p>
</td></tr>
<tr><td><code id="sfplsim.kernel.fit_+3A_range.grid">range.grid</code></td>
<td>

<p>Vector of length 2 containing the endpoints of the grid at which the observations of the functional covariate <code>x</code> are evaluated (i.e. the range of the discretization). If <code>range.grid=NULL</code>, then <code>range.grid=c(1,p)</code> is considered, where <code>p</code> is the size of the discretization size of <code>x</code> (i.e. <code>ncol(x))</code>.
</p>
</td></tr>
<tr><td><code id="sfplsim.kernel.fit_+3A_kind.of.kernel">kind.of.kernel</code></td>
<td>

<p>The type of kernel function used. Only Epanechnikov kernel (<code>"quad"</code>) is available.
</p>
</td></tr>
<tr><td><code id="sfplsim.kernel.fit_+3A_nknot">nknot</code></td>
<td>

<p>Positive integer indicating the number of interior knots for the B-spline representation of the functional covariate. The default value is <code>(p - order.Bspline - 1)%/%2</code>.
</p>
</td></tr>
<tr><td><code id="sfplsim.kernel.fit_+3A_lambda.min">lambda.min</code></td>
<td>

<p>The smallest value for lambda (i. e., the smallest value  of the sequence in which <code>lambda.opt</code> is selected), as fraction of <code>lambda.max</code>.
The default is <code>lambda.min.l</code> if the number of observations is larger than <code>factor.pn</code> times the number of covariates and <code>lambda.min.h</code> otherwise.
</p>
</td></tr>
<tr><td><code id="sfplsim.kernel.fit_+3A_lambda.min.h">lambda.min.h</code></td>
<td>

<p>The smallest value of the sequence in which <code>lambda.opt</code> is selected if the number of observations is smaller than <code>factor.pn</code> times the number of scalar covariates. The default is 0.05. 
</p>
</td></tr>
<tr><td><code id="sfplsim.kernel.fit_+3A_lambda.min.l">lambda.min.l</code></td>
<td>

<p>The smallest value of the sequence in which <code>lambda.opt</code> is selected if the number of observations is larger than <code>factor.pn</code> times the number of scalar covariates. The default is 0.0001.
</p>
</td></tr>
<tr><td><code id="sfplsim.kernel.fit_+3A_factor.pn">factor.pn</code></td>
<td>

<p>Positive integer used to set <code>lambda.min</code>. The default value is 1.
</p>
</td></tr>
<tr><td><code id="sfplsim.kernel.fit_+3A_nlambda">nlambda</code></td>
<td>

<p>Positive integer indicating the number of values of the sequence in which <code>lambda.opt</code> is selected. The default is 100.
</p>
</td></tr>
<tr><td><code id="sfplsim.kernel.fit_+3A_lambda.seq">lambda.seq</code></td>
<td>

<p>Sequence of values in which <code>lambda.opt</code> is selected. If <code>lambda.seq=NULL</code>, then the programme builds the sequence automatically using <code>lambda.min</code> and <code>nlambda</code>. For non-penalized estimation, i. e. ordinary least squares estimation (OLS), set <code>lambda.seq=0</code>.
</p>
</td></tr>
<tr><td><code id="sfplsim.kernel.fit_+3A_vn">vn</code></td>
<td>
<p>Positive integer or vector of positive integers indicating the number of groups of consecutive variables to be penalised together. The default value is <code>vn=ncol(z)</code>, which leads to the individual penalisation of each scalar covariate.
</p>
</td></tr>
<tr><td><code id="sfplsim.kernel.fit_+3A_nfolds">nfolds</code></td>
<td>

<p>Positive integer indicating the number of cross-validation folds (used if <code>criterion="k-fold-CV"</code>). The default is 10.
</p>
</td></tr>
<tr><td><code id="sfplsim.kernel.fit_+3A_seed">seed</code></td>
<td>

<p>You may set the seed of the random number generator to obtain reproducible results (used if <code>criterion="k-fold-CV"</code>). The default is 123.
</p>
</td></tr>
<tr><td><code id="sfplsim.kernel.fit_+3A_criterion">criterion</code></td>
<td>

<p>The criterion by which to select the regularization parameter <code>lambda.opt</code> and <code>h.opt</code>. One of <code>"GCV", "BIC", "AIC"</code> or <code>"k-fold-CV"</code>. The default is <code>"GCV"</code>.
</p>
</td></tr>
<tr><td><code id="sfplsim.kernel.fit_+3A_penalty">penalty</code></td>
<td>

<p>The penalty function to be applied in the penalized least squares procedure. Only &quot;grLasso&quot; and &quot;grSCAD&quot; are implemented.
</p>
</td></tr>
<tr><td><code id="sfplsim.kernel.fit_+3A_max.iter">max.iter</code></td>
<td>

<p>Maximum number of iterations (total across entire path). The default is 1000.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The sparse semi-functional partial linear single-index model (SSFPLSIM) is given by the expression:
</p>
<p style="text-align: center;"><code class="reqn">
Y_i=Z_{i1}\beta_{01}+\dots+Z_{ip_n}\beta_{0p_n}+r(\left&lt;\theta_0,X_i\right&gt;)+\varepsilon_i\ \ \ i=1,\dots,n,
</code>
</p>

<p>where <code class="reqn">Y_i</code> denotes a scalar response, <code class="reqn">Z_{i1},\dots,Z_{ip_n}</code> are real random covariates and <code class="reqn">X_i</code> is a functional random covariate valued in a separable Hilbert space <code class="reqn">\mathcal{H}</code> with inner product <code class="reqn">\left\langle \cdot, \cdot \right\rangle</code>. In this equation,
<code class="reqn">\mathbf{\beta}_0=(\beta_{01},\dots,\beta_{0p_n})^{\top}</code>, <code class="reqn">\theta_0\in\mathcal{H}</code> and <code class="reqn">r(\cdot)</code> are a vector of unknown real parameters, an unknown functional direction and an unknown smooth real-valued function, respectively. In addition, <code class="reqn">\varepsilon_i</code> is the random error.
</p>
<p>The SSFPLSIM is fitted using the penalised least-squares approach. The first idea is to transform the SSFPLSIM into a linear model by extracting from <code class="reqn">Y_i</code> and <code class="reqn">Z_{ij}</code> (<code class="reqn">j=1,\ldots,p_n</code>) the effect of the functional covariate <code class="reqn">X_i</code> using functional single-index regression. This is made using nonparametric kernel estimation (see, for details, the documentation of the function <code>fsim.kernel.fit</code>).
</p>
<p>Then, an approximate linear model is obtained:
</p>
<p style="text-align: center;"><code class="reqn">\widetilde{\mathbf{Y}}_{\theta_0}\approx\widetilde{\mathbf{Z}}_{\theta_0}\mathbf{\beta}_0+\mathbf{\varepsilon},</code>
</p>

<p>and the penalised least-squares procedure is applied to this model by minimising over the pair <code class="reqn">(\mathbf{\beta},\theta)</code>
</p>
<p style="text-align: center;"><code class="reqn">
\mathcal{Q}\left(\mathbf{\beta},\theta\right)=\frac{1}{2}\left(\widetilde{\mathbf{Y}}_{\theta}-\widetilde{\mathbf{Z}}_{\theta}\mathbf{\beta}\right)^{\top}\left(\widetilde{\mathbf{Y}}_{\theta}-\widetilde{\mathbf{Z}}_{\theta}\mathbf{\beta}\right)+n\sum_{j=1}^{p_n}\mathcal{P}_{\lambda_{j_n}}\left(|\beta_j|\right), \quad (1)
</code>
</p>

<p>where <code class="reqn">\mathbf{\beta}=(\beta_1,\ldots,\beta_{p_n})^{\top}, \ \mathcal{P}_{\lambda_{j_n}}\left(\cdot\right)</code> is a penalty function (specified in the argument <code>penalty</code>) and <code class="reqn">\lambda_{j_n} &gt; 0</code> is a tuning parameter.
To reduce  the quantity of tuning parameters, <code class="reqn">\lambda_j</code>, to be selected for each sample, we consider <code class="reqn">\lambda_j = \lambda \widehat{\sigma}_{\beta_{0,j,OLS}}</code>, where <code class="reqn">\beta_{0,j,OLS}</code> denotes the OLS estimate of <code class="reqn">\beta_{0,j}</code> and <code class="reqn">\widehat{\sigma}_{\beta_{0,j,OLS}}</code> is the estimated standard deviation. Both <code class="reqn">\lambda</code> and <code class="reqn">h</code> (in the kernel estimation) are selected using the objetive criterion specified in the argument <code>criterion</code>.
</p>
<p>In addition, the function uses B-spline representation to build a set  <code class="reqn">\Theta_n</code> of eligible functional indexes <code class="reqn">\theta</code>. The dimension of the B-spline basis is <code>order.Bspline</code>+<code>nknot.theta</code> and the set of eligible coefficients is obtained by calibrating (to ensure the identifiability of the model) the set of initial coefficients given in <code>seed.coeff</code>. The larger this set, the higher the size of <code class="reqn">\Theta_n</code>. Since our approach requires intensive computation, we need a trade-off between the size of <code class="reqn">\Theta_n</code> and the performance of the estimator. For that, Ait-Saidi et al. (2008) suggested considering <code>order.Bspline=3</code> and <code>seed.coeff=c(-1,0,1)</code>. For details on the construction of <code class="reqn">\Theta_n</code> see Novo et al. (2019).
</p>
<p>Finally, after estimating <code class="reqn">\mathbf{\beta}_0</code> and <code class="reqn">\theta_0</code> by minimising (1), we deal with the estimation of the nonlinear function <code class="reqn">r_{\theta_0}(\cdot)\equiv r\left(\left&lt;\theta_0,\cdot\right&gt;\right)</code>.
For that, we employ again the  kernel procedure with Nadaraya-Watson weights to smooth the partial residuals <code class="reqn">Y_i-\mathbf{Z}_i^{\top}\widehat{\mathbf{\beta}}</code>.
</p>
<p>For further details on the estimation procedure of the SSFPLSIM, see Novo et al. (2021).
</p>
<p><b>Remark</b>: We should note that if we set <code>lambda.seq</code><code class="reqn">=0</code>, we can obtain the non-penalised estimation of the model, i.e. the OLS estimation. It is convenient to use <code>lambda.seq</code><code class="reqn">\not=0</code> when one suspects there are irrelevant variables.
</p>


<h3>Value</h3>

<table>
<tr><td><code>call</code></td>
<td>
<p>The matched call.</p>
</td></tr>
<tr><td><code>fitted.values</code></td>
<td>
<p>Estimated scalar response.</p>
</td></tr>
<tr><td><code>residuals</code></td>
<td>
<p>Differences between <code>y</code> and the <code>fitted.values</code></p>
</td></tr>
<tr><td><code>beta.est</code></td>
<td>
<p>Estimate of <code class="reqn">\beta_0</code> when the optimal tuning parameters <code>lambda.opt</code>, <code>h.opt</code> and <code>vn.opt</code> are used.</p>
</td></tr>
<tr><td><code>theta.est</code></td>
<td>
<p>Coefficients of <code class="reqn">\hat{\theta}</code> in the B-spline basis (when the optimal tuning parameters <code>lambda.opt</code>, <code>h.opt</code> and <code>vn.opt</code> are used): a vector of <code>length(order.Bspline+nknot.theta)</code>.</p>
</td></tr>
<tr><td><code>indexes.beta.nonnull</code></td>
<td>
<p>Indexes of the non-zero <code class="reqn">\hat{\beta_{j}}</code>.</p>
</td></tr>
<tr><td><code>h.opt</code></td>
<td>
<p>Selected bandwidth.</p>
</td></tr>
<tr><td><code>lambda.opt</code></td>
<td>
<p>Selected value of the penalisation parameter <code class="reqn">\lambda</code>.</p>
</td></tr>
<tr><td><code>IC</code></td>
<td>
<p>Value of the criterion function considered to select <code>lambda.opt</code>, <code>h.opt</code> and <code>vn.opt</code>.</p>
</td></tr>
<tr><td><code>Q.opt</code></td>
<td>
<p>Minimum value of the penalized criterion used to estimate <code class="reqn">\beta_0</code> and <code class="reqn">\theta_0</code>. That is, the value obtained using <code>theta.est</code> and <code>beta.est</code>.</p>
</td></tr>
<tr><td><code>Q</code></td>
<td>
<p>Vector of dimension equal to the cardinal of <code class="reqn">\Theta_n</code>, containing the values of the penalized criterion for each functional index in <code class="reqn">\Theta_n</code>.</p>
</td></tr>
<tr><td><code>m.opt</code></td>
<td>
<p>Index of <code class="reqn">\hat{\theta}</code> in the set <code class="reqn">\Theta_n</code>.</p>
</td></tr>
<tr><td><code>lambda.min.opt.max.mopt</code></td>
<td>
<p>A grid of values in [<code>lambda.min.opt.max.mopt[1], lambda.min.opt.max.mopt[3]</code>] is considered to seek for the <code>lambda.opt</code> (<code>lambda.opt=lambda.min.opt.max.mopt[2]</code>).
</p>
</td></tr>
<tr><td><code>lambda.min.opt.max.m</code></td>
<td>
<p>A grid of values in [<code>lambda.min.opt.max.m[m,1], lambda.min.opt.max.m[m,3]</code>] is considered to seek for the optimal <code class="reqn">\lambda</code> (<code>lambda.min.opt.max.m[m,2]</code>)
used by the optimal <code class="reqn">\beta</code> for each <code class="reqn">\theta</code> in <code class="reqn">\Theta_n</code>.</p>
</td></tr>
<tr><td><code>h.min.opt.max.mopt</code></td>
<td>
<p><code>h.opt=h.min.opt.max.mopt[2]</code> (used by <code>theta.est</code> and <code>beta.est</code>) was seeked between <code>h.min.opt.max.mopt[1]</code> and <code>h.min.opt.max.mopt[3]</code>.</p>
</td></tr>
<tr><td><code>h.min.opt.max.m</code></td>
<td>
<p>For each <code class="reqn">\theta</code> in <code class="reqn">\Theta_n</code>, the optimal <code class="reqn">h</code> (<code>h.min.opt.max.m[m,2]</code>) used by the optimal <code class="reqn">\beta</code> for this <code class="reqn">\theta</code> was seeked between <code>h.min.opt.max.m[m,1]</code> and <code>h.min.opt.max.m[m,3]</code>.</p>
</td></tr>
<tr><td><code>h.seq.opt</code></td>
<td>
<p>Sequence of eligible values for <code class="reqn">h</code> considered to seek for <code>h.opt</code>.</p>
</td></tr>
<tr><td><code>theta.seq.norm</code></td>
<td>
<p>The vector <code>theta.seq.norm[j,]</code> contains the coefficientes in the B-spline basis of the jth functional index in <code class="reqn">\Theta_n</code>.</p>
</td></tr>
<tr><td><code>vn.opt</code></td>
<td>
<p>Selected value of <code>vn</code>.</p>
</td></tr>
<tr><td><code>...</code></td>
<td>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>German Aneiros Perez <a href="mailto:german.aneiros@udc.es">german.aneiros@udc.es</a> 
</p>
<p>Silvia Novo Diaz  <a href="mailto:snovo@est-econ.uc3m.es">snovo@est-econ.uc3m.es</a>
</p>


<h3>References</h3>

<p>Ait-Saidi, A., Ferraty, F., Kassa, R., and Vieu, P. (2008) Cross-validated estimations in the single-functional index model. <em>Statistics</em>, <b>42(6)</b>, 475&ndash;494, <a href="https://doi.org/10.1080/02331880801980377">doi:10.1080/02331880801980377</a>.
</p>
<p>Novo S., Aneiros, G., and Vieu, P., (2019) Automatic and location-adaptive estimation in functional single-index regression. <em>Journal of Nonparametric Statistics</em>, <b>31(2)</b>, 364&ndash;392, <a href="https://doi.org/10.1080/10485252.2019.1567726">doi:10.1080/10485252.2019.1567726</a>.
</p>
<p>Novo, S., Aneiros, G., and Vieu, P., (2021) Sparse semiparametric regression
when predictors are mixture of functional and high-dimensional variables. <em>TEST</em>,
<b>30</b>, 481&ndash;504, <a href="https://doi.org/10.1007/s11749-020-00728-w">doi:10.1007/s11749-020-00728-w</a>.
</p>
<p>Novo, S., Aneiros, G., and Vieu, P., (2021) A kNN procedure in semiparametric
functional data analysis. <em>Statistics and Probability Letters</em>, <b>171</b>, 109028, <a href="https://doi.org/10.1016/j.spl.2020.109028">doi:10.1016/j.spl.2020.109028</a>.
</p>


<h3>See Also</h3>

<p>See also <code><a href="#topic+fsim.kernel.fit">fsim.kernel.fit</a></code>, <code><a href="#topic+predict.sfplsim.kernel">predict.sfplsim.kernel</a></code> and  <code><a href="#topic+plot.sfplsim.kernel">plot.sfplsim.kernel</a></code>
</p>
<p>Alternative procedure <code><a href="#topic+sfplsim.kNN.fit">sfplsim.kNN.fit</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

data("Tecator")
y&lt;-Tecator$fat
X&lt;-Tecator$absor.spectra2
z1&lt;-Tecator$protein       
z2&lt;-Tecator$moisture

#Quadratic, cubic and interaction effects of the scalar covariates.
z.com&lt;-cbind(z1,z2,z1^2,z2^2,z1^3,z2^3,z1*z2)
train&lt;-1:160


#SSFPLSIM fit. Convergence errors for some theta are obtained.
ptm=proc.time()
fit&lt;-sfplsim.kernel.fit(x=X[train,], z=z.com[train,], y=y[train],
      max.q.h=0.35,lambda.min.h=0.02,lambda.min.l=0.01,
      factor.pn=2, max.iter=5000, nknot.theta=4,criterion="BIC",
      penalty="grSCAD",nknot=20)
proc.time()-ptm

#Results
fit
names(fit)


</code></pre>

<hr>
<h2 id='sfplsim.kNN.fit'>
Sparse semi-functional partial linear single-index model fit using kNN estimation
</h2><span id='topic+sfplsim.kNN.fit'></span>

<h3>Description</h3>

<p>This function fits a sparse semi-functional partial linear single-index model between a scalar response, a functional explanatory variable and
a vector of scalar covariates. 
The function uses the penalised least-squares regularization procedure combined with <code class="reqn">k</code>-nearest neighbours (kNN) estimation with Nadaraya-Watson weights.
</p>
<p>The procedure requires the  B-spline representation to estimate the functional index <code class="reqn">\theta_0</code> and an objective criterion (<code>criterion</code>) to select the number of neighbours (<code>k.opt</code>) and the regularization parameter (<code>lambda.opt</code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sfplsim.kNN.fit(x, z, y, seed.coeff = c(-1, 0, 1), order.Bspline = 3, 
  nknot.theta = 3,t0 = NULL,knearest = NULL, min.knn = 2, max.knn = NULL,
  step = NULL,range.grid = NULL, kind.of.kernel = "quad", nknot = NULL,
  lambda.min = NULL, lambda.min.h = NULL, lambda.min.l = NULL, 
  factor.pn = 1, nlambda = 100, lambda.seq = NULL,vn = ncol(z), 
  nfolds = 10, seed = 123, criterion = c("GCV", "BIC", "AIC", "k-fold-CV"), 
  penalty = c("grLasso", "grMCP", "grSCAD", "gel", "cMCP", 
  "gBridge", "gLasso", "gMCP"), max.iter = 1000)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sfplsim.kNN.fit_+3A_x">x</code></td>
<td>

<p>Matrix containing the observations of the functional covariate collected by row.
</p>
</td></tr>
<tr><td><code id="sfplsim.kNN.fit_+3A_z">z</code></td>
<td>

<p>Matrix containing the observations of the scalar covariates collected by row.
</p>
</td></tr>
<tr><td><code id="sfplsim.kNN.fit_+3A_y">y</code></td>
<td>

<p>Vector containing the scalar response.
</p>
</td></tr>
<tr><td><code id="sfplsim.kNN.fit_+3A_seed.coeff">seed.coeff</code></td>
<td>

<p>Vector of initial values used to  build the set <code class="reqn">\Theta_n</code> (see section <code>Details</code>). The coefficients for the B-spline representation of each eligible functional index <code class="reqn">\theta \in \Theta_n</code> are obtained from <code>seed.coeff</code>.  The default is <code>c(-1,0,1)</code>.
</p>
</td></tr>
<tr><td><code id="sfplsim.kNN.fit_+3A_order.bspline">order.Bspline</code></td>
<td>

<p>Positive integer giving the order of the B-spline basis functions. This is the number of coefficients in each piecewise polynomial segment. The default is 3.
</p>
</td></tr>
<tr><td><code id="sfplsim.kNN.fit_+3A_nknot.theta">nknot.theta</code></td>
<td>

<p>Positive integer indicating the number of uniform interior knots of the B-spline basis for the B-spline representation of <code class="reqn">\theta_0</code>. The default is 3.
</p>
</td></tr>
<tr><td><code id="sfplsim.kNN.fit_+3A_t0">t0</code></td>
<td>

<p>Value in the domain of the functional indexes at which we evaluate them to  build the set <code class="reqn">\Theta_n</code>. We assume <code class="reqn">\theta_0(t_0)&gt;0</code> for some arbitrary <code class="reqn">t_0</code> in the domain to ensure model identifiability. If <code>t0=NULL</code>, then <code>mean(range.grid)</code> is considered.
</p>
</td></tr>
<tr><td><code id="sfplsim.kNN.fit_+3A_knearest">knearest</code></td>
<td>

<p>Vector of positive integers containing the sequence in which the  number of nearest neighbours <code>k.opt</code> is selected. If <code>knearest=NULL</code>, then <code>knearest &lt;- seq(from=min.knn, to=max.knn, by=step)</code>.
</p>
</td></tr>
<tr><td><code id="sfplsim.kNN.fit_+3A_min.knn">min.knn</code></td>
<td>

<p>Positive integer indicating the minumum value of the sequence in which the  number of nearest neighbours <code>k.opt</code> is selected (thus, this number must be smaller than the sample size). The default is 2.
</p>
</td></tr>
<tr><td><code id="sfplsim.kNN.fit_+3A_max.knn">max.knn</code></td>
<td>

<p>Positive integer indicating the maximum value of the sequence in which the number of nearest neighbours <code>k.opt</code> is selected (thus, this number must be larger than <code>min.kNN</code> and smaller than the sample size, <code>n</code>). The default is <code>max.knn &lt;- n%/%2</code>. 
</p>
</td></tr>
<tr><td><code id="sfplsim.kNN.fit_+3A_step">step</code></td>
<td>

<p>Positive integer used to build the sequence of k-nearest neighbours in the following way: <code>min.knn, min.knn + step, min.knn + 2*step, min.knn + 3*step,...</code> The default is <code>step&lt;-ceiling(n/100)</code>.
</p>
</td></tr>
<tr><td><code id="sfplsim.kNN.fit_+3A_range.grid">range.grid</code></td>
<td>

<p>Vector of length 2 containing the endpoints of the grid at which the observations of the functional covariate <code>x</code> are evaluated (i.e. the range of the discretization). If <code>range.grid=NULL</code>, then <code>range.grid=c(1,p)</code> is considered, where <code>p</code> is the size of the discretization size of <code>x</code> (i.e. <code>ncol(x))</code>.
</p>
</td></tr>
<tr><td><code id="sfplsim.kNN.fit_+3A_kind.of.kernel">kind.of.kernel</code></td>
<td>

<p>The type of kernel function used. Only Epanechnikov kernel (<code>"quad"</code>) is available.
</p>
</td></tr>
<tr><td><code id="sfplsim.kNN.fit_+3A_nknot">nknot</code></td>
<td>

<p>Positive integer indicating the number of interior knots for the B-spline representation of the functional covariate. The default value is <code>(p - order.Bspline - 1)%/%2</code>.
</p>
</td></tr>
<tr><td><code id="sfplsim.kNN.fit_+3A_lambda.min">lambda.min</code></td>
<td>

<p>The smallest value for lambda (i. e., the smallest value  of the sequence in which <code>lambda.opt</code> is selected), as fraction of <code>lambda.max</code>.
The defaults is <code>lambda.min.l</code> if the number of observations is larger than <code>factor.pn</code> times the number of covariates and <code>lambda.min.h</code> otherwise.
</p>
</td></tr>
<tr><td><code id="sfplsim.kNN.fit_+3A_lambda.min.h">lambda.min.h</code></td>
<td>

<p>The smallest value of the sequence in which <code>lambda.opt</code> is selected if the number of observations is smaller than <code>factor.pn</code> times the number of scalar covariates. The default is 0.05. 
</p>
</td></tr>
<tr><td><code id="sfplsim.kNN.fit_+3A_lambda.min.l">lambda.min.l</code></td>
<td>

<p>The smallest value of the sequence in which <code>lambda.opt</code> is selected if the number of observations is larger than <code>factor.pn</code> times the number of scalar covariates. The default is 0.0001.
</p>
</td></tr>
<tr><td><code id="sfplsim.kNN.fit_+3A_factor.pn">factor.pn</code></td>
<td>

<p>Positive integer used to set <code>lambda.min</code>. The default value is 1.
</p>
</td></tr>
<tr><td><code id="sfplsim.kNN.fit_+3A_nlambda">nlambda</code></td>
<td>

<p>Positive integer indicating the number of values of the sequence in which <code>lambda.opt</code> is selected. The default is 100.
</p>
</td></tr>
<tr><td><code id="sfplsim.kNN.fit_+3A_lambda.seq">lambda.seq</code></td>
<td>

<p>Sequence of values in which <code>lambda.opt</code> is selected. If <code>lambda.seq=NULL</code>, then the programme builds the sequence automatically using <code>lambda.min</code> and <code>nlambda</code>. For non-penalized estimation, i. e. ordinary least squares estimation, set <code>lambda.seq=0</code>.
</p>
</td></tr>
<tr><td><code id="sfplsim.kNN.fit_+3A_vn">vn</code></td>
<td>
<p>Positive integer or vector of positive integers indicating the number of groups of consecutive variables to be penalised together. The default value is <code>vn=ncol(z)</code>, which leads to the individual penalisation of each scalar covariate.
</p>
</td></tr>
<tr><td><code id="sfplsim.kNN.fit_+3A_nfolds">nfolds</code></td>
<td>

<p>Positive integer indicating the number of cross-validation folds (used if <code>criterion="k-fold-CV"</code>). The default is 10.
</p>
</td></tr>
<tr><td><code id="sfplsim.kNN.fit_+3A_seed">seed</code></td>
<td>

<p>You may set the seed of the random number generator to obtain reproducible results (used if <code>criterion="k-fold-CV"</code>). The default is 123.
</p>
</td></tr>
<tr><td><code id="sfplsim.kNN.fit_+3A_criterion">criterion</code></td>
<td>

<p>The criterion by which to select the regularization parameter <code>lambda.opt</code> and <code>k.opt</code>. One of <code>"GCV", "BIC", "AIC"</code> or <code>"k-fold-CV"</code>. The default is <code>"GCV"</code>.
</p>
</td></tr>
<tr><td><code id="sfplsim.kNN.fit_+3A_penalty">penalty</code></td>
<td>

<p>The penalty function to be applied in the penalized least squares procedure. Only &quot;grLasso&quot; and &quot;grSCAD&quot; are implemented.
</p>
</td></tr>
<tr><td><code id="sfplsim.kNN.fit_+3A_max.iter">max.iter</code></td>
<td>

<p>Maximum number of iterations (total across entire path). Default is 1000.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The sparse semi-functional partial linear single-index model (SSFPLSIM) is given by the expression:
</p>
<p style="text-align: center;"><code class="reqn">
Y_i=Z_{i1}\beta_{01}+\dots+Z_{ip_n}\beta_{0p_n}+r(\left&lt;\theta_0,X_i\right&gt;)+\varepsilon_i\ \ \ i=1,\dots,n,
</code>
</p>

<p>where <code class="reqn">Y_i</code> denotes a scalar response, <code class="reqn">Z_{i1},\dots,Z_{ip_n}</code> are real random covariates and <code class="reqn">X_i</code> is a functional random covariate valued in a separable Hilbert space <code class="reqn">\mathcal{H}</code> with inner product <code class="reqn">\left\langle \cdot, \cdot \right\rangle</code>. In this equation,
<code class="reqn">\mathbf{\beta}_0=(\beta_{01},\dots,\beta_{0p_n})^{\top}</code>, <code class="reqn">\theta_0\in\mathcal{H}</code> and <code class="reqn">r(\cdot)</code> are a vector of unknown real parameters, an unknown functional direction and an unknown smooth real-valued function, respectively. In addition, <code class="reqn">\varepsilon_i</code> is the random error.
</p>
<p>The SSFPLSIM is fitted using the penalised least-squares approach. The first idea is to transform the SSFPLSIM into a linear model by extracting from <code class="reqn">Y_i</code> and <code class="reqn">Z_{ij}</code> (<code class="reqn">j=1,\ldots,p_n</code>) the effect of the functional covariate <code class="reqn">X_i</code> using functional single-index regression. This is made using nonparametric kNN estimation (see, for details, the documentation of the function <code>fsim.kNN.fit</code>).
</p>
<p>Then, an approximate linear model is obtained:
</p>
<p style="text-align: center;"><code class="reqn">\widetilde{\mathbf{Y}}_{\theta_0}\approx\widetilde{\mathbf{Z}}_{\theta_0}\mathbf{\beta}_0+\mathbf{\varepsilon},</code>
</p>

<p>and the penalised least-squares procedure is applied to this model by minimising over the pair <code class="reqn">(\mathbf{\beta},\theta)</code>
</p>
<p style="text-align: center;"><code class="reqn">
\mathcal{Q}\left(\mathbf{\beta},\theta\right)=\frac{1}{2}\left(\widetilde{\mathbf{Y}}_{\theta}-\widetilde{\mathbf{Z}}_{\theta}\mathbf{\beta}\right)^{\top}\left(\widetilde{\mathbf{Y}}_{\theta}-\widetilde{\mathbf{Z}}_{\theta}\mathbf{\beta}\right)+n\sum_{j=1}^{p_n}\mathcal{P}_{\lambda_{j_n}}\left(|\beta_j|\right), \quad (1)
</code>
</p>

<p>where <code class="reqn">\mathbf{\beta}=(\beta_1,\ldots,\beta_{p_n})^{\top}, \ \mathcal{P}_{\lambda_{j_n}}\left(\cdot\right)</code> is a penalty function (specified in the argument <code>penalty</code>) and <code class="reqn">\lambda_{j_n} &gt; 0</code> is a tuning parameter.
To reduce  the quantity of tuning parameters, <code class="reqn">\lambda_j</code>, to be selected for each sample, we consider <code class="reqn">\lambda_j = \lambda \widehat{\sigma}_{\beta_{0,j,OLS}}</code>, where <code class="reqn">\beta_{0,j,OLS}</code> denotes the OLS estimate of <code class="reqn">\beta_{0,j}</code> and <code class="reqn">\widehat{\sigma}_{\beta_{0,j,OLS}}</code> is the estimated standard deviation. Both <code class="reqn">\lambda</code> and <code class="reqn">k</code> (in the kNN estimation) are selected using the objetive criterion specified in the argument <code>criterion</code>.
</p>
<p>In addition, the function uses B-spline representation to build a set  <code class="reqn">\Theta_n</code> of eligible functional indexes <code class="reqn">\theta</code>. The dimension of the B-spline basis is <code>order.Bspline</code>+<code>nknot.theta</code> and the set of eligible coefficients is obtained by calibrating (to ensure the identifiability of the model) the set of initial coefficients given in <code>seed.coeff</code>. The larger this set, the higher the size of <code class="reqn">\Theta_n</code>. Since our approach requires intensive computation, we need a trade-off between the size of <code class="reqn">\Theta_n</code> and the performance of the estimator. For that, Ait-Saidi et al. (2008) suggested considering <code>order.Bspline=3</code> and <code>seed.coeff=c(-1,0,1)</code>. For details on the construction of <code class="reqn">\Theta_n</code> see Novo et al. (2019).
</p>
<p>Finally, after estimating <code class="reqn">\mathbf{\beta}_0</code> and <code class="reqn">\theta_0</code> by minimising (1), we deal with the estimation of the nonlinear function <code class="reqn">r_{\theta_0}(\cdot)\equiv r\left(\left&lt;\theta_0,\cdot\right&gt;\right)</code>.
For that we employ again the kNN procedure with Nadaraya-Watson weights to smooth the partial residuals <code class="reqn">Y_i-\mathbf{Z}_i^{\top}\widehat{\mathbf{\beta}}</code>.
</p>
<p>For further details on the estimation procedure of the SSFPLSIM, see Novo et al. (2021).
</p>
<p><b>Remark</b>: We should note that if we set <code>lambda.seq</code><code class="reqn">=0</code>, we can obtain the non-penalised estimation of the model, i.e. the OLS estimation. It is convenient to use <code>lambda.seq</code><code class="reqn">\not=0</code> when one suspects there are irrelevant variables.
</p>


<h3>Value</h3>

<table>
<tr><td><code>call</code></td>
<td>
<p>The matched call.</p>
</td></tr>
<tr><td><code>fitted.values</code></td>
<td>
<p>Estimated scalar response.</p>
</td></tr>
<tr><td><code>residuals</code></td>
<td>
<p>Differences between <code>y</code> and the <code>fitted.values</code></p>
</td></tr>
<tr><td><code>beta.est</code></td>
<td>
<p><code class="reqn">\hat{\mathbf{\beta}}</code> (i. e. the estimate of <code class="reqn">\mathbf{\beta}_0</code> when the optimal tuning parameters <code>lambda.opt</code>, <code>k.opt</code> and <code>vn.opt</code> are used).</p>
</td></tr>
<tr><td><code>theta.est</code></td>
<td>
<p>Coefficients of <code class="reqn">\hat{\theta}</code> in the B-spline basis (when the optimal tuning parameters <code>lambda.opt</code>, <code>k.opt</code> and <code>vn.opt</code>) are used): a vector of <code>length(order.Bspline+nknot.theta)</code>.</p>
</td></tr>
<tr><td><code>indexes.beta.nonnull</code></td>
<td>
<p>Indexes of the non-zero <code class="reqn">\hat{\beta_{j}}</code>.</p>
</td></tr>
<tr><td><code>k.opt</code></td>
<td>
<p>Selected number of nearest neighbours.</p>
</td></tr>
<tr><td><code>lambda.opt</code></td>
<td>
<p>Selected value of the penalisation parameter <code class="reqn">\lambda</code>.</p>
</td></tr>
<tr><td><code>IC</code></td>
<td>
<p>Value of the criterion function considered to select <code>lambda.opt</code>, <code>k.opt</code> and <code>vn.opt</code>.</p>
</td></tr>
<tr><td><code>Q.opt</code></td>
<td>
<p>Minimum value of the penalized criterion used to estimate <code class="reqn">\mathbf{\beta}_0</code> and <code class="reqn">\theta_0</code>. That is, the value obtained using <code>theta.est</code> and <code>beta.est</code>.</p>
</td></tr>
<tr><td><code>Q</code></td>
<td>
<p>Vector of dimension equal to the cardinal of <code class="reqn">\Theta_n</code>, containing the values of the penalized criterion for each functional index in <code class="reqn">\Theta_n</code>.</p>
</td></tr>
<tr><td><code>m.opt</code></td>
<td>
<p>Index of <code class="reqn">\hat{\theta}</code> in the set <code class="reqn">\Theta_n</code>.</p>
</td></tr>
<tr><td><code>lambda.min.opt.max.mopt</code></td>
<td>
<p>A grid of values in [<code>lambda.min.opt.max.mopt[1], lambda.min.opt.max.mopt[3]</code>] is considered to seek for the <code>lambda.opt</code> (<code>lambda.opt=lambda.min.opt.max.mopt[2]</code>).
</p>
</td></tr>
<tr><td><code>lambda.min.opt.max.m</code></td>
<td>
<p>A grid of values in [<code>lambda.min.opt.max.m[m,1], lambda.min.opt.max.m[m,3]</code>] is considered to seek for the optimal <code class="reqn">\lambda</code> (<code>lambda.min.opt.max.m[m,2]</code>)
used by the optimal <code class="reqn">\mathbf{\beta}</code> for each <code class="reqn">\theta</code> in <code class="reqn">\Theta_n</code>.</p>
</td></tr>
<tr><td><code>knn.min.opt.max.mopt</code></td>
<td>
<p><code>k.opt=knn.min.opt.max.mopt[2]</code> (used by <code>theta.est</code> and <code>beta.est</code>) was seeked between <code>knn.min.opt.max.mopt[1]</code> and <code>knn.min.opt.max.mopt[3]</code> (no necessarly the step was 1).</p>
</td></tr>
<tr><td><code>knn.min.opt.max.m</code></td>
<td>
<p>For each <code class="reqn">\theta</code> in <code class="reqn">\Theta_n</code>, the optimal <code class="reqn">k</code> (<code>knn.min.opt.max.m[m,2]</code>) used by the optimal <code class="reqn">\beta</code> for this <code class="reqn">\theta</code> was seeked between <code>knn.min.opt.max.m[m,1]</code> and <code>knn.min.opt.max.m[m,3]</code> (no necessarly the step was 1).</p>
</td></tr>
<tr><td><code>knearest</code></td>
<td>
<p>Sequence of eligible values for <code class="reqn">k</code> considered to seek for <code>k.opt</code>.</p>
</td></tr>
<tr><td><code>theta.seq.norm</code></td>
<td>
<p>The vector <code>theta.seq.norm[j,]</code> contains the coefficientes in the B-spline basis of the jth functional index in <code class="reqn">\Theta_n</code>.</p>
</td></tr>
<tr><td><code>vn.opt</code></td>
<td>
<p>Selected value of <code>vn</code>.</p>
</td></tr>
<tr><td><code>...</code></td>
<td>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>German Aneiros Perez <a href="mailto:german.aneiros@udc.es">german.aneiros@udc.es</a> 
</p>
<p>Silvia Novo Diaz  <a href="mailto:snovo@est-econ.uc3m.es">snovo@est-econ.uc3m.es</a>
</p>


<h3>References</h3>

<p>Ait-Saidi, A., Ferraty, F., Kassa, R., and Vieu, P., (2008) Cross-validated estimations in the single-functional index model. <em>Statistics</em>, <b>42(6)</b>, 475&ndash;494, <a href="https://doi.org/10.1080/02331880801980377">doi:10.1080/02331880801980377</a>.
</p>
<p>Novo S., Aneiros, G., and Vieu, P., (2019) Automatic and location-adaptive estimation in functional single-index regression. <em>Journal of Nonparametric Statistics</em>, <b>31(2)</b>, 364&ndash;392, <a href="https://doi.org/10.1080/10485252.2019.1567726">doi:10.1080/10485252.2019.1567726</a>.
</p>
<p>Novo, S., Aneiros, G., and Vieu, P., (2021) Sparse semiparametric regression
when predictors are mixture of functional and high-dimensional variables. <em>TEST</em>,
<b>30</b>, 481&ndash;504, <a href="https://doi.org/10.1007/s11749-020-00728-w">doi:10.1007/s11749-020-00728-w</a>.
</p>
<p>Novo, S., Aneiros, G., and Vieu, P., (2021) A kNN procedure in semiparametric
functional data analysis. <em>Statistics and Probability Letters</em>, <b>171</b>, 109028, <a href="https://doi.org/10.1016/j.spl.2020.109028">doi:10.1016/j.spl.2020.109028</a>
</p>


<h3>See Also</h3>

<p>See also <code><a href="#topic+fsim.kNN.fit">fsim.kNN.fit</a></code>, <code><a href="#topic+predict.sfplsim.kNN">predict.sfplsim.kNN</a></code> and  <code><a href="#topic+plot.sfplsim.kNN">plot.sfplsim.kNN</a></code>
</p>
<p>Alternative procedure <code><a href="#topic+sfplsim.kernel.fit">sfplsim.kernel.fit</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

data("Tecator")
y&lt;-Tecator$fat
X&lt;-Tecator$absor.spectra2
z1&lt;-Tecator$protein       
z2&lt;-Tecator$moisture

#Quadratic, cubic and interaction effects of the scalar covariates.
z.com&lt;-cbind(z1,z2,z1^2,z2^2,z1^3,z2^3,z1*z2)
train&lt;-1:160


#SSFPLSIM fit. Convergence errors for some theta are obtained.
ptm=proc.time()
fit&lt;-sfplsim.kNN.fit(y=y[train],x=X[train,], z=z.com[train,], max.knn=20,
    lambda.min.h=0.02,lambda.min.l=0.01, factor.pn=2,  nknot.theta=4,
    criterion="BIC",range.grid=c(850,1050), penalty="grSCAD",
    nknot=20, max.iter=5000)
proc.time()-ptm

#Results
fit
names(fit)


  
</code></pre>

<hr>
<h2 id='Sugar'>
Sugar data
</h2><span id='topic+Sugar'></span>

<h3>Description</h3>

<p>Ash content and absorbance spectra at two different excitation wavelengths of 268 samples of sugar. Detailed information about this dataset can be found in <a href="https://ucphchemometrics.com/datasets/">https://ucphchemometrics.com/datasets/</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(Sugar)</code></pre>


<h3>Format</h3>

<p>A <code>list</code> containing:
</p>

<ul>
<li><p><code>ash</code>: A vector with the ash contents.
</p>
</li>
<li><p><code>wave.290</code>: A matrix containing the absorbance spectra observed on 571 equally spaced wavelengths in the range 275-560 nm at excitation wavelengths 290 nm.
</p>
</li>
<li><p><code>wave.240</code>: A matrix containing the absorbance spectra observed on 571 equally spaced wavelengths in the range 275-560 nm at excitation wavelengths 240 nm.
</p>
</li></ul>



<h3>References</h3>

<p>Aneiros, G., and Vieu, P. (2015) Partial linear modelling with multi-functional covariates. <em>Computational Statistics</em>, <b>30</b>, 647&ndash;671, <a href="https://doi.org/10.1007/s00180-015-0568-8">doi:10.1007/s00180-015-0568-8</a>.
</p>
<p>Novo, S., Vieu, P., and Aneiros, G., (2021) Fast and efficient algorithms for
sparse semiparametric bi-functional regression. <em>Australian and New Zealand
Journal of Statistics</em>, <b>63</b>, 606&ndash;638, <a href="https://doi.org/10.1111/anzs.12355">doi:10.1111/anzs.12355</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Sugar)
names(Sugar)
Sugar$ash
dim(Sugar$wave.290)
dim(Sugar$wave.240)
</code></pre>

<hr>
<h2 id='Tecator'>
Tecator data
</h2><span id='topic+Tecator'></span>

<h3>Description</h3>

<p>Fat, protein, moisture content and absorbance spectra (with the first and the second derivative) of 215 samples of meat.
A detailed description of the data can be seen in <a href="http://lib.stat.cmu.edu/datasets/tecator">http://lib.stat.cmu.edu/datasets/tecator</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(Tecator)</code></pre>


<h3>Format</h3>

<p>A <code>list</code> containing:
</p>

<ul>
<li><p><code>fat</code>: A vector with the fat contents.
</p>
</li>
<li><p><code>protein</code>: A vector with the protein contents.
</p>
</li>
<li><p><code>moisture</code>: A vector with the moisture contents.
</p>
</li>
<li><p><code>absor.spectra</code>: A matrix containing the near-infrared absorbance spectra observed on 100 equally spaced wavelengths in the range 850-1050 nm.
</p>
</li>
<li><p><code>absor.spectra1</code>: Fist derivative of the absorbance spectra (computed using B-spline representation of the curves).
</p>
</li>
<li><p><code>absor.spectra2</code>: Second derivative of the absorbance spectra (computed using B-spline representation of the curves).
</p>
</li></ul>



<h3>References</h3>

<p>Ferraty, F. and Vieu, P. (2006) <em>Nonparametric functional data analysis</em>, Springer Series in Statistics, New York.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Tecator)
names(Tecator)
Tecator$fat
Tecator$protein
Tecator$moisture
dim(Tecator$absor.spectra)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
