<!DOCTYPE html><html><head><title>Help for package fsemipar</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {fsemipar}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#fsemipar-package'>
<p>Estimation, Variable Selection and Prediction for Functional Semiparametric Models</p></a></li>
<li><a href='#FASSMR.kernel.fit'>
<p>Impact point selection with FASSMR and kernel estimation</p></a></li>
<li><a href='#FASSMR.kNN.fit'>
<p>Impact point selection with FASSMR and kNN estimation</p></a></li>
<li><a href='#fsemipar.internal'>
<p>Package fsemipar internal functions</p></a></li>
<li><a href='#fsim.kernel.fit'>
<p>Functional single-index model fit using kernel estimation and joint LOOCV minimisation</p></a></li>
<li><a href='#fsim.kernel.fit.optim'>
<p>Functional single-index model fit using kernel estimation and iterative LOOCV minimisation</p></a></li>
<li><a href='#fsim.kernel.test'>
<p>Functional single-index kernel predictor</p></a></li>
<li><a href='#fsim.kNN.fit'>
<p>Functional single-index model fit using kNN estimation and joint LOOCV minimisation</p></a></li>
<li><a href='#fsim.kNN.fit.optim'>
<p>Functional single-index model fit using kNN estimation and iterative LOOCV minimisation</p></a></li>
<li><a href='#fsim.kNN.test'>
<p>Functional single-index kNN predictor</p></a></li>
<li><a href='#IASSMR.kernel.fit'>
<p>Impact point selection with IASSMR and kernel estimation</p></a></li>
<li><a href='#IASSMR.kNN.fit'>
<p>Impact point selection with IASSMR and kNN estimation</p></a></li>
<li><a href='#lm.pels.fit'>
<p>Regularised fit of sparse linear regression</p></a></li>
<li><a href='#plot.classes'>
<p>Graphical representation of regression model outputs</p></a></li>
<li><a href='#predict.fsim'>
<p>Prediction for FSIM</p></a></li>
<li><a href='#predict.IASSMR'>
<p>Prediction for MFPLSIM</p></a></li>
<li><a href='#predict.lm'>
<p>Prediction for linear models</p></a></li>
<li><a href='#predict.mfplm.PVS'>
<p>Prediction for MFPLM</p></a></li>
<li><a href='#predict.sfpl'>
<p>Predictions for SFPLM</p></a></li>
<li><a href='#predict.sfplsim.FASSMR'>
<p>Prediction for SFPLSIM and MFPLSIM (using FASSMR)</p></a></li>
<li><a href='#print.summary.fsim'>
<p>Summarise information from FSIM estimation</p></a></li>
<li><a href='#print.summary.lm'>
<p>Summarise information from linear models estimation</p></a></li>
<li><a href='#print.summary.mfpl'>
<p>Summarise information from MFPLM estimation</p></a></li>
<li><a href='#print.summary.mfplsim'>
<p>Summarise information from MFPLSIM estimation</p></a></li>
<li><a href='#print.summary.sfpl'>
<p>Summarise information from SFPLM estimation</p></a></li>
<li><a href='#print.summary.sfplsim'>
<p>Summarise information from SFPLSIM estimation</p></a></li>
<li><a href='#projec'><p>Inner product computation</p></a></li>
<li><a href='#PVS.fit'>
<p>Impact point selection with PVS</p></a></li>
<li><a href='#PVS.kernel.fit'>
<p>Impact point selection with PVS and kernel estimation</p></a></li>
<li><a href='#PVS.kNN.fit'>
<p>Impact point selection with PVS and kNN estimation</p></a></li>
<li><a href='#semimetric.projec'>
<p>Projection semi-metric computation</p></a></li>
<li><a href='#sfpl.kernel.fit'>
<p>SFPLM regularised fit using kernel estimation</p></a></li>
<li><a href='#sfpl.kNN.fit'>
<p>SFPLM regularised fit using kNN estimation</p></a></li>
<li><a href='#sfplsim.kernel.fit'>
<p>SFPLSIM regularised fit using kernel estimation</p></a></li>
<li><a href='#sfplsim.kNN.fit'>
<p>SFPLSIM regularised fit using kNN estimation</p></a></li>
<li><a href='#Sugar'>
<p>Sugar data</p></a></li>
<li><a href='#Tecator'>
<p>Tecator data</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Estimation, Variable Selection and Prediction for Functional
Semiparametric Models</td>
</tr>
<tr>
<td>Version:</td>
<td>1.1.1</td>
</tr>
<tr>
<td>Date:</td>
<td>2024-05-21</td>
</tr>
<tr>
<td>Author:</td>
<td>German Aneiros [aut],
  Silvia Novo [aut, cre]</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0), grpreg</td>
</tr>
<tr>
<td>Imports:</td>
<td>DiceKriging, splines, gtools, stats, parallelly, doParallel,
parallel, foreach, ggplot2, gridExtra, tidyr</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Silvia Novo &lt;snovo@est-econ.uc3m.es&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Routines for the estimation or simultaneous estimation and variable selection in several functional semiparametric models with scalar responses are provided. These models include the functional single-index model, the semi-functional partial linear model, and the semi-functional partial linear single-index model. Additionally, the package offers algorithms for handling scalar covariates with linear effects that originate from the discretization of a curve. This functionality is applicable in the context of the linear model, the multi-functional partial linear model, and the multi-functional partial linear single-index model. </td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-05-21 14:53:24 UTC; SNOVO</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-05-21 16:50:03 UTC</td>
</tr>
</table>
<hr>
<h2 id='fsemipar-package'>
Estimation, Variable Selection and Prediction for Functional Semiparametric Models
</h2><span id='topic+fsemipar-package'></span><span id='topic+fsemipar'></span>

<h3>Description</h3>

<p>This package is dedicated to the estimation and simultaneous estimation and variable selection in several functional semiparametric models with scalar response. These include the functional single-index model, the semi-functional partial linear model, and the semi-functional partial linear single-index model. Additionally, it encompasses algorithms for addressing estimation and variable selection in linear models and bi-functional partial linear models  when the scalar covariates with linear effects are derived from the discretisation of a curve. Furthermore, the package offers routines for kernel- and kNN-based estimation using Nadaraya-Watson weights in models with a nonparametric or semiparametric component. It also includes S3 methods (predict, plot, print, summary) to facilitate statistical analysis across all the considered models and estimation procedures.
</p>


<h3>Details</h3>

<p>The package can be divided into several thematic sections:
</p>

<ol>
<li><p> Estimation of the functional single-index model.
</p>

<ul>
<li> <p><code><a href="#topic+projec">projec</a></code>.
</p>
</li>
<li> <p><code><a href="#topic+semimetric.projec">semimetric.projec</a></code>.
</p>
</li>
<li> <p><code><a href="#topic+fsim.kernel.fit">fsim.kernel.fit</a></code> and <code><a href="#topic+fsim.kNN.fit">fsim.kNN.fit</a></code>.
</p>
</li>
<li> <p><code><a href="#topic+fsim.kernel.fit.optim">fsim.kernel.fit.optim</a></code> and <code><a href="#topic+fsim.kNN.fit.optim">fsim.kNN.fit.optim</a></code>
</p>
</li>
<li> <p><code><a href="#topic+fsim.kernel.test">fsim.kernel.test</a></code> and <code><a href="#topic+fsim.kNN.test">fsim.kNN.test</a></code>.
</p>
</li>
<li> <p><code>predict, plot, summary</code> and <code>print</code> methods for <code>fsim.kernel</code> and <code>fsim.kNN</code> classes.
</p>
</li></ul>

</li>
<li><p> Simultaneous estimation and variable selection in linear and semi-functional partial linear models.
</p>

<ol>
<li><p> Linear model
</p>

<ul>
<li> <p><code><a href="#topic+lm.pels.fit">lm.pels.fit</a></code>. 
</p>
</li>
<li> <p><code>predict, summary, plot</code> and <code>print</code> methods for <code>lm.pels</code> class.
</p>
</li></ul>

</li>
<li><p> Semi-functional partial linear model.
</p>

<ul>
<li> <p><code><a href="#topic+sfpl.kernel.fit">sfpl.kernel.fit</a></code> and <code><a href="#topic+sfpl.kNN.fit">sfpl.kNN.fit</a></code>.
</p>
</li>
<li> <p><code>predict, summary, plot</code> and <code>print</code> methods for <code>sfpl.kernel</code> and <code>sfpl.kNN</code> classes.
</p>
</li></ul>

</li>
<li><p> Semi-functional partial linear single-index model.
</p>

<ul>
<li> <p><code><a href="#topic+sfplsim.kernel.fit">sfplsim.kernel.fit</a></code> and <code><a href="#topic+sfplsim.kNN.fit">sfplsim.kNN.fit</a></code>.
</p>
</li>
<li> <p><code>predict, summary, plot</code> and <code>print</code> methods for <code>sfplsim.kernel</code> and <code>sfplsim.kNN</code> classes.
</p>
</li></ul>

</li></ol>

</li>
<li><p> Algorithms for impact point selection in models with covariates derived from the discretisation of a curve.
</p>

<ol>
<li><p> Linear model
</p>

<ul>
<li> <p><code><a href="#topic+PVS.fit">PVS.fit</a></code>. 
</p>
</li>
<li> <p><code>predict, summary, plot</code> and <code>print</code> methods for <code>PVS</code> class.
</p>
</li></ul>

</li>
<li><p> Bi-functional partial linear model.
</p>

<ul>
<li> <p><code><a href="#topic+PVS.kernel.fit">PVS.kernel.fit</a></code> and <code><a href="#topic+PVS.kNN.fit">PVS.kNN.fit</a></code>.
</p>
</li>
<li> <p><code>predict, summary, plot</code> and <code>print</code> methods for <code>PVS.kernel</code> and <code>PVS.kNN</code> classes.
</p>
</li></ul>

</li>
<li><p> Bi-functional partial linear single-index model.
</p>

<ul>
<li> <p><code><a href="#topic+FASSMR.kernel.fit">FASSMR.kernel.fit</a></code> and <code><a href="#topic+FASSMR.kNN.fit">FASSMR.kNN.fit</a></code>.
</p>
</li>
<li> <p><code><a href="#topic+IASSMR.kernel.fit">IASSMR.kernel.fit</a></code> and <code><a href="#topic+IASSMR.kNN.fit">IASSMR.kNN.fit</a></code>.
</p>
</li>
<li> <p><code>predict, summary, plot</code> and <code>print</code> methods for <code>FASSMR.kernel</code>, <code>FASSMR.kNN</code>, <code>IASSMR.kernel</code> and <code>IASSMR.kNN</code> classes.
</p>
</li></ul>

</li></ol>

</li>
<li><p> Two datasets: <code><a href="#topic+Tecator">Tecator</a></code> and <code><a href="#topic+Sugar">Sugar</a></code>.
</p>
</li></ol>



<h3>Author(s)</h3>

<p>German Aneiros [aut],
  Silvia Novo [aut, cre]
</p>
<p>Maintainer: Silvia Novo &lt;snovo@est-econ.uc3m.es&gt;
</p>


<h3>References</h3>

<p>Aneiros, G. and Vieu, P., (2014) Variable selection in infinite-dimensional problems, <em>Statistics and Probability Letters</em>, <b>94</b>, 12&ndash;20. <a href="https://doi.org/10.1016/j.spl.2014.06.025">doi:10.1016/j.spl.2014.06.025</a>.
</p>
<p>Aneiros, G., Ferraty, F., and Vieu, P., (2015) Variable selection in partial linear regression with functional
covariate, <em>Statistics</em>, <b>49</b> 1322&ndash;1347, <a href="https://doi.org/10.1080/02331888.2014.998675">doi:10.1080/02331888.2014.998675</a>.
</p>
<p>Aneiros, G., and Vieu, P., (2015) Partial linear modelling with multi-functional covariates. <em>Computational Statistics</em>, <b>30</b>, 647&ndash;671. <a href="https://doi.org/10.1007/s00180-015-0568-8">doi:10.1007/s00180-015-0568-8</a>.
</p>
<p>Novo S., Aneiros, G., and Vieu, P., (2019) Automatic and location-adaptive estimation in functional single-index regression, <em>Journal of Nonparametric Statistics</em>, <b>31(2)</b>, 364&ndash;392, <a href="https://doi.org/10.1080/10485252.2019.1567726">doi:10.1080/10485252.2019.1567726</a>.
</p>
<p>Novo, S., Aneiros, G., and Vieu, P., (2021) Sparse semiparametric regression
when predictors are mixture of functional and high-dimensional variables, <em>TEST</em>,
<b>30</b>, 481&ndash;504, <a href="https://doi.org/10.1007/s11749-020-00728-w">doi:10.1007/s11749-020-00728-w</a>.
</p>
<p>Novo, S., Aneiros, G., and Vieu, P., (2021) A kNN procedure in semiparametric
functional data analysis, <em>Statistics and Probability Letters</em>, <b>171</b>, 109028, <a href="https://doi.org/10.1016/j.spl.2020.109028">doi:10.1016/j.spl.2020.109028</a>.
</p>
<p>Novo, S., Vieu, P., and Aneiros, G., (2021) Fast and efficient algorithms for
sparse semiparametric bi-functional regression, <em>Australian and New Zealand
Journal of Statistics</em>, <b>63</b>, 606&ndash;638, <a href="https://doi.org/10.1111/anzs.12355">doi:10.1111/anzs.12355</a>.
</p>

<hr>
<h2 id='FASSMR.kernel.fit'>
Impact point selection with FASSMR and kernel estimation
</h2><span id='topic+FASSMR.kernel.fit'></span>

<h3>Description</h3>

<p>This function implements the Fast Algorithm for Sparse Semiparametric Multi-functional Regression (FASSMR) with kernel estimation. This algorithm is specifically designed for estimating multi-functional partial linear single-index models, which incorporate multiple scalar variables and a functional covariate as predictors. These scalar variables are derived from the discretisation of a curve and have linear effect while the functional covariate exhibits a single-index effect. 
</p>
<p>FASSMR selects the impact points of the discretised curve and estimates the model. The algorithm employs a penalised least-squares regularisation procedure, integrated with kernel estimation using Nadaraya-Watson weights. It uses B-spline expansions to represent curves and eligible functional indexes. Additionally, it utilises an objective criterion (<code>criterion</code>) to determine the initial number of covariates in the reduced model (<code>w.opt</code>), the bandwidth (<code>h.opt</code>), and the penalisation parameter (<code>lambda.opt</code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>FASSMR.kernel.fit(x, z, y, seed.coeff = c(-1, 0, 1), order.Bspline = 3, 
nknot.theta = 3,  min.q.h = 0.05, max.q.h = 0.5, h.seq = NULL, num.h = 10,  
kind.of.kernel = "quad",range.grid = NULL, nknot = NULL, lambda.min = NULL, 
lambda.min.h = NULL, lambda.min.l = NULL, factor.pn = 1, nlambda = 100, 
vn = ncol(z), nfolds = 10, seed = 123, wn = c(10, 15, 20), criterion = "GCV", 
penalty = "grSCAD", max.iter = 1000, n.core = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="FASSMR.kernel.fit_+3A_x">x</code></td>
<td>

<p>Matrix containing the observations of the functional covariate collected by row (functional single-index component).
</p>
</td></tr>
<tr><td><code id="FASSMR.kernel.fit_+3A_z">z</code></td>
<td>

<p>Matrix containing the observations of the functional covariate that is discretised collected by row (linear component).
</p>
</td></tr>
<tr><td><code id="FASSMR.kernel.fit_+3A_y">y</code></td>
<td>

<p>Vector containing the scalar response.
</p>
</td></tr>
<tr><td><code id="FASSMR.kernel.fit_+3A_seed.coeff">seed.coeff</code></td>
<td>

<p>Vector of initial values used to  build the set <code class="reqn">\Theta_n</code> (see section <code>Details</code>). The coefficients for the B-spline representation of each eligible functional index <code class="reqn">\theta \in \Theta_n</code> are obtained from <code>seed.coeff</code>.  The default is <code>c(-1,0,1)</code>.
</p>
</td></tr>
<tr><td><code id="FASSMR.kernel.fit_+3A_order.bspline">order.Bspline</code></td>
<td>

<p>Positive integer giving the order of the B-spline basis functions. This is the number of coefficients in each piecewise polynomial segment. The default is 3.
</p>
</td></tr>
<tr><td><code id="FASSMR.kernel.fit_+3A_nknot.theta">nknot.theta</code></td>
<td>

<p>Positive integer indicating the number of regularly spaced interior knots in the B-spline expansion of <code class="reqn">\theta_0</code>. The default is 3.
</p>
</td></tr>
<tr><td><code id="FASSMR.kernel.fit_+3A_min.q.h">min.q.h</code></td>
<td>
<p>Minimum quantile order of the distances between curves, which are computed using the projection semi-metric. This value determines the lower endpoint of the range from which the bandwidth is selected. The default is 0.05.
</p>
</td></tr>
<tr><td><code id="FASSMR.kernel.fit_+3A_max.q.h">max.q.h</code></td>
<td>

<p>Maximum quantile order of the distances between curves, which are computed using the projection semi-metric. This value determines the upper endpoint of the range from which the bandwidth is selected. The default is 0.5.
</p>
</td></tr>
<tr><td><code id="FASSMR.kernel.fit_+3A_h.seq">h.seq</code></td>
<td>
<p>Vector containing the sequence of bandwidths. The default is a sequence of <code>num.h</code> equispaced bandwidths in the range constructed using <code>min.q.h</code> and <code>max.q.h</code>.
</p>
</td></tr>
<tr><td><code id="FASSMR.kernel.fit_+3A_num.h">num.h</code></td>
<td>
<p>Positive integer indicating the number of bandwidths in the grid. The default is 10.
</p>
</td></tr>
<tr><td><code id="FASSMR.kernel.fit_+3A_kind.of.kernel">kind.of.kernel</code></td>
<td>

<p>The type of kernel function used. Currently, only Epanechnikov kernel (<code>"quad"</code>) is available.
</p>
</td></tr>
<tr><td><code id="FASSMR.kernel.fit_+3A_range.grid">range.grid</code></td>
<td>

<p>Vector of length 2 containing the endpoints of the grid at which the observations of the functional covariate <code>x</code> are evaluated (i.e. the range of the discretisation). If <code>range.grid=NULL</code>, then <code>range.grid=c(1,p)</code> is considered, where <code>p</code> is the discretisation size of <code>x</code> (i.e. <code>ncol(x))</code>.
</p>
</td></tr>
<tr><td><code id="FASSMR.kernel.fit_+3A_nknot">nknot</code></td>
<td>

<p>Positive integer indicating the number of interior knots for the B-spline expansion of the functional covariate. The default value is <code>(p - order.Bspline - 1)%/%2</code>.
</p>
</td></tr>
<tr><td><code id="FASSMR.kernel.fit_+3A_lambda.min">lambda.min</code></td>
<td>

<p>The smallest value for lambda (i. e., the lower endpoint  of the sequence in which <code>lambda.opt</code> is selected), as fraction of <code>lambda.max</code>.
The defaults is <code>lambda.min.l</code> if the sample size is larger than <code>factor.pn</code> times the number of linear covariates and <code>lambda.min.h</code> otherwise.
</p>
</td></tr>
<tr><td><code id="FASSMR.kernel.fit_+3A_lambda.min.h">lambda.min.h</code></td>
<td>

<p>The lower endpoint of the sequence in which <code>lambda.opt</code> is selected if the sample size is smaller than <code>factor.pn</code> times the number of linear covariates. The default is 0.05. 
</p>
</td></tr>
<tr><td><code id="FASSMR.kernel.fit_+3A_lambda.min.l">lambda.min.l</code></td>
<td>

<p>The lower endpoint of the sequence in which <code>lambda.opt</code> is selected if the sample size is larger than <code>factor.pn</code> times the number of linear covariates. The default is 0.0001.
</p>
</td></tr>
<tr><td><code id="FASSMR.kernel.fit_+3A_factor.pn">factor.pn</code></td>
<td>

<p>Positive integer used to set <code>lambda.min</code>. The default value is 1.
</p>
</td></tr>
<tr><td><code id="FASSMR.kernel.fit_+3A_nlambda">nlambda</code></td>
<td>

<p>Positive integer indicating the number of values in the sequence from which <code>lambda.opt</code> is selected. The default is 100.
</p>
</td></tr>
<tr><td><code id="FASSMR.kernel.fit_+3A_vn">vn</code></td>
<td>

<p>Positive integer or vector of positive integers indicating the number of groups of consecutive variables to be penalised together. The default value is <code>vn=ncol(z)</code>, resulting in the individual penalization of each scalar covariate.
</p>
</td></tr>
<tr><td><code id="FASSMR.kernel.fit_+3A_nfolds">nfolds</code></td>
<td>

<p>Positive integer indicating the number of cross-validation folds (used when <code>criterion="k-fold-CV"</code>). Default is 10.
</p>
</td></tr>
<tr><td><code id="FASSMR.kernel.fit_+3A_seed">seed</code></td>
<td>

<p>You may set the seed for the random number generator to ensure reproducible results (applicable when <code>criterion="k-fold-CV"</code> is used). The default seed value is 123.
</p>
</td></tr>
<tr><td><code id="FASSMR.kernel.fit_+3A_wn">wn</code></td>
<td>

<p>A vector of positive integers indicating the eligible number of covariates in the reduced model. For more information, refer to the section <code>Details</code>. The default is <code>c(10,15,20)</code>.
</p>
</td></tr>
<tr><td><code id="FASSMR.kernel.fit_+3A_criterion">criterion</code></td>
<td>

<p>The criterion used to select the tuning and regularisation parameters: <code>wn.opt</code>, <code>lambda.opt</code> and <code>h.opt</code> (also <code>vn.opt</code> if needed). Options include <code>"GCV"</code>, <code>"BIC"</code>, <code>"AIC"</code>, or <code>"k-fold-CV"</code>. The default setting is <code>"GCV"</code>.
</p>
</td></tr>
<tr><td><code id="FASSMR.kernel.fit_+3A_penalty">penalty</code></td>
<td>

<p>The penalty function applied in the penalised least-squares procedure. Currently, only &quot;grLasso&quot; and &quot;grSCAD&quot; are implemented. The default is &quot;grSCAD&quot;.
</p>
</td></tr>
<tr><td><code id="FASSMR.kernel.fit_+3A_max.iter">max.iter</code></td>
<td>

<p>Maximum number of iterations allowed across the entire path. The default value is 1000.
</p>
</td></tr>
<tr><td><code id="FASSMR.kernel.fit_+3A_n.core">n.core</code></td>
<td>

<p>Number of CPU cores designated for parallel execution. The default is <code>n.core&lt;-availableCores(omit=1)</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The multi-functional partial linear single-index model (MFPLSIM) is given by the expression
</p>
<p style="text-align: center;"><code class="reqn">Y_i=\sum_{j=1}^{p_n}\beta_{0j}\zeta_i(t_j)+r\left(\left&lt;\theta_0,X_i\right&gt;\right)+\varepsilon_i,\ \ \ (i=1,\dots,n),</code>
</p>

<p>where: 
</p>

<ul>
<li> <p><code class="reqn">Y_i</code> is a real random response and <code class="reqn">X_i</code> denotes a random element belonging to some separable Hilbert space <code class="reqn">\mathcal{H}</code> with inner product denoted by <code class="reqn">\left\langle\cdot,\cdot\right\rangle</code>. The second functional predictor <code class="reqn">\zeta_i</code> is assumed to be a curve defined on some interval <code class="reqn">[a,b]</code> which  is observed at the points <code class="reqn">a\leq t_1&lt;\dots&lt;t_{p_n}\leq b</code>. 
</p>
</li>
<li>  <p><code class="reqn">\mathbf{\beta}_0=(\beta_{01},\dots,\beta_{0p_n})^{\top}</code> is a vector of unknown real coefficients and <code class="reqn">r(\cdot)</code> denotes a smooth unknown link function. In addition, <code class="reqn">\theta_0</code> is an unknown functional direction in <code class="reqn">\mathcal{H}</code>.  
</p>
</li>
<li> <p><code class="reqn">\varepsilon_i</code> denotes the random error.
</p>
</li></ul>

<p>In  the MFPLSIM, we assume that only a few scalar variables from the set <code class="reqn">\{\zeta(t_1),\dots,\zeta(t_{p_n})\}</code> form part of the model. Therefore, we must select the relevant variables in the linear component (the impact points of the curve <code class="reqn">\zeta</code> on the response) and estimate the model.
</p>
<p>In this function, the MFPLSIM is fitted using the FASSMR algorithm.  The main idea of this algorithm is to consider a reduced model, with only some (very few) linear covariates (but covering the entire discretization interval of <code class="reqn">\zeta</code>), and discarding directly the other linear covariates (since it is expected that they contain very similar information about the response). 
</p>
<p>To explain the algorithm, we assume, without loss of generality, that the number <code class="reqn">p_n</code> of linear covariates can be expressed as follows: <code class="reqn">p_n=q_nw_n</code> with <code class="reqn">q_n</code> and <code class="reqn">w_n</code> integers. 
This consideration allows us to build a subset of the initial <code class="reqn">p_n</code> linear covariates, containging only <code class="reqn">w_n</code> equally spaced discretised observations of  <code class="reqn">\zeta</code> covering the entire interval  <code class="reqn">[a,b]</code>. This subset is the following:
</p>
<p style="text-align: center;"><code class="reqn">
	\mathcal{R}_n^{\mathbf{1}}=\left\{\zeta\left(t_k^{\mathbf{1}}\right),\ \ k=1,\dots,w_n\right\},
</code>
</p>
 
<p>where  <code class="reqn">t_k^{\mathbf{1}}=t_{\left[(2k-1)q_n/2\right]}</code> and  <code class="reqn">\left[z\right]</code> denotes the smallest integer not less than the real number <code class="reqn">z</code>.
</p>
<p>We consider the following reduced model, which involves only the linear covariates belonging to <code class="reqn">\mathcal{R}_n^{\mathbf{1}}</code>:
</p>
<p style="text-align: center;"><code class="reqn">
	Y_i=\sum_{k=1}^{w_n}\beta_{0k}^{\mathbf{1}}\zeta_i(t_k^{\mathbf{1}})+r^{\mathbf{1}}\left(\left&lt;\theta_0^{\mathbf{1}},\mathcal{X}_i\right&gt;\right)+\varepsilon_i^{\mathbf{1}}.
</code>
</p>

<p>The program receives the eligible numbers of linear covariates for building the reduced model through the argument <code>wn</code>.
Then, the penalised least-squares variable selection procedure, with kernel estimation, is applied to the reduced model. This is done using the function <code><a href="#topic+sfplsim.kernel.fit">sfplsim.kernel.fit</a></code>, which requires the remaining arguments (for details, see the documentation of the function <code><a href="#topic+sfplsim.kernel.fit">sfplsim.kernel.fit</a></code>). The estimates obtained are the outputs of the FASSMR algorithm. For further details on this algorithm, see Novo et al. (2021).
</p>
<p><b>Remark</b>: If the condition  <code class="reqn">p_n=w_n q_n</code> is not met (then <code class="reqn">p_n/w_n</code> is not an integer number), the function considers variable  <code class="reqn">q_n=q_{n,k}</code> values <code class="reqn">k=1,\dots,w_n</code>. Specifically:
</p>
<p style="text-align: center;"><code class="reqn">
	q_{n,k}= \left\{\begin{array}{ll}
	[p_n/w_n]+1 &amp;   k\in\{1,\dots,p_n-w_n[p_n/w_n]\},\\
	{[p_n/w_n]} &amp; k\in\{p_n-w_n[p_n/w_n]+1,\dots,w_n\},
	\end{array}
	\right.
</code>
</p>

<p>where <code class="reqn">[z]</code> denotes the integer part of the real number <code class="reqn">z</code>.
</p>
<p>The function supports parallel computation. To avoid it, we can set <code>n.core=1</code>.
</p>


<h3>Value</h3>

<table>
<tr><td><code>call</code></td>
<td>
<p>The matched call.</p>
</td></tr>
<tr><td><code>fitted.values</code></td>
<td>
<p>Estimated scalar response.</p>
</td></tr>
<tr><td><code>residuals</code></td>
<td>
<p>Differences between <code>y</code> and the <code>fitted.values</code>.</p>
</td></tr>
<tr><td><code>beta.est</code></td>
<td>
<p><code class="reqn">\hat{\mathbf{\beta}}</code> (i.e. estimate of <code class="reqn">\mathbf{\beta}_0</code> when the optimal tuning parameters <code>w.opt</code>, <code>lambda.opt</code>, <code>h.opt</code> and <code>vn.opt</code> are used).</p>
</td></tr>
<tr><td><code>beta.red</code></td>
<td>
<p>Estimate of <code class="reqn">\beta_0^{\mathbf{1}}</code> in the reduced model when the optimal tuning parameters <code>w.opt</code>, <code>lambda.opt</code>, <code>h.opt</code> and <code>vn.opt</code> are used.</p>
</td></tr>
<tr><td><code>theta.est</code></td>
<td>
<p>Coefficients of <code class="reqn">\hat{\theta}</code> in the B-spline basis (i.e. estimate of <code class="reqn">\theta_0</code> when the optimal tuning parameters <code>w.opt</code>, <code>lambda.opt</code>, <code>h.opt</code> and <code>vn.opt</code> are used): a vector of <code>length(order.Bspline+nknot.theta)</code>.</p>
</td></tr>
<tr><td><code>indexes.beta.nonnull</code></td>
<td>
<p>Indexes of the non-zero <code class="reqn">\hat{\beta_{j}}</code>.</p>
</td></tr>
<tr><td><code>h.opt</code></td>
<td>
<p>Selected bandwidth (when <code>w.opt</code> is considered).</p>
</td></tr>
<tr><td><code>w.opt</code></td>
<td>
<p>Selected size for  <code class="reqn">\mathcal{R}_n^{\mathbf{1}}</code>.</p>
</td></tr>
<tr><td><code>lambda.opt</code></td>
<td>
<p>Selected value for the penalisation parameter (when <code>w.opt</code> is considered).</p>
</td></tr>
<tr><td><code>IC</code></td>
<td>
<p>Value of the criterion function considered to select <code>w.opt</code>, <code>lambda.opt</code>, <code>h.opt</code> and <code>vn.opt</code>.</p>
</td></tr>
<tr><td><code>vn.opt</code></td>
<td>
<p>Selected value of <code>vn</code> (when <code>w.opt</code> is considered).</p>
</td></tr>
<tr><td><code>beta.w</code></td>
<td>
<p>Estimate of <code class="reqn">\beta_0^{\mathbf{1}}</code>  for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>theta.w</code></td>
<td>
<p>Estimate of <code class="reqn">\theta_0^{\mathbf{1}}</code> for each value of the sequence <code>wn</code> (i.e. its coefficients in the B-spline basis).</p>
</td></tr> 
<tr><td><code>IC.w</code></td>
<td>
<p>Value of the criterion function for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>indexes.beta.nonnull.w</code></td>
<td>
<p>Indexes of the non-zero linear coefficients for each value of the sequence <code>wn</code>.</p>
</td></tr> 
<tr><td><code>lambda.w</code></td>
<td>
<p>Selected value of penalisation parameter for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>h.w</code></td>
<td>
<p>Selected bandwidth for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>index01</code></td>
<td>
<p>Indexes of the covariates (in the entire set of <code class="reqn">p_n</code>) used to build  <code class="reqn">\mathcal{R}_n^{\mathbf{1}}</code> for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>...</code></td>
<td>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>German Aneiros Perez <a href="mailto:german.aneiros@udc.es">german.aneiros@udc.es</a> 
</p>
<p>Silvia Novo Diaz  <a href="mailto:snovo@est-econ.uc3m.es">snovo@est-econ.uc3m.es</a>
</p>


<h3>References</h3>

<p>Novo, S., Vieu, P., and Aneiros, G., (2021) Fast and efficient algorithms for
sparse semiparametric bi-functional regression. <em>Australian and New Zealand
Journal of Statistics</em>, <b>63</b>, 606&ndash;638, <a href="https://doi.org/10.1111/anzs.12355">doi:10.1111/anzs.12355</a>.
</p>


<h3>See Also</h3>

<p>See also <code><a href="#topic+sfplsim.kernel.fit">sfplsim.kernel.fit</a></code>, <code><a href="#topic+predict.FASSMR.kernel">predict.FASSMR.kernel</a></code>,  <code><a href="#topic+plot.FASSMR.kernel">plot.FASSMR.kernel</a></code> and <code><a href="#topic+IASSMR.kernel.fit">IASSMR.kernel.fit</a></code>.
</p>
<p>Alternative method <code><a href="#topic+FASSMR.kNN.fit">FASSMR.kNN.fit</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(Sugar)

y&lt;-Sugar$ash
x&lt;-Sugar$wave.290
z&lt;-Sugar$wave.240

#Outliers
index.y.25 &lt;- y &gt; 25
index.atip &lt;- index.y.25
(1:268)[index.atip]


#Dataset to model
x.sug &lt;- x[!index.atip,]
z.sug&lt;- z[!index.atip,]
y.sug &lt;- y[!index.atip]

train&lt;-1:216

ptm=proc.time()
fit &lt;- FASSMR.kernel.fit(x=x.sug[train,],z=z.sug[train,], y=y.sug[train], 
        nknot.theta=2, lambda.min.l=0.03,
        max.q.h=0.35, nknot=20,criterion="BIC", 
        max.iter=5000)
proc.time()-ptm

</code></pre>

<hr>
<h2 id='FASSMR.kNN.fit'>
Impact point selection with FASSMR and kNN estimation
</h2><span id='topic+FASSMR.kNN.fit'></span>

<h3>Description</h3>

<p>This function implements the Fast Algorithm for Sparse Semiparametric Multi-functional Regression (FASSMR) with kNN estimation. This algorithm is specifically designed for estimating multi-functional partial linear single-index models, which incorporate multiple scalar variables and a functional covariate as predictors. These scalar variables are derived from the discretisation of a curve and have linear effect while the functional covariate exhibits a single-index effect. 
</p>
<p>FASSMR selects the impact points of the discretised curve and estimates the model. The algorithm employs a penalised least-squares regularisation procedure, integrated with kNN estimation using Nadaraya-Watson weights. It uses B-spline expansions to represent curves and eligible functional indexes. Additionally, it utilises an objective criterion (<code>criterion</code>) to determine the initial number of covariates in the reduced model (<code>w.opt</code>), the number of neighbours (<code>k.opt</code>), and the penalisation parameter (<code>lambda.opt</code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>FASSMR.kNN.fit(x, z, y, seed.coeff = c(-1, 0, 1), order.Bspline = 3, 
nknot.theta = 3,  knearest = NULL, min.knn = 2, max.knn = NULL, step = NULL,  
kind.of.kernel = "quad",range.grid = NULL, nknot = NULL, lambda.min = NULL, 
lambda.min.h = NULL, lambda.min.l = NULL, factor.pn = 1, nlambda = 100, 
vn = ncol(z), nfolds = 10, seed = 123, wn = c(10, 15, 20), criterion = "GCV", 
penalty = "grSCAD", max.iter = 1000, n.core = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="FASSMR.kNN.fit_+3A_x">x</code></td>
<td>

<p>Matrix containing the observations of the functional covariate collected by row (functional single-index component).
</p>
</td></tr>
<tr><td><code id="FASSMR.kNN.fit_+3A_z">z</code></td>
<td>

<p>Matrix containing the observations of the functional covariate that is discretised collected by row (linear component).
</p>
</td></tr>
<tr><td><code id="FASSMR.kNN.fit_+3A_y">y</code></td>
<td>

<p>Vector containing the scalar response.
</p>
</td></tr>
<tr><td><code id="FASSMR.kNN.fit_+3A_seed.coeff">seed.coeff</code></td>
<td>

<p>Vector of initial values used to  build the set <code class="reqn">\Theta_n</code> (see section <code>Details</code>). The coefficients for the B-spline representation of each eligible functional index <code class="reqn">\theta \in \Theta_n</code> are obtained from <code>seed.coeff</code>.  The default is <code>c(-1,0,1)</code>.
</p>
</td></tr>
<tr><td><code id="FASSMR.kNN.fit_+3A_order.bspline">order.Bspline</code></td>
<td>

<p>Positive integer giving the order of the B-spline basis functions. This is the number of coefficients in each piecewise polynomial segment. The default is 3.
</p>
</td></tr>
<tr><td><code id="FASSMR.kNN.fit_+3A_nknot.theta">nknot.theta</code></td>
<td>

<p>Positive integer indicating the number of regularly spaced interior knots in the B-spline expansion of <code class="reqn">\theta_0</code>. The default is 3.
</p>
</td></tr>
<tr><td><code id="FASSMR.kNN.fit_+3A_knearest">knearest</code></td>
<td>

<p>Vector of positive integers containing the sequence in which the  number of nearest neighbours <code>k.opt</code> is selected. If <code>knearest=NULL</code>, then <code>knearest &lt;- seq(from =min.knn, to = max.knn, by = step)</code>.
</p>
</td></tr>
<tr><td><code id="FASSMR.kNN.fit_+3A_min.knn">min.knn</code></td>
<td>

<p>A positive integer that represents the minimum value in the sequence for selecting the number of nearest neighbours <code>k.opt</code>. This value should be less than the sample size. The default is 2.
</p>
</td></tr>
<tr><td><code id="FASSMR.kNN.fit_+3A_max.knn">max.knn</code></td>
<td>

<p>A positive integer that represents the maximum value in the sequence for selecting number of nearest neighbours <code>k.opt</code>. This value should be less than the sample size. The default is <code>max.knn &lt;- n%/%5</code>.
</p>
</td></tr>
<tr><td><code id="FASSMR.kNN.fit_+3A_step">step</code></td>
<td>

<p>A positive integer used to construct the sequence of k-nearest neighbours as follows: <code>min.knn, min.knn + step, min.knn + 2*step, min.knn + 3*step,...</code>. The default value for <code>step</code> is <code>step&lt;-ceiling(n/100)</code>.
</p>
</td></tr>
<tr><td><code id="FASSMR.kNN.fit_+3A_kind.of.kernel">kind.of.kernel</code></td>
<td>

<p>The type of kernel function used. Currently, only Epanechnikov kernel (<code>"quad"</code>) is available.
</p>
</td></tr>
<tr><td><code id="FASSMR.kNN.fit_+3A_range.grid">range.grid</code></td>
<td>

<p>Vector of length 2 containing the endpoints of the grid at which the observations of the functional covariate <code>x</code> are evaluated (i.e. the range of the discretisation). If <code>range.grid=NULL</code>, then <code>range.grid=c(1,p)</code> is considered, where <code>p</code> is the discretisation size of <code>x</code> (i.e. <code>ncol(x))</code>.
</p>
</td></tr>
<tr><td><code id="FASSMR.kNN.fit_+3A_nknot">nknot</code></td>
<td>

<p>Positive integer indicating the number of interior knots for the B-spline expansion of the functional covariate. The default value is <code>(p - order.Bspline - 1)%/%2</code>.
</p>
</td></tr>
<tr><td><code id="FASSMR.kNN.fit_+3A_lambda.min">lambda.min</code></td>
<td>

<p>The smallest value for lambda (i. e., the lower endpoint  of the sequence in which <code>lambda.opt</code> is selected), as fraction of <code>lambda.max</code>.
The defaults is <code>lambda.min.l</code> if the sample size is larger than <code>factor.pn</code> times the number of linear covariates and <code>lambda.min.h</code> otherwise.
</p>
</td></tr>
<tr><td><code id="FASSMR.kNN.fit_+3A_lambda.min.h">lambda.min.h</code></td>
<td>

<p>The lower endpoint of the sequence in which <code>lambda.opt</code> is selected if the sample size is smaller than <code>factor.pn</code> times the number of linear covariates. The default is 0.05. 
</p>
</td></tr>
<tr><td><code id="FASSMR.kNN.fit_+3A_lambda.min.l">lambda.min.l</code></td>
<td>

<p>The lower endpoint of the sequence in which <code>lambda.opt</code> is selected if the sample size is larger than <code>factor.pn</code> times the number of linear covariates. The default is 0.0001.
</p>
</td></tr>
<tr><td><code id="FASSMR.kNN.fit_+3A_factor.pn">factor.pn</code></td>
<td>

<p>Positive integer used to set <code>lambda.min</code>. The default value is 1.
</p>
</td></tr>
<tr><td><code id="FASSMR.kNN.fit_+3A_nlambda">nlambda</code></td>
<td>

<p>Positive integer indicating the number of values in the sequence from which <code>lambda.opt</code> is selected. The default is 100.
</p>
</td></tr>
<tr><td><code id="FASSMR.kNN.fit_+3A_vn">vn</code></td>
<td>

<p>Positive integer or vector of positive integers indicating the number of groups of consecutive variables to be penalised together. The default value is <code>vn=ncol(z)</code>, resulting in the individual penalization of each scalar covariate.
</p>
</td></tr>
<tr><td><code id="FASSMR.kNN.fit_+3A_nfolds">nfolds</code></td>
<td>

<p>Positive integer indicating the number of cross-validation folds (used when <code>criterion="k-fold-CV"</code>). Default is 10.
</p>
</td></tr>
<tr><td><code id="FASSMR.kNN.fit_+3A_seed">seed</code></td>
<td>

<p>You may set the seed for the random number generator to ensure reproducible results (applicable when <code>criterion="k-fold-CV"</code> is used). The default seed value is 123.
</p>
</td></tr>
<tr><td><code id="FASSMR.kNN.fit_+3A_wn">wn</code></td>
<td>

<p>A vector of positive integers indicating the eligible number of covariates in the reduced model. For more information, refer to the section <code>Details</code>. The default is <code>c(10,15,20)</code>.
</p>
</td></tr>
<tr><td><code id="FASSMR.kNN.fit_+3A_criterion">criterion</code></td>
<td>

<p>The criterion used to select the tuning and regularisation parameters: <code>wn.opt</code>, <code>k.opt</code> and <code>lambda.opt</code> (also <code>vn.opt</code> if needed). Options include <code>"GCV"</code>, <code>"BIC"</code>, <code>"AIC"</code>, or <code>"k-fold-CV"</code>. The default setting is <code>"GCV"</code>.
</p>
</td></tr>
<tr><td><code id="FASSMR.kNN.fit_+3A_penalty">penalty</code></td>
<td>

<p>The penalty function applied in the penalised least-squares procedure. Currently, only &quot;grLasso&quot; and &quot;grSCAD&quot; are implemented. The default is &quot;grSCAD&quot;.
</p>
</td></tr>
<tr><td><code id="FASSMR.kNN.fit_+3A_max.iter">max.iter</code></td>
<td>

<p>Maximum number of iterations allowed across the entire path. The default value is 1000.
</p>
</td></tr>
<tr><td><code id="FASSMR.kNN.fit_+3A_n.core">n.core</code></td>
<td>

<p>Number of CPU cores designated for parallel execution. The default is <code>n.core&lt;-availableCores(omit=1)</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The multi-functional partial linear single-index model (MFPLSIM) is given by the expression
</p>
<p style="text-align: center;"><code class="reqn">Y_i=\sum_{j=1}^{p_n}\beta_{0j}\zeta_i(t_j)+r\left(\left&lt;\theta_0,X_i\right&gt;\right)+\varepsilon_i,\ \ \ (i=1,\dots,n),</code>
</p>

<p>where: 
</p>

<ul>
<li> <p><code class="reqn">Y_i</code> is a real random response and <code class="reqn">X_i</code> denotes a random element belonging to some separable Hilbert space <code class="reqn">\mathcal{H}</code> with inner product denoted by <code class="reqn">\left\langle\cdot,\cdot\right\rangle</code>. The second functional predictor <code class="reqn">\zeta_i</code> is assumed to be a curve defined on some interval <code class="reqn">[a,b]</code> which  is observed at the points <code class="reqn">a\leq t_1&lt;\dots&lt;t_{p_n}\leq b</code>. 
</p>
</li>
<li>  <p><code class="reqn">\mathbf{\beta}_0=(\beta_{01},\dots,\beta_{0p_n})^{\top}</code> is a vector of unknown real coefficients and <code class="reqn">r(\cdot)</code> denotes a smooth unknown link function. In addition, <code class="reqn">\theta_0</code> is an unknown functional direction in <code class="reqn">\mathcal{H}</code>.  
</p>
</li>
<li> <p><code class="reqn">\varepsilon_i</code> denotes the random error.
</p>
</li></ul>

<p>In  the MFPLSIM, we assume that only a few scalar variables from the set <code class="reqn">\{\zeta(t_1),\dots,\zeta(t_{p_n})\}</code> form part of the model. Therefore, we must select the relevant variables in the linear component (the impact points of the curve <code class="reqn">\zeta</code> on the response) and estimate the model.
</p>
<p>In this function, the MFPLSIM is fitted using the FASSMR algorithm.  The main idea of this algorithm is to consider a reduced model, with only some (very few) linear covariates (but covering the entire discretization interval of <code class="reqn">\zeta</code>), and discarding directly the other linear covariates (since it is expected that they contain very similar information about the response). 
</p>
<p>To explain the algorithm, we assume, without loss of generality, that the number <code class="reqn">p_n</code> of linear covariates can be expressed as follows: <code class="reqn">p_n=q_nw_n</code> with <code class="reqn">q_n</code> and <code class="reqn">w_n</code> integers. 
This consideration allows us to build a subset of the initial <code class="reqn">p_n</code> linear covariates, containging only <code class="reqn">w_n</code> equally spaced discretised observations of  <code class="reqn">\zeta</code> covering the entire interval  <code class="reqn">[a,b]</code>. This subset is the following:
</p>
<p style="text-align: center;"><code class="reqn">
	\mathcal{R}_n^{\mathbf{1}}=\left\{\zeta\left(t_k^{\mathbf{1}}\right),\ \ k=1,\dots,w_n\right\},
</code>
</p>
 
<p>where  <code class="reqn">t_k^{\mathbf{1}}=t_{\left[(2k-1)q_n/2\right]}</code> and  <code class="reqn">\left[z\right]</code> denotes the smallest integer not less than the real number <code class="reqn">z</code>.
</p>
<p>We consider the following reduced model, which involves only the linear covariates belonging to <code class="reqn">\mathcal{R}_n^{\mathbf{1}}</code>:
</p>
<p style="text-align: center;"><code class="reqn">
	Y_i=\sum_{k=1}^{w_n}\beta_{0k}^{\mathbf{1}}\zeta_i(t_k^{\mathbf{1}})+r^{\mathbf{1}}\left(\left&lt;\theta_0^{\mathbf{1}},\mathcal{X}_i\right&gt;\right)+\varepsilon_i^{\mathbf{1}}.
</code>
</p>

<p>The program receives the eligible numbers of linear covariates for building the reduced model through the argument <code>wn</code>.
Then, the penalised least-squares variable selection procedure, with kNN estimation, is applied to the reduced model. This is done using the function <code><a href="#topic+sfplsim.kNN.fit">sfplsim.kNN.fit</a></code>, which requires the remaining arguments (for details, see the documentation of the function <code><a href="#topic+sfplsim.kNN.fit">sfplsim.kNN.fit</a></code>). The estimates obtained are the outputs of the FASSMR algorithm. For further details on this algorithm, see Novo et al. (2021).
</p>
<p><b>Remark</b>: If the condition  <code class="reqn">p_n=w_n q_n</code> is not met (then <code class="reqn">p_n/w_n</code> is not an integer number), the function considers variable  <code class="reqn">q_n=q_{n,k}</code> values <code class="reqn">k=1,\dots,w_n</code>. Specifically:
</p>
<p style="text-align: center;"><code class="reqn">
	q_{n,k}= \left\{\begin{array}{ll}
	[p_n/w_n]+1 &amp;   k\in\{1,\dots,p_n-w_n[p_n/w_n]\},\\
	{[p_n/w_n]} &amp; k\in\{p_n-w_n[p_n/w_n]+1,\dots,w_n\},
	\end{array}
	\right.
</code>
</p>

<p>where <code class="reqn">[z]</code> denotes the integer part of the real number <code class="reqn">z</code>.
</p>
<p>The function supports parallel computation. To avoid it, we can set <code>n.core=1</code>.
</p>


<h3>Value</h3>

<table>
<tr><td><code>call</code></td>
<td>
<p>The matched call.</p>
</td></tr>
<tr><td><code>fitted.values</code></td>
<td>
<p>Estimated scalar response.</p>
</td></tr>
<tr><td><code>residuals</code></td>
<td>
<p>Differences between <code>y</code> and the <code>fitted.values</code>.</p>
</td></tr>
<tr><td><code>beta.est</code></td>
<td>
<p><code class="reqn">\hat{\mathbf{\beta}}</code> (i.e. estimate of <code class="reqn">\mathbf{\beta}_0</code> when the optimal tuning parameters <code>w.opt</code>, <code>lambda.opt</code>, <code>k.opt</code> and <code>vn.opt</code> are used).</p>
</td></tr>
<tr><td><code>beta.red</code></td>
<td>
<p>Estimate of <code class="reqn">\beta_0^{\mathbf{1}}</code> in the reduced model when the optimal tuning parameters <code>w.opt</code>, <code>lambda.opt</code>, <code>k.opt</code> and <code>vn.opt</code> are used.</p>
</td></tr>
<tr><td><code>theta.est</code></td>
<td>
<p>Coefficients of <code class="reqn">\hat{\theta}</code> in the B-spline basis (i.e. estimate of <code class="reqn">\theta_0</code> when the optimal tuning parameters <code>w.opt</code>, <code>lambda.opt</code>, <code>k.opt</code> and <code>vn.opt</code> are used): a vector of <code>length(order.Bspline+nknot.theta)</code>.</p>
</td></tr>
<tr><td><code>indexes.beta.nonnull</code></td>
<td>
<p>Indexes of the non-zero <code class="reqn">\hat{\beta_{j}}</code>.</p>
</td></tr>
<tr><td><code>k.opt</code></td>
<td>
<p>Selected number of nearest neighbours (when <code>w.opt</code> is considered).</p>
</td></tr>
<tr><td><code>w.opt</code></td>
<td>
<p>Selected size for  <code class="reqn">\mathcal{R}_n^{\mathbf{1}}</code>.</p>
</td></tr>
<tr><td><code>lambda.opt</code></td>
<td>
<p>Selected value for the penalisation parameter (when <code>w.opt</code> is considered).</p>
</td></tr>
<tr><td><code>IC</code></td>
<td>
<p>Value of the criterion function considered to select <code>w.opt</code>, <code>lambda.opt</code>, <code>k.opt</code> and <code>vn.opt</code>.</p>
</td></tr>
<tr><td><code>vn.opt</code></td>
<td>
<p>Selected value of <code>vn</code> (when <code>w.opt</code> is considered).</p>
</td></tr>
<tr><td><code>beta.w</code></td>
<td>
<p>Estimate of <code class="reqn">\beta_0^{\mathbf{1}}</code> for each value of the sequence <code>wn</code> (i.e. for each number of covariates in the reduced model).</p>
</td></tr>
<tr><td><code>theta.w</code></td>
<td>
<p>Estimate of <code class="reqn">\theta_0^{\mathbf{1}}</code> for each value of the sequence <code>wn</code> (i.e. its coefficients in the B-spline basis).</p>
</td></tr> 
<tr><td><code>IC.w</code></td>
<td>
<p>Value of the criterion function for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>indexes.beta.nonnull.w</code></td>
<td>
<p>Indexes of the non-zero linear coefficients for each value of the sequence <code>wn</code>.</p>
</td></tr> 
<tr><td><code>lambda.w</code></td>
<td>
<p>Selected value of penalisation parameter for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>k.w</code></td>
<td>
<p>Selected number of neighbours for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>index01</code></td>
<td>
<p>Indexes of the covariates (in the entire set of <code class="reqn">p_n</code>) used to build  <code class="reqn">\mathcal{R}_n^{\mathbf{1}}</code> for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>...</code></td>
<td>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>German Aneiros Perez <a href="mailto:german.aneiros@udc.es">german.aneiros@udc.es</a> 
</p>
<p>Silvia Novo Diaz  <a href="mailto:snovo@est-econ.uc3m.es">snovo@est-econ.uc3m.es</a>
</p>


<h3>References</h3>

<p>Novo, S., Vieu, P., and Aneiros, G., (2021) Fast and efficient algorithms for
sparse semiparametric bi-functional regression. <em>Australian and New Zealand
Journal of Statistics</em>, <b>63</b>, 606&ndash;638, <a href="https://doi.org/10.1111/anzs.12355">doi:10.1111/anzs.12355</a>.
</p>


<h3>See Also</h3>

<p>See also <code><a href="#topic+sfplsim.kNN.fit">sfplsim.kNN.fit</a>, <a href="#topic+predict.FASSMR.kNN">predict.FASSMR.kNN</a></code>, <code><a href="#topic+plot.FASSMR.kNN">plot.FASSMR.kNN</a></code> and <code><a href="#topic+IASSMR.kNN.fit">IASSMR.kNN.fit</a></code>.
</p>
<p>Alternative method <code><a href="#topic+FASSMR.kernel.fit">FASSMR.kernel.fit</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(Sugar)


y&lt;-Sugar$ash
x&lt;-Sugar$wave.290
z&lt;-Sugar$wave.240

#Outliers
index.y.25 &lt;- y &gt; 25
index.atip &lt;- index.y.25
(1:268)[index.atip]


#Dataset to model
x.sug &lt;- x[!index.atip,]
z.sug&lt;- z[!index.atip,]
y.sug &lt;- y[!index.atip]

train&lt;-1:216
ptm=proc.time()
fit&lt;- FASSMR.kNN.fit(x=x.sug[train,],z=z.sug[train,], y=y.sug[train], 
        nknot.theta=2, lambda.min.l=0.03, max.knn=20,nknot=20,criterion="BIC",
        max.iter=5000)
proc.time()-ptm

fit
names(fit)

  
</code></pre>

<hr>
<h2 id='fsemipar.internal'>
Package fsemipar internal functions
</h2><span id='topic+fsemipar.internal'></span>

<h3>Description</h3>

<p>The package includes the following internal functions, based on the code by F. Ferraty, which is available on his website at <a href="https://www.math.univ-toulouse.fr/~ferraty/SOFTWARES/NPFDA/index.html">https://www.math.univ-toulouse.fr/~ferraty/SOFTWARES/NPFDA/index.html</a>. 
</p>


<h3>Details</h3>


<ul>
<li> <p><code>approx.spline.deriv</code>
</p>
</li>
<li><p><code>Bspline.ini</code>
</p>
</li>
<li><p><code>fnp.kernel.fit</code>
</p>
</li>
<li><p><code>fnp.kernel.fit.test</code>
</p>
</li>
<li><p><code>fnp.kernel.test</code>
</p>
</li>
<li><p><code>fnp.kNN.fit</code>
</p>
</li>
<li><p><code>fnp.kNN.fit.test</code>
</p>
</li>
<li><p><code>fnp.kNN.fit.test.loc</code>
</p>
</li>
<li><p><code>fnp.kNN.GCV</code>
</p>
</li>
<li><p><code>fnp.kNN.test</code>
</p>
</li>
<li><p><code>fsim.kernel.fit.fixedtheta</code>
</p>
</li>
<li><p><code>fsim.kNN.fit.fixedtheta</code>
</p>
</li>
<li><p><code>fun.kernel</code>
</p>
</li>
<li><p><code>fun.kernel.fixedtheta</code>
</p>
</li>
<li><p><code>fun.kNN</code>
</p>
</li>
<li><p><code>fun.kNN.fixedtheta</code>
</p>
</li>
<li><p><code>funopare.kNN</code>
</p>
</li>
<li><p><code>H.fnp.kernel</code>
</p>
</li>
<li><p><code>H.fnp.kNN</code>
</p>
</li>
<li><p><code>H.fsim.kernel</code>
</p>
</li>
<li><p><code>H.fsim.kNN</code>
</p>
</li>
<li><p><code>interp.spline.deriv</code>
</p>
</li>
<li><p><code>quad</code>
</p>
</li>
<li><p><code>semimetric.deriv</code>
</p>
</li>
<li><p><code>semimetric.interv</code>
</p>
</li>
<li><p><code>semimetric.pca</code>
</p>
</li>
<li><p><code>sfplsim.kernel.fit.fixedtheta</code>
</p>
</li>
<li><p><code>sfplsim.kNN.fit.fixedtheta</code>
</p>
</li>
<li><p><code>Splinemlf</code>
</p>
</li>
<li><p><code>symsolve</code>
</p>
</li></ul>


<hr>
<h2 id='fsim.kernel.fit'>
Functional single-index model fit using kernel estimation and joint LOOCV minimisation
</h2><span id='topic+fsim.kernel.fit'></span>

<h3>Description</h3>

<p>This function fits a functional single-index model (FSIM) between a functional covariate and a
scalar response. 
It employs kernel estimation with Nadaraya-Watson weights and uses B-spline expansions to represent curves and eligible functional indexes. 
</p>
<p>The function also utilises the leave-one-out cross-validation (LOOCV) criterion to select the bandwidth (<code>h.opt</code>) and the coefficients of the functional index in the spline basis (<code>theta.est</code>). It performs a joint minimisation of the LOOCV objective function in both the bandwidth and the functional index.</p>


<h3>Usage</h3>

<pre><code class='language-R'>fsim.kernel.fit(x, y, seed.coeff = c(-1, 0, 1), order.Bspline = 3, 
nknot.theta = 3,  min.q.h = 0.05, max.q.h = 0.5, h.seq = NULL, num.h = 10, 
kind.of.kernel = "quad", range.grid = NULL, nknot = NULL, n.core = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fsim.kernel.fit_+3A_x">x</code></td>
<td>
<p>Matrix containing the observations of the functional covariate (i.e. curves) collected by row.</p>
</td></tr>
<tr><td><code id="fsim.kernel.fit_+3A_y">y</code></td>
<td>
<p>Vector containing the scalar response.</p>
</td></tr>
<tr><td><code id="fsim.kernel.fit_+3A_seed.coeff">seed.coeff</code></td>
<td>

<p>Vector of initial values used to  build the set <code class="reqn">\Theta_n</code> (see section <code>Details</code>). The coefficients for the B-spline representation of each eligible functional index <code class="reqn">\theta \in \Theta_n</code> are obtained from <code>seed.coeff</code>.  The default is <code>c(-1,0,1)</code>.
</p>
</td></tr>
<tr><td><code id="fsim.kernel.fit_+3A_order.bspline">order.Bspline</code></td>
<td>

<p>Positive integer giving the order of the B-spline basis functions. This is the number of coefficients in each piecewise polynomial segment. The default is 3</p>
</td></tr>
<tr><td><code id="fsim.kernel.fit_+3A_nknot.theta">nknot.theta</code></td>
<td>
<p>Positive integer indicating the number of regularly spaced interior knots in the B-spline expansion of <code class="reqn">\theta_0</code>. The default is 3. 
</p>
</td></tr>
<tr><td><code id="fsim.kernel.fit_+3A_min.q.h">min.q.h</code></td>
<td>
<p>Minimum quantile order of the distances between curves, which are computed using the projection semi-metric. This value determines the lower endpoint of the range from which the bandwidth is selected. The default is 0.05.
</p>
</td></tr>
<tr><td><code id="fsim.kernel.fit_+3A_max.q.h">max.q.h</code></td>
<td>

<p>Maximum quantile order of the distances between curves, which are computed using the projection semi-metric. This value determines the upper endpoint of the range from which the bandwidth is selected. The default is 0.5.
</p>
</td></tr>
<tr><td><code id="fsim.kernel.fit_+3A_h.seq">h.seq</code></td>
<td>
<p>Vector containing the sequence of bandwidths. The default is a sequence of <code>num.h</code> equispaced bandwidths in the range constructed using <code>min.q.h</code> and <code>max.q.h</code>.
</p>
</td></tr>
<tr><td><code id="fsim.kernel.fit_+3A_num.h">num.h</code></td>
<td>
<p>Positive integer indicating the number of bandwidths in the grid. The default is 10.
</p>
</td></tr>
<tr><td><code id="fsim.kernel.fit_+3A_kind.of.kernel">kind.of.kernel</code></td>
<td>

<p>The type of kernel function used. Currently, only Epanechnikov kernel (<code>"quad"</code>) is available.
</p>
</td></tr>
<tr><td><code id="fsim.kernel.fit_+3A_range.grid">range.grid</code></td>
<td>

<p>Vector of length 2 containing the endpoints of the grid at which the observations of the functional covariate <code>x</code> are evaluated (i.e. the range of the discretisation). If <code>range.grid=NULL</code>, then <code>range.grid=c(1,p)</code> is considered, where <code>p</code> is the discretisation size of <code>x</code> (i.e. <code>ncol(x))</code>.
</p>
</td></tr>
<tr><td><code id="fsim.kernel.fit_+3A_nknot">nknot</code></td>
<td>

<p>Positive integer indicating the number of interior knots for the B-spline expansion of the functional covariate. The default value is <code>(p - order.Bspline - 1)%/%2</code>.
</p>
</td></tr>
<tr><td><code id="fsim.kernel.fit_+3A_n.core">n.core</code></td>
<td>

<p>Number of CPU cores designated for parallel execution.The default is <code>n.core&lt;-availableCores(omit=1)</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The functional single-index model (FSIM) is given by the expression:
</p>
<p style="text-align: center;"><code class="reqn">Y_i=r(\langle\theta_0,X_i\rangle)+\varepsilon_i, \quad i=1,\dots,n,</code>
</p>

<p>where <code class="reqn">Y_i</code> denotes a scalar response, 
<code class="reqn">X_i</code> is a functional covariate valued in a separable Hilbert space <code class="reqn">\mathcal{H}</code> with an inner product <code class="reqn">\langle \cdot, \cdot\rangle</code>. The term <code class="reqn">\varepsilon</code> denotes the random error, <code class="reqn">\theta_0 \in \mathcal{H}</code> is the unknown functional index and <code class="reqn">r(\cdot)</code> denotes the unknown smooth link function.
</p>
<p>The FSIM is fitted using the kernel estimator
</p>
<p style="text-align: center;"><code class="reqn">
\widehat{r}_{h,\hat{\theta}}(x)=\sum_{i=1}^nw_{n,h,\hat{\theta}}(x,X_i)Y_i,  \quad   \forall x\in\mathcal{H},
</code>
</p>

<p>with Nadaraya-Watson weights
</p>
<p style="text-align: center;"><code class="reqn">
w_{n,h,\hat{\theta}}(x,X_i)=\frac{K\left(h^{-1}d_{\hat{\theta}}\left(X_i,x\right)\right)}{\sum_{i=1}^nK\left(h^{-1}d_{\hat{\theta}}\left(X_i,x\right)\right)},
</code>
</p>

<p>where
</p>

<ul>
<li><p> the real positive number <code class="reqn">h</code> is the bandwidth.
</p>
</li>
<li> <p><code class="reqn">K</code> is a kernel function (see the argument <code>kind.of.kernel</code>).
</p>
</li>
<li> <p><code class="reqn">d_{\hat{\theta}}(x_1,x_2)=|\langle\hat{\theta},x_1-x_2\rangle|</code> is the projection semi-metric, and <code class="reqn">\hat{\theta}</code> is an estimate of <code class="reqn">\theta_0</code>. 
</p>
</li></ul>

<p>The procedure requires the estimation of the function-parameter <code class="reqn">\theta_0</code>. Therefore, we use B-spline expansions to represent curves (dimension <code>nknot+order.Bspline</code>) and eligible functional indexes (dimension <code>nknot.theta+order.Bspline</code>). Then, we build a set <code class="reqn">\Theta_n</code> of eligible functional indexes by calibrating (to ensure the identifiability of the model) the set of initial coefficients given in <code>seed.coeff</code>. The larger this set is, the greater the size of <code class="reqn">\Theta_n</code>. Since our approach requires intensive computation, a trade-off between the size of <code class="reqn">\Theta_n</code> and the performance of the estimator is necessary. For that, Ait-Saidi et al. (2008) suggested considering <code>order.Bspline=3</code> and <code>seed.coeff=c(-1,0,1)</code>. For details on the construction of <code class="reqn">\Theta_n</code>, see Novo et al. (2019).
</p>
<p>We obtain the estimated coefficients of <code class="reqn">\theta_0</code> in the spline basis (<code>theta.est</code>) and the selected bandwidth (<code>h.opt</code>) by minimising the LOOCV criterion. This function performs a joint minimisation in both parameters, the bandwidth and the functional index, and supports parallel computation. To avoid parallel computation, we can set <code>n.core=1</code>.
</p>


<h3>Value</h3>

<table>
<tr><td><code>call</code></td>
<td>
<p>The matched call.</p>
</td></tr>
<tr><td><code>fitted.values</code></td>
<td>
<p>Estimated scalar response.</p>
</td></tr>
<tr><td><code>residuals</code></td>
<td>
<p>Differences between <code>y</code> and the <code>fitted.values</code>.</p>
</td></tr>
<tr><td><code>theta.est</code></td>
<td>
<p>Coefficients of <code class="reqn">\hat{\theta}</code> in the B-spline basis: a vector of <code>length(order.Bspline+nknot.theta)</code>.</p>
</td></tr>
<tr><td><code>h.opt</code></td>
<td>
<p>Selected bandwidth.</p>
</td></tr>
<tr><td><code>r.squared</code></td>
<td>
<p>Coefficient of determination.</p>
</td></tr> 
<tr><td><code>var.res</code></td>
<td>
<p>Redidual variance.</p>
</td></tr>
<tr><td><code>df</code></td>
<td>
<p>Residual degrees of freedom.</p>
</td></tr>
<tr><td><code>yhat.cv</code></td>
<td>
<p>Predicted values for the scalar response using leave-one-out samples.</p>
</td></tr>
<tr><td><code>CV.opt</code></td>
<td>
<p>Minimum value of the CV function, i.e. the value of CV for <code>theta.est</code> and <code>h.opt</code>.</p>
</td></tr>
<tr><td><code>CV.values</code></td>
<td>
<p>Vector containing CV values for each functional index in <code class="reqn">\Theta_n</code> and the value of <code class="reqn">h</code> that minimises the CV for such index (i.e. <code>CV.values[j]</code> contains the value of the CV function corresponding to <code>theta.seq.norm[j,]</code> and the best value of the <code>h.seq</code> for this functional index according to the CV criterion).</p>
</td></tr>
<tr><td><code>H</code></td>
<td>
<p>Hat matrix.</p>
</td></tr>
<tr><td><code>m.opt</code></td>
<td>
<p>Index of <code class="reqn">\hat{\theta}</code> in the set <code class="reqn">\Theta_n</code>.</p>
</td></tr>
<tr><td><code>theta.seq.norm</code></td>
<td>
<p>The vector <code>theta.seq.norm[j,]</code> contains the coefficientes in the B-spline basis of the jth functional index in <code class="reqn">\Theta_n</code>.</p>
</td></tr>
<tr><td><code>h.seq</code></td>
<td>
<p>Sequence of eligible values for <code class="reqn">h</code>.</p>
</td></tr>
<tr><td><code>...</code></td>
<td>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>German Aneiros Perez <a href="mailto:german.aneiros@udc.es">german.aneiros@udc.es</a> 
</p>
<p>Silvia Novo Diaz  <a href="mailto:snovo@est-econ.uc3m.es">snovo@est-econ.uc3m.es</a>
</p>


<h3>References</h3>

<p>Ait-Saidi, A., Ferraty, F., Kassa, R., and Vieu, P. (2008) Cross-validated estimations in the single-functional index model. <em>Statistics</em>, <b>42(6)</b>, 475&ndash;494, <a href="https://doi.org/10.1080/02331880801980377">doi:10.1080/02331880801980377</a>.
</p>
<p>Novo S., Aneiros, G., and Vieu, P., (2019) Automatic and location-adaptive estimation in functional single&ndash;index regression. <em>Journal of Nonparametric Statistics</em>, <b>31(2)</b>, 364&ndash;392, <a href="https://doi.org/10.1080/10485252.2019.1567726">doi:10.1080/10485252.2019.1567726</a>.
</p>


<h3>See Also</h3>

<p>See also <code><a href="#topic+fsim.kernel.test">fsim.kernel.test</a></code>, <code><a href="#topic+predict.fsim.kernel">predict.fsim.kernel</a></code>, <code><a href="#topic+plot.fsim.kernel">plot.fsim.kernel</a></code>.
</p>
<p>Alternative procedure <code><a href="#topic+fsim.kNN.fit">fsim.kNN.fit</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(Tecator)
y&lt;-Tecator$fat
X&lt;-Tecator$absor.spectra2

#FSIM fit.
ptm&lt;-proc.time()
fit&lt;-fsim.kernel.fit(y[1:160],x=X[1:160,],max.q.h=0.35, nknot=20,
range.grid=c(850,1050),nknot.theta=4)
proc.time()-ptm
fit
names(fit)

</code></pre>

<hr>
<h2 id='fsim.kernel.fit.optim'>
Functional single-index model fit using kernel estimation and iterative LOOCV minimisation
</h2><span id='topic+fsim.kernel.fit.optim'></span>

<h3>Description</h3>

<p>This function fits a functional single-index model (FSIM) between a functional covariate and a
scalar response. 
It employs kernel estimation with Nadaraya-Watson weights and uses B-spline expansions to represent curves and eligible functional indexes. 
</p>
<p>The function also utilises the leave-one-out cross-validation (LOOCV) criterion to select the bandwidth (<code>h.opt</code>) and the coefficients of the functional index in the spline basis (<code>theta.est</code>). It performs an iterative minimisation of the LOOCV objective function, starting from an initial set of coefficients (<code>gamma</code>) for the functional index.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fsim.kernel.fit.optim(x, y, nknot.theta = 3, order.Bspline = 3, gamma = NULL, 
min.q.h = 0.05, max.q.h = 0.5, h.seq = NULL, num.h = 10,
kind.of.kernel = "quad", range.grid = NULL, nknot = NULL, threshold = 0.005)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fsim.kernel.fit.optim_+3A_x">x</code></td>
<td>
<p>Matrix containing the observations of the functional covariate (i.e. curves) collected by row.</p>
</td></tr>
<tr><td><code id="fsim.kernel.fit.optim_+3A_y">y</code></td>
<td>
<p>Vector containing the scalar response.</p>
</td></tr>
<tr><td><code id="fsim.kernel.fit.optim_+3A_order.bspline">order.Bspline</code></td>
<td>

<p>Positive integer giving the order of the B-spline basis functions. This is the number of coefficients in each piecewise polynomial segment. The default is 3</p>
</td></tr>
<tr><td><code id="fsim.kernel.fit.optim_+3A_nknot.theta">nknot.theta</code></td>
<td>
<p>Positive integer indicating the number of regularly spaced interior knots in the B-spline expansion of <code class="reqn">\theta_0</code>. The default is 3. 
</p>
</td></tr>
<tr><td><code id="fsim.kernel.fit.optim_+3A_gamma">gamma</code></td>
<td>

<p>Vector indicating the initial coefficients for the functional index used in the iterative procedure. By default, it is a vector of ones. The size of the vector is determined by the sum <code>nknot.theta+order.Bspline</code>.
</p>
</td></tr>
<tr><td><code id="fsim.kernel.fit.optim_+3A_min.q.h">min.q.h</code></td>
<td>
<p>Minimum quantile order of the distances between curves, which are computed using the projection semi-metric. This value determines the lower endpoint of the range from which the bandwidth is selected. The default is 0.05.
</p>
</td></tr>
<tr><td><code id="fsim.kernel.fit.optim_+3A_max.q.h">max.q.h</code></td>
<td>

<p>Maximum quantile order of the distances between curves, which are computed using the projection semi-metric. This value determines the upper endpoint of the range from which the bandwidth is selected. The default is 0.5.
</p>
</td></tr>
<tr><td><code id="fsim.kernel.fit.optim_+3A_h.seq">h.seq</code></td>
<td>
<p>Vector containing the sequence of bandwidths. The default is a sequence of <code>num.h</code> equispaced bandwidths in the range constructed using <code>min.q.h</code> and <code>max.q.h</code>.
</p>
</td></tr>
<tr><td><code id="fsim.kernel.fit.optim_+3A_num.h">num.h</code></td>
<td>
<p>Positive integer indicating the number of bandwidths in the grid. The default is 10.
</p>
</td></tr>
<tr><td><code id="fsim.kernel.fit.optim_+3A_kind.of.kernel">kind.of.kernel</code></td>
<td>

<p>The type of kernel function used. Currently, only Epanechnikov kernel (<code>"quad"</code>) is available.
</p>
</td></tr>
<tr><td><code id="fsim.kernel.fit.optim_+3A_range.grid">range.grid</code></td>
<td>

<p>Vector of length 2 containing the endpoints of the grid at which the observations of the functional covariate <code>x</code> are evaluated (i.e. the range of the discretisation). If <code>range.grid=NULL</code>, then <code>range.grid=c(1,p)</code> is considered, where <code>p</code> is the discretisation size of <code>x</code> (i.e. <code>ncol(x))</code>.
</p>
</td></tr>
<tr><td><code id="fsim.kernel.fit.optim_+3A_nknot">nknot</code></td>
<td>

<p>Positive integer indicating the number of regularly spaced interior knots for the B-spline expansion of the functional covariate. The default value is <code>(p - order.Bspline - 1)%/%2</code>.
</p>
</td></tr>
<tr><td><code id="fsim.kernel.fit.optim_+3A_threshold">threshold</code></td>
<td>

<p>The convergence threshold for the LOOCV function (scaled by the variance of the response). The default is <code>5e-3</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The functional single-index model (FSIM) is given by the expression:
</p>
<p style="text-align: center;"><code class="reqn">Y_i=r(\langle\theta_0,X_i\rangle)+\varepsilon_i, \quad i=1,\dots,n,</code>
</p>

<p>where <code class="reqn">Y_i</code> denotes a scalar response, 
<code class="reqn">X_i</code> is a functional covariate valued in a separable Hilbert space <code class="reqn">\mathcal{H}</code> with an inner product <code class="reqn">\langle \cdot, \cdot\rangle</code>. The term <code class="reqn">\varepsilon</code> denotes the random error, <code class="reqn">\theta_0 \in \mathcal{H}</code> is the unknown functional index and <code class="reqn">r(\cdot)</code> denotes the unknown smooth link function.
</p>
<p>The FSIM is fitted using the kernel estimator
</p>
<p style="text-align: center;"><code class="reqn">
\widehat{r}_{h,\hat{\theta}}(x)=\sum_{i=1}^nw_{n,h,\hat{\theta}}(x,X_i)Y_i,  \quad   \forall x\in\mathcal{H},
</code>
</p>

<p>with Nadaraya-Watson weights
</p>
<p style="text-align: center;"><code class="reqn">
w_{n,h,\hat{\theta}}(x,X_i)=\frac{K\left(h^{-1}d_{\hat{\theta}}\left(X_i,x\right)\right)}{\sum_{i=1}^nK\left(h^{-1}d_{\hat{\theta}}\left(X_i,x\right)\right)},
</code>
</p>

<p>where
</p>

<ul>
<li><p> the real positive number <code class="reqn">h</code> is the bandwidth.
</p>
</li>
<li> <p><code class="reqn">K</code> is a kernel function (see the argument <code>kind.of.kernel</code>).
</p>
</li>
<li> <p><code class="reqn">d_{\hat{\theta}}(x_1,x_2)=|\langle\hat{\theta},x_1-x_2\rangle|</code> is the projection semi-metric, and <code class="reqn">\hat{\theta}</code> is an estimate of <code class="reqn">\theta_0</code>. 
</p>
</li></ul>

<p>The procedure requires the estimation of the function-parameter <code class="reqn">\theta_0</code>. Therefore, we use B-spline expansions to represent curves (dimension <code>nknot+order.Bspline</code>) and eligible functional indexes (dimension <code>nknot.theta+order.Bspline</code>).
We obtain the estimated coefficients of <code class="reqn">\theta_0</code> in the spline basis (<code>theta.est</code>) and the selected bandwidth (<code>h.opt</code>) by minimising the LOOCV criterion. This function performs an iterative minimisation procedure, starting from an initial set of coefficients (<code>gamma</code>) for the functional index. Given a functional index, the optimal bandwidth according to the LOOCV criterion is selected. For a given bandwidth, the minimisation in the functional index is performed using the R function <code>optim</code>. The procedure is iterated until convergence.  For details, see Ferraty et al. (2013). 
</p>


<h3>Value</h3>

<table>
<tr><td><code>call</code></td>
<td>
<p>The matched call.</p>
</td></tr>
<tr><td><code>fitted.values</code></td>
<td>
<p>Estimated scalar response.</p>
</td></tr>
<tr><td><code>residuals</code></td>
<td>
<p>Differences between <code>y</code> and the <code>fitted.values</code>.</p>
</td></tr>
<tr><td><code>theta.est</code></td>
<td>
<p>Coefficients of <code class="reqn">\hat{\theta}</code> in the B-spline basis: a vector of <code>length(order.Bspline+nknot.theta)</code>.</p>
</td></tr>
<tr><td><code>h.opt</code></td>
<td>
<p>Selected bandwidth.</p>
</td></tr>
<tr><td><code>r.squared</code></td>
<td>
<p>Coefficient of determination.</p>
</td></tr> 
<tr><td><code>var.res</code></td>
<td>
<p>Redidual variance.</p>
</td></tr>
<tr><td><code>df</code></td>
<td>
<p>Residual degrees of freedom.</p>
</td></tr>
<tr><td><code>CV.opt</code></td>
<td>
<p>Minimum value of the LOOCV function, i.e. the value of LOOCV for <code>theta.est</code> and <code>h.opt</code>.</p>
</td></tr>
<tr><td><code>err</code></td>
<td>
<p>Value of the LOOCV function divided by <code>var(y)</code> for each interaction.</p>
</td></tr>
<tr><td><code>H</code></td>
<td>
<p>Hat matrix.</p>
</td></tr>
<tr><td><code>h.seq</code></td>
<td>
<p>Sequence of eligible values for the bandwidth.</p>
</td></tr>
<tr><td><code>CV.hseq</code></td>
<td>
<p>CV values for each <code>h</code>.</p>
</td></tr>
<tr><td><code>...</code></td>
<td>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>German Aneiros Perez <a href="mailto:german.aneiros@udc.es">german.aneiros@udc.es</a> 
</p>
<p>Silvia Novo Diaz  <a href="mailto:snovo@est-econ.uc3m.es">snovo@est-econ.uc3m.es</a>
</p>


<h3>References</h3>

<p>Ferraty, F., Goia, A., Salinelli, E., and Vieu, P. (2013) Functional projection pursuit regression. <em>Test</em>, <b>22</b>, 293&ndash;320, <a href="https://doi.org/10.1007/s11749-012-0306-2">doi:10.1007/s11749-012-0306-2</a>.
</p>
<p>Novo S., Aneiros, G., and Vieu, P., (2019) Automatic and location-adaptive estimation in functional single&ndash;index regression. <em>Journal of Nonparametric Statistics</em>, <b>31(2)</b>, 364&ndash;392, <a href="https://doi.org/10.1080/10485252.2019.1567726">doi:10.1080/10485252.2019.1567726</a>.
</p>


<h3>See Also</h3>

<p>See also <code><a href="#topic+predict.fsim.kernel">predict.fsim.kernel</a></code> and <code><a href="#topic+plot.fsim.kernel">plot.fsim.kernel</a></code>.
</p>
<p>Alternative procedures <code><a href="#topic+fsim.kNN.fit.optim">fsim.kNN.fit.optim</a></code>, <code><a href="#topic+fsim.kernel.fit">fsim.kernel.fit</a></code> and <code><a href="#topic+fsim.kNN.fit">fsim.kNN.fit</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(Tecator)
y&lt;-Tecator$fat
X&lt;-Tecator$absor.spectra2

#FSIM fit.
ptm&lt;-proc.time()
fit&lt;-fsim.kernel.fit.optim(y[1:160],x=X[1:160,],max.q.h=0.35, nknot=20,
range.grid=c(850,1050),nknot.theta=4)
proc.time()-ptm
fit
names(fit)

</code></pre>

<hr>
<h2 id='fsim.kernel.test'>
Functional single-index kernel predictor
</h2><span id='topic+fsim.kernel.test'></span>

<h3>Description</h3>

<p>This function computes predictions for a functional single-index model (FSIM) with a scalar response, which is estimated using the Nadaraya-Watson kernel estimator. It requires a functional index (<code class="reqn">\theta</code>), a global bandwidth (<code>h</code>), and the new observations of the functional covariate (<code>x.test</code>) as inputs.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fsim.kernel.test(x, y, x.test, y.test=NULL, theta, nknot.theta = 3, 
order.Bspline = 3, h = 0.5, kind.of.kernel = "quad", range.grid = NULL,
nknot = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fsim.kernel.test_+3A_x">x</code></td>
<td>

<p>Matrix containing the observations of the functional covariate in the training sample, collected by row.
</p>
</td></tr>
<tr><td><code id="fsim.kernel.test_+3A_y">y</code></td>
<td>

<p>Vector containing the scalar responses in the training sample.
</p>
</td></tr>
<tr><td><code id="fsim.kernel.test_+3A_x.test">x.test</code></td>
<td>

<p>Matrix containing the observations of the functional covariate in the the testing sample, collected by row.
</p>
</td></tr>
<tr><td><code id="fsim.kernel.test_+3A_y.test">y.test</code></td>
<td>

<p>(optional) Vector or matrix containing the scalar responses in the testing sample.
</p>
</td></tr>
<tr><td><code id="fsim.kernel.test_+3A_theta">theta</code></td>
<td>

<p>Vector containing the coefficients of <code class="reqn">\theta</code> in a B-spline basis, such that <code>length(theta)=order.Bspline+nknot.theta</code>
</p>
</td></tr>
<tr><td><code id="fsim.kernel.test_+3A_nknot.theta">nknot.theta</code></td>
<td>
<p>Number of regularly spaced interior knots in the B-spline expansion of <code class="reqn">\theta_0</code>. The default is 3. 
</p>
</td></tr>
<tr><td><code id="fsim.kernel.test_+3A_order.bspline">order.Bspline</code></td>
<td>

<p>Order of the B-spline basis functions. This is the number of coefficients in each piecewise polynomial segment. The default is 3</p>
</td></tr>
<tr><td><code id="fsim.kernel.test_+3A_h">h</code></td>
<td>

<p>The global bandwidth. The default if 0.5.
</p>
</td></tr>
<tr><td><code id="fsim.kernel.test_+3A_kind.of.kernel">kind.of.kernel</code></td>
<td>

<p>The type of kernel function used. Currently, only Epanechnikov kernel (<code>"quad"</code>) is available.
</p>
</td></tr>
<tr><td><code id="fsim.kernel.test_+3A_range.grid">range.grid</code></td>
<td>

<p>Vector of length 2 containing the endpoints of the grid at which the observations of the functional covariate <code>x</code> are evaluated (i.e. the range of the discretisation). If <code>range.grid=NULL</code>, then <code>range.grid=c(1,p)</code> is considered, where <code>p</code> is the discretisation size of <code>x</code> (i.e. <code>ncol(x))</code>.
</p>
</td></tr>
<tr><td><code id="fsim.kernel.test_+3A_nknot">nknot</code></td>
<td>

<p>Number of regularly spaced interior knots for the B-spline expansion of the functional covariate. The default value is <code>(p - order.Bspline - 1)%/%2</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The functional single-index model (FSIM) is given by the expression:
</p>
<p style="text-align: center;"><code class="reqn">Y_i=r(\langle\theta_0,X_i\rangle)+\varepsilon_i, \quad i=1,\dots,n,</code>
</p>

<p>where <code class="reqn">Y_i</code> denotes a scalar response, 
<code class="reqn">X_i</code> is a functional covariate valued in a separable Hilbert space <code class="reqn">\mathcal{H}</code> with an inner product <code class="reqn">\langle \cdot, \cdot\rangle</code>. The term <code class="reqn">\varepsilon</code> denotes the random error, <code class="reqn">\theta_0 \in \mathcal{H}</code> is the unknown functional index and <code class="reqn">r(\cdot)</code> denotes the unknown smooth link function; <code class="reqn">n</code> is the training sample size.
</p>
<p>Given <code class="reqn">\theta \in \mathcal{H}</code>, <code class="reqn">h&gt;0</code> and a testing sample {<code class="reqn">X_j,\ j=1,\dots,n_{test}</code>}, the predicted responses (see the value <code>y.estimated.test</code>) can be computed using the kernel procedure using 
</p>
<p style="text-align: center;"><code class="reqn">
\widehat{r}_{h,\theta}(X_j)=\sum_{i=1}^nw_{n,h,\theta}(X_j,X_i)Y_i,\quad  j=1,\dots,n_{test}, 
</code>
</p>

<p>with Nadaraya-Watson weights
</p>
<p style="text-align: center;"><code class="reqn">
w_{n,h,\theta}(X_j,X_i)=\frac{K\left(h^{-1}d_{\theta}\left(X_i,X_j\right)\right)}{\sum_{i=1}^nK\left(h^{-1}d_{\theta}\left(X_i,X_j\right)\right)},
</code>
</p>

<p>where
</p>

<ul>
<li> <p><code class="reqn">K</code> is a kernel function (see the argument <code>kind.of.kernel</code>).
</p>
</li>
<li><p> for <code class="reqn">x_1,x_2 \in \mathcal{H}, </code>  <code class="reqn">d_{\theta}(x_1,x_2)=|\langle\theta,x_1-x_2\rangle|</code> is the projection semi-metric. 
</p>
</li></ul>

<p>If the argument <code>y.test</code> is provided to the program (i. e. <code>if(!is.null(y.test))</code>), the function calculates the mean squared error of prediction (see the value <code>MSE.test</code>). This is computed as <code>mean((y.test-y.estimated.test)^2)</code>.
</p>


<h3>Value</h3>

<table>
<tr><td><code>y.estimated.test</code></td>
<td>
<p>Predicted responses.</p>
</td></tr>
<tr><td><code>MSE.test</code></td>
<td>
<p>Mean squared error between predicted and observed responses in the testing sample.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>German Aneiros Perez <a href="mailto:german.aneiros@udc.es">german.aneiros@udc.es</a> 
</p>
<p>Silvia Novo Diaz  <a href="mailto:snovo@est-econ.uc3m.es">snovo@est-econ.uc3m.es</a>
</p>


<h3>References</h3>

<p>Novo S., Aneiros, G., and Vieu, P., (2019) Automatic and location-adaptive estimation in functional single&ndash;index regression. <em>Journal of Nonparametric Statistics</em>, <b>31(2)</b>, 364&ndash;392, <a href="https://doi.org/10.1080/10485252.2019.1567726">doi:10.1080/10485252.2019.1567726</a>.
</p>


<h3>See Also</h3>

<p>See also <code><a href="#topic+fsim.kernel.fit">fsim.kernel.fit</a></code>, <code><a href="#topic+fsim.kernel.fit.optim">fsim.kernel.fit.optim</a></code> and <code><a href="#topic+predict.fsim.kernel">predict.fsim.kernel</a></code>.
</p>
<p>Alternative procedure <code><a href="#topic+fsim.kNN.test">fsim.kNN.test</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(Tecator)
y&lt;-Tecator$fat
X&lt;-Tecator$absor.spectra2

train&lt;-1:160
test&lt;-161:215

#FSIM fit. 
ptm&lt;-proc.time()
fit&lt;-fsim.kernel.fit(y=y[train],x=X[train,],max.q.h=0.35, nknot=20,
        range.grid=c(850,1050),nknot.theta=4)
proc.time()-ptm
fit

#FSIM prediction
test&lt;-fsim.kernel.test(y=y[train],x=X[train,],x.test=X[test,],y.test=y[test],
        theta=fit$theta.est,h=fit$h.opt,nknot.theta=4,nknot=20,
        range.grid=c(850,1050))

#MSEP
test$MSE.test
  
</code></pre>

<hr>
<h2 id='fsim.kNN.fit'>
Functional single-index model fit using kNN estimation and joint LOOCV minimisation
</h2><span id='topic+fsim.kNN.fit'></span>

<h3>Description</h3>

<p>This function fits a functional single-index model (FSIM) between a functional covariate and a
scalar response. 
It employs kNN estimation with Nadaraya-Watson weights and uses B-spline expansions to represent curves and eligible functional indexes. 
</p>
<p>The function also utilises the leave-one-out cross-validation (LOOCV) criterion to select the number of neighbours (<code>k.opt</code>) and the coefficients of the functional index in the spline basis (<code>theta.est</code>). It performs a joint minimisation of the LOOCV objective function in both the number of neighbours and the functional index.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fsim.kNN.fit(x, y, seed.coeff = c(-1, 0, 1), order.Bspline = 3, nknot.theta = 3,
knearest = NULL, min.knn = 2, max.knn = NULL,  step = NULL, 
kind.of.kernel = "quad", range.grid = NULL, nknot = NULL, n.core = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fsim.kNN.fit_+3A_x">x</code></td>
<td>
<p>Matrix containing the observations of the functional covariate (i.e. curves) collected by row.</p>
</td></tr>
<tr><td><code id="fsim.kNN.fit_+3A_y">y</code></td>
<td>
<p>Vector containing the scalar response.</p>
</td></tr>
<tr><td><code id="fsim.kNN.fit_+3A_seed.coeff">seed.coeff</code></td>
<td>

<p>Vector of initial values used to  build the set <code class="reqn">\Theta_n</code> (see section <code>Details</code>). The coefficients for the B-spline representation of each eligible functional index <code class="reqn">\theta \in \Theta_n</code> are obtained from <code>seed.coeff</code>.  The default is <code>c(-1,0,1)</code>.
</p>
</td></tr>
<tr><td><code id="fsim.kNN.fit_+3A_order.bspline">order.Bspline</code></td>
<td>

<p>Positive integer giving the order of the B-spline basis functions. This is the number of coefficients in each piecewise polynomial segment. The default is 3</p>
</td></tr>
<tr><td><code id="fsim.kNN.fit_+3A_nknot.theta">nknot.theta</code></td>
<td>
<p>Positive integer indicating the number of regularly spaced interior knots in the B-spline expansion of <code class="reqn">\theta_0</code>. The default is 3. 
</p>
</td></tr> 
<tr><td><code id="fsim.kNN.fit_+3A_knearest">knearest</code></td>
<td>

<p>Vector of positive integers that defines the sequence within which the optimal number of nearest neighbours <code>k.opt</code> is selected. If <code>knearest=NULL</code>, then <code>knearest &lt;- seq(from =min.knn, to = max.knn, by = step)</code>.
</p>
</td></tr>
<tr><td><code id="fsim.kNN.fit_+3A_min.knn">min.knn</code></td>
<td>

<p>A positive integer that represents the minimum value in the sequence for selecting the number of nearest neighbours <code>k.opt</code>. This value should be less than the sample size. The default is 2.
</p>
</td></tr>
<tr><td><code id="fsim.kNN.fit_+3A_max.knn">max.knn</code></td>
<td>

<p>A positive integer that represents the maximum value in the sequence for selecting number of nearest neighbours <code>k.opt</code>. This value should be less than the sample size. The default is <code>max.knn &lt;- n%/%5</code>.
</p>
</td></tr>
<tr><td><code id="fsim.kNN.fit_+3A_step">step</code></td>
<td>

<p>A positive integer used to construct the sequence of k-nearest neighbours as follows: <code>min.knn, min.knn + step, min.knn + 2*step, min.knn + 3*step,...</code>. The default value for <code>step</code> is <code>step&lt;-ceiling(n/100)</code>.
</p>
</td></tr>
<tr><td><code id="fsim.kNN.fit_+3A_kind.of.kernel">kind.of.kernel</code></td>
<td>

<p>The type of kernel function used. Currently, only Epanechnikov kernel (<code>"quad"</code>) is available.
</p>
</td></tr>
<tr><td><code id="fsim.kNN.fit_+3A_range.grid">range.grid</code></td>
<td>

<p>Vector of length 2 containing the endpoints of the grid at which the observations of the functional covariate <code>x</code> are evaluated (i.e. the range of the discretisation). If <code>range.grid=NULL</code>, then <code>range.grid=c(1,p)</code> is considered, where <code>p</code> is the discretisation size of <code>x</code> (i.e. <code>ncol(x))</code>.
</p>
</td></tr>
<tr><td><code id="fsim.kNN.fit_+3A_nknot">nknot</code></td>
<td>

<p>Positive integer indicating the number of interior knots for the B-spline expansion of the functional covariate. The default value is <code>(p - order.Bspline - 1)%/%2</code>.
</p>
</td></tr>
<tr><td><code id="fsim.kNN.fit_+3A_n.core">n.core</code></td>
<td>

<p>Number of CPU cores designated for parallel execution.The default is <code>n.core&lt;-availableCores(omit=1)</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The functional single-index model (FSIM) is given by the expression:
</p>
<p style="text-align: center;"><code class="reqn">Y_i=r(\langle\theta_0,X_i\rangle)+\varepsilon_i, \quad i=1,\dots,n,</code>
</p>

<p>where <code class="reqn">Y_i</code> denotes a scalar response, 
<code class="reqn">X_i</code> is a functional covariate valued in a separable Hilbert space <code class="reqn">\mathcal{H}</code> with an inner product <code class="reqn">\langle \cdot, \cdot\rangle</code>. The term <code class="reqn">\varepsilon</code> denotes the random error, <code class="reqn">\theta_0 \in \mathcal{H}</code> is the unknown functional index and <code class="reqn">r(\cdot)</code> denotes the unknown smooth link function.
</p>
<p>The FSIM is fitted using the kNN estimator
</p>
<p style="text-align: center;"><code class="reqn">
\widehat{r}_{k,\hat{\theta}}(x)=\sum_{i=1}^nw_{n,k,\hat{\theta}}(x,X_i)Y_i,  \quad   \forall x\in\mathcal{H},
</code>
</p>

<p>with Nadaraya-Watson weights
</p>
<p style="text-align: center;"><code class="reqn">
w_{n,k,\hat{\theta}}(x,X_i)=\frac{K\left(H_{k,x,\hat{\theta}}^{-1}d_{\hat{\theta}}\left(X_i,x\right)\right)}{\sum_{i=1}^nK\left(H_{k,x,\hat{\theta}}^{-1}d_{\hat{\theta}}\left(X_i,x\right)\right)},
</code>
</p>

<p>where
</p>

<ul>
<li><p> the positive integer <code class="reqn">k</code> is a smoothing factor, representing the number of nearest neighbours.
</p>
</li>
<li> <p><code class="reqn">K</code> is a kernel function (see the argument <code>kind.of.kernel</code>).
</p>
</li>
<li> <p><code class="reqn">d_{\hat{\theta}}(x_1,x_2)=|\langle\hat{\theta},x_1-x_2\rangle|</code> is the projection semi-metric, computed using <code><a href="#topic+semimetric.projec">semimetric.projec</a></code> and <code class="reqn">\hat{\theta}</code> is an estimate of <code class="reqn">\theta_0</code>. 
</p>
</li>
<li> <p><code class="reqn">H_{k,x,\hat{\theta}}=\min\{h\in R^+ \text{ such that } \sum_{i=1}^n1_{B_{\hat{\theta}}(x,h)}(X_i)=k\}</code>, where <code class="reqn">1_{B_{\hat{\theta}}(x,h)}(\cdot)</code> is the indicator function of the open ball defined by the projection semi-metric, with centre <code class="reqn">x\in\mathcal{H}</code> and radius <code class="reqn">h</code>.
</p>
</li></ul>

<p>The procedure requires the estimation of the function-parameter <code class="reqn">\theta_0</code>. Therefore, we use B-spline expansions to represent curves (dimension <code>nknot+order.Bspline</code>) and eligible functional indexes (dimension <code>nknot.theta+order.Bspline</code>). Then, we build a set <code class="reqn">\Theta_n</code> of eligible functional indexes by calibrating (to ensure the identifiability of the model) the set of initial coefficients given in <code>seed.coeff</code>. The larger this set is, the greater the size of <code class="reqn">\Theta_n</code>. Since our approach requires intensive computation, a trade-off between the size of <code class="reqn">\Theta_n</code> and the performance of the estimator is necessary. For that, Ait-Saidi et al. (2008) suggested considering <code>order.Bspline=3</code> and <code>seed.coeff=c(-1,0,1)</code>. For details on the construction of <code class="reqn">\Theta_n</code>, see Novo et al. (2019).
</p>
<p>We obtain the estimated coefficients of <code class="reqn">\theta_0</code> in the spline basis (<code>theta.est</code>) and the selected number of neighbours (<code>k.opt</code>) by minimising the LOOCV criterion. This function performs a joint minimisation in both parameters, the number of neighbours and the functional index, and supports parallel computation. To avoid parallel computation, we can set <code>n.core=1</code>.
</p>


<h3>Value</h3>

<table>
<tr><td><code>call</code></td>
<td>
<p>The matched call.</p>
</td></tr>
<tr><td><code>fitted.values</code></td>
<td>
<p>Estimated scalar response.</p>
</td></tr>
<tr><td><code>residuals</code></td>
<td>
<p>Differences between <code>y</code> and the <code>fitted.values</code></p>
</td></tr>
<tr><td><code>theta.est</code></td>
<td>
<p>Coefficients of <code class="reqn">\hat{\theta}</code> in the B-spline basis: a vector of <code>length(order.Bspline+nknot.theta)</code>.</p>
</td></tr>
<tr><td><code>k.opt</code></td>
<td>
<p>Selected number of nearest neighbours.</p>
</td></tr>
<tr><td><code>r.squared</code></td>
<td>
<p>Coefficient of determination.</p>
</td></tr> 
<tr><td><code>var.res</code></td>
<td>
<p>Redidual variance.</p>
</td></tr>
<tr><td><code>df</code></td>
<td>
<p>Residual degrees of freedom.</p>
</td></tr>
<tr><td><code>yhat.cv</code></td>
<td>
<p>Predicted values for the scalar response using leave-one-out samples.</p>
</td></tr>
<tr><td><code>CV.opt</code></td>
<td>
<p>Minimum value of the CV function, i.e. the value of CV for <code>theta.est</code> and <code>k.opt</code>.</p>
</td></tr>
<tr><td><code>CV.values</code></td>
<td>
<p>Vector containing CV values for each functional index in <code class="reqn">\Theta_n</code> and the value of <code class="reqn">k</code> that minimises the CV for such index (i.e. <code>CV.values[j]</code> contains the value of the CV function corresponding to <code>theta.seq.norm[j,]</code> and the best value of the <code>k.seq</code> for this functional index according to the CV criterion).</p>
</td></tr>
<tr><td><code>H</code></td>
<td>
<p>Hat matrix.</p>
</td></tr>
<tr><td><code>m.opt</code></td>
<td>
<p>Index of <code class="reqn">\hat{\theta}</code> in the set <code class="reqn">\Theta_n</code>.</p>
</td></tr>
<tr><td><code>theta.seq.norm</code></td>
<td>
<p>The vector <code>theta.seq.norm[j,]</code> contains the coefficientes in the B-spline basis of the jth functional index in <code class="reqn">\Theta_n</code>.</p>
</td></tr>
<tr><td><code>k.seq</code></td>
<td>
<p>Sequence of eligible values for <code class="reqn">k</code>.</p>
</td></tr>
<tr><td><code>...</code></td>
<td>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>German Aneiros Perez <a href="mailto:german.aneiros@udc.es">german.aneiros@udc.es</a> 
</p>
<p>Silvia Novo Diaz  <a href="mailto:snovo@est-econ.uc3m.es">snovo@est-econ.uc3m.es</a>
</p>


<h3>References</h3>

<p>Ait-Saidi, A., Ferraty, F., Kassa, R., and Vieu, P. (2008) Cross-validated estimations in the single-functional index model, <em>Statistics</em>, <b>42(6)</b>, 475&ndash;494, <a href="https://doi.org/10.1080/02331880801980377">doi:10.1080/02331880801980377</a>.
</p>
<p>Novo S., Aneiros, G., and Vieu, P., (2019) Automatic and location-adaptive estimation in functional single&ndash;index regression, <em>Journal of Nonparametric Statistics</em>, <b>31(2)</b>, 364&ndash;392, <a href="https://doi.org/10.1080/10485252.2019.1567726">doi:10.1080/10485252.2019.1567726</a>.
</p>


<h3>See Also</h3>

<p>See also <code><a href="#topic+fsim.kNN.test">fsim.kNN.test</a></code>, <code><a href="#topic+predict.fsim.kNN">predict.fsim.kNN</a></code>, <code><a href="#topic+plot.fsim.kNN">plot.fsim.kNN</a></code>.
</p>
<p>Alternative procedures <code><a href="#topic+fsim.kernel.fit">fsim.kernel.fit</a></code>, <code><a href="#topic+fsim.kNN.fit.optim">fsim.kNN.fit.optim</a></code> and <code><a href="#topic+fsim.kernel.fit.optim">fsim.kernel.fit.optim</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(Tecator)
y&lt;-Tecator$fat
X&lt;-Tecator$absor.spectra2

#FSIM fit.
ptm&lt;-proc.time()
fit&lt;-fsim.kNN.fit(y=y[1:160],x=X[1:160,],max.knn=20,nknot.theta=4,nknot=20,
range.grid=c(850,1050))
proc.time()-ptm
fit
names(fit)


</code></pre>

<hr>
<h2 id='fsim.kNN.fit.optim'>
Functional single-index model fit using kNN estimation and iterative LOOCV minimisation
</h2><span id='topic+fsim.kNN.fit.optim'></span>

<h3>Description</h3>

<p>This function fits a functional single-index model (FSIM) between a functional covariate and a
scalar response. 
It employs kNN estimation with Nadaraya-Watson weights and uses B-spline expansions to represent curves and eligible functional indexes. 
</p>
<p>The function also utilises the leave-one-out cross-validation (LOOCV) criterion to select the bandwidth (<code>h.opt</code>) and the coefficients of the functional index in the spline basis (<code>theta.est</code>). It performs an iterative minimisation of the LOOCV objective function, starting from an initial set of coefficients (<code>gamma</code>) for the functional index.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fsim.kNN.fit.optim(x, y, order.Bspline = 3, nknot.theta = 3, gamma = NULL, 
knearest = NULL, min.knn = 2, max.knn = NULL,  step = NULL, 
kind.of.kernel = "quad", range.grid = NULL, nknot = NULL, threshold = 0.005)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fsim.kNN.fit.optim_+3A_x">x</code></td>
<td>
<p>Matrix containing the observations of the functional covariate (i.e. curves) collected by row.</p>
</td></tr>
<tr><td><code id="fsim.kNN.fit.optim_+3A_y">y</code></td>
<td>
<p>Vector containing the scalar response.</p>
</td></tr>
<tr><td><code id="fsim.kNN.fit.optim_+3A_order.bspline">order.Bspline</code></td>
<td>

<p>Positive integer giving the order of the B-spline basis functions. This is the number of coefficients in each piecewise polynomial segment. The default is 3</p>
</td></tr>
<tr><td><code id="fsim.kNN.fit.optim_+3A_nknot.theta">nknot.theta</code></td>
<td>
<p>Positive integer indicating the number of regularly spaced interior knots in the B-spline expansion of <code class="reqn">\theta_0</code>. The default is 3. 
</p>
</td></tr>
<tr><td><code id="fsim.kNN.fit.optim_+3A_gamma">gamma</code></td>
<td>

<p>Vector indicating the initial coefficients for the functional index used in the iterative procedure. By default, it is a vector of ones. The size of the vector is determined by the sum <code>nknot.theta+order.Bspline</code>.
</p>
</td></tr>
<tr><td><code id="fsim.kNN.fit.optim_+3A_knearest">knearest</code></td>
<td>

<p>Vector of positive integers that defines the sequence within which the optimal number of nearest neighbours <code>k.opt</code> is selected. If <code>knearest=NULL</code>, then <code>knearest &lt;- seq(from =min.knn, to = max.knn, by = step)</code>.
</p>
</td></tr>
<tr><td><code id="fsim.kNN.fit.optim_+3A_min.knn">min.knn</code></td>
<td>

<p>A positive integer that represents the minimum value in the sequence for selecting the number of nearest neighbours <code>k.opt</code>. This value should be less than the sample size. The default is 2.
</p>
</td></tr>
<tr><td><code id="fsim.kNN.fit.optim_+3A_max.knn">max.knn</code></td>
<td>

<p>A positive integer that represents the maximum value in the sequence for selecting number of nearest neighbours <code>k.opt</code>. This value should be less than the sample size. The default is <code>max.knn &lt;- n%/%5</code>.
</p>
</td></tr>
<tr><td><code id="fsim.kNN.fit.optim_+3A_step">step</code></td>
<td>

<p>A positive integer used to construct the sequence of k-nearest neighbours as follows: <code>min.knn, min.knn + step, min.knn + 2*step, min.knn + 3*step,...</code>. The default value for <code>step</code> is <code>step&lt;-ceiling(n/100)</code>.
</p>
</td></tr>
<tr><td><code id="fsim.kNN.fit.optim_+3A_kind.of.kernel">kind.of.kernel</code></td>
<td>

<p>The type of kernel function used. Currently, only Epanechnikov kernel (<code>"quad"</code>) is available.
</p>
</td></tr>
<tr><td><code id="fsim.kNN.fit.optim_+3A_range.grid">range.grid</code></td>
<td>

<p>Vector of length 2 containing the endpoints of the grid at which the observations of the functional covariate <code>x</code> are evaluated (i.e. the range of the discretisation). If <code>range.grid=NULL</code>, then <code>range.grid=c(1,p)</code> is considered, where <code>p</code> is the discretisation size of <code>x</code> (i.e. <code>ncol(x))</code>.
</p>
</td></tr>
<tr><td><code id="fsim.kNN.fit.optim_+3A_nknot">nknot</code></td>
<td>

<p>Positive integer indicating the number of regularly spaced interior knots for the B-spline expansion of the functional covariate. The default value is <code>(p - order.Bspline - 1)%/%2</code>.
</p>
</td></tr>
<tr><td><code id="fsim.kNN.fit.optim_+3A_threshold">threshold</code></td>
<td>

<p>The convergence threshold for the LOOCV function (scaled by the variance of the response). The default is <code>5e-3</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The functional single-index model (FSIM) is given by the expression:
</p>
<p style="text-align: center;"><code class="reqn">Y_i=r(\langle\theta_0,X_i\rangle)+\varepsilon_i, \quad i=1,\dots,n,</code>
</p>

<p>where <code class="reqn">Y_i</code> denotes a scalar response, 
<code class="reqn">X_i</code> is a functional covariate valued in a separable Hilbert space <code class="reqn">\mathcal{H}</code> with an inner product <code class="reqn">\langle \cdot, \cdot\rangle</code>. The term <code class="reqn">\varepsilon</code> denotes the random error, <code class="reqn">\theta_0 \in \mathcal{H}</code> is the unknown functional index and <code class="reqn">r(\cdot)</code> denotes the unknown smooth link function.
</p>
<p>The FSIM is fitted using the kNN estimator
</p>
<p style="text-align: center;"><code class="reqn">
\widehat{r}_{k,\hat{\theta}}(x)=\sum_{i=1}^nw_{n,k,\hat{\theta}}(x,X_i)Y_i,  \quad   \forall x\in\mathcal{H},
</code>
</p>

<p>with Nadaraya-Watson weights
</p>
<p style="text-align: center;"><code class="reqn">
w_{n,k,\hat{\theta}}(x,X_i)=\frac{K\left(H_{k,x,\hat{\theta}}^{-1}d_{\hat{\theta}}\left(X_i,x\right)\right)}{\sum_{i=1}^nK\left(H_{k,x,\hat{\theta}}^{-1}d_{\hat{\theta}}\left(X_i,x\right)\right)},
</code>
</p>

<p>where
</p>

<ul>
<li><p> the positive integer <code class="reqn">k</code> is a smoothing factor, representing the number of nearest neighbours.
</p>
</li>
<li> <p><code class="reqn">K</code> is a kernel function (see the argument <code>kind.of.kernel</code>).
</p>
</li>
<li> <p><code class="reqn">d_{\hat{\theta}}(x_1,x_2)=|\langle\hat{\theta},x_1-x_2\rangle|</code> is the projection semi-metric and <code class="reqn">\hat{\theta}</code> is an estimate of <code class="reqn">\theta_0</code>. 
</p>
</li>
<li> <p><code class="reqn">H_{k,x,\hat{\theta}}=\min\{h\in R^+ \text{ such that } \sum_{i=1}^n1_{B_{\hat{\theta}}(x,h)}(X_i)=k\}</code>, where <code class="reqn">1_{B_{\hat{\theta}}(x,h)}(\cdot)</code> is the indicator function of the open ball defined by the projection semi-metric, with centre <code class="reqn">x\in\mathcal{H}</code> and radius <code class="reqn">h</code>.
</p>
</li></ul>

<p>The procedure requires the estimation of the function-parameter <code class="reqn">\theta_0</code>. Therefore, we use B-spline expansions to represent curves (dimension <code>nknot+order.Bspline</code>) and eligible functional indexes (dimension <code>nknot.theta+order.Bspline</code>).
We obtain the estimated coefficients of <code class="reqn">\theta_0</code> in the spline basis (<code>theta.est</code>) and the selected number of neighbours (<code>k.opt</code>) by minimising the LOOCV criterion. This function performs an iterative minimisation procedure, starting from an initial set of coefficients (<code>gamma</code>) for the functional index. Given a functional index, the optimal number of neighbours according to the LOOCV criterion is selected. For a given number of neighbours, the minimisation in the functional index is performed using the R function <code>optim</code>. The procedure is iterated until convergence.  For details, see Ferraty et al. (2013). 
</p>


<h3>Value</h3>

<table>
<tr><td><code>call</code></td>
<td>
<p>The matched call.</p>
</td></tr>
<tr><td><code>fitted.values</code></td>
<td>
<p>Estimated scalar response.</p>
</td></tr>
<tr><td><code>residuals</code></td>
<td>
<p>Differences between <code>y</code> and the <code>fitted.values</code>.</p>
</td></tr>
<tr><td><code>theta.est</code></td>
<td>
<p>Coefficients of <code class="reqn">\hat{\theta}</code> in the B-spline basis: a vector of <code>length(order.Bspline+nknot.theta)</code>.</p>
</td></tr>
<tr><td><code>k.opt</code></td>
<td>
<p>Selected number of neighbours.</p>
</td></tr>
<tr><td><code>r.squared</code></td>
<td>
<p>Coefficient of determination.</p>
</td></tr> 
<tr><td><code>var.res</code></td>
<td>
<p>Redidual variance.</p>
</td></tr>
<tr><td><code>df</code></td>
<td>
<p>Residual degrees of freedom.</p>
</td></tr>
<tr><td><code>CV.opt</code></td>
<td>
<p>Minimum value of the LOOCV function, i.e. the value of LOOCV for <code>theta.est</code> and <code>k.opt</code>.</p>
</td></tr>
<tr><td><code>err</code></td>
<td>
<p>Value of the LOOCV function divided by <code>var(y)</code> for each interaction.</p>
</td></tr>
<tr><td><code>H</code></td>
<td>
<p>Hat matrix.</p>
</td></tr>
<tr><td><code>k.seq</code></td>
<td>
<p>Sequence of eligible values for <code class="reqn">k</code>.</p>
</td></tr>
<tr><td><code>CV.hseq</code></td>
<td>
<p>CV values for each <code class="reqn">k</code>.</p>
</td></tr>
<tr><td><code>...</code></td>
<td>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>German Aneiros Perez <a href="mailto:german.aneiros@udc.es">german.aneiros@udc.es</a> 
</p>
<p>Silvia Novo Diaz  <a href="mailto:snovo@est-econ.uc3m.es">snovo@est-econ.uc3m.es</a>
</p>


<h3>References</h3>

<p>Ferraty, F., Goia, A., Salinelli, E., and Vieu, P. (2013) Functional projection pursuit regression. <em>Test</em>, <b>22</b>, 293&ndash;320, <a href="https://doi.org/10.1007/s11749-012-0306-2">doi:10.1007/s11749-012-0306-2</a>.
</p>
<p>Novo S., Aneiros, G., and Vieu, P., (2019) Automatic and location-adaptive estimation in functional single&ndash;index regression. <em>Journal of Nonparametric Statistics</em>, <b>31(2)</b>, 364&ndash;392, <a href="https://doi.org/10.1080/10485252.2019.1567726">doi:10.1080/10485252.2019.1567726</a>.
</p>


<h3>See Also</h3>

<p>See also <code><a href="#topic+predict.fsim.kNN">predict.fsim.kNN</a></code> and <code><a href="#topic+plot.fsim.kNN">plot.fsim.kNN</a></code>.
</p>
<p>Alternative procedures <code><a href="#topic+fsim.kernel.fit.optim">fsim.kernel.fit.optim</a></code>, <code><a href="#topic+fsim.kernel.fit">fsim.kernel.fit</a></code> and <code><a href="#topic+fsim.kNN.fit">fsim.kNN.fit</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(Tecator)
y&lt;-Tecator$fat
X&lt;-Tecator$absor.spectra2

#FSIM fit.
ptm&lt;-proc.time()
fit&lt;-fsim.kNN.fit.optim(y=y[1:160],x=X[1:160,],max.knn=20,nknot.theta=4,nknot=20,
range.grid=c(850,1050))
proc.time()-ptm
fit
names(fit)


</code></pre>

<hr>
<h2 id='fsim.kNN.test'>
Functional single-index kNN predictor
</h2><span id='topic+fsim.kNN.test'></span>

<h3>Description</h3>

<p>This function computes predictions for a functional single-index model (FSIM) with a scalar response, which is estimated using the Nadaraya-Watson kNN estimator. It requires a functional index (<code class="reqn">\theta</code>), a global bandwidth (<code>h</code>), and the new observations of the functional covariate (<code>x.test</code>) as inputs.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fsim.kNN.test(x, y, x.test, y.test = NULL, theta, order.Bspline = 3, 
nknot.theta = 3, k = 4, kind.of.kernel = "quad", range.grid = NULL, 
nknot = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fsim.kNN.test_+3A_x">x</code></td>
<td>

<p>Matrix containing the observations of the functional covariate in the training sample, collected by row.
</p>
</td></tr>
<tr><td><code id="fsim.kNN.test_+3A_y">y</code></td>
<td>

<p>Vector containing the scalar responses in the training sample.
</p>
</td></tr>
<tr><td><code id="fsim.kNN.test_+3A_x.test">x.test</code></td>
<td>

<p>Matrix containing the observations of the functional covariate in the the testing sample, collected by row.
</p>
</td></tr>
<tr><td><code id="fsim.kNN.test_+3A_y.test">y.test</code></td>
<td>

<p>(optional) Vector or matrix containing the scalar responses in the testing sample.
</p>
</td></tr>
<tr><td><code id="fsim.kNN.test_+3A_theta">theta</code></td>
<td>

<p>Vector containing the coefficients of <code class="reqn">\theta</code> in a B-spline basis, such that <code>length(theta)=order.Bspline+nknot.theta</code>
</p>
</td></tr>
<tr><td><code id="fsim.kNN.test_+3A_nknot.theta">nknot.theta</code></td>
<td>
<p>Number of regularly spaced interior knots in the B-spline expansion of <code class="reqn">\theta_0</code>. The default is 3. 
</p>
</td></tr>
<tr><td><code id="fsim.kNN.test_+3A_order.bspline">order.Bspline</code></td>
<td>

<p>Order of the B-spline basis functions. This is the number of coefficients in each piecewise polynomial segment. The default is 3</p>
</td></tr>
<tr><td><code id="fsim.kNN.test_+3A_k">k</code></td>
<td>

<p>The number of nearest neighbours. The default is 4.
</p>
</td></tr>
<tr><td><code id="fsim.kNN.test_+3A_kind.of.kernel">kind.of.kernel</code></td>
<td>

<p>The type of kernel function used. Currently, only Epanechnikov kernel (<code>"quad"</code>) is available.
</p>
</td></tr>
<tr><td><code id="fsim.kNN.test_+3A_range.grid">range.grid</code></td>
<td>

<p>Vector of length 2 containing the endpoints of the grid at which the observations of the functional covariate <code>x</code> are evaluated (i.e. the range of the discretisation). If <code>range.grid=NULL</code>, then <code>range.grid=c(1,p)</code> is considered, where <code>p</code> is the discretisation size of <code>x</code> (i.e. <code>ncol(x))</code>.
</p>
</td></tr>
<tr><td><code id="fsim.kNN.test_+3A_nknot">nknot</code></td>
<td>

<p>Number of regularly spaced interior knots for the B-spline expansion of the functional covariate. The default value is <code>(p - order.Bspline - 1)%/%2</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The functional single-index model (FSIM) is given by the expression:
</p>
<p style="text-align: center;"><code class="reqn">Y_i=r(\langle\theta_0,X_i\rangle)+\varepsilon_i, \quad i=1,\dots,n,</code>
</p>

<p>where <code class="reqn">Y_i</code> denotes a scalar response, 
<code class="reqn">X_i</code> is a functional covariate valued in a separable Hilbert space <code class="reqn">\mathcal{H}</code> with an inner product <code class="reqn">\langle \cdot, \cdot\rangle</code>. The term <code class="reqn">\varepsilon</code> denotes the random error, <code class="reqn">\theta_0 \in \mathcal{H}</code> is the unknown functional index and <code class="reqn">r(\cdot)</code> denotes the unknown smooth link function; <code class="reqn">n</code> is the training sample size.
</p>
<p>Given <code class="reqn">\theta \in \mathcal{H}</code>, <code class="reqn">1&lt;k&lt;n</code> and a testing sample {<code class="reqn">X_j,\ j=1,\dots,n_{test}</code>}, the predicted responses (see the value <code>y.estimated.test</code>) can be computed using the kNN procedure by means of 
</p>
<p style="text-align: center;"><code class="reqn">
\widehat{r}_{k,\theta}(X_j)=\sum_{i=1}^nw_{n,k,\theta}(X_j,X_i)Y_i,\quad  j=1,\dots,n_{test}, 
</code>
</p>

<p>with Nadaraya-Watson weights
</p>
<p style="text-align: center;"><code class="reqn">
w_{n,k,\theta}(X_j,X_i)=\frac{K\left(H_{k,X_j,{\theta}}^{-1}d_{\theta}\left(X_i,X_j\right)\right)}{\sum_{i=1}^nK\left(H_{k,X_j,\theta}^{-1}d_{\theta}\left(X_i,X_j\right)\right)},
</code>
</p>

<p>where
</p>

<ul>
<li> <p><code class="reqn">K</code> is a kernel function (see the argument <code>kind.of.kernel</code>).
</p>
</li>
<li><p> for <code class="reqn">x_1,x_2 \in \mathcal{H}, </code>  <code class="reqn">d_{\theta}(x_1,x_2)=|\langle\theta,x_1-x_2\rangle|</code> is the projection semi-metric. 
</p>
</li>
<li> <p><code class="reqn">H_{k,x,\theta}=\min\left\{h\in R^+ \text{ such that } \sum_{i=1}^n1_{B_{\theta}(x,h)}(X_i)=k\right\}</code>, where <code class="reqn">1_{B_{\theta}(x,h)}(\cdot)</code> is the indicator function of the open ball  defined by the projection semi-metric, with centre <code class="reqn">x\in\mathcal{H}</code> and radius <code class="reqn">h</code>.
</p>
</li></ul>

<p>If the argument <code>y.test</code> is provided to the program (i. e. <code>if(!is.null(y.test))</code>), the function calculates the mean squared error of prediction (see the value <code>MSE.test</code>). This is computed as <code>mean((y.test-y.estimated.test)^2)</code>.
</p>


<h3>Value</h3>

<table>
<tr><td><code>y.estimated.test</code></td>
<td>
<p>Predicted responses.</p>
</td></tr>
<tr><td><code>MSE.test</code></td>
<td>
<p>Mean squared error between predicted and observed responses in the testing sample.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>German Aneiros Perez <a href="mailto:german.aneiros@udc.es">german.aneiros@udc.es</a> 
</p>
<p>Silvia Novo Diaz  <a href="mailto:snovo@est-econ.uc3m.es">snovo@est-econ.uc3m.es</a>
</p>


<h3>References</h3>

<p>Novo S., Aneiros, G., and Vieu, P., (2019) Automatic and location-adaptive estimation in functional single&ndash;index regression. <em>Journal of Nonparametric Statistics</em>, <b>31(2)</b>, 364&ndash;392, <a href="https://doi.org/10.1080/10485252.2019.1567726">doi:10.1080/10485252.2019.1567726</a>.
</p>


<h3>See Also</h3>

<p>See also <code><a href="#topic+fsim.kNN.fit">fsim.kNN.fit</a></code>, <code><a href="#topic+fsim.kNN.fit.optim">fsim.kNN.fit.optim</a></code>  and <code><a href="#topic+predict.fsim.kNN">predict.fsim.kNN</a></code>.
</p>
<p>Alternative procedure <code><a href="#topic+fsim.kernel.test">fsim.kernel.test</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(Tecator)
y&lt;-Tecator$fat
X&lt;-Tecator$absor.spectra2


train&lt;-1:160
test&lt;-161:215

#FSIM fit. 
ptm&lt;-proc.time()
fit&lt;-fsim.kNN.fit(y=y[train],x=X[train,],max.knn=20,nknot.theta=4,nknot=20,
      range.grid=c(850,1050))
proc.time()-ptm
fit

#FSIM prediction
test&lt;-fsim.kNN.test(y=y[train],x=X[train,],x.test=X[test,],y.test=y[test],
        theta=fit$theta.est,k=fit$k.opt,nknot.theta=4,nknot=20,
        range.grid=c(850,1050))

#MSEP
test$MSE.test

  
</code></pre>

<hr>
<h2 id='IASSMR.kernel.fit'>
Impact point selection with IASSMR and kernel estimation
</h2><span id='topic+IASSMR.kernel.fit'></span>

<h3>Description</h3>

<p>This function implements the Improved Algorithm for Sparse Semiparametric Multi-functional Regression (IASSMR) with kernel estimation. This algorithm is specifically designed for estimating multi-functional partial linear single-index models, which incorporate multiple scalar variables and a functional covariate as predictors. These scalar variables are derived from the discretisation of a curve and have linear effects while the functional covariate exhibits a single-index effect. 
</p>
<p>IASSMR is a two-stage procedure that selects the impact points of the discretised curve and estimates the model. The algorithm employs a penalised least-squares regularisation procedure, integrated with kernel estimation using Nadaraya-Watson weights. It uses B-spline expansions to represent curves and eligible functional indexes. Additionally, it utilises an objective criterion (<code>criterion</code>) to determine the initial number of covariates in the reduced model (<code>w.opt</code>), the bandwidth (<code>h.opt</code>), and the penalisation parameter (<code>lambda.opt</code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>IASSMR.kernel.fit(x, z, y, train.1 = NULL, train.2 = NULL, 
seed.coeff = c(-1, 0, 1), order.Bspline = 3, nknot.theta = 3, 
min.q.h = 0.05, max.q.h = 0.5, h.seq = NULL, num.h = 10, range.grid = NULL, 
kind.of.kernel = "quad", nknot = NULL, lambda.min = NULL, lambda.min.h = NULL, 
lambda.min.l = NULL, factor.pn = 1, nlambda = 100, vn = ncol(z), nfolds = 10, 
seed = 123, wn = c(10, 15, 20), criterion = "GCV", penalty = "grSCAD", 
max.iter = 1000, n.core = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="IASSMR.kernel.fit_+3A_x">x</code></td>
<td>

<p>Matrix containing the observations of the functional covariate (functional single-index component), collected by row .
</p>
</td></tr>
<tr><td><code id="IASSMR.kernel.fit_+3A_z">z</code></td>
<td>

<p>Matrix containing the observations of the functional covariate that is discretised (linear component), collected by row.
</p>
</td></tr>
<tr><td><code id="IASSMR.kernel.fit_+3A_y">y</code></td>
<td>

<p>Vector containing the scalar response.
</p>
</td></tr>
<tr><td><code id="IASSMR.kernel.fit_+3A_train.1">train.1</code></td>
<td>

<p>Positions of the data that are used as the training sample in the 1st step. The default setting is  <code>train.1&lt;-1:ceiling(n/2)</code>.
</p>
</td></tr>
<tr><td><code id="IASSMR.kernel.fit_+3A_train.2">train.2</code></td>
<td>

<p>Positions of the data that are used as the training sample in the 2nd step. The default setting is <code>train.2&lt;-(ceiling(n/2)+1):n</code>.
</p>
</td></tr>
<tr><td><code id="IASSMR.kernel.fit_+3A_seed.coeff">seed.coeff</code></td>
<td>

<p>Vector of initial values used to  build the set <code class="reqn">\Theta_n</code> (see section <code>Details</code>). The coefficients for the B-spline representation of each eligible functional index <code class="reqn">\theta \in \Theta_n</code> are obtained from <code>seed.coeff</code>.  The default is <code>c(-1,0,1)</code>.
</p>
</td></tr>
<tr><td><code id="IASSMR.kernel.fit_+3A_order.bspline">order.Bspline</code></td>
<td>

<p>Positive integer giving the order of the B-spline basis functions. This is the number of coefficients in each piecewise polynomial segment. The default is 3.
</p>
</td></tr>
<tr><td><code id="IASSMR.kernel.fit_+3A_nknot.theta">nknot.theta</code></td>
<td>

<p>Positive integer indicating the number of regularly spaced interior knots in the B-spline expansion of <code class="reqn">\theta_0</code>. The default is 3.
</p>
</td></tr>
<tr><td><code id="IASSMR.kernel.fit_+3A_min.q.h">min.q.h</code></td>
<td>
<p>Minimum quantile order of the distances between curves, which are computed using the projection semi-metric. This value determines the lower endpoint of the range from which the bandwidth is selected. The default is 0.05.
</p>
</td></tr>
<tr><td><code id="IASSMR.kernel.fit_+3A_max.q.h">max.q.h</code></td>
<td>

<p>Maximum quantile order of the distances between curves, which are computed using the projection semi-metric. This value determines the upper endpoint of the range from which the bandwidth is selected. The default is 0.5.
</p>
</td></tr>
<tr><td><code id="IASSMR.kernel.fit_+3A_h.seq">h.seq</code></td>
<td>
<p>Vector containing the sequence of bandwidths. The default is a sequence of <code>num.h</code> equispaced bandwidths in the range constructed using <code>min.q.h</code> and <code>max.q.h</code>.
</p>
</td></tr>
<tr><td><code id="IASSMR.kernel.fit_+3A_num.h">num.h</code></td>
<td>
<p>Positive integer indicating the number of bandwidths in the grid. The default is 10.
</p>
</td></tr>
<tr><td><code id="IASSMR.kernel.fit_+3A_range.grid">range.grid</code></td>
<td>

<p>Vector of length 2 containing the endpoints of the grid at which the observations of the functional covariate <code>x</code> are evaluated (i.e. the range of the discretisation). If <code>range.grid=NULL</code>, then <code>range.grid=c(1,p)</code> is considered, where <code>p</code> is the discretisation size of <code>x</code> (i.e. <code>ncol(x))</code>.
</p>
</td></tr>
<tr><td><code id="IASSMR.kernel.fit_+3A_kind.of.kernel">kind.of.kernel</code></td>
<td>

<p>The type of kernel function used. Currently, only Epanechnikov kernel (<code>"quad"</code>) is available.
</p>
</td></tr>
<tr><td><code id="IASSMR.kernel.fit_+3A_nknot">nknot</code></td>
<td>

<p>Positive integer indicating the number of interior knots for the B-spline expansion of the functional covariate. The default value is <code>(p - order.Bspline - 1)%/%2</code>.
</p>
</td></tr>
<tr><td><code id="IASSMR.kernel.fit_+3A_lambda.min">lambda.min</code></td>
<td>

<p>The smallest value for lambda (i. e., the lower endpoint  of the sequence in which <code>lambda.opt</code> is selected), as fraction of <code>lambda.max</code>.
The defaults is <code>lambda.min.l</code> if the sample size is larger than <code>factor.pn</code> times the number of linear covariates and <code>lambda.min.h</code> otherwise.
</p>
</td></tr>
<tr><td><code id="IASSMR.kernel.fit_+3A_lambda.min.h">lambda.min.h</code></td>
<td>

<p>The lower endpoint of the sequence in which <code>lambda.opt</code> is selected if the sample size is smaller than <code>factor.pn</code> times the number of linear covariates. The default is 0.05. 
</p>
</td></tr>
<tr><td><code id="IASSMR.kernel.fit_+3A_lambda.min.l">lambda.min.l</code></td>
<td>

<p>The lower endpoint of the sequence in which <code>lambda.opt</code> is selected if the sample size is larger than <code>factor.pn</code> times the number of linear covariates. The default is 0.0001.
</p>
</td></tr>
<tr><td><code id="IASSMR.kernel.fit_+3A_factor.pn">factor.pn</code></td>
<td>

<p>Positive integer used to set <code>lambda.min</code>. The default value is 1.
</p>
</td></tr>
<tr><td><code id="IASSMR.kernel.fit_+3A_nlambda">nlambda</code></td>
<td>

<p>Positive integer indicating the number of values in the sequence from which <code>lambda.opt</code> is selected. The default is 100.
</p>
</td></tr>
<tr><td><code id="IASSMR.kernel.fit_+3A_vn">vn</code></td>
<td>

<p>Positive integer or vector of positive integers indicating the number of groups of consecutive variables to be penalised together. The default value is <code>vn=ncol(z)</code>, resulting in the individual penalization of each scalar covariate.
</p>
</td></tr>
<tr><td><code id="IASSMR.kernel.fit_+3A_nfolds">nfolds</code></td>
<td>

<p>Number of cross-validation folds (used when <code>criterion="k-fold-CV"</code>). Default is 10.
</p>
</td></tr>
<tr><td><code id="IASSMR.kernel.fit_+3A_seed">seed</code></td>
<td>

<p>You may set the seed for the random number generator to ensure reproducible results (applicable when <code>criterion="k-fold-CV"</code> is used). The default seed value is 123.
</p>
</td></tr>
<tr><td><code id="IASSMR.kernel.fit_+3A_wn">wn</code></td>
<td>

<p>A vector of positive integers indicating the eligible number of covariates in the reduced model. For more information, refer to the section <code>Details</code>. The default is <code>c(10,15,20)</code>.
</p>
</td></tr>
<tr><td><code id="IASSMR.kernel.fit_+3A_criterion">criterion</code></td>
<td>

<p>The criterion used to select the tuning and regularisation parameters: <code>wn.opt</code>, <code>lambda.opt</code> and <code>h.opt</code> (also <code>vn.opt</code> if needed). Options include <code>"GCV"</code>, <code>"BIC"</code>, <code>"AIC"</code>, or <code>"k-fold-CV"</code>. The default setting is <code>"GCV"</code>.
</p>
</td></tr>
<tr><td><code id="IASSMR.kernel.fit_+3A_penalty">penalty</code></td>
<td>

<p>The penalty function applied in the penalised least-squares procedure. Currently, only &quot;grLasso&quot; and &quot;grSCAD&quot; are implemented. The default is &quot;grSCAD&quot;.
</p>
</td></tr>
<tr><td><code id="IASSMR.kernel.fit_+3A_max.iter">max.iter</code></td>
<td>

<p>Maximum number of iterations allowed across the entire path. The default value is 1000.
</p>
</td></tr>
<tr><td><code id="IASSMR.kernel.fit_+3A_n.core">n.core</code></td>
<td>

<p>Number of CPU cores designated for parallel execution. The default is <code>n.core&lt;-availableCores(omit=1)</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The multi-functional partial linear single-index model (MFPLSIM) is given by the expression
</p>
<p style="text-align: center;"><code class="reqn">Y_i=\sum_{j=1}^{p_n}\beta_{0j}\zeta_i(t_j)+r\left(\left&lt;\theta_0,X_i\right&gt;\right)+\varepsilon_i,\ \ \ (i=1,\dots,n),</code>
</p>

<p>where: 
</p>

<ul>
<li> <p><code class="reqn">Y_i</code> represents a real random response and <code class="reqn">X_i</code> denotes a random element belonging to some separable Hilbert space <code class="reqn">\mathcal{H}</code> with inner product denoted by <code class="reqn">\left\langle\cdot,\cdot\right\rangle</code>. The second functional predictor <code class="reqn">\zeta_i</code> is assumed to be a curve defined on the interval <code class="reqn">[a,b]</code>, observed at the points <code class="reqn">a\leq t_1&lt;\dots&lt;t_{p_n}\leq b</code>. 
</p>
</li>
<li>  <p><code class="reqn">\mathbf{\beta}_0=(\beta_{01},\dots,\beta_{0p_n})^{\top}</code> is a vector of unknown real coefficients, and <code class="reqn">r(\cdot)</code> denotes a smooth unknown link function. In addition, <code class="reqn">\theta_0</code> is an unknown functional direction in <code class="reqn">\mathcal{H}</code>.  
</p>
</li>
<li> <p><code class="reqn">\varepsilon_i</code> denotes the random error.
</p>
</li></ul>

<p>In  the MFPLSIM, it is assumed that only a few scalar variables from the set <code class="reqn">\{\zeta(t_1),\dots,\zeta(t_{p_n})\}</code> are part of the model. Therefore, the relevant variables in the linear component (the impact points of the curve <code class="reqn">\zeta</code> on the response) must be selected, and the model estimated.
</p>
<p>In this function, the MFPLSIM is fitted using the IASSMR. The IASSMR is a  two-step procedure. For this, we divide the sample into two independent subsamples, each asymptotically half the size of the original sample (<code class="reqn">n_1\sim n_2\sim n/2</code>). One subsample is used in the first stage of the method, and the other in the second stage.The subsamples are defined as follows:
</p>
<p style="text-align: center;"><code class="reqn">
\mathcal{E}^{\mathbf{1}}=\{(\zeta_i,\mathcal{X}_i,Y_i),\quad i=1,\dots,n_1\},
</code>
</p>

<p style="text-align: center;"><code class="reqn">
\mathcal{E}^{\mathbf{2}}=\{(\zeta_i,\mathcal{X}_i,Y_i),\quad i=n_1+1,\dots,n_1+n_2=n\}.
</code>
</p>
 
<p>Note that these two subsamples are specified to the program through the arguments <code>train.1</code> and <code>train.2</code>. The superscript <code class="reqn">\mathbf{s}</code>, where <code class="reqn">\mathbf{s}=\mathbf{1},\mathbf{2}</code>, indicates the stage of the method in which the sample, function, variable, or parameter is involved.
</p>
<p>To explain the algorithm, we assume that the number <code class="reqn">p_n</code> of linear covariates can be expressed as follows: <code class="reqn">p_n=q_nw_n</code>, with <code class="reqn">q_n</code> and <code class="reqn">w_n</code> being integers.
</p>

<ol>
<li> <p><b>First step</b>. The FASSMR (see <code><a href="#topic+FASSMR.kernel.fit">FASSMR.kernel.fit</a></code>) combined with kernel estimation is applied using only the subsample <code class="reqn">\mathcal{E}^{\mathbf{1}}</code>. Specifically:
</p>

<ul>
<li><p> Consider a subset of the initial <code class="reqn">p_n</code> linear covariates, which contains only <code class="reqn">w_n</code> equally spaced discretized observations of  <code class="reqn">\zeta</code> covering the interval <code class="reqn">[a,b]</code>. This subset is the following:
</p>
<p style="text-align: center;"><code class="reqn">
	\mathcal{R}_n^{\mathbf{1}}=\left\{\zeta\left(t_k^{\mathbf{1}}\right),\ \ k=1,\dots,w_n\right\},
</code>
</p>
 
<p>where  <code class="reqn">t_k^{\mathbf{1}}=t_{\left[(2k-1)q_n/2\right]}</code> and  <code class="reqn">\left[z\right]</code> denotes the smallest integer not less than the real number <code class="reqn">z</code>.The size (cardinality) of this subset is provided to the program in the argument <code>wn</code> (which contains a sequence of eligible sizes).
</p>
</li>
<li><p> Consider the following reduced model, which involves only the <code class="reqn">w_n</code> linear covariates belonging to <code class="reqn">\mathcal{R}_n^{\mathbf{1}}</code>:
</p>
<p style="text-align: center;"><code class="reqn">
	Y_i=\sum_{k=1}^{w_n}\beta_{0k}^{\mathbf{1}}\zeta_i(t_k^{\mathbf{1}})+r^{\mathbf{1}}\left(\left&lt;\theta_0^{\mathbf{1}},X_i\right&gt;\right)+\varepsilon_i^{\mathbf{1}}.
</code>
</p>

<p>The penalised least-squares variable selection procedure, with kernel estimation, is applied to the reduced model. This is done using the function <code><a href="#topic+sfplsim.kernel.fit">sfplsim.kernel.fit</a></code>, which requires the remaining arguments (see <code><a href="#topic+sfplsim.kernel.fit">sfplsim.kernel.fit</a></code>). The estimates obtained after that are the outputs of the first step of the algorithm.
</p>
</li></ul>

</li>
<li> <p><b>Second step</b>. The variables selected in the first step, along with those in their neighborhood, are included. The penalised least-squares procedure, combined with kernel estimation, is carried out again considering only the subsample <code class="reqn">\mathcal{E}^{\mathbf{2}}</code>. Specifically:
</p>

<ul>
<li><p> Consider a new set of variables:
</p>
<p style="text-align: center;"><code class="reqn">
		\mathcal{R}_n^{\mathbf{2}}=\bigcup_{\left\{k,\widehat{\beta}_{0k}^{\mathbf{1}}\not=0\right\}}\left\{\zeta(t_{(k-1)q_n+1}),\dots,\zeta(t_{kq_n})\right\}.
	</code>
</p>

<p>Denoting by <code class="reqn">r_n=\sharp(\mathcal{R}_n^{\mathbf{2}})</code>, the variables in <code class="reqn">\mathcal{R}_n^{\mathbf{2}}</code> can be renamed as follows:
</p>
<p style="text-align: center;"><code class="reqn">
		\mathcal{R}_n^{\mathbf{2}}=\left\{\zeta(t_1^{\mathbf{2}}),\dots,\zeta(t_{r_n}^{\mathbf{2}})\right\},
		</code>
</p>

</li>
<li><p>  Consider the following model, which involves only the linear covariates belonging to <code class="reqn">\mathcal{R}_n^{\mathbf{2}}</code>
</p>
<p style="text-align: center;"><code class="reqn">
		Y_i=\sum_{k=1}^{r_n}\beta_{0k}^{\mathbf{2}}\zeta_i(t_k^{\mathbf{2}})+r^{\mathbf{2}}\left(\left&lt;\theta_0^{\mathbf{2}},X_i\right&gt;\right)+\varepsilon_i^{\mathbf{2}}.</code>
</p>

<p>The penalised least-squares variable selection procedure, with kernel estimation, is applied to this model using the function <code><a href="#topic+sfplsim.kernel.fit">sfplsim.kernel.fit</a></code>. 
</p>
</li></ul>

</li></ol>

<p>The outputs of the second step are the estimates of the MFPLSIM. For further details on this algorithm, see Novo et al. (2021).
</p>
<p><b>Remark</b>: If the condition  <code class="reqn">p_n=w_n q_n</code> is not met (then <code class="reqn">p_n/w_n</code> is not an integer), the function considers variable  <code class="reqn">q_n=q_{n,k}</code> values <code class="reqn">k=1,\dots,w_n</code>. Specifically:
</p>
<p style="text-align: center;"><code class="reqn">
	q_{n,k}= \left\{\begin{array}{ll}
	[p_n/w_n]+1 &amp;   k\in\{1,\dots,p_n-w_n[p_n/w_n]\},\\
	{[p_n/w_n]} &amp; k\in\{p_n-w_n[p_n/w_n]+1,\dots,w_n\},
	\end{array}
	\right.
</code>
</p>

<p>where <code class="reqn">[z]</code> denotes the integer part of the real number <code class="reqn">z</code>.
</p>
<p>The function supports parallel computation. To avoid it, we can set <code>n.core=1</code>.
</p>


<h3>Value</h3>

<table>
<tr><td><code>call</code></td>
<td>
<p>The matched call.</p>
</td></tr>
<tr><td><code>fitted.values</code></td>
<td>
<p>Estimated scalar response.</p>
</td></tr>
<tr><td><code>residuals</code></td>
<td>
<p>Differences between <code>y</code> and the <code>fitted.values</code>.</p>
</td></tr>
<tr><td><code>beta.est</code></td>
<td>
<p><code class="reqn">\hat{\mathbf{\beta}}</code> (i.e. estimate of <code class="reqn">\mathbf{\beta}_0</code> when the optimal tuning parameters <code>w.opt</code>, <code>lambda.opt</code>, <code>h.opt</code> and <code>vn.opt</code> are used).</p>
</td></tr>
<tr><td><code>theta.est</code></td>
<td>
<p>Coefficients of <code class="reqn">\hat{\theta}</code> in the B-spline basis (i.e. estimate of <code class="reqn">\theta_0</code>when the optimal tuning parameters <code>w.opt</code>, <code>lambda.opt</code>, <code>h.opt</code> and <code>vn.opt</code> are used): a vector of <code>length(order.Bspline+nknot.theta)</code>.</p>
</td></tr>
<tr><td><code>indexes.beta.nonnull</code></td>
<td>
<p>Indexes of the non-zero <code class="reqn">\hat{\beta_{j}}</code>.</p>
</td></tr>
<tr><td><code>h.opt</code></td>
<td>
<p>Selected bandwidth (when <code>w.opt</code> is considered).</p>
</td></tr>
<tr><td><code>w.opt</code></td>
<td>
<p>Selected size for <code class="reqn">\mathcal{R}_n^{\mathbf{1}}</code>.</p>
</td></tr>
<tr><td><code>lambda.opt</code></td>
<td>
<p>Selected value of the penalisation parameter <code class="reqn">\lambda</code> (when <code>w.opt</code> is considered).</p>
</td></tr>
<tr><td><code>IC</code></td>
<td>
<p>Value of the criterion function considered to select <code>w.opt</code>, <code>lambda.opt</code>, <code>h.opt</code> and <code>vn.opt</code>.</p>
</td></tr>
<tr><td><code>vn.opt</code></td>
<td>
<p>Selected value of <code>vn</code> in the second step (when <code>w.opt</code> is considered).</p>
</td></tr>
<tr><td><code>beta2</code></td>
<td>
<p>Estimate of <code class="reqn">\mathbf{\beta}_0^{\mathbf{2}}</code> for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>theta2</code></td>
<td>
<p>Estimate of <code class="reqn">\theta_0^{\mathbf{2}}</code> for each value of the sequence <code>wn</code> (i.e. its coefficients in the B-spline basis).</p>
</td></tr> 
<tr><td><code>indexes.beta.nonnull2</code></td>
<td>
<p>Indexes of the non-zero linear coefficients after the step 2 of the method for each value of the sequence <code>wn</code>.</p>
</td></tr> 
<tr><td><code>h2</code></td>
<td>
<p>Selected bandwidth in the second step of the algorithm for each value of the sequence <code>wn</code>.</p>
</td></tr> 
<tr><td><code>IC2</code></td>
<td>
<p>Optimal value of the criterion function in the second step for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>lambda2</code></td>
<td>
<p>Selected value of penalisation parameter in the second step for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>index02</code></td>
<td>
<p>Indexes of the covariates (in the entire set of <code class="reqn">p_n</code>) used to build <code class="reqn">\mathcal{R}_n^{\mathbf{2}}</code> for each value of the sequence <code>wn</code>.</p>
</td></tr> 
<tr><td><code>beta1</code></td>
<td>
<p>Estimate of <code class="reqn">\mathbf{\beta}_0^{\mathbf{1}}</code> for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>theta1</code></td>
<td>
<p>Estimate of <code class="reqn">\theta_0^{\mathbf{1}}</code> for each value of the sequence <code>wn</code> (i.e. its coefficients in the B-spline basis).</p>
</td></tr>
<tr><td><code>h1</code></td>
<td>
<p>Selected bandwidth in the first step of the algorithm for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>IC1</code></td>
<td>
<p>Optimal value of the criterion function in the first step for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>lambda1</code></td>
<td>
<p>Selected value of penalisation parameter in the first step for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>index01</code></td>
<td>
<p>Indexes of the covariates (in the whole set of <code class="reqn">p_n</code>) used to build <code class="reqn">\mathcal{R}_n^{\mathbf{1}}</code> for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>index1</code></td>
<td>
<p>Indexes of the non-zero linear coefficients after the step 1 of the method for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>...</code></td>
<td>
<p>Further outputs to apply S3 methods.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>German Aneiros Perez <a href="mailto:german.aneiros@udc.es">german.aneiros@udc.es</a> 
</p>
<p>Silvia Novo Diaz  <a href="mailto:snovo@est-econ.uc3m.es">snovo@est-econ.uc3m.es</a>
</p>


<h3>References</h3>

<p>Novo, S., Vieu, P., and Aneiros, G., (2021) Fast and efficient algorithms for
sparse semiparametric bi-functional regression. <em>Australian and New Zealand
Journal of Statistics</em>, <b>63</b>, 606&ndash;638, <a href="https://doi.org/10.1111/anzs.12355">doi:10.1111/anzs.12355</a>.
</p>


<h3>See Also</h3>

<p>See also <code><a href="#topic+sfplsim.kernel.fit">sfplsim.kernel.fit</a></code>, <code><a href="#topic+predict.IASSMR.kernel">predict.IASSMR.kernel</a></code>, <code><a href="#topic+plot.IASSMR.kernel">plot.IASSMR.kernel</a></code> and <code><a href="#topic+FASSMR.kernel.fit">FASSMR.kernel.fit</a></code>.
</p>
<p>Alternative methods <code><a href="#topic+IASSMR.kNN.fit">IASSMR.kNN.fit</a></code>, <code><a href="#topic+FASSMR.kernel.fit">FASSMR.kernel.fit</a></code> and <code><a href="#topic+FASSMR.kNN.fit">FASSMR.kNN.fit</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(Sugar)

y&lt;-Sugar$ash
x&lt;-Sugar$wave.290
z&lt;-Sugar$wave.240

#Outliers
index.y.25 &lt;- y &gt; 25
index.atip &lt;- index.y.25
(1:268)[index.atip]

#Dataset to model
x.sug &lt;- x[!index.atip,]
z.sug&lt;- z[!index.atip,]
y.sug &lt;- y[!index.atip]

train&lt;-1:216

ptm=proc.time()
fit&lt;- IASSMR.kernel.fit(x=x.sug[train,],z=z.sug[train,], y=y.sug[train],
        train.1=1:108,train.2=109:216,nknot.theta=2,lambda.min.h=0.03,
        lambda.min.l=0.03,  max.q.h=0.35, nknot=20,
        criterion="BIC", max.iter=5000)
proc.time()-ptm

fit 
names(fit)

</code></pre>

<hr>
<h2 id='IASSMR.kNN.fit'>
Impact point selection with IASSMR and kNN estimation
</h2><span id='topic+IASSMR.kNN.fit'></span>

<h3>Description</h3>

<p>This function implements the Improved Algorithm for Sparse Semiparametric Multi-functional Regression (IASSMR) with kNN estimation. This algorithm is specifically designed for estimating multi-functional partial linear single-index models, which incorporate multiple scalar variables and a functional covariate as predictors. These scalar variables are derived from the discretisation of a curve and have linear effects while the functional covariate exhibits a single-index effect. 
</p>
<p>IASSMR is a two-stage procedure that selects the impact points of the discretised curve and estimates the model. The algorithm employs a penalised least-squares regularisation procedure, integrated with kNN estimation using Nadaraya-Watson weights. It uses B-spline expansions to represent curves and eligible functional indexes. Additionally, it utilises an objective criterion (<code>criterion</code>) to determine the initial number of covariates in the reduced model (<code>w.opt</code>), the number of neighbours (<code>k.opt</code>), and the penalisation parameter (<code>lambda.opt</code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>IASSMR.kNN.fit(x, z, y, train.1 = NULL, train.2 = NULL, 
seed.coeff = c(-1, 0, 1), order.Bspline = 3, nknot.theta = 3, knearest = NULL,
min.knn = 2, max.knn = NULL, step = NULL, range.grid = NULL, 
kind.of.kernel = "quad", nknot = NULL, lambda.min = NULL, lambda.min.h = NULL, 
lambda.min.l = NULL, factor.pn = 1, nlambda = 100, vn = ncol(z), nfolds = 10, 
seed = 123, wn = c(10, 15, 20), criterion = "GCV", penalty = "grSCAD", 
max.iter = 1000, n.core = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="IASSMR.kNN.fit_+3A_x">x</code></td>
<td>

<p>Matrix containing the observations of the functional covariate collected by row (functional single-index component).
</p>
</td></tr>
<tr><td><code id="IASSMR.kNN.fit_+3A_z">z</code></td>
<td>

<p>Matrix containing the observations of the functional covariate that is discretised collected by row (linear component).
</p>
</td></tr>
<tr><td><code id="IASSMR.kNN.fit_+3A_y">y</code></td>
<td>

<p>Vector containing the scalar response.
</p>
</td></tr>
<tr><td><code id="IASSMR.kNN.fit_+3A_train.1">train.1</code></td>
<td>

<p>Positions of the data that are used as the training sample in the 1st step. The default setting is  <code>train.1&lt;-1:ceiling(n/2)</code>.
</p>
</td></tr>
<tr><td><code id="IASSMR.kNN.fit_+3A_train.2">train.2</code></td>
<td>

<p>Positions of the data that are used as the training sample in the 2nd step. The default setting is <code>train.2&lt;-(ceiling(n/2)+1):n</code>.
</p>
</td></tr>
<tr><td><code id="IASSMR.kNN.fit_+3A_seed.coeff">seed.coeff</code></td>
<td>

<p>Vector of initial values used to  build the set <code class="reqn">\Theta_n</code> (see section <code>Details</code>). The coefficients for the B-spline representation of each eligible functional index <code class="reqn">\theta \in \Theta_n</code> are obtained from <code>seed.coeff</code>.  The default is <code>c(-1,0,1)</code>.
</p>
</td></tr>
<tr><td><code id="IASSMR.kNN.fit_+3A_order.bspline">order.Bspline</code></td>
<td>

<p>Positive integer giving the order of the B-spline basis functions. This is the number of coefficients in each piecewise polynomial segment. The default is 3.
</p>
</td></tr>
<tr><td><code id="IASSMR.kNN.fit_+3A_nknot.theta">nknot.theta</code></td>
<td>

<p>Positive integer indicating the number of regularly spaced interior knots in the B-spline expansion of <code class="reqn">\theta_0</code>. The default is 3.
</p>
</td></tr>
<tr><td><code id="IASSMR.kNN.fit_+3A_knearest">knearest</code></td>
<td>

<p>Vector of positive integers containing the sequence in which the  number of nearest neighbours <code>k.opt</code> is selected. If <code>knearest=NULL</code>, then <code>knearest &lt;- seq(from =min.knn, to = max.knn, by = step)</code>.
</p>
</td></tr>
<tr><td><code id="IASSMR.kNN.fit_+3A_min.knn">min.knn</code></td>
<td>

<p>A positive integer that represents the minimum value in the sequence for selecting the number of nearest neighbours <code>k.opt</code>. This value should be less than the sample size. The default is 2.
</p>
</td></tr>
<tr><td><code id="IASSMR.kNN.fit_+3A_max.knn">max.knn</code></td>
<td>

<p>A positive integer that represents the maximum value in the sequence for selecting number of nearest neighbours <code>k.opt</code>. This value should be less than the sample size. The default is <code>max.knn &lt;- n%/%5</code>.
</p>
</td></tr>
<tr><td><code id="IASSMR.kNN.fit_+3A_step">step</code></td>
<td>

<p>A positive integer used to construct the sequence of k-nearest neighbours as follows: <code>min.knn, min.knn + step, min.knn + 2*step, min.knn + 3*step,...</code>. The default value for <code>step</code> is <code>step&lt;-ceiling(n/100)</code>.
</p>
</td></tr>
<tr><td><code id="IASSMR.kNN.fit_+3A_range.grid">range.grid</code></td>
<td>

<p>Vector of length 2 containing the endpoints of the grid at which the observations of the functional covariate <code>x</code> are evaluated (i.e. the range of the discretisation). If <code>range.grid=NULL</code>, then <code>range.grid=c(1,p)</code> is considered, where <code>p</code> is the discretisation size of <code>x</code> (i.e. <code>ncol(x))</code>.
</p>
</td></tr>
<tr><td><code id="IASSMR.kNN.fit_+3A_kind.of.kernel">kind.of.kernel</code></td>
<td>

<p>The type of kernel function used. Currently, only Epanechnikov kernel (<code>"quad"</code>) is available.
</p>
</td></tr>
<tr><td><code id="IASSMR.kNN.fit_+3A_nknot">nknot</code></td>
<td>

<p>Positive integer indicating the number of interior knots for the B-spline expansion of the functional covariate. The default value is <code>(p - order.Bspline - 1)%/%2</code>.
</p>
</td></tr>
<tr><td><code id="IASSMR.kNN.fit_+3A_lambda.min">lambda.min</code></td>
<td>

<p>The smallest value for lambda (i. e., the lower endpoint  of the sequence in which <code>lambda.opt</code> is selected), as fraction of <code>lambda.max</code>.
The defaults is <code>lambda.min.l</code> if the sample size is larger than <code>factor.pn</code> times the number of linear covariates and <code>lambda.min.h</code> otherwise.
</p>
</td></tr>
<tr><td><code id="IASSMR.kNN.fit_+3A_lambda.min.h">lambda.min.h</code></td>
<td>

<p>The lower endpoint of the sequence in which <code>lambda.opt</code> is selected if the sample size is smaller than <code>factor.pn</code> times the number of linear covariates. The default is 0.05. 
</p>
</td></tr>
<tr><td><code id="IASSMR.kNN.fit_+3A_lambda.min.l">lambda.min.l</code></td>
<td>

<p>The lower endpoint of the sequence in which <code>lambda.opt</code> is selected if the sample size is larger than <code>factor.pn</code> times the number of linear covariates. The default is 0.0001.
</p>
</td></tr>
<tr><td><code id="IASSMR.kNN.fit_+3A_factor.pn">factor.pn</code></td>
<td>

<p>Positive integer used to set <code>lambda.min</code>. The default value is 1.
</p>
</td></tr>
<tr><td><code id="IASSMR.kNN.fit_+3A_nlambda">nlambda</code></td>
<td>

<p>Positive integer indicating the number of values in the sequence from which <code>lambda.opt</code> is selected. The default is 100.
</p>
</td></tr>
<tr><td><code id="IASSMR.kNN.fit_+3A_vn">vn</code></td>
<td>

<p>Positive integer or vector of positive integers indicating the number of groups of consecutive variables to be penalised together. The default value is <code>vn=ncol(z)</code>, resulting in the individual penalization of each scalar covariate.
</p>
</td></tr>
<tr><td><code id="IASSMR.kNN.fit_+3A_nfolds">nfolds</code></td>
<td>

<p>Number of cross-validation folds (used when <code>criterion="k-fold-CV"</code>). Default is 10.
</p>
</td></tr>
<tr><td><code id="IASSMR.kNN.fit_+3A_seed">seed</code></td>
<td>

<p>You may set the seed for the random number generator to ensure reproducible results (applicable when <code>criterion="k-fold-CV"</code> is used). The default seed value is 123.
</p>
</td></tr>
<tr><td><code id="IASSMR.kNN.fit_+3A_wn">wn</code></td>
<td>

<p>A vector of positive integers indicating the eligible number of covariates in the reduced model. For more information, refer to the section <code>Details</code>. The default is <code>c(10,15,20)</code>.
</p>
</td></tr>
<tr><td><code id="IASSMR.kNN.fit_+3A_criterion">criterion</code></td>
<td>

<p>The criterion used to select the tuning and regularisation parameters: <code>wn.opt</code>, <code>lambda.opt</code> and <code>k.opt</code> (also <code>vn.opt</code> if needed). Options include <code>"GCV"</code>, <code>"BIC"</code>, <code>"AIC"</code>, or <code>"k-fold-CV"</code>. The default setting is <code>"GCV"</code>.
</p>
</td></tr>
<tr><td><code id="IASSMR.kNN.fit_+3A_penalty">penalty</code></td>
<td>

<p>The penalty function applied in the penalised least-squares procedure. Currently, only &quot;grLasso&quot; and &quot;grSCAD&quot; are implemented. The default is &quot;grSCAD&quot;.
</p>
</td></tr>
<tr><td><code id="IASSMR.kNN.fit_+3A_max.iter">max.iter</code></td>
<td>

<p>Maximum number of iterations allowed across the entire path. The default value is 1000.
</p>
</td></tr>
<tr><td><code id="IASSMR.kNN.fit_+3A_n.core">n.core</code></td>
<td>

<p>Number of CPU cores designated for parallel execution. The default is <code>n.core&lt;-availableCores(omit=1)</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The multi-functional partial linear single-index model (MFPLSIM) is given by the expression
</p>
<p style="text-align: center;"><code class="reqn">Y_i=\sum_{j=1}^{p_n}\beta_{0j}\zeta_i(t_j)+r\left(\left&lt;\theta_0,X_i\right&gt;\right)+\varepsilon_i,\ \ \ (i=1,\dots,n),</code>
</p>

<p>where: 
</p>

<ul>
<li> <p><code class="reqn">Y_i</code> represents a real random response and <code class="reqn">X_i</code> denotes a random element belonging to some separable Hilbert space <code class="reqn">\mathcal{H}</code> with inner product denoted by <code class="reqn">\left\langle\cdot,\cdot\right\rangle</code>. The second functional predictor <code class="reqn">\zeta_i</code> is assumed to be a curve defined on the interval <code class="reqn">[a,b]</code>, observed at the points <code class="reqn">a\leq t_1&lt;\dots&lt;t_{p_n}\leq b</code>. 
</p>
</li>
<li>  <p><code class="reqn">\mathbf{\beta}_0=(\beta_{01},\dots,\beta_{0p_n})^{\top}</code> is a vector of unknown real coefficients, and <code class="reqn">r(\cdot)</code> denotes a smooth unknown link function. In addition, <code class="reqn">\theta_0</code> is an unknown functional direction in <code class="reqn">\mathcal{H}</code>.  
</p>
</li>
<li> <p><code class="reqn">\varepsilon_i</code> denotes the random error.
</p>
</li></ul>

<p>In  the MFPLSIM, it is assumed that only a few scalar variables from the set <code class="reqn">\{\zeta(t_1),\dots,\zeta(t_{p_n})\}</code> are part of the model. Therefore, the relevant variables in the linear component (the impact points of the curve <code class="reqn">\zeta</code> on the response) must be selected, and the model estimated.
</p>
<p>In this function, the MFPLSIM is fitted using the IASSMR. The IASSMR is a  two-step procedure. For this, we divide the sample into two independent subsamples, each asymptotically half the size of the original (<code class="reqn">n_1\sim n_2\sim n/2</code>). One subsample is used in the first stage of the method, and the other in the second stage.The subsamples are defined as follows:
</p>
<p style="text-align: center;"><code class="reqn">
\mathcal{E}^{\mathbf{1}}=\{(\zeta_i,\mathcal{X}_i,Y_i),\quad i=1,\dots,n_1\},
</code>
</p>

<p style="text-align: center;"><code class="reqn">
\mathcal{E}^{\mathbf{2}}=\{(\zeta_i,\mathcal{X}_i,Y_i),\quad i=n_1+1,\dots,n_1+n_2=n\}.
</code>
</p>
 
<p>Note that these two subsamples are specified in the program through the arguments <code>train.1</code> and <code>train.2</code>. The superscript <code class="reqn">\mathbf{s}</code>, where <code class="reqn">\mathbf{s}=\mathbf{1},\mathbf{2}</code>, indicates the stage of the method in which the sample, function, variable, or parameter is involved.
</p>
<p>To explain the algorithm, we assume that the number <code class="reqn">p_n</code> of linear covariates can be expressed as follows: <code class="reqn">p_n=q_nw_n</code>, with <code class="reqn">q_n</code> and <code class="reqn">w_n</code> being integers.
</p>

<ol>
<li> <p><b>First step</b>. The FASSMR (see <code><a href="#topic+FASSMR.kNN.fit">FASSMR.kNN.fit</a></code>) combined with kNN estimation is applied using only the subsample <code class="reqn">\mathcal{E}^{\mathbf{1}}</code>. Specifically:
</p>

<ul>
<li><p> Consider a subset of the initial <code class="reqn">p_n</code> linear covariates, which contains only <code class="reqn">w_n</code> equally spaced discretized observations of  <code class="reqn">\zeta</code> covering the entire interval  <code class="reqn">[a,b]</code>. This subset is the following:
</p>
<p style="text-align: center;"><code class="reqn">
	\mathcal{R}_n^{\mathbf{1}}=\left\{\zeta\left(t_k^{\mathbf{1}}\right),\ \ k=1,\dots,w_n\right\},
</code>
</p>
 
<p>where  <code class="reqn">t_k^{\mathbf{1}}=t_{\left[(2k-1)q_n/2\right]}</code> and  <code class="reqn">\left[z\right]</code> denotes the smallest integer not less than the real number <code class="reqn">z</code>.The size (cardinality) of this subset is provided to the program in the argument <code>wn</code> (which contains a sequence of eligible sizes).
</p>
</li>
<li><p> Consider the following reduced model, which involves only the <code class="reqn">w_n</code> linear covariates belonging to <code class="reqn">\mathcal{R}_n^{\mathbf{1}}</code>:
</p>
<p style="text-align: center;"><code class="reqn">
	Y_i=\sum_{k=1}^{w_n}\beta_{0k}^{\mathbf{1}}\zeta_i(t_k^{\mathbf{1}})+r^{\mathbf{1}}\left(\left&lt;\theta_0^{\mathbf{1}},X_i\right&gt;\right)+\varepsilon_i^{\mathbf{1}}.
</code>
</p>

<p>The penalised least-squares variable selection procedure, with kNN estimation, is applied to the reduced model. This is done using the function <code><a href="#topic+sfplsim.kNN.fit">sfplsim.kNN.fit</a></code>, which requires the remaining arguments (see <code><a href="#topic+sfplsim.kNN.fit">sfplsim.kNN.fit</a></code>). The estimates obtained after that are the outputs of the first step of the algorithm.
</p>
</li></ul>

</li>
<li> <p><b>Second step</b>. The variables selected in the first step, along with those in their neighborhood, are included. The penalised least-squares procedure, combined with kNN estimation, is carried out again considering only the subsample <code class="reqn">\mathcal{E}^{\mathbf{2}}</code>. Specifically:
</p>

<ul>
<li><p> Consider a new set of variables:
</p>
<p style="text-align: center;"><code class="reqn">
		\mathcal{R}_n^{\mathbf{2}}=\bigcup_{\left\{k,\widehat{\beta}_{0k}^{\mathbf{1}}\not=0\right\}}\left\{\zeta(t_{(k-1)q_n+1}),\dots,\zeta(t_{kq_n})\right\}.
	</code>
</p>

<p>Denoting by <code class="reqn">r_n=\sharp(\mathcal{R}_n^{\mathbf{2}})</code>, the variables in <code class="reqn">\mathcal{R}_n^{\mathbf{2}}</code> can be renamed as follows:
</p>
<p style="text-align: center;"><code class="reqn">
		\mathcal{R}_n^{\mathbf{2}}=\left\{\zeta(t_1^{\mathbf{2}}),\dots,\zeta(t_{r_n}^{\mathbf{2}})\right\},
		</code>
</p>

</li>
<li><p>  Consider the following model, which involves only the linear covariates belonging to <code class="reqn">\mathcal{R}_n^{\mathbf{2}}</code>
</p>
<p style="text-align: center;"><code class="reqn">
		Y_i=\sum_{k=1}^{r_n}\beta_{0k}^{\mathbf{2}}\zeta_i(t_k^{\mathbf{2}})+r^{\mathbf{2}}\left(\left&lt;\theta_0^{\mathbf{2}},X_i\right&gt;\right)+\varepsilon_i^{\mathbf{2}}.</code>
</p>

<p>The penalised least-squares variable selection procedure, with kNN estimation, is applied to this model using the function <code><a href="#topic+sfplsim.kNN.fit">sfplsim.kNN.fit</a></code>. 
</p>
</li></ul>

</li></ol>

<p>The outputs of the second step are the estimates of the MFPLSIM. For further details on this algorithm, see Novo et al. (2021).
</p>
<p><b>Remark</b>: If the condition  <code class="reqn">p_n=w_n q_n</code> is not met (then <code class="reqn">p_n/w_n</code> is not an integer number), the function considers variable  <code class="reqn">q_n=q_{n,k}</code> values <code class="reqn">k=1,\dots,w_n</code>. Specifically:
</p>
<p style="text-align: center;"><code class="reqn">
	q_{n,k}= \left\{\begin{array}{ll}
	[p_n/w_n]+1 &amp;   k\in\{1,\dots,p_n-w_n[p_n/w_n]\},\\
	{[p_n/w_n]} &amp; k\in\{p_n-w_n[p_n/w_n]+1,\dots,w_n\},
	\end{array}
	\right.
</code>
</p>

<p>where <code class="reqn">[z]</code> denotes the integer part of the real number <code class="reqn">z</code>.
</p>
<p>The function supports parallel computation. To avoid it, we can set <code>n.core=1</code>.
</p>


<h3>Value</h3>

<table>
<tr><td><code>call</code></td>
<td>
<p>The matched call.</p>
</td></tr>
<tr><td><code>fitted.values</code></td>
<td>
<p>Estimated scalar response.</p>
</td></tr>
<tr><td><code>residuals</code></td>
<td>
<p>Differences between <code>y</code> and the <code>fitted.values</code>.</p>
</td></tr>
<tr><td><code>beta.est</code></td>
<td>
<p><code class="reqn">\hat{\mathbf{\beta}}</code> (i.e. estimate of <code class="reqn">\mathbf{\beta}_0</code> when the optimal tuning parameters <code>w.opt</code>, <code>lambda.opt</code>, <code>vn.opt</code> and <code>k.opt</code> are used).</p>
</td></tr>
<tr><td><code>theta.est</code></td>
<td>
<p>Coefficients of <code class="reqn">\hat{\theta}</code> in the B-spline basis (i.e. estimate of <code class="reqn">\theta_0</code> when the optimal tuning parameters <code>w.opt</code>, <code>lambda.opt</code>, <code>vn.opt</code> and <code>k.opt</code> are used): a vector of <code>length(order.Bspline+nknot.theta)</code>.</p>
</td></tr>
<tr><td><code>indexes.beta.nonnull</code></td>
<td>
<p>Indexes of the non-zero <code class="reqn">\hat{\beta_{j}}</code>.</p>
</td></tr>
<tr><td><code>k.opt</code></td>
<td>
<p>Selected number of nearest neighbours (when <code>w.opt</code> is considered).</p>
</td></tr>
<tr><td><code>w.opt</code></td>
<td>
<p>Selected initial number of covariates in the reduced model.</p>
</td></tr>
<tr><td><code>lambda.opt</code></td>
<td>
<p>Selected value of the penalisation parameter <code class="reqn">\lambda</code> (when <code>w.opt</code> is considered).</p>
</td></tr>
<tr><td><code>IC</code></td>
<td>
<p>Value of the criterion function considered to select <code>w.opt</code>, <code>lambda.opt</code>, <code>vn.opt</code> and <code>k.opt</code>.</p>
</td></tr>
<tr><td><code>vn.opt</code></td>
<td>
<p>Selected value of <code>vn</code> in the second step (when <code>w.opt</code> is considered).</p>
</td></tr>
<tr><td><code>beta2</code></td>
<td>
<p>Estimate of <code class="reqn">\mathbf{\beta}_0^{\mathbf{2}}</code> for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>theta2</code></td>
<td>
<p>Estimate of <code class="reqn">\theta_0^{\mathbf{2}}</code> for each value of the sequence <code>wn</code> (i.e. its coefficients in the B-spline basis).</p>
</td></tr> 
<tr><td><code>indexes.beta.nonnull2</code></td>
<td>
<p>Indexes of the non-zero linear coefficients after the step 2 of the method for each value of the sequence <code>wn</code>.</p>
</td></tr> 
<tr><td><code>knn2</code></td>
<td>
<p>Selected number of neighbours in the second step of the algorithm for each value of the sequence <code>wn</code>.</p>
</td></tr> 
<tr><td><code>IC2</code></td>
<td>
<p>Optimal value of the criterion function in the second step for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>lambda2</code></td>
<td>
<p>Selected value of penalisation parameter in the second step for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>index02</code></td>
<td>
<p>Indexes of the covariates (in the entire set of <code class="reqn">p_n</code>) used to build <code class="reqn">\mathcal{R}_n^{\mathbf{2}}</code> for each value of the sequence <code>wn</code>.</p>
</td></tr> 
<tr><td><code>beta1</code></td>
<td>
<p>Estimate of <code class="reqn">\mathbf{\beta}_0^{\mathbf{1}}</code> for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>theta1</code></td>
<td>
<p>Estimate of <code class="reqn">\theta_0^{\mathbf{1}}</code> for each value of the sequence <code>wn</code> (i.e. its coefficients in the B-spline basis).</p>
</td></tr>
<tr><td><code>knn1</code></td>
<td>
<p>Selected number of neighbours in the first step of the algorithm for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>IC1</code></td>
<td>
<p>Optimal value of the criterion function in the first step for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>lambda1</code></td>
<td>
<p>Selected value of penalisation parameter in the first step for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>index01</code></td>
<td>
<p>Indexes of the covariates (in the whole set of <code class="reqn">p_n</code>) used to build <code class="reqn">\mathcal{R}_n^{\mathbf{1}}</code> for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>index1</code></td>
<td>
<p>Indexes of the non-zero linear coefficients after the step 1 of the method for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>...</code></td>
<td>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>German Aneiros Perez <a href="mailto:german.aneiros@udc.es">german.aneiros@udc.es</a> 
</p>
<p>Silvia Novo Diaz  <a href="mailto:snovo@est-econ.uc3m.es">snovo@est-econ.uc3m.es</a>
</p>


<h3>References</h3>

<p>Novo, S., Vieu, P., and Aneiros, G., (2021) Fast and efficient algorithms for
sparse semiparametric bi-functional regression. <em>Australian and New Zealand
Journal of Statistics</em>, <b>63</b>, 606&ndash;638, <a href="https://doi.org/10.1111/anzs.12355">doi:10.1111/anzs.12355</a>.
</p>


<h3>See Also</h3>

<p>See also <code><a href="#topic+sfplsim.kNN.fit">sfplsim.kNN.fit</a>, <a href="#topic+predict.IASSMR.kNN">predict.IASSMR.kNN</a></code>, <code><a href="#topic+plot.IASSMR.kNN">plot.IASSMR.kNN</a></code> and <code><a href="#topic+FASSMR.kNN.fit">FASSMR.kNN.fit</a></code>.
</p>
<p>Alternative method <code><a href="#topic+IASSMR.kernel.fit">IASSMR.kernel.fit</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(Sugar)

y&lt;-Sugar$ash
x&lt;-Sugar$wave.290
z&lt;-Sugar$wave.240

#Outliers
index.y.25 &lt;- y &gt; 25
index.atip &lt;- index.y.25
(1:268)[index.atip]

#Dataset to model
x.sug &lt;- x[!index.atip,]
z.sug&lt;- z[!index.atip,]
y.sug &lt;- y[!index.atip]

train&lt;-1:216

ptm=proc.time()
fit&lt;- IASSMR.kNN.fit(x=x.sug[train,],z=z.sug[train,], y=y.sug[train],
        train.1=1:108,train.2=109:216,nknot.theta=2,lambda.min.h=0.07, 
        lambda.min.l=0.07, max.knn=20, nknot=20,criterion="BIC", max.iter=5000)
proc.time()-ptm

fit 
names(fit)

</code></pre>

<hr>
<h2 id='lm.pels.fit'>
Regularised fit of sparse linear regression
</h2><span id='topic+lm.pels.fit'></span>

<h3>Description</h3>

<p>This function fits a sparse linear model between a scalar response and a vector of scalar covariates. It employs a penalised least-squares regularisation procedure, with either (group)SCAD or (group)LASSO penalties. The method utilises an objective criterion (<code>criterion</code>) to select the optimal regularisation parameter (<code>lambda.opt</code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lm.pels.fit(z, y, lambda.min = NULL, lambda.min.h = NULL, lambda.min.l = NULL,
factor.pn = 1, nlambda = 100, lambda.seq = NULL, vn = ncol(z), nfolds = 10, 
seed = 123, criterion = "GCV", penalty = "grSCAD", max.iter = 1000)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lm.pels.fit_+3A_z">z</code></td>
<td>

<p>Matrix containing the observations of the covariates collected by row.
</p>
</td></tr>
<tr><td><code id="lm.pels.fit_+3A_y">y</code></td>
<td>

<p>Vector containing the scalar response.
</p>
</td></tr>
<tr><td><code id="lm.pels.fit_+3A_lambda.min">lambda.min</code></td>
<td>

<p>The smallest value for lambda (i. e., the lower endpoint  of the sequence in which <code>lambda.opt</code> is selected), as fraction of <code>lambda.max</code>.
The defaults is <code>lambda.min.l</code> if the sample size is larger than <code>factor.pn</code> times the number of linear covariates and <code>lambda.min.h</code> otherwise.
</p>
</td></tr>
<tr><td><code id="lm.pels.fit_+3A_lambda.min.h">lambda.min.h</code></td>
<td>

<p>The lower endpoint of the sequence in which <code>lambda.opt</code> is selected if the sample size is smaller than <code>factor.pn</code> times the number of linear covariates. The default is 0.05. 
</p>
</td></tr>
<tr><td><code id="lm.pels.fit_+3A_lambda.min.l">lambda.min.l</code></td>
<td>

<p>The lower endpoint of the sequence in which <code>lambda.opt</code> is selected if the sample size is larger than <code>factor.pn</code> times the number of linear covariates. The default is 0.0001.
</p>
</td></tr>
<tr><td><code id="lm.pels.fit_+3A_factor.pn">factor.pn</code></td>
<td>

<p>Positive integer used to set <code>lambda.min</code>. The default value is 1.
</p>
</td></tr>
<tr><td><code id="lm.pels.fit_+3A_nlambda">nlambda</code></td>
<td>

<p>Positive integer indicating the number of values in the sequence from which <code>lambda.opt</code> is selected. The default is 100.
</p>
</td></tr>
<tr><td><code id="lm.pels.fit_+3A_lambda.seq">lambda.seq</code></td>
<td>

<p>Sequence of values in which <code>lambda.opt</code> is selected. If <code>lambda.seq=NULL</code>, then the programme builds the sequence automatically using <code>lambda.min</code> and <code>nlambda</code>.
</p>
</td></tr>
<tr><td><code id="lm.pels.fit_+3A_vn">vn</code></td>
<td>

<p>Positive integer or vector of positive integers indicating the number of groups of consecutive variables to be penalised together. The default value is <code>vn=ncol(z)</code>, resulting in the individual penalization of each scalar covariate.
</p>
</td></tr>
<tr><td><code id="lm.pels.fit_+3A_nfolds">nfolds</code></td>
<td>

<p>Number of cross-validation folds (used when <code>criterion="k-fold-CV"</code>). Default is 10.
</p>
</td></tr>
<tr><td><code id="lm.pels.fit_+3A_seed">seed</code></td>
<td>

<p>You may set the seed for the random number generator to ensure reproducible results (applicable when <code>criterion="k-fold-CV"</code> is used). The default seed value is 123.
</p>
</td></tr>
<tr><td><code id="lm.pels.fit_+3A_criterion">criterion</code></td>
<td>

<p>The criterion used to select the regularisation parameter <code>lambda.opt</code> (also <code>vn.opt</code> if needed). Options include <code>"GCV"</code>, <code>"BIC"</code>, <code>"AIC"</code>, or <code>"k-fold-CV"</code>. The default setting is <code>"GCV"</code>.
</p>
</td></tr>
<tr><td><code id="lm.pels.fit_+3A_penalty">penalty</code></td>
<td>

<p>The penalty function applied in the penalised least-squares procedure. Currently, only &quot;grLasso&quot; and &quot;grSCAD&quot; are implemented. The default is &quot;grSCAD&quot;.
</p>
</td></tr>
<tr><td><code id="lm.pels.fit_+3A_max.iter">max.iter</code></td>
<td>

<p>Maximum number of iterations allowed across the entire path. The default value is 1000.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The sparse linear model (SLM) is given by the expression:
</p>
<p style="text-align: center;"><code class="reqn">
Y_i=Z_{i1}\beta_{01}+\dots+Z_{ip_n}\beta_{0p_n}+\varepsilon_i\ \ \ i=1,\dots,n,
</code>
</p>

<p>where <code class="reqn">Y_i</code> denotes a scalar response, <code class="reqn">Z_{i1},\dots,Z_{ip_n}</code> are real covariates. In this equation, <code class="reqn">\mathbf{\beta}_0=(\beta_{01},\dots,\beta_{0p_n})^{\top}</code> is a vector of unknown real parameters and <code class="reqn">\varepsilon_i</code> represents the random error.
</p>
<p>In this function, the SLM is fitted using a penalised least-squares (PeLS) approach by minimising 
</p>
<p style="text-align: center;"><code class="reqn">
\mathcal{Q}\left(\mathbf{\beta}\right)=\frac{1}{2}\left(\mathbf{Y}-\mathbf{Z}\mathbf{\beta}\right)^{\top}\left(\mathbf{Y}-\mathbf{Z}\mathbf{\beta}\right)+n\sum_{j=1}^{p_n}\mathcal{P}_{\lambda_{j_n}}\left(|\beta_j|\right), \quad (1)
</code>
</p>

<p>where <code class="reqn">\mathbf{\beta}=(\beta_1,\ldots,\beta_{p_n})^{\top}, \ \mathcal{P}_{\lambda_{j_n}}\left(\cdot\right)</code> is a penalty function (specified in the argument <code>penalty</code>) and <code class="reqn">\lambda_{j_n} &gt; 0</code> is a tuning parameter.
To reduce the number of tuning parameters, <code class="reqn">\lambda_j</code>, to be selected for each sample, we consider <code class="reqn">\lambda_j = \lambda \widehat{\sigma}_{\beta_{0,j,OLS}}</code>, where <code class="reqn">\beta_{0,j,OLS}</code> denotes the OLS estimate of <code class="reqn">\beta_{0,j}</code> and <code class="reqn">\widehat{\sigma}_{\beta_{0,j,OLS}}</code> is the estimated standard deviation. The parameter <code class="reqn">\lambda</code> is selected using the objetive criterion specified in the argument <code>criterion</code>.
</p>
<p>For further details on the estimation procedure of the SLM, see e.g. Fan and Li. (2001). The PeLS objective function  is minimised using the R function <code>grpreg</code> of the package <code>grpreg</code> (Breheny and Huang, 2015).
</p>
<p><b>Remark</b>:  It should be noted that if  we set <code>lambda.seq</code> to <code class="reqn">=0</code>, we obtain the non-penalised estimation of the model, i.e. the OLS estimation. Using <code>lambda.seq</code> with a vaule <code class="reqn">\not=0</code> is advisable when suspecting the presence of irrelevant variables.
</p>


<h3>Value</h3>

<table>
<tr><td><code>call</code></td>
<td>
<p>The matched call.</p>
</td></tr>
<tr><td><code>fitted.values</code></td>
<td>
<p>Estimated scalar response.</p>
</td></tr>
<tr><td><code>residuals</code></td>
<td>
<p>Differences between <code>y</code> and the <code>fitted.values</code>.</p>
</td></tr>
<tr><td><code>beta.est</code></td>
<td>
<p>Estimate of <code class="reqn">\beta_0</code> when the optimal penalisation parameter <code>lambda.opt</code> and <code>vn.opt</code> are used.</p>
</td></tr>
<tr><td><code>indexes.beta.nonnull</code></td>
<td>
<p>Indexes of the non-zero <code class="reqn">\hat{\beta_{j}}</code>.</p>
</td></tr>
<tr><td><code>lambda.opt</code></td>
<td>
<p>Selected value of lambda.</p>
</td></tr>
<tr><td><code>IC</code></td>
<td>
<p>Value of the criterion function considered to select <code>lambda.opt</code> and <code>vn.opt</code>.</p>
</td></tr>
<tr><td><code>vn.opt</code></td>
<td>
<p>Selected value of <code>vn</code>.</p>
</td></tr>
<tr><td><code>...</code></td>
<td>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>German Aneiros Perez <a href="mailto:german.aneiros@udc.es">german.aneiros@udc.es</a> 
</p>
<p>Silvia Novo Diaz  <a href="mailto:snovo@est-econ.uc3m.es">snovo@est-econ.uc3m.es</a>
</p>


<h3>References</h3>

<p>Breheny, P., and Huang, J. (2015) Group descent algorithms for nonconvex penalized linear and
logistic regression models with grouped predictors. <em>Statistics and Computing</em>, <b>25</b>, 173&ndash;187, <a href="https://doi.org/10.1007/s11222-013-9424-2">doi:10.1007/s11222-013-9424-2</a>.
</p>
<p>Fan, J., and Li, R. (2001) Variable selection via nonconcave penalized
likelihood and its oracle properties. <em>Journal of the American Statistical Association</em>, <b>96</b>, 1348&ndash;1360, <a href="https://doi.org/10.1198/016214501753382273">doi:10.1198/016214501753382273</a>.
</p>


<h3>See Also</h3>

<p>See also <code><a href="#topic+PVS.fit">PVS.fit</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("Tecator")
y&lt;-Tecator$fat
z1&lt;-Tecator$protein       
z2&lt;-Tecator$moisture

#Quadratic, cubic and interaction effects of the scalar covariates.
z.com&lt;-cbind(z1,z2,z1^2,z2^2,z1^3,z2^3,z1*z2)
train&lt;-1:160

#LM fit 
ptm=proc.time()
fit&lt;-lm.pels.fit(z=z.com[train,], y=y[train],lambda.min.h=0.02,
      lambda.min.l=0.01,factor.pn=2, max.iter=5000, criterion="BIC")
proc.time()-ptm

#Results
fit
names(fit)
 
</code></pre>

<hr>
<h2 id='plot.classes'>
Graphical representation of regression model outputs
</h2><span id='topic+plot.FASSMR.kernel'></span><span id='topic+plot.FASSMR.kNN'></span><span id='topic+plot.fsim.kernel'></span><span id='topic+plot.fsim.kNN'></span><span id='topic+plot.IASSMR.kernel'></span><span id='topic+plot.IASSMR.kNN'></span><span id='topic+plot.lm.pels'></span><span id='topic+plot.PVS'></span><span id='topic+plot.PVS.kernel'></span><span id='topic+plot.PVS.kNN'></span><span id='topic+plot.sfpl.kernel'></span><span id='topic+plot.sfpl.kNN'></span><span id='topic+plot.sfplsim.kernel'></span><span id='topic+plot.sfplsim.kNN'></span>

<h3>Description</h3>

<p><code>plot</code> functions to generate visual representations for the outputs of several fitting functions:  <code>FASSMR.kernel.fit</code>, <code>FASSMR.kNN.fit</code>,  <code>fsim.kernel.fit</code>,  <code>fsim.kernel.fit.optim</code>, <code>fsim.kNN.fit</code>, <code>fsim.kNN.fit.optim</code>, <code>IASSMR.kernel.fit</code>, <code>IASSMR.kNN.fit</code>, <code>lm.pels.fit</code>, <code>PVS.fit</code>, <code>PVS.kernel.fit</code>, <code>PVS.kNN.fit</code>, <code>sfpl.kernel.fit</code>, <code>sfpl.kNN.fit</code>,<code>sfplsim.kernel.fit</code> and <code>sfplsim.kNN.fit</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'FASSMR.kernel'
plot(x,ind=1:10, size=15,col1=1,col2=2,col3=4,option=0,...)

## S3 method for class 'FASSMR.kNN'
plot(x,ind=1:10, size=15,col1=1,col2=2,col3=4,option=0, ...)

## S3 method for class 'fsim.kernel'
plot(x,size=15,col1=1,col2=2, ...)

## S3 method for class 'fsim.kNN'
plot(x,size=15,col1=1,col2=2,...)

## S3 method for class 'IASSMR.kernel'
plot(x,ind=1:10, size=15,col1=1,col2=2,col3=4,option=0, ...)

## S3 method for class 'IASSMR.kNN'
plot(x,ind=1:10, size=15,col1=1,col2=2,col3=4,option=0, ...)

## S3 method for class 'lm.pels'
plot(x,size=15,col1=1,col2=2,col3=4, ...)

## S3 method for class 'PVS'
plot(x,ind=1:10, size=15,col1=1,col2=2,col3=4,option=0, ...)

## S3 method for class 'PVS.kernel'
plot(x,ind=1:10, size=15,col1=1,col2=2,col3=4,option=0, ...)

## S3 method for class 'PVS.kNN'
plot(x,ind=1:10, size=15,col1=1,col2=2,col3=4,option=0, ...)

## S3 method for class 'sfpl.kernel'
plot(x,size=15,col1=1,col2=2,col3=4, ...)

## S3 method for class 'sfpl.kNN'
plot(x,size=15,col1=1,col2=2,col3=4, ...)

## S3 method for class 'sfplsim.kernel'
plot(x,size=15,col1=1,col2=2,col3=4, ...)

## S3 method for class 'sfplsim.kNN'
plot(x,size=15,col1=1,col2=2,col3=4, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.classes_+3A_x">x</code></td>
<td>

<p>Output of the functions mentioned in the <code>Description</code> (i.e. an object of the class <code>FASSMR.kernel</code>, <code>FASSMR.kNN</code>, <code>fsim.kernel</code>,<code>fsim.kNN</code>, <code>IASSMR.kernel</code>, <code>IASSMR.kNN</code>, <code>lm.pels</code>, <code>PVS</code>, <code>PVS.kernel</code>, <code>PVS.kNN</code>, <code>sfpl.kernel</code>,<code>sfpl.kNN</code>, <code>sfplsim.kernel</code> or <code>sfplsim.kNN</code>).
</p>
</td></tr>
<tr><td><code id="plot.classes_+3A_ind">ind</code></td>
<td>
<p>Indexes of the colors for the curves in the chart of estimated impact points. The default is <code>1:10</code></p>
</td></tr>
<tr><td><code id="plot.classes_+3A_size">size</code></td>
<td>

<p>The size for title and axis labels in pts. The default is 15.
</p>
</td></tr>
<tr><td><code id="plot.classes_+3A_col1">col1</code></td>
<td>

<p>Color of the points in the charts. Also, color of the estimated functional index representation. The default is black.
</p>
</td></tr>
<tr><td><code id="plot.classes_+3A_col2">col2</code></td>
<td>

<p>Color of the nonparametric fit representation in FSIM functions, and of the straight line in 'Response vs Fitted Values' charts. The default is red.
</p>
</td></tr>
<tr><td><code id="plot.classes_+3A_col3">col3</code></td>
<td>

<p>Color of the nonparametric fit of the residuals in 'Residuals vs Fitted Values' charts. The default is blue.
</p>
</td></tr>
<tr><td><code id="plot.classes_+3A_option">option</code></td>
<td>
<p>Selection of charts to be plotted. The default, <code>option = 0</code>, means all charts are plotted. See the section <code>Details</code>.</p>
</td></tr>
<tr><td><code id="plot.classes_+3A_...">...</code></td>
<td>
<p>Further arguments passed to or from other methods.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The functions return different graphical representations.
</p>

<ul>
<li><p> For the classes <code>fsim.kNN</code> and <code>fsim.kernel</code>:
</p>

<ol>
<li><p> The estimated functional index: <code class="reqn">\hat{\theta}</code>.
</p>
</li>
<li><p> The regression fit.
</p>
</li></ol>

</li>
<li><p> For the classes <code>lm.pels</code>, <code>sfpl.kernel</code> and <code>sfpl.kNN</code>:
</p>

<ol>
<li><p> The response over the <code>fitted.values</code>.
</p>
</li>
<li><p> The <code>residuals</code> over the <code>fitted.values</code>.
</p>
</li></ol>

</li>
<li><p> For the classes <code>sfplsim.kernel</code> and <code>sfplsim.kNN</code>:
</p>

<ol>
<li><p>  The estimated functional index: <code class="reqn">\hat{\theta}</code>.
</p>
</li>
<li><p> The response over the <code>fitted.values</code>.
</p>
</li>
<li><p> The <code>residuals</code> over the <code>fitted.values</code>.
</p>
</li></ol>

</li>
<li><p> For the classes <code>FASSMR.kernel</code>, <code>FASSMR.kNN</code>, <code>IASSMR.kernel</code>, <code>IASSMR.kNN</code>, <code>sfplsim.kernel</code> and <code>sfplsim.kNN</code>:
</p>

<ol>
<li><p> If <code>option=1</code>: The curves with the estimated impact points (in dashed vertical lines).
</p>
</li>
<li><p> If <code>option=2</code>: The estimated functional index: <code class="reqn">\hat{\theta}</code>.
</p>
</li>
<li><p> If <code>option=3</code>:
</p>

<ul>
<li><p> The response over the <code>fitted.values</code>.
</p>
</li>
<li><p> The <code>residuals</code> over the <code>fitted.values</code>.</p>
</li></ul>

</li>
<li><p> If <code>option=0</code>: All chart are plotted.
</p>
</li></ol>

</li>
<li><p> For the classes <code>PVS</code>, <code>PVS.kNN</code>, <code>PVS.kernel</code>:
</p>

<ol>
<li><p> If <code>option=1</code>: The curves with the estimated impact points (in dashed vertical lines).
</p>
</li>
<li><p> If <code>option=2</code>: </p>
 
<ul>
<li><p> The response over the <code>fitted.values</code>.
</p>
</li>
<li><p> The <code>residuals</code> over the <code>fitted.values</code>.</p>
</li></ul>

</li>
<li><p> If <code>option=0</code>: All chart are plotted.
</p>
</li></ol>

</li></ul>

<p>All the routines implementing the plot S3 method use internally the R package <code>ggplot2</code> to produce elegant and high quality
charts.
</p>


<h3>Author(s)</h3>

<p>German Aneiros Perez <a href="mailto:german.aneiros@udc.es">german.aneiros@udc.es</a> 
</p>
<p>Silvia Novo Diaz  <a href="mailto:snovo@est-econ.uc3m.es">snovo@est-econ.uc3m.es</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+FASSMR.kernel.fit">FASSMR.kernel.fit</a></code>, <code><a href="#topic+FASSMR.kNN.fit">FASSMR.kNN.fit</a></code>,  <code><a href="#topic+fsim.kernel.fit">fsim.kernel.fit</a></code>, <code><a href="#topic+fsim.kNN.fit">fsim.kNN.fit</a></code>, <code><a href="#topic+IASSMR.kernel.fit">IASSMR.kernel.fit</a></code>, <code><a href="#topic+IASSMR.kNN.fit">IASSMR.kNN.fit</a></code>, <code><a href="#topic+lm.pels.fit">lm.pels.fit</a></code>, <code><a href="#topic+PVS.fit">PVS.fit</a></code>, <code><a href="#topic+PVS.kernel.fit">PVS.kernel.fit</a></code>, <code><a href="#topic+PVS.kNN.fit">PVS.kNN.fit</a></code>, <code><a href="#topic+sfpl.kernel.fit">sfpl.kernel.fit</a></code>, <code><a href="#topic+sfpl.kNN.fit">sfpl.kNN.fit</a></code>, <code><a href="#topic+sfplsim.kernel.fit">sfplsim.kernel.fit</a></code> and <code><a href="#topic+sfplsim.kNN.fit">sfplsim.kNN.fit</a></code>.
</p>

<hr>
<h2 id='predict.fsim'>
Prediction for FSIM
</h2><span id='topic+predict.fsim.kernel'></span><span id='topic+predict.fsim.kNN'></span>

<h3>Description</h3>

<p><code>predict</code> method for the functional single-index model (FSIM) fitted using <code>fsim.kernel.fit</code>, <code>fsim.kernel.fit.optim</code>, <code>fsim.kNN.fit</code> and <code>fsim.kNN.fit.optim</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'fsim.kernel'
predict(object, newdata = NULL, y.test = NULL, ...)
## S3 method for class 'fsim.kNN'
predict(object, newdata = NULL, y.test = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.fsim_+3A_object">object</code></td>
<td>

<p>Output of the <code>fsim.kernel.fit</code>, <code>fsim.kernel.fit.optim</code>, <code>fsim.kNN.fit</code> or <code>fsim.kNN.fit.optim</code> functions (i.e. an object of the class <code>fsim.kernel</code> or <code>fsim.kNN</code>).
</p>
</td></tr>
<tr><td><code id="predict.fsim_+3A_newdata">newdata</code></td>
<td>

<p>A matrix containing new observations of the functional covariate collected by row.
</p>
</td></tr>
<tr><td><code id="predict.fsim_+3A_y.test">y.test</code></td>
<td>

<p>(optional) A vector containing the new observations of the response.
</p>
</td></tr>
<tr><td><code id="predict.fsim_+3A_...">...</code></td>
<td>
<p>Further arguments passed to or from other methods.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The prediction is computed using the functions <code>fsim.kernel.test</code> and <code>fsim.kernel.fit</code>, respectively.
</p>


<h3>Value</h3>

<p>The function returns the predicted values of the response (<code>y</code>) for <code>newdata</code>. If <code>!is.null(y.test)</code>, it also provides the mean squared error of prediction (<code>MSEP</code>) computed as <code>mean((y-y.test)^2)</code>.
If <code>is.null(newdata)</code> the function returns the fitted values.
</p>


<h3>Author(s)</h3>

<p>German Aneiros Perez <a href="mailto:german.aneiros@udc.es">german.aneiros@udc.es</a> 
</p>
<p>Silvia Novo Diaz  <a href="mailto:snovo@est-econ.uc3m.es">snovo@est-econ.uc3m.es</a>
</p>


<h3>See Also</h3>

<p><code>fsim.kernel.fit</code> and  <code>fsim.kernel.test</code>  or <code>fsim.kNN.fit</code> and <code>fsim.kNN.test</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(Tecator)
y&lt;-Tecator$fat
X&lt;-Tecator$absor.spectra2

train&lt;-1:160
test&lt;-161:215

#FSIM fit. 
fit.kernel&lt;-fsim.kernel.fit(y[train],x=X[train,],max.q.h=0.35, nknot=20,
range.grid=c(850,1050),nknot.theta=4)
fit.kNN&lt;-fsim.kNN.fit(y=y[train],x=X[train,],max.knn=20,nknot=20,
nknot.theta=4, range.grid=c(850,1050))

test&lt;-161:215

pred.kernel&lt;-predict(fit.kernel,newdata=X[test,],y.test=y[test])
pred.kernel$MSEP
pred.kNN&lt;-predict(fit.kNN,newdata=X[test,],y.test=y[test])
pred.kNN$MSEP

</code></pre>

<hr>
<h2 id='predict.IASSMR'>
Prediction for MFPLSIM
</h2><span id='topic+predict.IASSMR.kernel'></span><span id='topic+predict.IASSMR.kNN'></span>

<h3>Description</h3>

<p><code>predict</code> method for the multi-functional partial linear single-index model (MFPLSIM) fitted using <code>IASSMR.kernel.fit</code> or <code>IASSMR.kNN.fit</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
## S3 method for class 'IASSMR.kernel'
predict(object, newdata.x = NULL, newdata.z = NULL,
  y.test = NULL, option = NULL, ...)
## S3 method for class 'IASSMR.kNN'
predict(object, newdata.x = NULL, newdata.z = NULL,
  y.test = NULL, option = NULL, knearest.n = object$knearest, 
  min.knn.n = object$min.knn, max.knn.n = object$max.knn.n, 
  step.n = object$step, ...)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.IASSMR_+3A_object">object</code></td>
<td>

<p>Output of the functions mentioned in the <code>Description</code> (i.e. an object of the class <code>IASSMR.kernel</code> or <code>IASSMR.kNN</code>).
</p>
</td></tr>
<tr><td><code id="predict.IASSMR_+3A_newdata.x">newdata.x</code></td>
<td>

<p>A matrix containing new observations of the functional covariate in the functional single-index component, collected by row.
</p>
</td></tr>
<tr><td><code id="predict.IASSMR_+3A_newdata.z">newdata.z</code></td>
<td>

<p>Matrix containing the new observations of the scalar covariates derived from the discretisation  of a curve,  collected by row. 
</p>
</td></tr>
<tr><td><code id="predict.IASSMR_+3A_y.test">y.test</code></td>
<td>

<p>(optional) A vector containing the new observations of the response.
</p>
</td></tr>
<tr><td><code id="predict.IASSMR_+3A_option">option</code></td>
<td>

<p>Allows the choice between 1, 2 and 3. The default is 1. See the section <code>Details</code>.
</p>
</td></tr>
<tr><td><code id="predict.IASSMR_+3A_...">...</code></td>
<td>

<p>Further arguments.
</p>
</td></tr>
<tr><td><code id="predict.IASSMR_+3A_knearest.n">knearest.n</code></td>
<td>

<p>Only used for objects <code>IASSMR.kNN</code> if <code>option=2</code> or <code>option=3</code>: vector of positive integers containing the sequence in which the  number of nearest neighbours <code>k.opt</code> is selected. The default is <code>object$knearest</code>.
</p>
</td></tr>
<tr><td><code id="predict.IASSMR_+3A_min.knn.n">min.knn.n</code></td>
<td>

<p>Only used for objects <code>IASSMR.kNN</code> if <code>option=2</code> or <code>option=3</code>: minumum value of the sequence in which the  number of neighbours <code>k.opt</code> is selected (thus, this number must be smaller than the sample size). The default is <code>object$min.knn</code>.
</p>
</td></tr>
<tr><td><code id="predict.IASSMR_+3A_max.knn.n">max.knn.n</code></td>
<td>

<p>Only used for objects <code>IASSMR.kNN</code> if <code>option=2</code> or <code>option=3</code>: maximum value of the sequence in which the number of neighbours <code>k.opt</code> is selected (thus, this number must be larger than <code>min.kNN</code> and smaller than the sample size). The default is <code>object$max.knn</code>.
</p>
</td></tr>
<tr><td><code id="predict.IASSMR_+3A_step.n">step.n</code></td>
<td>

<p>Only used for objects <code>IASSMR.kNN</code> if <code>option=2</code> or <code>option=3</code>: positive integer used to build the sequence of k-nearest neighbours as follows: <code>min.knn, min.knn + step.n, min.knn + 2*step.n, min.knn + 3*step.n,...</code>. The default is  <code>object$step</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Three options are provided to obtain the predictions of the response for <code>newdata.x</code> and <code>newdata.z</code>:
</p>

<ul>
<li><p> If <code>option=1</code>, we maintain all the estimates (<code>k.opt</code> or <code>h.opt</code>, <code>theta.est</code> and <code>beta.est</code>) to predict the functional single-index component of the model. As we use the estimates of the second step of the algorithm, only the <code>train.2</code> is used as training sample to predict.
Then, it should be noted that <code>k.opt</code> or <code>h.opt</code> may not be suitable to predict the functional single-index component of the model.
</p>
</li>
<li><p> If <code>option=2</code>, we maintain <code>theta.est</code> and <code>beta.est</code>, while the tuning parameter (<code class="reqn">h</code> or <code class="reqn">k</code>) is selected again to predict the functional single-index component of the model. This selection is performed using the leave-one-out cross-validation criterion in the functional single-index model associated and the complete training sample (i.e. <code>train=c(train.1,train.2)</code>). As we use the entire training sample (not just a subsample of it), the sample size is modified and, as a consequence,  the parameters <code>knearest</code>, <code>min.knn</code>, <code>max.knn</code>, <code>step</code> given to the function <code>IASSMR.kNN.fit</code> may need to be provided again to compute predictions. For that, we add the arguments <code>knearest.n</code>, <code>min.knn.n</code>, <code>max.knn.n</code> and <code>step.n</code>.
</p>
</li>
<li><p>  If <code>option=3</code>, we maintain only the indexes of the relevant variables selected by the IASSMR. We estimate again the linear coefficients and the functional index  by means of <code>sfplsim.kernel.fit</code> or <code>sfplsim.kNN.fit</code>, respectively, without penalisation (setting <code>lambda.seq=0</code>) and using the whole training sample (<code>train=c(train.1,train.2)</code>). The method provides two predictions (and MSEPs):
</p>

<ul>
<li><p> a) The prediction associated with <code>option=1</code> for <code>sfplsim.kernel</code> or <code>sfplsim.kNN</code> class.
</p>
</li>
<li><p> b) The prediction associated with <code>option=2</code> for <code>sfplsim.kernel</code> or <code>sfplsim.kNN</code> class.
</p>
</li></ul>

<p>(see the documentation of the functions <code>predict.sfplsim.kernel</code> and <code>predict.sfplsim.kNN</code>)
</p>
</li></ul>



<h3>Value</h3>

<p>The function returns the predicted values of the response (<code>y</code>) for <code>newdata.x</code> and <code>newdata.z</code>. If <code>!is.null(y.test)</code>, it also provides the mean squared error of prediction (<code>MSEP</code>) computed as <code>mean((y-y.test)^2)</code>.
If <code>option=3</code>, two sets of predictions (and two MSEPs) are provided, corresponding to the items a) and b) mentioned in the section <code>Details.</code>
If <code>is.null(newdata.x)</code> or <code>is.null(newdata.z)</code>,  the function returns the fitted values.
</p>


<h3>Author(s)</h3>

<p>German Aneiros Perez <a href="mailto:german.aneiros@udc.es">german.aneiros@udc.es</a> 
</p>
<p>Silvia Novo Diaz  <a href="mailto:snovo@est-econ.uc3m.es">snovo@est-econ.uc3m.es</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+sfplsim.kernel.fit">sfplsim.kernel.fit</a></code>, <code><a href="#topic+sfplsim.kNN.fit">sfplsim.kNN.fit</a></code>, <code><a href="#topic+IASSMR.kernel.fit">IASSMR.kernel.fit</a></code>, <code><a href="#topic+IASSMR.kNN.fit">IASSMR.kNN.fit</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(Sugar)

y&lt;-Sugar$ash
x&lt;-Sugar$wave.290
z&lt;-Sugar$wave.240

#Outliers
index.y.25 &lt;- y &gt; 25
index.atip &lt;- index.y.25
(1:268)[index.atip]

#Dataset to model
x.sug &lt;- x[!index.atip,]
z.sug&lt;- z[!index.atip,]
y.sug &lt;- y[!index.atip]

train&lt;-1:216
test&lt;-217:266

#Fit
fit.kernel&lt;-IASSMR.kernel.fit(x=x.sug[train,],z=z.sug[train,], y=y.sug[train],
            train.1=1:108,train.2=109:216,nknot.theta=2,lambda.min.h=0.03,
            lambda.min.l=0.03,  max.q.h=0.35,  nknot=20,criterion="BIC",
            max.iter=5000)

fit.kNN&lt;- IASSMR.kNN.fit(x=x.sug[train,],z=z.sug[train,], y=y.sug[train],
          train.1=1:108,train.2=109:216,nknot.theta=2,lambda.min.h=0.07,
          lambda.min.l=0.07, max.knn=20, nknot=20,criterion="BIC",
          max.iter=5000)

#Predictions
predict(fit.kernel,newdata.x=x.sug[test,],newdata.z=z.sug[test,],y.test=y.sug[test],option=2)
predict(fit.kNN,newdata.x=x.sug[test,],newdata.z=z.sug[test,],y.test=y.sug[test],option=2)

</code></pre>

<hr>
<h2 id='predict.lm'>
Prediction for linear models
</h2><span id='topic+predict.PVS'></span><span id='topic+predict.lm.pels'></span>

<h3>Description</h3>

<p><code>predict</code> method for: 
</p>

<ul>
<li><p> Linear model (LM) fitted using <code>lm.pels.fit</code>. 
</p>
</li>
<li><p> Linear model with covariates derived from the discretization of a curve fitted using <code>PVS.fit</code>.
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'lm.pels'
predict(object, newdata = NULL, y.test = NULL, ...)
## S3 method for class 'PVS'
predict(object, newdata = NULL, y.test = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.lm_+3A_object">object</code></td>
<td>

<p>Output of the <code>lm.pels.fit</code> or <code>PVS.fit</code> functions (i.e. an object of the class <code>lm.pels</code> or <code>PVS</code>)
</p>
</td></tr>
<tr><td><code id="predict.lm_+3A_newdata">newdata</code></td>
<td>

<p>Matrix containing the new observations of the scalar covariates (LM), or the scalar covariates resulting from the discretisation of a curve. Observations are collected by row.
</p>
</td></tr>
<tr><td><code id="predict.lm_+3A_y.test">y.test</code></td>
<td>

<p>(optional) A vector containing the new observations of the response.
</p>
</td></tr>
<tr><td><code id="predict.lm_+3A_...">...</code></td>
<td>
<p>Further arguments passed to or from other methods.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The function returns the predicted values of the response (<code>y</code>) for <code>newdata</code>. If <code>!is.null(y.test)</code>, it also provides the mean squared error of prediction (<code>MSEP</code>) computed as <code>mean((y-y.test)^2)</code>.
If <code>is.null(newdata)</code>, then the function returns the fitted values.
</p>


<h3>Author(s)</h3>

<p>German Aneiros Perez <a href="mailto:german.aneiros@udc.es">german.aneiros@udc.es</a> 
</p>
<p>Silvia Novo Diaz  <a href="mailto:snovo@est-econ.uc3m.es">snovo@est-econ.uc3m.es</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+lm.pels.fit">lm.pels.fit</a></code> and <code><a href="#topic+PVS.fit">PVS.fit</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data("Tecator")
y&lt;-Tecator$fat
z1&lt;-Tecator$protein       
z2&lt;-Tecator$moisture

#Quadratic, cubic and interaction effects of the scalar covariates.
z.com&lt;-cbind(z1,z2,z1^2,z2^2,z1^3,z2^3,z1*z2)
train&lt;-1:160
test&lt;-161:215

#LM fit. 
fit&lt;-lm.pels.fit(z=z.com[train,], y=y[train],lambda.min.l=0.01,
      factor.pn=2, max.iter=5000, criterion="BIC")

#Predictions
predict(fit,newdata=z.com[test,],y.test=y[test])


data(Sugar)

y&lt;-Sugar$ash
z&lt;-Sugar$wave.240

#Outliers
index.y.25 &lt;- y &gt; 25
index.atip &lt;- index.y.25
(1:268)[index.atip]


#Dataset to model
z.sug&lt;- z[!index.atip,]
y.sug &lt;- y[!index.atip]

train&lt;-1:216
test&lt;-217:266

#Fit
fit.pvs&lt;-PVS.fit(z=z.sug[train,], y=y.sug[train],train.1=1:108,train.2=109:216,
          lambda.min.h=0.2,criterion="BIC", max.iter=5000)


#Predictions
predict(fit.pvs,newdata=z.sug[test,],y.test=y.sug[test])


</code></pre>

<hr>
<h2 id='predict.mfplm.PVS'>
Prediction for MFPLM
</h2><span id='topic+predict.PVS.kernel'></span><span id='topic+predict.PVS.kNN'></span>

<h3>Description</h3>

<p><code>predict</code> method for the multi-functional partial linear model (MFPLM) fitted using <code>PVS.kernel.fit</code> or <code>PVS.kNN.fit</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'PVS.kernel'
predict(object, newdata.x = NULL, newdata.z = NULL,
  y.test = NULL, option = NULL, ...)
## S3 method for class 'PVS.kNN'
predict(object, newdata.x = NULL, newdata.z = NULL, 
  y.test = NULL, option = NULL, knearest.n = object$knearest, 
  min.knn.n = object$min.knn, max.knn.n = object$max.knn.n, 
  step.n = object$step, ...)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.mfplm.PVS_+3A_object">object</code></td>
<td>

<p>Output of the functions mentioned in the <code>Description</code> (i.e. an object of the class <code>PVS.kernel</code> or <code>PVS.kNN</code>).
</p>
</td></tr>
<tr><td><code id="predict.mfplm.PVS_+3A_newdata.x">newdata.x</code></td>
<td>

<p>A matrix containing new observations of the functional covariate in the functional nonparametric component, collected by row.
</p>
</td></tr>
<tr><td><code id="predict.mfplm.PVS_+3A_newdata.z">newdata.z</code></td>
<td>

<p>Matrix containing the new observations of the scalar covariates derived from the discretisation  of a curve,  collected by row. 
</p>
</td></tr>
<tr><td><code id="predict.mfplm.PVS_+3A_y.test">y.test</code></td>
<td>

<p>(optional) A vector containing the new observations of the response.
</p>
</td></tr>
<tr><td><code id="predict.mfplm.PVS_+3A_option">option</code></td>
<td>

<p>Allows the selection among the choices 1, 2 and 3 for <code>PVS.kernel</code> objects, and  1, 2, 3, and 4 for <code>PVS.kNN</code> objects. The default setting is 1. See the section <code>Details</code>.
</p>
</td></tr>
<tr><td><code id="predict.mfplm.PVS_+3A_...">...</code></td>
<td>

<p>Further arguments.
</p>
</td></tr>
<tr><td><code id="predict.mfplm.PVS_+3A_knearest.n">knearest.n</code></td>
<td>

<p>Only used for objects <code>PVS.kNN</code> if <code>option=2</code>, <code>option=3</code> or <code>option=4</code>: sequence in which the  number of nearest neighbours <code>k.opt</code> is selected. The default is <code>object$knearest</code>.
</p>
</td></tr>
<tr><td><code id="predict.mfplm.PVS_+3A_min.knn.n">min.knn.n</code></td>
<td>

<p>Only used for objects <code>PVS.kNN</code> if <code>option=2</code>, <code>option=3</code> or <code>option=4</code>: minumum value of the sequence in which the  number of nearest neighbours <code>k.opt</code> is selected (thus, this number must be smaller than the sample size). The default is <code>object$min.knn</code>.
</p>
</td></tr>
<tr><td><code id="predict.mfplm.PVS_+3A_max.knn.n">max.knn.n</code></td>
<td>

<p>Only used for objects <code>PVS.kNN</code> if <code>option=2</code>, <code>option=3</code> or <code>option=4</code>: maximum value of the sequence in which the number of nearest neighbours <code>k.opt</code> is selected (thus, this number must be larger than <code>min.kNN</code> and smaller than the sample size). The default is <code>object$max.knn</code>.
</p>
</td></tr>
<tr><td><code id="predict.mfplm.PVS_+3A_step.n">step.n</code></td>
<td>

<p>Only used for objects <code>PVS.kNN</code> if <code>option=2</code>, <code>option=3</code> or <code>option=4</code>: positive integer used to build the sequence of k-nearest neighbours in the following way: <code>min.knn, min.knn + step.n, min.knn + 2*step.n, min.knn + 3*step.n,...</code>. The default is  <code>object$step</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>To obtain the predictions of the response for <code>newdata.x</code> and <code>newdata.z</code>, the following options are provided:
</p>

<ul>
<li><p> If <code>option=1</code>, we maintain all the estimates (<code>k.opt</code> or <code>h.opt</code> and <code>beta.est</code>) to predict the functional nonparametric component of the model. As we use the estimates of the second step of the algorithm, only the <code>train.2</code> is used as training sample to predict.
Then, it should be noted that <code>k.opt</code> or <code>h.opt</code> may not be suitable to predict the functional nonparametric component of the model.
</p>
</li>
<li><p> If <code>option=2</code>, we maintain <code>beta.est</code>, while the tuning parameter (<code class="reqn">h</code> or <code class="reqn">k</code>) is selected again to predict the functional nonparametric component of the model. This selection is performed using the leave-one-out cross-validation (LOOCV) criterion in the associated functional nonparametric model and the complete training sample (i.e. <code>train=c(train.1,train.2)</code>), obtaining a global selection for <code class="reqn">h</code> or <code class="reqn">k</code>. As we use the entire training sample (not just a subsample of it), the sample size is modified and, as a consequence,  the parameters <code>knearest</code>, <code>min.knn</code>, <code>max.knn</code>, and <code>step</code> given to the function <code>IASSMR.kNN.fit</code> may need to be provided again to compute predictions. For that, we add the arguments <code>knearest.n</code>, <code>min.knn.n</code>, <code>max.knn.n</code> and <code>step.mn</code>. 
</p>
</li>
<li><p>  If <code>option=3</code>, we maintain only the indexes of the relevant variables selected by the IASSMR. We estimate again the linear coefficients  using <code>sfpl.kernel.fit</code> or <code>sfpl.kNN.fit</code>, respectively, without penalisation (setting <code>lambda.seq=0</code>) and using the entire training sample (<code>train=c(train.1,train.2)</code>). The method provides two predictions (and MSEPs):
</p>

<ul>
<li><p> a) The prediction associated with <code>option=1</code> for <code>sfpl.kernel</code> or <code>sfpl.kNN</code> class.
</p>
</li>
<li><p> b) The prediction associated with <code>option=2</code> for <code>sfpl.kernel</code> or <code>sfpl.kNN</code> class.
</p>
</li></ul>

<p>(see the documentation of the functions <code>predict.sfpl.kernel</code> and <code>predict.sfpl.kNN</code>)
</p>
</li>
<li><p> If <code>option=4</code> (an option only available for the class <code>PVS.kNN</code>) we maintain <code>beta.est</code>, while the tuning parameter <code class="reqn">k</code> is selected again to predict the functional nonparametric component of the model. This selection is performed using LOOCV criterion in the functional nonparametric model associated and the complete training sample (i.e. <code>train=c(train.1,train.2)</code>), obtaining a local selection for <code class="reqn">k</code>.
</p>
</li></ul>



<h3>Value</h3>

<p>The function returns the predicted values of the response (<code>y</code>) for <code>newdata.x</code> and <code>newdata.z</code>. If <code>!is.null(y.test)</code>, it also provides the mean squared error of prediction (<code>MSEP</code>) computed as <code>mean((y-y.test)^2)</code>.
If <code>option=3</code>, two sets of predictions (and two MSEPs) are provided, corresponding to the items a) and b) mentioned in the section <code>Details.</code>
If <code>is.null(newdata.x)</code> or <code>is.null(newdata.z)</code>, then the function returns the fitted values.
</p>


<h3>Author(s)</h3>

<p>German Aneiros Perez <a href="mailto:german.aneiros@udc.es">german.aneiros@udc.es</a> 
</p>
<p>Silvia Novo Diaz  <a href="mailto:snovo@est-econ.uc3m.es">snovo@est-econ.uc3m.es</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+PVS.kernel.fit">PVS.kernel.fit</a></code>, <code><a href="#topic+sfpl.kernel.fit">sfpl.kernel.fit</a></code> and <code><a href="#topic+predict.sfpl.kernel">predict.sfpl.kernel</a></code> or <code><a href="#topic+PVS.kNN.fit">PVS.kNN.fit</a></code>,
<code><a href="#topic+sfpl.kNN.fit">sfpl.kNN.fit</a></code> and <code><a href="#topic+predict.sfpl.kNN">predict.sfpl.kNN</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(Sugar)

y&lt;-Sugar$ash
x&lt;-Sugar$wave.290
z&lt;-Sugar$wave.240

#Outliers
index.y.25 &lt;- y &gt; 25
index.atip &lt;- index.y.25
(1:268)[index.atip]

#Dataset to model
x.sug &lt;- x[!index.atip,]
z.sug&lt;- z[!index.atip,]
y.sug &lt;- y[!index.atip]

train&lt;-1:216
test&lt;-217:266

#Fit
fit.kernel&lt;- PVS.kernel.fit(x=x.sug[train,],z=z.sug[train,], 
              y=y.sug[train],train.1=1:108,train.2=109:216,
              lambda.min.h=0.03,lambda.min.l=0.03,
              max.q.h=0.35, nknot=20,criterion="BIC",
              max.iter=5000)
fit.kNN&lt;- PVS.kNN.fit(x=x.sug[train,],z=z.sug[train,], y=y.sug[train],
            train.1=1:108,train.2=109:216,lambda.min.h=0.07, 
            lambda.min.l=0.07, nknot=20,criterion="BIC",
            max.iter=5000)

#Preditions
predict(fit.kernel,newdata.x=x.sug[test,],newdata.z=z.sug[test,],y.test=y.sug[test],option=2)
predict(fit.kNN,newdata.x=x.sug[test,],newdata.z=z.sug[test,],y.test=y.sug[test],option=2)

</code></pre>

<hr>
<h2 id='predict.sfpl'>
Predictions for SFPLM
</h2><span id='topic+predict.sfpl.kernel'></span><span id='topic+predict.sfpl.kNN'></span>

<h3>Description</h3>

<p><code>predict</code> method for the semi-functional partial linear  model (SFPLM) fitted using <code>sfpl.kernel.fit</code> or <code>sfpl.kNN.fit</code>. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'sfpl.kernel'
predict(object, newdata.x = NULL, newdata.z = NULL,
  y.test = NULL, option = NULL, ...)
## S3 method for class 'sfpl.kNN'
predict(object, newdata.x = NULL, newdata.z = NULL, 
  y.test = NULL, option = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.sfpl_+3A_object">object</code></td>
<td>

<p>Output of the functions mentioned in the <code>Description</code> (i.e. an object of the class <code>sfpl.kernel</code> or <code>sfpl.kNN</code>.
</p>
</td></tr>
<tr><td><code id="predict.sfpl_+3A_newdata.x">newdata.x</code></td>
<td>

<p>Matrix containing new observations of the functional covariate collected by row.
</p>
</td></tr>
<tr><td><code id="predict.sfpl_+3A_newdata.z">newdata.z</code></td>
<td>

<p>Matrix containing the new observations of the scalar covariate collected by row.
</p>
</td></tr>
<tr><td><code id="predict.sfpl_+3A_y.test">y.test</code></td>
<td>

<p>(optional) A vector containing the new observations of the response.
</p>
</td></tr>
<tr><td><code id="predict.sfpl_+3A_option">option</code></td>
<td>

<p>Allows the selection among the choices 1 and 2 for <code>sfpl.kernel</code> objects, and 1, 2 and 3 for <code>sfpl.kNN</code> objects. The default setting is 1. See the section <code>Details</code>.
</p>
</td></tr>
<tr><td><code id="predict.sfpl_+3A_...">...</code></td>
<td>
<p>Further arguments passed to or from other methods.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The following options are provided to obtain the predictions of the response for <code>newdata.x</code> and <code>newdata.z</code>: 
</p>

<ul>
<li><p> If <code>option=1</code>, we maintain all the estimations (<code>k.opt</code> or <code>h.opt</code> and <code>beta.est</code>) to predict the functional nonparametric component of the model. 
</p>
</li>
<li><p> If <code>option=2</code>, we maintain <code>beta.est</code>, while the tuning parameter (<code class="reqn">h</code> or <code class="reqn">k</code>) is selected again to predict the functional nonparametric component of the model. This selection is performed using the leave-one-out cross-validation (LOOCV) criterion in the associated functional nonparametric model, obtaining a global selection for <code class="reqn">h</code> or <code class="reqn">k</code>.
</p>
</li></ul>

<p>In the case of <code>sfpl.kNN</code> objects if <code>option=3</code>, we maintain <code>beta.est</code>, while the tuning parameter <code class="reqn">k</code> is seleted again to predict the functional nonparametric component of the model. This selection is performed using the LOOCV criterion in the associated functional nonparametric model, performing a local selection for <code class="reqn">k</code>.
</p>


<h3>Value</h3>

<p>The function returns the predicted values of the response (<code>y</code>) for <code>newdata.x</code> and <code>newdata.z</code>. If <code>!is.null(y.test)</code>, it also provides the mean squared error of prediction (<code>MSEP</code>) computed as <code>mean((y-y.test)^2)</code>.
If <code>is.null(newdata.x)</code> or <code>is.null(newdata.z)</code>, then the function returns the fitted values.
</p>


<h3>Author(s)</h3>

<p>German Aneiros Perez <a href="mailto:german.aneiros@udc.es">german.aneiros@udc.es</a> 
</p>
<p>Silvia Novo Diaz  <a href="mailto:snovo@est-econ.uc3m.es">snovo@est-econ.uc3m.es</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+sfpl.kernel.fit">sfpl.kernel.fit</a></code> and <code><a href="#topic+sfpl.kNN.fit">sfpl.kNN.fit</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data("Tecator")
y&lt;-Tecator$fat
X&lt;-Tecator$absor.spectra
z1&lt;-Tecator$protein       
z2&lt;-Tecator$moisture

#Quadratic, cubic and interaction effects of the scalar covariates.
z.com&lt;-cbind(z1,z2,z1^2,z2^2,z1^3,z2^3,z1*z2)
train&lt;-1:160
test&lt;-161:215

 
#Fit
fit.kernel&lt;-sfpl.kernel.fit(x=X[train,], z=z.com[train,], y=y[train],q=2,
  max.q.h=0.35,lambda.min.l=0.01, factor.pn=2,  
  criterion="BIC", range.grid=c(850,1050), nknot=20, max.iter=5000)
fit.kNN&lt;-sfpl.kNN.fit(y=y[train],x=X[train,], z=z.com[train,],q=2, 
  max.knn=20,lambda.min.l=0.01, factor.pn=2, 
  criterion="BIC",range.grid=c(850,1050), nknot=20, max.iter=5000)

#Predictions
predict(fit.kernel,newdata.x=X[test,],newdata.z=z.com[test,],y.test=y[test],
  option=2)
predict(fit.kNN,newdata.x=X[test,],newdata.z=z.com[test,],y.test=y[test],
  option=2)

</code></pre>

<hr>
<h2 id='predict.sfplsim.FASSMR'>
Prediction for SFPLSIM and MFPLSIM (using FASSMR) 
</h2><span id='topic+predict.FASSMR.kernel'></span><span id='topic+predict.FASSMR.kNN'></span><span id='topic+predict.sfplsim.kernel'></span><span id='topic+predict.sfplsim.kNN'></span>

<h3>Description</h3>

<p><code>predict</code> S3 method for: 
</p>

<ul>
<li><p> Semi-functional partial linear single-index model (SFPLSIM) fitted using <code>sfplsim.kernel.fit</code> or <code>sfplsim.kNN.fit</code>. 
</p>
</li>
<li><p> Multi-functional partial linear single-index model (MFPLSIM) fitted using <code>FASSMR.kernel.fit</code> or <code>FASSMR.kNN.fit</code>.
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'sfplsim.kernel'
predict(object, newdata.x = NULL, newdata.z = NULL,
  y.test = NULL, option = NULL, ...)
## S3 method for class 'sfplsim.kNN'
predict(object, newdata.x = NULL, newdata.z = NULL, 
  y.test = NULL, option = NULL, ...)
## S3 method for class 'FASSMR.kernel'
predict(object, newdata.x = NULL, newdata.z = NULL,
  y.test = NULL, option = NULL, ...)
## S3 method for class 'FASSMR.kNN'
predict(object, newdata.x = NULL, newdata.z = NULL,
  y.test = NULL, option = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.sfplsim.FASSMR_+3A_object">object</code></td>
<td>

<p>Output of the functions mentioned in the <code>Description</code> (i.e. an object of the class <code>sfplsim.kernel</code>, <code>sfplsim.kNN</code>, <code>FASSMR.kernel</code> or <code>FASSMR.kNN</code>).
</p>
</td></tr>
<tr><td><code id="predict.sfplsim.FASSMR_+3A_newdata.x">newdata.x</code></td>
<td>

<p>A matrix containing new observations of the functional covariate in the functional-single index component collected by row.
</p>
</td></tr>
<tr><td><code id="predict.sfplsim.FASSMR_+3A_newdata.z">newdata.z</code></td>
<td>

<p>Matrix containing the new observations of the scalar covariates (SFPLSIM) or of the scalar covariates coming from the discretisation  of a curve (MFPLSIM), collected by row. 
</p>
</td></tr>
<tr><td><code id="predict.sfplsim.FASSMR_+3A_y.test">y.test</code></td>
<td>

<p>(optional) A vector containing the new observations of the response.
</p>
</td></tr>
<tr><td><code id="predict.sfplsim.FASSMR_+3A_option">option</code></td>
<td>

<p>Allows the choice between 1 and 2. The default is 1. See the section <code>Details</code>.
</p>
</td></tr>
<tr><td><code id="predict.sfplsim.FASSMR_+3A_...">...</code></td>
<td>
<p>Further arguments passed to or from other methods.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Two options are provided to obtain the predictions of the response for <code>newdata.x</code> and <code>newdata.z</code>: 
</p>

<ul>
<li><p> If <code>option=1</code>, we maintain all the estimations (<code>k.opt</code> or <code>h.opt</code>, <code>theta.est</code> and <code>beta.est</code>) to predict the functional single-index component of the model. 
</p>
</li>
<li><p> If <code>option=2</code>, we maintain <code>theta.est</code> and <code>beta.est</code>, while the tuning parameter (<code class="reqn">h</code> or <code class="reqn">k</code>) is selected again to predict the functional single-index component of the model. This selection is performed using the leave-one-out cross-validation criterion in the associated functional single-index model.
</p>
</li></ul>



<h3>Value</h3>

<p>The function returns the predicted values of the response (<code>y</code>) for <code>newdata.x</code> and <code>newdata.z</code>. If <code>!is.null(y.test)</code>, it also provides the mean squared error of prediction (<code>MSEP</code>) computed as <code>mean((y-y.test)^2)</code>.
If <code>is.null(newdata.x)</code> or <code>is.null(newdata.z)</code>, then the function returns the fitted values.
</p>


<h3>Author(s)</h3>

<p>German Aneiros Perez <a href="mailto:german.aneiros@udc.es">german.aneiros@udc.es</a> 
</p>
<p>Silvia Novo Diaz  <a href="mailto:snovo@est-econ.uc3m.es">snovo@est-econ.uc3m.es</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+sfplsim.kernel.fit">sfplsim.kernel.fit</a></code>, <code><a href="#topic+sfplsim.kNN.fit">sfplsim.kNN.fit</a></code>, <code><a href="#topic+FASSMR.kernel.fit">FASSMR.kernel.fit</a></code>  or <code><a href="#topic+FASSMR.kNN.fit">FASSMR.kNN.fit</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data("Tecator")
y&lt;-Tecator$fat
X&lt;-Tecator$absor.spectra2
z1&lt;-Tecator$protein       
z2&lt;-Tecator$moisture

#Quadratic, cubic and interaction effects of the scalar covariates.
z.com&lt;-cbind(z1,z2,z1^2,z2^2,z1^3,z2^3,z1*z2)
train&lt;-1:160
test&lt;-161:215

#SFPLSIM fit. Convergence errors for some theta are obtained.
s.fit.kernel&lt;-sfplsim.kernel.fit(x=X[train,], z=z.com[train,], y=y[train],
            max.q.h=0.35,lambda.min.l=0.01, factor.pn=2, nknot.theta=4,
            criterion="BIC", range.grid=c(850,1050), 
            nknot=20, max.iter=5000)
s.fit.kNN&lt;-sfplsim.kNN.fit(y=y[train],x=X[train,], z=z.com[train,], 
        max.knn=20,lambda.min.l=0.01, factor.pn=2, nknot.theta=4,
        criterion="BIC",range.grid=c(850,1050), 
        nknot=20, max.iter=5000)


predict(s.fit.kernel,newdata.x=X[test,],newdata.z=z.com[test,],
  y.test=y[test],option=2)
predict(s.fit.kNN,newdata.x=X[test,],newdata.z=z.com[test,],
  y.test=y[test],option=2)


data(Sugar)
y&lt;-Sugar$ash
x&lt;-Sugar$wave.290
z&lt;-Sugar$wave.240

#Outliers
index.y.25 &lt;- y &gt; 25
index.atip &lt;- index.y.25
(1:268)[index.atip]


#Dataset to model
x.sug &lt;- x[!index.atip,]
z.sug&lt;- z[!index.atip,]
y.sug &lt;- y[!index.atip]

train&lt;-1:216
test&lt;-217:266

m.fit.kernel &lt;- FASSMR.kernel.fit(x=x.sug[train,],z=z.sug[train,], 
                  y=y.sug[train],  nknot.theta=2, 
                  lambda.min.l=0.03, max.q.h=0.35,num.h = 10, 
                  nknot=20,criterion="BIC", max.iter=5000)


m.fit.kNN&lt;- FASSMR.kNN.fit(x=x.sug[train,],z=z.sug[train,], y=y.sug[train], 
            nknot.theta=2, lambda.min.l=0.03, 
            max.knn=20,nknot=20,criterion="BIC",max.iter=5000)


predict(m.fit.kernel,newdata.x=x.sug[test,],newdata.z=z.sug[test,],
  y.test=y.sug[test],option=2)
predict(m.fit.kNN,newdata.x=x.sug[test,],newdata.z=z.sug[test,],
  y.test=y.sug[test],option=2)

</code></pre>

<hr>
<h2 id='print.summary.fsim'>
Summarise information from FSIM estimation 
</h2><span id='topic+print.fsim.kernel'></span><span id='topic+print.fsim.kNN'></span><span id='topic+summary.fsim.kernel'></span><span id='topic+summary.fsim.kNN'></span>

<h3>Description</h3>

<p><code>summary</code> and <code>print</code> functions for <code>fsim.kNN.fit</code>, <code>fsim.kNN.fit.optim</code>, <code>fsim.kernel.fit</code> and <code>fsim.kernel.fit.optim</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
## S3 method for class 'fsim.kernel'
print(x, ...)
## S3 method for class 'fsim.kNN'
print(x, ...)
## S3 method for class 'fsim.kernel'
summary(object, ...)
## S3 method for class 'fsim.kNN'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.summary.fsim_+3A_x">x</code></td>
<td>

<p>Output of the <code>fsim.kernel.fit</code>, <code>fsim.kernel.fit.optim</code>, <code>fsim.kNN.fit</code> or <code>fsim.kNN.fit.optim</code> functions (i.e. an object of the class <code>fsim.kernel</code> or <code>fsim.kNN</code>).
</p>
</td></tr>
<tr><td><code id="print.summary.fsim_+3A_...">...</code></td>
<td>
<p> Further arguments.</p>
</td></tr>
<tr><td><code id="print.summary.fsim_+3A_object">object</code></td>
<td>

<p>Output of the <code>fsim.kernel.fit</code>, <code>fsim.kernel.fit.optim</code>, <code>fsim.kNN.fit</code> or <code>fsim.kNN.fit.optim</code> functions (i.e. an object of the class <code>fsim.kernel</code> or <code>fsim.kNN</code>).
</p>
</td></tr>
</table>


<h3>Value</h3>


<ul>
<li><p>The matched call.
</p>
</li>
<li><p>The optimal value of the tunning parameter (<code>h.opt</code> or <code>k.opt</code>).
</p>
</li>
<li><p>Coefficients of <code class="reqn">\hat{\theta}</code> in the B-spline basis (<code>theta.est</code>: a vector of <code>length(order.Bspline+nknot.theta).</code>
</p>
</li>
<li><p>Minimum value of the CV function, i.e. the value of CV for <code>theta.est</code> and <code>h.opt</code>/<code>k.opt</code>.
</p>
</li>
<li><p>R squared.
</p>
</li>
<li><p>Residual variance.
</p>
</li>
<li><p>Residual degrees of freedom.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>German Aneiros Perez <a href="mailto:german.aneiros@udc.es">german.aneiros@udc.es</a> 
</p>
<p>Silvia Novo Diaz  <a href="mailto:snovo@est-econ.uc3m.es">snovo@est-econ.uc3m.es</a>
</p>


<h3>See Also</h3>

<p><code>fsim.kernel.fit</code> and <code>fsim.kNN.fit</code>.
</p>

<hr>
<h2 id='print.summary.lm'>
Summarise information from linear models estimation
</h2><span id='topic+print.lm.pels'></span><span id='topic+print.PVS'></span><span id='topic+summary.lm.pels'></span><span id='topic+summary.PVS'></span>

<h3>Description</h3>

<p><code>summary</code> and <code>print</code> functions for <code>lm.pels.fit</code> and <code>PVS.fit</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'lm.pels'
print(x, ...)
## S3 method for class 'PVS'
print(x, ...)
## S3 method for class 'lm.pels'
summary(object, ...)
## S3 method for class 'PVS'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.summary.lm_+3A_x">x</code></td>
<td>

<p>Output of the <code>lm.pels.fit</code> or <code>PVS.fit</code> functions (i.e. an object of the class <code>lm.pels</code> or <code>PVS</code>).
</p>
</td></tr>
<tr><td><code id="print.summary.lm_+3A_...">...</code></td>
<td>
<p>Further arguments.
</p>
</td></tr>
<tr><td><code id="print.summary.lm_+3A_object">object</code></td>
<td>

<p>Output of the <code>lm.pels.fit</code> or <code>PVS.fit</code> functions (i.e. an object of the class <code>lm.pels</code> or <code>PVS</code>).
</p>
</td></tr>
</table>


<h3>Value</h3>


<ul>
<li><p>The matched call.
</p>
</li>
<li><p>The estimated intercept of the model.
</p>
</li>
<li><p>The estimated vector of linear coefficients (<code>beta.est</code>).
</p>
</li>
<li><p>The number of non-zero components in <code>beta.est</code>.
</p>
</li>
<li><p>The indexes of the non-zero components in <code>beta.est</code>.
</p>
</li>
<li><p>The optimal value of the penalisation parameter (<code>lambda.opt</code>).
</p>
</li>
<li><p>The optimal value of the criterion function, i.e. the value  obtained with <code>lambda.opt</code> and <code>vn.opt</code> (and <code>w.opt</code> in the case of the PVS).
</p>
</li>
<li><p>Minimum value of the penalised least-squares function. That is, the value obtained using <code>beta.est</code> and <code>lambda.opt</code>.
</p>
</li>
<li><p>The penalty function used.
</p>
</li>
<li><p>The criterion used to select the penalisation parameter and <code>vn</code>.
</p>
</li>
<li><p>The optimal value of <code>vn</code> in the case of the <code>lm.pels</code> object.
</p>
</li></ul>

<p>In the case of the <code>PVS</code> objects, these functions also return
the optimal number of covariates required to construct the reduced model in the first step of the algorithm (<code>w.opt</code>). This value is selected using the same criterion employed for selecting the penalisation parameter.
</p>


<h3>Author(s)</h3>

<p>German Aneiros Perez <a href="mailto:german.aneiros@udc.es">german.aneiros@udc.es</a> 
</p>
<p>Silvia Novo Diaz  <a href="mailto:snovo@est-econ.uc3m.es">snovo@est-econ.uc3m.es</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+lm.pels.fit">lm.pels.fit</a></code> and <code><a href="#topic+PVS.fit">PVS.fit</a></code>.
</p>

<hr>
<h2 id='print.summary.mfpl'>
Summarise information from MFPLM estimation 
</h2><span id='topic+print.PVS.kernel'></span><span id='topic+print.PVS.kNN'></span><span id='topic+summary.PVS.kernel'></span><span id='topic+summary.PVS.kNN'></span>

<h3>Description</h3>

<p><code>summary</code> and <code>print</code> functions for <code>PVS.kernel.fit</code> and <code>PVS.kNN.fit</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'PVS.kernel'
print(x, ...)
## S3 method for class 'PVS.kNN'
print(x, ...)
## S3 method for class 'PVS.kernel'
summary(object, ...)
## S3 method for class 'PVS.kNN'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.summary.mfpl_+3A_x">x</code></td>
<td>

<p>Output of the <code>PVS.kernel.fit</code> or <code>PVS.kNN.fit</code> functions (i.e. an object of the class <code>PVS.kernel</code> or <code>PVS.kNN</code>).
</p>
</td></tr>
<tr><td><code id="print.summary.mfpl_+3A_...">...</code></td>
<td>
<p>Further arguments.
</p>
</td></tr>
<tr><td><code id="print.summary.mfpl_+3A_object">object</code></td>
<td>

<p>Output of the <code>PVS.kernel.fit</code> or <code>PVS.kNN.fit</code> functions (i.e. an object of the class <code>PVS.kernel</code> or <code>PVS.kNN</code>).
</p>
</td></tr>
</table>


<h3>Value</h3>


<ul>
<li><p>The matched call.
</p>
</li>
<li><p>The optimal value of the tunning parameter (<code>h.opt</code> or <code>k.opt</code>).
</p>
</li>
<li><p>The optimal initial number of covariates to build the reduced model (<code>w.opt</code>).
</p>
</li>
<li><p>The estimated vector of linear coefficients (<code>beta.est</code>).
</p>
</li>
<li><p>The number of non-zero components in <code>beta.est</code>.
</p>
</li>
<li><p>The indexes of the non-zero components in <code>beta.est</code>.
</p>
</li>
<li><p>The optimal value of the penalisation parameter (<code>lambda.opt</code>).
</p>
</li>
<li><p>The optimal value of the criterion function, i.e. the value  obtained with <code>w.opt</code>, <code>lambda.opt</code>, <code>vn.opt</code>  and <code>h.opt</code>/<code>k.opt</code>
</p>
</li>
<li><p>Minimum value of the penalised least-squares function. That is, the value obtained using <code>beta.est</code> and <code>lambda.opt</code>.
</p>
</li>
<li><p>The penalty function used.
</p>
</li>
<li><p>The criterion used to select the number of covariates employed to construct the reduced model, the tuning parameter, the penalisation parameter and <code>vn</code>.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>German Aneiros Perez <a href="mailto:german.aneiros@udc.es">german.aneiros@udc.es</a> 
</p>
<p>Silvia Novo Diaz  <a href="mailto:snovo@est-econ.uc3m.es">snovo@est-econ.uc3m.es</a>
</p>


<h3>See Also</h3>

<p><code>PVS.kernel.fit</code> and <code>PVS.kNN.fit</code>.
</p>

<hr>
<h2 id='print.summary.mfplsim'>
Summarise information from MFPLSIM estimation
</h2><span id='topic+print.FASSMR.kernel'></span><span id='topic+print.FASSMR.kNN'></span><span id='topic+print.IASSMR.kernel'></span><span id='topic+print.IASSMR.kNN'></span><span id='topic+summary.FASSMR.kernel'></span><span id='topic+summary.FASSMR.kNN'></span><span id='topic+summary.IASSMR.kernel'></span><span id='topic+summary.IASSMR.kNN'></span>

<h3>Description</h3>

<p><code>summary</code> and <code>print</code> functions for <code>FASSMR.kernel.fit</code>, <code>FASSMR.kNN.fit</code>, <code>IASSMR.kernel.fit</code> and <code>IASSMR.kNN.fit</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'FASSMR.kernel'
print(x, ...)
## S3 method for class 'FASSMR.kNN'
print(x, ...)
## S3 method for class 'IASSMR.kernel'
print(x, ...)
## S3 method for class 'IASSMR.kNN'
print(x, ...)
## S3 method for class 'FASSMR.kernel'
summary(object, ...)
## S3 method for class 'FASSMR.kNN'
summary(object, ...)
## S3 method for class 'IASSMR.kernel'
summary(object, ...)
## S3 method for class 'IASSMR.kNN'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.summary.mfplsim_+3A_x">x</code></td>
<td>

<p>Output of the <code>FASSMR.kernel.fit</code>, <code>FASSMR.kNN.fit</code>, <code>IASSMR.kernel.fit</code> or <code>IASSMR.kNN.fit</code> functions (i.e. an object of the class <code>FASSMR.kernel</code>,  <code>FASSMR.kNN</code>, <code>IASSMR.kernel</code> or <code>IASSMR.kNN</code>).
</p>
</td></tr>
<tr><td><code id="print.summary.mfplsim_+3A_...">...</code></td>
<td>
<p>Further arguments passed to or from other methods.
</p>
</td></tr>
<tr><td><code id="print.summary.mfplsim_+3A_object">object</code></td>
<td>

<p>Output of the <code>FASSMR.kernel.fit</code>, <code>FASSMR.kNN.fit</code>, <code>IASSMR.kernel.fit</code> or <code>IASSMR.kNN.fit</code> functions (i.e. an object of the class <code>FASSMR.kernel</code>,  <code>FASSMR.kNN</code>, <code>IASSMR.kernel</code> or <code>IASSMR.kNN</code>).
</p>
</td></tr>
</table>


<h3>Value</h3>


<ul>
<li><p>The matched call.
</p>
</li>
<li><p>The optimal value of the tunning parameter (<code>h.opt</code> or <code>k.opt</code>).
</p>
</li>
<li><p>The optimal initial number of covariates to build the reduced model (<code>w.opt</code>).
</p>
</li>
<li><p>Coefficients of <code class="reqn">\hat{\theta}</code> in the B-spline basis (<code>theta.est</code>): a vector of <code>length(order.Bspline+nknot.theta).</code>
</p>
</li>
<li><p>The estimated vector of linear coefficients (<code>beta.est</code>).
</p>
</li>
<li><p>The number of non-zero components in <code>beta.est</code>.
</p>
</li>
<li><p>The indexes of the non-zero components in <code>beta.est</code>.
</p>
</li>
<li><p>The optimal value of the penalisation parameter (<code>lambda.opt</code>).
</p>
</li>
<li><p>The optimal value of the criterion function, i.e. the value  obtained with <code>w.opt</code>, <code>lambda.opt</code>, <code>vn.opt</code>  and <code>h.opt</code>/<code>k.opt</code>
</p>
</li>
<li><p>Minimum value of the penalised least-squares function. That is, the value obtained using <code>theta.est</code>, <code>beta.est</code> and <code>lambda.opt</code>.
</p>
</li>
<li><p>The penalty function used.
</p>
</li>
<li><p>The criterion used to select the number of covariates employed to construct the reduced model, the tuning parameter, the penalisation parameter and <code>vn</code>.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>German Aneiros Perez <a href="mailto:german.aneiros@udc.es">german.aneiros@udc.es</a> 
</p>
<p>Silvia Novo Diaz  <a href="mailto:snovo@est-econ.uc3m.es">snovo@est-econ.uc3m.es</a>
</p>


<h3>See Also</h3>

<p><code>FASSMR.kernel.fit</code>, <code>FASSMR.kNN.fit</code>, <code>IASSMR.kernel.fit</code> and <code>IASSMR.kNN.fit</code>.
</p>

<hr>
<h2 id='print.summary.sfpl'>
Summarise information from SFPLM estimation 
</h2><span id='topic+print.sfpl.kernel'></span><span id='topic+print.sfpl.kNN'></span><span id='topic+summary.sfpl.kernel'></span><span id='topic+summary.sfpl.kNN'></span>

<h3>Description</h3>

<p><code>summary</code> and <code>print</code> functions for <code>sfpl.kNN.fit</code> and <code>sfpl.kernel.fit</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'sfpl.kernel'
print(x, ...)
## S3 method for class 'sfpl.kNN'
print(x, ...)
## S3 method for class 'sfpl.kernel'
summary(object, ...)
## S3 method for class 'sfpl.kNN'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.summary.sfpl_+3A_x">x</code></td>
<td>

<p>Output of the <code>sfpl.kernel.fit</code> or <code>sfpl.kNN.fit</code> functions (i.e. an object of the class <code>sfpl.kernel</code> or <code>sfpl.kNN</code>).
</p>
</td></tr>
<tr><td><code id="print.summary.sfpl_+3A_...">...</code></td>
<td>

<p>Further arguments.
</p>
</td></tr>
<tr><td><code id="print.summary.sfpl_+3A_object">object</code></td>
<td>

<p>Output of the <code>sfpl.kernel.fit</code> or <code>sfpl.kNN.fit</code> functions (i.e. an object of the class <code>sfpl.kernel</code> or <code>sfpl.kNN</code>).
</p>
</td></tr>
</table>


<h3>Value</h3>


<ul>
<li><p>The matched call.
</p>
</li>
<li><p>The optimal value of the tunning parameter (<code>h.opt</code> or <code>k.opt</code>).
</p>
</li>
<li><p>The estimated vector of linear coefficients (<code>beta.est</code>).
</p>
</li>
<li><p>The number of non-zero components in <code>beta.est</code>.
</p>
</li>
<li><p>The indexes of the non-zero components in <code>beta.est</code>.
</p>
</li>
<li><p>The optimal value of the penalisation parameter (<code>lambda.opt</code>).
</p>
</li>
<li><p>The optimal value of the criterion function, i.e. the value  obtained with <code>lambda.opt</code>, <code>vn.opt</code>  and <code>h.opt</code>/<code>k.opt</code>
</p>
</li>
<li><p>Minimum value of the penalised least-squares function. That is, the value obtained using <code>beta.est</code> and <code>lambda.opt</code>.
</p>
</li>
<li><p>The penalty function used.
</p>
</li>
<li><p>The criterion used to select the tuning parameter, the penalisation parameter and <code>vn</code>.
</p>
</li>
<li><p>The optimal value of <code>vn</code>.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>German Aneiros Perez <a href="mailto:german.aneiros@udc.es">german.aneiros@udc.es</a> 
</p>
<p>Silvia Novo Diaz  <a href="mailto:snovo@est-econ.uc3m.es">snovo@est-econ.uc3m.es</a>
</p>


<h3>See Also</h3>

<p><code>sfpl.kernel.fit</code> and <code>sfpl.kNN.fit</code>.
</p>

<hr>
<h2 id='print.summary.sfplsim'>
Summarise information from SFPLSIM estimation 
</h2><span id='topic+print.sfplsim.kernel'></span><span id='topic+print.sfplsim.kNN'></span><span id='topic+summary.sfplsim.kernel'></span><span id='topic+summary.sfplsim.kNN'></span>

<h3>Description</h3>

<p><code>summary</code> and <code>print</code> functions for <code>sfplsim.kNN.fit</code> and <code>sfplsim.kernel.fit</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'sfplsim.kernel'
print(x, ...)
## S3 method for class 'sfplsim.kNN'
print(x, ...)
## S3 method for class 'sfplsim.kernel'
summary(object, ...)
## S3 method for class 'sfplsim.kNN'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.summary.sfplsim_+3A_x">x</code></td>
<td>

<p>Output of the <code>sfplsim.kernel.fit</code> or <code>sfplsim.kNN.fit</code> functions (i.e. an object of the class <code>sfplsim.kernel</code> or <code>sfplsim.kNN</code>).
</p>
</td></tr>
<tr><td><code id="print.summary.sfplsim_+3A_...">...</code></td>
<td>

<p>Further arguments.
</p>
</td></tr>
<tr><td><code id="print.summary.sfplsim_+3A_object">object</code></td>
<td>

<p>Output of the <code>sfplsim.kernel.fit</code> or <code>sfplsim.kNN.fit</code> functions (i.e. an object of the class <code>sfplsim.kernel</code> or <code>sfplsim.kNN</code>).
</p>
</td></tr>
</table>


<h3>Value</h3>


<ul>
<li><p>The matched call.
</p>
</li>
<li><p>The optimal value of the tunning parameter (<code>h.opt</code> or <code>k.opt</code>).
</p>
</li>
<li><p>Coefficients of <code class="reqn">\hat{\theta}</code> in the B-spline basis (<code>theta.est</code>): a vector of <code>length(order.Bspline+nknot.theta).</code>
</p>
</li>
<li><p>The estimated vector of linear coefficients (<code>beta.est</code>).
</p>
</li>
<li><p>The number of non-zero components in <code>beta.est</code>.
</p>
</li>
<li><p>The indexes of the non-zero components in <code>beta.est</code>.
</p>
</li>
<li><p>The optimal value of the penalisation parameter (<code>lambda.opt</code>).
</p>
</li>
<li><p>The optimal value of the criterion function, i.e. the value  obtained with <code>lambda.opt</code>, <code>vn.opt</code>  and <code>h.opt</code>/<code>k.opt</code>
</p>
</li>
<li><p>Minimum value of the penalised least-squares function. That is, the value obtained using <code>theta.est</code>, <code>beta.est</code> and <code>lambda.opt</code>.
</p>
</li>
<li><p>The penalty function used.
</p>
</li>
<li><p>The criterion used to select the tuning parameter, the penalisation parameter and <code>vn</code>.
</p>
</li>
<li><p>The optimal value of <code>vn</code>.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>German Aneiros Perez <a href="mailto:german.aneiros@udc.es">german.aneiros@udc.es</a> 
</p>
<p>Silvia Novo Diaz  <a href="mailto:snovo@est-econ.uc3m.es">snovo@est-econ.uc3m.es</a>
</p>


<h3>See Also</h3>

<p><code>sfplsim.kernel.fit</code> and <code>sfplsim.kNN.fit</code>.
</p>

<hr>
<h2 id='projec'>Inner product computation</h2><span id='topic+projec'></span>

<h3>Description</h3>

<p>Computes the inner product between each curve collected in <code>data</code> and a particular curve  <code class="reqn">\theta</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>projec(data, theta, order.Bspline = 3, nknot.theta = 3, range.grid = NULL, nknot = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="projec_+3A_data">data</code></td>
<td>

<p>Matrix containing functional data collected by row
</p>
</td></tr>
<tr><td><code id="projec_+3A_theta">theta</code></td>
<td>

<p>Vector containing the coefficients of <code class="reqn">\theta</code> in a B-spline basis, so that <code>length(theta)=order.Bspline+nknot.theta</code>
</p>
</td></tr>
<tr><td><code id="projec_+3A_order.bspline">order.Bspline</code></td>
<td>

<p>Order of the B-spline basis functions for the B-spline representation of <code class="reqn">\theta</code>. This is the number of coefficients in each piecewise polynomial segment. The default is 3.
</p>
</td></tr>
<tr><td><code id="projec_+3A_nknot.theta">nknot.theta</code></td>
<td>

<p>Number of regularly spaced interior knots of the B-spline basis. The default is 3.
</p>
</td></tr>
<tr><td><code id="projec_+3A_range.grid">range.grid</code></td>
<td>

<p>Vector of length 2 containing  the range of the discretisation of the functional data. If <code>range.grid=NULL</code>, then <code>range.grid=c(1,p)</code> is considered, where <code>p</code> is the discretisation size of <code>data</code> (i.e. <code>ncol(data)</code>).
</p>
</td></tr>
<tr><td><code id="projec_+3A_nknot">nknot</code></td>
<td>

<p>Number of regularly spaced interior knots for the B-spline representation of the functional data. The default value is <code>(p - order.Bspline - 1)%/%2</code>.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A matrix containing the inner products.
</p>


<h3>Note</h3>

<p>The construction of this code is based on that by Frederic Ferraty, which is available on his website <a href="https://www.math.univ-toulouse.fr/~ferraty/SOFTWARES/NPFDA/index.html">https://www.math.univ-toulouse.fr/~ferraty/SOFTWARES/NPFDA/index.html</a>.</p>


<h3>Author(s)</h3>

<p>German Aneiros Perez <a href="mailto:german.aneiros@udc.es">german.aneiros@udc.es</a> 
</p>
<p>Silvia Novo Diaz  <a href="mailto:snovo@est-econ.uc3m.es">snovo@est-econ.uc3m.es</a>
</p>


<h3>References</h3>

<p>Novo S., Aneiros, G., and Vieu, P., (2019) Automatic and location-adaptive estimation in functional single&ndash;index regression. <em>Journal of Nonparametric Statistics</em>, <b>31(2)</b>, 364&ndash;392, <a href="https://doi.org/10.1080/10485252.2019.1567726">doi:10.1080/10485252.2019.1567726</a>.
</p>


<h3>See Also</h3>

<p>See also <code><a href="#topic+semimetric.projec">semimetric.projec</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data("Tecator")
names(Tecator)
y&lt;-Tecator$fat
X&lt;-Tecator$absor.spectra

#length(theta)=6=order.Bspline+nknot.theta 
projec(X,theta=c(1,0,0,1,1,-1),nknot.theta=3,nknot=20,range.grid=c(850,1050))
</code></pre>

<hr>
<h2 id='PVS.fit'>
Impact point selection with PVS
</h2><span id='topic+PVS.fit'></span>

<h3>Description</h3>

<p>This function implements the Partitioning Variable Selection (PVS) algorithm. This algorithm is specifically designed for estimating multivarite linear models, where the scalar covariates are derived from the discretisation of a curve.
</p>
<p>PVS is a two-stage procedure that selects the impact points of the discretised curve and estimates the model. The algorithm employs a penalised least-squares regularisation procedure. Additionally, it utilises an objective criterion (<code>criterion</code>) to determine the initial number of covariates in the reduced model (<code>w.opt</code>) of the first stage, and the penalisation parameter (<code>lambda.opt</code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>PVS.fit(z, y, train.1 = NULL, train.2 = NULL, lambda.min = NULL, 
lambda.min.h = NULL, lambda.min.l = NULL, factor.pn = 1, nlambda = 100, 
vn = ncol(z), nfolds = 10, seed = 123, wn = c(10, 15, 20), range.grid = NULL, 
criterion = "GCV", penalty = "grSCAD", max.iter = 1000)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="PVS.fit_+3A_z">z</code></td>
<td>

<p>Matrix containing the observations of the functional covariate collected by row (linear component).
</p>
</td></tr>
<tr><td><code id="PVS.fit_+3A_y">y</code></td>
<td>

<p>Vector containing the scalar response.
</p>
</td></tr>
<tr><td><code id="PVS.fit_+3A_train.1">train.1</code></td>
<td>

<p>Positions of the data that are used as the training sample in the 1st step. The default setting is  <code>train.1&lt;-1:ceiling(n/2)</code>.
</p>
</td></tr>
<tr><td><code id="PVS.fit_+3A_train.2">train.2</code></td>
<td>

<p>Positions of the data that are used as the training sample in the 2nd step. The default setting is <code>train.2&lt;-(ceiling(n/2)+1):n</code>.
</p>
</td></tr>
<tr><td><code id="PVS.fit_+3A_lambda.min">lambda.min</code></td>
<td>

<p>The smallest value for lambda (i. e., the lower endpoint  of the sequence in which <code>lambda.opt</code> is selected), as fraction of <code>lambda.max</code>.
The defaults is <code>lambda.min.l</code> if the sample size is larger than <code>factor.pn</code> times the number of linear covariates and <code>lambda.min.h</code> otherwise.
</p>
</td></tr>
<tr><td><code id="PVS.fit_+3A_lambda.min.h">lambda.min.h</code></td>
<td>

<p>The lower endpoint of the sequence in which <code>lambda.opt</code> is selected if the sample size is smaller than <code>factor.pn</code> times the number of linear covariates. The default is 0.05. 
</p>
</td></tr>
<tr><td><code id="PVS.fit_+3A_lambda.min.l">lambda.min.l</code></td>
<td>

<p>The lower endpoint of the sequence in which <code>lambda.opt</code> is selected if the sample size is larger than <code>factor.pn</code> times the number of linear covariates. The default is 0.0001.
</p>
</td></tr>
<tr><td><code id="PVS.fit_+3A_factor.pn">factor.pn</code></td>
<td>

<p>Positive integer used to set <code>lambda.min</code>. The default value is 1.
</p>
</td></tr>
<tr><td><code id="PVS.fit_+3A_nlambda">nlambda</code></td>
<td>

<p>Number of values in the sequence from which <code>lambda.opt</code> is selected. The default is 100.
</p>
</td></tr>
<tr><td><code id="PVS.fit_+3A_vn">vn</code></td>
<td>

<p>Positive integer or vector of positive integers indicating the number of groups of consecutive variables to be penalised together. The default value is <code>vn=ncol(z)</code>, resulting in the individual penalization of each scalar covariate.
</p>
</td></tr>
<tr><td><code id="PVS.fit_+3A_nfolds">nfolds</code></td>
<td>

<p>Number of cross-validation folds (used when <code>criterion="k-fold-CV"</code>). Default is 10.
</p>
</td></tr>
<tr><td><code id="PVS.fit_+3A_seed">seed</code></td>
<td>

<p>You may set the seed for the random number generator to ensure reproducible results (applicable when <code>criterion="k-fold-CV"</code> is used). The default seed value is 123.
</p>
</td></tr>
<tr><td><code id="PVS.fit_+3A_wn">wn</code></td>
<td>

<p>A vector of positive integers indicating the eligible number of covariates in the reduced model. For more information, refer to the section <code>Details</code>. The default is <code>c(10,15,20)</code>.
</p>
</td></tr>
<tr><td><code id="PVS.fit_+3A_range.grid">range.grid</code></td>
<td>

<p>Vector of length 2 containing the endpoints of the grid at which the observations of the functional covariate <code>x</code> are evaluated (i.e. the range of the discretisation). If <code>range.grid=NULL</code>, then <code>range.grid=c(1,p)</code> is considered, where <code>p</code> is the discretisation size of <code>x</code> (i.e. <code>ncol(x))</code>.
</p>
</td></tr>
<tr><td><code id="PVS.fit_+3A_criterion">criterion</code></td>
<td>

<p>The criterion used to select the tuning and regularisation parameters: <code>wn.opt</code> and <code>lambda.opt</code> (also <code>vn.opt</code> if needed). Options include <code>"GCV"</code>, <code>"BIC"</code>, <code>"AIC"</code>, or <code>"k-fold-CV"</code>. The default setting is <code>"GCV"</code>.
</p>
</td></tr>
<tr><td><code id="PVS.fit_+3A_penalty">penalty</code></td>
<td>

<p>The penalty function applied in the penalised least-squares procedure. Currently, only &quot;grLasso&quot; and &quot;grSCAD&quot; are implemented. The default is &quot;grSCAD&quot;.
</p>
</td></tr>
<tr><td><code id="PVS.fit_+3A_max.iter">max.iter</code></td>
<td>

<p>Maximum number of iterations allowed across the entire path. The default value is 1000.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The sparse linear model with covariates coming from the discretization of a curve is given by the expression
</p>
<p style="text-align: center;"><code class="reqn">Y_i=\sum_{j=1}^{p_n}\beta_{0j}\zeta_i(t_j)+\varepsilon_i,\ \ \ (i=1,\dots,n)</code>
</p>

<p>where 
</p>

<ul>
<li> <p><code class="reqn">Y_i</code> is a real random response and  <code class="reqn">\zeta_i</code> is assumed to be a random curve defined on some interval <code class="reqn">[a,b]</code>, which is observed at the points <code class="reqn">a\leq t_1&lt;\dots&lt;t_{p_n}\leq b</code>. 
</p>
</li>
<li>  <p><code class="reqn">\mathbf{\beta}_0=(\beta_{01},\dots,\beta_{0p_n})^{\top}</code> is a vector of unknown real coefficients.
</p>
</li>
<li> <p><code class="reqn">\varepsilon_i</code> denotes the random error.
</p>
</li></ul>

<p>In  this model, it is assumed that only a few scalar variables from the set <code class="reqn">\{\zeta(t_1),\dots,\zeta(t_{p_n})\}</code> are part of the model. Therefore, the relevant variables (the impact points of the curve <code class="reqn">\zeta</code> on the response) must be selected, and the model estimated.
</p>
<p>In this function, this model is fitted using the PVS. The PVS is a two-steps procedure. So we divide the sample into two independent subsamples, each asymptotically half the size of the original sample (<code class="reqn">n_1\sim n_2\sim n/2</code>). One subsample is used in the first stage of the method, and the other in the second stage.The subsamples are defined as follows:
</p>
<p style="text-align: center;"><code class="reqn">
\mathcal{E}^{\mathbf{1}}=\{(\zeta_i,\mathcal{X}_i,Y_i),\quad i=1,\dots,n_1\},
</code>
</p>

<p style="text-align: center;"><code class="reqn">
\mathcal{E}^{\mathbf{2}}=\{(\zeta_i,\mathcal{X}_i,Y_i),\quad i=n_1+1,\dots,n_1+n_2=n\}.
</code>
</p>
 
<p>Note that these two subsamples are specified to the program through the arguments <code>train.1</code> and <code>train.2</code>. The superscript <code class="reqn">\mathbf{s}</code>, where <code class="reqn">\mathbf{s}=\mathbf{1},\mathbf{2}</code>, indicates the stage of the method in which the sample, function, variable, or parameter is involved.
</p>
<p>To explain the algorithm, we assume that the number <code class="reqn">p_n</code> of linear covariates can be expressed as follows: <code class="reqn">p_n=q_nw_n</code>, with <code class="reqn">q_n</code> and <code class="reqn">w_n</code> being integers.
</p>

<ol>
<li> <p><b>First step</b>. A reduced model is considered, discarding many linear covariates. The penalised least-squares procedure is applied to the reduced model using only the subsample <code class="reqn">\mathcal{E}^{\mathbf{1}}</code>. Specifically:
</p>

<ul>
<li><p> Consider a subset of the initial <code class="reqn">p_n</code> linear covariates, containing only <code class="reqn">w_n</code> equally spaced discretized observations of <code class="reqn">\zeta</code> covering the interval <code class="reqn">[a,b]</code>. This subset is the following:
</p>
<p style="text-align: center;"><code class="reqn">
	\mathcal{R}_n^{\mathbf{1}}=\left\{\zeta\left(t_k^{\mathbf{1}}\right),\ \ k=1,\dots,w_n\right\},
</code>
</p>
 
<p>where  <code class="reqn">t_k^{\mathbf{1}}=t_{\left[(2k-1)q_n/2\right]}</code> and  <code class="reqn">\left[z\right]</code> denotes the smallest integer not less than the real number <code class="reqn">z</code>. The size (cardinality) of this subset is provided to the program in the argument <code>wn</code> (which contains a sequence of eligible sizes).
</p>
</li>
<li><p>  Consider the following reduced model involving only the <code class="reqn">w_n</code> linear covariates from <code class="reqn">\mathcal{R}_n^{\mathbf{1}}</code>: <code class="reqn">\mathcal{R}_n^{\mathbf{1}}</code>:
</p>
<p style="text-align: center;"><code class="reqn">
	Y_i=\sum_{k=1}^{w_n}\beta_{0k}^{\mathbf{1}}\zeta_i(t_k^{\mathbf{1}})+\varepsilon_i^{\mathbf{1}}.
</code>
</p>

<p>The penalised least-squares variable selection procedure is applied to the reduced model using the function <code><a href="#topic+lm.pels.fit">lm.pels.fit</a></code>, which requires the remaining arguments (for details, see the documentation of the function <code><a href="#topic+lm.pels.fit">lm.pels.fit</a></code>). The estimates obtained are the outputs of the first step of the algorithm.
</p>
</li></ul>

</li>
<li> <p><b>Second step</b>. The variables selected in the first step, along with the variables in their neighborhood, are included. Then the penalised least-squares procedure is carried out again considering only the subsample <code class="reqn">\mathcal{E}^{\mathbf{2}}</code>. Specifically:
</p>

<ul>
<li><p> Consider a new set of variables :
</p>
<p style="text-align: center;"><code class="reqn">
		\mathcal{R}_n^{\mathbf{2}}=\bigcup_{\left\{k,\widehat{\beta}_{0k}^{\mathbf{1}}\not=0\right\}}\left\{\zeta(t_{(k-1)q_n+1}),\dots,\zeta(t_{kq_n})\right\}.
	</code>
</p>

<p>Denoting by <code class="reqn">r_n=\sharp(\mathcal{R}_n^{\mathbf{2}})</code>, we can rename the variables in <code class="reqn">\mathcal{R}_n^{\mathbf{2}}</code> as follows:
</p>
<p style="text-align: center;"><code class="reqn">
		\mathcal{R}_n^{\mathbf{2}}=\left\{\zeta(t_1^{\mathbf{2}}),\dots,\zeta(t_{r_n}^{\mathbf{2}})\right\},
		</code>
</p>

</li>
<li><p>  Consider the following model, which involves only the linear covariates belonging to <code class="reqn">\mathcal{R}_n^{\mathbf{2}}</code>
</p>
<p style="text-align: center;"><code class="reqn">
		Y_i=\sum_{k=1}^{r_n}\beta_{0k}^{\mathbf{2}}\zeta_i(t_k^{\mathbf{2}})+\varepsilon_i^{\mathbf{2}}.</code>
</p>

<p>The penalised least-squares variable selection procedure is applied to this model using <code><a href="#topic+lm.pels.fit">lm.pels.fit</a></code>. 
</p>
</li></ul>

</li></ol>

<p>The outputs of the second step are the estimates of the model. For further details on this algorithm, see Aneiros and Vieu (2014).
</p>
<p><b>Remark</b>: If the condition  <code class="reqn">p_n=w_n q_n</code> is not met (then <code class="reqn">p_n/w_n</code> is not an integer), the function considers variable <code class="reqn">q_n=q_{n,k}</code> values <code class="reqn">k=1,\dots,w_n</code>. Specifically:
</p>
<p style="text-align: center;"><code class="reqn">
	q_{n,k}= \left\{\begin{array}{ll}
	[p_n/w_n]+1 &amp;   k\in\{1,\dots,p_n-w_n[p_n/w_n]\},\\
	{[p_n/w_n]} &amp; k\in\{p_n-w_n[p_n/w_n]+1,\dots,w_n\},
	\end{array}
	\right.
</code>
</p>

<p>where <code class="reqn">[z]</code> denotes the integer part of the real number <code class="reqn">z</code>.
</p>


<h3>Value</h3>

<table>
<tr><td><code>call</code></td>
<td>
<p>The matched call.</p>
</td></tr>
<tr><td><code>fitted.values</code></td>
<td>
<p>Estimated scalar response.</p>
</td></tr>
<tr><td><code>residuals</code></td>
<td>
<p>Differences between <code>y</code> and the <code>fitted.values</code>.</p>
</td></tr>
<tr><td><code>beta.est</code></td>
<td>
<p><code class="reqn">\hat{\mathbf{\beta}}</code> (i. e. estimate of <code class="reqn">\mathbf{\beta}_0</code> when the optimal tuning parameters <code>w.opt</code> and <code>lambda.opt</code> are used).</p>
</td></tr>
<tr><td><code>indexes.beta.nonnull</code></td>
<td>
<p>Indexes of the non-zero <code class="reqn">\hat{\beta_{j}}</code>.</p>
</td></tr>
<tr><td><code>w.opt</code></td>
<td>
<p>Selected size for  <code class="reqn">\mathcal{R}_n^{\mathbf{1}}</code>.</p>
</td></tr>
<tr><td><code>lambda.opt</code></td>
<td>
<p>Selected value of the penalisation parameter <code class="reqn">\lambda</code> (when <code>w.opt</code> is considered).</p>
</td></tr>
<tr><td><code>IC</code></td>
<td>
<p>Value of the criterion function considered to select <code>w.opt</code> and <code>lambda.opt</code>.</p>
</td></tr>
<tr><td><code>beta2</code></td>
<td>
<p>Estimate of <code class="reqn">\mathbf{\beta}_0^{\mathbf{2}}</code> for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>indexes.beta.nonnull2</code></td>
<td>
<p>Indexes of the non-zero linear coefficients after the step 2 of the method for each value of the sequence <code>wn</code>.</p>
</td></tr> 
<tr><td><code>IC2</code></td>
<td>
<p>Optimal value of the criterion function in the second step for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>lambda2</code></td>
<td>
<p>Selected value of penalisation parameter in the second step for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>index02</code></td>
<td>
<p>Indexes of the covariates (in the entire set of <code class="reqn">p_n</code>) used to build <code class="reqn">\mathcal{R}_n^{\mathbf{2}}</code> for each value of the sequence <code>wn</code>.</p>
</td></tr> 
<tr><td><code>beta1</code></td>
<td>
<p>Estimate of <code class="reqn">\mathbf{\beta}_0^{\mathbf{1}}</code> for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>IC1</code></td>
<td>
<p>Optimal value of the criterion function in the first step for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>lambda1</code></td>
<td>
<p>Selected value of penalisation parameter in the first step for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>index01</code></td>
<td>
<p>Indexes of the covariates (in the entire set of <code class="reqn">p_n</code>) used to build <code class="reqn">\mathcal{R}_n^{\mathbf{1}}</code> for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>index1</code></td>
<td>
<p>Indexes of the non-zero linear coefficients after the step 1 of the method for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>...</code></td>
<td>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>German Aneiros Perez <a href="mailto:german.aneiros@udc.es">german.aneiros@udc.es</a> 
</p>
<p>Silvia Novo Diaz  <a href="mailto:snovo@est-econ.uc3m.es">snovo@est-econ.uc3m.es</a>
</p>


<h3>References</h3>

<p>Aneiros, G. and Vieu, P. (2014) Variable selection in infinite-dimensional problems. <em>Statistics  &amp; Probability Letters</em>, <b>94</b>, 12&ndash;20, <a href="https://doi.org/10.1016/j.spl.2014.06.025">doi:10.1016/j.spl.2014.06.025</a>.
</p>


<h3>See Also</h3>

<p>See also <code><a href="#topic+lm.pels.fit">lm.pels.fit</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Sugar)

y&lt;-Sugar$ash
z&lt;-Sugar$wave.240

#Outliers
index.y.25 &lt;- y &gt; 25
index.atip &lt;- index.y.25
(1:268)[index.atip]


#Dataset to model
z.sug&lt;- z[!index.atip,]
y.sug &lt;- y[!index.atip]

train&lt;-1:216

ptm=proc.time()
fit&lt;- PVS.fit(z=z.sug[train,], y=y.sug[train],train.1=1:108,train.2=109:216,
        lambda.min.h=0.2,criterion="BIC", max.iter=5000)
proc.time()-ptm

fit 
names(fit)
</code></pre>

<hr>
<h2 id='PVS.kernel.fit'>
Impact point selection with PVS and kernel estimation
</h2><span id='topic+PVS.kernel.fit'></span>

<h3>Description</h3>

<p>This function computes the partitioning variable selection (PVS) algorithm for multi-functional partial linear models (MFPLM).
</p>
<p>PVS is a two-stage procedure that selects the impact points of the discretised curve and estimates the model. The algorithm employs a penalised least-squares regularisation procedure, integrated with kernel estimation with Nadaraya-Watson weights.
Additionally, it utilises an objective criterion (<code>criterion</code>) to select the number of covariates in the reduced model (<code>w.opt</code>), the bandwidth (<code>h.opt</code>) and the penalisation parameter (<code>lambda.opt</code>). 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>PVS.kernel.fit(x, z, y, train.1 = NULL, train.2 = NULL, semimetric = "deriv", 
q = NULL, min.q.h = 0.05, max.q.h = 0.5, h.seq = NULL, num.h = 10, 
range.grid = NULL, kind.of.kernel = "quad", nknot = NULL, lambda.min = NULL, 
lambda.min.h = NULL, lambda.min.l = NULL, factor.pn = 1, nlambda = 100, 
vn = ncol(z), nfolds = 10, seed = 123, wn = c(10, 15, 20), criterion = "GCV", 
penalty = "grSCAD", max.iter = 1000)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="PVS.kernel.fit_+3A_x">x</code></td>
<td>

<p>Matrix containing the observations of the functional covariate (functional nonparametric component), collected by row.
</p>
</td></tr>
<tr><td><code id="PVS.kernel.fit_+3A_z">z</code></td>
<td>

<p>Matrix containing the observations of the functional covariate that is discretised (linear component), collected by row.
</p>
</td></tr>
<tr><td><code id="PVS.kernel.fit_+3A_y">y</code></td>
<td>

<p>Vector containing the scalar response.
</p>
</td></tr>
<tr><td><code id="PVS.kernel.fit_+3A_train.1">train.1</code></td>
<td>

<p>Positions of the data that are used as the training sample in the 1st step. The default setting is  <code>train.1&lt;-1:ceiling(n/2)</code>.
</p>
</td></tr>
<tr><td><code id="PVS.kernel.fit_+3A_train.2">train.2</code></td>
<td>

<p>Positions of the data that are used as the training sample in the 2nd step. The default setting is <code>train.2&lt;-(ceiling(n/2)+1):n</code>.
</p>
</td></tr>
<tr><td><code id="PVS.kernel.fit_+3A_semimetric">semimetric</code></td>
<td>

<p>Semi-metric function.  Only <code>"deriv"</code> and <code>"pca"</code> are implemented.  By default <code>semimetric="deriv"</code>.
</p>
</td></tr>
<tr><td><code id="PVS.kernel.fit_+3A_q">q</code></td>
<td>

<p>Order of the derivative (if <code>semimetric="deriv"</code>) or number of principal components (if <code>semimetric="pca"</code>). The default values are 0 and 2, respectively.
</p>
</td></tr>
<tr><td><code id="PVS.kernel.fit_+3A_min.q.h">min.q.h</code></td>
<td>
<p>Minimum quantile order of the distances between curves, which are computed using the projection semi-metric. This value determines the lower endpoint of the range from which the bandwidth is selected. The default is 0.05.
</p>
</td></tr>
<tr><td><code id="PVS.kernel.fit_+3A_max.q.h">max.q.h</code></td>
<td>

<p>Maximum quantile order of the distances between curves, which are computed using the projection semi-metric. This value determines the upper endpoint of the range from which the bandwidth is selected. The default is 0.5.
</p>
</td></tr>
<tr><td><code id="PVS.kernel.fit_+3A_h.seq">h.seq</code></td>
<td>
<p>Vector containing the sequence of bandwidths. The default is a sequence of <code>num.h</code> equispaced bandwidths in the range constructed using <code>min.q.h</code> and <code>max.q.h</code>.
</p>
</td></tr>
<tr><td><code id="PVS.kernel.fit_+3A_num.h">num.h</code></td>
<td>
<p>Positive integer indicating the number of bandwidths in the grid. The default is 10.
</p>
</td></tr>
<tr><td><code id="PVS.kernel.fit_+3A_range.grid">range.grid</code></td>
<td>

<p>Vector of length 2 containing the endpoints of the grid at which the observations of the functional covariate <code>x</code> are evaluated (i.e. the range of the discretisation). If <code>range.grid=NULL</code>, then <code>range.grid=c(1,p)</code> is considered, where <code>p</code> is the discretisation size of <code>x</code> (i.e. <code>ncol(x))</code>.
</p>
</td></tr>
<tr><td><code id="PVS.kernel.fit_+3A_kind.of.kernel">kind.of.kernel</code></td>
<td>

<p>The type of kernel function used. Currently, only Epanechnikov kernel (<code>"quad"</code>) is available.
</p>
</td></tr>
<tr><td><code id="PVS.kernel.fit_+3A_nknot">nknot</code></td>
<td>

<p>Positive integer indicating the number of interior knots for the B-spline expansion of the functional covariate. The default value is <code>(p - order.Bspline - 1)%/%2</code>.
</p>
</td></tr>
<tr><td><code id="PVS.kernel.fit_+3A_lambda.min">lambda.min</code></td>
<td>

<p>The smallest value for lambda (i.e. the lower endpoint  of the sequence in which <code>lambda.opt</code> is selected), as fraction of <code>lambda.max</code>.
The defaults is <code>lambda.min.l</code> if the sample size is larger than <code>factor.pn</code> times the number of linear covariates and <code>lambda.min.h</code> otherwise.
</p>
</td></tr>
<tr><td><code id="PVS.kernel.fit_+3A_lambda.min.h">lambda.min.h</code></td>
<td>

<p>The lower endpoint of the sequence in which <code>lambda.opt</code> is selected if the sample size is smaller than <code>factor.pn</code> times the number of linear covariates. The default is 0.05. 
</p>
</td></tr>
<tr><td><code id="PVS.kernel.fit_+3A_lambda.min.l">lambda.min.l</code></td>
<td>

<p>The lower endpoint of the sequence in which <code>lambda.opt</code> is selected if the sample size is larger than <code>factor.pn</code> times the number of linear covariates. The default is 0.0001.
</p>
</td></tr>
<tr><td><code id="PVS.kernel.fit_+3A_factor.pn">factor.pn</code></td>
<td>

<p>Positive integer used to set <code>lambda.min</code>. The default value is 1.
</p>
</td></tr>
<tr><td><code id="PVS.kernel.fit_+3A_nlambda">nlambda</code></td>
<td>

<p>Positive integer indicating the number of values in the sequence from which <code>lambda.opt</code> is selected. The default is 100.
</p>
</td></tr>
<tr><td><code id="PVS.kernel.fit_+3A_vn">vn</code></td>
<td>

<p>Positive integer or vector of positive integers indicating the number of groups of consecutive variables to be penalised together. The default value is <code>vn=ncol(z)</code>, resulting in the individual penalization of each scalar covariate.
</p>
</td></tr>
<tr><td><code id="PVS.kernel.fit_+3A_nfolds">nfolds</code></td>
<td>

<p>Number of cross-validation folds (used when <code>criterion="k-fold-CV"</code>). Default is 10.
</p>
</td></tr>
<tr><td><code id="PVS.kernel.fit_+3A_seed">seed</code></td>
<td>

<p>You may set the seed for the random number generator to ensure reproducible results (applicable when <code>criterion="k-fold-CV"</code> is used). The default seed value is 123.
</p>
</td></tr>
<tr><td><code id="PVS.kernel.fit_+3A_wn">wn</code></td>
<td>

<p>A vector of positive integers indicating the eligible number of covariates in the reduced model. For more information, refer to the section <code>Details</code>. The default is <code>c(10,15,20)</code>.
</p>
</td></tr>
<tr><td><code id="PVS.kernel.fit_+3A_criterion">criterion</code></td>
<td>

<p>The criterion used to select the tuning and regularisation parameters: <code>wn.opt</code>, <code>lambda.opt</code> and <code>h.opt</code> (also <code>vn.opt</code> if needed). Options include <code>"GCV"</code>, <code>"BIC"</code>, <code>"AIC"</code>, or <code>"k-fold-CV"</code>. The default setting is <code>"GCV"</code>.
</p>
</td></tr>
<tr><td><code id="PVS.kernel.fit_+3A_penalty">penalty</code></td>
<td>

<p>The penalty function applied in the penalised least-squares procedure. Currently, only &quot;grLasso&quot; and &quot;grSCAD&quot; are implemented. The default is &quot;grSCAD&quot;.
</p>
</td></tr>
<tr><td><code id="PVS.kernel.fit_+3A_max.iter">max.iter</code></td>
<td>

<p>Maximum number of iterations allowed across the entire path. The default value is 1000.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The multi-functional partial linear model (MFPLM) is given by the expression
</p>
<p style="text-align: center;"><code class="reqn">Y_i=\sum_{j=1}^{p_n}\beta_{0j}\zeta_i(t_j)+m\left(X_i\right)+\varepsilon_i,\ \ \ (i=1,\dots,n),</code>
</p>

<p>where: 
</p>

<ul>
<li> <p><code class="reqn">Y_i</code> is a real random response and <code class="reqn">X_i</code> denotes a random element belonging to some semi-metric space <code class="reqn">\mathcal{H}</code>. The second functional predictor <code class="reqn">\zeta_i</code> is assumed to be a curve defined on some interval <code class="reqn">[a,b]</code>,  observed at the points <code class="reqn">a\leq t_1&lt;\dots&lt;t_{p_n}\leq b</code>. 
</p>
</li>
<li>  <p><code class="reqn">\mathbf{\beta}_0=(\beta_{01},\dots,\beta_{0p_n})^{\top}</code> is a vector of unknown real coefficients and <code class="reqn">m(\cdot)</code> represents a smooth unknown real-valued link function. 
</p>
</li>
<li> <p><code class="reqn">\varepsilon_i</code> denotes the random error.
</p>
</li></ul>

<p>In  the MFPLM, it is assumed that only a few scalar variables from the set <code class="reqn">\{\zeta(t_1),\dots,\zeta(t_{p_n})\}</code> are part of the model. Therefore, the relevant variables in the linear component (the impact points of the curve <code class="reqn">\zeta</code> on the response) must be selected, and the model estimated.
</p>
<p>In this function, the MFPLM is fitted using the PVS procedure, a  two-step algorithm. For this, we divide the sample into two two independent subsamples (asymptotically of the same size <code class="reqn">n_1\sim n_2\sim n/2</code>). One subsample is used in the first stage of the method, and the other in the second stage.The subsamples are defined as follows:
</p>
<p style="text-align: center;"><code class="reqn">
\mathcal{E}^{\mathbf{1}}=\{(\zeta_i,\mathcal{X}_i,Y_i),\quad i=1,\dots,n_1\},
</code>
</p>

<p style="text-align: center;"><code class="reqn">
\mathcal{E}^{\mathbf{2}}=\{(\zeta_i,\mathcal{X}_i,Y_i),\quad i=n_1+1,\dots,n_1+n_2=n\}.
</code>
</p>
 
<p>Note that these two subsamples are specified to the program through the arguments <code>train.1</code> and <code>train.2</code>.
The superscript <code class="reqn">\mathbf{s}</code>, where <code class="reqn">\mathbf{s}=\mathbf{1},\mathbf{2}</code>, indicates the stage of the method in which the sample, function, variable, or parameter is involved.
</p>
<p>To explain the algorithm, let's assume that the number <code class="reqn">p_n</code> of linear covariates can be expressed as follows: <code class="reqn">p_n=q_nw_n</code> with <code class="reqn">q_n</code> and <code class="reqn">w_n</code> being integers.
</p>

<ol>
<li> <p><b>First step</b>. A reduced model is considered, discarding many linear covariates. The penalised least-squares procedure is applied to the reduced model using only the subsample <code class="reqn">\mathcal{E}^{\mathbf{1}}</code>. Specifically:
</p>

<ul>
<li><p> Consider a subset of the initial <code class="reqn">p_n</code> linear covariates containing only <code class="reqn">w_n</code> equally spaced discretised observations of <code class="reqn">\zeta</code> covering the interval <code class="reqn">[a,b]</code>. This subset is the following:
</p>
<p style="text-align: center;"><code class="reqn">
	\mathcal{R}_n^{\mathbf{1}}=\left\{\zeta\left(t_k^{\mathbf{1}}\right),\ \ k=1,\dots,w_n\right\},
</code>
</p>
 
<p>where  <code class="reqn">t_k^{\mathbf{1}}=t_{\left[(2k-1)q_n/2\right]}</code> and  <code class="reqn">\left[z\right]</code> denotes the smallest integer not less than the real number <code class="reqn">z</code>. The size (cardinality) of this subset is provided to the program through the argument <code>wn</code>, which contains the sequence of eligible sizes.
</p>
</li>
<li><p> Consider the following reduced model involving only the <code class="reqn">w_n</code> linear covariates from <code class="reqn">\mathcal{R}_n^{\mathbf{1}}</code>:
</p>
<p style="text-align: center;"><code class="reqn">
	Y_i=\sum_{k=1}^{w_n}\beta_{0k}^{\mathbf{1}}\zeta_i(t_k^{\mathbf{1}})+m^{\mathbf{1}}\left(X_i\right)+\varepsilon_i^{\mathbf{1}}.
</code>
</p>

<p>The penalised least-squares variable selection procedure, with kernel estimation, is applied to the reduced model using the function <code><a href="#topic+sfpl.kernel.fit">sfpl.kernel.fit</a></code>, which requires the remaining arguments (for details, see the documentation of the function <code><a href="#topic+sfpl.kernel.fit">sfpl.kernel.fit</a></code>). The estimates obtained after that are the outputs of the first step of the algorithm.
</p>
</li></ul>

</li>
<li> <p><b>Second step</b>. The variables selected in the first step, along with those in their neighborhood, are included. Then the penalised least-squares procedure, combined with kernel estimation, is carried out again, considering only the subsample <code class="reqn">\mathcal{E}^{\mathbf{2}}</code>. Specifically:
</p>

<ul>
<li><p> Consider a new set of variables:
</p>
<p style="text-align: center;"><code class="reqn">
		\mathcal{R}_n^{\mathbf{2}}=\bigcup_{\left\{k,\widehat{\beta}_{0k}^{\mathbf{1}}\not=0\right\}}\left\{\zeta(t_{(k-1)q_n+1}),\dots,\zeta(t_{kq_n})\right\}.
	</code>
</p>

<p>Denoting by <code class="reqn">r_n=\sharp(\mathcal{R}_n^{\mathbf{2}})</code>, we can rename the variables in <code class="reqn">\mathcal{R}_n^{\mathbf{2}}</code> as follows:
</p>
<p style="text-align: center;"><code class="reqn">
		\mathcal{R}_n^{\mathbf{2}}=\left\{\zeta(t_1^{\mathbf{2}}),\dots,\zeta(t_{r_n}^{\mathbf{2}})\right\},
		</code>
</p>

</li>
<li><p>  Consider the following model, which involves only the linear covariates belonging to <code class="reqn">\mathcal{R}_n^{\mathbf{2}}</code>
</p>
<p style="text-align: center;"><code class="reqn">
		Y_i=\sum_{k=1}^{r_n}\beta_{0k}^{\mathbf{2}}\zeta_i(t_k^{\mathbf{2}})+m^{\mathbf{2}}\left(X_i\right)+\varepsilon_i^{\mathbf{2}}.</code>
</p>

<p>The penalised least-squares variable selection procedure, with kernel estimation, is applied to this model using <code><a href="#topic+sfpl.kernel.fit">sfpl.kernel.fit</a></code>. 
</p>
</li></ul>

</li></ol>

<p>The outputs of the second step are the estimates of the MFPLM. For further details on this algorithm, see Aneiros and Vieu (2015).
</p>
<p><b>Remark</b>: If the condition  <code class="reqn">p_n=w_n q_n</code> is not met (then <code class="reqn">p_n/w_n</code> is not an integer), the function considers variable  <code class="reqn">q_n=q_{n,k}</code> values <code class="reqn">k=1,\dots,w_n</code>. Specifically:
</p>
<p style="text-align: center;"><code class="reqn">
	q_{n,k}= \left\{\begin{array}{ll}
	[p_n/w_n]+1 &amp;   k\in\{1,\dots,p_n-w_n[p_n/w_n]\},\\
	{[p_n/w_n]} &amp; k\in\{p_n-w_n[p_n/w_n]+1,\dots,w_n\},
	\end{array}
	\right.
</code>
</p>

<p>where <code class="reqn">[z]</code> denotes the integer part of the real number <code class="reqn">z</code>.
</p>


<h3>Value</h3>

<table>
<tr><td><code>call</code></td>
<td>
<p>The matched call.</p>
</td></tr>
<tr><td><code>fitted.values</code></td>
<td>
<p>Estimated scalar response.</p>
</td></tr>
<tr><td><code>residuals</code></td>
<td>
<p>Differences between <code>y</code> and the <code>fitted.values</code>.</p>
</td></tr>
<tr><td><code>beta.est</code></td>
<td>
<p><code class="reqn">\hat{\mathbf{\beta}}</code> (i.e. estimate of <code class="reqn">\mathbf{\beta}_0</code> when the optimal tuning parameters <code>w.opt</code>, <code>lambda.opt</code>, <code>vn.opt</code> and <code>h.opt</code> are used).</p>
</td></tr>
<tr><td><code>indexes.beta.nonnull</code></td>
<td>
<p>Indexes of the non-zero <code class="reqn">\hat{\beta_{j}}</code>.</p>
</td></tr>
<tr><td><code>h.opt</code></td>
<td>
<p>Selected bandwidth (when <code>w.opt</code> is considered).</p>
</td></tr>
<tr><td><code>w.opt</code></td>
<td>
<p>Selected size for  <code class="reqn">\mathcal{R}_n^{\mathbf{1}}</code>.</p>
</td></tr>
<tr><td><code>lambda.opt</code></td>
<td>
<p>Selected value of the penalisation parameter <code class="reqn">\lambda</code> (when <code>w.opt</code> is considered).</p>
</td></tr>
<tr><td><code>IC</code></td>
<td>
<p>Value of the criterion function considered to select <code>w.opt</code>, <code>lambda.opt</code>, <code>vn.opt</code> and <code>h.opt</code>.</p>
</td></tr>
<tr><td><code>vn.opt</code></td>
<td>
<p>Selected value of <code>vn</code> in the second step (when <code>w.opt</code> is considered).</p>
</td></tr>
<tr><td><code>beta2</code></td>
<td>
<p>Estimate of <code class="reqn">\mathbf{\beta}_0^{\mathbf{2}}</code> for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>indexes.beta.nonnull2</code></td>
<td>
<p>Indexes of the non-zero linear coefficients after the step 2 of the method for each value of the sequence <code>wn</code>.</p>
</td></tr> 
<tr><td><code>h2</code></td>
<td>
<p>Selected bandwidth in the second step of the algorithm for each value of the sequence <code>wn</code>.</p>
</td></tr> 
<tr><td><code>IC2</code></td>
<td>
<p>Optimal value of the criterion function in the second step for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>lambda2</code></td>
<td>
<p>Selected value of penalisation parameter in the second step for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>index02</code></td>
<td>
<p>Indexes of the covariates (in the entire set of <code class="reqn">p_n</code>) used to construct <code class="reqn">\mathcal{R}_n^{\mathbf{2}}</code> for each value of the sequence <code>wn</code>.</p>
</td></tr> 
<tr><td><code>beta1</code></td>
<td>
<p>Estimate of <code class="reqn">\mathbf{\beta}_0^{\mathbf{1}}</code> for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>h1</code></td>
<td>
<p>Selected bandwidth in the first step of the algorithm for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>IC1</code></td>
<td>
<p>Optimal value of the criterion function in the first step for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>lambda1</code></td>
<td>
<p>Selected value of penalisation parameter in the first step for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>index01</code></td>
<td>
<p>Indexes of the covariates (in the entire set of <code class="reqn">p_n</code>) used to construct <code class="reqn">\mathcal{R}_n^{\mathbf{1}}</code> for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>index1</code></td>
<td>
<p>Indexes of the non-zero linear coefficients after the step 1 of the method for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>...</code></td>
<td>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>German Aneiros Perez <a href="mailto:german.aneiros@udc.es">german.aneiros@udc.es</a> 
</p>
<p>Silvia Novo Diaz  <a href="mailto:snovo@est-econ.uc3m.es">snovo@est-econ.uc3m.es</a>
</p>


<h3>References</h3>

<p>Aneiros, G., and Vieu, P. (2015) Partial linear modelling with multi-functional covariates. <em>Computational Statistics</em>, <b>30</b>, 647&ndash;671, <a href="https://doi.org/10.1007/s00180-015-0568-8">doi:10.1007/s00180-015-0568-8</a>.
</p>


<h3>See Also</h3>

<p>See also <code><a href="#topic+sfpl.kernel.fit">sfpl.kernel.fit</a>, <a href="#topic+predict.PVS.kernel">predict.PVS.kernel</a></code> and <code><a href="#topic+plot.PVS.kernel">plot.PVS.kernel</a></code>.
</p>
<p>Alternative method <code><a href="#topic+PVS.kNN.fit">PVS.kNN.fit</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(Sugar)

y&lt;-Sugar$ash
x&lt;-Sugar$wave.290
z&lt;-Sugar$wave.240

#Outliers
index.y.25 &lt;- y &gt; 25
index.atip &lt;- index.y.25
(1:268)[index.atip]

#Dataset to model
x.sug &lt;- x[!index.atip,]
z.sug&lt;- z[!index.atip,]
y.sug &lt;- y[!index.atip]

train&lt;-1:216

ptm=proc.time()
fit&lt;- PVS.kernel.fit(x=x.sug[train,],z=z.sug[train,], y=y.sug[train],
        train.1=1:108,train.2=109:216,lambda.min.h=0.03, 
        lambda.min.l=0.03,  max.q.h=0.35, nknot=20,
        criterion="BIC", max.iter=5000)
proc.time()-ptm

fit 
names(fit)

</code></pre>

<hr>
<h2 id='PVS.kNN.fit'>
Impact point selection with PVS and kNN estimation
</h2><span id='topic+PVS.kNN.fit'></span>

<h3>Description</h3>

<p>This function computes the partitioning variable selection (PVS) algorithm for multi-functional partial linear models (MFPLM).
</p>
<p>PVS is a two-stage procedure that selects the impact points of the discretised curve and estimates the model. The algorithm employs a penalised least-squares regularisation procedure, integrated with kNN estimation using Nadaraya-Watson weights.
Additionally, it utilises an objective criterion (<code>criterion</code>) to select the number of covariates in the reduced model (<code>w.opt</code>), the number of neighbours (<code>k.opt</code>) and the penalisation parameter (<code>lambda.opt</code>). 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>PVS.kNN.fit(x, z, y, train.1 = NULL, train.2 = NULL, semimetric = "deriv", 
q = NULL, knearest = NULL, min.knn = 2, max.knn = NULL, step = NULL, 
range.grid = NULL, kind.of.kernel = "quad", nknot = NULL, lambda.min = NULL,
lambda.min.h = NULL, lambda.min.l = NULL, factor.pn = 1, nlambda = 100, 
vn = ncol(z), nfolds = 10, seed = 123, wn = c(10, 15, 20), criterion = "GCV",
penalty = "grSCAD", max.iter = 1000)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="PVS.kNN.fit_+3A_x">x</code></td>
<td>

<p>Matrix containing the observations of the functional covariate (functional nonparametric component), collected by row.
</p>
</td></tr>
<tr><td><code id="PVS.kNN.fit_+3A_z">z</code></td>
<td>

<p>Matrix containing the observations of the functional covariate that is discretised (linear component), collected by row.
</p>
</td></tr>
<tr><td><code id="PVS.kNN.fit_+3A_y">y</code></td>
<td>

<p>Vector containing the scalar response.
</p>
</td></tr>
<tr><td><code id="PVS.kNN.fit_+3A_train.1">train.1</code></td>
<td>

<p>Positions of the data that are used as the training sample in the 1st step. The default setting is  <code>train.1&lt;-1:ceiling(n/2)</code>.
</p>
</td></tr>
<tr><td><code id="PVS.kNN.fit_+3A_train.2">train.2</code></td>
<td>

<p>Positions of the data that are used as the training sample in the 2nd step. The default setting is <code>train.2&lt;-(ceiling(n/2)+1):n</code>.
</p>
</td></tr>
<tr><td><code id="PVS.kNN.fit_+3A_semimetric">semimetric</code></td>
<td>

<p>Semi-metric function.  Currently, only <code>"deriv"</code> and <code>"pca"</code> are implemented.  By default <code>semimetric="deriv"</code>.
</p>
</td></tr>
<tr><td><code id="PVS.kNN.fit_+3A_q">q</code></td>
<td>

<p>Order of the derivative (if <code>semimetric="deriv"</code>) or number of principal components (if <code>semimetric="pca"</code>). The default values are 0 and 2, respectively.
</p>
</td></tr>
<tr><td><code id="PVS.kNN.fit_+3A_knearest">knearest</code></td>
<td>

<p>Vector of positive integers containing the sequence in which the  number of nearest neighbours <code>k.opt</code> is selected. If <code>knearest=NULL</code>, then <code>knearest &lt;- seq(from =min.knn, to = max.knn, by = step)</code>.
</p>
</td></tr>
<tr><td><code id="PVS.kNN.fit_+3A_min.knn">min.knn</code></td>
<td>

<p>A positive integer that represents the minimum value in the sequence for selecting the number of nearest neighbours <code>k.opt</code>. This value should be less than the sample size. The default is 2.
</p>
</td></tr>
<tr><td><code id="PVS.kNN.fit_+3A_max.knn">max.knn</code></td>
<td>

<p>A positive integer that represents the maximum value in the sequence for selecting number of nearest neighbours <code>k.opt</code>. This value should be less than the sample size. The default is <code>max.knn &lt;- n%/%5</code>.
</p>
</td></tr>
<tr><td><code id="PVS.kNN.fit_+3A_step">step</code></td>
<td>

<p>A positive integer used to construct the sequence of k-nearest neighbours as follows: <code>min.knn, min.knn + step, min.knn + 2*step, min.knn + 3*step,...</code>. The default value for <code>step</code> is <code>step&lt;-ceiling(n/100)</code>.
</p>
</td></tr>
<tr><td><code id="PVS.kNN.fit_+3A_range.grid">range.grid</code></td>
<td>

<p>Vector of length 2 containing the endpoints of the grid at which the observations of the functional covariate <code>x</code> are evaluated (i.e. the range of the discretisation). If <code>range.grid=NULL</code>, then <code>range.grid=c(1,p)</code> is considered, where <code>p</code> is the discretisation size of <code>x</code> (i.e. <code>ncol(x))</code>.
</p>
</td></tr>
<tr><td><code id="PVS.kNN.fit_+3A_kind.of.kernel">kind.of.kernel</code></td>
<td>

<p>The type of kernel function used. Currently, only Epanechnikov kernel (<code>"quad"</code>) is available.
</p>
</td></tr>
<tr><td><code id="PVS.kNN.fit_+3A_nknot">nknot</code></td>
<td>

<p>Positive integer indicating the number of interior knots for the B-spline expansion of the functional covariate. The default value is <code>(p - order.Bspline - 1)%/%2</code>.
</p>
</td></tr>
<tr><td><code id="PVS.kNN.fit_+3A_lambda.min">lambda.min</code></td>
<td>

<p>The smallest value for lambda (i.e. the lower endpoint  of the sequence in which <code>lambda.opt</code> is selected), as fraction of <code>lambda.max</code>.
The defaults is <code>lambda.min.l</code> if the sample size is larger than <code>factor.pn</code> times the number of linear covariates and <code>lambda.min.h</code> otherwise.
</p>
</td></tr>
<tr><td><code id="PVS.kNN.fit_+3A_lambda.min.h">lambda.min.h</code></td>
<td>

<p>The lower endpoint of the sequence in which <code>lambda.opt</code> is selected if the sample size is smaller than <code>factor.pn</code> times the number of linear covariates. The default is 0.05. 
</p>
</td></tr>
<tr><td><code id="PVS.kNN.fit_+3A_lambda.min.l">lambda.min.l</code></td>
<td>

<p>The lower endpoint of the sequence in which <code>lambda.opt</code> is selected if the sample size is larger than <code>factor.pn</code> times the number of linear covariates. The default is 0.0001.
</p>
</td></tr>
<tr><td><code id="PVS.kNN.fit_+3A_factor.pn">factor.pn</code></td>
<td>

<p>Positive integer used to set <code>lambda.min</code>. The default value is 1.
</p>
</td></tr>
<tr><td><code id="PVS.kNN.fit_+3A_nlambda">nlambda</code></td>
<td>

<p>Positive integer indicating the number of values in the sequence from which <code>lambda.opt</code> is selected. The default is 100.
</p>
</td></tr>
<tr><td><code id="PVS.kNN.fit_+3A_vn">vn</code></td>
<td>

<p>Positive integer or vector of positive integers indicating the number of groups of consecutive variables to be penalised together. The default value is <code>vn=ncol(z)</code>, resulting in the individual penalization of each scalar covariate.
</p>
</td></tr>
<tr><td><code id="PVS.kNN.fit_+3A_nfolds">nfolds</code></td>
<td>

<p>Number of cross-validation folds (used when <code>criterion="k-fold-CV"</code>). Default is 10.
</p>
</td></tr>
<tr><td><code id="PVS.kNN.fit_+3A_seed">seed</code></td>
<td>

<p>You may set the seed for the random number generator to ensure reproducible results (applicable when <code>criterion="k-fold-CV"</code> is used). The default seed value is 123.
</p>
</td></tr>
<tr><td><code id="PVS.kNN.fit_+3A_wn">wn</code></td>
<td>

<p>A vector of positive integers indicating the eligible number of covariates in the reduced model. For more information, refer to the section <code>Details</code>. The default is <code>c(10,15,20)</code>.
</p>
</td></tr>
<tr><td><code id="PVS.kNN.fit_+3A_criterion">criterion</code></td>
<td>

<p>The criterion used to select the tuning and regularisation parameters: <code>wn.opt</code>, <code>lambda.opt</code> and <code>k.opt</code> (also <code>vn.opt</code> if needed). Options include <code>"GCV"</code>, <code>"BIC"</code>, <code>"AIC"</code>, or <code>"k-fold-CV"</code>. The default setting is <code>"GCV"</code>.
</p>
</td></tr>
<tr><td><code id="PVS.kNN.fit_+3A_penalty">penalty</code></td>
<td>

<p>The penalty function applied in the penalised least-squares procedure. Currently, only &quot;grLasso&quot; and &quot;grSCAD&quot; are implemented. The default is &quot;grSCAD&quot;.
</p>
</td></tr>
<tr><td><code id="PVS.kNN.fit_+3A_max.iter">max.iter</code></td>
<td>

<p>Maximum number of iterations allowed across the entire path. The default value is 1000.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The multi-functional partial linear model (MFPLM) is given by the expression
</p>
<p style="text-align: center;"><code class="reqn">Y_i=\sum_{j=1}^{p_n}\beta_{0j}\zeta_i(t_j)+m\left(X_i\right)+\varepsilon_i,\ \ \ (i=1,\dots,n),</code>
</p>

<p>where: 
</p>

<ul>
<li> <p><code class="reqn">Y_i</code> is a real random response and <code class="reqn">X_i</code> denotes a random element belonging to some semi-metric space <code class="reqn">\mathcal{H}</code>. The second functional predictor <code class="reqn">\zeta_i</code> is assumed to be a curve defined on some interval <code class="reqn">[a,b]</code>, observed at the points <code class="reqn">a\leq t_1&lt;\dots&lt;t_{p_n}\leq b</code>. 
</p>
</li>
<li>  <p><code class="reqn">\mathbf{\beta}_0=(\beta_{01},\dots,\beta_{0p_n})^{\top}</code> is a vector of unknown real coefficients and <code class="reqn">m(\cdot)</code> represents a smooth unknown real-valued link function. 
</p>
</li>
<li> <p><code class="reqn">\varepsilon_i</code> denotes the random error.
</p>
</li></ul>

<p>In  the MFPLM, it is assumed that only a few scalar variables from the set <code class="reqn">\{\zeta(t_1),\dots,\zeta(t_{p_n})\}</code> are part of the model. Therefore, the relevant variables in the linear component (the impact points of the curve <code class="reqn">\zeta</code> on the response) must be selected, and the model estimated.
</p>
<p>In this function, the MFPLM is fitted using the PVS procedure, a  two-step algorithm. For this, we divide the sample into two two independent subsamples (asymptotically of the same size <code class="reqn">n_1\sim n_2\sim n/2</code>). One subsample is used in the first stage of the method, and the other in the second stage.The subsamples are defined as follows:
</p>
<p style="text-align: center;"><code class="reqn">
\mathcal{E}^{\mathbf{1}}=\{(\zeta_i,\mathcal{X}_i,Y_i),\quad i=1,\dots,n_1\},
</code>
</p>

<p style="text-align: center;"><code class="reqn">
\mathcal{E}^{\mathbf{2}}=\{(\zeta_i,\mathcal{X}_i,Y_i),\quad i=n_1+1,\dots,n_1+n_2=n\}.
</code>
</p>
 
<p>Note that these two subsamples are specified to the program through the arguments <code>train.1</code> and <code>train.2</code>.
The superscript <code class="reqn">\mathbf{s}</code>, where <code class="reqn">\mathbf{s}=\mathbf{1},\mathbf{2}</code>, indicates the stage of the method in which the sample, function, variable, or parameter is involved.
</p>
<p>To explain the algorithm, let's assume that the number <code class="reqn">p_n</code> of linear covariates can be expressed as follows: <code class="reqn">p_n=q_nw_n</code> with <code class="reqn">q_n</code> and <code class="reqn">w_n</code> being integers.
</p>

<ol>
<li> <p><b>First step</b>. A reduced model is considered, discarding many linear covariates. The penalised least-squares procedure is applied to the reduced model using only the subsample <code class="reqn">\mathcal{E}^{\mathbf{1}}</code>. Specifically:
</p>

<ul>
<li><p> Consider a subset of the initial <code class="reqn">p_n</code> linear covariates containing only <code class="reqn">w_n</code> equally spaced discretised observations of <code class="reqn">\zeta</code> covering the interval <code class="reqn">[a,b]</code>. This subset is the following:
</p>
<p style="text-align: center;"><code class="reqn">
	\mathcal{R}_n^{\mathbf{1}}=\left\{\zeta\left(t_k^{\mathbf{1}}\right),\ \ k=1,\dots,w_n\right\},
</code>
</p>
 
<p>where  <code class="reqn">t_k^{\mathbf{1}}=t_{\left[(2k-1)q_n/2\right]}</code> and  <code class="reqn">\left[z\right]</code> denotes the smallest integer not less than the real number <code class="reqn">z</code>. The size (cardinality) of this subset is provided to the program through the argument <code>wn</code>, which contains the sequence of eligible sizes.
</p>
</li>
<li><p> Consider the following reduced model involving only the <code class="reqn">w_n</code> linear covariates from <code class="reqn">\mathcal{R}_n^{\mathbf{1}}</code>:
</p>
<p style="text-align: center;"><code class="reqn">
	Y_i=\sum_{k=1}^{w_n}\beta_{0k}^{\mathbf{1}}\zeta_i(t_k^{\mathbf{1}})+m^{\mathbf{1}}\left(X_i\right)+\varepsilon_i^{\mathbf{1}}.
</code>
</p>

<p>The penalised least-squares variable selection procedure, with kNN estimation, is applied to the reduced model using the function <code><a href="#topic+sfpl.kNN.fit">sfpl.kNN.fit</a></code>, which requires the remaining arguments (for details, see the documentation of the function <code><a href="#topic+sfpl.kNN.fit">sfpl.kNN.fit</a></code>). The estimates obtained after that are the outputs of the first step of the algorithm.
</p>
</li></ul>

</li>
<li> <p><b>Second step</b>. The variables selected in the first step, along with those in their neighborhood, are included. Then the penalised least-squares procedure, combined with kNN estimation, is carried out again, considering only the subsample <code class="reqn">\mathcal{E}^{\mathbf{2}}</code>. Specifically:
</p>

<ul>
<li><p> Consider a new set of variables:
</p>
<p style="text-align: center;"><code class="reqn">
		\mathcal{R}_n^{\mathbf{2}}=\bigcup_{\left\{k,\widehat{\beta}_{0k}^{\mathbf{1}}\not=0\right\}}\left\{\zeta(t_{(k-1)q_n+1}),\dots,\zeta(t_{kq_n})\right\}.
	</code>
</p>

<p>Denoting by <code class="reqn">r_n=\sharp(\mathcal{R}_n^{\mathbf{2}})</code>, we can rename the variables in <code class="reqn">\mathcal{R}_n^{\mathbf{2}}</code> as follows:
</p>
<p style="text-align: center;"><code class="reqn">
		\mathcal{R}_n^{\mathbf{2}}=\left\{\zeta(t_1^{\mathbf{2}}),\dots,\zeta(t_{r_n}^{\mathbf{2}})\right\},
		</code>
</p>

</li>
<li><p>  Consider the following model, which involves only the linear covariates belonging to <code class="reqn">\mathcal{R}_n^{\mathbf{2}}</code>
</p>
<p style="text-align: center;"><code class="reqn">
		Y_i=\sum_{k=1}^{r_n}\beta_{0k}^{\mathbf{2}}\zeta_i(t_k^{\mathbf{2}})+m^{\mathbf{2}}\left(X_i\right)+\varepsilon_i^{\mathbf{2}}.</code>
</p>

<p>The penalised least-squares variable selection procedure, with kNN estimation, is applied to this model using <code><a href="#topic+sfpl.kNN.fit">sfpl.kNN.fit</a></code>. 
</p>
</li></ul>

</li></ol>

<p>The outputs of the second step are the estimates of the MFPLM. For further details on this algorithm, see Aneiros and Vieu (2015).
</p>
<p><b>Remark</b>: If the condition  <code class="reqn">p_n=w_n q_n</code> is not met (then <code class="reqn">p_n/w_n</code> is not an integer), the function considers variable  <code class="reqn">q_n=q_{n,k}</code> values <code class="reqn">k=1,\dots,w_n</code>. Specifically:
</p>
<p style="text-align: center;"><code class="reqn">
	q_{n,k}= \left\{\begin{array}{ll}
	[p_n/w_n]+1 &amp;   k\in\{1,\dots,p_n-w_n[p_n/w_n]\},\\
	{[p_n/w_n]} &amp; k\in\{p_n-w_n[p_n/w_n]+1,\dots,w_n\},
	\end{array}
	\right.
</code>
</p>

<p>where <code class="reqn">[z]</code> denotes the integer part of the real number <code class="reqn">z</code>.
</p>


<h3>Value</h3>

<table>
<tr><td><code>call</code></td>
<td>
<p>The matched call.</p>
</td></tr>
<tr><td><code>fitted.values</code></td>
<td>
<p>Estimated scalar response.</p>
</td></tr>
<tr><td><code>residuals</code></td>
<td>
<p>Differences between <code>y</code> and the <code>fitted.values</code>.</p>
</td></tr>
<tr><td><code>beta.est</code></td>
<td>
<p><code class="reqn">\hat{\mathbf{\beta}}</code> (i.e. estimate of <code class="reqn">\mathbf{\beta}_0</code> when the optimal tuning parameters <code>w.opt</code>, <code>lambda.opt</code>, <code>vn.opt</code> and <code>k.opt</code> are used).</p>
</td></tr>
<tr><td><code>indexes.beta.nonnull</code></td>
<td>
<p>Indexes of the non-zero <code class="reqn">\hat{\beta_{j}}</code>.</p>
</td></tr>
<tr><td><code>k.opt</code></td>
<td>
<p>Selected number of nearest neighbours (when <code>w.opt</code> is considered).</p>
</td></tr>
<tr><td><code>w.opt</code></td>
<td>
<p>Selected initial number of covariates in the reduced model.</p>
</td></tr>
<tr><td><code>lambda.opt</code></td>
<td>
<p>Selected value of the penalisation parameter <code class="reqn">\lambda</code> (when <code>w.opt</code> is considered).</p>
</td></tr>
<tr><td><code>IC</code></td>
<td>
<p>Value of the criterion function considered to select <code>w.opt</code>, <code>lambda.opt</code>, <code>vn.opt</code> and <code>k.opt</code>.</p>
</td></tr>
<tr><td><code>vn.opt</code></td>
<td>
<p>Selected value of <code>vn</code> in the second step (when <code>w.opt</code> is considered).</p>
</td></tr>
<tr><td><code>beta2</code></td>
<td>
<p>Estimate of <code class="reqn">\mathbf{\beta}_0^{\mathbf{2}}</code> for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>indexes.beta.nonnull2</code></td>
<td>
<p>Indexes of the non-zero linear coefficients after the step 2 of the method for each value of the sequence <code>wn</code>.</p>
</td></tr> 
<tr><td><code>knn2</code></td>
<td>
<p>Selected number of neighbours in the second step of the algorithm for each value of the sequence <code>wn</code>.</p>
</td></tr> 
<tr><td><code>IC2</code></td>
<td>
<p>Optimal value of the criterion function in the second step for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>lambda2</code></td>
<td>
<p>Selected value of penalisation parameter in the second step for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>index02</code></td>
<td>
<p>Indexes of the covariates (in the entire set of <code class="reqn">p_n</code>) used to build <code class="reqn">\mathcal{R}_n^{\mathbf{2}}</code> for each value of the sequence <code>wn</code>.</p>
</td></tr> 
<tr><td><code>beta1</code></td>
<td>
<p>Estimate of <code class="reqn">\mathbf{\beta}_0^{\mathbf{1}}</code> for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>knn1</code></td>
<td>
<p>Selected number of neighbours in the first step of the algorithm for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>IC1</code></td>
<td>
<p>Optimal value of the criterion function in the first step for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>lambda1</code></td>
<td>
<p>Selected value of penalisation parameter in the first step for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>index01</code></td>
<td>
<p>Indexes of the covariates (in the entire set of <code class="reqn">p_n</code>) used to build <code class="reqn">\mathcal{R}_n^{\mathbf{1}}</code> for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>index1</code></td>
<td>
<p>Indexes of the non-zero linear coefficients after the step 1 of the method for each value of the sequence <code>wn</code>.</p>
</td></tr>
<tr><td><code>...</code></td>
<td>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>German Aneiros Perez <a href="mailto:german.aneiros@udc.es">german.aneiros@udc.es</a> 
</p>
<p>Silvia Novo Diaz  <a href="mailto:snovo@est-econ.uc3m.es">snovo@est-econ.uc3m.es</a>
</p>


<h3>References</h3>

<p>Aneiros, G., and Vieu, P. (2015) Partial linear modelling with multi-functional covariates. <em>Computational Statistics</em>, <b>30</b>, 647&ndash;671, <a href="https://doi.org/10.1007/s00180-015-0568-8">doi:10.1007/s00180-015-0568-8</a>.
</p>


<h3>See Also</h3>

<p>See also <code><a href="#topic+sfpl.kNN.fit">sfpl.kNN.fit</a>, <a href="#topic+predict.PVS.kNN">predict.PVS.kNN</a></code> and <code><a href="#topic+plot.PVS.kNN">plot.PVS.kNN</a></code>.
</p>
<p>Alternative method <code><a href="#topic+PVS.kernel.fit">PVS.kernel.fit</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(Sugar)

y&lt;-Sugar$ash
x&lt;-Sugar$wave.290
z&lt;-Sugar$wave.240

#Outliers
index.y.25 &lt;- y &gt; 25
index.atip &lt;- index.y.25
(1:268)[index.atip]

#Dataset to model
x.sug &lt;- x[!index.atip,]
z.sug&lt;- z[!index.atip,]
y.sug &lt;- y[!index.atip]

train&lt;-1:216

ptm=proc.time()
fit&lt;- PVS.kNN.fit(x=x.sug[train,],z=z.sug[train,], y=y.sug[train],
        train.1=1:108,train.2=109:216,lambda.min.h=0.07, 
        lambda.min.l=0.07, nknot=20,criterion="BIC",  
        max.iter=5000)
proc.time()-ptm

fit 
names(fit)

    
</code></pre>

<hr>
<h2 id='semimetric.projec'>
Projection semi-metric computation
</h2><span id='topic+semimetric.projec'></span>

<h3>Description</h3>

<p>Computes the projection semi-metric between each curve in <code>data1</code> and each curve in <code>data2</code>, given a functional index <code class="reqn">\theta</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>semimetric.projec(data1, data2, theta, order.Bspline = 3, nknot.theta = 3,
  range.grid = NULL, nknot = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="semimetric.projec_+3A_data1">data1</code></td>
<td>

<p>Matrix containing functional data collected by row.
</p>
</td></tr>
<tr><td><code id="semimetric.projec_+3A_data2">data2</code></td>
<td>

<p>Matrix containing functional data collected by row.
</p>
</td></tr>
<tr><td><code id="semimetric.projec_+3A_theta">theta</code></td>
<td>

<p>Vector containing the coefficients of <code class="reqn">\theta</code> in a B-spline basis, so that <code>length(theta)=order.Bspline+nknot.theta</code>.
</p>
</td></tr>
<tr><td><code id="semimetric.projec_+3A_order.bspline">order.Bspline</code></td>
<td>

<p>Order of the B-spline basis functions for the B-spline representation of <code class="reqn">\theta</code>. This is the number of coefficients in each piecewise polynomial segment. The default is 3.
</p>
</td></tr>
<tr><td><code id="semimetric.projec_+3A_nknot.theta">nknot.theta</code></td>
<td>

<p>Number of regularly spaced interior knots of the B-spline basis. The default is 3.
</p>
</td></tr>
<tr><td><code id="semimetric.projec_+3A_range.grid">range.grid</code></td>
<td>

<p>Vector of length 2 containing  the range of the discretisation of the functional data. If <code>range.grid=NULL</code>, then <code>range.grid=c(1,p)</code> is considered, where <code>p</code> is the discretization size of <code>data</code> (i.e. <code>ncol(data)</code>).
</p>
</td></tr>
<tr><td><code id="semimetric.projec_+3A_nknot">nknot</code></td>
<td>

<p>Number of  regularly spaced interior knots for the B-spline representation of the functional data. The default value is <code>(p - order.Bspline - 1)%/%2</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For <code class="reqn">x_1,x_2 \in \mathcal{H}, </code>, where <code class="reqn">\mathcal{H}</code> is a separable Hilbert space, the projection semi-metric in the direction <code class="reqn">\theta\in \mathcal{H}</code> is defined as   </p>
<p style="text-align: center;"><code class="reqn">d_{\theta}(x_1,x_2)=|\langle\theta,x_1-x_2\rangle|.</code>
</p>

<p>The function <code>semimetric.projec</code> computes this projection semi-metric using the B-spline representation of the curves and <code class="reqn">\theta</code>. The dimension of the B-spline basis for <code class="reqn">\theta</code> is determined by <code>order.Bspline</code>+<code>nknot.theta</code>.
</p>


<h3>Value</h3>

<p>A matrix containing the projection semi-metrics for each pair of curves.
</p>


<h3>Author(s)</h3>

<p>German Aneiros Perez <a href="mailto:german.aneiros@udc.es">german.aneiros@udc.es</a> 
</p>
<p>Silvia Novo Diaz  <a href="mailto:snovo@est-econ.uc3m.es">snovo@est-econ.uc3m.es</a>
</p>


<h3>References</h3>

<p>Novo S., Aneiros, G., and Vieu, P., (2019) Automatic and location-adaptive estimation in functional single&ndash;index regression. <em>Journal of Nonparametric Statistics</em>, <b>31(2)</b>, 364&ndash;392, <a href="https://doi.org/10.1080/10485252.2019.1567726">doi:10.1080/10485252.2019.1567726</a>.
</p>


<h3>See Also</h3>

<p>See also <code><a href="#topic+projec">projec</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data("Tecator")
names(Tecator)
y&lt;-Tecator$fat
X&lt;-Tecator$absor.spectra

#length(theta)=6=order.Bspline+nknot.theta 
semimetric.projec(data1=X[1:5,], data2=X[5:10,],theta=c(1,0,0,1,1,-1),
  nknot.theta=3,nknot=20,range.grid=c(850,1050))

</code></pre>

<hr>
<h2 id='sfpl.kernel.fit'>
SFPLM regularised fit using kernel estimation
</h2><span id='topic+sfpl.kernel.fit'></span>

<h3>Description</h3>

<p>This function fits a sparse semi-functional partial linear model (SFPLM). It employs a penalised least-squares regularisation procedure, integrated with nonparametric kernel estimation using Nadaraya-Watson weights.
</p>
<p>The procedure utilises an objective criterion (<code>criterion</code>) to select both the bandwidth (<code>h.opt</code>) and the regularisation parameter (<code>lambda.opt</code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sfpl.kernel.fit(x, z, y, semimetric = "deriv", q = NULL, min.q.h = 0.05, 
max.q.h = 0.5, h.seq = NULL, num.h = 10, range.grid = NULL, 
kind.of.kernel = "quad", nknot = NULL, lambda.min = NULL, lambda.min.h = NULL, 
lambda.min.l = NULL, factor.pn = 1, nlambda = 100, lambda.seq = NULL, 
vn = ncol(z), nfolds = 10, seed = 123, criterion = "GCV", penalty = "grSCAD", 
max.iter = 1000)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sfpl.kernel.fit_+3A_x">x</code></td>
<td>

<p>Matrix containing the observations of the functional covariate (functional nonparametric component), collected by row.
</p>
</td></tr>
<tr><td><code id="sfpl.kernel.fit_+3A_z">z</code></td>
<td>

<p>Matrix containing the observations of the scalar covariates (linear component), collected by row.
</p>
</td></tr>
<tr><td><code id="sfpl.kernel.fit_+3A_y">y</code></td>
<td>

<p>Vector containing the scalar response.
</p>
</td></tr>
<tr><td><code id="sfpl.kernel.fit_+3A_semimetric">semimetric</code></td>
<td>

<p>Semi-metric function.  Only <code>"deriv"</code> and <code>"pca"</code> are implemented.  By default <code>semimetric="deriv"</code>.
</p>
</td></tr>
<tr><td><code id="sfpl.kernel.fit_+3A_q">q</code></td>
<td>

<p>Order of the derivative (if <code>semimetric="deriv"</code>) or number of principal components (if <code>semimetric="pca"</code>). The default values are 0 and 2, respectively.
</p>
</td></tr>
<tr><td><code id="sfpl.kernel.fit_+3A_min.q.h">min.q.h</code></td>
<td>
<p>Minimum quantile order of the distances between curves, which are computed using the projection semi-metric. This value determines the lower endpoint of the range from which the bandwidth is selected. The default is 0.05.
</p>
</td></tr>
<tr><td><code id="sfpl.kernel.fit_+3A_max.q.h">max.q.h</code></td>
<td>

<p>Maximum quantile order of the distances between curves, which are computed using the projection semi-metric. This value determines the upper endpoint of the range from which the bandwidth is selected. The default is 0.5.
</p>
</td></tr>
<tr><td><code id="sfpl.kernel.fit_+3A_h.seq">h.seq</code></td>
<td>
<p>Vector containing the sequence of bandwidths. The default is a sequence of <code>num.h</code> equispaced bandwidths in the range constructed using <code>min.q.h</code> and <code>max.q.h</code>.
</p>
</td></tr>
<tr><td><code id="sfpl.kernel.fit_+3A_num.h">num.h</code></td>
<td>
<p>Positive integer indicating the number of bandwidths in the grid. The default is 10.
</p>
</td></tr>
<tr><td><code id="sfpl.kernel.fit_+3A_range.grid">range.grid</code></td>
<td>

<p>Vector of length 2 containing the endpoints of the grid at which the observations of the functional covariate <code>x</code> are evaluated (i.e. the range of the discretisation). If <code>range.grid=NULL</code>, then <code>range.grid=c(1,p)</code> is considered, where <code>p</code> is the discretisation size of <code>x</code> (i.e. <code>ncol(x))</code>.
</p>
</td></tr>
<tr><td><code id="sfpl.kernel.fit_+3A_kind.of.kernel">kind.of.kernel</code></td>
<td>

<p>The type of kernel function used. Currently, only Epanechnikov kernel (<code>"quad"</code>) is available.
</p>
</td></tr>
<tr><td><code id="sfpl.kernel.fit_+3A_nknot">nknot</code></td>
<td>

<p>Positive integer indicating the number of interior knots for the B-spline expansion of the functional covariate. The default value is <code>(p - order.Bspline - 1)%/%2</code>.
</p>
</td></tr>
<tr><td><code id="sfpl.kernel.fit_+3A_lambda.min">lambda.min</code></td>
<td>

<p>The smallest value for lambda (i.e. the lower endpoint  of the sequence in which <code>lambda.opt</code> is selected), as fraction of <code>lambda.max</code>.
The defaults is <code>lambda.min.l</code> if the sample size is larger than <code>factor.pn</code> times the number of linear covariates and <code>lambda.min.h</code> otherwise.
</p>
</td></tr>
<tr><td><code id="sfpl.kernel.fit_+3A_lambda.min.h">lambda.min.h</code></td>
<td>

<p>The lower endpoint of the sequence in which <code>lambda.opt</code> is selected if the sample size is smaller than <code>factor.pn</code> times the number of linear covariates. The default is 0.05. 
</p>
</td></tr>
<tr><td><code id="sfpl.kernel.fit_+3A_lambda.min.l">lambda.min.l</code></td>
<td>

<p>The lower endpoint of the sequence in which <code>lambda.opt</code> is selected if the sample size is larger than <code>factor.pn</code> times the number of linear covariates. The default is 0.0001.
</p>
</td></tr>
<tr><td><code id="sfpl.kernel.fit_+3A_factor.pn">factor.pn</code></td>
<td>

<p>Positive integer used to set <code>lambda.min</code>. The default value is 1.
</p>
</td></tr>
<tr><td><code id="sfpl.kernel.fit_+3A_nlambda">nlambda</code></td>
<td>

<p>Positive integer indicating the number of values in the sequence from which <code>lambda.opt</code> is selected. The default is 100.
</p>
</td></tr>
<tr><td><code id="sfpl.kernel.fit_+3A_lambda.seq">lambda.seq</code></td>
<td>

<p>Sequence of values in which <code>lambda.opt</code> is selected. If <code>lambda.seq=NULL</code>, then the programme builds the sequence automatically using <code>lambda.min</code> and <code>nlambda</code>.
</p>
</td></tr>
<tr><td><code id="sfpl.kernel.fit_+3A_vn">vn</code></td>
<td>

<p>Positive integer or vector of positive integers indicating the number of groups of consecutive variables to be penalised together. The default value is <code>vn=ncol(z)</code>, resulting in the individual penalization of each scalar covariate.
</p>
</td></tr>
<tr><td><code id="sfpl.kernel.fit_+3A_nfolds">nfolds</code></td>
<td>

<p>Number of cross-validation folds (used when <code>criterion="k-fold-CV"</code>). Default is 10.
</p>
</td></tr>
<tr><td><code id="sfpl.kernel.fit_+3A_seed">seed</code></td>
<td>

<p>You may set the seed for the random number generator to ensure reproducible results (applicable when <code>criterion="k-fold-CV"</code> is used). The default seed value is 123.
</p>
</td></tr>
<tr><td><code id="sfpl.kernel.fit_+3A_criterion">criterion</code></td>
<td>

<p>The criterion used to select the tuning and regularisation parameter: <code>h.opt</code>  and <code>lambda.opt</code> (also <code>vn.opt</code> if needed). Options include <code>"GCV"</code>, <code>"BIC"</code>, <code>"AIC"</code>, or <code>"k-fold-CV"</code>. The default setting is <code>"GCV"</code>.
</p>
</td></tr>
<tr><td><code id="sfpl.kernel.fit_+3A_penalty">penalty</code></td>
<td>

<p>The penalty function applied in the penalised least-squares procedure. Currently, only &quot;grLasso&quot; and &quot;grSCAD&quot; are implemented. The default is &quot;grSCAD&quot;.
</p>
</td></tr>
<tr><td><code id="sfpl.kernel.fit_+3A_max.iter">max.iter</code></td>
<td>

<p>Maximum number of iterations allowed across the entire path. The default value is 1000.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The sparse semi-functional partial linear model (SFPLM) is given by the expression:
</p>
<p style="text-align: center;"><code class="reqn">
Y_i = Z_{i1}\beta_{01} + \dots + Z_{ip_n}\beta_{0p_n} + m(X_i) + \varepsilon_i,\ \ \ i = 1, \dots, n,
</code>
</p>

<p>where <code class="reqn">Y_i</code> denotes a scalar response, <code class="reqn">Z_{i1}, \dots, Z_{ip_n}</code> are real random covariates, and <code class="reqn">X_i</code> is a functional random covariate valued in a semi-metric space <code class="reqn">\mathcal{H}</code>. In this equation,
<code class="reqn">\mathbf{\beta}_0 = (\beta_{01}, \dots, \beta_{0p_n})^{\top}</code> and <code class="reqn">m(\cdot)</code> represent a vector of unknown real parameters and an unknown smooth real-valued function, respectively. Additionally, <code class="reqn">\varepsilon_i</code> is the random error.
</p>
<p>In this function, the SFPLM is fitted using a penalised least-squares approach. The approach involves transforming the SFPLM into a linear model by extracting from <code class="reqn">Y_i</code> and <code class="reqn">Z_{ij}</code> (<code class="reqn">j = 1, \ldots, p_n</code>) the effect of the functional covariate <code class="reqn">X_i</code> using functional nonparametric regression (for details, see Ferraty and Vieu, 2006). This transformation is achieved using kernel estimation with Nadaraya-Watson weights.
</p>
<p>An approximate linear model is then obtained:
</p>
<p style="text-align: center;"><code class="reqn">\widetilde{\mathbf{Y}}\approx\widetilde{\mathbf{Z}}\mathbf{\beta}_0+\mathbf{\varepsilon},</code>
</p>

<p>and the penalised least-squares procedure is applied to this model by minimising
</p>
<p style="text-align: center;"><code class="reqn">
\mathcal{Q}\left(\mathbf{\beta}\right)=\frac{1}{2}\left(\widetilde{\mathbf{Y}}-\widetilde{\mathbf{Z}}\mathbf{\beta}\right)^{\top}\left(\widetilde{\mathbf{Y}}-\widetilde{\mathbf{Z}}\mathbf{\beta}\right)+n\sum_{j=1}^{p_n}\mathcal{P}_{\lambda_{j_n}}\left(|\beta_j|\right), \quad (1)
</code>
</p>

<p>where <code class="reqn">\mathbf{\beta} = (\beta_1, \ldots, \beta_{p_n})^{\top}, \ \mathcal{P}_{\lambda_{j_n}}(\cdot)</code> is a penalty function (specified in the argument <code>penalty</code>) and <code class="reqn">\lambda_{j_n} &gt; 0</code> is a tuning parameter.
To reduce the number of tuning parameters, <code class="reqn">\lambda_j</code>, to be selected for each sample, we consider <code class="reqn">\lambda_j = \lambda \widehat{\sigma}_{\beta_{0,j,OLS}}</code>, where <code class="reqn">\beta_{0,j,OLS}</code> denotes the OLS estimate of <code class="reqn">\beta_{0,j}</code> and <code class="reqn">\widehat{\sigma}_{\beta_{0,j,OLS}}</code> is the estimated standard deviation. Both <code class="reqn">\lambda</code> and <code class="reqn">h</code> (in the kernel estimation) are selected using the objective criterion specified in the argument <code>criterion</code>.
</p>
<p>Finally, after estimating <code class="reqn">\mathbf{\beta}_0</code> by minimising (1), we address the estimation of the nonlinear function <code class="reqn">m(\cdot)</code>.
For this, we again employ the kernel procedure with Nadaraya-Watson weights to smooth the partial residuals <code class="reqn">Y_i - \mathbf{Z}_i^{\top}\widehat{\mathbf{\beta}}</code>.
</p>
<p>For further details on the estimation procedure of the sparse SFPLM, see Aneiros et al. (2015).
</p>
<p><b>Remark</b>: It should be noted that if we set <code>lambda.seq</code> to <code class="reqn">0</code>, we can obtain the non-penalised estimation of the model, i.e. the OLS estimation. Using <code>lambda.seq</code> with a value <code class="reqn">\not= 0</code> is advisable when suspecting the presence of irrelevant variables.
</p>


<h3>Value</h3>

<table>
<tr><td><code>call</code></td>
<td>
<p>The matched call.</p>
</td></tr>
<tr><td><code>fitted.values</code></td>
<td>
<p>Estimated scalar response.</p>
</td></tr>
<tr><td><code>residuals</code></td>
<td>
<p>Differences between <code>y</code> and the <code>fitted.values</code>.</p>
</td></tr>
<tr><td><code>beta.est</code></td>
<td>
<p>Estimate of <code class="reqn">\beta_0</code> when the optimal tuning parameters <code>lambda.opt</code>, <code>h.opt</code> and <code>vn.opt</code> are used.</p>
</td></tr>
<tr><td><code>indexes.beta.nonnull</code></td>
<td>
<p>Indexes of the non-zero <code class="reqn">\hat{\beta_{j}}</code>.</p>
</td></tr>
<tr><td><code>h.opt</code></td>
<td>
<p>Selected bandwidth.</p>
</td></tr>
<tr><td><code>lambda.opt</code></td>
<td>
<p>Selected value of lambda.</p>
</td></tr>
<tr><td><code>IC</code></td>
<td>
<p>Value of the criterion function considered to select <code>lambda.opt</code>, <code>h.opt</code> and <code>vn.opt</code>.</p>
</td></tr>
<tr><td><code>h.min.opt.max.mopt</code></td>
<td>
<p><code>h.opt=h.min.opt.max.mopt[2]</code> (used by <code>beta.est</code>) was seeked between <code>h.min.opt.max.mopt[1]</code> and <code>h.min.opt.max.mopt[3]</code>.</p>
</td></tr>
<tr><td><code>vn.opt</code></td>
<td>
<p>Selected value of <code>vn</code>.</p>
</td></tr>
<tr><td><code>...</code></td>
<td>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>German Aneiros Perez <a href="mailto:german.aneiros@udc.es">german.aneiros@udc.es</a> 
</p>
<p>Silvia Novo Diaz  <a href="mailto:snovo@est-econ.uc3m.es">snovo@est-econ.uc3m.es</a>
</p>


<h3>References</h3>

<p>Aneiros, G., Ferraty, F., Vieu, P. (2015) Variable selection in partial linear regression with functional
covariate. <em>Statistics</em>, <b>49</b>, 1322&ndash;1347, <a href="https://doi.org/10.1080/02331888.2014.998675">doi:10.1080/02331888.2014.998675</a>.
</p>
<p>Ferraty, F. and Vieu, P. (2006) <em>Nonparametric Functional Data Analysis</em>. Springer Series in Statistics, New York.
</p>


<h3>See Also</h3>

<p>See also <code><a href="#topic+predict.sfpl.kernel">predict.sfpl.kernel</a></code> and  <code><a href="#topic+plot.sfpl.kernel">plot.sfpl.kernel</a></code>.
</p>
<p>Alternative method <code><a href="#topic+sfpl.kNN.fit">sfpl.kNN.fit</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("Tecator")
y&lt;-Tecator$fat
X&lt;-Tecator$absor.spectra
z1&lt;-Tecator$protein       
z2&lt;-Tecator$moisture

#Quadratic, cubic and interaction effects of the scalar covariates.
z.com&lt;-cbind(z1,z2,z1^2,z2^2,z1^3,z2^3,z1*z2)
train&lt;-1:160

#SFPLM fit. 
ptm=proc.time()
fit&lt;-sfpl.kernel.fit(x=X[train,], z=z.com[train,], y=y[train],q=2, 
      max.q.h=0.35, lambda.min.l=0.01,
      max.iter=5000, criterion="BIC", nknot=20)
proc.time()-ptm

#Results
fit
names(fit)
</code></pre>

<hr>
<h2 id='sfpl.kNN.fit'>
SFPLM regularised fit using kNN estimation
</h2><span id='topic+sfpl.kNN.fit'></span>

<h3>Description</h3>

<p>This function fits a sparse semi-functional partial linear model (SFPLM). It employs a penalised least-squares regularisation procedure, integrated with nonparametric kNN estimation using Nadaraya-Watson weights.
</p>
<p>The procedure utilises an objective criterion (<code>criterion</code>) to select both the bandwidth (<code>h.opt</code>) and the regularisation parameter (<code>lambda.opt</code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sfpl.kNN.fit(x, z, y, semimetric = "deriv", q = NULL, knearest = NULL,
min.knn = 2, max.knn = NULL, step = NULL, range.grid = NULL, 
kind.of.kernel = "quad", nknot = NULL, lambda.min = NULL, lambda.min.h = NULL, 
lambda.min.l = NULL, factor.pn = 1, nlambda = 100, lambda.seq = NULL, 
vn = ncol(z), nfolds = 10, seed = 123, criterion = "GCV", penalty = "grSCAD", 
max.iter = 1000)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sfpl.kNN.fit_+3A_x">x</code></td>
<td>

<p>Matrix containing the observations of the functional covariate (functional nonparametric component), collected by row.
</p>
</td></tr>
<tr><td><code id="sfpl.kNN.fit_+3A_z">z</code></td>
<td>

<p>Matrix containing the observations of the scalar covariates (linear component), collected by row.
</p>
</td></tr>
<tr><td><code id="sfpl.kNN.fit_+3A_y">y</code></td>
<td>

<p>Vector containing the scalar response.
</p>
</td></tr>
<tr><td><code id="sfpl.kNN.fit_+3A_semimetric">semimetric</code></td>
<td>

<p>Semi-metric function.  Only <code>"deriv"</code> and <code>"pca"</code> are implemented.  By default <code>semimetric="deriv"</code>.
</p>
</td></tr>
<tr><td><code id="sfpl.kNN.fit_+3A_q">q</code></td>
<td>

<p>Order of the derivative (if <code>semimetric="deriv"</code>) or number of principal components (if <code>semimetric="pca"</code>). The default values are 0 and 2, respectively.
</p>
</td></tr>
<tr><td><code id="sfpl.kNN.fit_+3A_knearest">knearest</code></td>
<td>

<p>Vector of positive integers containing the sequence in which the  number of nearest neighbours <code>k.opt</code> is selected. If <code>knearest=NULL</code>, then <code>knearest &lt;- seq(from =min.knn, to = max.knn, by = step)</code>.
</p>
</td></tr>
<tr><td><code id="sfpl.kNN.fit_+3A_min.knn">min.knn</code></td>
<td>

<p>A positive integer that represents the minimum value in the sequence for selecting the number of nearest neighbours <code>k.opt</code>. This value should be less than the sample size. The default is 2.
</p>
</td></tr>
<tr><td><code id="sfpl.kNN.fit_+3A_max.knn">max.knn</code></td>
<td>

<p>A positive integer that represents the maximum value in the sequence for selecting number of nearest neighbours <code>k.opt</code>. This value should be less than the sample size. The default is <code>max.knn &lt;- n%/%5</code>.
</p>
</td></tr>
<tr><td><code id="sfpl.kNN.fit_+3A_step">step</code></td>
<td>

<p>A positive integer used to construct the sequence of k-nearest neighbours as follows: <code>min.knn, min.knn + step, min.knn + 2*step, min.knn + 3*step,...</code>. The default value for <code>step</code> is <code>step&lt;-ceiling(n/100)</code>.
</p>
</td></tr>
<tr><td><code id="sfpl.kNN.fit_+3A_range.grid">range.grid</code></td>
<td>

<p>Vector of length 2 containing the endpoints of the grid at which the observations of the functional covariate <code>x</code> are evaluated (i.e. the range of the discretisation). If <code>range.grid=NULL</code>, then <code>range.grid=c(1,p)</code> is considered, where <code>p</code> is the discretisation size of <code>x</code> (i.e. <code>ncol(x))</code>.
</p>
</td></tr>
<tr><td><code id="sfpl.kNN.fit_+3A_kind.of.kernel">kind.of.kernel</code></td>
<td>

<p>The type of kernel function used. Currently, only Epanechnikov kernel (<code>"quad"</code>) is available.
</p>
</td></tr>
<tr><td><code id="sfpl.kNN.fit_+3A_nknot">nknot</code></td>
<td>

<p>Positive integer indicating the number of interior knots for the B-spline expansion of the functional covariate. The default value is <code>(p - order.Bspline - 1)%/%2</code>.
</p>
</td></tr>
<tr><td><code id="sfpl.kNN.fit_+3A_lambda.min">lambda.min</code></td>
<td>

<p>The smallest value for lambda (i.e. the lower endpoint  of the sequence in which <code>lambda.opt</code> is selected), as fraction of <code>lambda.max</code>.
The defaults is <code>lambda.min.l</code> if the sample size is larger than <code>factor.pn</code> times the number of linear covariates and <code>lambda.min.h</code> otherwise.
</p>
</td></tr>
<tr><td><code id="sfpl.kNN.fit_+3A_lambda.min.h">lambda.min.h</code></td>
<td>

<p>The lower endpoint of the sequence in which <code>lambda.opt</code> is selected if the sample size is smaller than <code>factor.pn</code> times the number of linear covariates. The default is 0.05. 
</p>
</td></tr>
<tr><td><code id="sfpl.kNN.fit_+3A_lambda.min.l">lambda.min.l</code></td>
<td>

<p>The lower endpoint of the sequence in which <code>lambda.opt</code> is selected if the sample size is larger than <code>factor.pn</code> times the number of linear covariates. The default is 0.0001.
</p>
</td></tr>
<tr><td><code id="sfpl.kNN.fit_+3A_factor.pn">factor.pn</code></td>
<td>

<p>Positive integer used to set <code>lambda.min</code>. The default value is 1.
</p>
</td></tr>
<tr><td><code id="sfpl.kNN.fit_+3A_nlambda">nlambda</code></td>
<td>

<p>Positive integer indicating the number of values in the sequence from which <code>lambda.opt</code> is selected. The default is 100.
</p>
</td></tr>
<tr><td><code id="sfpl.kNN.fit_+3A_lambda.seq">lambda.seq</code></td>
<td>

<p>Sequence of values in which <code>lambda.opt</code> is selected. If <code>lambda.seq=NULL</code>, then the programme builds the sequence automatically using <code>lambda.min</code> and <code>nlambda</code>.
</p>
</td></tr>
<tr><td><code id="sfpl.kNN.fit_+3A_vn">vn</code></td>
<td>

<p>Positive integer or vector of positive integers indicating the number of groups of consecutive variables to be penalised together. The default value is <code>vn=ncol(z)</code>, resulting in the individual penalization of each scalar covariate.
</p>
</td></tr>
<tr><td><code id="sfpl.kNN.fit_+3A_nfolds">nfolds</code></td>
<td>

<p>Number of cross-validation folds (used when <code>criterion="k-fold-CV"</code>). Default is 10.
</p>
</td></tr>
<tr><td><code id="sfpl.kNN.fit_+3A_seed">seed</code></td>
<td>

<p>You may set the seed for the random number generator to ensure reproducible results (applicable when <code>criterion="k-fold-CV"</code> is used). The default seed value is 123.
</p>
</td></tr>
<tr><td><code id="sfpl.kNN.fit_+3A_criterion">criterion</code></td>
<td>

<p>The criterion used to select the tuning and regularisation parameter: <code>k.opt</code>  and <code>lambda.opt</code> (also <code>vn.opt</code> if needed). Options include <code>"GCV"</code>, <code>"BIC"</code>, <code>"AIC"</code>, or <code>"k-fold-CV"</code>. The default setting is <code>"GCV"</code>.
</p>
</td></tr>
<tr><td><code id="sfpl.kNN.fit_+3A_penalty">penalty</code></td>
<td>

<p>The penalty function applied in the penalised least-squares procedure. Currently, only &quot;grLasso&quot; and &quot;grSCAD&quot; are implemented. The default is &quot;grSCAD&quot;.
</p>
</td></tr>
<tr><td><code id="sfpl.kNN.fit_+3A_max.iter">max.iter</code></td>
<td>

<p>Maximum number of iterations allowed across the entire path. The default value is 1000.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The sparse semi-functional partial linear model (SFPLM) is given by the expression:
</p>
<p style="text-align: center;"><code class="reqn">
Y_i = Z_{i1}\beta_{01} + \dots + Z_{ip_n}\beta_{0p_n} + m(X_i) + \varepsilon_i,\ \ \ i = 1, \dots, n,
</code>
</p>

<p>where <code class="reqn">Y_i</code> denotes a scalar response, <code class="reqn">Z_{i1}, \dots, Z_{ip_n}</code> are real random covariates, and <code class="reqn">X_i</code> is a functional random covariate valued in a semi-metric space <code class="reqn">\mathcal{H}</code>. In this equation,
<code class="reqn">\mathbf{\beta}_0 = (\beta_{01}, \dots, \beta_{0p_n})^{\top}</code> and <code class="reqn">m(\cdot)</code> represent a vector of unknown real parameters and an unknown smooth real-valued function, respectively. Additionally, <code class="reqn">\varepsilon_i</code> is the random error.
</p>
<p>In this function, the SFPLM is fitted using a penalised least-squares approach. The approach involves transforming the SFPLM into a linear model by extracting from <code class="reqn">Y_i</code> and <code class="reqn">Z_{ij}</code> (<code class="reqn">j = 1, \ldots, p_n</code>) the effect of the functional covariate <code class="reqn">X_i</code> using functional nonparametric regression (for details, see Ferraty and Vieu, 2006). This transformation is achieved using kNN estimation with Nadaraya-Watson weights.
</p>
<p>An approximate linear model is then obtained:
</p>
<p style="text-align: center;"><code class="reqn">\widetilde{\mathbf{Y}}\approx\widetilde{\mathbf{Z}}\mathbf{\beta}_0+\mathbf{\varepsilon},</code>
</p>

<p>and the penalised least-squares procedure is applied to this model by minimising
</p>
<p style="text-align: center;"><code class="reqn">
\mathcal{Q}\left(\mathbf{\beta}\right)=\frac{1}{2}\left(\widetilde{\mathbf{Y}}-\widetilde{\mathbf{Z}}\mathbf{\beta}\right)^{\top}\left(\widetilde{\mathbf{Y}}-\widetilde{\mathbf{Z}}\mathbf{\beta}\right)+n\sum_{j=1}^{p_n}\mathcal{P}_{\lambda_{j_n}}\left(|\beta_j|\right), \quad (1)
</code>
</p>

<p>where <code class="reqn">\mathbf{\beta} = (\beta_1, \ldots, \beta_{p_n})^{\top}, \ \mathcal{P}_{\lambda_{j_n}}(\cdot)</code> is a penalty function (specified in the argument <code>penalty</code>) and <code class="reqn">\lambda_{j_n} &gt; 0</code> is a tuning parameter.
To reduce the number of tuning parameters, <code class="reqn">\lambda_j</code>, to be selected for each sample, we consider <code class="reqn">\lambda_j = \lambda \widehat{\sigma}_{\beta_{0,j,OLS}}</code>, where <code class="reqn">\beta_{0,j,OLS}</code> denotes the OLS estimate of <code class="reqn">\beta_{0,j}</code> and <code class="reqn">\widehat{\sigma}_{\beta_{0,j,OLS}}</code> is the estimated standard deviation. Both <code class="reqn">\lambda</code> and <code class="reqn">k</code> (in the kNN estimation) are selected using the objective criterion specified in the argument <code>criterion</code>.
</p>
<p>Finally, after estimating <code class="reqn">\mathbf{\beta}_0</code> by minimising (1), we address the estimation of the nonlinear function <code class="reqn">m(\cdot)</code>.
For this, we again employ the kNN procedure with Nadaraya-Watson weights to smooth the partial residuals <code class="reqn">Y_i - \mathbf{Z}_i^{\top}\widehat{\mathbf{\beta}}</code>.
</p>
<p>For further details on the estimation procedure of the sparse SFPLM, see Aneiros et al. (2015).
</p>
<p><b>Remark</b>: It should be noted that if we set <code>lambda.seq</code> to <code class="reqn">0</code>, we can obtain the non-penalised estimation of the model, i.e. the OLS estimation. Using <code>lambda.seq</code> with a value <code class="reqn">\not= 0</code> is advisable when suspecting the presence of irrelevant variables.
</p>


<h3>Value</h3>

<table>
<tr><td><code>call</code></td>
<td>
<p>The matched call.</p>
</td></tr>
<tr><td><code>fitted.values</code></td>
<td>
<p>Estimated scalar response.</p>
</td></tr>
<tr><td><code>residuals</code></td>
<td>
<p>Differences between <code>y</code> and the <code>fitted.values</code></p>
</td></tr>
<tr><td><code>beta.est</code></td>
<td>
<p>Estimate of <code class="reqn">\beta_0</code> when the optimal tuning parameters <code>lambda.opt</code>, <code>k.opt</code> and <code>vn.opt</code> are used.</p>
</td></tr>
<tr><td><code>indexes.beta.nonnull</code></td>
<td>
<p>Indexes of the non-zero <code class="reqn">\hat{\beta_{j}}</code>.</p>
</td></tr>
<tr><td><code>k.opt</code></td>
<td>
<p>Selected number of nearest neighbours.</p>
</td></tr>
<tr><td><code>lambda.opt</code></td>
<td>
<p>Selected value of lambda.</p>
</td></tr>
<tr><td><code>IC</code></td>
<td>
<p>Value of the criterion function considered to select both <code>lambda.opt</code>, <code>h.opt</code> and <code>vn.opt</code>.</p>
</td></tr>
<tr><td><code>vn.opt</code></td>
<td>
<p>Selected value of <code>vn</code>.</p>
</td></tr>
<tr><td><code>...</code></td>
<td>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>German Aneiros Perez <a href="mailto:german.aneiros@udc.es">german.aneiros@udc.es</a> 
</p>
<p>Silvia Novo Diaz  <a href="mailto:snovo@est-econ.uc3m.es">snovo@est-econ.uc3m.es</a>
</p>


<h3>References</h3>

<p>Aneiros, G., Ferraty, F., Vieu, P. (2015) Variable selection in partial linear regression with functional
covariate. <em>Statistics</em>, <b>49</b>, 1322&ndash;1347, <a href="https://doi.org/10.1080/02331888.2014.998675">doi:10.1080/02331888.2014.998675</a>.
</p>


<h3>See Also</h3>

<p>See also <code><a href="#topic+predict.sfpl.kNN">predict.sfpl.kNN</a></code> and  <code><a href="#topic+plot.sfpl.kNN">plot.sfpl.kNN</a></code>.
</p>
<p>Alternative method <code><a href="#topic+sfpl.kernel.fit">sfpl.kernel.fit</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data("Tecator")
y&lt;-Tecator$fat
X&lt;-Tecator$absor.spectra
z1&lt;-Tecator$protein       
z2&lt;-Tecator$moisture

#Quadratic, cubic and interaction effects of the scalar covariates.
z.com&lt;-cbind(z1,z2,z1^2,z2^2,z1^3,z2^3,z1*z2)
train&lt;-1:160

#SFPLM fit. 
ptm=proc.time()
fit&lt;-sfpl.kNN.fit(y=y[train],x=X[train,], z=z.com[train,],q=2, max.knn=20,
  lambda.min.l=0.01, criterion="BIC",
  range.grid=c(850,1050), nknot=20, max.iter=5000)
proc.time()-ptm

#Results
fit
names(fit)
</code></pre>

<hr>
<h2 id='sfplsim.kernel.fit'>
SFPLSIM regularised fit using kernel estimation
</h2><span id='topic+sfplsim.kernel.fit'></span>

<h3>Description</h3>

<p>This function fits a sparse semi-functional partial linear single-index (SFPLSIM). It employs a penalised least-squares regularisation procedure, integrated with nonparametric kernel estimation using Nadaraya-Watson weights.
</p>
<p>The function uses B-spline expansions to represent curves and eligible functional indexes.  It also utilises an objective criterion (<code>criterion</code>) to select both the bandwidth (<code>h.opt</code>) and the regularisation parameter (<code>lambda.opt</code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sfplsim.kernel.fit(x, z, y, seed.coeff = c(-1, 0, 1), order.Bspline = 3, 
nknot.theta = 3, min.q.h = 0.05, max.q.h = 0.5, h.seq = NULL, num.h = 10, 
range.grid = NULL, kind.of.kernel = "quad", nknot = NULL, lambda.min = NULL,
lambda.min.h = NULL, lambda.min.l = NULL, factor.pn = 1, nlambda = 100, 
lambda.seq = NULL, vn = ncol(z), nfolds = 10, seed = 123, criterion = "GCV",
penalty = "grSCAD", max.iter = 1000, n.core = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sfplsim.kernel.fit_+3A_x">x</code></td>
<td>

<p>Matrix containing the observations of the functional covariate (functional single-index component), collected by row.
</p>
</td></tr>
<tr><td><code id="sfplsim.kernel.fit_+3A_z">z</code></td>
<td>

<p>Matrix containing the observations of the scalar covariates (linear component), collected by row.
</p>
</td></tr>
<tr><td><code id="sfplsim.kernel.fit_+3A_y">y</code></td>
<td>

<p>Vector containing the scalar response.
</p>
</td></tr>
<tr><td><code id="sfplsim.kernel.fit_+3A_seed.coeff">seed.coeff</code></td>
<td>

<p>Vector of initial values used to  build the set <code class="reqn">\Theta_n</code> (see section <code>Details</code>). The coefficients for the B-spline representation of each eligible functional index <code class="reqn">\theta \in \Theta_n</code> are obtained from <code>seed.coeff</code>.  The default is <code>c(-1,0,1)</code>.
</p>
</td></tr>
<tr><td><code id="sfplsim.kernel.fit_+3A_order.bspline">order.Bspline</code></td>
<td>

<p>Positive integer giving the order of the B-spline basis functions. This is the number of coefficients in each piecewise polynomial segment. The default is 3.
</p>
</td></tr>
<tr><td><code id="sfplsim.kernel.fit_+3A_nknot.theta">nknot.theta</code></td>
<td>

<p>Positive integer indicating the number of regularly spaced interior knots in the B-spline expansion of <code class="reqn">\theta_0</code>. The default is 3.
</p>
</td></tr>
<tr><td><code id="sfplsim.kernel.fit_+3A_min.q.h">min.q.h</code></td>
<td>
<p>Minimum quantile order of the distances between curves, which are computed using the projection semi-metric. This value determines the lower endpoint of the range from which the bandwidth is selected. The default is 0.05.
</p>
</td></tr>
<tr><td><code id="sfplsim.kernel.fit_+3A_max.q.h">max.q.h</code></td>
<td>

<p>Maximum quantile order of the distances between curves, which are computed using the projection semi-metric. This value determines the upper endpoint of the range from which the bandwidth is selected. The default is 0.5.
</p>
</td></tr>
<tr><td><code id="sfplsim.kernel.fit_+3A_h.seq">h.seq</code></td>
<td>
<p>Vector containing the sequence of bandwidths. The default is a sequence of <code>num.h</code> equispaced bandwidths in the range constructed using <code>min.q.h</code> and <code>max.q.h</code>.
</p>
</td></tr>
<tr><td><code id="sfplsim.kernel.fit_+3A_num.h">num.h</code></td>
<td>
<p>Positive integer indicating the number of bandwidths in the grid. The default is 10.
</p>
</td></tr>
<tr><td><code id="sfplsim.kernel.fit_+3A_range.grid">range.grid</code></td>
<td>

<p>Vector of length 2 containing the endpoints of the grid at which the observations of the functional covariate <code>x</code> are evaluated (i.e. the range of the discretisation). If <code>range.grid=NULL</code>, then <code>range.grid=c(1,p)</code> is considered, where <code>p</code> is the discretisation size of <code>x</code> (i.e. <code>ncol(x))</code>.
</p>
</td></tr>
<tr><td><code id="sfplsim.kernel.fit_+3A_kind.of.kernel">kind.of.kernel</code></td>
<td>

<p>The type of kernel function used. Currently, only Epanechnikov kernel (<code>"quad"</code>) is available.
</p>
</td></tr>
<tr><td><code id="sfplsim.kernel.fit_+3A_nknot">nknot</code></td>
<td>

<p>Positive integer indicating the number of interior knots for the B-spline expansion of the functional covariate. The default value is <code>(p - order.Bspline - 1)%/%2</code>.
</p>
</td></tr>
<tr><td><code id="sfplsim.kernel.fit_+3A_lambda.min">lambda.min</code></td>
<td>

<p>The smallest value for lambda (i. e., the lower endpoint  of the sequence in which <code>lambda.opt</code> is selected), as fraction of <code>lambda.max</code>.
The defaults is <code>lambda.min.l</code> if the sample size is larger than <code>factor.pn</code> times the number of linear covariates and <code>lambda.min.h</code> otherwise.
</p>
</td></tr>
<tr><td><code id="sfplsim.kernel.fit_+3A_lambda.min.h">lambda.min.h</code></td>
<td>

<p>The lower endpoint of the sequence in which <code>lambda.opt</code> is selected if the sample size is smaller than <code>factor.pn</code> times the number of linear covariates. The default is 0.05. 
</p>
</td></tr>
<tr><td><code id="sfplsim.kernel.fit_+3A_lambda.min.l">lambda.min.l</code></td>
<td>

<p>The lower endpoint of the sequence in which <code>lambda.opt</code> is selected if the sample size is larger than <code>factor.pn</code> times the number of linear covariates. The default is 0.0001.
</p>
</td></tr>
<tr><td><code id="sfplsim.kernel.fit_+3A_factor.pn">factor.pn</code></td>
<td>

<p>Positive integer used to set <code>lambda.min</code>. The default value is 1.
</p>
</td></tr>
<tr><td><code id="sfplsim.kernel.fit_+3A_nlambda">nlambda</code></td>
<td>

<p>Positive integer indicating the number of values in the sequence from which <code>lambda.opt</code> is selected. The default is 100.
</p>
</td></tr>
<tr><td><code id="sfplsim.kernel.fit_+3A_lambda.seq">lambda.seq</code></td>
<td>

<p>Sequence of values in which <code>lambda.opt</code> is selected. If <code>lambda.seq=NULL</code>, then the programme builds the sequence automatically using <code>lambda.min</code> and <code>nlambda</code>.
</p>
</td></tr>
<tr><td><code id="sfplsim.kernel.fit_+3A_vn">vn</code></td>
<td>

<p>Positive integer or vector of positive integers indicating the number of groups of consecutive variables to be penalised together. The default value is <code>vn=ncol(z)</code>, resulting in the individual penalization of each scalar covariate.
</p>
</td></tr>
<tr><td><code id="sfplsim.kernel.fit_+3A_nfolds">nfolds</code></td>
<td>

<p>Number of cross-validation folds (used when <code>criterion="k-fold-CV"</code>). Default is 10.
</p>
</td></tr>
<tr><td><code id="sfplsim.kernel.fit_+3A_seed">seed</code></td>
<td>

<p>You may set the seed for the random number generator to ensure reproducible results (applicable when <code>criterion="k-fold-CV"</code> is used). The default seed value is 123.
</p>
</td></tr>
<tr><td><code id="sfplsim.kernel.fit_+3A_criterion">criterion</code></td>
<td>

<p>The criterion used to select the tuning and regularisation parameter: <code>h.opt</code> and <code>lambda.opt</code>  (also <code>vn.opt</code> if needed). Options include <code>"GCV"</code>, <code>"BIC"</code>, <code>"AIC"</code>, or <code>"k-fold-CV"</code>. The default setting is <code>"GCV"</code>.
</p>
</td></tr>
<tr><td><code id="sfplsim.kernel.fit_+3A_penalty">penalty</code></td>
<td>

<p>The penalty function applied in the penalised least-squares procedure. Currently, only &quot;grLasso&quot; and &quot;grSCAD&quot; are implemented. The default is &quot;grSCAD&quot;.
</p>
</td></tr>
<tr><td><code id="sfplsim.kernel.fit_+3A_max.iter">max.iter</code></td>
<td>

<p>Maximum number of iterations allowed across the entire path. The default value is 1000.
</p>
</td></tr>
<tr><td><code id="sfplsim.kernel.fit_+3A_n.core">n.core</code></td>
<td>

<p>Number of CPU cores designated for parallel execution. The default is <code>n.core&lt;-availableCores(omit=1)</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The sparse semi-functional partial linear single-index model (SFPLSIM) is given by the expression:
</p>
<p style="text-align: center;"><code class="reqn">
Y_i=Z_{i1}\beta_{01}+\dots+Z_{ip_n}\beta_{0p_n}+r(\left&lt;\theta_0,X_i\right&gt;)+\varepsilon_i\ \ \ i=1,\dots,n,
</code>
</p>

<p>where <code class="reqn">Y_i</code> denotes a scalar response, <code class="reqn">Z_{i1},\dots,Z_{ip_n}</code> are real random covariates and <code class="reqn">X_i</code> is a functional random covariate valued in a separable Hilbert space <code class="reqn">\mathcal{H}</code> with inner product <code class="reqn">\left\langle \cdot, \cdot \right\rangle</code>. In this equation,
<code class="reqn">\mathbf{\beta}_0=(\beta_{01},\dots,\beta_{0p_n})^{\top}</code>, <code class="reqn">\theta_0\in\mathcal{H}</code> and <code class="reqn">r(\cdot)</code> are a vector of unknown real parameters, an unknown functional direction and an unknown smooth real-valued function, respectively. In addition, <code class="reqn">\varepsilon_i</code> is the random error.
</p>
<p>The sparse SFPLSIM is fitted using the penalised least-squares approach. The first step is to transform the SSFPLSIM into a linear model by extracting from <code class="reqn">Y_i</code> and <code class="reqn">Z_{ij}</code> (<code class="reqn">j=1,\ldots,p_n</code>) the effect of the functional covariate <code class="reqn">X_i</code> using functional single-index regression.  This transformation is achieved using nonparametric kernel estimation (see, for details, the documentation of the function <code>fsim.kernel.fit</code>).
</p>
<p>An approximate linear model is then obtained:
</p>
<p style="text-align: center;"><code class="reqn">\widetilde{\mathbf{Y}}_{\theta_0}\approx\widetilde{\mathbf{Z}}_{\theta_0}\mathbf{\beta}_0+\mathbf{\varepsilon},</code>
</p>

<p>and the penalised least-squares procedure is applied to this model by minimising over the pair <code class="reqn">(\mathbf{\beta},\theta)</code>
</p>
<p style="text-align: center;"><code class="reqn">
\mathcal{Q}\left(\mathbf{\beta},\theta\right)=\frac{1}{2}\left(\widetilde{\mathbf{Y}}_{\theta}-\widetilde{\mathbf{Z}}_{\theta}\mathbf{\beta}\right)^{\top}\left(\widetilde{\mathbf{Y}}_{\theta}-\widetilde{\mathbf{Z}}_{\theta}\mathbf{\beta}\right)+n\sum_{j=1}^{p_n}\mathcal{P}_{\lambda_{j_n}}\left(|\beta_j|\right), \quad (1)
</code>
</p>

<p>where <code class="reqn">\mathbf{\beta}=(\beta_1,\ldots,\beta_{p_n})^{\top}, \ \mathcal{P}_{\lambda_{j_n}}\left(\cdot\right)</code> is a penalty function (specified in the argument <code>penalty</code>) and <code class="reqn">\lambda_{j_n} &gt; 0</code> is a tuning parameter.
To reduce  the quantity of tuning parameters, <code class="reqn">\lambda_j</code>, to be selected for each sample, we consider <code class="reqn">\lambda_j = \lambda \widehat{\sigma}_{\beta_{0,j,OLS}}</code>, where <code class="reqn">\beta_{0,j,OLS}</code> denotes the OLS estimate of <code class="reqn">\beta_{0,j}</code> and <code class="reqn">\widehat{\sigma}_{\beta_{0,j,OLS}}</code> is the estimated standard deviation. Both <code class="reqn">\lambda</code> and <code class="reqn">h</code> (in the kernel estimation) are selected using the objetive criterion specified in the argument <code>criterion</code>.
</p>
<p>In addition, the function uses a B-spline representation to construct a set  <code class="reqn">\Theta_n</code> of eligible functional indexes <code class="reqn">\theta</code>. The dimension of the B-spline basis is <code>order.Bspline</code>+<code>nknot.theta</code> and the set of eligible coefficients is obtained by calibrating (to ensure the identifiability of the model) the set of initial coefficients given in <code>seed.coeff</code>. The larger this set, the greater the size of <code class="reqn">\Theta_n</code>. ue to the intensive computation required by our approach, a balance between the size of <code class="reqn">\Theta_n</code> and the performance of the estimator is necessary. For that, Ait-Saidi et al. (2008) suggested considering <code>order.Bspline=3</code> and <code>seed.coeff=c(-1,0,1)</code>. For details on the construction of <code class="reqn">\Theta_n</code> see Novo et al. (2019).
</p>
<p>Finally, after estimating <code class="reqn">\mathbf{\beta}_0</code> and <code class="reqn">\theta_0</code> by minimising (1), we proceed to estimate the nonlinear function <code class="reqn">r_{\theta_0}(\cdot)\equiv r\left(\left&lt;\theta_0,\cdot\right&gt;\right)</code>.
For this purporse, we again apply the kernel procedure with Nadaraya-Watson weights to smooth the partial residuals <code class="reqn">Y_i-\mathbf{Z}_i^{\top}\widehat{\mathbf{\beta}}</code>.
</p>
<p>For further details on the estimation procedure of the SSFPLSIM, see Novo et al. (2021).
</p>
<p><b>Remark</b>: It should be noted that if we set <code>lambda.seq</code> to <code class="reqn">0</code>, we can obtain the non-penalised estimation of the model, i.e. the OLS estimation. Using <code>lambda.seq</code> with a value <code class="reqn">\not= 0</code> is advisable when suspecting the presence of irrelevant variables.
</p>


<h3>Value</h3>

<table>
<tr><td><code>call</code></td>
<td>
<p>The matched call.</p>
</td></tr>
<tr><td><code>fitted.values</code></td>
<td>
<p>Estimated scalar response.</p>
</td></tr>
<tr><td><code>residuals</code></td>
<td>
<p>Differences between <code>y</code> and the <code>fitted.values</code>.</p>
</td></tr>
<tr><td><code>beta.est</code></td>
<td>
<p>Estimate of <code class="reqn">\beta_0</code> when the optimal tuning parameters <code>lambda.opt</code>, <code>h.opt</code> and <code>vn.opt</code> are used.</p>
</td></tr>
<tr><td><code>theta.est</code></td>
<td>
<p>Coefficients of <code class="reqn">\hat{\theta}</code> in the B-spline basis (when the optimal tuning parameters <code>lambda.opt</code>, <code>h.opt</code> and <code>vn.opt</code> are used): a vector of <code>length(order.Bspline+nknot.theta)</code>.</p>
</td></tr>
<tr><td><code>indexes.beta.nonnull</code></td>
<td>
<p>Indexes of the non-zero <code class="reqn">\hat{\beta_{j}}</code>.</p>
</td></tr>
<tr><td><code>h.opt</code></td>
<td>
<p>Selected bandwidth.</p>
</td></tr>
<tr><td><code>lambda.opt</code></td>
<td>
<p>Selected value of the penalisation parameter <code class="reqn">\lambda</code>.</p>
</td></tr>
<tr><td><code>IC</code></td>
<td>
<p>Value of the criterion function considered to select <code>lambda.opt</code>, <code>h.opt</code> and <code>vn.opt</code>.</p>
</td></tr>
<tr><td><code>Q.opt</code></td>
<td>
<p>Minimum value of the penalized criterion used to estimate <code class="reqn">\beta_0</code> and <code class="reqn">\theta_0</code>. That is, the value obtained using <code>theta.est</code> and <code>beta.est</code>.</p>
</td></tr>
<tr><td><code>Q</code></td>
<td>
<p>Vector of dimension equal to the cardinal of <code class="reqn">\Theta_n</code>, containing the values of the penalized criterion for each functional index in <code class="reqn">\Theta_n</code>.</p>
</td></tr>
<tr><td><code>m.opt</code></td>
<td>
<p>Index of <code class="reqn">\hat{\theta}</code> in the set <code class="reqn">\Theta_n</code>.</p>
</td></tr>
<tr><td><code>lambda.min.opt.max.mopt</code></td>
<td>
<p>A grid of values in [<code>lambda.min.opt.max.mopt[1], lambda.min.opt.max.mopt[3]</code>] is considered to seek for the <code>lambda.opt</code> (<code>lambda.opt=lambda.min.opt.max.mopt[2]</code>).
</p>
</td></tr>
<tr><td><code>lambda.min.opt.max.m</code></td>
<td>
<p>A grid of values in [<code>lambda.min.opt.max.m[m,1], lambda.min.opt.max.m[m,3]</code>] is considered to seek for the optimal <code class="reqn">\lambda</code> (<code>lambda.min.opt.max.m[m,2]</code>)
used by the optimal <code class="reqn">\beta</code> for each <code class="reqn">\theta</code> in <code class="reqn">\Theta_n</code>.</p>
</td></tr>
<tr><td><code>h.min.opt.max.mopt</code></td>
<td>
<p><code>h.opt=h.min.opt.max.mopt[2]</code> (used by <code>theta.est</code> and <code>beta.est</code>) was seeked between <code>h.min.opt.max.mopt[1]</code> and <code>h.min.opt.max.mopt[3]</code>.</p>
</td></tr>
<tr><td><code>h.min.opt.max.m</code></td>
<td>
<p>For each <code class="reqn">\theta</code> in <code class="reqn">\Theta_n</code>, the optimal <code class="reqn">h</code> (<code>h.min.opt.max.m[m,2]</code>) used by the optimal <code class="reqn">\beta</code> for this <code class="reqn">\theta</code> was seeked between <code>h.min.opt.max.m[m,1]</code> and <code>h.min.opt.max.m[m,3]</code>.</p>
</td></tr>
<tr><td><code>h.seq.opt</code></td>
<td>
<p>Sequence of eligible values for <code class="reqn">h</code> considered to seek for <code>h.opt</code>.</p>
</td></tr>
<tr><td><code>theta.seq.norm</code></td>
<td>
<p>The vector <code>theta.seq.norm[j,]</code> contains the coefficientes in the B-spline basis of the jth functional index in <code class="reqn">\Theta_n</code>.</p>
</td></tr>
<tr><td><code>vn.opt</code></td>
<td>
<p>Selected value of <code>vn</code>.</p>
</td></tr>
<tr><td><code>...</code></td>
<td>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>German Aneiros Perez <a href="mailto:german.aneiros@udc.es">german.aneiros@udc.es</a> 
</p>
<p>Silvia Novo Diaz  <a href="mailto:snovo@est-econ.uc3m.es">snovo@est-econ.uc3m.es</a>
</p>


<h3>References</h3>

<p>Ait-Saidi, A., Ferraty, F., Kassa, R., and Vieu, P. (2008) Cross-validated estimations in the single-functional index model. <em>Statistics</em>, <b>42(6)</b>, 475&ndash;494, <a href="https://doi.org/10.1080/02331880801980377">doi:10.1080/02331880801980377</a>.
</p>
<p>Novo S., Aneiros, G., and Vieu, P., (2019) Automatic and location-adaptive estimation in functional single-index regression. <em>Journal of Nonparametric Statistics</em>, <b>31(2)</b>, 364&ndash;392, <a href="https://doi.org/10.1080/10485252.2019.1567726">doi:10.1080/10485252.2019.1567726</a>.
</p>
<p>Novo, S., Aneiros, G., and Vieu, P., (2021) Sparse semiparametric regression
when predictors are mixture of functional and high-dimensional variables. <em>TEST</em>,
<b>30</b>, 481&ndash;504, <a href="https://doi.org/10.1007/s11749-020-00728-w">doi:10.1007/s11749-020-00728-w</a>.
</p>
<p>Novo, S., Aneiros, G., and Vieu, P., (2021) A kNN procedure in semiparametric
functional data analysis. <em>Statistics and Probability Letters</em>, <b>171</b>, 109028, <a href="https://doi.org/10.1016/j.spl.2020.109028">doi:10.1016/j.spl.2020.109028</a>.
</p>


<h3>See Also</h3>

<p>See also <code><a href="#topic+fsim.kernel.fit">fsim.kernel.fit</a></code>, <code><a href="#topic+predict.sfplsim.kernel">predict.sfplsim.kernel</a></code> and  <code><a href="#topic+plot.sfplsim.kernel">plot.sfplsim.kernel</a></code>
</p>
<p>Alternative procedure <code><a href="#topic+sfplsim.kNN.fit">sfplsim.kNN.fit</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data("Tecator")
y&lt;-Tecator$fat
X&lt;-Tecator$absor.spectra2
z1&lt;-Tecator$protein       
z2&lt;-Tecator$moisture

#Quadratic, cubic and interaction effects of the scalar covariates.
z.com&lt;-cbind(z1,z2,z1^2,z2^2,z1^3,z2^3,z1*z2)
train&lt;-1:160

#SSFPLSIM fit. Convergence errors for some theta are obtained.
ptm=proc.time()
fit&lt;-sfplsim.kernel.fit(x=X[train,], z=z.com[train,], y=y[train],
      max.q.h=0.35,lambda.min.l=0.01,
      max.iter=5000, nknot.theta=4,criterion="BIC",nknot=20)
proc.time()-ptm

#Results
fit
names(fit)

</code></pre>

<hr>
<h2 id='sfplsim.kNN.fit'>
SFPLSIM regularised fit using kNN estimation
</h2><span id='topic+sfplsim.kNN.fit'></span>

<h3>Description</h3>

<p>This function fits a sparse semi-functional partial linear single-index (SFPLSIM). It employs a penalised least-squares regularisation procedure, integrated with nonparametric kNN estimation using Nadaraya-Watson weights.
</p>
<p>The function uses B-spline expansions to represent curves and eligible functional indexes.  It also utilises an objective criterion (<code>criterion</code>) to select both the number of neighbours (<code>k.opt</code>) and the regularisation parameter (<code>lambda.opt</code>).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sfplsim.kNN.fit(x, z, y, seed.coeff = c(-1, 0, 1), order.Bspline = 3, 
nknot.theta = 3, knearest = NULL, min.knn = 2, max.knn = NULL, step = NULL,
range.grid = NULL, kind.of.kernel = "quad", nknot = NULL, lambda.min = NULL,
lambda.min.h = NULL, lambda.min.l = NULL, factor.pn = 1, nlambda = 100, 
lambda.seq = NULL, vn = ncol(z), nfolds = 10, seed = 123, criterion = "GCV",
penalty = "grSCAD", max.iter = 1000, n.core = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="sfplsim.kNN.fit_+3A_x">x</code></td>
<td>

<p>Matrix containing the observations of the functional covariate (functional single-index component), collected by row.
</p>
</td></tr>
<tr><td><code id="sfplsim.kNN.fit_+3A_z">z</code></td>
<td>

<p>Matrix containing the observations of the scalar covariates (linear component), collected by row.
</p>
</td></tr>
<tr><td><code id="sfplsim.kNN.fit_+3A_y">y</code></td>
<td>

<p>Vector containing the scalar response.
</p>
</td></tr>
<tr><td><code id="sfplsim.kNN.fit_+3A_seed.coeff">seed.coeff</code></td>
<td>

<p>Vector of initial values used to  build the set <code class="reqn">\Theta_n</code> (see section <code>Details</code>). The coefficients for the B-spline representation of each eligible functional index <code class="reqn">\theta \in \Theta_n</code> are obtained from <code>seed.coeff</code>.  The default is <code>c(-1,0,1)</code>.
</p>
</td></tr>
<tr><td><code id="sfplsim.kNN.fit_+3A_order.bspline">order.Bspline</code></td>
<td>

<p>Positive integer giving the order of the B-spline basis functions. This is the number of coefficients in each piecewise polynomial segment. The default is 3.
</p>
</td></tr>
<tr><td><code id="sfplsim.kNN.fit_+3A_nknot.theta">nknot.theta</code></td>
<td>

<p>Positive integer indicating the number of regularly spaced interior knots in the B-spline expansion of <code class="reqn">\theta_0</code>. The default is 3.
</p>
</td></tr>
<tr><td><code id="sfplsim.kNN.fit_+3A_knearest">knearest</code></td>
<td>

<p>Vector of positive integers containing the sequence in which the  number of nearest neighbours <code>k.opt</code> is selected. If <code>knearest=NULL</code>, then <code>knearest &lt;- seq(from =min.knn, to = max.knn, by = step)</code>.
</p>
</td></tr>
<tr><td><code id="sfplsim.kNN.fit_+3A_min.knn">min.knn</code></td>
<td>

<p>A positive integer that represents the minimum value in the sequence for selecting the number of nearest neighbours <code>k.opt</code>. This value should be less than the sample size. The default is 2.
</p>
</td></tr>
<tr><td><code id="sfplsim.kNN.fit_+3A_max.knn">max.knn</code></td>
<td>

<p>A positive integer that represents the maximum value in the sequence for selecting number of nearest neighbours <code>k.opt</code>. This value should be less than the sample size. The default is <code>max.knn &lt;- n%/%5</code>.
</p>
</td></tr>
<tr><td><code id="sfplsim.kNN.fit_+3A_step">step</code></td>
<td>

<p>A positive integer used to construct the sequence of k-nearest neighbours as follows: <code>min.knn, min.knn + step, min.knn + 2*step, min.knn + 3*step,...</code>. The default value for <code>step</code> is <code>step&lt;-ceiling(n/100)</code>.
</p>
</td></tr>
<tr><td><code id="sfplsim.kNN.fit_+3A_range.grid">range.grid</code></td>
<td>

<p>Vector of length 2 containing the endpoints of the grid at which the observations of the functional covariate <code>x</code> are evaluated (i.e. the range of the discretisation). If <code>range.grid=NULL</code>, then <code>range.grid=c(1,p)</code> is considered, where <code>p</code> is the discretisation size of <code>x</code> (i.e. <code>ncol(x))</code>.
</p>
</td></tr>
<tr><td><code id="sfplsim.kNN.fit_+3A_kind.of.kernel">kind.of.kernel</code></td>
<td>

<p>The type of kernel function used. Currently, only Epanechnikov kernel (<code>"quad"</code>) is available.
</p>
</td></tr>
<tr><td><code id="sfplsim.kNN.fit_+3A_nknot">nknot</code></td>
<td>

<p>Positive integer indicating the number of interior knots for the B-spline expansion of the functional covariate. The default value is <code>(p - order.Bspline - 1)%/%2</code>.
</p>
</td></tr>
<tr><td><code id="sfplsim.kNN.fit_+3A_lambda.min">lambda.min</code></td>
<td>

<p>The smallest value for lambda (i. e., the lower endpoint  of the sequence in which <code>lambda.opt</code> is selected), as fraction of <code>lambda.max</code>.
The defaults is <code>lambda.min.l</code> if the sample size is larger than <code>factor.pn</code> times the number of linear covariates and <code>lambda.min.h</code> otherwise.
</p>
</td></tr>
<tr><td><code id="sfplsim.kNN.fit_+3A_lambda.min.h">lambda.min.h</code></td>
<td>

<p>The lower endpoint of the sequence in which <code>lambda.opt</code> is selected if the sample size is smaller than <code>factor.pn</code> times the number of linear covariates. The default is 0.05. 
</p>
</td></tr>
<tr><td><code id="sfplsim.kNN.fit_+3A_lambda.min.l">lambda.min.l</code></td>
<td>

<p>The lower endpoint of the sequence in which <code>lambda.opt</code> is selected if the sample size is larger than <code>factor.pn</code> times the number of linear covariates. The default is 0.0001.
</p>
</td></tr>
<tr><td><code id="sfplsim.kNN.fit_+3A_factor.pn">factor.pn</code></td>
<td>

<p>Positive integer used to set <code>lambda.min</code>. The default value is 1.
</p>
</td></tr>
<tr><td><code id="sfplsim.kNN.fit_+3A_nlambda">nlambda</code></td>
<td>

<p>Positive integer indicating the number of values in the sequence from which <code>lambda.opt</code> is selected. The default is 100.
</p>
</td></tr>
<tr><td><code id="sfplsim.kNN.fit_+3A_lambda.seq">lambda.seq</code></td>
<td>

<p>Sequence of values in which <code>lambda.opt</code> is selected. If <code>lambda.seq=NULL</code>, then the programme builds the sequence automatically using <code>lambda.min</code> and <code>nlambda</code>.
</p>
</td></tr>
<tr><td><code id="sfplsim.kNN.fit_+3A_vn">vn</code></td>
<td>

<p>Positive integer or vector of positive integers indicating the number of groups of consecutive variables to be penalised together. The default value is <code>vn=ncol(z)</code>, resulting in the individual penalization of each scalar covariate.
</p>
</td></tr>
<tr><td><code id="sfplsim.kNN.fit_+3A_nfolds">nfolds</code></td>
<td>

<p>Number of cross-validation folds (used when <code>criterion="k-fold-CV"</code>). Default is 10.
</p>
</td></tr>
<tr><td><code id="sfplsim.kNN.fit_+3A_seed">seed</code></td>
<td>

<p>You may set the seed for the random number generator to ensure reproducible results (applicable when <code>criterion="k-fold-CV"</code> is used). The default seed value is 123.
</p>
</td></tr>
<tr><td><code id="sfplsim.kNN.fit_+3A_criterion">criterion</code></td>
<td>

<p>The criterion used to select the tuning and regularisation parameter: <code>h.opt</code> and <code>lambda.opt</code>  (also <code>vn.opt</code> if needed). Options include <code>"GCV"</code>, <code>"BIC"</code>, <code>"AIC"</code>, or <code>"k-fold-CV"</code>. The default setting is <code>"GCV"</code>.
</p>
</td></tr>
<tr><td><code id="sfplsim.kNN.fit_+3A_penalty">penalty</code></td>
<td>

<p>The penalty function applied in the penalised least-squares procedure. Currently, only &quot;grLasso&quot; and &quot;grSCAD&quot; are implemented. The default is &quot;grSCAD&quot;.
</p>
</td></tr>
<tr><td><code id="sfplsim.kNN.fit_+3A_max.iter">max.iter</code></td>
<td>

<p>Maximum number of iterations allowed across the entire path. The default value is 1000.
</p>
</td></tr>
<tr><td><code id="sfplsim.kNN.fit_+3A_n.core">n.core</code></td>
<td>

<p>Number of CPU cores designated for parallel execution. The default is <code>n.core&lt;-availableCores(omit=1)</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The sparse semi-functional partial linear single-index model (SFPLSIM) is given by the expression:
</p>
<p style="text-align: center;"><code class="reqn">
Y_i=Z_{i1}\beta_{01}+\dots+Z_{ip_n}\beta_{0p_n}+r(\left&lt;\theta_0,X_i\right&gt;)+\varepsilon_i\ \ \ i=1,\dots,n,
</code>
</p>

<p>where <code class="reqn">Y_i</code> denotes a scalar response, <code class="reqn">Z_{i1},\dots,Z_{ip_n}</code> are real random covariates and <code class="reqn">X_i</code> is a functional random covariate valued in a separable Hilbert space <code class="reqn">\mathcal{H}</code> with inner product <code class="reqn">\left\langle \cdot, \cdot \right\rangle</code>. In this equation,
<code class="reqn">\mathbf{\beta}_0=(\beta_{01},\dots,\beta_{0p_n})^{\top}</code>, <code class="reqn">\theta_0\in\mathcal{H}</code> and <code class="reqn">r(\cdot)</code> are a vector of unknown real parameters, an unknown functional direction and an unknown smooth real-valued function, respectively. In addition, <code class="reqn">\varepsilon_i</code> is the random error.
</p>
<p>The sparse SFPLSIM is fitted using the penalised least-squares approach. The first step is to transform the SSFPLSIM into a linear model by extracting from <code class="reqn">Y_i</code> and <code class="reqn">Z_{ij}</code> (<code class="reqn">j=1,\ldots,p_n</code>) the effect of the functional covariate <code class="reqn">X_i</code> using functional single-index regression.  This transformation is achieved using nonparametric kNN estimation (see, for details, the documentation of the function <code>fsim.kNN.fit</code>).
</p>
<p>An approximate linear model is then obtained:
</p>
<p style="text-align: center;"><code class="reqn">\widetilde{\mathbf{Y}}_{\theta_0}\approx\widetilde{\mathbf{Z}}_{\theta_0}\mathbf{\beta}_0+\mathbf{\varepsilon},</code>
</p>

<p>and the penalised least-squares procedure is applied to this model by minimising over the pair <code class="reqn">(\mathbf{\beta},\theta)</code>
</p>
<p style="text-align: center;"><code class="reqn">
\mathcal{Q}\left(\mathbf{\beta},\theta\right)=\frac{1}{2}\left(\widetilde{\mathbf{Y}}_{\theta}-\widetilde{\mathbf{Z}}_{\theta}\mathbf{\beta}\right)^{\top}\left(\widetilde{\mathbf{Y}}_{\theta}-\widetilde{\mathbf{Z}}_{\theta}\mathbf{\beta}\right)+n\sum_{j=1}^{p_n}\mathcal{P}_{\lambda_{j_n}}\left(|\beta_j|\right), \quad (1)
</code>
</p>

<p>where <code class="reqn">\mathbf{\beta}=(\beta_1,\ldots,\beta_{p_n})^{\top}, \ \mathcal{P}_{\lambda_{j_n}}\left(\cdot\right)</code> is a penalty function (specified in the argument <code>penalty</code>) and <code class="reqn">\lambda_{j_n} &gt; 0</code> is a tuning parameter.
To reduce  the quantity of tuning parameters, <code class="reqn">\lambda_j</code>, to be selected for each sample, we consider <code class="reqn">\lambda_j = \lambda \widehat{\sigma}_{\beta_{0,j,OLS}}</code>, where <code class="reqn">\beta_{0,j,OLS}</code> denotes the OLS estimate of <code class="reqn">\beta_{0,j}</code> and <code class="reqn">\widehat{\sigma}_{\beta_{0,j,OLS}}</code> is the estimated standard deviation. Both <code class="reqn">\lambda</code> and <code class="reqn">k</code> (in the kNN estimation) are selected using the objetive criterion specified in the argument <code>criterion</code>.
</p>
<p>In addition, the function uses a B-spline representation to construct a set  <code class="reqn">\Theta_n</code> of eligible functional indexes <code class="reqn">\theta</code>. The dimension of the B-spline basis is <code>order.Bspline</code>+<code>nknot.theta</code> and the set of eligible coefficients is obtained by calibrating (to ensure the identifiability of the model) the set of initial coefficients given in <code>seed.coeff</code>. The larger this set, the greater the size of <code class="reqn">\Theta_n</code>. ue to the intensive computation required by our approach, a balance between the size of <code class="reqn">\Theta_n</code> and the performance of the estimator is necessary. For that, Ait-Saidi et al. (2008) suggested considering <code>order.Bspline=3</code> and <code>seed.coeff=c(-1,0,1)</code>. For details on the construction of <code class="reqn">\Theta_n</code> see Novo et al. (2019).
</p>
<p>Finally, after estimating <code class="reqn">\mathbf{\beta}_0</code> and <code class="reqn">\theta_0</code> by minimising (1), we proceed to estimate the nonlinear function <code class="reqn">r_{\theta_0}(\cdot)\equiv r\left(\left&lt;\theta_0,\cdot\right&gt;\right)</code>.
For this purporse, we again apply the kNN procedure with Nadaraya-Watson weights to smooth the partial residuals <code class="reqn">Y_i-\mathbf{Z}_i^{\top}\widehat{\mathbf{\beta}}</code>.
</p>
<p>For further details on the estimation procedure of the sparse SFPLSIM, see Novo et al. (2021).
</p>
<p><b>Remark</b>: It should be noted that if we set <code>lambda.seq</code> to <code class="reqn">0</code>, we can obtain the non-penalised estimation of the model, i.e. the OLS estimation. Using <code>lambda.seq</code> with a value <code class="reqn">\not= 0</code> is advisable when suspecting the presence of irrelevant variables.
</p>


<h3>Value</h3>

<table>
<tr><td><code>call</code></td>
<td>
<p>The matched call.</p>
</td></tr>
<tr><td><code>fitted.values</code></td>
<td>
<p>Estimated scalar response.</p>
</td></tr>
<tr><td><code>residuals</code></td>
<td>
<p>Differences between <code>y</code> and the <code>fitted.values</code>.</p>
</td></tr>
<tr><td><code>beta.est</code></td>
<td>
<p><code class="reqn">\hat{\mathbf{\beta}}</code> (i.e. the estimate of <code class="reqn">\mathbf{\beta}_0</code> when the optimal tuning parameters <code>lambda.opt</code>, <code>k.opt</code> and <code>vn.opt</code> are used).</p>
</td></tr>
<tr><td><code>theta.est</code></td>
<td>
<p>Coefficients of <code class="reqn">\hat{\theta}</code> in the B-spline basis (when the optimal tuning parameters <code>lambda.opt</code>, <code>k.opt</code> and <code>vn.opt</code>) are used): a vector of <code>length(order.Bspline+nknot.theta)</code>.</p>
</td></tr>
<tr><td><code>indexes.beta.nonnull</code></td>
<td>
<p>Indexes of the non-zero <code class="reqn">\hat{\beta_{j}}</code>.</p>
</td></tr>
<tr><td><code>k.opt</code></td>
<td>
<p>Selected number of nearest neighbours.</p>
</td></tr>
<tr><td><code>lambda.opt</code></td>
<td>
<p>Selected value of the penalisation parameter <code class="reqn">\lambda</code>.</p>
</td></tr>
<tr><td><code>IC</code></td>
<td>
<p>Value of the criterion function considered to select <code>lambda.opt</code>, <code>k.opt</code> and <code>vn.opt</code>.</p>
</td></tr>
<tr><td><code>Q.opt</code></td>
<td>
<p>Minimum value of the penalized criterion used to estimate <code class="reqn">\mathbf{\beta}_0</code> and <code class="reqn">\theta_0</code>. That is, the value obtained using <code>theta.est</code> and <code>beta.est</code>.</p>
</td></tr>
<tr><td><code>Q</code></td>
<td>
<p>Vector of dimension equal to the cardinal of <code class="reqn">\Theta_n</code>, containing the values of the penalized criterion for each functional index in <code class="reqn">\Theta_n</code>.</p>
</td></tr>
<tr><td><code>m.opt</code></td>
<td>
<p>Index of <code class="reqn">\hat{\theta}</code> in the set <code class="reqn">\Theta_n</code>.</p>
</td></tr>
<tr><td><code>lambda.min.opt.max.mopt</code></td>
<td>
<p>A grid of values in [<code>lambda.min.opt.max.mopt[1], lambda.min.opt.max.mopt[3]</code>] is considered to seek for the <code>lambda.opt</code> (<code>lambda.opt=lambda.min.opt.max.mopt[2]</code>).
</p>
</td></tr>
<tr><td><code>lambda.min.opt.max.m</code></td>
<td>
<p>A grid of values in [<code>lambda.min.opt.max.m[m,1], lambda.min.opt.max.m[m,3]</code>] is considered to seek for the optimal <code class="reqn">\lambda</code> (<code>lambda.min.opt.max.m[m,2]</code>)
used by the optimal <code class="reqn">\mathbf{\beta}</code> for each <code class="reqn">\theta</code> in <code class="reqn">\Theta_n</code>.</p>
</td></tr>
<tr><td><code>knn.min.opt.max.mopt</code></td>
<td>
<p><code>k.opt=knn.min.opt.max.mopt[2]</code> (used by <code>theta.est</code> and <code>beta.est</code>) was seeked between <code>knn.min.opt.max.mopt[1]</code> and <code>knn.min.opt.max.mopt[3]</code> (no necessarly the step was 1).</p>
</td></tr>
<tr><td><code>knn.min.opt.max.m</code></td>
<td>
<p>For each <code class="reqn">\theta</code> in <code class="reqn">\Theta_n</code>, the optimal <code class="reqn">k</code> (<code>knn.min.opt.max.m[m,2]</code>) used by the optimal <code class="reqn">\beta</code> for this <code class="reqn">\theta</code> was seeked between <code>knn.min.opt.max.m[m,1]</code> and <code>knn.min.opt.max.m[m,3]</code> (no necessarly the step was 1).</p>
</td></tr>
<tr><td><code>knearest</code></td>
<td>
<p>Sequence of eligible values for <code class="reqn">k</code> considered to seek for <code>k.opt</code>.</p>
</td></tr>
<tr><td><code>theta.seq.norm</code></td>
<td>
<p>The vector <code>theta.seq.norm[j,]</code> contains the coefficientes in the B-spline basis of the jth functional index in <code class="reqn">\Theta_n</code>.</p>
</td></tr>
<tr><td><code>vn.opt</code></td>
<td>
<p>Selected value of <code>vn</code>.</p>
</td></tr>
<tr><td><code>...</code></td>
<td>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>German Aneiros Perez <a href="mailto:german.aneiros@udc.es">german.aneiros@udc.es</a> 
</p>
<p>Silvia Novo Diaz  <a href="mailto:snovo@est-econ.uc3m.es">snovo@est-econ.uc3m.es</a>
</p>


<h3>References</h3>

<p>Ait-Saidi, A., Ferraty, F., Kassa, R., and Vieu, P., (2008) Cross-validated estimations in the single-functional index model. <em>Statistics</em>, <b>42(6)</b>, 475&ndash;494, <a href="https://doi.org/10.1080/02331880801980377">doi:10.1080/02331880801980377</a>.
</p>
<p>Novo S., Aneiros, G., and Vieu, P., (2019) Automatic and location-adaptive estimation in functional single-index regression. <em>Journal of Nonparametric Statistics</em>, <b>31(2)</b>, 364&ndash;392, <a href="https://doi.org/10.1080/10485252.2019.1567726">doi:10.1080/10485252.2019.1567726</a>.
</p>
<p>Novo, S., Aneiros, G., and Vieu, P., (2021) Sparse semiparametric regression
when predictors are mixture of functional and high-dimensional variables. <em>TEST</em>,
<b>30</b>, 481&ndash;504, <a href="https://doi.org/10.1007/s11749-020-00728-w">doi:10.1007/s11749-020-00728-w</a>.
</p>
<p>Novo, S., Aneiros, G., and Vieu, P., (2021) A kNN procedure in semiparametric
functional data analysis. <em>Statistics and Probability Letters</em>, <b>171</b>, 109028, <a href="https://doi.org/10.1016/j.spl.2020.109028">doi:10.1016/j.spl.2020.109028</a>
</p>


<h3>See Also</h3>

<p>See also <code><a href="#topic+fsim.kNN.fit">fsim.kNN.fit</a></code>, <code><a href="#topic+predict.sfplsim.kNN">predict.sfplsim.kNN</a></code> and  <code><a href="#topic+plot.sfplsim.kNN">plot.sfplsim.kNN</a></code>
</p>
<p>Alternative procedure <code><a href="#topic+sfplsim.kernel.fit">sfplsim.kernel.fit</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data("Tecator")
y&lt;-Tecator$fat
X&lt;-Tecator$absor.spectra2
z1&lt;-Tecator$protein       
z2&lt;-Tecator$moisture

#Quadratic, cubic and interaction effects of the scalar covariates.
z.com&lt;-cbind(z1,z2,z1^2,z2^2,z1^3,z2^3,z1*z2)
train&lt;-1:160

#SSFPLSIM fit. Convergence errors for some theta are obtained.
ptm=proc.time()
fit&lt;-sfplsim.kNN.fit(y=y[train],x=X[train,], z=z.com[train,], max.knn=20,
    lambda.min.l=0.01, factor.pn=2,  nknot.theta=4,
    criterion="BIC",range.grid=c(850,1050), 
    nknot=20, max.iter=5000)
proc.time()-ptm

#Results
fit
names(fit)

</code></pre>

<hr>
<h2 id='Sugar'>
Sugar data
</h2><span id='topic+Sugar'></span>

<h3>Description</h3>

<p>Ash content and absorbance spectra at two different excitation wavelengths of 268 sugar samples. Detailed information about this dataset can be found at <a href="https://ucphchemometrics.com/datasets/">https://ucphchemometrics.com/datasets/</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(Sugar)</code></pre>


<h3>Format</h3>

<p>A <code>list</code> containing:
</p>

<ul>
<li><p><code>ash</code>: A vector with the ash content.
</p>
</li>
<li><p><code>wave.290</code>: A matrix containing the absorbance spectra observed at 571 equally spaced wavelengths in the range of 275-560nm, at an excitation wavelengths of 290nm.
</p>
</li>
<li><p><code>wave.240</code>: A matrix containing the absorbance spectra observed at 571 equally spaced wavelengths in the range of 275-560nm, at an excitation wavelengths of 240nm.
</p>
</li></ul>



<h3>References</h3>

<p>Aneiros, G., and Vieu, P. (2015) Partial linear modelling with multi-functional covariates. <em>Computational Statistics</em>, <b>30</b>, 647&ndash;671, <a href="https://doi.org/10.1007/s00180-015-0568-8">doi:10.1007/s00180-015-0568-8</a>.
</p>
<p>Novo, S., Vieu, P., and Aneiros, G., (2021) Fast and efficient algorithms for
sparse semiparametric bi-functional regression. <em>Australian and New Zealand
Journal of Statistics</em>, <b>63</b>, 606&ndash;638, <a href="https://doi.org/10.1111/anzs.12355">doi:10.1111/anzs.12355</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Sugar)
names(Sugar)
Sugar$ash
dim(Sugar$wave.290)
dim(Sugar$wave.240)
</code></pre>

<hr>
<h2 id='Tecator'>
Tecator data
</h2><span id='topic+Tecator'></span>

<h3>Description</h3>

<p>Fat, protein, and moisture content, along with absorbance spectra (including the first and second derivatives), of 215 meat samples. 
A detailed description of the data can be found at <a href="http://lib.stat.cmu.edu/datasets/tecator">http://lib.stat.cmu.edu/datasets/tecator</a>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(Tecator)</code></pre>


<h3>Format</h3>

<p>A <code>list</code> containing:
</p>

<ul>
<li><p><code>fat</code>: A vector with the fat content.
</p>
</li>
<li><p><code>protein</code>: A vector with the protein content.
</p>
</li>
<li><p><code>moisture</code>: A vector with the moisture content.
</p>
</li>
<li><p><code>absor.spectra</code>: A matrix containing the near-infrared absorbance spectra observed at 100 equally spaced wavelengths in the range of 850-1050nm.
</p>
</li>
<li><p><code>absor.spectra1</code>: Fist derivative of the absorbance spectra (computed using B-spline representation of the curves).
</p>
</li>
<li><p><code>absor.spectra2</code>: Second derivative of the absorbance spectra (computed using B-spline representation of the curves).
</p>
</li></ul>



<h3>References</h3>

<p>Ferraty, F. and Vieu, P. (2006) <em>Nonparametric functional data analysis</em>, Springer Series in Statistics, New York.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(Tecator)
names(Tecator)
Tecator$fat
Tecator$protein
Tecator$moisture
dim(Tecator$absor.spectra)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
