<!DOCTYPE html><html lang="en"><head><title>Help for package NLPclient</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {NLPclient}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#nlp_server_docker_run'><p>Create the Docker Run Command</p></a></li>
<li><a href='#ping_nlp_client'><p>Ping Stanford CoreNLP Server</p></a></li>
<li><a href='#StanfordCoreNLP_Pipeline'><p>Stanford <code>CoreNLP</code> annotator pipeline</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Version:</td>
<td>1.0</td>
</tr>
<tr>
<td>Title:</td>
<td>Stanford 'CoreNLP' Annotation Client</td>
</tr>
<tr>
<td>Description:</td>
<td>Stanford 'CoreNLP' annotation client.
  Stanford 'CoreNLP' <a href="https://stanfordnlp.github.io/CoreNLP/index.html">https://stanfordnlp.github.io/CoreNLP/index.html</a> integrates all 
  NLP tools from the Stanford Natural Language Processing Group, 
  including a part-of-speech (POS) tagger, a named entity recognizer (NER), 
  a parser, and a coreference resolution system, and provides model files 
  for the analysis of English. More information can be found in the README.</td>
</tr>
<tr>
<td>Imports:</td>
<td>NLP (&ge; 0.1-10), utils, xml2, curl</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.2.0)</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a></td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>6.1.1</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2019-12-04 21:29:21 UTC; florian</td>
</tr>
<tr>
<td>Author:</td>
<td>Florian Schwendinger [aut, cre],
  Kurt Hornik [aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Florian Schwendinger &lt;FlorianSchwendinger@gmx.at&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2019-12-13 14:20:15 UTC</td>
</tr>
</table>
<hr>
<h2 id='nlp_server_docker_run'>Create the Docker Run Command</h2><span id='topic+nlp_server_docker_run'></span>

<h3>Description</h3>

<p>This function helps to create the docker run command 
which can be copied into the terminal (Unix) or docker toolbox (Windows).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nlp_server_docker_run(name = "coreNLP", docker = "schwe/corenlp",
  memory = "-mx8g", threads = NA, port = 9000L,
  status_port = 9000L, timeout = 15000L, strict = FALSE,
  quiet = FALSE, ssl = FALSE,
  key = "edu/stanford/nlp/pipeline/corenlp.jks", username = "",
  password = "", annotators = "all", preload = "",
  server_properties = "", default_properties = "", cleanup = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="nlp_server_docker_run_+3A_name">name</code></td>
<td>
<p>a character string giving the name of the docker container.</p>
</td></tr>
<tr><td><code id="nlp_server_docker_run_+3A_docker">docker</code></td>
<td>
<p>a character string giving the image name.</p>
</td></tr>
<tr><td><code id="nlp_server_docker_run_+3A_memory">memory</code></td>
<td>
<p>a character string giving the java memory options.</p>
</td></tr>
<tr><td><code id="nlp_server_docker_run_+3A_threads">threads</code></td>
<td>
<p>an integer giving the number of threads to be used. The default is <code>NA</code> in which case <code>CoreNLP</code> will choose the number of threads to be used.</p>
</td></tr>
<tr><td><code id="nlp_server_docker_run_+3A_port">port</code></td>
<td>
<p>an integer giving the server port.</p>
</td></tr>
<tr><td><code id="nlp_server_docker_run_+3A_status_port">status_port</code></td>
<td>
<p>an integer giving the port to run the liveness and readiness server on.</p>
</td></tr>
<tr><td><code id="nlp_server_docker_run_+3A_timeout">timeout</code></td>
<td>
<p>an integer giving the maximum amount of time, in milliseconds, to wait for an annotation to finish before cancelling it.</p>
</td></tr>
<tr><td><code id="nlp_server_docker_run_+3A_strict">strict</code></td>
<td>
<p>a logical controlling whether server strictly follows the <code>HTTP</code> standards.</p>
</td></tr>
<tr><td><code id="nlp_server_docker_run_+3A_quiet">quiet</code></td>
<td>
<p>a logical controlling whether the incoming requests are logged to <code>STDOUT</code>.</p>
</td></tr>
<tr><td><code id="nlp_server_docker_run_+3A_ssl">ssl</code></td>
<td>
<p>a logical controlling whether an <code>SSL</code> should be run.</p>
</td></tr>
<tr><td><code id="nlp_server_docker_run_+3A_key">key</code></td>
<td>
<p>a character string giving the classpath or filepath to the <code>*.jks</code> key to use for creating an SSL connection.</p>
</td></tr>
<tr><td><code id="nlp_server_docker_run_+3A_username">username</code></td>
<td>
<p>a character string giving the username of the server.</p>
</td></tr>
<tr><td><code id="nlp_server_docker_run_+3A_password">password</code></td>
<td>
<p>a character string giving the password of the server.</p>
</td></tr>
<tr><td><code id="nlp_server_docker_run_+3A_annotators">annotators</code></td>
<td>
<p>a character string giving the default annotators (e.g. <code>"tokenize,ssplit,pos"</code>).</p>
</td></tr>
<tr><td><code id="nlp_server_docker_run_+3A_preload">preload</code></td>
<td>
<p>a character string giving the set of annotators to warm up in the cache when the server boots up.</p>
</td></tr>
<tr><td><code id="nlp_server_docker_run_+3A_server_properties">server_properties</code></td>
<td>
<p>a character giving the path to the default properties.</p>
</td></tr>
<tr><td><code id="nlp_server_docker_run_+3A_default_properties">default_properties</code></td>
<td>
<p>a character string giving the server properties, to be written into the file <code>"default.properties"</code>.</p>
</td></tr>
<tr><td><code id="nlp_server_docker_run_+3A_cleanup">cleanup</code></td>
<td>
<p>a logical giving if docker should automatically clean up the container and remove the file system when the container exits.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A character string which can be copied into the terminal (or docker toolbox) to start the <code>coreNLP</code> server.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>nlp_server_docker_run(memory = "-mx6g")
</code></pre>

<hr>
<h2 id='ping_nlp_client'>Ping Stanford CoreNLP Server</h2><span id='topic+ping_nlp_client'></span>

<h3>Description</h3>

<p>Ping Stanford CoreNLP server.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ping_nlp_client(port = 9000L, host = "localhost",
  user_agent = "NLPclient")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ping_nlp_client_+3A_port">port</code></td>
<td>
<p>an integer giving the port (default is <code>9000L</code>).</p>
</td></tr>
<tr><td><code id="ping_nlp_client_+3A_host">host</code></td>
<td>
<p>a character string giving the hostname of the server.</p>
</td></tr>
<tr><td><code id="ping_nlp_client_+3A_user_agent">user_agent</code></td>
<td>
<p>a character string giving the client name.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns <code>"pong"</code> if the server is online <code>""</code> otherwise.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>ping_nlp_client()
</code></pre>

<hr>
<h2 id='StanfordCoreNLP_Pipeline'>Stanford <code>CoreNLP</code> annotator pipeline</h2><span id='topic+StanfordCoreNLP_Pipeline'></span>

<h3>Description</h3>

<p>Create a Stanford <code>CoreNLP</code> annotator pipeline.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>StanfordCoreNLP_Pipeline(annotators = c("pos", "lemma"),
  language = "en", control = list(), port = 9000L,
  host = "localhost")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="StanfordCoreNLP_Pipeline_+3A_annotators">annotators</code></td>
<td>
<p>a character string specifying the annotators to be used in addition
to &lsquo;ssplit&rsquo; (sentence token annotation) and &lsquo;tokenize&rsquo;
(word token annotations), with elements
<code>"pos"</code> (POS tagging),
<code>"lemma"</code> (lemmatizing),
<code>"ner"</code> (named entity recognition),
<code>"regexner"</code> (rule-based named entity recognition over token
sequences using Java regular expressions),
<code>"parse"</code> (constituency parsing),
<code>"depparse"</code> (dependency parsing),
<code>"sentiment"</code> (sentiment analysis),
<code>"coref"</code> (coference resolution),
<code>"dcoref"</code> (deterministic coference resolution),
<code>"cleanxml"</code> (clean XML tags),
or
<code>"relation"</code> (relation extraction),
or unique abbreviations thereof.
Ignored for languages other than English.</p>
</td></tr>
<tr><td><code id="StanfordCoreNLP_Pipeline_+3A_language">language</code></td>
<td>
<p>a character string giving the ISO-639 code of the
language being processed by the annotator pipeline.</p>
</td></tr>
<tr><td><code id="StanfordCoreNLP_Pipeline_+3A_control">control</code></td>
<td>
<p>a named or empty (default) list vector with annotator control
options, with the names giving the option names.  See
<a href="https://stanfordnlp.github.io/CoreNLP/annotators.html">https://stanfordnlp.github.io/CoreNLP/annotators.html</a> for
available control options.</p>
</td></tr>
<tr><td><code id="StanfordCoreNLP_Pipeline_+3A_port">port</code></td>
<td>
<p>an integer giving the port (default is <code>9000L</code>).</p>
</td></tr>
<tr><td><code id="StanfordCoreNLP_Pipeline_+3A_host">host</code></td>
<td>
<p>a character string giving the hostname of the server.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An <code><a href="NLP.html#topic+Annotator">Annotator</a></code> object providing the annotator pipeline.
</p>


<h3>Note</h3>

<p>See <a href="https://stanfordnlp.github.io/CoreNLP/#citing-stanford-corenlp-in-papers">https://stanfordnlp.github.io/CoreNLP/#citing-stanford-corenlp-in-papers</a>
for information on citing Stanford CoreNLP.
</p>
<p>Using the parse annotator requires considerable amounts of (Java)
memory.  The Stanford CoreNLP documentation suggests starting the JVM
with at least 3GB of memory on 64-bit systems (and in fact, not using
32-bit systems), and hence have the JVM started with <span class="option">-Xmx3g</span>
unless option <code>java.parameters</code> is set to something non-empty
(hence, this option should be set appropriately to accommodate
different memory requirements or constraints).
</p>
<p>Using the coreference annotators nowadays requires even more (Java)
memory.  The Stanford CoreNLP documentation suggests starting the JVM
with at least 5GB of memory; we find 4GB sufficient.  Hence, to use
these annotators one needs to set option <code>java.parameters</code> as
appropriate before starting the JVM.
</p>


<h3>See Also</h3>

<p><a href="https://stanfordnlp.github.io/CoreNLP/">https://stanfordnlp.github.io/CoreNLP/</a> for more 
information about the Stanford CoreNLP tools.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require("NLP")
s &lt;- as.String(paste("Stanford University is located in California.",
                     "It is a great university."))
s

## Annotators: ssplit, tokenize:
if ( ping_nlp_client() == "pong" ) {
p &lt;- StanfordCoreNLP_Pipeline(NULL)
a &lt;- p(s)
a

## Annotators: ssplit, tokenize, pos, lemma (default):
p &lt;- StanfordCoreNLP_Pipeline()
a &lt;- p(s)
a

## Equivalently:
annotate(s, p)

## Annotators: ssplit, tokenize, parse:
p &lt;- StanfordCoreNLP_Pipeline("parse")
a &lt;- p(s)
a

## Respective formatted parse trees using Penn Treebank notation
## (see &lt;https://catalog.ldc.upenn.edu/docs/LDC95T7/cl93.html&gt;):
ptexts &lt;- sapply(subset(a, type == "sentence")$features, `[[`, "parse")
ptexts

## Read into NLP Tree objects.
ptrees &lt;- lapply(ptexts, Tree_parse)
ptrees

## Basic dependencies:
depends &lt;- lapply(subset(a, type == "sentence")$features, `[[`,
                  "basic-dependencies")
depends
## Note that the non-zero ids (gid for governor and did for dependent)
## refer to word token positions within the respective sentences, and
## not the ids of these token in the annotation: these can easily be
## matched using the sentence constituents features:
lapply(subset(a, type == "sentence")$features, `[[`, "constituents")

## (Similarly for sentence ids used in dcoref document features.)

## Note also that the dependencies are returned as a data frame 
## inheriting from class "Stanford_typed_dependencies" which has print
## and format methods for obtaining the usual formatting.
depends[[1L]]
## Use as.data.frame() to obtain strip this class:
as.data.frame(depends[[1L]])
}
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
