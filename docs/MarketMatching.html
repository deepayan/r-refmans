<!DOCTYPE html><html><head><title>Help for package MarketMatching</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {MarketMatching}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#MarketMatching'><p>Market Matching and Causal Impact Inference</p></a></li>
<li><a href='#best_matches'><p>For each market, find the best matching control market</p></a></li>
<li><a href='#inference'><p>Given a test market, analyze the impact of an intervention</p></a></li>
<li><a href='#roll_up_optimal_pairs'><p>Roll up the suggested test/control optimal pairs for pseudo power analysis (testing fake lift)</p></a></li>
<li><a href='#test_fake_lift'><p>Given a test market, analyze the impact of fake interventions (prospective power analysis)</p></a></li>
<li><a href='#weather'><p>Weather dataset</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Market Matching and Causal Impact Inference</td>
</tr>
<tr>
<td>Version:</td>
<td>1.2.1</td>
</tr>
<tr>
<td>Date:</td>
<td>2024-01-11</td>
</tr>
<tr>
<td>Description:</td>
<td>For a given test market find the best control markets using time series matching and analyze the impact of an intervention. The intervention could be a marketing event or some other local business tactic that is being tested. The workflow implemented in the Market Matching package utilizes dynamic time warping (the 'dtw' package) to do the matching and the 'CausalImpact' package to analyze the causal impact. In fact, this package can be considered a "workflow wrapper" for those two packages. In addition, if you don't have a chosen set of test markets to match, the Market Matching package can provide suggested test/control market pairs and pseudo prospective power analysis (measuring causal impact at fake interventions). </td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 4.3.0)</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Imports:</td>
<td>ggplot2, dplyr, utils, iterators, doParallel, parallel,
foreach, reshape2, CausalImpact, tidyr, zoo, bsts, scales,
Boom, utf8, dtw</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Suggests:</td>
<td>knitr, rmarkdown</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-01-12 01:26:41 UTC; kim.larsen</td>
</tr>
<tr>
<td>Author:</td>
<td>Larsen Kim [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Larsen Kim &lt;kblarsen4@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-01-31 09:40:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='MarketMatching'>Market Matching and Causal Impact Inference</h2><span id='topic+MarketMatching'></span><span id='topic+MarketMatching-package'></span>

<h3>Description</h3>

<p>For a given test market find the best matching control markets using time series matching and analyze the impact of an intervention (prospective or historical).
The intervention could be be a marketing event or some other local business tactic that is being tested. 
The package utilizes dynamic time warping to do the matching and the CausalImpact package to analyze the causal impact. 
In fact, MarketMatching is simply a wrapper and workflow for those two packages. 
MarketMatching does not provide any functionality that cannot be found in these packages 
but simplifies the workflow of using dtw and CausalImpact together. In addition, if you don't already have a set of test markets to match, 'MarketMatching' 
can provide suggested test/control market pairs using the 'suggest_market_splits' option 
in the &lsquo;best_matches()' function. Also, the 'test_fake_lift()' function provides pseudo prospective power analysis if you&rsquo;re using the 'MarketMatching' 
package to create your test design (i.e., not just doing the post inference).
</p>


<h3>Details</h3>

<p>The MarketMatching package can be used to perform the following analyses:
</p>
<p>- For all markets in the input dataset, find the best control markets using time series matching.
</p>
<p>- Given a test market and a matching control market (from above), analyze the causal impact of an intervention.
</p>
<p>- Create optimal test/control market splits and run pseudo prospective power analyses.
</p>
<p>The package utilizes the dtw package in CRAN to do the time series matching, and the CausalImpact package to do the inference. 
(Created by Kay Brodersen at Google). For more information about the CausualImpact package, see the following reference:  
</p>
<p>CausalImpact version 1.0.3, Brodersen et al., Annals of Applied Statistics (2015). http://google.github.io/CausalImpact/
</p>
<p>The MarketMatching has two separate functions to perform the tasks described above:
</p>
<p>- best_matches(): This function finds the best matching control markets for all markets in the input dataset. If you don't know the test markets
the function can also provide suggested optimized test/control pairs.
</p>
<p>- inference(): Given an object from best_matches(), this function analyzes the causal impact of an intervention.
</p>
<p>- test_fake_lift(): Calculate the probability of a causal impact for fake interventions (prospective pseudo power).
</p>
<p>For more details, check out the vignette: browseVignettes(&quot;MarketMatching&quot;)
</p>


<h3>Author(s)</h3>

<p>Kim Larsen (kblarsen4 at gmail.com)
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 

##-----------------------------------------------------------------------
## Find best matches for CPH
## If we leave test_market as NULL, best matches are found for all markets
##-----------------------------------------------------------------------
library(MarketMatching)
data(weather, package="MarketMatching")
mm &lt;- MarketMatching::best_matches(data=weather, 
                   id="Area",
                   date_variable="Date",
                   matching_variable="Mean_TemperatureF",
                   parallel=FALSE,
                   markets_to_be_matched="CPH",
                   warping_limit=1, # warping limit=1
                   dtw_emphasis=1, # rely only on dtw for pre-screening
                   matches=5, # request 5 matches
                   start_match_period="2014-01-01",
                   end_match_period="2014-10-01")
head(mm$Distances)

##-----------------------------------------------------------------------
## Analyze causal impact of a made-up weather intervention in Copenhagen
## Since this is weather data it is a not a very meaningful example. 
## This is merely to demonstrate the functionality.
##-----------------------------------------------------------------------
results &lt;- MarketMatching::inference(matched_markets = mm, 
                                     test_market = "CPH", 
                                     analyze_betas=FALSE,
                                     end_post_period = "2015-10-01", 
                                     prior_level_sd = 0.002)

## Plot the impact
results$PlotCumulativeEffect

## Plot actual observations for test market (CPH) versus the expectation (based on the control)
results$PlotActualVersusExpected

##-----------------------------------------------------------------------
## Power analysis for a fake intervention ending at 2015-10-01
## The maximum lift analyzed is 5 percent, the minimum is 0 (using 5 steps)
## Since this is weather data it is a not a very meaningful example. 
## This is merely to demonstrate the functionality.
##-----------------------------------------------------------------------
power &lt;- MarketMatching::test_fake_lift(matched_markets = mm, 
                                     test_market = "CPH", 
                                     end_fake_post_period = "2015-10-01", 
                                     prior_level_sd = 0.002, 
                                     steps=20,
                                     max_fake_lift=0.05)

## Plot the curve
power$ResultsGraph

##-----------------------------------------------------------------------
## Generate suggested test/control pairs
##-----------------------------------------------------------------------

data(weather, package="MarketMatching")
mm &lt;- MarketMatching::best_matches(data=weather,
                                  id_variable="Area",
                                  date_variable="Date",
                                  matching_variable="Mean_TemperatureF",
                                  suggest_market_splits=TRUE,
                                  parallel=FALSE,
                                  dtw_emphasis=1, # rely only on correlation for this analysis
                                  start_match_period="2014-01-01",
                                  end_match_period="2014-10-01")

##-----------------------------------------------------------------------
## The file that contains the suggested test/control splits
## The file is sorted from the strongest market pair to the weakest pair.
##-----------------------------------------------------------------------
head(mm$SuggestedTestControlSplits)

##-----------------------------------------------------------------------
## Pass the results to test_fake_lift to get pseudo power curves for the splits.
## This tells us how well the design can detect various lifts.
## Not a meaningful example for this data. Just to illustrate.
## Note that the rollup() function will aggregate the test and control markets. 
## The new aggregated test markets will be labeled "TEST."
##-----------------------------------------------------------------------
rollup &lt;- MarketMatching::roll_up_optimal_pairs(matched_markets = mm, 
                                               synthetic=FALSE)

power &lt;- MarketMatching::test_fake_lift(matched_markets = rollup, 
                                       test_market = "TEST",
                                       end_fake_post_period = "2015-10-01",
                                       lift_pattern_type = "constant",
                                       max_fake_lift = 0.1)

## End(Not run)
</code></pre>

<hr>
<h2 id='best_matches'>For each market, find the best matching control market</h2><span id='topic+best_matches'></span>

<h3>Description</h3>

<p><code>best_matches</code> finds the best matching control markets for each market in the dataset
using dynamic time warping (<code>dtw</code> package). The algorithm simply loops through all viable candidates for each
market in a parallel fashion, and then ranks by distance and/or correlation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>best_matches(data=NULL,
             markets_to_be_matched=NULL,
             id_variable=NULL,
             date_variable=NULL,
             matching_variable=NULL,
             parallel=TRUE,
             warping_limit=1,
             start_match_period=NULL,
             end_match_period=NULL,
             matches=NULL,
             dtw_emphasis=1, 
             suggest_market_splits=FALSE,
             splitbins=10,
             log_for_splitting=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="best_matches_+3A_data">data</code></td>
<td>
<p>input data.frame for analysis. The dataset should be structured as &quot;stacked&quot; time series (i.e., a panel dataset).
In other words, markets are rows and not columns &ndash; we have a unique row for each area/time combination.</p>
</td></tr>
<tr><td><code id="best_matches_+3A_markets_to_be_matched">markets_to_be_matched</code></td>
<td>
<p>Use this parameter if you only want to get control matches for a subset of markets or a single market
The default is NULL which means that all markets will be paired with matching markets</p>
</td></tr>
<tr><td><code id="best_matches_+3A_id_variable">id_variable</code></td>
<td>
<p>the name of the variable that identifies the markets</p>
</td></tr>
<tr><td><code id="best_matches_+3A_date_variable">date_variable</code></td>
<td>
<p>the time stamp variable</p>
</td></tr>
<tr><td><code id="best_matches_+3A_matching_variable">matching_variable</code></td>
<td>
<p>the variable (metric) used to match the markets. For example, this could be sales or new customers</p>
</td></tr>
<tr><td><code id="best_matches_+3A_parallel">parallel</code></td>
<td>
<p>set to TRUE for parallel processing. Default is TRUE</p>
</td></tr>
<tr><td><code id="best_matches_+3A_warping_limit">warping_limit</code></td>
<td>
<p>the warping limit used for matching. Default is 1, 
which means that a single query value can be mapped to at most 2 reference values.</p>
</td></tr>
<tr><td><code id="best_matches_+3A_start_match_period">start_match_period</code></td>
<td>
<p>the start date of the matching period (pre period).
Must be a character of format &quot;YYYY-MM-DD&quot; &ndash; e.g., &quot;2015-01-01&quot;</p>
</td></tr>
<tr><td><code id="best_matches_+3A_end_match_period">end_match_period</code></td>
<td>
<p>the end date of the matching period (pre period).
Must be a character of format &quot;YYYY-MM-DD&quot; &ndash; e.g., &quot;2015-10-01&quot;</p>
</td></tr>
<tr><td><code id="best_matches_+3A_matches">matches</code></td>
<td>
<p>Number of matching markets to keep in the output (to use less markets for inference, use the control_matches parameter when calling inference). Default is to keep all matches.</p>
</td></tr>
<tr><td><code id="best_matches_+3A_dtw_emphasis">dtw_emphasis</code></td>
<td>
<p>Number from 0 to 1. The amount of emphasis placed on dtw distances, versus correlation, when ranking markets.
Default is 1 (all emphasis on dtw). If emphasis is set to 0, all emphasis would be put on correlation, which is recommended when optimal splits are requested.
An emphasis of 0.5 would yield equal weighting.</p>
</td></tr>
<tr><td><code id="best_matches_+3A_suggest_market_splits">suggest_market_splits</code></td>
<td>
<p>if set to TRUE, best_matches will return suggested test/control splits based on correlation and market sizes. Default is FALSE.
For this option to be invoked, markets_to_be_matched must be NULL (i.e., you must run a full match).
Note that the algorithm will force test and control to have the same number of markets. So if the total number of markets is odd, one market will be left out.</p>
</td></tr>
<tr><td><code id="best_matches_+3A_splitbins">splitbins</code></td>
<td>
<p>Number of size-based bins used to stratify when splitting markets into test and control.
Only markets inside the same bin can be matched. More bins means more emphasis on market size when splitting.
Less bins means more emphasis on correlation. Default is 10.</p>
</td></tr>
<tr><td><code id="best_matches_+3A_log_for_splitting">log_for_splitting</code></td>
<td>
<p>This parameter determines if optimal splitting is based on correlations of the raw 
matching metric values or the correlations of log(matching metric). Only relevant if suggest_market_splits is TRUE. Default is FALSE.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns an object of type <code>market_matching</code>. The object has the
following elements:
</p>
<table>
<tr><td><code>BestMatches</code></td>
<td>
<p>A data.frame that contains the best matches for each market. All stats reflect data after the market pairs have been joined by date. Thus SUMTEST and SUMCNTL can have smaller values than what you see in the Bins output table</p>
</td></tr>
<tr><td><code>Data</code></td>
<td>
<p>The raw data used to do the matching</p>
</td></tr>
<tr><td><code>MarketID</code></td>
<td>
<p>The name of the market identifier</p>
</td></tr>
<tr><td><code>MatchingMetric</code></td>
<td>
<p>The name of the matching variable</p>
</td></tr>
<tr><td><code>DateVariable</code></td>
<td>
<p>The name of the date variable</p>
</td></tr>
<tr><td><code>SuggestedTestControlSplits</code></td>
<td>
<p>Suggested test/control splits. SUMTEST and SUMCNTL are the total market volumes, not volume after joining with other markets. They're greater or equal to the values in the BestMatches file.</p>
</td></tr>
<tr><td><code>Bins</code></td>
<td>
<p>Bins used for splitting and corresponding volumes</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
##-----------------------------------------------------------------------
## Find the best matches for the CPH airport time series
##-----------------------------------------------------------------------
library(MarketMatching)
data(weather, package="MarketMatching")
mm &lt;- best_matches(data=weather, 
                   id="Area",
                   markets_to_be_matched=c("CPH", "SFO"),
                   date_variable="Date",
                   matching_variable="Mean_TemperatureF",
                   parallel=FALSE,
                   start_match_period="2014-01-01",
                   end_match_period="2014-10-01")
head(mm$BestMatches)

## End(Not run)

</code></pre>

<hr>
<h2 id='inference'>Given a test market, analyze the impact of an intervention</h2><span id='topic+inference'></span>

<h3>Description</h3>

<p><code>inference</code> Analyzes the causal impact of an intervention using the CausalImpact package, given a test market and a matched_market object from the best_matches function.
The function returns an object of type &quot;market_inference&quot; which contains the estimated impact of the intervention (absolute and relative).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>inference(matched_markets=NULL,
          bsts_modelargs=NULL,
          test_market=NULL,
          end_post_period=NULL,
          alpha=0.05,
          prior_level_sd=0.01,
          control_matches=5, 
          analyze_betas=FALSE, 
          nseasons=NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="inference_+3A_matched_markets">matched_markets</code></td>
<td>
<p>A matched_market object created by the market_matching function</p>
</td></tr>
<tr><td><code id="inference_+3A_bsts_modelargs">bsts_modelargs</code></td>
<td>
<p>A list() that passes model parameters directly to bsts &ndash; such as list(niter = 1000, nseasons = 52, prior.level.sd=0.1)
This parameter will overwrite the values specified in prior_level_sd and nseasons. ONLY use this if you're using intricate bsts settings
For most use-cases, using the prior_level_sd and nseasons parameters should be sufficient</p>
</td></tr>
<tr><td><code id="inference_+3A_test_market">test_market</code></td>
<td>
<p>The name of the test market (character)</p>
</td></tr>
<tr><td><code id="inference_+3A_end_post_period">end_post_period</code></td>
<td>
<p>The end date of the post period. Must be a character of format &quot;YYYY-MM-DD&quot; &ndash; e.g., &quot;2015-11-01&quot;</p>
</td></tr>
<tr><td><code id="inference_+3A_alpha">alpha</code></td>
<td>
<p>Desired tail-area probability for posterior intervals. For example, 0.05 yields 0.95 intervals</p>
</td></tr>
<tr><td><code id="inference_+3A_prior_level_sd">prior_level_sd</code></td>
<td>
<p>Prior SD for the local level term (Gaussian random walk). Default is 0.01. The bigger this number is, the more wiggliness is allowed for the local level term.
Note that more wiggly local level terms also translate into larger posterior intervals
This parameter will be overwritten if you're using the bsts_modelargs parameter</p>
</td></tr>
<tr><td><code id="inference_+3A_control_matches">control_matches</code></td>
<td>
<p>Number of matching control markets to use in the analysis (default is 5)</p>
</td></tr>
<tr><td><code id="inference_+3A_analyze_betas">analyze_betas</code></td>
<td>
<p>Controls whether to test the model under a variety of different values for prior_level_sd.</p>
</td></tr>
<tr><td><code id="inference_+3A_nseasons">nseasons</code></td>
<td>
<p>Seasonality for the bsts model &ndash; e.g., 52 for weekly seasonality</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns an object of type <code>inference</code>. The object has the
following elements:
</p>
<table>
<tr><td><code>AbsoluteEffect</code></td>
<td>
<p>The estimated absolute effect of the intervention</p>
</td></tr>
<tr><td><code>AbsoluteEffectLower</code></td>
<td>
<p>The lower limit of the estimated absolute effect of the intervention.
This is based on the posterior interval of the counterfactual predictions.
The width of the interval is determined by the <code>alpha</code> parameter.</p>
</td></tr>
<tr><td><code>AbsoluteEffectUpper</code></td>
<td>
<p>The upper limit of the estimated absolute effect of the intervention.
This is based on the posterior interval of the counterfactual predictions.
The width of the interval is determined by the <code>alpha</code> parameter.</p>
</td></tr>
<tr><td><code>RelativeEffectLower</code></td>
<td>
<p>Same as the above, just for relative (percentage) effects</p>
</td></tr>
<tr><td><code>RelativeEffectUpper</code></td>
<td>
<p>Same as the above, just for relative (percentage) effects</p>
</td></tr>
<tr><td><code>TailProb</code></td>
<td>
<p>Posterior probability of a non-zero effect</p>
</td></tr>
<tr><td><code>PrePeriodMAPE</code></td>
<td>
<p>Pre-intervention period MAPE</p>
</td></tr>
<tr><td><code>DW</code></td>
<td>
<p>Durbin-Watson statistic. Should be close to 2.</p>
</td></tr>
<tr><td><code>PlotActualVersusExpected</code></td>
<td>
<p>Plot of actual versus expected using <code>ggplot2</code></p>
</td></tr>
<tr><td><code>PlotCumulativeEffect</code></td>
<td>
<p>Plot of the cumulative effect using <code>ggplot2</code></p>
</td></tr>
<tr><td><code>PlotPointEffect</code></td>
<td>
<p>Plot of the pointwise effect using <code>ggplot2</code></p>
</td></tr>
<tr><td><code>PlotActuals</code></td>
<td>
<p>Plot of the actual values for the test and control markets using <code>ggplot2</code></p>
</td></tr>
<tr><td><code>PlotPriorLevelSdAnalysis</code></td>
<td>
<p>Plot of DW and MAPE for different values of the local level SE using <code>ggplot2</code></p>
</td></tr>
<tr><td><code>PlotLocalLevel</code></td>
<td>
<p>Plot of the local level term using <code>ggplot2</code></p>
</td></tr>
<tr><td><code>TestData</code></td>
<td>
<p>A <code>data.frame</code> with the test market data</p>
</td></tr>
<tr><td><code>ControlData</code></td>
<td>
<p>A <code>data.frame</code> with the data for the control markets</p>
</td></tr>
<tr><td><code>PlotResiduals</code></td>
<td>
<p>Plot of the residuals using <code>ggplot2</code></p>
</td></tr>
<tr><td><code>TestName</code></td>
<td>
<p>The name of the test market</p>
</td></tr>
<tr><td><code>TestName</code></td>
<td>
<p>The name of the control market</p>
</td></tr>
<tr><td><code>zooData</code></td>
<td>
<p>A <code>zoo</code> time series object with the test and control data</p>
</td></tr>
<tr><td><code>Predictions</code></td>
<td>
<p>Actual versus predicted values</p>
</td></tr>
<tr><td><code>CausalImpactObject</code></td>
<td>
<p>The CausalImpact object created</p>
</td></tr>
<tr><td><code>Coefficients</code></td>
<td>
<p>The average posterior coefficients</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library(MarketMatching)
##-----------------------------------------------------------------------
## Analyze causal impact of a made-up weather intervention in Copenhagen
## Since this is weather data it is a not a very meaningful example. 
## This is merely to demonstrate the function.
##-----------------------------------------------------------------------
data(weather, package="MarketMatching")
mm &lt;- best_matches(data=weather, 
                   id="Area",
                   markets_to_be_matched=c("CPH", "SFO"),
                   date_variable="Date",
                   matching_variable="Mean_TemperatureF",
                   parallel=FALSE,
                   warping_limit=1, # warping limit=1
                   dtw_emphasis=0, # rely only on dtw for pre-screening
                   matches=5, # request 5 matches
                   start_match_period="2014-01-01",
                   end_match_period="2014-10-01")
library(CausalImpact)
results &lt;- inference(matched_markets=mm,
                     test_market="CPH",
                     analyze_betas=FALSE,
                     control_matches=5, # use all 5 matches for inference
                     end_post_period="2015-12-15",
                     prior_level_sd=0.002)

## End(Not run)
</code></pre>

<hr>
<h2 id='roll_up_optimal_pairs'>Roll up the suggested test/control optimal pairs for pseudo power analysis (testing fake lift)</h2><span id='topic+roll_up_optimal_pairs'></span>

<h3>Description</h3>

<p><code>roll_up_optimal_pairs</code> Takes the suggested optimal pairs from best_matches() and aggregates the data for pseudo power analysis (test_fake_lift()).
You run this function and then pass the result (a matched markets object) to test_fake_lift.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>roll_up_optimal_pairs(matched_markets=NULL,
                      percent_cutoff=1,
                      synthetic=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="roll_up_optimal_pairs_+3A_matched_markets">matched_markets</code></td>
<td>
<p>A matched market object from best_matches.</p>
</td></tr>
<tr><td><code id="roll_up_optimal_pairs_+3A_percent_cutoff">percent_cutoff</code></td>
<td>
<p>The percent of data (by volume) to be included in the future study. Default is 1. 0.5 would be 50 percent.</p>
</td></tr>
<tr><td><code id="roll_up_optimal_pairs_+3A_synthetic">synthetic</code></td>
<td>
<p>If set to TRUE, the control markets are not aggregated so BSTS can determine weights for each market and create a synthetic control.
If set to FALSE then the control markets are aggregated and each market will essentially get the same weight.
If you have many control markets (say, more than 25) it is recommended to choose FALSE. Default is FALSE.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns an object of type <code>market_matching</code>. The object has the
following elements:
</p>
<table>
<tr><td><code>BestMatches</code></td>
<td>
<p>A data.frame that contains the best matches for each market in the input dataset</p>
</td></tr>
<tr><td><code>Data</code></td>
<td>
<p>The raw data used to do the matching</p>
</td></tr>
<tr><td><code>MarketID</code></td>
<td>
<p>The name of the market identifier</p>
</td></tr>
<tr><td><code>MatchingMetric</code></td>
<td>
<p>The name of the matching variable</p>
</td></tr>
<tr><td><code>DateVariable</code></td>
<td>
<p>The name of the date variable</p>
</td></tr>
<tr><td><code>SuggestedTestControlSplits</code></td>
<td>
<p>Always NULL</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
##-----------------------------------------------------------------------
## Generate the suggested test/control pairs
##-----------------------------------------------------------------------
library(MarketMatching)
data(weather, package="MarketMatching")
mm &lt;- best_matches(data=weather, 
                   id="Area",
                   date_variable="Date",
                   matching_variable="Mean_TemperatureF",
                   parallel=FALSE,
                   suggest_market_splits=TRUE,
                   start_match_period="2014-01-01",
                   end_match_period="2014-10-01")
                   
##-----------------------------------------------------------------------
## Roll up the pairs to generate test and control markets
## Synthetic=FALSE means that the control markets will be aggregated 
## -- i.e., equal weighhs in CausalImpact
##-----------------------------------------------------------------------
                   
rollup &lt;- roll_up_optimal_pairs(matched_markets=mm, 
                                percent_cutoff=1, 
                                synthetic=FALSE)
                                
##-----------------------------------------------------------------------
## Pseudo power analysis (fake lift analysis)
##-----------------------------------------------------------------------

results &lt;- test_fake_lift(matched_markets=rollup,
                     test_market="TEST",
                     lift_pattern_type="constant",
                     end_fake_post_period="2015-12-15",
                     prior_level_sd=0.002, 
                     max_fake_lift=0.1)

## End(Not run)
</code></pre>

<hr>
<h2 id='test_fake_lift'>Given a test market, analyze the impact of fake interventions (prospective power analysis)</h2><span id='topic+test_fake_lift'></span>

<h3>Description</h3>

<p><code>test_fake_lift</code> Given a matched_market object from the best_matches function, this function analyzes the causal impact of fake interventions using the CausalImpact package.
The function returns an object of type &quot;market_inference&quot; which contains the estimated impact of the intervention (absolute and relative).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>test_fake_lift(matched_markets=NULL,
          test_market=NULL,
          end_fake_post_period=NULL,
          alpha=0.05,
          prior_level_sd=0.01,
          control_matches=NULL, 
          nseasons=NULL, 
          max_fake_lift=NULL, 
          steps=10,
          lift_pattern_type="constant")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="test_fake_lift_+3A_matched_markets">matched_markets</code></td>
<td>
<p>A matched_market object created by the market_matching function
This parameter will overwrite the values specified in prior_level_sd and nseasons. ONLY use this if you're using intricate bsts settings
For most use-cases, using the prior_level_sd and nseasons parameters should be sufficient</p>
</td></tr>
<tr><td><code id="test_fake_lift_+3A_test_market">test_market</code></td>
<td>
<p>The name of the test market (character)</p>
</td></tr>
<tr><td><code id="test_fake_lift_+3A_end_fake_post_period">end_fake_post_period</code></td>
<td>
<p>The end date of the post period. Must be a character of format &quot;YYYY-MM-DD&quot; &ndash; e.g., &quot;2015-11-01&quot;</p>
</td></tr>
<tr><td><code id="test_fake_lift_+3A_alpha">alpha</code></td>
<td>
<p>Desired tail-area probability for posterior intervals. For example, 0.05 yields 0.95 intervals</p>
</td></tr>
<tr><td><code id="test_fake_lift_+3A_prior_level_sd">prior_level_sd</code></td>
<td>
<p>Prior SD for the local level term (Gaussian random walk). Default is 0.01. The bigger this number is, the more wiggliness is allowed for the local level term.
Note that more wiggly local level terms also translate into larger posterior intervals
This parameter will be overwritten if you're using the bsts_modelargs parameter</p>
</td></tr>
<tr><td><code id="test_fake_lift_+3A_control_matches">control_matches</code></td>
<td>
<p>Number of matching control markets to use in the analysis (default is 5)</p>
</td></tr>
<tr><td><code id="test_fake_lift_+3A_nseasons">nseasons</code></td>
<td>
<p>Seasonality for the bsts model &ndash; e.g., 52 for weekly seasonality</p>
</td></tr>
<tr><td><code id="test_fake_lift_+3A_max_fake_lift">max_fake_lift</code></td>
<td>
<p>The maximum absolute fake lift &ndash; e.g., 0.1 means that the max lift evaluated is 10 percent and the min lift is -10 percent
Note that randomization is injected into the lift, which means that the max lift will not be exactly as specified</p>
</td></tr>
<tr><td><code id="test_fake_lift_+3A_steps">steps</code></td>
<td>
<p>The number of steps used to calculate the power curve (default is 10)</p>
</td></tr>
<tr><td><code id="test_fake_lift_+3A_lift_pattern_type">lift_pattern_type</code></td>
<td>
<p>Lift pattern. Default is constant. The other choice is a random lift..</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns an object of type <code>matched_market_power</code>. The object has the
following elements:
</p>
<table>
<tr><td><code>ResultsData</code></td>
<td>
<p>The results stored in a data.frame</p>
</td></tr>
<tr><td><code>ResultsGraph</code></td>
<td>
<p>The results stored in a ggplot graph</p>
</td></tr>
<tr><td><code>LiftPattern</code></td>
<td>
<p>The random pattern applied to the lift</p>
</td></tr>
<tr><td><code>FitCharts</code></td>
<td>
<p>The underlying actual versus fitted charts for each fake lift</p>
</td></tr> 
<tr><td><code>FitData</code></td>
<td>
<p>The underlying actual versus fitted data for each fake lift</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
library(MarketMatching)
##-----------------------------------------------------------------------
## Create a pseudo power curve for various levels of lift
## Since this is weather data it is a not a very meaningful example. 
## This is merely to demonstrate the function.
##-----------------------------------------------------------------------
data(weather, package="MarketMatching")
mm &lt;- best_matches(data=weather, 
                   id="Area",
                   markets_to_be_matched=c("CPH", "SFO"),
                   date_variable="Date",
                   matching_variable="Mean_TemperatureF",
                   warping_limit=1, # warping limit=1
                   dtw_emphasis=0, # rely only on dtw for pre-screening
                   matches=5, # request 5 matches
                   start_match_period="2014-01-01",
                   end_match_period="2014-10-01")
library(CausalImpact)
results &lt;- test_fake_lift(matched_markets=mm,
                     test_market="CPH",
                     lift_pattern_type="constant",
                     control_matches=5, # use all 5 matches for inference
                     end_fake_post_period="2015-12-15",
                     prior_level_sd=0.002, 
                     max_fake_lift=0.1)

## End(Not run)
</code></pre>

<hr>
<h2 id='weather'>Weather dataset</h2><span id='topic+weather'></span>

<h3>Description</h3>

<p>The data was extracted using the weatherData package
It contains average temperature readings for 19 airports for 2014.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>weather
</code></pre>


<h3>Format</h3>

<p>A time series dataset with 6,935 rows and 3 variables (19 airports and 365 days):
</p>

<ul>
<li><p> Area: Airport code
</p>
</li>
<li><p> Date: Date
</p>
</li>
<li><p> Mean_TemperatureF: Average temperature
</p>
</li></ul>


</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
