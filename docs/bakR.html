<!DOCTYPE html><html lang="en"><head><title>Help for package bakR</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {bakR}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#bakR-package'><p>The 'bakR' package.</p></a></li>
<li><a href='#avg_and_regularize'><p>Efficiently average replicates of nucleotide recoding data and regularize</p></a></li>
<li><a href='#bakRData'><p>bakR Data object helper function for users</p></a></li>
<li><a href='#bakRFit'><p>Estimating kinetic parameters from nucleotide recoding RNA-seq data</p></a></li>
<li><a href='#bakRFnData'><p>bakRFnData object helper function for users</p></a></li>
<li><a href='#cB_small'><p>Example cB data frame</p></a></li>
<li><a href='#cBprocess'><p>Curate data in bakRData object for statistical modeling</p></a></li>
<li><a href='#CorrectDropout'><p>Correcting for metabolic labeling induced RNA dropout</p></a></li>
<li><a href='#DissectMechanism'><p>Construct heatmap for non-steady state (NSS) analysis with improved mechanism score</p></a></li>
<li><a href='#fast_analysis'><p>Efficiently analyze nucleotide recoding data</p></a></li>
<li><a href='#fn_process'><p>Curate data in bakRFnData object for statistical modeling</p></a></li>
<li><a href='#FnPCA'><p>Creating PCA plots with logit(fn) estimates</p></a></li>
<li><a href='#FnPCA2'><p>Creating PCA plots with logit(fn) estimates</p></a></li>
<li><a href='#fns'><p>Example fraction news (fns) data frame</p></a></li>
<li><a href='#GS_table'><p>Example cB data frame</p></a></li>
<li><a href='#GSprocessing'><p>Prep GRAND-SLAM output for <code>bakRFnData</code></p></a></li>
<li><a href='#Heatmap_kdeg'><p>Creating a L2FC(kdeg) matrix that can be passed to heatmap functions</p></a></li>
<li><a href='#metadf'><p>Example meatdf data frame</p></a></li>
<li><a href='#new_bakRData'><p>bakRData object constructor for internal use</p></a></li>
<li><a href='#new_bakRFnData'><p>bakRFnData object constructor for internal use</p></a></li>
<li><a href='#NSSHeat'><p>Construct heatmap for non-steady state (NSS) analysis</p></a></li>
<li><a href='#plotMA'><p>Creating L2FC(kdeg) MA plot from fit objects</p></a></li>
<li><a href='#plotVolcano'><p>Creating L2FC(kdeg) volcano plot from fit objects</p></a></li>
<li><a href='#QC_checks'><p>Check data quality and make suggestions to user about what analyses to run.</p></a></li>
<li><a href='#QuantifyDropout'><p>Fit dropout model to quantify dropout frequency</p></a></li>
<li><a href='#reliableFeatures'><p>Identify features (e.g., transcripts) with high quality data</p></a></li>
<li><a href='#Simulate_bakRData'><p>Simulating nucleotide recoding data</p></a></li>
<li><a href='#Simulate_relative_bakRData'><p>Simulating nucleotide recoding data with relative count data</p></a></li>
<li><a href='#TL_stan'><p>Fit 'Stan' models to nucleotide recoding RNA-seq data analysis</p></a></li>
<li><a href='#validate_bakRData'><p>bakR Data object validator</p></a></li>
<li><a href='#validate_bakRFnData'><p>bakRFnData object validator</p></a></li>
<li><a href='#VisualizeDropout'><p>Visualize dropout</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Title:</td>
<td>Analyze and Compare Nucleotide Recoding RNA Sequencing Datasets</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.1</td>
</tr>
<tr>
<td>Description:</td>
<td>Several implementations of a novel Bayesian hierarchical statistical model of nucleotide
	recoding RNA-seq experiments (NR-seq; TimeLapse-seq, SLAM-seq, TUC-seq, etc.)
	for analyzing and comparing NR-seq datasets (see 'Vock and Simon' (2023) &lt;<a href="https://doi.org/10.1261%2Frna.079451.122">doi:10.1261/rna.079451.122</a>&gt;).
	NR-seq is a powerful extension of RNA-seq that provides information about the kinetics
	of RNA metabolism (e.g., RNA degradation rate constants), which is notably lacking
	in standard RNA-seq data. The statistical model makes maximal use of these high-throughput
	datasets by sharing information across transcripts to significantly improve
	uncertainty quantification and increase statistical power. 'bakR' includes a maximally
	efficient implementation of this model for conservative initial investigations of datasets. 'bakR'
	also provides more highly powered implementations using the probabilistic programming language
	'Stan' to sample from the full posterior distribution. 'bakR' performs multiple-test
	adjusted statistical inference with the output of these model implementations to
	help biologists separate signal from background. Methods to automatically visualize key
	results and detect batch effects are also provided.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://opensource.org/licenses/mit-license.php">MIT</a> + file LICENSE</td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>Biarch:</td>
<td>true</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>purrr, methods, Rcpp (&ge; 0.12.0), RcppParallel (&ge; 5.0.1),
rstan (&ge; 2.26.0), rstantools (&ge; 2.1.1), dplyr, tidyr, stats,
magrittr, Hmisc, ggplot2, data.table</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>BH (&ge; 1.66.0), Rcpp (&ge; 0.12.0), RcppEigen (&ge; 0.3.3.3.0),
RcppParallel (&ge; 5.0.1), rstan (&ge; 2.26.0), StanHeaders (&ge;
2.26.0)</td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>GNU make C++17</td>
</tr>
<tr>
<td>Suggests:</td>
<td>rmarkdown, knitr, DESeq2, pheatmap, Ckmeans.1d.dp, corrplot</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://simonlabcode.github.io/bakR/">https://simonlabcode.github.io/bakR/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/simonlabcode/bakR/issues/">https://github.com/simonlabcode/bakR/issues/</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-01-13 16:37:02 UTC; isaac</td>
</tr>
<tr>
<td>Author:</td>
<td>Isaac Vock <a href="https://orcid.org/0000-0002-7178-6886"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut,
    cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Isaac Vock &lt;isaac.vock@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-01-13 17:10:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='bakR-package'>The 'bakR' package.</h2><span id='topic+bakR-package'></span><span id='topic+bakR'></span>

<h3>Description</h3>

<p>A DESCRIPTION OF THE PACKAGE
</p>


<h3>References</h3>

<p>Stan Development Team (2020). RStan: the R interface to Stan. R package version 2.21.2. https://mc-stan.org
</p>

<hr>
<h2 id='avg_and_regularize'>Efficiently average replicates of nucleotide recoding data and regularize</h2><span id='topic+avg_and_regularize'></span>

<h3>Description</h3>

<p><code>avg_and_regularize</code> pools and regularizes replicate estimates of kinetic parameters. There are two key steps in this
downstream analysis. 1st, the uncertainty for each feature is used to fit a linear ln(uncertainty) vs. log10(read depth) trend,
and uncertainties for individual features are shrunk towards the regression line. The uncertainty for each feature is a combination of the
Fisher Information asymptotic uncertainty as well as the amount of variability seen between estimates. Regularization of uncertainty
estimates is performed using the analytic results of a Normal distribution likelihood with known mean and unknown variance and conjugate
priors. The prior parameters are estimated from the regression and amount of variability about the regression line. The strength of
regularization can be tuned by adjusting the <code>prior_weight</code> parameter, with larger numbers yielding stronger shrinkage towards
the regression line. The 2nd step is to regularize the average kdeg estimates. This is done using the analytic results of a
Normal distribution likelihood model with unknown mean and known variance and conjugate priors. The prior parameters are estimated from the
population wide kdeg distribution (using its mean and standard deviation as the mean and standard deviation of the normal prior).
In the 1st step, the known mean is assumed to be the average kdeg, averaged across replicates and weighted by the number of reads
mapping to the feature in each replicate. In the 2nd step, the known variance is assumed to be that obtained following regularization
of the uncertainty estimates.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>avg_and_regularize(
  Mut_data_est,
  nreps,
  sample_lookup,
  feature_lookup,
  nbin = NULL,
  NSS = FALSE,
  Chase = FALSE,
  BDA_model = FALSE,
  null_cutoff = 0,
  Mutrates = NULL,
  ztest = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="avg_and_regularize_+3A_mut_data_est">Mut_data_est</code></td>
<td>
<p>Dataframe with fraction new estimation information. Required columns are:
</p>

<ul>
<li><p> fnum; numerical ID of feature
</p>
</li>
<li><p> reps; numerical ID of replicate
</p>
</li>
<li><p> mut; numerical ID of experimental condition (Exp_ID)
</p>
</li>
<li><p> logit_fn_rep; logit(fn) estimate
</p>
</li>
<li><p> kd_rep_est; kdeg estimate
</p>
</li>
<li><p> log_kd_rep_est; log(kdeg) estimate
</p>
</li>
<li><p> logit_fn_se; logit(fn) estimate uncertainty
</p>
</li>
<li><p> log_kd_se; log(kdeg) estimate uncertainty
</p>
</li></ul>
</td></tr>
<tr><td><code id="avg_and_regularize_+3A_nreps">nreps</code></td>
<td>
<p>Vector of number of replicates in each experimental condition</p>
</td></tr>
<tr><td><code id="avg_and_regularize_+3A_sample_lookup">sample_lookup</code></td>
<td>
<p>Dictionary mapping sample names to various experimental details</p>
</td></tr>
<tr><td><code id="avg_and_regularize_+3A_feature_lookup">feature_lookup</code></td>
<td>
<p>Dictionary mapping feature IDs to original feature names</p>
</td></tr>
<tr><td><code id="avg_and_regularize_+3A_nbin">nbin</code></td>
<td>
<p>Number of bins for mean-variance relationship estimation. If NULL, max of 10 or (number of logit(fn) estimates)/100 is used</p>
</td></tr>
<tr><td><code id="avg_and_regularize_+3A_nss">NSS</code></td>
<td>
<p>Logical; if TRUE, logit(fn)s are compared rather than log(kdeg) so as to avoid steady-state assumption.</p>
</td></tr>
<tr><td><code id="avg_and_regularize_+3A_chase">Chase</code></td>
<td>
<p>Logical; Set to TRUE if analyzing a pulse-chase experiment. If TRUE, kdeg = -ln(fn)/tl where fn is the fraction of
reads that are s4U (more properly referred to as the fraction old in the context of a pulse-chase experiment)</p>
</td></tr>
<tr><td><code id="avg_and_regularize_+3A_bda_model">BDA_model</code></td>
<td>
<p>Logical; if TRUE, variance is regularized with scaled inverse chi-squared model. Otherwise a log-normal
model is used.</p>
</td></tr>
<tr><td><code id="avg_and_regularize_+3A_null_cutoff">null_cutoff</code></td>
<td>
<p>bakR will test the null hypothesis of |effect size| &lt; |null_cutoff|</p>
</td></tr>
<tr><td><code id="avg_and_regularize_+3A_mutrates">Mutrates</code></td>
<td>
<p>List containing new and old mutation rate estimates</p>
</td></tr>
<tr><td><code id="avg_and_regularize_+3A_ztest">ztest</code></td>
<td>
<p>TRUE; if TRUE, then a z-test is used for p-value calculation rather than the more conservative moderated t-test.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Effect sizes (changes in kdeg) are obtained as the difference in log(kdeg) means between the reference and experimental
sample(s), and the log(kdeg)s are assumed to be independent so that the variance of the effect size is the sum of the
log(kdeg) variances. P-values assessing the significance of the effect size are obtained using a moderated t-test with number
of degrees of freedom determined from the uncertainty regression hyperparameters and are adjusted for multiple testing using the Benjamini-
Hochberg procedure to control false discovery rates (FDRs).
</p>
<p>In some cases, the assumed ODE model of RNA metabolism will not accurately model the dynamics of a biological system being analyzed.
In these cases, it is best to compare logit(fraction new)s directly rather than converting fraction new to log(kdeg).
This analysis strategy is implemented when <code>NSS</code> is set to TRUE. Comparing logit(fraction new) is only valid
If a single metabolic label time has been used for all samples. For example, if a label time of 1 hour was used for NR-seq
data from WT cells and a 2 hour label time was used in KO cells, this comparison is no longer valid as differences in
logit(fraction new) could stem from differences in kinetics or label times.
</p>


<h3>Value</h3>

<p>List with dataframes providing information about replicate-specific and pooled analysis results. The output includes:
</p>

<ul>
<li><p> Fn_Estimates; dataframe with estimates for the fraction new and fraction new uncertainty for each feature in each replicate.
The columns of this dataframe are:
</p>

<ul>
<li><p> Feature_ID; Numerical ID of feature
</p>
</li>
<li><p> Exp_ID; Numerical ID for experimental condition (Exp_ID from metadf)
</p>
</li>
<li><p> Replicate; Numerical ID for replicate
</p>
</li>
<li><p> logit_fn; logit(fraction new) estimate, unregularized
</p>
</li>
<li><p> logit_fn_se; logit(fraction new) uncertainty, unregularized and obtained from Fisher Information
</p>
</li>
<li><p> nreads; Number of reads mapping to the feature in the sample for which the estimates were obtained
</p>
</li>
<li><p> log_kdeg; log of degradation rate constant (kdeg) estimate, unregularized
</p>
</li>
<li><p> kdeg; degradation rate constant (kdeg) estimate
</p>
</li>
<li><p> log_kd_se; log(kdeg) uncertainty, unregularized and obtained from Fisher Information
</p>
</li>
<li><p> sample; Sample name
</p>
</li>
<li><p> XF; Original feature name
</p>
</li></ul>

</li>
<li><p> Regularized_ests; dataframe with average fraction new and kdeg estimates, averaged across the replicates and regularized
using priors informed by the entire dataset. The columns of this dataframe are:
</p>

<ul>
<li><p> Feature_ID; Numerical ID of feature
</p>
</li>
<li><p> Exp_ID; Numerical ID for experimental condition (Exp_ID from metadf)
</p>
</li>
<li><p> avg_log_kdeg; Weighted average of log(kdeg) from each replicate, weighted by sample and feature-specific read depth
</p>
</li>
<li><p> sd_log_kdeg; Standard deviation of the log(kdeg) estimates
</p>
</li>
<li><p> nreads; Total number of reads mapping to the feature in that condition
</p>
</li>
<li><p> sdp; Prior standard deviation for fraction new estimate regularization
</p>
</li>
<li><p> theta_o; Prior mean for fraction new estimate regularization
</p>
</li>
<li><p> sd_post; Posterior uncertainty
</p>
</li>
<li><p> log_kdeg_post; Posterior mean for log(kdeg) estimate
</p>
</li>
<li><p> kdeg; exp(log_kdeg_post)
</p>
</li>
<li><p> kdeg_sd; kdeg uncertainty
</p>
</li>
<li><p> XF; Original feature name
</p>
</li></ul>

</li>
<li><p> Effects_df; dataframe with estimates of the effect size (change in logit(fn)) comparing each experimental condition to the
reference sample for each feature. This dataframe also includes p-values obtained from a moderated t-test. The columns of this
dataframe are:
</p>

<ul>
<li><p> Feature_ID; Numerical ID of feature
</p>
</li>
<li><p> Exp_ID; Numerical ID for experimental condition (Exp_ID from metadf)
</p>
</li>
<li><p> L2FC(kdeg); Log2 fold change (L2FC) kdeg estimate or change in logit(fn) if NSS TRUE
</p>
</li>
<li><p> effect; LFC(kdeg)
</p>
</li>
<li><p> se; Uncertainty in L2FC_kdeg
</p>
</li>
<li><p> pval; P-value obtained using effect_size, se, and a z-test
</p>
</li>
<li><p> padj; pval adjusted for multiple testing using Benjamini-Hochberg procedure
</p>
</li>
<li><p> XF; Original feature name
</p>
</li></ul>

</li>
<li><p> Mut_rates; list of two elements. The 1st element is a dataframe of s4U induced mutation rate estimates, where the mut column
represents the experimental ID and the rep column represents the replicate ID. The 2nd element is the single background mutation
rate estimate used
</p>
</li>
<li><p> Hyper_Parameters; vector of two elements, named a and b. These are the hyperparameters estimated from the uncertainties for each
feature, and represent the two parameters of a Scaled Inverse Chi-Square distribution. Importantly, a is the number of additional
degrees of freedom provided by the sharing of uncertainty information across the dataset, to be used in the moderated t-test.
</p>
</li>
<li><p> Mean_Variance_lms; linear model objects obtained from the uncertainty vs. read count regression model. One model is run for each Exp_ID
</p>
</li></ul>


<hr>
<h2 id='bakRData'>bakR Data object helper function for users</h2><span id='topic+bakRData'></span>

<h3>Description</h3>

<p>This function creates an object of class bakRData
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bakRData(cB, metadf)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="bakRData_+3A_cb">cB</code></td>
<td>
<p>Dataframe with columns corresponding to feature ID, number of Ts, number of mutations, sample ID, and number of identical observations</p>
</td></tr>
<tr><td><code id="bakRData_+3A_metadf">metadf</code></td>
<td>
<p>Dataframe detailing s4U label time and experimental ID of each sample</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A bakRData object. This has two components: a data frame describing experimental
details (metadf) and a data frame containing the NR-seq data (cB).
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load cB
data("cB_small")

# Load metadf
data("metadf")

# Create bakRData object
bakRData &lt;- bakRData(cB_small, metadf)

</code></pre>

<hr>
<h2 id='bakRFit'>Estimating kinetic parameters from nucleotide recoding RNA-seq data</h2><span id='topic+bakRFit'></span>

<h3>Description</h3>

<p><code>bakRFit</code> analyzes nucleotide recoding RNA-seq data to estimate
kinetic parameters relating to RNA stability and changes in RNA
stability induced by experimental perturbations. Several statistical
models of varying efficiency and accuracy can be used to fit data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bakRFit(
  obj,
  StanFit = FALSE,
  HybridFit = FALSE,
  high_p = 0.2,
  totcut = 50,
  totcut_all = 10,
  Ucut = 0.25,
  AvgU = 4,
  FastRerun = FALSE,
  FOI = c(),
  concat = TRUE,
  StanRateEst = FALSE,
  RateEst_size = 30,
  low_reads = 100,
  high_reads = 5e+05,
  chains = 1,
  NSS = FALSE,
  Chase = FALSE,
  BDA_model = FALSE,
  multi_pold = FALSE,
  Long = FALSE,
  kmeans = FALSE,
  ztest = FALSE,
  Fisher = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="bakRFit_+3A_obj">obj</code></td>
<td>
<p><code>bakRData</code> object produced by <code>bakRData</code>, <code>bakRFit</code> object produced by <code>bakRFit</code>
<code>bakRFnData</code> object produced by <code>bakRFnData</code>, or <code>bakRFnFit</code> object produced by <code>bakRFit</code>.</p>
</td></tr>
<tr><td><code id="bakRFit_+3A_stanfit">StanFit</code></td>
<td>
<p>Logical; if TRUE, then the MCMC implementation is run. Will only be used if <code>obj</code>
is a <code>bakRFit</code> object</p>
</td></tr>
<tr><td><code id="bakRFit_+3A_hybridfit">HybridFit</code></td>
<td>
<p>Logical; if TRUE, then the Hybrid implementation is run. Will only be used if <code>obj</code>
is a <code>bakRFit</code> object</p>
</td></tr>
<tr><td><code id="bakRFit_+3A_high_p">high_p</code></td>
<td>
<p>Numeric; Any features with a mutation rate (number of mutations / number of Ts in reads) higher than this in any -s4U control
samples (i.e., samples that were not treated with s4U) are filtered out</p>
</td></tr>
<tr><td><code id="bakRFit_+3A_totcut">totcut</code></td>
<td>
<p>Numeric; Any features with less than this number of sequencing reads in any replicate of all experimental conditions are filtered out</p>
</td></tr>
<tr><td><code id="bakRFit_+3A_totcut_all">totcut_all</code></td>
<td>
<p>Numeric; Any features with less than this number of sequencing reads in any sample are filtered out</p>
</td></tr>
<tr><td><code id="bakRFit_+3A_ucut">Ucut</code></td>
<td>
<p>Numeric; All features must have a fraction of reads with 2 or less Us less than this cutoff in all samples</p>
</td></tr>
<tr><td><code id="bakRFit_+3A_avgu">AvgU</code></td>
<td>
<p>Numeric; All features must have an average number of Us greater than this cutoff in all samples</p>
</td></tr>
<tr><td><code id="bakRFit_+3A_fastrerun">FastRerun</code></td>
<td>
<p>Logical; only matters if a bakRFit object is passed to <code>bakRFit</code>. If TRUE, then the Stan-free
model implemented in <code>fast_analysis</code> is rerun on data, foregoing fitting of either of the 'Stan' models.</p>
</td></tr>
<tr><td><code id="bakRFit_+3A_foi">FOI</code></td>
<td>
<p>Features of interest; character vector containing names of features to analyze</p>
</td></tr>
<tr><td><code id="bakRFit_+3A_concat">concat</code></td>
<td>
<p>Logical; If TRUE, FOI is concatenated with output of reliableFeatures</p>
</td></tr>
<tr><td><code id="bakRFit_+3A_stanrateest">StanRateEst</code></td>
<td>
<p>Logical; if TRUE, a simple 'Stan' model is used to estimate mutation rates for fast_analysis; this may add a couple minutes
to the runtime of the analysis.</p>
</td></tr>
<tr><td><code id="bakRFit_+3A_rateest_size">RateEst_size</code></td>
<td>
<p>Numeric; if StanRateEst is TRUE, then data from RateEst_size genes are used for mutation rate estimation. This can be as low
as 1 and should be kept low to ensure maximum efficiency</p>
</td></tr>
<tr><td><code id="bakRFit_+3A_low_reads">low_reads</code></td>
<td>
<p>Numeric; if StanRateEst is TRUE, then only features with more than low_reads reads in all samples will be used for mutation rate estimation</p>
</td></tr>
<tr><td><code id="bakRFit_+3A_high_reads">high_reads</code></td>
<td>
<p>Numeric; if StanRateEst is TRUE, then only features with less than high_read reads in all samples will be used for mutation rate estimation.
A high read count cutoff is as important as a low read count cutoff in this case because you don't want the fraction labeled of chosen features to be
extreme (e.g., close to 0 or 1), and high read count features are likely low fraction new features.</p>
</td></tr>
<tr><td><code id="bakRFit_+3A_chains">chains</code></td>
<td>
<p>Number of Markov chains to sample from. 1 should suffice since these are validated models. Running more chains is generally
preferable, but memory constraints can make this unfeasible.</p>
</td></tr>
<tr><td><code id="bakRFit_+3A_nss">NSS</code></td>
<td>
<p>Logical; if TRUE, logit(fn)s are directly compared to avoid assuming steady-state</p>
</td></tr>
<tr><td><code id="bakRFit_+3A_chase">Chase</code></td>
<td>
<p>Logical; Set to TRUE if analyzing a pulse-chase experiment. If TRUE, kdeg = -ln(fn)/tl where fn is the fraction of
reads that are s4U (more properly referred to as the fraction old in the context of a pulse-chase experiment).</p>
</td></tr>
<tr><td><code id="bakRFit_+3A_bda_model">BDA_model</code></td>
<td>
<p>Logical; if TRUE, variance is regularized with scaled inverse chi-squared model. Otherwise a log-normal
model is used.</p>
</td></tr>
<tr><td><code id="bakRFit_+3A_multi_pold">multi_pold</code></td>
<td>
<p>Logical; if TRUE, pold is estimated for each sample rather than use a global pold estimate.</p>
</td></tr>
<tr><td><code id="bakRFit_+3A_long">Long</code></td>
<td>
<p>Logical; if TRUE, long read optimized fraction new estimation strategy is used.</p>
</td></tr>
<tr><td><code id="bakRFit_+3A_kmeans">kmeans</code></td>
<td>
<p>Logical; if TRUE, kmeans clustering on read-specific mutation rates is used to estimate pnews and pold.</p>
</td></tr>
<tr><td><code id="bakRFit_+3A_ztest">ztest</code></td>
<td>
<p>Logical; if TRUE and the MLE implementation is being used, then a z-test will be used for p-value calculation
rather than the more conservative moderated t-test.</p>
</td></tr>
<tr><td><code id="bakRFit_+3A_fisher">Fisher</code></td>
<td>
<p>Logical; if TRUE, Fisher information is used to estimate logit(fn) uncertainty. Else, a less conservative binomial model is used, which
can be preferable in instances where the Fisher information strategy often drastically overestimates uncertainty
(i.e., low coverage or low pnew).</p>
</td></tr>
<tr><td><code id="bakRFit_+3A_...">...</code></td>
<td>
<p>Arguments passed to either <code>fast_analysis</code> (if a bakRData object)
or <code>TL_Stan</code> and <code>Hybrid_fit</code> (if a bakRFit object)</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If <code>bakRFit</code> is run on a bakRData object, <code>cBprocess</code>
and then <code>fast_analysis</code> will always be called. The former will generate the processed
data that can be passed to the model fitting functions (<code>fast_analysis</code>
and <code>TL_Stan</code>). The call to <code>fast_analysis</code> will generate a list of dataframes
containing information regarding the <code>fast_analysis</code> fit. <code>fast_analysis</code> is always
called because its output is required for both <code>Hybrid_fit</code> and <code>TL_Stan</code>.
</p>
<p>If <code>bakRFit</code> is run on a bakRFit object, <code>cBprocess</code> will not be called again,
as the output of <code>cBprocess</code> will already be contained in the bakRFit object. Similarly,
<code>fast_analysis</code> will not be called again unless bakRFit is rerun on the bakRData object.
or if <code>FastRerun</code> is set to TRUE. If you want to generate model fits using different parameters for cBprocess,
you will have to rerun <code>bakRFit</code> on the bakRData object.
</p>
<p>If <code>bakRFit</code> is run on a bakRFnData object, <code>fn_process</code> and then <code>avg_and_regularize</code>
will always be called. The former will generate the processed data that can be passed to the model
fitting functions (<code>avg_and_regularize</code> and <code>TL_Stan</code>, the latter only with HybridFit = TRUE).
</p>
<p>If <code>bakRFit</code> is run on a bakRFnFit object. <code>fn_process</code> will not be called again, as
the output of <code>fn_process</code> will already be contained in the bakRFnFit object. Similary,
<code>avg_and_regularize</code> will not be called unless <code>bakRFit</code> is rerun on the bakRData object,
or if <code>FastRerun</code> is set to TRUE. If you want to generate model fits using different
parameters for <code>fn_process</code>, you will have to rerun <code>bakRFit</code> on the bakRData object.
</p>
<p>See the documentation for the individual fitting functions for details regarding how they analyze nucleotide
recoding data. What follows is a brief overview of how each works
</p>
<p><code>fast_analysis</code> (referred to as the MLE implementation in the bakR paper)
either estimates mutation rates from + and (if available) - s4U samples or uses mutation rate estimates
provided by the user to perform maximum likelihood estimation (MLE) of the fraction of RNA that is labeled for each
replicate of nucleotide recoding data provided. Uncertainties for each replicate's estimate are approximated using
asymptotic results involving the Fisher Information and assuming known mutation rates. Replicate data
is pooled using an approximation to hierarchical modeling that relies on analytic solutions to simple Bayesian models.
Linear regression is used to estimate the relationship between read depths and replicate variability for uncertainty
estimation regularization, again performed using analytic solutions to Bayesian models.
</p>
<p><code>TL_Stan</code> with Hybrid_Fit set to TRUE (referred to as the Hybrid implementation in the bakR paper)
takes as input estimates of the logit(fraction new) and uncertainty provided by <code>fast_analysis</code>.
It then uses 'Stan' on the backend to implement a hierarchical model that pools data across replicates and the dataset
to estimate effect sizes (L2FC(kdeg)) and uncertainties. Replicate variability information is pooled across each experimental
condition to regularize variance estimates using a hierarchical linear regression model.
</p>
<p>The default behavior of <code>TL_Stan</code> (referred to as the MCMC implementation in the bakR paper)
is to use 'Stan' on the back end to implement a U-content exposure adjusted Poisson mixture model
to estimate fraction news from the mutational data. Partial pooling of replicate variability estimates
is performed as with the Hybrid implementation.
</p>


<h3>Value</h3>

<p>bakRFit object with results from statistical modeling and data processing. Objects possibly included are:
</p>

<ul>
<li><p> Fast_Fit; Always will be present. Output of <code>fast_analysis</code>
</p>
</li>
<li><p> Hybrid_Fit; Only present if HybridFit = TRUE. Output of <code>TL_stan</code>
</p>
</li>
<li><p> Stan_Fit; Only present if StanFit = TRUE. Output of <code>TL_stan</code>
</p>
</li>
<li><p> Data_lists; Always will be present. Output of <code>cBprocess</code> with Fast and Stan == TRUE
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>
# Simulate data for 1000 genes, 2 replicates, 2 conditions
simdata &lt;- Simulate_bakRData(1000, nreps = 2)

# You always must fit fast implementation before any others
Fit &lt;- bakRFit(simdata$bakRData)



</code></pre>

<hr>
<h2 id='bakRFnData'>bakRFnData object helper function for users</h2><span id='topic+bakRFnData'></span>

<h3>Description</h3>

<p>This function creates an object of class bakRFnData
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bakRFnData(fns, metadf)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="bakRFnData_+3A_fns">fns</code></td>
<td>
<p>Dataframe with columns corresponding to sample names (sample), feature IDs (XF),
fraction new estimates (fn), and number of sequencing reads (nreads). <code>fns</code> can optionally
contain a column of fraction new estimate uncertainties (se).</p>
</td></tr>
<tr><td><code id="bakRFnData_+3A_metadf">metadf</code></td>
<td>
<p>Dataframe detailing s4U label time and experimental ID of each sample. Identical to <code>bakRData</code>
input</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A bakRFnData object. This has two components: a data frame describing experimental
details (metadf) and a data frame containing the fraction new estimates (fns).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### NEED TO ADD EXAMPLE DATA
# Load cB
data("cB_small")

# Load metadf
data("metadf")

# Create bakRData object
bakRData &lt;- bakRData(cB_small, metadf)

</code></pre>

<hr>
<h2 id='cB_small'>Example cB data frame</h2><span id='topic+cB_small'></span>

<h3>Description</h3>

<p>Subset of a cB file from the DCP2 dataset published in Luo et al. 2020.
The original file is large (69 MB), so the example cB file has been
downsampled and contains only 10 genes (rather than 25012). The columns
are described in the Getting_Started vignette.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(cB_small)
</code></pre>


<h3>Format</h3>

<p>A dataframe with 5788 rows and 5 variables; each row corresponds to a group of sequencing reads
</p>

<dl>
<dt>sample</dt><dd><p>Sample name</p>
</dd>
<dt>TC</dt><dd><p>Number of T-to-C mutations</p>
</dd>
<dt>nT</dt><dd><p>Number of Ts</p>
</dd>
<dt>XF</dt><dd><p>Name of feature to which the group of reads map; usually a gene name</p>
</dd>
<dt>n</dt><dd><p>Number of identical sequencing reads</p>
</dd>
</dl>



<h3>References</h3>

<p>Luo et al. (2020) Biochemistry. 59(42), 4121-4142
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(cB_small)
data(metadf)
bakRdat &lt;- bakRData(cB_small, metadf)
</code></pre>

<hr>
<h2 id='cBprocess'>Curate data in bakRData object for statistical modeling</h2><span id='topic+cBprocess'></span>

<h3>Description</h3>

<p><code>cBprocess</code> creates the data structures necessary to analyze nucleotide recoding RNA-seq data with any of the
statistical model implementations in <code>bakRFit</code>. The input to <code>cBprocess</code> must be an object of class
<code>bakRData</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cBprocess(
  obj,
  high_p = 0.2,
  totcut = 50,
  totcut_all = 10,
  Ucut = 0.25,
  AvgU = 4,
  Stan = TRUE,
  Fast = TRUE,
  FOI = c(),
  concat = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="cBprocess_+3A_obj">obj</code></td>
<td>
<p>An object of class bakRData</p>
</td></tr>
<tr><td><code id="cBprocess_+3A_high_p">high_p</code></td>
<td>
<p>Numeric; Any transcripts with a mutation rate (number of mutations / number of Ts in reads) higher than this in any no s4U control
samples are filtered out</p>
</td></tr>
<tr><td><code id="cBprocess_+3A_totcut">totcut</code></td>
<td>
<p>Numeric; Any transcripts with less than this number of sequencing reads in any replicate of all experimental conditions are filtered out</p>
</td></tr>
<tr><td><code id="cBprocess_+3A_totcut_all">totcut_all</code></td>
<td>
<p>Numeric; Any transcripts with less than this number of sequencing reads in any sample are filtered out</p>
</td></tr>
<tr><td><code id="cBprocess_+3A_ucut">Ucut</code></td>
<td>
<p>Numeric; All transcripts must have a fraction of reads with 2 or less Us less than this cutoff in all samples</p>
</td></tr>
<tr><td><code id="cBprocess_+3A_avgu">AvgU</code></td>
<td>
<p>Numeric; All transcripts must have an average number of Us greater than this cutoff in all samples</p>
</td></tr>
<tr><td><code id="cBprocess_+3A_stan">Stan</code></td>
<td>
<p>Boolean; if TRUE, then data_list that can be passed to 'Stan' is curated</p>
</td></tr>
<tr><td><code id="cBprocess_+3A_fast">Fast</code></td>
<td>
<p>Boolean; if TRUE, then dataframe that can be passed to fast_analysis() is curated</p>
</td></tr>
<tr><td><code id="cBprocess_+3A_foi">FOI</code></td>
<td>
<p>Features of interest; character vector containing names of features to analyze. If <code>FOI</code> is non-null and <code>concat</code> is TRUE, then
all minimally reliable FOIs will be combined with reliable features passing all set filters (<code>high_p</code>, <code>totcut</code>, <code>totcut_all</code>,
<code>Ucut</code>, and <code>AvgU</code>). If <code>concat</code> is FALSE, only the minimally reliable FOIs will be kept. A minimally reliable FOI is one that passes
filtering with minimally stringent parameters.</p>
</td></tr>
<tr><td><code id="cBprocess_+3A_concat">concat</code></td>
<td>
<p>Boolean; If TRUE, FOI is concatenated with output of reliableFeatures</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The 1st step executed by <code>cBprocess</code> is to find the names of features which are deemed &quot;reliable&quot;. A reliable feature is one with
sufficient read coverage in every single sample (i.e., &gt; totcut_all reads in all samples), sufficient read coverage in at all replicates
of at least one experimental condition (i.e., &gt; totcut reads in all replicates for one or more experimental conditions) and limited mutation content in all -s4U
control samples (i.e., &lt; high_p mutation rate in all samples lacking s4U feeds). In addition, if analyzing short read sequencing data, two additional
definitons of reliable features become pertinent: the fraction of reads that can have 2 or less Us in each sample (Ucut) and the
minimum average number of Us for a feature's reads in each sample (AvgU). This is done with a call to <code>reliableFeatures</code>.
</p>
<p>The 2nd step is to extract only reliableFeatures from the cB dataframe in the <code>bakRData</code> object. During this process, a numerical
ID is given to each reliableFeature, with the numerical ID corresponding to their order when arranged using <code>dplyr::arrange</code>.
</p>
<p>The 3rd step is to prepare a dataframe where each row corresponds to a set of n identical reads (that is they come from the same sample
and have the same number of mutations and Us). Part of this process involves assigning an arbitrary numerical ID to each replicate in each
experimental condition. The numerical ID will correspond to the order the sample appears in metadf. The outcome of this step is multiple
dataframes with variable information content. These include a dataframe with information about read counts in each sample, one which logs
the U-contents of each feature, one which is compatible with <code>fast_analysis</code> and thus groups reads by their number of mutations as
well as their number of Us, and one which is compatible with <code>TL_stan</code> with StanFit == TRUE and thus groups ready by only their number
of mutations. At the end of this step, two other smaller data structures are created, one which is an average count matrix (a count matrix
where the ith row and jth column corresponds to the average number of reads mappin to feature i in experimental condition j, averaged over
all replicates) and the other which is a sample lookup table that relates the numerical experimental and replicate IDs to the original
sample name.
</p>


<h3>Value</h3>

<p>returns list of objects that can be passed to <code>TL_stan</code> and/or <code>fast_analysis</code>. Those objects are:
</p>

<ul>
<li><p> Stan_data; list that can be passed to <code>TL_stan</code> with Hybrid_Fit = FALSE. Consists of metadata as well as data that
'Stan' will analyze. Data to be analyzed consists of equal length vectors. The contents of Stan_data are:
</p>

<ul>
<li><p> NE; Number of datapoints for 'Stan' to analyze (NE = Number of Elements)
</p>
</li>
<li><p> NF; Number of features in dataset
</p>
</li>
<li><p> TP; Numerical indicator of s4U feed (0 = no s4U feed, 1 = s4U fed)
</p>
</li>
<li><p> FE; Numerical indicator of feature
</p>
</li>
<li><p> num_mut; Number of U-to-C mutations observed in a particular set of reads
</p>
</li>
<li><p> MT; Numerical indicator of experimental condition (Exp_ID from metadf)
</p>
</li>
<li><p> nMT; Number of experimental conditions
</p>
</li>
<li><p> R; Numerical indicator of replicate
</p>
</li>
<li><p> nrep; Number of replicates (analysis requires same number of replicates of all conditions)
</p>
</li>
<li><p> num_obs; Number of reads with identical data (number of mutations, feature of origin, and sample of origin)
</p>
</li>
<li><p> tl; Vector of label times for each experimental condition
</p>
</li>
<li><p> U_cont; Log2-fold-difference in U-content for a feature in a sample relative to average U-content for that sample
</p>
</li>
<li><p> Avg_Reads; Standardized log10(average read counts) for a particular feature in a particular condition, averaged over
replicates
</p>
</li>
<li><p> Avg_Reads_natural; Unstandardized average read counts for a particular feature in a particular condition, averaged over
replicates. Used for <code>plotMA</code>
</p>
</li>
<li><p> sdf; Dataframe that maps numerical feature ID to original feature name. Also has read depth information
</p>
</li>
<li><p> sample_lookup; Lookup table relating MT and R to the original sample name
</p>
</li></ul>

</li>
<li><p> Fast_df; A data frame that can be passed to <code>fast_analysis</code>. The contents of Fast_df are:
</p>

<ul>
<li><p> sample; Original sample name
</p>
</li>
<li><p> XF; Original feature name
</p>
</li>
<li><p> TC; Number of T to C mutations
</p>
</li>
<li><p> nT; Number of Ts in read
</p>
</li>
<li><p> n; Number of identical observations
</p>
</li>
<li><p> fnum; Numerical indicator of feature
</p>
</li>
<li><p> type; Numerical indicator of s4U feed (0 = no s4U feed, 1 = s4U fed)
</p>
</li>
<li><p> mut; Numerical indicator of experimental condition (Exp_ID from metadf)
</p>
</li>
<li><p> reps; Numerical indicator of replicate
</p>
</li></ul>

</li>
<li><p> Count_Matrix; A matrix with read count information. Each column represents a sample and each row represents a feature.
Each entry is the raw number of read counts mapping to a particular feature in a particular sample. Column names are the corresponding
sample names and row names are the corresponding feature names.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>

# Load cB
data("cB_small")

# Load metadf
data("metadf")

# Create bakRData
bakRData &lt;- bakRData(cB_small, metadf)

# Preprocess data
data_for_bakR &lt;- cBprocess(obj = bakRData)

</code></pre>

<hr>
<h2 id='CorrectDropout'>Correcting for metabolic labeling induced RNA dropout</h2><span id='topic+CorrectDropout'></span>

<h3>Description</h3>

<p>Dropout is the name given to a phenomenon originally identified by our lab and
further detailed in two independent publications (<a href="https://www.biorxiv.org/content/10.1101/2023.05.24.542133v1" >Zimmer et al. (2023)</a>,
and <a href="https://www.biorxiv.org/content/10.1101/2023.04.21.537786v1"> Berg et al. (2023)</a>).
Dropout is the under-representation of reads from RNA containing metabolic label
(4-thiouridine or 6-thioguanidine most commonly). Loss of 4-thiouridine (s4U)
containing RNA on plastic surfaces and RT dropoff caused by
modifications on s4U introduced by recoding chemistry have been attributed as the likely
causes of this phenomenon. While protocols can be altered in ways to drastically reduce this
source of dropout, you may still have datasets that you want to analyze with bakR collected
with suboptimal handling. That is where <code>CorrectDropout</code> comes in.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>CorrectDropout(
  obj,
  scale_init = 1.05,
  pdo_init = 0.3,
  recalc_uncertainty = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="CorrectDropout_+3A_obj">obj</code></td>
<td>
<p>bakRFit object</p>
</td></tr>
<tr><td><code id="CorrectDropout_+3A_scale_init">scale_init</code></td>
<td>
<p>Numeric; initial estimate for -s4U/+s4U scale factor. This is the factor
difference in RPM normalized read counts for completely unlabeled transcripts (i.e., highly stable
transcript) between the +s4U and -s4U samples.</p>
</td></tr>
<tr><td><code id="CorrectDropout_+3A_pdo_init">pdo_init</code></td>
<td>
<p>Numeric; initial estimtae for the dropout rate. This is the probability
that an s4U labeled RNA molecule is lost during library prepartion.</p>
</td></tr>
<tr><td><code id="CorrectDropout_+3A_recalc_uncertainty">recalc_uncertainty</code></td>
<td>
<p>Logical; if TRUE, then fraction new uncertainty is recalculated
using adjusted fn and a simple binomial model of estimate uncertainty. This will provide a
slight underestimate of the fn uncertainty, but will be far less biased for low coverage features,
or for samples with low pnews.</p>
</td></tr>
<tr><td><code id="CorrectDropout_+3A_...">...</code></td>
<td>
<p>Additional (optional) parameters to be passed to <code>stats::nls()</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>CorrectDropout</code> estimates the percentage of 4-thiouridine containing RNA
that was lost during library preparation (pdo). It then uses this estimate of pdo
to correct fraction new estimates and read counts. Both corrections are analytically
derived from a rigorous generative model of NR-seq data. Importantly, the read count
correction preserves the total library size to avoid artificially inflating read counts.
</p>


<h3>Value</h3>

<p>A <code>bakRFit</code> or <code>bakRFnFit</code> object (same type as was passed in). Fraction new estimates and read counts
in <code>Fast_Fit$Fn_Estimates</code> and (in the case of a <code>bakRFnFit</code> input) <code>Data_lists$Fn_Est</code>are dropout corrected.
A count matrix with corrected read counts (<code>Data_lists$Count_Matrix_corrected</code>) is also output, along with a
data frame with information about the dropout rate estimated for each sample (<code>Data_lists$Dropout_df</code>).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Simulate data for 500 genes and 2 replicates with 40% dropout
sim &lt;- Simulate_relative_bakRData(500, 100000, nreps = 2, p_do = 0.4)

# Fit data with fast implementation
Fit &lt;- bakRFit(sim$bakRData)

# Correct for dropout
Fit &lt;- CorrectDropout(Fit)


</code></pre>

<hr>
<h2 id='DissectMechanism'>Construct heatmap for non-steady state (NSS) analysis with improved mechanism score</h2><span id='topic+DissectMechanism'></span>

<h3>Description</h3>

<p>This uses the output of bakR and a differential expression analysis software to construct
a dataframe that can be passed to pheatmap::pheatmap(). This heatmap will display the
result of a steady-state quasi-independent analysis of NR-seq data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>DissectMechanism(
  bakRFit,
  DE_df,
  bakRModel = c("MLE", "Hybrid", "MCMC"),
  DE_cutoff = 0.05,
  bakR_cutoff = 0.3,
  Exp_ID = 2,
  sims = 1e+07
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="DissectMechanism_+3A_bakrfit">bakRFit</code></td>
<td>
<p>bakRFit object</p>
</td></tr>
<tr><td><code id="DissectMechanism_+3A_de_df">DE_df</code></td>
<td>
<p>dataframe of required format with differential expression analysis results. See
Further-Analyses vignette for details on what this dataframe should look like</p>
</td></tr>
<tr><td><code id="DissectMechanism_+3A_bakrmodel">bakRModel</code></td>
<td>
<p>Model fit from which bakR implementation should be used? Options are MLE, Hybrid,
or MCMC</p>
</td></tr>
<tr><td><code id="DissectMechanism_+3A_de_cutoff">DE_cutoff</code></td>
<td>
<p>padj cutoff for calling a gene differentially expressed</p>
</td></tr>
<tr><td><code id="DissectMechanism_+3A_bakr_cutoff">bakR_cutoff</code></td>
<td>
<p>padj cutoff for calling a fraction new significantly changed. As discussed in the mechanistic
dissection vignette, it is best to keep this more conservative (higher padj) than is typical. Thus, default is 0.3 rather
than the more standard (though admittedly arbitrary) 0.05.</p>
</td></tr>
<tr><td><code id="DissectMechanism_+3A_exp_id">Exp_ID</code></td>
<td>
<p>Exp_ID of experimental sample whose comparison to the reference sample you want to use.
Only one reference vs. experimental sample comparison can be used at a time</p>
</td></tr>
<tr><td><code id="DissectMechanism_+3A_sims">sims</code></td>
<td>
<p>Number of simulation draws from null distribution for mechanism p value calculation</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Unlike NSSHeat, DissectMechanism uses a mechanism scoring function that is not discontinuous
as the &quot;degradation driven&quot; vs. &quot;synthesis driven&quot; boundary. Instead, the score
approaches 0 as the function approaches the boundary from either side.
</p>
<p>In addition, DissectMechanism now defines a null model for the purpose of p value calculation using
the mechanism score. Under the null hypothesis, the mechanism score is the product of two
normal distributions with unit variance, one which has a non-zero mean. Simulation is used
to estimate the integral of this distribution, and the number of draws (which determines the
precision of the p value estimate) is determined by the <code>sims</code> parameter.
</p>
<p>DissectMechanism also provides &quot;meta-analysis p values&quot;, which can be interpreted as the p-value that
a particular RNA feature is observing differential expression or differential kinetics (or both).
This meta_pval is estimated using Fisher's method for meta analysis.
</p>


<h3>Value</h3>

<p>returns list of data frames: heatmap_df and NSS_stats.
The heatmap_dfdata frame can be passed to pheatmap::pheatmap().
The NSS_stats data frame contains all of the information passed to NSS_stats as well
as the raw mechanism scores. It also has p values calculated from the mechanism z scores.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Simulate small dataset
sim &lt;- Simulate_bakRData(100, nreps = 2)

# Analyze data with bakRFit
Fit &lt;- bakRFit(sim$bakRData)

# Number of features that made it past filtering
NF &lt;- nrow(Fit$Fast_Fit$Effects_df)

# Simulate mock differential expression data frame
DE_df &lt;- data.frame(XF = as.character(1:NF),
                       L2FC_RNA = stats::rnorm(NF, 0, 2))

DE_df$DE_score &lt;- DE_df$L2FC_RNA/0.5
DE_df$DE_se &lt;- 0.5

DE_df$DE_pval &lt;- 2*stats::dnorm(-abs(DE_df$DE_score))
DE_df$DE_padj &lt;- 2*stats::p.adjust(DE_df$DE_pval, method = "BH")

# perform NSS analysis
NSS_analysis &lt;- DissectMechanism(bakRFit = Fit,
               DE_df = DE_df,
               bakRModel = "MLE")


</code></pre>

<hr>
<h2 id='fast_analysis'>Efficiently analyze nucleotide recoding data</h2><span id='topic+fast_analysis'></span>

<h3>Description</h3>

<p><code>fast_analysis</code> analyzes nucleotide recoding data with maximum likelihood estimation
implemented by <code>stats::optim</code> combined with analytic solutions to simple Bayesian models to perform
approximate partial pooling. Output includes kinetic parameter estimates in each replicate, kinetic parameter estimates
averaged across replicates, and log-2 fold changes in the degradation rate constant (L2FC(kdeg)).
Averaging takes into account uncertainties estimated using the Fisher Information and estimates
are regularized using analytic solutions of fully Bayesian models. The result is that kdegs are
shrunk towards population means and that uncertainties are shrunk towards a mean-variance trend estimated as part of the analysis.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fast_analysis(
  df,
  pnew = NULL,
  pold = NULL,
  no_ctl = FALSE,
  read_cut = 50,
  features_cut = 50,
  nbin = NULL,
  prior_weight = 2,
  MLE = TRUE,
  ztest = FALSE,
  lower = -7,
  upper = 7,
  se_max = 2.5,
  mut_reg = 0.1,
  p_mean = 0,
  p_sd = 1,
  StanRate = FALSE,
  Stan_data = NULL,
  null_cutoff = 0,
  NSS = FALSE,
  Chase = FALSE,
  BDA_model = FALSE,
  multi_pold = FALSE,
  Long = FALSE,
  kmeans = FALSE,
  Fisher = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="fast_analysis_+3A_df">df</code></td>
<td>
<p>Dataframe in form provided by cB_to_Fast</p>
</td></tr>
<tr><td><code id="fast_analysis_+3A_pnew">pnew</code></td>
<td>
<p>Labeled read mutation rate; default of 0 means that model estimates rate from s4U fed data. If pnew is provided by user, must be  a vector
of length == number of s4U fed samples. The 1st element corresponds to the s4U induced mutation rate estimate for the 1st replicate of the 1st
experimental condition; the 2nd element corresponds to the s4U induced mutation rate estimate for the 2nd replicate of the 1st experimental condition,
etc.</p>
</td></tr>
<tr><td><code id="fast_analysis_+3A_pold">pold</code></td>
<td>
<p>Unlabeled read mutation rate; default of 0 means that model estimates rate from no-s4U fed data</p>
</td></tr>
<tr><td><code id="fast_analysis_+3A_no_ctl">no_ctl</code></td>
<td>
<p>Logical; if TRUE, then -s4U control is not used for background mutation rate estimation</p>
</td></tr>
<tr><td><code id="fast_analysis_+3A_read_cut">read_cut</code></td>
<td>
<p>Minimum number of reads for a given feature-sample combo to be used for mut rate estimates</p>
</td></tr>
<tr><td><code id="fast_analysis_+3A_features_cut">features_cut</code></td>
<td>
<p>Number of features to estimate sample specific mutation rate with</p>
</td></tr>
<tr><td><code id="fast_analysis_+3A_nbin">nbin</code></td>
<td>
<p>Number of bins for mean-variance relationship estimation. If NULL, max of 10 or (number of logit(fn) estimates)/100 is used</p>
</td></tr>
<tr><td><code id="fast_analysis_+3A_prior_weight">prior_weight</code></td>
<td>
<p>Determines extent to which logit(fn) variance is regularized to the mean-variance regression line</p>
</td></tr>
<tr><td><code id="fast_analysis_+3A_mle">MLE</code></td>
<td>
<p>Logical; if TRUE then replicate logit(fn) is estimated using maximum likelihood; if FALSE more conservative Bayesian hypothesis testing is used</p>
</td></tr>
<tr><td><code id="fast_analysis_+3A_ztest">ztest</code></td>
<td>
<p>TRUE; if TRUE, then a z-test is used for p-value calculation rather than the more conservative moderated t-test.</p>
</td></tr>
<tr><td><code id="fast_analysis_+3A_lower">lower</code></td>
<td>
<p>Lower bound for MLE with L-BFGS-B algorithm</p>
</td></tr>
<tr><td><code id="fast_analysis_+3A_upper">upper</code></td>
<td>
<p>Upper bound for MLE with L-BFGS-B algorithm</p>
</td></tr>
<tr><td><code id="fast_analysis_+3A_se_max">se_max</code></td>
<td>
<p>Uncertainty given to those transcripts with estimates at the upper or lower bound sets. This prevents downstream errors due to
abnormally high standard errors due to transcripts with extreme kinetics</p>
</td></tr>
<tr><td><code id="fast_analysis_+3A_mut_reg">mut_reg</code></td>
<td>
<p>If MLE has instabilities, empirical mut rate will be used to estimate fn, multiplying pnew by 1+mut_reg and pold by 1-mut_reg to regularize fn</p>
</td></tr>
<tr><td><code id="fast_analysis_+3A_p_mean">p_mean</code></td>
<td>
<p>Mean of normal distribution used as prior penalty in MLE of logit(fn)</p>
</td></tr>
<tr><td><code id="fast_analysis_+3A_p_sd">p_sd</code></td>
<td>
<p>Standard deviation of normal distribution used as prior penalty in MLE of logit(fn)</p>
</td></tr>
<tr><td><code id="fast_analysis_+3A_stanrate">StanRate</code></td>
<td>
<p>Logical; if TRUE, a simple 'Stan' model is used to estimate mutation rates for fast_analysis; this may add a couple minutes
to the runtime of the analysis.</p>
</td></tr>
<tr><td><code id="fast_analysis_+3A_stan_data">Stan_data</code></td>
<td>
<p>List; if StanRate is TRUE, then this is the data passed to the 'Stan' model to estimate mutation rates. If using the <code>bakRFit</code>
wrapper of <code>fast_analysis</code>, then this is created automatically.</p>
</td></tr>
<tr><td><code id="fast_analysis_+3A_null_cutoff">null_cutoff</code></td>
<td>
<p>bakR will test the null hypothesis of |effect size| &lt; |null_cutoff|</p>
</td></tr>
<tr><td><code id="fast_analysis_+3A_nss">NSS</code></td>
<td>
<p>Logical; if TRUE, logit(fn)s are compared rather than log(kdeg) so as to avoid steady-state assumption.</p>
</td></tr>
<tr><td><code id="fast_analysis_+3A_chase">Chase</code></td>
<td>
<p>Logical; Set to TRUE if analyzing a pulse-chase experiment. If TRUE, kdeg = -ln(fn)/tl where fn is the fraction of
reads that are s4U (more properly referred to as the fraction old in the context of a pulse-chase experiment)</p>
</td></tr>
<tr><td><code id="fast_analysis_+3A_bda_model">BDA_model</code></td>
<td>
<p>Logical; if TRUE, variance is regularized with scaled inverse chi-squared model. Otherwise a log-normal
model is used.</p>
</td></tr>
<tr><td><code id="fast_analysis_+3A_multi_pold">multi_pold</code></td>
<td>
<p>Logical; if TRUE, pold is estimated for each sample rather than use a global pold estimate.</p>
</td></tr>
<tr><td><code id="fast_analysis_+3A_long">Long</code></td>
<td>
<p>Logical; if TRUE, long read optimized fraction new estimation strategy is used.</p>
</td></tr>
<tr><td><code id="fast_analysis_+3A_kmeans">kmeans</code></td>
<td>
<p>Logical; if TRUE, kmeans clustering on read-specific mutation rates is used to estimate pnews and pold.</p>
</td></tr>
<tr><td><code id="fast_analysis_+3A_fisher">Fisher</code></td>
<td>
<p>Logical; if TRUE, Fisher information is used to estimate logit(fn) uncertainty. Else, a less conservative binomial model is used, which
can be preferable in instances where the Fisher information strategy often drastically overestimates uncertainty
(i.e., low coverage or low pnew).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Unless the user supplies estimates for pnew and pold, the first step of <code>fast_analysis</code> is to estimate the background (pold)
and metabolic label (will refer to as s4U for simplicity, though bakR is compatible with other metabolic labels such as s6G)
induced (pnew) mutation rates. Several pnew and pold estimation strategies are implemented in bakR. For pnew estimation,
the two strategies are likelihood maximization of a binomial mixture model (default) and sampling from the full posterior of
a U-content adjusted Poisson mixture model with HMC (when <code>StanRateEst</code> is set to TRUE in <code>bakRFit</code>).
</p>
<p>The default pnew estimation strategy involves combining the mutational data for all features into sample-wide mutational
data vectors (# of T-to-C conversions, # of Ts, and # of such reads vectors). These data vectors are then
downsampled (to prevent float overflow) and used to maximize the likelihood of a two-component binomial mixture model. The
two components correspond to reads from old and new RNA, and the three estimated paramters are the fraction of
all reads that are new (nuisance parameter in this case), and the old and new read mutation rates.
</p>
<p>The alternative strategy involves running a fully Bayesian implementation of a similar mixture model using Stan, a
probalistic programming language that bakR makes use of in several functions. This strategy can yield more accurate
mutation rate estimates when the label times are much shorter or longer than the average half-lives of the sequenced RNA
(i.e., the fraction news are mostly close to 0 or 1, respectively). To improve the efficiency of this approach, only a small
subset of RNA features are analyzed, the number of which is set by the <code>RateEst_size</code> parameter in <code>bakRFit</code>. By default,
this number is set to 30, which on the average NR-seq dataset yields a several minute runtime for mutation rate estimation.
</p>
<p>Estimation of pold can be performed with three strategies: the same two strategies discussed for pnew estimation, and a third
strategy that relies on the presence of -s4U control data. If -s4U control data is present, the default pold estimation
strategy is to use the average mutation rate in reads from all -s4U control datasets as the global pold estimate. Thus,
a single pold estimate is used for all samples. The likelihood maximization strategy can be used by
setting <code>no_ctl</code> to TRUE, and this strategy becomes the default strategy if no -s4U data is present.
In addition, as of version 1.0.0 of bakR (released late June of 2023), users can decide to estimate
pold for each +s4U sample independently by setting <code>multi_pold</code> to TRUE. In this case, independent -s4U datasets can no longer be used for
mutation rate estimation purposes, and thus the strategies for pold estimation are identical to the
set of pnew estimation strategies.
</p>
<p>Once mutation rates are estimated, fraction news for each feature in each sample are estimated. The approach utilized is MLE
using the L-BFGS-B algorithm implemented in <code>stats::optim</code>. The assumed likelihood function is derived from a Poisson mixture
model with rates adjusted according to each feature's empirical U-content (the average number of Us present in sequencing reads mapping
to that feature in a particular sample). Fraction new estimates are then converted to degradation rate constant estimates using
a solution to a simple ordinary differential equation model of RNA metabolism.
</p>
<p>Once fraction new and kdegs are estimated, the uncertainty in these parameters is estimated using the Fisher Information. In the limit of
large datasets, the variance of the MLE is inversely proportional to the Fisher Information evaluated at the MLE. Mixture models are
typically singular, meaning that the Fisher information matrix is not positive definite and asymptotic results for the variance
do not necessarily hold. As the mutation rates are estimated a priori and fixed to be &gt; 0, these problems are eliminated. In addition, when assessing
the uncertainty of replicate fraction new estimates, the size of the dataset is the raw number of sequencing reads that map to a
particular feature. This number is often large (&gt;100) which increases the validity of invoking asymptotics.
As of version 1.0.0, users can opt for an alternative uncertainty estimation strategy by setting
<code>Fisher</code> to FALSE. This strategy makes use of the standard error for the estimator of a binomial random
variables rate of success parameter. If we can uniquely identify new and old reads, then the
variance in our estimate for the fraction of reads that is new is fn*(1-fn)/n. This uncertainty
estimate will typically underestimate fraction new replicate uncertainties. We showed in the bakR
paper though that the Fisher information strategy often significantly overestimates uncertainties
of low coverage or extreme fraction new features. Therefore, this more bullish, underconservative
uncertainty quantification can be useful on datasets with low mutation rates, extreme label times,
or low sequecning depth. We have found that false discovery rates are still well controlled when using
this alternative uncertainty quantification strategy.
</p>
<p>With kdegs and their uncertainties estimated, replicate estimates are pooled and regularized. There are two key steps in this
downstream analysis. 1st, the uncertainty for each feature is used to fit a linear ln(uncertainty) vs. log10(read depth) trend,
and uncertainties for individual features are shrunk towards the regression line. The uncertainty for each feature is a combination of the
Fisher Information asymptotic uncertainty as well as the amount of variability seen between estimates. Regularization of uncertainty
estimates is performed using the analytic results of a Normal distribution likelihood with known mean and unknown variance and conjugate
priors. The prior parameters are estimated from the regression and amount of variability about the regression line. The strength of
regularization can be tuned by adjusting the <code>prior_weight</code> parameter, with larger numbers yielding stronger shrinkage towards
the regression line. The 2nd step is to regularize the average kdeg estimates. This is done using the analytic results of a
Normal distribution likelihood model with unknown mean and known variance and conjugate priors. The prior parameters are estimated from the
population wide kdeg distribution (using its mean and standard deviation as the mean and standard deviation of the normal prior).
In the 1st step, the known mean is assumed to be the average kdeg, averaged across replicates and weighted by the number of reads
mapping to the feature in each replicate. In the 2nd step, the known variance is assumed to be that obtained following regularization
of the uncertainty estimates.
</p>
<p>Effect sizes (changes in kdeg) are obtained as the difference in log(kdeg) means between the reference and experimental
sample(s), and the log(kdeg)s are assumed to be independent so that the variance of the effect size is the sum of the
log(kdeg) variances. P-values assessing the significance of the effect size are obtained using a moderated t-test with number
of degrees of freedom determined from the uncertainty regression hyperparameters and are adjusted for multiple testing using the Benjamini-
Hochberg procedure to control false discovery rates (FDRs).
</p>
<p>In some cases, the assumed ODE model of RNA metabolism will not accurately model the dynamics of a biological system being analyzed.
In these cases, it is best to compare logit(fraction new)s directly rather than converting fraction new to log(kdeg).
This analysis strategy is implemented when <code>NSS</code> is set to TRUE. Comparing logit(fraction new) is only valid
If a single metabolic label time has been used for all samples. For example, if a label time of 1 hour was used for NR-seq
data from WT cells and a 2 hour label time was used in KO cells, this comparison is no longer valid as differences in
logit(fraction new) could stem from differences in kinetics or label times.
</p>


<h3>Value</h3>

<p>List with dataframes providing information about replicate-specific and pooled analysis results. The output includes:
</p>

<ul>
<li><p> Fn_Estimates; dataframe with estimates for the fraction new and fraction new uncertainty for each feature in each replicate.
The columns of this dataframe are:
</p>

<ul>
<li><p> Feature_ID; Numerical ID of feature
</p>
</li>
<li><p> Exp_ID; Numerical ID for experimental condition (Exp_ID from metadf)
</p>
</li>
<li><p> Replicate; Numerical ID for replicate
</p>
</li>
<li><p> logit_fn; logit(fraction new) estimate, unregularized
</p>
</li>
<li><p> logit_fn_se; logit(fraction new) uncertainty, unregularized and obtained from Fisher Information
</p>
</li>
<li><p> nreads; Number of reads mapping to the feature in the sample for which the estimates were obtained
</p>
</li>
<li><p> log_kdeg; log of degradation rate constant (kdeg) estimate, unregularized
</p>
</li>
<li><p> kdeg; degradation rate constant (kdeg) estimate
</p>
</li>
<li><p> log_kd_se; log(kdeg) uncertainty, unregularized and obtained from Fisher Information
</p>
</li>
<li><p> sample; Sample name
</p>
</li>
<li><p> XF; Original feature name
</p>
</li></ul>

</li>
<li><p> Regularized_ests; dataframe with average fraction new and kdeg estimates, averaged across the replicates and regularized
using priors informed by the entire dataset. The columns of this dataframe are:
</p>

<ul>
<li><p> Feature_ID; Numerical ID of feature
</p>
</li>
<li><p> Exp_ID; Numerical ID for experimental condition (Exp_ID from metadf)
</p>
</li>
<li><p> avg_log_kdeg; Weighted average of log(kdeg) from each replicate, weighted by sample and feature-specific read depth
</p>
</li>
<li><p> sd_log_kdeg; Standard deviation of the log(kdeg) estimates
</p>
</li>
<li><p> nreads; Total number of reads mapping to the feature in that condition
</p>
</li>
<li><p> sdp; Prior standard deviation for fraction new estimate regularization
</p>
</li>
<li><p> theta_o; Prior mean for fraction new estimate regularization
</p>
</li>
<li><p> sd_post; Posterior uncertainty
</p>
</li>
<li><p> log_kdeg_post; Posterior mean for log(kdeg) estimate
</p>
</li>
<li><p> kdeg; exp(log_kdeg_post)
</p>
</li>
<li><p> kdeg_sd; kdeg uncertainty
</p>
</li>
<li><p> XF; Original feature name
</p>
</li></ul>

</li>
<li><p> Effects_df; dataframe with estimates of the effect size (change in logit(fn)) comparing each experimental condition to the
reference sample for each feature. This dataframe also includes p-values obtained from a moderated t-test. The columns of this
dataframe are:
</p>

<ul>
<li><p> Feature_ID; Numerical ID of feature
</p>
</li>
<li><p> Exp_ID; Numerical ID for experimental condition (Exp_ID from metadf)
</p>
</li>
<li><p> L2FC(kdeg); Log2 fold change (L2FC) kdeg estimate or change in logit(fn) if NSS TRUE
</p>
</li>
<li><p> effect; LFC(kdeg)
</p>
</li>
<li><p> se; Uncertainty in L2FC_kdeg
</p>
</li>
<li><p> pval; P-value obtained using effect_size, se, and a z-test
</p>
</li>
<li><p> padj; pval adjusted for multiple testing using Benjamini-Hochberg procedure
</p>
</li>
<li><p> XF; Original feature name
</p>
</li></ul>

</li>
<li><p> Mut_rates; list of two elements. The 1st element is a dataframe of s4U induced mutation rate estimates, where the mut column
represents the experimental ID and the rep column represents the replicate ID. The 2nd element is the single background mutation
rate estimate used
</p>
</li>
<li><p> Hyper_Parameters; vector of two elements, named a and b. These are the hyperparameters estimated from the uncertainties for each
feature, and represent the two parameters of a Scaled Inverse Chi-Square distribution. Importantly, a is the number of additional
degrees of freedom provided by the sharing of uncertainty information across the dataset, to be used in the moderated t-test.
</p>
</li>
<li><p> Mean_Variance_lms; linear model objects obtained from the uncertainty vs. read count regression model. One model is run for each Exp_ID
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>

# Simulate small dataset
sim &lt;- Simulate_bakRData(300, nreps = 2)

# Fit fast model to get fast_df
Fit &lt;- bakRFit(sim$bakRData)

# Fit fast model with fast_analysis
Fast_Fit &lt;- fast_analysis(Fit$Data_lists$Fast_df)

</code></pre>

<hr>
<h2 id='fn_process'>Curate data in bakRFnData object for statistical modeling</h2><span id='topic+fn_process'></span>

<h3>Description</h3>

<p><code>fn_process</code> creates the data structures necessary to analyze nucleotide recoding RNA-seq data with the
MLE and Hybrid implementations in <code>bakRFit</code>. The input to <code>fn_process</code> must be an object of class
<code>bakRFnData</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fn_process(
  obj,
  totcut = 50,
  totcut_all = 10,
  Chase = FALSE,
  FOI = c(),
  concat = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="fn_process_+3A_obj">obj</code></td>
<td>
<p>An object of class bakRFnData</p>
</td></tr>
<tr><td><code id="fn_process_+3A_totcut">totcut</code></td>
<td>
<p>Numeric; Any transcripts with less than this number of sequencing reads in any replicate of all experimental conditions are filtered out</p>
</td></tr>
<tr><td><code id="fn_process_+3A_totcut_all">totcut_all</code></td>
<td>
<p>Numeric; Any transcripts with less than this number of sequencing reads in any sample are filtered out</p>
</td></tr>
<tr><td><code id="fn_process_+3A_chase">Chase</code></td>
<td>
<p>Boolean; if TRUE, pulse-chase analysis strategy is implemented</p>
</td></tr>
<tr><td><code id="fn_process_+3A_foi">FOI</code></td>
<td>
<p>Features of interest; character vector containing names of features to analyze. If <code>FOI</code> is non-null and <code>concat</code> is TRUE, then
all minimally reliable FOIs will be combined with reliable features passing all set filters (<code>totcut</code> and <code>totcut_all</code>).
If <code>concat</code> is FALSE, only the minimally reliable FOIs will be kept. A minimally reliable FOI is one that passes
filtering with minimally stringent parameters.</p>
</td></tr>
<tr><td><code id="fn_process_+3A_concat">concat</code></td>
<td>
<p>Boolean; If TRUE, FOI is concatenated with output of reliableFeatures</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>fn_process</code> first filters out features with less than totcut reads in any sample. It then
creates the necessary data structures for analysis with <code>bakRFit</code> and some of the visualization
functions (namely <code>plotMA</code>).
</p>
<p>The 1st step executed by <code>fn_process</code> is to find the names of features which are deemed &quot;reliable&quot;. A reliable feature is one with
sufficient read coverage in every single sample (i.e., &gt; totcut_all reads in all samples) and sufficient read coverage in at all replicates
of at least one experimental condition (i.e., &gt; totcut reads in all replicates for one or more experimental conditions). This is done with a call to <code>reliableFeatures</code>.
</p>
<p>The 2nd step is to extract only reliableFeatures from the fns dataframe in the <code>bakRFnData</code> object. During this process, a numerical
ID is given to each reliableFeature, with the numerical ID corresponding to their order when arranged using <code>dplyr::arrange</code>.
</p>
<p>The 3rd step is to prepare data structures that can be passed to <code>fast_analysis</code> and <code>TL_stan</code> (usually accessed via the
<code>bakRFit</code> helper function).
</p>


<h3>Value</h3>

<p>returns list of objects that can be passed to <code>TL_stan</code> and/or <code>fast_analysis</code>. Those objects are:
</p>

<ul>
<li><p> Stan_data; list that can be passed to <code>TL_stan</code> with Hybrid_Fit = TRUE. Consists of metadata as well as data that
<code>Stan</code> will analyze. Data to be analyzed consists of equal length vectors. The contents of Stan_data are:
</p>

<ul>
<li><p> NE; Number of datapoints for 'Stan' to analyze (NE = Number of Elements)
</p>
</li>
<li><p> NF; Number of features in dataset
</p>
</li>
<li><p> TP; Numerical indicator of s4U feed (0 = no s4U feed, 1 = s4U fed)
</p>
</li>
<li><p> FE; Numerical indicator of feature
</p>
</li>
<li><p> num_mut; Number of U-to-C mutations observed in a particular set of reads
</p>
</li>
<li><p> MT; Numerical indicator of experimental condition (Exp_ID from metadf)
</p>
</li>
<li><p> nMT; Number of experimental conditions
</p>
</li>
<li><p> R; Numerical indicator of replicate
</p>
</li>
<li><p> nrep; Number of replicates (maximum across experimental conditions)
</p>
</li>
<li><p> nrep_vect; Vector of number of replicates in each experimental condition
</p>
</li>
<li><p> tl; Vector of label times for each experimental condition
</p>
</li>
<li><p> Avg_Reads; Standardized log10(average read counts) for a particular feature in a particular condition, averaged over
replicates
</p>
</li>
<li><p> sdf; Dataframe that maps numerical feature ID to original feature name. Also has read depth information
</p>
</li>
<li><p> sample_lookup; Lookup table relating MT and R to the original sample name
</p>
</li></ul>

</li>
<li><p> Fn_est; A data frame containing fraction new estimates for +s4U samples:
</p>

<ul>
<li><p> sample; Original sample name
</p>
</li>
<li><p> XF; Original feature name
</p>
</li>
<li><p> fn; Fraction new estimate
</p>
</li>
<li><p> n; Number of reads
</p>
</li>
<li><p> Feature_ID; Numerical ID for each feature
</p>
</li>
<li><p> Replicate; Numerical ID for each replicate
</p>
</li>
<li><p> Exp_ID; Numerical ID for each experimental condition
</p>
</li>
<li><p> tl; s4U label time
</p>
</li>
<li><p> logit_fn; logit of fraction new estimate
</p>
</li>
<li><p> kdeg; degradation rate constant estimate
</p>
</li>
<li><p> log_kdeg; log of degradation rate constant estimate
</p>
</li>
<li><p> logit_fn_se; Uncertainty of logit(fraction new) estimate
</p>
</li>
<li><p> log_kd_se; Uncertainty of log(kdeg) estimate
</p>
</li></ul>

</li>
<li><p> Count_Matrix; A matrix with read count information. Each column represents a sample and each row represents a feature.
Each entry is the raw number of read counts mapping to a particular feature in a particular sample. Column names are the corresponding
sample names and row names are the corresponding feature names.
</p>
</li>
<li><p> Ctl_data; Identical content to Fn_est but for any -s4U data (and thus with fn estimates set to 0). Will be <code>NULL</code> if no -s4U
data is present
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>

# Load cB
data("cB_small")

# Load metadf
data("metadf")

# Create bakRData
bakRData &lt;- bakRData(cB_small, metadf)

# Preprocess data
data_for_bakR &lt;- cBprocess(obj = bakRData)

</code></pre>

<hr>
<h2 id='FnPCA'>Creating PCA plots with logit(fn) estimates</h2><span id='topic+FnPCA'></span>

<h3>Description</h3>

<p>This function creates a 2-component PCA plot using logit(fn) estimates.
<code>FnPCA</code> has been deprecated in favor of <code>FnPCA2</code>. The latter accepts a full
bakRFit as input and handles imbalanced replicates.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>FnPCA(obj, log_kdeg = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="FnPCA_+3A_obj">obj</code></td>
<td>
<p>Object contained within output of <code>bakRFit</code>. So, either Fast_Fit (MLE implementation fit),
Stan_Fit (MCMC implementation fit), or Hybrid_Fit (Hybrid implementation fit)</p>
</td></tr>
<tr><td><code id="FnPCA_+3A_log_kdeg">log_kdeg</code></td>
<td>
<p>Boolean; if TRUE, then log(kdeg) estimates used for PCA rather than logit(fn). Currently
only compatible with Fast_Fit</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A ggplot object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Simulate data for 500 genes and 2 replicates
sim &lt;- Simulate_bakRData(500, nreps = 2)

# Fit data with fast implementation
Fit &lt;- bakRFit(sim$bakRData)

# Fn PCA
FnPCA2(Fit, Model = "MLE")

# log(kdeg) PCA
FnPCA2(Fit, Model = "MLE", log_kdeg = TRUE)


</code></pre>

<hr>
<h2 id='FnPCA2'>Creating PCA plots with logit(fn) estimates</h2><span id='topic+FnPCA2'></span>

<h3>Description</h3>

<p>This function creates a 2-component PCA plot using logit(fn) or log(kdeg) estimates.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>FnPCA2(obj, Model = c("MLE", "Hybrid", "MCMC"), log_kdeg = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="FnPCA2_+3A_obj">obj</code></td>
<td>
<p>bakRFit object</p>
</td></tr>
<tr><td><code id="FnPCA2_+3A_model">Model</code></td>
<td>
<p>String identifying implementation for which you want to generate a PCA plot</p>
</td></tr>
<tr><td><code id="FnPCA2_+3A_log_kdeg">log_kdeg</code></td>
<td>
<p>Boolean; if TRUE, then log(kdeg) estimates used for PCA rather than logit(fn). Currently
only compatible with MLE implementation</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A ggplot object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Simulate data for 500 genes and 2 replicates
sim &lt;- Simulate_bakRData(500, nreps = 2)

# Fit data with fast implementation
Fit &lt;- bakRFit(sim$bakRData)

# Fn PCA
FnPCA2(Fit, Model = "MLE")

# log(kdeg) PCA
FnPCA2(Fit, Model = "MLE", log_kdeg = TRUE)


</code></pre>

<hr>
<h2 id='fns'>Example fraction news (fns) data frame</h2><span id='topic+fns'></span>

<h3>Description</h3>

<p>Subset of fraction new estimates for dataset published by Luo et al. (2020).
Fraction new estimates, uncertainties, and read counts are included for 300
genes to keep the file size small.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(fns)
</code></pre>


<h3>Format</h3>

<p>A dataframe with 1,800 rows and 5 variables. Input to <code>bakRFndata</code>
</p>

<dl>
<dt>sample</dt><dd><p>Sample name</p>
</dd>
<dt>XF</dt><dd><p>Name of feature (e.g., ENSEMBL gene ID)</p>
</dd>
<dt>fn</dt><dd><p>Estimate of fraction of reads from feature that were new</p>
</dd>
<dt>se</dt><dd><p>Uncertainty in fraction new estimate (optional in bakRFnData)</p>
</dd>
<dt>n</dt><dd><p>Number of sequencing reads</p>
</dd>
</dl>



<h3>References</h3>

<p>Luo et al. (2020) Biochemistry. 59(42), 4121-4142
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(fns)
data(metadf)
bakRFndataobj &lt;- bakRFnData(fns, metadf)
</code></pre>

<hr>
<h2 id='GS_table'>Example cB data frame</h2><span id='topic+GS_table'></span>

<h3>Description</h3>

<p>Subset of a GRAND-SLAM main output table from anlaysis of a dataset published in Luo et al. 2020.
Data for 300 randomly selected genes is included to keep file size small.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(GS_table)
</code></pre>


<h3>Format</h3>

<p>A dataframe with 300 rows and 63 variables; each row corresponds to GRAND-SLAM parameter
estimates for a single gene and 6 different samples (4 +s4U and 2 -s4U). Description of
all columns can be found on <a href="https://github.com/erhard-lab/gedi/wiki/GRAND-SLAM">GRAND-SLAM wiki</a>
</p>


<h3>References</h3>

<p>Luo et al. (2020) Biochemistry. 59(42), 4121-4142
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(GS_table)
data(metadf)
fns &lt;- GSprocessing(GS_table)
bdfo &lt;- bakRFnData(fns, metadf)
</code></pre>

<hr>
<h2 id='GSprocessing'>Prep GRAND-SLAM output for <code>bakRFnData</code></h2><span id='topic+GSprocessing'></span>

<h3>Description</h3>

<p>This function creates a fraction new estimate data frame that can be passed to
<code>bakRFnData</code>, using main .tsv file output by GRAND-SLAM.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>GSprocessing(GS, use_symbol = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="GSprocessing_+3A_gs">GS</code></td>
<td>
<p>Table of read counts and NTR (fraction new) estimate parameters output by GRAND-SLAM. Corresponds
to the <em>run_name</em>.tsv file included in GRAND-SLAM output</p>
</td></tr>
<tr><td><code id="GSprocessing_+3A_use_symbol">use_symbol</code></td>
<td>
<p>Logical; if TRUE, then Symbol column rather than Gene column is used
as feature column (XF) in output data frame.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A data frame that can be passed as the <code>fns</code> parameter to <code>bakRFnData</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Load GRAND-SLAM table
data("GS_table")


# Create bakRData object
fns &lt;- GSprocessing(GS_table)

</code></pre>

<hr>
<h2 id='Heatmap_kdeg'>Creating a L2FC(kdeg) matrix that can be passed to heatmap functions</h2><span id='topic+Heatmap_kdeg'></span>

<h3>Description</h3>

<p><code>Heatmap_kdeg</code> creates a matrix where each column represents a pair of samples (reference and experimental) and each
row represents a feature. The entry in the ith row and jth column is the L2FC(kdeg) for feature i when comparing sample with
experimental ID j+1 to the reference sample
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Heatmap_kdeg(obj, zscore = FALSE, filter_sig = FALSE, FDR = 0.05)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Heatmap_kdeg_+3A_obj">obj</code></td>
<td>
<p>Object outputted by <code>bakRFit</code></p>
</td></tr>
<tr><td><code id="Heatmap_kdeg_+3A_zscore">zscore</code></td>
<td>
<p>Logical; if TRUE, then each matrix entry is log-odds fold change in the fraction new (a.k.a the effect size) divided by
the uncertainty in the effect size</p>
</td></tr>
<tr><td><code id="Heatmap_kdeg_+3A_filter_sig">filter_sig</code></td>
<td>
<p>Logical; if TRUE, then only features which have a statistically significant L2FC(kdeg) in at least one comparison
are kept</p>
</td></tr>
<tr><td><code id="Heatmap_kdeg_+3A_fdr">FDR</code></td>
<td>
<p>Numeric; False discovery to control at if filter_sig is TRUE.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A matrix. Rows represent transcripts which were differentially expressed
and columns represent (from left to right) differential kinetics z-score,
differential expression z-score, and a mechanism score where positive represents
synthesis driven and negative degradation driven changes in expression.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Simulate data
sim &lt;- Simulate_bakRData(1000)

# Fit data with fast implementation
Fit &lt;- bakRFit(sim$bakRData)

# L2FC(kdeg) heatmap matrix
L2FC_kdeg_heat &lt;- Heatmap_kdeg(Fit$Fast_Fit)


</code></pre>

<hr>
<h2 id='metadf'>Example meatdf data frame</h2><span id='topic+metadf'></span>

<h3>Description</h3>

<p>metadf dataframe describing the data present in the cB file that
can be loaded with <code>data(cB_small)</code>. The contents are discussed
in great detail in the Getting_started vignette.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>data(metadf)
</code></pre>


<h3>Format</h3>

<p>A dataframe with 6 rows and 2 variables: row names are samples in the corresponding
cB
</p>

<dl>
<dt>tl</dt><dd><p>time of s4U labeling, in hours</p>
</dd>
<dt>Exp_ID</dt><dd><p>numerical ID of reference and experimental conditions; 1 is reference and 2 is the single experimental condition</p>
</dd>
</dl>



<h3>Examples</h3>

<pre><code class='language-R'>data(cB_small)
data(metadf)
bakRdat &lt;- bakRData(cB_small, metadf)
</code></pre>

<hr>
<h2 id='new_bakRData'>bakRData object constructor for internal use</h2><span id='topic+new_bakRData'></span>

<h3>Description</h3>

<p>This function efficiently creates an object of class bakRData
without performing rigorous checks
</p>


<h3>Usage</h3>

<pre><code class='language-R'>new_bakRData(cB, metadf)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="new_bakRData_+3A_cb">cB</code></td>
<td>
<p>Dataframe with columns corresponding to feature ID, number of Ts, number of mutations, sample ID, and number of identical observations</p>
</td></tr>
<tr><td><code id="new_bakRData_+3A_metadf">metadf</code></td>
<td>
<p>Dataframe detailing s4U label time and experimental ID of each sample</p>
</td></tr>
</table>

<hr>
<h2 id='new_bakRFnData'>bakRFnData object constructor for internal use</h2><span id='topic+new_bakRFnData'></span>

<h3>Description</h3>

<p>This function efficiently creates an object of class bakRFnData
without performing rigorous checks
</p>


<h3>Usage</h3>

<pre><code class='language-R'>new_bakRFnData(fns, metadf)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="new_bakRFnData_+3A_fns">fns</code></td>
<td>
<p>Dataframe with columns corresponding to sample names (sample), feature IDs (XF),
fraction new estimates (fn), and number of sequencing reads (nreads)</p>
</td></tr>
<tr><td><code id="new_bakRFnData_+3A_metadf">metadf</code></td>
<td>
<p>Dataframe detailing s4U label time and experimental ID of each sample</p>
</td></tr>
</table>

<hr>
<h2 id='NSSHeat'>Construct heatmap for non-steady state (NSS) analysis</h2><span id='topic+NSSHeat'></span>

<h3>Description</h3>

<p>This uses the output of bakR and a differential expression analysis software to construct
a dataframe that can be passed to pheatmap::pheatmap(). This heatmap will display the
result of a steady-state quasi-independent analysis of NR-seq data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>NSSHeat(
  bakRFit,
  DE_df,
  bakRModel = c("MLE", "Hybrid", "MCMC"),
  DE_cutoff = 0.05,
  bakR_cutoff = 0.05,
  Exp_ID = 2,
  lid = 4
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="NSSHeat_+3A_bakrfit">bakRFit</code></td>
<td>
<p>bakRFit object</p>
</td></tr>
<tr><td><code id="NSSHeat_+3A_de_df">DE_df</code></td>
<td>
<p>dataframe of required format with differential expression analysis results. See
Further-Analyses vignette for details on what this dataframe should look like</p>
</td></tr>
<tr><td><code id="NSSHeat_+3A_bakrmodel">bakRModel</code></td>
<td>
<p>Model fit from which bakR implementation should be used? Options are MLE, Hybrid,
or MCMC</p>
</td></tr>
<tr><td><code id="NSSHeat_+3A_de_cutoff">DE_cutoff</code></td>
<td>
<p>padj cutoff for calling a gene differentially expressed</p>
</td></tr>
<tr><td><code id="NSSHeat_+3A_bakr_cutoff">bakR_cutoff</code></td>
<td>
<p>padj cutoff for calling a fraction new significantly changed</p>
</td></tr>
<tr><td><code id="NSSHeat_+3A_exp_id">Exp_ID</code></td>
<td>
<p>Exp_ID of experimental sample whose comparison to the reference sample you want to use.
Only one reference vs. experimental sample comparison can be used at a time</p>
</td></tr>
<tr><td><code id="NSSHeat_+3A_lid">lid</code></td>
<td>
<p>Maximum absolute value for standardized score present in output. This is for improving
aesthetics of any heatmap generated with the output.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>returns data frame that can be passed to pheatmap::pheatmap()
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Simulate small dataset
sim &lt;- Simulate_bakRData(100, nreps = 2)

# Analyze data with bakRFit
Fit &lt;- bakRFit(sim$bakRData)

# Number of features that made it past filtering
NF &lt;- nrow(Fit$Fast_Fit$Effects_df)

# Simulate mock differential expression data frame
DE_df &lt;- data.frame(XF = as.character(1:NF),
                       L2FC_RNA = stats::rnorm(NF, 0, 2))

DE_df$DE_score &lt;- DE_df$L2FC_RNA/0.5
DE_df$DE_se &lt;- 0.5

DE_df$DE_pval &lt;- 2*stats::dnorm(-abs(DE_df$DE_score))
DE_df$DE_padj &lt;- 2*stats::p.adjust(DE_df$DE_pval, method = "BH")

# perform NSS analysis
NSS_analysis &lt;- DissectMechanism(bakRFit = Fit,
               DE_df = DE_df,
               bakRModel = "MLE")


</code></pre>

<hr>
<h2 id='plotMA'>Creating L2FC(kdeg) MA plot from fit objects</h2><span id='topic+plotMA'></span>

<h3>Description</h3>

<p>This function outputs a L2FC(kdeg) MA plot. Plots are colored according to statistical
significance and the sign of L2FC(kdeg)
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotMA(
  obj,
  Model = c("MLE", "Hybrid", "MCMC"),
  FDR = 0.05,
  Exps = 2,
  Exp_shape = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plotMA_+3A_obj">obj</code></td>
<td>
<p>Object of class bakRFit outputted by <code>bakRFit</code> function</p>
</td></tr>
<tr><td><code id="plotMA_+3A_model">Model</code></td>
<td>
<p>String identifying implementation for which you want to generate an MA plot</p>
</td></tr>
<tr><td><code id="plotMA_+3A_fdr">FDR</code></td>
<td>
<p>False discovery rate to control at for significance assessment</p>
</td></tr>
<tr><td><code id="plotMA_+3A_exps">Exps</code></td>
<td>
<p>Vector of Experimental IDs to include in plot; must only contain elements within 2:(# of experimental IDs).
If NULL, data for all Experimental IDs is plotted.</p>
</td></tr>
<tr><td><code id="plotMA_+3A_exp_shape">Exp_shape</code></td>
<td>
<p>Logical indicating whether to use Experimental ID as factor determining point shape in volcano plot</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A ggplot object. Each point represents a transcript. The
x-axis is log-10 transformed replicate average read counts,
y-axis is the log-2 fold-change in the degradation rate constant.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Simulate data for 500 genes and 2 replicates
sim &lt;- Simulate_bakRData(500, nreps = 2)

# Fit data with fast implementation
Fit &lt;- bakRFit(sim$bakRData)

# Volcano plot
plotMA(Fit, Model = "MLE")


</code></pre>

<hr>
<h2 id='plotVolcano'>Creating L2FC(kdeg) volcano plot from fit objects</h2><span id='topic+plotVolcano'></span>

<h3>Description</h3>

<p>This function creates a L2FC(kdeg) volcano plot.
Plots are colored according to statistical significance and sign of L2FC(kdeg).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotVolcano(obj, FDR = 0.05, Exps = 2, Exp_shape = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plotVolcano_+3A_obj">obj</code></td>
<td>
<p>Object contained within output of <code>bakRFit</code>. So, either Fast_Fit (MLE implementation fit),
Stan_Fit (MCMC implementation fit), or Hybrid_Fit (Hybrid implementation fit)</p>
</td></tr>
<tr><td><code id="plotVolcano_+3A_fdr">FDR</code></td>
<td>
<p>False discovery rate to control at for significance assessment</p>
</td></tr>
<tr><td><code id="plotVolcano_+3A_exps">Exps</code></td>
<td>
<p>Vector of Experimental IDs to include in plot; must only contain elements within 2:(# of experimental IDs).
If NULL, data for all Experimental IDs is plotted.</p>
</td></tr>
<tr><td><code id="plotVolcano_+3A_exp_shape">Exp_shape</code></td>
<td>
<p>Logical indicating whether to use Experimental ID as factor determining point shape in volcano plot</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A ggplot object. Each point represents a transcript. The x-axis is the
log-2 fold change in the degradation rate constant and the y-axis is the log-10
transformed multiple test adjusted p value.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Simulate data for 500 genes and 2 replicates
sim &lt;- Simulate_bakRData(500, nreps = 2)

# Fit data with fast implementation
Fit &lt;- bakRFit(sim$bakRData)

# Volcano plot
plotVolcano(Fit$Fast_Fit)


</code></pre>

<hr>
<h2 id='QC_checks'>Check data quality and make suggestions to user about what analyses to run.</h2><span id='topic+QC_checks'></span>

<h3>Description</h3>

<p><code>QC_checks</code> takes as input a <code>bakRFit</code> or <code>bakRFnFit</code> object and uses the Fast_Fit object to assess
data quality and make suggestions about which implementation to run next. QC_checks
takes into account the mutation rates in all samples, the fraction new distributions, the reproducibility
of fraction new estimates, and the read lengths. It then outputs a number of
diagnostic plots that might alert users to problems in their data. It also
outputs messages informing users what implementation is best used next.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>QC_checks(obj)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="QC_checks_+3A_obj">obj</code></td>
<td>
<p>bakRFit object</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with 3 components:
</p>

<ul>
<li><p> raw_mutrates. This is a plot of the raw T-to-C mutation rates in all samples
analyzed by bakR. It includes horizontal lines as reference for what could be
considered &quot;too low&quot; to be useful in s4U fed samples.
</p>
</li>
<li><p> conversion_rates. This is a plot of the estimated T-to-C mutation rates
in new and old reads. Thus, each bar represents the probability that a U in
a new/old read is mutated. It includes horizontal lines as reference for what could
be considered good mutation rates.
</p>
</li>
<li><p> correlation_plots. This is a list of ggplot objects. Each is a scatter plot
comparing estimates of the fraction new in one replicate to another replicate
in the same experimental condition. A y=x guide line is included to reveal any
estimation biases.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>
# Simulate data for 500 genes and 2 replicates
sim &lt;- Simulate_bakRData(500, nreps = 2)

# Fit data with fast implementation
Fit &lt;- bakRFit(sim$bakRData)

# Run QC
QC &lt;- QC_checks(Fit)


</code></pre>

<hr>
<h2 id='QuantifyDropout'>Fit dropout model to quantify dropout frequency</h2><span id='topic+QuantifyDropout'></span>

<h3>Description</h3>

<p><code>QuantifyDropout</code> estimates the percentage of 4-thiouridine containing RNA
that was lost during library preparation (pdo).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>QuantifyDropout(
  obj,
  scale_init = 1.05,
  pdo_init = 0.3,
  keep_data = FALSE,
  no_message = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="QuantifyDropout_+3A_obj">obj</code></td>
<td>
<p>bakRFit object</p>
</td></tr>
<tr><td><code id="QuantifyDropout_+3A_scale_init">scale_init</code></td>
<td>
<p>Numeric; initial estimate for -s4U/+s4U scale factor. This is the factor
difference in RPM normalized read counts for completely unlabeled transcripts (i.e., highly stable
transcript) between the +s4U and -s4U samples.</p>
</td></tr>
<tr><td><code id="QuantifyDropout_+3A_pdo_init">pdo_init</code></td>
<td>
<p>Numeric; initial estimtae for the dropout rate. This is the probability
that an s4U labeled RNA molecule is lost during library prepartion.</p>
</td></tr>
<tr><td><code id="QuantifyDropout_+3A_keep_data">keep_data</code></td>
<td>
<p>Logical; if TRUE, will return list with two elements. First element
is the regular return (data frame with dropout quantified), and the second element
will be the data frame that was used for fitting the dropout model. This is useful
if wanting to visualize the fit. See Return documetation for more details</p>
</td></tr>
<tr><td><code id="QuantifyDropout_+3A_no_message">no_message</code></td>
<td>
<p>Logical; if TRUE, will not output message regarding estimated
rates of dropout in each sample</p>
</td></tr>
<tr><td><code id="QuantifyDropout_+3A_...">...</code></td>
<td>
<p>Additional (optional) parameters to be passed to <code>stats::nls()</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>If keep_data is FALSE, then only a data frame with the dropout rate estimates (pdo)
in each sample is returned. If keep_data is TRUE, then a list with two elements is returned. One element is
the pdo data frame always returned, and the second is the data frame containing information passed
to <code>stats::nls</code> for pdo estimation.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Simulate data for 500 genes and 2 replicates with 40% dropout
sim &lt;- Simulate_relative_bakRData(500, depth = 100000,
                                  nreps = 2, p_do = 0.4)

# Fit data with fast implementation
Fit &lt;- bakRFit(sim$bakRData)

# Quantify dropout
Fit &lt;- QuantifyDropout(Fit)


</code></pre>

<hr>
<h2 id='reliableFeatures'>Identify features (e.g., transcripts) with high quality data</h2><span id='topic+reliableFeatures'></span>

<h3>Description</h3>

<p>This function identifies all features (e.g., transcripts, exons, etc.) for which the mutation rate
is below a set threshold in the control (-s4U) sample and which have more reads than a set threshold
in all samples. If there is no -s4U sample, then only the read count cutoff is considered. Additional
filtering options are only relevant if working with short RNA-seq read data. This includes filtering out
features with extremely low empirical U-content (i.e., the average number of Us in sequencing reads from
that feature) and those with very few reads having at least 3 Us in them.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>reliableFeatures(
  obj,
  high_p = 0.2,
  totcut = 50,
  totcut_all = 10,
  Ucut = 0.25,
  AvgU = 4
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="reliableFeatures_+3A_obj">obj</code></td>
<td>
<p>Object of class bakRData</p>
</td></tr>
<tr><td><code id="reliableFeatures_+3A_high_p">high_p</code></td>
<td>
<p>highest mutation rate accepted in control samples</p>
</td></tr>
<tr><td><code id="reliableFeatures_+3A_totcut">totcut</code></td>
<td>
<p>Numeric; Any transcripts with less than this number of sequencing reads in any replicate of all experimental conditions are filtered out</p>
</td></tr>
<tr><td><code id="reliableFeatures_+3A_totcut_all">totcut_all</code></td>
<td>
<p>Numeric; Any transcripts with less than this number of sequencing reads in any sample are filtered out</p>
</td></tr>
<tr><td><code id="reliableFeatures_+3A_ucut">Ucut</code></td>
<td>
<p>Must have a fraction of reads with 2 or less Us less than this cutoff in all samples</p>
</td></tr>
<tr><td><code id="reliableFeatures_+3A_avgu">AvgU</code></td>
<td>
<p>Must have an average number of Us greater than this</p>
</td></tr>
</table>


<h3>Value</h3>

<p>vector of gene names that passed reliability filter
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

# Load cB
data("cB_small")

# Load metadf
data("metadf")

# Create bakRData
bakRData &lt;- bakRData(cB_small, metadf)

# Find reliable features
features_to_keep &lt;- reliableFeatures(obj = bakRData)

</code></pre>

<hr>
<h2 id='Simulate_bakRData'>Simulating nucleotide recoding data</h2><span id='topic+Simulate_bakRData'></span>

<h3>Description</h3>

<p><code>Simulate_bakRData</code> simulates a <code>bakRData</code> object. It's output also includes the simulated
values of all kinetic parameters of interest. Only the number of genes (<code>ngene</code>) has to be set by the
user, but an extensive list of additional parameters can be adjusted.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Simulate_bakRData(
  ngene,
  num_conds = 2L,
  nreps = 3L,
  eff_sd = 0.75,
  eff_mean = 0,
  fn_mean = 0,
  fn_sd = 1,
  kslog_c = 0.8,
  kslog_sd = 0.95,
  tl = 60,
  p_new = 0.05,
  p_old = 0.001,
  read_lengths = 200L,
  p_do = 0,
  noise_deg_a = -0.3,
  noise_deg_b = -1.5,
  noise_synth = 0.1,
  sd_rep = 0.05,
  low_L2FC_ks = -1,
  high_L2FC_ks = 1,
  num_kd_DE = c(0L, as.integer(rep(round(as.integer(ngene)/2), times =
    as.integer(num_conds) - 1))),
  num_ks_DE = rep(0L, times = as.integer(num_conds)),
  scale_factor = 150,
  sim_read_counts = TRUE,
  a1 = 5,
  a0 = 0.01,
  nreads = 50L,
  alpha = 25,
  beta = 75,
  STL = FALSE,
  STL_len = 40,
  lprob_U_sd = 0,
  lp_sd = 0
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Simulate_bakRData_+3A_ngene">ngene</code></td>
<td>
<p>Number of genes to simulate data for</p>
</td></tr>
<tr><td><code id="Simulate_bakRData_+3A_num_conds">num_conds</code></td>
<td>
<p>Number of experimental conditions (including the reference condition) to simulate</p>
</td></tr>
<tr><td><code id="Simulate_bakRData_+3A_nreps">nreps</code></td>
<td>
<p>Number of replicates to simulate</p>
</td></tr>
<tr><td><code id="Simulate_bakRData_+3A_eff_sd">eff_sd</code></td>
<td>
<p>Effect size; more specifically, the standard deviation of the normal distribution from which non-zero
changes in logit(fraction new) are pulled from.</p>
</td></tr>
<tr><td><code id="Simulate_bakRData_+3A_eff_mean">eff_mean</code></td>
<td>
<p>Effect size mean; mean of normal distribution from which non-zero changes in logit(fraction new) are pulled from.
Note, setting this to 0 does not mean that some of the significant effect sizes will be 0, as any exact integer is impossible
to draw from a continuous random number generator. Setting this to 0 just means that there is symmetric stabilization and destabilization</p>
</td></tr>
<tr><td><code id="Simulate_bakRData_+3A_fn_mean">fn_mean</code></td>
<td>
<p>Mean of fraction news of simulated transcripts in reference condition. The logit(fraction) of RNA from each transcript that is
metabolically labeled (new) is drawn from a normal distribution with this mean</p>
</td></tr>
<tr><td><code id="Simulate_bakRData_+3A_fn_sd">fn_sd</code></td>
<td>
<p>Standard deviation of fraction news of simulated transcripts in reference condition. The logit(fraction) of RNA
from each transcript that is metabolically labeled (new) is drawn from a normal distribution with this sd</p>
</td></tr>
<tr><td><code id="Simulate_bakRData_+3A_kslog_c">kslog_c</code></td>
<td>
<p>Synthesis rate constants will be drawn from a lognormal distribution with meanlog = <code>kslog_c</code> - mean(log(kd_mean)) where kd_mean
is determined from the fraction new simulated for each gene as well as the label time (<code>tl</code>).</p>
</td></tr>
<tr><td><code id="Simulate_bakRData_+3A_kslog_sd">kslog_sd</code></td>
<td>
<p>Synthesis rate lognormal standard deviation; see kslog_c documentation for details</p>
</td></tr>
<tr><td><code id="Simulate_bakRData_+3A_tl">tl</code></td>
<td>
<p>metabolic label feed time</p>
</td></tr>
<tr><td><code id="Simulate_bakRData_+3A_p_new">p_new</code></td>
<td>
<p>metabolic label (e.g., s4U) induced mutation rate. Can be a vector of length num_conds</p>
</td></tr>
<tr><td><code id="Simulate_bakRData_+3A_p_old">p_old</code></td>
<td>
<p>background mutation rate</p>
</td></tr>
<tr><td><code id="Simulate_bakRData_+3A_read_lengths">read_lengths</code></td>
<td>
<p>Total read length for each sequencing read (e.g., PE100 reads correspond to read_lengths = 200)</p>
</td></tr>
<tr><td><code id="Simulate_bakRData_+3A_p_do">p_do</code></td>
<td>
<p>Rate at which metabolic label containing reads are lost due to dropout; must be between 0 and 1</p>
</td></tr>
<tr><td><code id="Simulate_bakRData_+3A_noise_deg_a">noise_deg_a</code></td>
<td>
<p>Slope of trend relating log10(standardized read counts) to log(replicate variability)</p>
</td></tr>
<tr><td><code id="Simulate_bakRData_+3A_noise_deg_b">noise_deg_b</code></td>
<td>
<p>Intercept of trend relating log10(standardized read counts) to log(replicate variability)</p>
</td></tr>
<tr><td><code id="Simulate_bakRData_+3A_noise_synth">noise_synth</code></td>
<td>
<p>Homoskedastic variability of L2FC(ksyn)</p>
</td></tr>
<tr><td><code id="Simulate_bakRData_+3A_sd_rep">sd_rep</code></td>
<td>
<p>Variance of lognormal distribution from which replicate variability is drawn</p>
</td></tr>
<tr><td><code id="Simulate_bakRData_+3A_low_l2fc_ks">low_L2FC_ks</code></td>
<td>
<p>Most negative L2FC(ksyn) that can be simulated</p>
</td></tr>
<tr><td><code id="Simulate_bakRData_+3A_high_l2fc_ks">high_L2FC_ks</code></td>
<td>
<p>Most positive L2FC(ksyn) that can be simulated</p>
</td></tr>
<tr><td><code id="Simulate_bakRData_+3A_num_kd_de">num_kd_DE</code></td>
<td>
<p>Vector where each element represents the number of genes that show a significant change in stability relative
to the reference. 1st entry must be 0 by definition (since relative to the reference the reference sample is unchanged)</p>
</td></tr>
<tr><td><code id="Simulate_bakRData_+3A_num_ks_de">num_ks_DE</code></td>
<td>
<p>Same as num_kd_DE but for significant changes in synthesis rates.</p>
</td></tr>
<tr><td><code id="Simulate_bakRData_+3A_scale_factor">scale_factor</code></td>
<td>
<p>Factor relating RNA concentration (in arbitrary units) to average number of read counts</p>
</td></tr>
<tr><td><code id="Simulate_bakRData_+3A_sim_read_counts">sim_read_counts</code></td>
<td>
<p>Logical; if TRUE, read counts are simulated as coming from a heterodisperse negative binomial distribution</p>
</td></tr>
<tr><td><code id="Simulate_bakRData_+3A_a1">a1</code></td>
<td>
<p>Heterodispersion 1/reads dependence parameter</p>
</td></tr>
<tr><td><code id="Simulate_bakRData_+3A_a0">a0</code></td>
<td>
<p>High read depth limit of negative binomial dispersion parameter</p>
</td></tr>
<tr><td><code id="Simulate_bakRData_+3A_nreads">nreads</code></td>
<td>
<p>Number of reads simulated if sim_read_counts is FALSE</p>
</td></tr>
<tr><td><code id="Simulate_bakRData_+3A_alpha">alpha</code></td>
<td>
<p>shape1 parameter of the beta distribution from which U-contents (probability that a nucleotide in a read from a transcript is a U) are
drawn for each gene.</p>
</td></tr>
<tr><td><code id="Simulate_bakRData_+3A_beta">beta</code></td>
<td>
<p>shape2 parameter of the beta distribution from which U-contents (probability that a nucleotide in a read from a transcript is a U) are
drawn for each gene.</p>
</td></tr>
<tr><td><code id="Simulate_bakRData_+3A_stl">STL</code></td>
<td>
<p>logical; if TRUE, simulation is of STL-seq rather than a standard TL-seq experiment. The two big changes are that a short read length is required
(&lt; 60 nt) and that every read for a particular feature will have the same number of Us. Only one read length is simulated for simplicity.</p>
</td></tr>
<tr><td><code id="Simulate_bakRData_+3A_stl_len">STL_len</code></td>
<td>
<p>Average length of simulated STL-seq length. Since Pol II typically pauses about 20-60 bases
from the promoter, this should be around 40</p>
</td></tr>
<tr><td><code id="Simulate_bakRData_+3A_lprob_u_sd">lprob_U_sd</code></td>
<td>
<p>Standard deviation of the logit(probability nt is a U) for each sequencing read. The number of Us in a
sequencing read are drawn from a binomial distribution with prob drawn from a logit-Normal distribution with this logit-sd.</p>
</td></tr>
<tr><td><code id="Simulate_bakRData_+3A_lp_sd">lp_sd</code></td>
<td>
<p>Standard deviation of logit(probability a U is mutated) for each U. The number of mutations in a given read is the sum of
nU Bernoulli random variables, where nU is the number of Us, and p is drawn from a logit-normal distribution with lp_sd standard deviation
on logit scale.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>Simulate_bakRData</code> simulates a <code>bakRData</code> object using a realistic generative model with many
adjustable parameters. Average RNA kinetic parameters are drawn from biologically inspired
distributions. Replicate variability is simulated by drawing a feature's
fraction new in a given replicate from a logit-Normal distribution with a heteroskedastic
variance term with average magnitude given by the chosen read count vs. variance relationship.
For each replicate, a feature's ksyn is drawn from a homoskedastic lognormal distribution. Read counts
can either be set to the same value for all simulated features or can be simulated according to
a heterodisperse negative binomial distribution. The latter is the default
</p>
<p>The number of Us in each sequencing read is drawn from a binomial distribution with number of trials
equal to the read length and probability of each nucleotide being a U drawn from a beta distribution. Each read is assigned to the
new or old population according to a Bernoulli distribution with p = fraction new. The number of
mutations in each read are then drawn from one of two binomial distributions; if the read is assigned to the
population of new RNA, the number of mutations are drawn from a binomial distribution with number of trials equal
to the number of Us and probability of mutation = <code>p_new</code>; if the read is assigned to the population of old RNA,
the number of mutations is instead drawn from a binomial distribution with the same number of trials but with the probability
of mutation = <code>p_old</code>. <code>p_new</code> must be greater than <code>p_old</code> because mutations in new RNA
arise from both background mutations that occur with probability <code>p_old</code> as well as metabolic label induced mutations
</p>
<p>Simulated read counts should be treated as if they are spike-in and RPKM normalized, so the same scale factor of 1 can be applied
to each sample when comparing the sequencing reads (e.g., if you are performing differential expression analysis).
</p>
<p>Function to simulate a <code>bakRData</code> object according to a realistic generative model
</p>


<h3>Value</h3>

<p>A list containing a simulated <code>bakRData</code> object as well as a list of simulated kinetic parameters of interest.
The contents of the latter list are:
</p>

<ul>
<li><p> Effect_sim; Dataframe meant to mimic formatting of Effect_df that are part of <code>bakRFit(StanFit = TRUE)</code>, <code>bakRFit(HybridFit = TRUE)</code> and <code>bakRFit(bakRData object)</code> output.
</p>
</li>
<li><p> Fn_mean_sim; Dataframe meant to mimic formatting of Regularized_ests that is part of <code>bakRFit(bakRData object)</code> output. Contains information
about the true fraction new simulated in each condition (the mean of the normal distribution from which replicate fraction news are simulated)
</p>
</li>
<li><p> Fn_rep_sim; Dataframe meant to mimic formatting of Fn_Estimates that is part of <code>bakRFit(bakRData object)</code> output. Contains information
about the fraction new simulated for each feature in each replicate of each condition.
</p>
</li>
<li><p> L2FC_ks_mean; The true L2FC(ksyn) for each feature in each experimental condition. The i-th column corresponds to the L2FC(ksyn) when comparing
the i-th condition to the reference condition (defined as the 1st condition) so the 1st column is always all 0s
</p>
</li>
<li><p> RNA_conc; The average number of normalized read counts expected for each feature in each sample.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>
# 2 replicate, 2 experimental condition, 1000 gene simulation
sim_2reps &lt;- Simulate_bakRData(ngene = 1000, nreps = 2)

# 3 replicate, 2 experimental condition, 1000 gene simulation
# with 100 instances of differential degradation kinetics
sim_3reps &lt;- Simulate_bakRData(ngene = 1000, num_kd_DE = c(0, 100))

# 2 replicates, 3 experimental condition, 1000 gene simulation
# with 100 instances of differential degradation kinetics in the 1st
# condition and no instances of differential degradation kinetics in the
# 2nd condition
sim_3es &lt;- Simulate_bakRData(ngene = 1000,
                             nreps = 2,
                             num_conds = 3,
                             num_kd_DE = c(0, 100, 0))


</code></pre>

<hr>
<h2 id='Simulate_relative_bakRData'>Simulating nucleotide recoding data with relative count data</h2><span id='topic+Simulate_relative_bakRData'></span>

<h3>Description</h3>

<p><code>Simulate_relative_bakRData</code> simulates a <code>bakRData</code> object. It's output also includes the simulated
values of all kinetic parameters of interest.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Simulate_relative_bakRData(
  ngene,
  depth,
  num_conds = 2L,
  nreps = 3L,
  eff_sd = 0.75,
  eff_mean = 0,
  kdlog_mean = -1.8,
  kdlog_sd = 0.65,
  kslog_mean = 1,
  kslog_sd = 0.65,
  tl = 2,
  p_new = 0.05,
  p_old = 0.001,
  read_lengths = 200L,
  p_do = 0,
  noise_deg_a = -0.3,
  noise_deg_b = -1.5,
  noise_synth = 0.1,
  sd_rep = 0.05,
  low_L2FC_ks = -1,
  high_L2FC_ks = 1,
  num_kd_DE = c(0L, as.integer(rep(round(as.integer(ngene)/2), times =
    as.integer(num_conds) - 1))),
  num_ks_DE = rep(0L, times = as.integer(num_conds)),
  sim_read_counts = TRUE,
  a1 = 5,
  a0 = 0.01,
  nreads = 50L,
  alpha = 25,
  beta = 75,
  STL = FALSE,
  STL_len = 40,
  lprob_U_sd = 0,
  lp_sd = 0
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Simulate_relative_bakRData_+3A_ngene">ngene</code></td>
<td>
<p>Number of genes to simulate data for</p>
</td></tr>
<tr><td><code id="Simulate_relative_bakRData_+3A_depth">depth</code></td>
<td>
<p>Total number of reads to simulate</p>
</td></tr>
<tr><td><code id="Simulate_relative_bakRData_+3A_num_conds">num_conds</code></td>
<td>
<p>Number of experimental conditions (including the reference condition) to simulate</p>
</td></tr>
<tr><td><code id="Simulate_relative_bakRData_+3A_nreps">nreps</code></td>
<td>
<p>Number of replicates to simulate</p>
</td></tr>
<tr><td><code id="Simulate_relative_bakRData_+3A_eff_sd">eff_sd</code></td>
<td>
<p>Effect size; more specifically, the standard deviation of the normal distribution from which non-zero
changes in logit(fraction new) are pulled from.</p>
</td></tr>
<tr><td><code id="Simulate_relative_bakRData_+3A_eff_mean">eff_mean</code></td>
<td>
<p>Effect size mean; mean of normal distribution from which non-zero changes in logit(fraction new) are pulled from.
Note, setting this to 0 does not mean that some of the significant effect sizes will be 0, as any exact integer is impossible
to draw from a continuous random number generator. Setting this to 0 just means that there is symmetric stabilization and destabilization</p>
</td></tr>
<tr><td><code id="Simulate_relative_bakRData_+3A_kdlog_mean">kdlog_mean</code></td>
<td>
<p>Degradation rate constants will be drawn from lognormal distribution with this logmean</p>
</td></tr>
<tr><td><code id="Simulate_relative_bakRData_+3A_kdlog_sd">kdlog_sd</code></td>
<td>
<p>Degradation rate constants will be drawn from lognormal distribution with this logsd</p>
</td></tr>
<tr><td><code id="Simulate_relative_bakRData_+3A_kslog_mean">kslog_mean</code></td>
<td>
<p>Synthesis rate constants will be drawn from a lognormal distribution with this mean</p>
</td></tr>
<tr><td><code id="Simulate_relative_bakRData_+3A_kslog_sd">kslog_sd</code></td>
<td>
<p>Synthesis rate constants will be drawn from a lognormal distribution with this logsd</p>
</td></tr>
<tr><td><code id="Simulate_relative_bakRData_+3A_tl">tl</code></td>
<td>
<p>metabolic label feed time</p>
</td></tr>
<tr><td><code id="Simulate_relative_bakRData_+3A_p_new">p_new</code></td>
<td>
<p>metabolic label (e.g., s4U) induced mutation rate. Can be a vector of length num_conds</p>
</td></tr>
<tr><td><code id="Simulate_relative_bakRData_+3A_p_old">p_old</code></td>
<td>
<p>background mutation rate</p>
</td></tr>
<tr><td><code id="Simulate_relative_bakRData_+3A_read_lengths">read_lengths</code></td>
<td>
<p>Total read length for each sequencing read (e.g., PE100 reads correspond to read_lengths = 200)</p>
</td></tr>
<tr><td><code id="Simulate_relative_bakRData_+3A_p_do">p_do</code></td>
<td>
<p>Rate at which metabolic label containing reads are lost due to dropout; must be between 0 and 1</p>
</td></tr>
<tr><td><code id="Simulate_relative_bakRData_+3A_noise_deg_a">noise_deg_a</code></td>
<td>
<p>Slope of trend relating log10(standardized read counts) to log(replicate variability)</p>
</td></tr>
<tr><td><code id="Simulate_relative_bakRData_+3A_noise_deg_b">noise_deg_b</code></td>
<td>
<p>Intercept of trend relating log10(standardized read counts) to log(replicate variability)</p>
</td></tr>
<tr><td><code id="Simulate_relative_bakRData_+3A_noise_synth">noise_synth</code></td>
<td>
<p>Homoskedastic variability of L2FC(ksyn)</p>
</td></tr>
<tr><td><code id="Simulate_relative_bakRData_+3A_sd_rep">sd_rep</code></td>
<td>
<p>Variance of lognormal distribution from which replicate variability is drawn</p>
</td></tr>
<tr><td><code id="Simulate_relative_bakRData_+3A_low_l2fc_ks">low_L2FC_ks</code></td>
<td>
<p>Most negative L2FC(ksyn) that can be simulated</p>
</td></tr>
<tr><td><code id="Simulate_relative_bakRData_+3A_high_l2fc_ks">high_L2FC_ks</code></td>
<td>
<p>Most positive L2FC(ksyn) that can be simulated</p>
</td></tr>
<tr><td><code id="Simulate_relative_bakRData_+3A_num_kd_de">num_kd_DE</code></td>
<td>
<p>Vector where each element represents the number of genes that show a significant change in stability relative
to the reference. 1st entry must be 0 by definition (since relative to the reference the reference sample is unchanged)</p>
</td></tr>
<tr><td><code id="Simulate_relative_bakRData_+3A_num_ks_de">num_ks_DE</code></td>
<td>
<p>Same as num_kd_DE but for significant changes in synthesis rates.</p>
</td></tr>
<tr><td><code id="Simulate_relative_bakRData_+3A_sim_read_counts">sim_read_counts</code></td>
<td>
<p>Logical; if TRUE, read counts are simulated as coming from a heterodisperse negative binomial distribution</p>
</td></tr>
<tr><td><code id="Simulate_relative_bakRData_+3A_a1">a1</code></td>
<td>
<p>Heterodispersion 1/reads dependence parameter</p>
</td></tr>
<tr><td><code id="Simulate_relative_bakRData_+3A_a0">a0</code></td>
<td>
<p>High read depth limit of negative binomial dispersion parameter</p>
</td></tr>
<tr><td><code id="Simulate_relative_bakRData_+3A_nreads">nreads</code></td>
<td>
<p>Number of reads simulated if sim_read_counts is FALSE</p>
</td></tr>
<tr><td><code id="Simulate_relative_bakRData_+3A_alpha">alpha</code></td>
<td>
<p>shape1 parameter of the beta distribution from which U-contents (probability that a nucleotide in a read from a transcript is a U) are
drawn for each gene.</p>
</td></tr>
<tr><td><code id="Simulate_relative_bakRData_+3A_beta">beta</code></td>
<td>
<p>shape2 parameter of the beta distribution from which U-contents (probability that a nucleotide in a read from a transcript is a U) are
drawn for each gene.</p>
</td></tr>
<tr><td><code id="Simulate_relative_bakRData_+3A_stl">STL</code></td>
<td>
<p>logical; if TRUE, simulation is of STL-seq rather than a standard TL-seq experiment. The two big changes are that a short read length is required
(&lt; 60 nt) and that every read for a particular feature will have the same number of Us. Only one read length is simulated for simplicity.</p>
</td></tr>
<tr><td><code id="Simulate_relative_bakRData_+3A_stl_len">STL_len</code></td>
<td>
<p>Average length of simulated STL-seq length. Since Pol II typically pauses about 20-60 bases
from the promoter, this should be around 40</p>
</td></tr>
<tr><td><code id="Simulate_relative_bakRData_+3A_lprob_u_sd">lprob_U_sd</code></td>
<td>
<p>Standard deviation of the logit(probability nt is a U) for each sequencing read. The number of Us in a
sequencing read are drawn from a binomial distribution with prob drawn from a logit-Normal distribution with this logit-sd.</p>
</td></tr>
<tr><td><code id="Simulate_relative_bakRData_+3A_lp_sd">lp_sd</code></td>
<td>
<p>Standard deviation of logit(probability a U is mutated) for each U. The number of mutations in a given read is the sum of
nU Bernoulli random variables, where nU is the number of Us, and p is drawn from a logit-normal distribution with lp_sd standard deviation
on logit scale.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The main difference between <code>Simulate_relative_bakRData</code>
and <code>Simulate_bakRData</code> is that the former requires both the number of
genes (<code>ngene</code>) and the total number of reads (<code>depth</code>) has to be set.
In the latter, only the number of genes is set, and the number of reads for each
gene is simulated so that no matter how many genes are simulated, the number of
reads given default parameters is reflective of what is seen in 20,000,000 read
human RNA-seq libraries. The benefit of <code>Simulate_relative_bakRData</code> is that it is
easier to test the impact of depth on model performance. This can theoretically
be done by changing the synthesis rate constant parameters in <code>Simulate_bakRData</code>,
but the relationship between these parameters and sequencing depth is unintuitive. The
benefit of <code>Simulate_bakRData</code> is that fewer genes can be simulated
while still yielding reasonable per-gene coverage without figuring out what the
total depth in the small gene subset should be. This is nice for testing bakR and
other analysis tools on small datasets. <code>Simulate_relative_bakRData</code> is a more
realistic simulation that better accounts for the relative nature of RNA-seq read
counts (i.e., expected number of reads from a given feature is related to proportion of RNA molecules
coming from that feature).
</p>
<p>Another difference between <code>Simulate_relative_bakRData</code> and <code>Simulate_bakRData</code>
is that <code>Simulate_relative_bakRData</code> uses the label time and simulated degradation
rate constants to infer the fraction new, whereas <code>Simulate_bakRData</code> uses simulated
fraction news and the label time to infer the degradation rate constants. Thus,
<code>Simulate_relative_bakRData</code> is preferable for assessing the impact of label
time on model performance (since it will have a realistic impact on the fraction new,
and the distribution of fraction news has a major impact on model performance).
Similarly, <code>Simulate_bakRData</code> is preferable for directly assessing the impact of
fraction news on model performance, without having to think about how both the label
time and simulated degradation rate constant distribution.
</p>
<p>If investigating dropout, only <code>Simulate_relative_bakRData</code> should be used, as the
accurate simulation of read counts as being a function of the relative abundance of
each RNA feature is crucial to accurately simulate dropout.
</p>
<p>Function to simulate a <code>bakRData</code> object according to a realistic generative model
</p>


<h3>Value</h3>

<p>A list containing a simulated <code>bakRData</code> object as well as a list of simulated kinetic parameters of interest.
The contents of the latter list are:
</p>

<ul>
<li><p> Effect_sim; Dataframe meant to mimic formatting of Effect_df that are part of <code>bakRFit(StanFit = TRUE)</code>, <code>bakRFit(HybridFit = TRUE)</code> and <code>bakRFit(bakRData object)</code> output.
</p>
</li>
<li><p> Fn_mean_sim; Dataframe meant to mimic formatting of Regularized_ests that is part of <code>bakRFit(bakRData object)</code> output. Contains information
about the true fraction new simulated in each condition (the mean of the normal distribution from which replicate fraction news are simulated)
</p>
</li>
<li><p> Fn_rep_sim; Dataframe meant to mimic formatting of Fn_Estimates that is part of <code>bakRFit(bakRData object)</code> output. Contains information
about the fraction new simulated for each feature in each replicate of each condition.
</p>
</li>
<li><p> L2FC_ks_mean; The true L2FC(ksyn) for each feature in each experimental condition. The i-th column corresponds to the L2FC(ksyn) when comparing
the i-th condition to the reference condition (defined as the 1st condition) so the 1st column is always all 0s
</p>
</li>
<li><p> RNA_conc; The average number of normalized read counts expected for each feature in each sample.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>
# 2 replicate, 2 experimental condition, 1000 gene simulation
sim_2reps &lt;- Simulate_relative_bakRData(ngene = 1000, depth = 100000,
                               nreps = 2)

# 3 replicate, 2 experimental condition, 1000 gene simulation
# with 100 instances of differential degradation kinetics
sim_3reps &lt;- Simulate_relative_bakRData(ngene = 1000, depth = 100000,
                                        num_kd_DE = c(0, 100))

# 2 replicates, 3 experimental condition, 1000 gene simulation
# with 100 instances of differential degradation kinetics in the 1st
# condition and no instances of differential degradation kinetics in the
# 2nd condition
sim_3es &lt;- Simulate_relative_bakRData(ngene = 1000, depth = 100000,
                             nreps = 2,
                             num_conds = 3,
                             num_kd_DE = c(0, 100, 0))


</code></pre>

<hr>
<h2 id='TL_stan'>Fit 'Stan' models to nucleotide recoding RNA-seq data analysis</h2><span id='topic+TL_stan'></span>

<h3>Description</h3>

<p><code>TL_stan</code> is an internal function to analyze nucleotide recoding RNA-seq data with a fully
Bayesian hierarchical model implemented in the PPL <code>Stan</code>. <code>TL_stan</code> estimates
kinetic parameters and differences in kinetic parameters between experimental
conditions. When assessing differences, a single reference sample is compared to
each collection of experimental samples provided.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>TL_stan(
  data_list,
  Hybrid_Fit = FALSE,
  keep_fit = FALSE,
  NSS = FALSE,
  chains = 1,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="TL_stan_+3A_data_list">data_list</code></td>
<td>
<p>List to pass to 'Stan' of form given by <code>cBprocess</code></p>
</td></tr>
<tr><td><code id="TL_stan_+3A_hybrid_fit">Hybrid_Fit</code></td>
<td>
<p>Logical; if TRUE, Hybrid 'Stan' model that takes as data output of <code>fast_analysis</code> is run.</p>
</td></tr>
<tr><td><code id="TL_stan_+3A_keep_fit">keep_fit</code></td>
<td>
<p>Logical; if TRUE, 'Stan' fit object is included in output; typically large file so default FALSE.</p>
</td></tr>
<tr><td><code id="TL_stan_+3A_nss">NSS</code></td>
<td>
<p>Logical; if TRUE, models that directly compare logit(fn)s are used to avoid steady-state assumption</p>
</td></tr>
<tr><td><code id="TL_stan_+3A_chains">chains</code></td>
<td>
<p>Number of Markov chains to sample from. The default is to only run a single chain. Typical NR-seq datasets
yield very memory intensive analyses, but running a single chain should decrease this burden. For reference, running
the MCMC implementation (Hybrid_Fit = FALSE) with 3 chains on an NR-seq dataset with 3 replicates of 2 experimental conditions
with around 20 million raw (unmapped) reads per sample requires over 100 GB of RAM. With a single chain, this burden drops to
around 20 GB. Due to memory demands and time constraints (runtimes for the MCMC implementation border will likely be around 1-2 days)
means that these models should usually be run in a specialized High Performance Computing (HPC) system.</p>
</td></tr>
<tr><td><code id="TL_stan_+3A_...">...</code></td>
<td>
<p>Arguments passed to <code>rstan::sampling</code> (e.g. iter, warmup, etc.).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Two implementations of a similar model can be fit with TL_stan: a complete nucleotide recoding RNA-seq
analysis and a hybrid analysis that takes as input results from <code>fast_analysis</code>.
In the complete analysis (referred to in the bakR publication as the MCMC implementation),
U-to-C mutations are modeled as coming from a Poisson distribution
with rate parameter adjusted by the empirical U-content of each feature analyzed. Features
represent whatever the user defined them to be when constructing the bakR data object.
Typical feature categories are genes, exons, etc. Hierarchical modeling is used to pool data
across replicates and across features. More specifically, replicate data for the
same feature are partially pooled to estimate feature-specific mean fraction news and uncertainties.
Feature means are partially pooled to estimate dataset-wide mean fraction news and standard deviations.
The replicate variability for each feature is also partially pooled to determine a condition-wide
heteroskedastic relationship between read depths and replicate variability. Partial pooling
reduces subjectivity when determining priors by allowing the model to determine what priors make sense
given the data. Partial pooling also regularizes estimates, reducing estimate variability and thus increasing
estimate accuracy. This is particularly important for replicate variability estimates, which often rely
on only a few replicates of data per feature and thus are typically highly unstable.
</p>
<p>The hybrid analysis (referred to in the bakR publication as the Hybrid implementation)
inherits the hierarchical modeling structure of the complete analysis, but reduces computational
burden by foregoing per-replicate-and-feature fraction new estimation and uncertainty quantification. Instead,
the hybrid analysis takes as data fraction new estimates and approximate uncertainties from <code>fast_analysis</code>.
Runtimes of the hybrid analysis are thus often an order of magnitude shorter than with the complete analysis, but
loses some accuracy by relying on point estimates and uncertainty quantification that is only valid in the
limit of large dataset sizes (where the dataset size for the per-replicate-and-feature fraction new estimate is the raw number
of sequencing reads mapping to the feature in that replicate).
</p>
<p>Users also have the option to save or discard the <code>Stan</code> fit object. Fit objects can be exceedingly large (&gt; 10 GB) for most
nucleotide recoding RNA-seq datasets. Therefore, if you don't want to store such a large object, a summary object will be saved instead,
which greatly reduces the size of the output (~ 10-50 MB) while still retaining much of the important information. In addition,
the output of <code>TL_stan</code> provides the estimates and uncertainties for key parameters (L2FC(kdeg), kdeg, and fraction new)
that will likely be of most interest. That being said, there are some analyses that are only possible if the original fit object
is saved. For example, the fit object will contain all of the samples from the posterior collected during model fitting. Thus,
new parameters (e.g., L2FC(kdeg)'s comparing two experimental samples) not naturally generated by the model can be estimated
post-hoc. Still, there are often approximate estimates that can be obtained for such parameters that don't rely on the full
fit object. One analysis that is impossible without the original fit object is generating model diagnostic plots. These include
trace plots (to show mixing and efficient parameter space exploration of the Markov chains), pairs plots (to show correlations
between parameters and where any divergences occurred), and other visualizations that can help users assess how well a model
ran. Because the models implemented by <code>TL_stan</code> are extensively validated, it is less likely that such diagnostics will be helpful,
but often anomalies on your data can lead to poor model convergence, in which case assessing model diagnostics can help you
identify the source of problems in your data. Summary statistics describing how well the model was able to estimate each parameter
(n_eff and rhat) are provided in the fit summaries, but can often obscure some of the nuanced details of model fitting.
</p>


<h3>Value</h3>

<p>A list of objects:
</p>

<ul>
<li><p> Effects_df; dataframe with estimates of the effect size (change in logit(fn)) comparing each experimental condition to the
reference sample for each feature. This dataframe also includes p-values obtained from a moderated t-test. The columns of this
dataframe are:
</p>

<ul>
<li><p> Feature_ID; Numerical ID of feature
</p>
</li>
<li><p> Exp_ID; Numerical ID for experimental condition (Exp_ID from metadf)
</p>
</li>
<li><p> L2FC_kdeg; L2FC(kdeg) posterior mean
</p>
</li>
<li><p> L2FC_kd_sd; L2FC(kdeg) posterior sd
</p>
</li>
<li><p> effect; identical to L2FC_kdeg (kept for symmetry with MLE fit output)
</p>
</li>
<li><p> se; identical to L2FC_kd_sd (kept for symmetry with MLE fit output)
</p>
</li>
<li><p> XF; Feature name
</p>
</li>
<li><p> pval; p value obtained from effect and se + z-test
</p>
</li>
<li><p> padj; p value adjusted for multiple testing using Benjamini-Hochberg procedure
</p>
</li></ul>

</li>
<li><p> Kdeg_df; dataframe with estimates of the kdeg (RNA degradation rate constant) for each feature, averaged across replicate data.
The columns of this dataframe are:
</p>

<ul>
<li><p> Feature_ID; Numerical ID of feature
</p>
</li>
<li><p> Exp_ID; Numerical ID for experimental condition
</p>
</li>
<li><p> kdeg; Degradation rate constant posterior mean
</p>
</li>
<li><p> kdeg_sd; Degradation rate constant posterior standard deviation
</p>
</li>
<li><p> log_kdeg; Log of degradation rate constant posterior mean (as of version 1.0.0)
</p>
</li>
<li><p> log_kdeg_sd; Log of degradation rate constant posterior standard deviation (as of version 1.0.0)
</p>
</li>
<li><p> XF; Original feature name
</p>
</li></ul>

</li>
<li><p> Fn_Estimates; dataframe with estimates of the logit(fraction new) for each feature in each replicate.
The columns of this dataframe are:
</p>

<ul>
<li><p> Feature_ID; Numerical ID for feature
</p>
</li>
<li><p> Exp_ID; Numerical ID for experimental condition (Exp_ID from metadf)
</p>
</li>
<li><p> Replicate; Numerical ID for replicate
</p>
</li>
<li><p> logit_fn; Logit(fraction new) posterior mean
</p>
</li>
<li><p> logit_fn_se; Logit(fraction new) posterior standard deviation
</p>
</li>
<li><p> sample; Sample name
</p>
</li>
<li><p> XF; Original feature name
</p>
</li></ul>

</li>
<li><p> Fit_Summary; only outputted if keep_fit == FALSE. Summary of 'Stan' fit object with each row corresponding to a particular
parameter. All posterior point descriptions are descriptions of the marginal posterior distribution for the parameter in that row.
For example, the posterior mean is the average value for the parameter when averaging over all other parameter values.
The columns of this dataframe are:
</p>

<ul>
<li><p> mean; Posterior mean for the parameter given by the row name
</p>
</li>
<li><p> se_mean; Standard error of the posterior mean; essentially how confident the model is that what it estimates to be the
posterior mean is what the posterior mean actually is. This will depend on the number of chains run on the number of iterations
each chain is run for.
</p>
</li>
<li><p> sd; Posterior standard deviation
</p>
</li>
<li><p> 2.5%; 2.5th percentile of the posterior distribution. 2.5% of the posterior mass is below this point
</p>
</li>
<li><p> 25%; 25th percentile of the posterior distribution
</p>
</li>
<li><p> 50%; 50th percentile of the posterior distribution
</p>
</li>
<li><p> 75%; 75th percentile of the posterior distribution
</p>
</li>
<li><p> 97.5%; 97.5th percentile of the posterior distribution
</p>
</li>
<li><p> n_eff; Effective sample size. The larger this is the better, though it should preferably be around the total number of
iterations (iter x chains). Small values of this could represent poor model convergence
</p>
</li>
<li><p> Rhat; Describes how well separate Markov chains mixed. This is preferably as close to 1 as possible, and values higher
than 1 could represent poor model convergence
</p>
</li></ul>

</li>
<li><p> Stan_Fit; only outputted if keep_fit == TRUE. This is the full 'Stan' fit object, an R6 object of class <code>stanfit</code>
</p>
</li>
<li><p> Mutation_Rates; data frame with information about mutation rate estimates. Has the same columns as Fit_Summary.
Each row corresponds to either a background mutation rate (log_lambda_o) or an s4U induced mutation rate (log_lambda_n),
denoted in the parameter column. The bracketed portion of the parameter name will contain two numbers. The first corresponds
to the Exp_ID and the second corresponds to the Replicate_ID. For example, if the parameter name is log_lambda_o[1,2] then
that row corresponds to the background mutation rate in the second replicate of experimental condition one. A final point to
mention is that the estimates are on a log(avg. # of mutations) scale. So a log_lambda_n of 1 means that on average, there
are an estimated 2.72 (exp(1)) mutations in reads from new RNA (i.e., RNA synthesized during s4U labeling).
</p>
</li></ul>


<hr>
<h2 id='validate_bakRData'>bakR Data object validator</h2><span id='topic+validate_bakRData'></span>

<h3>Description</h3>

<p>This functions ensures that input for bakRData object construction is valid
</p>


<h3>Usage</h3>

<pre><code class='language-R'>validate_bakRData(obj)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="validate_bakRData_+3A_obj">obj</code></td>
<td>
<p>An object of class bakRData</p>
</td></tr>
</table>

<hr>
<h2 id='validate_bakRFnData'>bakRFnData object validator</h2><span id='topic+validate_bakRFnData'></span>

<h3>Description</h3>

<p>This functions ensures that input for bakRFnData object construction is valid
</p>


<h3>Usage</h3>

<pre><code class='language-R'>validate_bakRFnData(obj)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="validate_bakRFnData_+3A_obj">obj</code></td>
<td>
<p>An object of class bakRFnData</p>
</td></tr>
</table>

<hr>
<h2 id='VisualizeDropout'>Visualize dropout</h2><span id='topic+VisualizeDropout'></span>

<h3>Description</h3>

<p><code>VisualizeDropout</code> fits dropout model with <code>QuantifyDropout</code>,
reports the fit results, and then generates a ggplot object showing the
data used to infer the fit as well as the fitted nonlinear trend.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>VisualizeDropout(obj, keep_data = FALSE, no_message = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="VisualizeDropout_+3A_obj">obj</code></td>
<td>
<p>bakRFit or bakRFnFit object</p>
</td></tr>
<tr><td><code id="VisualizeDropout_+3A_keep_data">keep_data</code></td>
<td>
<p>Logical; if TRUE, will return data used to make plots along with
the plots themselves</p>
</td></tr>
<tr><td><code id="VisualizeDropout_+3A_no_message">no_message</code></td>
<td>
<p>Logical; if TRUE, will not output message regarding estimated
rates of dropout in each sample</p>
</td></tr>
</table>


<h3>Value</h3>

<p>If keep_data is FALSE, then a list of <code>ggplot</code> objects are returned, one
for each +s4U sample. The plots show the relationship between a feature's fraction new
and the difference between its +s4U and -s4U read coverage. Nonlinear-least squares fit
is plotted on top of points as a blue line. If keep_data is TRUE, then the data used
to make the plots is returned in addition to the list of plots.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Simulate data for 500 genes and 2 replicates with 40% dropout
sim &lt;- Simulate_relative_bakRData(500, 100000, nreps = 2, p_do = 0.4)

# Fit data with fast implementation
Fit &lt;- bakRFit(sim$bakRData)

# Quantify dropout
DO_plots &lt;- VisualizeDropout(Fit)


</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
