<!DOCTYPE html><html lang="en"><head><title>Help for package Ckmeans.1d.dp</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {Ckmeans.1d.dp}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#Ckmeans.1d.dp-package'>
<p>Optimal, Fast, and Reproducible Univariate Clustering</p></a></li>
<li><a href='#ahist'><p>Adaptive Histograms</p></a></li>
<li><a href='#MultiChannel.WUC'>
<p>Optimal Multi-channel Weighted Univariate Clustering</p></a></li>
<li><a href='#plot.Ckmeans.1d.dp'><p>Plot Optimal Univariate Clustering Results</p></a></li>
<li><a href='#plot.Cksegs.1d.dp'><p>Plot Optimal Univariate Segmentation Results</p></a></li>
<li><a href='#plot.MultiChannelClusters'><p>Plot Multi-Channel Clustering Results</p></a></li>
<li><a href='#plotBIC'><p>Plot Bayesian Information Criterion as a Function of Number of Clusters</p></a></li>
<li><a href='#print.Ckmeans.1d.dp'><p>Print Optimal Univariate Clustering Results</p></a></li>
<li><a href='#print.Cksegs.1d.dp'><p>Print Optimal Univariate Segmentation Results</p></a></li>
<li><a href='#Univariate+20Clustering'><p>Optimal (Weighted) Univariate Clustering</p></a></li>
<li><a href='#Univariate+20Segmentation'><p>Optimal Univariate Segmentation</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Optimal, Fast, and Reproducible Univariate Clustering</td>
</tr>
<tr>
<td>Version:</td>
<td>4.3.5</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-08-19</td>
</tr>
<tr>
<td>Author:</td>
<td>Joe Song <a href="https://orcid.org/0000-0002-6883-6547"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut,
    cre],
  Hua Zhong <a href="https://orcid.org/0000-0003-1962-2603"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut],
  Haizhou Wang [aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Joe Song &lt;joemsong@cs.nmsu.edu&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Fast, optimal, and reproducible weighted univariate
 clustering by dynamic programming. Four problems are solved, including
 univariate k-means (Wang &amp; Song 2011) &lt;<a href="https://doi.org/10.32614%2FRJ-2011-015">doi:10.32614/RJ-2011-015</a>&gt;
 (Song &amp; Zhong 2020) &lt;<a href="https://doi.org/10.1093%2Fbioinformatics%2Fbtaa613">doi:10.1093/bioinformatics/btaa613</a>&gt;, k-median,
 k-segments, and multi-channel weighted k-means. Dynamic programming
 is used to minimize the sum of (weighted) within-cluster distances
 using respective metrics. Its advantage over heuristic clustering in
 efficiency and accuracy is pronounced when there are many clusters.
 Multi-channel weighted k-means groups multiple univariate
 signals into k clusters. An auxiliary function generates histograms
 adaptive to patterns in data. This package provides a powerful set
 of tools for univariate data analysis with guaranteed optimality,
 efficiency, and reproducibility, useful for peak calling on temporal,
 spatial, and spectral data.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/LGPL-3">LGPL (&ge; 3)</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp, Rdpack (&ge; 0.6-1)</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat, knitr, rmarkdown, RColorBrewer</td>
</tr>
<tr>
<td>RdMacros:</td>
<td>Rdpack</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-08-19 17:13:22 UTC; joesong</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-08-19 18:12:31 UTC</td>
</tr>
</table>
<hr>
<h2 id='Ckmeans.1d.dp-package'>
Optimal, Fast, and Reproducible Univariate Clustering
</h2><span id='topic+Ckmeans.1d.dp-package'></span>

<h3>Description</h3>

<p>This package provides a powerful set of tools for univariate data analysis with guaranteed optimality, efficiency, and reproducibility.
Four problems including univariate <code class="reqn">k</code>-means, <code class="reqn">k</code>-median, <code class="reqn">k</code>-segments, and multi-channel weighted <code class="reqn">k</code>-means are solved with guaranteed optimality and reproducibility. The core algorithm minimizes the (weighted) sum of within-cluster distances using respective metrics. Its advantage over heuristic clustering in efficiency and accuracy is increasingly pronounced as the number of clusters <code class="reqn">k</code> increases. Weighted <code class="reqn">k</code>-means can also optimally segment time series to perform peak calling. An auxiliary function generates histograms that are adaptive to patterns in data.
</p>
<p>The Ckmeans.1d.dp algorithm clusters (weighted) univariate data given by a numeric vector <code class="reqn">x</code> into <code class="reqn">k</code> groups by dynamic programming (Wang and Song 2011; Song and Zhong 2020). It guarantees the optimality of clustering&mdash;the total of within-cluster sums of squares is always the minimum given the number of clusters <code class="reqn">k</code>. In contrast, heuristic univariate clustering algorithms may be non-optimal or inconsistent from run to run. As non-negative weights are supported, the algorithm can partition a time course using time points as input and corresponding values as weight. Based on an optimal clustering, a function can generate histograms adaptive to patterns in data. The implementation of this algorithm is numerically stable.
</p>
<p>A linear time solution. Excluding the time for sorting <code class="reqn">x</code>, the weighted univariate clustering algorithm takes a runtime of <code class="reqn">O(kn)</code> (Song and Zhong 2020), linear in both sample size <code class="reqn">n</code> and the number of clusters <code class="reqn">k</code>. The approach was first introduced into version 3.4.9 (not publicly released) on July 16, 2016, using a new divide-and-conquer strategy integrating a previous theoretical result on matrix search (Aggarwal et al. 1987) and a novel in-place search space reduction method (Song and Zhong 2020).
</p>
<p>A log-linear time solution. Since version 3.4.6, a divide-and-conquer strategy that is simple to code reduces the runtime from <code class="reqn">O(kn^2)</code> down to <code class="reqn">O(kn\lg n)</code> (Song and Zhong 2020).
</p>
<p>A quadratic time solution. Before version 3.4.6, Ckmeans.1d.dp uses an algorithm that runs in quadratic runtime <code class="reqn">O(kn^2)</code> (Wang and Song 2011).
</p>
<p>A cubic time solution. Bellman (1973) first described a general dynamic programming strategy for solving univariate clustering problems with additive optimality measures. The strategy, however, did not address any specific characteristics of the <code class="reqn">k</code>-means problem and its implied general algorithm will have a time complexity of <code class="reqn">O(kn^3)</code> on an input of <code class="reqn">n</code> points.
</p>
<p>The space complexity in all cases is <code class="reqn">O(kn)</code>.
</p>
<p>This package provides a powerful alternative to heuristic clustering and also new functionality for weighted clustering, segmentation, and peak calling with guaranteed optimality. It is practical for Ckmeans.1d.dp to find a few clusters on millions of sample points with optional weights in seconds using a single core on a typical desktop computer.
</p>
<p>Third parties have ported various versions of this package to JavaScript, MATLAB, Python, Ruby, SAS, and Scala.
</p>


<h3>Details</h3>

<p>The most important function of this package is <code><a href="#topic+Ckmeans.1d.dp">Ckmeans.1d.dp</a></code> that implements optimal univariate clustering either weighted or unweighted. It also contains an adaptive histogram function <code><a href="#topic+ahist">ahist</a></code>, plotting functions <code><a href="#topic+plot.Ckmeans.1d.dp">plot.Ckmeans.1d.dp</a></code> and <code><a href="#topic+plotBIC">plotBIC</a></code>, and a print function <code><a href="#topic+print.Ckmeans.1d.dp">print.Ckmeans.1d.dp</a></code>.
</p>


<h3>Disclaimer</h3>

<p>This program is free software: you can redistribute it and/or modify
it under the terms of the GNU Lesser General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.
</p>
<p>This program is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU Lesser General Public License for more details.
</p>
<p>You should have received a copy of the GNU General Public License
along with this program.  If not, see <a href="https://www.gnu.org/licenses/">https://www.gnu.org/licenses/</a>.</p>


<h3>Author(s)</h3>

<p>Joe Song, Hua Zhong, and Haizhou Wang
</p>


<h3>References</h3>

<p>Aggarwal A, Klawe MM, Moran S, Shor P, Wilber R (1987).
&ldquo;Geometric applications of a matrix-searching algorithm.&rdquo;
<em>Algorithmica</em>, <b>2</b>(1-4), 195&ndash;208.
<a href="https://doi.org/10.1007/BF01840359">doi:10.1007/BF01840359</a>.<br /><br /> Bellman R (1973).
&ldquo;A note on cluster analysis and dynamic programming.&rdquo;
<em>Mathematical Biosciences</em>, <b>18</b>(3), 311&ndash;312.
<a href="https://doi.org/10.1016/0025-5564%2873%2990007-2">doi:10.1016/0025-5564(73)90007-2</a>.<br /><br /> Song M, Zhong H (2020).
&ldquo;Efficient weighted univariate clustering maps outstanding dysregulated genomic zones in human cancers.&rdquo;
<em>Bioinformatics</em>, <b>36</b>(20), 5027&ndash;5036.
<a href="https://doi.org/10.1093/bioinformatics/btaa613">doi:10.1093/bioinformatics/btaa613</a>.<br /><br /> Wang H, Song M (2011).
&ldquo;Ckmeans.1d.dp: Optimal <code class="reqn">k</code>-means clustering in one dimension by dynamic programming.&rdquo;
<em>The R Journal</em>, <b>3</b>(2), 29&ndash;33.
<a href="https://doi.org/10.32614/RJ-2011-015">doi:10.32614/RJ-2011-015</a>.
</p>


<h3>See Also</h3>

<p>The <code><a href="stats.html#topic+kmeans">kmeans</a></code> function in package <span class="pkg"><a href="stats.html#topic+stats">stats</a></span> that implements several heuristic <code class="reqn">k</code>-means algorithms.
</p>

<hr>
<h2 id='ahist'>Adaptive Histograms</h2><span id='topic+ahist'></span>

<h3>Description</h3>

<p>Generate or plot histograms adaptive to patterns in univariate data. The number and widths of histogram bins are automatically calculated based on an optimal univariate clustering of input data. Thus the bins are unlikely of equal width.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ahist(x, k = c(1,9), breaks=NULL, data=NULL, weight=1,
      plot = TRUE, xlab = deparse(substitute(x)),
      wlab = deparse(substitute(weight)),
      main = NULL, col = "lavender", border = graphics::par("fg"),
      lwd = graphics::par("lwd"),
      col.stick = "gray", lwd.stick = 1, add.sticks=TRUE,
      style = c("discontinuous", "midpoints"),
      skip.empty.bin.color=TRUE,
      ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ahist_+3A_x">x</code></td>
<td>
<p>a numeric vector of data or an object of class <code>"Ckmeans.1d.dp"</code>.
</p>
<p>If <code>x</code> is a numeric vector, all <code>NA</code> elements must be removed from <code>x</code> before calling this function.
</p>
<p>If <code>x</code> is an object of class <code>"Ckmeans.1d.dp"</code>, the clustering information in <code>x</code> will be used and the <code>data</code> argument contains the numeric vector to be plotted.</p>
</td></tr>
<tr><td><code id="ahist_+3A_k">k</code></td>
<td>
<p>either an exact integer number of bins/clusters, or a vector of length two specifying the minimum and maximum numbers of bins/clusters to be examined. The default is <code>c(1,9)</code>. When <code>k</code> is a range, the actual number of clusters is determined by Bayesian information criterion. This argument is ignored if <code>x</code> is an object of class <code>"Ckmeans.1d.dp"</code>.</p>
</td></tr>
<tr><td><code id="ahist_+3A_breaks">breaks</code></td>
<td>
<p>This argument is defined in <code><a href="graphics.html#topic+hist">hist</a></code>. If this argument is provided, optimal univariate clustering is not applied to obtain the histogram, but instead the histogram will be generated by the <code><a href="graphics.html#topic+hist">hist</a></code> function in <span class="pkg"><a href="graphics.html#topic+graphics">graphics</a></span>, except that sticks representing data can still be optionally plotted by specifying the <code>add.sticks=TRUE</code> argument.</p>
</td></tr>
<tr><td><code id="ahist_+3A_data">data</code></td>
<td>
<p>a numeric vector. If <code>x</code> is an object of class <code>"Ckmeans.1d.dp"</code>, the data argument must be provided. If <code>x</code> is a numeric vector, this argument is ignored.</p>
</td></tr>
<tr><td><code id="ahist_+3A_weight">weight</code></td>
<td>
<p>a value of 1 to specify equal weights or a numeric vector of unequal weights for each element. The default weight is one. It is highly recommended to use positive (instead of zero) weights to account for the influence of every element. The weights have a strong impact on the clustering result.</p>
</td></tr>
<tr><td><code id="ahist_+3A_plot">plot</code></td>
<td>
<p>a logical. If <code>TRUE</code>, the histogram will be plotted.</p>
</td></tr>
<tr><td><code id="ahist_+3A_xlab">xlab</code></td>
<td>
<p>a character string. The x-axis label for the plot.</p>
</td></tr>
<tr><td><code id="ahist_+3A_wlab">wlab</code></td>
<td>
<p>a character string. The weight-axis label for the plot. It is the vertical axis to the right of the plot.</p>
</td></tr>
<tr><td><code id="ahist_+3A_main">main</code></td>
<td>
<p>a character string. The title for the plot.</p>
</td></tr>
<tr><td><code id="ahist_+3A_col">col</code></td>
<td>
<p>a character string. The fill color of the histogram bars.</p>
</td></tr>
<tr><td><code id="ahist_+3A_border">border</code></td>
<td>
<p>a character string. The color of the histogram bar borders.</p>
</td></tr>
<tr><td><code id="ahist_+3A_lwd">lwd</code></td>
<td>
<p>a numeric value. The line width of the border of the histogram bars</p>
</td></tr>
<tr><td><code id="ahist_+3A_col.stick">col.stick</code></td>
<td>
<p>a character string. The color of the sticks above the x-axis. See Details.</p>
</td></tr>
<tr><td><code id="ahist_+3A_lwd.stick">lwd.stick</code></td>
<td>
<p>a numeric value. The line width of the sticks above the x-axis. See Details.</p>
</td></tr>
<tr><td><code id="ahist_+3A_add.sticks">add.sticks</code></td>
<td>
<p>a logical. If <code>TRUE</code> (default), the sticks representing the data will be added to the bottom of the histogram. Otherwise, sticks are not plotted.</p>
</td></tr>
<tr><td><code id="ahist_+3A_style">style</code></td>
<td>
<p>a character string. The style of the adaptive histogram. See details.</p>
</td></tr>
<tr><td><code id="ahist_+3A_skip.empty.bin.color">skip.empty.bin.color</code></td>
<td>
<p>a logical. If <code>TRUE</code> (default), an empty bin (invisible) will be assigned the same bar color with the next bin. This is useful when all provided colors are to be used for non-empty bins. If <code>FALSE</code>, each bin will be assigned a bar color from <code>col</code>. A value of <code>TRUE</code> will coordinate the bar and stick colors.</p>
</td></tr>
<tr><td><code id="ahist_+3A_...">...</code></td>
<td>
<p>additional arguments to be passed to <code><a href="graphics.html#topic+hist">hist</a></code> or <code><a href="graphics.html#topic+plot.histogram">plot.histogram</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The histogram is by default plotted using the <code><a href="graphics.html#topic+plot.histogram">plot.histogram</a></code> method. The plot can be optionally disabled with the <code>plot=FALSE</code> argument. The original input data are shown as sticks just above the horizontal axis.
</p>
<p>If the <code>breaks</code> argument is not specified, the number of histogram bins is the optimal number of clusters estimated using Bayesian information criterion evaluated on Gaussian mixture models fitted to the input data in <code>x</code>.
</p>
<p>If not provided with the <code>breaks</code> argument, breaks in the histogram are derived from clusters identified by optimal univariate clustering (<code><a href="#topic+Ckmeans.1d.dp">Ckmeans.1d.dp</a></code>) in two styles. With the default <code>"discontinuous"</code> style,  the bin width of each bar is determined according to a data-adaptive rule; the <code>"midpoints"</code> style uses the midpoints of cluster border points to determine the bin-width. For clustered data, the <code>"midpoints"</code> style generates bins that are too wide to capture the cluster patterns. In contrast, the <code>"discontinuous"</code> style is more adaptive to the data by allowing some bins to be empty making the histogram bars discontinuous.</p>


<h3>Value</h3>

<p>An object of class <code>histogram</code> defined in <code><a href="graphics.html#topic+hist">hist</a></code>. It has a S3 <code>plot</code> method <code><a href="graphics.html#topic+plot.histogram">plot.histogram</a></code>.
</p>


<h3>Author(s)</h3>

<p>Joe Song
</p>


<h3>See Also</h3>

<p><code><a href="graphics.html#topic+hist">hist</a></code> in package <span class="pkg"><a href="graphics.html#topic+graphics">graphics</a></span>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Example 1: plot an adaptive histogram from data generated by
#   a Gaussian mixture model with three components
x &lt;- c(rnorm(40, mean=-2, sd=0.3),
       rnorm(45, mean=1, sd=0.1),
       rnorm(70, mean=3, sd=0.2))
ahist(x, col="lightblue", sub=paste("n =", length(x)),
      col.stick="salmon", lwd=2,
      main=paste("Example 1. Gaussian mixture model with 3 components",
                 "(one bin per component)", sep="\n"))


# Example 2: plot an adaptive histogram from data generated by
#   a Gaussian mixture model with three components using a given
#   number of bins
ahist(x, k=9, col="lavender", col.stick="salmon",
      sub=paste("n =", length(x)), lwd=2,
      main=paste("Example 2. Gaussian mixture model with 3 components",
                 "(on average 3 bins per component)", sep="\n"))

# Example 3: The DNase data frame has 176 rows and 3 columns of
#   data obtained during development of an ELISA assay for the
#   recombinant protein DNase in rat serum.

data(DNase)
res &lt;- Ckmeans.1d.dp(DNase$density)
kopt &lt;- length(res$size)
ahist(res, data=DNase$density, col=rainbow(kopt),
      col.stick=rainbow(kopt)[res$cluster],
      sub=paste("n =", length(x)), border="transparent",
      xlab="Optical density of protein DNase",
      main="Example 3. Elisa assay of DNase in rat serum")


# Example 4: Add sticks to histograms with the R provided
#   hist() function.

ahist(DNase$density, breaks="Sturges", col="palegreen",
      add.sticks=TRUE, col.stick="darkgreen",
      main=paste("Example 4. Elisa assay of DNase in rat serum",
                 "(Equal width bins)", sep="\n"),
      xlab="Optical density of protein DNase")

# Example 5: Weighted adatpive histograms

x &lt;- sort(c(rnorm(40, mean=-2, sd=0.3),
       rnorm(45, mean=2, sd=0.1),
       rnorm(70, mean=4, sd=0.2)))

y &lt;- (1 + sin(0.10 * seq_along(x))) * (x-1)^2

ahist(x, weight=y, sub=paste("n =", length(x)),
      col.stick="forestgreen", lwd.stick=0.25, lwd=2,
      main="Example 5. Weighted adaptive histogram")


# Example 6: Cluster data with repetitive elements

ahist(c(1,1,1,1, 3,4,4, 6,6,6), k=c(2,4), col="gray",
      lwd=2, lwd.stick=6, col.stick="chocolate",
      main=paste("Example 6. Adaptive histogram of",
                 "repetitive elements", sep="\n"))

</code></pre>

<hr>
<h2 id='MultiChannel.WUC'>
Optimal Multi-channel Weighted Univariate Clustering
</h2><span id='topic+MultiChannel.WUC'></span>

<h3>Description</h3>

<p>Perform optimal multi-channel weighted univariate <code class="reqn">k</code>-means clustering in linear time.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  MultiChannel.WUC(x, y,  k=c(1,9))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="MultiChannel.WUC_+3A_x">x</code></td>
<td>
<p>a numeric vector of data to be clustered. All <code>NA</code> elements must be removed from <code>x</code> before calling this function. The function will run faster on sorted <code>x</code> (in non-decreasing order) than an unsorted input.</p>
</td></tr>
<tr><td><code id="MultiChannel.WUC_+3A_y">y</code></td>
<td>
<p>a numeric matrix of non-negative weights for each element in <code>x</code>. Columns of the matrix are channels. It is highly recommended to use positive (instead of zero) weights to account for the influence of every element. Weights strongly influence clustering results. When the number of clusters <code>k</code> is given as a range, the weights should be linearly scaled to sum up to the observed sample size.</p>
</td></tr>
<tr><td><code id="MultiChannel.WUC_+3A_k">k</code></td>
<td>
<p>either an exact integer number of clusters, or a vector of length two specifying the minimum and maximum numbers of clusters to be examined. The default is <code>c(1,9)</code>. When <code>k</code> is a range, the actual number of clusters is determined by Bayesian information criterion (BIC).</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>MultiChannel.WUC</code> minimizes the total weighted within-cluster sum of squared distance (Zhong 2019). It uses the SMAWK algorithm (Aggarwal et al. 1987) with modified data structure to speed up the dynamic programming to linear runtime. The method selects an optimal <code>k</code> based on an approximate Gaussian mixture model using the BIC.
</p>


<h3>Value</h3>

<p>A list object containing the following components:
</p>
<table role = "presentation">
<tr><td><code>cluster</code></td>
<td>
<p>a vector of clusters assigned to each element in <code>x</code>. Each cluster is indexed by an integer from 1 to <code>k</code>.</p>
</td></tr>
<tr><td><code>centers</code></td>
<td>
<p>a numeric vector of the (weighted) means for each cluster.</p>
</td></tr>
<tr><td><code>withinss</code></td>
<td>
<p>a numeric vector of the (weighted) within-cluster sum of squares for each cluster.</p>
</td></tr>
<tr><td><code>size</code></td>
<td>
<p>a vector of the (weighted) number of elements in each cluster.</p>
</td></tr>
<tr><td><code>totss</code></td>
<td>
<p>total sum of (weighted) squared distances between each element and the sample mean. This statistic is not dependent on the clustering result.</p>
</td></tr>
<tr><td><code>tot.withinss</code></td>
<td>
<p>total sum of (weighted) within-cluster squared distances between each element and its cluster mean. This statistic is minimized given the number of clusters.</p>
</td></tr>
<tr><td><code>betweenss</code></td>
<td>
<p>sum of (weighted) squared distances between each cluster mean and sample mean. This statistic is maximized given the number of clusters.</p>
</td></tr>
<tr><td><code>xname</code></td>
<td>
<p>a character string. The actual name of the <code>x</code> argument.</p>
</td></tr>
<tr><td><code>yname</code></td>
<td>
<p>a character string. The actual name of the <code>y</code> argument.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Hua Zhong and Mingzhou Song
</p>


<h3>References</h3>

<p>Aggarwal A, Klawe MM, Moran S, Shor P, Wilber R (1987).
&ldquo;Geometric applications of a matrix-searching algorithm.&rdquo;
<em>Algorithmica</em>, <b>2</b>(1-4), 195&ndash;208.
<a href="https://doi.org/10.1007/BF01840359">doi:10.1007/BF01840359</a>.<br /><br /> Zhong H (2019).
<em>Model-free Gene-to-zone Network Inference of Molecular Mechanisms in Biology</em>.
Ph.D. thesis, Department of Computer Science, New Mexico State University, Las Cruces, NM, USA.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  x &lt;- sample(x = c(1:100), size = 20, replace = TRUE)
  Y &lt;- matrix(sample(x = c(1:100), size = 40, replace = TRUE), ncol=2, nrow=length(x))

  res &lt;- MultiChannel.WUC(x = x, y = Y, k = c(1:10))
  plot(res)

  n &lt;- c(20, 20, 20)
  x &lt;- c(rnorm(n[1], mean=-6),
         rnorm(n[2], mean=0),
         rnorm(n[3], mean=6))

  Y &lt;- matrix(c(
    rep(c(1,0,0), times=n[1]),
    rep(c(0,1,0), times=n[2]),
    rep(c(0,0,1), times=n[3])
  ), byrow=TRUE, nrow=length(x))

  res &lt;- MultiChannel.WUC(x = x, y = Y, k = 3)

  opar &lt;- par(mar=c(3,3,2.5,1), mgp=c(1.5,0.5,0))
  plot(res)
  par(opar)

</code></pre>

<hr>
<h2 id='plot.Ckmeans.1d.dp'>Plot Optimal Univariate Clustering Results</h2><span id='topic+plot.Ckmeans.1d.dp'></span><span id='topic+plot.Ckmedian.1d.dp'></span>

<h3>Description</h3>

<p>Plot optimal univariate clustering results returned from <code>Ckmeans.1d.dp</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'Ckmeans.1d.dp'
plot(x, xlab=NULL, ylab=NULL, main=NULL,
      sub=NULL, col.clusters=NULL, ...)
## S3 method for class 'Ckmedian.1d.dp'
plot(x, xlab=NULL, ylab=NULL, main=NULL,
      sub=NULL, col.clusters=NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.Ckmeans.1d.dp_+3A_x">x</code></td>
<td>
<p>an object of class as returned by <code><a href="#topic+Ckmeans.1d.dp">Ckmeans.1d.dp</a></code> or <code><a href="#topic+Ckmedian.1d.dp">Ckmedian.1d.dp</a></code>.</p>
</td></tr>
<tr><td><code id="plot.Ckmeans.1d.dp_+3A_xlab">xlab</code></td>
<td>
<p>a character string. The x-axis label for the plot.</p>
</td></tr>
<tr><td><code id="plot.Ckmeans.1d.dp_+3A_ylab">ylab</code></td>
<td>
<p>a character string. The x-axis label for the plot.</p>
</td></tr>
<tr><td><code id="plot.Ckmeans.1d.dp_+3A_main">main</code></td>
<td>
<p>a character string. The title for the plot.</p>
</td></tr>
<tr><td><code id="plot.Ckmeans.1d.dp_+3A_sub">sub</code></td>
<td>
<p>a character string. The subtitle for the plot.</p>
</td></tr>
<tr><td><code id="plot.Ckmeans.1d.dp_+3A_col.clusters">col.clusters</code></td>
<td>
<p>a vector of colors, defined either by integers or by color names. If the length is shorter than the number of clusters, the colors will be reused.</p>
</td></tr>
<tr><td><code id="plot.Ckmeans.1d.dp_+3A_...">...</code></td>
<td>
<p>arguments passed to <code><a href="base.html#topic+plot">plot</a></code> function in package <span class="pkg">graphics</span>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The functions <code>plot.Ckmeans.1d.dp</code> and <code>plot.Ckmedian.1d.dp</code> visualize the input data as sticks whose heights are the weights. They use different colors to indicate clusters.
</p>


<h3>Value</h3>

<p>An object of class &quot;<code>Ckmeans.1d.dp</code>&quot; or &quot;<code>Ckmedian.1d.dp</code>&quot; defined in <code><a href="#topic+Ckmeans.1d.dp">Ckmeans.1d.dp</a></code>.
</p>


<h3>Author(s)</h3>

<p>Joe Song
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Example: clustering data generated from a Gaussian
#          mixture model of three components
x &lt;- c(rnorm(50, mean=-1, sd=0.3),
       rnorm(50, mean=1, sd=0.3),
       rnorm(50, mean=3, sd=0.3))

res &lt;- Ckmeans.1d.dp(x)
plot(res)

y &lt;- (rnorm(length(x)))^2
res &lt;- Ckmeans.1d.dp(x, y=y)
plot(res)

res &lt;- Ckmedian.1d.dp(x)
plot(res)
</code></pre>

<hr>
<h2 id='plot.Cksegs.1d.dp'>Plot Optimal Univariate Segmentation Results</h2><span id='topic+plot.Cksegs.1d.dp'></span>

<h3>Description</h3>

<p>Plot optimal univariate segmentation results returned from <code>Cksegs.1d.dp</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'Cksegs.1d.dp'
plot(x, xlab=NULL, ylab=NULL, main=NULL,
      sub=NULL, col.clusters=NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.Cksegs.1d.dp_+3A_x">x</code></td>
<td>
<p>an object of class as returned by <code><a href="#topic+Cksegs.1d.dp">Cksegs.1d.dp</a></code>.</p>
</td></tr>
<tr><td><code id="plot.Cksegs.1d.dp_+3A_xlab">xlab</code></td>
<td>
<p>a character string. The x-axis label for the plot.</p>
</td></tr>
<tr><td><code id="plot.Cksegs.1d.dp_+3A_ylab">ylab</code></td>
<td>
<p>a character string. The x-axis label for the plot.</p>
</td></tr>
<tr><td><code id="plot.Cksegs.1d.dp_+3A_main">main</code></td>
<td>
<p>a character string. The title for the plot.</p>
</td></tr>
<tr><td><code id="plot.Cksegs.1d.dp_+3A_sub">sub</code></td>
<td>
<p>a character string. The subtitle for the plot.</p>
</td></tr>
<tr><td><code id="plot.Cksegs.1d.dp_+3A_col.clusters">col.clusters</code></td>
<td>
<p>a vector of colors, defined either by integers or by color names. If the length is shorter than the number of clusters, the colors will be reused.</p>
</td></tr>
<tr><td><code id="plot.Cksegs.1d.dp_+3A_...">...</code></td>
<td>
<p>arguments passed to <code><a href="base.html#topic+plot">plot</a></code> function in package <span class="pkg">graphics</span>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function <code>plot.Cksegs.1d.dp</code> shows segments as horizontal lines from the univariate
segmentation results obtained from function <code>Cksegs.1d.dp</code>. It uses different colors to indicate segments.
</p>


<h3>Value</h3>

<p>An object of class &quot;<code>Cksegs.1d.dp</code>&quot; defined in <code><a href="#topic+Cksegs.1d.dp">Cksegs.1d.dp</a></code>.
</p>


<h3>Author(s)</h3>

<p>Joe Song
</p>


<h3>References</h3>

<p>Wang, H. and Song, M. (2011) Ckmeans.1d.dp: optimal <var>k</var>-means clustering in one dimension by dynamic programming. <em>The R Journal</em> <b>3</b>(2), 29&ndash;33. Retrieved from <a href="https://journal.r-project.org/archive/2011-2/RJournal_2011-2_Wang+Song.pdf">https://journal.r-project.org/archive/2011-2/RJournal_2011-2_Wang+Song.pdf</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Example: clustering data generated from a Gaussian
#          mixture model of three components
x &lt;- c(rnorm(50, mean=-1, sd=0.3),
       rnorm(50, mean=1, sd=0.3),
       rnorm(50, mean=3, sd=0.3))

y &lt;- x^3
res &lt;- Cksegs.1d.dp(y, x=x)
plot(res, lwd=2)

</code></pre>

<hr>
<h2 id='plot.MultiChannelClusters'>Plot Multi-Channel Clustering Results</h2><span id='topic+plot.MultiChannelClusters'></span>

<h3>Description</h3>

<p>Plot multi-channel clustering results returned from <code>MultiChannel.WUC</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'MultiChannelClusters'
plot(x, xlab=NULL, ylab=NULL, main=NULL,
      sub=NULL, col.clusters=NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.MultiChannelClusters_+3A_x">x</code></td>
<td>
<p>an object of class as returned by <code><a href="#topic+MultiChannel.WUC">MultiChannel.WUC</a></code>.</p>
</td></tr>
<tr><td><code id="plot.MultiChannelClusters_+3A_xlab">xlab</code></td>
<td>
<p>a character string. The x-axis label for the plot.</p>
</td></tr>
<tr><td><code id="plot.MultiChannelClusters_+3A_ylab">ylab</code></td>
<td>
<p>a character string. The x-axis label for the plot.</p>
</td></tr>
<tr><td><code id="plot.MultiChannelClusters_+3A_main">main</code></td>
<td>
<p>a character string. The title for the plot.</p>
</td></tr>
<tr><td><code id="plot.MultiChannelClusters_+3A_sub">sub</code></td>
<td>
<p>a character string. The subtitle for the plot.</p>
</td></tr>
<tr><td><code id="plot.MultiChannelClusters_+3A_col.clusters">col.clusters</code></td>
<td>
<p>a vector of colors, defined either by integers or by color names. If the length is shorter than the number of clusters, the colors will be reused.</p>
</td></tr>
<tr><td><code id="plot.MultiChannelClusters_+3A_...">...</code></td>
<td>
<p>arguments passed to <code><a href="base.html#topic+plot">plot</a></code> function in package <span class="pkg">graphics</span>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function visualizes the input data as sticks whose heights are the weights. Colors indicate clusters. See <code><a href="#topic+MultiChannel.WUC">MultiChannel.WUC</a></code> for examples.
</p>


<h3>Value</h3>

<p>An object of class &quot;<code>MultiChannelClusters</code>&quot; defined in <code><a href="#topic+MultiChannel.WUC">MultiChannel.WUC</a></code>.
</p>


<h3>Author(s)</h3>

<p>Joe Song
</p>

<hr>
<h2 id='plotBIC'>Plot Bayesian Information Criterion as a Function of Number of Clusters</h2><span id='topic+plotBIC'></span>

<h3>Description</h3>

<p>Plot Bayesian information criterion (BIC) as a function of the number of clusters obtained from optimal univariate clustering results returned from <code>Ckmeans.1d.dp</code>. The BIC normalized by sample size (BIC/n) is shown.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotBIC(
  ck, xlab="Number of clusters k",
  ylab = "BIC/n", type="b",
  sub=paste("n =", length(ck$cluster)),
  main=paste("Bayesian information criterion",
             "(normalized by sample size)", sep="\n"),
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plotBIC_+3A_ck">ck</code></td>
<td>
<p>an object of class <code>Ckmeans.1d.dp</code> returned by <code><a href="#topic+Ckmeans.1d.dp">Ckmeans.1d.dp</a></code>.</p>
</td></tr>
<tr><td><code id="plotBIC_+3A_xlab">xlab</code></td>
<td>
<p>a character string. The x-axis label for the plot.</p>
</td></tr>
<tr><td><code id="plotBIC_+3A_ylab">ylab</code></td>
<td>
<p>a character string. The x-axis label for the plot.</p>
</td></tr>
<tr><td><code id="plotBIC_+3A_type">type</code></td>
<td>
<p>the type of plot to be drawn. See <code><a href="graphics.html#topic+plot">plot</a></code>.</p>
</td></tr>
<tr><td><code id="plotBIC_+3A_main">main</code></td>
<td>
<p>a character string. The title for the plot.</p>
</td></tr>
<tr><td><code id="plotBIC_+3A_sub">sub</code></td>
<td>
<p>a character string. The subtitle for the plot.</p>
</td></tr>
<tr><td><code id="plotBIC_+3A_...">...</code></td>
<td>
<p>arguments passed to <code><a href="base.html#topic+plot">plot</a></code> function in package <span class="pkg">graphics</span>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function visualizes the input data as sticks whose heights are the weights. It uses different colors to indicate optimal <var>k</var>-means clusters. The method to calcualte BIC based on Gaussian mixture models estimated on a univariate clustering is described in (Song and Zhong 2020).
</p>


<h3>Value</h3>

<p>An object of class &quot;<code>Ckmeans.1d.dp</code>&quot; defined in <code><a href="#topic+Ckmeans.1d.dp">Ckmeans.1d.dp</a></code>.
</p>


<h3>Author(s)</h3>

<p>Joe Song
</p>


<h3>References</h3>

<p>Song M, Zhong H (2020).
&ldquo;Efficient weighted univariate clustering maps outstanding dysregulated genomic zones in human cancers.&rdquo;
<em>Bioinformatics</em>, <b>36</b>(20), 5027&ndash;5036.
<a href="https://doi.org/10.1093/bioinformatics/btaa613">doi:10.1093/bioinformatics/btaa613</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Example: clustering data generated from a Gaussian mixture
#          model of two components
x &lt;- rnorm(50, mean=-1, sd=0.3)
x &lt;- append(x, rnorm(50, mean=1, sd=0.3) )
res &lt;- Ckmeans.1d.dp(x)
plotBIC(res)

y &lt;- (rnorm(length(x)))^2
res &lt;- Ckmeans.1d.dp(x, y=y)
plotBIC(res)
</code></pre>

<hr>
<h2 id='print.Ckmeans.1d.dp'>Print Optimal Univariate Clustering Results</h2><span id='topic+print.Ckmeans.1d.dp'></span><span id='topic+print.Ckmedian.1d.dp'></span>

<h3>Description</h3>

<p>Print optimal univariate clustering results obtained from <code>Ckmeans.1d.dp</code> or <code>Ckmedian.1d.dp</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'Ckmeans.1d.dp'
print(x, ...)
## S3 method for class 'Ckmedian.1d.dp'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print.Ckmeans.1d.dp_+3A_x">x</code></td>
<td>
<p>object returned by calling <code>Ckmeans.1d.dp</code> or <code>Cksegs.1d.dp</code>.</p>
</td></tr>
<tr><td><code id="print.Ckmeans.1d.dp_+3A_...">...</code></td>
<td>
<p>arguments passed to <code><a href="base.html#topic+print">print</a></code> function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Function <code>print.Ckmeans.1d.dp</code> and <code>print.Ckmedian.1d.dp</code> prints the maximum ratio of between-cluster sum of squares to total sum of squares unless all input elements are zero. The ratio is an indicator of maximum achievable clustering quality given the number of clusters: 100% for a perfect clustering and 0% for no cluster patterns.
</p>


<h3>Value</h3>

<p>An object of class &quot;<code>Ckmeans.1d.dp</code>&quot; or &quot;<code>Ckmedian.1d.dp</code>&quot; as defined in <code><a href="#topic+Ckmeans.1d.dp">Ckmeans.1d.dp</a></code>.
</p>


<h3>Author(s)</h3>

<p>Joe Song and Haizhou Wang
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Example: clustering data generated from a Gaussian
#          mixture model of two components
x &lt;- c(rnorm(15, mean=-1, sd=0.3),
       rnorm(15, mean=1, sd=0.3))
res &lt;- Ckmeans.1d.dp(x)
print(res)

res &lt;- Ckmedian.1d.dp(x)
print(res)

y &lt;- (rnorm(length(x)))^2
res &lt;- Ckmeans.1d.dp(x, y=y)
print(res)

res &lt;- Ckmedian.1d.dp(x)
print(res)
</code></pre>

<hr>
<h2 id='print.Cksegs.1d.dp'>Print Optimal Univariate Segmentation Results</h2><span id='topic+print.Cksegs.1d.dp'></span>

<h3>Description</h3>

<p>Print optimal univariate segmentation results obtained from <code>Cksegs.1d.dp</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'Cksegs.1d.dp'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print.Cksegs.1d.dp_+3A_x">x</code></td>
<td>
<p>object returned by calling <code>Cksegs.1d.dp</code>.</p>
</td></tr>
<tr><td><code id="print.Cksegs.1d.dp_+3A_...">...</code></td>
<td>
<p>arguments passed to <code><a href="base.html#topic+print">print</a></code> function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Function <code>print.Cksegs.1d.dp</code> prints the maximum ratio of between-cluster sum of squares to total sum of squares unless all input elements are zero. The ratio is an indicator of maximum achievable clustering quality given the number of clusters: 100% for a perfect clustering and 0% for no cluster patterns.
</p>


<h3>Value</h3>

<p>An object of class &quot;<code>Cksegs.1d.dp</code>&quot; as defined in <code><a href="#topic+Cksegs.1d.dp">Cksegs.1d.dp</a></code>.
</p>


<h3>Author(s)</h3>

<p>Joe Song
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Example: clustering data generated from a Gaussian
#          mixture model of two components
x &lt;- c(rnorm(15, mean=-1, sd=0.3),
       rnorm(15, mean=1, sd=0.3))

y &lt;- x^3
res &lt;- Cksegs.1d.dp(y, x=x)
print(res, lwd=2)
</code></pre>

<hr>
<h2 id='Univariate+20Clustering'>Optimal (Weighted) Univariate Clustering</h2><span id='topic+Univariate+20Clustering'></span><span id='topic+Ckmeans.1d.dp'></span><span id='topic+Ckmedian.1d.dp'></span>

<h3>Description</h3>

<p>Perform optimal univariate <code class="reqn">k</code>-means or <code class="reqn">k</code>-median clustering in linear (fastest), loglinear, or quadratic (slowest) time.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Ckmeans.1d.dp(x, k=c(1,9), y=1,
              method=c("linear", "loglinear", "quadratic"),
              estimate.k=c("BIC", "BIC 3.4.12"))

Ckmedian.1d.dp(x, k=c(1,9), y=1,
               method=c("linear", "loglinear", "quadratic"),
               estimate.k=c("BIC", "BIC 3.4.12"))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Univariate+2B20Clustering_+3A_x">x</code></td>
<td>
<p>a numeric vector of data to be clustered. All <code>NA</code> elements must be removed from <code>x</code> before calling this function. The function will run faster on sorted <code>x</code> (in non-decreasing order) than an unsorted input.</p>
</td></tr>
<tr><td><code id="Univariate+2B20Clustering_+3A_k">k</code></td>
<td>
<p>either an exact integer number of clusters, or a vector of length two specifying the minimum and maximum numbers of clusters to be examined. The default is <code>c(1,9)</code>. When <code>k</code> is a range, the actual number of clusters is determined by Bayesian information criterion.</p>
</td></tr>
<tr><td><code id="Univariate+2B20Clustering_+3A_y">y</code></td>
<td>
<p>a value of 1 (default) to specify equal weights of 1 for each element in <code>x</code>, or a numeric vector of unequal non-negative weights for each element in <code>x</code>. It is highly recommended to use positive (instead of zero) weights to account for the influence of every element. The weights have a strong impact on the clustering result. When the number of clusters <code>k</code> is given as a range, the weights should be linearly scaled to sum up to the observed sample size. Currently, <code>Ckmedian.1d.dp</code> only works with an equal weight of 1.</p>
</td></tr>
<tr><td><code id="Univariate+2B20Clustering_+3A_method">method</code></td>
<td>
<p>a character string to specify the speedup method to the original cubic runtime dynamic programming. The default is <code>"linear"</code>. All methods generate the same optimal results but differ in runtime or memory usage. See Details.</p>
</td></tr>
<tr><td><code id="Univariate+2B20Clustering_+3A_estimate.k">estimate.k</code></td>
<td>
<p>a character string to specify the method to estimate optimal <code>k</code>. This argument is effective only when a range for <code>k</code> is provided. The default is <code>"BIC"</code>. See Details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>Ckmean.1d.dp</code> minimizes unweighted or weighted within-cluster sum of squared distance (L2).
</p>
<p><code>Ckmedian.1d.dp</code> minimizes within-cluster sum of distance (L1). Only unweighted solution is implemented and guarantees optimality.
</p>
<p>In contrast to the heuristic <var>k</var>-means algorithms implemented in function <code><a href="stats.html#topic+kmeans">kmeans</a></code>, this function optimally assigns elements in numeric vector <code>x</code> into <code>k</code> clusters by dynamic programming (Wang and Song 2011; Song and Zhong 2020). It minimizes the total of within-cluster sums of squared distances (<var>withinss</var>) between each element and its corresponding cluster mean. When a range is provided for <code>k</code>, the exact number of clusters is determined by Bayesian information criterion (Song and Zhong 2020). Different from the heuristic <var>k</var>-means algorithms whose results may be non-optimal or change from run to run, the result of Ckmeans.1d.dp is guaranteed to be optimal and reproducible, and its advantage in efficiency and accuracy over heuristic <var>k</var>-means methods is most pronounced at large <var>k</var>.
</p>
<p>The <code>estimate.k</code> argument specifies the method to select optimal <code>k</code> based on the Gaussian mixture model using the Bayesian information criterion (BIC). When <code>estimate.k="BIC"</code>, it effectively deals with variance estimation for a cluster with identical values. When <code>estimate.k="BIC 3.4.12"</code>, it uses the code in version 3.4.12 and earlier to estimate <code>k</code>.
</p>
<p>The <code>method</code> argument specifies one of three options to speed up the original dynamic programming taking a runtime cubic in sample size <var>n</var>. The default <code>"linear"</code> option, giving a total runtime of <code class="reqn">O(n \lg n + kn)</code> or <code class="reqn">O(kn)</code> (if <code>x</code> is already sorted in ascending order) is the fastest option but uses the most memory (still <code class="reqn">O(kn)</code>) (Song and Zhong 2020); the <code>"loglinear"</code> option, with a runtime of <code class="reqn">O(kn \lg n)</code>, is slightly slower but uses the least memory (Song and Zhong 2020); the slowest <code>"quadratic"</code> option (Wang and Song 2011), with a runtime of <code class="reqn">O(kn^2)</code>, is provided for the purpose of testing on small data sets.
</p>
<p>When the sample size <var>n</var> is too large to create two <code class="reqn">k \times n</code> dynamic programming matrices in memory, we recommend the heuristic solutions implemented in the <code><a href="stats.html#topic+kmeans">kmeans</a></code> function in package <span class="pkg"><a href="stats.html#topic+stats">stats</a></span>.
</p>


<h3>Value</h3>

<p>An object of class &quot;<code>Ckmeans.1d.dp</code>&quot; or &quot;<code>Ckmedian.1d.dp</code>&quot;. It is a list containing the following components:
</p>
<table role = "presentation">
<tr><td><code>cluster</code></td>
<td>
<p>a vector of clusters assigned to each element in <code>x</code>. Each cluster is indexed by an integer from 1 to <code>k</code>.</p>
</td></tr>
<tr><td><code>centers</code></td>
<td>
<p>a numeric vector of the (weighted) means for each cluster.</p>
</td></tr>
<tr><td><code>withinss</code></td>
<td>
<p>a numeric vector of the (weighted) within-cluster sum of squares for each cluster.</p>
</td></tr>
<tr><td><code>size</code></td>
<td>
<p>a vector of the (weighted) number of elements in each cluster.</p>
</td></tr>
<tr><td><code>totss</code></td>
<td>
<p>total sum of (weighted) squared distances between each element and the sample mean. This statistic is not dependent on the clustering result.</p>
</td></tr>
<tr><td><code>tot.withinss</code></td>
<td>
<p>total sum of (weighted) within-cluster squared distances between each element and its cluster mean. This statistic is minimized given the number of clusters.</p>
</td></tr>
<tr><td><code>betweenss</code></td>
<td>
<p>sum of (weighted) squared distances between each cluster mean and sample mean. This statistic is maximized given the number of clusters.</p>
</td></tr>
<tr><td><code>xname</code></td>
<td>
<p>a character string. The actual name of the <code>x</code> argument.</p>
</td></tr>
<tr><td><code>yname</code></td>
<td>
<p>a character string. The actual name of the <code>y</code> argument.</p>
</td></tr>
</table>
<p>Each class has a print and a plot method, which are described along with <code><a href="#topic+print.Ckmeans.1d.dp">print.Ckmeans.1d.dp</a></code> and <code><a href="#topic+plot.Ckmeans.1d.dp">plot.Ckmeans.1d.dp</a></code>.
</p>


<h3>Author(s)</h3>

<p>Joe Song and Haizhou Wang
</p>


<h3>References</h3>

<p>Song M, Zhong H (2020).
&ldquo;Efficient weighted univariate clustering maps outstanding dysregulated genomic zones in human cancers.&rdquo;
<em>Bioinformatics</em>, <b>36</b>(20), 5027&ndash;5036.
<a href="https://doi.org/10.1093/bioinformatics/btaa613">doi:10.1093/bioinformatics/btaa613</a>.<br /><br /> Wang H, Song M (2011).
&ldquo;Ckmeans.1d.dp: Optimal <code class="reqn">k</code>-means clustering in one dimension by dynamic programming.&rdquo;
<em>The R Journal</em>, <b>3</b>(2), 29&ndash;33.
<a href="https://doi.org/10.32614/RJ-2011-015">doi:10.32614/RJ-2011-015</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ahist">ahist</a></code>, <code><a href="#topic+plot.Ckmeans.1d.dp">plot.Ckmeans.1d.dp</a></code>, <code><a href="#topic+print.Ckmeans.1d.dp">print.Ckmeans.1d.dp</a></code> in this package.
</p>
<p><code><a href="stats.html#topic+kmeans">kmeans</a></code> in package <span class="pkg"><a href="stats.html#topic+stats">stats</a></span> that implements several heuristic <code class="reqn">k</code>-means algorithms.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Ex. 1 The number of clusters is provided.
# Generate data from a Gaussian mixture model of three components
x &lt;- c(rnorm(50, sd=0.2), rnorm(50, mean=1, sd=0.3), rnorm(100,
       mean=-1, sd=0.25))
# Divide x into 3 clusters
k &lt;- 3

result &lt;- Ckmedian.1d.dp(x, k)

plot(result, main="Optimal univariate k-median given k")

result &lt;- Ckmeans.1d.dp(x, k)

plot(result, main="Optimal univariate k-means given k")

plot(x, col=result$cluster, pch=result$cluster, cex=1.5,
     main="Optimal univariate k-means clustering given k",
     sub=paste("Number of clusters given:", k))
abline(h=result$centers, col=1:k, lty="dashed", lwd=2)
legend("bottomleft", paste("Cluster", 1:k), col=1:k, pch=1:k,
       cex=1.5, bty="n")

# Ex. 2 The number of clusters is determined by Bayesian
#       information criterion
# Generate data from a Gaussian mixture model of three components
x &lt;- c(rnorm(50, mean=-3, sd=1), rnorm(50, mean=0, sd=.5),
       rnorm(50, mean=3, sd=1))
# Divide x into k clusters, k automatically selected (default: 1~9)

result &lt;- Ckmedian.1d.dp(x)

plot(result, main="Optimal univariate k-median with k estimated")

result &lt;- Ckmeans.1d.dp(x)

plot(result, main="Optimal univariate k-means with k estimated")

k &lt;- max(result$cluster)
plot(x, col=result$cluster, pch=result$cluster, cex=1.5,
     main="Optimal univariate k-means clustering with k estimated",
     sub=paste("Number of clusters is estimated to be", k))
abline(h=result$centers, col=1:k, lty="dashed", lwd=2)
legend("topleft", paste("Cluster", 1:k), col=1:k, pch=1:k,
       cex=1.5, bty="n")

# Ex. 3 Segmenting a time course using optimal weighted
#       univariate clustering
n &lt;- 160
t &lt;- seq(0, 2*pi*2, length=n)
n1 &lt;- 1:(n/2)
n2 &lt;- (max(n1)+1):n
y1 &lt;- abs(sin(1.5*t[n1]) + 0.1*rnorm(length(n1)))
y2 &lt;- abs(sin(0.5*t[n2]) + 0.1*rnorm(length(n2)))
y &lt;- c(y1, y2)

w &lt;- y^8 # stress the peaks
res &lt;- Ckmeans.1d.dp(t, k=c(1:10), w)
plot(res)
plot(t, w, main = "Time course weighted k-means",
     col=res$cluster, pch=res$cluster,
     xlab="Time t", ylab="Transformed intensity w",
     type="h")
abline(v=res$centers, col="chocolate", lty="dashed")
text(res$centers, max(w) * .95, cex=0.5, font=2,
     paste(round(res$size / sum(res$size) * 100), "/ 100"))
</code></pre>

<hr>
<h2 id='Univariate+20Segmentation'>Optimal Univariate Segmentation</h2><span id='topic+Univariate+20Segmentation'></span><span id='topic+Cksegs.1d.dp'></span>

<h3>Description</h3>

<p>Perform optimal univariate <code class="reqn">k</code>-segmentation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Cksegs.1d.dp(y, k=c(1,9), x=seq_along(y),
             method=c("quadratic", "linear", "loglinear"),
             estimate.k=c("BIC", "BIC 3.4.12"))
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="Univariate+2B20Segmentation_+3A_y">y</code></td>
<td>
<p>a numeric vector of y values. Values can be negative.</p>
</td></tr>
<tr><td><code id="Univariate+2B20Segmentation_+3A_k">k</code></td>
<td>
<p>either an exact integer number of clusters, or a vector of length two specifying the minimum and maximum numbers of clusters to be examined. The default is <code>c(1,9)</code>. When <code>k</code> is a range, the actual number of clusters is determined by Bayesian information criterion.</p>
</td></tr>
<tr><td><code id="Univariate+2B20Segmentation_+3A_x">x</code></td>
<td>
<p>an optional numeric vector of data to be clustered. All <code>NA</code> elements must be removed from <code>x</code> before calling this function. The function will run faster on sorted <code>x</code> (in non-decreasing order) than an unsorted input.</p>
</td></tr>
<tr><td><code id="Univariate+2B20Segmentation_+3A_method">method</code></td>
<td>
<p>a character string to specify the speedup method to the original cubic runtime dynamic programming. The default is <code>"quadratic"</code>, which generates optimal results. The other options do not guarantee optimal solution and differ in runtime or memory usage. See Details.</p>
</td></tr>
<tr><td><code id="Univariate+2B20Segmentation_+3A_estimate.k">estimate.k</code></td>
<td>
<p>a character string to specify the method to estimate optimal <code>k</code>. The default is <code>"BIC"</code>. See Details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>Cksegs.1d.dp</code> minimizes within-cluster sum of squared distance on <code>y</code>. It offers optimal piece-wise constant approximation of <code>y</code> within clusters of <code>x</code>. Only <code>method="quadratic"</code> guarantees optimality. The &quot;linear&quot; and &quot;loglinear&quot; options are faster but not always optimal and are provided for comparison purposes.
</p>
<p>The Bayesian information criterion (BIC) method to select optimal <code>k</code> is updated to deal with duplicates in the data. Otherwise, the estimated k would be the same with previous versions. Set <code>estimate.k="BIC"</code> to use the latest method; use <code>estimate.k="BIC 3.4.12"</code> to use the BIC method in version 3.4.12 or earlier to estimated <code>k</code> from the given range. This option is effective only when a range for <code>k</code> is provided.
</p>
<p><code>method</code> specifies one of three options to speed up the original dynamic programming taking a runtime cubic in sample size <var>n</var>. The default <code>"quadratic"</code> option, with a runtime of <code class="reqn">O(kn^2)</code>, guarantees optimality. The next two options do not guarantee optimality. The <code>"linear"</code> option, giving a total runtime of <code class="reqn">O(n \lg n + kn)</code> or <code class="reqn">O(kn)</code> (if <code>x</code> is already sorted in ascending order) is the fastest option but uses the most memory (still <code class="reqn">O(kn)</code>); the <code>"loglinear"</code> option, with a runtime of <code class="reqn">O(kn \lg n)</code>, is slightly slower but uses the least memory.
</p>


<h3>Value</h3>

<p>An object of class &quot;<code>Cksegs.1d.dp</code>&quot;. It is a list containing the following components:
</p>
<table role = "presentation">
<tr><td><code>cluster</code></td>
<td>
<p>a vector of clusters assigned to each element in <code>x</code>. Each cluster is indexed by an integer from 1 to <code>k</code>.</p>
</td></tr>
<tr><td><code>centers</code></td>
<td>
<p>a numeric vector of the (weighted) means for each cluster.</p>
</td></tr>
<tr><td><code>withinss</code></td>
<td>
<p>a numeric vector of the (weighted) within-cluster sum of squares for each cluster.</p>
</td></tr>
<tr><td><code>size</code></td>
<td>
<p>a vector of the (weighted) number of elements in each cluster.</p>
</td></tr>
<tr><td><code>totss</code></td>
<td>
<p>total sum of (weighted) squared distances between each element and the sample mean. This statistic is not dependent on the clustering result.</p>
</td></tr>
<tr><td><code>tot.withinss</code></td>
<td>
<p>total sum of (weighted) within-cluster squared distances between each element and its cluster mean. This statistic is minimized given the number of clusters.</p>
</td></tr>
<tr><td><code>betweenss</code></td>
<td>
<p>sum of (weighted) squared distances between each cluster mean and sample mean. This statistic is maximized given the number of clusters.</p>
</td></tr>
<tr><td><code>xname</code></td>
<td>
<p>a character string. The actual name of the <code>x</code> argument.</p>
</td></tr>
<tr><td><code>yname</code></td>
<td>
<p>a character string. The actual name of the <code>y</code> argument.</p>
</td></tr>
</table>
<p>The class has a print and a plot method: <code><a href="#topic+print.Cksegs.1d.dp">print.Cksegs.1d.dp</a></code> and <code><a href="#topic+plot.Cksegs.1d.dp">plot.Cksegs.1d.dp</a></code>.
</p>


<h3>Author(s)</h3>

<p>Joe Song
</p>


<h3>See Also</h3>

<p><code><a href="#topic+plot.Cksegs.1d.dp">plot.Cksegs.1d.dp</a></code> and <code><a href="#topic+print.Cksegs.1d.dp">print.Cksegs.1d.dp</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Ex 1. Segmenting by y

y &lt;- c(1,1,1,2,2,2,4,4,4,4)

res &lt;- Cksegs.1d.dp(y, k=c(1:10))

main &lt;- "k-segs giving 3 clusters\nsucceeded in finding segments"

opar &lt;- par(mfrow=c(1,2))

plot(res, main=main, xlab="x")

res &lt;- Ckmeans.1d.dp(x=seq_along(y), k=c(1:10), y)
main &lt;- "Weighted k-means giving 1 cluster\nfailed to find segments"

plot(res, main=main, xlab="x")

par(opar)

# Ex 2. Segmenting by y

y &lt;- c(1,1,1.1,1, 2,2.5,2, 4,5,4,4)
res &lt;- Cksegs.1d.dp(y, k=c(1:10))
plot(res, xlab="x")

# Ex 3. Segmenting a sinusoidal curve by y
x &lt;- 1:125
y &lt;- sin(x * .2)
res.q &lt;- Cksegs.1d.dp(y, k=8, x=x)
plot(res.q, lwd=3, xlab="x")

# Ex 4. Segmenting by y

y &lt;- rep(c(1,-3,4,-2), each=20)
y &lt;- y + 0.5*rnorm(length(y))
k &lt;- 1:10
res.q &lt;- Cksegs.1d.dp(y, k=k, method="quadratic")
main &lt;- paste("Cksegs (method=\"quadratic\"):\ntot.withinss =",
              format(res.q$tot.withinss, digits=4), "BIC =",
              format(res.q$BIC[length(res.q$size)], digits=4),
              "\nGUARANTEE TO BE OPTIMAL")
plot(res.q, main=main, xlab="x")
res.l &lt;- Cksegs.1d.dp(y, k=k, method="linear")
main &lt;- paste("Cksegs (method=\"linear\"):\ntot.withinss =",
               format(res.l$tot.withinss, digits=4), "BIC =",
              format(res.l$BIC[length(res.l$size)], digits=4),
               "\nFAST BUT MAY NOT BE OPTIMAL")
plot(res.l, main=main, xlab="x")
res.g &lt;- Cksegs.1d.dp(y, k=k, method="loglinear")
main &lt;- paste("Cksegs (method=\"loglinear\"):\ntot.withinss =",
              format(res.g$tot.withinss, digits=4), "BIC =",
              format(res.g$BIC[length(res.g$size)], digits=4),
              "\nFAST BUT MAY NOT BE OPTIMAL")
plot(res.g, main=main, xlab="x")
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
