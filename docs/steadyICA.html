<!DOCTYPE html><html lang="en"><head><title>Help for package steadyICA</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {steadyICA}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#steadyICA-package'>
<p>ICA via distance covariance, tests of mutual independence, and other ICA functions</p></a></li>
<li><a href='#compInd'>
<p>Complete Measure of Mutual Multivariate Independence</p></a></li>
<li><a href='#dcovICA'>
<p>ICA via distance covariance for 2 components</p></a></li>
<li><a href='#dcovustat'>
<p>Calculate distance covariance via U-statistics</p></a></li>
<li><a href='#frobICA'><p>match mixing matrices or ICs and calculate their Frobenius distance</p></a></li>
<li><a href='#gmultidcov'>
<p>Symmetric multivariate distance covariance for grouped components</p></a></li>
<li><a href='#infomaxICA'><p>Estimates independent components via infomax</p></a></li>
<li><a href='#matchICA'>
<p>match independent components using the Hungarian method</p></a></li>
<li><a href='#multidcov'>
<p>Symmetric multivariate distance covariance</p></a></li>
<li><a href='#permTest'>
<p>Permutation test for mutual independence.</p></a></li>
<li><a href='#rightskew'>
<p>force ICs to have positive skewness and order by skewness</p></a></li>
<li><a href='#steadyICA'>
<p>Estimate independent components by minimizing distance covariance</p></a></li>
<li><a href='#theta2W'>
<p>Convert angles to an orthogonal matrix.</p></a></li>
<li><a href='#W2theta'>
<p>Convert an orthogonal matrix to its angular parameterization.</p></a></li>
<li><a href='#whitener'><p>Whitening function</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>ICA and Tests of Independence via Multivariate Distance
Covariance</td>
</tr>
<tr>
<td>Version:</td>
<td>1.0.1</td>
</tr>
<tr>
<td>Date:</td>
<td>2015-11-08</td>
</tr>
<tr>
<td>Author:</td>
<td>Benjamin B. Risk and Nicholas A. James and David S. Matteson</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Benjamin Risk &lt;bbr28@cornell.edu&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Functions related to multivariate measures of independence and ICA:
 -estimate independent components by minimizing distance covariance;
 -conduct a test of mutual independence based on distance covariance; 
 -estimate independent components via infomax (a popular method but generally performs poorer than mdcovica, ProDenICA, and/or fastICA, but is useful for comparisons);
 -order indepedent components by skewness;
 -match independent components from multiple estimates;
 -other functions useful in ICA.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>Depends:</td>
<td>Rcpp (&ge; 0.9.13), MASS, clue, combinat</td>
</tr>
<tr>
<td>Suggests:</td>
<td>irlba, JADE, ProDenICA, fastICA, energy</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-11-09 19:48:29 UTC; ripley</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-11-11 17:35:45 UTC</td>
</tr>
</table>
<hr>
<h2 id='steadyICA-package'>
ICA via distance covariance, tests of mutual independence, and other ICA functions
</h2><span id='topic+steadyICA-package'></span>

<h3>Description</h3>

<p>Functions related to multivariate measures of independence and ICA: <br />
-estimate independent components by minimizing distance covariance;<br />
-conduct a test of mutual independence based on distance covariance; <br />
-estimate independent components via infomax (a popular method but generally performs poorer than steadyICA or ProDenICA but is useful for comparisons);<br />
-order independent components by skewness;<br />
-match independent components from multiple estimates;<br />
-other functions useful in ICA.
</p>


<h3>Details</h3>


<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> steadyICA</td>
</tr>
<tr>
 <td style="text-align: left;">
Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
Version: </td><td style="text-align: left;"> 1.0</td>
</tr>
<tr>
 <td style="text-align: left;">
Date: </td><td style="text-align: left;"> 2015-11-08</td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> GPL (&gt;= 2)</td>
</tr>
<tr>
 <td style="text-align: left;">
Depends: </td><td style="text-align: left;"> Rcpp (&gt;= 0.9.13), MASS</td>
</tr>
<tr>
 <td style="text-align: left;">
Suggests: </td><td style="text-align: left;"> irlba, JADE, ProDenICA, fastICA
</td>
</tr>

</table>



<h3>Author(s)</h3>

<p>Benjamin B. Risk and Nicholas A. James and David S. Matteson.<br />
Maintainer: Benjamin Risk &lt;bbr28@cornell.edu&gt;
</p>


<h3>References</h3>

<p>Bernaards, C. &amp; Jennrich, R. (2005) Gradient projection algorithms and software for arbitrary rotation criteria in factor analysis. <em>Educational and Psychological Measurement</em> 65, 676-696
</p>
<p>Matteson, D. S. &amp; Tsay, R. Independent component analysis via U-Statistics. 
&lt;http://www.stat.cornell.edu/~matteson/#ICA&gt;
</p>
<p>Szekely, G., Rizzo, M. &amp; Bakirov, N. Measuring and testing dependence by correlation of distances. (2007) <em>The Annals of Statistics</em>, 35, 2769-2794.
</p>
<p>Tichavsky, P. &amp; Koldovsky, Z. Optimal pairing of signal components separated by blind techniques. (2004) <em>Signal Processing Letters</em> 11, 119-122.
</p>


<h3>See Also</h3>

<p><code><a href="fastICA.html#topic+fastICA">fastICA</a></code>
<code><a href="ProDenICA.html#topic+ProDenICA">ProDenICA::ProDenICA</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#see steadyICA
</code></pre>

<hr>
<h2 id='compInd'>
Complete Measure of Mutual Multivariate Independence</h2><span id='topic+compInd'></span>

<h3>Description</h3>

<p>Calculates a complete empirical measure of mutual multivariate independence. Makes 
use of the utils::combn function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>compInd(S,group=1:ncol(S),alpha=1)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="compInd_+3A_s">S</code></td>
<td>
<p>The n x d matrix for which you wish to calculate the 
dependence between d columns from n samples.</p>
</td></tr>
<tr><td><code id="compInd_+3A_group">group</code></td>
<td>
<p>A length d vector which indicates group membership for each component.</p>
</td></tr>
<tr><td><code id="compInd_+3A_alpha">alpha</code></td>
<td>
<p>The index used in calculating the distance between sample observations.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a scalar equal to the empirical multivariate distance between the observed samples, 
and their grouped counterpart.</p>


<h3>Note</h3>

<p>Suppose that the each component belongs to exactly one of C groups. 
This method makes use of the utils::combn and combinat::permn functions. As a result it will be both computationally and memory intensive, even for small to moderate n and small C.
</p>


<h3>Author(s)</h3>

<p>Nicholas James
</p>


<h3>References</h3>

<p>Chasalow, Scott (2012) combinat: Combinatorics Utilities &lt;http://CRAN.R-project.org/package=combinat
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dcovustat">dcovustat</a></code>,
<code><a href="energy.html#topic+dcov">energy::dcov</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(steadyICA)
library(combinat)
set.seed(100)
S = matrix(rnorm(40),ncol=4)
group = c(1,2,3,3)
compInd(S,group,1)

</code></pre>

<hr>
<h2 id='dcovICA'>
ICA via distance covariance for 2 components</h2><span id='topic+dcovICA'></span>

<h3>Description</h3>

<p>This algorithm finds the rotation which minimizes the distance covariance between two orthogonal components via the angular parameterization of a 2x2 orthogonal matrix with the function stats::optimize. The results will be (approximately)
equivalent to steadyICA but this function is much faster (but does not extend to higher dimensions).</p>


<h3>Usage</h3>

<pre><code class='language-R'>dcovICA(Z, theta.0 = 0)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="dcovICA_+3A_z">Z</code></td>
<td>
<p>The whitened n x d data matrix, where n is the number of observations and d the number of components.</p>
</td></tr>
<tr><td><code id="dcovICA_+3A_theta.0">theta.0</code></td>
<td>
<p>Determines the interval to be searched by the optimizer: lower bound = theta.0, upper
bound = pi/2. Changing theta.0 affects the initial value,
where the initial value = theta.0+(1/2+sqrt(5)/2)*pi/2, see <a href="stats.html#topic+optimize">optimize</a>.</p>
</td></tr>
</table>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>theta.hat</code></td>
<td>
<p>Estimated minimum.</p>
</td></tr>
<tr><td><code>W</code></td>
<td>
<p>W = t(theta2W(theta.hat))</p>
</td></tr> 
<tr><td><code>S</code></td>
<td>
<p>Estimated independent components.</p>
</td></tr>
<tr><td><code>obj</code></td>
<td>
<p>The distance covariance of S.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>David Matteson and Benjamin Risk
</p>


<h3>References</h3>

<p>Matteson, D. S. &amp; Tsay, R. Independent component analysis via U-Statistics. 
&lt;http://www.stat.cornell.edu/~matteson/#ICA&gt;
</p>


<h3>See Also</h3>

<p><code><a href="#topic+steadyICA">steadyICA</a></code>,
<code><a href="stats.html#topic+optimize">optimize</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(JADE)
library(ProDenICA)
set.seed(123)
simS = cbind(rjordan(letter='j',n=1024),rjordan(letter='m',n=1024))
simM = mixmat(p=2)
xData = simS%*%simM
xWhitened = whitener(xData)

#Define true unmixing matrix as true M multiplied by the estimated whitener:
#Call this the target matrix:
W.true &lt;- solve(simM%*%xWhitened$whitener) 


a=Sys.time()
est.dCovICA = dcovICA(Z = xWhitened$Z,theta.0=0)
Sys.time()-a

#See the example with steadyICA for an explanation
#of the parameterization used in amari.error:
amari.error(t(est.dCovICA$W),W.true)

##NOTE: also try theta.0 = pi/4 since there may be local minima
  ## Not run: est.dcovICA = dcovICA(Z = xWhitened$Z,theta.0=pi/4)
  amari.error(t(est.dcovICA$W),W.true)
## End(Not run)

a=Sys.time()
est.steadyICA = steadyICA(X=xWhitened$Z,verbose=TRUE)
Sys.time()-a
amari.error(t(est.steadyICA$W),W.true)
##theta parameterization with optimize is much faster
</code></pre>

<hr>
<h2 id='dcovustat'>
Calculate distance covariance via U-statistics</h2><span id='topic+dcovustat'></span>

<h3>Description</h3>

<p>Calculates the square of the U-statistic formulation of distance covariance. This is  faster than the function 'dcov' in the R package 'energy'  and requires less memory. Note that negative values are possible in this version.</p>


<h3>Usage</h3>

<pre><code class='language-R'>dcovustat(x,y,alpha=1)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="dcovustat_+3A_x">x</code></td>
<td>
<p>A vector or matrix.</p>
</td></tr>
<tr><td><code id="dcovustat_+3A_y">y</code></td>
<td>
<p>A vector or matrix with the same number of observations as x, though the number of columns of x and y may differ</p>
</td></tr>
<tr><td><code id="dcovustat_+3A_alpha">alpha</code></td>
<td>
<p>A scaling parameter in the interval (0,2] used for calculating distances.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns the distance covariance U-statistic.
</p>


<h3>Note</h3>

<p>The value returned by dcovustat is equal to the square of the value returned by energy::dcov in the limit. 
</p>
<p>In dcovustat, a vector of length n is stored; in energy::dcov, an n x n matrix is stored. Thus, dcovustat requires far less memory and works for very large datasets.
</p>
<p>Even though dcovustat converges to the square of the distance covariance of the random variables x and y, it can be negative.
</p>


<h3>Author(s)</h3>

<p>David Matteson
</p>


<h3>References</h3>

<p>Matteson, D. S. &amp; Tsay, R. Independent component analysis via U-Statistics. 
&lt;http://www.stat.cornell.edu/~matteson/#ICA&gt;
</p>
<p>Szekely, G., Rizzo, M. &amp; Bakirov, N. Measuring and testing dependence by correlation of distances. (2007) <em>The Annals of Statistics</em>, 35, 2769-2794.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+multidcov">multidcov</a></code>,
<code><a href="energy.html#topic+dcov">energy::dcov</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x = rnorm(5000)
y = rbinom(5000,1,0.5)
y = y - 1*(y==0)
z = y*exp(-x) #some non-linear dependence

dcovustat(x[1:1000],y[1:1000]) #close to zero

a = Sys.time()
dcovustat(x[1:1000],z[1:1000]) #greater than zero
a = Sys.time() - a

#measures of linear dependence close to zero:
cov(x,z)
cor(rank(x),rank(z))


## Not run: 
#dcovustat differs from energy::dcov but are equal in the limit
library(energy)
b = Sys.time()
(dcov(x[1:1000],z[1:1000]))^2
b = Sys.time() - b
as.double(b)/as.double(a) #dcovustat is much faster

## energy::dcov and dcovustat become approximately equal as n increases:
c = Sys.time()
dcovustat(x,z)
c = difftime(Sys.time(), c, sec)
d = Sys.time()
(dcov(x,z)^2)
d = difftime(Sys.time(), d, sec)
as.double(d)/as.double(c) 

## End(Not run)
</code></pre>

<hr>
<h2 id='frobICA'>match mixing matrices or ICs and calculate their Frobenius distance</h2><span id='topic+frobICA'></span>

<h3>Description</h3>

<p>The ICA model is only identifiable up to signed permutations of the ICs. This function provides a similarity measure between two mixing matrices for the model X = S M + E, where X is n x p, S is n x d, and M is d x p. The input is either two mixing matrices M1 and M2 or two matrices of independent components S1 and S2. For M1 and M2, frobICA() finds the signed row permutation of M2 that minimizes the Frobenius norm between M1 and M2 using the Hungarian method. For S1 and S2, frobICA() finds the signed column permutation of S2 that minimizes the Frobenius norm between S1 and S2. This function allows the mixing matrices (or independent components) to have differing numbers of rows (respectively, columns) such that the similarity measure is defined by the matching rows (resp., columns), and the non-matching rows (resp., columns) are discarded.</p>


<h3>Usage</h3>

<pre><code class='language-R'>frobICA(M1 = NULL, M2 = NULL, S1 = NULL, S2 = NULL, standardize = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="frobICA_+3A_m1">M1</code></td>
<td>
<p>A d x p mixing matrix</p>
</td></tr>
<tr><td><code id="frobICA_+3A_m2">M2</code></td>
<td>
<p>A d x q mixing matrix</p>
</td></tr>
<tr><td><code id="frobICA_+3A_s1">S1</code></td>
<td>
<p>An n x d matrix of independent components</p>
</td></tr>
<tr><td><code id="frobICA_+3A_s2">S2</code></td>
<td>
<p>An n x q matrix of independent components</p>
</td></tr>
<tr><td><code id="frobICA_+3A_standardize">standardize</code></td>
<td>
<p>Logical. See Note.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>frobICA(M1,M2) = 0 if there exists a signed permutation of the rows of M2 such that M1 = P%*%M2, where P is a d x q signed permutation matrix, i.e., composed of 0, 1, and -1, with d &lt;= q; the function also allows d &gt; q, in which case frobICA(M1,M2) = 0 if there exists a P such that P%*% M1 = M2. Unlike other ICA performance measures, this function can accomodate non-square mixing matrices.
</p>


<h3>Value</h3>

<p>returns the Frobenius norm divided by p*min(d,q) (or n*min(d,q)) of the matched mixing matrices (resp., matched independent components).</p>


<h3>Note</h3>

<p>If standardize=TRUE, then scales the rows of M1 and M2 to have unit norm or the columns of S1 and S2 to have zero mean and sample variance equal to one. The user can supply either M1 and M2 or S1 and S2 but not both.</p>


<h3>Author(s)</h3>

<p>Benjamin Risk
</p>


<h3>References</h3>

<p>Kuhn, H. The Hungarian Method for the assignment problem Naval Research Logistics Quarterly, 1955, 2, 83 - 97
</p>
<p>Risk, B.B., D.S. Matteson, D. Ruppert, A. Eloyan, B.S. Caffo. In review, 2013. Evaluating ICA methods with an application to resting state fMRI. 
</p>


<h3>See Also</h3>

<p><code><a href="JADE.html#topic+MD">JADE::MD</a></code>
<code><a href="clue.html#topic+solve_LSAP">clue::solve_LSAP</a></code>
<code><a href="#topic+matchICA">matchICA</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>mat1 &lt;- matrix(rnorm(4*6),nrow=4)
perm &lt;- matrix(c(-1,0,0,0,0,0,1,0,0,1,0,0,0,0,0,1),4,4)
mat2 &lt;- perm%*%mat1
sqrt(sum((mat1-mat2)^2))
frobICA(M1=mat1,M2=mat2)

#Another example showing invariance to permutations:
covMat &lt;- t(mat1)%*%mat1
mvsample &lt;- matrix(rnorm(400),100,4)%*%mat1
frobICA(M1=cov(mvsample),M2=covMat)
frobICA(M1=cov(mvsample),M2=covMat[sample(1:6),])

#Example using independent components:
nObs=300
simS&lt;-cbind(rgamma(nObs, shape = 1, scale = 2),
            rgamma(nObs, shape = 3, scale = 2),
            rgamma(nObs, shape = 3, scale = 2),
            rgamma(nObs, shape = 9, scale = 0.5))
              
#not necessary in this example, but this should be done when used with ICA:            
simS &lt;- apply(simS,2,scale) 
frobICA(S1=simS,S2=simS%*%perm)
## Not run: 
#returns an error if S1 and S2 are not explicitly defined:
frobICA(simS,simS%*%perm)            

## End(Not run)            

</code></pre>

<hr>
<h2 id='gmultidcov'>
Symmetric multivariate distance covariance for grouped components</h2><span id='topic+gmultidcov'></span>

<h3>Description</h3>

<p>Calculate either the symmetric or asymmetric multivariate
distance covariance statistic for a given grouping of the components.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gmultidcov(S,group=1:ncol(S),alpha=1,symmetric=TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="gmultidcov_+3A_s">S</code></td>
<td>
<p>The n x d matrix for which you wish to calculate the 
dependence between d columns from n samples</p>
</td></tr>
<tr><td><code id="gmultidcov_+3A_group">group</code></td>
<td>
<p>A length d vector which indicates group membership for each component</p>
</td></tr>
<tr><td><code id="gmultidcov_+3A_alpha">alpha</code></td>
<td>
<p>A scaling parameter in the interval (0,2] used for calculating distances.</p>
</td></tr>
<tr><td><code id="gmultidcov_+3A_symmetric">symmetric</code></td>
<td>
<p>logical; if TRUE (the default), calculates the symmetric version of the multivariate distance covariance. See details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Suppose that the groups are numbered 1,2,...,C and that group is a vector indicating 
group membership for each component.
If symmetric==TRUE, calculates: sum_i=1^C dcovustat(S[,group==i],S[,group!=i])
If symmetric==FALSE, calculates: sum_i=1^C-1 dcovustat(S[,group==i],S[,group&gt;i])
</p>


<h3>Value</h3>

<p>Returns a scalar equal to the multivariate distance covariance statistic for grouped 
components of S.
</p>


<h3>Author(s)</h3>

<p>Nicholas James
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dcovustat">dcovustat</a></code>,
<code><a href="energy.html#topic+dcov">energy::dcov</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
library(steadyICA)
S = matrix(rnorm(300),ncol=3)
group = c(1,2,2)
gmultidcov(S,group,TRUE) # close to zero
gmultidcov(S,group,FALSE) # sill close to zero

Sigma = matrix(c(1,0.7,0,0.7,1,-0.2,0,-0.2,1),ncol=3)
X = MASS::mvrnorm(100,rep(0,3),Sigma)
gmultidcov(X,group,TRUE) # further from zero
gmultidcov(X,group,FALSE) # further from zero

</code></pre>

<hr>
<h2 id='infomaxICA'>Estimates independent components via infomax</h2><span id='topic+infomaxICA'></span>

<h3>Description</h3>

<p>Estimate independent components using the infomax criteria, which is equivalent to maximum likelihood using the logistic density, exp(-S)/(1+exp(-S))^2.</p>


<h3>Usage</h3>

<pre><code class='language-R'>infomaxICA(X, n.comp, W.list = NULL, whiten = FALSE, maxit = 500, eps = 1e-08, 
alpha.eps = 1e-08, verbose = FALSE, restarts=0) 
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="infomaxICA_+3A_x">X</code></td>
<td>
<p>the n x p data matrix</p>
</td></tr>
<tr><td><code id="infomaxICA_+3A_n.comp">n.comp</code></td>
<td>
<p>number of components to be estimated</p>
</td></tr>
<tr><td><code id="infomaxICA_+3A_w.list">W.list</code></td>
<td>
<p>list of orthogonal matrices for initialization</p>
</td></tr>
<tr><td><code id="infomaxICA_+3A_whiten">whiten</code></td>
<td>
<p>Whitens the data before applying ICA, i.e., X%*%whitener = Z, where Z has mean zero and empirical covariance equal to the identity matrix, and Z is then used as the input.</p>
</td></tr>
<tr><td><code id="infomaxICA_+3A_maxit">maxit</code></td>
<td>
<p>maximum number of iterations</p>
</td></tr>
<tr><td><code id="infomaxICA_+3A_eps">eps</code></td>
<td>
<p>algorithm terminates when the norm of the gradient is less than eps</p>
</td></tr>
<tr><td><code id="infomaxICA_+3A_alpha.eps">alpha.eps</code></td>
<td>
<p>tolerance controlling the level of annealing: algorithm terminates with a warning if the learning parameter is less than alpha.eps</p>
</td></tr>
<tr><td><code id="infomaxICA_+3A_verbose">verbose</code></td>
<td>
<p>if TRUE, prints (1) the value of the infomax objective function at each iteration, (2) the norm of the gradient, and (3) current value of the learning parameter alpha.</p>
</td></tr>
<tr><td><code id="infomaxICA_+3A_restarts">restarts</code></td>
<td>
<p>An integer determining the number of initial matrices to use in estimating the ICA model. The objective function has local optima, so multiple starting values are recommended. If whiten=TRUE, then generates random orthogonal matrices. If whiten=FALSE, generate random matrices from rnorm(). See code for details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is an R version of ICA using the infomax criteria that provides an alternative to Matlab code (<a href="ftp://ftp.cnl.salk.edu/pub/tony/sep96.public">ftp://ftp.cnl.salk.edu/pub/tony/sep96.public</a>), but with a few modifications. First, we use the full data (the so-called offline algorithm) in each iteration rather than an online algorithm with batches. Secondly, we use an adaptive method to choose the step size (based upon Bernaards and Jennrich 2005), which speeds up convergence. We also omitted the bias term (intercept) included in the original formulation because we centered our data. 
</p>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>S</code></td>
<td>
<p>the estimated independent components</p>
</td></tr>
<tr><td><code>W</code></td>
<td>
<p>if whiten=TRUE, returns the orthogonal unmixing matrix; no value is returned when whiten=FALSE</p>
</td></tr>
<tr><td><code>M</code></td>
<td>
<p>Returns the estimated mixing matrix for the model X = S M, where X is not pre-whitened (although X is centered)</p>
</td></tr>
<tr><td><code>f</code></td>
<td>
<p>the value of the objective function at the estimated S</p>
</td></tr>
<tr><td><code>Table</code></td>
<td>
<p>summarizes algorithm status at each iteration</p>
</td></tr>
<tr><td><code>convergence</code></td>
<td>
<p>1 if norm of the gradient is less than eps, 2 if the learning parameter was smaller than alpha.eps, which usually means the gradient is sufficiently small, 0 otherwise</p>
</td></tr>
</table>


<h3>Note</h3>

<p>In contrast to most other ICA methods, W is not contrained to be orthogonal. 
</p>


<h3>Author(s)</h3>

<p>Benjamin Risk
</p>


<h3>References</h3>

<p>Bell, A. &amp; Sejnowski, T. An information-maximization approach to blind separation and blind deconvolution Neural computation, <em>Neural computation</em>, 1995, 7, 1129-1159.
</p>
<p>Bernaards, C. A. and Jennrich, R. I. (2005) Gradient Projection Algorithms and
Software for Arbitrary Rotation Criteria in Factor Analysis, <em>Educational and
Psychological Measurement</em> 65, 676-696. &lt;http://www.stat.ucla.edu/research/gpa&gt;
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Example when p &gt; d. The MD function and amari measures 
# are not defined for M. We can compare the 
# "true W inverse", which is the mixing matrix multiplied 
# by the whitening matrix; alternatively, we can use
# multidcov::frobICA. These two approaches are
# demonstrated below:

set.seed(999)
nObs &lt;- 1024
nComp &lt;- 3

# simulate from gamma distributions with
# varying amounts of skewness:
simS&lt;-cbind(rgamma(nObs, shape = 1, scale = 2),
            rgamma(nObs, shape = 3, scale = 2),
            rgamma(nObs, shape = 9, scale = 0.5))

#standardize by expected value and variance:
simS[,1] = (simS[,1] - 1*2)/sqrt(1*2^2)
simS[,2] = (simS[,2] - 3*2)/sqrt(3*2^2)
simS[,3] = (simS[,3] - 9*0.5)/sqrt(9*0.5^2)

# slightly revised 'mixmat' function (from ProDenICA)
# for p&gt;=d: uses fastICA and ProDenICA parameterization:
myMixmat &lt;- function (p = 2, d = NULL) {
  if(is.null(d)) d = p
  a &lt;- matrix(rnorm(d * p), d, p)
  sa &lt;- La.svd(a)
  dL &lt;- sort(runif(d) + 1)
  mat &lt;- sa$u%*%(sa$vt*dL)
  attr(mat, "condition") &lt;- dL[d]/dL[1]
  mat
}

simM &lt;- myMixmat(p = 6, d = nComp)
xData &lt;- simS%*%simM
xWhitened &lt;- whitener(xData, n.comp = nComp)
#Define a 'true' W (uses the estimated whitening matrix):
W.true &lt;- solve(simM%*%xWhitened$whitener)

estInfomax &lt;- infomaxICA(X = xData, n.comp = nComp, whiten = TRUE, verbose = TRUE)

frobICA(estInfomax$M,simM)
library(JADE)
MD(t(estInfomax$W),t(solve(W.true)))
amari.error(t(estInfomax$W),t(solve(W.true)))
</code></pre>

<hr>
<h2 id='matchICA'>
match independent components using the Hungarian method
</h2><span id='topic+matchICA'></span>

<h3>Description</h3>

<p>The ICA model is only identifiable up to signed permutations of the ICs. This function finds the signed permutation of a matrix S such that ||S%*%P - template|| is minimized. Optionally also matches the mixing matrix M.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>matchICA(S, template, M = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="matchICA_+3A_s">S</code></td>
<td>

<p>the n x d matrix of ICs to be matched
</p>
</td></tr>
<tr><td><code id="matchICA_+3A_template">template</code></td>
<td>

<p>the n x d matrix that S is matched to.
</p>
</td></tr>
<tr><td><code id="matchICA_+3A_m">M</code></td>
<td>

<p>an optional d x p mixing matrix corresponding to S that will also be matched to the template
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns the signed permutation of S that is matched to the template. If the optional argument M is provided, returns a list with the permuted S and M matrices. 
</p>


<h3>Author(s)</h3>

<p>Benjamin Risk
</p>


<h3>References</h3>

<p>Kuhn, H. The Hungarian Method for the assignment problem Naval Research Logistics Quarterly, 1955, 2, 83 - 97
</p>
<p>Risk, B.B., D.S. Matteson, D. Ruppert, A. Eloyan, B.S. Caffo. In review, 2013. Evaluating ICA methods with an application to resting state fMRI. 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+frobICA">frobICA</a></code>
<code><a href="clue.html#topic+solve_LSAP">clue::solve_LSAP</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(999)
nObs &lt;- 1024
nComp &lt;- 3
# simulate from some gamma distributions:
simS&lt;-cbind(rgamma(nObs, shape = 1, scale = 2),
            rgamma(nObs, shape = 3, scale = 2),
            rgamma(nObs, shape = 9, scale = 0.5))

simM &lt;- matrix(rnorm(9),3)
pMat &lt;- matrix(c(0,-1,0,1,0,0,0,0,-1),3)
permS &lt;- simS%*%pMat
permM &lt;- t(pMat)%*%simM

matchedS &lt;- matchICA(S = permS, template = simS, M = permM)
sum(abs(matchedS$S - simS))
sum(abs(simM - matchedS$M))
</code></pre>

<hr>
<h2 id='multidcov'>
Symmetric multivariate distance covariance</h2><span id='topic+multidcov'></span>

<h3>Description</h3>

<p>Calculate either the symmetric or asymmetric multivariate
distance covariance statistic.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>multidcov(S,symmetric=TRUE,alpha=1)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="multidcov_+3A_s">S</code></td>
<td>
<p>the n x d matrix for which you wish to calculate the 
dependence between d columns from n samples</p>
</td></tr>
<tr><td><code id="multidcov_+3A_alpha">alpha</code></td>
<td>
<p>A scaling parameter in the interval (0,2] used for calculating distances.</p>
</td></tr>
<tr><td><code id="multidcov_+3A_symmetric">symmetric</code></td>
<td>
<p>logical; if TRUE (the default), calculates the symmetric version of the multivariate distance covariance. See details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If symmetric==TRUE, calculates: sum_i=1^d dcovustat(S[,i],S[,-i])
If symmetric==FALSE, calculates: sum_i=1^d-1 dcovustat(S[,i],S[,(i+1):d])
</p>


<h3>Value</h3>

<p>returns a scalar equal to the multivariate distance covariance statistic
for the columns of S
</p>


<h3>Author(s)</h3>

<p>David Matteson
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dcovustat">dcovustat</a></code>,
<code><a href="energy.html#topic+dcov">energy::dcov</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>nObs &lt;- 1024
nComp &lt;- 3

simM &lt;- matrix(rnorm(nComp*nComp),nComp)

# simulate some data:
simS&lt;-cbind(rgamma(nObs, shape = 1, scale = 2),
            rgamma(nObs, shape = 3, scale = 2),
            rgamma(nObs, shape = 9, scale = 0.5))


simS &lt;- scale(simS) #Standardize variance for identifiability

#mix the sources:
xData &lt;- simS %*% simM

multidcov(simS) #close to zero
multidcov(whitener(xData)$Z) #should be larger than simS
multidcov(xData) #greater than zero
</code></pre>

<hr>
<h2 id='permTest'>
Permutation test for mutual independence.
</h2><span id='topic+permTest'></span>

<h3>Description</h3>

<p>Calculates an approximate p-values based upon a permutation test for mutual 
independence.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>permTest(S, group=1:ncol(S), R=199, FUN=c('gmultidcov','compInd'), ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="permTest_+3A_s">S</code></td>
<td>
<p>The n x d matrix for which you wish to test the 
dependence between d columns from n samples</p>
</td></tr>
<tr><td><code id="permTest_+3A_group">group</code></td>
<td>
<p>A length d vector which indicates group membership for each component</p>
</td></tr>
<tr><td><code id="permTest_+3A_r">R</code></td>
<td>
<p>The number of permutations to perform in order to obtain the approximate p-value.</p>
</td></tr>
<tr><td><code id="permTest_+3A_fun">FUN</code></td>
<td>
<p>The function used to determine mutual independence. This is one of either gmultidcov 
or compInd.</p>
</td></tr>
<tr><td><code id="permTest_+3A_...">...</code></td>
<td>
<p>Additionl arguments passed to FUN. See details.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Suppose that the groups are numbered 1,2,...,C and that group is a vector indicating 
group membership for each component.
If symmetric==TRUE, calculates: sum_i=1^C dcovustat(S[,group==i],S[,group!=i])
If symmetric==FALSE, calculates: sum_i=1^C-1 dcovustat(S[,group==i],S[,group&gt;i])
</p>
<p>If no additional arguments are supplied for FUN then the default values are used. In the case of gmultidcov, values for alpha and symmetric can be supplied. While for compInd only the value of alpha is needed.
</p>


<h3>Value</h3>

<p>Returns an approximate p-values based upon a permutation test.
</p>


<h3>Author(s)</h3>

<p>Nicholas James
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dcovustat">dcovustat</a></code>,
<code><a href="energy.html#topic+dcov">energy::dcov</a></code>
</p>

<hr>
<h2 id='rightskew'>
force ICs to have positive skewness and order by skewness
</h2><span id='topic+rightskew'></span>

<h3>Description</h3>

<p>The ICA model is only identifiable up to signed permutations. This function provides a canonical ordering for ICA that is useful for fMRI or studies where signals are skewed. Multiplies columns of S that are left-skewed by -1 to force right skewness. Optionally orders the columns by descending skewness.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rightskew(S, M = NULL, order.skew = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="rightskew_+3A_s">S</code></td>
<td>

<p>n x d matrix 
</p>
</td></tr>
<tr><td><code id="rightskew_+3A_m">M</code></td>
<td>

<p>d x p mixing matrix
</p>
</td></tr>
<tr><td><code id="rightskew_+3A_order.skew">order.skew</code></td>
<td>

<p>Option to return the permutation of columns of S from largest to smallest skewness. Also returns a permuted version of M that corresponds with the permuted S.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns the matrix S such that all columns have positive skewness. If optional argument M is supplied, returns a list with the new S and corresponding M.
</p>


<h3>Author(s)</h3>

<p>Benjamin Risk
</p>


<h3>References</h3>

<p>Eloyan, A. &amp; Ghosh, S. A Semiparametric Approach to Source Separation using Independent Component Analysis Computational Statistics and Data Analysis, 2013, 58, 383 - 396.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>nObs = 1024
simS&lt;-cbind(rgamma(nObs, shape = 1, scale = 2),
            rgamma(nObs, shape = 9, scale = 0.5),
            -1*rgamma(nObs, shape = 3, scale = 2))

apply(simS,2,function(x){ 
  (sum((x - mean(x))^3)/length(x))/(sum((x - mean(x))^2)/length(x))^(3/2)})

canonicalS &lt;- rightskew(simS)

apply(canonicalS,2,function(x){
 (sum((x - mean(x))^3)/length(x))/(sum((x - mean(x))^2)/length(x))^(3/2)})

</code></pre>

<hr>
<h2 id='steadyICA'>
Estimate independent components by minimizing distance covariance </h2><span id='topic+steadyICA'></span>

<h3>Description</h3>

<p>The model is: X = S M + E, where X is n x p and has mean zero, S is n x d, M is d x p, and E is measurement error. For whitened data, we have Z = S t(W), where W is orthogonal. We find the matrix M such that S minimizes the distance covariance dependency measure.</p>


<h3>Usage</h3>

<pre><code class='language-R'>steadyICA(X, n.comp = ncol(X), w.init = NULL, PIT = FALSE, bw = 'SJ', adjust = 1,
whiten = FALSE, irlba = FALSE, symmetric = FALSE, eps = 1e-08, alpha.eps = 1e-08,
maxit = 100, method = c('Cpp','R'), verbose = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="steadyICA_+3A_x">X</code></td>
<td>
<p>The n x p data matrix, where n is the number of observations.</p>
</td></tr>
<tr><td><code id="steadyICA_+3A_n.comp">n.comp</code></td>
<td>
<p>number of components to be estimated</p>
</td></tr>
<tr><td><code id="steadyICA_+3A_w.init">w.init</code></td>
<td>
<p>a p x d initial unmixing matrix</p>
</td></tr>
<tr><td><code id="steadyICA_+3A_pit">PIT</code></td>
<td>
<p>logical; if TRUE, the distribution and density of the independent components are estimated using gaussian kernel density estimates.</p>
</td></tr>
<tr><td><code id="steadyICA_+3A_bw">bw</code></td>
<td>
<p>Argument for bandwidth selection method; defaults to 'SJ'; see stats::density</p>
</td></tr>
<tr><td><code id="steadyICA_+3A_adjust">adjust</code></td>
<td>
<p>adjust bandwidth selection; e.g., if observations are correlated, consider using adjust &gt; 1; see stats::density</p>
</td></tr>
<tr><td><code id="steadyICA_+3A_whiten">whiten</code></td>
<td>
<p>logical; if TRUE, whitens the data before applying ICA, i.e., X%*%whitener = Z, where Z has mean zero and empirical covariance equal to the identity matrix, and Z is then used as the input.</p>
</td></tr>
<tr><td><code id="steadyICA_+3A_irlba">irlba</code></td>
<td>
<p>logical; when whiten=TRUE, irlbA=TRUE uses the R-package 'irlba' in the whitening, which is generally faster than base::svd though sometimes less accurate</p>
</td></tr>
<tr><td><code id="steadyICA_+3A_symmetric">symmetric</code></td>
<td>
<p>logical; if TRUE, implements the symmetric version of the ICA algorithm, which is invariant to the ordering of the columns of X but is slower</p>
</td></tr>
<tr><td><code id="steadyICA_+3A_eps">eps</code></td>
<td>
<p>algorithm terminates when the norm of the gradient of multidcov is less than eps</p>
</td></tr>
<tr><td><code id="steadyICA_+3A_maxit">maxit</code></td>
<td>
<p>maximum number of iterations</p>
</td></tr>
<tr><td><code id="steadyICA_+3A_alpha.eps">alpha.eps</code></td>
<td>
<p>tolerance controlling the level of annealing: algorithm terminates with a warning if the learning parameter is less than alpha.eps</p>
</td></tr>
<tr><td><code id="steadyICA_+3A_method">method</code></td>
<td>
<p>options 'Cpp' (default), which requires the package 'Rcpp', or 'R', which is solely written in R but is much slower</p>
</td></tr>
<tr><td><code id="steadyICA_+3A_verbose">verbose</code></td>
<td>
<p>logical; if TRUE, prints the value of multidcov, norm of the gradient, and current value of the learning parameter.</p>
</td></tr>
</table>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>S</code></td>
<td>
<p>the estimated independent components</p>
</td></tr>
<tr><td><code>W</code></td>
<td>
<p>the estimated unmixing matrix: if whiten=TRUE, W is orthogonal and corresponds to Z W = S; if whiten=FALSE, corresponds to X ginv(M) = S</p>
</td></tr>
<tr><td><code>M</code></td>
<td>
<p>Returns the estimated mixing matrix for the model X = S M, where X is not pre-whitened (although X is centered)</p>
</td></tr>
<tr><td><code>f</code></td>
<td>
<p>the value of the objective function at the estimated S</p>
</td></tr>
<tr><td><code>Table</code></td>
<td>
<p>summarizes algorithm status at each iteration</p>
</td></tr>
<tr><td><code>convergence</code></td>
<td>
<p>1 if norm of the gradient is less than eps, 2 if the learning parameter was smaller than alpha.eps, which usually means the gradient is sufficiently small, 0 otherwise</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Benjamin Risk
</p>


<h3>References</h3>

<p>Matteson, D. S. &amp; Tsay, R. Independent component analysis via U-Statistics. 
&lt;http://www.stat.cornell.edu/~matteson/#ICA&gt;
</p>


<h3>See Also</h3>

<p><code><a href="#topic+multidcov">multidcov</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(999)
nObs &lt;- 1024
nComp &lt;- 3
# simulate from some gamma distributions:
simS&lt;-cbind(rgamma(nObs, shape = 1, scale = 2),
            rgamma(nObs, shape = 3, scale = 2),
            rgamma(nObs, shape = 9, scale = 0.5))

#standardize by expected value and variance:
simS[,1] = (simS[,1] - 1*2)/sqrt(1*2^2)
simS[,2] = (simS[,2] - 3*2)/sqrt(3*2^2)
simS[,3] = (simS[,3] - 9*0.5)/sqrt(9*0.5^2)

# slightly revised 'mixmat' function (from ProDenICA)
# for p&gt;=d: uses fastICA and ProDenICA parameterization:
myMixmat &lt;- function (p = 2, d = NULL) {
  if(is.null(d)) d = p
  a &lt;- matrix(rnorm(d * p), d, p)
  sa &lt;- La.svd(a)
  dL &lt;- sort(runif(d) + 1)
  mat &lt;- sa$u%*%(sa$vt*dL)
  attr(mat, "condition") &lt;- dL[d]/dL[1]
  mat
}

simM &lt;- myMixmat(p = 6, d = nComp)
xData &lt;- simS%*%simM
xWhitened &lt;- whitener(xData, n.comp = nComp)

#estimate mixing matrix:
est.steadyICA.v1 = steadyICA(X = xData,whiten=TRUE,n.comp=nComp,verbose = TRUE)

#Define the 'true' W:
W.true &lt;- solve(simM%*%xWhitened$whitener)

frobICA(M1=est.steadyICA.v1$M,M2=simM)
frobICA(S1=est.steadyICA.v1$S,S2=simS)

## Not run: 
#now initiate from target:
est.steadyICA.v2 = steadyICA(X = xData, w.init= W.true, n.comp = nComp, whiten=TRUE, verbose=TRUE)

#estimate using PIT steadyICA such that dimension reduction is via ICA:
est.steadyICA.v3 = steadyICA(X = xData, w.init=ginv(est.steadyICA.v2$M),
PIT=TRUE, n.comp = nComp, whiten=FALSE, verbose=TRUE)

frobICA(M1=est.steadyICA.v2$M,M2=simM)
frobICA(M1=est.steadyICA.v3$M,M2=simM) 
frobICA(S1=est.steadyICA.v2$S,S2=simS)

#tends to be lower than PCA-based (i.e., whitening) methods:
frobICA(S1=est.steadyICA.v3$S,S2=simS) 

# JADE uses a different parameterization and different notation.
# Using our parameterization and notation, the arguments for 
# JADE::amari.error correspond to:
amari.error(t(W.hat), W.true)

library(JADE)

amari.error(t(est.steadyICA.v1$W), W.true) 
amari.error(t(est.steadyICA.v2$W), W.true)
##note that a square W is not estimated if PIT=TRUE and whiten=FALSE

#Compare performance to fastICA:
library(fastICA)
est.fastICA = fastICA(X = xData, n.comp = 3, tol=1e-07)
amari.error(t(est.fastICA$W), W.true)
##steadyICA usually outperforms fastICA

##Compare performance to ProDenICA:
library(ProDenICA)
est.ProDenICA = ProDenICA(x = xWhitened$Z, k = 3, maxit=40,trace=TRUE)
amari.error(t(est.ProDenICA$W), W.true)
##ProDenICA and steadyICA tend to be similar when sources
##are continuously differentiable

## End(Not run)
</code></pre>

<hr>
<h2 id='theta2W'>
Convert angles to an orthogonal matrix.
</h2><span id='topic+theta2W'></span>

<h3>Description</h3>

<p>Convert d*(d-1)/2 angles from a sequence of Givens rotations to a d x d orthogonal matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>theta2W(theta)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="theta2W_+3A_theta">theta</code></td>
<td>
<p>A scalar or vector of length d*(d-1)/2 of values from which the d x d orthogonal matrix is calculated.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A d x d orthogonal matrix resulting from the sequence of d*(d-1)/2 Givens rotation matrices.
</p>


<h3>Author(s)</h3>

<p>David S. Matteson</p>


<h3>References</h3>

<p>Golub, G. &amp; Van Loan, C. 1996. Matrix computations. Johns Hopkins University Press.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+W2theta">W2theta</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#Generate orthogonal matrix:
mat &lt;- matrix(rnorm(9),3,3)
W = svd(mat)$u

theta &lt;- W2theta(W)

#Recovers W:
theta2W(theta)
</code></pre>

<hr>
<h2 id='W2theta'>
Convert an orthogonal matrix to its angular parameterization. </h2><span id='topic+W2theta'></span>

<h3>Description</h3>

<p>Convert a d x d orthogonal matrix to a sequence of d*(d-1)/2 Givens rotations.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>W2theta(W)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="W2theta_+3A_w">W</code></td>
<td>

<p>A d x d orthogonal matrix.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A d x d orthogonal matrix can be decomposed into a series of d*(d-1)/2 Givens rotation matrices, where each matrix is parameterized by a single angle.
</p>


<h3>Value</h3>

<p>A vector of length d*(d-1)/2 comprised of the angles.
</p>


<h3>Author(s)</h3>

<p>David S. Matteson
</p>


<h3>References</h3>

<p>Golub, G. &amp; Van Loan, C. 1996. Matrix computations. Johns Hopkins University Press.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+theta2W">theta2W</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>theta = c(pi/6,pi/4,pi/2)

(W = theta2W(theta))

#Recover theta:
W2theta(W)
</code></pre>

<hr>
<h2 id='whitener'>Whitening function</h2><span id='topic+whitener'></span><span id='topic+whiten'></span>

<h3>Description</h3>

<p>Subtract column means and transform columns such that the empirical covariance is equal to the identity matrix. Uses the SVD.</p>


<h3>Usage</h3>

<pre><code class='language-R'>whitener(X, n.comp = ncol(X), center.row = FALSE, irlba = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="whitener_+3A_x">X</code></td>
<td>

<p>n x p matrix
</p>
</td></tr>
<tr><td><code id="whitener_+3A_n.comp">n.comp</code></td>
<td>

<p>number of components to retain, i.e., first n.comp left eigenvectors from svd are retained
</p>
</td></tr>
<tr><td><code id="whitener_+3A_center.row">center.row</code></td>
<td>

<p>center both rows and columns prior to applying SVD (the resulting whitened data does not have zero-mean rows)
</p>
</td></tr>
<tr><td><code id="whitener_+3A_irlba">irlba</code></td>
<td>
<p>if TRUE, uses irlba to approximate the first n.comp left eigenvectors. See Note.
</p>
</td></tr>
</table>


<h3>Value</h3>

<table role = "presentation">
<tr><td><code>whitener</code></td>
<td>
<p>the matrix such that X%*%whitener has zero mean and covariance equal to the identity matrix</p>
</td></tr>
<tr><td><code>Z</code></td>
<td>
<p>the whitened data, i.e., X%*%whitener = Z</p>
</td></tr>
</table>


<h3>Note</h3>

<p>The use of the option 'irlba = TRUE' requires the package irlba and is very useful
for large p. The function irlba only calculates
the first n.comp eigenvectors and is much faster than svd for p &gt;&gt; n.comp, for e.g., in groupICA of fMRI data.</p>


<h3>Author(s)</h3>

<p>Benjamin Risk</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+svd">svd</a></code>, 
<code><a href="irlba.html#topic+irlba">irlba::irlba</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>simData &lt;- cbind(rnorm(1000,1,2),rnorm(1000,-1,3),rnorm(1000,4,1))
simMVN &lt;- simData%*%matrix(rnorm(12),3,4)
simWhiten &lt;- whitener(simMVN,n.comp = 3)
colMeans(simWhiten$Z)
cov(simWhiten$Z)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
