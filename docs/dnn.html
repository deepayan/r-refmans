<!DOCTYPE html><html><head><title>Help for package dnn</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {dnn}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#dnn-package'>
<p>An R package for the deep neural networks probability and statistics models</p></a></li>
<li><a href='#activation'>
<p>Activation function</p></a></li>
<li><a href='#bwdNN'>
<p>Back propagation for dnn Models</p></a></li>
<li><a href='#deepAFT'>
<p>Deep learning for the accelerated failure time (AFT) model</p></a></li>
<li><a href='#deepGLM'>
<p>Deep learning for the generalized linear model</p></a></li>
<li><a href='#deepSurv'>
<p>Deep learning for the Cox proportional hazards model</p></a></li>
<li><a href='#dnnControl'>
<p>Auxiliary function for <code>dnnFit</code> dnnFit</p></a></li>
<li><a href='#dnnFit'>
<p>Fitting a Deep Learning model with a given loss function</p></a></li>
<li><a href='#dNNmodel'>
<p>Specify a deep neural network model</p></a></li>
<li><a href='#fwdNN'>
<p>Feed forward and back propagation for dnn Models</p></a></li>
<li><a href='#hyperTuning'>
<p>A function for tuning of the hyper parameters</p></a></li>
<li><a href='#ibs'>
<p>Calculate integrated Brier Score for deepAFT</p></a></li>
<li><a href='#msePICW'>
<p>Mean Square Error (mse) for a survival Object</p></a></li>
<li><a href='#optimizerSGD'>
<p>Functions to optimize the gradient descent of a cost function</p></a></li>
<li><a href='#plot'>
<p>Plot methods in dnn package</p></a></li>
<li><a href='#predict'>
<p>Predicted Values for a deepAFT Object</p></a></li>
<li><a href='#print'>
<p>print a summary of fitted deep learning model object</p></a></li>
<li><a href='#residuals'>
<p>Calculate Residuals for a deepAFT Fit.</p></a></li>
<li><a href='#rsurv'>
<p>The Survival Distribution</p></a></li>
<li><a href='#survfit'>
<p>Compute a Survival Curve from a deepAFT or a deepSurv Model</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Deep Neural Network Tools for Probability and Statistic Models</td>
</tr>
<tr>
<td>Version:</td>
<td>0.0.6</td>
</tr>
<tr>
<td>Date:</td>
<td>2024-03-12</td>
</tr>
<tr>
<td>Author:</td>
<td>Bingshu E. Chen [aut, cre],
  Patrick Norman [aut, ctb],
  Wenyu Jiang [ctb],
  Wanlu Li [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Bingshu E. Chen &lt;bingshu.chen@queensu.ca&gt;</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0), ggplot2, survival, Rcpp</td>
</tr>
<tr>
<td>Imports:</td>
<td>methods</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo</td>
</tr>
<tr>
<td>Description:</td>
<td>Contains tools to build deep neural network with flexible users define loss function and probability models. Several applications included in this package are, 1) The (deepAFT) model, a deep neural network model for accelerated failure time (AFT) model for survival data. 2) The (deepGLM) model, a deep neural network model for generalized linear model (glm) for continuous, categorical and Poisson data.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>LazyLoad:</td>
<td>yes</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-03-14 15:42:28 UTC; b3chen</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-03-14 20:50:05 UTC</td>
</tr>
</table>
<hr>
<h2 id='dnn-package'>
An R package for the deep neural networks probability and statistics models 
</h2><span id='topic+dnn-package'></span><span id='topic+dnn-doc'></span><span id='topic+dnn'></span>

<h3>Description</h3>

<p>This package provides tools for deep neural network which allow user define loss function for complex outcome data with probability and statistics models such as generalized linear models, accelerated failure time (AFT) models, and Cox proportional hazards models.
</p>
<p>It contains the essential building blocks such as feed forward network and back propagation. This gives users the flexibility to write their own loss function (i.e. cost function) and train the neural network.
</p>


<h3>Details</h3>

<p>{dnn} is a R package for deep learning neural network with probability models that use the negative of the log-likelihood as the loss function. It provides functions for feed forward network from covariates to the output layer and back propagation to find the derivatives of the weight parameters. 
Different optimization methods such as stochastic gradient descent (SGD), Momentum and ADAM can be used to train the network.
</p>
<p>Currently, { dnn } can be install by 
</p>
<p>the package source file 'dnn.tar.gz', use
</p>
<p>install.packages(&quot;dnn.tar.gz&quot;, repos = NULL, type = &quot;source&quot;)
</p>
<p>users can use the following steps to install the most recent version of 'dnn' package:
</p>
<p>1. First, you need to install the 'devtools' package. 
You can skip this step if you have 'devtools' installed in your R. Invoke R and then type
</p>
<p>install.packages(&quot;devtools&quot;)
</p>
<p>2. Load the devtools package.
</p>
<p>library(devtools)
</p>
<p>3. Install &quot;dnn&quot; package from github with R command
</p>
<p>install_github(&quot;statapps/dnn&quot;)
</p>
<p>A stable version of View the &quot;dnn&quot; package is also available from the Comprehensive R Archive Network 
(https://CRAN.R-project.org/package=dnn) and can be installed using R command 
</p>
<p>install.packages(&quot;dnn&quot;)
</p>


<h3>Author(s)</h3>

<p>Bingshu E. Chen
</p>
<p>Maintainer: Bingshu E. Chen &lt;bingshu.chen@queensu.ca&gt;
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dNNmodel">dNNmodel</a></code>,  
<code><a href="#topic+bwdNN">bwdNN</a></code>,
<code><a href="#topic+fwdNN">fwdNN</a></code>,
<code><a href="#topic+deepAFT">deepAFT</a></code>, 
<code><a href="#topic+deepGLM">deepGLM</a></code>, 
<code><a href="#topic+deepSurv">deepSurv</a></code>, 
<code><a href="survival.html#topic+coxph">coxph</a></code>, 
<code><a href="stats.html#topic+glm">glm</a></code>
<code><a href="boot.html#topic+survival">survival</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Create the models with 3 layers
  model = dNNmodel(units=c(8, 6, 1), activation = c('elu', 'relu', 'sigmoid'), 
          input_shape = c(3))
  print(model)
# 
# Feed forward network with dummy data x
  x = matrix(runif(15), nrow = 5, ncol = 3)
  cache = fwdNN(x, model)
#
# Back propagation with dummy dy = dL/dyhat and minin batch for SGD
  dy = as.matrix(runif(5, -0.1, 0.1), nrow = 5)
  dW = bwdNN(dy, cache, model)
#
# Gradient descent with SGD 
  lr_rate = 0.0001
  sgd = function(w, dw) {w-lr_rate*dw}
  model$params = mapply(sgd, w = model$params, dw = dW)
</code></pre>

<hr>
<h2 id='activation'>
Activation function
</h2><span id='topic+activation'></span><span id='topic+sigmoid'></span><span id='topic+elu'></span><span id='topic+relu'></span><span id='topic+lrelu'></span><span id='topic+idu'></span><span id='topic+dsigmoid'></span><span id='topic+delu'></span><span id='topic+drelu'></span><span id='topic+dlrelu'></span><span id='topic+dtanh'></span><span id='topic+didu'></span>

<h3>Description</h3>

<p>Different type of activation functions and the corresponding derivatives 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  sigmoid(x)
  elu(x)
  relu(x)
  lrelu(x)
  idu(x)
  dsigmoid(y)
  delu(y)
  drelu(y)
  dlrelu(y)
  dtanh(y)   #activation function tanh(x) is already available in R
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="activation_+3A_x">x</code></td>
<td>
<p>input of the activation function</p>
</td></tr>
<tr><td><code id="activation_+3A_y">y</code></td>
<td>
<p>input of the derivative of the activation function</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Each function returns either the activation function (e.g. sigmoid, relu) or its derivative (e.g. dsigmoid, drelu).
</p>


<h3>Value</h3>

<p>An activation function is applied to x and returns a matrix the same size as x. 
The detail formula for each activation function is:
</p>
<table>
<tr><td><code>sigmoid</code></td>
<td>
<p>return 1/(1+exp(-x))</p>
</td></tr>
<tr><td><code>elu</code></td>
<td>
<p>return x for x&gt;0 and exp(x)-1 for x&lt;0</p>
</td></tr>
<tr><td><code>relu</code></td>
<td>
<p>return x for x&gt;0 and 0 for x&lt;0</p>
</td></tr>
<tr><td><code>lrelu</code></td>
<td>
<p>return x for x&gt;0 and 0.1*x for x&lt;0</p>
</td></tr>
<tr><td><code>tanh</code></td>
<td>
<p>return tanh(x)</p>
</td></tr>
<tr><td><code>idu</code></td>
<td>
<p>return (x)</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Bingshu E. Chen</p>


<h3>See Also</h3>

<p><code><a href="#topic+bwdNN">bwdNN</a></code>, 
<code><a href="#topic+fwdNN">fwdNN</a></code>,
<code><a href="#topic+dNNmodel">dNNmodel</a></code>, 
<code><a href="#topic+optimizerSGD">optimizerSGD</a></code>,
<code><a href="#topic+optimizerNAG">optimizerNAG</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>  # Specify a dnn nodel with user define activation function in layer 2.
  softmax  = function(x) {log(1+exp(x))}    # y = log(1+exp(x))
  dsoftmax = function(y) {sigmoid(y)}       # x = exp(y)/(1+exp(y))
  model = dNNmodel(units=c(8, 6, 1), activation= c('relu', 'softmax', 'sigmoid'), 
          input_shape = c(3))
  print(model)
</code></pre>

<hr>
<h2 id='bwdNN'>
Back propagation for dnn Models
</h2><span id='topic+bwdNN'></span><span id='topic+bwdNN2'></span><span id='topic+bwdCheck'></span>

<h3>Description</h3>

<p>{bwdNN} is an R function for back propagation in DNN network.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>#
# To apply back propagation in with a feed forward model 
#
# use 
#
   bwdNN(dy, cache, model)
#
# to calculate derivative of dL/dW
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bwdNN_+3A_dy">dy</code></td>
<td>
<p>the derivative of the cost function with respect to the output layer of the fwdNN function.</p>
</td></tr>
<tr><td><code id="bwdNN_+3A_cache">cache</code></td>
<td>
<p>the cached output of fwdNN.</p>
</td></tr>
<tr><td><code id="bwdNN_+3A_model">model</code></td>
<td>
<p>a model return from dNNmodel function.</p>
</td></tr>


</table>


<h3>Details</h3>

<p>Here 'dy' plays an import role in the back propagation { bwdNN } 
since the probability model's loss function takes the output 
layer of the { dnn } (denote as yhat) as one of its parameter. 
Then 'dy' equals to the partial derivative of the loss function (-Log Likelihood) with respect to yhat, 
that is, dy = dL/d(yhat). 
For example, if the 'dnn' predicts the probability (yhat = p) for the mixture of two populations f1 and f2, 
then the likelihood function is f = p*f1 + (1-p)*f2, and 
the loss function is L = -log(p*f1+(1-p)*f2). Hence, dy = dL/dp = -(f1-f2)/f.
</p>
<p>'cache' is the cache of each input layer generated from the { fwdNN } function.
</p>
<p>The function { bwdCheck } calculates the numerical derivatives of dL/dW, which can be used to check if the back propagation is correct or not, see example below.
</p>


<h3>Value</h3>

<p>A list contains the derivatives of weight parameter W is returned.
</p>


<h3>Author(s)</h3>

<p>Bingshu E. Chen (bingshu.chen@queensu.ca)
</p>


<h3>See Also</h3>

<p><code><a href="#topic+dNNmodel">dNNmodel</a></code>,
<code><a href="#topic+fwdNN">fwdNN</a></code>,
<code><a href="#topic+plot.dNNmodel">plot.dNNmodel</a></code>,
<code><a href="#topic+print.dNNmodel">print.dNNmodel</a></code>,
<code><a href="#topic+summary.dNNmodel">summary.dNNmodel</a></code>,
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### define a dnn model, calculate the feed forward network
   model = dNNmodel(units = c(8, 6, 1), 
           activation = c("elu", "sigmoid", "sigmoid"), input_shape = 3)
   print(model)
   x = matrix(runif(15), nrow = 5, ncol = 3)
   cache = fwdNN(x, model)
   # dy = dL/dp, where L is the cost function such as the 
   # log-likehood and p is the output layer parameter of the DNN
   dy = as.matrix(runif(5, -0.1, 0.1), nrow = 5)  # a dummy dy for bwdNN input
   y  = predict(model, x) + dy
   
   # back propagation 
   dW = bwdNN(dy, cache, model)
   dw = bwdCheck(x, y, model)
   print(dW[[1]])
   print(dw[[1]])
</code></pre>

<hr>
<h2 id='deepAFT'>
Deep learning for the accelerated failure time (AFT) model 
</h2><span id='topic+deepAFT'></span><span id='topic+deepAFT.default'></span><span id='topic+deepAFT.formula'></span><span id='topic+deepAFT.ipcw'></span><span id='topic+deepAFT.trans'></span>

<h3>Description</h3>

<p>Fit a deep learning survival regression model. 
These are location-scale models for an arbitrary transform of the time variable; 
the most common cases use a log transformation, leading to accelerated failure time models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>deepAFT(x, ...)

## S3 method for class 'formula'
deepAFT(formula, model, data, control = list(...), method =
                 c("BuckleyJames", "ipcw", "transform"), ...)

## Default S3 method:
deepAFT(x, y, model, control, ...)

## S3 method for class 'ipcw'
deepAFT(x, y, model, control, ...)
# use:
#   deepAFT.ipcw(x, y, model, control)
# or
#   class(x) = "ipcw"
#   deepAFT(x, y, model, control)
# 
## S3 method for class 'trans'
deepAFT(x, y, model, control, ...)
# use:
#   class(x) = "transform"
#   deepAFT(x, y, model, control)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="deepAFT_+3A_formula">formula</code></td>
<td>
<p>a formula expression as for other regression models. 
The response is usually a survival object as returned by the
'Surv' function.  See the documentation for 'Surv', 'lm' and
'formula' for details.</p>
</td></tr>
<tr><td><code id="deepAFT_+3A_model">model</code></td>
<td>
<p>deep neural network model, see below for details.</p>
</td></tr>
<tr><td><code id="deepAFT_+3A_data">data</code></td>
<td>
<p>a data.frame in which to interpret the variables named in the formula.</p>
</td></tr>
<tr><td><code id="deepAFT_+3A_x">x</code></td>
<td>
<p>Covariates for the AFT model</p>
</td></tr>
<tr><td><code id="deepAFT_+3A_y">y</code></td>
<td>
<p>Surv object for the AFT model</p>
</td></tr>
<tr><td><code id="deepAFT_+3A_method">method</code></td>
<td>
<p>methods to handle censoring data in deep AFT model fit, 'BuckleyJames' for Buckley and James method, 
'ipcw' for inverse probability censoring weights method.
'transform' for transformation based on book of Fan and Gijbels (1996, page 168)</p>
</td></tr>
<tr><td><code id="deepAFT_+3A_control">control</code></td>
<td>
<p>a list of control values, in the format produced by
'dnnControl'. The default value 'dnnControl()'</p>
</td></tr>
<tr><td><code id="deepAFT_+3A_...">...</code></td>
<td>
<p>optional arguments</p>
</td></tr>
</table>


<h3>Details</h3>

<p>See &quot;Deep learning with R&quot; for details on how to build a deep learning model. 
</p>
<p>The following parameters in 'dnnControl' will be used to control the model fit process.
</p>
<p>'epochs': number of deep learning epochs, default is 100.
</p>
<p>'batch_size': batch size, default is 128. 'NaN' may be generated if batch size is too small and there is not event in a batch. 
</p>
<p>'verbose': verbose = 1 for print out verbose during the model fit, 0 for not print.
</p>
<p>'epsilon': epsilon for convergence check, default is epsilon = 0.001.
</p>
<p>'max.iter': number of maximum iteration, default is max.iter = 100.
</p>
<p>'censor.groups': a vector for censoring groups. A KM curve for censoring will be fit for each group. If a matrix is provided, then a Cox model will be used to predict the censoring probability.
</p>
<p>When the variance for covariance matrix X is too large, please use xbar = apply(x, 2, stndx) to standardize X.
</p>


<h3>Value</h3>

<p>An object of class &quot;deepAFT&quot; is returned. The deepAFT object contains the following list components:
</p>
<table>
<tr><td><code>x</code></td>
<td>
<p>Covariates for the AFT model</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>Survival object for the AFT model, y = Surv(time, event)</p>
</td></tr>
<tr><td><code>model</code></td>
<td>
<p>A fitted artificial neural network (ANN) model</p>
</td></tr>
<tr><td><code>mean.ipt</code></td>
<td>
<p>mean survival or censoring time</p>
</td></tr>
<tr><td><code>predictor</code></td>
<td>
<p>predictor score mu = f(x)</p>
</td></tr> 
<tr><td><code>risk</code></td>
<td>
<p>risk score = exp(predictor)</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>method for deepAFT fitting, either Buckley-James, IPCW or transformed model</p>
</td></tr>
</table>


<h3>Note</h3>

<p>For right censored survival time only</p>


<h3>Author(s)</h3>

<p>Chen, B. E. and Norman P.
</p>


<h3>References</h3>

<p>Buckley, J. and James, I. (1979). Linear regression with cencored data. Biometrika, 66, page 429-436.
</p>
<p>Norman, P. Li, W., Jiang, W. and Chen, B. E. (2024). DeepAFT: A nonparametric accelerated failure time model with artificial neural network. Manuscript submitted to Statistics in Medicine.
</p>
<p>Chollet, F. and Allaire J. J. (2017). Deep learning with R. Manning.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+print.deepAFT">print.deepAFT</a></code>, <code><a href="survival.html#topic+survreg">survreg</a></code>, <code><a href="#topic+ibs.deepAFT">ibs.deepAFT</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Example for deep learning model for AFT survival data
  set.seed(101)
### define model layers
  model = dNNmodel(units = c(4, 3, 1), activation = c("elu", "sigmoid", "sigmoid"), 
                   input_shape = 3)
  x = matrix(runif(15), nrow = 5, ncol = 3)
  time = exp(x[, 1])
  status = c(1, 0, 1, 1, 1)
  fit = deepAFT(Surv(time, status) ~ x, model)
</code></pre>

<hr>
<h2 id='deepGLM'>
Deep learning for the generalized linear model
</h2><span id='topic+deepGLM'></span><span id='topic+deepGlm'></span><span id='topic+predict.deepGlm'></span><span id='topic+summary.deepGlm'></span><span id='topic+residuals.deepGlm'></span>

<h3>Description</h3>

<p>Fit generalized linear models (Gaussian, Binomial and Poisson) using deep learning neural network (DNN). The glm formula is specified by giving a symbolic description of the predictor and a description of the error distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>deepGlm(formula, model, family = c("gaussian", "binomial",
        "poisson"), data, epochs = 200, lr_rate = 1e-04,
         batch_size = 64, alpha = 0.7, lambda = 1, verbose = 0,
         weights = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="deepGLM_+3A_formula">formula</code></td>
<td>
<p>a formula expression as for other regression models. 
The response is usually an object for glm response variable. See the documentation for 'glm', 'lm' and 'formula' for details.</p>
</td></tr>
<tr><td><code id="deepGLM_+3A_model">model</code></td>
<td>
<p>a deep neural network model, created by function dNNmodel().</p>
</td></tr>
<tr><td><code id="deepGLM_+3A_family">family</code></td>
<td>
<p>a description of the error distribution and link function to
be used in the model. This can be either a character
string of 'gaussian', 'binomial', or 'poisson', naming a family function, 
or result of a call to a family function (See 'family' for details of family functions).)</p>
</td></tr>
<tr><td><code id="deepGLM_+3A_data">data</code></td>
<td>
<p>a data.frame in which to interpret the variables named in the formula.</p>
</td></tr>
<tr><td><code id="deepGLM_+3A_epochs">epochs</code></td>
<td>
<p>number of deep learning epochs, default is 200.</p>
</td></tr>
<tr><td><code id="deepGLM_+3A_batch_size">batch_size</code></td>
<td>
<p>batch size, default is 64. 'NaN' may be generated if batch size is too
small and there is not event in a batch.</p>
</td></tr>
<tr><td><code id="deepGLM_+3A_lr_rate">lr_rate</code></td>
<td>
<p>learning rate for the gradient descent algorithm, default is lr_rate = 1e-04.</p>
</td></tr>
<tr><td><code id="deepGLM_+3A_weights">weights</code></td>
<td>
<p>an optional vector of 'prior weights' to be used in the
fitting process. Should be NULL or a numeric vector.</p>
</td></tr>
<tr><td><code id="deepGLM_+3A_alpha">alpha</code></td>
<td>
<p>momentum rate for the gradient descent method, alpha takes value in [0, 1), default is alpha = 0.70.</p>
</td></tr>
<tr><td><code id="deepGLM_+3A_lambda">lambda</code></td>
<td>
<p>L2 regularization parameter for deep learning.</p>
</td></tr> 
<tr><td><code id="deepGLM_+3A_verbose">verbose</code></td>
<td>
<p>verbose = 1 for print out verbose during the model fit, 0 for not print.</p>
</td></tr>
<tr><td><code id="deepGLM_+3A_...">...</code></td>
<td>
<p>optional arguments</p>
</td></tr>
</table>


<h3>Details</h3>

<p>See <code><a href="#topic+dNNmodel">dNNmodel</a></code> for details on how to specify a deep learning model. 
</p>
<p>The following parameters in 'dnnControl' will be used to control the model fit process.
</p>
<p>'epochs': number of deep learning epochs, default is 30.
</p>
<p>'verbose': verbose = 1 for print out verbose during the model fit, 0 for not print.
</p>
<p>When the variance for covariance matrix X is too large, please use xbar = scale(x) to standardize X.
</p>


<h3>Value</h3>

<p>An object of class &quot;deepGlm&quot; is returned. The deepGlm object contains the following list components:
</p>
<table>
<tr><td><code>x</code></td>
<td>
<p>Covariates for glm model</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>Object for glm model</p>
</td></tr>
<tr><td><code>model</code></td>
<td>
<p>dnn model</p>
</td></tr>
<tr><td><code>predictor</code></td>
<td>
<p>predictor score mu = f(x)</p>
</td></tr> 
<tr><td><code>risk</code></td>
<td>
<p>risk score = exp(predictor)</p>
</td></tr>
</table>


<h3>Note</h3>

<p>For glm models with Gaussian, Binomial and Poisson only</p>


<h3>Author(s)</h3>

<p>Chen, B. E. 
</p>


<h3>References</h3>

<p>Chollet, F. and Allaire J. J. (2017). Deep learning with R. Manning.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+deepAFT">deepAFT</a></code>, <code><a href="#topic+dNNmodel">dNNmodel</a></code>, 
<code><a href="#topic+predict.deepGlm">predict.deepGlm</a></code>,
<code><a href="#topic+print.deepSurv">print.deepSurv</a></code>, <code><a href="stats.html#topic+glm">glm</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Example for deep learning for glm models
  set.seed(101)
### define model layers
  model = dNNmodel(units = c(4, 3, 1), activation = c("elu", "sigmoid", "sigmoid"), 
                   input_shape = 3)
  x = matrix(runif(15), nrow = 5, ncol = 3)
  y = exp(x[, 1] + rnorm(5))
  
  fit = deepGlm(y ~ x, model, family = "gaussian") 
</code></pre>

<hr>
<h2 id='deepSurv'>
Deep learning for the Cox proportional hazards model 
</h2><span id='topic+deepSurv'></span><span id='topic+deepSurv.default'></span><span id='topic+summary.deepSurv'></span>

<h3>Description</h3>

<p>Fit a survival regression model under the Cox proportional hazards assumption using 
deep learning neural network (DNN). 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>deepSurv(formula, model, data, epochs = 200, lr_rate = 1e-04,
        batch_size = 64, alpha = 0.7, lambda = 1, verbose = 0,
        weights = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="deepSurv_+3A_formula">formula</code></td>
<td>
<p>a formula expression as for other regression models. 
The response is usually a survival object as returned by the
'Surv' function.  See the documentation for 'Surv', 'lm' and
'formula' for details.</p>
</td></tr>
<tr><td><code id="deepSurv_+3A_model">model</code></td>
<td>
<p>a deep neural network model, created by function dNNmodel().</p>
</td></tr>
<tr><td><code id="deepSurv_+3A_data">data</code></td>
<td>
<p>a data.frame in which to interpret the variables named in the formula.</p>
</td></tr>
<tr><td><code id="deepSurv_+3A_epochs">epochs</code></td>
<td>
<p>number of deep learning epochs, default is 200.</p>
</td></tr>
<tr><td><code id="deepSurv_+3A_batch_size">batch_size</code></td>
<td>
<p>batch size, default is 64. 'NaN' may be generated if batch size is too
small and there is not event in a batch.</p>
</td></tr>
<tr><td><code id="deepSurv_+3A_lr_rate">lr_rate</code></td>
<td>
<p>learning rate for the gradient descent algorithm, default is lr_rate = 1e-04.</p>
</td></tr>
<tr><td><code id="deepSurv_+3A_weights">weights</code></td>
<td>
<p>an optional vector of 'prior weights' to be used in the
fitting process. Should be NULL or a numeric vector.</p>
</td></tr>
<tr><td><code id="deepSurv_+3A_alpha">alpha</code></td>
<td>
<p>momentum rate for the gradient descent method, alpha takes value in [0, 1), default is alpha = 0.70.</p>
</td></tr>
<tr><td><code id="deepSurv_+3A_lambda">lambda</code></td>
<td>
<p>L2 regularization parameter for deep learning.</p>
</td></tr> 
<tr><td><code id="deepSurv_+3A_verbose">verbose</code></td>
<td>
<p>verbose = 1 for print out verbose during the model fit, 0 for not print.</p>
</td></tr>
<tr><td><code id="deepSurv_+3A_...">...</code></td>
<td>
<p>optional arguments</p>
</td></tr>
</table>


<h3>Details</h3>

<p>See &quot;Deep learning with R&quot; for details on how to build a deep learning model. 
</p>
<p>The following parameters in 'dnnControl' will be used to control the model fit process.
</p>
<p>'epochs': number of deep learning epochs, default is 30.
</p>
<p>'verbose': verbose = 1 for print out verbose during the model fit, 0 for not print.
</p>
<p>'epsilon': epsilon for convergence check, default is epsilon = 0.001.
</p>
<p>'max.iter': number of maximum iteration, default is max.iter = 30.
</p>
<p>When the variance for covariance matrix X is too large, please use xbar = scale(x) to standardize X.
</p>


<h3>Value</h3>

<p>An object of class &quot;deepSurv&quot; is returned. The deepSurv object contains the following list components:
</p>
<table>
<tr><td><code>x</code></td>
<td>
<p>Covariates for Cox model</p>
</td></tr>
<tr><td><code>y</code></td>
<td>
<p>Surv object for Cox model</p>
</td></tr>
<tr><td><code>model</code></td>
<td>
<p>dnn model</p>
</td></tr>
<tr><td><code>predictor</code></td>
<td>
<p>predictor score mu = f(x)</p>
</td></tr> 
<tr><td><code>risk</code></td>
<td>
<p>risk score = exp(predictor)</p>
</td></tr>
</table>


<h3>Note</h3>

<p>For right censored survival time only</p>


<h3>Author(s)</h3>

<p>Chen, B. E. wrote the R code using the partial likelihood cost function proposed by Katzman et al (2018). 
</p>


<h3>References</h3>

<p>Katzman JL, Shaham U, Cloninger A, Bates J, Jiang T, Kluger Y. DeepSurv: Personalized treatment recommender system using a Cox proportional hazards deep neural network. BMC Medical Research Methodology 2018; 18: 24.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+deepAFT">deepAFT</a></code>, <code><a href="#topic+deepGlm">deepGlm</a></code>,
<code><a href="#topic+print.deepSurv">print.deepSurv</a></code>, <code><a href="survival.html#topic+survreg">survreg</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Example for deep learning proportional hazards survival model
  set.seed(101)
### define model layers
  model = dNNmodel(units = c(4, 3, 1), activation = c("elu", "sigmoid", "sigmoid"), 
                   input_shape = 3)
  x = matrix(runif(15), nrow = 5, ncol = 3)
  time = exp(x[, 1])
  status = c(1, 0, 1, 1, 1)
  fit = deepSurv(Surv(time, status) ~ x, model = model)
</code></pre>

<hr>
<h2 id='dnnControl'>
Auxiliary function for <code><a href="#topic+dnnFit">dnnFit</a></code> dnnFit
</h2><span id='topic+dnnControl'></span>

<h3>Description</h3>

<p>dnnControl is an auxiliary function for <code><a href="#topic+dnnFit">dnnFit</a></code>. Typically only used internally by the dnn package, may be used to construct a control argument for the deep learning neural network model to specify parameters such as a loss function. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  dnnControl(loss = c("mse", "cox", "bin", "log", "mae"), epochs = 300, 
	     batch_size = 64, verbose = 0, lr_rate = 0.0001,  
	     alpha = 0.5, lambda = 1.0, epsilon = 0.01, max.iter = 100, 
	     censor.group = NULL, weights = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dnnControl_+3A_loss">loss</code></td>
<td>
<p>loss function for the neural network model, &quot;mse&quot; for mean square error (guassian glm model), 
&quot;mae&quot; for mean absolute error, 
&quot;cox&quot; for the Cox partial likelihood (proportional hazards model), 
&quot;bin&quot; for cross-entropy (binomial glm model), 
&quot;log&quot; for log-linear (poisson glm model).</p>
</td></tr>
<tr><td><code id="dnnControl_+3A_epochs">epochs</code></td>
<td>
<p>number of deep learning epochs, default is 30.</p>
</td></tr>
<tr><td><code id="dnnControl_+3A_batch_size">batch_size</code></td>
<td>
<p>batch size, default is 64. 'NaN' may be generated if batch size is too small and there is not event in a batch.</p>
</td></tr> 
<tr><td><code id="dnnControl_+3A_lr_rate">lr_rate</code></td>
<td>
<p>learning rate, default is 0.0001.</p>
</td></tr>
<tr><td><code id="dnnControl_+3A_weights">weights</code></td>
<td>
<p>an optional vector of 'prior weights' to be used in the
fitting process. Should be NULL or a numeric vector, default is NULL.</p>
</td></tr>
<tr><td><code id="dnnControl_+3A_alpha">alpha</code></td>
<td>
<p>alpha decay rate for momentum gradient descent, default is 0.5.</p>
</td></tr>
<tr><td><code id="dnnControl_+3A_lambda">lambda</code></td>
<td>
<p>regularization term for dnn weighting parameters, 0.5*lambda*W*W), default is 1.0.</p>
</td></tr>
<tr><td><code id="dnnControl_+3A_verbose">verbose</code></td>
<td>
<p>verbose = 1 for print out verbose during the model fit, 0 for not print.</p>
</td></tr>
<tr><td><code id="dnnControl_+3A_epsilon">epsilon</code></td>
<td>
<p>epsilon for convergence check, default is epsilon = 0.01.</p>
</td></tr>
<tr><td><code id="dnnControl_+3A_max.iter">max.iter</code></td>
<td>
<p>number of maximum iteration, default is max.iter = 100. This is used in the deepAFT function</p>
</td></tr>
<tr><td><code id="dnnControl_+3A_censor.group">censor.group</code></td>
<td>
<p>a vector for censoring groups. A KM curve for censoring will be fit for each group. If a matrix is provided, then a Cox model will be used to predict the censoring probability. Used only in the deepAFT function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>dnnControl is used in model fitting of &quot;dnnFit&quot;. Additional loss functions will be added to the library in the future. 
</p>


<h3>Value</h3>

<p>This function checks the internal consistency and returns a list of values as input to control model fitting of &quot;dnnFit&quot;.
</p>



<h3>Note</h3>

<p>For right censored survival time only</p>


<h3>Author(s)</h3>

<p>Chen, B. E.
</p>


<h3>References</h3>

<p>Norman, P. and Chen, B. E. (2023). DeepAFAT: A nonparametric accelerated failure time model with artificial neural network. Manuscript to be submitted. 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+deepAFT">deepAFT</a></code>, <code><a href="#topic+deepGLM">deepGLM</a></code>, <code><a href="#topic+deepSurv">deepSurv</a></code>, <code><a href="#topic+dnnFit">dnnFit</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Example for dnnControl
##
# model = dNNmodel()

  control = dnnControl(loss='mse')
  
# can also be used in   
# fit = dnnFit(y ~ x, model, control) 
# print(fit)
</code></pre>

<hr>
<h2 id='dnnFit'>
Fitting a Deep Learning model with a given loss function
</h2><span id='topic+dnnFit'></span><span id='topic+dnnFit2'></span>

<h3>Description</h3>

<p>dnnFit is used to train a deep learning neural network model based on a specified loss function. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dnnFit(x, y, model, control)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dnnFit_+3A_x">x</code></td>
<td>
<p>covariates for the neural network model</p>
</td></tr>
<tr><td><code id="dnnFit_+3A_y">y</code></td>
<td>
<p>output (target) value for neural network model</p>
</td></tr>
<tr><td><code id="dnnFit_+3A_model">model</code></td>
<td>
<p>the neural network model, see below for details</p>
</td></tr>
<tr><td><code id="dnnFit_+3A_control">control</code></td>
<td>
<p>a list of control values, in the format produced by
'dnnControl'. The default value is dnnControl(loss='mse')</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The 'dnnFit' function takes the input data, the target values, the network architecture, and the loss function as arguments, and returns a trained model that minimizes the loss function. The function also supports various options for regularization and optimization of the model.
</p>
<p>See <code><a href="#topic+dNNmodel">dNNmodel</a></code> for details on how to specify a deep learning model. 
</p>
<p>Parameters in <code><a href="#topic+dnnControl">dnnControl</a></code> will be used to control the model fit process. The loss function can be specified as dnnControl(loss = &quot;lossFunction&quot;). Currently, the following loss functions are supported: 
</p>
<p>'mse': Mean square error loss = 0.5*sum(dy^2)
</p>
<p>'cox': Cox partial likelihood loss = -sum(delta*(yhat - log(S0)))
</p>
<p>'bin': Cross-entropy = -sum(y*log(p) + (1-y)*log(1-p))
</p>
<p>'log': Log linear cost = -sum(y*log(lambda)-lambda)
</p>
<p>'mae': Mean absolute error loss = sum(abs(dy))
</p>
<p>Additional loss functions will be added to the library in the future.
</p>
<p>{ dnnFit2 } is a C++ version of dnnFit, which runs about 20% faster, however, only loss = 'mse' and 'cox' are currently supported. 
</p>
<p>When the variance for covariance matrix X is too large, please use xbar = scale(x) to standardize X.
</p>


<h3>Value</h3>

<p>An object of class &quot;dnnFit&quot; is returned. The dnnFit object contains the following list components:
</p>
<table>
<tr><td><code>cost</code></td>
<td>
<p>cost at the final epoch.</p>
</td></tr>
<tr><td><code>dW</code></td>
<td>
<p>the gradient at the final epoch dW = dL/dW.</p>
</td></tr>
<tr><td><code>fitted.values</code></td>
<td>
<p>predictor value mu = f(x).</p>
</td></tr>
<tr><td><code>history</code></td>
<td>
<p>a cost history at each epoch.</p>
</td></tr>
<tr><td><code>lp</code></td>
<td>
<p>predictor value mu = f(x).</p>
</td></tr>
<tr><td><code>logLik</code></td>
<td>
<p>-2*log Likelihood = cost.</p>
</td></tr>
<tr><td><code>model</code></td>
<td>
<p>a dNNmodel object.</p>
</td></tr>
<tr><td><code>residuals</code></td>
<td>
<p>raw residual dy = d log(L)/dmu</p>
</td></tr>
<tr><td><code>dvi</code></td>
<td>
<p>deviance dvi = dy*dy</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Chen, B. E. and Norman P.
</p>


<h3>References</h3>

<p>Buckley, J. and James, I. (1979). Linear regression with censored data. Biometrika, 66, page 429-436.
</p>
<p>Norman, P. and Chen, B. E. (2019). DeepAFAT: A nonparametric accelerated failure time model with artificial neural network. Manuscript to be submitted. 
</p>
<p>Chollet, F. and Allaire J. J. (2017). Deep learning with R. Manning.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+deepAFT">deepAFT</a></code>, <code><a href="#topic+deepGlm">deepGlm</a></code>, <code><a href="#topic+deepSurv">deepSurv</a></code>,   <code><a href="#topic+dnnControl">dnnControl</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Example for dnnFit with MSE loss function to do a non-linear regression
  set.seed(101)
### define model layers
  model = dNNmodel(units = c(4, 3, 1), activation = c("elu", "sigmoid", "sigmoid"), 
                   input_shape = 3)
  x = matrix(runif(15), nrow = 5, ncol = 3)
  y = exp(x[, 1])
  control = dnnControl(loss='mse')
  fit = dnnFit(x, y, model, control) 
</code></pre>

<hr>
<h2 id='dNNmodel'>
Specify a deep neural network model
</h2><span id='topic+dNNmodel'></span>

<h3>Description</h3>

<p>{dNNmodel} is an R function to create a deep neural network model that is to be used 
in the feed forward network { fwdNN } and back propagation { bwdNN }.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  dNNmodel(units, activation=NULL, input_shape = NULL, type = NULL, 
           N = NULL, Rcpp=TRUE, optimizer = c("momentum", "nag", "adam"))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="dNNmodel_+3A_units">units</code></td>
<td>
<p>number of nodes for each layer</p>
</td></tr>
<tr><td><code id="dNNmodel_+3A_activation">activation</code></td>
<td>
<p>activation function</p>
</td></tr>
<tr><td><code id="dNNmodel_+3A_input_shape">input_shape</code></td>
<td>
<p>the number of columns of input X, default is NULL.</p>
</td></tr> 
<tr><td><code id="dNNmodel_+3A_n">N</code></td>
<td>
<p>the number of training sample, default is NULL.</p>
</td></tr> 
<tr><td><code id="dNNmodel_+3A_type">type</code></td>
<td>
<p>default is &quot;dense&quot;, currently only support dense layer.</p>
</td></tr> 
<tr><td><code id="dNNmodel_+3A_rcpp">Rcpp</code></td>
<td>
<p>use Rcpp (C++ for R) to speed up the fwdNN and bwdNN, default is &quot;TRUE&quot;.</p>
</td></tr> 
<tr><td><code id="dNNmodel_+3A_optimizer">optimizer</code></td>
<td>
<p>optimizer used in SGD, default is &quot;momentum&quot;.</p>
</td></tr> 

</table>


<h3>Details</h3>

<p>dNNmodel returns an object of class &quot;dNNmodel&quot;.
</p>
<p>The function &quot;print&quot; (i.e., &quot;print.dNNmodel&quot;) can be used to print a summary of the dnn model,
</p>
<p>The function &quot;summary&quot; (i.e., &quot;summary.dNNmodel&quot;) can be used to print a summary of the dnn model,
</p>


<h3>Value</h3>

<p>An object of class &quot;dNNmodel&quot; is a list containing at least the following components:
</p>
<table>
<tr><td><code>units</code></td>
<td>
<p>number of nodes for each layer</p>
</td></tr>
<tr><td><code>activation</code></td>
<td>
<p>activation function</p>
</td></tr>
<tr><td><code>drvfun</code></td>
<td>
<p>derivative of the activation function</p>
</td></tr>
<tr><td><code>params</code></td>
<td>
<p>the initial values of the parameters, to be updated in model training.</p>
</td></tr> 
<tr><td><code>input_shape</code></td>
<td>
<p>the number of columns of input X, default is NULL.</p>
</td></tr>
<tr><td><code>N</code></td>
<td>
<p>the number of training sample, default is NULL.</p>
</td></tr>
<tr><td><code>type</code></td>
<td>
<p>default is &quot;dense&quot;, currently only support dense layer.</p>
</td></tr> 
</table>


<h3>Author(s)</h3>

<p>Bingshu E. Chen (bingshu.chen@queensu.ca)
</p>


<h3>See Also</h3>

<p><code><a href="#topic+plot.dNNmodel">plot.dNNmodel</a></code>,
<code><a href="#topic+print.dNNmodel">print.dNNmodel</a></code>,
<code><a href="#topic+summary.dNNmodel">summary.dNNmodel</a></code>,
<code><a href="#topic+fwdNN">fwdNN</a></code>,
<code><a href="#topic+bwdNN">bwdNN</a></code>,
<code><a href="#topic+optimizerSGD">optimizerSGD</a></code>,
<code><a href="#topic+optimizerNAG">optimizerNAG</a></code>,
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### To define a dnn model
 model = dNNmodel(units = c(8, 6, 1), activation = c("relu", "sigmoid", "sigmoid"), 
         input_shape = c(3))
</code></pre>

<hr>
<h2 id='fwdNN'>
Feed forward and back propagation for dnn Models
</h2><span id='topic+fwdNN'></span><span id='topic+fwdNN2'></span><span id='topic+predict.dNNmodel'></span>

<h3>Description</h3>

<p>{fwdNN} is an R function for feed forward network.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>   fwdNN(X, model)             
#
# to calculate a feed feedward model 
#
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fwdNN_+3A_x">X</code></td>
<td>
<p>For &quot;dNNmodel&quot;, X is a design matrix of dimension n * p.</p>
</td></tr>
<tr><td><code id="fwdNN_+3A_model">model</code></td>
<td>
<p>a model return from dNNmodel function.</p>
</td></tr>




</table>


<h3>Details</h3>

<p>'cache' is the cache of each input layer, will be used in the bwdNN function.
</p>


<h3>Value</h3>

<p>The function fwdNN return a list containing at least the following components:
</p>
<table>
<tr><td><code>cache</code></td>
<td>
<p>a list contains the values of each output layer after activation function transformation and adding the 
intercept term (i.e. the bias term). The intercept does not add to the output layer in the cache.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Bingshu E. Chen (bingshu.chen@queensu.ca)
</p>


<h3>See Also</h3>

<p><code><a href="#topic+bwdNN">bwdNN</a></code>,
<code><a href="#topic+plot.dNNmodel">plot.dNNmodel</a></code>,
<code><a href="#topic+print.dNNmodel">print.dNNmodel</a></code>,
<code><a href="#topic+summary.dNNmodel">summary.dNNmodel</a></code>,
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### define a dnn model, calculate the feed forward network
   model = dNNmodel(units = c(8, 6, 1), activation = c("elu", "sigmoid", "sigmoid"), 
                   input_shape = 3)
  
### feed forward with a dummy x matrix
   x = matrix(runif(15), nrow = 5, ncol = 3)
   cache = fwdNN(x, model)
</code></pre>

<hr>
<h2 id='hyperTuning'>
A function for tuning of the hyper parameters
</h2><span id='topic+hyperTuning'></span><span id='topic+CVpredErr'></span>

<h3>Description</h3>

<p>{ hyperTuning} is a tuning tool to find the optimal hyper parameter for the ANN model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>   hyperTuning(x, y, model, ER = c("cindex", "mse"), 
          method = c('BuckleyJames', 'ipcw', 'transform', 'deepSurv'), 
          lower = NULL, upper = NULL, node = FALSE,
          K = 5, R = 25)
### additional function used in hyperTuning is cross-validation prediction error
#
#  CVpredErr(x, y, model, control, method)
#
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hyperTuning_+3A_x">x</code></td>
<td>
<p>Covariates for the deep neural network model</p>
</td></tr>
<tr><td><code id="hyperTuning_+3A_y">y</code></td>
<td>
<p>Surv object for the deep neural network model</p>
</td></tr>
<tr><td><code id="hyperTuning_+3A_model">model</code></td>
<td>
<p>A deep neural network model, created by function dNNmodel().</p>
</td></tr>
<tr><td><code id="hyperTuning_+3A_er">ER</code></td>
<td>
<p>Prediction error measurement to be used in the cross vaditation, can be either a concordance index (cindex) or a mean square error (mse), default is cindex</p>
</td></tr>
<tr><td><code id="hyperTuning_+3A_method">method</code></td>
<td>
<p>Methods to handle censoring data in deep AFT model fit, 'BuckleyJames' for the Buckley and James method, 
'ipcw' for the inverse probability censoring weights method.
'transform' for the transformation method based on book of Fan and Gijbels (1996, page 168).
'deepSurv' for the deepSurv model(Katzman, 2017)</p>
</td></tr>
<tr><td><code id="hyperTuning_+3A_node">node</code></td>
<td>
<p>Tuning the number of nodes in each hidden layer, default is FALSE</p>
</td></tr>
<tr><td><code id="hyperTuning_+3A_k">K</code></td>
<td>
<p>Number of folders of the cross-validatin, default is K = 5.</p>
</td></tr>
<tr><td><code id="hyperTuning_+3A_lower">lower</code>, <code id="hyperTuning_+3A_upper">upper</code></td>
<td>
<p>Bounds on the hyper parameters for the deep learning method. If NULL, then the default value for lower = dnnControl(alpha = 0.5, lambda = 1.0, lr_rate = 0.0001), upper = dnnControl(alpha = 0.97, lambda = 10, lr_rate = 0.001). </p>
</td></tr>
<tr><td><code id="hyperTuning_+3A_r">R</code></td>
<td>
<p>Number of random sample draw from the hyper parameter space, default is R = 25.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A random search method is used to optimal hyper parameter (Bergstra and Bengio, 2012). 
The function { CVpredErr} will be call to calculate the cross-validation prediction error for the given x and y with the specified method from the input argument.
</p>


<h3>Value</h3>

<p>A list of &quot;model&quot; and &quot;dnnControl&quot; is returned.  The list contains at least the following components, 
</p>
<table>
<tr><td><code>model</code></td>
<td>
<p>The &quot;model&quot; contains the optimal number of nodes for each hidden layer in the model specified by <code><a href="#topic+dNNmodel">dNNmodel</a></code></p>
</td></tr>
<tr><td><code>control</code></td>
<td>
<p>The &quot;control&quot; contains the optimal tuning parameters with list components the same as those created by <code><a href="#topic+dnnControl">dnnControl</a></code></p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Chen, B. E. (chenbe@queensu.ca)
</p>


<h3>References</h3>

<p>Bergstra, J. and Bengio, Y. (2012). Random search for hyper-parameter optimization. The Journal of Machine Learning Research. 13, page 281-305.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+deepAFT">deepAFT</a></code>, <code><a href="#topic+deepGLM">deepGLM</a></code>, <code><a href="#topic+deepSurv">deepSurv</a></code>, <code><a href="#topic+dnnFit">dnnFit</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>### Tuning the hyper parameter for a deepAFT model: 
#### cross-validation take a long time to run.

  set.seed(101)
### define model layers
  model = dNNmodel(units = c(4, 3, 1), activation = c("elu", "sigmoid", "sigmoid"), 
                   input_shape = 3)
  x = matrix(runif(45), nrow = 15, ncol = 3)
  time = exp(x[, 1])
  status = rbinom(15, 1, 0.5)
  y = Surv(time, status)
  ctl = dnnControl(epochs = 30)
  hyperTuning(x, y, model, method = "BuckleyJames", K = 2, R = 2, lower = ctl)

</code></pre>

<hr>
<h2 id='ibs'>
Calculate integrated Brier Score for deepAFT 
</h2><span id='topic+ibs'></span><span id='topic+ibs.deepAFT'></span><span id='topic+ibs.default'></span>

<h3>Description</h3>

<p>The function ibs is used to calculate integrated Brier Score for deepAFT.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ibs(object, ...)
### To calculate Brier score for the original fitted data
## Default S3 method:
ibs(object, ...)
### To calculate Brier score for new data with new outcomes
## S3 method for class 'deepAFT'
ibs(object, newdata=NULL, newy = NULL, ...)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ibs_+3A_object">object</code></td>
<td>
<p>the results of a deepAFT fit.</p>
</td></tr>
<tr><td><code id="ibs_+3A_newdata">newdata</code></td>
<td>
<p>optional argument, if no null, new data and new y will be used for calculation.</p>
</td></tr>
<tr><td><code id="ibs_+3A_newy">newy</code></td>
<td>
<p>optional argument, used together with new data.</p>
</td></tr>
<tr><td><code id="ibs_+3A_...">...</code></td>
<td>
<p>other unused arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>ibs is called to calculate integrate Brier score for the deepAFT model <code><a href="#topic+deepAFT">deepAFT</a></code>.
</p>


<h3>Value</h3>

<p>A list contains the integrate Brier score and the Brier score is returned:
</p>
<table>
<tr><td><code>ibs</code></td>
<td>
<p>Integerate Brier score</p>
</td></tr>
<tr><td><code>bs</code></td>
<td>
<p>Brier score</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Bingshu E. Chen</p>


<h3>See Also</h3>

<p><code><a href="#topic+deepAFT">deepAFT</a></code>
</p>

<hr>
<h2 id='msePICW'>
Mean Square Error (mse) for a survival Object
</h2><span id='topic+mseIPCW'></span>

<h3>Description</h3>

<p>Compute Mean Square Error (mse) values for a survival object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'deepAFT'
mseIPCW(object, newdata, newy)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="msePICW_+3A_object">object</code></td>
<td>
<p>the results of a model fit using a deepAFT or a survreg function.</p>
</td></tr>
<tr><td><code id="msePICW_+3A_newdata">newdata</code></td>
<td>
<p>optional new data at which to do predictions. If absent, predictions are for the dataframe used in the original fit.</p>
</td></tr>
<tr><td><code id="msePICW_+3A_newy">newy</code></td>
<td>
<p>optional new outcome variable y.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>predict is called to predict object from a deepAFT <code><a href="#topic+deepAFT">deepAFT</a></code> or a survreg model.
</p>
<p>IPCW method is used to calcuate the mean square error for censored survival time.
</p>


<h3>Value</h3>

<p>mseIPCW returns the mse for the predicted survival data.
</p>


<h3>Author(s)</h3>

<p>Bingshu E. Chen</p>


<h3>See Also</h3>

<p>The default method for predict <code><a href="stats.html#topic+predict">predict</a></code>,
<code><a href="#topic+deepAFT">deepAFT</a></code>, <code><a href="#topic+survfit.dSurv">survfit.dSurv</a></code>
</p>

<hr>
<h2 id='optimizerSGD'>
Functions to optimize the gradient descent of a cost function
</h2><span id='topic+optimizerSGD'></span><span id='topic+optimizerMomentum'></span><span id='topic+optimizerAdamG'></span><span id='topic+optimizerNAG'></span>

<h3>Description</h3>

<p>Different type of optimizer functions such as SGD, Momentum, AdamG and NAG.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  optimizerMomentum(V, dW, W, alpha = 0.63, lr = 1e-4, lambda = 1) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="optimizerSGD_+3A_v">V</code></td>
<td>
<p>Momentum V = alpha*V - lr*(dW + lambda*W); W = W + V. 
NAG V = alpha*(V - lr*(dW + lambda*W); W = W + V - lr*(dW + lambda*W)</p>
</td></tr>
<tr><td><code id="optimizerSGD_+3A_dw">dW</code></td>
<td>
<p>derivative of cost with respect to W, can be founde by dW = bwdNN2(dy, cache, model), </p>
</td></tr>
<tr><td><code id="optimizerSGD_+3A_w">W</code></td>
<td>
<p>weights for DNN model, optimizerd by W = W + V</p>
</td></tr>
<tr><td><code id="optimizerSGD_+3A_alpha">alpha</code></td>
<td>
<p>Momentum rate 0 &lt; alpha &lt; 1, default is alpah = 0.5.</p>
</td></tr>
<tr><td><code id="optimizerSGD_+3A_lr">lr</code></td>
<td>
<p>learning rate, default is lr = 0.001.</p>
</td></tr>
<tr><td><code id="optimizerSGD_+3A_lambda">lambda</code></td>
<td>
<p>regulation rate for cost + 0.5*lambda*||W||, default is lambda = 1.0.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For SGD with momentum, use
</p>
<p>V = 0; obj = optimizerMomentum(V, dW, W); V = obj$V; W = obj$W
</p>
<p>For SDG with MAG
</p>
<p>V = 0; obj = optimizerNAG(V, dW, W); V = obj$V; W = obj$W
</p>


<h3>Value</h3>

<p>return and updated W and other parameters such as V, V1 and V2 that will be used on SGD.
</p>


<h3>Author(s)</h3>

<p>Bingshu E. Chen</p>


<h3>See Also</h3>

<p><code><a href="#topic+activation">activation</a></code>, 
<code><a href="#topic+bwdNN">bwdNN</a></code>, 
<code><a href="#topic+fwdNN">fwdNN</a></code>,
<code><a href="#topic+dNNmodel">dNNmodel</a></code>, 
<code><a href="#topic+dnnFit">dnnFit</a></code> 
</p>

<hr>
<h2 id='plot'>
Plot methods in dnn package
</h2><span id='topic+plot.deepAFT'></span><span id='topic+plot.dNNmodel'></span>

<h3>Description</h3>

<p>Plot function for plotting of R objects in the dnn package.
</p>
<p>Several different type of plots can be produced for the deep learning mdels. 
Plot method is used to provide a summary of outputs from &quot;deepAFT&quot;, &quot;deepGLM&quot;, &quot;deepSurv&quot; and &quot;dnn&quot;.
</p>
<p>Use &quot;methods(plot)&quot; and the documentation for these for other plot methods.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'dNNmodel'
plot(x, ...)
## S3 method for class 'deepAFT'
plot(x, type = c("predicted", "residuals", "baselineKM"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot_+3A_x">x</code></td>
<td>
<p>a class of &quot;dNNmodel&quot;.</p>
</td></tr>
<tr><td><code id="plot_+3A_type">type</code></td>
<td>
<p>type of plot in deepAFT object, &quot;predicted&quot; to plot the linear predicted values, &quot;residuals&quot; to plot residuals, &quot;baselineKM&quot; to plot baseline Kaplan-Meier survival curve.</p>
</td></tr>
<tr><td><code id="plot_+3A_...">...</code></td>
<td>
<p>other options used in plot().</p>
</td></tr>
</table>


<h3>Details</h3>

<p>plot.deepAFT is called to plot the fitted deep learning AFT model.
</p>
<p>plot.dNNmodel is called to plot fitted dnn model
</p>
<p>The default method, plot.default has its own help page. Use methods(&quot;plot&quot;) to get all the methods for the plot generic.
</p>


<h3>Value</h3>

<p>No return value, called to plot a figure.</p>


<h3>Author(s)</h3>

<p>Bingshu E. Chen</p>


<h3>See Also</h3>

<p>The default method for plot <code><a href="graphics.html#topic+plot.default">plot.default</a></code>.
<code><a href="stats.html#topic+glm">glm</a></code>
</p>

<hr>
<h2 id='predict'>
Predicted Values for a deepAFT Object
</h2><span id='topic+predict.dSurv'></span>

<h3>Description</h3>

<p>Compute predicted values for a deepAFT object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'deepAFT'
## S3 method for class 'dSurv'
predict(object, newdata, newy=NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict_+3A_object">object</code></td>
<td>
<p>the results of a model fit using the deepAFT function.</p>
</td></tr>
<tr><td><code id="predict_+3A_newdata">newdata</code></td>
<td>
<p>optional new data at which to do predictions. If absent, predictions are for the dataframe used in the original fit.</p>
</td></tr>
<tr><td><code id="predict_+3A_newy">newy</code></td>
<td>
<p>optional new outcome variable y.</p>
</td></tr>
<tr><td><code id="predict_+3A_...">...</code></td>
<td>
<p>other options used in predict().</p>
</td></tr>
</table>


<h3>Details</h3>

<p>predict.dSurv is called to predict object from the deepAFT or deepSurv model <code><a href="#topic+deepAFT">deepAFT</a></code>.
</p>
<p>The default method, predict has its own help page. Use methods(&quot;predict&quot;) to get all the methods for the predict generic.
</p>


<h3>Value</h3>

<p>predict.dSurv returns a list of predicted values, prediction error and residuals. 
</p>
<table>
<tr><td><code>lp</code></td>
<td>
<p>linear predictor of beta(w)*Z, where beta(w) is the fitted regression coefficient and Z is covariance matrix.</p>
</td></tr>
<tr><td><code>risk</code></td>
<td>
<p>risk score, exp(lp). When new y is provided, both lp and risk will be ordered by survival time of the new y.</p>
</td></tr>
<tr><td><code>cumhaz</code></td>
<td>
<p>cumulative hzard function.</p>
</td></tr>
<tr><td><code>time</code></td>
<td>
<p>time for cumulative hazard function. Time from new y will be used is provided</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Bingshu E. Chen</p>


<h3>See Also</h3>

<p>The default method for predict <code><a href="stats.html#topic+predict">predict</a></code>,
<code><a href="#topic+deepAFT">deepAFT</a></code>, <code><a href="#topic+survfit.dSurv">survfit.dSurv</a></code>
</p>

<hr>
<h2 id='print'>
print a summary of fitted deep learning model object
</h2><span id='topic+print.deepAFT'></span><span id='topic+print.deepGlm'></span><span id='topic+print.deepSurv'></span><span id='topic+print.dNNmodel'></span><span id='topic+print.summary.deepAFT'></span><span id='topic+print.summary.deepGlm'></span><span id='topic+print.summary.deepSurv'></span><span id='topic+print.summary.dNNmodel'></span><span id='topic+summary.dNNmodel'></span><span id='topic+summary.deepAFT'></span>

<h3>Description</h3>

<p>print is used to provide a short summary of outputs from <code><a href="#topic+deepAFT">deepAFT</a></code>, <code><a href="#topic+deepSurv">deepSurv</a></code>, <code><a href="#topic+deepGLM">deepGLM</a></code>, and <code><a href="#topic+dNNmodel">dNNmodel</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'deepAFT'
print(x, ...)
## S3 method for class 'summary.deepAFT'
print(x, ...)
## S3 method for class 'deepAFT'
summary(object, ...)

## S3 method for class 'dNNmodel'
print(x, ...)
## S3 method for class 'dNNmodel'
summary(object, ...)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print_+3A_x">x</code></td>
<td>
<p>a class returned from deepAFT, deepSurv, deepGLM model fit or a dNNmodel</p>
</td></tr>
<tr><td><code id="print_+3A_object">object</code></td>
<td>
<p>a class of deepAFT object</p>
</td></tr>
<tr><td><code id="print_+3A_...">...</code></td>
<td>
<p>other options used in print()</p>
</td></tr>
</table>


<h3>Details</h3>

<p>print.deepAFT is called to print object or summary of object from the deep learning AFT models <code><a href="#topic+deepAFT">deepAFT</a></code>.  
summary(fit) provides detail summary of &lsquo;deepAFT&rsquo; model fit, including predictors, 
baseline survival function for T0=T/exp(mu), and martingale residuals for the fitted model.
</p>
<p>print.dNNmodel is called to print object or summary of object from the dNNmodel.
</p>
<p>The default method, print.default has its own help page. Use methods(&quot;print&quot;) to get all the methods for the print generic.
</p>


<h3>Value</h3>


<p>An object of class &quot;summary.deepAFT&quot; is returned. The object contains the following list components:
</p>

<table>
<tr><td><code>location</code></td>
<td>
<p>location parameter exp(mu), to predice the mean value of survival time.</p>
</td></tr>
<tr><td><code>sfit</code></td>
<td>
<p>survfit object of the baselie survival function of T0=T/exp(mu).</p>
</td></tr>
<tr><td><code>cindex</code></td>
<td>
<p>Concordance index of the fitted deepAFT model.</p>
</td></tr>
<tr><td><code>resid</code></td>
<td>
<p>martingle residuals of the fitted deepAFT model.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>the model used to fit the deepAFT model.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Bingshu E. Chen</p>


<h3>See Also</h3>

<p>The default method for print <code><a href="base.html#topic+print.default">print.default</a></code>. Other methods include
<code><a href="survival.html#topic+survreg">survreg</a></code>, 
<code><a href="#topic+deepAFT">deepAFT</a></code>, 
<code><a href="base.html#topic+summary">summary</a></code>
</p>

<hr>
<h2 id='residuals'>
Calculate Residuals for a deepAFT Fit. 
</h2><span id='topic+residuals.deepAFT'></span><span id='topic+residuals.dSurv'></span>

<h3>Description</h3>

<p>Calculates martingale, deviance or Cox-Snell residuals for a previously fitted (deepAFT) model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'deepAFT'
## S3 method for class 'dSurv'
residuals(object, type = c("martingale", "deviance", "coxSnell"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="residuals_+3A_object">object</code></td>
<td>
<p>the results of a (deepAFT) fit.</p>
</td></tr>
<tr><td><code id="residuals_+3A_type">type</code></td>
<td>
<p>character string indicating the type of residual desired. Possible values are &quot;martingale&quot;, &quot;deviance&quot;. Only enough of the string to determine a unique match is required.</p>
</td></tr>
<tr><td><code id="residuals_+3A_...">...</code></td>
<td>
<p>other unused arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>residuals.deepAFT is called to compute baseline survival function S_T0(t) from the deepAFT model <code><a href="#topic+deepAFT">deepAFT</a></code>, where T0 = T/exp(mu), or log(T) = log(T) - mu.
</p>
<p>The default method, residuals has its own help page. Use methods(&quot;residuals&quot;) to get all the methods for the residuals generic.
</p>


<h3>Value</h3>

<p>For martingale and deviance residuals, the returned object is a vector with one element for each subject.
The row order will match the input data for the original fit.
</p>
<p>See <code><a href="stats.html#topic+residuals">residuals</a></code> for more detail about other output values. 
</p>


<h3>Note</h3>

<p>For deviance residuals, the status variable may need to be reconstructed.</p>


<h3>Author(s)</h3>

<p>Bingshu E. Chen</p>


<h3>See Also</h3>

<p>The default method for residuals <code><a href="stats.html#topic+residuals">residuals</a></code>,
<code><a href="#topic+predict.dSurv">predict.dSurv</a></code>, <code><a href="#topic+survfit.dSurv">survfit.dSurv</a></code>, and <code><a href="#topic+deepAFT">deepAFT</a></code>.
</p>

<hr>
<h2 id='rsurv'>
The Survival Distribution
</h2><span id='topic+rsurv'></span><span id='topic+rSurv'></span><span id='topic+dsurv'></span><span id='topic+psurv'></span><span id='topic+qsurv'></span><span id='topic+rcoxph'></span>

<h3>Description</h3>

<p>Density, distribution function, quantile function and random variable generation for a survival distribution with a provided hazard function or cumulative hazard function 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>  dsurv(x, h0 = NULL, H0 = function(x){x}, log=FALSE)
  psurv(q, h0 = NULL, H0 = function(x){x}, low.tail=TRUE, log.p=FALSE) 
  qsurv(p, h0 = NULL, H0 = function(x){x}, low.tail=TRUE)
  rsurv(n, h0 = NULL, H0 = function(x){x})
  rcoxph(n, h0 = NULL, H0 = function(x){x}, lp = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rsurv_+3A_x">x</code>, <code id="rsurv_+3A_q">q</code></td>
<td>
<p>vector of quantiles.</p>
</td></tr>
<tr><td><code id="rsurv_+3A_p">p</code></td>
<td>
<p>vector of probabilities.</p>
</td></tr>
<tr><td><code id="rsurv_+3A_n">n</code></td>
<td>
<p>number of observations.</p>
</td></tr>
<tr><td><code id="rsurv_+3A_h0">h0</code></td>
<td>
<p>hazard function, default is h0 = NULL.</p>
</td></tr>
<tr><td><code id="rsurv_+3A_h0">H0</code></td>
<td>
<p>cumulative hazard function, default is H0(x) = x.</p>
</td></tr>
<tr><td><code id="rsurv_+3A_lp">lp</code></td>
<td>
<p>linear predictor for rcoxph, H(x) = H0(x)exp(lp).</p>
</td></tr>
<tr><td><code id="rsurv_+3A_log">log</code>, <code id="rsurv_+3A_log.p">log.p</code></td>
<td>
<p>logical; if TRUE, probabilities p are give as log(p).</p>
</td></tr>
<tr><td><code id="rsurv_+3A_low.tail">low.tail</code></td>
<td>
<p>logical; if TRUE, probabilities are P[X &lt; or = x] otherwise, S(x) = P[X&gt;x].</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If { h0 } or { H0 } are not specified, they assume the default values of h0(x) = 1 and H0(x) = x, respectively. 
</p>
<p>The survival distribution function is given by,
</p>
<p>S(x) = exp(-H0(x)), 
</p>
<p>where H0(x) is the cumulative hazard function. Only one of h0 or H0 can be specified, if h0 is given, then H0(x) = integrate(h0, 0, x, subdivisions = 500L)
</p>
<p>To generate Cox PH survival time, use 
</p>
<p>u = exp(-H(t)*exp(lp))
</p>
<p>then, -log(u)*exp(-lp) = H(t). Find t such that H(t) = -log(u)exp(-lp).
</p>


<h3>Value</h3>

<p>{ dsurv } gives the density h(x)/S(x), { psurv } gives the distribution function, { qsurv } gives the quantile function,  { rsurv } generates random survival time, and { rcoxph } generates random survival time with Cox proportional hazards model.
</p>
<p>The length of the result is determined by n for rsurv and rcoxph.
</p>


<h3>Author(s)</h3>

<p>Bingshu E. Chen
</p>


<h3>References</h3>

<p>Johnson, N. L., Kotz, S. and Balakrishnan, N. (1995). Continuous Univariate Distributions, volume 1. Wiley, New York.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+Distributions">Distributions</a></code> for other standard distributions, including <code><a href="stats.html#topic+dweibull">dweibull</a></code> for the Weibull distribution.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#### use qsurv to generate quantiles for weibull distribution
H1 = function(x) x^3
qsurv(seq(0.1, 0.9, 0.2), H0 = H1) ### shall be the same as
qweibull(seq(0.1, 0.9, 0.2), 3)
#### to get random survival time from the cumulative hazard function H1(t)
rsurv(15, H0 = H1)
</code></pre>

<hr>
<h2 id='survfit'>
Compute a Survival Curve from a deepAFT or a deepSurv Model 
</h2><span id='topic+survfit.dSurv'></span>

<h3>Description</h3>

<p>Computes the predicted survival function of a previously fitted deepAFT or deepSurv model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'deepAFT' or 'deepSurv'
## S3 method for class 'dSurv'
survfit(formula, se.fit=TRUE, conf.int=.95, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="survfit_+3A_formula">formula</code></td>
<td>
<p>a deepAFT or deepSurv fit object.</p>
</td></tr>
<tr><td><code id="survfit_+3A_se.fit">se.fit</code></td>
<td>
<p>a logical value indicating whether standard errors shall be computed. Default is TRUE</p>
</td></tr>
<tr><td><code id="survfit_+3A_conf.int">conf.int</code></td>
<td>
<p>the level for a two-sided confidence interval on the survival curve. Default is 0.95</p>
</td></tr>
<tr><td><code id="survfit_+3A_...">...</code></td>
<td>
<p>other unused arguments.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>survfit.dSurv is called to compuate baseline survival function S_T0(t) from the deepAFT model <code><a href="#topic+deepAFT">deepAFT</a></code>, where T0 = T/exp(mu), or log(T) = log(T) - mu.
</p>
<p>For the deepSurv model  <code><a href="#topic+deepAFT">deepAFT</a></code>, survfit.dSurv evaluates the Nelson-Aalen estimate of the baseline survival function.
</p>
<p>The default method, survfit has its own help page. Use methods(&quot;survfit&quot;) to get all the methods for the survfit generic.
</p>


<h3>Value</h3>

<p>survfit.deepAFT returns a list of predicted baseline survival function, cumulative hazard function and residuals. 
</p>
<table>
<tr><td><code>surv</code></td>
<td>
<p>Predicted baseline survival function for T0=T/exp(mu).</p>
</td></tr>
<tr><td><code>cumhaz</code></td>
<td>
<p>Baseline cumulative hazard function, -log(surv).</p>
</td></tr>
<tr><td><code>hazard</code></td>
<td>
<p>Baseline hazard function.</p>
</td></tr> 
<tr><td><code>varhaz</code></td>
<td>
<p>Variance of the baseline hazard.</p>
</td></tr>
<tr><td><code>residuals</code></td>
<td>
<p>Martingale residuals of the (deepAFT) model.</p>
</td></tr> 
<tr><td><code>std.err</code></td>
<td>
<p>Standard error for the cumulative hazard function, if se.fit = TRUE.</p>
</td></tr>
</table>
<p>See <code><a href="survival.html#topic+survfit">survfit</a></code> for more detail about other output values such as upper, lower, conf.type. 
Confidence interval is based on log-transformation of survival function. 
</p>


<h3>Author(s)</h3>

<p>Bingshu E. Chen</p>


<h3>See Also</h3>

<p>The default method for survfit <code><a href="survival.html#topic+survfit">survfit</a></code>,
<code><a href="#topic+predict.dSurv">predict.dSurv</a></code>
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
