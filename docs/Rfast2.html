<!DOCTYPE html><html><head><title>Help for package Rfast2</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {Rfast2}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='# Benchmark - Measure time '>
<p>Benchmark - Measure time</p></a></li>
<li><a href='# Check if a matrix is Lower or Upper triangular '>
<p>Check if a matrix is Lower or Upper triangular</p></a></li>
<li><a href='# Column-wise summary statistics with grouping variables '>
<p>Column-wise summary statistics with grouping variables</p></a></li>
<li><a href='# Merge 2 sorted vectors in 1 sorted vector '>
<p>Merge 2 sorted vectors in 1 sorted vector</p></a></li>
<li><a href='# Split the matrix in lower, upper triangular and diagonal '>
<p>Split the matrix in lower, upper triangular and diagonal</p></a></li>
<li><a href='#Add many single terms to a model'>
<p>Add many single terms to a model</p></a></li>
<li><a href='#Angular Gaussian random values simulation'>
<p>Angular Gaussian random values simulation</p></a></li>
<li><a href='#Anova for circular data'>
<p>Analysis of variance for circular data</p></a></li>
<li><a href='#Backward selection with the F test or the partial correlation coefficient'>
<p>backward selection with the F test or the partial correlation coefficient</p></a></li>
<li><a href='#BIC of many simple univariate regressions'>
<p>BIC of many simple univariate regressions.</p></a></li>
<li><a href='#Binomial regression'>
<p>Binomial regression</p></a></li>
<li><a href='#Bootstrap James and Hotelling test for 2 independent sample mean vectors'>
<p>Bootstrap James and Hotelling test for 2 independent sample mean vectors</p></a></li>
<li><a href='#Bootstrap Student's t-test for 2 independent samples'>
<p>Bootstrap Student's t-test for 2 independent samples</p></a></li>
<li><a href='#Censored Weibull regression model'>
<p>Censored Weibull regression model</p></a></li>
<li><a href='#Check whether a square matrix is skew-symmetric'>
<p>Check whether a square matrix is skew-symmetric</p></a></li>
<li><a href='#Circurlar correlations between two circular variables'><p>Circurlar correlations between two circular variables</p></a></li>
<li><a href='#Cluster robust wild bootstrap for linear models'>
<p>Cluster robust wild bootstrap for linear models</p></a></li>
<li><a href='#Column and row-wise jackknife sample means'>
<p>Column and row-wise jackknife sample means</p></a></li>
<li><a href='#Column-wise means and variances'>
<p>Column-wise means and variances of a matrix</p></a></li>
<li><a href='#Column-wise MLE of some univariate distributions'>
<p>Column-wise MLE of some univariate distributions</p></a></li>
<li><a href='#Column-wise MLE of the angular Gaussian distribution'>
<p>Column-wise MLE of the angular Gaussian distribution</p></a></li>
<li><a href='#Column-wise pooled variances across groups'>
<p>Column-wise pooled variances across groups</p></a></li>
<li><a href='#Column-wise weighted least squares meta analysis'>
<p>Column-wise weighted least squares meta analysis</p></a></li>
<li><a href='#Conditional least-squares estimate for Poisson INAR(1) models'>
<p>Conditional least-squares estimate for Poisson INAR(1) models</p></a></li>
<li><a href='#Constrained least squares'>
<p>Constrained least squares</p></a></li>
<li><a href='#Contour plots of some bivariate distributions'>
<p>Contour plots of some bivariate distributions</p></a></li>
<li><a href='#Correlation significance testing using Fisher's z-transformation'>
<p>Correlation significance testing using Fisher's z-transformation</p></a></li>
<li><a href='#Covariance between a variable and a set of variables'><p>Covariance between a variable and a set of variables</p></a></li>
<li><a href='#Cross-validation for the k-NN algorithm for really lage scale data'>
<p>Cross-validation for the k-NN algorithm for really lage scale data</p></a></li>
<li><a href='#Cross-validation for the multinomial regression'>
<p>Cross-validation for the multinomial regression</p></a></li>
<li><a href='#Cross-validation for the naive Bayes classifiers'>
<p>Cross-validation for the naive Bayes classifiers</p></a></li>
<li><a href='#Cross-validation for the regularised maximum likelihood linear discriminant analysis'>
<p>Cross-validation for the regularised maximum likelihood linear discriminant analysis</p></a></li>
<li><a href='#Diagonal values of the Hat matrix'>
<p>Diagonal values of the Hat matrix</p></a></li>
<li><a href='#Distance between two covariance matrices'>
<p>Distance between two covariance matrices</p></a></li>
<li><a href='#Distance correlation matrix'>
<p>Distance correlation matrix</p></a></li>
<li><a href='#Empirical and exponential empirical likelihood test for a correlation coefficient'>
<p>Empirical and exponential empirical likelihood test for a correlation coefficient</p></a></li>
<li><a href='#Empirical entropy'>
<p>Empirical entropy</p></a></li>
<li><a href='#Energy based normality test'>
<p>Energy based normality test</p></a></li>
<li><a href='#Fisher's linear discriminant analysis'><p>Fisher's linear discriminant analysis</p></a></li>
<li><a href='#Fixed effects regression'>
<p>Fixed effects regression</p></a></li>
<li><a href='#Fixed intercepts Poisson regression'>
<p>Fixed intercepts Poisson regression</p></a></li>
<li><a href='#Forward Backward Early Dropping selection regression'>
<p>Forward Backward Early Dropping selection regression</p></a></li>
<li><a href='#Fractional polynomial regression with one independent variable'>
<p>Fractional polynomial regression with one independent variable.</p></a></li>
<li><a href='#Gamma regression with a log-link'>
<p>Gamma regression with a log-link</p></a></li>
<li><a href='#GEE Gaussian regression'>
<p>GEE Gaussian regression</p></a></li>
<li><a href='#Gumbel regression'>
<p>Gumbel regression</p></a></li>
<li><a href='#Hellinger distance based regression for count data'>
<p>Hellinger distance based regression for count data</p></a></li>
<li><a href='#Hellinger distance based univariate regression for proportions'>
<p>Hellinger distance based univariate regression for proportions</p></a></li>
<li><a href='#Heteroscedastic linear models for large scale data'>
<p>Heteroscedastic linear models for large scale data</p></a></li>
<li><a href='#Hurdle-Poisson regression'>
<p>Hurdle-Poisson regression</p></a></li>
<li><a href='#Hypothesis test for equality of a covariance matrix'>
<p>Hypothesis test for equality of a covariance matrix</p></a></li>
<li><a href='#Hypothesis tests for equality of multiple covariance matrices'>
<p>Hypothesis tests for equality of multiple covariance matrices</p></a></li>
<li><a href='#Intersect'>
<p>Intersect Operation</p></a></li>
<li><a href='#Item difficulty and discrimination'><p>Item difficulty and discrimination</p></a></li>
<li><a href='#Jackknife sample mean'>
<p>Jackknife sample mean</p></a></li>
<li><a href='#Kaplan-Meier estimate of a survival function'><p>Kaplan-Meier estimate of a survival function</p></a></li>
<li><a href='#Linear model with sandwich robust covariance estimator'>
<p>Linear model with sandwich robust covariance estimator</p></a></li>
<li><a href='#Linear regression with clustered data'>
<p>Linear regression with clustered data</p></a></li>
<li><a href='#Logistic regression for large scale data'>
<p>Logistic regression for large scale data</p></a></li>
<li><a href='#Mahalanobis depth'>
<p>Mahalanobis depth</p></a></li>
<li><a href='#Many 2 sample student's t-tests'>
<p>Many 2 sample student's t-tests</p></a></li>
<li><a href='#Many approximate simple logistic regressions'>
<p>Many approximate simple logistic regressions.</p></a></li>
<li><a href='#Many binary classification metrics'>
<p>Many binary classification metrics</p></a></li>
<li><a href='#Many Gamma regressions'>
<p>Many Gamma regressions</p></a></li>
<li><a href='#Many Jarque-Bera normality tests'>
<p>Many Jarque-Bera normality tests</p></a></li>
<li><a href='#Many metrics for a continuous response variable'>
<p>any metrics for a continuous response variable</p></a></li>
<li><a href='#Many negative binomial regressions'>
<p>Many negative binomial regressions</p></a></li>
<li><a href='#Many score based regressions with muliple response variables and a single predictor variable'>
<p>Many score based regressions with muliple response variables and a single predictor variable</p></a></li>
<li><a href='#Many score based zero inflated Poisson regressions'>
<p>Many score based zero inflated Poisson regressions</p></a></li>
<li><a href='#Many simple quantile regressions using logistic regressions'>
<p>Many simple quantile regressions using logistic regressions.</p></a></li>
<li><a href='#Many simple Weibull regressions'>
<p>Many simple Weibull regressions.</p></a></li>
<li><a href='#Many Welch tests'>
<p>Many Welch tests.</p></a></li>
<li><a href='#Max-Min Parents and Children variable selection algorithm for continuous responses'>
<p>Max-Min Parents and Children variable selection algorithm for continuous responses</p></a></li>
<li><a href='#Max-Min Parents and Children variable selection algorithm for non continuous responses'>
<p>Max-Min Parents and Children variable selection algorithm for non continuous responses</p></a></li>
<li><a href='#Maximum likelihood linear discriminant analysis'><p>Maximum likelihood linear discriminant analysis</p></a></li>
<li><a href='#MLE of continuous univariate distributions defined on the positive line'>
<p>MLE of continuous univariate distributions defined on the positive line</p></a></li>
<li><a href='#MLE of distributions defined for proportions'>
<p>MLE of distributions defined for proportions</p></a></li>
<li><a href='#MLE of some circular distributions with multiple samples'>
<p>MLE of some circular distributions with multiple samples</p></a></li>
<li><a href='#MLE of some truncated distributions'><p>MLE of some truncated distributions</p></a></li>
<li><a href='#MLE of the Cauchy and generalised normal distributions with zero location'>
<p>MLE of the Cauchy and generalised normal distributions with zero location</p></a></li>
<li><a href='#MLE of the censored Weibull distribution'>
<p>MLE of the censored Weibull distribution</p></a></li>
<li><a href='#MLE of the gamma-Poisson distribution'><p>MLE of the gamma-Poisson distribution</p></a></li>
<li><a href='#MLE of the left censored Poisson distribution'>
<p>MLE of the left censored Poisson distribution</p></a></li>
<li><a href='#MLE of the Purkayastha distribution'>
<p>MLE of the Purkayastha distribution</p></a></li>
<li><a href='#MLE of the zero inflated Gamma and Weibull distributions'><p>MLE of the zero inflated Gamma and Weibull distributions</p></a></li>
<li><a href='#Monte Carlo integration with a normal distribution'>
<p>Monte Carlo Integration with a normal distribution</p></a></li>
<li><a href='#Moran's I measure of spatial autocorrelation'>
<p>Moran's I measure of spatial autocorrelation</p></a></li>
<li><a href='#Multinomial regression'>
<p>Multinomial regression</p></a></li>
<li><a href='#Naive Bayes classifier for binary (Bernoulli) data'>
<p>Naive Bayes classifier for binary Bernoulli data</p></a></li>
<li><a href='#Naive Bayes classifiers'>
<p>Naive Bayes classifiers</p></a></li>
<li><a href='#Naive Bayes classifiers for circular data'>
<p>Naive Bayes classifiers for directional data</p></a></li>
<li><a href='#Negative binomial regression'>
<p>Negative binomial regression</p></a></li>
<li><a href='#Non linear least squares regression for percentages or proportions'>
<p>Non linear least squares regression for percentages or proportions</p></a></li>
<li><a href='#One sample bootstrap t-test for a vector'>
<p>One sample bootstrap t-test for a vector</p></a></li>
<li><a href='#Orthogonal matching pursuit variable selection'>
<p>Orthogonal matching variable selection</p></a></li>
<li><a href='#Parametric and non-parametric bootstrap for linear regression model'>
<p>Parametric and non-parametric bootstrap for linear regression model</p></a></li>
<li><a href='#Permutation t-test for 2 independent samples'>
<p>Permutation t-test for 2 independent samples</p></a></li>
<li><a href='#Prediction with naive Bayes classifier for binary (Bernoulli) data'>
<p>Prediction with naive Bayes classifier for binary (Bernoulli) data</p></a></li>
<li><a href='#Prediction with some naive Bayes classifiers'>
<p>Prediction with some naive Bayes classifiers</p></a></li>
<li><a href='#Prediction with some naive Bayes classifiers for circular data'>
<p>Prediction with some naive Bayes classifiers for circular data</p></a></li>
<li><a href='#Principal component analysis'>
<p>Principal component analysis</p></a></li>
<li><a href='#Principal components regression'>
<p>Principal components regression</p></a></li>
<li><a href='#Random effects and weighted least squares meta analysis'>
<p>Random effects and weighted least squares meta analysis</p></a></li>
<li><a href='#Random integer values simulation'>
<p>Random integer values simulation</p></a></li>
<li><a href='#Random values generation from a Be(a, 1) distribution'>
<p>Random values generation from a Be(a, 1) distribution</p></a></li>
<li><a href='#Random values simulation from the uniform distribution'>
<p>Random values simulation from the uniform distribution</p></a></li>
<li><a href='#Regularised maximum likelihood linear discriminant analysis'><p>Regularised maximum likelihood linear discriminant analysis</p></a></li>
<li><a href='#Repeated measures ANOVA (univariate data) using Hotelling's T2 test'>
<p>Repeated measures ANOVA (univariate data) using Hotelling's <code class="reqn">T^2</code> test</p></a></li>
<li><a href='#Rfast2-package'>
<p>Really fast R functions</p></a></li>
<li><a href='#Sample quantiles and col/row wise quantiles'>
<p>Sample quantiles and col/row wise quantiles</p></a></li>
<li><a href='#Scaled logistic regression'>
<p>Scaled logistic regression</p></a></li>
<li><a href='#Score test for overdispersion in Poisson regression'>
<p>Score test for overdispersion in Poisson regression</p></a></li>
<li><a href='#Single terms deletion hypothesis testing in a linear regression model'>
<p>Single terms deletion hypothesis testing in a linear regression model</p></a></li>
<li><a href='#Skeleton of the FEDHC algorithm'>
<p>The skeleton of a Bayesian network produced by the FEDHC algorithm</p></a></li>
<li><a href='#Skeleton of the MMHC algorithm'>
<p>The skeleton of a Bayesian network learned with the MMHC algorithm</p></a></li>
<li><a href='#The k-NN algorithm for really lage scale data'>
<p>The k-NN algorithm for really lage scale data</p></a></li>
<li><a href='#Tobit regression'>
<p>Tobit regression</p></a></li>
<li><a href='#Trimmed mean'><p>Trimmed mean</p></a></li>
<li><a href='#Variable selection using the PC-simple algorithm'><p>Variable selection using the PC-simple algorithm</p></a></li>
<li><a href='#Wald confidence interval for the ratio of two Poisson variables'>
<p>Wald confidence interval for the ratio of two Poisson variables</p></a></li>
<li><a href='#Walter's confidence interval for the ratio of two binomial variables (and the relative risk)'><p>Walter's confidence interval for the ratio of two binomial variables (and the relative risk)</p></a></li>
<li><a href='#Zero inflated Gamma regression'>
<p>Zero inflated Gamma regression</p></a></li>
<li><a href='#Zero truncated Poisson regression'>
<p>Zero truncated Poisson regression</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>A Collection of Efficient and Extremely Fast R Functions II</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.5.2</td>
</tr>
<tr>
<td>Date:</td>
<td>2024-03-10</td>
</tr>
<tr>
<td>Author:</td>
<td>Manos Papadakis, Michail Tsagris, Stefanos Fafalios, Marios Dimitriadis and Manos Lasithiotakis.</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Manos Papadakis &lt;rfastofficial@gmail.com&gt;</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0), Rcpp (&ge; 0.12.3), RcppParallel</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp (&ge; 0.12.3), RcppArmadillo, RcppParallel</td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>C++17</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rfast, Rnanoflann</td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/RfastOfficial/Rfast2/issues">https://github.com/RfastOfficial/Rfast2/issues</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/RfastOfficial/Rfast2">https://github.com/RfastOfficial/Rfast2</a></td>
</tr>
<tr>
<td>Description:</td>
<td>A collection of fast statistical and utility functions for data analysis. Functions for regression, maximum likelihood, column-wise statistics and many more have been included. C++ has been utilized to speed up the functions. References: Tsagris M., Papadakis M. (2018). Taking R to its limits: 70+ tips. PeerJ Preprints 6:e26605v1 &lt;<a href="https://doi.org/10.7287%2Fpeerj.preprints.26605v1">doi:10.7287/peerj.preprints.26605v1</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2.0)]</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-03-09 22:42:29 UTC; epapadakis</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-03-10 08:30:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='+20Benchmark+20-+20Measure+20time+20'>
Benchmark - Measure time
</h2><span id='topic+benchmark'></span><span id='topic+print.benchmark'></span>

<h3>Description</h3>

<p>Benchmark - Measure time.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>benchmark(...,times,envir=parent.frame(),order=NULL)
## S3 method for class 'benchmark'
print(x,...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="+2B20Benchmark+2B20-+2B20Measure+2B20time+2B20_+3A_...">...</code></td>
<td>

<p>Expressions to the benchmark function.
</p>
</td></tr>
<tr><td><code id="+2B20Benchmark+2B20-+2B20Measure+2B20time+2B20_+3A_x">x</code></td>
<td>

<p>Object of class &quot;benchmark&quot; to print.
</p>
</td></tr>
<tr><td><code id="+2B20Benchmark+2B20-+2B20Measure+2B20time+2B20_+3A_times">times</code></td>
<td>

<p>Number of time to measure execution time of the expression.
</p>
</td></tr>
<tr><td><code id="+2B20Benchmark+2B20-+2B20Measure+2B20time+2B20_+3A_envir">envir</code></td>
<td>

<p>Environment to evaluate the expressions.
</p>
</td></tr>
<tr><td><code id="+2B20Benchmark+2B20-+2B20Measure+2B20time+2B20_+3A_order">order</code></td>
<td>

<p>An integer vector to execute the epxressions with this order, otherwise the execution order is random.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For measuring time we have used C++'s new library &quot;chrono&quot;.
</p>


<h3>Value</h3>

<p>The execution time for each expression.
</p>


<h3>Author(s)</h3>

<p>Manos Papadakis
</p>
<p>R implementation and documentation: Manos Papadakis <a href="mailto:papadakm95@gmail.com">papadakm95@gmail.com</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Quantile">Quantile</a>,<a href="#topic+trim.mean">trim.mean</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
benchmark(x &lt;- matrix(runif(10*10),10,10),times=10)

</code></pre>

<hr>
<h2 id='+20Check+20if+20a+20matrix+20is+20Lower+20or+20Upper+20triangular+20'>
Check if a matrix is Lower or Upper triangular
</h2><span id='topic+is.lower.tri'></span><span id='topic+is.upper.tri'></span>

<h3>Description</h3>

<p>Lower/upper triangular matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.lower.tri(x, diag = FALSE)
is.upper.tri(x, diag = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="+2B20Check+2B20if+2B20a+2B20matrix+2B20is+2B20Lower+2B20or+2B20Upper+2B20triangular+2B20_+3A_x">x</code></td>
<td>

<p>A matrix with data.
</p>
</td></tr>
<tr><td><code id="+2B20Check+2B20if+2B20a+2B20matrix+2B20is+2B20Lower+2B20or+2B20Upper+2B20triangular+2B20_+3A_diag">diag</code></td>
<td>

<p>A logical value include the diagonal to the result.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Check if a matrix is lower or upper triangular. You can also include diagonal to the check.
</p>


<h3>Author(s)</h3>

<p>Manos Papadakis.
</p>
<p>R implementation and documentation: Manos Papadakis <a href="mailto:papadakm95@gmail.com">papadakm95@gmail.com</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Intersect">Intersect</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
x &lt;- matrix(runif(10*10),10,10)

is.lower.tri(x)
is.lower.tri(x,TRUE)


is.upper.tri(x)
is.upper.tri(x,TRUE)

</code></pre>

<hr>
<h2 id='+20Column-wise+20summary+20statistics+20with+20grouping+20variables+20'>
Column-wise summary statistics with grouping variables
</h2><span id='topic+colGroup'></span>

<h3>Description</h3>

<p>Column-wise summary statistics with grouping variables.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>colGroup(x,ina,method="sum",names=TRUE, std = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="+2B20Column-wise+2B20summary+2B20statistics+2B20with+2B20grouping+2B20variables+2B20_+3A_x">x</code></td>
<td>

<p>A matrix with data.
</p>
</td></tr>
<tr><td><code id="+2B20Column-wise+2B20summary+2B20statistics+2B20with+2B20grouping+2B20variables+2B20_+3A_ina">ina</code></td>
<td>

<p>A numerical vector specifying the groups. If you have numerical values, do not put zeros, but 1, 2, 3 and so on. 
<b>The numbers must be consecutive</b>, like 1,2,3,.. Do not put 1, 3, 4 as this will cause C++ to crash. 
</p>
</td></tr>
<tr><td><code id="+2B20Column-wise+2B20summary+2B20statistics+2B20with+2B20grouping+2B20variables+2B20_+3A_method">method</code></td>
<td>

<p>One of the: &quot;sum&quot;, &quot;min&quot;, &quot;max&quot;, &quot;median&quot;, &quot;var&quot;.
</p>
</td></tr>
<tr><td><code id="+2B20Column-wise+2B20summary+2B20statistics+2B20with+2B20grouping+2B20variables+2B20_+3A_names">names</code></td>
<td>

<p>Set the name of the result vector with the unique numbers of group variable.
</p>
</td></tr>
<tr><td><code id="+2B20Column-wise+2B20summary+2B20statistics+2B20with+2B20grouping+2B20variables+2B20_+3A_std">std</code></td>
<td>

<p>A boolean variable specyfying whether you want the variances (FALSE) or the standard deviations (TRUE) of each column. 
This is taken into account only when method = &quot;var&quot;.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Column wise of grouping variables. You can also include diagonal to the check.
</p>


<h3>Author(s)</h3>

<p>Manos Papadakis.
</p>
<p>R implementation and documentation: Manos Papadakis <a href="mailto:papadakm95@gmail.com">papadakm95@gmail.com</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Quantile">Quantile</a>, <a href="#topic+colQuantile">colQuantile</a>, <a href="#topic+rowQuantile">rowQuantile</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
x &lt;- matrix(runif(100 * 5), 100, 5)
id &lt;- sample(1:3, 100, TRUE)

all.equal( colGroup(x, id), rowsum(x, id) )

</code></pre>

<hr>
<h2 id='+20Merge+202+20sorted+20vectors+20in+201+20sorted+20vector+20'>
Merge 2 sorted vectors in 1 sorted vector
</h2><span id='topic+Merge'></span>

<h3>Description</h3>

<p>Merge 2 sorted vectors in 1 sorted vector.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Merge(x,y)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="+2B20Merge+2B202+2B20sorted+2B20vectors+2B20in+2B201+2B20sorted+2B20vector+2B20_+3A_x">x</code></td>
<td>

<p>A sorted vector with data.
</p>
</td></tr>
<tr><td><code id="+2B20Merge+2B202+2B20sorted+2B20vectors+2B20in+2B201+2B20sorted+2B20vector+2B20_+3A_y">y</code></td>
<td>

<p>A sorted vector with data.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A sorted vector of the 2 arguments.
</p>


<h3>Author(s)</h3>

<p>Manos Papadakis.
</p>
<p>R implementation and documentation: Manos Papadakis <a href="mailto:papadakm95@gmail.com">papadakm95@gmail.com</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+is.lower.tri">is.lower.tri</a>,<a href="#topic+is.upper.tri">is.upper.tri</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
x &lt;- 1:10
y &lt;- 1:20

Merge(x,y)

x &lt;- y &lt;- NULL
</code></pre>

<hr>
<h2 id='+20Split+20the+20matrix+20in+20lower+2C+20upper+20triangular+20and+20diagonal+20'>
Split the matrix in lower, upper triangular and diagonal
</h2><span id='topic+lud'></span>

<h3>Description</h3>

<p>Split the matrix in lower, upper triangular and diagonal.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lud(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="+2B20Split+2B20the+2B20matrix+2B20in+2B20lower+2B2C+2B20upper+2B20triangular+2B20and+2B20diagonal+2B20_+3A_x">x</code></td>
<td>

<p>A matrix with data.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with 3 fields:
</p>
<table>
<tr><td><code>lower</code></td>
<td>

<p>The lower triangular of argument &quot;x&quot;.
</p>
</td></tr>
<tr><td><code>upper</code></td>
<td>

<p>The upper triangular of argument &quot;x&quot;.
</p>
</td></tr>
<tr><td><code>diagonal</code></td>
<td>

<p>The diagonal elements.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Manos Papadakis.
</p>
<p>R implementation and documentation: Manos Papadakis <a href="mailto:papadakm95@gmail.com">papadakm95@gmail.com</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Intersect">Intersect</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
x &lt;- matrix(runif(10*10),10,10)

b&lt;-lud(x)

</code></pre>

<hr>
<h2 id='Add+20many+20single+20terms+20to+20a+20model'>
Add many single terms to a model
</h2><span id='topic+add.term'></span>

<h3>Description</h3>

<p>Add many single terms to a model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>add.term(y, xinc, xout, devi_0, type = "logistic", logged = FALSE,
tol = 1e-07, maxiters = 100, parallel = FALSE)    
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Add+2B20many+2B20single+2B20terms+2B20to+2B20a+2B20model_+3A_y">y</code></td>
<td>

<p>The response variable. It must be a numerical vector. 
</p>
</td></tr>
<tr><td><code id="Add+2B20many+2B20single+2B20terms+2B20to+2B20a+2B20model_+3A_xinc">xinc</code></td>
<td>

<p>The already included indendent variable(s). 
</p>
</td></tr>
<tr><td><code id="Add+2B20many+2B20single+2B20terms+2B20to+2B20a+2B20model_+3A_xout">xout</code></td>
<td>

<p>The independent variables whose conditional association with the response is to be calculated.
</p>
</td></tr>
<tr><td><code id="Add+2B20many+2B20single+2B20terms+2B20to+2B20a+2B20model_+3A_devi_0">devi_0</code></td>
<td>

<p>The deviance for Poisson, logistic, qpoisson, qlogistic and normlog regression or the log-likelihood for the 
Weibull, spml and multinomial regressions. See the example to understand better.
</p>
</td></tr>
<tr><td><code id="Add+2B20many+2B20single+2B20terms+2B20to+2B20a+2B20model_+3A_type">type</code></td>
<td>

<p>The type of regression, &quot;poisson&quot;, &quot;logistic&quot;, &quot;qpoisson&quot; (quasi Poisson), &quot;qlogistic&quot; (quasi logistic) 
&quot;normlog&quot; (Gaussian regression with log-link) &quot;weibull&quot;, &quot;spml&quot; and &quot;multinom&quot;.
</p>
</td></tr>
<tr><td><code id="Add+2B20many+2B20single+2B20terms+2B20to+2B20a+2B20model_+3A_logged">logged</code></td>
<td>

<p>Should the logarithm of the p-value be returned? TRUE or FALSE.
</p>
</td></tr>
<tr><td><code id="Add+2B20many+2B20single+2B20terms+2B20to+2B20a+2B20model_+3A_tol">tol</code></td>
<td>

<p>The tolerance value to terminate the Newton-Raphson algorithm when fitting the regression models.
</p>
</td></tr>
<tr><td><code id="Add+2B20many+2B20single+2B20terms+2B20to+2B20a+2B20model_+3A_maxiters">maxiters</code></td>
<td>

<p>The maximum number of iterations the Newton-Raphson algorithm will perform.
</p>
</td></tr>
<tr><td><code id="Add+2B20many+2B20single+2B20terms+2B20to+2B20a+2B20model_+3A_parallel">parallel</code></td>
<td>

<p>Should the computations take place in parallel? TRUE or FALSE.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function is similar to the built-in function add1. You have already fitted a regression model with some independent variables (xinc).
You then add each of the xout variables and test their significance.
</p>


<h3>Value</h3>

<p>A matrix with two columns. The test statistic and its associated (logged) p-value.
</p>


<h3>Author(s)</h3>

<p>Stefanos Fafalios.
</p>
<p>R implementation and documentation: Stefanos Fafalios &lt;stefanosfafalios@gmail.com&gt;. 
</p>


<h3>References</h3>

<p>McCullagh, Peter, and John A. Nelder. Generalized linear models.  CRC press, USA, 2nd edition, 1989.
</p>
<p>Presnell Brett, Morrison Scott P. and Littell Ramon C. (1998). Projected multivariate linear models for directional data. 
Journal of the American Statistical Association, 93(443): 1068-1077.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+bic.regs">bic.regs</a>, <a href="#topic+logiquant.regs">logiquant.regs</a>, <a href="#topic+sp.logiregs">sp.logiregs</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- matrix( rnorm(200 * 10), ncol = 10)
y &lt;- rpois(200, 10)
devi_0 &lt;- deviance( glm(y ~ x[, 1:2], poisson) )
a &lt;- add.term(y, xinc = x[,1:2], xout = x[, 3:10], devi_0 = devi_0, type= "poisson")

y &lt;- rbinom(200, 1, 0.5)
devi_0 &lt;- deviance( glm(y ~ x[, 1:2], binomial) )
a &lt;- add.term(y, xinc = x[,1:2], xout = x[, 3:10], devi_0 = devi_0, type= "logistic")


y &lt;- rbinom(200, 2, 0.5)
devi_0 &lt;- Rfast::multinom.reg(y, x[, 1:2])$loglik
a &lt;- add.term(y, xinc = x[,1:2], xout = x[, 3:10], devi_0 = devi_0, type= "multinom")

y &lt;- rgamma(200, 3, 1)
devi_0 &lt;- Rfast::weib.reg(y, x[, 1:2])$loglik
a &lt;- add.term(y, xinc = x[,1:2], xout = x[, 3:10], devi_0 = devi_0, type= "weibull")


</code></pre>

<hr>
<h2 id='Angular+20Gaussian+20random+20values+20simulation'>
Angular Gaussian random values simulation
</h2><span id='topic+riag'></span>

<h3>Description</h3>

<p>Angular Gaussian random values simulation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>riag(n, mu)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Angular+2B20Gaussian+2B20random+2B20values+2B20simulation_+3A_n">n</code></td>
<td>

<p>The sample size, a numerical value.
</p>
</td></tr>
<tr><td><code id="Angular+2B20Gaussian+2B20random+2B20values+2B20simulation_+3A_mu">mu</code></td>
<td>

<p>The mean vector in <code class="reqn">R^d</code>.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The algorithm uses univariate normal random values and with some mean.
The vectors are then scaled to have unit length. 
</p>


<h3>Value</h3>

<p>A matrix with the simulated data.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Mardia, K. V. and Jupp, P. E. (2000). Directional statistics. Chicester: John Wiley &amp; Sons.
</p>
<p>Paine P.J., Preston S.P., Tsagris M and Wood A.T.A. (2018). An Elliptically Symmetric Angular Gaussian Distribution. Statistics and Computing, 28(3):689&ndash;697.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+colspml.mle">colspml.mle</a>, <a href="#topic+circ.cor1">circ.cor1</a>, <a href="#topic+circ.cors1">circ.cors1</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- riag(20, rnorm(4, 3, 1))  
</code></pre>

<hr>
<h2 id='Anova+20for+20circular+20data'>
Analysis of variance for circular data
</h2><span id='topic+hcf.circaov'></span><span id='topic+lr.circaov'></span><span id='topic+embed.circaov'></span><span id='topic+het.circaov'></span>

<h3>Description</h3>

<p>Analysis of variance for circular data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hcf.circaov(u, ina)

lr.circaov(u, ina)

het.circaov(u, ina)

embed.circaov(u, ina)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Anova+2B20for+2B20circular+2B20data_+3A_u">u</code></td>
<td>

<p>A numeric vector containing the data that are expressed in rads.
</p>
</td></tr>
<tr><td><code id="Anova+2B20for+2B20circular+2B20data_+3A_ina">ina</code></td>
<td>

<p>A numerical or factor variable indicating the group of each value.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The high concentration (hcf.circaov), log-likelihood ratio (lr.circaov), embedding approach (embed.circaov) 
or the non equal concentration parameters approach (het.circaov) is used.
</p>


<h3>Value</h3>

<p>A vector including:
</p>
<table>
<tr><td><code>test</code></td>
<td>

<p>The value of the test statistic.
</p>
</td></tr>
<tr><td><code>p-value</code></td>
<td>

<p>The p-value of the test.
</p>
</td></tr>
<tr><td><code>kapa</code></td>
<td>

<p>The concentration parameter based on all the data. If the het.circaov is used this argument is not returned.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Mardia, K. V. and Jupp, P. E. (2000). Directional statistics. Chicester: John Wiley &amp; Sons.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+multivm.mle">multivm.mle</a>, <a href="#topic+vm.nb">vm.nb</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- rnorm(60, 2.3, 0.3)
ina &lt;- rep(1:3,each = 20)
hcf.circaov(x, ina)
lr.circaov(x, ina)
het.circaov(x, ina)
embed.circaov(x, ina)
</code></pre>

<hr>
<h2 id='Backward+20selection+20with+20the+20F+20test+20or+20the+20partial+20correlation+20coefficient'>
backward selection with the F test or the partial correlation coefficient
</h2><span id='topic+lm.bsreg'></span>

<h3>Description</h3>

<p>backward selection with the F test or the partial correlation coefficient.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lm.bsreg(y, x, alpha = 0.05, type = "F") 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Backward+2B20selection+2B20with+2B20the+2B20F+2B20test+2B20or+2B20the+2B20partial+2B20correlation+2B20coefficient_+3A_y">y</code></td>
<td>

<p>The dependent variable, a numerical vector with numbers. 
</p>
</td></tr>
<tr><td><code id="Backward+2B20selection+2B20with+2B20the+2B20F+2B20test+2B20or+2B20the+2B20partial+2B20correlation+2B20coefficient_+3A_x">x</code></td>
<td>

<p>A numerical matrix with the indendent variables. We add, internally, the first column of ones.
</p>
</td></tr>
<tr><td><code id="Backward+2B20selection+2B20with+2B20the+2B20F+2B20test+2B20or+2B20the+2B20partial+2B20correlation+2B20coefficient_+3A_alpha">alpha</code></td>
<td>

<p>If you want to perform the usual F (or t) test set this equal to &quot;F&quot;. 
For the test based on the partial correlation set this equal to &quot;cor&quot;.
</p>
</td></tr>
<tr><td><code id="Backward+2B20selection+2B20with+2B20the+2B20F+2B20test+2B20or+2B20the+2B20partial+2B20correlation+2B20coefficient_+3A_type">type</code></td>
<td>

<p>The type of backward selection to be used, &quot;F&quot; stands for F-test, where &quot;cor&quot; stands for partial correlation.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>It performs backward selection with the F test or the partial correlation coefficient. 
For the linear regression model, the Wald test is equivalent to the partial F test. 
So, instead of performing many regression models with single term deletions
we perform one regression model with all variables and compute their Wald test effectively. Note, that this 
is true, only if the design matrix &quot;x&quot; contains the vectors of ones and in our case this must be, strictly, 
the first column. The second option is to compute the p-value of the partial correlation. 
</p>


<h3>Value</h3>

<p>A matrix with two columns. The removed variables and their associated pvalue.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Hastie T., Tibshirani R. and Friedman J. (2008). The Elements of Statistical Learning (2nd Ed.), Springer. 
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+lm.drop1">lm.drop1</a>, <a href="#topic+mmpc2">mmpc2</a>, <a href="#topic+gee.reg">gee.reg</a>, <a href="#topic+pc.sel">pc.sel</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>y &lt;- rnorm(150)
x &lt;- as.matrix(iris[, 1:4])
a &lt;- lm(y ~., data.frame(x) )
lm.bsreg(y, x)
</code></pre>

<hr>
<h2 id='BIC+20of+20many+20simple+20univariate+20regressions'>
BIC of many simple univariate regressions.
</h2><span id='topic+bic.regs'></span>

<h3>Description</h3>

<p>BIC of many simple univariate regressions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bic.regs(y, x, family = "normal")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="BIC+2B20of+2B20many+2B20simple+2B20univariate+2B20regressions_+3A_y">y</code></td>
<td>

<p>The dependent variable, a numerical vector.
</p>
</td></tr>
<tr><td><code id="BIC+2B20of+2B20many+2B20simple+2B20univariate+2B20regressions_+3A_x">x</code></td>
<td>

<p>A matrix with the indendent variables.
</p>
</td></tr>
<tr><td><code id="BIC+2B20of+2B20many+2B20simple+2B20univariate+2B20regressions_+3A_family">family</code></td>
<td>

<p>The family of the regression models. &quot;normal&quot;, &quot;binomial&quot;, &quot;poisson&quot;,
&quot;multinomial&quot;, &quot;normlog&quot; (Gaussian regression with log link), &quot;spmpl&quot; (SPML regression)
or &quot;weibull&quot; for Weibull regression.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Many simple univariate regressions are fitted and the BIC of every model is computed.
</p>


<h3>Value</h3>

<p>A vector with the BIC of each regression model.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>See Also</h3>

<p><code> logistic_only, poisson_only </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>y &lt;- rbinom(100, 1, 0.6)
x &lt;- matrix( rnorm(100 * 50), ncol = 50 )
bic.regs(y, x, "binomial")
</code></pre>

<hr>
<h2 id='Binomial+20regression'>
Binomial regression
</h2><span id='topic+binom.reg'></span>

<h3>Description</h3>

<p>Binomial regression.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>binom.reg(y, ni, x, full = FALSE, tol = 1e-07, maxiters = 100)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Binomial+2B20regression_+3A_y">y</code></td>
<td>

<p>The dependent variable; a numerical vector with integer values, 0, 1, 2,... The successes.
</p>
</td></tr>
<tr><td><code id="Binomial+2B20regression_+3A_ni">ni</code></td>
<td>

<p>A vector with integer values, greater than or equal to y. The trials.
</p>
</td></tr>
<tr><td><code id="Binomial+2B20regression_+3A_x">x</code></td>
<td>

<p>A matrix with the data, where the rows denote the samples (and the two groups) and the columns are the variables. 
This can be a matrix or a data.frame (with factors).
</p>
</td></tr>
<tr><td><code id="Binomial+2B20regression_+3A_full">full</code></td>
<td>

<p>If this is FALSE, the coefficients and the deviance will be returned only. If this is TRUE, more information is returned.
</p>
</td></tr>
<tr><td><code id="Binomial+2B20regression_+3A_tol">tol</code></td>
<td>

<p>The tolerance value to terminate the Newton-Raphson algorithm.
</p>
</td></tr>
<tr><td><code id="Binomial+2B20regression_+3A_maxiters">maxiters</code></td>
<td>

<p>The max number of iterations that can take place in each regression.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The difference from logistic regression is that in the binomial regression the binomial distribution 
is used and not the Bernoulli.
</p>


<h3>Value</h3>

<p>When full is FALSE a list including:
</p>
<table>
<tr><td><code>be</code></td>
<td>

<p>The regression coefficients.
</p>
</td></tr>
<tr><td><code>devi</code></td>
<td>

<p>The deviance of the model.
</p>
</td></tr>
</table>
<p>When full is TRUE a list including:
</p>
<table>
<tr><td><code>info</code></td>
<td>

<p>The regression coefficients, their standard error, their Wald test statistic and their p-value.
</p>
</td></tr>
<tr><td><code>devi</code></td>
<td>

<p>The deviance.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>McCullagh Peter and John A. Nelder. Generalized linear models. CRC Press, USA, 2nd edition, 1989.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+negbin.reg">negbin.reg</a>, <a href="#topic+hp.reg">hp.reg</a>, <a href="#topic+ztp.reg">ztp.reg</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- matrix(rnorm(100 * 2), ncol = 2)
y &lt;- rbinom(100, 20, 0.5)   ## binary logistic regression
ni &lt;- rep(20, 100)
a &lt;- binom.reg(y, ni, x, full = TRUE) 
x &lt;- NULL
</code></pre>

<hr>
<h2 id='Bootstrap+20James+20and+20Hotelling+20test+20for+202+20independent+20sample+20mean+20vectors'>
Bootstrap James and Hotelling test for 2 independent sample mean vectors
</h2><span id='topic+boot.james'></span><span id='topic+boot.hotel2'></span>

<h3>Description</h3>

<p>Bootstrap James and Hotelling test for 2 independent sample mean vectors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>boot.james(y1, y2, R = 999)
boot.hotel2(y1, y2, R = 999)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Bootstrap+2B20James+2B20and+2B20Hotelling+2B20test+2B20for+2B202+2B20independent+2B20sample+2B20mean+2B20vectors_+3A_y1">y1</code></td>
<td>

<p>A numerical matrix with the data of the one sample.
</p>
</td></tr>
<tr><td><code id="Bootstrap+2B20James+2B20and+2B20Hotelling+2B20test+2B20for+2B202+2B20independent+2B20sample+2B20mean+2B20vectors_+3A_y2">y2</code></td>
<td>

<p>A numerical matrix with the data of the other sample.
</p>
</td></tr>
<tr><td><code id="Bootstrap+2B20James+2B20and+2B20Hotelling+2B20test+2B20for+2B202+2B20independent+2B20sample+2B20mean+2B20vectors_+3A_r">R</code></td>
<td>

<p>The number of bootstrap samples to use.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>We bootstrap the 2-samples James (does not assume equal covariance matrics) and Hotelling test 
(assumes equal covariance matrics). The difference is that the Hotelling test statistic assumes 
equaility of the covariance matrices, which if violated leads to inlfated type I errors. 
Bootstrap calibration though takes care of this issue. As for the bootstrap calibration, instead 
of sampling B times from each sample, we sample <code class="reqn">sqrt{B}</code> from each of them and then take all pairs. 
Each bootstrap sample is independent of each other, hence there is no violation of the theory 
(Chatzipantsiou et al., 2019). 
</p>


<h3>Value</h3>

<p>The bootstrap p-value. 
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>G.S. James (1954). Tests of Linear Hypothese in Univariate and Multivariate Analysis
when the Ratios of the Population Variances are Unknown. Biometrika, 41(1/2): 19-43
</p>
<p>Efron Bradley and Robert J. Tibshirani (1993). An introduction to the bootstrap. New York: Chapman &amp; Hall/CRC.
</p>
<p>Chatzipantsiou C., Dimitriadis M., Papadakis M. and Tsagris M. (2019). 
Extremely efficient permutation and bootstrap hypothesis tests using R. 
To appear in the Journal of Modern Applied Statistical Methods.
</p>
<p><a href="https://arxiv.org/ftp/arxiv/papers/1806/1806.10947.pdf">https://arxiv.org/ftp/arxiv/papers/1806/1806.10947.pdf</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+welch.tests">welch.tests</a>, <a href="#topic+trim.mean">trim.mean</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>boot.james( as.matrix(iris[1:25, 1:4]), as.matrix(iris[26:50, 1:4]) )
</code></pre>

<hr>
<h2 id='Bootstrap+20Student+27s+20t-test+20for+202+20independent+20samples'>
Bootstrap Student's t-test for 2 independent samples
</h2><span id='topic+boot.student2'></span>

<h3>Description</h3>

<p>Bootstrap Student's t-test for 2 independent samples.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>boot.student2(x, y, B = 999)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Bootstrap+2B20Student+2B27s+2B20t-test+2B20for+2B202+2B20independent+2B20samples_+3A_x">x</code></td>
<td>

<p>A numerical vector with the data.
</p>
</td></tr>
<tr><td><code id="Bootstrap+2B20Student+2B27s+2B20t-test+2B20for+2B202+2B20independent+2B20samples_+3A_y">y</code></td>
<td>

<p>A numerical vector with the data.
</p>
</td></tr>
<tr><td><code id="Bootstrap+2B20Student+2B27s+2B20t-test+2B20for+2B202+2B20independent+2B20samples_+3A_b">B</code></td>
<td>

<p>The number of bootstrap samples to use.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>We bootstrap Student's (Gosset's) t-test statistic and not the Welch t-test statistic. For the latter case 
see the &quot;boot.ttest2&quot; function in Rfast. The difference is that Gosset's test statistic assumes equaility of the 
variances, which if violated leads to inlfated type I errors. Bootstrap calibration though takes care of this issue.
As for the bootstrap calibration, instead of sampling B times from each sample, we sample <code class="reqn">\sqrt{B}</code> from each 
of them and then take all pairs. Each bootstrap sample is independent of each other, hence there is no violation 
of the theory (Chatzipantsiou et al., 2019). 
</p>


<h3>Value</h3>

<p>A vector with the test statistic and the bootstrap p-value. 
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Efron Bradley and Robert J. Tibshirani (1993). An introduction to the bootstrap. New York: Chapman &amp; Hall/CRC.
</p>
<p>Chatzipantsiou C., Dimitriadis M., Papadakis M. and Tsagris M. (2019). 
Extremely efficient permutation and bootstrap hypothesis tests using R. 
To appear in the Journal of Modern Applied Statistical Methods.
</p>
<p><a href="https://arxiv.org/ftp/arxiv/papers/1806/1806.10947.pdf">https://arxiv.org/ftp/arxiv/papers/1806/1806.10947.pdf</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+welch.tests">welch.tests</a>, <a href="#topic+trim.mean">trim.mean</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- rexp(40, 4)
y &lt;- rbeta(50, 2.5, 7.5)
t.test(x, y, var.equal = TRUE)
boot.student2(x, y, 9999)
</code></pre>

<hr>
<h2 id='Censored+20Weibull+20regression+20model'>
Censored Weibull regression model
</h2><span id='topic+censweib.reg'></span>

<h3>Description</h3>

<p>Censored Weibull regression model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>censweib.reg(y, x, di, tol = 1e-07, maxiters = 100)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Censored+2B20Weibull+2B20regression+2B20model_+3A_y">y</code></td>
<td>

<p>The dependent variable; a numerical vector with strictly positive data, i.e. greater than zero. 
</p>
</td></tr>
<tr><td><code id="Censored+2B20Weibull+2B20regression+2B20model_+3A_x">x</code></td>
<td>

<p>A matrix with the data, where the rows denote the samples (and the two groups) and the columns are the variables. 
This can be a matrix or a data.frame (with factors).
</p>
</td></tr>
<tr><td><code id="Censored+2B20Weibull+2B20regression+2B20model_+3A_di">di</code></td>
<td>

<p>A vector with 1s and 0s indicating the censored value. The value of 1 means uncesored value, 
whereas the value of 0 means censored value.
</p>
</td></tr>
<tr><td><code id="Censored+2B20Weibull+2B20regression+2B20model_+3A_tol">tol</code></td>
<td>

<p>The tolerance value to terminate the Newton-Raphson algorithm.
</p>
</td></tr>
<tr><td><code id="Censored+2B20Weibull+2B20regression+2B20model_+3A_maxiters">maxiters</code></td>
<td>

<p>The max number of iterations that can take place in each regression.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function is written in C++ and this is why it is very fast. No standard errors are returned as they are not 
corectly estimated. We focused on speed.
</p>


<h3>Value</h3>

<p>When full is FALSE a list including:
</p>
<table>
<tr><td><code>iters</code></td>
<td>

<p>The iterations required by the Newton-Raphson.
</p>
</td></tr>
<tr><td><code>loglik</code></td>
<td>

<p>The log-likelihood of the model.
</p>
</td></tr>
<tr><td><code>shape</code></td>
<td>

<p>The shape parameter of the Weibull regression.
</p>
</td></tr>
<tr><td><code>be</code></td>
<td>

<p>The regression coefficients. 
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris and Stefanos Fafalios.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a> and 
Stefanos Fafalios <a href="mailto:stefanosfafalios@gmail.com">stefanosfafalios@gmail.com</a>.
</p>


<h3>References</h3>

<p>McCullagh, Peter, and John A. Nelder. Generalized linear models. CRC press, USA, 2nd edition, 1989.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+censweibull.mle">censweibull.mle</a>, <a href="#topic+km">km</a>, <a href="#topic+gumbel.reg">gumbel.reg</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
x &lt;- matrix(rnorm(100 * 2), ncol = 2)
y &lt;- rexp(100, 1)
di &lt;- rbinom(100, 1, 0.8)
mod &lt;- censweib.reg(y, x, di)
x &lt;- NULL

## End(Not run)
</code></pre>

<hr>
<h2 id='Check+20whether+20a+20square+20matrix+20is+20skew-symmetric'>
Check whether a square matrix is skew-symmetric
</h2><span id='topic+is.skew.symmetric'></span>

<h3>Description</h3>

<p>Check whether a square matrix is skew-symmetric.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.skew.symmetric(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Check+2B20whether+2B20a+2B20square+2B20matrix+2B20is+2B20skew-symmetric_+3A_x">x</code></td>
<td>

<p>A square matrix with data. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Instead of going through the whole matrix, the function will stop if the first disagreement is met. 
</p>


<h3>Value</h3>

<p>A boolean value, TRUE of FALSE.
</p>


<h3>Author(s)</h3>

<p>Manos Papadakis.
</p>
<p>R implementation and documentation: Manos Papadakis <a href="mailto:papadakm95@gmail.com">papadakm95@gmail.com</a>.
</p>


<h3>See Also</h3>

<p><code>cholesky, cora, cova
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;-matrix( rnorm( 100 * 400), ncol = 400 )
s1 &lt;- cor(x)
is.skew.symmetric(s1)
x &lt;- x[1:100, ]
is.skew.symmetric(x)

x&lt;-s1&lt;-NULL
</code></pre>

<hr>
<h2 id='Circurlar+20correlations+20between+20two+20circular+20variables'>Circurlar correlations between two circular variables
</h2><span id='topic+circ.cor1'></span><span id='topic+circ.cors1'></span>

<h3>Description</h3>

<p>Circurlar correlations between two circular variables.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>circ.cor1(theta, phi, pvalue = FALSE)

circ.cors1(theta, phi, pvalue = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Circurlar+2B20correlations+2B20between+2B20two+2B20circular+2B20variables_+3A_theta">theta</code></td>
<td>

<p>The first cirular variable expressed in radians, not degrees.  
</p>
</td></tr>
<tr><td><code id="Circurlar+2B20correlations+2B20between+2B20two+2B20circular+2B20variables_+3A_phi">phi</code></td>
<td>

<p>The other cirular variable. In the case of &quot;circ.cors1&quot; this is a matrix with many 
circular variables. In either case, the values must be in radians, not degrees. 
</p>
</td></tr>
<tr><td><code id="Circurlar+2B20correlations+2B20between+2B20two+2B20circular+2B20variables_+3A_pvalue">pvalue</code></td>
<td>

<p>If you want the p-value of the zero correlation hypothesis testing set this to TRUE,
otherwise leave it FALSE.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Correlation for circular variables using the cosinus and sinus formula of Jammaladaka and SenGupta (1988).
</p>


<h3>Value</h3>

<p>If you set pvalue = TRUE, then for the &quot;circ.cor1&quot; a vector with two values, the 
correlation and its associated p-value, otherwise the correlation only. For the 
&quot;circ.cors1&quot;, either a vector with the correlations only or a matrix with two columns, 
the correlation and the p-values.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Jammalamadaka, R. S. and Sengupta, A. (2001). Topics in circular statistics. World Scientific.
</p>
<p>Jammalamadaka, S. R. and Sarma, Y. R. (1988) . A correlation coefficient for angular variables. Statistical Theory and Data Analysis, 2:349&ndash;364.
</p>


<h3>See Also</h3>

<p><code> spml.reg
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>y &lt;- runif(50, 0, 2 * pi)
x &lt;- runif(50, 0, 2 * pi)
circ.cor1(y, x, TRUE)
x &lt;- matrix(runif(50 * 10, 0, 2 * pi), ncol = 10)
circ.cors1(y, x, TRUE)
</code></pre>

<hr>
<h2 id='Cluster+20robust+20wild+20bootstrap+20for+20linear+20models'>
Cluster robust wild bootstrap for linear models
</h2><span id='topic+wild.boot'></span>

<h3>Description</h3>

<p>Cluster robust wild bootstrap for linear models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>wild.boot(y, x, cluster, ind = NULL, R = 999, parallel = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Cluster+2B20robust+2B20wild+2B20bootstrap+2B20for+2B20linear+2B20models_+3A_y">y</code></td>
<td>

<p>The dependent variable, a numerical vector with numbers.
</p>
</td></tr>
<tr><td><code id="Cluster+2B20robust+2B20wild+2B20bootstrap+2B20for+2B20linear+2B20models_+3A_x">x</code></td>
<td>

<p>A matrix or a data.frame with the indendent variables.
</p>
</td></tr>
<tr><td><code id="Cluster+2B20robust+2B20wild+2B20bootstrap+2B20for+2B20linear+2B20models_+3A_cluster">cluster</code></td>
<td>

<p>A vector indicating the clusters. 
</p>
</td></tr>
<tr><td><code id="Cluster+2B20robust+2B20wild+2B20bootstrap+2B20for+2B20linear+2B20models_+3A_ind">ind</code></td>
<td>

<p>A vector with the indices of the variables for which wild bootstrap p-values will be computed. If NULL (default value), 
the p-values are computed for each variable.
</p>
</td></tr>
<tr><td><code id="Cluster+2B20robust+2B20wild+2B20bootstrap+2B20for+2B20linear+2B20models_+3A_r">R</code></td>
<td>

<p>The number of bootstrap replicates to perform. 
</p>
</td></tr>
<tr><td><code id="Cluster+2B20robust+2B20wild+2B20bootstrap+2B20for+2B20linear+2B20models_+3A_parallel">parallel</code></td>
<td>

<p>Do you want the process to take place in parallel? If yes, then set this equal to TRUE.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A linear regression model for clustered data is fitted. For more information see Chapter 4.21 of Hansen (2019).
</p>


<h3>Value</h3>

<p>A matrix with 5 columns, the estimated coefficients of the linear model, their cluster robust standard error, 
their cluster robust test statistic, their cluster robust p-value, and their cluster robust wild bootstrap p-value.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris and Stefanos Fafalios.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a> and Stefanos Fafalios <a href="mailto:stefanosfafalios@gmail.com">stefanosfafalios@gmail.com</a>.
</p>


<h3>References</h3>

<p>Cameron A. Colin, Gelbach J.B., and Miller D.L. (2008). Bootstrap-Based Improvements for Inference with Clustered Errors. 
The Review of Economics and Statistics 90(3): 414-427.  
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+gee.reg">gee.reg</a>, <a href="#topic+cluster.lm">cluster.lm</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>y &lt;- rnorm(200)
id &lt;- sample(1:20, 200, replace = TRUE)
x &lt;- matrix( rnorm(200 * 3), ncol = 3 )
wild.boot(y, x, cluster = id)
</code></pre>

<hr>
<h2 id='Column+20and+20row-wise+20jackknife+20sample+20means'>
Column and row-wise jackknife sample means
</h2><span id='topic+coljack.means'></span><span id='topic+rowjack.means'></span>

<h3>Description</h3>

<p>Column and row-wise jackknife sample means.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>coljack.means(x)
rowjack.means(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Column+2B20and+2B20row-wise+2B20jackknife+2B20sample+2B20means_+3A_x">x</code></td>
<td>

<p>A numerical matrix with data.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>An efficient implementation of the jackknife mean is provided.
</p>


<h3>Value</h3>

<p>A vector with the jackknife sample means. 
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Efron Bradley and Robert J. Tibshirani (1993). An introduction to the bootstrap. New York: Chapman &amp; Hall/CRC.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+welch.tests">welch.tests</a>, <a href="#topic+trim.mean">trim.mean</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- as.matrix(iris[1:50, 1:4])
coljack.means(x)
</code></pre>

<hr>
<h2 id='Column-wise+20means+20and+20variances'>
Column-wise means and variances of a matrix
</h2><span id='topic+colmeansvars'></span>

<h3>Description</h3>

<p>Column-wise means and variances of a matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>colmeansvars(x, std = FALSE, parallel = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Column-wise+2B20means+2B20and+2B20variances_+3A_x">x</code></td>
<td>

<p>A matrix with the data.
</p>
</td></tr>
<tr><td><code id="Column-wise+2B20means+2B20and+2B20variances_+3A_std">std</code></td>
<td>

<p>A boolean variable specyfying whether you want the variances (FALSE) or the standard deviations 
(TRUE) of each column.
</p>
</td></tr>
<tr><td><code id="Column-wise+2B20means+2B20and+2B20variances_+3A_parallel">parallel</code></td>
<td>

<p>A boolean value for parallel version.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function cacluates the column-wise means and variances (or standard deviations).
</p>


<h3>Value</h3>

<p>A matrix with two rows. The first contains the means and the second contains the variances 
(or standard deviations).
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a> and Manos Papadakis <a href="mailto:papadakm95@gmail.com">papadakm95@gmail.com</a>.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+pooled.colVars">pooled.colVars</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>colmeansvars( as.matrix(iris[, 1:4]) )
</code></pre>

<hr>
<h2 id='Column-wise+20MLE+20of+20some+20univariate+20distributions'>
Column-wise MLE of some univariate distributions
</h2><span id='topic+collognorm.mle'></span><span id='topic+collogitnorm.mle'></span><span id='topic+colborel.mle'></span><span id='topic+colhalfnorm.mle'></span><span id='topic+colordinal.mle'></span><span id='topic+colcauchy.mle'></span><span id='topic+colbeta.mle'></span><span id='topic+colunitweibull.mle'></span><span id='topic+colpowerlaw.mle'></span><span id='topic+colsp.mle'></span><span id='topic+colhalfcauchy.mle'></span><span id='topic+colcensweibull.mle'></span><span id='topic+colcenspois.mle'></span>

<h3>Description</h3>

<p>Column-wise MLE of some univariate distributions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>collognorm.mle(x)
collogitnorm.mle(x)
colborel.mle(x)
colhalfnorm.mle(x)
colordinal.mle(x, link = "logit")
colcauchy.mle(x, tol = 1e-07, maxiters = 100, parallel = FALSE)
colbeta.mle(x, tol = 1e-07, maxiters = 100, parallel = FALSE)
colunitweibull.mle(x, tol = 1e-07, maxiters = 100, parallel = FALSE)
colpowerlaw.mle(x)
colsp.mle(x)
colhalfcauchy.mle(x, tol = 1e-07, parallel = FALSE, cores = 0)
colcensweibull.mle(x, di, tol = 1e-07, parallel = FALSE, cores = 0)
colcenspois.mle(x, tol = 1e-07, parallel = FALSE, cores = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Column-wise+2B20MLE+2B20of+2B20some+2B20univariate+2B20distributions_+3A_x">x</code></td>
<td>

<p>A numerical matrix with data. Each column refers to a different vector of observations of the same distribution. 
The values of for lognormal must be greater than zero, for the logitnormal, beta, unit Weibull and sp they must be 
numbers between 0 and 1, exluding 0 and 1, whereas for the Borel distribution the x must contain integer values 
greater than 1.  For the halfnormal and powerlaw the numbers must be strictly positive, while for the ordinal 
this can be a numerical matrix with values 1, 2, 3,..., not zeros. The censored Poisson (colcenspois.mle) requires 
discrete data (counts). 
</p>
</td></tr>
<tr><td><code id="Column-wise+2B20MLE+2B20of+2B20some+2B20univariate+2B20distributions_+3A_di">di</code></td>
<td>

<p>A vector of 0s (censored) and 1s (not censored) vales. 
</p>
</td></tr>
<tr><td><code id="Column-wise+2B20MLE+2B20of+2B20some+2B20univariate+2B20distributions_+3A_link">link</code></td>
<td>

<p>This can either be &quot;logit&quot; or &quot;probit&quot;. It is the link function to be used. 
</p>
</td></tr>
<tr><td><code id="Column-wise+2B20MLE+2B20of+2B20some+2B20univariate+2B20distributions_+3A_tol">tol</code></td>
<td>

<p>The tolerance value to terminate the Newton-Fisher algorithm.
</p>
</td></tr>
<tr><td><code id="Column-wise+2B20MLE+2B20of+2B20some+2B20univariate+2B20distributions_+3A_maxiters">maxiters</code></td>
<td>

<p>The maximum number of iterations to implement.
</p>
</td></tr>
<tr><td><code id="Column-wise+2B20MLE+2B20of+2B20some+2B20univariate+2B20distributions_+3A_parallel">parallel</code></td>
<td>

<p>Do you want to calculations to take place in parallel? The default value is FALSE
</p>
</td></tr>
<tr><td><code id="Column-wise+2B20MLE+2B20of+2B20some+2B20univariate+2B20distributions_+3A_cores">cores</code></td>
<td>

<p>In case you set parallel = TRUE, then you need to specify the number of cores.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For each column, the same distribution is fitted and its parameters and log-likelihood are computed.
</p>


<h3>Value</h3>

<p>A matrix with two or three columns. The first one or the first two contain the parameter(s) of the distribution
and the second or third column the relevant log-likelihood.
For the colordinal.mle() a list including:
</p>
<table>
<tr><td><code>param</code></td>
<td>

<p>A matrix with the intercepts (threshold coefficients) of the model applied to each column (or variable).
</p>
</td></tr>
<tr><td><code>loglik</code></td>
<td>

<p>The log-likelihood values.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris and Stefanos Fafalios.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a> and Stefanos Fafalios
<a href="mailto:stefanosfafalios@gmail.com">stefanosfafalios@gmail.com</a>.
</p>


<h3>References</h3>

<p>N.L. Johnson, S. Kotz and N. Balakrishnan (1994). Continuous Univariate Distributions, Volume 1 (2nd Edition).
</p>
<p>N.L. Johnson, S. Kotz and N. Balakrishnan (1970). Distributions in statistics: continuous univariate distributions,
Volume 2.
</p>
<p>Agresti A. (2002) Categorical Data. Second edition. Wiley. 
</p>
<p>J. Mazucheli A. F. B. Menezes L. B. Fernandes R. P. de Oliveira &amp; M. E. Ghitany (2020). 
The unit-Weibull distribution as an alternative to the Kumaraswamy distribution for the modeling of
quantiles conditional on covariates. Journal of Applied Statistics, 47(6): 954&ndash;974.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+censpois.mle">censpois.mle</a>, <a href="#topic+gammapois.mle">gammapois.mle</a>, <a href="#topic+powerlaw.mle">powerlaw.mle</a>, <a href="#topic+unitweibull.mle">unitweibull.mle</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- matrix( exp( rnorm(500 * 50) ), ncol = 50)
a &lt;- collognorm.mle(x)
x &lt;- NULL
</code></pre>

<hr>
<h2 id='Column-wise+20MLE+20of+20the+20angular+20Gaussian+20distribution'>
Column-wise MLE of the angular Gaussian distribution
</h2><span id='topic+colspml.mle'></span>

<h3>Description</h3>

<p>Column-wise MLE of the angular Gaussian distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>colspml.mle(x ,tol = 1e-07, maxiters = 100, parallel = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Column-wise+2B20MLE+2B20of+2B20the+2B20angular+2B20Gaussian+2B20distribution_+3A_x">x</code></td>
<td>

<p>A numerical matrix with data. Each column refers to a different vector of observations of the same distribution. 
The values of for Lognormal must be greater than zero, for the logitnormal they must by percentages, exluding 0 and 1, 
whereas for the Borel distribution the x must contain integer values greater than 1.
</p>
</td></tr>
<tr><td><code id="Column-wise+2B20MLE+2B20of+2B20the+2B20angular+2B20Gaussian+2B20distribution_+3A_tol">tol</code></td>
<td>

<p>The tolerance value to terminate the Newton-Raphson algorithm.
</p>
</td></tr>
<tr><td><code id="Column-wise+2B20MLE+2B20of+2B20the+2B20angular+2B20Gaussian+2B20distribution_+3A_maxiters">maxiters</code></td>
<td>

<p>The maximum number of iterations that can take place in each regression.
</p>
</td></tr>
<tr><td><code id="Column-wise+2B20MLE+2B20of+2B20the+2B20angular+2B20Gaussian+2B20distribution_+3A_parallel">parallel</code></td>
<td>

<p>Do you want this to be executed in parallel or not. The parallel takes place in C++, and the number of threads 
is defined by each system's availiable cores.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For each column, spml.mle function is applied that fits the angular Gaussian distribution estimates 
its parameters and computes the maximum log-likelihood.
</p>


<h3>Value</h3>

<p>A matrix with four columns. The first two are the mean vector, then the <code class="reqn">\gamma</code> parameter, and the fourth 
column contains maximum log-likelihood.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris and Stefanos Fafalios.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a> and 
Stefanos Fafalios <a href="mailto:stefanosfafalios@gmail.com">stefanosfafalios@gmail.com</a>.
</p>


<h3>References</h3>

<p>Presnell Brett, Morrison Scott P. and Littell Ramon C. (1998). Projected multivariate linear models for directional data. 
Journal of the American Statistical Association, 93(443): 1068-1077. 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+collognorm.mle">collognorm.mle</a>, <a href="#topic+gammapois.mle">gammapois.mle</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- matrix( runif(100 * 10), ncol = 10)
a &lt;- colspml.mle(x)
x &lt;- NULL
</code></pre>

<hr>
<h2 id='Column-wise+20pooled+20variances+20across+20groups'>
Column-wise pooled variances across groups
</h2><span id='topic+pooled.colVars'></span>

<h3>Description</h3>

<p>Column-wise pooled variances across groups.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pooled.colVars(x, ina, std = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Column-wise+2B20pooled+2B20variances+2B20across+2B20groups_+3A_x">x</code></td>
<td>

<p>A matrix with the data.
</p>
</td></tr>
<tr><td><code id="Column-wise+2B20pooled+2B20variances+2B20across+2B20groups_+3A_ina">ina</code></td>
<td>

<p>A numerical vector specifying the groups. If you have numerical values, do not put zeros, but 1, 2, 3 and so on. 
</p>
</td></tr>
<tr><td><code id="Column-wise+2B20pooled+2B20variances+2B20across+2B20groups_+3A_std">std</code></td>
<td>

<p>A boolean variable specyfying whether you want the variances (FALSE) or the standard deviations (TRUE) of each column.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function cacluates the pooled variance (or standard deviation) for a range of groups for each column.
</p>


<h3>Value</h3>

<p>A vector with the pooled column variances or standard deviations.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a> and Manos Papadakis <a href="mailto:papadakm95@gmail.com">papadakm95@gmail.com</a>.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+colmeansvars">colmeansvars</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>pooled.colVars( as.matrix(iris[, 1:4]), as.numeric(iris[, 5]) )
</code></pre>

<hr>
<h2 id='Column-wise+20weighted+20least+20squares+20meta+20analysis'>
Column-wise weighted least squares meta analysis
</h2><span id='topic+colwlsmeta'></span>

<h3>Description</h3>

<p>Column-wise weighted least squares meta analysis.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>colwlsmeta(yi, vi) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Column-wise+2B20weighted+2B20least+2B20squares+2B20meta+2B20analysis_+3A_yi">yi</code></td>
<td>

<p>A matrix with the observations.
</p>
</td></tr>
<tr><td><code id="Column-wise+2B20weighted+2B20least+2B20squares+2B20meta+2B20analysis_+3A_vi">vi</code></td>
<td>

<p>A matrix with the variances of the observations.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The weighted least squares (WLS) meta analysis is performed in a column-wise fashion.
This function is suitable for simulation studies, where one can perform 
multiple WLS meta analyses at once. See references for this.
</p>


<h3>Value</h3>

<p>A vector with many elements. The fixed effects mean estimate, the <code class="reqn">\bar{v}</code> 
estimate, the <code class="reqn">I^2</code>, the <code class="reqn">H^2</code>, the Q test statistic and it's p-value,
the <code class="reqn">\tau^2</code> estimate and the random effects mean estimate.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Stanley T. D. and Doucouliagos H. (2015). 
Neither fixed nor random: weighted least squares meta-analysis. 
Statistics in Medicine, 34(13), 2116-2127.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+bic.regs">bic.regs</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>y &lt;- matrix( rnorm(50* 5), ncol = 5)
vi &lt;- matrix( rexp(50* 5), ncol = 5)
colwlsmeta(y, vi)
wlsmeta(y[, 1], vi[, 1])
</code></pre>

<hr>
<h2 id='Conditional+20least-squares+20estimate+20for+20Poisson+20INAR+281+29+20models'>
Conditional least-squares estimate for Poisson INAR(1) models
</h2><span id='topic+pinar1'></span><span id='topic+colpinar1'></span>

<h3>Description</h3>

<p>Conditional least-squares estimate for Poisson INAR(1) models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pinar1(x, unbiased = FALSE)
colpinar1(x, unbiased = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Conditional+2B20least-squares+2B20estimate+2B20for+2B20Poisson+2B20INAR+2B281+2B29+2B20models_+3A_x">x</code></td>
<td>

<p>Either a numerical vector or a matrix, depending on the function.
</p>
</td></tr>
<tr><td><code id="Conditional+2B20least-squares+2B20estimate+2B20for+2B20Poisson+2B20INAR+2B281+2B29+2B20models_+3A_unbiased">unbiased</code></td>
<td>

<p>If you want the unbiased estimation select TRUE.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function computes the constant and slope coefficients of the Poisson Integer 
Autoregressive of order 1 (Poisson INAR(1)) model using the conditional 
least-squares method.
</p>


<h3>Value</h3>

<p>For pinar1() a vector with two values, the <code class="reqn">\lambda</code> coefficient (constant) 
and the <code class="reqn">\alpha</code> coefficient (slope). See references for more information.
</p>
<p>For the colpinar1() a matrix with two columns, the <code class="reqn">\lambda</code> coefficient 
(constant) and the <code class="reqn">\alpha</code> coefficient (slope) for each variable (column of x).
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>M. Bourguignon and K.L.P. Vasconcellos (2015). 
Improved estimation for Poisson INAR(1) models. 
Journal of Statistical Computation and Simulation, 85(12): 2425-2441
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+fipois.reg">fipois.reg</a>, <a href="#topic+hp.reg">hp.reg</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- rpois(200, 10)
pinar1(x)
</code></pre>

<hr>
<h2 id='Constrained+20least+20squares'>
Constrained least squares
</h2><span id='topic+cls'></span>

<h3>Description</h3>

<p>Constrained least squares.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cls(y, x, R, ca) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Constrained+2B20least+2B20squares_+3A_y">y</code></td>
<td>

<p>The response variables, a numerical vector with observations.
</p>
</td></tr>
<tr><td><code id="Constrained+2B20least+2B20squares_+3A_x">x</code></td>
<td>

<p>A matrix with independent variables, the design matrix.
</p>
</td></tr>
<tr><td><code id="Constrained+2B20least+2B20squares_+3A_r">R</code></td>
<td>

<p>The R vector that contains the values that will multiply the beta coefficients. See details and examples.
</p>
</td></tr>
<tr><td><code id="Constrained+2B20least+2B20squares_+3A_ca">ca</code></td>
<td>

<p>The value of the constraint, <code class="reqn">R^T \beta = c</code>. See details and examples.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is described in Chapter 8.2 of Hansen (2019). The idea is to inimise the sum of squares of the residuals under the constraint <code class="reqn">R^T \beta = c</code>. 
As mentioned above, be careful with the input you give in the x matrix and the R vector. 
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>bols</code></td>
<td>

<p>The OLS (Ordinary Least Squares) beta coefficients.
</p>
</td></tr>
<tr><td><code>bcls</code></td>
<td>

<p>The CLS (Constrained Least Squares) beta coefficients.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Hansen, B. E. (2022). Econometrics, Princeton University Press. 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+gee.reg">gee.reg</a>, <a href="#topic+bic.regs">bic.regs</a>, <a href="#topic+ztp.reg">ztp.reg</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- as.matrix( iris[1:50, 1:4] )
y &lt;- rnorm(50)
R &lt;- c(1, 1, 1, 1)
cls(y, x, R, 1)
</code></pre>

<hr>
<h2 id='Contour+20plots+20of+20some+20bivariate+20distributions'>
Contour plots of some bivariate distributions
</h2><span id='topic+den.contours'></span>

<h3>Description</h3>

<p>Contour plots of some bivariate distributions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>den.contours(x, type = "normal", v = 5) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Contour+2B20plots+2B20of+2B20some+2B20bivariate+2B20distributions_+3A_x">x</code></td>
<td>

<p>A matrix with two columns containing the data.
</p>
</td></tr>
<tr><td><code id="Contour+2B20plots+2B20of+2B20some+2B20bivariate+2B20distributions_+3A_type">type</code></td>
<td>

<p>The distribution whose contours will appear. This can be &quot;normal&quot;, &quot;t&quot; or &quot;mlnorm&quot;, standing for the bivariate normal, t and bivariate log-normal.
</p>
</td></tr>
<tr><td><code id="Contour+2B20plots+2B20of+2B20some+2B20bivariate+2B20distributions_+3A_v">v</code></td>
<td>

<p>The degrees of freedom of the bivariate t distribtuion.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The contour plot.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+collognorm.mle">collognorm.mle</a>, <a href="#topic+halfcauchy.mle">halfcauchy.mle</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
x &lt;- as.matrix(iris[, 1:2])
den.contours(x)

</code></pre>

<hr>
<h2 id='Correlation+20significance+20testing+20using+20Fisher+27s+20z-transformation'>
Correlation significance testing using Fisher's z-transformation
</h2><span id='topic+cor_test'></span>

<h3>Description</h3>

<p>Correlation significance testing using Fisher's z-transformation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cor_test(y, x, type = "pearson", rho = 0, a = 0.05 )
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Correlation+2B20significance+2B20testing+2B20using+2B20Fisher+2B27s+2B20z-transformation_+3A_y">y</code></td>
<td>

<p>A numerical vector.
</p>
</td></tr>
<tr><td><code id="Correlation+2B20significance+2B20testing+2B20using+2B20Fisher+2B27s+2B20z-transformation_+3A_x">x</code></td>
<td>

<p>A numerical vector.
</p>
</td></tr>
<tr><td><code id="Correlation+2B20significance+2B20testing+2B20using+2B20Fisher+2B27s+2B20z-transformation_+3A_type">type</code></td>
<td>

<p>The type of correlation you want. &quot;pearson&quot; and &quot;spearman&quot; are the two supported types 
because their standard error is easily calculated. 
</p>
</td></tr>
<tr><td><code id="Correlation+2B20significance+2B20testing+2B20using+2B20Fisher+2B27s+2B20z-transformation_+3A_rho">rho</code></td>
<td>

<p>The value of the hypothesised correlation to be used in the hypothesis testing.
</p>
</td></tr>
<tr><td><code id="Correlation+2B20significance+2B20testing+2B20using+2B20Fisher+2B27s+2B20z-transformation_+3A_a">a</code></td>
<td>

<p>The significance level used for the confidence intervals.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function uses the built-in function &quot;cor&quot; which is very fast, then computes a confidence interval 
and produces a p-value for the hypothesis test. 
</p>


<h3>Value</h3>

<p>A vector with 5 numbers; the correlation, the p-value for the hypothesis test that each of them is 
equal to &quot;rho&quot;, the test statistic and the <code class="reqn">a/2\%</code> lower and upper confidence limits.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>See Also</h3>

<p><code>allbetas, univglms
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- rcauchy(60)
y &lt;- rnorm(60)
cor_test(y, x)  
</code></pre>

<hr>
<h2 id='Covariance+20between+20a+20variable+20and+20a+20set+20of+20variables'>Covariance between a variable and a set of variables</h2><span id='topic+covar'></span>

<h3>Description</h3>

<p>Covariance between a variable and a set of variables.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>covar(y, x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Covariance+2B20between+2B20a+2B20variable+2B20and+2B20a+2B20set+2B20of+2B20variables_+3A_y">y</code></td>
<td>

<p>A numerical vector.  
</p>
</td></tr>
<tr><td><code id="Covariance+2B20between+2B20a+2B20variable+2B20and+2B20a+2B20set+2B20of+2B20variables_+3A_x">x</code></td>
<td>

<p>A numerical matrix.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function calculates the covariance between a variable and many others. 
</p>


<h3>Value</h3>

<p>A vector with the covariances.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris and Manos Papadakis.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a> and Manos Papadakis <a href="mailto:papadakm95@gmail.com">papadakm95@gmail.com</a>.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+circ.cors1">circ.cors1</a>, <a href="#topic+bic.regs">bic.regs</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>y &lt;- rnorm(40)
x &lt;- matrix( rnorm(40 * 10), ncol = 10 )
covar(y, x)
cov(y, x)
</code></pre>

<hr>
<h2 id='Cross-validation+20for+20the+20k-NN+20algorithm+20for+20really+20lage+20scale+20data'>
Cross-validation for the k-NN algorithm for really lage scale data
</h2><span id='topic+bigknn.cv'></span>

<h3>Description</h3>

<p>Cross-validation for the k-NN algorithm for really lage scale data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bigknn.cv(y, x, k = 5:10, type = "C", folds = NULL, nfolds = 10,
stratified = TRUE, seed = FALSE, pred.ret = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Cross-validation+2B20for+2B20the+2B20k-NN+2B20algorithm+2B20for+2B20really+2B20lage+2B20scale+2B20data_+3A_y">y</code></td>
<td>

<p>A vector of data. The response variable, which can be either continuous or 
categorical (factor is acceptable).  
</p>
</td></tr>
<tr><td><code id="Cross-validation+2B20for+2B20the+2B20k-NN+2B20algorithm+2B20for+2B20really+2B20lage+2B20scale+2B20data_+3A_x">x</code></td>
<td>

<p>A matrix with the available data, the predictor variables. 
</p>
</td></tr>
<tr><td><code id="Cross-validation+2B20for+2B20the+2B20k-NN+2B20algorithm+2B20for+2B20really+2B20lage+2B20scale+2B20data_+3A_k">k</code></td>
<td>

<p>A vector with the possible numbers of nearest neighbours to be considered.
</p>
</td></tr>
<tr><td><code id="Cross-validation+2B20for+2B20the+2B20k-NN+2B20algorithm+2B20for+2B20really+2B20lage+2B20scale+2B20data_+3A_type">type</code></td>
<td>

<p>If your response variable y is numerical data, then this should be &quot;R&quot; (regression). 
If y is in general categorical set this argument to &quot;C&quot; (classification). 
</p>
</td></tr>
<tr><td><code id="Cross-validation+2B20for+2B20the+2B20k-NN+2B20algorithm+2B20for+2B20really+2B20lage+2B20scale+2B20data_+3A_folds">folds</code></td>
<td>

<p>A list with the indices of the folds.
</p>
</td></tr>
<tr><td><code id="Cross-validation+2B20for+2B20the+2B20k-NN+2B20algorithm+2B20for+2B20really+2B20lage+2B20scale+2B20data_+3A_nfolds">nfolds</code></td>
<td>

<p>The number of folds to be used. This is taken into consideration only if &quot;folds&quot; is NULL.
</p>
</td></tr>
<tr><td><code id="Cross-validation+2B20for+2B20the+2B20k-NN+2B20algorithm+2B20for+2B20really+2B20lage+2B20scale+2B20data_+3A_stratified">stratified</code></td>
<td>

<p>Do you want the folds to be selected using stratified random sampling? This preserves the analogy of the samples of each group. 
Make this TRUE if you wish, but only for the classification. If you have regression (type = &quot;R&quot;), do not put this to TRUE as 
it will cause problems or return wrong results.
</p>
</td></tr>
<tr><td><code id="Cross-validation+2B20for+2B20the+2B20k-NN+2B20algorithm+2B20for+2B20really+2B20lage+2B20scale+2B20data_+3A_seed">seed</code></td>
<td>

<p>If you set this to TRUE, the same folds will be created every time. 
</p>
</td></tr>
<tr><td><code id="Cross-validation+2B20for+2B20the+2B20k-NN+2B20algorithm+2B20for+2B20really+2B20lage+2B20scale+2B20data_+3A_pred.ret">pred.ret</code></td>
<td>

<p>If you want the predicted values returned set this to TRUE.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The concept behind k-NN is simple. Suppose we have a matrix with predictor variables and a vector with the 
response variable (numerical or categorical). When a new vector with observations (predictor variables) is 
available, its corresponding response value, numerical or categorical, is to be predicted. Instead of using a 
model, parametric or not, one can use this ad hoc algorithm. 
</p>
<p>The k smallest distances between the new predictor variables and the existing ones are calculated. In the 
case of regression, the average, median, or harmonic mean of the corresponding response values of these closest
predictor values are calculated. In the case of classification, i.e. categorical response value, a voting rule 
is applied. The most frequent group (response value) is where the new observation is to be allocated. 
</p>
<p>This function does the cross-validation procedure to select the optimal k, the optimal number of nearest neighbours. 
The optimal in terms of some accuracy metric. For the classification it is the percentage of correct classification 
and for the regression the mean squared error.
</p>
<p>This function allows for the Euclidean distance only.
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>preds</code></td>
<td>

<p>If pred.ret is TRUE the predicted values for each fold are returned as elements in a list.
</p>
</td></tr>
<tr><td><code>crit</code></td>
<td>

<p>A vector whose length is equal to the number of k and is the accuracy metric for each k. 
For the classification case it is the percentage of correct classification. For the regression
case the mean square of prediction error. If you want to compute other metrics of accuracy we suggest
you choose &quot;pred.ret = TRUE&quot; when running the function and then write a simple function to compute 
more metrics. See <code><a href="#topic+colmses">colmses</a></code>.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Friedman J., Hastie T. and Tibshirani R. (2017). The elements of statistical learning. 
New York: Springer.
</p>
<p>Cover TM and Hart PE (1967). Nearest neighbor pattern classification. IEEE Transactions on 
Information Theory. 13(1):21-27.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+big.knn">big.knn</a>, <a href="#topic+regmlelda.cv">regmlelda.cv</a>, <a href="#topic+multinomreg.cv">multinomreg.cv</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- as.matrix(iris[, 1:4])
mod &lt;- bigknn.cv(y = iris[, 5], x = x, k = c(3, 4) )
</code></pre>

<hr>
<h2 id='Cross-validation+20for+20the+20multinomial+20regression'>
Cross-validation for the multinomial regression
</h2><span id='topic+multinomreg.cv'></span>

<h3>Description</h3>

<p>Cross-validation for the multinomial regression.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>multinomreg.cv(y, x, folds = NULL, nfolds = 10, stratified = TRUE, 
               seed = FALSE, pred.ret = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Cross-validation+2B20for+2B20the+2B20multinomial+2B20regression_+3A_y">y</code></td>
<td>

<p>The response variable. A numerical or a factor type vector. 
</p>
</td></tr>
<tr><td><code id="Cross-validation+2B20for+2B20the+2B20multinomial+2B20regression_+3A_x">x</code></td>
<td>

<p>A matrix or a data.frame with the predictor variables. 
</p>
</td></tr>
<tr><td><code id="Cross-validation+2B20for+2B20the+2B20multinomial+2B20regression_+3A_folds">folds</code></td>
<td>

<p>A list with the indices of the folds.
</p>
</td></tr>
<tr><td><code id="Cross-validation+2B20for+2B20the+2B20multinomial+2B20regression_+3A_nfolds">nfolds</code></td>
<td>

<p>The number of folds to be used. This is taken into consideration only if &quot;folds&quot; is NULL.
</p>
</td></tr>
<tr><td><code id="Cross-validation+2B20for+2B20the+2B20multinomial+2B20regression_+3A_stratified">stratified</code></td>
<td>

<p>Do you want the folds to be selected using stratified random sampling? This preserves the analogy of the samples of each group. 
Make this TRUE if you wish, but only for the classification. If you have regression (type = &quot;R&quot;), do not put this to TRUE as 
it will cause problems or return wrong results.
</p>
</td></tr>
<tr><td><code id="Cross-validation+2B20for+2B20the+2B20multinomial+2B20regression_+3A_seed">seed</code></td>
<td>

<p>If you set this to TRUE, the same folds will be created every time. 
</p>
</td></tr>
<tr><td><code id="Cross-validation+2B20for+2B20the+2B20multinomial+2B20regression_+3A_pred.ret">pred.ret</code></td>
<td>

<p>If you want the predicted values returned set this to TRUE.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>preds</code></td>
<td>

<p>If pred.ret is TRUE the predicted values for each fold are returned as elements in a list.
</p>
</td></tr>
<tr><td><code>crit</code></td>
<td>

<p>A vector whose length is equal to the number of k and is the accuracy metric for each k. 
For the classification case it is the percentage of correct classification. 
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Friedman J., Hastie T. and Tibshirani R. (2017). The elements of statistical learning. 
New York: Springer.
</p>
<p>Bohning, D. (1992). Multinomial logistic regression algorithm. 
Annals of the Institute of Statistical Mathematics, 44(1): 197-200. 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+bigknn.cv">bigknn.cv</a>, <a href="#topic+mle.lda">mle.lda</a>, <a href="#topic+reg.mle.lda">reg.mle.lda</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- as.matrix(iris[, 1:2])
mod &lt;- multinomreg.cv(iris[, 5], x)
</code></pre>

<hr>
<h2 id='Cross-validation+20for+20the+20naive+20Bayes+20classifiers'>
Cross-validation for the naive Bayes classifiers
</h2><span id='topic+nb.cv'></span>

<h3>Description</h3>

<p>Cross-validation for the naive Bayes classifiers.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nb.cv(x, ina, type = "gaussian", folds = NULL, nfolds = 10, 
      stratified = TRUE, seed = FALSE, pred.ret = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Cross-validation+2B20for+2B20the+2B20naive+2B20Bayes+2B20classifiers_+3A_x">x</code></td>
<td>

<p>A matrix with the available data, the predictor variables. 
</p>
</td></tr>
<tr><td><code id="Cross-validation+2B20for+2B20the+2B20naive+2B20Bayes+2B20classifiers_+3A_ina">ina</code></td>
<td>

<p>A vector of data. The response variable, which is categorical (factor is acceptable).  
</p>
</td></tr>
<tr><td><code id="Cross-validation+2B20for+2B20the+2B20naive+2B20Bayes+2B20classifiers_+3A_type">type</code></td>
<td>

<p>The type of naive Bayes, &quot;gaussian&quot;, &quot;gamma&quot;, &quot;weibull&quot;, &quot;normlog&quot;, &quot;laplace&quot;, &quot;cauchy&quot;, 
&quot;logitnorm&quot;, &quot;beta&quot;, &quot;vm&quot; or &quot;spml&quot;, &quot;poisson&quot;, &quot;multinom&quot;, &quot;geom&quot; or &quot;bernoulli&quot;. 
</p>
</td></tr>
<tr><td><code id="Cross-validation+2B20for+2B20the+2B20naive+2B20Bayes+2B20classifiers_+3A_folds">folds</code></td>
<td>

<p>A list with the indices of the folds.
</p>
</td></tr>
<tr><td><code id="Cross-validation+2B20for+2B20the+2B20naive+2B20Bayes+2B20classifiers_+3A_nfolds">nfolds</code></td>
<td>

<p>The number of folds to be used. This is taken into consideration only if &quot;folds&quot; is NULL.
</p>
</td></tr>
<tr><td><code id="Cross-validation+2B20for+2B20the+2B20naive+2B20Bayes+2B20classifiers_+3A_stratified">stratified</code></td>
<td>

<p>Do you want the folds to be selected using stratified random sampling? This preserves the 
analogy of the samples of each group. 
Make this TRUE if you wish. 
</p>
</td></tr>
<tr><td><code id="Cross-validation+2B20for+2B20the+2B20naive+2B20Bayes+2B20classifiers_+3A_seed">seed</code></td>
<td>

<p>If you set this to TRUE, the same folds will be created every time. 
</p>
</td></tr>
<tr><td><code id="Cross-validation+2B20for+2B20the+2B20naive+2B20Bayes+2B20classifiers_+3A_pred.ret">pred.ret</code></td>
<td>

<p>If you want the predicted values returned set this to TRUE.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>preds</code></td>
<td>

<p>If pred.ret is TRUE the predicted values for each fold are returned as elements in a list.
</p>
</td></tr>
<tr><td><code>crit</code></td>
<td>

<p>A vector whose length is equal to the number of k and is the accuracy metric for each k. 
For the classification case it is the percentage of correct classification. 
If you want to compute other metrics of accuracy we suggest you choose &quot;pred.ret = TRUE&quot; 
when running the function and then write a simple function to compute 
more metrics. See .
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Friedman J., Hastie T. and Tibshirani R. (2017). The elements of statistical learning. 
New York: Springer.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+weibullnb.pred">weibullnb.pred</a>, <a href="#topic+weibull.nb">weibull.nb</a>, <a href="#topic+vm.nb">vm.nb</a>, <a href="#topic+vmnb.pred">vmnb.pred</a>, <a href="#topic+mle.lda">mle.lda</a>, 
<a href="#topic+reg.mle.lda">reg.mle.lda</a>, <a href="#topic+multinom.reg">multinom.reg</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- as.matrix(iris[, 1:4])
mod &lt;- nb.cv(ina = iris[, 5], x = x )
</code></pre>

<hr>
<h2 id='Cross-validation+20for+20the+20regularised+20maximum+20likelihood+20linear+20discriminant+20analysis'>
Cross-validation for the regularised maximum likelihood linear discriminant analysis
</h2><span id='topic+regmlelda.cv'></span>

<h3>Description</h3>

<p>Cross-validation for the regularised maximum likelihood linear discriminant analysis.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>regmlelda.cv(x, ina, lambda = seq(0, 1, by = 0.1), folds = NULL, nfolds = 10, 
             stratified = TRUE, seed = FALSE, pred.ret = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Cross-validation+2B20for+2B20the+2B20regularised+2B20maximum+2B20likelihood+2B20linear+2B20discriminant+2B20analysis_+3A_x">x</code></td>
<td>

<p>A matrix with numerical data.
</p>
</td></tr>
<tr><td><code id="Cross-validation+2B20for+2B20the+2B20regularised+2B20maximum+2B20likelihood+2B20linear+2B20discriminant+2B20analysis_+3A_ina">ina</code></td>
<td>

<p>A numerical vector or factor with consecutive numbers indicating the group to which each
observation belongs to.
</p>
</td></tr>
<tr><td><code id="Cross-validation+2B20for+2B20the+2B20regularised+2B20maximum+2B20likelihood+2B20linear+2B20discriminant+2B20analysis_+3A_lambda">lambda</code></td>
<td>

<p>A vector of regularization values <code class="reqn">\lambda</code> such as (0, 0.1, 0.2,...). 
</p>
</td></tr>
<tr><td><code id="Cross-validation+2B20for+2B20the+2B20regularised+2B20maximum+2B20likelihood+2B20linear+2B20discriminant+2B20analysis_+3A_folds">folds</code></td>
<td>

<p>A list with the indices of the folds.
</p>
</td></tr>
<tr><td><code id="Cross-validation+2B20for+2B20the+2B20regularised+2B20maximum+2B20likelihood+2B20linear+2B20discriminant+2B20analysis_+3A_nfolds">nfolds</code></td>
<td>

<p>The number of folds to be used. This is taken into consideration only if &quot;folds&quot; is NULL.
</p>
</td></tr>
<tr><td><code id="Cross-validation+2B20for+2B20the+2B20regularised+2B20maximum+2B20likelihood+2B20linear+2B20discriminant+2B20analysis_+3A_stratified">stratified</code></td>
<td>

<p>Do you want the folds to be selected using stratified random sampling? This preserves the analogy of the samples of each group. 
Make this TRUE if you wish, but only for the classification. If you have regression (type = &quot;R&quot;), do not put this to TRUE as 
it will cause problems or return wrong results.
</p>
</td></tr>
<tr><td><code id="Cross-validation+2B20for+2B20the+2B20regularised+2B20maximum+2B20likelihood+2B20linear+2B20discriminant+2B20analysis_+3A_seed">seed</code></td>
<td>

<p>If you set this to TRUE, the same folds will be created every time. 
</p>
</td></tr>
<tr><td><code id="Cross-validation+2B20for+2B20the+2B20regularised+2B20maximum+2B20likelihood+2B20linear+2B20discriminant+2B20analysis_+3A_pred.ret">pred.ret</code></td>
<td>

<p>If you want the predicted values returned set this to TRUE.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Cross-validation for the regularised maximum likelihood linear discriminant analysis is performed. 
The function is not extremely fast, yet is pretty fast.
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>preds</code></td>
<td>

<p>If pred.ret is TRUE the predicted values for each fold are returned as elements in a list.
</p>
</td></tr>
<tr><td><code>crit</code></td>
<td>

<p>A vector whose length is equal to the number of k and is the accuracy metric for each k. 
For the classification case it is the percentage of correct classification. For the regression
case the mean square of prediction error. If you want to compute other metrics of accuracy we suggest
you choose &quot;pred.ret = TRUE&quot; when running the function and then write a simple function to compute 
more metrics. See .
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Friedman J., Hastie T. and Tibshirani R. (2017). The elements of statistical learning. 
New York: Springer.
</p>
<p>Cover TM and Hart PE (1967). Nearest neighbor pattern classification. IEEE Transactions on 
Information Theory. 13(1):21-27.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+reg.mle.lda">reg.mle.lda</a>, <a href="#topic+bigknn.cv">bigknn.cv</a>, <a href="#topic+mle.lda">mle.lda</a>, <a href="#topic+big.knn">big.knn</a>, <a href="#topic+weibull.nb">weibull.nb</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- as.matrix(iris[, 1:4])
mod &lt;- regmlelda.cv(x, iris[, 5])
</code></pre>

<hr>
<h2 id='Diagonal+20values+20of+20the+20Hat+20matrix'>
Diagonal values of the Hat matrix
</h2><span id='topic+leverage'></span>

<h3>Description</h3>

<p>Diagonal values of the Hat matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>leverage(x) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Diagonal+2B20values+2B20of+2B20the+2B20Hat+2B20matrix_+3A_x">x</code></td>
<td>

<p>A matrix with independent variables, the design matrix.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function returns the diagonal values of the Hat matrix used in linear regression. We did not call it &quot;hatvalues&quot; as R contains
a built-in function with such a name. 
</p>


<h3>Value</h3>

<p>A vector with the diagonal Hat matrix values, the leverage of each observation.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Hansen, B. E. (2019). Econometrics. 
</p>


<h3>See Also</h3>

<p><code><a href="#topic+gee.reg">gee.reg</a>, <a href="#topic+bic.regs">bic.regs</a>, <a href="#topic+ztp.reg">ztp.reg</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- as.matrix( iris[1:50, 1:4] )
a &lt;- leverage(x)
</code></pre>

<hr>
<h2 id='Distance+20between+20two+20covariance+20matrices'>
Distance between two covariance matrices
</h2><span id='topic+covdist'></span>

<h3>Description</h3>

<p>Distance between two covariance matrices.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>covdist(s1, s2)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Distance+2B20between+2B20two+2B20covariance+2B20matrices_+3A_s1">s1</code></td>
<td>

<p>The firt covariance matrix.
</p>
</td></tr>
<tr><td><code id="Distance+2B20between+2B20two+2B20covariance+2B20matrices_+3A_s2">s2</code></td>
<td>

<p>The second covariance matrix.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A metric for covariance matrices is the title of a paper by Forstner and Moonen (2003). 
The metric is computed for two non-singular covariance matrices. 
</p>


<h3>Value</h3>

<p>The distance between the two covariance matrices.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Forstner W. and Moonen B. (2003). A metric for covariance matrices. 
In Geodesy-The Challenge of the 3rd Millennium, p. 299-309. Springer.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+covlikel">covlikel</a>, <a href="#topic+covequal">covequal</a>, <a href="#topic+covar">covar</a>, <a href="#topic+cor_test">cor_test</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>s1 &lt;- cov(iris[1:50, 1:4])
s2 &lt;- cov(iris[51:100, 1:4])
covdist(s1, s2)
</code></pre>

<hr>
<h2 id='Distance+20correlation+20matrix'>
Distance correlation matrix
</h2><span id='topic+dcora'></span>

<h3>Description</h3>

<p>Distance correlation matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dcora(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Distance+2B20correlation+2B20matrix_+3A_x">x</code></td>
<td>

<p>A numerical matrix.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The distance correlation matrix is computed. 
</p>


<h3>Value</h3>

<p>A matrix with the pairwise distance correlations between all variables in x.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>G.J. Szekely, M.L. Rizzo and N. K. Bakirov (2007). Measuring and Testing Independence 
by Correlation of Distances. Annals of Statistics, 35(6):2769-2794.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+cor_test">cor_test</a>, <a href="#topic+covar">covar</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- as.matrix( iris[1:50, 1:4] )
res &lt;- dcora(x)
</code></pre>

<hr>
<h2 id='Empirical+20and+20exponential+20empirical+20likelihood+20test+20for+20a+20correlation+20coefficient'>
Empirical and exponential empirical likelihood test for a correlation coefficient
</h2><span id='topic+el.cor.test'></span><span id='topic+eel.cor.test'></span>

<h3>Description</h3>

<p>Empirical and exponential empirical likelihood test for a correlation coefficient.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>el.cor.test(y, x, rho, tol = 1e-07)
eel.cor.test(y, x, rho, tol = 1e-07)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Empirical+2B20and+2B20exponential+2B20empirical+2B20likelihood+2B20test+2B20for+2B20a+2B20correlation+2B20coefficient_+3A_y">y</code></td>
<td>

<p>A numerical vector.
</p>
</td></tr>
<tr><td><code id="Empirical+2B20and+2B20exponential+2B20empirical+2B20likelihood+2B20test+2B20for+2B20a+2B20correlation+2B20coefficient_+3A_x">x</code></td>
<td>

<p>A numerical vector.
</p>
</td></tr>
<tr><td><code id="Empirical+2B20and+2B20exponential+2B20empirical+2B20likelihood+2B20test+2B20for+2B20a+2B20correlation+2B20coefficient_+3A_rho">rho</code></td>
<td>

<p>The hypothesized value of the true partial correlation.
</p>
</td></tr>
<tr><td><code id="Empirical+2B20and+2B20exponential+2B20empirical+2B20likelihood+2B20test+2B20for+2B20a+2B20correlation+2B20coefficient_+3A_tol">tol</code></td>
<td>

<p>The tolerance vlaue to terminate the Newton-Raphson algorithm.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The empirical or the exponential empirical likelihood test is performed for the Pearson correlation coefficient.
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>iters</code></td>
<td>

<p>The number of iterations required by the Newton-Raphson. If no convergence occured this is NULL.
</p>
</td></tr>
<tr><td><code>info</code></td>
<td>

<p>A vector with three values, the value of <code class="reqn">\lambda</code>, the test statistic and its associated asymptotic p-value.
If no convergence occured, the value of the <code class="reqn">\lambda</code> is NA, the value of test statistic is <code class="reqn">10^5</code>
and the p-value is 0. No convergence can be interpreted as rejection of the hypothesis test.
</p>
</td></tr>
<tr><td><code>p</code></td>
<td>

<p>The probabilities of the EL or of the EEL. If no covnergence occured this is NULL.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Efron B. (1981) Nonparametric standard errors and confidence intervals. Canadian Journal of
Statistics, 9(2): 139&ndash;158.
</p>
<p>Owen A. B. (2001). Empirical likelihood. Chapman and Hall/CRC Press.
</p>


<h3>See Also</h3>

<p><code> permcor
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>el.cor.test( iris[, 1], iris[, 2], 0 )$info
eel.cor.test( iris[, 1], iris[, 2], 0 )$info
</code></pre>

<hr>
<h2 id='Empirical+20entropy'>
Empirical entropy
</h2><span id='topic+empirical.entropy'></span>

<h3>Description</h3>

<p>Empirical entropy.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>empirical.entropy(x, k = NULL, pretty = FALSE) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Empirical+2B20entropy_+3A_x">x</code></td>
<td>

<p>A numerical vector with continuous values.
</p>
</td></tr>
<tr><td><code id="Empirical+2B20entropy_+3A_k">k</code></td>
<td>

<p>If you want to cut the data into a specific range plug it here, otherwise this decide based upon the Freedman-Diaconis' rule. 
</p>
</td></tr>
<tr><td><code id="Empirical+2B20entropy_+3A_pretty">pretty</code></td>
<td>

<p>Should the breaks be equally space upon the range of x? If yes, let this FALSE. If this is TRUE, the breaks are decided using the 
base command pretty.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function computes the empirical entropy.
</p>


<h3>Value</h3>

<p>The estimated empirical entropy.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p><a href="https://en.wikipedia.org/wiki/Entropy_estimation">https://en.wikipedia.org/wiki/Entropy_estimation</a>
</p>
<p><a href="https://en.wikipedia.org/wiki/Histogram">https://en.wikipedia.org/wiki/Histogram</a>
</p>
<p>Freedman David and Diaconis P. (1981). On the histogram as a density estimator: L2 theory. 
Zeitschrift fur Wahrscheinlichkeitstheorie und Verwandte Gebiete. 57(4): 453-476. 
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+Quantile">Quantile</a>, <a href="base.html#topic+pretty">pretty</a> 
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- rnorm(100)
empirical.entropy(x)
empirical.entropy(x, pretty = TRUE)
</code></pre>

<hr>
<h2 id='Energy+20based+20normality+20test'>
Energy based normality test
</h2><span id='topic+normal.etest'></span>

<h3>Description</h3>

<p>Energy based normality test.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>normal.etest(x, R = 999)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Energy+2B20based+2B20normality+2B20test_+3A_x">x</code></td>
<td>

<p>A numerical vector.
</p>
</td></tr>
<tr><td><code id="Energy+2B20based+2B20normality+2B20test_+3A_r">R</code></td>
<td>

<p>The number of Monte Carlo samples to generate.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The energy based normality test is performed where the p-value is computed via parametric bootstrap. 
The function is faster than the original implementation in the R package &quot;energy&quot;. 
</p>


<h3>Value</h3>

<p>A vector with two values, the test statistic value and the Monte Carlo (parametric bootstrap) based p-value. 
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris &lt;mtsagris@uoc.gr&gt;.
</p>


<h3>References</h3>

<p>Szekely G. J. and Rizzo M.L. (2005) A New Test for Multivariate Normality. 
Journal of Multivariate Analysis, 93(1): 58&ndash;80. 
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+jbtest">jbtest</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- rnorm(100)
normal.etest(x)
</code></pre>

<hr>
<h2 id='Fisher+27s+20linear+20discriminant+20analysis'>Fisher's linear discriminant analysis
</h2><span id='topic+fisher.da'></span>

<h3>Description</h3>

<p>Fisher's linear discriminant analysis.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fisher.da(xnew, x, ina)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Fisher+2B27s+2B20linear+2B20discriminant+2B20analysis_+3A_xnew">xnew</code></td>
<td>

<p>A numerical vector or a matrix with the new observations, continuous data.  
</p>
</td></tr>
<tr><td><code id="Fisher+2B27s+2B20linear+2B20discriminant+2B20analysis_+3A_x">x</code></td>
<td>

<p>A matrix with numerical data.
</p>
</td></tr>
<tr><td><code id="Fisher+2B27s+2B20linear+2B20discriminant+2B20analysis_+3A_ina">ina</code></td>
<td>

<p>A numerical vector or factor with consecutive numbers indicating the group to which each
observation belongs to.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Maximum likelihood linear discriminant analysis is performed.
</p>


<h3>Value</h3>

<p>A vector with the predicted group of each observation in &quot;xnew&quot;.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>. 
</p>


<h3>References</h3>

<p>Kanti V. Mardia, John T. Kent and John M. Bibby (1979). Multivariate analysis. Academic Press, London.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+mle.lda">mle.lda</a>, <a href="#topic+reg.mle.lda">reg.mle.lda</a>, <a href="#topic+big.knn">big.knn</a>, <a href="#topic+weibull.nb">weibull.nb</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- as.matrix(iris[, 1:4])
ina &lt;- iris[, 5]
a &lt;- fisher.da(x, x, ina)
</code></pre>

<hr>
<h2 id='Fixed+20effects+20regression'>
Fixed effects regression
</h2><span id='topic+fe.lmfit'></span>

<h3>Description</h3>

<p>Fixed effects regression.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fe.lmfit(y, x, id)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Fixed+2B20effects+2B20regression_+3A_y">y</code></td>
<td>

<p>A numerical vector or a numerical matrix. 
</p>
</td></tr>
<tr><td><code id="Fixed+2B20effects+2B20regression_+3A_x">x</code></td>
<td>

<p>A numerical matrix with the predictor variables.
</p>
</td></tr>
<tr><td><code id="Fixed+2B20effects+2B20regression_+3A_id">id</code></td>
<td>

<p>A vector with the subject ids. This can be factor or a numerical. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function performs fixed effects regression (within estimator) for panel (longitudinal) data. 
It can also handle unblanced designs. A main difference from the package &quot;plm&quot; is that it returns 
much fewer information, but much faster.  
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>be</code></td>
<td>

<p>The beta coefficients.
</p>
</td></tr>
<tr><td><code>fe</code></td>
<td>

<p>The fixed effect deviations.
</p>
</td></tr>
<tr><td><code>residuals</code></td>
<td>

<p>The residuals of the linear model(s). 
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>https://www.econometrics-with-r.org/10-rwpd.html
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+cluster.lm">cluster.lm</a>, <a href="#topic+gee.reg">gee.reg</a>, <a href="#topic+fipois.reg">fipois.reg</a>, <a href="#topic+wild.boot">wild.boot</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>y &lt;- rnorm(100)
x &lt;- rnorm(100)
id &lt;- rep(1:10, 10)
mod &lt;- fe.lmfit(y, x, id)
</code></pre>

<hr>
<h2 id='Fixed+20intercepts+20Poisson+20regression'>
Fixed intercepts Poisson regression
</h2><span id='topic+fipois.reg'></span>

<h3>Description</h3>

<p>Fixed intercepts Poisson regression.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fipois.reg(y, x, id, tol = 1e-07, maxiters = 100)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Fixed+2B20intercepts+2B20Poisson+2B20regression_+3A_y">y</code></td>
<td>

<p>The dependent variable, a numerical vector with integer, non negative valued data.
</p>
</td></tr>
<tr><td><code id="Fixed+2B20intercepts+2B20Poisson+2B20regression_+3A_x">x</code></td>
<td>

<p>A matrix with the indendent variables.
</p>
</td></tr>
<tr><td><code id="Fixed+2B20intercepts+2B20Poisson+2B20regression_+3A_id">id</code></td>
<td>

<p>A numerical variable with 1, 2, ... indicating the subject. 
Unbalanced design is of course welcome.
</p>
</td></tr>
<tr><td><code id="Fixed+2B20intercepts+2B20Poisson+2B20regression_+3A_tol">tol</code></td>
<td>

<p>The tolerance value to terminate the Newton-Raphson algorithm. 
This is set to <code class="reqn">10^{-7}</code> by default.
</p>
</td></tr>
<tr><td><code id="Fixed+2B20intercepts+2B20Poisson+2B20regression_+3A_maxiters">maxiters</code></td>
<td>

<p>The maximum number of iterations that can take place during the fitting.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Fixed intercepts Poisson regression for clustered count data is fitted. According to Demidenko (2013), when the 
number of clusters (N) is small and the number of observations per cluster (<code class="reqn">n_i</code>) is relatively large, 
say <code class="reqn">min(n_i) &gt; N</code>, one may assume that the intercept <code class="reqn">\alpha_i = \beta + u_i</code> is fixed and unknown 
(<code class="reqn">i=1,...,N</code>). 
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>be</code></td>
<td>

<p>The regression coefficients.
</p>
</td></tr>
<tr><td><code>seb</code></td>
<td>

<p>The standard errors of the regression coefficients.
</p>
</td></tr>
<tr><td><code>ai</code></td>
<td>

<p>The estimated fixed intercepts fore ach cluster of observations.
</p>
</td></tr>
<tr><td><code>covbeta</code></td>
<td>

<p>The covariance matrix of the regression coefficients.
</p>
</td></tr>
<tr><td><code>loglik</code></td>
<td>

<p>The maximised log-likelihood value.
</p>
</td></tr>
<tr><td><code>iters</code></td>
<td>

<p>The number of iteration the Newton-Raphson required.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Eugene Demidenko (2013). Mixed Models: Theory and Applications with R, pages 388-389, 2nd Edition. 
New Jersey: Wiley &amp; Sons (excellent book). 
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+cluster.lm">cluster.lm</a>, <a href="#topic+fe.lmfit">fe.lmfit</a>, <a href="#topic+gee.reg">gee.reg</a>, <a href="#topic+covar">covar</a>, <a href="#topic+welch.tests">welch.tests</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>y &lt;- rpois(200, 10)
id &lt;- sample(1:10, 200, replace = TRUE)
x &lt;- rpois(200, 10)
fipois.reg(y, x, id)
</code></pre>

<hr>
<h2 id='Forward+20Backward+20Early+20Dropping+20selection+20regression'>
Forward Backward Early Dropping selection regression
</h2><span id='topic+fbed.reg'></span>

<h3>Description</h3>

<p>Forward Backward Early Dropping selection regression.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fbed.reg(y, x, alpha = 0.05, type = "logistic", K = 0, backward = FALSE, 
         parallel = FALSE, tol = 1e-07, maxiters = 100) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Forward+2B20Backward+2B20Early+2B20Dropping+2B20selection+2B20regression_+3A_y">y</code></td>
<td>

<p>The response variable, a numeric vector. 
</p>
</td></tr>
<tr><td><code id="Forward+2B20Backward+2B20Early+2B20Dropping+2B20selection+2B20regression_+3A_x">x</code></td>
<td>

<p>A matrix with continuous variables.
</p>
</td></tr>
<tr><td><code id="Forward+2B20Backward+2B20Early+2B20Dropping+2B20selection+2B20regression_+3A_alpha">alpha</code></td>
<td>

<p>The significance threshold value for assessing p-values. Default value is 0.05.
</p>
</td></tr>






<tr><td><code id="Forward+2B20Backward+2B20Early+2B20Dropping+2B20selection+2B20regression_+3A_type">type</code></td>
<td>

<p>The available types are: &quot;logistic&quot; (binary logistic regression), 
&quot;qlogistic&quot; (quasi logistic regression, for binary value or proportions including 0 and 1), 
&quot;poisson&quot; (Poisson regression), &quot;qpoisson&quot; (quasi Poisson regression), 
&quot;weibull&quot; (Weibull regression) and &quot;spml&quot; (SPML regression).
</p>
</td></tr>
<tr><td><code id="Forward+2B20Backward+2B20Early+2B20Dropping+2B20selection+2B20regression_+3A_k">K</code></td>
<td>

<p>How many times should the process be repeated? The default value is 0. 
</p>
</td></tr>
<tr><td><code id="Forward+2B20Backward+2B20Early+2B20Dropping+2B20selection+2B20regression_+3A_backward">backward</code></td>
<td>

<p>After the Forward Early Dropping phase, the algorithm proceeds witha the usual Backward Selection phase. 
The default value is set to TRUE. It is advised to perform this step as maybe some variables are false positives, 
they were wrongly selected. This is rather experimental now and there could be some mistakes in the indices of the 
selected variables. Do not use it for now. 
</p>
</td></tr>
<tr><td><code id="Forward+2B20Backward+2B20Early+2B20Dropping+2B20selection+2B20regression_+3A_parallel">parallel</code></td>
<td>

<p>If you want the algorithm to run in parallel set this TRUE.
</p>
</td></tr>
<tr><td><code id="Forward+2B20Backward+2B20Early+2B20Dropping+2B20selection+2B20regression_+3A_tol">tol</code></td>
<td>

<p>The tolerance value to terminate the Newton-Raphson algorithm. 
</p>
</td></tr>
<tr><td><code id="Forward+2B20Backward+2B20Early+2B20Dropping+2B20selection+2B20regression_+3A_maxiters">maxiters</code></td>
<td>

<p>The maximum number of iterations Newton-Raphson will perform.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The algorithm is a variation of the usual forward selection. At every step, the most 
significant variable enters the selected variables set. In addition, only the significant 
variables stay and are further examined. The non signifcant ones are dropped. This goes 
until no variable can enter the set. The user has the option to re-do this step 1 or more times 
(the argument K). In the end, a backward selection is performed to remove falsely selected variables. 
Note that you may have specified, for example, K=10, but the maximum value FBED used can be 4 for example. 
</p>
<p>The &quot;qlogistic&quot; and &quot;qpoisson&quot; proceed with the Wald test and no backward is performed, while for all the other 
regression types, the log-likelihood ratio test is used and backward phase is available.
</p>


<h3>Value</h3>

<p>If K is a single number a list including:



Note, that the &quot;gam&quot; argument must be the same though.

</p>
<table>
<tr><td><code>res</code></td>
<td>

<p>A matrix with the selected variables and their test statistic.
</p>
</td></tr>
<tr><td><code>info</code></td>
<td>

<p>A matrix with the number of variables and the number of tests performed 
(or models fitted) at each round (value of K). This refers to the
forward phase only.
</p>
</td></tr>
<tr><td><code>runtime</code></td>
<td>

<p>The runtime required.
</p>
</td></tr>






</table>


<h3>Author(s)</h3>

<p>Michail Tsagris and Stefanos Fafalios.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a> and 
Stefanos Fafalios <a href="mailto:stefanosfafalios@gmail.com">stefanosfafalios@gmail.com</a>.
</p>


<h3>References</h3>

<p>Borboudakis G. and Tsamardinos I. (2019). Forward-backward selection with early dropping. 
Journal of Machine Learning Research, 20(8): 1-39.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+logiquant.regs">logiquant.regs</a>, <a href="#topic+bic.regs">bic.regs</a>, <a href="#topic+gee.reg">gee.reg</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#simulate a dataset with continuous data
x &lt;- matrix( runif(100 * 50, 1, 100), ncol = 50 )
y &lt;- rnbinom(100, 10, 0.5)
a &lt;- fbed.reg(y, x, type = "poisson") 
</code></pre>

<hr>
<h2 id='Fractional+20polynomial+20regression+20with+20one+20independent+20variable'>
Fractional polynomial regression with one independent variable.
</h2><span id='topic+fp'></span>

<h3>Description</h3>

<p>Fractional polynomial regression with one independent variable.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fp(y, x, aa, di = NULL, type = "normal", full = FALSE, seb = FALSE, 
tol = 1e-07, maxiters = 100) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Fractional+2B20polynomial+2B20regression+2B20with+2B20one+2B20independent+2B20variable_+3A_y">y</code></td>
<td>

<p>The dependent variable, a numerical vector.
</p>
</td></tr>
<tr><td><code id="Fractional+2B20polynomial+2B20regression+2B20with+2B20one+2B20independent+2B20variable_+3A_x">x</code></td>
<td>

<p>A vector, the independent variable.
</p>
</td></tr>
<tr><td><code id="Fractional+2B20polynomial+2B20regression+2B20with+2B20one+2B20independent+2B20variable_+3A_aa">aa</code></td>
<td>

<p>A vector with two values indicating the range of the optimal value of <code class="reqn">\alpha</code> to search within.
</p>
</td></tr>
<tr><td><code id="Fractional+2B20polynomial+2B20regression+2B20with+2B20one+2B20independent+2B20variable_+3A_di">di</code></td>
<td>

<p>This is valid only for the Weibull regression. A vector with 1s and 0s indicating the censored value. 
The value of 1 means uncesored value, whereas the value of 0 means censored value.
</p>
</td></tr>
<tr><td><code id="Fractional+2B20polynomial+2B20regression+2B20with+2B20one+2B20independent+2B20variable_+3A_type">type</code></td>
<td>

<p>The type of regression model: &quot;normal&quot;, &quot;logistic&quot;, &quot;poisson&quot;, &quot;spml&quot; (SPML regression),
&quot;gamma&quot;, &quot;normlog&quot;, &quot;weibull&quot;, &quot;negbin&quot;.
</p>
</td></tr>
<tr><td><code id="Fractional+2B20polynomial+2B20regression+2B20with+2B20one+2B20independent+2B20variable_+3A_full">full</code></td>
<td>

<p>If this is FALSE, the coefficients and the deviance will be returned only. If this is TRUE, more information is returned.
</p>
</td></tr>
<tr><td><code id="Fractional+2B20polynomial+2B20regression+2B20with+2B20one+2B20independent+2B20variable_+3A_seb">seb</code></td>
<td>

<p>Do you want the standard error of the estimates to be returned? TRUE or FALSE.
</p>
</td></tr>
<tr><td><code id="Fractional+2B20polynomial+2B20regression+2B20with+2B20one+2B20independent+2B20variable_+3A_tol">tol</code></td>
<td>

<p>The tolerance value to terminate the Newton-Raphson algorithm.
</p>
</td></tr>
<tr><td><code id="Fractional+2B20polynomial+2B20regression+2B20with+2B20one+2B20independent+2B20variable_+3A_maxiters">maxiters</code></td>
<td>

<p>The max number of iterations that can take place in each regression.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The independent variable is power transformed and this function searches for the optimal power.
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>a</code></td>
<td>

<p>The power that yields the optimal fit.
</p>
</td></tr>
<tr><td><code>mod</code></td>
<td>

<p>The model with the independent variable power transformed.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Royston P. and Altman D. G. (1994). Regression using fractional polynomials of continuous covariates: parsimonious 
parametric modelling. Journal of the Royal Statistical Society: Series C (Applied Statistics), 43(3): 429-453.
</p>


<h3>See Also</h3>

<p><code> logistic_only, poisson_only </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>y &lt;- rnorm(100)
x &lt;- abs( rnorm(100) )
mod &lt;- fp(y, x, c(-2, 2) )
</code></pre>

<hr>
<h2 id='Gamma+20regression+20with+20a+20log-link'>
Gamma regression with a log-link
</h2><span id='topic+gammareg'></span>

<h3>Description</h3>

<p>Gamma regression with a log-link.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gammareg(y, x, tol = 1e-07, maxiters = 100)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Gamma+2B20regression+2B20with+2B20a+2B20log-link_+3A_y">y</code></td>
<td>

<p>The dependent variable, a numerical variable with non negative numbers.
</p>
</td></tr>
<tr><td><code id="Gamma+2B20regression+2B20with+2B20a+2B20log-link_+3A_x">x</code></td>
<td>

<p>A matrix or data.frame with the indendent variables.
</p>
</td></tr>
<tr><td><code id="Gamma+2B20regression+2B20with+2B20a+2B20log-link_+3A_tol">tol</code></td>
<td>

<p>The tolerance value to terminate the Newton-Raphson algorithm.
</p>
</td></tr>
<tr><td><code id="Gamma+2B20regression+2B20with+2B20a+2B20log-link_+3A_maxiters">maxiters</code></td>
<td>

<p>The maximum number of iterations that can take place in the regression.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The gamma.reg fits a Gamma regression with a log-link. The gamma.con fits a Gamma regression
with a log link with the intercept only ( glm(y ~ 1, Gamma(log) ) ). 
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>iters</code></td>
<td>

<p>The number of iterations required by the newton-Raphson.
</p>
</td></tr>
<tr><td><code>deviance</code></td>
<td>

<p>The deviance value.
</p>
</td></tr>
<tr><td><code>phi</code></td>
<td>

<p>The dispersion parameter (<code class="reqn">\phi</code>) of the regression. This is necessary if you want to 
perform an F hypothesis test for the significance of one or more independent variables.
</p>
</td></tr>
<tr><td><code>be</code></td>
<td>

<p>The regression coefficient(s). 
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Stefanos Fafalios and Michail Tsagris.
</p>
<p>R implementation and documentation: Stefanos Fafalios <a href="mailto:stefanosfafalios@gmail.com">stefanosfafalios@gmail.com</a> and Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>McCullagh, Peter, and John A. Nelder. Generalized linear models. CRC press, USA, 2nd edition, 1989. 
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+gammaregs">gammaregs</a>, <a href="#topic+zigamma.mle">zigamma.mle</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
y &lt;- rgamma(100, 3, 4)
x &lt;- matrix( rnorm(100 * 2), ncol = 2)
m1 &lt;- glm(y ~ x, family = Gamma(log) )
m2 &lt;- gammareg(y, x)

## End(Not run)
</code></pre>

<hr>
<h2 id='GEE+20Gaussian+20regression'>
GEE Gaussian regression
</h2><span id='topic+gee.reg'></span>

<h3>Description</h3>

<p>GEE Gaussian regression.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gee.reg(y, x, id, tol = 1e-07, maxiters = 100)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="GEE+2B20Gaussian+2B20regression_+3A_y">y</code></td>
<td>

<p>The dependent variable, a numerical vector.
</p>
</td></tr>
<tr><td><code id="GEE+2B20Gaussian+2B20regression_+3A_x">x</code></td>
<td>

<p>A matrix with the indendent variables.
</p>
</td></tr>
<tr><td><code id="GEE+2B20Gaussian+2B20regression_+3A_id">id</code></td>
<td>

<p>A numerical variable with 1, 2, ... indicating the subject. 
Unbalanced design is of course welcome.
</p>
</td></tr>
<tr><td><code id="GEE+2B20Gaussian+2B20regression_+3A_tol">tol</code></td>
<td>

<p>The tolerance value to terminate the Newton-Raphson algorithm. 
This is set to <code class="reqn">10^{-7}</code> by default.
</p>
</td></tr>
<tr><td><code id="GEE+2B20Gaussian+2B20regression_+3A_maxiters">maxiters</code></td>
<td>

<p>The maximum number of iterations that can take place during the fitting.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Gaussin GEE regression is fitted.
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>be</code></td>
<td>

<p>The regression coefficients.
</p>
</td></tr>
<tr><td><code>seb</code></td>
<td>

<p>The standard errors of the regression coefficients.
</p>
</td></tr>
<tr><td><code>phi</code></td>
<td>

<p>The <code class="reqn">\phi</code> parameter.
</p>
</td></tr>
<tr><td><code>a</code></td>
<td>

<p>The <code class="reqn">\alpha</code> parameter.
</p>
</td></tr>
<tr><td><code>covbeta</code></td>
<td>

<p>The covariance matrix of the regression coefficients.
</p>
</td></tr>
<tr><td><code>iters</code></td>
<td>

<p>The number of iteration the Newton-Raphson required.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Wang M. (2014). Generalized estimating equations in longitudinal data analysis: 
a review and recent developments. Advances in Statistics, 2014.
</p>
<p>Hardin J. W. and Hilbe J. M. (2002). Generalized estimating equations. 
Chapman and Hall/CRC.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+cluster.lm">cluster.lm</a>, <a href="#topic+fe.lmfit">fe.lmfit</a>, <a href="#topic+wild.boot">wild.boot</a>, <a href="#topic+fipois.reg">fipois.reg</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
y &lt;- rnorm(200)
id &lt;- sample(1:20, 200, replace = TRUE)
x &lt;- rnorm(200, 3)
gee.reg(y, x, id)

</code></pre>

<hr>
<h2 id='Gumbel+20regression'>
Gumbel regression
</h2><span id='topic+gumbel.reg'></span>

<h3>Description</h3>

<p>Gumbel regression.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gumbel.reg(y, x, tol = 1e-07, maxiters = 100)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Gumbel+2B20regression_+3A_y">y</code></td>
<td>

<p>The dependent variable, a numerical vector with real valued numbers.
</p>
</td></tr>
<tr><td><code id="Gumbel+2B20regression_+3A_x">x</code></td>
<td>

<p>A matrix or a data.frame with the indendent variables.
</p>
</td></tr>
<tr><td><code id="Gumbel+2B20regression_+3A_tol">tol</code></td>
<td>

<p>The tolerance value required by the Newton-Raphson to stop.
</p>
</td></tr>
<tr><td><code id="Gumbel+2B20regression_+3A_maxiters">maxiters</code></td>
<td>

<p>The maximum iterations allowed.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A Gumbel regression model is fitted. the standard errors of the regressions are not returned as we do not
compute the full Hessian matrix at each step of the Newton-Raphson.
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>be</code></td>
<td>

<p>The regression coefficients.
</p>
</td></tr>
<tr><td><code>sigma</code></td>
<td>

<p>The scale parameter.
</p>
</td></tr>
<tr><td><code>loglik</code></td>
<td>

<p>The loglikelihood of the regression model.
</p>
</td></tr>
<tr><td><code>iters</code></td>
<td>

<p>The iterations required by the Newton-Raphson.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+negbin.reg">negbin.reg</a>, <a href="#topic+ztp.reg">ztp.reg</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
y &lt;- rnorm(100)
x &lt;- matrix(rnorm(100 * 3), ncol = 3)
mod &lt;- gumbel.reg(y, x)

</code></pre>

<hr>
<h2 id='Hellinger+20distance+20based+20regression+20for+20count+20data'>
Hellinger distance based regression for count data
</h2><span id='topic+hellinger.countreg'></span>

<h3>Description</h3>

<p>Hellinger distance based regression for count data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hellinger.countreg(y, x, tol = 1e-07, maxiters = 100)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Hellinger+2B20distance+2B20based+2B20regression+2B20for+2B20count+2B20data_+3A_y">y</code></td>
<td>

<p>The dependent variable, a numerical vector with integer valued data, counts. 
</p>
</td></tr>
<tr><td><code id="Hellinger+2B20distance+2B20based+2B20regression+2B20for+2B20count+2B20data_+3A_x">x</code></td>
<td>

<p>A numerical matrix with the indendent variables. We add, internally, the first column of ones.
</p>
</td></tr>
<tr><td><code id="Hellinger+2B20distance+2B20based+2B20regression+2B20for+2B20count+2B20data_+3A_tol">tol</code></td>
<td>

<p>The tolerance value to terminate the Newton-Raphson algorithm. 
</p>
</td></tr>
<tr><td><code id="Hellinger+2B20distance+2B20based+2B20regression+2B20for+2B20count+2B20data_+3A_maxiters">maxiters</code></td>
<td>

<p>The max number of iterations that can take place in each regression. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>We minimise the Hellinger distance instead of the ordinarily used divergence, the Kullback-Leibler. 
Both of them fall under the <code class="reqn">\phi</code>-divergence class models and hance this one produces asympottically 
normal regression coefficients as well.
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>be</code></td>
<td>

<p>The regression coefficients.
</p>
</td></tr>
<tr><td><code>seb</code></td>
<td>

<p>The sandwich standard errors of the coefficients.
</p>
</td></tr>
<tr><td><code>covbe</code></td>
<td>

<p>The sandwich covariance matrix of the regression coefficients.
</p>
</td></tr>
<tr><td><code>H</code></td>
<td>

<p>The final Hellinger distance.
</p>
</td></tr>
<tr><td><code>iters</code></td>
<td>

<p>The number of iterations required by Newton-Raphson.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+negbin.reg">negbin.reg</a>, <a href="#topic+gee.reg">gee.reg</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>y &lt;- rpois(100, 10)
x &lt;- iris[1:100, 1]
a &lt;- hellinger.countreg(y, x)
</code></pre>

<hr>
<h2 id='Hellinger+20distance+20based+20univariate+20regression+20for+20proportions'>
Hellinger distance based univariate regression for proportions
</h2><span id='topic+prophelling.reg'></span>

<h3>Description</h3>

<p>Hellinger distance based univariate regression for proportions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>prophelling.reg(y, x, cov = FALSE, tol = 1e-07, maxiters = 100) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Hellinger+2B20distance+2B20based+2B20univariate+2B20regression+2B20for+2B20proportions_+3A_y">y</code></td>
<td>

<p>The dependent variable, a numerical vector with percentages. 
</p>
</td></tr>
<tr><td><code id="Hellinger+2B20distance+2B20based+2B20univariate+2B20regression+2B20for+2B20proportions_+3A_x">x</code></td>
<td>

<p>A numerical matrix with the indendent variables. We add, internally, the first column of ones.
</p>
</td></tr>
<tr><td><code id="Hellinger+2B20distance+2B20based+2B20univariate+2B20regression+2B20for+2B20proportions_+3A_cov">cov</code></td>
<td>

<p>Should the sandwich covariance matrix and the standard errors be returned? If yes, set this equal to TRUE.
</p>
</td></tr>
<tr><td><code id="Hellinger+2B20distance+2B20based+2B20univariate+2B20regression+2B20for+2B20proportions_+3A_tol">tol</code></td>
<td>

<p>The tolerance value to terminate the Newton-Raphson algorithm. 
</p>
</td></tr>
<tr><td><code id="Hellinger+2B20distance+2B20based+2B20univariate+2B20regression+2B20for+2B20proportions_+3A_maxiters">maxiters</code></td>
<td>

<p>The max number of iterations that can take place in each regression. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>We minimise the Jensen-Shannon divergence instead of the ordinarily used divergence, the Kullback-Leibler. 
Both of them fall under the <code class="reqn">\phi</code>-divergence class models and hance this one produces asympottically 
normal regression coefficients as well.
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>be</code></td>
<td>

<p>The regression coefficients.
</p>
</td></tr>
<tr><td><code>seb</code></td>
<td>

<p>The sandwich standard errors of the beta coefficients, if the input argument argument was set to TRUE.
</p>
</td></tr>
<tr><td><code>covb</code></td>
<td>

<p>The sandwich covariance matrix of the beta coefficients, if the input argument argument was set to TRUE.
</p>
</td></tr>
<tr><td><code>js</code></td>
<td>

<p>The final Jensen-Shannon divergence.
</p>
</td></tr>
<tr><td><code>H</code></td>
<td>

<p>The final Hellinger distance.
</p>
</td></tr>
<tr><td><code>iters</code></td>
<td>

<p>The number of iterations required by Newton-Raphson.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Tsagris, Michail (2015). A novel, divergence based, regression for compositional data. 
Proceedings of the 28th Panhellenic Statistics Conference, 15-18/4/2015, Athens, Greece.
https://arxiv.org/pdf/1511.07600.pdf 
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+propols.reg">propols.reg</a>, <a href="#topic+simplex.mle">simplex.mle</a>, <a href="#topic+kumar.mle">kumar.mle</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>y &lt;- rbeta(150, 3, 4)
x &lt;- iris
a &lt;- prophelling.reg(y, x)
</code></pre>

<hr>
<h2 id='Heteroscedastic+20linear+20models+20for+20large+20scale+20data'>
Heteroscedastic linear models for large scale data
</h2><span id='topic+het.lmfit'></span>

<h3>Description</h3>

<p>Heteroscedastic linear models for large scale data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>het.lmfit(x, y, type = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Heteroscedastic+2B20linear+2B20models+2B20for+2B20large+2B20scale+2B20data_+3A_x">x</code></td>
<td>

<p>The design matrix with the data, where each column refers to a different sample of subjects. 
You must supply the design matrix, with the column of 1s. This function is the analogue of 
lm.fit and .lm.fit. 
</p>
</td></tr>
<tr><td><code id="Heteroscedastic+2B20linear+2B20models+2B20for+2B20large+2B20scale+2B20data_+3A_y">y</code></td>
<td>

<p>A numerical vector with the response variable. 
</p>
</td></tr>
<tr><td><code id="Heteroscedastic+2B20linear+2B20models+2B20for+2B20large+2B20scale+2B20data_+3A_type">type</code></td>
<td>

<p>The type of regression to be fit in order to find the weights. The type 1 is described in Wooldridge (2012, page 287), 
whereas type 2 is described in page Wooldridge (2012, page 287).
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>We have simply exploitted R's powerful function and managed to do better than .lm.fit which is a really powerful 
function as well. This is a bare bones function as it returns only two things, the coefficients and the residuals. 
<a href="stats.html#topic+.lm.fit">.lm.fit</a> returns more and <a href="stats.html#topic+lm.fit">lm.fit</a> even more and finally <a href="stats.html#topic+lm">lm</a> returns too much. The addition is 
that we allow for estimation of the regression coefficients when heteroscedasticity is present.
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>be</code></td>
<td>

<p>The beta coefficients.
</p>
</td></tr>
<tr><td><code>residuals</code></td>
<td>

<p>The residuals of the linear model(s). 
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Introductory Econometrics. A modern approach. Mason, South-Western Cengage Learning, 5th Edition.
</p>
<p>Draper, N.R. and Smith H. (1988). Applied regression analysis. New York, Wiley, 3rd edition. 
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+covrob.lm">covrob.lm</a>, <a href="#topic+cls">cls</a>, <a href="#topic+cluster.lm">cluster.lm</a>, <a href="#topic+lm.parboot">lm.parboot</a>, <a href="#topic+cor_test">cor_test</a>, <a href="#topic+lm.drop1">lm.drop1</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- cbind(1, matrix( rnorm( 100 * 4), ncol = 4 ) )
y &lt;- rnorm(100)
a &lt;- het.lmfit(x, y) 
x &lt;- NULL
</code></pre>

<hr>
<h2 id='Hurdle-Poisson+20regression'>
Hurdle-Poisson regression
</h2><span id='topic+hp.reg'></span>

<h3>Description</h3>

<p>Hurdle-Poisson regression.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hp.reg(y, x, full = FALSE, tol = 1e-07, maxiters = 100) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Hurdle-Poisson+2B20regression_+3A_y">y</code></td>
<td>

<p>The dependent variable, a numerical vector with numbers. 
</p>
</td></tr>
<tr><td><code id="Hurdle-Poisson+2B20regression_+3A_x">x</code></td>
<td>

<p>A numerical matrix with the indendent variables. We add, internally, the first column of ones.
</p>
</td></tr>
<tr><td><code id="Hurdle-Poisson+2B20regression_+3A_full">full</code></td>
<td>

<p>If this is FALSE, the coefficients and the log-likelihood will be returned only. 
If this is TRUE, more information is returned.
</p>
</td></tr>
<tr><td><code id="Hurdle-Poisson+2B20regression_+3A_tol">tol</code></td>
<td>

<p>The tolerance value to terminate the Newton-Raphson algorithm. 
</p>
</td></tr>
<tr><td><code id="Hurdle-Poisson+2B20regression_+3A_maxiters">maxiters</code></td>
<td>

<p>The max number of iterations that can take place in each regression. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Two regression models are fitted, a binary logistic regression and a zero truncated Poisson regression model.
</p>


<h3>Value</h3>

<p>Depending on whether &quot;full&quot; is TRUE or not different outputs are returned.
In general, the regression coefficients, the iterations required by Newton-Raphson and the deviances are returned. 
If full is TRUE, a matrix with their standard errors and the Wald test statistics is returned as well.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Mullahy J (1986). Specification and Testing of Some Modified Count Data Models. 
Journal of Econometrics, 33(3): 341&ndash;365.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+negbin.reg">negbin.reg</a>, <a href="#topic+ztp.reg">ztp.reg</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>y &lt;- rpois(100, 4)
x &lt;- iris[1:100, 1]
a &lt;- hp.reg(y, x)
</code></pre>

<hr>
<h2 id='Hypothesis+20test+20for+20equality+20of+20a+20covariance+20matrix'>
Hypothesis test for equality of a covariance matrix
</h2><span id='topic+covequal'></span>

<h3>Description</h3>

<p>Hypothesis test for equality of a covariance matrix.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>covequal(x, sigma, a = 0.05)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Hypothesis+2B20test+2B20for+2B20equality+2B20of+2B20a+2B20covariance+2B20matrix_+3A_x">x</code></td>
<td>

<p>A numerical matrix with the data whose covariance matrix will be tested for equality.
</p>
</td></tr>
<tr><td><code id="Hypothesis+2B20test+2B20for+2B20equality+2B20of+2B20a+2B20covariance+2B20matrix_+3A_sigma">sigma</code></td>
<td>

<p>The covariance matrix that is to be tested for equality.
</p>
</td></tr>
<tr><td><code id="Hypothesis+2B20test+2B20for+2B20equality+2B20of+2B20a+2B20covariance+2B20matrix_+3A_a">a</code></td>
<td>

<p>The level of significance, default value is equal to 0.05.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The likelihood-ratio test is used to test whether the sample covariance matrix from some data 
is equal to some pre-specifief covariance matrix.
</p>


<h3>Value</h3>

<p>A vector with the test statistic, its p-value, the degrees of freedom and the critical 
value of the test.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Mardia K. V., Kent J. T. and Bibby J. M. (1979, pg. 126-127). Multivariate Analysis. 
London: Academic Press.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+covlikel">covlikel</a>, <a href="#topic+covdist">covdist</a>, <a href="#topic+covar">covar</a>, <a href="#topic+cor_test">cor_test</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- as.matrix(iris[1:50, 1:4])
sigma &lt;- cov(iris[, 1:4]) 
covequal(x, sigma)
</code></pre>

<hr>
<h2 id='Hypothesis+20tests+20for+20equality+20of+20multiple+20covariance+20matrices'>
Hypothesis tests for equality of multiple covariance matrices
</h2><span id='topic+covlikel'></span><span id='topic+covmtest'></span>

<h3>Description</h3>

<p>Hypothesis tests for equality of multiple covariance matrices.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>covlikel(x, ina, a = 0.05)
covmtest(x, ina, a = 0.05)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Hypothesis+2B20tests+2B20for+2B20equality+2B20of+2B20multiple+2B20covariance+2B20matrices_+3A_x">x</code></td>
<td>

<p>A numerical matrix with the data whose covariance matrices will be tested for equality.
</p>
</td></tr>
<tr><td><code id="Hypothesis+2B20tests+2B20for+2B20equality+2B20of+2B20multiple+2B20covariance+2B20matrices_+3A_ina">ina</code></td>
<td>

<p>A vector with the grouping variable that defines the groups.
</p>
</td></tr>
<tr><td><code id="Hypothesis+2B20tests+2B20for+2B20equality+2B20of+2B20multiple+2B20covariance+2B20matrices_+3A_a">a</code></td>
<td>

<p>The level of significance, default value is equal to 0.05.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The likelihood-ratio test and the Box's M-test for testing equality of multiple 
covariance matrices. The log-likelihood ratio test is the multivariate generalization 
of Bartlett's test of homogeneity of variances. According to Mardia (1979, pg. 140), 
it may be argued that if <code class="reqn">n_i</code> is small, then the log-likelihood ratio test gives 
too much weight to the contribution of <code class="reqn">\bf S</code>. This consideration led Box (1949) 
to propose his test statistic.
</p>


<h3>Value</h3>

<p>A vector with the test statistic, its p-value, the degrees of freedom and the critical 
value of the test.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Aitchison J. (2003, pg. 155). The Statistical Analysis of Compositional Data. 
New Jersey: (Reprinted by) The Blackburn Press.
</p>
<p>Mardia K. V., Kent J. T. and Bibby J. M. (1979, p.g. 140). Multivariate Analysis. 
London: Academic Press.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+covequal">covequal</a>, <a href="#topic+covdist">covdist</a>, <a href="#topic+covar">covar</a>, <a href="#topic+cor_test">cor_test</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- as.matrix(iris[, 1:4])
ina &lt;- iris[, 5]
covlikel(x, ina)
</code></pre>

<hr>
<h2 id='Intersect'>
Intersect Operation
</h2><span id='topic+Intersect'></span>

<h3>Description</h3>

<p>Performs intersection in the same manner as R's base package intersect works.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Intersect(x, y)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Intersect_+3A_x">x</code>, <code id="Intersect_+3A_y">y</code></td>
<td>

<p>vectors containing a sequence of items, ideally of the same mode
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function will discard any duplicated values in the arguments. 
</p>


<h3>Value</h3>

<p>The function will return a vector of the same mode as the arguments given.
NAs will be removed.
</p>


<h3>Author(s)</h3>

<p>Marios Dimitriadis.
</p>
<p>R implementation and documentation: Marios Dimitriadis &lt;kmdimitriadis@gmail.com&gt;.
</p>


<h3>See Also</h3>

<p><code> <a href="base.html#topic+intersect">intersect</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- c(sort(sample(1:20, 9)))
y &lt;- c(sort(sample(3, 23, 7)))
Intersect(x, y)
</code></pre>

<hr>
<h2 id='Item+20difficulty+20and+20discrimination'>Item difficulty and discrimination</h2><span id='topic+diffic'></span><span id='topic+discrim'></span>

<h3>Description</h3>

<p>Item difficulty and discrimination.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>diffic(x)

discrim(x, frac = 1/3)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Item+2B20difficulty+2B20and+2B20discrimination_+3A_x">x</code></td>
<td>

<p>A numerical matrix with 0s (wrong answer) and 1s (correct answer).  
</p>
</td></tr>
<tr><td><code id="Item+2B20difficulty+2B20and+2B20discrimination_+3A_frac">frac</code></td>
<td>

<p>A number between 0 and 1 used to calculate the difficulty of each question.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The difficulty and the discrimination of each question (item) are calculated.
</p>


<h3>Value</h3>

<p>A vector with the item difficulties or item discriminations.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>. 
</p>


<h3>References</h3>

<p>Kaplan E. L. and Meier P. (1958). Nonparametric estimation from incomplete observations. 
Journal of the American Statistical Association, 53(282): 457-481.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+Quantile">Quantile</a>, <a href="#topic+colmeansvars">colmeansvars</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- matrix(rbinom(100 * 10, 1, 0.7), ncol = 10)
diffic(x)
discrim(x)
</code></pre>

<hr>
<h2 id='Jackknife+20sample+20mean'>
Jackknife sample mean
</h2><span id='topic+jack.mean'></span>

<h3>Description</h3>

<p>Jackknife sample mean.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>jack.mean(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Jackknife+2B20sample+2B20mean_+3A_x">x</code></td>
<td>

<p>A numerical vector with data.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>An efficient implementation of the jackknife mean is provided.
</p>


<h3>Value</h3>

<p>The jackknife sample mean. 
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Efron Bradley and Robert J. Tibshirani (1993). An introduction to the bootstrap. New York: Chapman &amp; Hall/CRC.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+welch.tests">welch.tests</a>, <a href="#topic+trim.mean">trim.mean</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- rnorm(50)
jack.mean(x)
</code></pre>

<hr>
<h2 id='Kaplan-Meier+20estimate+20of+20a+20survival+20function'>Kaplan-Meier estimate of a survival function</h2><span id='topic+km'></span>

<h3>Description</h3>

<p>Kaplan-Meier estimate of a survival function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>km(ti, di)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Kaplan-Meier+2B20estimate+2B20of+2B20a+2B20survival+2B20function_+3A_ti">ti</code></td>
<td>

<p>A numerical vector with the survival times.  
</p>
</td></tr>
<tr><td><code id="Kaplan-Meier+2B20estimate+2B20of+2B20a+2B20survival+2B20function_+3A_di">di</code></td>
<td>

<p>A numerical vector indicating the censorings. 0 = censored, 1 = not censored.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Kaplan-Meier estimate of the survival function takes place. 
</p>


<h3>Value</h3>

<p>A matrix with 4 columns. The non censored times, the number of subjects at risk, 
the number of events at each time and the estimated survival 
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>. 
</p>


<h3>References</h3>

<p>Kaplan E. L. and Meier P. (1958). Nonparametric estimation from incomplete observations. 
Journal of the American Statistical Association, 53(282): 457-481.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+sp.logiregs">sp.logiregs</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>y &lt;- rgamma(40, 10, 1)
di &lt;- rbinom(40, 1, 0.6)
a &lt;- km(y, di)
</code></pre>

<hr>
<h2 id='Linear+20model+20with+20sandwich+20robust+20covariance+20estimator'>
Linear model with sandwich robust covariance estimator
</h2><span id='topic+covrob.lm'></span>

<h3>Description</h3>

<p>Linear model with sandwich robust covariance estimator.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>covrob.lm(y, x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Linear+2B20model+2B20with+2B20sandwich+2B20robust+2B20covariance+2B20estimator_+3A_y">y</code></td>
<td>

<p>A numerical vector with the response variable. 
</p>
</td></tr>
<tr><td><code id="Linear+2B20model+2B20with+2B20sandwich+2B20robust+2B20covariance+2B20estimator_+3A_x">x</code></td>
<td>

<p>The design matrix with the data, where each column refers to a different sample of subjects. 
You must supply the design matrix, with the column of 1s. This function is the analogue of 
lm.fit and .lm.fit. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function performs the usual linear regression model but returns robust standard errors
using the sandwich covariance estimator.
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>info</code></td>
<td>

<p>A matrix with the beta coefficients, their robust standard error, their t-test statistic, 
and their associated p-value.
</p>
</td></tr>
<tr><td><code>robcov</code></td>
<td>

<p>The sandwich robust covariance matrix. 
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Introductory Econometrics. A modern approach. Mason, South-Western Cengage Learning, 5th Edition.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+het.lmfit">het.lmfit</a>, <a href="#topic+cluster.lm">cluster.lm</a>, <a href="#topic+lm.parboot">lm.parboot</a>, <a href="#topic+cor_test">cor_test</a>, <a href="#topic+lm.drop1">lm.drop1</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- matrix( rnorm( 100 * 4), ncol = 4 )
y &lt;- rnorm(100)
a &lt;- covrob.lm(y, x) 
x &lt;- NULL
</code></pre>

<hr>
<h2 id='Linear+20regression+20with+20clustered+20data'>
Linear regression with clustered data
</h2><span id='topic+cluster.lm'></span>

<h3>Description</h3>

<p>Linear regression with clustered data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cluster.lm(y, x, id) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Linear+2B20regression+2B20with+2B20clustered+2B20data_+3A_y">y</code></td>
<td>

<p>The dependent variable, a numerical vector with numbers.
</p>
</td></tr>
<tr><td><code id="Linear+2B20regression+2B20with+2B20clustered+2B20data_+3A_x">x</code></td>
<td>

<p>A matrix or a data.frame with the indendent variables.
</p>
</td></tr>
<tr><td><code id="Linear+2B20regression+2B20with+2B20clustered+2B20data_+3A_id">id</code></td>
<td>

<p>A numerical variable with 1, 2, ... indicating the subject. Unbalanced design is of course welcome. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A linear regression model for clustered data is fitted. For more information see Chapter 4.21 of Hansen (2019).
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>be</code></td>
<td>

<p>The (beta) regression coefficients.
</p>
</td></tr>
<tr><td><code>becov</code></td>
<td>

<p>Robust covariance matrix of the regression coefficients.
</p>
</td></tr>
<tr><td><code>seb</code></td>
<td>

<p>Robust standard errors of the regression coefficients.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Hansen, B. E. (2022). Econometrics. 
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+gee.reg">gee.reg</a>, <a href="#topic+fe.lmfit">fe.lmfit</a>, <a href="#topic+wild.boot">wild.boot</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
y &lt;- rnorm(200)
id &lt;- sample(1:20, 200, replace = TRUE)
x &lt;- rnorm(200, 3)
cluster.lm(y, x, id)

</code></pre>

<hr>
<h2 id='Logistic+20regression+20for+20large+20scale+20data'>
Logistic regression for large scale data
</h2><span id='topic+batch.logistic'></span>

<h3>Description</h3>

<p>Logistic regression for large scale data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>batch.logistic(y, x, k = 10)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Logistic+2B20regression+2B20for+2B20large+2B20scale+2B20data_+3A_y">y</code></td>
<td>

<p>The dependent variable, a numerical vector with 0s and 1s.
</p>
</td></tr>
<tr><td><code id="Logistic+2B20regression+2B20for+2B20large+2B20scale+2B20data_+3A_x">x</code></td>
<td>

<p>A matrix with the continuous indendent variables.
</p>
</td></tr>
<tr><td><code id="Logistic+2B20regression+2B20for+2B20large+2B20scale+2B20data_+3A_k">k</code></td>
<td>

<p>The number of batches to use (see details). 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The batch logistic regression cuts the data into k distinct batches. Then performs logistic regression
on each of these batches and the in end combines the coefficients in a meta-analytic form, using the 
fixed effects form. Using these coefficients, the deviance of the model is computed for all data. 
This method is pretty accurate for large scale data, with say millions, or even tens of millions 
of observations.
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>res</code></td>
<td>

<p>A two-column matrix with the regression coefficients and their associated standard errors.
</p>
</td></tr>
<tr><td><code>devi</code></td>
<td>

<p>The deviance of the logistic regression.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+binom.reg">binom.reg</a>, <a href="#topic+sclr">sclr</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>y &lt;- rbinom(1000, 1, 0.5)
x &lt;- matrix( rnorm(1000 * 5), ncol = 5 )
## not a very good approximation since the data are not of large scale
batch.logistic(y, x, k = 2) 
</code></pre>

<hr>
<h2 id='Mahalanobis+20depth'>
Mahalanobis depth
</h2><span id='topic+depth.mahala'></span>

<h3>Description</h3>

<p>Mahalanobis depth.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>depth.mahala(x, data)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Mahalanobis+2B20depth_+3A_x">x</code></td>
<td>

<p>A numerical vector or matrix whose depth you want to compute.
</p>
</td></tr>
<tr><td><code id="Mahalanobis+2B20depth_+3A_data">data</code></td>
<td>

<p>A numerical matrix used to compute the depth of x.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function computes the Mahalanobis depth of x with respect to data.
</p>


<h3>Value</h3>

<p>A numevrical vector with the Mahalanobis depth for each value of x. 
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Mahalanobis P. (1936). On the generalized distance in statistics. Proceedings of the National Academy India, 12 49&ndash;55.
</p>
<p>Liu R.Y. (1992). Data depth and multivariate rank tests. In Dodge Y. (editors), L1-Statistics and Related Methods, 279&ndash;294.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+welch.tests">welch.tests</a>, <a href="#topic+trim.mean">trim.mean</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- as.matrix(iris[1:50, 1:4])
depth.mahala(x, x)
</code></pre>

<hr>
<h2 id='Many+202+20sample+20student+27s+20t-tests'>
Many 2 sample student's t-tests
</h2><span id='topic+stud.ttests'></span>

<h3>Description</h3>

<p>It performs very many 2 sample student's t-tests.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>stud.ttests(x, y = NULL, ina, logged = FALSE, parallel = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Many+2B202+2B20sample+2B20student+2B27s+2B20t-tests_+3A_x">x</code></td>
<td>

<p>A matrix with the data, where the rows denote the samples and the columns are the variables.
</p>
</td></tr>
<tr><td><code id="Many+2B202+2B20sample+2B20student+2B27s+2B20t-tests_+3A_y">y</code></td>
<td>

<p>A second matrix with the data of the second group. If this is NULL (default value) then the 
argument ina must be supplied. Notice that when you supply the two matrices the procedure is 
two times faster.
</p>
</td></tr> 
<tr><td><code id="Many+2B202+2B20sample+2B20student+2B27s+2B20t-tests_+3A_ina">ina</code></td>
<td>

<p>A numerical vector with 1s and 2s indicating the two groups. Be careful, the function is 
designed to accept only these two numbers. In addition, if your &quot;y&quot; is NULL, you must specify &quot;ina&quot;.
</p>
</td></tr>
<tr><td><code id="Many+2B202+2B20sample+2B20student+2B27s+2B20t-tests_+3A_logged">logged</code></td>
<td>

<p>Should the p-values be returned (FALSE) or their logarithm (TRUE)?
</p>
</td></tr>
<tr><td><code id="Many+2B202+2B20sample+2B20student+2B27s+2B20t-tests_+3A_parallel">parallel</code></td>
<td>

<p>Should parallel implentations take place in C++? The default value is FALSE.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For the t-tests, the student's t-test (that assumes equal variances) is 
performed.   
</p>


<h3>Value</h3>

<p>A matrix with the test statistic, the degrees of freedom and the p-value 
(or their logarithm) of each test.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>&quot;Student&quot; William Sealy Gosset (1908). The probable error of a mean. 
Biometrika. 6(1): 1-25.  
</p>


<h3>See Also</h3>

<p><code><a href="#topic+boot.student2">boot.student2</a>, <a href="#topic+boot.ttest1">boot.ttest1</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## 1000 variables, hence 20 t-tests will be performed
x = matrix( rnorm(100 * 20), ncol = 20)
## 100 observations in total
ina = rbinom(100, 1, 0.6) + 1   ## independent samples t-test
stud.ttests(x, ina = ina)
x1 = x[ina == 1, ]
x2 = x[ina == 2, ]
stud.ttests(x1, x2)
x &lt;- NULL
</code></pre>

<hr>
<h2 id='Many+20approximate+20simple+20logistic+20regressions'>
Many approximate simple logistic regressions.
</h2><span id='topic+sp.logiregs'></span>

<h3>Description</h3>

<p>Many approximate simple logistic regressions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sp.logiregs(y, x, logged = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Many+2B20approximate+2B20simple+2B20logistic+2B20regressions_+3A_y">y</code></td>
<td>

<p>The dependent variable, a numerical vector with 0s or 1s.
</p>
</td></tr>
<tr><td><code id="Many+2B20approximate+2B20simple+2B20logistic+2B20regressions_+3A_x">x</code></td>
<td>

<p>A matrix with the indendent variables.
</p>
</td></tr>
<tr><td><code id="Many+2B20approximate+2B20simple+2B20logistic+2B20regressions_+3A_logged">logged</code></td>
<td>

<p>Should the p-values be returned (FALSE) or their logarithm (TRUE)?
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Many simple approximate logistic regressions are performed and hypothesis testing
for the singificance of each coefficient is returned. The code is available in the paper by
Sikorska et al. (2013). We simply took the code and made some minor modifications. The explanation
and the motivation can be found in their paper. They call it semi-parallel logistic regressions, hence 
we named the function sp.logiregs. 
</p>


<h3>Value</h3>

<p>A two-column matrix with the test statistics (Wald statistic) and their 
associated p-values (or their logarithm).
</p>


<h3>Author(s)</h3>

<p>Initial author Karolina Sikorska. Modifications by Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Karolina Sikorska, Emmanuel Lesaffre, Patrick FJ Groenen and Paul HC Eilers (2013): 14:166.
GWAS on your notebook: fast semi-parallel linear and logistic regression for genome-wide
association studies. 
https://bmcbioinformatics.biomedcentral.com/track/pdf/10.1186/1471-2105-14-166
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+logiquant.regs">logiquant.regs</a>, <a href="#topic+bic.regs">bic.regs</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>y &lt;- rbinom(200, 1, 0.5)
x &lt;- matrix( rnorm(200 * 50), ncol = 50 )
a &lt;- sp.logiregs(y, x)
</code></pre>

<hr>
<h2 id='Many+20binary+20classification+20metrics'>
Many binary classification metrics
</h2><span id='topic+colaccs'></span><span id='topic+colsens'></span><span id='topic+colspecs'></span><span id='topic+colprecs'></span><span id='topic+colfscores'></span><span id='topic+colfbscores'></span><span id='topic+colfmis'></span>

<h3>Description</h3>

<p>Many binary classification metrics.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>colaccs(group, preds)
colsens(group, preds)
colspecs(group, preds)
colprecs(group, preds)
colfscores(group, preds)
colfbscores(group, preds, b)
colfmis(group, preds)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Many+2B20binary+2B20classification+2B20metrics_+3A_group">group</code></td>
<td>

<p>A numerical vector with two values, 0 and 1. 
</p>
</td></tr>
<tr><td><code id="Many+2B20binary+2B20classification+2B20metrics_+3A_preds">preds</code></td>
<td>

<p>A numerical matrix with scores, probabilities or any other measure. 
</p>
</td></tr>
<tr><td><code id="Many+2B20binary+2B20classification+2B20metrics_+3A_b">b</code></td>
<td>

<p>The <code class="reqn">\beta</code> parameter in the <code class="reqn">F_{\beta}</code>-score. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The accuracies, sensitivities, specificities, precisions, F-scores, <code class="reqn">F_{\beta}</code>-scores 
and the Fowlkes-Mallows index are calculated column-wise. The colaccs is the 
only metric that can be used with a multinomial response as well. 
</p>


<h3>Value</h3>

<p>A vector with length equal to the number of columns of the &quot;preds&quot; argument containing the 
relevant values computed for each column. 
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>https://en.wikipedia.org/wiki/Sensitivity_and_specificity
</p>
<p>https://en.wikipedia.org/wiki/Precision_and_recall
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+colmses">colmses</a>, <a href="#topic+bernoulli.nb">bernoulli.nb</a>, <a href="#topic+bigknn.cv">bigknn.cv</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## 20 variables, hence 20 accuracies will be calculated
ina &lt;- rbinom(100, 1, 0.6)
x &lt;- matrix( rnorm(100 * 20), ncol = 20 )
a &lt;- colaccs(ina, x)
</code></pre>

<hr>
<h2 id='Many+20Gamma+20regressions'>
Many Gamma regressions
</h2><span id='topic+gammaregs'></span>

<h3>Description</h3>

<p>Many Gamma regressions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gammaregs(y, x, tol = 1e-07, logged = FALSE, parallel = FALSE, maxiters = 100)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Many+2B20Gamma+2B20regressions_+3A_y">y</code></td>
<td>

<p>The dependent variable, a numerical variable with non negative numbers for the Gamma and inverse Gaussian regressions. 
For the Gaussian with a log-link zero values are allowed.
</p>
</td></tr>
<tr><td><code id="Many+2B20Gamma+2B20regressions_+3A_x">x</code></td>
<td>

<p>A matrix with the indendent variables.
</p>
</td></tr>
<tr><td><code id="Many+2B20Gamma+2B20regressions_+3A_tol">tol</code></td>
<td>

<p>The tolerance value to terminate the Newton-Raphson algorithm.
</p>
</td></tr>
<tr><td><code id="Many+2B20Gamma+2B20regressions_+3A_logged">logged</code></td>
<td>

<p>A boolean variable; it will return the logarithm of the pvalue if set to TRUE.
</p>
</td></tr>
<tr><td><code id="Many+2B20Gamma+2B20regressions_+3A_parallel">parallel</code></td>
<td>

<p>Do you want this to be executed in parallel or not. The parallel takes place in C++, therefore you do not have the option to set the number of cores.
</p>
</td></tr>
<tr><td><code id="Many+2B20Gamma+2B20regressions_+3A_maxiters">maxiters</code></td>
<td>

<p>The maximum number of iterations that can take place in each regression.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Many simple Gamma regressions with a log-link are fitted.
</p>


<h3>Value</h3>

<p>A matrix with the test statistic values and their relevant (logged) p-values.
</p>


<h3>Author(s)</h3>

<p>Stefanos Fafalios and and Michail Tsagris.
</p>
<p>R implementation and documentation: Stefanos Fafalios <a href="mailto:stefanosfafalios@gmail.com">stefanosfafalios@gmail.com</a> and Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>McCullagh, Peter, and John A. Nelder. Generalized linear models. CRC press, USA, 2nd edition, 1989. 
</p>
<p>Zakariya Yahya Algamal and Intisar Ibrahim Allyas (2017). Prediction of blood lead level in maternal and fetal 
using generalized linear model. International Journal of Advanced Statistics and Probability, 5(2): 65-69.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+bic.regs">bic.regs</a>, <a href="#topic+gammareg">gammareg</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
y &lt;- rgamma(100, 3, 10)
x &lt;- matrix( rnorm( 100 * 10), ncol = 10 )
b &lt;- glm(y ~ x[, 1], family = Gamma(log) )
anova(b, test= "F")
a &lt;- gammaregs(y, x)
x &lt;- NULL

## End(Not run)
</code></pre>

<hr>
<h2 id='Many+20Jarque-Bera+20normality+20tests'>
Many Jarque-Bera normality tests
</h2><span id='topic+jbtests'></span><span id='topic+jbtest'></span>

<h3>Description</h3>

<p>Many Jarque-Bera normality tests.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>jbtests(x)
jbtest(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Many+2B20Jarque-Bera+2B20normality+2B20tests_+3A_x">x</code></td>
<td>

<p>A matrix with the data, where the rows denote the observations and the columns are the variables. 
In the case of a single sample, then this must be a vector and &quot;jbtest&quot; is to be used.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Jarque-Bera univariate normality test is performed for each column (variable) of the matrix x. 
</p>


<h3>Value</h3>

<p>A matrix with two columns, or a vector with two elements. 
Either way, the test statistic value and its associated p-value.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris
</p>
<p>R implementation and documentation: Michail Tsagris &lt;mtsagris@uoc.gr&gt;.
</p>


<h3>References</h3>

<p>Yazici B. and Yolacan S. (2007). A comparison of various tests of normality. 
Journal of Statistical Computation and Simulation, 77(2): 175&ndash;183.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+normal.etest">normal.etest</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- matrix( rnorm(100 * 20), ncol = 20 )
a &lt;- jbtests(x) 
x &lt;- rnorm(100)
jbtest(x)
</code></pre>

<hr>
<h2 id='Many+20metrics+20for+20a+20continuous+20response+20variable'>
any metrics for a continuous response variable
</h2><span id='topic+colmses'></span><span id='topic+colmaes'></span><span id='topic+colpkl'></span><span id='topic+colukl'></span>

<h3>Description</h3>

<p>any metrics for a continuous response variable.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>colmses(y, yhat, parallel = FALSE)
colmaes(y, yhat, parallel = FALSE)
colpkl(y, yhat, parallel = FALSE)
colukl(y, yhat, parallel = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Many+2B20metrics+2B20for+2B20a+2B20continuous+2B20response+2B20variable_+3A_y">y</code></td>
<td>

<p>A numerical vector. 
</p>
</td></tr>
<tr><td><code id="Many+2B20metrics+2B20for+2B20a+2B20continuous+2B20response+2B20variable_+3A_yhat">yhat</code></td>
<td>

<p>A numerical matrix with with the predictions. 
</p>
</td></tr>
<tr><td><code id="Many+2B20metrics+2B20for+2B20a+2B20continuous+2B20response+2B20variable_+3A_parallel">parallel</code></td>
<td>

<p>If you want parallel computations set this equal to TRUE.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The mean squared errors, mean absolute errors, and Kullback-Leibler divergence for percentages (colpkl) 
and non-negative values or discrete values (colukl) are computed.  
</p>


<h3>Value</h3>

<p>A vector with length equal to the number of columns of the &quot;yhat&quot; argument containing the 
relevant values computed for each column. 
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+colaccs">colaccs</a>, <a href="#topic+bigknn.cv">bigknn.cv</a>, <a href="#topic+mmpc">mmpc</a>, <a href="#topic+pc.sel">pc.sel</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## 20 variables, hence 20 MSEs will be calculated
y &lt;- rnorm(100, 1, 0.6)
yhat &lt;- matrix( rnorm(100 * 20), ncol = 20 )
a &lt;- colmses(y, yhat)
</code></pre>

<hr>
<h2 id='Many+20negative+20binomial+20regressions'>
Many negative binomial regressions
</h2><span id='topic+negbin.regs'></span>

<h3>Description</h3>

<p>Many negative binomial regressions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>negbin.regs(y, x, type = 1, tol = 1e-07, logged = FALSE, parallel = FALSE, maxiters = 100)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Many+2B20negative+2B20binomial+2B20regressions_+3A_y">y</code></td>
<td>

<p>The dependent variable, a numerical variable with non negative numbers
</p>
</td></tr>
<tr><td><code id="Many+2B20negative+2B20binomial+2B20regressions_+3A_x">x</code></td>
<td>

<p>A matrix with the indendent variables.
</p>
</td></tr>
<tr><td><code id="Many+2B20negative+2B20binomial+2B20regressions_+3A_type">type</code></td>
<td>

<p>You can choose which way your prefer. Type 1 is for smal sample sizes, whereas type 2 is for larger ones as is faster. 
</p>
</td></tr>
<tr><td><code id="Many+2B20negative+2B20binomial+2B20regressions_+3A_tol">tol</code></td>
<td>

<p>The tolerance value to terminate the Newton-Raphson algorithm.
</p>
</td></tr>
<tr><td><code id="Many+2B20negative+2B20binomial+2B20regressions_+3A_logged">logged</code></td>
<td>

<p>If you want the logarithm of the p-values set this equal to TRUE.
</p>
</td></tr>
<tr><td><code id="Many+2B20negative+2B20binomial+2B20regressions_+3A_parallel">parallel</code></td>
<td>

<p>Do you want this to be executed in parallel or not. The parallel takes place in C++, therefore 
you do not have the option to set the number of cores.
</p>
</td></tr>
<tr><td><code id="Many+2B20negative+2B20binomial+2B20regressions_+3A_maxiters">maxiters</code></td>
<td>

<p>The maximum number of iterations that can take place in each regression.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Many simple negative binomial regressions with a log-link are fitted.
</p>


<h3>Value</h3>

<p>A matrix with the test statistic values and their relevant (logged) p-values.
</p>


<h3>Author(s)</h3>

<p>Stefanos Fafalios and and Michail Tsagris.
</p>
<p>R implementation and documentation: Stefanos Fafalios <a href="mailto:stefanosfafalios@gmail.com">stefanosfafalios@gmail.com</a> and 
Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>McCullagh, Peter, and John A. Nelder. Generalized linear models. CRC press, USA, 2nd edition, 1989. 
</p>
<p>Zakariya Yahya Algamal and Intisar Ibrahim Allyas (2017). Prediction of blood lead level in maternal and fetal 
using generalized linear model. International Journal of Advanced Statistics and Probability, 5(2): 65-69.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+bic.regs">bic.regs</a>, <a href="#topic+negbin.reg">negbin.reg</a>, <a href="#topic+score.zipregs">score.zipregs</a>, <a href="#topic+ztp.reg">ztp.reg</a> 
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
y &lt;- rnbinom(100, 10, 0.6)
x &lt;- matrix( rnorm( 100 * 200), ncol = 200 )
a &lt;- negbin.regs(y, x)
x &lt;- NULL

## End(Not run)
</code></pre>

<hr>
<h2 id='Many+20score+20based+20regressions+20with+20muliple+20response+20variables+20and+20a+20single+20predictor+20variable'>
Many score based regressions with muliple response variables and a single predictor variable
</h2><span id='topic+mv.score.glms'></span><span id='topic+mv.score.weibregs'></span><span id='topic+mv.score.betaregs'></span><span id='topic+mv.score.gammaregs'></span><span id='topic+mv.score.expregs'></span><span id='topic+mv.score.invgaussregs'></span>

<h3>Description</h3>

<p>Many score based regressions with muliple response variables and a single predictor variable.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mv.score.glms(y, x, oiko = NULL, logged = FALSE) 
mv.score.weibregs(y, x, logged = FALSE) 
mv.score.betaregs(y, x, logged = FALSE) 
mv.score.gammaregs(y, x, logged = FALSE) 
mv.score.expregs(y, x, logged = FALSE)
mv.score.invgaussregs(y, x, logged = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Many+2B20score+2B20based+2B20regressions+2B20with+2B20muliple+2B20response+2B20variables+2B20and+2B20a+2B20single+2B20predictor+2B20variable_+3A_y">y</code></td>
<td>

<p>A matrix with either discrete or binary data for the Poisson or binary logistic regression respectively. 
For the Weibull, gamma, inverse Gaussian and exponential regressions the values of y must be strictly positive data, 
lifetimes or durations for example. For the beta regression they must be numbers between 0 and 1. 
</p>
</td></tr>
<tr><td><code id="Many+2B20score+2B20based+2B20regressions+2B20with+2B20muliple+2B20response+2B20variables+2B20and+2B20a+2B20single+2B20predictor+2B20variable_+3A_x">x</code></td>
<td>

<p>A vector with continuous data, the predictor variable. 
</p>
</td></tr>
<tr><td><code id="Many+2B20score+2B20based+2B20regressions+2B20with+2B20muliple+2B20response+2B20variables+2B20and+2B20a+2B20single+2B20predictor+2B20variable_+3A_oiko">oiko</code></td>
<td>

<p>This can be either &quot;poisson&quot; or &quot;binomial&quot;. 
</p>
</td></tr>
<tr><td><code id="Many+2B20score+2B20based+2B20regressions+2B20with+2B20muliple+2B20response+2B20variables+2B20and+2B20a+2B20single+2B20predictor+2B20variable_+3A_logged">logged</code></td>
<td>

<p>A boolean variable; it will return the logarithm of the pvalue if set to TRUE.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Instead of maximising the log-likelihood via the Newton-Raphson algorithm in order to perform the hypothesis 
testing that <code class="reqn">\beta_i=0</code> we use the score test. This is dramatcially faster as no model needs to be fitted. 
The first derivative (score) of the log-likelihood is known and in closed form and under the null hypothesis 
the fitted values are all equal to the mean of the response variable y. The variance of the score is also known 
in closed form. The test is not the same as the likelihood ratio test. It is size correct nonetheless but it 
is a bit less efficient and less powerful. For big sample sizes though (5000 or more) the results are the same. 
We have seen via simulation studies is that it is size correct to large sample sizes, at elast a few thousands. 
You can try for yourselves and see that even with 500 the results are pretty close. The score test is pretty faster 
then the classical log-likelihood ratio test.  
</p>


<h3>Value</h3>

<p>A matrix with two columns, the test statistic and its associated p-value. For the Poisson and logistic 
regression the p-value is derived via the t distribution, whereas for all other regression models 
via the <code class="reqn">\chi^2</code> distribution. 
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Tsagris M., Alenazi A. and Fafalios S. (2020). Computationally efficient univariate filtering for massive data. 
Electronic Journal of Applied Statistical Analysis, 13(2):390-412.
</p>
<p>Hosmer DW. JR, Lemeshow S. and Sturdivant R.X. (2013). Applied Logistic Regression. New Jersey ,Wiley, 3rd Edition.
</p>
<p>Campbell M.J. (2001). Statistics at Square Two: Understand Modern Statistical Applications in Medicine, pg. 112.
London, BMJ Books. 
</p>
<p>McCullagh Peter, and John A. Nelder. Generalized linear models.  CRC press, USA, 2nd edition, 1989.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+score.zipregs">score.zipregs</a>, <a href="#topic+gammaregs">gammaregs</a>, <a href="#topic+weib.regs">weib.regs</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>y &lt;- matrix(rbeta(100 * 10, 2, 3), ncol = 10)
x &lt;- rnorm(100)
a &lt;- mv.score.betaregs(y, x)
y &lt;- NULL
</code></pre>

<hr>
<h2 id='Many+20score+20based+20zero+20inflated+20Poisson+20regressions'>
Many score based zero inflated Poisson regressions
</h2><span id='topic+score.zipregs'></span>

<h3>Description</h3>

<p>Many score based zero inflated Poisson regressions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>score.zipregs(y, x, logged = FALSE ) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Many+2B20score+2B20based+2B20zero+2B20inflated+2B20Poisson+2B20regressions_+3A_y">y</code></td>
<td>

<p>A vector with discrete data, counts.
</p>
</td></tr>
<tr><td><code id="Many+2B20score+2B20based+2B20zero+2B20inflated+2B20Poisson+2B20regressions_+3A_x">x</code></td>
<td>

<p>A matrix with data, the predictor variables. 
</p>
</td></tr>
<tr><td><code id="Many+2B20score+2B20based+2B20zero+2B20inflated+2B20Poisson+2B20regressions_+3A_logged">logged</code></td>
<td>

<p>A boolean variable; it will return the logarithm of the pvalue if set to TRUE.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Instead of maximising the log-likelihood via the Newton-Raphson algorithm in order to perform the hypothesis testing that <code class="reqn">\beta_i=0</code> we use the score test. 
This is dramatcially faster as no model need to be fitted. The first derivative of the log-likelihood is known in closed form and under the null hypothesis the 
fitted values are all equal to the mean of the response variable y. The test is not the same as the likelihood ratio test. It is size correct nonetheless but it is 
a bit less efficient and less powerful. For big sample sizes though (5000 or more) the results are the same. It is also much faster then the classical likelihood ratio test.  
</p>


<h3>Value</h3>

<p>A matrix with two columns, the test statistic and its associated (logged) p-value.  
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris..
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Tsagris M., Alenazi A. and Fafalios S. (2020). Computationally efficient univariate filtering for massive data. 
Electronic Journal of Applied Statistical Analysis, 13(2):390-412.
</p>
<p>Lambert D. (1992). Zero-inflated Poisson regression, with an application to defects in manufacturing. Technometrics, 34(1):1-14.
</p>
<p>Campbell, M.J. (2001). Statistics at Square Two: Understand Modern Statistical Applications in Medicine, pg. 112.
London, BMJ Books. 
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+ztp.reg">ztp.reg</a>, <a href="#topic+censpois.mle">censpois.mle</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- matrix( rnorm(1000 * 100), ncol = 100 )
y &lt;- rpois(1000, 10)
y[1:150] &lt;- 0
a &lt;- score.zipregs(y, x)
x &lt;- NULL
mean(a &lt; 0.05) ## estimated type I error 
</code></pre>

<hr>
<h2 id='Many+20simple+20quantile+20regressions+20using+20logistic+20regressions'>
Many simple quantile regressions using logistic regressions.
</h2><span id='topic+logiquant.regs'></span>

<h3>Description</h3>

<p>Many simple quantile regressions using logistic regressions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>logiquant.regs(y, x, logged = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Many+2B20simple+2B20quantile+2B20regressions+2B20using+2B20logistic+2B20regressions_+3A_y">y</code></td>
<td>

<p>The dependent variable, a numerical vector.
</p>
</td></tr>
<tr><td><code id="Many+2B20simple+2B20quantile+2B20regressions+2B20using+2B20logistic+2B20regressions_+3A_x">x</code></td>
<td>

<p>A matrix with the indendent variables.
</p>
</td></tr>
<tr><td><code id="Many+2B20simple+2B20quantile+2B20regressions+2B20using+2B20logistic+2B20regressions_+3A_logged">logged</code></td>
<td>

<p>Should the p-values be returned (FALSE) or their logarithm (TRUE)?
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Instead of fitting quantile regression models, one for each predictor variable and trying to assess its significance, 
Redden et al. (2004) proposed a simple singificance test based on logistic regression. 
Create an indicator variable I where 1 indicates a response value above its median and 0 elsewhere. 
Since I is binary, perform logistic regression for the predictor and assess its significance using the likelihood 
ratio test. We perform many logistic regression models since we have many predictors whose univariate association with the 
response variable we want to test.
</p>


<h3>Value</h3>

<p>A two-column matrix with the test statistics (likelihood ratio test statistic) and their 
associated p-values (or their logarithm).
</p>


<h3>Author(s)</h3>

<p>Author: Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>David T. Redden, Jose R. Fernandez and David B. Allison (2004). A simple significance test for quantile regression.
Statistics in Medicine, 23(16): 2587-2597 
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+bic.regs">bic.regs</a>, <a href="#topic+sp.logiregs">sp.logiregs</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>y &lt;- rcauchy(100, 3, 2)
x &lt;- matrix( rnorm(100 * 50), ncol = 50 )
a &lt;- logiquant.regs(y, x)
</code></pre>

<hr>
<h2 id='Many+20simple+20Weibull+20regressions'>
Many simple Weibull regressions.
</h2><span id='topic+weib.regs'></span>

<h3>Description</h3>

<p>Many simple Weibull regressions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>weib.regs(y, x, tol = 1e-07, logged = FALSE, parallel = FALSE, maxiters = 100)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Many+2B20simple+2B20Weibull+2B20regressions_+3A_y">y</code></td>
<td>

<p>The dependent variable, either a numerical variable with numbers greater than zero.
</p>
</td></tr>
<tr><td><code id="Many+2B20simple+2B20Weibull+2B20regressions_+3A_x">x</code></td>
<td>

<p>A matrix with the indendent variables.
</p>
</td></tr>
<tr><td><code id="Many+2B20simple+2B20Weibull+2B20regressions_+3A_tol">tol</code></td>
<td>

<p>The tolerance value to terminate the Newton-Raphson algorithm.
</p>
</td></tr>
<tr><td><code id="Many+2B20simple+2B20Weibull+2B20regressions_+3A_logged">logged</code></td>
<td>

<p>A boolean variable; it will return the logarithm of the pvalue if set to TRUE.
</p>
</td></tr>
<tr><td><code id="Many+2B20simple+2B20Weibull+2B20regressions_+3A_parallel">parallel</code></td>
<td>

<p>Do you want this to be executed in parallel or not. The parallel takes place in C++, and the number of threads 
is defined by each system's availiable cores.
</p>
</td></tr>
<tr><td><code id="Many+2B20simple+2B20Weibull+2B20regressions_+3A_maxiters">maxiters</code></td>
<td>

<p>The maximum number of iterations that can take place in each regression.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Many simple weibull regressions are fitted.
</p>


<h3>Value</h3>

<p>A matrix with the test statistic values and their associated (logged) p-values.
</p>


<h3>Author(s)</h3>

<p>Stefanos Fafalios.
</p>
<p>R implementation and documentation: Stefanos Fafalios <a href="mailto:stefanosfafalios@gmail.com">stefanosfafalios@gmail.com</a>.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+bic.regs">bic.regs</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>y &lt;- rgamma(100, 3, 4)
x &lt;- matrix( rnorm( 100 * 30 ), ncol = 30 )
a &lt;- weib.regs(y, x)
x &lt;- NULL
</code></pre>

<hr>
<h2 id='Many+20Welch+20tests'>
Many Welch tests.
</h2><span id='topic+welch.tests'></span>

<h3>Description</h3>

<p>Many Welch tests.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>welch.tests(y, x, logged = FALSE, parallel = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Many+2B20Welch+2B20tests_+3A_y">y</code></td>
<td>

<p>The dependent variable, a numerical vector.
</p>
</td></tr>
<tr><td><code id="Many+2B20Welch+2B20tests_+3A_x">x</code></td>
<td>

<p>A matrix with the indendent variables. They must be integer valued 
data starting from 1, not 0 and be consecutive numbers. Instead of a 
data.frame with factor variables, the user must use a matrix with integers.
</p>
</td></tr>
<tr><td><code id="Many+2B20Welch+2B20tests_+3A_logged">logged</code></td>
<td>

<p>Should the p-values be returned (FALSE) or their logarithm (TRUE)?
</p>
</td></tr>
<tr><td><code id="Many+2B20Welch+2B20tests_+3A_parallel">parallel</code></td>
<td>

<p>If you want to run the function in parallel set this equal to TRUE.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For each categorical predictor variable, a Welch test is performed. This is useful
in feature selection algorithms, to determine for which variable, the means of the 
dependent variable differ across the different values. 
</p>


<h3>Value</h3>

<p>A two-column matrix with the test statistics (F test statistic) and their 
associated p-values (or their logarithm).
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>B.L. Welch (1951). On the comparison of several mean values: an alternative approach. 
Biometrika, 38(3/4), 330-336.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+sp.logiregs">sp.logiregs</a>, <a href="#topic+pc.sel">pc.sel</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>y &lt;- rnorm(200)
x &lt;- matrix(rbinom(200 * 50, 2, 0.5), ncol = 50) + 1
a &lt;- welch.tests(y, x)
</code></pre>

<hr>
<h2 id='Max-Min+20Parents+20and+20Children+20variable+20selection+20algorithm+20for+20continuous+20responses'>
Max-Min Parents and Children variable selection algorithm for continuous responses
</h2><span id='topic+mmpc'></span>

<h3>Description</h3>

<p>Max-Min Parents and Children variable selection algorithm for continuous responses.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mmpc(y, x, max_k = 3, alpha = 0.05, method = "pearson", 
ini = NULL, hash = FALSE, hashobject = NULL, backward = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Max-Min+2B20Parents+2B20and+2B20Children+2B20variable+2B20selection+2B20algorithm+2B20for+2B20continuous+2B20responses_+3A_y">y</code></td>
<td>

<p>The class variable. Provide a numeric vector.
</p>
</td></tr>
<tr><td><code id="Max-Min+2B20Parents+2B20and+2B20Children+2B20variable+2B20selection+2B20algorithm+2B20for+2B20continuous+2B20responses_+3A_x">x</code></td>
<td>

<p>The main dataset. Provide a numeric matrix.
</p>
</td></tr>
<tr><td><code id="Max-Min+2B20Parents+2B20and+2B20Children+2B20variable+2B20selection+2B20algorithm+2B20for+2B20continuous+2B20responses_+3A_max_k">max_k</code></td>
<td>

<p>The maximum conditioning set to use in the conditional independence test. Provide an integer.
</p>
<p>The default value set is 3.
</p>
</td></tr>
<tr><td><code id="Max-Min+2B20Parents+2B20and+2B20Children+2B20variable+2B20selection+2B20algorithm+2B20for+2B20continuous+2B20responses_+3A_alpha">alpha</code></td>
<td>

<p>Threshold for assessing p-values' significance. 
Provide a double value, between 0.0 and 1.0. 
</p>
<p>The default value set is 0.05.
</p>
</td></tr>
<tr><td><code id="Max-Min+2B20Parents+2B20and+2B20Children+2B20variable+2B20selection+2B20algorithm+2B20for+2B20continuous+2B20responses_+3A_method">method</code></td>
<td>

<p>Currently only &quot;pearson&quot; is supported.
</p>
</td></tr>
<tr><td><code id="Max-Min+2B20Parents+2B20and+2B20Children+2B20variable+2B20selection+2B20algorithm+2B20for+2B20continuous+2B20responses_+3A_ini">ini</code></td>
<td>

<p>This argument is used for the avoidance of the univariate associations re-calculations, in the case of them being present. Provide it in the form of a list.
</p>
</td></tr>
<tr><td><code id="Max-Min+2B20Parents+2B20and+2B20Children+2B20variable+2B20selection+2B20algorithm+2B20for+2B20continuous+2B20responses_+3A_hash">hash</code></td>
<td>

<p>Boolean value for the activation of the statistics storage in a hash type object.
</p>
<p>The default value is false.
</p>
</td></tr>
<tr><td><code id="Max-Min+2B20Parents+2B20and+2B20Children+2B20variable+2B20selection+2B20algorithm+2B20for+2B20continuous+2B20responses_+3A_hashobject">hashobject</code></td>
<td>

<p>This argument is used for the avoidance of the hash re-calculation, in the case of them being present, similarly to ini argument. Provide it in the form of a hash.
</p>
<p>Please note that the generated hash object should be used only when the same dataset is re-analyzed, possibly with different values of max_k and alpha.
</p>
</td></tr>
<tr><td><code id="Max-Min+2B20Parents+2B20and+2B20Children+2B20variable+2B20selection+2B20algorithm+2B20for+2B20continuous+2B20responses_+3A_backward">backward</code></td>
<td>

<p>Boolean value for the activation of the backward/symmetry correction phase. This option removes and falsely included variables in the parents and children set of the target variable. It calls the <code>link{mmpc_bp}</code> for this purpose. The backward option seems dubious. Please do not use at the moment.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The MMPC function implements the MMPC algorithm as presented in
&quot;Tsamardinos, Brown and Aliferis. The max-min hill-climbing
Bayesian network structure learning algorithm&quot;
http://www.dsl-lab.org/supplements/mmhc_paper/paper_online.pdf
</p>


<h3>Value</h3>

<p>The output of the algorithm is an list including:
</p>
<table>
<tr><td><code>selected</code></td>
<td>

<p>The order of the selected variables according to the increasing pvalues.
</p>
</td></tr>
<tr><td><code>hashobject</code></td>
<td>

<p>The hash object containing the statistics calculated in the current run.
</p>
</td></tr>
<tr><td><code>pvalues</code></td>
<td>

<p>For each feature included in the dataset, this vector reports the strength of its association with the target in the context of all other variables. Particularly, 
this vector reports the max p-values found when the association of each variable with the target is tested against different conditional sets. Lower values indicate
higher association.
</p>
</td></tr>
<tr><td><code>stats</code></td>
<td>

<p>The statistics corresponding to the aforementioned pvalues (higher values indicate higher association).
</p>
</td></tr>
<tr><td><code>univ</code></td>
<td>

<p>This is a list with the univariate associations; the test
statistics and their corresponding logged p-values. 
</p>
</td></tr>
<tr><td><code>max_k</code></td>
<td>

<p>The max_k value used in the current execution. 
</p>
</td></tr>
<tr><td><code>alpha</code></td>
<td>

<p>The alpha value used in the current execution.
</p>
</td></tr>
<tr><td><code>n.tests</code></td>
<td>

<p>If hash = TRUE, the number of tests performed will be returned.
If hash != TRUE, the number of univariate associations will be returned.
</p>
</td></tr>
<tr><td><code>runtime</code></td>
<td>

<p>The time (in seconds) that was needed for the execution of algorithm.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Marios Dimitriadis.
</p>
<p>R implementation and documentation: Marios Dimitriadis &lt;kmdimitriadis@gmail.com&gt;.
</p>


<h3>References</h3>

<p>Tsagris M. and Tsamardinos I. (2019). Feature selection with the R package MXM. F1000Research 7: 1505
</p>
<p>Feature Selection with the R Package MXM: Discovering Statistically Equivalent Feature Subsets, Lagani V. and Athineou 
G. and Farcomeni A. and Tsagris M. and Tsamardinos I. (2017). Journal of Statistical Software, 80(7).
</p>
<p>Tsamardinos, I., Aliferis, C. F. and Statnikov, A. (2003). Time and sample efficient discovery of Markov blankets and direct causal 
relations. In Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining (pp. 673-678). ACM.
</p>
<p>Brown L. E., Tsamardinos, I. and Aliferis C. F. (2004). A novel algorithm for scalable and accurate Bayesian network learning. 
Medinfo, 711-715.
</p>
<p>Tsamardinos, Brown and Aliferis (2006). The max-min hill-climbing Bayesian network structure learning algorithm. Machine learning, 
65(1), 31-78.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+mmpc">mmpc</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(123)

# Dataset with continuous data
ds &lt;- matrix(runif(100 * 500, 1, 100), ncol = 500)

# Class variable
tar &lt;- 3 * ds[, 10] + 2 * ds[, 100] + 3 * ds[, 20] + rnorm(100, 0, 5)

mmpc(tar, ds, max_k = 3, alpha = 0.05, method = "pearson")
</code></pre>

<hr>
<h2 id='Max-Min+20Parents+20and+20Children+20variable+20selection+20algorithm+20for+20non+20continuous+20responses'>
Max-Min Parents and Children variable selection algorithm for non continuous responses
</h2><span id='topic+mmpc2'></span>

<h3>Description</h3>

<p>Max-Min Parents and Children variable selection algorithm for non continuous responses.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mmpc2(y, x, max_k = 3, threshold = 0.05, test = "logistic", init = NULL, 
tol = 1e-07, backward = FALSE, maxiters = 100, parallel = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Max-Min+2B20Parents+2B20and+2B20Children+2B20variable+2B20selection+2B20algorithm+2B20for+2B20non+2B20continuous+2B20responses_+3A_y">y</code></td>
<td>

<p>The response variable, a numeric vector with either count data or binary data.
</p>
</td></tr>
<tr><td><code id="Max-Min+2B20Parents+2B20and+2B20Children+2B20variable+2B20selection+2B20algorithm+2B20for+2B20non+2B20continuous+2B20responses_+3A_x">x</code></td>
<td>

<p>A numerical matrix with the independent (predictor) variables.
</p>
</td></tr>
<tr><td><code id="Max-Min+2B20Parents+2B20and+2B20Children+2B20variable+2B20selection+2B20algorithm+2B20for+2B20non+2B20continuous+2B20responses_+3A_max_k">max_k</code></td>
<td>

<p>The maximum conditioning set to use in the conditional indepedence test (see Details). Integer, default value is 3.
</p>
</td></tr>
<tr><td><code id="Max-Min+2B20Parents+2B20and+2B20Children+2B20variable+2B20selection+2B20algorithm+2B20for+2B20non+2B20continuous+2B20responses_+3A_threshold">threshold</code></td>
<td>

<p>Threshold (suitable values in (0, 1)) for assessing p-values significance. Default value is 0.05.
</p>
</td></tr>
<tr><td><code id="Max-Min+2B20Parents+2B20and+2B20Children+2B20variable+2B20selection+2B20algorithm+2B20for+2B20non+2B20continuous+2B20responses_+3A_test">test</code></td>
<td>

<p>One of the following: &quot;logistic&quot;, &quot;poisson&quot;, &quot;qpoisson&quot;. 
</p>
</td></tr>
<tr><td><code id="Max-Min+2B20Parents+2B20and+2B20Children+2B20variable+2B20selection+2B20algorithm+2B20for+2B20non+2B20continuous+2B20responses_+3A_init">init</code></td>
<td>

<p>A numeric vector with the logged p-values of the univariate associations. <b>Do not use this at the moment.</b>
</p>
</td></tr>
<tr><td><code id="Max-Min+2B20Parents+2B20and+2B20Children+2B20variable+2B20selection+2B20algorithm+2B20for+2B20non+2B20continuous+2B20responses_+3A_tol">tol</code></td>
<td>

<p>The tolerance value to stop the Newtn-Raphson algorithm inside a regression model.
</p>
</td></tr>
<tr><td><code id="Max-Min+2B20Parents+2B20and+2B20Children+2B20variable+2B20selection+2B20algorithm+2B20for+2B20non+2B20continuous+2B20responses_+3A_backward">backward</code></td>
<td>

<p>If TRUE, the backward (or symmetry correction) phase will be implemented. This removes any falsely included variables in the parents and children set of the target variable. It calls the <code>link{mmpcbackphase}</code> for this purpose.
</p>
</td></tr>
<tr><td><code id="Max-Min+2B20Parents+2B20and+2B20Children+2B20variable+2B20selection+2B20algorithm+2B20for+2B20non+2B20continuous+2B20responses_+3A_maxiters">maxiters</code></td>
<td>

<p>The maximum number of iterations a Newtn-Raphson algorithm will perform inside a regression model.
</p>
</td></tr>
<tr><td><code id="Max-Min+2B20Parents+2B20and+2B20Children+2B20variable+2B20selection+2B20algorithm+2B20for+2B20non+2B20continuous+2B20responses_+3A_parallel">parallel</code></td>
<td>

<p>Do you want the computations to take part in parallel? Set TRUE if yes.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>MMPC tests each feature for inclusion (selection). It is a variant of the forward selection procedure.
a) at every step it removes the non significant variables and does not check thema again.
b) Instead of testing a candidate variable conditioning on all previously selected variables, it uses subsets of the
previously selected variables. All possible subsets of maximum size equal to max_k. With the approrpiate pre-computations,
at every step, it performs only the tests that were not exeucyted before, so it is not that time consuming. 
</p>


<h3>Value</h3>

<p>The output of the algorithm is an S3 object including:
</p>
<table>
<tr><td><code>selectedVars</code></td>
<td>

<p>The selected variables, i.e., the signature of the target variable.
</p>
</td></tr>
<tr><td><code>pvalues</code></td>
<td>

<p>For each feature included in the dataset, this vector reports the strength of its association with the target in the context of all other variable. Particularly, this vector reports the max p-values found when the association of each variable with the target is tested against different conditional sets. Lower values indicate higher association.
</p>
</td></tr>
<tr><td><code>univ</code></td>
<td>

<p>A vector with the logged p-values of the univariate associations. This vector is very important for subsequent runs of MMPC with different hyper-parameters. After running SES with some hyper-parameters you might want to run MMPCagain with different hyper-parameters. To avoid calculating the univariate associations (first step) again, you can take this list from the first run of SES and plug it in the argument &quot;ini&quot; in the next run(s) of MMPC. This can speed up the second run (and subequent runs of course) by 50%. See the argument &quot;univ&quot; in the output values. 
</p>
</td></tr>
<tr><td><code>kapa_pval</code></td>
<td>

<p>A list with the same number of elements as the max$_k$. Every element in the list is a matrix. 
The first column is the logged p-values, the second column is the variable whose conditional association with the 
response variable was tested and the other columns are the conditioning variables. 
</p>
</td></tr>
<tr><td><code>max_k</code></td>
<td>

<p>The max_k option used in the current run.
</p>
</td></tr>
<tr><td><code>threshold</code></td>
<td>

<p>The threshold option used in the current run.
</p>
</td></tr>
<tr><td><code>n.tests</code></td>
<td>

<p>The number of tests performed by MMPC will be returned. 
</p>
</td></tr>
<tr><td><code>runtime</code></td>
<td>

<p>The run time of the algorithm. A numeric vector. The first element is the user time, the second element is the system time and the third element is the elapsed time.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Manos Papadakis.
</p>
<p>R implementation and documentation: Manos Papadakis <a href="mailto:papadakm95@gmail.com">papadakm95@gmail.com</a>. 
</p>


<h3>References</h3>

<p>Tsagris M. and Tsamardinos I. (2019). Feature selection with the R package MXM. F1000Research 7: 1505
</p>
<p>Feature Selection with the R Package MXM: Discovering Statistically Equivalent Feature Subsets, Lagani, V. and Athineou, 
G. and Farcomeni, A. and Tsagris, M. and Tsamardinos, I. (2017). Journal of Statistical Software, 80(7).
</p>
<p>Tsamardinos I., Aliferis C. F. and Statnikov, A. (2003). Time and sample efficient discovery of Markov blankets and direct causal 
relations. In Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining (pp. 673-678). ACM.
</p>
<p>Brown L. E., Tsamardinos I. and Aliferis, C. F. (2004). A novel algorithm for scalable and accurate Bayesian network learning. 
Medinfo, 711-715.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+mmpc">mmpc</a>, <a href="#topic+pc.sel">pc.sel</a>, <a href="#topic+fbed.reg">fbed.reg</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
y &lt;- rbinom(100, 1, 0.5)
x &lt;- matrix( rnorm(100 * 500), ncol = 500 )
m1 &lt;- mmpc2(y, x, max_k = 3, threshold = 0.05, test = "logistic")
m2 &lt;- fbed.reg(y, x, type = "logistic")

</code></pre>

<hr>
<h2 id='Maximum+20likelihood+20linear+20discriminant+20analysis'>Maximum likelihood linear discriminant analysis
</h2><span id='topic+mle.lda'></span>

<h3>Description</h3>

<p>Maximum likelihood linear discriminant analysis.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mle.lda(xnew, x, ina)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Maximum+2B20likelihood+2B20linear+2B20discriminant+2B20analysis_+3A_xnew">xnew</code></td>
<td>

<p>A numerical vector or a matrix with the new observations, continuous data.  
</p>
</td></tr>
<tr><td><code id="Maximum+2B20likelihood+2B20linear+2B20discriminant+2B20analysis_+3A_x">x</code></td>
<td>

<p>A matrix with numerical data.
</p>
</td></tr>
<tr><td><code id="Maximum+2B20likelihood+2B20linear+2B20discriminant+2B20analysis_+3A_ina">ina</code></td>
<td>

<p>A numerical vector or factor with consecutive numbers indicating the group to which each
observation belongs to.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Maximum likelihood linear discriminant analysis is performed.
</p>


<h3>Value</h3>

<p>A vector with the predicted group of each observation in &quot;xnew&quot;.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>. 
</p>


<h3>References</h3>

<p>Kanti V. Mardia, John T. Kent and John M. Bibby (1979). Multivariate analysis. Academic Press, London.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+fisher.da">fisher.da</a>, <a href="#topic+reg.mle.lda">reg.mle.lda</a>, <a href="#topic+big.knn">big.knn</a>, <a href="#topic+weibull.nb">weibull.nb</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- as.matrix(iris[, 1:4])
ina &lt;- iris[, 5]
a &lt;- mle.lda(x, x, ina)
</code></pre>

<hr>
<h2 id='MLE+20of+20continuous+20univariate+20distributions+20defined+20on+20the+20positive+20line'>
MLE of continuous univariate distributions defined on the positive line
</h2><span id='topic+halfcauchy.mle'></span><span id='topic+powerlaw.mle'></span>

<h3>Description</h3>

<p>MLE of continuous univariate distributions defined on the positive line.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>halfcauchy.mle(x, tol = 1e-07) 
powerlaw.mle(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MLE+2B20of+2B20continuous+2B20univariate+2B20distributions+2B20defined+2B20on+2B20the+2B20positive+2B20line_+3A_x">x</code></td>
<td>

<p>A vector with positive valued data (zeros are not allowed).
</p>
</td></tr>
<tr><td><code id="MLE+2B20of+2B20continuous+2B20univariate+2B20distributions+2B20defined+2B20on+2B20the+2B20positive+2B20line_+3A_tol">tol</code></td>
<td>

<p>The tolerance level up to which the maximisation stops; set to 1e-09 by default.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Instead of maximising the log-likelihood via a numerical optimiser we have used a Newton-Raphson algorithm which is faster. 
See wikipedia for the equations to be solved. For the power law we assume that the minimum value of x is above zero in order to perform 
the maximum likelihood estimation in the usual way.  
</p>


<h3>Value</h3>

<p>Usually a list with three elements, but this is not for all cases. 
</p>
<table>
<tr><td><code>iters</code></td>
<td>
<p>The number of iterations required for the Newton-Raphson to converge.
</p>
</td></tr>
<tr><td><code>loglik</code></td>
<td>
<p>The value of the maximised log-likelihood.
</p>
</td></tr>
<tr><td><code>scale</code></td>
<td>
<p>The scale parameter of the half Cauchy distribution.
</p>
</td></tr>
<tr><td><code>alpha</code></td>
<td>
<p>The value of the power parameter for the power law distribution.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>N.L. Johnson, S. Kotz and N. Balakrishnan (1994). Continuous Univariate Distributions, Volume 1 (2nd Edition).
</p>
<p>N.L. Johnson, S. Kotz and N. Balakrishnan (1970). Distributions in statistics: continuous univariate 
distributions, Volume 2
</p>
<p>You can also check the relevant wikipedia pages for these distributions.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+zigamma.mle">zigamma.mle</a>, <a href="#topic+censweibull.mle">censweibull.mle</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- abs( rcauchy(1000, 0, 2) )
halfcauchy.mle(x)
</code></pre>

<hr>
<h2 id='MLE+20of+20distributions+20defined+20for+20proportions'>
MLE of distributions defined for proportions
</h2><span id='topic+kumar.mle'></span><span id='topic+simplex.mle'></span><span id='topic+zil.mle'></span><span id='topic+unitweibull.mle'></span><span id='topic+cbern.mle'></span><span id='topic+sp.mle'></span>

<h3>Description</h3>

<p>MLE of distributions defined for proportions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>kumar.mle(x, tol = 1e-07, maxiters = 50)
simplex.mle(x, tol = 1e-07)
zil.mle(x)
unitweibull.mle(x, tol = 1e-07, maxiters = 100) 
cbern.mle(x, tol = 1e-6) 
sp.mle(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MLE+2B20of+2B20distributions+2B20defined+2B20for+2B20proportions_+3A_x">x</code></td>
<td>

<p>A vector with proportions or percentages. Zeros are allowed only for the zero inflated 
logistirc normal distribution (zil.mle).
</p>
</td></tr>
<tr><td><code id="MLE+2B20of+2B20distributions+2B20defined+2B20for+2B20proportions_+3A_tol">tol</code></td>
<td>

<p>The tolerance level up to which the maximisation stops.
</p>
</td></tr>
<tr><td><code id="MLE+2B20of+2B20distributions+2B20defined+2B20for+2B20proportions_+3A_maxiters">maxiters</code></td>
<td>

<p>The maximum number of iterations the Newton-Raphson will perform. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The distributions included are the Kumaraswamy, zero inflated logistic 
normal, simplex, unit Weibull and continuous Bernoulli and standard power. 
Instead of maximising the log-likelihood via a numerical optimiser we 
have used a Newton-Raphson algorithm which is faster. 
See wikipedia for the equations to be solved.  
</p>


<h3>Value</h3>

<p>Usually a list with three elements, but this is not for all cases. 
</p>
<table>
<tr><td><code>iters</code></td>
<td>
<p>The number of iterations required for the Newton-Raphson to converge.
</p>
</td></tr>
<tr><td><code>param</code></td>
<td>

<p>The two parameters (shape and scale) of the Kumaraswamy distribution. 
For the zero inflated logistic normal, the probability of non zeros, 
the mean and the unbiased variance. 
</p>
</td></tr>
<tr><td><code>loglik</code></td>
<td>
<p>The value of the maximised log-likelihood.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Kumaraswamy P. (1980). A generalized probability density function for double-bounded 
random processes. Journal of Hydrology 46(1-2): 79-88.
</p>
<p>Jones M.C. (2009). Kumaraswamy's distribution: A beta-type distribution with some 
tractability advantages. Statistical Methodology, 6(1): 70-81. 
</p>
<p>Mazucheli J., Menezes A.F.B., Fernandes L.B., de Oliveira R.P. and Ghitany M.E. (2020). 
The unit-Weibull distribution as an alternative to the Kumaraswamy distribution for the 
modeling of quantiles conditional on covariates. 
Journal of Applied Statistics, DOI:10.1080/02664763.2019.1657813
</p>
<p>Leemis L.M. and McQueston J.T. (2008). Univariate Distribution Relationships.
The American Statistician, 62(1): 45-53.
</p>
<p>You can also check the relevant wikipedia pages.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+zigamma.mle">zigamma.mle</a>, <a href="#topic+censweibull.mle">censweibull.mle</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>u &lt;- runif(1000)
a &lt;- 0.4  ;  b &lt;- 1
x &lt;- ( 1 - (1 - u)^(1/b) )^(1/a)
kumar.mle(x)
</code></pre>

<hr>
<h2 id='MLE+20of+20some+20circular+20distributions+20with+20multiple+20samples'>
MLE of some circular distributions with multiple samples
</h2><span id='topic+multivm.mle'></span><span id='topic+multispml.mle'></span>

<h3>Description</h3>

<p>MLE of some circular distributions with multiple samples.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>multivm.mle(x, ina, tol = 1e-07, ell = FALSE)
multispml.mle(x, ina, tol = 1e-07, ell = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MLE+2B20of+2B20some+2B20circular+2B20distributions+2B20with+2B20multiple+2B20samples_+3A_x">x</code></td>
<td>

<p>A numerical vector with the circular data. They must be expressed in radians. For the &quot;spml.mle&quot; this can also
be a matrix with two columns, the cosinus and the sinus of the circular data.
</p>
</td></tr>
<tr><td><code id="MLE+2B20of+2B20some+2B20circular+2B20distributions+2B20with+2B20multiple+2B20samples_+3A_ina">ina</code></td>
<td>

<p>A numerical vector with discrete numbers starting from 1, i.e. 1, 2, 3, 4,... or a factor variable. 
Each number denotes a sample or group. If you supply a continuous valued vector the function will 
obviously provide wrong results. 
</p>
</td></tr>
<tr><td><code id="MLE+2B20of+2B20some+2B20circular+2B20distributions+2B20with+2B20multiple+2B20samples_+3A_tol">tol</code></td>
<td>

<p>The tolerance level to stop the iterative process of finding the MLEs.
</p>
</td></tr>
<tr><td><code id="MLE+2B20of+2B20some+2B20circular+2B20distributions+2B20with+2B20multiple+2B20samples_+3A_ell">ell</code></td>
<td>

<p>Do you want the log-likelihood returned? The default value is FALSE. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The parameters of the von Mises and of the bivariate angular Gaussian distributions 
are estimated for multiple samples. 
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>iters</code></td>
<td>

<p>The iterations required until convergence. This is returned in the wrapped Cauchy distribution only.
</p>
</td></tr>
<tr><td><code>loglik</code></td>
<td>

<p>A vector with the value of the maximised log-likelihood for each sample.
</p>
</td></tr>
<tr><td><code>mi</code></td>
<td>

<p>For the von Mises, this is a vector with the means of each sample. For the angular Gaussian (spml), a matrix with the 
mean vector of each sample
</p>
</td></tr>
<tr><td><code>ki</code></td>
<td>

<p>A vector with the concentration parameter of the von Mises distribution at each sample.
</p>
</td></tr>
<tr><td><code>gi</code></td>
<td>

<p>A vector with the norm of the mean vector of the angular Gaussian distribution at each sample.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris. 
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Mardia K. V. and Jupp P. E. (2000). Directional statistics. Chicester: John Wiley &amp; Sons.
</p>
<p>Sra S. (2012). A short note on parameter approximation for von Mises-Fisher distributions:
and a fast implementation of Is(x). Computational Statistics, 27(1): 177-190.
</p>
<p>Presnell Brett, Morrison Scott P. and Littell Ramon C. (1998). Projected multivariate linear models for directional data.
Journal of the American Statistical Association, 93(443): 1068-1077.
</p>
<p>Kent J. and Tyler D. (1988). Maximum likelihood estimation for the wrapped Cauchy distribution.
Journal of Applied Statistics, 15(2): 247&ndash;254.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+colspml.mle">colspml.mle</a>, <a href="#topic+purka.mle">purka.mle</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>y &lt;- rcauchy(100, 3, 1)
x &lt;- y 
ina &lt;- rep(1:2, 50)
multivm.mle(x, ina)
multispml.mle(x, ina)
</code></pre>

<hr>
<h2 id='MLE+20of+20some+20truncated+20distributions'>MLE of some truncated distributions
</h2><span id='topic+trunccauchy.mle'></span><span id='topic+truncexpmle'></span>

<h3>Description</h3>

<p>MLE of some truncated distributions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>trunccauchy.mle(x, a, b, tol = 1e-07)
truncexpmle(x, b, tol = 1e-07)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MLE+2B20of+2B20some+2B20truncated+2B20distributions_+3A_x">x</code></td>
<td>

<p>A numerical vector with continuous data. For the Cauchy distribnution, they can be anywhere on the real line. 
For the exponential distribution they must be strictly positive.  
</p>
</td></tr>
<tr><td><code id="MLE+2B20of+2B20some+2B20truncated+2B20distributions_+3A_a">a</code></td>
<td>

<p>The lower value at which the Cauchy distribution is truncated.
</p>
</td></tr>
<tr><td><code id="MLE+2B20of+2B20some+2B20truncated+2B20distributions_+3A_b">b</code></td>
<td>

<p>The upper value at which the Cauchy or the exponential distribution is truncated. For the exponential this must 
be greater than zero.
</p>
</td></tr>
<tr><td><code id="MLE+2B20of+2B20some+2B20truncated+2B20distributions_+3A_tol">tol</code></td>
<td>

<p>The tolerance value to terminate the fitting algorithm.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Maximum likelihood of some truncated distributions is performed.
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>iters</code></td>
<td>

<p>The number of iterations reuired by the Newton-Raphson algorithm.
</p>
</td></tr>
<tr><td><code>loglik</code></td>
<td>

<p>The log-likelihood.
</p>
</td></tr>
<tr><td><code>lambda</code></td>
<td>

<p>The <code class="reqn">\lambda</code> parameter in the exponential distribution.
</p>
</td></tr>
<tr><td><code>param</code></td>
<td>

<p>The location and scale parameters in the Cauchy distribution.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>. 
</p>


<h3>References</h3>

<p>David Olive (2018). Applied Robust Statistics (Chapter 4).
</p>
<p>http://lagrange.math.siu.edu/Olive/ol-bookp.htm
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+purka.mle">purka.mle</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- rnorm(500)
trunccauchy.mle(x, -1, 1)
</code></pre>

<hr>
<h2 id='MLE+20of+20the+20Cauchy+20and+20generalised+20normal+20distributions+20with+20zero+20location'>
MLE of the Cauchy and generalised normal distributions with zero location
</h2><span id='topic+cauchy0.mle'></span><span id='topic+gnormal0.mle'></span>

<h3>Description</h3>

<p>MLE of the Cauchy and generalised normal distributions with zero location.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>cauchy0.mle(x, tol = 1e-07) 
gnormal0.mle(x, tol = 1e-06)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MLE+2B20of+2B20the+2B20Cauchy+2B20and+2B20generalised+2B20normal+2B20distributions+2B20with+2B20zero+2B20location_+3A_x">x</code></td>
<td>

<p>A numerical vector with positive real numbers. 
</p>
</td></tr>
<tr><td><code id="MLE+2B20of+2B20the+2B20Cauchy+2B20and+2B20generalised+2B20normal+2B20distributions+2B20with+2B20zero+2B20location_+3A_tol">tol</code></td>
<td>

<p>The tolerance level up to which the maximisation stops set to 1e-07 by default. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Cauchy is the t distribution with 1 degree of freedom. The cauchy0.mle estimates the usual Cauchy 
distribution, over the real line, but assumes a zero location. The gnormal0.mle estimates the generalised 
normal distribution assuming a zero location. The generalised normal distribution is also known as the 
exponential power distribution or the generalized error distribution.
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>iters</code></td>
<td>

<p>The number of iterations required by the Newton-Raphson algorithm.
</p>
</td></tr>
<tr><td><code>loglik</code></td>
<td>

<p>The value of the maximised log-likelihood.
</p>
</td></tr>
<tr><td><code>scale</code></td>
<td>

<p>The estimated scale parameter of the Cauchy distribution.
</p>
</td></tr>
<tr><td><code>param</code></td>
<td>

<p>The estimated scale and shape parameters of the generalised normal distribution.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Do M.N. and Vetterli M. (2002). Wavelet-based Texture Retrieval Using Generalised Gaussian Density 
and Kullback-Leibler Distance. Transaction on Image Processing. 11(2): 146-158. 
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+censweibull.mle">censweibull.mle</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- rcauchy(150, 0, 2) 
cauchy0.mle(x)

x &lt;- rnorm(200)
gnormal0.mle(x)
</code></pre>

<hr>
<h2 id='MLE+20of+20the+20censored+20Weibull+20distribution'>
MLE of the censored Weibull distribution</h2><span id='topic+censweibull.mle'></span>

<h3>Description</h3>

<p>MLE of the censored Weibull distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>censweibull.mle(x, di, tol = 1e-07)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MLE+2B20of+2B20the+2B20censored+2B20Weibull+2B20distribution_+3A_x">x</code></td>
<td>

<p>A vector with positive valued data (zeros are not allowed).
</p>
</td></tr>
<tr><td><code id="MLE+2B20of+2B20the+2B20censored+2B20Weibull+2B20distribution_+3A_di">di</code></td>
<td>

<p>A vector of 0s (censored) and 1s (not censored) vales.
</p>
</td></tr>
<tr><td><code id="MLE+2B20of+2B20the+2B20censored+2B20Weibull+2B20distribution_+3A_tol">tol</code></td>
<td>

<p>The tolerance level up to which the maximisation stops; set to 1e-07 by default.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Instead of maximising the log-likelihood via a numerical optimiser we have used a Newton-Raphson algorithm which is faster. 
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>iters</code></td>
<td>
<p>The number of iterations required for the Newton-Raphson to converge.
</p>
</td></tr>
<tr><td><code>loglik</code></td>
<td>
<p>The value of the maximised log-likelihood.
</p>
</td></tr>
<tr><td><code>param</code></td>
<td>
<p>The vector of the parameters.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Fritz Scholz (1996). Maximum Likelihood Estimation for
Type I Censored Weibull Data Including Covariates. 
Technical report. ISSTECH-96-022, Boeing Information &amp; Support Services, P.O. Box 24346, MS-7L-22.
</p>
<p><a href="http://faculty.washington.edu/fscholz/Reports/weibcensmle.pdf">http://faculty.washington.edu/fscholz/Reports/weibcensmle.pdf</a>
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+km">km</a>, <a href="#topic+censpois.mle">censpois.mle</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- rweibull(300, 3, 6)
censweibull.mle(x, di = rep(1, 300))
di &lt;- rbinom(300, 1, 0.9)
censweibull.mle(x, di)
</code></pre>

<hr>
<h2 id='MLE+20of+20the+20gamma-Poisson+20distribution'>MLE of the gamma-Poisson distribution
</h2><span id='topic+gammapois.mle'></span>

<h3>Description</h3>

<p>MLE of the gamma-Poisson distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gammapois.mle(x, tol = 1e-07) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MLE+2B20of+2B20the+2B20gamma-Poisson+2B20distribution_+3A_x">x</code></td>
<td>

<p>A numerical vector with positive data and zeros.  
</p>
</td></tr>
<tr><td><code id="MLE+2B20of+2B20the+2B20gamma-Poisson+2B20distribution_+3A_tol">tol</code></td>
<td>

<p>The tolerance value to terminate the Newton-Raphson algorithm.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>MLE of the gamma-Poisson distribution is fitted. When the rate in the Poisson follows a gamma 
distribution with shape = r and scale <code class="reqn">\theta</code>, the resulting distribution is the gamm-Poisson. 
If the shape r is integer, the distribution is called negative binomial distribution. 
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>iters</code></td>
<td>

<p>The iterations required by the Newton-Raphson to estimate the parameters
of the distribution for the non zero data.
</p>
</td></tr>
<tr><td><code>loglik</code></td>
<td>

<p>The full log-likelihood of the model.
</p>
</td></tr>
<tr><td><code>param</code></td>
<td>

<p>The parameters of the model.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>. 
</p>


<h3>References</h3>

<p>Johnson Norman L., Kotz Samuel and Kemp Adrienne W. (1992). Univariate Discrete Distributions 
(2nd ed.). Wiley.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+zigamma.mle">zigamma.mle</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- rnbinom(200, 20, 0.7)
gammapois.mle(x)
</code></pre>

<hr>
<h2 id='MLE+20of+20the+20left+20censored+20Poisson+20distribution'>
MLE of the left censored Poisson distribution</h2><span id='topic+censpois.mle'></span>

<h3>Description</h3>

<p>MLE of the left censored Poisson distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>censpois.mle(x, tol = 1e-07)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MLE+2B20of+2B20the+2B20left+2B20censored+2B20Poisson+2B20distribution_+3A_x">x</code></td>
<td>

<p>A vector with positive valued data (zeros are not allowed).
</p>
</td></tr>
<tr><td><code id="MLE+2B20of+2B20the+2B20left+2B20censored+2B20Poisson+2B20distribution_+3A_tol">tol</code></td>
<td>

<p>The tolerance level up to which the maximisation stops; set to 1e-07 by default.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Instead of maximising the log-likelihood via a numerical optimiser we have used a Newton-Raphson algorithm which is faster. 
The lowest value in x is taken as the censored point. Values below are censored.
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>iters</code></td>
<td>
<p>The number of iterations required for the Newton-Raphson to converge.
</p>
</td></tr>
<tr><td><code>loglik</code></td>
<td>
<p>The value of the maximised log-likelihood.
</p>
</td></tr>
<tr><td><code>lambda</code></td>
<td>
<p>The estimated <code class="reqn">\lambda</code> parameter.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+km">km</a>, <a href="#topic+censweibull.mle">censweibull.mle</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

x1 &lt;- rpois(10000,15)
x &lt;- x1
x[x&lt;=10] = 10
mean(x)
censpois.mle(x)$lambda
</code></pre>

<hr>
<h2 id='MLE+20of+20the+20Purkayastha+20distribution'>
MLE of the Purkayastha distribution
</h2><span id='topic+purka.mle'></span>

<h3>Description</h3>

<p>MLE of the Purkayastha distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>purka.mle(x, tol = 1e-07)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MLE+2B20of+2B20the+2B20Purkayastha+2B20distribution_+3A_x">x</code></td>
<td>

<p>A numerical vector with data expressed in radians or a matrix with spherical data. 
</p>
</td></tr>
<tr><td><code id="MLE+2B20of+2B20the+2B20Purkayastha+2B20distribution_+3A_tol">tol</code></td>
<td>

<p>The tolerance value to terminate the Brent algorithm.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>MLE of the Purkayastha distribution is performed. 
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>theta</code></td>
<td>

<p>The median direction.
</p>
</td></tr>
<tr><td><code>alpha</code></td>
<td>

<p>The concentration parameter.
</p>
</td></tr>
<tr><td><code>loglik</code></td>
<td>

<p>The log-likelihood.
</p>
</td></tr>
<tr><td><code>alpha.sd</code></td>
<td>

<p>The standard error of the concentration parameter.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Purkayastha S. (1991).  A Rotationally Symmetric Directional Distribution: Obtained through Maximum 
Likelihood Characterization. The Indian Journal of Statistics, Series A, 53(1): 70-83
</p>
<p>Cabrera J. and Watson G. S. (1990). On a spherical median related distribution. 
Communications in Statistics-Theory and Methods, 19(6): 1973-1986.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+circ.cor1">circ.cor1</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- cbind( rnorm(100,1,1), rnorm(100, 2, 1) )
x &lt;- x / sqrt(rowSums(x^2))
purka.mle(x)
</code></pre>

<hr>
<h2 id='MLE+20of+20the+20zero+20inflated+20Gamma+20and+20Weibull+20distributions'>MLE of the zero inflated Gamma and Weibull distributions
</h2><span id='topic+zigamma.mle'></span><span id='topic+ziweibull.mle'></span>

<h3>Description</h3>

<p>MLE of the zero inflated Gamma and Weibull distributions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>zigamma.mle(x, tol = 1e-07)
ziweibull.mle(x, tol = 1e-07)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="MLE+2B20of+2B20the+2B20zero+2B20inflated+2B20Gamma+2B20and+2B20Weibull+2B20distributions_+3A_x">x</code></td>
<td>

<p>A numerical vector with positive data and zeros.  
</p>
</td></tr>
<tr><td><code id="MLE+2B20of+2B20the+2B20zero+2B20inflated+2B20Gamma+2B20and+2B20Weibull+2B20distributions_+3A_tol">tol</code></td>
<td>

<p>The tolerance value to terminate the Newton-Raphson algorithm.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>MLE of some zero inflated models is performed. 
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>iters</code></td>
<td>

<p>The iterations required by the Newton-Raphson to estimate the parameters
of the distribution for the non zero data.
</p>
</td></tr>
<tr><td><code>loglik</code></td>
<td>

<p>The full log-likelihood of the model.
</p>
</td></tr>
<tr><td><code>param</code></td>
<td>

<p>The parameters of the model.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Sandra Taylor and Katherine Pollard (2009). Hypothesis Tests for Point-Mass 
Mixture Data with Application to Omics Data with Many Zero Values. 
Statistical Applications in Geneticsand Molecular Biology, 8(1): 1&ndash;43.
</p>
<p>Kalimuthu Krishnamoorthy, Meesook Lee and Wang Xiao (2015). Likelihood ratio tests 
for comparing several gamma distributions. Environmetrics, 26(8):571-583.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+zigamma.reg">zigamma.reg</a>, <a href="#topic+gammapois.mle">gammapois.mle</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- rgamma(200, 4, 1)
x[sample(1:200, 20)] &lt;- 0
zigamma.mle(x)
</code></pre>

<hr>
<h2 id='Monte+20Carlo+20integration+20with+20a+20normal+20distribution'>
Monte Carlo Integration with a normal distribution
</h2><span id='topic+mci'></span>

<h3>Description</h3>

<p>Monte Carlo Integration with a normal distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mci(fun, R = 10^6)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Monte+2B20Carlo+2B20integration+2B20with+2B20a+2B20normal+2B20distribution_+3A_fun">fun</code></td>
<td>

<p>A function denoting the inside part of the expectation to be computed.
</p>
</td></tr>
<tr><td><code id="Monte+2B20Carlo+2B20integration+2B20with+2B20a+2B20normal+2B20distribution_+3A_r">R</code></td>
<td>

<p>The number of draws from the normal distribution.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>The result of the integral.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Morgan B. J. (2018). Elements of simulation. Chapman &amp; Hall/CRC.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+riag">riag</a>, <a href="#topic+rbeta1">rbeta1</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## compute the expectation of abs(x)
fun &lt;- function(x) abs(x)
mci(fun, R = 10^5)
a &lt;- function(x)  abs(x)  * dnorm(x)
integrate(a, -Inf, Inf)
</code></pre>

<hr>
<h2 id='Moran+27s+20I+20measure+20of+20spatial+20autocorrelation'>
Moran's I measure of spatial autocorrelation
</h2><span id='topic+moranI'></span>

<h3>Description</h3>

<p>Moran's I measure of spatial autocorrelation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>moranI(x, w, scaled = FALSE, R = 999)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Moran+2B27s+2B20I+2B20measure+2B20of+2B20spatial+2B20autocorrelation_+3A_x">x</code></td>
<td>

<p>A numerical vector with observations.
</p>
</td></tr>
<tr><td><code id="Moran+2B27s+2B20I+2B20measure+2B20of+2B20spatial+2B20autocorrelation_+3A_w">w</code></td>
<td>

<p>The inverse of a (symmetric) distance matrix. After computing the distance matrix, you invert all its elements and the elements 
which are zero (diagonal) and have become Inf. set them to 0. This is the w matrix the functions requires. If you want an extra step,
you can row standardise this matrix by dividing each row by its total. This will makw the rowsums equal to 1. 
</p>
</td></tr>
<tr><td><code id="Moran+2B27s+2B20I+2B20measure+2B20of+2B20spatial+2B20autocorrelation_+3A_scaled">scaled</code></td>
<td>

<p>If the matrix is row-standardised (all rowsums are equal to 1) then this is TRUE and FALSE otherwise.
</p>
</td></tr>
<tr><td><code id="Moran+2B27s+2B20I+2B20measure+2B20of+2B20spatial+2B20autocorrelation_+3A_r">R</code></td>
<td>

<p>The number of permutations to use in order to obtain the permutation based-pvalue. If R is 1 or less no permutation p-value is returned.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Moran' I index is a measure of spatial autocorrelation. that was proposed in 1950. Instead of computing an asymptotic 
p-value we compute a permutation based p-value utilizing the fast method of Chatzipantsiou et al. (2019).
</p>


<h3>Value</h3>

<p>A vector of two values, the Moran's I index and its permutation based p-value. If R is 1 or less no permutation p-value is returned, 
and the second element is &quot;NA&quot;. 
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Moran, P. A. P. (1950). Notes on Continuous Stochastic Phenomena. Biometrika. 37(1): 17-23. 
</p>
<p>Chatzipantsiou C., Dimitriadis M., Papadakis M. and Tsagris M. (2019). Extremely efficient permutation and bootstrap 
hypothesis tests using R. Journal of Modern Applied Statistical Methods (To appear).
<a href="https://arxiv.org/ftp/arxiv/papers/1806/1806.10947.pdf">https://arxiv.org/ftp/arxiv/papers/1806/1806.10947.pdf</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+censpois.mle">censpois.mle</a>, <a href="#topic+gammapois.mle">gammapois.mle</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- rnorm(50)
w &lt;- as.matrix( dist(iris[1:50, 1:4]) )
w &lt;- 1/w
diag(w) &lt;- 0
moranI(x, w, scaled = FALSE)
</code></pre>

<hr>
<h2 id='Multinomial+20regression'>
Multinomial regression
</h2><span id='topic+multinom.reg'></span>

<h3>Description</h3>

<p>Multinomial regression.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>multinom.reg(y, x, tol = 1e-07, maxiters = 100)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Multinomial+2B20regression_+3A_y">y</code></td>
<td>

<p>The response variable. A numerical or a factor type vector.
</p>
</td></tr>
<tr><td><code id="Multinomial+2B20regression_+3A_x">x</code></td>
<td>

<p>A matrix or a data.frame with the predictor variables.
</p>
</td></tr>
<tr><td><code id="Multinomial+2B20regression_+3A_tol">tol</code></td>
<td>

<p>The tolerance value to terminate the Newton-Raphson algorithm. 
</p>
</td></tr>
<tr><td><code id="Multinomial+2B20regression_+3A_maxiters">maxiters</code></td>
<td>

<p>The maximum number of iterations Newton-Raphson will perform.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>iters</code></td>
<td>

<p>The number of iterations required by the Newton-Raphson.
</p>
</td></tr>
<tr><td><code>loglik</code></td>
<td>

<p>The value of the maximised log-likelihood.
</p>
</td></tr>
<tr><td><code>be</code></td>
<td>

<p>A matrix with the estimated regression coefficients.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris and Stefanos Fafalios.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a> and 
Stefanos Fafalios <a href="mailto:stefanosfafalios@gmail.com">stefanosfafalios@gmail.com</a>.
</p>


<h3>References</h3>

<p>Bohning, D. (1992). Multinomial logistic regression algorithm. Annals of the 
Institute of Statistical Mathematics, 44(1): 197-200.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+logiquant.regs">logiquant.regs</a>, <a href="#topic+fbed.reg">fbed.reg</a> 
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
y &lt;- iris[, 5]
x &lt;- matrix( rnorm(150 * 2), ncol = 2 )
mod &lt;- multinom.reg(y, x)

</code></pre>

<hr>
<h2 id='Naive+20Bayes+20classifier+20for+20binary+20+28Bernoulli+29+20data'>
Naive Bayes classifier for binary Bernoulli data
</h2><span id='topic+bernoulli.nb'></span>

<h3>Description</h3>

<p>Naive Bayes classifier for binary (Bernoulli) data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bernoulli.nb(xnew = NULL, x, ina)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Naive+2B20Bayes+2B20classifier+2B20for+2B20binary+2B20+2B28Bernoulli+2B29+2B20data_+3A_xnew">xnew</code></td>
<td>

<p>A numerical matrix with new predictor variables whose group is to be predicted. 
Each column contains binary (0 or 1) data.
</p>
</td></tr>
<tr><td><code id="Naive+2B20Bayes+2B20classifier+2B20for+2B20binary+2B20+2B28Bernoulli+2B29+2B20data_+3A_x">x</code></td>
<td>

<p>A numerical matrix with observed predictor variables. Each column contains binary (0 or 1) data.
</p>
</td></tr>
<tr><td><code id="Naive+2B20Bayes+2B20classifier+2B20for+2B20binary+2B20+2B28Bernoulli+2B29+2B20data_+3A_ina">ina</code></td>
<td>

<p>A numerical vector with strictly positive numbers, i.e. 1,2,3 indicating the groups of the dataset. 
Alternatively this can be a factor variable.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Each column is supposed to contain binary data. Thus, for each column a Berboulli distributions 
is fitted. The product of the densities is the joint multivariate distribution.
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>pi</code></td>
<td>

<p>A matrix with the estimated probabilities of each group and variable.
</p>
</td></tr>
<tr><td><code>ni</code></td>
<td>

<p>The sample size of each group in the dataset. 
</p>
</td></tr>
<tr><td><code>est</code></td>
<td>

<p>The estimated group of the xnew observations. It returns a numerical value back regardless of the target 
variable being numerical as well or factor. Hence, it is suggested that you do \&quot;as.numeric(ina)\&quot; in order to 
see what is the predicted class of the new data. 
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+bernoullinb.pred">bernoullinb.pred</a>, <a href="#topic+nb.cv">nb.cv</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- matrix( rbinom(50 * 4, 1, 0.5), ncol = 4 )
ina &lt;- rbinom(50, 1, 0.5) + 1
a &lt;- bernoulli.nb(x, x, ina)
</code></pre>

<hr>
<h2 id='Naive+20Bayes+20classifiers'>
Naive Bayes classifiers
</h2><span id='topic+weibull.nb'></span><span id='topic+normlog.nb'></span><span id='topic+laplace.nb'></span><span id='topic+logitnorm.nb'></span><span id='topic+beta.nb'></span><span id='topic+cauchy.nb'></span>

<h3>Description</h3>

<p>Naive Bayes classifiers.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>weibull.nb(xnew = NULL, x, ina, tol = 1e-07)
normlog.nb(xnew = NULL, x, ina)
laplace.nb(xnew = NULL, x, ina)
logitnorm.nb(xnew = NULL, x, ina) 
beta.nb(xnew = NULL, x, ina) 
cauchy.nb(xnew = NULL, x, ina)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Naive+2B20Bayes+2B20classifiers_+3A_xnew">xnew</code></td>
<td>

<p>A numerical matrix with new predictor variables whose group is to be predicted. This is set to NUUL, as you might want just the model 
and not to predict the membership of new observations. 
For the normlog this contains positive numbers (greater than or equal to zero), but for the multinomial 
and Poisson cases, the matrix must contain integer valued numbers only. For the logistic normal (logitnorm.nb) and beta (beta.nb) 
the data must be strictly between 0 and 1.
</p>
</td></tr>
<tr><td><code id="Naive+2B20Bayes+2B20classifiers_+3A_x">x</code></td>
<td>

<p>A numerical matrix with the observed predictor variable values. For the Gaussian case (normlognb.nb) this 
contains positive numbers (greater than or equal to zero), but for the multinomial and Poisson cases, 
the matrix must contain integer valued numbers only. For the logistic normal (logitnorm.nb) and beta (beta.nb) 
the data must be strictly between 0 and 1.
</p>
</td></tr>
<tr><td><code id="Naive+2B20Bayes+2B20classifiers_+3A_ina">ina</code></td>
<td>

<p>A numerical vector with strictly positive numbers, i.e. 1,2,3 indicating the groups of the dataset. 
Alternatively this can be a factor variable.
</p>
</td></tr>
<tr><td><code id="Naive+2B20Bayes+2B20classifiers_+3A_tol">tol</code></td>
<td>

<p>The tolerance value to terminate the Newton-Raphson algorithm in the Weibull distribution.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Depending on the classifier a list including (the ni and est are common for all classifiers):
</p>
<table>
<tr><td><code>shape</code></td>
<td>

<p>A matrix with the shape parameters.
</p>
</td></tr>
<tr><td><code>scale</code></td>
<td>

<p>A matrix with the scale parameters.
</p>
</td></tr>
<tr><td><code>expmu</code></td>
<td>

<p>A matrix with the mean parameters.
</p>
</td></tr>
<tr><td><code>sigma</code></td>
<td>

<p>A matrix with the (MLE, hence biased) variance parameters.
</p>
</td></tr>
<tr><td><code>location</code></td>
<td>

<p>A matrix with the location parameters (medians).
</p>
</td></tr>
<tr><td><code>scale</code></td>
<td>

<p>A matrix with the scale parameters.
</p>
</td></tr>
<tr><td><code>mean</code></td>
<td>

<p>A matrix with the scale parameters.
</p>
</td></tr>
<tr><td><code>var</code></td>
<td>

<p>A matrix with the variance parameters.
</p>
</td></tr>
<tr><td><code>a</code></td>
<td>

<p>A matrix with the &quot;alpha&quot; parameters.
</p>
</td></tr>
<tr><td><code>b</code></td>
<td>

<p>A matrix with the &quot;beta&quot; parameters.
</p>
</td></tr>
<tr><td><code>ni</code></td>
<td>

<p>The sample size of each group in the dataset. 
</p>
</td></tr>
<tr><td><code>est</code></td>
<td>

<p>The estimated group of the xnew observations. It returns a numerical value back regardless of the target 
variable being numerical as well or factor. Hence, it is suggested that you do \&quot;as.numeric(ina)\&quot; in order to 
see what is the predicted class of the new data. 
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+weibullnb.pred">weibullnb.pred</a>, <a href="#topic+vm.nb">vm.nb</a>, <a href="#topic+nb.cv">nb.cv</a>, <a href="#topic+mle.lda">mle.lda</a>, <a href="#topic+big.knn">big.knn</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- matrix( rweibull( 100, 3, 4 ), ncol = 2 )
ina &lt;- rbinom(50, 1, 0.5) + 1
a &lt;- weibull.nb(x, x, ina)
</code></pre>

<hr>
<h2 id='Naive+20Bayes+20classifiers+20for+20circular+20data'>
Naive Bayes classifiers for directional data
</h2><span id='topic+vm.nb'></span><span id='topic+spml.nb'></span>

<h3>Description</h3>

<p>Naive Bayes classifiers for directional data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vm.nb(xnew = NULL, x, ina, tol = 1e-07)
spml.nb(xnew = NULL, x, ina, tol = 1e-07)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Naive+2B20Bayes+2B20classifiers+2B20for+2B20circular+2B20data_+3A_xnew">xnew</code></td>
<td>

<p>A numerical matrix with new predictor variables whose group is to be predicted. Each column refers to an angular variable.
</p>
</td></tr>
<tr><td><code id="Naive+2B20Bayes+2B20classifiers+2B20for+2B20circular+2B20data_+3A_x">x</code></td>
<td>

<p>A numerical matrix with observed predictor variables. Each column refers to an angular variable.
</p>
</td></tr>
<tr><td><code id="Naive+2B20Bayes+2B20classifiers+2B20for+2B20circular+2B20data_+3A_ina">ina</code></td>
<td>

<p>A numerical vector with strictly positive numbers, i.e. 1,2,3 indicating the groups of the dataset. 
Alternatively this can be a factor variable.
</p>
</td></tr>
<tr><td><code id="Naive+2B20Bayes+2B20classifiers+2B20for+2B20circular+2B20data_+3A_tol">tol</code></td>
<td>

<p>The tolerance value to terminate the Newton-Raphson algorithm.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Each column is supposed to contain angular measurements. Thus, for each column a von Mises distribution 
or an circular angular Gaussian distribution is fitted. The product of the densities is the joint multivariate
distribution.
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>mu</code></td>
<td>

<p>A matrix with the mean vectors expressed in radians.
</p>
</td></tr>
<tr><td><code>mu1</code></td>
<td>

<p>A matrix with the first set of mean vectors.
</p>
</td></tr>
<tr><td><code>mu2</code></td>
<td>

<p>A matrix with the second set of mean vectors.
</p>
</td></tr>
<tr><td><code>kappa</code></td>
<td>

<p>A matrix with the kappa parameters for the vonMises distribution or with the norm of the 
mean vectors for the circular angular Gaussian distribution.
</p>
</td></tr>
<tr><td><code>ni</code></td>
<td>

<p>The sample size of each group in the dataset. 
</p>
</td></tr>
<tr><td><code>est</code></td>
<td>

<p>The estimated group of the xnew observations. It returns a numerical value back regardless of the target 
variable being numerical as well or factor. Hence, it is suggested that you do \&quot;as.numeric(ina)\&quot; in order to 
see what is the predicted class of the new data. 
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+vmnb.pred">vmnb.pred</a>, <a href="#topic+weibull.nb">weibull.nb</a>, <a href="#topic+nb.cv">nb.cv</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- matrix( runif( 100, 0, 1 ), ncol = 2 )
ina &lt;- rbinom(50, 1, 0.5) + 1
a &lt;- vm.nb(x, x, ina)
</code></pre>

<hr>
<h2 id='Negative+20binomial+20regression'>
Negative binomial regression
</h2><span id='topic+negbin.reg'></span>

<h3>Description</h3>

<p>Negative binomial regression.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>negbin.reg(y, x, tol = 1e-07, maxiters = 100)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Negative+2B20binomial+2B20regression_+3A_y">y</code></td>
<td>

<p>The dependent variable, a numerical vector with integer valued numbers.
</p>
</td></tr>
<tr><td><code id="Negative+2B20binomial+2B20regression_+3A_x">x</code></td>
<td>

<p>A matrix or a data.frame with the indendent variables.
</p>
</td></tr>
<tr><td><code id="Negative+2B20binomial+2B20regression_+3A_tol">tol</code></td>
<td>

<p>The tolerance value required by the Newton-Raphson to stop.
</p>
</td></tr>
<tr><td><code id="Negative+2B20binomial+2B20regression_+3A_maxiters">maxiters</code></td>
<td>

<p>The maximum iterations allowed.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A negative binomial regression model is fitted. The standard errors of the regressions are not returned as we do not
compute the full Hessian matrix at each step of the Newton-Raphson.
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>be</code></td>
<td>

<p>The regression coefficients.
</p>
</td></tr>
<tr><td><code>loglik</code></td>
<td>

<p>The loglikelihood of the regression model.
</p>
</td></tr>
<tr><td><code>iters</code></td>
<td>

<p>The iterations required by the Newton-Raphson.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a> and 
Stefanos Fafalios <a href="mailto:stefanosfafalios@gmail.com">stefanosfafalios@gmail.com</a>.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+ztp.reg">ztp.reg</a>, <a href="#topic+binom.reg">binom.reg</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
y &lt;- rnbinom(100, 10, 0.7)
x &lt;- matrix( rnorm(100 * 3), ncol = 3 )
mod &lt;- negbin.reg(y, x)

</code></pre>

<hr>
<h2 id='Non+20linear+20least+20squares+20regression+20for+20percentages+20or+20proportions'>
Non linear least squares regression for percentages or proportions
</h2><span id='topic+propols.reg'></span>

<h3>Description</h3>

<p>Non linear least squares regression for percentages or proportions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>propols.reg(y, x, cov = FALSE, tol = 1e-07 ,maxiters = 100)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Non+2B20linear+2B20least+2B20squares+2B20regression+2B20for+2B20percentages+2B20or+2B20proportions_+3A_y">y</code></td>
<td>

<p>The dependent variable, a numerical vector with percentages or proporions, including 0s and or 1s.
</p>
</td></tr>
<tr><td><code id="Non+2B20linear+2B20least+2B20squares+2B20regression+2B20for+2B20percentages+2B20or+2B20proportions_+3A_x">x</code></td>
<td>

<p>A matrix with the indendent variables.
</p>
</td></tr>
<tr><td><code id="Non+2B20linear+2B20least+2B20squares+2B20regression+2B20for+2B20percentages+2B20or+2B20proportions_+3A_cov">cov</code></td>
<td>

<p>Should the sandwich covariance matrix and the standard errors be returned? If yes, set this equal to TRUE.
</p>
</td></tr>
<tr><td><code id="Non+2B20linear+2B20least+2B20squares+2B20regression+2B20for+2B20percentages+2B20or+2B20proportions_+3A_tol">tol</code></td>
<td>

<p>The tolerance value to terminate the Newton-Raphson algorithm. 
This is set to <code class="reqn">10^{-7}</code> by default.
</p>
</td></tr>
<tr><td><code id="Non+2B20linear+2B20least+2B20squares+2B20regression+2B20for+2B20percentages+2B20or+2B20proportions_+3A_maxiters">maxiters</code></td>
<td>

<p>The maximum number of iterations that can take place during the fitting.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The ordinary least squares between the observed and the fitted percentages is adopted 
as the objective function. This involves numerical optimization since the relationship 
is non-linear. There is no log-likelihood. This is the univariate version of the 
OLS regression for compositional data mentioned in Murteira and Ramalho (2016).
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>sse</code></td>
<td>

<p>The sum of squares of the raw residuals.
</p>
</td></tr>
<tr><td><code>be</code></td>
<td>

<p>The beta coefficients.
</p>
</td></tr>
<tr><td><code>seb</code></td>
<td>

<p>The sandwich standard errors of the beta coefficients, if the input argument argument was set to TRUE.
</p>
</td></tr>
<tr><td><code>covb</code></td>
<td>

<p>The sandwich covariance matrix of the beta coefficients, if the input argument argument was set to TRUE.
</p>
</td></tr>
<tr><td><code>iters</code></td>
<td>

<p>The number of iterations required by the Newton-Raphson algorithm.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Murteira, Jose MR, and Joaquim JS Ramalho 2016. Regression analysis of multivariate fractional data.
Econometric Reviews 35(4): 515-552.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+prophelling.reg">prophelling.reg</a>, <a href="#topic+simplex.mle">simplex.mle</a>, <a href="#topic+kumar.mle">kumar.mle</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
y &lt;- rbeta(150, 3, 4)
x &lt;- iris
a &lt;- propols.reg(y, x)

</code></pre>

<hr>
<h2 id='One+20sample+20bootstrap+20t-test+20for+20a+20vector'>
One sample bootstrap t-test for a vector
</h2><span id='topic+boot.ttest1'></span>

<h3>Description</h3>

<p>One sample bootstrap t-test for a vector.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>boot.ttest1(x, m, R = 999)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="One+2B20sample+2B20bootstrap+2B20t-test+2B20for+2B20a+2B20vector_+3A_x">x</code></td>
<td>

<p>A numerical vector with the data. 
</p>
</td></tr>
<tr><td><code id="One+2B20sample+2B20bootstrap+2B20t-test+2B20for+2B20a+2B20vector_+3A_m">m</code></td>
<td>

<p>The assumed mean value.
</p>
</td></tr>
<tr><td><code id="One+2B20sample+2B20bootstrap+2B20t-test+2B20for+2B20a+2B20vector_+3A_r">R</code></td>
<td>

<p>The number of bootstrap resamples to draw.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The usual one sample bootstrap t-test is implemented, only faster.
</p>


<h3>Value</h3>

<table>
<tr><td><code>res</code></td>
<td>

<p>A two valued vector with the test statistic and its p-value.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a> 
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+boot.student2">boot.student2</a>, <a href="#topic+perm.ttest2">perm.ttest2</a>, <a href="#topic+welch.tests">welch.tests</a>, <a href="#topic+jack.mean">jack.mean</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- rexp(30)
a &lt;- t.test(x, mu = 0)
b &lt;- boot.ttest1(x, 0)
</code></pre>

<hr>
<h2 id='Orthogonal+20matching+20pursuit+20variable+20selection'>
Orthogonal matching variable selection
</h2><span id='topic+omp2'></span>

<h3>Description</h3>

<p>Orthogonal matching variable selection.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>omp2(y, x, xstand = TRUE, tol = qchisq(0.95, 1), type = "gamma" ) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Orthogonal+2B20matching+2B20pursuit+2B20variable+2B20selection_+3A_y">y</code></td>
<td>

<p>The response variable, a numeric vector. For &quot;omp&quot; this can be either a vector with discrete 
(count) data, 0 and 1, non negative values, strictly positive or a factor (categorical) variable.
</p>
</td></tr>
<tr><td><code id="Orthogonal+2B20matching+2B20pursuit+2B20variable+2B20selection_+3A_x">x</code></td>
<td>

<p>A matrix with the data, where the rows denote the observations and the columns are the variables. 
</p>
</td></tr>
<tr><td><code id="Orthogonal+2B20matching+2B20pursuit+2B20variable+2B20selection_+3A_xstand">xstand</code></td>
<td>

<p>If this is TRUE the independent variables are standardised. 
</p>
</td></tr>
<tr><td><code id="Orthogonal+2B20matching+2B20pursuit+2B20variable+2B20selection_+3A_tol">tol</code></td>
<td>

<p>The tolerance value to terminate the algorithm. This is the change in the criterion value 
between two successive steps. For &quot;ompr&quot; the default value is 2 because the default method
is &quot;BIC&quot;. The default value is the 95% quantile of the <code class="reqn">\chi^2</code> distribution.
</p>
</td></tr>
<tr><td><code id="Orthogonal+2B20matching+2B20pursuit+2B20variable+2B20selection_+3A_type">type</code></td>
<td>

<p>This denotes the parametric model to be used each time. It depends upon the nature of y. 
The possible values are &quot;gamma&quot;, &quot;negbin&quot;, or &quot;multinomial&quot;.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is the continuation of the &quot;omp&quot; function of the Rfast. We added some more regression models. 
The &quot;gamma&quot; and the &quot;multinomial&quot; models have now been implemented in C++.
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>runtime</code></td>
<td>

<p>The runtime of the algorithm.
</p>
</td></tr>
<tr><td><code>info</code></td>
<td>

<p>A matrix with two columns. The selected variable(s) and the criterion value at every step. 
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Pati Y. C., Rezaiifar R. and Krishnaprasad P. S. (1993). Orthogonal matching pursuit: 
Recursive function approximation with applications to wavelet decomposition. 
In Signals, Systems and Computers. 1993 Conference Record of The Twenty-Seventh Asilomar Conference on. IEEE.
</p>
<p>Mazin Abdulrasool Hameed (2012). Comparative analysis of orthogonal matching pursuit and least angle regression. MSc thesis, Michigan State University.
https://www.google.gr/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;ved=0ahUKEwik9P3Yto7XAhUiCZoKHQ8XDr8QFgglMAA&amp;url=https
</p>
<p>Lozano A., Swirszcz G. and Abe N. (2011). Group orthogonal matching pursuit for logistic regression. 
In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics.
</p>
<p>The <code class="reqn">\gamma</code>-OMP algorithm for feature selection with application to gene expression data. 
IEEE/ACM Transactions on Computational Biology and Bioinformatics, 19(2): 1214-1224. 
https://arxiv.org/pdf/2004.00281.pdf
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+mmpc2">mmpc2</a>, <a href="#topic+pc.sel">pc.sel</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- matrix( rnorm(100 * 50), ncol = 50 )
y &lt;- rgamma(100, 4, 1)
a &lt;- omp2(y, x)
a
x &lt;- NULL
</code></pre>

<hr>
<h2 id='Parametric+20and+20non-parametric+20bootstrap+20for+20linear+20regression+20model'>
Parametric and non-parametric bootstrap for linear regression model
</h2><span id='topic+lm.parboot'></span><span id='topic+lm.nonparboot'></span>

<h3>Description</h3>

<p>Parametric and non-parametric bootstrap for linear regression model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lm.parboot(x, y, R = 1000)
lm.nonparboot(x, y, R = 1000)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Parametric+2B20and+2B20non-parametric+2B20bootstrap+2B20for+2B20linear+2B20regression+2B20model_+3A_x">x</code></td>
<td>

<p>The predictor variables, a vector or a matrix or a data frame. 
</p>
</td></tr>
<tr><td><code id="Parametric+2B20and+2B20non-parametric+2B20bootstrap+2B20for+2B20linear+2B20regression+2B20model_+3A_y">y</code></td>
<td>

<p>The response variable, a numerical vector with data.
</p>
</td></tr>
<tr><td><code id="Parametric+2B20and+2B20non-parametric+2B20bootstrap+2B20for+2B20linear+2B20regression+2B20model_+3A_r">R</code></td>
<td>

<p>The number of parametric bootstrap replications to perform.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>An efficient implementation of parametric or non-parametric bootstrapping the residuals for linear models is provided.
</p>


<h3>Value</h3>

<p>A matrix with R columns and rows equal to the number of the regression parameters. Each column contains the
set of a bootstrap beta regression coefficients.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Efron Bradley and Robert J. Tibshirani (1993). An introduction to the bootstrap. New York: Chapman &amp; Hall/CRC.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+lm.drop1">lm.drop1</a>, <a href="#topic+leverage">leverage</a>, <a href="#topic+pc.sel">pc.sel</a>, <a href="#topic+mmpc">mmpc</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
y &lt;- rnorm(50)
x &lt;- matrix( rnorm( 50 * 2), ncol = 2 )
a &lt;- lm.parboot(x, y, 500)

</code></pre>

<hr>
<h2 id='Permutation+20t-test+20for+202+20independent+20samples'>
Permutation t-test for 2 independent samples
</h2><span id='topic+perm.ttest2'></span>

<h3>Description</h3>

<p>Permutation t-test for 2 independent samples.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>perm.ttest2(x, y, B = 999)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Permutation+2B20t-test+2B20for+2B202+2B20independent+2B20samples_+3A_x">x</code></td>
<td>

<p>A numerical vector with the data.
</p>
</td></tr>
<tr><td><code id="Permutation+2B20t-test+2B20for+2B202+2B20independent+2B20samples_+3A_y">y</code></td>
<td>

<p>A numerical vector with the data.
</p>
</td></tr>
<tr><td><code id="Permutation+2B20t-test+2B20for+2B202+2B20independent+2B20samples_+3A_b">B</code></td>
<td>

<p>The number of permutations to perform.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The usual permutation based p-value is computed.
</p>


<h3>Value</h3>

<p>A vector with the test statistic and the permutation based p-value. 
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Good P. I. (2005). Permutation, parametric and bootstrap tests of hypotheses: 
a practical guide to resampling methods for testing hypotheses. Springer 3rd Edition.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+jack.mean">jack.mean</a>, <a href="#topic+trim.mean">trim.mean</a>, <a href="#topic+moranI">moranI</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- rexp(30, 4)
y &lt;- rbeta(30, 2.5, 7.5)
perm.ttest2(x, y, 999)
</code></pre>

<hr>
<h2 id='Prediction+20with+20naive+20Bayes+20classifier+20for+20binary+20+28Bernoulli+29+20data'>
Prediction with naive Bayes classifier for binary (Bernoulli) data
</h2><span id='topic+bernoullinb.pred'></span>

<h3>Description</h3>

<p>Prediction with naive Bayes classifier for binary (Bernoulli) data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bernoullinb.pred(xnew, pi, ni)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Prediction+2B20with+2B20naive+2B20Bayes+2B20classifier+2B20for+2B20binary+2B20+2B28Bernoulli+2B29+2B20data_+3A_xnew">xnew</code></td>
<td>

<p>A numerical matrix with new predictor variables whose group is to be predicted. 
Each column refers to an angular variable.
</p>
</td></tr>
<tr><td><code id="Prediction+2B20with+2B20naive+2B20Bayes+2B20classifier+2B20for+2B20binary+2B20+2B28Bernoulli+2B29+2B20data_+3A_pi">pi</code></td>
<td>

<p>A matrix with the estimated probabilities of each group.
</p>
</td></tr>
<tr><td><code id="Prediction+2B20with+2B20naive+2B20Bayes+2B20classifier+2B20for+2B20binary+2B20+2B28Bernoulli+2B29+2B20data_+3A_ni">ni</code></td>
<td>

<p>The sample size of each group in the dataset. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Each column is supposed to contain binary data. Thus, for each column a Berboulli distributions 
is fitted. The product of the densities is the joint multivariate distribution.
</p>


<h3>Value</h3>

<p>A numerical vector with 1, 2, ... denoting the predicted group. 
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+bernoulli.nb">bernoulli.nb</a>, <a href="#topic+nb.cv">nb.cv</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- matrix( rbinom(50 * 4, 1, 0.5), ncol = 4 )
ina &lt;- rbinom(50, 1, 0.5) + 1
a &lt;- bernoulli.nb(x, x, ina)
</code></pre>

<hr>
<h2 id='Prediction+20with+20some+20naive+20Bayes+20classifiers'>
Prediction with some naive Bayes classifiers
</h2><span id='topic+weibullnb.pred'></span><span id='topic+normlognb.pred'></span><span id='topic+laplacenb.pred'></span><span id='topic+logitnormnb.pred'></span><span id='topic+betanb.pred'></span><span id='topic+cauchynb.pred'></span>

<h3>Description</h3>

<p>Prediction with some naive Bayes classifiers.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>weibullnb.pred(xnew, shape, scale, ni)
normlognb.pred(xnew, expmu, sigma, ni)
laplacenb.pred(xnew, location, scale, ni)
logitnormnb.pred(xnew, m, s, ni) 
betanb.pred(xnew, a, b, ni)
cauchynb.pred(xnew, location, scale, ni)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Prediction+2B20with+2B20some+2B20naive+2B20Bayes+2B20classifiers_+3A_xnew">xnew</code></td>
<td>

<p>A numerical matrix with new predictor variables whose group is to be predicted. For the Gaussian naive Bayes, 
this is set to NUUL, as you might want just the model and not to predict the membership of new observations. 
For the Gaussian case this contains positive numbers (greater than or equal to zero), but for the multinomial 
and Poisson cases, the matrix must contain integer valued numbers only. For the logistic normal (logitnormnb.pred) 
the data must be percentages strictly between 0 and 1.
</p>
</td></tr>
<tr><td><code id="Prediction+2B20with+2B20some+2B20naive+2B20Bayes+2B20classifiers_+3A_shape">shape</code></td>
<td>

<p>A matrix with the group shape parameters. Each row corresponds to a group.
</p>
</td></tr>
<tr><td><code id="Prediction+2B20with+2B20some+2B20naive+2B20Bayes+2B20classifiers_+3A_scale">scale</code></td>
<td>

<p>A matrix with the group scale parameters of the Laplace or the Cauchy distribution. Each row corresponds to a group.
</p>
</td></tr>
<tr><td><code id="Prediction+2B20with+2B20some+2B20naive+2B20Bayes+2B20classifiers_+3A_expmu">expmu</code></td>
<td>

<p>A matrix with the group mean parameters. Each row corresponds to a group.
</p>
</td></tr>
<tr><td><code id="Prediction+2B20with+2B20some+2B20naive+2B20Bayes+2B20classifiers_+3A_m">m</code></td>
<td>

<p>A matrix with the group mean parameters. Each row corresponds to a group.
</p>
</td></tr>
<tr><td><code id="Prediction+2B20with+2B20some+2B20naive+2B20Bayes+2B20classifiers_+3A_sigma">sigma</code></td>
<td>

<p>A matrix with the group (MLE, hence biased) variance parameters. Each row corresponds to a group.
</p>
</td></tr>
<tr><td><code id="Prediction+2B20with+2B20some+2B20naive+2B20Bayes+2B20classifiers_+3A_s">s</code></td>
<td>

<p>A matrix with the group MLE variance parameters. Each row corresponds to a group.
</p>
</td></tr>
<tr><td><code id="Prediction+2B20with+2B20some+2B20naive+2B20Bayes+2B20classifiers_+3A_location">location</code></td>
<td>

<p>A matrix with the group location parameters of the Laplace or of the Cauchy distribution. Each row corresponds to a group.
</p>
</td></tr>
<tr><td><code id="Prediction+2B20with+2B20some+2B20naive+2B20Bayes+2B20classifiers_+3A_a">a</code></td>
<td>

<p>A matrix with the group &quot;alpha&quot; parameters of the beta distribution. Each row corresponds to a group.
</p>
</td></tr>
<tr><td><code id="Prediction+2B20with+2B20some+2B20naive+2B20Bayes+2B20classifiers_+3A_b">b</code></td>
<td>

<p>A matrix with the group &quot;beta&quot; parameters of the beta distribution. Each row corresponds to a group.
</p>
</td></tr>
<tr><td><code id="Prediction+2B20with+2B20some+2B20naive+2B20Bayes+2B20classifiers_+3A_ni">ni</code></td>
<td>

<p>A vector with the frequencies of each group.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A numerical vector with 1, 2, ... denoting the predicted group. 
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+weibull.nb">weibull.nb</a>, <a href="#topic+vmnb.pred">vmnb.pred</a>, <a href="#topic+nb.cv">nb.cv</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- matrix( rweibull( 100, 3, 4 ), ncol = 2 )
ina &lt;- rbinom(50, 1, 0.5) + 1
a &lt;- weibull.nb(x, x, ina)
est &lt;- weibullnb.pred(x, a$shape, a$scale, a$ni)
table(ina, est)
</code></pre>

<hr>
<h2 id='Prediction+20with+20some+20naive+20Bayes+20classifiers+20for+20circular+20data'>
Prediction with some naive Bayes classifiers for circular data
</h2><span id='topic+vmnb.pred'></span><span id='topic+spmlnb.pred'></span>

<h3>Description</h3>

<p>Prediction with some naive Bayes classifiers for circular data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vmnb.pred(xnew, mu, kappa, ni)
spmlnb.pred(xnew, mu1, mu2, ni)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Prediction+2B20with+2B20some+2B20naive+2B20Bayes+2B20classifiers+2B20for+2B20circular+2B20data_+3A_xnew">xnew</code></td>
<td>

<p>A numerical matrix with new predictor variables whose group is to be predicted. 
Each column refers to an angular variable.
</p>
</td></tr>
<tr><td><code id="Prediction+2B20with+2B20some+2B20naive+2B20Bayes+2B20classifiers+2B20for+2B20circular+2B20data_+3A_mu">mu</code></td>
<td>

<p>A matrix with the mean vectors expressed in radians.
</p>
</td></tr>
<tr><td><code id="Prediction+2B20with+2B20some+2B20naive+2B20Bayes+2B20classifiers+2B20for+2B20circular+2B20data_+3A_mu1">mu1</code></td>
<td>

<p>A matrix with the first set of mean vectors.
</p>
</td></tr>
<tr><td><code id="Prediction+2B20with+2B20some+2B20naive+2B20Bayes+2B20classifiers+2B20for+2B20circular+2B20data_+3A_mu2">mu2</code></td>
<td>

<p>A matrix with the second set of mean vectors.
</p>
</td></tr>
<tr><td><code id="Prediction+2B20with+2B20some+2B20naive+2B20Bayes+2B20classifiers+2B20for+2B20circular+2B20data_+3A_kappa">kappa</code></td>
<td>

<p>A matrix with the kappa parameters for the vonMises distribution or with the norm of the 
mean vectors for the circular angular Gaussian distribution.
</p>
</td></tr>
<tr><td><code id="Prediction+2B20with+2B20some+2B20naive+2B20Bayes+2B20classifiers+2B20for+2B20circular+2B20data_+3A_ni">ni</code></td>
<td>

<p>The sample size of each group in the dataset. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Each column is supposed to contain angular measurements. Thus, for each column a von Mises distribution 
or an circular angular Gaussian distribution is fitted. The product of the densities is the joint multivariate
distribution.
</p>


<h3>Value</h3>

<p>A numerical vector with 1, 2, ... denoting the predicted group. 
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+vm.nb">vm.nb</a>, <a href="#topic+weibullnb.pred">weibullnb.pred</a>, <a href="#topic+nb.cv">nb.cv</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- matrix( runif( 100, 0, 1 ), ncol = 2 )
ina &lt;- rbinom(50, 1, 0.5) + 1
a &lt;- vm.nb(x, x, ina)
a2 &lt;- vmnb.pred(x, a$mu, a$kappa, a$ni)
</code></pre>

<hr>
<h2 id='Principal+20component+20analysis'>
Principal component analysis
</h2><span id='topic+pca'></span>

<h3>Description</h3>

<p>Principal component analysis.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pca(x, center = TRUE, scale = TRUE, k = NULL, vectors = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Principal+2B20component+2B20analysis_+3A_x">x</code></td>
<td>

<p>A numerical <code class="reqn">n \times p</code> matrix with data where the rows are the observations and the columns are the variables.
</p>
</td></tr>
<tr><td><code id="Principal+2B20component+2B20analysis_+3A_center">center</code></td>
<td>

<p>Do you want your data centered? TRUE or FALSE.
</p>
</td></tr>
<tr><td><code id="Principal+2B20component+2B20analysis_+3A_scale">scale</code></td>
<td>

<p>Do you want each of your variables scaled, i.e. to have unit variance? TRUE or FALSE.
</p>
</td></tr>
<tr><td><code id="Principal+2B20component+2B20analysis_+3A_k">k</code></td>
<td>

<p>If you want a specific number of eigenvalues and eigenvectors set it here, otherwise all 
eigenvalues (and eigenvectors if requested) will be returned.
</p>
</td></tr>
<tr><td><code id="Principal+2B20component+2B20analysis_+3A_vectors">vectors</code></td>
<td>

<p>Do you want the eigenvectors be returned? By dafault this is FALSE.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function is a faster version of R's prcomp.
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>values</code></td>
<td>

<p>The eigenvalues.
</p>
</td></tr>
<tr><td><code>vectors</code></td>
<td>

<p>The eigenvectors.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+reg.mle.lda">reg.mle.lda</a> 
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- matrix( rnorm(1000 * 20 ), ncol = 20)
a &lt;- pca(x)
x &lt;- NULL
</code></pre>

<hr>
<h2 id='Principal+20components+20regression'>
Principal components regression
</h2><span id='topic+pcr'></span>

<h3>Description</h3>

<p>Principal components regression.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pcr(y, x, k = 1, xnew = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Principal+2B20components+2B20regression_+3A_y">y</code></td>
<td>

<p>A real values vector.
</p>
</td></tr>
<tr><td><code id="Principal+2B20components+2B20regression_+3A_x">x</code></td>
<td>

<p>A matrix with the predictor variable(s), they have to be continuous.
</p>
</td></tr>
<tr><td><code id="Principal+2B20components+2B20regression_+3A_k">k</code></td>
<td>

<p>The number of principal components to use. This can be a single number or a vector 
starting from 1. In the second case you get results for the sequence of principal
components.
</p>
</td></tr>
<tr><td><code id="Principal+2B20components+2B20regression_+3A_xnew">xnew</code></td>
<td>

<p>If you have new data use it, otherwise leave it NULL.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The principal components of the cross product of the independent variables are 
obtained and classical regression is performed. 
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>be</code></td>
<td>

<p>The beta coefficients of the predictor variables computed via the principcal components.
</p>
</td></tr>
<tr><td><code>per</code></td>
<td>

<p>The percentage of variance of the cross product of the independent variables explained by the k components.
</p>
</td></tr>
<tr><td><code>vec</code></td>
<td>

<p>The principal components, the loadings.
</p>
</td></tr>
<tr><td><code>est</code></td>
<td>

<p>The fitted or the predicted values (if xnew is not NULL).
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Jolliffe I.T. (2002). Principal Component Analysis.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+pca">pca</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>y &lt;- as.vector(iris[, 1])
x &lt;- as.matrix(iris[, 2:4])
mod1 &lt;- pcr(y, x, 1)
mod2 &lt;- pcr(y, x, 2)
mod &lt;- pcr(y, x, k = 1:3)  ## all results at once
</code></pre>

<hr>
<h2 id='Random+20effects+20and+20weighted+20least+20squares+20meta+20analysis'>
Random effects and weighted least squares meta analysis
</h2><span id='topic+refmeta'></span><span id='topic+wlsmeta'></span>

<h3>Description</h3>

<p>Random effects and weighted least squares meta analysis.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>refmeta(yi, vi, tol = 1e-07) 
wlsmeta(yi, vi) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Random+2B20effects+2B20and+2B20weighted+2B20least+2B20squares+2B20meta+2B20analysis_+3A_yi">yi</code></td>
<td>

<p>The observations.
</p>
</td></tr>
<tr><td><code id="Random+2B20effects+2B20and+2B20weighted+2B20least+2B20squares+2B20meta+2B20analysis_+3A_vi">vi</code></td>
<td>

<p>The variances of the observations.
</p>
</td></tr>
<tr><td><code id="Random+2B20effects+2B20and+2B20weighted+2B20least+2B20squares+2B20meta+2B20analysis_+3A_tol">tol</code></td>
<td>

<p>The toleranve value to terminate Brent's algorithm.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The refmeta command performs random effects estimation, via restricted maximum 
likelihood estimation (REML), of the common mean. The wlsmeta command implements 
weighted least squares (WLS) meta analysis. See references for this.
</p>


<h3>Value</h3>

<p>A vector with many elements. The fixed effects mean estimate, the <code class="reqn">\bar{v}</code> 
estimate, the <code class="reqn">I^2</code>, the <code class="reqn">H^2</code>, the Q test statistic and it's p-value,
the <code class="reqn">\tau^2</code> estimate and the random effects mean estimate.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Annamaria Guolo and Cristiano Varin (2017). 
Random-effects meta-analysis: The number of studies matters. 
Statistical Methods in Medical Research, 26(3): 1500-1518.
</p>
<p>Stanley T. D. and Doucouliagos H. (2015). 
Neither fixed nor random: weighted least squares meta-analysis. 
Statistics in Medicine, 34(13): 2116-2127.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+bic.regs">bic.regs</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>y &lt;- rnorm(30)
vi &lt;- rexp(30, 3)
refmeta(y, vi)
wlsmeta(y, vi)
</code></pre>

<hr>
<h2 id='Random+20integer+20values+20simulation'>
Random integer values simulation
</h2><span id='topic+Sample.int'></span><span id='topic+Sample'></span>

<h3>Description</h3>

<p>Random integer values simulation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Sample.int(n, size = n, replace = FALSE) 
Sample(x, size, replace = FALSE) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Random+2B20integer+2B20values+2B20simulation_+3A_x">x</code></td>
<td>

<p>A numeric vector for sampling.
</p>
</td></tr>
<tr><td><code id="Random+2B20integer+2B20values+2B20simulation_+3A_n">n</code></td>
<td>

<p>This must be an integer value. The function will then draw random integer values from 1:n. 
</p>
</td></tr>
<tr><td><code id="Random+2B20integer+2B20values+2B20simulation_+3A_size">size</code></td>
<td>

<p>The number of integer values to sample.
</p>
</td></tr>
<tr><td><code id="Random+2B20integer+2B20values+2B20simulation_+3A_replace">replace</code></td>
<td>

<p>Do you want to sample with replacement? If yes, set this equal to TRUE.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function does the same job, up to some level, with R's built-in fuction <code><a href="base.html#topic+sample.int">sample.int</a></code>. 
</p>


<h3>Value</h3>

<p>A vector with integer values.
</p>


<h3>Author(s)</h3>

<p>Manos Papadakis.
</p>
<p>R implementation: Manos Papadakis &lt;papadakm95@gmail.com&gt;.
R documentation: Michail Tsagris &lt;mtsagris@yahoo.gr&gt;.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Runif">Runif</a>, <a href="#topic+rbeta1">rbeta1</a>, <a href="#topic+riag">riag</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- Sample.int(10, 1000, replace = TRUE)
Sample(x,length(x))
</code></pre>

<hr>
<h2 id='Random+20values+20generation+20from+20a+20Be+28a+2C+201+29+20distribution'>
Random values generation from a Be(a, 1) distribution
</h2><span id='topic+rbeta1'></span>

<h3>Description</h3>

<p>Random values generation from a Be(a, 1) distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rbeta1(n, a)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Random+2B20values+2B20generation+2B20from+2B20a+2B20Be+2B28a+2B2C+2B201+2B29+2B20distribution_+3A_n">n</code></td>
<td>

<p>The sample size, a numerical value.
</p>
</td></tr>
<tr><td><code id="Random+2B20values+2B20generation+2B20from+2B20a+2B20Be+2B28a+2B2C+2B201+2B29+2B20distribution_+3A_a">a</code></td>
<td>

<p>The shape parameter of the beta distribution. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The function genrates random values from a Be(a, 1) distribution.
</p>


<h3>Value</h3>

<p>A vector with the simulated data.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+kumar.mle">kumar.mle</a>, <a href="#topic+simplex.mle">simplex.mle</a>, <a href="#topic+collogitnorm.mle">collogitnorm.mle</a>, <a href="#topic+propols.reg">propols.reg</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- rbeta1(100, 3) 
</code></pre>

<hr>
<h2 id='Random+20values+20simulation+20from+20the+20uniform+20distribution'>
Random values simulation from the uniform distribution
</h2><span id='topic+Runif'></span>

<h3>Description</h3>

<p>Random values simulation from the uniform distribution.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>Runif(n, min = 0, max = 1) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Random+2B20values+2B20simulation+2B20from+2B20the+2B20uniform+2B20distribution_+3A_n">n</code></td>
<td>

<p>The number of values to generate.
</p>
</td></tr>
<tr><td><code id="Random+2B20values+2B20simulation+2B20from+2B20the+2B20uniform+2B20distribution_+3A_min">min</code></td>
<td>

<p>The lower value of the uniform distribution.
</p>
</td></tr>
<tr><td><code id="Random+2B20values+2B20simulation+2B20from+2B20the+2B20uniform+2B20distribution_+3A_max">max</code></td>
<td>

<p>The upper value of the uniform distribution.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function does the same job as R's built-in function <code><a href="stats.html#topic+runif">runif</a></code>.
</p>


<h3>Value</h3>

<p>A vector with simulated values.
</p>


<h3>Author(s)</h3>

<p>Manos Papadakis.
</p>
<p>R implementation and documentation: Manos Papadakis &lt;papadakm95@gmail.com&gt;.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+Sample.int">Sample.int</a>, <a href="#topic+rbeta1">rbeta1</a>, <a href="#topic+riag">riag</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- Runif(1000, 0, 1)
</code></pre>

<hr>
<h2 id='Regularised+20maximum+20likelihood+20linear+20discriminant+20analysis'>Regularised maximum likelihood linear discriminant analysis
</h2><span id='topic+reg.mle.lda'></span>

<h3>Description</h3>

<p>Regularised maximum likelihood linear discriminant analysis.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>reg.mle.lda(xnew, x, ina, lambda)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Regularised+2B20maximum+2B20likelihood+2B20linear+2B20discriminant+2B20analysis_+3A_xnew">xnew</code></td>
<td>

<p>A numerical vector or a matrix with the new observations, continuous data.  
</p>
</td></tr>
<tr><td><code id="Regularised+2B20maximum+2B20likelihood+2B20linear+2B20discriminant+2B20analysis_+3A_x">x</code></td>
<td>

<p>A matrix with numerical data.
</p>
</td></tr>
<tr><td><code id="Regularised+2B20maximum+2B20likelihood+2B20linear+2B20discriminant+2B20analysis_+3A_ina">ina</code></td>
<td>

<p>A numerical vector or factor with consecutive numbers indicating the group to which each
observation belongs to.
</p>
</td></tr>
<tr><td><code id="Regularised+2B20maximum+2B20likelihood+2B20linear+2B20discriminant+2B20analysis_+3A_lambda">lambda</code></td>
<td>

<p>A vector of regularization values <code class="reqn">\lambda</code> such as (0, 0.1, 0.2,...). 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Regularised maximum likelihood linear discriminant analysis is performed. The function is not extremely fast, yet is pretty fast.
</p>


<h3>Value</h3>

<p>A matrix with the predicted group of each observation in &quot;xnew&quot;. 
Every column corresponds to a <code class="reqn">\lambda</code> value. If you have just on value of <code class="reqn">\lambda</code>, then 
you will have one column only.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+regmlelda.cv">regmlelda.cv</a> <a href="#topic+mle.lda">mle.lda</a>, <a href="#topic+fisher.da">fisher.da</a>, <a href="#topic+big.knn">big.knn</a>, <a href="#topic+weibull.nb">weibull.nb</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- as.matrix(iris[, 1:4])
ina &lt;- iris[, 5]
a &lt;- reg.mle.lda(x, x, ina, lambda = seq(0, 1, by = 0.1) )
</code></pre>

<hr>
<h2 id='Repeated+20measures+20ANOVA+20+28univariate+20data+29+20using+20Hotelling+27s+20T2+20test'>
Repeated measures ANOVA (univariate data) using Hotelling's <code class="reqn">T^2</code> test
</h2><span id='topic+rm.hotel'></span>

<h3>Description</h3>

<p>Repeated measures ANOVA (univariate data) using Hotelling's <code class="reqn">T^2</code> test.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rm.hotel(x, a = 0.05)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Repeated+2B20measures+2B20ANOVA+2B20+2B28univariate+2B20data+2B29+2B20using+2B20Hotelling+2B27s+2B20T2+2B20test_+3A_x">x</code></td>
<td>

<p>A numerical matrix with the repeated measurements. Each column contains the 
values of the repeated measurements.
</p>
</td></tr>
<tr><td><code id="Repeated+2B20measures+2B20ANOVA+2B20+2B28univariate+2B20data+2B29+2B20using+2B20Hotelling+2B27s+2B20T2+2B20test_+3A_a">a</code></td>
<td>

<p>The level of significance, default value is equal to 0.05.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a multivariate test for the equality of means of repeated measurements. 
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>m</code></td>
<td>

<p>The mean vector.
</p>
</td></tr>
<tr><td><code>result</code></td>
<td>

<p>A vector with the test statistic value, it's associated p-value, the numerator and denominator
degrees of freedom and the critical value.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+sp.logiregs">sp.logiregs</a>, <a href="#topic+pc.sel">pc.sel</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- as.matrix(iris[, 1:4]) ## assume they are repeated measurements
rm.hotel(x)
</code></pre>

<hr>
<h2 id='Rfast2-package'>
Really fast R functions
</h2><span id='topic+Rfast2-package'></span>

<h3>Description</h3>

<p>A collection of Rfast2 functions for data analysis. 
Note 1: The vast majority of the functions accept matrices only, not data.frames. 
Note 2: Do not have matrices or vectors with have missing data (i.e NAs). We do no check about them and C++ internally transforms them into zeros (0), so you may get wrong results.
Note 3: In general, make sure you give the correct input, in order to get the correct output. We do no checks and this is one of the many reasons we are fast.   
</p>


<h3>Details</h3>


<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> Rfast2</td>
</tr>
<tr>
 <td style="text-align: left;">
Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
Version: </td><td style="text-align: left;"> 0.1.5.2 </td>
</tr>
<tr>
 <td style="text-align: left;">
Date: </td><td style="text-align: left;"> 2024-03-10</td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> GPL (&gt;= 2.0)</td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>



<h3>Maintainers </h3>

<p>Maintainer: Manos Papadakis &lt;rfastofficial@gmail.com&gt;
</p>


<h3>Author(s)</h3>

<p>Manos Papadakis <a href="mailto:papadakm95@gmail.com">papadakm95@gmail.com</a>, Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>, Stefanos Fafalios &lt;stefanosfafalios@gmail.com&gt;, Marios Dimitriadis &lt;kmdimitriadis@gmail.com&gt;.
</p>

<hr>
<h2 id='Sample+20quantiles+20and+20col+2Frow+20wise+20quantiles'>
Sample quantiles and col/row wise quantiles
</h2><span id='topic+Quantile'></span><span id='topic+colQuantile'></span><span id='topic+colQuantile.matrix'></span><span id='topic+colQuantile.data.frame'></span><span id='topic+rowQuantile'></span>

<h3>Description</h3>

<p>Sample quantiles and col/row wise quantiles. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>colQuantile(x,probs,parallel=FALSE,cores=0)
## S3 method for class 'matrix'
colQuantile(x,probs,parallel=FALSE,cores=0)
## S3 method for class 'data.frame'
colQuantile(x,probs,parallel=FALSE,cores=0)
rowQuantile(x,probs,parallel=FALSE,cores=0)
Quantile(x,probs,parallel=FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Sample+2B20quantiles+2B20and+2B20col+2B2Frow+2B20wise+2B20quantiles_+3A_x">x</code></td>
<td>

<p>Numeric vector whose sample quantiles are wanted. NA and NaN values are not allowed in numeric vectors.
For the col/row versions a numerical matrix or data.frame.	
</p>
</td></tr>
<tr><td><code id="Sample+2B20quantiles+2B20and+2B20col+2B2Frow+2B20wise+2B20quantiles_+3A_probs">probs</code></td>
<td>

<p>Numeric vector of probabilities with values in [0,1], not missing values. 
Values up to 2e-14 outside that range are accepted and automatically moved to the nearby endpoint by C++.
</p>
</td></tr>
<tr><td><code id="Sample+2B20quantiles+2B20and+2B20col+2B2Frow+2B20wise+2B20quantiles_+3A_parallel">parallel</code></td>
<td>

<p>Do you want to do it in parallel, for column - row major, in C++? TRUE or FALSE.
</p>
</td></tr>
<tr><td><code id="Sample+2B20quantiles+2B20and+2B20col+2B2Frow+2B20wise+2B20quantiles_+3A_cores">cores</code></td>
<td>

<p>Number of cores to use for parallelism. Valid only when argument parallel is set to TRUE. 
Default value is 0 and it means the maximum supported cores.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is the same function as R's built in &quot;quantile&quot; with its default option, <b>type = 7</b>. We have also implemented it in a col/row-wise fashion. 
</p>


<h3>Value</h3>

<p>The function will return a vector of the same mode as the arguments given.
NAs will be removed.
</p>


<h3>Author(s)</h3>

<p>Manos Papadakis.
</p>
<p>R implementation and documentation: Manos Papadakis <a href="mailto:papadakm95@gmail.com">papadakm95@gmail.com</a>.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+trim.mean">trim.mean</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x&lt;-rnorm(1000)
probs&lt;-runif(10)
sum( quantile(x, probs = probs) - Quantile(x, probs) )
</code></pre>

<hr>
<h2 id='Scaled+20logistic+20regression'>
Scaled logistic regression
</h2><span id='topic+sclr'></span>

<h3>Description</h3>

<p>Scaled logistic regression.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>sclr(y, x, full = FALSE, tol = 1e-07, maxiters = 100)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Scaled+2B20logistic+2B20regression_+3A_y">y</code></td>
<td>

<p>The dependent variable; a numerical vector with two values (0 and 1).
</p>
</td></tr>
<tr><td><code id="Scaled+2B20logistic+2B20regression_+3A_x">x</code></td>
<td>

<p>A matrix with the data, where the rows denote the samples (and the two groups) and the columns 
are the variables. This can be a matrix or a data.frame (with factors).
</p>
</td></tr>
<tr><td><code id="Scaled+2B20logistic+2B20regression_+3A_full">full</code></td>
<td>

<p>If this is FALSE, the coefficients and the log-likelihood will be returned only. 
If this is TRUE, more information is returned.
</p>
</td></tr>
<tr><td><code id="Scaled+2B20logistic+2B20regression_+3A_tol">tol</code></td>
<td>

<p>The tolerance value to terminate the Newton-Raphson algorithm.
</p>
</td></tr>
<tr><td><code id="Scaled+2B20logistic+2B20regression_+3A_maxiters">maxiters</code></td>
<td>

<p>The max number of iterations that can take place in each regression.
</p>
</td></tr>
</table>


<h3>Value</h3>

<p>When full is FALSE a list including:
</p>
<table>
<tr><td><code>theta</code></td>
<td>

<p>The  estimated <code class="reqn">theta</code> parameter.
</p>
</td></tr>
<tr><td><code>be</code></td>
<td>

<p>The estimated regression coefficients.
</p>
</td></tr>
<tr><td><code>loglik</code></td>
<td>

<p>The log-likelihood of the model.
</p>
</td></tr>
<tr><td><code>iters</code></td>
<td>

<p>The number of iterations required by Newton-Raphson.
</p>
</td></tr>
</table>
<p>When full is TRUE a list including:
</p>
<table>
<tr><td><code>info</code></td>
<td>

<p>The estimated <code class="reqn">theta</code>, regression coefficients, their standard error, their Wald test statistic and their p-value.
</p>
</td></tr>
<tr><td><code>loglik</code></td>
<td>

<p>The log-likelihood.
</p>
</td></tr>
<tr><td><code>iters</code></td>
<td>

<p>The number of iterations required by Newton-Raphson.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Dunning AJ (2006). A model for immunological correlates of protection. 
Statistics in Medicine, 25(9): 1485-1497. https://doi.org/10.1002/sim.2282. 
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+propols.reg">propols.reg</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- matrix(rnorm(100 * 2), ncol = 2)
y &lt;- rbinom(100, 1, 0.6)   ## binary logistic regression
a &lt;- sclr(y, x) 
</code></pre>

<hr>
<h2 id='Score+20test+20for+20overdispersion+20in+20Poisson+20regression'>
Score test for overdispersion in Poisson regression
</h2><span id='topic+overdispreg.test'></span>

<h3>Description</h3>

<p>Score test for overdispersion in Poisson regression.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>overdispreg.test(y, x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Score+2B20test+2B20for+2B20overdispersion+2B20in+2B20Poisson+2B20regression_+3A_y">y</code></td>
<td>

<p>A vector with count data. 
</p>
</td></tr>
<tr><td><code id="Score+2B20test+2B20for+2B20overdispersion+2B20in+2B20Poisson+2B20regression_+3A_x">x</code></td>
<td>

<p>A numerical matrix with predictor variables.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A score test for overdispersion in Poisson regression is implemented. 
</p>


<h3>Value</h3>

<p>A vector with two values. The test statistic and its associated p-value.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Yang Z., Hardin J.W. and Addy C.L. (2009). A score test for overdierpsdion in 
Poisson regression based on the generalised Poisson-2 model. 
Journal of Statistical Planning and Inference, 139(4): 1514&ndash;1521.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+ztp.reg">ztp.reg</a>, <a href="#topic+censpois.mle">censpois.mle</a> <a href="#topic+wald.poisrat">wald.poisrat</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
y &lt;- rnbinom(100, 10, 0.4)
x &lt;- rnorm(100)
overdispreg.test(y, x)

</code></pre>

<hr>
<h2 id='Single+20terms+20deletion+20hypothesis+20testing+20in+20a+20linear+20regression+20model'>
Single terms deletion hypothesis testing in a linear regression model
</h2><span id='topic+lm.drop1'></span>

<h3>Description</h3>

<p>Single terms deletion hypothesis testing in a linear regression model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lm.drop1(y, x, type = "F") 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Single+2B20terms+2B20deletion+2B20hypothesis+2B20testing+2B20in+2B20a+2B20linear+2B20regression+2B20model_+3A_y">y</code></td>
<td>

<p>The dependent variable, a numerical vector with numbers. 
</p>
</td></tr>
<tr><td><code id="Single+2B20terms+2B20deletion+2B20hypothesis+2B20testing+2B20in+2B20a+2B20linear+2B20regression+2B20model_+3A_x">x</code></td>
<td>

<p>A numerical matrix with the indendent variables. We add, internally, the first column of ones.
</p>
</td></tr>
<tr><td><code id="Single+2B20terms+2B20deletion+2B20hypothesis+2B20testing+2B20in+2B20a+2B20linear+2B20regression+2B20model_+3A_type">type</code></td>
<td>

<p>If you want to perform the usual F (or t) test set this equal to &quot;F&quot;. 
For the test based on the partial correlation set this equal to &quot;cor&quot;.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is the same function as R's built in <a href="stats.html#topic+drop1">drop1</a> that it works with the F test or the partial 
correlation coefficient. For the linear regression model, the Wald test is equivalent to the partial F test. 
So, instead of performing many regression models with single term deletions
we perform one regression model with all variables and compute their Wald test effectively. Note, that this 
is true, only if the design matrix &quot;x&quot; contains the vectors of ones and in our case this must be, strictly, 
the first column. The second option is to compute the p-value of the partial correlation. 
</p>


<h3>Value</h3>

<p>A matrix with two columns. The test statistic and its associated pvalue for each independent variable.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Hastie T., Tibshirani R. and Friedman J. (2008). The Elements of Statistical Learning (2nd Ed.), Springer. 
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+lm.bsreg">lm.bsreg</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
y &lt;- rnorm(150)
x &lt;- as.matrix(iris[, 1:4])
a &lt;- lm(y~., data.frame(x) )
drop1(a, test = "F")
lm.drop1(y, x )

</code></pre>

<hr>
<h2 id='Skeleton+20of+20the+20FEDHC+20algorithm'>
The skeleton of a Bayesian network produced by the FEDHC algorithm
</h2><span id='topic+fedhc.skel'></span>

<h3>Description</h3>

<p>The skeleton of a Bayesian network produced by the FEDHC algorithm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>fedhc.skel(x, method = "pearson", alpha = 0.05,
ini.stat = NULL, R = NULL, parallel = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Skeleton+2B20of+2B20the+2B20FEDHC+2B20algorithm_+3A_x">x</code></td>
<td>

<p>A numerical matrix with the variables. If you have a data.frame (i.e. categorical data) turn them into a matrix
using <code>data.frame.to_matrix</code> from the R package <b>Rfast</b>. Note, that for the categorical case data, 
the numbers must start from 0. No missing data are allowed.
</p>
</td></tr>
<tr><td><code id="Skeleton+2B20of+2B20the+2B20FEDHC+2B20algorithm_+3A_method">method</code></td>
<td>

<p>If you have continuous data, this &quot;pearson&quot;. If you have categorical data though,
this must be &quot;cat&quot;. In this case, make sure the minimum value of each variable is zero.
The function &quot;g2Test&quot; in the R package <b>Rfast</b> and the relevant functions work that way.
</p>
</td></tr>
<tr><td><code id="Skeleton+2B20of+2B20the+2B20FEDHC+2B20algorithm_+3A_alpha">alpha</code></td>
<td>

<p>The significance level (suitable values in (0, 1)) for assessing the p-values. Default value is 0.05.
</p>
</td></tr>
<tr><td><code id="Skeleton+2B20of+2B20the+2B20FEDHC+2B20algorithm_+3A_ini.stat">ini.stat</code></td>
<td>

<p>If the initial test statistics (univariate associations) are available, pass them through this parameter.
</p>
</td></tr>
<tr><td><code id="Skeleton+2B20of+2B20the+2B20FEDHC+2B20algorithm_+3A_r">R</code></td>
<td>

<p>If the correlation matrix is available, pass it here.
</p>
</td></tr>
<tr><td><code id="Skeleton+2B20of+2B20the+2B20FEDHC+2B20algorithm_+3A_parallel">parallel</code></td>
<td>

<p>Set this to TRUE for parallel computations.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Similar to MMHC and PCHC the first phase consists of a variable selection procedure, the FBED algortihm
(Borboudakis and Tsamardinos, 2019).
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>ini.stat</code></td>
<td>

<p>The test statistics of the univariate associations.
</p>
</td></tr>
<tr><td><code>ini.pvalue</code></td>
<td>

<p>The initial p-values univariate associations.
</p>
</td></tr>
<tr><td><code>pvalue</code></td>
<td>

<p>A matrix with the logarithm of the p-values of the updated associations. 
This final p-value is the maximum p-value among the two p-values in the end.
</p>
</td></tr>
<tr><td><code>runtime</code></td>
<td>

<p>The duration of the algorithm.
</p>
</td></tr>
<tr><td><code>ntests</code></td>
<td>

<p>The number of tests conducted during each k.
</p>
</td></tr>
<tr><td><code>G</code></td>
<td>

<p>The adjancency matrix. A value of 1 in G[i, j] appears in G[j, i] also, indicating that i and j 
have an edge between them.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris and Stefanos Fafalios.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a> and 
Stefanos Fafalios <a href="mailto:stefanosfafalios@gmail.com">stefanosfafalios@gmail.com</a>.
</p>


<h3>References</h3>

<p>Tsagris M. (2020). The FEDHC Bayesian network learning algorithm. https://arxiv.org/pdf/2012.00113.pdf.
</p>
<p>Borboudakis G. and Tsamardinos I. (2019). Forward-backward selection with early dropping. 
Journal of Machine Learning Research, 20(8): 1-39.
</p>
<p>Tsamardinos I., Brown E.L. and Aliferis F.C. (2006). The max-min hill-climbing Bayesian network 
structure learning algorithm. Machine Learning 65(1): 31-78.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+mmhc.skel">mmhc.skel</a>, <a href="#topic+mmpc">mmpc</a>, <a href="#topic+mmpc2">mmpc2</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># simulate a dataset with continuous data
x &lt;- matrix( rnorm(200 * 50, 1, 10), nrow = 200 )
a &lt;- fedhc.skel(x)
</code></pre>

<hr>
<h2 id='Skeleton+20of+20the+20MMHC+20algorithm'>
The skeleton of a Bayesian network learned with the MMHC algorithm
</h2><span id='topic+mmhc.skel'></span>

<h3>Description</h3>

<p>The skeleton of a Bayesian network learned with the MMHC algorithm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mmhc.skel(x, method = "pearson", max_k = 3, alpha = 0.05,
ini.stat = NULL, R = NULL, parallel = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Skeleton+2B20of+2B20the+2B20MMHC+2B20algorithm_+3A_x">x</code></td>
<td>

<p>A numerical matrix with the variables. If you have a data.frame (i.e. categorical data) turn them into a matrix
using <code>data.frame.to_matrix</code> from the R package <b>Rfast</b>. Note, that for the categorical case data, 
the numbers must start from 0. No missing data are allowed.
</p>
</td></tr>
<tr><td><code id="Skeleton+2B20of+2B20the+2B20MMHC+2B20algorithm_+3A_method">method</code></td>
<td>

<p>If you have continuous data, this &quot;pearson&quot;. If you have categorical data though,
this must be &quot;cat&quot;. In this case, make sure the minimum value of each variable is zero. The function &quot;g2Test&quot; 
in the R package <b>Rfast</b> and the relevant functions work that way.
</p>
</td></tr>
<tr><td><code id="Skeleton+2B20of+2B20the+2B20MMHC+2B20algorithm_+3A_max_k">max_k</code></td>
<td>

<p>The maximum conditioning set to use in the conditional indepedence test (see Details). Integer, default value is 3.
</p>
</td></tr>
<tr><td><code id="Skeleton+2B20of+2B20the+2B20MMHC+2B20algorithm_+3A_alpha">alpha</code></td>
<td>

<p>The significance level (suitable values in (0, 1)) for assessing the p-values. Default value is 0.05.
</p>
</td></tr>
<tr><td><code id="Skeleton+2B20of+2B20the+2B20MMHC+2B20algorithm_+3A_ini.stat">ini.stat</code></td>
<td>

<p>If the initial test statistics (univariate associations) are available, pass them through this parameter.
</p>
</td></tr>
<tr><td><code id="Skeleton+2B20of+2B20the+2B20MMHC+2B20algorithm_+3A_r">R</code></td>
<td>

<p>If the correlation matrix is available, pass it here.
</p>
</td></tr>
<tr><td><code id="Skeleton+2B20of+2B20the+2B20MMHC+2B20algorithm_+3A_parallel">parallel</code></td>
<td>

<p>Set this to TRUE for parallel computations.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The max_k option: the maximum size of the conditioning set to use in the conditioning independence test. 
Larger values provide more accurate results, at the cost of higher computational times. When the sample size is 
small (e.g., <code class="reqn">&lt;50</code> observations) the max_k parameter should be 3 for example, otherwise the conditional 
independence test may not be able to provide reliable results.
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>ini.stat</code></td>
<td>

<p>The test statistics of the univariate associations.
</p>
</td></tr>
<tr><td><code>ini.pvalue</code></td>
<td>

<p>The initial p-values univariate associations.
</p>
</td></tr>
<tr><td><code>pvalue</code></td>
<td>

<p>A matrix with the logarithm of the p-values of the updated associations. This final p-value is the 
maximum p-value among the two p-values in the end.
</p>
</td></tr>
<tr><td><code>runtime</code></td>
<td>

<p>The duration of the algorithm.
</p>
</td></tr>
<tr><td><code>ntests</code></td>
<td>

<p>The number of tests conducted during each k.
</p>
</td></tr>
<tr><td><code>G</code></td>
<td>

<p>The adjancency matrix. A value of 1 in G[i, j] appears in G[j, i] also, indicating that i and j have an 
edge between them.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris and Stefanos Fafalios.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a> and 
Stefanos Fafalios <a href="mailto:stefanosfafalios@gmail.com">stefanosfafalios@gmail.com</a>.
</p>


<h3>References</h3>

<p>Tsamardinos, I., Aliferis, C. F. and Statnikov, A. (2003). Time and sample efficient discovery of 
Markov blankets and direct causal relations. In Proceedings of the ninth ACM SIGKDD International 
Conference on Knowledge Discovery and Data Mining (pp. 673-678). ACM.
</p>
<p>Brown, L. E., Tsamardinos, I. and Aliferis, C. F. (2004). A novel algorithm for scalable and accurate 
Bayesian network learning. Medinfo, 711-715.
</p>
<p>Tsamardinos I., Brown E.L. and Aliferis F.C. (2006). The max-min hill-climbing Bayesian network
structure learning algorithm. Machine Learning 65(1):31-78.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+fedhc.skel">fedhc.skel</a>, <a href="#topic+mmpc">mmpc</a>, <a href="#topic+mmpc2">mmpc2</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># simulate a dataset with continuous data
x &lt;- matrix( rnorm(300 * 30, 1, 100), nrow = 300 )
a &lt;- mmhc.skel(x)
</code></pre>

<hr>
<h2 id='The+20k-NN+20algorithm+20for+20really+20lage+20scale+20data'>
The k-NN algorithm for really lage scale data
</h2><span id='topic+big.knn'></span>

<h3>Description</h3>

<p>The k-NN algorithm for really lage scale data.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>big.knn(xnew, y, x, k = 2:100, type = "C")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="The+2B20k-NN+2B20algorithm+2B20for+2B20really+2B20lage+2B20scale+2B20data_+3A_xnew">xnew</code></td>
<td>

<p>A matrix with new data, new predictor variables whose response variable must be predicted. 
</p>
</td></tr>
<tr><td><code id="The+2B20k-NN+2B20algorithm+2B20for+2B20really+2B20lage+2B20scale+2B20data_+3A_y">y</code></td>
<td>

<p>A vector of data. The response variable, which can be either continuous or 
categorical (factor is acceptable).  
</p>
</td></tr>
<tr><td><code id="The+2B20k-NN+2B20algorithm+2B20for+2B20really+2B20lage+2B20scale+2B20data_+3A_x">x</code></td>
<td>

<p>A matrix with the available data, the predictor variables. 
</p>
</td></tr>
<tr><td><code id="The+2B20k-NN+2B20algorithm+2B20for+2B20really+2B20lage+2B20scale+2B20data_+3A_k">k</code></td>
<td>

<p>A vector with the possible numbers of nearest neighbours to be considered.
</p>
</td></tr>
<tr><td><code id="The+2B20k-NN+2B20algorithm+2B20for+2B20really+2B20lage+2B20scale+2B20data_+3A_type">type</code></td>
<td>

<p>If your response variable y is numerical data, then this should be &quot;R&quot; (regression). 
If y is in general categorical set this argument to &quot;C&quot; (classification). 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The concept behind k-NN is simple. Suppose we have a matrix with predictor variables and a vector with the 
response variable (numerical or categorical). When a new vector with observations (predictor variables) is 
available, its corresponding response value, numerical or categorical, is to be predicted. Instead of using a 
model, parametric or not, one can use this ad hoc algorithm. 
</p>
<p>The k smallest distances between the new predictor variables and the existing ones are calculated. In the 
case of regression, the average, median, or harmonic mean of the corresponding response values of these closest
predictor values are calculated. In the case of classification, i.e. categorical response value, a voting rule 
is applied. The most frequent group (response value) is where the new observation is to be allocated. 
</p>
<p>This function allows for the Euclidean distance only.
</p>


<h3>Value</h3>

<p>A matrix whose number of columns is equal to the size of k. If in the input you provided there is just one value of k, 
then a matrix with one column is returned containing the predicted values. 
If more than one value was supplied, the matrix will contain the predicted values for every value of k. 
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Friedman J., Hastie T. and Tibshirani R. (2017). The elements of statistical learning. 
New York: Springer.
</p>
<p>Cover TM and Hart PE (1967). Nearest neighbor pattern classification. IEEE Transactions on 
Information Theory. 13(1):21-27.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+bigknn.cv">bigknn.cv</a>, <a href="#topic+reg.mle.lda">reg.mle.lda</a>, <a href="#topic+multinom.reg">multinom.reg</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- as.matrix(iris[, 1:4])
mod &lt;- big.knn(xnew = x, y = iris[, 5], x = x, k = c(6, 7) )
</code></pre>

<hr>
<h2 id='Tobit+20regression'>
Tobit regression
</h2><span id='topic+tobit.reg'></span>

<h3>Description</h3>

<p>Tobit regression.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tobit.reg(y, x, ylow = 0, full = FALSE, tol = 1e-07, maxiters = 100) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Tobit+2B20regression_+3A_y">y</code></td>
<td>

<p>The dependent variable; a numerical vector with values.
</p>
</td></tr>
<tr><td><code id="Tobit+2B20regression_+3A_x">x</code></td>
<td>

<p>A matrix with the data, where the rows denote the samples (and the two groups) and the columns 
are the variables. This can be a matrix or a data.frame (with factors).
</p>
</td></tr>
<tr><td><code id="Tobit+2B20regression_+3A_ylow">ylow</code></td>
<td>

<p>The lowest value below which nothing is observed. The cut-off value.
</p>
</td></tr>
<tr><td><code id="Tobit+2B20regression_+3A_full">full</code></td>
<td>

<p>If this is FALSE, the coefficients and the log-likelihood will be returned only. 
If this is TRUE, more information is returned.
</p>
</td></tr>
<tr><td><code id="Tobit+2B20regression_+3A_tol">tol</code></td>
<td>

<p>The tolerance value to terminate the Newton-Raphson algorithm.
</p>
</td></tr>
<tr><td><code id="Tobit+2B20regression_+3A_maxiters">maxiters</code></td>
<td>

<p>The max number of iterations that can take place in each regression.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The tobit regression model is fitted.
</p>


<h3>Value</h3>

<p>When full is FALSE a list including:
</p>
<table>
<tr><td><code>be</code></td>
<td>

<p>The estimated regression coefficients.
</p>
</td></tr>
<tr><td><code>s</code></td>
<td>

<p>The estimated scale parameter.
</p>
</td></tr>
<tr><td><code>loglik</code></td>
<td>

<p>The log-likelihood of the model.
</p>
</td></tr>
<tr><td><code>iters</code></td>
<td>

<p>The number of iterations required by Newton-Raphson.
</p>
</td></tr>
</table>
<p>When full is TRUE a list including:
</p>
<table>
<tr><td><code>info</code></td>
<td>

<p>The estimated <code class="reqn">theta</code>, regression coefficients, their standard error, their Wald test statistic and their p-value.
</p>
</td></tr>
<tr><td><code>loglik</code></td>
<td>

<p>The log-likelihood.
</p>
</td></tr>
<tr><td><code>iters</code></td>
<td>

<p>The number of iterations required by Newton-Raphson.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Tobin James (1958). Estimation of Relationships for Limited Dependent Variables. Econometrica, 26(1): 24&ndash;36. 
</p>
<p>https://en.wikipedia.org/wiki/Tobit_model
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+hp.reg">hp.reg</a>, <a href="#topic+ztp.reg">ztp.reg</a>, <a href="#topic+censweibull.mle">censweibull.mle</a>, <a href="#topic+censpois.mle">censpois.mle</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- rnorm(100)
y &lt;- rnorm(100)
y[y &lt; 0] &lt;- 0
a &lt;- tobit.reg(y, x, full = TRUE) 
</code></pre>

<hr>
<h2 id='Trimmed+20mean'>Trimmed mean</h2><span id='topic+trim.mean'></span><span id='topic+colTrimMean'></span><span id='topic+colTrimMean.matrix'></span><span id='topic+colTrimMean.data.frame'></span><span id='topic+rowTrimMean'></span>

<h3>Description</h3>

<p>Trimmed mean.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>trim.mean(x, a = 0.05,parallel=FALSE)
colTrimMean(x, a = 0.05,parallel=FALSE,cores=0)
## S3 method for class 'matrix'
colTrimMean(x,a = 0.05,parallel=FALSE,cores=0)
## S3 method for class 'data.frame'
colTrimMean(x,a = 0.05,parallel=FALSE,cores=0)
rowTrimMean(x, a = 0.05,parallel=FALSE,cores=0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Trimmed+2B20mean_+3A_x">x</code></td>
<td>

<p>A numerical vector or a numerical matrix or data.frame. 
</p>
</td></tr>
<tr><td><code id="Trimmed+2B20mean_+3A_a">a</code></td>
<td>

<p>A number in (0, 1), the proportion of data to trim. 
</p>
</td></tr>
<tr><td><code id="Trimmed+2B20mean_+3A_parallel">parallel</code></td>
<td>

<p>Run the algorithm parallel in C++.
</p>
</td></tr>
<tr><td><code id="Trimmed+2B20mean_+3A_cores">cores</code></td>
<td>

<p>Number of cores to use for parallelism. Valid only when argument parallel is set to TRUE. 
Default value is 0 and it means the maximum supported cores.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The trimmed mean is computed. The lower and upper a% of the data are removed and the mean is calculated 
using the rest of the data.
</p>


<h3>Value</h3>

<p>The trimmed mean.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris and Manos Papadakis.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a> and Manos Papadakis <a href="mailto:papadakm95@gmail.com">papadakm95@gmail.com</a>.
</p>


<h3>References</h3>

<p>Wilcox R.R. (2005). Introduction to robust estimation and hypothesis testing. 
Academic Press.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+Quantile">Quantile</a>
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- rnorm(100, 1, 1)
all.equal(trim.mean(x, 0.05),mean(x, 0.05))

x&lt;-matrix(x,10,10)

colTrimMean(x,0.05)
rowTrimMean(x,0.05)
</code></pre>

<hr>
<h2 id='Variable+20selection+20using+20the+20PC-simple+20algorithm'>Variable selection using the PC-simple algorithm
</h2><span id='topic+pc.sel'></span>

<h3>Description</h3>

<p>Variable selection using the PC-simple algorithm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>pc.sel(y, x, ystand = TRUE, xstand = TRUE, alpha = 0.05)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Variable+2B20selection+2B20using+2B20the+2B20PC-simple+2B20algorithm_+3A_y">y</code></td>
<td>

<p>A numerical vector with continuous data.  
</p>
</td></tr>
<tr><td><code id="Variable+2B20selection+2B20using+2B20the+2B20PC-simple+2B20algorithm_+3A_x">x</code></td>
<td>

<p>A matrix with numerical data; the independent variables, of which some will probably be selected.
</p>
</td></tr>
<tr><td><code id="Variable+2B20selection+2B20using+2B20the+2B20PC-simple+2B20algorithm_+3A_ystand">ystand</code></td>
<td>

<p>If this is TRUE the response variable is centered. The mean is subtracted from every value.
</p>
</td></tr>
<tr><td><code id="Variable+2B20selection+2B20using+2B20the+2B20PC-simple+2B20algorithm_+3A_xstand">xstand</code></td>
<td>

<p>If this is TRUE the independent variables are standardised. 
</p>
</td></tr>
<tr><td><code id="Variable+2B20selection+2B20using+2B20the+2B20PC-simple+2B20algorithm_+3A_alpha">alpha</code></td>
<td>

<p>The significance level.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Variable selection for continuous data only is performed using the PC-simple algorithm 
(Buhlmann, Kalisch and Maathuis, 2010). The PC algorithm used to infer the skeleton of a Bayesian
Network has been adopted in the context of variable selection. In other words, the PC algorithm
is used for a single node.
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>vars</code></td>
<td>

<p>A vector with the selected variables.
</p>
</td></tr>
<tr><td><code>n.tests</code></td>
<td>

<p>The number of tests performed.
</p>
</td></tr>
<tr><td><code>runtime</code></td>
<td>

<p>The runtime of the algorithm.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>. 
</p>


<h3>References</h3>

<p>Buhlmann P., Kalisch M. and Maathuis M. H. (2010). Variable selection in high-dimensional linear models:
partially faithful distributions and the PC-simple algorithm. Biometrika, 97(2): 261-278.
<a href="https://arxiv.org/pdf/0906.3204.pdf">https://arxiv.org/pdf/0906.3204.pdf</a>
</p>


<h3>See Also</h3>

<p><code> pc.skel, omp
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>y &lt;- rnorm(100)
x &lt;- matrix( rnorm(100 * 50), ncol = 50)
a &lt;- pc.sel(y, x)
</code></pre>

<hr>
<h2 id='Wald+20confidence+20interval+20for+20the+20ratio+20of+20two+20Poisson+20variables'>
Wald confidence interval for the ratio of two Poisson variables
</h2><span id='topic+wald.poisrat'></span><span id='topic+col.waldpoisrat'></span>

<h3>Description</h3>

<p>Wald confidence interval for the ratio of two Poisson variables.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>wald.poisrat(x, y, alpha = 0.05)
col.waldpoisrat(x, y, alpha = 0.05)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Wald+2B20confidence+2B20interval+2B20for+2B20the+2B20ratio+2B20of+2B20two+2B20Poisson+2B20variables_+3A_x">x</code></td>
<td>

<p>A numeric vector or a matrix with count data.
</p>
</td></tr>
<tr><td><code id="Wald+2B20confidence+2B20interval+2B20for+2B20the+2B20ratio+2B20of+2B20two+2B20Poisson+2B20variables_+3A_y">y</code></td>
<td>

<p>A numeric vector or a matrix with count data.
</p>
</td></tr>
<tr><td><code id="Wald+2B20confidence+2B20interval+2B20for+2B20the+2B20ratio+2B20of+2B20two+2B20Poisson+2B20variables_+3A_alpha">alpha</code></td>
<td>

<p>The 1 - confidence level. The default value is 0.05.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>wald confidence interval for the ratio of two Poisson means is/are calculated. 
</p>


<h3>Value</h3>

<p>For the wald.poisrat a vector with three elements, the ratio and the lower and upper confidence interval limits.
For the col.waldpoisrat a matrix with three columns, the ratio and the lower and upper confidence interval limits.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Krishnamoorthy K., Peng J. and Zhang D. (2016). Modified large sample confidence intervals for Poisson distributions: 
Ratio, weighted average, and product of means. Communications in Statistics-Theory and Methods, 45(1): 83-97.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+censpois.mle">censpois.mle</a>, 
</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- rpois(100, 10)
y &lt;- rpois(100, 10)
wald.poisrat(x, y)
</code></pre>

<hr>
<h2 id='Walter+27s+20confidence+20interval+20for+20the+20ratio+20of+20two+20binomial+20variables+20+28and+20the+20relative+20risk+29'>Walter's confidence interval for the ratio of two binomial variables (and the relative risk)
</h2><span id='topic+walter.ci'></span>

<h3>Description</h3>

<p>Walter's confidence interval for the ratio of two binomial variables (and the relative risk).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>walter.ci(x1, x2, n1, n2, a = 0.05) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Walter+2B27s+2B20confidence+2B20interval+2B20for+2B20the+2B20ratio+2B20of+2B20two+2B20binomial+2B20variables+2B20+2B28and+2B20the+2B20relative+2B20risk+2B29_+3A_x1">x1</code></td>
<td>

<p>An integer number, greater than or equal to zero.  
</p>
</td></tr>
<tr><td><code id="Walter+2B27s+2B20confidence+2B20interval+2B20for+2B20the+2B20ratio+2B20of+2B20two+2B20binomial+2B20variables+2B20+2B28and+2B20the+2B20relative+2B20risk+2B29_+3A_x2">x2</code></td>
<td>

<p>A secondinteger number, greater than or equal to zero.
</p>
</td></tr>
<tr><td><code id="Walter+2B27s+2B20confidence+2B20interval+2B20for+2B20the+2B20ratio+2B20of+2B20two+2B20binomial+2B20variables+2B20+2B28and+2B20the+2B20relative+2B20risk+2B29_+3A_n1">n1</code></td>
<td>

<p>An integer number, greater than or x1.  
</p>
</td></tr>
<tr><td><code id="Walter+2B27s+2B20confidence+2B20interval+2B20for+2B20the+2B20ratio+2B20of+2B20two+2B20binomial+2B20variables+2B20+2B28and+2B20the+2B20relative+2B20risk+2B29_+3A_n2">n2</code></td>
<td>

<p>A secondinteger number, greater than or equal to x2.
</p>
</td></tr>
<tr><td><code id="Walter+2B27s+2B20confidence+2B20interval+2B20for+2B20the+2B20ratio+2B20of+2B20two+2B20binomial+2B20variables+2B20+2B28and+2B20the+2B20relative+2B20risk+2B29_+3A_a">a</code></td>
<td>

<p>The significance level. The produced confidence interval has a confidence level equal to 1-a.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This calculates a (1-a)% confidence interval for the ratio of two binomial variables (and hence for the relative risk) 
using Walter's suggestion (Walter, 1975). That is, to add 0.5 in each number. This not only overcomes the problem of zero values, 
but produces intervals that are more accurate than the classical asymptotic confidence interval (Alharbi and Tsagris, 2018). 
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>rat</code></td>
<td>

<p>The ratio of the two binomial distributions.
</p>
</td></tr>
<tr><td><code>ci</code></td>
<td>

<p>Walter's confidence interval.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Walter S. (1975). The distribution of Levin's measure of attributable risk. Biometrika, 62(2): 371-372.
</p>
<p>Alharbi N. and Tsagris M. (2018). Confidence Intervals for the Relative Risk. Biostatistics and Biometrics, 4(5). 
doi:10.19080/BBOAJ.2018.04.555647
</p>
<p>https://juniperpublishers.com/bboaj/pdf/BBOAJ.MS.ID.555647.pdf
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+mle.lda">mle.lda</a>, <a href="#topic+welch.tests">welch.tests</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x1 &lt;- rbinom(1, 20, 0.7)
x2 &lt;- rbinom(1, 30, 0.6)
n1 &lt;- 20
n2 &lt;- 30
walter.ci(x1,x2,n1,n2)
</code></pre>

<hr>
<h2 id='Zero+20inflated+20Gamma+20regression'>
Zero inflated Gamma regression
</h2><span id='topic+zigamma.reg'></span>

<h3>Description</h3>

<p>Zero inflated Gamma regression.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>zigamma.reg(y, x, full = FALSE, tol = 1e-07, maxiters = 100) 
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Zero+2B20inflated+2B20Gamma+2B20regression_+3A_y">y</code></td>
<td>

<p>The dependent variable, a numerical vector with numbers, zeros and higher.
</p>
</td></tr>
<tr><td><code id="Zero+2B20inflated+2B20Gamma+2B20regression_+3A_x">x</code></td>
<td>

<p>A numerical matrix with the indendent variables. We add, internally, the first column of ones.
</p>
</td></tr>
<tr><td><code id="Zero+2B20inflated+2B20Gamma+2B20regression_+3A_full">full</code></td>
<td>

<p>If this is FALSE, the coefficients and the log-likelihood will be returned only. 
If this is TRUE, more information is returned.
</p>
</td></tr>
<tr><td><code id="Zero+2B20inflated+2B20Gamma+2B20regression_+3A_tol">tol</code></td>
<td>

<p>The tolerance value to terminate the Newton-Raphson algorithm. 
</p>
</td></tr>
<tr><td><code id="Zero+2B20inflated+2B20Gamma+2B20regression_+3A_maxiters">maxiters</code></td>
<td>

<p>The maximum number of iterations that can take place in each regression. 
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Two regression models are fitted, a binary logistic regression and a Gamma regression model to the non-zero responses.
</p>


<h3>Value</h3>

<p>Depending on whether &quot;full&quot; is TRUE or not different outputs are returned.
In general, the regression coefficients, the iterations required by Newton-Raphson and the deviances are returned. 
If full is TRUE, a matrix with their standard errors and the Wald test statistics is returned as well.
</p>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>References</h3>

<p>Mills, Elizabeth Dastrup (2013).
Adjusting for covariates in zero-inflated gamma and zero-inflated log-normal models for semicontinuous data.
PhD thesis, University of Iowa.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+zigamma.mle">zigamma.mle</a>, <a href="#topic+ztp.reg">ztp.reg</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>y &lt;- rgamma(100, 4, 1)
y[sample(100, 10)] &lt;- 0
x &lt;- rnorm(100)
a &lt;- zigamma.reg(y, x)
</code></pre>

<hr>
<h2 id='Zero+20truncated+20Poisson+20regression'>
Zero truncated Poisson regression
</h2><span id='topic+ztp.reg'></span>

<h3>Description</h3>

<p>Zero truncated Poisson regression.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ztp.reg(y, x, full = FALSE, tol = 1e-07, maxiters = 100)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Zero+2B20truncated+2B20Poisson+2B20regression_+3A_y">y</code></td>
<td>

<p>The dependent variable, a numerical vector with integer valued numbers.
</p>
</td></tr>
<tr><td><code id="Zero+2B20truncated+2B20Poisson+2B20regression_+3A_x">x</code></td>
<td>

<p>A matrix or a data.frame with the indendent variables.
</p>
</td></tr>
<tr><td><code id="Zero+2B20truncated+2B20Poisson+2B20regression_+3A_full">full</code></td>
<td>

<p>If you want full information (standard errors, Walt test statistics and p-values of the regression coefficients) 
set this equal to TRUE.
</p>
</td></tr>
<tr><td><code id="Zero+2B20truncated+2B20Poisson+2B20regression_+3A_tol">tol</code></td>
<td>

<p>The tolerance value required by the Newton-Raphson to stop.
</p>
</td></tr>
<tr><td><code id="Zero+2B20truncated+2B20Poisson+2B20regression_+3A_maxiters">maxiters</code></td>
<td>

<p>The maximum iterations allowed.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>A zero truncated poisson regression model is fitted.
</p>


<h3>Value</h3>

<p>A list including:
</p>
<table>
<tr><td><code>be</code></td>
<td>

<p>The regression coefficients if &quot;full&quot; was set to FALSE.
</p>
</td></tr>
<tr><td><code>info</code></td>
<td>

<p>This is returned only if &quot;full&quot; was set to TRUE. It is a matrix with the regression coefficients, 
their standard errors, Walt test statistics and p-values.
</p>
</td></tr>
<tr><td><code>loglik</code></td>
<td>

<p>The loglikelihood of the regression model.
</p>
</td></tr>
<tr><td><code>iter</code></td>
<td>

<p>The iterations required by the Newton-Raphson.
</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Michail Tsagris.
</p>
<p>R implementation and documentation: Michail Tsagris <a href="mailto:mtsagris@uoc.gr">mtsagris@uoc.gr</a>.
</p>


<h3>See Also</h3>

<p><code> <a href="#topic+bic.regs">bic.regs</a> </code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
y &lt;- rpois(100, 5)
y[y == 0] &lt;- 1
x &lt;- matrix( rnorm(100 * 5), ncol = 5 )
mod &lt;- ztp.reg(y, x)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
