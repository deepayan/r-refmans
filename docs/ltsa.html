<!DOCTYPE html><html><head><title>Help for package ltsa</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {ltsa}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#DHSimulate'><p> Simulate General Linear Process</p></a></li>
<li><a href='#DLAcfToAR'><p> Autocorrelations to AR parameters</p></a></li>
<li><a href='#DLLoglikelihood'><p>Durbin-Levinsion Loglikelihood</p></a></li>
<li><a href='#DLResiduals'><p> Prediction residuals</p></a></li>
<li><a href='#DLSimulate'><p> Simulate linear time series</p></a></li>
<li><a href='#exactLoglikelihood'><p>Exact log-likelihood and MLE for variance</p></a></li>
<li><a href='#innovationVariance'>
<p>Nonparametric estimate of the innovation variance</p></a></li>
<li><a href='#is.toeplitz'><p> test if argument is a symmetric Toeplitz matrix</p></a></li>
<li><a href='#ltsa-package'>
<p>Linear Time Series Analysis</p></a></li>
<li><a href='#PredictionVariance'><p> Prediction variance</p></a></li>
<li><a href='#SimGLP'><p> Simulate GLP given innovations</p></a></li>
<li><a href='#tacvfARMA'><p> theoretical autocovariance function (acvf) of ARMA</p></a></li>
<li><a href='#ToeplitzInverseUpdate'><p> Inverse of Toeplitz matrix of order n+1 given inverse of order n</p></a></li>
<li><a href='#TrenchForecast'><p> Minimum Mean Square Forecast</p></a></li>
<li><a href='#TrenchInverse'><p>compute the matrix inverse of a positive-definite Toepliz matrix</p></a></li>
<li><a href='#TrenchLoglikelihood'><p>Loglikelihood function of stationary time series</p>
using Trench algorithm</a></li>
<li><a href='#TrenchMean'><p> Exact MLE for mean given the autocorrelation function</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Version:</td>
<td>1.4.6</td>
</tr>
<tr>
<td>Date:</td>
<td>2015-12-20</td>
</tr>
<tr>
<td>Title:</td>
<td>Linear Time Series Analysis</td>
</tr>
<tr>
<td>Author:</td>
<td>A.I. McLeod, Hao Yu, Zinovi Krougly</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>A.I. McLeod &lt;aimcleod@uwo.ca&gt;</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.1.0)</td>
</tr>
<tr>
<td>Description:</td>
<td>Methods of developing linear time series modelling.
 Methods are given for loglikelihood computation, forecasting
  and simulation.</td>
</tr>
<tr>
<td>Classification/ACM:</td>
<td>G.3, G.4, I.5.1</td>
</tr>
<tr>
<td>Classification/MSC:</td>
<td>62M10, 91B84</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="http://www.stats.uwo.ca/faculty/aim">http://www.stats.uwo.ca/faculty/aim</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2015-12-21 02:25:34 UTC; IanMcLeod</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2015-12-21 08:55:04</td>
</tr>
</table>
<hr>
<h2 id='DHSimulate'> Simulate General Linear Process </h2><span id='topic+DHSimulate'></span>

<h3>Description</h3>

<p>Uses the Davies-Harte algorithm to simulate a Gaussian time
series with specified autocovariance function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>DHSimulate(n, r, ReportTestOnly = FALSE, rand.gen = rnorm, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="DHSimulate_+3A_n">n</code></td>
<td>
<p> length of time series to be generated </p>
</td></tr>
<tr><td><code id="DHSimulate_+3A_r">r</code></td>
<td>
<p> autocovariances at lags 0,1,...</p>
</td></tr>
<tr><td><code id="DHSimulate_+3A_reporttestonly">ReportTestOnly</code></td>
<td>
<p> FALSE &ndash; Run normally so terminates with an error if Davies-Harte condition
does not hold. Othewise if TRUE, then output is TRUE if the Davies-Harte condition holds
and FALSE if it does not.</p>
</td></tr>
<tr><td><code id="DHSimulate_+3A_rand.gen">rand.gen</code></td>
<td>
<p> random number generator to use. It is assumed to have mean zero and
variance one.</p>
</td></tr>
<tr><td><code id="DHSimulate_+3A_...">...</code></td>
<td>
<p>optional arguments passed to <code>rand.gen</code> </p>
</td></tr>
</table>


<h3>Details</h3>

<p>The method uses the FFT and so is most efficient if the series length, n,
is a power of 2.
The method requires that a complicated non-negativity condition be satisfed.
Craigmile (2003) discusses this condition in more detail and shows for 
anti-persistent time series this condition will always be satisfied.
Sometimes, as in the case of fractinally differenced white noise with
parameter d=0.45 and n=5000, this condition fails and the algorithm doesn't
work.  
In this case, an error message is generated and the function halts.
</p>


<h3>Value</h3>

<p>Either a vector of length containing the simulated time series if Davies-Harte condition
holds and ReportTestOnly = FALSE. 
If argument ReportTestOnly is set to TRUE, then output is logical variable indicating
if Davies-Harte condition holds, TRUE, or if it does not, FALSE.
</p>


<h3>Author(s)</h3>

<p> A.I. McLeod</p>


<h3>References</h3>

 
<p>Craigmile, P.F. (2003).
Simulating a class of stationary Gaussian processes using the Davies-Harte algorithm, 
with application to long memory processes.
Journal of Time Series Analysis, 24, 505-511.
</p>
<p>Davies,  R. B. and Harte, D. S. (1987).
Tests for Hurst Effect. Biometrika 74,  95&ndash;101.
</p>
<p>McLeod, A.I., Yu, Hao, Krougly, Zinovi L.  (2007).
Algorithms for Linear Time Series Analysis,
Journal of Statistical Software.
</p>


<h3>See Also</h3>

<p> \
<code><a href="#topic+DLSimulate">DLSimulate</a></code> ,
<code><a href="#topic+SimGLP">SimGLP</a></code>,
<code><a href="stats.html#topic+arima.sim">arima.sim</a></code> 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#simulate a process with autocovariance function 1/(k+1), k=0,1,...
# and plot it
n&lt;-2000
r&lt;-1/sqrt(1:n)
z&lt;-DHSimulate(n, r)
plot.ts(z)

#simulate AR(1) and produce a table comparing the theoretical and sample
# autocovariances and autocorrelations
phi&lt;- -0.8
n&lt;-4096
g0&lt;-1/(1-phi^2)
#theoretical autocovariances
tacvf&lt;-g0*(phi^(0:(n-1)))
z&lt;-DHSimulate(n, tacvf)
#autocorrelations
sacf&lt;-acf(z, plot=FALSE)$acf
#autocovariances
sacvf&lt;-acf(z, plot=FALSE,type="covariance")$acf
tacf&lt;-tacvf/tacvf[1]
tb&lt;-matrix(c(tacvf[1:10],sacvf[1:10],tacf[1:10],sacf[1:10]),ncol=4)
dimnames(tb)&lt;-list(0:9, c("Tacvf","Sacvf","Tacf","Sacf"))
tb

#Show the Davies-Harte condition sometimes hold and sometimes does not
#   in the case of fractionally differenced white noise
#
#Define autocovariance function for fractionally differenced white noise
`tacvfFdwn` &lt;-
function(d, maxlag)
{
    x &lt;- numeric(maxlag + 1)
    x[1] &lt;- gamma(1 - 2 * d)/gamma(1 - d)^2
    for(i in 1:maxlag) 
        x[i + 1] &lt;- ((i - 1 + d)/(i - d)) * x[i]
    x
}
#Build table to show values of d for which condition is TRUE when n=5000
n&lt;-5000
ds&lt;-c(-0.45, -0.25, -0.05, 0.05, 0.25, 0.45)
tb&lt;-logical(length(ds))
names(tb)&lt;-ds
for (kd in 1:length(ds)){
    d&lt;-ds[kd]
    r&lt;-tacvfFdwn(d, n-1)
    tb[kd]&lt;-DHSimulate(n, r, ReportTestOnly = TRUE)
    }
tb

</code></pre>

<hr>
<h2 id='DLAcfToAR'> Autocorrelations to AR parameters </h2><span id='topic+DLAcfToAR'></span>

<h3>Description</h3>

<p>Given autocorrelations at lags 1,...,n the AR parameters corresponding
to the AR coefficients, partial autocorrelations (pacf) and standarized 
minimum-mean-square predictor variance (sigsqk)
are computed. Can also be used as a test for valid acf sequence.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>DLAcfToAR(r, useC = TRUE, PDSequenceTestQ = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="DLAcfToAR_+3A_r">r</code></td>
<td>
<p> autocorrelations starting at lag 1</p>
</td></tr>
<tr><td><code id="DLAcfToAR_+3A_usec">useC</code></td>
<td>
<p> TRUE, C-interface function used. Otherwise if FALSE
calculations are done in R </p>
</td></tr>
<tr><td><code id="DLAcfToAR_+3A_pdsequencetestq">PDSequenceTestQ</code></td>
<td>
<p> FALSE, an error message is given if the 
autocorrelation sequence in not pd otherwise test for pd </p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function is more general than the built-in <code>acf2AR</code> since
it provides the pacf and standardized minimum-mean-square error predictors.
The standardized minimum-mean-square error predictor variances are
defined as the minimum-mean-square error predictor variance for an AR
process with unit variance.  So for a sufficiently high-order, an
approximation to the innovation variance is obtained.
</p>
<p>The pacf may be used as an alternative parameterization for the
linear time series model (McLeod and Zhang, 2006). 
</p>


<h3>Value</h3>

<p>a matrix with 3 columns and length(r) rows is returned corresponding to
the ar coefficients, pacf and sigsqk when PDSequenceTestQ = FALSE.
Otherwise when PDSequenceTestQ = TRUE, the result is TRUE or FALSE
according as the autocorrelation is a valid positive-definite sequence.
</p>


<h3>Author(s)</h3>

<p>A.I. McLeod  </p>


<h3>References</h3>

<p>McLeod, A.I. and Zhang, Y. (2006).
Partial autocorrelation parameterization for subset autoregression.  
Journal of Time Series Analysis, 27, 599-612. 
</p>
<p>McLeod, A.I., Yu, Hao, Krougly, Zinovi L.  (2007).
Algorithms for Linear Time Series Analysis,
Journal of Statistical Software.
</p>


<h3>See Also</h3>

 
<p><code><a href="stats.html#topic+acf2AR">acf2AR</a></code>, 
<code><a href="stats.html#topic+ar">ar</a></code> 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#Example 1:  Yule-Walker estimates
z&lt;-log(lynx)
p&lt;-11
r&lt;-(acf(z, lag.max=p, plot=FALSE)$acf)[-1]
ans&lt;-DLAcfToAR(r)
#compare with built-in ar
phiAR&lt;-ar(z,aic=FALSE, order.max=p, method="yw")$ar
#yet another way is to use acf2AR
phi2&lt;-(acf2AR(c(1,r)))[p,]
cbind(ans,phiAR,phi2)
#   
#Example 2:  AR(1) illustration
#For AR(1) case compare useC = T and F
r&lt;-0.9^(1:3)
DLAcfToAR(r, useC=TRUE)
DLAcfToAR(r, useC=FALSE)
DLAcfToAR(r, useC=TRUE, PDSequenceTestQ=TRUE)
DLAcfToAR(r, useC=FALSE, PDSequenceTestQ=TRUE)
#
#Example 3: test for valid tacf
r&lt;-c(0.8, rep(0,99))
DLAcfToAR(r, PDSequenceTestQ=TRUE)
#   
#Example 4: Fractional-difference example
#Hosking (1981), pacf, zeta[k]=d/(k-d)
#we compare this numerically with our procedure
`tacvfFdwn` &lt;-
function(d, maxlag)
{
    x &lt;- numeric(maxlag + 1)
    x[1] &lt;- gamma(1 - 2 * d)/gamma(1 - d)^2
    for(i in 1:maxlag) 
        x[i + 1] &lt;- ((i - 1 + d)/(i - d)) * x[i]
    x
}
n&lt;-10
d&lt;-0.4
r&lt;-tacvfFdwn(d, n)
r&lt;-(r/r[1])[-1]
HoskingPacf&lt;-d/(-d+(1:n))
cbind(DLAcfToAR(r),HoskingPacf)
#
# Example 5: Determining a suitable MA approximation
#Find MA approximation to hyperbolic decay series
N&lt;-10^4  #pick N so large that mmse forecast error converged
r&lt;-1/sqrt(1:N)
out&lt;-DLAcfToAR(r[-1])
InnovationVariance&lt;-out[nrow(out),3]
phi&lt;-out[,1]
psi&lt;-ARMAtoMA(ar=phi, lag.max=N)
Error&lt;-r[1]-InnovationVariance*(1+sum(psi^2))
</code></pre>

<hr>
<h2 id='DLLoglikelihood'>Durbin-Levinsion Loglikelihood </h2><span id='topic+DLLoglikelihood'></span>

<h3>Description</h3>

<p>The Durbin-Levinsion algorithm is used for the computation of the exact
loglikelihood function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>DLLoglikelihood(r, z, useC = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="DLLoglikelihood_+3A_r">r</code></td>
<td>
<p>autocovariance or autocorrelation at lags 0,...,n-1, where n is length(z) </p>
</td></tr>
<tr><td><code id="DLLoglikelihood_+3A_z">z</code></td>
<td>
<p>time series data</p>
</td></tr>
<tr><td><code id="DLLoglikelihood_+3A_usec">useC</code></td>
<td>
<p> TRUE, use compiled C, otherwise R </p>
</td></tr>
</table>


<h3>Details</h3>

<p>The concentrated loglikelihood function may be written Lm(beta) = -(n/2)*log(S/n)-0.5*g,
where beta is the parameter vector, n is the length of the time series, S=z'M z,
z is the mean-corrected time series, M is the inverse of the covariance matrix setting
the innovation variance to one and g=-log(det(M)).
This method was given in Li (1981) for evaluating the loglikelihood function
in the case of the fractionally differenced white noise.
</p>


<h3>Value</h3>

<p>The loglikelihood concentrated over the parameter for the innovation
variance is returned.
</p>


<h3>Note</h3>

 
<p>The purpose of this function is to provide a check on the TrenchLoglikelihod function. 
Completely different algorithms are used in each case but the numerical values should
agree.
</p>


<h3>Author(s)</h3>

<p> A.I. McLeod </p>


<h3>References</h3>

 
<p>W.K. Li (1981). 
Topics in Time Series Analysis. 
Ph.D. Thesis, 
University of Western Ontario.
</p>
<p>McLeod, A.I., Yu, Hao, Krougly, Zinovi L.  (2007).
Algorithms for Linear Time Series Analysis,
Journal of Statistical Software.
</p>


<h3>See Also</h3>

 
<p><code><a href="#topic+TrenchLoglikelihood">TrenchLoglikelihood</a></code> 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#Example 1
#compute loglikelihood for white noise
z&lt;-rnorm(100)
DLLoglikelihood(c(1,rep(0,length(z)-1)), z)

#Example 2
#simulate a time series and compute the concentrated loglikelihood using DLLoglikelihood and
#compare this with the value given by TrenchLoglikelihood.
phi&lt;-0.8
n&lt;-200
r&lt;-phi^(0:(n-1))
z&lt;-arima.sim(model=list(ar=phi), n=n)
LD&lt;-DLLoglikelihood(r,z)
LT&lt;-TrenchLoglikelihood(r,z)
ans&lt;-c(LD,LT)
names(ans)&lt;-c("DLLoglikelihood","TrenchLoglikelihood")

#Example 3
## Not run: 
#Compare direct evaluation of AR(1) loglikelihood with DL method
#First define the exact concentrated loglikelihood function for AR(1)
AR1Loglikelihood &lt;-function(phi,z){
n&lt;-length(z)
S&lt;-(z[1]^2)*(1-phi^2) + sum((z[-1]-phi*z[-n])^2)
0.5*log(1-phi^2)-(n/2)*log(S/n)
}
#Next run script to compare numerically the loglikelihoods.
#They should be identical.
phi&lt;-0.8
n&lt;-200
z&lt;-arima.sim(list(ar=phi), n=n)
phis&lt;-seq(0.1,0.95,0.05)
ansAR1&lt;-ansDL&lt;-numeric(length(phis))
for (i in 1:length(phis)) {
    ansAR1[i] &lt;- AR1Loglikelihood(phis[i],z)
    r&lt;-(1/(1-phis[i]^2))*phis[i]^(0:(n-1))
    ansDL[i] &lt;- DLLoglikelihood(r,z,useC=FALSE)
}
ans&lt;-matrix(c(ansDL,ansAR1),ncol=2)
dimnames(ans)&lt;-list(phis, c("DL-method","AR1-method"))

## End(Not run)

#Example 4
## Not run: 
#compare timings. See (McLeod, Yu, Krougly, Table 8).
 n&lt;-5000
 ds&lt;-c(-0.45, -0.25, -0.05, 0.05, 0.25, 0.45)
 tim&lt;-matrix(numeric(3*length(ds)),ncol=3)
 for (i in 1:length(ds)){
    d&lt;-ds[i]
    alpha &lt;- 1-2*d #equivalent hyperbolic autocorrelation
    r &lt;- (1/(1:n))^alpha
    z&lt;-DLSimulate(n,r)
    tim1a&lt;-system.time(LL1&lt;-DLLoglikelihood(r,z))[1]
    tim1b&lt;-system.time(LL1&lt;-DLLoglikelihood(r,z,useC=FALSE))[1]
    tim2&lt;-system.time(LL2&lt;-TrenchLoglikelihood(r,z))[1]
    tim[i,]&lt;-c(tim1a,tim1b, tim2)
    }
 dimnames(tim)&lt;-list(ds, c("DL-C","DL-R","Trench"))
 tim

## End(Not run)
</code></pre>

<hr>
<h2 id='DLResiduals'> Prediction residuals </h2><span id='topic+DLResiduals'></span>

<h3>Description</h3>

<p>The Durbin-Levison algorithm is used to compute the one-step
prediction residuals.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>DLResiduals(r, z, useC = TRUE, StandardizedQ=TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="DLResiduals_+3A_r">r</code></td>
<td>
<p> vector of length n containing the autocovariances or autocorrelations at lags 0,...,n-1 </p>
</td></tr>
<tr><td><code id="DLResiduals_+3A_z">z</code></td>
<td>
<p> vector of length n, mean-corrected time series data </p>
</td></tr>
<tr><td><code id="DLResiduals_+3A_usec">useC</code></td>
<td>
<p> if TRUE, the compiled C code is used, otherwise
the computations are done entirely in R and much slower</p>
</td></tr>
<tr><td><code id="DLResiduals_+3A_standardizedq">StandardizedQ</code></td>
<td>
<p>TRUE, the residuals are divided by their standard deviation or FALSE, the
raw prediction residuals are computed</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If the model is correct the standardized prediction residuals are approximately NID(0,1) and 
are asymptotically
equivalent to the usual innovation residuals divided by the residual sd. This means that
the usual diagnotic checks, such as the Ljung-Box test may be used.
</p>


<h3>Value</h3>

<p>Vector of length n containing the residuals
</p>


<h3>Author(s)</h3>

<p> A.I. McLeod </p>


<h3>References</h3>

 
<p>W.K. Li (1981). 
Topics in Time Series Analysis. 
Ph.D. Thesis, 
University of Western Ontario.
</p>
<p>McLeod, A.I., Yu, Hao, Krougly, Zinovi L.  (2007).
Algorithms for Linear Time Series Analysis,
Journal of Statistical Software.
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+DLLoglikelihood">DLLoglikelihood</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'># For the AR(1) the prediction residuals and innovation residuals are the same (except for
# t=1).  In this example we demonstrate the equality of these two types of residuals.
#
phi&lt;-0.8
sde&lt;-30
n&lt;-30
z&lt;-arima.sim(n=30,list(ar=phi),sd=sde)
r&lt;-phi^(0:(n-1))/(1-phi^2)*sde^2
e&lt;-DLResiduals(r,z)
a&lt;-numeric(n)
for (i in 2:n)
    a[i]=z[i]-phi*z[i-1]
a&lt;-a/sde
ERR&lt;-sum(abs(e[-1]-a[-1]))
ERR
#
#Simulate AR(1) and compute the MLE for the innovation variance
phi &lt;- 0.5
n &lt;- 2000
sigsq &lt;- 9
z&lt;-arima.sim(model=list(ar=phi), n=n, sd=sqrt(sigsq))
g0 &lt;- sigsq/(1-phi^2)
r &lt;- g0*phi^(0:(n-1))
#comparison of estimate with actual
e&lt;-DLResiduals(r,z,useC=FALSE, StandardizedQ=FALSE)
sigsqHat &lt;- var(e)
ans&lt;-c(sigsqHat,sigsq)
names(ans)&lt;-c("estimate","theoretical")
ans
</code></pre>

<hr>
<h2 id='DLSimulate'> Simulate linear time series </h2><span id='topic+DLSimulate'></span>

<h3>Description</h3>

<p>The Durbin-Levinsion recursions are used to simulate a stationary
time series given an unit innovation sequence and given autocovariance
function. Requires </p>
<p style="text-align: center;"><code class="reqn">O(n^2)</code>
</p>
<p> flops.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>DLSimulate(n, r, useC = TRUE, rand.gen = rnorm, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="DLSimulate_+3A_n">n</code></td>
<td>
<p> length of time series to be generated </p>
</td></tr>
<tr><td><code id="DLSimulate_+3A_r">r</code></td>
<td>
<p> autocovariances, lags 0, ...,  </p>
</td></tr>
<tr><td><code id="DLSimulate_+3A_usec">useC</code></td>
<td>
<p> =TRUE, use C interface. Otherwise direct
computation. </p>
</td></tr>
<tr><td><code id="DLSimulate_+3A_rand.gen">rand.gen</code></td>
<td>
<p> random number generator to use</p>
</td></tr>
<tr><td><code id="DLSimulate_+3A_...">...</code></td>
<td>
<p> optional arguments passed to <code>rand.gen</code> </p>
</td></tr>
</table>


<h3>Details</h3>

<p>See Hipel and McLeod (1994) or
McLeod, Yu and Krougly (2007).
</p>


<h3>Value</h3>

<p>simulated time series of length n
</p>


<h3>Author(s)</h3>

<p> A.I. McLeod </p>


<h3>References</h3>

 
<p>McLeod, A.I., Yu, Hao, Krougly, Zinovi L.  (2007).
Algorithms for Linear Time Series Analysis,
Journal of Statistical Software.
</p>


<h3>See Also</h3>

 
<p><code><a href="#topic+DHSimulate">DHSimulate</a></code>,
<code><a href="#topic+SimGLP">SimGLP</a></code>,
code<a href="stats.html#topic+arima.sim">arima.sim</a> 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#Simulate hyperbolic decay time series 
#with Hurst coefficient, H=0.9
n&lt;-2000
H&lt;-0.9
alpha&lt;-2*(1-H)  #hyperbolic decay parameter
r&lt;-(1/(1:n))^alpha
z&lt;-DLSimulate(n, r)
plot.ts(z)
#can use HurstK function in FGN library to estimate H

</code></pre>

<hr>
<h2 id='exactLoglikelihood'>Exact log-likelihood and MLE for variance</h2><span id='topic+exactLoglikelihood'></span>

<h3>Description</h3>

<p>Provides an exact log-likelihood that is exactly equal to the value of the
probability density function with the random variables replaced by data and the
parameters replaced by their estimated value. The corresponding estimate of the
variance term is return.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>exactLoglikelihood(r, z, innovationVarianceQ = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="exactLoglikelihood_+3A_r">r</code></td>
<td>
<p>the portion of autocovariance function which when multiplied by
the variance term equals the full autocovariance function.
</p>
</td></tr>
<tr><td><code id="exactLoglikelihood_+3A_z">z</code></td>
<td>
<p>the time series assumed to have mean zero
</p>
</td></tr>
<tr><td><code id="exactLoglikelihood_+3A_innovationvarianceq">innovationVarianceQ</code></td>
<td>

<p>When TRUE, the variance term is the innovation variance and when FALSE it is the
variance of the time series. For ARFIMA models, set to TRUE. But FGN requires
setting innovationVarianceQ to FALSE since only the innovation variance is not
known and so the likelihood has a slightly different form.
</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function uses the trench algorithm that is implememented in C.
This function is provided to include all multiplicative constants.
For many purposes, such as MLE, we only need to likelihood function up to
a multiplicative constant. But for information criteria, we may need the constant
terms so we can compare our results with other types of models or with other software
such as arima(). The arima() function also computes the exact log-likelihood
and uses it in the computation of the AIC and BIC.
</p>


<h3>Value</h3>

<table>
<tr><td><code>LL</code></td>
<td>
<p>exact log-likelihood</p>
</td></tr>
<tr><td><code>sigmaSq</code></td>
<td>
<p>MLE for the variance term. If innovationVarianceQ is TRUE, is the an
estimate of the residual variance otherwise it is an estimate of the variance of the time series.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>A. I. McLeod, aimcleod@uwo.ca
</p>


<h3>See Also</h3>

<p><code><a href="#topic+TrenchLoglikelihood">TrenchLoglikelihood</a></code>,
<code><a href="#topic+DLLoglikelihood">DLLoglikelihood</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(7773311)
n &lt;- 200
z &lt;- arima.sim(model=list(ar=0.9, ma=-0.6), n=n, n.start=10^4)
out &lt;- arima(z, order=c(1,0,1), include.mean=FALSE)
out
#note
#sigma^2 estimated as 0.9558:  log likelihood = -279.66,  aic = 565.31
r &lt;- tacvfARMA(phi=coef(out)[1], theta=-coef(out)[2], maxLag=n-1)
exactLoglikelihood(r, z, innovationVarianceQ = TRUE)
#agrees!
</code></pre>

<hr>
<h2 id='innovationVariance'>
Nonparametric estimate of the innovation variance
</h2><span id='topic+innovationVariance'></span>

<h3>Description</h3>

<p>The innovation variance is estimated using a high order AR approximation determined by the AIC or
by using Kolmogoroff's formula with a smoothed periodogram. Default is AR.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>innovationVariance(z, method = c("AR", "Kolmogoroff"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="innovationVariance_+3A_z">z</code></td>
<td>

<p>time series
</p>
</td></tr>
<tr><td><code id="innovationVariance_+3A_method">method</code></td>
<td>

<p>Default &quot;AR&quot;. Set to &quot;Kolmogoroff&quot; for non-parametric periodogram estimate.
</p>
</td></tr>
<tr><td><code id="innovationVariance_+3A_...">...</code></td>
<td>

<p>optional arguments that are passed to spec.pgram()
</p>
</td></tr>
</table>


<h3>Value</h3>

<p> the innovation variance
</p>


<h3>Author(s)</h3>

<p>A. I. McLeod
</p>


<h3>See Also</h3>

<p><code><a href="#topic+exactLoglikelihood">exactLoglikelihood</a></code>,
<code><a href="#topic+PredictionVariance">PredictionVariance</a></code>,
</p>


<h3>Examples</h3>

<pre><code class='language-R'>z&lt;-sunspot.year
#fitting high-order AR
innovationVariance(z)
#using periodogram
innovationVariance(z, method="Kolmogoroff")
#using smoothed periodogram
innovationVariance(z, method="Kolmogoroff", span=c(3, 3))
#the plot argument for spec.pgram() works too
innovationVariance(z, method="Kolmogoroff", span=c(3, 3), plot=TRUE)
</code></pre>

<hr>
<h2 id='is.toeplitz'> test if argument is a symmetric Toeplitz matrix </h2><span id='topic+is.toeplitz'></span>

<h3>Description</h3>

<p>Auxilary function, used to validate the input of TrenchInverse
</p>


<h3>Usage</h3>

<pre><code class='language-R'>is.toeplitz(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="is.toeplitz_+3A_x">x</code></td>
<td>
<p> value to be tested </p>
</td></tr>
</table>


<h3>Details</h3>

<p>A symmetric Toeplitz matrix of order n has (i,j)-entry of the
form g[abs(1+i-j)], where g is a vector of length n.
</p>


<h3>Value</h3>

<p> returns True or False according to whether x is or is not
a symmetric Toeplitz matrix
</p>


<h3>Author(s)</h3>

<p> A.I. McLeod </p>


<h3>See Also</h3>

 <p><code><a href="#topic+TrenchInverse">TrenchInverse</a></code>, <code><a href="stats.html#topic+toeplitz">toeplitz</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>is.toeplitz(toeplitz(1:5))
is.toeplitz(5)
</code></pre>

<hr>
<h2 id='ltsa-package'>
Linear Time Series Analysis
</h2><span id='topic+ltsa-package'></span><span id='topic+ltsa'></span>

<h3>Description</h3>

<p>Linear time series modelling.  
Methods are given for loglikelihood computation, forecasting and simulation.
</p>


<h3>Details</h3>


<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> ltsa</td>
</tr>
<tr>
 <td style="text-align: left;">
Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
Version: </td><td style="text-align: left;"> 1.4.5</td>
</tr>
<tr>
 <td style="text-align: left;">
Date: </td><td style="text-align: left;"> 2015-08-22</td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> GPL (&gt;= 2)</td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>


<table>
<tr>
 <td style="text-align: left;">
FUNCTION </td><td style="text-align: left;"> SUMMARY </td>
</tr>
<tr>
 <td style="text-align: left;">
DHSimulate </td><td style="text-align: left;"> Davies and Harte algorithm for time series simulation </td>
</tr>
<tr>
 <td style="text-align: left;">
DLAcfToAR </td><td style="text-align: left;"> from Acf to AR using Durbin-Levinson recursion </td>
</tr>
<tr>
 <td style="text-align: left;">
DLLoglikelihood </td><td style="text-align: left;"> exact loglikelihood using Durbin-Levinson algorithm </td>
</tr>
<tr>
 <td style="text-align: left;">
DLResiduals </td><td style="text-align: left;"> exact one-step residuals, Durbin-Levision algorithm </td>
</tr>
<tr>
 <td style="text-align: left;">
DLSimulate </td><td style="text-align: left;"> exact simulation of Gaussian time series using DL </td>
</tr>
<tr>
 <td style="text-align: left;">
is.toeplitz </td><td style="text-align: left;"> test for Toeplitz matrix </td>
</tr>
<tr>
 <td style="text-align: left;">
PredictionVariance </td><td style="text-align: left;"> two methods provided </td>
</tr>
<tr>
 <td style="text-align: left;">
tacvfARMA </td><td style="text-align: left;"> theoretical autocovariances </td>
</tr>
<tr>
 <td style="text-align: left;">
ToeplitzInverseUpdate </td><td style="text-align: left;"> update inverse </td>
</tr>
<tr>
 <td style="text-align: left;">
TrenchForecast </td><td style="text-align: left;"> general algorithm for forecasting </td>
</tr>
<tr>
 <td style="text-align: left;">
TrenchInverse </td><td style="text-align: left;"> efficient algorithm for inverse of Toeplitz matrix </td>
</tr>
<tr>
 <td style="text-align: left;">
TrenchLogLikelihood </td><td style="text-align: left;"> exact loglikelihood </td>
</tr>
<tr>
 <td style="text-align: left;">
TrenchMean </td><td style="text-align: left;"> exact MLE for mean </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>



<h3>Author(s)</h3>

<p>A. I. McLeod, Hao Yu and Zinovi Krougly.
</p>
<p>Maintainer: aimcleod@uwo.ca
</p>


<h3>References</h3>

<p>Hipel, K.W. and McLeod, A.I., (2005).
Time Series Modelling of Water Resources and Environmental Systems.
Electronic reprint of our book orginally published in 1994.
<a href="http://www.stats.uwo.ca/faculty/aim/1994Book/">http://www.stats.uwo.ca/faculty/aim/1994Book/</a>.
</p>
<p>McLeod, A.I., Yu, Hao, Krougly, Zinovi L.  (2007).
Algorithms for Linear Time Series Analysis,
Journal of Statistical Software.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+DHSimulate">DHSimulate</a></code>, 
<code><a href="#topic+DLAcfToAR">DLAcfToAR</a></code>,
<code><a href="#topic+DLLoglikelihood">DLLoglikelihood</a></code>,
<code><a href="#topic+DLResiduals">DLResiduals</a></code>,
<code><a href="#topic+DLSimulate">DLSimulate</a></code>,
<code><a href="#topic+exactLoglikelihood">exactLoglikelihood</a></code>,
<code><a href="#topic+is.toeplitz">is.toeplitz</a></code>,
<code><a href="#topic+PredictionVariance">PredictionVariance</a></code>,
<code><a href="#topic+tacvfARMA">tacvfARMA</a></code>,
<code><a href="#topic+ToeplitzInverseUpdate">ToeplitzInverseUpdate</a></code>,
<code><a href="#topic+TrenchForecast">TrenchForecast</a></code>,
<code><a href="#topic+TrenchInverse">TrenchInverse</a></code>,
<code><a href="#topic+TrenchLoglikelihood">TrenchLoglikelihood</a></code>,
<code><a href="#topic+TrenchMean">TrenchMean</a></code>,
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
#Example 1: DHSimulate
#First define acf for fractionally-differenced white noise and then simulate using DHSimulate
`tacvfFdwn` &lt;-
function(d, maxlag)
{
    x &lt;- numeric(maxlag + 1)
    x[1] &lt;- gamma(1 - 2 * d)/gamma(1 - d)^2
    for(i in 1:maxlag) 
        x[i + 1] &lt;- ((i - 1 + d)/(i - d)) * x[i]
    x
}
n&lt;-1000
rZ&lt;-tacvfFdwn(0.25, n-1) #length 1000
Z&lt;-DHSimulate(n, rZ)
acf(Z)

#Example 2: DLAcfToAR
#
n&lt;-10
d&lt;-0.4
r&lt;-tacvfFdwn(d, n)
r&lt;-(r/r[1])[-1]
HoskingPacf&lt;-d/(-d+(1:n))
cbind(DLAcfToAR(r),HoskingPacf)

#Example 3: DLLoglikelihood
#Using Z and rZ in Example 1.
DLLoglikelihood(rZ, Z)

#Example 4: DLResiduals
#Using Z and rZ in Example 1.
DLResiduals(rZ, Z)

#Example 5: DLSimulate
#Using Z in Example 1.
z&lt;-DLSimulate(n, rZ)
plot.ts(z)

#Example 6: is.toeplitz
is.toeplitz(toeplitz(1:5))

#Example 7: PredictionVariance
#Compare with predict.Arima
#general script, just change z, p, q, ML
z&lt;-sqrt(sunspot.year)
n&lt;-length(z)
p&lt;-9
q&lt;-0
ML&lt;-10
#for different data/model just reset above
out&lt;-arima(z, order=c(p,0,q))
sda&lt;-as.vector(predict(out, n.ahead=ML)$se)
#
phi&lt;-theta&lt;-numeric(0)
if (p&gt;0) phi&lt;-coef(out)[1:p]
if (q&gt;0) theta&lt;-coef(out)[(p+1):(p+q)]
zm&lt;-coef(out)[p+q+1]
sigma2&lt;-out$sigma2
r&lt;-sigma2*tacvfARMA(phi, theta, maxLag=n+ML-1)
sdb&lt;-sqrt(PredictionVariance(r, maxLead=ML))
cbind(sda,sdb)

#Example 8: tacfARMA
#There are two methods: tacvfARMA and ARMAacf.
#tacvfARMA is more general since it computes the autocovariances function
# given the ARMA parameters and the innovation variance whereas ARMAacf
# only computes the autocorrelations. Sometimes tacvfARMA is more suitable
# for what is needed and provides a better result than ARMAacf as in the
# the following example.
#
#general script, just change z, p, q, ML
z&lt;-sqrt(sunspot.year)
n&lt;-length(z)
p&lt;-9
q&lt;-0
ML&lt;-5
#for different data/model just reset above
out&lt;-arima(z, order=c(p,0,q))
phi&lt;-theta&lt;-numeric(0)
if (p&gt;0) phi&lt;-coef(out)[1:p]
if (q&gt;0) theta&lt;-coef(out)[(p+1):(p+q)]
zm&lt;-coef(out)[p+q+1]
sigma2&lt;-out$sigma2
rA&lt;-tacvfARMA(phi, theta, maxLag=n+ML-1, sigma2=sigma2)
rB&lt;-var(z)*ARMAacf(ar=phi, ma=theta, lag.max=n+ML-1)
#rA and rB are slighly different
cbind(rA[1:5],rB[1:5])


#Example 9: ToeplitzInverseUpdate
#In this example we compute the update inverse directly and using ToeplitzInverseUpdate and
#compare the result.
phi&lt;-0.8
sde&lt;-30
n&lt;-30
r&lt;-arima.sim(n=30,list(ar=phi),sd=sde)
r&lt;-phi^(0:(n-1))/(1-phi^2)*sde^2
n1&lt;-25
G&lt;-toeplitz(r[1:n1])
GI&lt;-solve(G) #could also use TrenchInverse
GIupdate&lt;-ToeplitzInverseUpdate(GI,r[1:n1],r[n1+1])
GIdirect&lt;-solve(toeplitz(r[1:(n1+1)]))
ERR&lt;-sum(abs(GIupdate-GIdirect))
ERR


#Example 10: TrenchForecast
#Compare TrenchForecast and predict.Arima
#general script, just change z, p, q, ML
z&lt;-sqrt(sunspot.year)
n&lt;-length(z)
p&lt;-9
q&lt;-0
ML&lt;-10
#for different data/model just reset above
out&lt;-arima(z, order=c(p,0,q))
Fp&lt;-predict(out, n.ahead=ML)
phi&lt;-theta&lt;-numeric(0)
if (p&gt;0) phi&lt;-coef(out)[1:p]
if (q&gt;0) theta&lt;-coef(out)[(p+1):(p+q)]
zm&lt;-coef(out)[p+q+1]
sigma2&lt;-out$sigma2
#r&lt;-var(z)*ARMAacf(ar=phi, ma=theta, lag.max=n+ML-1)
#When r is computed as above, it is not identical to below
r&lt;-sigma2*tacvfARMA(phi, theta, maxLag=n+ML-1)
F&lt;-TrenchForecast(z, r, zm, n, maxLead=ML)
#the forecasts are identical using tacvfARMA
#  


#Example 11: TrenchInverse
#invert a matrix of order n and compute the maximum absolute error
# in the product of this inverse with the original matrix
n&lt;-5	   
r&lt;-0.8^(0:(n-1))
G&lt;-toeplitz(r)
Gi&lt;-TrenchInverse(G)
GGi&lt;-crossprod(t(G),Gi)
id&lt;-matrix(0, nrow=n, ncol=n)
diag(id)&lt;-1
err&lt;-max(abs(id-GGi))
err


#Example 12: TrenchLoglikelihood
#simulate a time series and compute the concentrated loglikelihood using DLLoglikelihood and
#compare this with the value given by TrenchLoglikelihood.
phi&lt;-0.8
n&lt;-200
r&lt;-phi^(0:(n-1))
z&lt;-arima.sim(model=list(ar=phi), n=n)
LD&lt;-DLLoglikelihood(r,z)
LT&lt;-TrenchLoglikelihood(r,z)
ans&lt;-c(LD,LT)
names(ans)&lt;-c("DLLoglikelihood","TrenchLoglikelihood")

#Example 13: TrenchMean
phi&lt;- -0.9
a&lt;-rnorm(100)
z&lt;-numeric(length(a))
phi&lt;- -0.9
n&lt;-100
a&lt;-rnorm(n)
z&lt;-numeric(n)
mu&lt;-100
sig&lt;-10
z[1]&lt;-a[1]*sig/sqrt(1-phi^2)
for (i in 2:n)
	z[i]&lt;-phi*z[i-1]+a[i]*sig
z&lt;-z+mu
r&lt;-phi^(0:(n-1))
meanMLE&lt;-TrenchMean(r,z)
meanBLUE&lt;-mean(z)
ans&lt;-c(meanMLE, meanBLUE)
names(ans)&lt;-c("BLUE", "MLE")
ans


</code></pre>

<hr>
<h2 id='PredictionVariance'> Prediction variance</h2><span id='topic+PredictionVariance'></span>

<h3>Description</h3>

<p>The prediction variance of the forecast for lead times l=1,...,maxLead
is computed given theoretical autocovariances. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>PredictionVariance(r, maxLead = 1, DLQ = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="PredictionVariance_+3A_r">r</code></td>
<td>
<p> the autocovariances at lags 0, 1, 2, ... </p>
</td></tr>
<tr><td><code id="PredictionVariance_+3A_maxlead">maxLead</code></td>
<td>
<p> maximum lead time of forecast </p>
</td></tr>
<tr><td><code id="PredictionVariance_+3A_dlq">DLQ</code></td>
<td>
<p> Using Durbin-Levinson if TRUE. Otherwise Trench algorithm used. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>Two algorithms are available which 
are described in detail in McLeod, Yu and Krougly (2007).
The default method, DLQ=TRUE, uses the autocovariances provided in r to
determine the optimal linear mean-square error predictor of order
length(r)-1.  
The mean-square error of this predictor is the lead-one error variance.
The moving-average expansion of this model is used to compute any
remaining variances (McLeod, Yu and Krougly, 2007).
With the other Trench algorithm, when DLQ=FALSE, a direct matrix representation
of the forecast variances is used (McLeod, Yu and Krougly, 2007).
The Trench method is exact.  Provided the length of r is large enough,
the two methods will agree.
</p>


<h3>Value</h3>

<p>vector of length maxLead containing the variances
</p>


<h3>Author(s)</h3>

<p> A.I. McLeod </p>


<h3>References</h3>

 
<p>McLeod, A.I., Yu, Hao, Krougly, Zinovi L.  (2007).
Algorithms for Linear Time Series Analysis,
Journal of Statistical Software.
</p>


<h3>See Also</h3>

 
<p><code><a href="stats.html#topic+predict.Arima">predict.Arima</a></code>, 
<code><a href="#topic+TrenchForecast">TrenchForecast</a></code>,
<code><a href="#topic+exactLoglikelihood">exactLoglikelihood</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#Example 1. Compare using DL method or Trench method
va&lt;-PredictionVariance(0.9^(0:10), maxLead=10)
vb&lt;-PredictionVariance(0.9^(0:10), maxLead=10, DLQ=FALSE)
cbind(va,vb)
# 
#Example 2. Compare with predict.Arima
#general script, just change z, p, q, ML
z&lt;-sqrt(sunspot.year)
n&lt;-length(z)
p&lt;-9
q&lt;-0
ML&lt;-10
#for different data/model just reset above
out&lt;-arima(z, order=c(p,0,q))
sda&lt;-as.vector(predict(out, n.ahead=ML)$se)
#
phi&lt;-theta&lt;-numeric(0)
if (p&gt;0) phi&lt;-coef(out)[1:p]
if (q&gt;0) theta&lt;-coef(out)[(p+1):(p+q)]
zm&lt;-coef(out)[p+q+1]
sigma2&lt;-out$sigma2
r&lt;-sigma2*tacvfARMA(phi, theta, maxLag=n+ML-1)
sdb&lt;-sqrt(PredictionVariance(r, maxLead=ML))
cbind(sda,sdb)
#
# 
#Example 3. DL and Trench method can give different results
#  when the acvf is slowly decaying. Trench is always
#  exact based on a finite-sample.
L&lt;-5
r&lt;-1/sqrt(1:(L+1))
va&lt;-PredictionVariance(r, maxLead=L)
vb&lt;-PredictionVariance(r, maxLead=L, DLQ=FALSE)
cbind(va,vb) #results are slightly different
r&lt;-1/sqrt(1:(1000)) #larger number of autocovariances
va&lt;-PredictionVariance(r, maxLead=L)
vb&lt;-PredictionVariance(r, maxLead=L, DLQ=FALSE)
cbind(va,vb) #results now agree
</code></pre>

<hr>
<h2 id='SimGLP'> Simulate GLP given innovations</h2><span id='topic+SimGLP'></span>

<h3>Description</h3>

<p>Simulates a General Linear Time Series that can have nonGaussian innovations.
It uses the FFT so it is O(N log(N)) flops where N=length(a) and N is assumed
to be a power of 2. 
The R function <code>convolve</code> is used which implements the FFT.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>SimGLP(psi, a)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="SimGLP_+3A_psi">psi</code></td>
<td>
<p> vector, length Q, of MA coefficients starting with 1. </p>
</td></tr>
<tr><td><code id="SimGLP_+3A_a">a</code></td>
<td>
<p> vector, length Q+n, of innovations, where n is the length of time series
to be generated. </p>
</td></tr>
</table>


<h3>Details</h3>

<p style="text-align: center;"><code class="reqn"> z_t = \sum_{k=0}^Q psi_k a_{t-k} </code>
</p>

<p>where <code class="reqn">t=1,\ldots,n</code> and the innovations
$a_t, t=1-Q, ..., 0, 1, ..., n$ are
given in the input vector a.
</p>
<p>Since <code>convolve</code> uses the FFT this is faster than direct computation.
</p>


<h3>Value</h3>

<p>vector of length n, where n=length(a)-length(psi)
</p>


<h3>Author(s)</h3>

<p> A.I. McLeod </p>


<h3>See Also</h3>

 
<p><code><a href="stats.html#topic+convolve">convolve</a></code>, 
<code><a href="stats.html#topic+arima.sim">arima.sim</a></code> 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#Simulate an AR(1) process with parameter phi=0.8 of length n=100 with
#  innovations from a t-distribution with 5 df and plot it.
#
phi&lt;-0.8
psi&lt;-phi^(0:127)
n&lt;-100
Q&lt;-length(psi)-1
a&lt;-rt(n+Q,5)
z&lt;-SimGLP(psi,a)
z&lt;-ts(z)
plot(z)
</code></pre>

<hr>
<h2 id='tacvfARMA'> theoretical autocovariance function (acvf) of ARMA </h2><span id='topic+tacvfARMA'></span>

<h3>Description</h3>

<p>The theoretical autocovariance function of ARMA(p,q) process is computed.
This is more useful in some situations than the built-in R function <code><a href="stats.html#topic+ARMAacf">ARMAacf</a></code>.
See Details.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tacvfARMA(phi = numeric(0), theta = numeric(0), maxLag = 1, sigma2 = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tacvfARMA_+3A_phi">phi</code></td>
<td>
<p> ar parameters </p>
</td></tr>
<tr><td><code id="tacvfARMA_+3A_theta">theta</code></td>
<td>
<p> ma parameters </p>
</td></tr>
<tr><td><code id="tacvfARMA_+3A_maxlag">maxLag</code></td>
<td>
<p> acvf is computed at lags 0, ..., maxLag </p>
</td></tr>
<tr><td><code id="tacvfARMA_+3A_sigma2">sigma2</code></td>
<td>
<p> innovation variance </p>
</td></tr>
</table>


<h3>Details</h3>

<p>The details of the autocovariance computation are given in McLeod (1975).
</p>
<p>In addition to this computation, we also test if the model is stationary-causal
or not.  The test, which is included directly in the function, uses
the Durbin-Levison recursion to transform from the phi parameters to the pacf.
See McLeod and Zhang (2006, eqn. (1)) for more details.
Formally, the stationary-causal condition requires that all roots of the
polynomial equation, 
</p>
<p style="text-align: center;"><code class="reqn">1 - phi[1]*B -...- phi[p]*B^p = 0 </code>
</p>
 
<p>must lie outside
the unit circle (Brockwell and Davis, 1991, Section 3.3).
</p>
<p>This function is included because it is necessary to demonstrate that in
the case of ARMA models, TrenchInverse and the built-in R function
predict.Arima produce equivalent results.
See Example 1 in the documentation for <code><a href="#topic+TrenchForecast">TrenchForecast</a></code>
and the example discussed in McLeod, Yu and Krougly (2007, 3.2). 
</p>


<h3>Value</h3>

<p>Vector of length maxLag containing the autocovariances at
lags 0, ..., maxLag. But see Warning below.
</p>


<h3>Note</h3>

<p>An error is returned if the model is not stationary-causal.
</p>


<h3>Author(s)</h3>

<p>A.I. McLeod </p>


<h3>References</h3>

  
<p>P.J. Brockwell and R.A. Davis (1991)
Time Series: Theory and Methods. Springer.
</p>
<p>A.I. McLeod (1975) 
Derivation of the theoretical autocovariance function of autoregressive-moving
average models, Applied Statistics 24, 255-256.
</p>
<p>A.I. McLeod and Zhang, Y. (2006)
Partial autocorrelation parameterizations for subset autoregression,
Journal of Time Series Analysis,  
</p>
<p>McLeod, A.I., Yu, Hao, Krougly, Zinovi L.  (2007).
Algorithms for Linear Time Series Analysis,
Journal of Statistical Software.
</p>


<h3>See Also</h3>

 
<p><code><a href="stats.html#topic+ARMAacf">ARMAacf</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#Example 1.  Estimate the acvf of a fitted ARMA model
#There are two methods but they give slighly different results,
#general script, just change z, p, q, ML
z&lt;-sqrt(sunspot.year)
n&lt;-length(z)
p&lt;-9
q&lt;-0
ML&lt;-5
#for different data/model just reset above
out&lt;-arima(z, order=c(p,0,q))
phi&lt;-theta&lt;-numeric(0)
if (p&gt;0) phi&lt;-coef(out)[1:p]
if (q&gt;0) theta&lt;-coef(out)[(p+1):(p+q)]
zm&lt;-coef(out)[p+q+1]
sigma2&lt;-out$sigma2
rA&lt;-tacvfARMA(phi, theta, maxLag=n+ML-1, sigma2=sigma2)
rB&lt;-var(z)*ARMAacf(ar=phi, ma=theta, lag.max=n+ML-1)
#rA and rB are slighly different
cbind(rA[1:5],rB[1:5])
#
#Example 2. Compute Rsq for fitted ARMA model
#Rsq = 1 - (series variance / innovation variance)
#Again there are two methods but only the first method is guaranteed to
#produce an Rsq which is non-negative!
#Run last example and then evaluate the script below:
RsqA &lt;- 1 - rA/sigma2
RsqB &lt;- 1 - rB/sigma2
#
#Example 3. Test if model is stationary-causal or not.
StationaryQ &lt;- function(phi) tryCatch(is.vector(tacvfARMA(phi=phi)),error=function(e) FALSE )
StationaryQ(1.1) #AR(1) with phi=1.1 is not stationary-causal.
#try with parameters from Example 1 above
StationaryQ(phi)
</code></pre>

<hr>
<h2 id='ToeplitzInverseUpdate'> Inverse of Toeplitz matrix of order n+1 given inverse of order n </h2><span id='topic+ToeplitzInverseUpdate'></span>

<h3>Description</h3>

<p>Let G be a Toeplitz matrix of order n and with (i,j)-element, r[Abs[i-j]].
So the first row of G may be written (r[0],...,r[n-1]).  Suppose the next
element in the sequence is r[n].  Then the inverse of the Toeplitz matrix
whose first row is (r[0],...,r[n]) may be obtained either using
ToeplitzInverseUpdate or directly using TrenchInverse.  ToeplitzInverseUpdate
is somewhat faster. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ToeplitzInverseUpdate(GI, r, rnew)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ToeplitzInverseUpdate_+3A_gi">GI</code></td>
<td>
<p> inverse of Toeplitz matrix G of order n </p>
</td></tr>
<tr><td><code id="ToeplitzInverseUpdate_+3A_r">r</code></td>
<td>
<p> first row of G , ie r[0],...,r[n-1]</p>
</td></tr>
<tr><td><code id="ToeplitzInverseUpdate_+3A_rnew">rnew</code></td>
<td>
<p> next element, r[n] </p>
</td></tr>
</table>


<h3>Details</h3>

<p>Although this update requires <code class="reqn">O(n^2)</code> flops, the same as TrenchInverse,
it is somewhat faster in practice.
</p>


<h3>Value</h3>

<p>inverse matrix of order n+1
</p>


<h3>Author(s)</h3>

<p> A.I. McLeod </p>


<h3>References</h3>

<p>Graybill, F.A. (1983). Matrices with Applications in Statistics.
</p>
<p>McLeod, A.I., Yu, Hao, Krougly, Zinovi L.  (2007).
Algorithms for Linear Time Series Analysis,
Journal of Statistical Software.
</p>


<h3>See Also</h3>

 
<p><code><a href="#topic+TrenchInverse">TrenchInverse</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#In this example we compute the update inverse directly and using ToeplitzInverseUpdate and
#compare the result.
phi&lt;-0.8
sde&lt;-30
n&lt;-30
r&lt;-arima.sim(n=30,list(ar=phi),sd=sde)
r&lt;-phi^(0:(n-1))/(1-phi^2)*sde^2
n1&lt;-25
G&lt;-toeplitz(r[1:n1])
GI&lt;-solve(G) #could also use TrenchInverse
GIupdate&lt;-ToeplitzInverseUpdate(GI,r[1:n1],r[n1+1])
GIdirect&lt;-solve(toeplitz(r[1:(n1+1)]))
ERR&lt;-sum(abs(GIupdate-GIdirect))
ERR
</code></pre>

<hr>
<h2 id='TrenchForecast'> Minimum Mean Square Forecast</h2><span id='topic+TrenchForecast'></span>

<h3>Description</h3>

<p>Given time series of length n+m, the forecasts for lead times k=1,...,L are 
computed starting with forecast origin at time t=n and continuing up to t=n+m.
The input time series is of length n+m.
For purely out-of-sample forecasts we may take n=length(z).
Note that the parameter m is inferred using the fact that m=length(z)-n.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>TrenchForecast(z, r, zm, n, maxLead, UpdateAlgorithmQ = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="TrenchForecast_+3A_z">z</code></td>
<td>
<p>time series data, length n+m  </p>
</td></tr>
<tr><td><code id="TrenchForecast_+3A_r">r</code></td>
<td>
<p>autocovariances of length(z)+L-1 or until damped out  </p>
</td></tr>
<tr><td><code id="TrenchForecast_+3A_zm">zm</code></td>
<td>
<p>mean parameter in model  </p>
</td></tr>
<tr><td><code id="TrenchForecast_+3A_n">n</code></td>
<td>
<p>forecast origin, n </p>
</td></tr>
<tr><td><code id="TrenchForecast_+3A_maxlead">maxLead</code></td>
<td>
<p> =L, the maximum lead time </p>
</td></tr>
<tr><td><code id="TrenchForecast_+3A_updatealgorithmq">UpdateAlgorithmQ</code></td>
<td>
<p> = TRUE, use efficient update method, otherwise if
UpdateAlgorithmQ=FALSE, the direct inverse matrix is computed each time</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The minimum mean-square error forecast of z[N+k] given time series data z[1],...,z[N]
is denoted by <code class="reqn">z_N(k)</code>, where N is called the forecast origin and k is
the lead time.
This algorithm computes a table for 
<code class="reqn">z_N(k), N=n,\dots,n+m; k=1,\ldots,m</code> 
The minimum mean-square error forecast is simply the conditional expectation
of <code class="reqn">z_{N+k}</code> given the time series up to including time <code class="reqn">t=N</code>.
This conditional expectation works out to the same thing as the conditional
expectation in an appropriate multivariate normal distribution &ndash; even
if no normality assumption is made.
See McLeod, Yu, Krougly (2007, eqn. 8).
Similar remarks hold for the variance of the forecast.
An error message is given if length(r) &lt; n + L -1.
</p>


<h3>Value</h3>

<p>A list with components
</p>
<table>
<tr><td><code>Forecasts</code></td>
<td>
<p>matrix with m+1 rows and maxLead columns with the forecasts</p>
</td></tr>
<tr><td><code>SDForecasts</code></td>
<td>
<p>matrix with m+1 rows and maxLead columns with the sd of the forecasts</p>
</td></tr>
</table>


<h3>Note</h3>

<p>An error message is given if r is not a pd sequence, that is, the Toeplitz
matrix of r must be pd.
This could occur if you were to approximate a GLP which is near the stationary
boundary by a MA(Q) with Q not large enough.
In the bootstrap simulation experiment reported in our paper McLeod, Yu and Krougly (2007)
we initially approximated the FGN autocorrelations by setting them to zero after lag
553 but in this case the ARMA(2,1) forecasts were always better.
When we used all required lags of the acvf then the FGN forecasts were better as
we expected. 
From this experience, we don't recommend setting high-order acf lags to zero unless
the values are in fact very small.
</p>


<h3>Author(s)</h3>

<p> A.I. McLeod </p>


<h3>References</h3>

<p>McLeod, A.I., Yu, Hao, Krougly, Zinovi L.  (2007).
Algorithms for Linear Time Series Analysis,
Journal of Statistical Software.
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+TrenchInverse">TrenchInverse</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>#Example 1. Compare TrenchForecast and predict.Arima
#general script, just change z, p, q, ML
z&lt;-sqrt(sunspot.year)
n&lt;-length(z)
p&lt;-9
q&lt;-0
ML&lt;-10
#for different data/model just reset above
out&lt;-arima(z, order=c(p,0,q))
Fp&lt;-predict(out, n.ahead=ML)

phi&lt;-theta&lt;-numeric(0)
if (p&gt;0) phi&lt;-coef(out)[1:p]
if (q&gt;0) theta&lt;-coef(out)[(p+1):(p+q)]
zm&lt;-coef(out)[p+q+1]
sigma2&lt;-out$sigma2
#r&lt;-var(z)*ARMAacf(ar=phi, ma=theta, lag.max=n+ML-1)
#When r is computed as above, it is not identical to below
r&lt;-sigma2*tacvfARMA(phi, theta, maxLag=n+ML-1)
F&lt;-TrenchForecast(z, r, zm, n, maxLead=ML)
#the forecasts are identical using tacvfARMA
#    
#Example 2. Compare AR(1) Forecasts.  Show how 
#Forecasts from AR(1) are easily calculated directly. 
#We compare AR(1) forecasts and their sd's.
#Define a function for the AR(1) case
AR1Forecast &lt;- function(z,phi,n,maxLead){
        nz&lt;-length(z)
        m&lt;-nz-n
        zf&lt;-vf&lt;-matrix(numeric(maxLead*m),ncol=maxLead)
        zorigin&lt;-z[n:nz]
        zf&lt;-outer(zorigin,phi^(1:maxLead))
        vf&lt;-matrix(rep(1-phi^(2*(1:maxLead)),m+1),byrow=TRUE,ncol=maxLead)/(1-phi^2)
        list(zf=zf,sdf=sqrt(vf))
        }
#generate AR(1) series and compare the forecasts
phi&lt;-0.9
n&lt;-200
m&lt;-5
N&lt;-n+m
z&lt;-arima.sim(list(ar=phi), n=N)
maxLead&lt;-3
nr&lt;-N+maxLead-1
r&lt;-(1/(1-phi^2))*phi^(0:nr) 
ansT1&lt;-TrenchForecast(z,r,0,n,maxLead)
ansT2&lt;-TrenchForecast(z,r,0,n,maxLead,UpdateAlgorithmQ=FALSE)
ansAR1&lt;-AR1Forecast(z,phi,n,maxLead)
</code></pre>

<hr>
<h2 id='TrenchInverse'>compute the matrix inverse of a positive-definite Toepliz matrix </h2><span id='topic+TrenchInverse'></span>

<h3>Description</h3>

<p>The Trench algorithm (Golub and Vanload, 1983) is implemented in C and
interfaced to R.
This provides an expedient method for obtaining the
matrix inverse of the covariance matrix of n successive observations
from a stationary time series.
Some applications of this are discussed by McLeod and Krougly (2005).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>TrenchInverse(G)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="TrenchInverse_+3A_g">G</code></td>
<td>
<p> a positive definite Toeplitz matrix </p>
</td></tr>
</table>


<h3>Value</h3>

<p>the matrix inverse of G is computed</p>


<h3>Warning </h3>

<p>You should test the input x using is.toeplitz(x) if you are not sure if
x is a symmetric Toeplitz matix.
</p>


<h3>Note</h3>

<p>TrenchInverse(x) assumes that x is a symmetric Toeplitz matrix but it
does not specifically test for this.
Instead it merely takes the first row of x and passes this directly
to the C code program which uses this more compact storage format.
The C code program then computes the inverse. 
An error message is given if the C code algorithm encounters
a non-positive definite input.
</p>


<h3>Author(s)</h3>

<p> A.I. McLeod </p>


<h3>References</h3>

 
<p>Golub, G. and Van Loan (1983).
Matrix Computations, 2nd Ed.
John Hoptkins University Press, Baltimore.
Algorithm 5.7-3.
</p>
<p>McLeod, A.I., Yu, Hao, Krougly, Zinovi L.  (2007).
Algorithms for Linear Time Series Analysis,
Journal of Statistical Software.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+TrenchLoglikelihood">TrenchLoglikelihood</a></code>, <code><a href="#topic+is.toeplitz">is.toeplitz</a></code>, 
<code><a href="#topic+DLLoglikelihood">DLLoglikelihood</a></code>, 
<code><a href="#topic+TrenchMean">TrenchMean</a></code>, <code><a href="Matrix.html#topic+solve">solve</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#compute inverse of matrix and compare with result from solve
data(LakeHuron)
r&lt;-acf(LakeHuron, plot=FALSE, lag.max=4)$acf
R&lt;-toeplitz(c(r))
Ri&lt;-TrenchInverse(R)
Ri2&lt;-solve(R)
Ri
Ri2

#invert a matrix of order n and compute the maximum absolute error
# in the product of this inverse with the original matrix
n&lt;-5	   
r&lt;-0.8^(0:(n-1))
G&lt;-toeplitz(r)
Gi&lt;-TrenchInverse(G)
GGi&lt;-crossprod(t(G),Gi)
id&lt;-matrix(0, nrow=n, ncol=n)
diag(id)&lt;-1
err&lt;-max(abs(id-GGi))
err
</code></pre>

<hr>
<h2 id='TrenchLoglikelihood'>Loglikelihood function of stationary time series
using Trench algorithm</h2><span id='topic+TrenchLoglikelihood'></span>

<h3>Description</h3>

<p>The Trench matrix inversion algorithm is used to compute the
exact concentrated loglikelihood function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>TrenchLoglikelihood(r, z)</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="TrenchLoglikelihood_+3A_r">r</code></td>
<td>
<p>autocovariance or autocorrelation at lags 0,...,n-1, where n is length(z) </p>
</td></tr>
<tr><td><code id="TrenchLoglikelihood_+3A_z">z</code></td>
<td>
<p>time series data</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The concentrated loglikelihood function may be written Lm(beta) = -(n/2)*log(S/n)-0.5*g,
where beta is the parameter vector, n is the length of the time series, S=z'M z,
z is the mean-corrected time series, M is the inverse of the covariance matrix setting
the innovation variance to one and g=-log(det(M)).
</p>


<h3>Value</h3>

<p>The loglikelihood concentrated over the parameter for the innovation
variance is returned.
</p>


<h3>Author(s)</h3>

<p> A.I. McLeod </p>


<h3>References</h3>

 
<p>McLeod, A.I., Yu, Hao, Krougly, Zinovi L.  (2007).
Algorithms for Linear Time Series Analysis,
Journal of Statistical Software.
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+DLLoglikelihood">DLLoglikelihood</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>#compute loglikelihood for white noise
z&lt;-rnorm(100)
TrenchLoglikelihood(c(1,rep(0,length(z)-1)), z)


#simulate a time series and compute the concentrated loglikelihood using DLLoglikelihood and
#compare this with the value given by TrenchLoglikelihood.
phi&lt;-0.8
n&lt;-200
r&lt;-phi^(0:(n-1))
z&lt;-arima.sim(model=list(ar=phi), n=n)
LD&lt;-DLLoglikelihood(r,z)
LT&lt;-TrenchLoglikelihood(r,z)
ans&lt;-c(LD,LT)
names(ans)&lt;-c("DLLoglikelihood","TrenchLoglikelihood")
</code></pre>

<hr>
<h2 id='TrenchMean'> Exact MLE for mean given the autocorrelation function </h2><span id='topic+TrenchMean'></span>

<h3>Description</h3>

<p>Sometimes this is also referred to as the BLUE.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>TrenchMean(r, z)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="TrenchMean_+3A_r">r</code></td>
<td>
<p> vector of autocorrelations or autocovariances of length n </p>
</td></tr>
<tr><td><code id="TrenchMean_+3A_z">z</code></td>
<td>
<p> time series data vector of length n </p>
</td></tr>
</table>


<h3>Value</h3>

<p>the estimate of the mean
</p>


<h3>Note</h3>

<p> An error is given if r is not a postive-definite sequence or
if the lengths of <code>r</code> and <code>z</code> are not equal.
</p>


<h3>Author(s)</h3>

<p> A.I. McLeod </p>


<h3>References</h3>

 
<p>McLeod, A.I., Yu, Hao, Krougly, Zinovi L.  (2007).
Algorithms for Linear Time Series Analysis,
Journal of Statistical Software.
</p>


<h3>See Also</h3>

 <p><code><a href="#topic+TrenchInverse">TrenchInverse</a></code> </p>


<h3>Examples</h3>

<pre><code class='language-R'>#compare BLUE and sample mean
phi&lt;- -0.9
a&lt;-rnorm(100)
z&lt;-numeric(length(a))
phi&lt;- -0.9
n&lt;-100
a&lt;-rnorm(n)
z&lt;-numeric(n)
mu&lt;-100
sig&lt;-10
z[1]&lt;-a[1]*sig/sqrt(1-phi^2)
for (i in 2:n)
	z[i]&lt;-phi*z[i-1]+a[i]*sig
z&lt;-z+mu
r&lt;-phi^(0:(n-1))
meanMLE&lt;-TrenchMean(r,z)
meanBLUE&lt;-mean(z)
ans&lt;-c(meanMLE, meanBLUE)
names(ans)&lt;-c("BLUE", "MLE")
ans
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
