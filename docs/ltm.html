<!DOCTYPE html><html><head><title>Help for package ltm</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {ltm}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#ltm-package'>
<p>Latent Trait Models for Item Response Theory Analyses</p></a></li>
<li><a href='#Abortion'><p> Attitude Towards Abortion</p></a></li>
<li><a href='#anova'><p> Anova method for fitted IRT models</p></a></li>
<li><a href='#biserial.cor'>
<p>Point-Biserial Correlation</p>
</p></a></li>
<li><a href='#coef'><p> Extract Estimated Loadings</p></a></li>
<li><a href='#cronbach.alpha'>
<p>Cronbach's alpha</p>
</p></a></li>
<li><a href='#descript'>
<p>Descriptive Statistics</p></a></li>
<li><a href='#Environment'><p>Attitude to the Environment</p></a></li>
<li><a href='#factor.scores'><p> Factor Scores - Ability Estimates</p></a></li>
<li><a href='#fitted'><p> Fitted Values for IRT model</p></a></li>
<li><a href='#gh'><p>Gauss-Hermite Quadrature Points</p></a></li>
<li><a href='#GoF'><p> Goodness of Fit for Rasch Models</p></a></li>
<li><a href='#gpcm'><p>Generalized Partial Credit Model - Polytomous IRT</p></a></li>
<li><a href='#grm'><p> Graded Response Model - Polytomous IRT</p></a></li>
<li><a href='#information'>
<p>Area under the Test or Item Information Curves</p></a></li>
<li><a href='#item.fit'><p> Item-Fit Statistics and P-values</p></a></li>
<li><a href='#LSAT'><p> The Law School Admission Test (LSAT), Section VI</p></a></li>
<li><a href='#ltm'><p> Latent Trait Model - Latent Variable Model for Binary Data</p></a></li>
<li><a href='#margins'>
<p>Fit of the model on the margins</p></a></li>
<li><a href='#Mobility'><p> Women's Mobility</p></a></li>
<li><a href='#mult.choice'>
<p>Multiple Choice Items to Binary Responses</p></a></li>
<li><a href='#person.fit'><p> Person-Fit Statistics and P-values</p></a></li>
<li><a href='#plot descript'><p>Descriptive Statistics Plot method</p></a></li>
<li><a href='#plot fscores'><p>Factor Scores - Ability Estimates Plot method</p></a></li>
<li><a href='#plot IRT'><p> Plot method for fitted IRT models</p></a></li>
<li><a href='#rasch'><p> Rasch Model</p></a></li>
<li><a href='#rcor.test'>
<p>Pairwise Associations between Items using a Correlation Coefficient</p></a></li>
<li><a href='#residuals'><p> Residuals for IRT models</p></a></li>
<li><a href='#rmvlogis'>
<p>Generate Random Responses Patterns under Dichotomous and Polytomous IRT models</p></a></li>
<li><a href='#Science'><p>Attitude to Science and Technology</p></a></li>
<li><a href='#summary'><p> Summary method for fitted IRT models</p></a></li>
<li><a href='#testEquatingData'>
<p>Prepares Data for Test Equating</p></a></li>
<li><a href='#tpm'><p> Birnbaum's Three Parameter Model</p></a></li>
<li><a href='#unidimTest'><p> Unidimensionality Check using Modified Parallel Analysis</p></a></li>
<li><a href='#vcov'><p> vcov method for fitted IRT models</p></a></li>
<li><a href='#WIRS'><p>Workplace Industrial Relation Survey Data</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Latent Trait Models under IRT</td>
</tr>
<tr>
<td>Version:</td>
<td>1.2-0</td>
</tr>
<tr>
<td>Date:</td>
<td>2022-02-18</td>
</tr>
<tr>
<td>Author:</td>
<td>Dimitris Rizopoulos &lt;d.rizopoulos@erasmusmc.nl&gt;</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Dimitris Rizopoulos &lt;d.rizopoulos@erasmusmc.nl&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Analysis of multivariate dichotomous and polytomous data using latent trait models under the Item Response Theory approach. It includes the Rasch, the Two-Parameter Logistic, the Birnbaum's Three-Parameter, the Graded Response, and the Generalized Partial Credit Models.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.0.0), MASS, msm, polycor</td>
</tr>
<tr>
<td>LazyLoad:</td>
<td>yes</td>
</tr>
<tr>
<td>LazyData:</td>
<td>yes</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/drizopoulos/ltm">https://github.com/drizopoulos/ltm</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-02-18 09:10:55 UTC; drizo</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-02-18 09:40:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='ltm-package'>
Latent Trait Models for Item Response Theory Analyses
</h2><span id='topic+ltm-package'></span>

<h3>Description</h3>

<p>This package provides a flexible framework for Item Response Theory analyses for dichotomous and polytomous
data under a Marginal Maximum Likelihood approach. The fitting algorithms provide valid inferences under Missing At 
Random missing data mechanisms.
</p>


<h3>Details</h3>


<table>
<tr>
 <td style="text-align: left;">
Package: </td><td style="text-align: left;"> ltm</td>
</tr>
<tr>
 <td style="text-align: left;">
Type: </td><td style="text-align: left;"> Package</td>
</tr>
<tr>
 <td style="text-align: left;">
Version: </td><td style="text-align: left;"> 1.2-0</td>
</tr>
<tr>
 <td style="text-align: left;">
Date: </td><td style="text-align: left;"> 2022-02-18</td>
</tr>
<tr>
 <td style="text-align: left;">
License: </td><td style="text-align: left;"> <abbr><span class="acronym">GPL</span></abbr> </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>

<p>The following options are available:
</p>

<dl>
<dt>Descriptives:</dt><dd><p>samples proportions, missing values information, biserial correlation of items with total score, 
pairwise associations between items, Cronbach's <code class="reqn">\alpha</code>, unidimensionality check using modified 
parallel analysis, nonparametric correlation coefficient, plotting of sample proportions versus total score.</p>
</dd>
<dt>Dichotomous data:</dt><dd><p>Rasch Model, Two Parameter Logistic Model, Birnbaum's Three Parameter Model, and 
Latent Trait Model up to two latent variables (allowing also for nonlinear terms between the latent traits).</p>
</dd>
<dt>Polytomous data:</dt><dd><p>Samejima's Graded Response Model and the Generalized Partial Credit Model.</p>
</dd>
<dt>Goodness-of-Fit:</dt><dd><p>Bootstrapped Pearson <code class="reqn">\chi^2</code> for Rasch and Generalized Partial Credit models, fit on the two- and three-way margins 
for all models, likelihood ratio tests between nested models (including AIC and BIC criteria values),
and item- and person-fit statistics.</p>
</dd>
<dt>Factor Scoring - Ability Estimates:</dt><dd><p>Empirical Bayes (i.e., posterior modes), Expected a posteriori (i.e., 
posterior means), Multiple Imputed Empirical Bayes, and Component Scores for dichotomous data.</p>
</dd>
<dt>Test Equating:</dt><dd><p>Alternate Form Equating (where common and unique items are analyzed simultaneously) and Across 
Sample Equating (where different sets of unique items are analyzed separately based on previously calibrated 
anchor items).</p>
</dd>
<dt>Plotting:</dt><dd><p>Item Characteristic Curves, Item Information Curves, Test Information Functions, Standard Error 
of Measurement, Standardized Loadings Scatterplot (for the two-factor latent trait model), Item Operation 
Characteristic Curves (for ordinal polytomous data), Item Person Maps.</p>
</dd>  
</dl>



<h3>Author(s)</h3>

<p>Dimitris Rizopoulos
</p>
<p>Maintainer: Dimitris Rizopoulos &lt;d.rizopoulos@erasmusmc.nl&gt;
</p>


<h3>References</h3>

<p>Baker, F. and Kim, S-H. (2004) <em>Item Response Theory</em>, 2nd ed. 
New York: Marcel Dekker.
</p>
<p>Rizopoulos, D. (2006) <b>ltm</b>: An R package for latent variable modelling and item response theory analyses. 
<em>Journal of Statistical Software</em>, <b>17(5)</b>, 1&ndash;25. URL doi: <a href="https://doi.org/10.18637/jss.v017.i05">10.18637/jss.v017.i05</a>
</p>

<hr>
<h2 id='Abortion'> Attitude Towards Abortion </h2><span id='topic+Abortion'></span>

<h3>Description</h3>

<p>The data contain responses given by 410 individuals to four out of seven items
concerning attitude to abortion. A small number of individual did not answer
to some of the questions and this data set contains only the complete cases.
</p>


<h3>Format</h3>

<p>379 individuals answered to the following questions after being asked if
the law should allow abortion under the circumstances presented under each item,
</p>

<dl>
<dt>Item 1</dt><dd><p>The woman decides on her own that she does not.</p>
</dd>
<dt>Item 2</dt><dd><p>The couple agree that they do not wish to have a child.</p>
</dd>
<dt>Item 3</dt><dd><p>The woman is not married and does not wish to marry the man.</p>
</dd>
<dt>Item 4</dt><dd><p>The couple cannot afford any more children.</p>
</dd>
</dl>



<h3>Source</h3>

<p>1986 British Social Attitudes Survey (McGrath and Waterton, 1986).
</p>


<h3>References</h3>

<p>Bartholomew, D., Steel, F., Moustaki, I. and Galbraith, J. (2002) <em>The Analysis and Interpretation of 
Multivariate Data for Social Scientists</em>. London: Chapman and Hall.
</p>
<p>Knott, M., Albanese, M. and Galbraith, J. (1990) Scoring attitudes
to abortion. <em>The Statistician</em>, <b>40</b>, 217&ndash;223.
</p>
<p>McGrath, K. and Waterton, J. (1986) <em>British social attitudes, 1983-86 panel
survey</em>. London: SCPR.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Descriptive statistics for Abortion data
dsc &lt;- descript(Abortion)
dsc
plot(dsc)

</code></pre>

<hr>
<h2 id='anova'> Anova method for fitted IRT models</h2><span id='topic+anova.gpcm'></span><span id='topic+anova.grm'></span><span id='topic+anova.ltm'></span><span id='topic+anova.rasch'></span><span id='topic+anova.tpm'></span>

<h3>Description</h3>

<p>Performs a Likelihood Ratio Test between two nested IRT models. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'gpcm'
anova(object, object2, simulate.p.value = FALSE, 
    B = 200, verbose = getOption("verbose"), seed = NULL, ...)

## S3 method for class 'grm'
anova(object, object2, ...)

## S3 method for class 'ltm'
anova(object, object2, ...)

## S3 method for class 'rasch'
anova(object, object2, ...)

## S3 method for class 'tpm'
anova(object, object2, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="anova_+3A_object">object</code></td>
<td>
<p>an object inheriting from either class <code>gpcm</code>, class <code>grm</code>, class <code>ltm</code>, class <code>rasch</code> 
or class <code>tpm</code>, representing the model under the null hypothesis.</p>
</td></tr>
<tr><td><code id="anova_+3A_object2">object2</code></td>
<td>
<p>an object inheriting from either class <code>gpcm</code>, class <code>grm</code>, class <code>ltm</code>, class <code>rasch</code>,
or class <code>tpm</code>, representing the model under the alternative hypothesis.</p>
</td></tr>
<tr><td><code id="anova_+3A_simulate.p.value">simulate.p.value</code></td>
<td>
<p>logical; if <code>TRUE</code>, the reported <code class="reqn">p</code>-value is based on a parametric Bootstrap approach.</p>
</td></tr>
<tr><td><code id="anova_+3A_b">B</code></td>
<td>
<p>the number of Bootstrap samples.</p>
</td></tr>
<tr><td><code id="anova_+3A_verbose">verbose</code></td>
<td>
<p>logical; if <code>TRUE</code>, information is printed in the console during the parametric Bootstrap.</p>
</td></tr>
<tr><td><code id="anova_+3A_seed">seed</code></td>
<td>
<p>the seed to be used during the parametric Bootstrap; if <code>NULL</code>, a random seed is used.</p>
</td></tr>
<tr><td><code id="anova_+3A_...">...</code></td>
<td>
<p> additional arguments; currently none is used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>anova.gpcm()</code> also includes the option to estimate the <code class="reqn">p</code>-value of the LRT using a parametric Bootstrap approach. 
In particular, <code>B</code> data sets are simulated under the null hypothesis (i.e., under the generalized partial credit model 
<code>object</code>), and both the null and alternative models are fitted and the value of LRT is computed. Then the <code class="reqn">p</code>-value is 
approximate using <code class="reqn">[1 + \sum\limits_{i=1}^B I(T_i  &gt; T_{obs})] / (B + 1),</code> where <code class="reqn">T_{obs}</code>
is the value of the likelihood ratio statistic in the original data set, and <code class="reqn">T_i</code> the value of the statistic in the <code class="reqn">i</code>th 
Bootstrap sample.
</p>
<p>In addition, when <code>simulate.p.value = TRUE</code> objects of class <code>aov.gpcm</code> have a method for the <code>plot()</code> generic function
that produces a QQ plot comparing the Bootstrap sample of likelihood ration statistic with the asymptotic chi-squared distribution. For instance,
you can use something like the following: <code>lrt &lt;- anova(obj1, obj2, simulate.p.value = TRUE); plot(lrt)</code>. 
</p>


<h3>Value</h3>

<p>An object of either class <code>aov.gpcm</code>, <code>aov.grm</code>, class <code>aov.ltm</code> or class <code>aov.rasch</code> with components,
</p>
<table>
<tr><td><code>nam0</code></td>
<td>
<p>the name of <code>object</code>.</p>
</td></tr>
<tr><td><code>L0</code></td>
<td>
<p>the log-likelihood under the null hypothesis (<code>object</code>).</p>
</td></tr>
<tr><td><code>nb0</code></td>
<td>
<p>the number of parameter in <code>object</code>; returned only in <code>aov.gpcm</code>.</p>
</td></tr>
<tr><td><code>aic0</code></td>
<td>
<p>the AIC value for the model given by <code>object</code>.</p>
</td></tr>
<tr><td><code>bic0</code></td>
<td>
<p>the BIC value for the model given by <code>object</code>. </p>
</td></tr>
<tr><td><code>nam1</code></td>
<td>
<p>the name of <code>object2</code>.</p>
</td></tr>
<tr><td><code>L1</code></td>
<td>
<p>the log-likelihood under the alternative hypothesis (<code>object2</code>).</p>
</td></tr>
<tr><td><code>nb1</code></td>
<td>
<p>the number of parameter in <code>object</code>; returned only in <code>aov.gpcm</code>.</p>
</td></tr>
<tr><td><code>aic1</code></td>
<td>
<p>the AIC value for the model given by <code>object2</code>.</p>
</td></tr>
<tr><td><code>bic1</code></td>
<td>
<p>the BIC value for the model given by <code>object2</code>.</p>
</td></tr>
<tr><td><code>LRT</code></td>
<td>
<p>the value of the Likelihood Ratio Test statistic. </p>
</td></tr>
<tr><td><code>df</code></td>
<td>
<p>the degrees of freedom for the test (i.e., the difference in the number of parameters).</p>
</td></tr>
<tr><td><code>p.value</code></td>
<td>
<p>the <code class="reqn">p</code>-value of the test.</p>
</td></tr>
</table>


<h3>Warning</h3>

<p>The code does not check if the models are nested! The user is responsible to supply nested models in
order the LRT to be valid.
</p>
<p>When <code>object2</code> represents a three parameter model, note that the
null hypothesis in on the boundary of the parameter space for the guessing parameters. Thus, the Chi-squared reference 
distribution used by these function might not be totally appropriate.
</p>


<h3>Author(s)</h3>

<p>Dimitris Rizopoulos <a href="mailto:d.rizopoulos@erasmusmc.nl">d.rizopoulos@erasmusmc.nl</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+GoF.gpcm">GoF.gpcm</a></code>,
<code><a href="#topic+GoF.rasch">GoF.rasch</a></code>,
<code><a href="#topic+gpcm">gpcm</a></code>,
<code><a href="#topic+grm">grm</a></code>,
<code><a href="#topic+ltm">ltm</a></code>,
<code><a href="#topic+rasch">rasch</a></code>,
<code><a href="#topic+tpm">tpm</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## LRT between the constrained and unconstrained GRMs 
## for the Science data:
fit0 &lt;- grm(Science[c(1,3,4,7)], constrained = TRUE)
fit1 &lt;- grm(Science[c(1,3,4,7)])
anova(fit0, fit1)


## LRT between the one- and two-factor models 
## for the WIRS data:
anova(ltm(WIRS ~ z1), ltm(WIRS ~ z1 + z2))


## An LRT between the Rasch and a constrained 
## two-parameter logistic model for the WIRS data: 
fit0 &lt;- rasch(WIRS)
fit1 &lt;- ltm(WIRS ~ z1, constraint = cbind(c(1, 3, 5), 2, 1))
anova(fit0, fit1)


## An LRT between the constrained (discrimination 
## parameter equals 1) and the unconstrained Rasch
## model for the LSAT data: 
fit0 &lt;- rasch(LSAT, constraint = rbind(c(6, 1)))
fit1 &lt;- rasch(LSAT)
anova(fit0, fit1)


## An LRT between the Rasch and the two-parameter 
## logistic model for the LSAT data: 
anova(rasch(LSAT), ltm(LSAT ~ z1))

</code></pre>

<hr>
<h2 id='biserial.cor'>
Point-Biserial Correlation
</h2><span id='topic+biserial.cor'></span>

<h3>Description</h3>

<p>Computes the point-biserial correlation between a dichotomous and a continuous variable.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
biserial.cor(x, y, use = c("all.obs", "complete.obs"), level = 1)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="biserial.cor_+3A_x">x</code></td>
<td>
<p>a numeric vector representing the continuous variable.</p>
</td></tr>
<tr><td><code id="biserial.cor_+3A_y">y</code></td>
<td>
<p>a factor or a numeric vector (that will be converted to a factor) representing the dichotomous variable.</p>
</td></tr>
<tr><td><code id="biserial.cor_+3A_use">use</code></td>
<td>
<p>If <code>use</code> is &quot;all.obs&quot;, then the presence of missing observations will produce an error. If <code>use</code>
is &quot;complete.obs&quot; then missing values are handled by casewise deletion.</p>
</td></tr>
<tr><td><code id="biserial.cor_+3A_level">level</code></td>
<td>
<p>which level of <code>y</code> to use.</p>
</td></tr>
</table>


<h3>Details</h3>

 
<p>The point biserial correlation computed by <code>biserial.cor()</code> is defined as follows </p>
<p style="text-align: center;"><code class="reqn">r = 
    \frac{(\overline{X}_1 - \overline{X}_0)\sqrt{\pi (1 - \pi)}}{S_x},</code>
</p>
 
<p>where <code class="reqn">\overline{X}_1</code> and <code class="reqn">\overline{X}_0</code> denote the sample means of the <code class="reqn">X</code>-values 
corresponding to the first and second level of <code class="reqn">Y</code>, respectively, <code class="reqn">S_x</code> is the sample standard deviation of
<code class="reqn">X</code>, and <code class="reqn">\pi</code> is the sample proportion for <code class="reqn">Y = 1</code>. The first level of <code class="reqn">Y</code> is defined by the
<code>level</code> argument; see <b>Examples</b>. 
</p>


<h3>Value</h3>

<p>the (numeric) value of the point-biserial correlation.
</p>


<h3>Note</h3>

<p>Changing the order of the levels for <code>y</code> will produce a different result. By default, the first level is used
as a reference level
</p>


<h3>Author(s)</h3>

<p>Dimitris Rizopoulos <a href="mailto:d.rizopoulos@erasmusmc.nl">d.rizopoulos@erasmusmc.nl</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# the point-biserial correlation between
# the total score and the first item, using
# '0' as the reference level
biserial.cor(rowSums(LSAT), LSAT[[1]])

# and using '1' as the reference level
biserial.cor(rowSums(LSAT), LSAT[[1]], level = 2)

</code></pre>

<hr>
<h2 id='coef'> Extract Estimated Loadings </h2><span id='topic+coef.gpcm'></span><span id='topic+coef.grm'></span><span id='topic+coef.ltm'></span><span id='topic+coef.rasch'></span><span id='topic+coef.tpm'></span>

<h3>Description</h3>

<p>Extracts the estimated parameters from either <code>grm</code>, <code>ltm</code>, <code>rasch</code> or <code>tpm</code> objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'gpcm'
coef(object, ...)

## S3 method for class 'grm'
coef(object, ...)

## S3 method for class 'ltm'
coef(object, standardized = FALSE, prob = FALSE, order = FALSE, ...)

## S3 method for class 'rasch'
coef(object, prob = FALSE, order = FALSE, ...)

## S3 method for class 'tpm'
coef(object, prob = FALSE, order = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="coef_+3A_object">object</code></td>
<td>
<p> an object inheriting from either class <code>gpcm</code>, class <code>grm</code>, class <code>ltm</code>, class <code>rasch</code> or class <code>tpm</code>. </p>
</td></tr>
<tr><td><code id="coef_+3A_standardized">standardized</code></td>
<td>
<p> logical; if <code>TRUE</code> the standardized loadings are also returned. See  <b>Details</b> 
for more info.</p>
</td></tr>
<tr><td><code id="coef_+3A_prob">prob</code></td>
<td>
<p> logical; if <code>TRUE</code> the probability of a positive response for the median individual
(i.e., <code class="reqn">Pr(x_i = 1 | z = 0)</code>, with <code class="reqn">i = 1, \ldots, p</code> denoting the items) 
is also returned.</p>
</td></tr>
<tr><td><code id="coef_+3A_order">order</code></td>
<td>
<p> logical; if <code>TRUE</code> the items are sorted according to the difficulty estimates. </p>
</td></tr>
<tr><td><code id="coef_+3A_...">...</code></td>
<td>
<p> additional arguments; currently none is used. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>The standardization of the factor loadings is useful in order to form a link to the 
Underlying Variable approach. In particular, the standardized form of the factor loadings
represents the correlation coefficient between the latent variables and the underlying continuous variables
based on which the dichotomous outcomes arise (see Bartholomew and Knott, 1999, p.87-88 or Bartholomew 
<em>et al.</em>, 2002, p.191).
</p>
<p>The standardized factor loadings are computed only for the linear one- and two-factor models, fitted by <code>ltm()</code>.
</p>


<h3>Value</h3>

<p>A list or a matrix of the estimated parameters for the fitted model.</p>


<h3>Author(s)</h3>

<p>Dimitris Rizopoulos <a href="mailto:d.rizopoulos@erasmusmc.nl">d.rizopoulos@erasmusmc.nl</a>
</p>


<h3>References</h3>

<p>Bartholomew, D. and Knott, M. (1999) <em>Latent Variable Models
and Factor Analysis</em>, 2nd ed. London: Arnold.
</p>
<p>Bartholomew, D., Steel, F., Moustaki, I. and Galbraith, J. (2002)
<em>The Analysis and Interpretation of Multivariate Data for
Social Scientists</em>. London: Chapman and Hall.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+gpcm">gpcm</a></code>,
<code><a href="#topic+grm">grm</a></code>,
<code><a href="#topic+ltm">ltm</a></code>,
<code><a href="#topic+rasch">rasch</a></code>,
<code><a href="#topic+tpm">tpm</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
fit &lt;- grm(Science[c(1,3,4,7)])
coef(fit)

fit &lt;- ltm(LSAT ~ z1)
coef(fit, TRUE, TRUE)

m &lt;- rasch(LSAT)
coef(fit, TRUE, TRUE)

</code></pre>

<hr>
<h2 id='cronbach.alpha'>
Cronbach's alpha
</h2><span id='topic+cronbach.alpha'></span>

<h3>Description</h3>

<p>Computes Cronbach's alpha for a given data-set.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
cronbach.alpha(data, standardized = FALSE, CI = FALSE, 
    probs = c(0.025, 0.975), B = 1000, na.rm = FALSE)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="cronbach.alpha_+3A_data">data</code></td>
<td>
<p>a <code>matrix</code> or a <code>data.frame</code> containing the items as columns.</p>
</td></tr>
<tr><td><code id="cronbach.alpha_+3A_standardized">standardized</code></td>
<td>
<p>logical; if <code>TRUE</code> the standardized Cronbach's alpha is computed.</p>
</td></tr>
<tr><td><code id="cronbach.alpha_+3A_ci">CI</code></td>
<td>
<p>logical; if <code>TRUE</code> a Bootstrap confidence interval for Cronbach's alpha is computed.</p>
</td></tr>
<tr><td><code id="cronbach.alpha_+3A_probs">probs</code></td>
<td>
<p>a numeric vector of length two indicating which quantiles to use for the Bootstrap CI.</p>
</td></tr>
<tr><td><code id="cronbach.alpha_+3A_b">B</code></td>
<td>
<p>the number of Bootstrap samples to use.</p>
</td></tr>
<tr><td><code id="cronbach.alpha_+3A_na.rm">na.rm</code></td>
<td>
<p>logical; what to do with <code>NA</code>'s.</p>
</td></tr>
</table>


<h3>Details</h3>

 
<p>The Cronbach's alpha computed by <code>cronbach.alpha()</code> is defined as follows </p>
<p style="text-align: center;"><code class="reqn">\alpha = 
  \frac{p}{p - 1}\left(1 - \frac{\sum_{i=1}^p \sigma_{y_i}^2}{\sigma_x^2}\right),</code>
</p>
<p> where <code class="reqn">p</code> is the number of items <code class="reqn">\sigma_x^2</code> 
is the variance of the observed total test scores, and <code class="reqn">\sigma_{y_i}^2</code> is the variance 
of the <code class="reqn">i</code>th item.
</p>
<p>The standardized Cronbach's alpha computed by <code>cronbach.alpha()</code> is defined as follows </p>
<p style="text-align: center;"><code class="reqn">\alpha_s = 
  \frac{p \cdot \bar{r}}{1 + (p - 1) \cdot \bar{r}},</code>
</p>
<p> where <code class="reqn">p</code> is the 
number of items, and <code class="reqn">\bar{r}</code> is the average of all (Pearson) correlation coefficients between the 
items. In this case if <code>na.rm = TRUE</code>, then the complete observations (i.e., rows) are used.
</p>
<p>The Bootstrap confidence interval is calculated by simply taking <code>B</code> samples with replacement from <code>data</code>,
calculating for each <code class="reqn">\alpha</code> or <code class="reqn">\alpha_s</code>, and computing the quantiles according to 
<code>probs</code>.
</p>


<h3>Value</h3>

<p><code>cronbach.alpha()</code> returns an object of class <code>cronbachAlpha</code> with components
</p>
<table>
<tr><td><code>alpha</code></td>
<td>
<p>the value of Cronbach's alpha.</p>
</td></tr>
<tr><td><code>n</code></td>
<td>
<p>the number of sample units.</p>
</td></tr>
<tr><td><code>p</code></td>
<td>
<p>the number of items.</p>
</td></tr>
<tr><td><code>standardized</code></td>
<td>
<p>a copy of the <code>standardized</code> argument.</p>
</td></tr>
<tr><td><code>name</code></td>
<td>
<p>the name of argument <code>data</code>.</p>
</td></tr>
<tr><td><code>ci</code></td>
<td>
<p>the confidence interval for alpha; returned if <code>CI = TRUE</code>.</p>
</td></tr>
<tr><td><code>probs</code></td>
<td>
<p>a copy of the <code>probs</code> argument; returned if <code>CI = TRUE</code>.</p>
</td></tr>
<tr><td><code>B</code></td>
<td>
<p>a copy of the <code>B</code> argument; returned if <code>CI = TRUE</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Dimitris Rizopoulos <a href="mailto:d.rizopoulos@erasmusmc.nl">d.rizopoulos@erasmusmc.nl</a>
</p>


<h3>References</h3>

<p>Cronbach, L. J. (1951) Coefficient alpha and the internal structure of tests. 
<em>Psychometrika</em>, <b>16</b>, 297&ndash;334.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Cronbach's alpha for the LSAT data-set
# with a Bootstrap 95% CI
cronbach.alpha(LSAT, CI = TRUE, B = 500)

</code></pre>

<hr>
<h2 id='descript'>
Descriptive Statistics
</h2><span id='topic+descript'></span>

<h3>Description</h3>

<p>Computes descriptive statistics for dichotomous and polytomous response matrices.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>descript(data, n.print = 10, chi.squared = TRUE, B = 1000)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="descript_+3A_data">data</code></td>
<td>
<p> a <code>matrix</code> or a <code>data.frame</code> containing the manifest variables as columns. </p>
</td></tr>
<tr><td><code id="descript_+3A_n.print">n.print</code></td>
<td>
<p>numeric indicating the number of pairwise associations with the highest <code class="reqn">p</code>-values 
to be printed.</p>
</td></tr>
<tr><td><code id="descript_+3A_chi.squared">chi.squared</code></td>
<td>
<p>logical; if <code>TRUE</code> the chi-squared test for the pairwise associations between items
is performed. See <b>Details</b> for more info.</p>
</td></tr>
<tr><td><code id="descript_+3A_b">B</code></td>
<td>
<p>an integer specifying the number of replicates used in the Monte Carlo test (i.e., this is the <code>B</code> 
argument of <code>chisq.test()</code>).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The following descriptive statistics are returned by <code>descript()</code>:
</p>

<dl>
<dt>(i)</dt><dd><p>the proportions for all the possible response categories for each item. In case all items are
dichotomous, the logit of the proportion for the positive responses is also included.</p>
</dd>
<dt>(ii)</dt><dd><p>the frequencies of all possible total scores. The total score of a response pattern is simply its sum.
For dichotomous items this is the number of positive responses, whereas for polytomous items this is 
the sum of the levels represented as numeric values (e.g., the response categories &quot;very concerned&quot;, 
&quot;slightly concerned&quot;, and &quot;not very concerned&quot; in <code><a href="#topic+Environment">Environment</a></code> are represented as 1, 2, 
and 3).</p>
</dd>
<dt>(iii)</dt><dd><p>Cronbach's alpha, for all items and excluding each time one of the items.</p>
</dd>
<dt>(iv)</dt><dd><p>for dichotomous response matrices two versions of the point biserial correlation of each item with the 
total score are returned. In the first one the item is included in the computation of the
total score, and in the second one is excluded.</p>
</dd>
<dt>(v)</dt><dd><p>pairwise associations between items. Before an analysis with latent variable models, it is useful to 
inspect the data for evidence of positive correlations. In the case of binary or polytomous data, this 
ad hoc check is performed by constructing the <code class="reqn">2 \times 2</code> contingency tables for all 
possible pairs of items and examine the Chi-squared <code class="reqn">p</code>-values. In case any expected frequencies 
are smaller than 5, <code>simulate.p.value</code> is turned to <code>TRUE</code> in <code>chisq.test()</code>, 
using <code>B</code> resamples.</p>
</dd>
</dl>



<h3>Value</h3>

<p><code>descript()</code> returns an object of class <code>descript</code> with components,
</p>
<table>
<tr><td><code>sample</code></td>
<td>
<p>a numeric vector of length 2, with elements the number of items and the number of sample units.</p>
</td></tr>
<tr><td><code>perc</code></td>
<td>
<p>a numeric matrix containing the percentages of negative and positive responses for each item. If
<code>data</code> contains only dichotomous manifest variables the logit of the positive responses (i.e.,
second row) is also included.</p>
</td></tr>
<tr><td><code>items</code></td>
<td>
<p>a numeric matrix containing the frequencies for the total scores.</p>
</td></tr>
<tr><td><code>pw.ass</code></td>
<td>
<p>a matrix containing the <code class="reqn">p</code>-values for the pairwise association between the items.</p>
</td></tr>
<tr><td><code>n.print</code></td>
<td>
<p>the value of the <code>n.print</code> argument.</p>
</td></tr>
<tr><td><code>name</code></td>
<td>
<p>the name of argument <code>data</code>.</p>
</td></tr>
<tr><td><code>missin</code></td>
<td>
<p>a numeric matrix containing the frequency and percentages of missing values for each item;
returned only if any <code>NA</code>'s exist in <code>data</code>.</p>
</td></tr>
<tr><td><code>bisCorr</code></td>
<td>
<p>a numeric vector containing sample estimates of the biserial correlation of dichotomous manifest
variables with the total score.</p>
</td></tr>
<tr><td><code>ExBisCorr</code></td>
<td>
<p>a numeric vector containing sample estimates of the biserial correlation of dichotomous manifest
variables with the total score, where the latter is computed by excluding the specific item.</p>
</td></tr>        
<tr><td><code>data</code></td>
<td>
<p>a copy of the <code>data</code>.</p>
</td></tr>
<tr><td><code>alpha</code></td>
<td>
<p>a numeric matrix with one column containing the sample estimates of Cronbach's alpha, for all items
and excluding each time one item.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Dimitris Rizopoulos <a href="mailto:d.rizopoulos@erasmusmc.nl">d.rizopoulos@erasmusmc.nl</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+plot.descript">plot.descript</a></code>,
<code><a href="#topic+unidimTest">unidimTest</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Descriptives for LSAT data:
dsc &lt;- descript(LSAT, 3)
dsc
plot(dsc, type = "b", lty = 1, pch = 1:5)
legend("topleft", names(LSAT), pch = 1:5, col = 1:5, lty = 1, bty = "n")

</code></pre>

<hr>
<h2 id='Environment'>Attitude to the Environment</h2><span id='topic+Environment'></span>

<h3>Description</h3>

<p>This data set comes from the Environment section of the 1990 British Social Attitudes Survey (Brook et al., 1991). 
A sample of 291 responded to the questions below:
</p>


<h3>Format</h3>

<p>All of the below items were measured on a three-group scale with response categories &quot;very concerned&quot;, 
&quot;slightly concerned&quot; and &quot;not very concerned&quot;:
</p>

<dl>
<dt>LeadPetrol</dt><dd><p>Lead from petrol.</p>
</dd>
<dt>RiverSea</dt><dd><p>River and sea pollution.</p>
</dd>
<dt>RadioWaste</dt><dd><p>Transport and storage of radioactive waste.</p>
</dd>
<dt>AirPollution</dt><dd><p>Air pollution.</p>
</dd>
<dt>Chemicals</dt><dd><p>Transport and disposal of poisonous chemicals.</p>
</dd>
<dt>Nuclear</dt><dd><p>Risks from nuclear power station.</p>
</dd>
</dl>



<h3>References</h3>

<p>Bartholomew, D., Steel, F., Moustaki, I. and Galbraith, J. (2002) <em>The Analysis and Interpretation of 
Multivariate Data for Social Scientists</em>. London: Chapman and Hall.
</p>
<p>Brook, L., Taylor, B. and Prior, G. (1991) <em>British Social Attitudes, 1990, Survey</em>. London: SCPR.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Descriptive statistics for Environment data
descript(Environment)

</code></pre>

<hr>
<h2 id='factor.scores'> Factor Scores - Ability Estimates </h2><span id='topic+factor.scores'></span><span id='topic+factor.scores.gpcm'></span><span id='topic+factor.scores.grm'></span><span id='topic+factor.scores.ltm'></span><span id='topic+factor.scores.rasch'></span><span id='topic+factor.scores.tpm'></span>

<h3>Description</h3>

<p>Computation of factor scores for <code>grm</code>, <code>ltm</code>, <code>rasch</code> and <code>tpm</code> models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>factor.scores(object, ...)

## S3 method for class 'gpcm'
factor.scores(object, resp.patterns = NULL, 
        method = c("EB", "EAP", "MI"), B = 5, robust.se = FALSE, 
        prior = TRUE, return.MIvalues = FALSE, ...)

## S3 method for class 'grm'
factor.scores(object, resp.patterns = NULL, 
        method = c("EB", "EAP", "MI"), B = 5, prior = TRUE, 
        return.MIvalues = FALSE, ...)

## S3 method for class 'ltm'
factor.scores(object, resp.patterns = NULL, 
        method = c("EB", "EAP", "MI", "Component"), B = 5, 
        robust.se = FALSE, prior = TRUE, return.MIvalues = FALSE, 
        ...)

## S3 method for class 'rasch'
factor.scores(object, resp.patterns = NULL, 
        method = c("EB", "EAP", "MI"), B = 5, robust.se = FALSE,
	    prior = TRUE, return.MIvalues = FALSE, ...)

## S3 method for class 'tpm'
factor.scores(object, resp.patterns = NULL, 
        method = c("EB", "EAP", "MI"), B = 5, prior = TRUE, 
        return.MIvalues = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="factor.scores_+3A_object">object</code></td>
<td>
<p>an object inheriting from either class <code>gpcm</code>, class <code>grm</code>, class <code>ltm</code>, class <code>rasch</code> or class 
<code>tpm</code>.</p>
</td></tr>
<tr><td><code id="factor.scores_+3A_resp.patterns">resp.patterns</code></td>
<td>
<p>a matrix or a data.frame of response patterns with columns denoting the items; if <code>NULL</code> 
the factor scores are computed for the observed response patterns.</p>
</td></tr>
<tr><td><code id="factor.scores_+3A_method">method</code></td>
<td>
<p>a character supplying the scoring method; available methods are:
Empirical Bayes, Expected a Posteriori, Multiple Imputation, and Component. See <b>Details</b> section for 
more info.</p>
</td></tr>
<tr><td><code id="factor.scores_+3A_b">B</code></td>
<td>
<p>the number of multiple imputations to be used if <code>method = "MI"</code>.</p>
</td></tr>
<tr><td><code id="factor.scores_+3A_robust.se">robust.se</code></td>
<td>
<p>logical; if <code>TRUE</code> the sandwich estimator is used for the estimation of the covariance
matrix of the MLEs. See <b>Details</b> section for more info.</p>
</td></tr>
<tr><td><code id="factor.scores_+3A_prior">prior</code></td>
<td>
<p>logical. If <code>TRUE</code>, then the prior normal distribution for the latent abilities is taken into
account in the calculation of the posterior modes, when <code>method = "EB"</code>.</p>
</td></tr>
<tr><td><code id="factor.scores_+3A_return.mivalues">return.MIvalues</code></td>
<td>
<p>logical. If <code>TRUE</code>, then the estimated z-values and their covariance matrix are contained
as extra attributes <code>"zvalues.MI"</code> and <code>"var.zvalues.MI"</code>, respectively, in the returned 
<code>score.dat</code> data frame.</p>
</td></tr>
<tr><td><code id="factor.scores_+3A_...">...</code></td>
<td>
<p> additional arguments; currently none is used. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>Factor scores or ability estimates are summary measures of the posterior distribution <code class="reqn">p(z|x)</code>, 
where <code class="reqn">z</code> denotes the vector of latent variables and <code class="reqn">x</code> the vector of manifest variables.
</p>
<p>Usually as factor scores we assign the modes of the above posterior distribution evaluated at the MLEs. These
Empirical Bayes estimates (use <code>method = "EB"</code>) and their associated variance are good measures of the 
posterior distribution while <code class="reqn">p \rightarrow \infty</code>, where <code class="reqn">p</code> is the number of items. 
This is based on the result </p>
<p style="text-align: center;"><code class="reqn">p(z|x)=p(z|x; \hat{\theta})(1+O(1/p)),</code>
</p>

<p>where <code class="reqn">\hat{\theta}</code> are the MLEs. However, in cases where <code class="reqn">p</code> and/or <code class="reqn">n</code> (the sample size) is small 
we ignore the variability of plugging-in estimates but not the <em>true</em> parameter values. A solution to this 
problem can be given using Multiple Imputation (MI; use <code>method = "MI"</code>). In particular, MI is used the 
other way around, i.e.,
</p>

<dl>
<dt>Step 1:</dt><dd><p>Simulate new parameter values, say <code class="reqn">\theta^*</code>, from <code class="reqn">N(\hat{\theta}, C(\hat{\theta}))</code>,
where <code class="reqn">C(\hat{\theta})</code> is the large sample covariance matrix of <code class="reqn">\hat{\theta}</code> (if <code>robust.se = TRUE</code>, 
<code class="reqn">C(\hat{\theta})</code> is based on the sandwich estimator).</p>
</dd>
<dt>Step 2:</dt><dd><p>Maximize <code class="reqn">p(z|x; \theta^*)</code> <em>wrt</em> <code class="reqn">z</code> and also compute the associated 
variance to this mode.</p>
</dd>
<dt>Step 3:</dt><dd><p>Repeat steps 1-2 <code>B</code> times and combine the estimates using the known formulas of MI.</p>
</dd>
</dl>

<p>This scheme explicitly acknowledges the ignorance of the true parameter values by drawing from their large sample
posterior distribution while taking into account the sampling error. The modes of the posterior distribution 
<code class="reqn">p(z|x; \theta)</code> are numerically approximated using the BFGS algorithm in <code>optim()</code>. 
</p>
<p>The Expected a posteriori scores (use <code>method = "EAP"</code>) computed by <code>factor.scores()</code> are defined as 
follows: </p>
<p style="text-align: center;"><code class="reqn">\int z p(z | x; \hat{\theta}) dz.</code>
</p>

<p>The Component scores (use <code>method = "Component"</code>) proposed by Bartholomew (1984) is an alternative method
to scale the sample units in the latent dimensions identified by the model that avoids the calculation of the 
posterior mode. However, this method is not valid in the general case where nonlinear latent terms are assumed.
</p>


<h3>Value</h3>

<p>An object of class <code>fscores</code> is a list with components,
</p>
<table>
<tr><td><code>score.dat</code></td>
<td>
<p>the <code>data.frame</code> of observed response patterns including, observed and expected 
frequencies (only if the observed data response matrix contains no missing vales), the factor scores 
and their standard errors.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>a character giving the scoring method used.</p>
</td></tr>
<tr><td><code>B</code></td>
<td>
<p>the number of multiple imputations used; relevant only if <code>method = "MI"</code>.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>a copy of the matched call of <code>object</code>.</p>
</td></tr>
<tr><td><code>resp.pats</code></td>
<td>
<p>logical; is <code>TRUE</code> if <code>resp.patterns</code> argument has been specified.</p>
</td></tr>
<tr><td><code>coef</code></td>
<td>
<p>the parameter estimates returned by <code>coef(object)</code>; this is <code>NULL</code> when <code>object</code>
inherits from class <code>grm</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Dimitris Rizopoulos <a href="mailto:d.rizopoulos@erasmusmc.nl">d.rizopoulos@erasmusmc.nl</a>
</p>


<h3>References</h3>

<p>Bartholomew, D. (1984) Scaling binary data using a factor model. <em>Journal of the Royal 
Statistical Society, Series B</em>, <b>46</b>, 120&ndash;123.
</p>
<p>Bartholomew, D. and Knott, M. (1999) <em>Latent Variable Models
and Factor Analysis</em>, 2nd ed. London: Arnold.
</p>
<p>Bartholomew, D., Steel, F., Moustaki, I. and Galbraith, J. (2002)
<em>The Analysis and Interpretation of Multivariate Data for
Social Scientists</em>. London: Chapman and Hall.
</p>
<p>Rizopoulos, D. (2006) <b>ltm</b>: An R package for latent variable modelling and item response theory analyses. 
<em>Journal of Statistical Software</em>, <b>17(5)</b>, 1&ndash;25. URL doi: <a href="https://doi.org/10.18637/jss.v017.i05">10.18637/jss.v017.i05</a>
</p>
<p>Rizopoulos, D. and Moustaki, I. (2008) Generalized latent variable models
with nonlinear effects. <em>British Journal of Mathematical and Statistical Psychology</em>, <b>61</b>, 415&ndash;438.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+plot.fscores">plot.fscores</a></code>,
<code><a href="#topic+gpcm">gpcm</a></code>,
<code><a href="#topic+grm">grm</a></code>,
<code><a href="#topic+ltm">ltm</a></code>,
<code><a href="#topic+rasch">rasch</a></code>,
<code><a href="#topic+tpm">tpm</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Factor Scores for the Rasch model
fit &lt;- rasch(LSAT)
factor.scores(fit) # Empirical Bayes


## Factor scores for all subjects in the
## original dataset LSAT
factor.scores(fit, resp.patterns = LSAT)


## Factor scores for specific patterns,
## including NA's, can be obtained by 
factor.scores(fit, resp.patterns = rbind(c(1,0,1,0,1), c(NA,1,0,NA,1)))


## Not run: 
## Factor Scores for the two-parameter logistic model
fit &lt;- ltm(Abortion ~ z1)
factor.scores(fit, method = "MI", B = 20) # Multiple Imputation

## Factor Scores for the graded response model
fit &lt;- grm(Science[c(1,3,4,7)])
factor.scores(fit, resp.patterns = rbind(1:4, c(NA,1,2,3)))

## End(Not run)
</code></pre>

<hr>
<h2 id='fitted'> Fitted Values for IRT model</h2><span id='topic+fitted.gpcm'></span><span id='topic+fitted.grm'></span><span id='topic+fitted.ltm'></span><span id='topic+fitted.rasch'></span><span id='topic+fitted.tpm'></span>

<h3>Description</h3>

<p>Computes the expected frequencies for vectors of response patterns.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'gpcm'
fitted(object, resp.patterns = NULL, 
    type = c("expected", "marginal-probabilities",
    "conditional-probabilities"), ...)

## S3 method for class 'grm'
fitted(object, resp.patterns = NULL, 
    type = c("expected", "marginal-probabilities",
    "conditional-probabilities"), ...)

## S3 method for class 'ltm'
fitted(object, resp.patterns = NULL, 
    type = c("expected", "marginal-probabilities", 
    "conditional-probabilities"), ...)

## S3 method for class 'rasch'
fitted(object, resp.patterns = NULL, 
    type = c("expected", "marginal-probabilities", 
    "conditional-probabilities"), ...)

## S3 method for class 'tpm'
fitted(object, resp.patterns = NULL, 
    type = c("expected", "marginal-probabilities", 
    "conditional-probabilities"), ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="fitted_+3A_object">object</code></td>
<td>
<p> an object inheriting either from class <code>gpcm</code>, class <code>grm</code>, class <code>ltm</code>, class <code>rasch</code>, or 
class <code>tpm</code>. </p>
</td></tr>
<tr><td><code id="fitted_+3A_resp.patterns">resp.patterns</code></td>
<td>
<p>a <code>matrix</code> or a <code>data.frame</code> of response patterns with columns denoting the 
items; if <code>NULL</code> the expected frequencies are computed for the observed response patterns.</p>
</td></tr>
<tr><td><code id="fitted_+3A_type">type</code></td>
<td>
<p>if <code>type == "marginal-probabilities"</code> the marginal probabilities for each response are
computed; these are given by <code class="reqn">\int \{ \prod_{i = 1}^p Pr(x_i = 1 | z)^{x_i} \times
        (1 - Pr(x_i = 1 | z))^{1 - x_i} \}p(z) dz</code>, where <code class="reqn">x_i</code> denotes
the <code class="reqn">i</code>th item and <code class="reqn">z</code> the latent variable. If <code>type == "expected"</code> the expected frequencies
for each response are computed, which are the marginal probabilities times the number of sample units. If
<code>type == "conditional-probabilities"</code> the conditional probabilities for each response and item are
computed; these are <code class="reqn">Pr(x_i = 1 | \hat{z})</code>, where <code class="reqn">\hat{z}</code> is the ability estimate .</p>
</td></tr>
<tr><td><code id="fitted_+3A_...">...</code></td>
<td>
<p> additional arguments; currently none is used. </p>
</td></tr>
</table>


<h3>Value</h3>

<p>a numeric <code>matrix</code> or a <code>list</code> containing either the response patterns of interest with their expected 
frequencies or marginal probabilities, if <code>type == "expected" || "marginal-probabilities"</code> or the conditional 
probabilities for each response pattern and item, if <code>type == "conditional-probabilities"</code>.
</p>


<h3>Author(s)</h3>

<p>Dimitris Rizopoulos <a href="mailto:d.rizopoulos@erasmusmc.nl">d.rizopoulos@erasmusmc.nl</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+residuals.gpcm">residuals.gpcm</a></code>,
<code><a href="#topic+residuals.grm">residuals.grm</a></code>,
<code><a href="#topic+residuals.ltm">residuals.ltm</a></code>,
<code><a href="#topic+residuals.rasch">residuals.rasch</a></code>,
<code><a href="#topic+residuals.tpm">residuals.tpm</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>fit &lt;- grm(Science[c(1,3,4,7)])
fitted(fit, resp.patterns = matrix(1:4, nr = 4, nc = 4))

fit &lt;- rasch(LSAT)
fitted(fit, type = "conditional-probabilities")
</code></pre>

<hr>
<h2 id='gh'>Gauss-Hermite Quadrature Points</h2><span id='topic+gh'></span>

<h3>Description</h3>

<p>Table with Gauss-Hermite Quadrature Points
</p>

<hr>
<h2 id='GoF'> Goodness of Fit for Rasch Models</h2><span id='topic+GoF.gpcm'></span><span id='topic+GoF.rasch'></span>

<h3>Description</h3>

<p>Performs a parametric Bootstrap test for Rasch and Generalized Partial Credit models. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>GoF.gpcm(object, simulate.p.value = TRUE, B = 99, seed = NULL, ...)

GoF.rasch(object, B = 49, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="GoF_+3A_object">object</code></td>
<td>
<p>an object inheriting from either class <code>gpcm</code> or class <code>rasch</code>.</p>
</td></tr>
<tr><td><code id="GoF_+3A_simulate.p.value">simulate.p.value</code></td>
<td>
<p>logical; if <code>TRUE</code>, the reported <code class="reqn">p</code>-value is based on a parametric Bootstrap approach.
Otherwise the <code class="reqn">p</code>-value is based on the asymptotic chi-squared distribution.</p>
</td></tr>
<tr><td><code id="GoF_+3A_b">B</code></td>
<td>
<p>the number of Bootstrap samples. See <b>Details</b> section for more info.</p>
</td></tr>
<tr><td><code id="GoF_+3A_seed">seed</code></td>
<td>
<p>the seed to be used during the parametric Bootstrap; if <code>NULL</code>, a random seed is used.</p>
</td></tr>
<tr><td><code id="GoF_+3A_...">...</code></td>
<td>
<p>additional arguments; currently none is used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>GoF.gpcm</code> and <code>GoF.rasch</code> perform a parametric Bootstrap test based on Pearson's chi-squared statistic defined as
</p>
<p style="text-align: center;"><code class="reqn">\sum\limits_{r = 1}^{2^p} \frac{\{O(r) - E(r)\}^2}{E(r)},</code>
</p>
<p> where <code class="reqn">r</code>
represents a response pattern, <code class="reqn">O(r)</code> and <code class="reqn">E(r)</code> represent the observed and expected frequencies, 
respectively and <code class="reqn">p</code> denotes the number of items. The Bootstrap approximation to the reference distribution is preferable compared with 
the ordinary Chi-squared approximation since the latter is not valid especially for large number of items 
(=&gt; many response patterns with expected frequencies smaller than 1).
</p>
<p>In particular, the Bootstrap test is implemented as follows:
</p>

<dl>
<dt>Step 0:</dt><dd><p>Based on <code>object</code> compute the observed value of the statistic <code class="reqn">T_{obs}</code>.</p>
</dd>
<dt>Step 1:</dt><dd><p>Simulate new parameter values, say <code class="reqn">\theta^*</code>, from <code class="reqn">N(\hat{\theta}, C(\hat{\theta}))</code>,
where <code class="reqn">\hat{\theta}</code> are the MLEs and <code class="reqn">C(\hat{\theta})</code> their large sample covariance 
matrix.</p>
</dd>
<dt>Step 2:</dt><dd><p>Using <code class="reqn">\theta^*</code> simulate new data (with the same dimensions as the 
observed ones), fit the generalized partial credit or the Rasch model and based on this fit calculate the value 
of the statistic <code class="reqn">T_i</code>.</p>
</dd>
<dt>Step 3:</dt><dd><p>Repeat steps 1-2 <code>B</code> times and estimate the <code class="reqn">p</code>-value using 
<code class="reqn">[1 + \sum\limits_{i=1}^B I(T_i  &gt; T_{obs})] / (B + 1).</code></p>
</dd>
</dl>

<p>Furthermore, in <code>GoF.gpcm</code> when <code>simulate.p.value = FALSE</code>, then the <code class="reqn">p</code>-value is based on the asymptotic
chi-squared distribution.
</p>


<h3>Value</h3>

<p>An object of class <code>GoF.gpcm</code> or <code>GoF.rasch</code> with components,
</p>
<table>
<tr><td><code>Tobs</code></td>
<td>
<p>the value of the Pearson's chi-squared statistic for the observed data.</p>
</td></tr>
<tr><td><code>B</code></td>
<td>
<p>the <code>B</code> argument specifying the number of Bootstrap samples used.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>the matched call of <code>object</code>.</p>
</td></tr>
<tr><td><code>p.value</code></td>
<td>
<p>the <code class="reqn">p</code>-value of the test.</p>
</td></tr>
<tr><td><code>simulate.p.value</code></td>
<td>
<p>the value of <code>simulate.p.value</code> argument (returned on for class <code>GoF.gpcm</code>).</p>
</td></tr>
<tr><td><code>df</code></td>
<td>
<p>the degrees of freedom for the asymptotic chi-squared distribution (returned on for class <code>GoF.gpcm</code>).</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Dimitris Rizopoulos <a href="mailto:d.rizopoulos@erasmusmc.nl">d.rizopoulos@erasmusmc.nl</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+person.fit">person.fit</a></code>,
<code><a href="#topic+item.fit">item.fit</a></code>,
<code><a href="#topic+margins">margins</a></code>,
<code><a href="#topic+gpcm">gpcm</a></code>,
<code><a href="#topic+rasch">rasch</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## GoF for the Rasch model for the LSAT data:
fit &lt;- rasch(LSAT)
GoF.rasch(fit)

</code></pre>

<hr>
<h2 id='gpcm'>Generalized Partial Credit Model - Polytomous IRT</h2><span id='topic+gpcm'></span>

<h3>Description</h3>

<p>Fits the Generalized Partial Credit model for ordinal polytomous data, under the Item Response Theory approach.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gpcm(data, constraint = c("gpcm", "1PL", "rasch"), IRT.param = TRUE, 
    start.val = NULL, na.action = NULL, control = list())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gpcm_+3A_data">data</code></td>
<td>
<p>a <code>data.frame</code> or a numeric <code>matrix</code> of manifest variables.</p>
</td></tr>
<tr><td><code id="gpcm_+3A_constraint">constraint</code></td>
<td>
<p>a character string specifying which version of the Generalized Partial Credit Model to fit. See <b>Details</b> and 
<b>Examples</b> for more info.</p>
</td></tr>
<tr><td><code id="gpcm_+3A_irt.param">IRT.param</code></td>
<td>
<p>logical; if <code>TRUE</code> then the coefficients' estimates are reported under the 
usual IRT parameterization. See <b>Details</b> for more info.</p>
</td></tr>
<tr><td><code id="gpcm_+3A_start.val">start.val</code></td>
<td>
<p>a list of starting values or the character string <code>"random"</code>. If a list, each one of its 
elements corresponds to each item and should contain a numeric vector with initial values for the 
threshold parameters and discrimination parameter; even if <code>constraint = "rasch"</code> or <code>constraint = "1PL"</code>, 
the discrimination parameter should be provided for all the items. If <code>"random"</code>, random starting values are computed.</p>
</td></tr>
<tr><td><code id="gpcm_+3A_na.action">na.action</code></td>
<td>
<p>the <code>na.action</code> to be used on <code>data</code>; default <code>NULL</code> the model uses the available 
cases, i.e., it takes into account the observed part of sample units with missing values (valid under MAR 
mechanisms if the model is correctly specified).</p>
</td></tr>
<tr><td><code id="gpcm_+3A_control">control</code></td>
<td>
<p>a named list of control values with components,
</p>

<dl>
<dt>iter.qN</dt><dd><p>the number of quasi-Newton iterations. Default 150.</p>
</dd>
<dt>GHk</dt><dd><p>the number of Gauss-Hermite quadrature points. Default 21.</p>
</dd>
<dt>optimizer</dt><dd><p>which optimization routine to use; options are &quot;optim&quot; and &quot;nlminb&quot;, the latter being the default.</p>
</dd>
<dt>optimMethod</dt><dd><p>the optimization method to be used in <code>optim()</code>. Default is &quot;BFGS&quot;.</p>
</dd>
<dt>numrDeriv</dt><dd><p>which numerical derivative algorithm to use to approximate the Hessian matrix; options are &quot;fd&quot; for
forward difference approximation and &quot;cd&quot; for central difference approximation. Default is &quot;fd&quot;.</p>
</dd>
<dt>epsHes</dt><dd><p>step size to be used in the numerical derivative. Default is 1e-06. If you choose <code>numrDeriv = "cd"</code>, then
change this to a larger value, e.g., 1e-03 or 1e-04.</p>
</dd>
<dt>parscale</dt><dd><p>the <code>parscale</code> control argument of <code>optim()</code>. Default is 0.5 for all parameters.</p>
</dd>        
<dt>verbose</dt><dd><p>logical; if <code>TRUE</code> info about the optimization procedure are printed.</p>
</dd>
</dl>

</td></tr>
</table>


<h3>Details</h3>

 
<p>The Generalized Partial Credit Model is an IRT model, that can handle ordinal manifest variables.
This model was discussed by Masters (1982) and it was extended by Muraki (1992).
</p>
<p>The model is defined as follows </p>
<p style="text-align: center;"><code class="reqn">P_{ik}(z) = \frac{\exp \sum \limits_{c = 0}^k \beta_i (z - \beta_{ic}^*)}{
    \sum \limits_{r = 0}^{m_i} \exp \sum \limits_{c = 0}^r \beta_i (z - \beta_{ic}^*)},</code>
</p>
<p> where <code class="reqn">P_{ik}(z)</code> denotes the 
probability of responding in category <code class="reqn">k</code> for item <code class="reqn">i</code>, given the latent ability <code class="reqn">z</code>, <code class="reqn">\beta_{ic}^*</code> are the item-category 
parameters, <code class="reqn">\beta_i</code> is the discrimination parameter, <code class="reqn">m_i</code> is the number of categories for item <code class="reqn">i</code>, and 
</p>
<p style="text-align: center;"><code class="reqn">\sum \limits_{c = 0}^0 \beta_i (z - \beta_{ic}^*) \equiv 0.</code>
</p>
 
<p>If <code>constraint = "rasch"</code>, then the discrimination parameter <code class="reqn">\beta_i</code> is assumed equal for all items and fixed at one. If 
<code>constraint = "1PL"</code>, then the discrimination parameter <code class="reqn">\beta_i</code> is assumed equal for all items but is estimated. 
If <code>constraint = "gpcm"</code>, then each item has its one discrimination parameter <code class="reqn">\beta_i</code> that is estimated. See 
<b>Examples</b> for more info. 
</p>
<p>If <code>IRT.param = FALSE</code>, then the linear predictor is of the form <code class="reqn">\beta_i z + \beta_{ic}</code>.
</p>
<p>The fit of the model is based on approximate marginal Maximum Likelihood, using the Gauss-Hermite quadrature rule 
for the approximation of the required integrals.
</p>


<h3>Value</h3>

<p>An object of class <code>gpcm</code> with components,
</p>
<table>
<tr><td><code>coefficients</code></td>
<td>
<p>a named list with components the parameter values at convergence for each item.</p>
</td></tr>
<tr><td><code>log.Lik</code></td>
<td>
<p>the log-likelihood value at convergence.</p>
</td></tr>
<tr><td><code>convergence</code></td>
<td>
<p>the convergence identifier returned by <code>optim()</code> or <code>nlminb()</code>.</p>
</td></tr>
<tr><td><code>hessian</code></td>
<td>
<p>the approximate Hessian matrix at convergence.</p>
</td></tr>
<tr><td><code>counts</code></td>
<td>
<p>the number of function and gradient evaluations used by the quasi-Newton algorithm.</p>
</td></tr>
<tr><td><code>patterns</code></td>
<td>
<p>a list with two components: (i) <code>X</code>: a numeric matrix 
that contains the observed response patterns, and (ii) <code>obs</code>: a numeric vector that contains the observed 
frequencies for each observed response pattern.</p>
</td></tr>
<tr><td><code>GH</code></td>
<td>
<p>a list with two components used in the Gauss-Hermite rule: (i) <code>Z</code>: a numeric matrix that contains 
the abscissas, and (ii) <code>GHw</code>: a numeric vector that contains the corresponding  weights.</p>
</td></tr> 
<tr><td><code>max.sc</code></td>
<td>
<p>the maximum absolute value of the score vector at convergence.</p>
</td></tr>
<tr><td><code>constraint</code></td>
<td>
<p>the value of the <code>constraint</code> argument.</p>
</td></tr>
<tr><td><code>IRT.param</code></td>
<td>
<p>the value of the <code>IRT.param</code> argument.</p>
</td></tr>
<tr><td><code>X</code></td>
<td>
<p>a copy of the response data matrix.</p>
</td></tr>
<tr><td><code>control</code></td>
<td>
<p>the values used in the <code>control</code> argument.</p>
</td></tr>
<tr><td><code>na.action</code></td>
<td>
<p>the value of the <code>na.action</code> argument.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>the matched call.</p>
</td></tr>
</table>


<h3>Warning</h3>

 
<p>In case the Hessian matrix at convergence is not positive definite try to re-fit the model by specifying the starting values or using
<code>start.val = "random"</code>.
</p>


<h3>Note</h3>

<p><code>gpcm()</code> can also handle binary items and can be used instead of <code><a href="#topic+rasch">rasch</a></code> and <code><a href="#topic+ltm">ltm</a></code> though it is less
efficient. However, <code>gpcm()</code> can handle a mix of dichotomous and polytomous items that neither <code><a href="#topic+rasch">rasch</a></code> nor 
<code><a href="#topic+ltm">ltm</a></code> can.
</p>


<h3>Author(s)</h3>

<p>Dimitris Rizopoulos <a href="mailto:d.rizopoulos@erasmusmc.nl">d.rizopoulos@erasmusmc.nl</a>
</p>


<h3>References</h3>

<p>Masters, G. (1982). A Rasch model for partial credit scoring. <em>Psychometrika</em>, <b>47</b>, 149&ndash;174.
</p>
<p>Muraki, E. (1992). A generalized partial credit model: application of an EM algorithm. <em>Applied Psychological 
Measurement</em>, <b>16</b>, 159&ndash;176.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+coef.gpcm">coef.gpcm</a></code>,
<code><a href="#topic+fitted.gpcm">fitted.gpcm</a></code>,
<code><a href="#topic+summary.gpcm">summary.gpcm</a></code>,
<code><a href="#topic+anova.gpcm">anova.gpcm</a></code>,
<code><a href="#topic+plot.gpcm">plot.gpcm</a></code>,
<code><a href="#topic+vcov.gpcm">vcov.gpcm</a></code>,
<code><a href="#topic+GoF.gpcm">GoF.gpcm</a></code>,
<code><a href="#topic+margins">margins</a></code>,
<code><a href="#topic+factor.scores">factor.scores</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## The Generalized Partial Credit Model for the Science data:
gpcm(Science[c(1,3,4,7)])

## The Generalized Partial Credit Model for the Science data,
## assuming equal discrimination parameters across items:
gpcm(Science[c(1,3,4,7)], constraint = "1PL")

## The Generalized Partial Credit Model for the Science data,
## assuming equal discrimination parameters across items
## fixed at 1:
gpcm(Science[c(1,3,4,7)], constraint = "rasch")

## more examples can be found at:
## http://wiki.r-project.org/rwiki/doku.php?id=packages:cran:ltm#sample_analyses

</code></pre>

<hr>
<h2 id='grm'> Graded Response Model - Polytomous IRT</h2><span id='topic+grm'></span>

<h3>Description</h3>

<p>Fits the Graded Response model for ordinal polytomous data, under the Item Response Theory approach.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>grm(data, constrained = FALSE, IRT.param = TRUE, Hessian = FALSE, 
    start.val = NULL, na.action = NULL, control = list())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="grm_+3A_data">data</code></td>
<td>
<p>a <code>data.frame</code> (that will be converted to a numeric matrix using 
<code>data.matrix()</code>) or a numeric <code>matrix</code> of manifest variables.</p>
</td></tr>
<tr><td><code id="grm_+3A_constrained">constrained</code></td>
<td>
<p>logical; if <code>TRUE</code> the model with equal discrimination parameters across items is fitted. 
See <b>Examples</b> for more info.</p>
</td></tr>
<tr><td><code id="grm_+3A_irt.param">IRT.param</code></td>
<td>
<p>logical; if <code>TRUE</code> then the coefficients' estimates are reported under the 
usual IRT parameterization. See <b>Details</b> for more info.</p>
</td></tr>
<tr><td><code id="grm_+3A_hessian">Hessian</code></td>
<td>
<p>logical; if <code>TRUE</code> the Hessian matrix is computed.</p>
</td></tr>
<tr><td><code id="grm_+3A_start.val">start.val</code></td>
<td>
<p> a list of starting values or the character string <code>"random"</code>. If a list, each one of its 
elements corresponds to each item and should contain a numeric vector with initial values for the 
extremity parameters and discrimination parameter; even if <code>constrained = TRUE</code> the discrimination 
parameter should be provided for all the items. If <code>"random"</code> random starting values are computed.</p>
</td></tr>
<tr><td><code id="grm_+3A_na.action">na.action</code></td>
<td>
<p> the <code>na.action</code> to be used on <code>data</code>; default <code>NULL</code> the model uses the available 
cases, i.e., it takes into account the observed part of sample units with missing values (valid under MAR 
mechanisms if the model is correctly specified)..</p>
</td></tr>
<tr><td><code id="grm_+3A_control">control</code></td>
<td>
<p>a list of control values,
</p>

<dl>
<dt>iter.qN</dt><dd><p> the number of quasi-Newton iterations. Default 150.</p>
</dd>
<dt>GHk</dt><dd><p> the number of Gauss-Hermite quadrature points. Default 21.</p>
</dd>
<dt>method</dt><dd><p> the optimization method to be used in <code>optim()</code>. Default &quot;BFGS&quot;.</p>
</dd>
<dt>verbose</dt><dd><p> logical; if <code>TRUE</code> info about the optimization procedure are printed.</p>
</dd>
<dt>digits.abbrv</dt><dd><p> numeric value indicating the number of digits used in abbreviating the Item's names.
Default 6.</p>
</dd>
</dl>

</td></tr>
</table>


<h3>Details</h3>

 
<p>The Graded Response Model is a type of polytomous IRT model, specifically designed for ordinal manifest variables.
This model was first discussed by Samejima (1969) and it is mainly used in cases where the assumption of ordinal 
levels of response options is plausible.
</p>
<p>The model is defined as follows </p>
<p style="text-align: center;"><code class="reqn">\log\left(\frac{\gamma_{ik}}{1-\gamma_{ik}}\right) = \beta_i z - 
    \beta_{ik},</code>
</p>
<p> where <code class="reqn">\gamma_{ik}</code> denotes the cumulative 
probability of a response in category <code class="reqn">k</code>th or lower to the <code class="reqn">i</code>th item, given the latent ability <code class="reqn">z</code>. 
If <code>constrained = TRUE</code> it is assumed that <code class="reqn">\beta_i = \beta</code> for all <code class="reqn">i</code>.
</p>
<p>If <code>IRT.param = TRUE</code>, then the parameters estimates are reported under the usual IRT parameterization,
i.e., </p>
<p style="text-align: center;"><code class="reqn">\log\left(\frac{\gamma_{ik}}{1-\gamma_{ik}}\right) = \beta_i (z - \beta_{ik}^*),</code>
</p>
<p> where <code class="reqn">\beta_{ik}^* = \beta_{ik} / \beta_i</code>.
</p>
<p>The fit of the model is based on approximate marginal Maximum Likelihood, using the Gauss-Hermite quadrature rule 
for the approximation of the required integrals.
</p>


<h3>Value</h3>

<p> An object of class <code>grm</code> with components,
</p>
<table>
<tr><td><code>coefficients</code></td>
<td>
<p>a named list with components the parameter values at convergence for each item. These are always 
the estimates of <code class="reqn">\beta_{ik}, \beta_i</code> parameters, even if <code>IRT.param = TRUE</code>.</p>
</td></tr>
<tr><td><code>log.Lik</code></td>
<td>
<p>the log-likelihood value at convergence.</p>
</td></tr>
<tr><td><code>convergence</code></td>
<td>
<p>the convergence identifier returned by <code>optim()</code>.</p>
</td></tr>
<tr><td><code>hessian</code></td>
<td>
<p>the approximate Hessian matrix at convergence returned by <code>optim()</code>; returned 
only if <code>Hessian = TRUE</code>.</p>
</td></tr>
<tr><td><code>counts</code></td>
<td>
<p>the number of function and gradient evaluations used by the quasi-Newton algorithm.</p>
</td></tr>
<tr><td><code>patterns</code></td>
<td>
<p>a list with two components: (i) <code>X</code>: a numeric matrix 
that contains the observed response patterns, and (ii) <code>obs</code>: a numeric vector that contains the observed 
frequencies for each observed response pattern.</p>
</td></tr>
<tr><td><code>GH</code></td>
<td>
<p>a list with two components used in the Gauss-Hermite rule: (i) <code>Z</code>: a numeric matrix that contains 
the abscissas, and (ii) <code>GHw</code>: a numeric vector that contains the corresponding  weights.</p>
</td></tr> 
<tr><td><code>max.sc</code></td>
<td>
<p>the maximum absolute value of the score vector at convergence.</p>
</td></tr>
<tr><td><code>constrained</code></td>
<td>
<p>the value of the <code>constrained</code> argument.</p>
</td></tr>
<tr><td><code>IRT.param</code></td>
<td>
<p>the value of the <code>IRT.param</code> argument.</p>
</td></tr>
<tr><td><code>X</code></td>
<td>
<p>a copy of the response data matrix.</p>
</td></tr>
<tr><td><code>control</code></td>
<td>
<p>the values used in the <code>control</code> argument.</p>
</td></tr>
<tr><td><code>na.action</code></td>
<td>
<p>the value of the <code>na.action</code> argument.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>the matched call.</p>
</td></tr>
</table>


<h3>Warning</h3>

 
<p>In case the Hessian matrix at convergence is not positive definite try to re-fit the model,
using <code>start.val = "random"</code>.
</p>


<h3>Note</h3>

<p><code>grm()</code> returns the parameter estimates such that the discrimination parameter for the first item
<code class="reqn">\beta_1</code> is positive.
</p>
<p>When the coefficients' estimates are reported under the usual IRT parameterization (i.e., <code>IRT.param = TRUE</code>),
their standard errors are calculated using the Delta method.
</p>
<p><code>grm()</code> can also handle binary items, which should be coded as &lsquo;1, 2&rsquo; instead of &lsquo;0, 1&rsquo;.
</p>
<p>Some parts of the code used for the calculation of the log-likelihood and the score vector have been based 
on <code>polr()</code> from package MASS.
</p>


<h3>Author(s)</h3>

<p>Dimitris Rizopoulos <a href="mailto:d.rizopoulos@erasmusmc.nl">d.rizopoulos@erasmusmc.nl</a>
</p>


<h3>References</h3>

<p>Baker, F. and Kim, S-H. (2004) <em>Item Response Theory</em>, 2nd ed. 
New York: Marcel Dekker.
</p>
<p>Samejima, F. (1969). Estimation of latent ability using a response pattern of graded scores. 
<em>Psychometrika Monograph Supplement</em>, <b>34</b>, 100&ndash;114.
</p>
<p>Rizopoulos, D. (2006) <b>ltm</b>: An R package for latent variable modelling and item response theory analyses. 
<em>Journal of Statistical Software</em>, <b>17(5)</b>, 1&ndash;25. URL doi: <a href="https://doi.org/10.18637/jss.v017.i05">10.18637/jss.v017.i05</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+coef.grm">coef.grm</a></code>,
<code><a href="#topic+fitted.grm">fitted.grm</a></code>,
<code><a href="#topic+summary.grm">summary.grm</a></code>,
<code><a href="#topic+anova.grm">anova.grm</a></code>,
<code><a href="#topic+plot.grm">plot.grm</a></code>,
<code><a href="#topic+vcov.grm">vcov.grm</a></code>,
<code><a href="#topic+margins">margins</a></code>,
<code><a href="#topic+factor.scores">factor.scores</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## The Graded Response model for the Science data:
grm(Science[c(1,3,4,7)])

## The Graded Response model for the Science data,
## assuming equal discrimination parameters across items:
grm(Science[c(1,3,4,7)], constrained = TRUE)

## The Graded Response model for the Environment data
grm(Environment)

</code></pre>

<hr>
<h2 id='information'>
Area under the Test or Item Information Curves
</h2><span id='topic+information'></span>

<h3>Description</h3>

<p>Computes the amount of test or item information for a fitted IRT model, in a specified range.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>information(object, range, items = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="information_+3A_object">object</code></td>
<td>
<p>an object inheriting from either class <code>gpcm</code>, class <code>grm</code>, class <code>ltm</code>, class <code>rasch</code> or class <code>tpm</code>.</p>
</td></tr>
<tr><td><code id="information_+3A_range">range</code></td>
<td>
<p>a numeric interval for which the test information should be computed.</p>
</td></tr>
<tr><td><code id="information_+3A_items">items</code></td>
<td>
<p>the items for which the information should be computed; the default <code>NULL</code> corresponds
to all the items, which is equivalent to the test information.</p>
</td></tr>
<tr><td><code id="information_+3A_...">...</code></td>
<td>
<p>extra arguments passed to <code>integrate()</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The amount of information is computed as the area under the Item or Test Information Curve in the specified 
interval, using <code>integrate()</code>.
</p>


<h3>Value</h3>

<p>A list of class <code>information</code> with components,
</p>
<table>
<tr><td><code>InfoRange</code></td>
<td>
<p>the amount of information in the specified interval.</p>
</td></tr>
<tr><td><code>InfoTotal</code></td>
<td>
<p>the total amount of information; typically this is computed as the amount of information 
in the interval <code class="reqn">(-10, 10)</code>.</p>
</td></tr>
<tr><td><code>PropRange</code></td>
<td>
<p>the proportion of information in the specified range, i.e., 
<code>"Info in range" / "Total Info"</code>.</p>
</td></tr>
<tr><td><code>range</code></td>
<td>
<p>the value of <code>range</code> argument.</p>
</td></tr>
<tr><td><code>items</code></td>
<td>
<p>the value of <code>items</code> argument.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>the matched call for <code>object</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Dimitris Rizopoulos <a href="mailto:d.rizopoulos@erasmusmc.nl">d.rizopoulos@erasmusmc.nl</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+plot.gpcm">plot.gpcm</a></code>,
<code><a href="#topic+plot.grm">plot.grm</a></code>,
<code><a href="#topic+plot.ltm">plot.ltm</a></code>,
<code><a href="#topic+plot.rasch">plot.rasch</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
fit &lt;- rasch(LSAT)
information(fit, c(-2, 0))
information(fit, c(0, 2), items = c(3, 5))

</code></pre>

<hr>
<h2 id='item.fit'> Item-Fit Statistics and P-values </h2><span id='topic+item.fit'></span>

<h3>Description</h3>

<p>Computation of item fit statistics for <code>ltm</code>, <code>rasch</code> and <code>tpm</code> models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>item.fit(object, G = 10, FUN = median, 
         simulate.p.value = FALSE, B = 100)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="item.fit_+3A_object">object</code></td>
<td>
<p>a model object inheriting either from class <code>ltm</code>, class <code>rasch</code> or class <code>tpm</code>.</p>
</td></tr>
<tr><td><code id="item.fit_+3A_g">G</code></td>
<td>
<p>either a number or a numeric vector. If a number, then it denotes the number of categories sample units
are grouped according to their ability estimates.</p>
</td></tr>
<tr><td><code id="item.fit_+3A_fun">FUN</code></td>
<td>
<p>a function to summarize the ability estimate with each group (e.g., median, mean, etc.).</p>
</td></tr>
<tr><td><code id="item.fit_+3A_simulate.p.value">simulate.p.value</code></td>
<td>
<p>logical; if <code>TRUE</code>, then the Monte Carlo procedure described in the <b>Details</b> 
section is used to approximate the the distribution of the item-fit statistic under the null hypothesis.</p>
</td></tr>  
<tr><td><code id="item.fit_+3A_b">B</code></td>
<td>
<p>the number of replications in the Monte Carlo procedure.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The item-fit statistic computed by <code>item.fit()</code> has the form: </p>
<p style="text-align: center;"><code class="reqn">\sum \limits_{j = 1}^G \frac{N_j 
  (O_{ij} - E_{ij})^2}{E_{ij} (1 - E_{ij})},</code>
</p>

<p>where <code class="reqn">i</code> is the item, <code class="reqn">j</code> is the interval created by grouping sample units on the basis of their ability
estimates, <code class="reqn">G</code> is the number of sample units groupings (i.e., <code>G</code> argument), <code class="reqn">N_j</code> is the number of
sample units with ability estimates falling in a given interval <code class="reqn">j</code>, <code class="reqn">O_{ij}</code> is the observed proportion of 
keyed responses on item <code class="reqn">i</code> for interval <code class="reqn">j</code>, and <code class="reqn">E_{ij}</code> is the expected proportion of keyed responses 
on item <code class="reqn">i</code> for interval <code class="reqn">j</code> based on the IRT model (i.e., <code>object</code>) evaluated at the ability estimate 
<code class="reqn">z^*</code> within the interval, with <code class="reqn">z^*</code> denoting the result of <code>FUN</code> applied to the ability estimates in 
group <code class="reqn">j</code>.
</p>
<p>If <code>simulate.p.value = FALSE</code>, then the <code class="reqn">p</code>-values are computed assuming a chi-squared distribution with 
degrees of freedom equal to the number of groups <code>G</code> minus the number of estimated parameters. If 
<code>simulate.p.value = TRUE</code>, a Monte Carlo procedure is used to approximate the distribution of the item-fit 
statistic under the null hypothesis. In particular, the following steps are replicated <code>B</code> times:
</p>

<dl>
<dt>Step 1:</dt><dd><p>Simulate a new data-set of dichotomous responses under the assumed IRT model, using the maximum
likelihood estimates <code class="reqn">\hat{\theta}</code> in the original data-set, extracted from <code>object</code>.</p>
</dd>
<dt>Step 2:</dt><dd><p>Fit the model to the simulated data-set, extract the maximum likelihood estimates 
<code class="reqn">\theta^*</code> and compute the ability estimates <code class="reqn">z^*</code> for each response pattern.</p>
</dd>
<dt>Step 3:</dt><dd><p>For the new data-set, and using <code class="reqn">z^*</code> and <code class="reqn">\theta^*</code>, compute the value of the 
item-fit statistic.</p>
</dd>
</dl>

<p>Denote by <code class="reqn">T_{obs}</code> the value of the item-fit statistic for the original data-set. Then the <code class="reqn">p</code>-value is 
approximated according to the formula </p>
<p style="text-align: center;"><code class="reqn">\left(1 + \sum_{b = 1}^B I(T_b \geq T_{obs})\right) / (1 + B),</code>
</p>
<p> where <code class="reqn">I(.)</code> denotes the indicator function, and <code class="reqn">T_b</code> denotes 
the value of the item-fit statistic in the <code class="reqn">b</code>th simulated data-set.
</p>


<h3>Value</h3>

<p>An object of class <code>itemFit</code> is a list with components,
</p>
<table>
<tr><td><code>Tobs</code></td>
<td>
<p>a numeric vector with item-fit statistics.</p>
</td></tr>
<tr><td><code>p.values</code></td>
<td>
<p>a numeric vector with the corresponding <code class="reqn">p</code>-values.</p>
</td></tr>
<tr><td><code>G</code></td>
<td>
<p>the value of the <code>G</code> argument.</p>
</td></tr>
<tr><td><code>simulate.p.value</code></td>
<td>
<p>the value of the <code>simulate.p.value</code> argument.</p>
</td></tr>
<tr><td><code>B</code></td>
<td>
<p>the value of the <code>B</code> argument.</p>
</td></tr>  
<tr><td><code>call</code></td>
<td>
<p>a copy of the matched call of <code>object</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Dimitris Rizopoulos <a href="mailto:d.rizopoulos@erasmusmc.nl">d.rizopoulos@erasmusmc.nl</a>
</p>


<h3>References</h3>

<p>Reise, S. (1990) A comparison of item- and person-fit methods of assessing model-data fit in IRT. <em>Applied
Psychological Measurement</em>, <b>14</b>, 127&ndash;137.
</p>
<p>Yen, W. (1981) Using simulation results to choose a latent trait model. <em>Applied Psychological Measurement</em>, 
<b>5</b>, 245&ndash;262.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+person.fit">person.fit</a></code>,
<code><a href="#topic+margins">margins</a></code>,
<code><a href="#topic+GoF.gpcm">GoF.gpcm</a></code>,
<code><a href="#topic+GoF.rasch">GoF.rasch</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# item-fit statistics for the Rasch model
# for the Abortion data-set
item.fit(rasch(Abortion))

# Yen's Q1 item-fit statistic (i.e., 10 latent ability groups; the
# mean ability in each group is used to compute fitted proportions) 
# for the two-parameter logistic model for the LSAT data-set
item.fit(ltm(LSAT ~ z1), FUN = mean)

</code></pre>

<hr>
<h2 id='LSAT'> The Law School Admission Test (LSAT), Section VI </h2><span id='topic+LSAT'></span>

<h3>Description</h3>

<p>The LSAT is a classical example in educational testing for measuring
ability traits. This test was designed to measure a <em>single</em>
latent ability scale.
</p>


<h3>Format</h3>

<p>A data frame with the responses of 1000 individuals to 5 questions.
</p>


<h3>Source</h3>

<p>This LSAT example is a part of a data set given in Bock and Lieberman (1970).
</p>


<h3>References</h3>

<p>Bartholomew, D., Steel, F., Moustaki, I. and Galbraith, J. (2002)
<em>The Analysis and Interpretation of Multivariate Data for
Social Scientists</em>. London: Chapman and Hall.
</p>
<p>Bock, R. and Lieberman, M. (1970) Fitting a response model for <code class="reqn">n</code>
dichotomously scored items. <em>Psychometrika</em>, <b>35</b>, 179&ndash;197.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Descriptive statistics for LSAT data
dsc &lt;- descript(LSAT)
dsc
plot(dsc)

</code></pre>

<hr>
<h2 id='ltm'> Latent Trait Model - Latent Variable Model for Binary Data</h2><span id='topic+ltm'></span>

<h3>Description</h3>

<p>Fit a latent trait model under the Item Response Theory (IRT) approach.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ltm(formula, constraint = NULL, IRT.param, start.val, 
    na.action = NULL, control = list())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ltm_+3A_formula">formula</code></td>
<td>
<p> a two-sided formula providing the responses data matrix and describing the latent 
structure. In the left side of <code>formula</code> either a <code>data.frame</code> (that will be converted to 
a numeric matrix using <code>data.matrix()</code>) or a numeric <code>matrix</code> of manifest variables must be 
supplied. In the right side of <code>formula</code> only 
two latent variables are allowed with codenames <code>z1</code>, <code>z2</code>. Interaction and quadratic 
terms can also be used (see <b>Details</b> and <b>Examples</b> for more info).</p>
</td></tr>
<tr><td><code id="ltm_+3A_constraint">constraint</code></td>
<td>
<p>a three-column numeric matrix with at most <code class="reqn">pq - 1</code> rows (where <code class="reqn">p</code> is the number 
of items and <code class="reqn">q</code> the number of latent components plus the intercept), specifying fixed-value 
constraints. The first column represents the item (i.e., <code class="reqn">1</code> denotes the first item, <code class="reqn">2</code> the 
second, etc.), the second column represents the component of the latent structure (i.e., <code class="reqn">1</code> 
denotes the intercept <code class="reqn">\beta_{0i}</code>, <code class="reqn">2</code> the loadings of the first factor <code class="reqn">\beta_
        {1i}</code>, etc.) and the third column denotes the value at which the corresponding parameter 
should be fixed. See <b>Details</b> and <b>Examples</b> for more info.</p>
</td></tr>
<tr><td><code id="ltm_+3A_irt.param">IRT.param</code></td>
<td>
<p>logical; if <code>TRUE</code> then the coefficients' estimates for the two-parameter logistic 
model are reported under the usual IRT parameterization. See <b>Details</b> for more info.</p>
</td></tr>
<tr><td><code id="ltm_+3A_start.val">start.val</code></td>
<td>
<p>the character string &quot;random&quot; or a numeric matrix supplying starting values with <code class="reqn">p</code> rows and 
<code class="reqn">q</code> columns, with <code class="reqn">p</code> denoting the number of items, and <code>q</code> denoting the number of terms in the 
right-hand side of <code>formula</code>. If <code>NULL</code> starting values are automatically computed. If &quot;random&quot;, 
random starting values are used. If a matrix, then depending on the latent structure specified in <code>formula</code>, 
the first column should contain <code class="reqn">\beta_{0i}</code>, the second <code class="reqn">\beta_{1i}</code>, the third <code class="reqn">\beta_{2i}</code>, and 
the remaing columns <code class="reqn">\beta_{nl,i}</code> (see <b>Details</b>)</p>
</td></tr></table>
<p>.
</p>
<table>
<tr><td><code id="ltm_+3A_na.action">na.action</code></td>
<td>
<p> the <code>na.action</code> to be used on the data frame in the left side of <code>formula</code>.
In case of missing data, if <code>na.action = NULL</code> the model uses the available cases, i.e., it takes 
into account the observed part of sample units with missing values (valid under MAR mechanisms if the
model is correctly specified). If you want to apply a complete case analysis then use 
<code>na.action = na.exclude</code>.</p>
</td></tr>
<tr><td><code id="ltm_+3A_control">control</code></td>
<td>
<p> a list of control values,
</p>

<dl>
<dt>iter.em</dt><dd><p> the number of EM iterations. Default 40.</p>
</dd>
<dt>iter.qN</dt><dd><p> the number of quasi-Newton iterations. Default 150.</p>
</dd>
<dt>GHk</dt><dd><p> the number of Gauss-Hermite quadrature points. Default 15.</p>
</dd>
<dt>method</dt><dd><p> the optimization method to be used in <code>optim()</code>. Default &quot;BFGS&quot;.</p>
</dd>
<dt>verbose</dt><dd><p> logical; if <code>TRUE</code> info about the optimization procedure are printed.</p>
</dd>
</dl>

</td></tr>
</table>


<h3>Details</h3>

<p>The latent trait model is the analogue of the factor analysis model for binary observed data. 
The model assumes that the dependencies between the observed response variables (known as items) 
can be interpreted by a small number of latent variables. The model formulation is under the IRT 
approach; in particular, </p>
<p style="text-align: center;"><code class="reqn">\log\left(\frac{\pi_{i}}{1-\pi_{i}}\right)=\beta_{0i} + \beta_{1i}z_1 + 
    \beta_{2i}z_2,</code>
</p>
<p> where <code class="reqn">\pi_i</code> is the the 
probability of a positive response in the <code class="reqn">i</code>th item, <code class="reqn">\beta_{i0}</code> is the easiness parameter, 
<code class="reqn">\beta_{ij}</code> (<code class="reqn">j=1,2</code>) are the discrimination parameters and <code class="reqn">z_1, z_2</code> denote the two 
latent variables.
</p>
<p>The usual form of the latent trait model assumes linear latent variable effects (Bartholomew and 
Knott, 1999; Moustaki and Knott, 2000). <code>ltm()</code> fits the linear one- and two-factor models but 
also provides extensions described by Rizopoulos and Moustaki (2006) to include nonlinear latent 
variable effects. These are incorporated in the linear predictor of the model, i.e., </p>
<p style="text-align: center;"><code class="reqn">\log\left
    (\frac{\pi_{i}}{1-\pi_{i}}\right)=\beta_{0i} + \beta_{1i}z_1 + \beta_{2i}z_2 + \beta_{nl}^tf(z_1, z_2),
    </code>
</p>
<p> where <code class="reqn">f(z_1, z_2)</code> is
a function of <code class="reqn">z_1</code> and <code class="reqn">z_2</code> (e.g., <code class="reqn">f(z_1, z_2) = z_1z_2</code>, <code class="reqn">f(z_1, z_2) = z_1^2</code>, etc.) and 
<code class="reqn">\beta_{nl}</code> is a matrix of nonlinear terms parameters (look also at the <b>Examples</b>).
</p>
<p>If <code>IRT.param = TRUE</code>, then the parameters estimates for the two-parameter logistic
model (i.e., the model with one factor) are reported under the usual IRT parameterization, i.e., 
</p>
<p style="text-align: center;"><code class="reqn">\log\left(\frac{\pi_i}{1-\pi_i}\right) = \beta_{1i} (z - \beta_{0i}^*).</code>
</p>
 
<p>The linear two-factor model is unidentified under orthogonal rotations on the factors' 
space. To achieve identifiability you can fix the value of one loading using the <code>constraint</code> 
argument. 
</p>
<p>The parameters are estimated by maximizing the approximate marginal log-likelihood under the conditional 
independence assumption, i.e., conditionally on the latent structure the items are independent Bernoulli 
variates under the logit link. The required integrals are approximated using the Gauss-Hermite rule. The 
optimization procedure used is a hybrid algorithm. The procedure initially uses a moderate number of EM 
iterations (see <code>control</code> argument <code>iter.em</code>) and then switches to quasi-Newton (see <code>control</code> 
arguments <code>method</code> and <code>iter.qN</code>) iterations until convergence.
</p>


<h3>Value</h3>

<p>An object of class <code>ltm</code> with components,
</p>
<table>
<tr><td><code>coefficients</code></td>
<td>
<p>a matrix with the parameter values at convergence. These are always the estimates of 
<code class="reqn">\beta_{li}, l = 0, 1, \ldots</code> parameters, even if <code>IRT.param = TRUE</code>.</p>
</td></tr>
<tr><td><code>log.Lik</code></td>
<td>
<p>the log-likelihood value at convergence.</p>
</td></tr>
<tr><td><code>convergence</code></td>
<td>
<p>the convergence identifier returned by <code>optim()</code>.</p>
</td></tr>
<tr><td><code>hessian</code></td>
<td>
<p>the approximate Hessian matrix at convergence returned by <code>optim()</code>.</p>
</td></tr>
<tr><td><code>counts</code></td>
<td>
<p>the number of function and gradient evaluations used by the quasi-Newton algorithm.</p>
</td></tr>
<tr><td><code>patterns</code></td>
<td>
<p>a list with two components: (i) <code>X</code>: a numeric matrix 
that contains the observed response patterns, and (ii) <code>obs</code>: a numeric vector that contains the observed 
frequencies for each observed response pattern.</p>
</td></tr>
<tr><td><code>GH</code></td>
<td>
<p>a list with two components used in the Gauss-Hermite rule: (i) <code>Z</code>: a numeric matrix that contains 
the abscissas, and (ii) <code>GHw</code>: a numeric vector that contains the corresponding  weights.</p>
</td></tr> 
<tr><td><code>max.sc</code></td>
<td>
<p>the maximum absolute value of the score vector at convergence.</p>
</td></tr>
<tr><td><code>ltst</code></td>
<td>
<p>a list describing the latent structure.</p>
</td></tr>
<tr><td><code>X</code></td>
<td>
<p>a copy of the response data matrix.</p>
</td></tr>
<tr><td><code>control</code></td>
<td>
<p>the values used in the <code>control</code> argument.</p>
</td></tr>
<tr><td><code>IRT.param</code></td>
<td>
<p>the value of the <code>IRT.param</code> argument.</p>
</td></tr>
<tr><td><code>constraint</code></td>
<td>
<p><code>if(!is.null(constraint))</code>, then it contains the value of the <code>constraint</code> argument.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>the matched call.</p>
</td></tr>
</table>


<h3>Warning</h3>

 
<p>In case the Hessian matrix at convergence is not positive definite, try
to re-fit the model; <code>ltm()</code> will use new random starting values.
</p>
<p>The inclusion of nonlinear latent variable effects produces more
complex likelihood surfaces which might possess a number of local
maxima. To ensure that the maximum likelihood value has been
reached re-fit the model a number of times (simulations showed
that usually 10 times are adequate to ensure global convergence).
</p>
<p>Conversion of the parameter estimates to the usual IRT parameterization
works only for the two-parameter logistic model.
</p>


<h3>Note</h3>

<p>In the case of the one-factor model, the optimization algorithm works under the constraint that 
the discrimination parameter of the first item <code class="reqn">\beta_{11}</code> is always positive. If you wish 
to change its sign, then in the fitted model, say <code>m</code>, use <code>m$coef[, 2] &lt;- -m$coef[, 2]</code>.
</p>
<p>When the coefficients' estimates are reported under the usual IRT parameterization (i.e., <code>IRT.param = TRUE</code>),
their standard errors are calculated using the Delta method.
</p>


<h3>Author(s)</h3>

<p>Dimitris Rizopoulos <a href="mailto:d.rizopoulos@erasmusmc.nl">d.rizopoulos@erasmusmc.nl</a>
</p>


<h3>References</h3>

<p>Baker, F. and Kim, S-H. (2004) <em>Item Response Theory</em>, 2nd ed. 
New York: Marcel Dekker.
</p>
<p>Bartholomew, D. and Knott, M. (1999) <em>Latent Variable Models
and Factor Analysis</em>, 2nd ed. London: Arnold.
</p>
<p>Bartholomew, D., Steel, F., Moustaki, I. and Galbraith, J. (2002)
<em>The Analysis and Interpretation of Multivariate Data for
Social Scientists</em>. London: Chapman and Hall.
</p>
<p>Moustaki, I. and Knott, M. (2000) Generalized latent trait
models. <em>Psychometrika</em>, <b>65</b>, 391&ndash;411.
</p>
<p>Rizopoulos, D. (2006) <b>ltm</b>: An R package for latent variable modelling and item response theory analyses. 
<em>Journal of Statistical Software</em>, <b>17(5)</b>, 1&ndash;25. URL doi: <a href="https://doi.org/10.18637/jss.v017.i05">10.18637/jss.v017.i05</a>
</p>
<p>Rizopoulos, D. and Moustaki, I. (2008) Generalized latent variable models
with nonlinear effects. <em>British Journal of Mathematical and Statistical Psychology</em>, <b>61</b>, 415&ndash;438.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+coef.ltm">coef.ltm</a></code>,
<code><a href="#topic+fitted.ltm">fitted.ltm</a></code>,
<code><a href="#topic+summary.ltm">summary.ltm</a></code>,
<code><a href="#topic+anova.ltm">anova.ltm</a></code>,
<code><a href="#topic+plot.ltm">plot.ltm</a></code>,
<code><a href="#topic+vcov.ltm">vcov.ltm</a></code>,
<code><a href="#topic+item.fit">item.fit</a></code>,
<code><a href="#topic+person.fit">person.fit</a></code>,  
<code><a href="#topic+margins">margins</a></code>,
<code><a href="#topic+factor.scores">factor.scores</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## The two-parameter logistic model for the WIRS data
## with the constraint that (i) the easiness parameter 
## for the 1st item equals 1 and (ii) the discrimination
## parameter for the 6th item equals -0.5

ltm(WIRS ~ z1, constr = rbind(c(1, 1, 1), c(6, 2, -0.5)))


## One-factor and a quadratic term
## using the Mobility data
ltm(Mobility ~ z1 + I(z1^2))

## Two-factor model with an interaction term
## using the WIRS data
ltm(WIRS ~ z1 * z2)


## The two-parameter logistic model for the Abortion data 
## with 20 quadrature points and 20 EM iterations;
## report results under the usual IRT parameterization
ltm(Abortion ~ z1, control = list(GHk = 20, iter.em = 20))

</code></pre>

<hr>
<h2 id='margins'>
Fit of the model on the margins
</h2><span id='topic+margins'></span><span id='topic+margins.gpcm'></span><span id='topic+margins.grm'></span><span id='topic+margins.ltm'></span><span id='topic+margins.rasch'></span><span id='topic+margins.tpm'></span>

<h3>Description</h3>

<p>Checks the fit on the two- and three-way margins for <code>grm</code>, <code>ltm</code>, <code>rasch</code> and <code>tpm</code> objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
margins(object, ...)

## S3 method for class 'gpcm'
margins(object, type = c("two-way", "three-way"), rule = 3.5, ...)

## S3 method for class 'grm'
margins(object, type = c("two-way", "three-way"), rule = 3.5, ...)

## S3 method for class 'ltm'
margins(object, type = c("two-way", "three-way"), rule = 3.5, 
        nprint = 3, ...)

## S3 method for class 'rasch'
margins(object, type = c("two-way", "three-way"), rule = 3.5, 
        nprint = 3, ...)

## S3 method for class 'tpm'
margins(object, type = c("two-way", "three-way"), rule = 3.5, 
        nprint = 3, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="margins_+3A_object">object</code></td>
<td>
<p> an object inheriting either from class <code>gpcm</code>, class <code>grm</code>, class <code>ltm</code> or class <code>rasch</code>.</p>
</td></tr>
<tr><td><code id="margins_+3A_type">type</code></td>
<td>
<p> the type of margins to be used. See <b>Details</b> for more info. </p>
</td></tr>
<tr><td><code id="margins_+3A_rule">rule</code></td>
<td>
<p> the rule of thumb used in determining the indicative goodness-of-fit.</p>
</td></tr>
<tr><td><code id="margins_+3A_nprint">nprint</code></td>
<td>
<p> a numeric value determining the number of margins with the largest Chi-squared residuals 
to be printed; only for <code>ltm</code> and <code>rasch</code> objects.</p>
</td></tr>
<tr><td><code id="margins_+3A_...">...</code></td>
<td>
<p> additional argument; currently none is used. </p>
</td></tr>
</table>


<h3>Details</h3>

<p>Rather than looking at the whole set of response patterns, we can look at the two- and three-way margins. 
For the former, we construct the <code class="reqn">2 \times 2</code> contingency tables obtained by taking 
the variables two at a time. Comparing the observed and expected two-way margins is analogous to comparing 
the observed and expected correlations when judging the fit of a factor analysis model. For Bernoulli and
Ordinal variates, the comparison is made using the so called Chi-squared residuals. As a rule of thumb residuals 
greater than 3.5 are indicative of poor fit. For a more strict rule of thumb use the <code>rule</code> argument. 
The analogous procedure is followed for the three-way margins.
</p>


<h3>Value</h3>

<p>An object of either class <code>margins.ltm</code> if <code>object</code> inherits from class <code>ltm</code>, class <code>rasch</code> or class <code>tpm</code>, 
or an object of class <code>margins.grm</code> if <code>object</code> inherits from class <code>grm</code>, with components,
</p>
<table>
<tr><td><code>margins</code></td>
<td>
<p>for <code>margins.ltm</code> is an array containing the values of chi-squared residuals; 
for <code>margins.gpcm</code> and <code>margins.grm</code> is a list of length either the number of all possible pairs or all possible 
triplets of items, containing the observed and expected frequencies, the values of chi-squared 
residuals, the value of the total residual and the value of the rule of thumb times the product of
the number of categories of the items under consideration.</p>
</td></tr>
<tr><td><code>type</code></td>
<td>
<p>the type of margins that were calculated.</p>
</td></tr>
<tr><td><code>nprint</code></td>
<td>
<p>the value of the <code>nprint</code> argument; returned only from <code>margins.ltm</code>.</p>
</td></tr>
<tr><td><code>combs</code></td>
<td>
<p>all possible two- or three-way combinations of the items; returned only from <code>margins.ltm</code>.</p>
</td></tr>
<tr><td><code>rule</code></td>
<td>
<p>the value of the <code>rule</code> argument; returned only from <code>margins.ltm</code>.</p>
</td></tr>
<tr><td><code>nitems</code></td>
<td>
<p>the number of items in <code>object</code>; returned only from <code>margins.grm</code>.</p>
</td></tr>
<tr><td><code>names</code></td>
<td>
<p>the names of items in <code>object</code>; returned only from <code>margins.grm</code>.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>a copy of the matched call of <code>object</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Dimitris Rizopoulos <a href="mailto:d.rizopoulos@erasmusmc.nl">d.rizopoulos@erasmusmc.nl</a>
</p>


<h3>References</h3>

<p>Bartholomew, D. (1998) Scaling unobservable constructs in social science. 
<em>Applied Statistics</em>, <b>47</b>, 1&ndash;13.
</p>
<p>Bartholomew, D. and Knott, M. (1999) <em>Latent Variable Models
and Factor Analysis</em>, 2nd ed. London: Arnold.
</p>
<p>Bartholomew, D., Steel, F., Moustaki, I. and Galbraith, J. (2002)
<em>The Analysis and Interpretation of Multivariate Data for
Social Scientists</em>. London: Chapman and Hall.
</p>
<p>Rizopoulos, D. (2006) <b>ltm</b>: An R package for latent variable modelling and item response theory analyses. 
<em>Journal of Statistical Software</em>, <b>17(5)</b>, 1&ndash;25. URL doi: <a href="https://doi.org/10.18637/jss.v017.i05">10.18637/jss.v017.i05</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+person.fit">person.fit</a></code>,
<code><a href="#topic+item.fit">item.fit</a></code>,
<code><a href="#topic+GoF.rasch">GoF.rasch</a></code>,
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Two- and Three-way residuals for the Rasch model
fit &lt;- rasch(LSAT)
margins(fit)
margins(fit, "three")


## Two- and Three-way residuals for the one-factor model
fit &lt;- ltm(WIRS ~ z1)
margins(fit)
margins(fit, "three")


## Two- and Three-way residuals for the graded response model
fit &lt;- grm(Science[c(1,3,4,7)])
margins(fit)
margins(fit, "three")

</code></pre>

<hr>
<h2 id='Mobility'> Women's Mobility </h2><span id='topic+Mobility'></span>

<h3>Description</h3>

<p>A rural subsample of 8445 women from the Bangladesh Fertility Survey of 1989.
</p>


<h3>Format</h3>

<p>The dimension of interest is women's mobility of social freedom. Women were asked
whether they could engage in the following activities alone (1 = yes, 0 = no):  
</p>

<dl>
<dt>Item 1</dt><dd><p>Go to any part of the village/town/city.</p>
</dd>
<dt>Item 2</dt><dd><p>Go outside the village/town/city.</p>
</dd>
<dt>Item 3</dt><dd><p>Talk to a man you do not know.</p>
</dd>
<dt>Item 4</dt><dd><p>Go to a cinema/cultural show.</p>
</dd>
<dt>Item 5</dt><dd><p>Go shopping.</p>
</dd>
<dt>Item 6</dt><dd><p>Go to a cooperative/mothers' club/other club.</p>
</dd>
<dt>Item 7</dt><dd><p>Attend a political meeting.</p>
</dd>
<dt>Item 8</dt><dd><p>Go to a health centre/hospital.</p>
</dd>
</dl>



<h3>Source</h3>

<p>Bangladesh Fertility Survey of 1989 (Huq and Cleland, 1990).
</p>


<h3>References</h3>

<p>Bartholomew, D., Steel, F., Moustaki, I. and Galbraith, J. (2002) <em>The Analysis and Interpretation of 
Multivariate Data for Social Scientists</em>. London: Chapman and Hall.
</p>
<p>Huq, N. and Cleland, J. (1990) <em>Bangladesh Fertility Survey, 1989</em>. Dhaka: National
Institute of Population Research and Training (NIPORT).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Descriptive statistics for Mobility data
descript(Mobility)

</code></pre>

<hr>
<h2 id='mult.choice'>
Multiple Choice Items to Binary Responses 
</h2><span id='topic+mult.choice'></span>

<h3>Description</h3>

<p>It converts multiple choice items to a matrix of binary responses.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mult.choice(data, correct)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="mult.choice_+3A_data">data</code></td>
<td>
<p> a <code>matrix</code> or a <code>data.frame</code> containing the manifest variables as columns. </p>
</td></tr>
<tr><td><code id="mult.choice_+3A_correct">correct</code></td>
<td>
<p>a vector of length <code>ncol(data)</code> with the correct responses.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a matrix of 0/1 values indicating wrong/correct answers.
</p>


<h3>Author(s)</h3>

<p>Dimitris Rizopoulos <a href="mailto:d.rizopoulos@erasmusmc.nl">d.rizopoulos@erasmusmc.nl</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
dat &lt;- data.frame(It1 = sample(4, 100, TRUE),
                  It2 = sample(4, 100, TRUE),
                  It3 = sample(5, 100, TRUE),
                  It4 = sample(5, 100, TRUE),
                  It5 = sample(4, 100, TRUE),
                  It6 = sample(5, 100, TRUE))
dat[] &lt;- lapply(dat, function (x) { x[sample(100, 4)] &lt;- NA; x })
crct &lt;- c(3, 2, 5, 3, 4, 5)
####################
mult.choice(dat, crct)

</code></pre>

<hr>
<h2 id='person.fit'> Person-Fit Statistics and P-values </h2><span id='topic+person.fit'></span>

<h3>Description</h3>

<p>Computation of person fit statistics for <code>ltm</code>, <code>rasch</code> and <code>tpm</code> models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>person.fit(object, alternative = c("less", "greater", "two.sided"), 
           resp.patterns = NULL, FUN = NULL, simulate.p.value = FALSE, 
           B = 1000)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="person.fit_+3A_object">object</code></td>
<td>
<p>a model object inheriting either from class <code>ltm</code>, class <code>rasch</code> or class <code>tpm</code>.</p>
</td></tr>
<tr><td><code id="person.fit_+3A_alternative">alternative</code></td>
<td>
<p>the alternative hypothesis; see <b>Details</b> for more info.</p>
</td></tr>
<tr><td><code id="person.fit_+3A_resp.patterns">resp.patterns</code></td>
<td>
<p>a matrix or a data.frame of response patterns with columns denoting the items; if <code>NULL</code> 
the person fit statistics are computed for the observed response patterns.</p>
</td></tr>
<tr><td><code id="person.fit_+3A_fun">FUN</code></td>
<td>
<p>a function with three arguments calculating a user-defined person-fit statistic. The first argument must
be a numeric matrix of (0, 1) response patterns. The second argument must be a numeric vector of length equal to 
the number of rows of the first argument, providing the ability estimates for each response pattern. The third
argument must be a numeric matrix with number of rows equal to the number of items, providing the IRT model
parameters. For <code>ltm</code> and <code>rasch</code> objects, this should be a two-column matrix, where the first
column contains the easiness and the second one the discrimination parameters (i.e., the additive 
parameterization is assumed, which has the form <code class="reqn">\beta_{i0} + \beta_{i1}z</code>, where 
<code class="reqn">\beta_{i0}</code> is the easiness and <code class="reqn">\beta_{i1}</code> the discrimination parameter for 
the <code class="reqn">i</code>th item). For <code>tpm</code> objects the first column of the third argument of <code>FUN</code> should contain 
the logit (i.e., use <code>qlogis()</code>) of the guessing parameters, the second column the easiness, and the third
column the discrimination parameters. The function should return a numeric vector of length equal to the number
of response patterns, containing the values of the user-defined person-fit statistics.</p>
</td></tr>
<tr><td><code id="person.fit_+3A_simulate.p.value">simulate.p.value</code></td>
<td>
<p>logical; if <code>TRUE</code>, then the Monte Carlo procedure described in the <b>Details</b> 
section is used to approximate the the distribution of the person-fit statistic(s) under the null hypothesis.</p>
</td></tr>
<tr><td><code id="person.fit_+3A_b">B</code></td>
<td>
<p>the number of replications in the Monte Carlo procedure.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The statistics calculated by default (i.e., if <code>FUN = NULL</code>) by <code>person.fit()</code> are the <code class="reqn">L_0</code> statistic 
of Levine and Rubin (1979) and its standardized version <code class="reqn">L_z</code> proposed by Drasgow et al. (1985). 
If <code>simulate.p.value = FALSE</code>, the <code class="reqn">p</code>-values are calculated for the <code class="reqn">L_z</code> assuming a standard normal 
distribution for the statistic under the null. If <code>simulate.p.value = TRUE</code>, a Monte Carlo procedure is used to 
approximate the distribution of the person-fit statistic(s) under the null hypothesis. In particular, the following 
steps are replicated <code>B</code> times for each response pattern:
</p>

<dl>
<dt>Step 1:</dt><dd><p>Simulate a new ability estimate, say <code class="reqn">z^*</code>, from a normal distribution with mean the ability 
estimate of the response pattern under the fitted model (i.e., <code>object</code>), and standard 
deviation the standard error of the ability estimate, as returned by the <code><a href="#topic+factor.scores">factor.scores</a></code> function.</p>
</dd>
<dt>Step 2:</dt><dd><p>Simulate a new response pattern of dichotomous items under the assumed IRT model, using <code class="reqn">z^*</code> and 
the maximum likelihood estimates under <code>object</code>.</p>
</dd>
<dt>Step 4:</dt><dd><p>For the new response pattern and using <code class="reqn">z^*</code> and the MLEs, compute the values 
of the person-fit statistic.</p>
</dd>
</dl>

<p>Denote by <code class="reqn">T_{obs}</code> the value of the person-fit statistic for the original data-set. Then the <code class="reqn">p</code>-value is 
approximated according to the formula </p>
<p style="text-align: center;"><code class="reqn">\left(1 + \sum_{b = 1}^B I(T_b \leq T_{obs})\right) / (1 + B),</code>
</p>
<p> if <code>alternative = "less"</code>, </p>
<p style="text-align: center;"><code class="reqn">\left(1 + \sum_{b = 1}^B I(T_b \geq 
  T_{obs})\right) / (1 + B),</code>
</p>
<p> if <code>alternative = "greater"</code>, or 
</p>
<p style="text-align: center;"><code class="reqn">\left(1 + \sum_{b = 1}^B I(|T_b| \geq |T_{obs}|)\right) / (1 + B),</code>
</p>
<p> if <code>alternative = "two.sided"</code>, where <code class="reqn">T_b</code> denotes the value of the person-fit statistic in the 
<code class="reqn">b</code>th simulated data-set, <code class="reqn">I(.)</code> denotes the indicator function, and <code class="reqn">|.|</code> denotes the absolute value. 
For the <code class="reqn">L_z</code> statistic, negative values (i.e., <code>alternative = "less"</code>) indicate response patterns that 
are unlikely, given the measurement model and the ability estimate. Positive values (i.e., <code>alternative = 
  "greater"</code>) indicate that the examinee's response pattern is more consistent than the probabilistic IRT model 
expected. Finally, when <code>alternative = "two.sided"</code> both the above settings are captured.
</p>
<p>This simulation scheme explicitly accounts for the fact that ability values are estimated, by drawing 
from their large sample distribution. Strictly speaking, drawing <code class="reqn">z^*</code> from a normal distribution is not 
theoretically appropriate, since the posterior distribution for the latent abilities is not normal. However, the 
normality assumption will work reasonably well, especially when a large number of items is considered.
</p>


<h3>Value</h3>

<p>An object of class <code>persFit</code> is a list with components,
</p>
<table>
<tr><td><code>resp.patterns</code></td>
<td>
<p>the response patterns for which the fit statistics have been computed.</p>
</td></tr>
<tr><td><code>Tobs</code></td>
<td>
<p>a numeric matrix with person-fit statistics for each response pattern.</p>
</td></tr>
<tr><td><code>p.values</code></td>
<td>
<p>a numeric matrix with the corresponding <code class="reqn">p</code>-values.</p>
</td></tr>
<tr><td><code>statistic</code></td>
<td>
<p>the value of the <code>statistic</code> argument.</p>
</td></tr>
<tr><td><code>FUN</code></td>
<td>
<p>the value of the <code>FUN</code> argument.</p>
</td></tr>
<tr><td><code>alternative</code></td>
<td>
<p>the value of the <code>alternative</code> argument.</p>
</td></tr>
<tr><td><code>B</code></td>
<td>
<p>the value of the <code>B</code> argument.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>a copy of the matched call of <code>object</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Dimitris Rizopoulos <a href="mailto:d.rizopoulos@erasmusmc.nl">d.rizopoulos@erasmusmc.nl</a>
</p>


<h3>References</h3>

<p>Drasgow, F., Levine, M. and Williams, E. (1985) Appropriateness measurement with polychotomous item
response models and standardized indices. <em>British Journal of Mathematical and Statistical Psychology</em>, 
<b>38</b>, 67&ndash;86.
</p>
<p>Levine, M. and Rubin, D. (1979) Measuring the appropriateness of multiple-choice test scores. <em>Journal of
Educational Statistics</em>, <b>4</b>, 269&ndash;290.
</p>
<p>Meijer, R. and Sijtsma, K. (2001) Methodology review: Evaluating person fit. <em>Applied
Psychological Measurement</em>, <b>25</b>, 107&ndash;135.
</p>
<p>Reise, S. (1990) A comparison of item- and person-fit methods of assessing model-data fit in IRT. <em>Applied
Psychological Measurement</em>, <b>14</b>, 127&ndash;137.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+item.fit">item.fit</a></code>,
<code><a href="#topic+margins">margins</a></code>,
<code><a href="#topic+GoF.gpcm">GoF.gpcm</a></code>,
<code><a href="#topic+GoF.rasch">GoF.rasch</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# person-fit statistics for the Rasch model
# for the Abortion data-set
person.fit(rasch(Abortion))

# person-fit statistics for the two-parameter logistic model
# for the LSAT data-set
person.fit(ltm(LSAT ~ z1), simulate.p.value = TRUE, B = 100)

</code></pre>

<hr>
<h2 id='plot+20descript'>Descriptive Statistics Plot method</h2><span id='topic+plot.descript'></span>

<h3>Description</h3>

<p>The plot method for <code>descript</code> objects currently works for dichotomous response patterns, and produces the 
xy-plot of the total score versus the proportion of correct responses for each item.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
## S3 method for class 'descript'
plot(x, items = NULL, includeFirstLast = FALSE, xlab, ylab, ...)

</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot+2B20descript_+3A_x">x</code></td>
<td>
<p>an object inheriting from class <code>descript</code>.</p>
</td></tr>
<tr><td><code id="plot+2B20descript_+3A_items">items</code></td>
<td>
<p>a numeric vector indicating which items to plot.</p>
</td></tr>
<tr><td><code id="plot+2B20descript_+3A_includefirstlast">includeFirstLast</code></td>
<td>
<p>logical; if <code>TRUE</code> the first and last total scores categories are included.</p>
</td></tr>
<tr><td><code id="plot+2B20descript_+3A_xlab">xlab</code>, <code id="plot+2B20descript_+3A_ylab">ylab</code></td>
<td>
<p>character string or an <code><a href="base.html#topic+expression">expression</a></code>; see <code><a href="graphics.html#topic+title">title</a></code>.</p>
</td></tr>
<tr><td><code id="plot+2B20descript_+3A_...">...</code></td>
<td>
<p> extra graphical parameters to be passed to <code>matplot()</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Dimitris Rizopoulos <a href="mailto:d.rizopoulos@erasmusmc.nl">d.rizopoulos@erasmusmc.nl</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+descript">descript</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Descriptives for WIRS data:
dsc &lt;- descript(WIRS)
dsc
plot(dsc, includeFirstLast = TRUE, type = "b", lty = 1, pch = 1:6)
legend("topleft", names(WIRS), pch = 1:6, col = 1:6, lty = 1, bty = "n")

</code></pre>

<hr>
<h2 id='plot+20fscores'>Factor Scores - Ability Estimates Plot method</h2><span id='topic+plot.fscores'></span>

<h3>Description</h3>

<p>Plots a Kernel Density Estimation of the distribution of the factor scores (i.e., person parameters). Provides 
also the option to include in the plot the item difficulty parameters (similar to the Item Person Maps).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
## S3 method for class 'fscores'
plot(x, bw = "nrd0", adjust = 2, kernel = "gaussian", 
    include.items = FALSE, tol = 0.2, xlab = "Ability", ylab = "Density", 
    main = "Kernel Density Estimation for Ability Estimates", 
    pch = 16, cex = 1.5, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot+2B20fscores_+3A_x">x</code></td>
<td>
<p>an object inheriting from class <code>fscores</code>.</p>
</td></tr>
<tr><td><code id="plot+2B20fscores_+3A_bw">bw</code>, <code id="plot+2B20fscores_+3A_adjust">adjust</code>, <code id="plot+2B20fscores_+3A_kernel">kernel</code></td>
<td>
<p>arguments to <code>density()</code>.</p>
</td></tr>
<tr><td><code id="plot+2B20fscores_+3A_include.items">include.items</code></td>
<td>
<p>logical; if <code>TRUE</code> the item difficulty parameters are included in the plot.</p>
</td></tr>
<tr><td><code id="plot+2B20fscores_+3A_tol">tol</code></td>
<td>
<p>the tolerance used to group the item difficulty parameters, i.e., when <code>include.items = TRUE</code>
the values <code>round(betas / tol) * tol</code> are plotted, where <code>beta</code> is the numeric vector of
item difficulty parameters.</p>
</td></tr>
<tr><td><code id="plot+2B20fscores_+3A_xlab">xlab</code>, <code id="plot+2B20fscores_+3A_ylab">ylab</code>, <code id="plot+2B20fscores_+3A_main">main</code></td>
<td>
<p>character string or an <code><a href="base.html#topic+expression">expression</a></code>; see <code><a href="graphics.html#topic+title">title</a></code>.</p>
</td></tr>
<tr><td><code id="plot+2B20fscores_+3A_pch">pch</code>, <code id="plot+2B20fscores_+3A_cex">cex</code></td>
<td>
<p>arguments to <code>stripchart()</code>; used when <code>include.items = TRUE</code>.</p>
</td></tr>
<tr><td><code id="plot+2B20fscores_+3A_...">...</code></td>
<td>
<p>extra graphical parameters to be passed to <code>plot.density()</code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Dimitris Rizopoulos <a href="mailto:d.rizopoulos@erasmusmc.nl">d.rizopoulos@erasmusmc.nl</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+factor.scores">factor.scores</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Factor Scores for LSAT data:
fsc &lt;- factor.scores(rasch(LSAT))
plot(fsc, include.items = TRUE, main = "KDE for Person Parameters")
legend("left", "item parameters", pch = 16, cex = 1.5, bty = "n")

</code></pre>

<hr>
<h2 id='plot+20IRT'> Plot method for fitted IRT models</h2><span id='topic+plot.gpcm'></span><span id='topic+plot.grm'></span><span id='topic+plot.ltm'></span><span id='topic+plot.rasch'></span><span id='topic+plot.tpm'></span>

<h3>Description</h3>

<p>Produces the Item Characteristic or Item Information Curves for fitted IRT models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'gpcm'
plot(x, type = c("ICC", "IIC", "OCCu", "OCCl"), 
    items = NULL, category = NULL, zrange = c(-3.8, 3.8), 
    z = seq(zrange[1], zrange[2], length = 100), annot,
    labels = NULL, legend = FALSE, cx = "top", cy = NULL, ncol = 1, 
    bty = "n", col = palette(), lty = 1, pch, xlab, ylab, main, sub = NULL, 
    cex = par("cex"), cex.lab = par("cex.lab"), cex.main = par("cex.main"), 
    cex.sub = par("cex.sub"), cex.axis = par("cex.axis"), ask = TRUE,
    plot = TRUE, ...)

## S3 method for class 'grm'
plot(x, type = c("ICC", "IIC", "OCCu", "OCCl"), 
    items = NULL, category = NULL, zrange = c(-3.8, 3.8), 
    z = seq(zrange[1], zrange[2], length = 100), annot,
    labels = NULL, legend = FALSE,
    cx = "top", cy = NULL, ncol = 1, bty = "n", col = palette(), lty = 1, pch,
    xlab, ylab, main, sub = NULL, cex = par("cex"), cex.lab = par("cex.lab"),
    cex.main = par("cex.main"), cex.sub = par("cex.sub"), 
    cex.axis = par("cex.axis"), ask = TRUE, plot = TRUE, ...)

## S3 method for class 'ltm'
plot(x, type = c("ICC", "IIC", "loadings"), 
    items = NULL, zrange = c(-3.8, 3.8),
    z = seq(zrange[1], zrange[2], length = 100), annot,
    labels = NULL, legend = FALSE, cx = "topleft", cy = NULL, 
    ncol = 1, bty = "n", col = palette(), lty = 1, pch, xlab, 
    ylab, zlab, main, sub = NULL, cex = par("cex"),
    cex.lab = par("cex.lab"), cex.main = par("cex.main"), 
    cex.sub = par("cex.sub"), ask = TRUE,
    cex.axis = par("cex.axis"), plot = TRUE, ...)

## S3 method for class 'rasch'
plot(x, type = c("ICC", "IIC"), items = NULL, 
    zrange = c(-3.8, 3.8), z = seq(zrange[1], zrange[2], length = 100), 
    annot, labels = NULL, legend = FALSE, cx = "topleft", cy = NULL, 
    ncol = 1, bty = "n", col = palette(), lty = 1, pch, xlab, ylab, 
    main, sub = NULL, cex = par("cex"), cex.lab = par("cex.lab"),
    cex.main = par("cex.main"), cex.sub = par("cex.sub"), 
    cex.axis = par("cex.axis"), plot = TRUE, ...)

## S3 method for class 'tpm'
plot(x, type = c("ICC", "IIC"), items = NULL, 
    zrange = c(-3.8, 3.8), z = seq(zrange[1], zrange[2], length = 100), 
    annot, labels = NULL, legend = FALSE, cx = "topleft", cy = NULL, 
    ncol = 1, bty = "n", col = palette(), lty = 1, 
    pch, xlab, ylab, main, sub = NULL, cex = par("cex"), 
    cex.lab = par("cex.lab"), cex.main = par("cex.main"), 
    cex.sub = par("cex.sub"), cex.axis = par("cex.axis"), 
    plot = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot+2B20IRT_+3A_x">x</code></td>
<td>
<p> an object inheriting either from class <code>gpcm</code>, class <code>grm</code>, class <code>ltm</code>, class <code>rasch</code> or class <code>tpm</code>. </p>
</td></tr>
<tr><td><code id="plot+2B20IRT_+3A_type">type</code></td>
<td>
<p> the type of plot; &quot;ICC&quot; refers to Item Response Category Characteristic Curves whereas &quot;IIC&quot; to 
Item Information Curves. For <code>ltm</code> objects the option &quot;loadings&quot; is also available that produces
the scatter plot of the standardized loadings. For <code>grm</code> objects the options &quot;OCCu&quot; and &quot;OCCl&quot; are 
also available that produces the item operation characteristic curves. </p>
</td></tr>
<tr><td><code id="plot+2B20IRT_+3A_items">items</code></td>
<td>
<p> a numeric vector denoting which items to plot; if <code>NULL</code> all items are plotted; if 0 and
<code>type = "IIC"</code> the Test Information Curve is plotted. </p>
</td></tr>
<tr><td><code id="plot+2B20IRT_+3A_category">category</code></td>
<td>
<p> a scalar indicating the response category for which the curves should be plotted; if <code>NULL</code>
all categories are considered. This argument is only relevant for <code>grm</code> objects. </p>
</td></tr>
<tr><td><code id="plot+2B20IRT_+3A_zrange">zrange</code></td>
<td>
<p>a numeric vector of length 2 indicating the range for the latent variable values.</p>
</td></tr>
<tr><td><code id="plot+2B20IRT_+3A_z">z</code></td>
<td>
<p> a numeric vector denoting the values for the latent variable(s) values to be used in the plots. </p>
</td></tr>
<tr><td><code id="plot+2B20IRT_+3A_annot">annot</code></td>
<td>
<p>logical; if <code>TRUE</code> the plotted lines are annotated. </p>
</td></tr>
<tr><td><code id="plot+2B20IRT_+3A_labels">labels</code></td>
<td>
<p>character vector; the labels to use in either the annotation or legend. If <code>NULL</code> 
adequate labels are produced.</p>
</td></tr>
<tr><td><code id="plot+2B20IRT_+3A_legend">legend</code></td>
<td>
<p> logical; if <code>TRUE</code> a legend is printed. </p>
</td></tr>
<tr><td><code id="plot+2B20IRT_+3A_cx">cx</code>, <code id="plot+2B20IRT_+3A_cy">cy</code>, <code id="plot+2B20IRT_+3A_ncol">ncol</code>, <code id="plot+2B20IRT_+3A_bty">bty</code></td>
<td>
<p> arguments of <code><a href="graphics.html#topic+legend">legend</a></code>; <code>cx</code> and <code>cy</code> correspond to
the <code>x</code> and <code>y</code> arguments of <code><a href="graphics.html#topic+legend">legend</a></code>. </p>
</td></tr>
<tr><td><code id="plot+2B20IRT_+3A_col">col</code>, <code id="plot+2B20IRT_+3A_lty">lty</code>, <code id="plot+2B20IRT_+3A_pch">pch</code></td>
<td>
<p> control values, see <code><a href="graphics.html#topic+par">par</a></code>; recycling is used if necessary. </p>
</td></tr>
<tr><td><code id="plot+2B20IRT_+3A_xlab">xlab</code>, <code id="plot+2B20IRT_+3A_ylab">ylab</code>, <code id="plot+2B20IRT_+3A_zlab">zlab</code>, <code id="plot+2B20IRT_+3A_main">main</code>, <code id="plot+2B20IRT_+3A_sub">sub</code></td>
<td>
<p> character string or an <code><a href="base.html#topic+expression">expression</a></code>; see <code><a href="graphics.html#topic+title">title</a></code>. </p>
</td></tr>
<tr><td><code id="plot+2B20IRT_+3A_cex">cex</code>, <code id="plot+2B20IRT_+3A_cex.lab">cex.lab</code>, <code id="plot+2B20IRT_+3A_cex.main">cex.main</code>, <code id="plot+2B20IRT_+3A_cex.sub">cex.sub</code>, <code id="plot+2B20IRT_+3A_cex.axis">cex.axis</code>, <code id="plot+2B20IRT_+3A_ask">ask</code></td>
<td>
<p> the cex family of argument; see <code><a href="graphics.html#topic+par">par</a></code>. </p>
</td></tr>
<tr><td><code id="plot+2B20IRT_+3A_plot">plot</code></td>
<td>
<p>logical; if <code>TRUE</code> the plot(s) is(are) produced otherwise only the values used to create the plot(s)
are returned.</p>
</td></tr>
<tr><td><code id="plot+2B20IRT_+3A_...">...</code></td>
<td>
<p> extra graphical parameters to be passed to <code>plot()</code>, <code>lines()</code>, <code>legend()</code> and 
<code>text()</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Item response category characteristic curves show how the probability of responding in the <code class="reqn">k</code>th category, 
in each item, changes with the values of the latent variable (ability).
</p>
<p>The item information curves indicate the relative ability of an item to discriminate among contiguous trait scores
at various locations along the trait continuum. The test information curve, which is the sum of item information 
curves, provides a visual depiction of where along the trait continuum a test is most discriminating 
(Reise and Waller, 2002).
</p>


<h3>Value</h3>

<p>The values used to create the plot, i.e., the x-, y-coordinates. This is either a matrix or a list in which
the first column or element provides the latent variable values used, and the remaining columns or elements
correspond to either probabilities or information or loadings, depending on the value of the <code>type</code> argument.
</p>


<h3>Author(s)</h3>

<p>Dimitris Rizopoulos <a href="mailto:d.rizopoulos@erasmusmc.nl">d.rizopoulos@erasmusmc.nl</a>
</p>


<h3>References</h3>

<p>Reise, S. and Waller, N. (2002) Item response theory for dichotomous assessment data. In Drasgow, F. and Schmitt, N.,
editors, <em>Measuring and Analyzing Behavior in Organizations</em>. San Francisco: Jossey-Bass.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+information">information</a></code>,
<code><a href="#topic+gpcm">gpcm</a></code>,
<code><a href="#topic+grm">grm</a></code>,
<code><a href="#topic+ltm">ltm</a></code>,
<code><a href="#topic+rasch">rasch</a></code>,
<code><a href="#topic+tpm">tpm</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Examples for plot.grm()

fit &lt;- grm(Science[c(1,3,4,7)])

## Item Response Category Characteristic Curves for 
## the Science data
op &lt;- par(mfrow = c(2, 2))
plot(fit, lwd = 2, legend = TRUE, ncol = 2)
# re-set par()
par(op)

## Item Characteristic Curves for the 2nd category,
## and items 1 and 3
plot(fit, category = 2, items = c(1, 3), lwd = 2, legend = TRUE, cx = "right")

## Item Information Curves for the Science data;
plot(fit, type = "IIC", legend = TRUE, cx = "topright", lwd = 2, cex = 1.4)

## Test Information Function for the Science data;
plot(fit, type = "IIC", items = 0, lwd = 2)


###################################################


# Examples for plot.ltm()

## Item Characteristic Curves for the two-parameter logistic
## model; plot only items 1, 2, 4 and 6; take the range of the
## latent ability to be (-2.5, 2.5):
fit &lt;- ltm(WIRS ~ z1)
plot(fit, items = c(1, 2, 4, 6), zrange = c(-2.5, 2.5), lwd = 3, cex = 1.4)

## Test Information Function under the two-parameter logistic
## model for the Lsat data
fit &lt;- ltm(LSAT ~ z1)
plot(fit, type = "IIC", items = 0, lwd = 2, cex.lab = 1.2, cex.main = 1.3)
info &lt;- information(fit, c(-3, 0))
text(x = 2, y = 0.5, labels = paste("Total Information:", round(info$InfoTotal, 3), 
    "\n\nInformation in (-3, 0):", round(info$InfoRange, 3), 
    paste("(", round(100 * info$PropRange, 2), "%)", sep = "")), cex = 1.2)

## Item Characteristic Surfaces for the interaction model:
fit &lt;- ltm(WIRS ~ z1 * z2)
plot(fit, ticktype = "detailed", theta = 30, phi = 30, expand = 0.5, d = 2, 
     cex = 0.7, col = "lightblue")

###################################################


# Examples for plot.rasch()

## Item Characteristic Curves for the WIRS data;
## plot only items 1, 3 and 5:
fit &lt;- rasch(WIRS)
plot(fit, items = c(1, 3, 5), lwd = 3, cex = 1.4)
abline(v = -4:4, h = seq(0, 1, 0.2), col = "lightgray", lty = "dotted")

fit &lt;- rasch(LSAT)

## Item Characteristic Curves for the LSAT data;
## plot all items plus a legend and use only black:
plot(fit, legend = TRUE, cx = "right", lwd = 3, cex = 1.4, 
     cex.lab = 1.6, cex.main = 2, col = 1, lty = c(1, 1, 1, 2, 2),
     pch = c(16, 15, 17, 0, 1))
abline(v = -4:4, h = seq(0, 1, 0.2), col = "lightgray", lty = "dotted")

## Item Information Curves, for the first 3 items; include a legend
plot(fit, type = "IIC", items = 1:3, legend = TRUE, lwd = 2, cx = "topright")

## Test Information Function
plot(fit, type = "IIC", items = 0, lwd = 2, cex.lab = 1.1, 
     sub = paste("Call: ", deparse(fit$call)))

## Total information in (-2, 0) based on all the items
info.Tot &lt;- information(fit, c(-2, 0))$InfoRange
## Information in (-2, 0) based on items 2 and 4
info.24 &lt;- information(fit, c(-2, 0), items = c(2, 4))$InfoRange
text(x = 2, y = 0.5, labels = paste("Total Information in (-2, 0):", 
    round(info.Tot, 3), 
    "\n\nInformation in (-2, 0) based on\n Items 2 and 4:", round(info.24, 3), 
    paste("(", round(100 * info.24 / info.Tot, 2), "%)", sep = "")), 
    cex = 1.2)

## The Standard Error of Measurement can be plotted by
vals &lt;- plot(fit, type = "IIC", items = 0, plot = FALSE)
plot(vals[, "z"], 1 / sqrt(vals[, "info"]), type = "l", lwd = 2,
     xlab = "Ability", ylab = "Standard Error", 
     main = "Standard Error of Measurement")

###################################################


# Examples for plot.tpm()

## Compare the Item Characteristic Curves for the LSAT data,
## under the constraint Rasch model, the unconstraint Rasch model,
## and the three parameter model assuming equal discrimination
## across items
par(mfrow = c(2, 2))
pl1 &lt;- plot(rasch(LSAT, constr = cbind(length(LSAT) + 1, 1)))
text(2, 0.35, "Rasch model\nDiscrimination = 1")
pl2 &lt;- plot(rasch(LSAT))
text(2, 0.35, "Rasch model")
pl3 &lt;- plot(tpm(LSAT, type = "rasch", max.guessing = 1))
text(2, 0.35, "Rasch model\nwith Guessing parameter")

## Compare the Item Characteristic Curves for Item 4
## (you have to run the above first)
plot(range(pl1[, "z"]), c(0, 1), type = "n", xlab = "Ability", 
     ylab = "Probability", main = "Item Characteristic Curves - Item 4")
lines(pl1[, c("z", "Item 4")], lwd = 2, col = "black")
lines(pl2[, c("z", "Item 4")], lwd = 2, col = "red")
lines(pl3[, c("z", "Item 4")], lwd = 2, col = "blue")
legend("right", c("Rasch model Discrimination = 1", "Rasch model", 
       "Rasch model with\nGuessing parameter"), lwd = 2, col = c("black", 
       "red", "blue"), bty = "n")

</code></pre>

<hr>
<h2 id='rasch'> Rasch Model </h2><span id='topic+rasch'></span>

<h3>Description</h3>

<p>Fit the Rasch model under the Item Response Theory approach.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rasch(data, constraint = NULL, IRT.param = TRUE, start.val = NULL, 
    na.action = NULL, control = list(), Hessian = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rasch_+3A_data">data</code></td>
<td>
<p>a <code>data.frame</code> (that will be converted to a numeric matrix using 
<code>data.matrix()</code>) or a numeric <code>matrix</code> of manifest variables.</p>
</td></tr>
<tr><td><code id="rasch_+3A_constraint">constraint</code></td>
<td>
<p>a two-column numeric matrix with at most <code class="reqn">p</code> rows (where <code class="reqn">p</code> is the number of items), 
specifying fixed-value constraints. The first column represents the item (i.e., <code class="reqn">1</code> denotes the first item,
<code class="reqn">2</code> the second, etc., and <code class="reqn">p+1</code> the discrimination parameter) and the second column the 
value at which the corresponding parameter should be fixed. See <b>Examples</b> for more info.</p>
</td></tr>
<tr><td><code id="rasch_+3A_irt.param">IRT.param</code></td>
<td>
<p>logical; if <code>TRUE</code> then the coefficients' estimates are reported under the 
usual IRT parameterization. See <b>Details</b> for more info.</p>
</td></tr>
<tr><td><code id="rasch_+3A_start.val">start.val</code></td>
<td>
<p> the character string &quot;random&quot; or a numeric vector of <code class="reqn">p+1</code> starting values, 
where the first <code class="reqn">p</code> values correspond to the easiness parameters while the last value corresponds to the 
discrimination parameter. If &quot;random&quot;, random starting values are used. If <code>NULL</code> starting values
are automatically computed. </p>
</td></tr>
<tr><td><code id="rasch_+3A_na.action">na.action</code></td>
<td>
<p> the <code>na.action</code> to be used on <code>data</code>. In case of missing data, if 
<code>na.action = NULL</code> the model uses the available cases, i.e., it takes into account the observed 
part of sample units with missing values (valid under MAR mechanisms if the model is correctly specified). 
If you want to apply a complete case analysis then use <code>na.action = na.exclude</code>.</p>
</td></tr>
<tr><td><code id="rasch_+3A_control">control</code></td>
<td>
<p>a list of control values,
</p>

<dl>
<dt>iter.qN</dt><dd><p> the number of quasi-Newton iterations. Default 150.</p>
</dd>
<dt>GHk</dt><dd><p> the number of Gauss-Hermite quadrature points. Default 21.</p>
</dd>
<dt>method</dt><dd><p> the optimization method to be used in <code>optim()</code>. Default &quot;BFGS&quot;.</p>
</dd>
<dt>verbose</dt><dd><p> logical; if <code>TRUE</code> info about the optimization procedure are printed.</p>
</dd>
</dl>

</td></tr>
<tr><td><code id="rasch_+3A_hessian">Hessian</code></td>
<td>
<p>logical; if <code>TRUE</code>, then the Hessian matrix is computed. Warning: setting this argument to <code>FALSE</code>
will cause many methods (e.g., <code>summary()</code>) to fail; setting to <code>FALSE</code> is intended for simulation 
purposes in order <code>rasch()</code> to run faster.</p>
</td></tr>
</table>


<h3>Details</h3>

 
<p>The Rasch model is a special case of the unidimensional latent trait model when all the discrimination 
parameters are equal. This model was first discussed by Rasch (1960) and it is mainly used in educational 
testing where the aim is to study the abilities of a particular set of individuals.
</p>
<p>The model is defined as follows </p>
<p style="text-align: center;"><code class="reqn">\log\left(\frac{\pi_i}{1-\pi_i}\right) = \beta_{i} + \beta z,</code>
</p>
<p> where <code class="reqn">\pi_i</code> denotes the conditional probability of responding correctly 
to the <code class="reqn">i</code>th item given <code class="reqn">z</code>, <code class="reqn">\beta_{i}</code> is the easiness parameter for the <code class="reqn">i</code>th 
item, <code class="reqn">\beta</code> is the discrimination parameter (the same for all the items) and <code class="reqn">z</code> denotes the latent 
ability.
</p>
<p>If <code>IRT.param = TRUE</code>, then the parameters estimates are reported under the usual IRT parameterization,
i.e., </p>
<p style="text-align: center;"><code class="reqn">\log\left(\frac{\pi_i}{1-\pi_i}\right) = \beta (z - \beta_i^*).</code>
</p>

<p>The fit of the model is based on approximate marginal Maximum Likelihood, using the Gauss-Hermite quadrature rule 
for the approximation of the required integrals.
</p>


<h3>Value</h3>

<p> An object of class <code>rasch</code> with components,
</p>
<table>
<tr><td><code>coefficients</code></td>
<td>
<p>a matrix with the parameter values at convergence. These are always the estimates of 
<code class="reqn">\beta_i, \beta</code> parameters, even if <code>IRT.param = TRUE</code>.</p>
</td></tr>
<tr><td><code>log.Lik</code></td>
<td>
<p>the log-likelihood value at convergence.</p>
</td></tr>
<tr><td><code>convergence</code></td>
<td>
<p>the convergence identifier returned by <code>optim()</code>.</p>
</td></tr>
<tr><td><code>hessian</code></td>
<td>
<p>the approximate Hessian matrix at convergence returned by <code>optim()</code>.</p>
</td></tr>
<tr><td><code>counts</code></td>
<td>
<p>the number of function and gradient evaluations used by the quasi-Newton algorithm.</p>
</td></tr>
<tr><td><code>patterns</code></td>
<td>
<p>a list with two components: (i) <code>X</code>: a numeric matrix 
that contains the observed response patterns, and (ii) <code>obs</code>: a numeric vector that contains the observed 
frequencies for each observed response pattern.</p>
</td></tr>
<tr><td><code>GH</code></td>
<td>
<p>a list with two components used in the Gauss-Hermite rule: (i) <code>Z</code>: a numeric matrix that contains 
the abscissas, and (ii) <code>GHw</code>: a numeric vector that contains the corresponding  weights.</p>
</td></tr> 
<tr><td><code>max.sc</code></td>
<td>
<p>the maximum absolute value of the score vector at convergence.</p>
</td></tr>
<tr><td><code>constraint</code></td>
<td>
<p>the value of the <code>constraint</code> argument.</p>
</td></tr>
<tr><td><code>IRT.param</code></td>
<td>
<p>the value of the <code>IRT.param</code> argument.</p>
</td></tr>
<tr><td><code>X</code></td>
<td>
<p>a copy of the response data matrix.</p>
</td></tr>
<tr><td><code>control</code></td>
<td>
<p>the values used in the <code>control</code> argument.</p>
</td></tr>
<tr><td><code>na.action</code></td>
<td>
<p>the value of the <code>na.action</code> argument.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>the matched call.</p>
</td></tr>
</table>


<h3>Warning</h3>

 
<p>In case the Hessian matrix at convergence is not positive definite, try to re-fit the model using 
<code>rasch(..., start.val = "random")</code>.
</p>


<h3>Note</h3>

<p>Although the common formulation of the Rasch model assumes that the discrimination parameter is fixed to 1,
<code>rasch()</code> estimates it. If you wish to fit the constrained version of the model, use the <code>constraint</code> 
argument accordingly. See <b>Examples</b> for more info.
</p>
<p>The optimization algorithm works under the constraint that the discrimination parameter 
<code class="reqn">\beta</code> is always positive.
</p>
<p>When the coefficients' estimates are reported under the usual IRT parameterization (i.e., <code>IRT.param = TRUE</code>),
their standard errors are calculated using the Delta method.
</p>


<h3>Author(s)</h3>

<p>Dimitris Rizopoulos <a href="mailto:d.rizopoulos@erasmusmc.nl">d.rizopoulos@erasmusmc.nl</a>
</p>


<h3>References</h3>

<p>Baker, F. and Kim, S-H. (2004) <em>Item Response Theory</em>, 2nd ed. 
New York: Marcel Dekker.
</p>
<p>Rasch, G. (1960) <em>Probabilistic Models for Some 
Intelligence and  Attainment Tests</em>. Copenhagen: Paedagogiske 
Institute.
</p>
<p>Rizopoulos, D. (2006) <b>ltm</b>: An R package for latent variable modelling and item response theory analyses. 
<em>Journal of Statistical Software</em>, <b>17(5)</b>, 1&ndash;25. URL doi: <a href="https://doi.org/10.18637/jss.v017.i05">10.18637/jss.v017.i05</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+coef.rasch">coef.rasch</a></code>,
<code><a href="#topic+fitted.rasch">fitted.rasch</a></code>,
<code><a href="#topic+summary.rasch">summary.rasch</a></code>,
<code><a href="#topic+anova.rasch">anova.rasch</a></code>,
<code><a href="#topic+plot.rasch">plot.rasch</a></code>,
<code><a href="#topic+vcov.rasch">vcov.rasch</a></code>,
<code><a href="#topic+GoF.rasch">GoF.rasch</a></code>,
<code><a href="#topic+item.fit">item.fit</a></code>,
<code><a href="#topic+person.fit">person.fit</a></code>,  
<code><a href="#topic+margins">margins</a></code>,
<code><a href="#topic+factor.scores">factor.scores</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## The common form of the Rasch model for the 
## LSAT data, assuming that the discrimination
## parameter equals 1
rasch(LSAT, constraint = cbind(ncol(LSAT) + 1, 1))


## The Rasch model for the LSAT data under the 
## normal ogive; to do that fix the discrimination
## parameter to 1.702
rasch(LSAT, constraint = cbind(ncol(LSAT) + 1, 1.702))

## The Rasch model for the LSAT data with
## unconstraint discrimination parameter
rasch(LSAT)

## The Rasch model with (artificially created) 
## missing data
data &lt;- LSAT
data[] &lt;- lapply(data, function(x){
    x[sample(1:length(x), sample(15, 1))] &lt;- NA
    x
})
rasch(data)
</code></pre>

<hr>
<h2 id='rcor.test'>
Pairwise Associations between Items using a Correlation Coefficient
</h2><span id='topic+rcor.test'></span>

<h3>Description</h3>

<p>Computes and tests the pairwise associations between items using a correlation coefficient
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rcor.test(mat, p.adjust = FALSE, p.adjust.method = "holm", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rcor.test_+3A_mat">mat</code></td>
<td>
<p> a numeric <code>matrix</code> or a numeric <code>data.frame</code> containing the manifest variables as columns. </p>
</td></tr>
<tr><td><code id="rcor.test_+3A_p.adjust">p.adjust</code></td>
<td>
<p>logical; if <code>TRUE</code> the <code class="reqn">p</code>-values are adjusted for multiple comparisons.</p>
</td></tr>
<tr><td><code id="rcor.test_+3A_p.adjust.method">p.adjust.method</code></td>
<td>
<p>the <code>method</code> argument of <code>p.adjust()</code>.</p>
</td></tr>
<tr><td><code id="rcor.test_+3A_...">...</code></td>
<td>
<p>extra arguments passed to <code>cor()</code> and <code>cor.test()</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <code>rcor.test</code> with components,
</p>
<table>
<tr><td><code>cor.mat</code></td>
<td>
<p>the correlation matrix.</p>
</td></tr>
<tr><td><code>p.values</code></td>
<td>
<p>a three column numeric matrix containing the <code class="reqn">p</code>-values for all the combinations of items.</p>
</td></tr>
</table>
<p>The print method for class <code>rcor.test</code> returns a square matrix in which the upper diagonal part contains
the estimates of the correlation coefficients, and the lower diagonal part contains the corresponding <code class="reqn">p</code>-values.
</p>


<h3>Note</h3>

<p><code>rcor.test()</code> is more appropriate for informal testing of association between polytomous items.
</p>


<h3>Author(s)</h3>

<p>Dimitris Rizopoulos <a href="mailto:d.rizopoulos@erasmusmc.nl">d.rizopoulos@erasmusmc.nl</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## pairwise associations for Environment data:
rcor.test(data.matrix(Environment), method = "kendall")

## pairwise associations for independent normal random variates:
mat &lt;- matrix(rnorm(1000), 100, 10, dimnames = list(NULL, LETTERS[1:10]))
rcor.test(mat)
rcor.test(mat, method = "kendall")
rcor.test(mat, method = "spearman")

</code></pre>

<hr>
<h2 id='residuals'> Residuals for IRT models </h2><span id='topic+residuals.gpcm'></span><span id='topic+residuals.grm'></span><span id='topic+residuals.ltm'></span><span id='topic+residuals.rasch'></span><span id='topic+residuals.tpm'></span>

<h3>Description</h3>

<p>Computes the residuals for vectors of response patterns.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'gpcm'
residuals(object, resp.patterns = NULL, order = TRUE, ...)

## S3 method for class 'grm'
residuals(object, resp.patterns = NULL, order = TRUE, ...)

## S3 method for class 'ltm'
residuals(object, resp.patterns = NULL, order = TRUE, ...)

## S3 method for class 'rasch'
residuals(object, resp.patterns = NULL, order = TRUE, ...)

## S3 method for class 'tpm'
residuals(object, resp.patterns = NULL, order = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="residuals_+3A_object">object</code></td>
<td>
<p>an object inheriting either from class <code>gpcm</code>, class <code>grm</code>, class <code>ltm</code>, class <code>rasch</code> or class 
<code>tpm</code>.</p>
</td></tr>
<tr><td><code id="residuals_+3A_resp.patterns">resp.patterns</code></td>
<td>
<p>a <code>matrix</code> or a <code>data.frame</code> of response patterns with columns denoting the 
items; if <code>NULL</code> the expected frequencies are computed for the observed response patterns.</p>
</td></tr>
<tr><td><code id="residuals_+3A_order">order</code></td>
<td>
<p>logical; if <code>TRUE</code> the response patterns are sorted according to the residual estimates.</p>
</td></tr>
<tr><td><code id="residuals_+3A_...">...</code></td>
<td>
<p>additional arguments; currently none is used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The following residuals are computed: </p>
<p style="text-align: center;"><code class="reqn">\frac{O_i - E_i}{\sqrt{E_i}},</code>
</p>
<p> where
<code class="reqn">O_i</code> and <code class="reqn">E_i</code> denote the observed and expected frequencies for the <code class="reqn">i</code>th response pattern.
</p>


<h3>Value</h3>

<p>A numeric <code>matrix</code> containing the observed and expected frequencies as well as the residual value for
each response pattern.
</p>


<h3>Author(s)</h3>

<p>Dimitris Rizopoulos <a href="mailto:d.rizopoulos@erasmusmc.nl">d.rizopoulos@erasmusmc.nl</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+fitted.gpcm">fitted.gpcm</a></code>,
<code><a href="#topic+fitted.grm">fitted.grm</a></code>,
<code><a href="#topic+fitted.ltm">fitted.ltm</a></code>,
<code><a href="#topic+fitted.rasch">fitted.rasch</a></code>,
<code><a href="#topic+fitted.tpm">fitted.tpm</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
fit &lt;- ltm(LSAT ~ z1)
residuals(fit)
residuals(fit, order = FALSE)

</code></pre>

<hr>
<h2 id='rmvlogis'>
Generate Random Responses Patterns under Dichotomous and Polytomous IRT models
</h2><span id='topic+rmvlogis'></span><span id='topic+rmvordlogis'></span>

<h3>Description</h3>

<p>Produces Bernoulli or Multinomial random variates under the Rasch, the two-parameter logistic, the three parameter, 
the graded response, and the generalized partial credit models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rmvlogis(n, thetas, IRT = TRUE, link = c("logit", "probit"), 
         distr = c("normal", "logistic", "log-normal", "uniform"), 
         z.vals = NULL)

rmvordlogis(n, thetas, IRT = TRUE, model = c("gpcm", "grm"), 
    link = c("logit", "probit"), 
    distr = c("normal", "logistic", "log-normal", "uniform"), 
    z.vals = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rmvlogis_+3A_n">n</code></td>
<td>
<p>a scalar indicating the number of response patterns to simulate.</p>
</td></tr>
<tr><td><code id="rmvlogis_+3A_thetas">thetas</code></td>
<td>
<p>for <code>rmvlogis()</code> a numeric matrix with rows representing the items and columns the parameters.
For <code>rmvordlogis()</code> a list with numeric vector elements, with first the threshold parameters and last the discrimination
parameter. See <b>Details</b> for more info.</p>
</td></tr>
<tr><td><code id="rmvlogis_+3A_irt">IRT</code></td>
<td>
<p> logical; if <code>TRUE</code> <code>thetas</code> are under the IRT parameterization. 
See <b>Details</b> for more info.</p>
</td></tr>
<tr><td><code id="rmvlogis_+3A_model">model</code></td>
<td>
<p>from which model to simulate.</p>
</td></tr>
<tr><td><code id="rmvlogis_+3A_link">link</code></td>
<td>
<p> a character string indicating the link function to use. Options are logit and probit. </p>
</td></tr>
<tr><td><code id="rmvlogis_+3A_distr">distr</code></td>
<td>
<p> a character string indicating the distribution of the latent variable. Options are Normal, Logistic, 
log-Normal, and Uniform. </p>
</td></tr>
<tr><td><code id="rmvlogis_+3A_z.vals">z.vals</code></td>
<td>
<p> a numeric vector of length <code>n</code> providing the values of the latent variable (ability) to be used 
in the simulation of the dichotomous responses; if specified the value of <code>distr</code> is ignored.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The binary variates can be simulated under the following parameterizations for the probability of correctly responding in 
the <code class="reqn">i</code>th item. If <code>IRT = TRUE</code> </p>
<p style="text-align: center;"><code class="reqn">\pi_i = c_i + (1 - c_i) g(\beta_{2i} (z - \beta_{1i})),</code>
</p>
<p> whereas if <code>IRT = FALSE</code>  </p>
<p style="text-align: center;"><code class="reqn">\pi_i = c_i + (1 - c_i) g(\beta_{1i} + 
\beta_{2i} z),</code>
</p>
 <p><code class="reqn">z</code> denotes the latent variable, 
<code class="reqn">\beta_{1i}</code> and <code class="reqn">\beta_{2i}</code> are the first and second columns of <code>thetas</code>, respectively, and <code class="reqn">g()</code> 
is the link function. If <code>thetas</code> is a three-column matrix then the third column should contain the guessing 
parameters <code class="reqn">c_i</code>'s.
</p>
<p>The ordinal variates are simulated according to the generalized partial credit model or the graded response model depending
on the value of the <code>model</code> argument. Check <code><a href="#topic+gpcm">gpcm</a></code> and <code><a href="#topic+grm">grm</a></code> to see how these models are defined,
under both parameterizations.
</p>


<h3>Value</h3>

<p>a numeric matrix with <code>n</code> rows and columns the number of items, containing the simulated binary or ordinal variates.
</p>


<h3>Note</h3>

<p>For options <code>distr = "logistic"</code>, <code>distr = "log-normal"</code> and <code>distr = "uniform"</code> the simulated random 
variates for <code class="reqn">z</code> simulated under the Logistic distribution with <code>location = 0</code> and <code>scale = 1</code>, the 
log-Normal distribution with <code>meanlog = 0</code> and <code>sdlog = 1</code> and the Uniform distribution with <code>min = -3.5</code>
and <code>max = 3.5</code>, respectively. Then, the simulated <code class="reqn">z</code> variates are standardized, using the theoretical mean 
and variance of the Logistic, log-Normal and Uniform distribution, respectively.
</p>


<h3>Author(s)</h3>

<p>Dimitris Rizopoulos <a href="mailto:d.rizopoulos@erasmusmc.nl">d.rizopoulos@erasmusmc.nl</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+gpcm">gpcm</a></code>,
<code><a href="#topic+grm">grm</a></code>,
<code><a href="#topic+ltm">ltm</a></code>,
<code><a href="#topic+rasch">rasch</a></code>,
<code><a href="#topic+tpm">tpm</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# 10 response patterns under a Rasch model
# with 5 items
rmvlogis(10, cbind(seq(-2, 2, 1), 1))

# 10 response patterns under a GPCM model
# with 5 items, with 3 categories each
thetas &lt;- lapply(1:5, function(u) c(seq(-1, 1, len = 2), 1.2))
rmvordlogis(10, thetas)

</code></pre>

<hr>
<h2 id='Science'>Attitude to Science and Technology</h2><span id='topic+Science'></span>

<h3>Description</h3>

<p>This data set comes from the Consumer Protection and Perceptions of Science and Technology section of the 
1992 Euro-Barometer Survey (Karlheinz and Melich, 1992) based on a sample from Great Britain. The questions 
asked are given below:
</p>


<h3>Format</h3>

<p>All of the below items were measured on a four-group scale with response categories &quot;strongly disagree&quot;, 
&quot;disagree to some extent&quot;, &quot;agree to some extent&quot; and &quot;strongly agree&quot;:
</p>

<dl>
<dt>Comfort</dt><dd><p>Science and technology are making our lives healthier, easier and more comfortable.</p>
</dd>
<dt>Environment</dt><dd><p>Scientific and technological research cannot play an important role in protecting
the environment and repairing it.</p>
</dd>
<dt>Work</dt><dd><p>The application of science and new technology will make work more interesting.</p>
</dd>
<dt>Future</dt><dd><p>Thanks to science and technology, there will be more opportunities for the future generations.</p>
</dd>
<dt>Technology</dt><dd><p>New technology does not depend on basic scientific research.</p>
</dd>
<dt>Industry</dt><dd><p>Scientific and technological research do not play an important role in industrial development.</p>
</dd>
<dt>Benefit</dt><dd><p>The benefits of science are greater than any harmful effect it may have.</p>
</dd>
</dl>



<h3>References</h3>

<p>Bartholomew, D., Steel, F., Moustaki, I. and Galbraith, J. (2002) <em>The Analysis and Interpretation of 
Multivariate Data for Social Scientists</em>. London: Chapman and Hall.
</p>
<p>Karlheinz, R. and Melich, A. (1992) Euro-Barometer 38.1: <em>Consumer Protection and Perceptions of Science 
and Technology</em>. INRA (Europe), Brussels. [computer file]
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Descriptive statistics for Science data
descript(Science)

</code></pre>

<hr>
<h2 id='summary'> Summary method for fitted IRT models </h2><span id='topic+summary.gpcm'></span><span id='topic+summary.grm'></span><span id='topic+summary.ltm'></span><span id='topic+summary.rasch'></span><span id='topic+summary.tpm'></span>

<h3>Description</h3>

<p>Summarizes the fit of either <code>grm</code>, <code>ltm</code>, <code>rasch</code> or <code>tpm</code> objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'gpcm'
summary(object, robust.se = FALSE, ...)

## S3 method for class 'grm'
summary(object, ...)

## S3 method for class 'ltm'
summary(object, robust.se = FALSE, ...)

## S3 method for class 'rasch'
summary(object, robust.se = FALSE, ...)

## S3 method for class 'tpm'
summary(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary_+3A_object">object</code></td>
<td>
<p> an object inheriting from either class <code>gpcm</code>, either class <code>grm</code>, class <code>ltm</code>, 
class <code>rasch</code> or class <code>tpm</code>.</p>
</td></tr>
<tr><td><code id="summary_+3A_robust.se">robust.se</code></td>
<td>
<p>logical; if <code>TRUE</code> robust estimation of standard errors is used, based on the sandwich estimator.</p>
</td></tr>
<tr><td><code id="summary_+3A_...">...</code></td>
<td>
<p> additional argument; currently none is used. </p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of either class <code>summ.gpcm</code>, class <code>summ.grm</code>, class <code>summ.ltm</code> or class <code>summ.rasch</code> with components,
</p>
<table>
<tr><td><code>coefficients</code></td>
<td>
<p>the estimated coefficients' table.</p>
</td></tr>
<tr><td><code>Var.betas</code></td>
<td>
<p>the approximate covariance matrix for the estimated parameters; returned only in <code>summ.ltm</code> 
and <code>summ.rasch</code>.</p>
</td></tr>
<tr><td><code>logLik</code></td>
<td>
<p>the log-likelihood of <code>object</code>.</p>
</td></tr>
<tr><td><code>AIC</code></td>
<td>
<p>the AIC for <code>object</code>.</p>
</td></tr>
<tr><td><code>BIC</code></td>
<td>
<p>the BIC for <code>object</code>.</p>
</td></tr>
<tr><td><code>max.sc</code></td>
<td>
<p>the maximum absolute value of the score vector at convergence.</p>
</td></tr>
<tr><td><code>conv</code></td>
<td>
<p>the convergence identifier returned by <code>optim()</code>.</p>
</td></tr>
<tr><td><code>counts</code></td>
<td>
<p>the <code>counts</code> argument returned by <code>optim()</code>.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>the matched call of <code>object</code>.</p>
</td></tr>
<tr><td><code>ltn.struct</code></td>
<td>
<p>a character vector describing the latent structure used in <code>object</code>; returned only in 
<code>summ.ltm</code>.</p>
</td></tr>
<tr><td><code>control</code></td>
<td>
<p>the values used in the <code>control</code> argument in the fit of <code>object</code>.</p>
</td></tr>
<tr><td><code>nitems</code></td>
<td>
<p>the number of items in the data set; returned only in <code>summ.ltm</code> and <code>summ.rasch</code>.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>For the parameters that have been constrained, the standard errors and <code class="reqn">z</code>-values are printed as <code>NA</code>.
</p>
<p>When the coefficients' estimates are reported under the usual IRT parameterization (i.e., <code>IRT.param = TRUE</code>
in the call of either <code>grm</code>, <code>ltm</code> or <code>rasch</code>), their standard errors are calculated using the 
Delta method.
</p>


<h3>Author(s)</h3>

<p>Dimitris Rizopoulos <a href="mailto:d.rizopoulos@erasmusmc.nl">d.rizopoulos@erasmusmc.nl</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+gpcm">gpcm</a></code>,
<code><a href="#topic+grm">grm</a></code>,
<code><a href="#topic+ltm">ltm</a></code>,
<code><a href="#topic+rasch">rasch</a></code>,
<code><a href="#topic+tpm">tpm</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# use Hessian = TRUE if you want standard errors
fit &lt;- grm(Science[c(1,3,4,7)], Hessian = TRUE)
summary(fit)

## One factor model using the WIRS data;
## results are reported under the IRT
## parameterization
fit &lt;- ltm(WIRS ~ z1)
summary(fit)

</code></pre>

<hr>
<h2 id='testEquatingData'>
Prepares Data for Test Equating
</h2><span id='topic+testEquatingData'></span>

<h3>Description</h3>

<p>Test equating by common items.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>testEquatingData(DataList, AnchoringItems = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="testEquatingData_+3A_datalist">DataList</code></td>
<td>
<p> a list of <code>data.frame</code>s or <code>matrice</code>s containing common and unique items between 
several forms.</p>
</td></tr>
<tr><td><code id="testEquatingData_+3A_anchoringitems">AnchoringItems</code></td>
<td>
<p> a <code>data.frame</code> or a <code>matrix</code> containing anchoring items for across sample equating.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The purpose of this function is to combine items from different forms. Two cases are considered. Alternate 
Form Equating (where common and unique items are analyzed simultaneously) and Across Sample Equating (where different
sets of unique items are analyzed separately based on previously calibrated anchor items).
</p>


<h3>Value</h3>

<p>A <code>matrix</code> containing the common and unique items.
</p>


<h3>Author(s)</h3>

<p>Dimitris Rizopoulos <a href="mailto:d.rizopoulos@erasmusmc.nl">d.rizopoulos@erasmusmc.nl</a>
</p>


<h3>References</h3>

<p>Yu, C.-H. and Osborn Popp, S. (2005) Test equating by common items and common subjects: concepts and applications.
<em>Practical Assessment Research and Evaluation</em>, <b>10(4)</b>, 1&ndash;19.
</p>
<p>Rizopoulos, D. (2006) <b>ltm</b>: An R package for latent variable modelling and item response theory analyses. 
<em>Journal of Statistical Software</em>, <b>17(5)</b>, 1&ndash;25. URL doi: <a href="https://doi.org/10.18637/jss.v017.i05">10.18637/jss.v017.i05</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Let two data-sets with common and unique items
dat1 &lt;- as.data.frame(rmvlogis(20, cbind(c(-2, 1, 2, 1), 1)))
names(dat1) &lt;- c("CIt2", "CIt3", "CIt4", "W")

dat2 &lt;- as.data.frame(rmvlogis(10, cbind(c(-2, -1, 1, 2, 0.95), 1)))
names(dat2) &lt;- c("CIt1", "CIt2", "CIt3", "CIt4", "K")

# combine in one data-set by
lisForms &lt;- list(dat1, dat2)
testEquatingData(lisForms)

</code></pre>

<hr>
<h2 id='tpm'> Birnbaum's Three Parameter Model </h2><span id='topic+tpm'></span>

<h3>Description</h3>

<p>Fit Birnbaum's three parameter model under the Item Response Theory approach.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tpm(data, type = c("latent.trait", "rasch"), constraint = NULL, 
    max.guessing = 1, IRT.param = TRUE, start.val = NULL, 
    na.action = NULL, control = list())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tpm_+3A_data">data</code></td>
<td>
<p>a <code>data.frame</code> (that will be converted to a numeric matrix using 
<code>data.matrix()</code>) or a numeric <code>matrix</code> of manifest variables.</p>
</td></tr>
<tr><td><code id="tpm_+3A_type">type</code></td>
<td>
<p>a character string indicating the type of model to fit. Available options are &lsquo;rasch&rsquo; that
assumes equal discrimination parameter among items, and &lsquo;latent.trait&rsquo; (default) that assumes a different 
discrimination parameter per item.</p>
</td></tr>
<tr><td><code id="tpm_+3A_constraint">constraint</code></td>
<td>
<p>a three-column numeric matrix specifying fixed-value constraints. The first column represents 
the item (i.e., <code class="reqn">1</code> denotes the first item, <code class="reqn">2</code> the second, etc.); the second column denotes the type 
of parameter to fix for the item specified in the first column (i.e., <code class="reqn">1</code> denotes the guessing parameters,
<code class="reqn">2</code> the easiness parameters, and <code class="reqn">3</code> the discrimination parameters); the third column specifies the value
at which the corresponding parameter should be fixed. See <b>Examples</b> for more info.</p>
</td></tr>
<tr><td><code id="tpm_+3A_max.guessing">max.guessing</code></td>
<td>
<p>a scalar between 0 and 1 denoting the upper bound for the guessing parameters.</p>
</td></tr>
<tr><td><code id="tpm_+3A_irt.param">IRT.param</code></td>
<td>
<p>logical; if <code>TRUE</code> then the coefficients' estimates are reported under the 
usual IRT parameterization. See <b>Details</b> for more info.</p>
</td></tr>
<tr><td><code id="tpm_+3A_start.val">start.val</code></td>
<td>
<p>the character string &quot;random&quot; or a numeric matrix supplying starting values with <code class="reqn">p</code> rows and 
3 columns, with <code class="reqn">p</code> denoting the number of items. If <code>NULL</code> starting values are automatically computed. 
If &quot;random&quot;, random starting values are used. If a matrix, then the first column should contain the guessing 
parameter, the second <code class="reqn">\beta_{1i}</code>, and the third <code class="reqn">\beta_{2i}</code> (see <b>Details</b>). 
If <code>type == "rasch"</code>, then the third should contain the same number <code class="reqn">p</code> times.</p>
</td></tr>
<tr><td><code id="tpm_+3A_na.action">na.action</code></td>
<td>
<p> the <code>na.action</code> to be used on <code>data</code>. In case of missing data, if 
<code>na.action = NULL</code> the model uses the available cases, i.e., it takes into account the observed 
part of sample units with missing values (valid under MAR mechanisms if the model is correctly specified). 
If you want to apply a complete case analysis then use <code>na.action = na.exclude</code>.</p>
</td></tr>
<tr><td><code id="tpm_+3A_control">control</code></td>
<td>
<p>a list of control values with elements,
</p>

<dl>
<dt>optimizer</dt><dd><p>a character string denoting the optimizer to use, either <code>"optim"</code> (default) 
or <code>"nlminb"</code>.</p>
</dd>
<dt>iter.qN</dt><dd><p> scalar denoting the number of iterations in the optimization procedure. For <code>optim()</code>
this is passed to the control argument &lsquo;maxit&rsquo;, whereas for <code>nlminb()</code> this is passed
to both control arguments &lsquo;iter.max&rsquo; and &lsquo;eval.max&rsquo;. Default 1000.</p>
</dd>
<dt>GHk</dt><dd><p> scalar denoting the number of Gauss-Hermite quadrature points. Default 21.</p>
</dd>
<dt>method</dt><dd><p> a character string denoting the optimization method to be used in <code>optim()</code>. Default &quot;BFGS&quot;.</p>
</dd>
<dt>verbose</dt><dd><p> logical; if <code>TRUE</code> info about the optimization procedure are printed.</p>
</dd>
<dt>eps.hessian</dt><dd><p>the step-length to use in the central difference approximation that approximates the hessian.
Default is <code>1e-03</code>.</p>
</dd>
<dt>parscale</dt><dd><p>a scaling numeric vector of length equal to the parameters to be estimated (taking into account 
any constraints). This is passed to either to the &lsquo;parscale&rsquo; control argument of <code>optim()</code> 
or to the &lsquo;scale&rsquo; argument of <code>nlminb()</code>. Default is 0.5 for the guessing parameters and 1 
for the discrimination and easiness parameters.</p>
</dd>
</dl>

</td></tr>
</table>


<h3>Details</h3>

 
<p>Birnbaum's three parameter model is usually employed to handle the phenomenon of non-random guessing in the case
of difficult items.  
</p>
<p>The model is defined as follows </p>
<p style="text-align: center;"><code class="reqn">\pi_i = c_i + (1 - c_i) \frac{\exp(\beta_{1i} + \beta_{2i} z)}{1 + 
    \exp(\beta_{1i} + \beta_{2i} z)},</code>
</p>
<p> where 
<code class="reqn">\pi_i</code> denotes the conditional probability of responding correctly to the <code class="reqn">i</code>th item given <code class="reqn">z</code>, 
<code class="reqn">c_i</code> denotes the guessing parameter, <code class="reqn">\beta_{1i}</code> is the easiness parameter, 
<code class="reqn">\beta_{2i}</code> is the discrimination parameter, and <code class="reqn">z</code> denotes the 
latent ability. In case <code>type = "rasch"</code>, <code class="reqn">\beta_{2i}</code> is assumed equal for all items.
</p>
<p>If <code>IRT.param = TRUE</code>, then the parameters estimates are reported under the usual IRT parameterization,
i.e., </p>
<p style="text-align: center;"><code class="reqn">\pi_i = c_i + (1 - c_i) \frac{\exp[\beta_{2i} (z - \beta_{1i}^*)]}{1 + 
    \exp[\beta_{2i} (z - \beta_{1i}^*)]}.</code>
</p>

<p>The fit of the model is based on approximate marginal Maximum Likelihood, using the Gauss-Hermite quadrature rule 
for the approximation of the required integrals.
</p>


<h3>Value</h3>

<p> An object of class <code>tpm</code> with components,
</p>
<table>
<tr><td><code>coefficients</code></td>
<td>
<p>a matrix with the parameter values at convergence. These are always the estimates of 
<code class="reqn">\beta_i, \beta</code> parameters, even if <code>IRT.param = TRUE</code>.</p>
</td></tr>
<tr><td><code>log.Lik</code></td>
<td>
<p>the log-likelihood value at convergence.</p>
</td></tr>
<tr><td><code>convergence</code></td>
<td>
<p>the convergence identifier returned by <code>optim()</code>.</p>
</td></tr>
<tr><td><code>hessian</code></td>
<td>
<p>the approximate Hessian matrix at convergence obtained using a central difference approximation.</p>
</td></tr>
<tr><td><code>counts</code></td>
<td>
<p>the number of function and gradient evaluations used by the optimization algorithm.</p>
</td></tr>
<tr><td><code>patterns</code></td>
<td>
<p>a list with two components: (i) <code>X</code>: a numeric matrix 
that contains the observed response patterns, and (ii) <code>obs</code>: a numeric vector that contains the observed 
frequencies for each observed response pattern.</p>
</td></tr>
<tr><td><code>GH</code></td>
<td>
<p>a list with two components used in the Gauss-Hermite rule: (i) <code>Z</code>: a numeric matrix that contains 
the abscissas, and (ii) <code>GHw</code>: a numeric vector that contains the corresponding  weights.</p>
</td></tr> 
<tr><td><code>max.sc</code></td>
<td>
<p>the maximum absolute value of the score vector at convergence.</p>
</td></tr>
<tr><td><code>type</code></td>
<td>
<p>the value of the <code>type</code> argument.</p>
</td></tr>
<tr><td><code>constraint</code></td>
<td>
<p>the value of the <code>constraint</code> argument.</p>
</td></tr>
<tr><td><code>max.guessing</code></td>
<td>
<p>the value of the <code>max.guessing</code> argument.</p>
</td></tr>
<tr><td><code>IRT.param</code></td>
<td>
<p>the value of the <code>IRT.param</code> argument.</p>
</td></tr>
<tr><td><code>X</code></td>
<td>
<p>a copy of the response data matrix.</p>
</td></tr>
<tr><td><code>control</code></td>
<td>
<p>the values used in the <code>control</code> argument.</p>
</td></tr>
<tr><td><code>na.action</code></td>
<td>
<p>the value of the <code>na.action</code> argument.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>the matched call.</p>
</td></tr>
</table>


<h3>Warning</h3>

 
<p>The three parameter model is known to have numerical problems like non-convergence or convergence on the boundary,
especially for the guessing parameters. These problems usually result in a zero estimate for some guessing 
parameters and/or in a non positive definite Hessian matrix or in a high absolute value for the score vector 
(returned by the <code>summary</code> method) at convergence. In case of estimates on the boundary, the <code>constraint</code> 
argument can be used to set the guessing parameter(s) for the problematic item(s) to zero. In addition, 
<code>tpm()</code> has a number of control parameters that can be tuned in order to obtain successful convergence; 
the most important of these are the starting values, the parameter scaling vector and the optimizer.
</p>


<h3>Author(s)</h3>

<p>Dimitris Rizopoulos <a href="mailto:d.rizopoulos@erasmusmc.nl">d.rizopoulos@erasmusmc.nl</a>
</p>


<h3>References</h3>

<p>Baker, F. and Kim, S-H. (2004) <em>Item Response Theory</em>, 2nd ed. 
New York: Marcel Dekker.
</p>
<p>Birnbaum, A. (1968). Some latent trait models and their use in inferring an examinee's ability. In F. M. Lord and
M. R. Novick (Eds.), <em>Statistical Theories of Mental Test Scores</em>, 397&ndash;479. Reading, MA: Addison-Wesley.
</p>
<p>Rizopoulos, D. (2006) <b>ltm</b>: An R package for latent variable modelling and item response theory analyses. 
<em>Journal of Statistical Software</em>, <b>17(5)</b>, 1&ndash;25. URL doi: <a href="https://doi.org/10.18637/jss.v017.i05">10.18637/jss.v017.i05</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+coef.tpm">coef.tpm</a></code>,
<code><a href="#topic+fitted.tpm">fitted.tpm</a></code>,
<code><a href="#topic+summary.tpm">summary.tpm</a></code>,
<code><a href="#topic+anova.tpm">anova.tpm</a></code>,
<code><a href="#topic+plot.tpm">plot.tpm</a></code>,
<code><a href="#topic+vcov.tpm">vcov.tpm</a></code>,
<code><a href="#topic+item.fit">item.fit</a></code>,
<code><a href="#topic+person.fit">person.fit</a></code>,  
<code><a href="#topic+margins">margins</a></code>,
<code><a href="#topic+factor.scores">factor.scores</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# the three parameter model
tpm(LSAT)

# use 'nlminb' as optimizer
tpm(LSAT, control = list(optimizer = "nlminb"))


# the three parameter model with equal 
# discrimination parameter across items
# fix the guessing parameter for the third item to zero
tpm(LSAT, type = "rasch", constraint = cbind(3, 1, 0))


# the three parameter model for the Abortion data
fit &lt;- tpm(Abortion)
fit

# the guessing parameter estimates for items 1, 3, and 4 seem to be on
# the boundary; update the fit by fixing them to zero
update(fit, constraint = cbind(c(1, 3, 4), 1, 0))

</code></pre>

<hr>
<h2 id='unidimTest'> Unidimensionality Check using Modified Parallel Analysis </h2><span id='topic+unidimTest'></span>

<h3>Description</h3>

<p>An empirical check for the unidimensionality assumption for <code>ltm</code>, <code>rasch</code> and <code>tpm</code> models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>unidimTest(object, data, thetas, IRT = TRUE, z.vals = NULL, 
           B = 100, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="unidimTest_+3A_object">object</code></td>
<td>
<p>a model object inheriting either from class <code>ltm</code>, class <code>rasch</code> or class <code>tpm</code>. For
<code>ltm()</code> it is assumed that the two-parameter logistic model has been fitted (i.e., one latent variable and
no nonlinear terms); see <b>Note</b> for an extra option.</p>
</td></tr>
<tr><td><code id="unidimTest_+3A_data">data</code></td>
<td>
<p>a <code>matrix</code> or a <code>data.frame</code> of response patterns with columns denoting the items; used 
if <code>object</code> is missing.</p>
</td></tr>
<tr><td><code id="unidimTest_+3A_thetas">thetas</code></td>
<td>
<p>a numeric <code>matrix</code> with IRT model parameter values to be used in <code><a href="#topic+rmvlogis">rmvlogis</a></code>; used if 
<code>object</code> is missing.</p>
</td></tr>
<tr><td><code id="unidimTest_+3A_irt">IRT</code></td>
<td>
<p>logical, if <code>TRUE</code>, then argument <code>thetas</code> contains the measurement model parameters under the 
usual IRT parameterization (see <code><a href="#topic+rmvlogis">rmvlogis</a></code>); used if <code>object</code> is missing.</p>
</td></tr>
<tr><td><code id="unidimTest_+3A_z.vals">z.vals</code></td>
<td>
<p>a numeric vector of length equal to the number of rows of <code>data</code>, providing ability estimates.
If <code>object</code> is supplied then the abilities are estimated using <code><a href="#topic+factor.scores">factor.scores</a></code>. If <code>NULL</code>, 
the abilities are simulated from a standard normal distribution.</p>
</td></tr>
<tr><td><code id="unidimTest_+3A_b">B</code></td>
<td>
<p>the number of samples for the Monte Carlo procedure to approximate the distribution of the statistic under 
the null hypothesis.</p>
</td></tr>
<tr><td><code id="unidimTest_+3A_...">...</code></td>
<td>
<p>extra arguments to <code>polycor()</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function implements the procedure proposed by Drasgow and Lissak (1983) for examining the latent dimensionality
of dichotomously scored item responses. The statistic used for testing unidimensionality is the second eigenvalue of
the tetrachoric correlations matrix of the dichotomous items. The tetrachoric correlations between are computed 
using function <code>polycor()</code> from package &lsquo;polycor&rsquo;, and the largest one is taken as communality estimate.
</p>
<p>A Monte Carlo procedure is used to approximate the distribution of this statistic under the null hypothesis. 
In particular, the following steps are replicated <code>B</code> times:
</p>

<dl>
<dt>Step 1:</dt><dd><p>If <code>object</code> is supplied, then simulate new ability estimates, say <code class="reqn">z^*</code>, from a normal 
distribution with mean the ability estimates <code class="reqn">\hat{z}</code> in the original data-set, and standard deviation 
the standard error of <code class="reqn">\hat{z}</code> (in this case the <code>z.vals</code> argument is ignored). If <code>object</code> 
is not supplied and the <code>z.vals</code> argument has been specified, then set <code class="reqn">z^* =</code> <code>z.vals</code>. Finally, 
if <code>object</code> is not supplied and the <code>z.vals</code> argument has not been specified, then simulate <code class="reqn">z^*</code> 
from a standard normal distribution.</p>
</dd>
<dt>Step 2:</dt><dd><p>Simulate a new data-set of dichotomous responses, using <code class="reqn">z^*</code>, and parameters the estimated 
parameters extracted from <code>object</code> (if it is supplied) or the parameters given in the <code>thetas</code> 
argument.</p>
</dd>
<dt>Step 3:</dt><dd><p>For the new data-set simulated in Step 2, compute the tetrachoric correlations matrix and take the
largest correlations as communalities. For this matrix compute the eigenvalues.</p>
</dd>
</dl>

<p>Denote by <code class="reqn">T_{obs}</code> the value of the statistic (i.e., the second eigenvalue) for the original data-set. Then the 
<code class="reqn">p</code>-value is approximated according to the formula <code class="reqn">\left(1 + \sum_{b = 1}^B I(T_b \geq T_{obs})\right) / 
  (1 + B)</code>, where <code class="reqn">I(.)</code> denotes the indicator function, and 
<code class="reqn">T_b</code> denotes the value of the statistic in the <code class="reqn">b</code>th data-set.
</p>


<h3>Value</h3>

<p>An object of class <code>unidimTest</code> is a list with components,
</p>
<table>
<tr><td><code>Tobs</code></td>
<td>
<p>a numeric vector of the eigenvalues for the observed data-set.</p>
</td></tr>
<tr><td><code>Tboot</code></td>
<td>
<p>a numeric matrix of the eigenvalues for each simulated data-set.</p>
</td></tr>
<tr><td><code>p.value</code></td>
<td>
<p>the <code class="reqn">p</code>-value.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>a copy of the matched call of <code>object</code> if that was supplied.</p>
</td></tr>
</table>


<h3>Note</h3>

<p>For <code>ltm</code> objects you can also use a likelihood ratio test to check unidimensionality. In particular, 
<code>fit0 &lt;- ltm(data ~ z1); fit1 &lt;- ltm(data ~ z1 + z2); anova(fit0, fit1)</code>.
</p>


<h3>Author(s)</h3>

<p>Dimitris Rizopoulos <a href="mailto:d.rizopoulos@erasmusmc.nl">d.rizopoulos@erasmusmc.nl</a>
</p>


<h3>References</h3>

<p>Drasgow, F. and Lissak, R. (1983) Modified parallel analysis: a procedure for examining the latent dimensionality
of dichotomously scored item responses. <em>Journal of Applied Psychology</em>, <b>68</b>, 363&ndash;373.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+descript">descript</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Unidimensionality Check for the LSAT data-set
# under a Rasch model:
out &lt;- unidimTest(rasch(LSAT))
out
plot(out, type = "b", pch = 1:2)
legend("topright", c("Real Data", "Average Simulated Data"), lty = 1, 
    pch = 1:2, col = 1:2, bty = "n")

## End(Not run)
</code></pre>

<hr>
<h2 id='vcov'> vcov method for fitted IRT models </h2><span id='topic+vcov.gpcm'></span><span id='topic+vcov.grm'></span><span id='topic+vcov.ltm'></span><span id='topic+vcov.rasch'></span><span id='topic+vcov.tpm'></span>

<h3>Description</h3>

<p>Extracts the asymptotic variance-covariance matrix of the MLEs from either <code>gpcm</code>, <code>grm</code>,
<code>ltm</code>, <code>rasch</code> or <code>tpm</code> objects.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>
## S3 method for class 'gpcm'
vcov(object, robust = FALSE, ...)

## S3 method for class 'grm'
vcov(object, ...)

## S3 method for class 'ltm'
vcov(object, robust = FALSE, ...)

## S3 method for class 'rasch'
vcov(object, robust = FALSE, ...)

## S3 method for class 'tpm'
vcov(object, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vcov_+3A_object">object</code></td>
<td>
<p>an object inheriting from either class <code>gpcm</code>, class <code>grm</code>, class <code>ltm</code>, class <code>rasch</code> or class 
<code>tpm</code>.</p>
</td></tr>
<tr><td><code id="vcov_+3A_robust">robust</code></td>
<td>
<p>logical; if <code>TRUE</code> the sandwich estimator is used. </p>
</td></tr>
<tr><td><code id="vcov_+3A_...">...</code></td>
<td>
<p>additional arguments; currently none is used. </p>
</td></tr>
</table>


<h3>Value</h3>

<p>a numeric matrix representing the estimated covariance matrix of the maximum likelihood estimates. Note that this 
covariance matrix is for the parameter estimates under the additive parameterization and not under the usual IRT 
parameterization; for more info check the <b>Details</b> section of <code><a href="#topic+grm">grm</a></code>, <code><a href="#topic+ltm">ltm</a></code>, 
<code><a href="#topic+rasch">rasch</a></code>, and <code><a href="#topic+tpm">tpm</a></code>.
</p>


<h3>Author(s)</h3>

<p>Dimitris Rizopoulos <a href="mailto:d.rizopoulos@erasmusmc.nl">d.rizopoulos@erasmusmc.nl</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+gpcm">gpcm</a></code>,
<code><a href="#topic+grm">grm</a></code>,
<code><a href="#topic+ltm">ltm</a></code>,
<code><a href="#topic+rasch">rasch</a></code>,
<code><a href="#topic+tpm">tpm</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>fit &lt;- rasch(WIRS)
vcov(fit)
sqrt(diag(vcov(fit))) # standard errors under additive parameterization
</code></pre>

<hr>
<h2 id='WIRS'>Workplace Industrial Relation Survey Data</h2><span id='topic+WIRS'></span>

<h3>Description</h3>

<p>These data were taken from a section of the 1990 Workplace 
Industrial Relation Survey (WIRS) dealing with management/worker 
consultation in firms. The questions asked are given below:
</p>


<h3>Format</h3>

<p>Please consider the most recent change involving the introduction 
of the new plant, machinery and equipment. Were discussions or 
consultations of any of the type on this card held either about the 
introduction of the change or about the way it was to be implemented.
</p>

<dl>
<dt>Item 1</dt><dd><p>Informal discussion with individual workers.</p>
</dd>
<dt>Item 2</dt><dd><p>Meeting with groups of workers.</p>
</dd>
<dt>Item 3</dt><dd><p>Discussions in established joint consultative committee.</p>
</dd>
<dt>Item 4</dt><dd><p>Discussions in specially constituted committee to consider the change.</p>
</dd>
<dt>Item 5</dt><dd><p>Discussions with the union representatives at the establishment.</p>
</dd>
<dt>Item 6</dt><dd><p>Discussions with paid union officials from outside.</p>
</dd>
</dl>



<h3>References</h3>

<p>Bartholomew, D. (1998) Scaling unobservable constructs in social science. 
<em>Applied Statistics</em>, <b>47</b>, 1&ndash;13. 
</p>
<p>Bartholomew, D., Steel, F., Moustaki, I. and Galbraith, J. (2002) <em>The Analysis and Interpretation of 
Multivariate Data for Social Scientists</em>. London: Chapman and Hall.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
## Descriptive statistics for Wirs data
descript(WIRS)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
