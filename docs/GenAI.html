<!DOCTYPE html><html><head><title>Help for package GenAI</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {GenAI}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#available.models'><p>Get Supported Generative AI Models</p></a></li>
<li><a href='#chat'><p>Chat Generation with Text as the Input</p></a></li>
<li><a href='#chat.edit'><p>Chat Edit with New Text as the Input</p></a></li>
<li><a href='#chat.history.convert'><p>Chat History Convert</p></a></li>
<li><a href='#chat.history.export'><p>Chat History Export</p></a></li>
<li><a href='#chat.history.import'><p>Chat History Import</p></a></li>
<li><a href='#chat.history.print'><p>Chat History Print</p></a></li>
<li><a href='#chat.history.reset'><p>Chat History Reset</p></a></li>
<li><a href='#chat.history.save'><p>Chat History Save</p></a></li>
<li><a href='#genai.google'><p>Google Generative AI Object Creation</p></a></li>
<li><a href='#genai.moonshot'><p>Moonshot AI Object Creation</p></a></li>
<li><a href='#genai.openai'><p>OpenAI Object Creation</p></a></li>
<li><a href='#img'><p>Image Generation with Text as the Input</p></a></li>
<li><a href='#txt'><p>Text Generation with Text as the Input</p></a></li>
<li><a href='#txt.image'><p>Text Generation with Text and Image as the Input</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Generative Artificial Intelligence</td>
</tr>
<tr>
<td>Version:</td>
<td>0.2.0</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Li Yuan &lt;lyuan@gd.edu.kg&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Utilizing Generative Artificial Intelligence models like 'GPT-4' and 'Gemini Pro' as coding and writing assistants for 'R' users. Through these models, 'GenAI' offers a variety of functions, encompassing text generation, code optimization, natural language processing, chat, and image interpretation. The goal is to aid 'R' users in streamlining laborious coding and language processing tasks.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://creativecommons.org/licenses/by/4.0">CC BY 4.0</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://genai.gd.edu.kg/">https://genai.gd.edu.kg/</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/GitData-GA/GenAI/issues">https://github.com/GitData-GA/GenAI/issues</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.0</td>
</tr>
<tr>
<td>Depends:</td>
<td>magrittr</td>
</tr>
<tr>
<td>Imports:</td>
<td>base64enc, httr, jsonlite, tools, R6, listenv, magick,
ggplotify</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-02-15 03:28:07 UTC; lp130</td>
</tr>
<tr>
<td>Author:</td>
<td>Li Yuan <a href="https://orcid.org/0009-0008-1075-9922"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [aut,
    cre]</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-02-15 19:00:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='available.models'>Get Supported Generative AI Models</h2><span id='topic+available.models'></span>

<h3>Description</h3>

<p>This function sends a request to GenAI database API to retrieve information
about available generative AI models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>available.models()
</code></pre>


<h3>Details</h3>

<p>The function utilizes the GenAI database API to fetch the latest information about
available Generative AI models. The retrieved data includes details about different models
offered by various service providers.
</p>


<h3>Value</h3>

<p>If successful, the function returns a list containing generative AI
service providers and their corresponding models. If the function encounters an error,
it will halt execution and provide an error message.
</p>


<h3>See Also</h3>

<p><a href="https://genai.gd.edu.kg/r/documentation/">GenAI - R Package &quot;GenAI&quot; Documentation</a>
</p>
<p><a href="https://colab.research.google.com/github/GitData-GA/GenAI/blob/gh-pages/r/example/available_models.ipynb">Live Demo in Colab</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Assuming there is a GenAI object named 'genai.model' supporting this
# function, please refer to the "Live Demo in Colab" above for real
# examples. The following examples are just some basic guidelines.

all.models = available.models() %&gt;% print()

## End(Not run)

</code></pre>

<hr>
<h2 id='chat'>Chat Generation with Text as the Input</h2><span id='topic+chat'></span>

<h3>Description</h3>

<p>This function establishes a connection to a generative AI model through a generative AI object.
It generates a chat response based on the provided prompt and stores it in the chat history along
with the generative AI object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>chat(genai.object, prompt, verbose = FALSE, config = list())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="chat_+3A_genai.object">genai.object</code></td>
<td>
<p>A generative AI object containing necessary and correct information.</p>
</td></tr>
<tr><td><code id="chat_+3A_prompt">prompt</code></td>
<td>
<p>A character string representing the query for chat generation.</p>
</td></tr>
<tr><td><code id="chat_+3A_verbose">verbose</code></td>
<td>
<p>Optional. Default to <code>FALSE</code>. A boolean value determining whether or not to print
out the details of the chat request.</p>
</td></tr>
<tr><td><code id="chat_+3A_config">config</code></td>
<td>
<p>Optional. Default to <code>list()</code>. A list of configuration parameters for chat generation.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Providing accurate and valid information for each argument is crucial for successful chat
generation by the generative AI model. If any parameter is incorrect, the function responds with an
error message based on the API feedback. To view all supported generative AI models, use the
function <code><a href="#topic+available.models">available.models</a></code>.
</p>
<p>In addition, this function modifies the chat history along with the generative AI object directly,
meaning the chat history is mutable. You can print out the chat history using the
function <code><a href="#topic+chat.history.print">chat.history.print</a></code> or simply use <code>verbose = TRUE</code> in this function. If you
want to edit a message, use the function <code><a href="#topic+chat.edit">chat.edit</a></code>. To reset the chat history along with
the generative AI object, use the function <code><a href="#topic+chat.history.reset">chat.history.reset</a></code>.
</p>
<p>For <strong>Google Generative AI</strong> models, available configurations are as follows. For more detail,
please refer
to <code>https://ai.google.dev/api/rest/v1/HarmCategory</code>,
<code>https://ai.google.dev/api/rest/v1/SafetySetting</code>, and
<code>https://ai.google.dev/api/rest/v1/GenerationConfig</code>.
</p>

<ul>
<li> <p><code>harm.category.dangerous.content</code>
</p>
<p>Optional. An integer, from 1 to 5 inclusive, representing the threshold for dangerous content,
with a higher value representing a lower probability of being blocked.
</p>
</li>
<li> <p><code>harm.category.harassment</code>
</p>
<p>Optional. An integer, from 1 to 5 inclusive, representing the threshold for harasment content,
with a higher value representing a lower probability of being blocked.
</p>
</li>
<li> <p><code>harm.category.hate.speech</code>
</p>
<p>Optional. An integer, from 1 to 5 inclusive, representing the threshold for hate speech and
content, with a higher value representing a lower probability of being blocked.
</p>
</li>
<li> <p><code>harm.category.sexually.explicit</code>
</p>
<p>Optional. An integer, from 1 to 5 inclusive, representing the threshold for sexually explicit
content, with a higher value representing a lower probability of being blocked.
</p>
</li>
<li> <p><code>stop.sequences</code>
</p>
<p>Optional. A list of character sequences (up to 5) that will stop output generation. If specified,
the API will stop at the first appearance of a stop sequence. The stop sequence will not be
included as part of the response.
</p>
</li>
<li> <p><code>max.output.tokens</code>
</p>
<p>Optional. An integer, value varies by model, representing maximum number of tokens to include
in a candidate.
</p>
</li>
<li> <p><code>temperature</code>
</p>
<p>Optional. A number, from 0.0 to 1.0 inclusive, controlling the randomness of the output.
</p>
</li>
<li> <p><code>top.p</code>
</p>
<p>Optional. A number, value varies by model, representing maximum cumulative probability of tokens
to consider when sampling.
</p>
</li>
<li> <p><code>top.k</code>
</p>
<p>Optional. A number, value varies by model, representing maximum number of tokens to consider when sampling.
</p>
</li></ul>

<p>For <strong>Moonshot AI</strong> models, available configurations are as follows. For more detail, please refer to
<code>https://platform.moonshot.cn/api.html#chat-completion</code>.
</p>

<ul>
<li> <p><code>max.tokens</code>
</p>
<p>Optional. An integer. The maximum number of tokens that will be generated when the chat completes.
If the chat is not finished by the maximum number of tokens generated, the finish reason will be
&quot;length&quot;, otherwise it will be &quot;stop&quot;.
</p>
</li>
<li> <p><code>temperature</code>
</p>
<p>Optional. A number. What sampling temperature to use, between 0 and 1. Higher values (e.g. 0.7) will
make the output more random, while lower values (e.g. 0.2) will make it more focused and deterministic.
</p>
</li>
<li> <p><code>top.p</code>
</p>
<p>Optional. A number. Another sampling temperature.
</p>
</li></ul>

<p>For <strong>OpenAI</strong> models, available configurations are as follows. For more detail, please refer to
<code>https://platform.openai.com/docs/api-reference/chat/create</code>.
</p>

<ul>
<li> <p><code>frequency.penalty</code>
</p>
<p>Optional. A number from -2.0 to 2.0 inclusive. Positive values penalize new tokens based on their
existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
</p>
</li>
<li> <p><code>logit.bias</code>
</p>
<p>Optional. A map. Modify the likelihood of specified tokens appearing in the completion. Accepts a JSON object
that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to
100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact
effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection;
values like -100 or 100 should result in a ban or exclusive selection of the relevant token.
</p>
</li>
<li> <p><code>logprobs</code>
</p>
<p>Optional. A boolean value. Whether to return log probabilities of the output tokens or not. If true, returns the log
probabilities of each output token returned in the content of message
</p>
</li>
<li> <p><code>top.logprobs</code>
</p>
<p>Optional. An integer between 0 and 5 specifying the number of most likely tokens to return at each token
position, each with an associated log probability. <code>logprobs</code> must be set to <code>TRUE</code> if this
parameter is used.
</p>
</li>
<li> <p><code>max.tokens</code>
</p>
<p>Optional. An integer. The maximum number of tokens that can be generated in the chat completion. The total length of
input tokens and generated tokens is limited by the model's context length.
</p>
</li>
<li> <p><code>presence.penalty</code>
</p>
<p>Optional. A Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear
in the text so far, increasing the model's likelihood to talk about new topics.
</p>
</li>
<li> <p><code>response.format</code>
</p>
<p>Optional. An object specifying the format that the model must output. Compatible with GPT-4 Turbo and
all GPT-3.5 Turbo models newer than <code>gpt-3.5-turbo-1106</code>.
</p>
</li>
<li> <p><code>seed</code>
</p>
<p>Optional. An integer. If specified, our system will make a best effort to sample deterministically, such that repeated
requests with the same seed and parameters should return the same result.
</p>
</li>
<li> <p><code>stop</code>
</p>
<p>Optional. A character string or list contains up to 4 sequences where the API will stop generating further tokens.
</p>
</li>
<li> <p><code>temperature</code>
</p>
<p>Optional. A number. What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output
more random, while lower values like 0.2 will make it more focused and deterministic.
</p>
</li>
<li> <p><code>top.p</code>
</p>
<p>Optional. A number. An alternative to sampling with temperature, called nucleus sampling, where the model considers
the results of the tokens with <code>top.p</code> probability mass. So 0.1 means only the tokens comprising the top
10
</p>
</li>
<li> <p><code>tools</code>
</p>
<p>Optional. A list of tools the model may call. Currently, only functions are supported as a tool. Use this
to provide a list of functions the model may generate JSON inputs for.
</p>
</li>
<li> <p><code>tool.choice</code>
</p>
<p>Optional. A character string or object. Controls which (if any) function is called by the model. <code>none</code> means
the model will not call a function and instead generates a message. <code>auto</code> means the model can pick
between generating a message or calling a function.
</p>
</li>
<li> <p><code>user</code>
</p>
<p>Optional. A character string. A unique identifier representing your end-user, which can help OpenAI to monitor
and detect abuse.
</p>
</li></ul>



<h3>Value</h3>

<p>If successful, the most recent chat response will be returned. If the API response indicates
an error, the function halts execution and provides an error message.
</p>


<h3>See Also</h3>

<p><a href="https://genai.gd.edu.kg/r/documentation/">GenAI - R Package &quot;GenAI&quot; Documentation</a>
</p>
<p><a href="https://colab.research.google.com/github/GitData-GA/GenAI/blob/gh-pages/r/example/chat.ipynb">Live Demo in Colab</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Assuming there is a GenAI object named 'genai.model' supporting this
# function, please refer to the "Live Demo in Colab" above for real
# examples. The following examples are just some basic guidelines.

# Method 1 (recommended): use the pipe operator "%&gt;%"
genai.model %&gt;%
  chat(prompt = "Write a story about Mars in 50 words.") %&gt;%
  cat()

# Method 2: use the reference operator "$"
cat(genai.model$chat(prompt = "Write a story about Jupiter in 50 words."))

# Method 3: use the function chat() directly
cat(chat(genai.object = genai.model,
         prompt = "Summarize the chat."))

## End(Not run)

</code></pre>

<hr>
<h2 id='chat.edit'>Chat Edit with New Text as the Input</h2><span id='topic+chat.edit'></span>

<h3>Description</h3>

<p>This function establishes a connection to a generative AI model through a generative AI object.
It generates a chat response based on the new prompt and stores it in the chat history along
with the generative AI object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>chat.edit(
  genai.object,
  prompt,
  message.to.edit,
  verbose = FALSE,
  config = list()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="chat.edit_+3A_genai.object">genai.object</code></td>
<td>
<p>A generative AI object containing necessary and correct information.</p>
</td></tr>
<tr><td><code id="chat.edit_+3A_prompt">prompt</code></td>
<td>
<p>A character string representing the query for chat generation.</p>
</td></tr>
<tr><td><code id="chat.edit_+3A_message.to.edit">message.to.edit</code></td>
<td>
<p>An integer representing the index of the message to be edited.</p>
</td></tr>
<tr><td><code id="chat.edit_+3A_verbose">verbose</code></td>
<td>
<p>Optional. Default to <code>FALSE</code>. A boolean value determining whether or not to print
out the details of the chat request.</p>
</td></tr>
<tr><td><code id="chat.edit_+3A_config">config</code></td>
<td>
<p>Optional. Default to <code>list()</code>. A list of configuration parameters for chat generation.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Providing accurate and valid information for each argument is crucial for successful chat
generation by the generative AI model. If any parameter is incorrect, the function responds with an
error message based on the API feedback. To view all supported generative AI models, use the
function <code><a href="#topic+available.models">available.models</a></code>.
</p>
<p>In addition, this function modifies the chat history along with the generative AI object directly,
meaning the chat history is mutable. You can print out the chat history using the
function <code><a href="#topic+chat.history.print">chat.history.print</a></code> or simply use <code>verbose = TRUE</code> in this function. To reset the chat history along with
the chat history along with the generative AI object, use the function <code><a href="#topic+chat.history.reset">chat.history.reset</a></code>.
</p>
<p>For <strong>Google Generative AI</strong> models, available configurations are as follows. For more detail,
please refer
to <code>https://ai.google.dev/api/rest/v1/HarmCategory</code>,
<code>https://ai.google.dev/api/rest/v1/SafetySetting</code>, and
<code>https://ai.google.dev/api/rest/v1/GenerationConfig</code>.
</p>

<ul>
<li> <p><code>harm.category.dangerous.content</code>
</p>
<p>Optional. An integer, from 1 to 5 inclusive, representing the threshold for dangerous content,
with a higher value representing a lower probability of being blocked.
</p>
</li>
<li> <p><code>harm.category.harassment</code>
</p>
<p>Optional. An integer, from 1 to 5 inclusive, representing the threshold for harasment content,
with a higher value representing a lower probability of being blocked.
</p>
</li>
<li> <p><code>harm.category.hate.speech</code>
</p>
<p>Optional. An integer, from 1 to 5 inclusive, representing the threshold for hate speech and
content, with a higher value representing a lower probability of being blocked.
</p>
</li>
<li> <p><code>harm.category.sexually.explicit</code>
</p>
<p>Optional. An integer, from 1 to 5 inclusive, representing the threshold for sexually explicit
content, with a higher value representing a lower probability of being blocked.
</p>
</li>
<li> <p><code>stop.sequences</code>
</p>
<p>Optional. A list of character sequences (up to 5) that will stop output generation. If specified,
the API will stop at the first appearance of a stop sequence. The stop sequence will not be
included as part of the response.
</p>
</li>
<li> <p><code>max.output.tokens</code>
</p>
<p>Optional. An integer, value varies by model, representing maximum number of tokens to include
in a candidate.
</p>
</li>
<li> <p><code>temperature</code>
</p>
<p>Optional. A number, from 0.0 to 1.0 inclusive, controlling the randomness of the output.
</p>
</li>
<li> <p><code>top.p</code>
</p>
<p>Optional. A number, value varies by model, representing maximum cumulative probability of tokens
to consider when sampling.
</p>
</li>
<li> <p><code>top.k</code>
</p>
<p>Optional. A number, value varies by model, representing maximum number of tokens to consider when sampling.
</p>
</li></ul>

<p>For <strong>Moonshot AI</strong> models, available configurations are as follows. For more detail, please refer to
<code>https://platform.moonshot.cn/api.html#chat-completion</code>.
</p>

<ul>
<li> <p><code>max.tokens</code>
</p>
<p>Optional. An integer. The maximum number of tokens that will be generated when the chat completes.
If the chat is not finished by the maximum number of tokens generated, the finish reason will be
&quot;length&quot;, otherwise it will be &quot;stop&quot;.
</p>
</li>
<li> <p><code>temperature</code>
</p>
<p>Optional. A number. What sampling temperature to use, between 0 and 1. Higher values (e.g. 0.7) will
make the output more random, while lower values (e.g. 0.2) will make it more focused and deterministic.
</p>
</li>
<li> <p><code>top.p</code>
</p>
<p>Optional. A number. Another sampling temperature.
</p>
</li></ul>

<p>For <strong>OpenAI</strong> models, available configurations are as follows. For more detail, please refer to
<code>https://platform.openai.com/docs/api-reference/chat/create</code>.
</p>

<ul>
<li> <p><code>frequency.penalty</code>
</p>
<p>Optional. A number from -2.0 to 2.0 inclusive. Positive values penalize new tokens based on their
existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
</p>
</li>
<li> <p><code>logit.bias</code>
</p>
<p>Optional. A map. Modify the likelihood of specified tokens appearing in the completion. Accepts a JSON object
that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to
100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact
effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection;
values like -100 or 100 should result in a ban or exclusive selection of the relevant token.
</p>
</li>
<li> <p><code>logprobs</code>
</p>
<p>Optional. A boolean value. Whether to return log probabilities of the output tokens or not. If true, returns the log
probabilities of each output token returned in the content of message
</p>
</li>
<li> <p><code>top.logprobs</code>
</p>
<p>Optional. An integer between 0 and 5 specifying the number of most likely tokens to return at each token
position, each with an associated log probability. <code>logprobs</code> must be set to <code>TRUE</code> if this
parameter is used.
</p>
</li>
<li> <p><code>max.tokens</code>
</p>
<p>Optional. An integer. The maximum number of tokens that can be generated in the chat completion. The total length of
input tokens and generated tokens is limited by the model's context length.
</p>
</li>
<li> <p><code>presence.penalty</code>
</p>
<p>Optional. A Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear
in the text so far, increasing the model's likelihood to talk about new topics.
</p>
</li>
<li> <p><code>response.format</code>
</p>
<p>Optional. An object specifying the format that the model must output. Compatible with GPT-4 Turbo and
all GPT-3.5 Turbo models newer than <code>gpt-3.5-turbo-1106</code>.
</p>
</li>
<li> <p><code>seed</code>
</p>
<p>Optional. An integer. If specified, our system will make a best effort to sample deterministically, such that repeated
requests with the same seed and parameters should return the same result.
</p>
</li>
<li> <p><code>stop</code>
</p>
<p>Optional. A character string or list contains up to 4 sequences where the API will stop generating further tokens.
</p>
</li>
<li> <p><code>temperature</code>
</p>
<p>Optional. A number. What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output
more random, while lower values like 0.2 will make it more focused and deterministic.
</p>
</li>
<li> <p><code>top.p</code>
</p>
<p>Optional. A number. An alternative to sampling with temperature, called nucleus sampling, where the model considers
the results of the tokens with <code>top.p</code> probability mass. So 0.1 means only the tokens comprising the top
10
</p>
</li>
<li> <p><code>tools</code>
</p>
<p>Optional. A list of tools the model may call. Currently, only functions are supported as a tool. Use this
to provide a list of functions the model may generate JSON inputs for.
</p>
</li>
<li> <p><code>tool.choice</code>
</p>
<p>Optional. A character string or object. Controls which (if any) function is called by the model. <code>none</code> means
the model will not call a function and instead generates a message. <code>auto</code> means the model can pick
between generating a message or calling a function.
</p>
</li>
<li> <p><code>user</code>
</p>
<p>Optional. A character string. A unique identifier representing your end-user, which can help OpenAI to monitor
and detect abuse.
</p>
</li></ul>



<h3>Value</h3>

<p>If successful, the most recent chat response will be returned. If the API response indicates
an error, the function halts execution and provides an error message.
</p>


<h3>See Also</h3>

<p><a href="https://genai.gd.edu.kg/r/documentation/">GenAI - R Package &quot;GenAI&quot; Documentation</a>
</p>
<p><a href="https://colab.research.google.com/github/GitData-GA/GenAI/blob/gh-pages/r/example/chat_edit.ipynb">Live Demo in Colab</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Assuming there is a GenAI object named 'genai.model' supporting this
# function, please refer to the "Live Demo in Colab" above for real
# examples. The following examples are just some basic guidelines.

# Method 1 (recommended): use the pipe operator "%&gt;%"
genai.model %&gt;%
  chat.edit(prompt = "What is XGBoost?",
            message.to.edit = 5,
            verbose = TRUE,
            config = parameters) %&gt;%
  cat()

# Method 2: use the reference operator "$"
cat(genai.model$chat.edit(prompt = "What is CatBoost?",
                          message.to.edit = 3))

# Method 3: use the function chat.edit() directly
cat(chat.edit(genai.object = genai.model,
              prompt = "What is LightGBM?",
              message.to.edit = 1))

## End(Not run)

</code></pre>

<hr>
<h2 id='chat.history.convert'>Chat History Convert</h2><span id='topic+chat.history.convert'></span>

<h3>Description</h3>

<p>This function converts the chat history along with a generative AI object to a valid format
for another generative AI object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>chat.history.convert(from.genai.object, to.genai.object)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="chat.history.convert_+3A_from.genai.object">from.genai.object</code></td>
<td>
<p>A source generative AI object containing necessary and correct information.</p>
</td></tr>
<tr><td><code id="chat.history.convert_+3A_to.genai.object">to.genai.object</code></td>
<td>
<p>A target generative AI object containing necessary and correct information.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Providing accurate and valid information for each argument is crucial for successful chat
generation by the generative AI model. If any parameter is incorrect, the function responds with an
error message based on the API feedback. To view all supported generative AI models, use the
function <code><a href="#topic+available.models">available.models</a></code>. Moreover, you can print out the chat history using the
function <code><a href="#topic+chat.history.print">chat.history.print</a></code> or simply use <code>verbose = TRUE</code> during the chat.
</p>


<h3>Value</h3>

<p>If successful, the converted chat history list will be returned.
</p>


<h3>See Also</h3>

<p><a href="https://genai.gd.edu.kg/r/documentation/">GenAI - R Package &quot;GenAI&quot; Documentation</a>
</p>
<p><a href="https://colab.research.google.com/github/GitData-GA/GenAI/blob/gh-pages/r/example/chat_history_convert.ipynb">Live Demo in Colab</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Assuming there are two GenAI objects named 'genai.model' and 'another.genai.model'
# supporting this function, please refer to the "Live Demo in Colab" above for
# real examples. The following examples are just some basic guidelines.

# Method 1 (recommended): use the pipe operator "%&gt;%"
converted.history = genai.model %&gt;%
  chat.history.convert(to.genai.object = another.genai.model)

# Method 2: use the reference operator "$"
converted.history = genai.model$chat.history.convert(to.genai.object = another.genai.model)

# Method 3: use the function chat.history.convert() directly
converted.history = chat.history.convert(from.genai.object = genai.model,
                                         to.genai.object = another.genai.model)

## End(Not run)

</code></pre>

<hr>
<h2 id='chat.history.export'>Chat History Export</h2><span id='topic+chat.history.export'></span>

<h3>Description</h3>

<p>This function exports the chat history along with a generative AI object as a list.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>chat.history.export(genai.object)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="chat.history.export_+3A_genai.object">genai.object</code></td>
<td>
<p>A generative AI object containing necessary and correct information.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Providing accurate and valid information for each argument is crucial for successful chat
generation by the generative AI model. If any parameter is incorrect, the function responds with an
error message based on the API feedback. To view all supported generative AI models, use the
function <code><a href="#topic+available.models">available.models</a></code>.
</p>


<h3>Value</h3>

<p>If successful, the chat history list will be returned.
</p>


<h3>See Also</h3>

<p><a href="https://genai.gd.edu.kg/r/documentation/">GenAI - R Package &quot;GenAI&quot; Documentation</a>
</p>
<p><a href="https://colab.research.google.com/github/GitData-GA/GenAI/blob/gh-pages/r/example/chat_history_export.ipynb">Live Demo in Colab</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Assuming there is a GenAI object named 'genai.model' supporting this
# function, please refer to the "Live Demo in Colab" above for real
# examples. The following examples are just some basic guidelines.

# Method 1 (recommended): use the pipe operator "%&gt;%"
exported.history = genai.model %&gt;%
  chat.history.export()

# Method 2: use the reference operator "$"
exported.history = genai.model$chat.history.export()

# Method 3: use the function chat.history.export() directly
exported.history = chat.history.export(genai.object = genai.model)

## End(Not run)

</code></pre>

<hr>
<h2 id='chat.history.import'>Chat History Import</h2><span id='topic+chat.history.import'></span>

<h3>Description</h3>

<p>This function imports a chat history in list format to a generative AI object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>chat.history.import(genai.object, new.chat.history)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="chat.history.import_+3A_genai.object">genai.object</code></td>
<td>
<p>A generative AI object containing necessary and correct information.</p>
</td></tr>
<tr><td><code id="chat.history.import_+3A_new.chat.history">new.chat.history</code></td>
<td>
<p>A list containing a chat history in correct format.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Providing accurate and valid information for each argument is crucial for successful chat
generation by the generative AI model. If any parameter is incorrect, the function responds with an
error message based on the API feedback. To view all supported generative AI models, use the
function <code><a href="#topic+available.models">available.models</a></code>.
</p>


<h3>See Also</h3>

<p><a href="https://genai.gd.edu.kg/r/documentation/">GenAI - R Package &quot;GenAI&quot; Documentation</a>
</p>
<p><a href="https://colab.research.google.com/github/GitData-GA/GenAI/blob/gh-pages/r/example/chat_history_import.ipynb">Live Demo in Colab</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Assuming there is a GenAI object named 'genai.model' supporting this
# function and a valid chat history list named 'new.history', please
# refer to the "Live Demo in Colab" above for real examples. The
# following examples are just some basic guidelines.

# Method 1 (recommended): use the pipe operator "%&gt;%"
genai.model %&gt;%
  chat.history.import(new.chat.history = new.history)

# Method 2: use the reference operator "$"
genai.model$chat.history.import(new.chat.history = new.history)

# Method 3: use the function chat.history.import() directly
chat.history.import(genai.object = genai.model,
                    new.chat.history = new.history)

## End(Not run)

</code></pre>

<hr>
<h2 id='chat.history.print'>Chat History Print</h2><span id='topic+chat.history.print'></span>

<h3>Description</h3>

<p>This function prints out the chat history along with a generative AI object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>chat.history.print(genai.object, from = 1, to = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="chat.history.print_+3A_genai.object">genai.object</code></td>
<td>
<p>A generative AI object containing necessary and correct information.</p>
</td></tr>
<tr><td><code id="chat.history.print_+3A_from">from</code></td>
<td>
<p>Optional. Default to 1. An integer representing the first message in the chat history that needs
to be printed.</p>
</td></tr>
<tr><td><code id="chat.history.print_+3A_to">to</code></td>
<td>
<p>Optional. Default to <code>NULL</code>, prints until the last message in the chat history. An integer
representing the last message in the chat history that needs to be printed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Providing accurate and valid information for each argument is crucial for successful chat
generation by the generative AI model. If any parameter is incorrect, the function responds with an
error message based on the API feedback. To view all supported generative AI models, use the
function <code><a href="#topic+available.models">available.models</a></code>.
</p>


<h3>See Also</h3>

<p><a href="https://genai.gd.edu.kg/r/documentation/">GenAI - R Package &quot;GenAI&quot; Documentation</a>
</p>
<p><a href="https://colab.research.google.com/github/GitData-GA/GenAI/blob/gh-pages/r/example/chat_history_print.ipynb">Live Demo in Colab</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Assuming there is a GenAI object named 'genai.model' supporting this
# function, please refer to the "Live Demo in Colab" above for real
# examples. The following examples are just some basic guidelines.

# Method 1 (recommended): use the pipe operator "%&gt;%"
genai.model %&gt;%
  chat.history.print()

# Method 2: use the reference operator "$"
genai.model$chat.history.print(from = 3)

# Method 3: use the function chat.history.print() directly
chat.history.print(genai.object = genai.model,
                   from = 3,
                   to = 5)

## End(Not run)

</code></pre>

<hr>
<h2 id='chat.history.reset'>Chat History Reset</h2><span id='topic+chat.history.reset'></span>

<h3>Description</h3>

<p>This function resets the chat history along with a generative AI object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>chat.history.reset(genai.object)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="chat.history.reset_+3A_genai.object">genai.object</code></td>
<td>
<p>A generative AI object containing necessary and correct information.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Providing accurate and valid information for each argument is crucial for successful chat
generation by the generative AI model. If any parameter is incorrect, the function responds with an
error message based on the API feedback. To view all supported generative AI models, use the
function <code><a href="#topic+available.models">available.models</a></code>.
</p>


<h3>See Also</h3>

<p><a href="https://genai.gd.edu.kg/r/documentation/">GenAI - R Package &quot;GenAI&quot; Documentation</a>
</p>
<p><a href="https://colab.research.google.com/github/GitData-GA/GenAI/blob/gh-pages/r/example/chat_history_reset.ipynb">Live Demo in Colab</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Assuming there is a GenAI object named 'genai.model' supporting this
# function, please refer to the "Live Demo in Colab" above for real
# examples. The following examples are just some basic guidelines.

# Method 1 (recommended): use the pipe operator "%&gt;%"
genai.model %&gt;%
  chat.history.reset()

# Method 2: use the reference operator "$"
genai.model$chat.history.reset()

# Method 3: use the function chat.history.reset() directly
chat.history.reset(genai.object = genai.model)

## End(Not run)

</code></pre>

<hr>
<h2 id='chat.history.save'>Chat History Save</h2><span id='topic+chat.history.save'></span>

<h3>Description</h3>

<p>This function saves a chat history along with a generative AI object as a JSON file.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>chat.history.save(genai.object, file.name)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="chat.history.save_+3A_genai.object">genai.object</code></td>
<td>
<p>A generative AI object containing necessary and correct information.</p>
</td></tr>
<tr><td><code id="chat.history.save_+3A_file.name">file.name</code></td>
<td>
<p>A character string representing the name of the JSON file for the chat history.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Providing accurate and valid information for each argument is crucial for successful chat
generation by the generative AI model. If any parameter is incorrect, the function responds with an
error message based on the API feedback. To view all supported generative AI models, use the
function <code><a href="#topic+available.models">available.models</a></code>.
</p>


<h3>Value</h3>

<p>If successful, the chat history will be saved as a JSON file in your current or specified
directory.
</p>


<h3>See Also</h3>

<p><a href="https://genai.gd.edu.kg/r/documentation/">GenAI - R Package &quot;GenAI&quot; Documentation</a>
</p>
<p><a href="https://colab.research.google.com/github/GitData-GA/GenAI/blob/gh-pages/r/example/chat_history_save.ipynb">Live Demo in Colab</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Assuming there is a GenAI object named 'genai.model' supporting this
# function, please refer to the "Live Demo in Colab" above for real
# examples. The following examples are just some basic guidelines.

# Method 1 (recommended): use the pipe operator "%&gt;%"
genai.model %&gt;%
  chat.history.save(file.name = "saved_history")

# Method 2: use the reference operator "$"
genai.model$chat.history.save(file.name = "saved_history")

# Method 3: use the function chat.history.save() directly
chat.history.save(genai.object = genai.model,
                  file.name = "saved_history")

## End(Not run)

</code></pre>

<hr>
<h2 id='genai.google'>Google Generative AI Object Creation</h2><span id='topic+genai.google'></span>

<h3>Description</h3>

<p>This function establishes a connection to a Google generative AI model by providing essential
parameters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>genai.google(api, model, version, proxy = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="genai.google_+3A_api">api</code></td>
<td>
<p>A character string representing the API key required for accessing the model.</p>
</td></tr>
<tr><td><code id="genai.google_+3A_model">model</code></td>
<td>
<p>A character string representing the specific model.</p>
</td></tr>
<tr><td><code id="genai.google_+3A_version">version</code></td>
<td>
<p>A character string representing the version of the chosen model.</p>
</td></tr>
<tr><td><code id="genai.google_+3A_proxy">proxy</code></td>
<td>
<p>Optional. Default to <code>FALSE</code>. A boolean value indicating whether to use a
proxy for accessing the API URL. If your local internet cannot access the API, set this
parameter to <code>TRUE</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Providing accurate and valid information for each argument is crucial for successful text
generation by the generative AI model. If any parameter is incorrect, the function responds with an
error message based on the API feedback. To view all supported generative AI models, use the
function <code><a href="#topic+available.models">available.models</a></code>.
</p>
<p>Please refer to <code>https://ai.google.dev/tutorials/setup</code> for the API key.
</p>
<p>The API proxy service is designed to address the needs of users who hold a valid API key but find
themselves outside their home countries or regions due to reasons such as travel, work, or study
in locations that may not be covered by certain Generative AI service providers.
</p>
<p>Please be aware that although GenAI and its affiliated organization - GitData - do not gather user
information through this service, the server providers for GenAI API proxy service and the Generative
AI service providers may engage in such data collection. Furthermore, the proxy service cannot
guarantee a consistent connection speed. Users are strongly encouraged to utilize this service
with caution and at their own discretion.
</p>


<h3>Value</h3>

<p>If successful, the function returns a Google generative AI object. If the API response
indicates an error, the function halts execution and provides an error message.
</p>


<h3>See Also</h3>

<p><a href="https://genai.gd.edu.kg/r/documentation/">GenAI - R Package &quot;GenAI&quot; Documentation</a>
</p>
<p><a href="https://genai.gd.edu.kg/api/">GenAI - Generative Artificial Intelligence API Proxy Service</a>
</p>
<p><a href="https://colab.research.google.com/github/GitData-GA/GenAI/blob/gh-pages/r/example/genai_google.ipynb">Live Demo in Colab</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Please change YOUR_GOOGLE_API to your own API key of Google Generative AI
Sys.setenv(GOOGLE_API = "YOUR_GOOGLE_API")

all.models = available.models() %&gt;% print()

# Create a Google Generative AI object
google = genai.google(api = Sys.getenv("GOOGLE_API"),
                      model = all.models$google$model[1],
                      version = all.models$google$version[1],
                      proxy = FALSE)

## End(Not run)

</code></pre>

<hr>
<h2 id='genai.moonshot'>Moonshot AI Object Creation</h2><span id='topic+genai.moonshot'></span>

<h3>Description</h3>

<p>This function establishes a connection to a Moonshot AI model by providing essential parameters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>genai.moonshot(api, model, version, proxy = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="genai.moonshot_+3A_api">api</code></td>
<td>
<p>A character string representing the API key required for accessing the model.</p>
</td></tr>
<tr><td><code id="genai.moonshot_+3A_model">model</code></td>
<td>
<p>A character string representing the specific model.</p>
</td></tr>
<tr><td><code id="genai.moonshot_+3A_version">version</code></td>
<td>
<p>A character string representing the version of the chosen model.</p>
</td></tr>
<tr><td><code id="genai.moonshot_+3A_proxy">proxy</code></td>
<td>
<p>Optional. Default to <code>FALSE</code>. A boolean value indicating whether to use a
proxy for accessing the API URL. If your local internet cannot access the API, set this
parameter to <code>TRUE</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Providing accurate and valid information for each argument is crucial for successful text
generation by the generative AI model. If any parameter is incorrect, the function responds with an
error message based on the API feedback. To view all supported generative AI models, use the
function <code><a href="#topic+available.models">available.models</a></code>.
</p>
<p>Please refer to <code>https://platform.moonshot.cn/console/api-keys</code> for the API key.
</p>
<p>The API proxy service is designed to address the needs of users who hold a valid API key but find
themselves outside their home countries or regions due to reasons such as travel, work, or study
in locations that may not be covered by certain Generative AI service providers.
</p>
<p>Please be aware that although GenAI and its affiliated organization - GitData - do not gather user
information through this service, the server providers for GenAI API proxy service and the Generative
AI service providers may engage in such data collection. Furthermore, the proxy service cannot
guarantee a consistent connection speed. Users are strongly encouraged to utilize this service
with caution and at their own discretion.
</p>


<h3>Value</h3>

<p>If successful, the function returns an moonshot object. If the API response
indicates an error, the function halts execution and provides an error message.
</p>


<h3>See Also</h3>

<p><a href="https://genai.gd.edu.kg/r/documentation/">GenAI - R Package &quot;GenAI&quot; Documentation</a>
</p>
<p><a href="https://genai.gd.edu.kg/api/">GenAI - Generative Artificial Intelligence API Proxy Service</a>
</p>
<p><a href="https://colab.research.google.com/github/GitData-GA/GenAI/blob/gh-pages/r/example/genai_moonshot.ipynb">Live Demo in Colab</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Please change YOUR_MOONSHOT_API to your own API key of Moonshot AI
Sys.setenv(MOONSHOT_API = "YOUR_MOONSHOT_API")

all.models = available.models() %&gt;% print()

# Create an moonshot object
moonshot = genai.moonshot(api = Sys.getenv("MOONSHOT_API"),
                          model = all.models$moonshot$model[1],
                          version = all.models$moonshot$version[1],
                          proxy = FALSE)

## End(Not run)

</code></pre>

<hr>
<h2 id='genai.openai'>OpenAI Object Creation</h2><span id='topic+genai.openai'></span>

<h3>Description</h3>

<p>This function establishes a connection to an OpenAI model by providing essential parameters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>genai.openai(api, model, version, proxy = FALSE, organization.id = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="genai.openai_+3A_api">api</code></td>
<td>
<p>A character string representing the API key required for accessing the model.</p>
</td></tr>
<tr><td><code id="genai.openai_+3A_model">model</code></td>
<td>
<p>A character string representing the specific model.</p>
</td></tr>
<tr><td><code id="genai.openai_+3A_version">version</code></td>
<td>
<p>A character string representing the version of the chosen model.</p>
</td></tr>
<tr><td><code id="genai.openai_+3A_proxy">proxy</code></td>
<td>
<p>Optional. Default to <code>FALSE</code>. A boolean value indicating whether to use a
proxy for accessing the API URL. If your local internet cannot access the API, set this
parameter to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="genai.openai_+3A_organization.id">organization.id</code></td>
<td>
<p>Optional. Default to <code>NULL</code>. A character string representing the
organization ID.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Providing accurate and valid information for each argument is crucial for successful text
generation by the generative AI model. If any parameter is incorrect, the function responds with an
error message based on the API feedback. To view all supported generative AI models, use the
function <code><a href="#topic+available.models">available.models</a></code>.
</p>
<p>Please refer to <code>https://platform.openai.com/api-keys</code> for the API key. Moreover, please refer
to <code>https://platform.openai.com/account/organization</code> for the optional organization ID.
</p>
<p>The API proxy service is designed to address the needs of users who hold a valid API key but find
themselves outside their home countries or regions due to reasons such as travel, work, or study
in locations that may not be covered by certain Generative AI service providers.
</p>
<p>Please be aware that although GenAI and its affiliated organization - GitData - do not gather user
information through this service, the server providers for GenAI API proxy service and the Generative
AI service providers may engage in such data collection. Furthermore, the proxy service cannot
guarantee a consistent connection speed. Users are strongly encouraged to utilize this service
with caution and at their own discretion.
</p>


<h3>Value</h3>

<p>If successful, the function returns an OpenAI object. If the API response
indicates an error, the function halts execution and provides an error message.
</p>


<h3>See Also</h3>

<p><a href="https://genai.gd.edu.kg/r/documentation/">GenAI - R Package &quot;GenAI&quot; Documentation</a>
</p>
<p><a href="https://genai.gd.edu.kg/api/">GenAI - Generative Artificial Intelligence API Proxy Service</a>
</p>
<p><a href="https://colab.research.google.com/github/GitData-GA/GenAI/blob/gh-pages/r/example/genai_openai.ipynb">Live Demo in Colab</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Please change YOUR_OPENAI_API to your own API key of OpenAI
Sys.setenv(OPENAI_API = "YOUR_OPENAI_API")

# Oprional. Please change YOUR_OPENAI_ORG to your own organization ID for OpenAI
Sys.setenv(OPENAI_ORG = "YOUR_OPENAI_ORG")

all.models = available.models() %&gt;% print()

# Create an OpenAI object
openai = genai.openai(api = Sys.getenv("OPENAI_API"),
                      model = all.models$openai$model[1],
                      version = all.models$openai$version[1],
                      proxy = FALSE,
                      organization.id = Sys.getenv("OPENAI_ORG"))

## End(Not run)

</code></pre>

<hr>
<h2 id='img'>Image Generation with Text as the Input</h2><span id='topic+img'></span>

<h3>Description</h3>

<p>This function establishes a connection to a generative AI model through a generative AI object.
It generates an image response based on the provided prompt.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>img(genai.object, prompt, verbose = FALSE, config = list())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="img_+3A_genai.object">genai.object</code></td>
<td>
<p>A generative AI object containing necessary and correct information.</p>
</td></tr>
<tr><td><code id="img_+3A_prompt">prompt</code></td>
<td>
<p>A character string representing the query for image generation.</p>
</td></tr>
<tr><td><code id="img_+3A_verbose">verbose</code></td>
<td>
<p>Optional. Default to <code>FALSE</code>. A boolean value determining whether or not to print
out the details of the image request.</p>
</td></tr>
<tr><td><code id="img_+3A_config">config</code></td>
<td>
<p>Optional. Default to <code>list()</code>. A list of configuration parameters for image generation.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Providing accurate and valid information for each argument is crucial for successful image
generation by the generative AI model. If any parameter is incorrect, the function responds with an
error message based on the API feedback. To view all supported generative AI models, use the
function <code><a href="#topic+available.models">available.models</a></code>.
</p>
<p>This function is only available when using OpenAI's models.
</p>
<p>For <strong>OpenAI</strong> models, available configurations are as follows. For more detail, please refer to
<code>https://platform.openai.com/docs/api-reference/images/create</code>.
</p>

<ul>
<li> <p><code>quality</code>
</p>
<p>Optional. A character string. The quality of the image that will be generated. <code>hd</code> creates
images with finer details and greater consistency across the image.
</p>
</li>
<li> <p><code>size</code>
</p>
<p>Optional. A character string. The size of the generated images. Must be one of <code>256x256</code>,
<code>512x512</code>, or <code>1024x1024</code> for <code>dall-e-2</code>. Must be one of <code>1024x1024</code>, <code>1792x1024</code>, or
<code>1024x1792</code> for <code>dall-e-3</code> models.
</p>
</li>
<li> <p><code>style</code>
</p>
<p>Optional. The style of the generated images. Must be one of <code>vivid</code> or <code>natural</code>. Vivid causes
the model to lean towards generating hyper-real and dramatic images. Natural causes the model to produce
more natural, less hyper-real looking images.
</p>
</li>
<li> <p><code>user</code>
</p>
<p>Optional. A character string. A unique identifier representing your end-user, which can help OpenAI to monitor
and detect abuse.
</p>
</li></ul>



<h3>Value</h3>

<p>If successful, a image in <code>ggplot</code> format will be returned. If the API response indicates
an error, the function halts execution and provides an error message.
</p>


<h3>See Also</h3>

<p><a href="https://genai.gd.edu.kg/r/documentation/">GenAI - R Package &quot;GenAI&quot; Documentation</a>
</p>
<p><a href="https://colab.research.google.com/github/GitData-GA/GenAI/blob/gh-pages/r/example/img.ipynb">Live Demo in Colab</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Assuming there is a GenAI object named 'genai.model' supporting this
# function, please refer to the "Live Demo in Colab" above for real
# examples. The following examples are just some basic guidelines.

# Method 1 (recommended): use the pipe operator "%&gt;%"
generated.image = genai.model %&gt;%
  img(prompt = "A very cute panda eating banboo.")
generated.image

# Method 2: use the reference operator "$"
generated.image = genai.model$img(prompt = "A very cute sea otter on a rock.")
generated.image

# Method 3: use the function img() directly
generated.image = img(genai.object = genai.model,
                      prompt = "A very cute bear.")
generated.image

## End(Not run)

</code></pre>

<hr>
<h2 id='txt'>Text Generation with Text as the Input</h2><span id='topic+txt'></span>

<h3>Description</h3>

<p>This function establishes a connection to a generative AI model through a generative AI object.
It generates a text response based on the provided prompt.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>txt(genai.object, prompt, verbose = FALSE, config = list())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="txt_+3A_genai.object">genai.object</code></td>
<td>
<p>A generative AI object containing necessary and correct information.</p>
</td></tr>
<tr><td><code id="txt_+3A_prompt">prompt</code></td>
<td>
<p>A character string representing the query for text generation.</p>
</td></tr>
<tr><td><code id="txt_+3A_verbose">verbose</code></td>
<td>
<p>Optional. Default to <code>FALSE</code>. A boolean value determining whether or not to print
out the details of the text request.</p>
</td></tr>
<tr><td><code id="txt_+3A_config">config</code></td>
<td>
<p>Optional. Default to <code>list()</code>. A list of configuration parameters for text generation.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Providing accurate and valid information for each argument is crucial for successful text
generation by the generative AI model. If any parameter is incorrect, the function responds with an
error message based on the API feedback. To view all supported generative AI models, use the
function <code><a href="#topic+available.models">available.models</a></code>.
</p>
<p>For <strong>Google Generative AI</strong> models, available configurations are as follows. For more detail,
please refer
to <code>https://ai.google.dev/api/rest/v1/HarmCategory</code>,
<code>https://ai.google.dev/api/rest/v1/SafetySetting</code>, and
<code>https://ai.google.dev/api/rest/v1/GenerationConfig</code>.
</p>

<ul>
<li> <p><code>harm.category.dangerous.content</code>
</p>
<p>Optional. An integer, from 1 to 5 inclusive, representing the threshold for dangerous content,
with a higher value representing a lower probability of being blocked.
</p>
</li>
<li> <p><code>harm.category.harassment</code>
</p>
<p>Optional. An integer, from 1 to 5 inclusive, representing the threshold for harasment content,
with a higher value representing a lower probability of being blocked.
</p>
</li>
<li> <p><code>harm.category.hate.speech</code>
</p>
<p>Optional. An integer, from 1 to 5 inclusive, representing the threshold for hate speech and
content, with a higher value representing a lower probability of being blocked.
</p>
</li>
<li> <p><code>harm.category.sexually.explicit</code>
</p>
<p>Optional. An integer, from 1 to 5 inclusive, representing the threshold for sexually explicit
content, with a higher value representing a lower probability of being blocked.
</p>
</li>
<li> <p><code>stop.sequences</code>
</p>
<p>Optional. A list of character sequences (up to 5) that will stop output generation. If specified,
the API will stop at the first appearance of a stop sequence. The stop sequence will not be
included as part of the response.
</p>
</li>
<li> <p><code>max.output.tokens</code>
</p>
<p>Optional. An integer, value varies by model, representing maximum number of tokens to include
in a candidate.
</p>
</li>
<li> <p><code>temperature</code>
</p>
<p>Optional. A number, from 0.0 to 1.0 inclusive, controlling the randomness of the output.
</p>
</li>
<li> <p><code>top.p</code>
</p>
<p>Optional. A number, value varies by model, representing maximum cumulative probability of tokens
to consider when sampling.
</p>
</li>
<li> <p><code>top.k</code>
</p>
<p>Optional. A number, value varies by model, representing maximum number of tokens to consider when sampling.
</p>
</li></ul>

<p>For <strong>Moonshot AI</strong> models, available configurations are as follows. For more detail, please refer to
<code>https://platform.moonshot.cn/api.html#chat-completion</code>.
</p>

<ul>
<li> <p><code>max.tokens</code>
</p>
<p>Optional. An integer. The maximum number of tokens that will be generated when the chat completes.
If the chat is not finished by the maximum number of tokens generated, the finish reason will be
&quot;length&quot;, otherwise it will be &quot;stop&quot;.
</p>
</li>
<li> <p><code>temperature</code>
</p>
<p>Optional. A number. What sampling temperature to use, between 0 and 1. Higher values (e.g. 0.7) will
make the output more random, while lower values (e.g. 0.2) will make it more focused and deterministic.
</p>
</li>
<li> <p><code>top.p</code>
</p>
<p>Optional. A number. Another sampling temperature.
</p>
</li></ul>

<p>For <strong>OpenAI</strong> models, available configurations are as follows. For more detail, please refer to
<code>https://platform.openai.com/docs/api-reference/chat/create</code>.
</p>

<ul>
<li> <p><code>frequency.penalty</code>
</p>
<p>Optional. A number from -2.0 to 2.0 inclusive. Positive values penalize new tokens based on their
existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
</p>
</li>
<li> <p><code>logit.bias</code>
</p>
<p>Optional. A map. Modify the likelihood of specified tokens appearing in the completion. Accepts a JSON object
that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to
100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact
effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection;
values like -100 or 100 should result in a ban or exclusive selection of the relevant token.
</p>
</li>
<li> <p><code>logprobs</code>
</p>
<p>Optional. A boolean value. Whether to return log probabilities of the output tokens or not. If true, returns the log
probabilities of each output token returned in the content of message
</p>
</li>
<li> <p><code>top.logprobs</code>
</p>
<p>Optional. An integer between 0 and 5 specifying the number of most likely tokens to return at each token
position, each with an associated log probability. <code>logprobs</code> must be set to <code>TRUE</code> if this
parameter is used.
</p>
</li>
<li> <p><code>max.tokens</code>
</p>
<p>Optional. An integer. The maximum number of tokens that can be generated in the chat completion. The total length of
input tokens and generated tokens is limited by the model's context length.
</p>
</li>
<li> <p><code>presence.penalty</code>
</p>
<p>Optional. A Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear
in the text so far, increasing the model's likelihood to talk about new topics.
</p>
</li>
<li> <p><code>response.format</code>
</p>
<p>Optional. An object specifying the format that the model must output. Compatible with GPT-4 Turbo and
all GPT-3.5 Turbo models newer than <code>gpt-3.5-turbo-1106</code>.
</p>
</li>
<li> <p><code>seed</code>
</p>
<p>Optional. An integer. If specified, our system will make a best effort to sample deterministically, such that repeated
requests with the same seed and parameters should return the same result.
</p>
</li>
<li> <p><code>stop</code>
</p>
<p>Optional. A character string or list contains up to 4 sequences where the API will stop generating further tokens.
</p>
</li>
<li> <p><code>temperature</code>
</p>
<p>Optional. A number. What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output
more random, while lower values like 0.2 will make it more focused and deterministic.
</p>
</li>
<li> <p><code>top.p</code>
</p>
<p>Optional. A number. An alternative to sampling with temperature, called nucleus sampling, where the model considers
the results of the tokens with <code>top.p</code> probability mass. So 0.1 means only the tokens comprising the top
10
</p>
</li>
<li> <p><code>tools</code>
</p>
<p>Optional. A list of tools the model may call. Currently, only functions are supported as a tool. Use this
to provide a list of functions the model may generate JSON inputs for.
</p>
</li>
<li> <p><code>tool.choice</code>
</p>
<p>Optional. A character string or object. Controls which (if any) function is called by the model. <code>none</code> means
the model will not call a function and instead generates a message. <code>auto</code> means the model can pick
between generating a message or calling a function.
</p>
</li>
<li> <p><code>user</code>
</p>
<p>Optional. A character string. A unique identifier representing your end-user, which can help OpenAI to monitor
and detect abuse.
</p>
</li></ul>



<h3>Value</h3>

<p>If successful, a text response will be returned. If the API response indicates
an error, the function halts execution and provides an error message.
</p>


<h3>See Also</h3>

<p><a href="https://genai.gd.edu.kg/r/documentation/">GenAI - R Package &quot;GenAI&quot; Documentation</a>
</p>
<p><a href="https://colab.research.google.com/github/GitData-GA/GenAI/blob/gh-pages/r/example/txt.ipynb">Live Demo in Colab</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Assuming there is a GenAI object named 'genai.model' supporting this
# function, please refer to the "Live Demo in Colab" above for real
# examples. The following examples are just some basic guidelines.

# Method 1 (recommended): use the pipe operator "%&gt;%"
genai.model %&gt;%
  txt(prompt = "Write a story about Mars in 50 words.") %&gt;%
  cat()

# Method 2: use the reference operator "$"
cat(genai.model$txt(prompt = "Write a story about Jupiter in 50 words."))

# Method 3: use the function txt() directly
# Set verbose to TRUE to see the detail
cat(txt(genai.object = genai.model,
        prompt = "Write a story about Earth in 50 words."))

## End(Not run)

</code></pre>

<hr>
<h2 id='txt.image'>Text Generation with Text and Image as the Input</h2><span id='topic+txt.image'></span>

<h3>Description</h3>

<p>This function establishes a connection to a generative AI model through a generative AI object.
It generates a text response based on the provided prompt.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>txt.image(genai.object, prompt, image.path, verbose = FALSE, config = list())
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="txt.image_+3A_genai.object">genai.object</code></td>
<td>
<p>A generative AI object containing necessary and correct information.</p>
</td></tr>
<tr><td><code id="txt.image_+3A_prompt">prompt</code></td>
<td>
<p>A character string representing the query for text generation.</p>
</td></tr>
<tr><td><code id="txt.image_+3A_image.path">image.path</code></td>
<td>
<p>A character string representing the path to the image. It should be a link
starting with <code>https</code>/<code>http</code> or a local directory path to an image.</p>
</td></tr>
<tr><td><code id="txt.image_+3A_verbose">verbose</code></td>
<td>
<p>Optional. Default to <code>FALSE</code>. A boolean value determining whether or not to print
out the details of the text request.</p>
</td></tr>
<tr><td><code id="txt.image_+3A_config">config</code></td>
<td>
<p>Optional. Default to <code>list()</code>. A list of configuration parameters for text generation.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Providing accurate and valid information for each argument is crucial for successful text
generation by the generative AI model. If any parameter is incorrect, the function responds with an
error message based on the API feedback. To view all supported generative AI models, use the
function <code><a href="#topic+available.models">available.models</a></code>.
</p>
<p>For <strong>Google Generative AI</strong> models, available configurations are as follows. For more detail,
please refer
to <code>https://ai.google.dev/api/rest/v1/HarmCategory</code>,
<code>https://ai.google.dev/api/rest/v1/SafetySetting</code>, and
<code>https://ai.google.dev/api/rest/v1/GenerationConfig</code>.
</p>

<ul>
<li> <p><code>harm.category.dangerous.content</code>
</p>
<p>Optional. An integer, from 1 to 5 inclusive, representing the threshold for dangerous content,
with a higher value representing a lower probability of being blocked.
</p>
</li>
<li> <p><code>harm.category.harassment</code>
</p>
<p>Optional. An integer, from 1 to 5 inclusive, representing the threshold for harasment content,
with a higher value representing a lower probability of being blocked.
</p>
</li>
<li> <p><code>harm.category.hate.speech</code>
</p>
<p>Optional. An integer, from 1 to 5 inclusive, representing the threshold for hate speech and
content, with a higher value representing a lower probability of being blocked.
</p>
</li>
<li> <p><code>harm.category.sexually.explicit</code>
</p>
<p>Optional. An integer, from 1 to 5 inclusive, representing the threshold for sexually explicit
content, with a higher value representing a lower probability of being blocked.
</p>
</li>
<li> <p><code>stop.sequences</code>
</p>
<p>Optional. A list of character sequences (up to 5) that will stop output generation. If specified,
the API will stop at the first appearance of a stop sequence. The stop sequence will not be
included as part of the response.
</p>
</li>
<li> <p><code>max.output.tokens</code>
</p>
<p>Optional. An integer, value varies by model, representing maximum number of tokens to include
in a candidate.
</p>
</li>
<li> <p><code>temperature</code>
</p>
<p>Optional. A number, from 0.0 to 1.0 inclusive, controlling the randomness of the output.
</p>
</li>
<li> <p><code>top.p</code>
</p>
<p>Optional. A number, value varies by model, representing maximum cumulative probability of tokens
to consider when sampling.
</p>
</li>
<li> <p><code>top.k</code>
</p>
<p>Optional. A number, value varies by model, representing maximum number of tokens to consider when sampling.
</p>
</li></ul>

<p>For <strong>OpenAI</strong> models, available configurations are as follows. For more detail, please refer to
<code>https://platform.openai.com/docs/api-reference/chat/create</code>.
</p>

<ul>
<li> <p><code>frequency.penalty</code>
</p>
<p>Optional. A number from -2.0 to 2.0 inclusive. Positive values penalize new tokens based on their
existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
</p>
</li>
<li> <p><code>logit.bias</code>
</p>
<p>Optional. A map. Modify the likelihood of specified tokens appearing in the completion. Accepts a JSON object
that maps tokens (specified by their token ID in the tokenizer) to an associated bias value from -100 to
100. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact
effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection;
values like -100 or 100 should result in a ban or exclusive selection of the relevant token.
</p>
</li>
<li> <p><code>logprobs</code>
</p>
<p>Optional. A boolean value. Whether to return log probabilities of the output tokens or not. If true, returns the log
probabilities of each output token returned in the content of message
</p>
</li>
<li> <p><code>top.logprobs</code>
</p>
<p>Optional. An integer between 0 and 5 specifying the number of most likely tokens to return at each token
position, each with an associated log probability. <code>logprobs</code> must be set to <code>TRUE</code> if this
parameter is used.
</p>
</li>
<li> <p><code>max.tokens</code>
</p>
<p>Optional. An integer. The maximum number of tokens that can be generated in the chat completion. The total length of
input tokens and generated tokens is limited by the model's context length.
</p>
</li>
<li> <p><code>presence.penalty</code>
</p>
<p>Optional. A Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear
in the text so far, increasing the model's likelihood to talk about new topics.
</p>
</li>
<li> <p><code>response.format</code>
</p>
<p>Optional. An object specifying the format that the model must output. Compatible with GPT-4 Turbo and
all GPT-3.5 Turbo models newer than <code>gpt-3.5-turbo-1106</code>.
</p>
</li>
<li> <p><code>seed</code>
</p>
<p>Optional. An integer. If specified, our system will make a best effort to sample deterministically, such that repeated
requests with the same seed and parameters should return the same result.
</p>
</li>
<li> <p><code>stop</code>
</p>
<p>Optional. A character string or list contains up to 4 sequences where the API will stop generating further tokens.
</p>
</li>
<li> <p><code>temperature</code>
</p>
<p>Optional. A number. What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output
more random, while lower values like 0.2 will make it more focused and deterministic.
</p>
</li>
<li> <p><code>top.p</code>
</p>
<p>Optional. A number. An alternative to sampling with temperature, called nucleus sampling, where the model considers
the results of the tokens with <code>top.p</code> probability mass. So 0.1 means only the tokens comprising the top
10
</p>
</li>
<li> <p><code>tools</code>
</p>
<p>Optional. A list of tools the model may call. Currently, only functions are supported as a tool. Use this
to provide a list of functions the model may generate JSON inputs for.
</p>
</li>
<li> <p><code>tool.choice</code>
</p>
<p>Optional. A character string or object. Controls which (if any) function is called by the model. <code>none</code> means
the model will not call a function and instead generates a message. <code>auto</code> means the model can pick
between generating a message or calling a function.
</p>
</li>
<li> <p><code>user</code>
</p>
<p>Optional. A character string. A unique identifier representing your end-user, which can help OpenAI to monitor
and detect abuse.
</p>
</li></ul>



<h3>Value</h3>

<p>If successful, a text response will be returned. If the API response indicates
an error, the function halts execution and provides an error message.
</p>


<h3>See Also</h3>

<p><a href="https://genai.gd.edu.kg/r/documentation/">GenAI - R Package &quot;GenAI&quot; Documentation</a>
</p>
<p><a href="https://colab.research.google.com/github/GitData-GA/GenAI/blob/gh-pages/r/example/txt_image.ipynb">Live Demo in Colab</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Assuming there is a GenAI object named 'genai.model' supporting this
# function, an image in your current directory named 'example.png', and
# an online image 'https://example.com/example.png/', please refer to
# the "Live Demo in Colab" above for real examples. The following examples
# are just some basic guidelines.

# Method 1 (recommended): use the pipe operator "%&gt;%"
genai.model %&gt;%
  txt.image(prompt = "Please describe the following image.",
            image.path = "https://example.com/example.png/") %&gt;%
  cat()

# Method 2: use the reference operator "$"
cat(genai.model$txt.image(prompt = "Please describe the following image.",
                          image.path = "https://example.com/example.png/"))

# Method 3: use the function txt.image() directly
cat(txt.image(genai.object = genai.model,
              prompt = "Please describe the following image.",
              image.path = "example.png"))

## End(Not run)

</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
