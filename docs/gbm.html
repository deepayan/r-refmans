<!DOCTYPE html><html><head><title>Help for package gbm</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {gbm}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#gbm-package'><p>Generalized Boosted Regression Models (GBMs)</p></a></li>
<li><a href='#basehaz.gbm'><p>Baseline hazard function</p></a></li>
<li><a href='#calibrate.plot'><p>Calibration plot</p></a></li>
<li><a href='#gbm'><p>Generalized Boosted Regression Modeling (GBM)</p></a></li>
<li><a href='#gbm.fit'><p>Generalized Boosted Regression Modeling (GBM)</p></a></li>
<li><a href='#gbm.more'><p>Generalized Boosted Regression Modeling (GBM)</p></a></li>
<li><a href='#gbm.object'><p>Generalized Boosted Regression Model Object</p></a></li>
<li><a href='#gbm.perf'><p>GBM performance</p></a></li>
<li><a href='#gbm.roc.area'><p>Compute Information Retrieval measures.</p></a></li>
<li><a href='#gbmCrossVal'><p>Cross-validate a gbm</p></a></li>
<li><a href='#guessDist'><p>gbm internal functions</p></a></li>
<li><a href='#interact.gbm'><p>Estimate the strength of interaction effects</p></a></li>
<li><a href='#plot.gbm'><p>Marginal plots of fitted gbm objects</p></a></li>
<li><a href='#predict.gbm'><p>Predict method for GBM Model Fits</p></a></li>
<li><a href='#pretty.gbm.tree'><p>Print gbm tree components</p></a></li>
<li><a href='#print.gbm'><p>Print model summary</p></a></li>
<li><a href='#quantile.rug'><p>Quantile rug plot</p></a></li>
<li><a href='#reconstructGBMdata'><p>Reconstruct a GBM's Source Data</p></a></li>
<li><a href='#relative.influence'><p>Methods for estimating relative influence</p></a></li>
<li><a href='#summary.gbm'><p>Summary of a gbm object</p></a></li>
<li><a href='#test.gbm'><p>Test the <code>gbm</code> package.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Version:</td>
<td>2.2.2</td>
</tr>
<tr>
<td>Title:</td>
<td>Generalized Boosted Regression Models</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 2.9.0)</td>
</tr>
<tr>
<td>Imports:</td>
<td>lattice, parallel, survival</td>
</tr>
<tr>
<td>Suggests:</td>
<td>covr, gridExtra, knitr, pdp, RUnit, splines, tinytest, vip,
viridis</td>
</tr>
<tr>
<td>Description:</td>
<td>An implementation of extensions to Freund and Schapire's AdaBoost 
  algorithm and Friedman's gradient boosting machine. Includes regression 
  methods for least squares, absolute loss, t-distribution loss, quantile 
  regression, logistic, multinomial logistic, Poisson, Cox proportional hazards 
  partial likelihood, AdaBoost exponential loss, Huberized hinge loss, and 
  Learning to Rank measures (LambdaMart). Originally developed by Greg Ridgeway.
  Newer version available at github.com/gbm-developers/gbm3.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> | file LICENSE [expanded from: GPL (&ge; 2) | file LICENSE]</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/gbm-developers/gbm">https://github.com/gbm-developers/gbm</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/gbm-developers/gbm/issues">https://github.com/gbm-developers/gbm/issues</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.1</td>
</tr>
<tr>
<td>VignetteBuilder:</td>
<td>knitr</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-06-26 12:33:00 UTC; greg_</td>
</tr>
<tr>
<td>Author:</td>
<td>Greg Ridgeway <a href="https://orcid.org/0000-0001-6911-0804"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre],
  Daniel Edwards [ctb],
  Brian Kriegler [ctb],
  Stefan Schroedl [ctb],
  Harry Southworth [ctb],
  Brandon Greenwell <a href="https://orcid.org/0000-0002-8120-0084"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [ctb],
  Bradley Boehmke <a href="https://orcid.org/0000-0002-3611-8516"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [ctb],
  Jay Cunningham [ctb],
  GBM Developers [aut] (https://github.com/gbm-developers)</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Greg Ridgeway &lt;gridge@upenn.edu&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-06-28 06:20:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='gbm-package'>Generalized Boosted Regression Models (GBMs)</h2><span id='topic+gbm-package'></span>

<h3>Description</h3>

<p>This package implements extensions to Freund and Schapire's AdaBoost
algorithm and J. Friedman's gradient boosting machine. Includes regression
methods for least squares, absolute loss, logistic, Poisson, Cox
proportional hazards partial likelihood, multinomial, t-distribution,
AdaBoost exponential loss, Learning to Rank, and Huberized hinge loss.
This gbm package is no longer under further development. Consider
https://github.com/gbm-developers/gbm3 for the latest version.
</p>


<h3>Details</h3>

<p>Further information is available in vignette: 
<code>browseVignettes(package = "gbm")</code>
</p>


<h3>Author(s)</h3>

<p>Greg Ridgeway <a href="mailto:gridge@upenn.edu">gridge@upenn.edu</a> with contributions by
Daniel Edwards, Brian Kriegler, Stefan Schroedl, Harry Southworth,
and Brandon Greenwell
</p>


<h3>References</h3>

<p>Y. Freund and R.E. Schapire (1997) &ldquo;A decision-theoretic
generalization of on-line learning and an application to boosting,&rdquo;
<em>Journal of Computer and System Sciences,</em> 55(1):119-139.
</p>
<p>G. Ridgeway (1999). &ldquo;The state of boosting,&rdquo; <em>Computing Science
and Statistics</em> 31:172-181.
</p>
<p>J.H. Friedman, T. Hastie, R. Tibshirani (2000). &ldquo;Additive Logistic
Regression: a Statistical View of Boosting,&rdquo; <em>Annals of Statistics</em>
28(2):337-374.
</p>
<p>J.H. Friedman (2001). &ldquo;Greedy Function Approximation: A Gradient
Boosting Machine,&rdquo; <em>Annals of Statistics</em> 29(5):1189-1232.
</p>
<p>J.H. Friedman (2002). &ldquo;Stochastic Gradient Boosting,&rdquo;
<em>Computational Statistics and Data Analysis</em> 38(4):367-378.
</p>
<p>The <a href="https://jerryfriedman.su.domains/R-MART.html">MART</a> website.
</p>


<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://github.com/gbm-developers/gbm">https://github.com/gbm-developers/gbm</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/gbm-developers/gbm/issues">https://github.com/gbm-developers/gbm/issues</a>
</p>
</li></ul>


<hr>
<h2 id='basehaz.gbm'>Baseline hazard function</h2><span id='topic+basehaz.gbm'></span>

<h3>Description</h3>

<p>Computes the Breslow estimator of the baseline hazard function for a
proportional hazard regression model.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>basehaz.gbm(t, delta, f.x, t.eval = NULL, smooth = FALSE, cumulative = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="basehaz.gbm_+3A_t">t</code></td>
<td>
<p>The survival times.</p>
</td></tr>
<tr><td><code id="basehaz.gbm_+3A_delta">delta</code></td>
<td>
<p>The censoring indicator.</p>
</td></tr>
<tr><td><code id="basehaz.gbm_+3A_f.x">f.x</code></td>
<td>
<p>The predicted values of the regression model on the log hazard
scale.</p>
</td></tr>
<tr><td><code id="basehaz.gbm_+3A_t.eval">t.eval</code></td>
<td>
<p>Values at which the baseline hazard will be evaluated.</p>
</td></tr>
<tr><td><code id="basehaz.gbm_+3A_smooth">smooth</code></td>
<td>
<p>If <code>TRUE</code> <code>basehaz.gbm</code> will smooth the estimated
baseline hazard using Friedman's super smoother <code><a href="stats.html#topic+supsmu">supsmu</a></code>.</p>
</td></tr>
<tr><td><code id="basehaz.gbm_+3A_cumulative">cumulative</code></td>
<td>
<p>If <code>TRUE</code> the cumulative survival function will be
computed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The proportional hazard model assumes h(t|x)=lambda(t)*exp(f(x)).
<code><a href="#topic+gbm">gbm</a></code> can estimate the f(x) component via partial likelihood.
After estimating f(x), <code>basehaz.gbm</code> can compute the a nonparametric
estimate of lambda(t).
</p>


<h3>Value</h3>

<p>A vector of length equal to the length of t (or of length
<code>t.eval</code> if <code>t.eval</code> is not <code>NULL</code>) containing the baseline
hazard evaluated at t (or at <code>t.eval</code> if <code>t.eval</code> is not
<code>NULL</code>). If <code>cumulative</code> is set to <code>TRUE</code> then the returned
vector evaluates the cumulative hazard function at those values.
</p>


<h3>Author(s)</h3>

<p>Greg Ridgeway <a href="mailto:gregridgeway@gmail.com">gregridgeway@gmail.com</a>
</p>


<h3>References</h3>

<p>N. Breslow (1972). &quot;Discussion of 'Regression Models and
Life-Tables' by D.R. Cox,&quot; Journal of the Royal Statistical Society, Series
B, 34(2):216-217.
</p>
<p>N. Breslow (1974). &quot;Covariance analysis of censored survival data,&quot;
Biometrics 30:89-99.
</p>


<h3>See Also</h3>

<p><code><a href="survival.html#topic+survfit">survfit</a></code>, <code><a href="#topic+gbm">gbm</a></code>
</p>

<hr>
<h2 id='calibrate.plot'>Calibration plot</h2><span id='topic+calibrate.plot'></span>

<h3>Description</h3>

<p>An experimental diagnostic tool that plots the fitted values versus the
actual average values. Currently only available when
<code>distribution = "bernoulli"</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calibrate.plot(
  y,
  p,
  distribution = "bernoulli",
  replace = TRUE,
  line.par = list(col = "black"),
  shade.col = "lightyellow",
  shade.density = NULL,
  rug.par = list(side = 1),
  xlab = "Predicted value",
  ylab = "Observed average",
  xlim = NULL,
  ylim = NULL,
  knots = NULL,
  df = 6,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="calibrate.plot_+3A_y">y</code></td>
<td>
<p>The outcome 0-1 variable.</p>
</td></tr>
<tr><td><code id="calibrate.plot_+3A_p">p</code></td>
<td>
<p>The predictions estimating E(y|x).</p>
</td></tr>
<tr><td><code id="calibrate.plot_+3A_distribution">distribution</code></td>
<td>
<p>The loss function used in creating <code>p</code>.
<code>bernoulli</code> and <code>poisson</code> are currently the only special options.
All others default to squared error assuming <code>gaussian</code>.</p>
</td></tr>
<tr><td><code id="calibrate.plot_+3A_replace">replace</code></td>
<td>
<p>Determines whether this plot will replace or overlay the
current plot. <code>replace=FALSE</code> is useful for comparing the calibration
of several methods.</p>
</td></tr>
<tr><td><code id="calibrate.plot_+3A_line.par">line.par</code></td>
<td>
<p>Graphics parameters for the line.</p>
</td></tr>
<tr><td><code id="calibrate.plot_+3A_shade.col">shade.col</code></td>
<td>
<p>Color for shading the 2 SE region. <code>shade.col=NA</code>
implies no 2 SE region.</p>
</td></tr>
<tr><td><code id="calibrate.plot_+3A_shade.density">shade.density</code></td>
<td>
<p>The <code>density</code> parameter for <code><a href="graphics.html#topic+polygon">polygon</a></code>.</p>
</td></tr>
<tr><td><code id="calibrate.plot_+3A_rug.par">rug.par</code></td>
<td>
<p>Graphics parameters passed to <code><a href="graphics.html#topic+rug">rug</a></code>.</p>
</td></tr>
<tr><td><code id="calibrate.plot_+3A_xlab">xlab</code></td>
<td>
<p>x-axis label corresponding to the predicted values.</p>
</td></tr>
<tr><td><code id="calibrate.plot_+3A_ylab">ylab</code></td>
<td>
<p>y-axis label corresponding to the observed average.</p>
</td></tr>
<tr><td><code id="calibrate.plot_+3A_xlim">xlim</code>, <code id="calibrate.plot_+3A_ylim">ylim</code></td>
<td>
<p>x- and y-axis limits. If not specified te function will
select limits.</p>
</td></tr>
<tr><td><code id="calibrate.plot_+3A_knots">knots</code>, <code id="calibrate.plot_+3A_df">df</code></td>
<td>
<p>These parameters are passed directly to
<code><a href="splines.html#topic+ns">ns</a></code> for constructing a natural spline smoother for the
calibration curve.</p>
</td></tr>
<tr><td><code id="calibrate.plot_+3A_...">...</code></td>
<td>
<p>Additional optional arguments to be passed onto 
<code><a href="graphics.html#topic+plot.default">plot</a></code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Uses natural splines to estimate E(y|p). Well-calibrated predictions imply
that E(y|p) = p. The plot also includes a pointwise 95
</p>


<h3>Value</h3>

<p>No return values.
</p>


<h3>Author(s)</h3>

<p>Greg Ridgeway <a href="mailto:gregridgeway@gmail.com">gregridgeway@gmail.com</a>
</p>


<h3>References</h3>

<p>J.F. Yates (1982). &quot;External correspondence: decomposition of
the mean probability score,&quot; Organisational Behaviour and Human Performance
30:132-156.
</p>
<p>D.J. Spiegelhalter (1986). &quot;Probabilistic Prediction in Patient Management
and Clinical Trials,&quot; Statistics in Medicine 5:421-433.
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Don't want R CMD check to think there is a dependency on rpart
# so comment out the example
#library(rpart)
#data(kyphosis)
#y &lt;- as.numeric(kyphosis$Kyphosis)-1
#x &lt;- kyphosis$Age
#glm1 &lt;- glm(y~poly(x,2),family=binomial)
#p &lt;- predict(glm1,type="response")
#calibrate.plot(y, p, xlim=c(0,0.6), ylim=c(0,0.6))
</code></pre>

<hr>
<h2 id='gbm'>Generalized Boosted Regression Modeling (GBM)</h2><span id='topic+gbm'></span>

<h3>Description</h3>

<p>Fits generalized boosted regression models. For technical details, see the 
vignette: <code>utils::browseVignettes("gbm")</code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gbm(
  formula = formula(data),
  distribution = "bernoulli",
  data = list(),
  weights,
  var.monotone = NULL,
  n.trees = 100,
  interaction.depth = 1,
  n.minobsinnode = 10,
  shrinkage = 0.1,
  bag.fraction = 0.5,
  train.fraction = 1,
  cv.folds = 0,
  keep.data = TRUE,
  verbose = FALSE,
  class.stratify.cv = NULL,
  n.cores = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gbm_+3A_formula">formula</code></td>
<td>
<p>A symbolic description of the model to be fit. The formula
may include an offset term (e.g. y~offset(n)+x). If 
<code>keep.data = FALSE</code> in the initial call to <code>gbm</code> then it is the 
user's responsibility to resupply the offset to <code><a href="#topic+gbm.more">gbm.more</a></code>.</p>
</td></tr>
<tr><td><code id="gbm_+3A_distribution">distribution</code></td>
<td>
<p>Either a character string specifying the name of the
distribution to use or a list with a component <code>name</code> specifying the
distribution and any additional parameters needed. If not specified,
<code>gbm</code> will try to guess: if the response has only 2 unique values,
bernoulli is assumed; otherwise, if the response is a factor, multinomial is
assumed; otherwise, if the response has class <code>"Surv"</code>, coxph is 
assumed; otherwise, gaussian is assumed.
</p>
<p>Currently available options are <code>"gaussian"</code> (squared error),
<code>"laplace"</code> (absolute loss), <code>"tdist"</code> (t-distribution loss),
<code>"bernoulli"</code> (logistic regression for 0-1 outcomes), 
<code>"huberized"</code> (huberized hinge loss for 0-1 outcomes),
<code>"adaboost"</code> (the AdaBoost exponential loss for 0-1 outcomes),
<code>"poisson"</code> (count outcomes), <code>"coxph"</code> (right censored 
observations), <code>"quantile"</code>, or <code>"pairwise"</code> (ranking measure 
using the LambdaMart algorithm).
</p>
<p>If quantile regression is specified, <code>distribution</code> must be a list of
the form <code>list(name = "quantile", alpha = 0.25)</code> where <code>alpha</code> is 
the quantile to estimate. The current version's quantile regression method 
does not handle non-constant weights and will stop.
</p>
<p>If <code>"tdist"</code> is specified, the default degrees of freedom is 4 and 
this can be controlled by specifying 
<code>distribution = list(name = "tdist", df = DF)</code> where <code>DF</code> is your 
chosen degrees of freedom.
</p>
<p>If &quot;pairwise&quot; regression is specified, <code>distribution</code> must be a list of
the form <code>list(name="pairwise",group=...,metric=...,max.rank=...)</code>
(<code>metric</code> and <code>max.rank</code> are optional, see below). <code>group</code> is
a character vector with the column names of <code>data</code> that jointly
indicate the group an instance belongs to (typically a query in Information
Retrieval applications). For training, only pairs of instances from the same
group and with different target labels can be considered. <code>metric</code> is
the IR measure to use, one of 
</p>
 
<dl>
<dt>list(&quot;conc&quot;)</dt><dd><p>Fraction of concordant pairs; for binary labels, this 
is equivalent to the Area under the ROC Curve</p>
</dd>
<dt>:</dt><dd><p>Fraction of concordant pairs; for binary labels, this is 
equivalent to the Area under the ROC Curve</p>
</dd> 
<dt>list(&quot;mrr&quot;)</dt><dd><p>Mean reciprocal rank of the highest-ranked positive 
instance</p>
</dd>
<dt>:</dt><dd><p>Mean reciprocal rank of the highest-ranked positive instance</p>
</dd>
<dt>list(&quot;map&quot;)</dt><dd><p>Mean average precision, a generalization of <code>mrr</code> 
to multiple positive instances</p>
</dd><dt>:</dt><dd><p>Mean average precision, a
generalization of <code>mrr</code> to multiple positive instances</p>
</dd>
<dt>list(&quot;ndcg:&quot;)</dt><dd><p>Normalized discounted cumulative gain. The score is 
the weighted sum (DCG) of the user-supplied target values, weighted 
by log(rank+1), and normalized to the maximum achievable value. This 
is the default if the user did not specify a metric.</p>
</dd> 
</dl>

<p><code>ndcg</code> and <code>conc</code> allow arbitrary target values, while binary
targets {0,1} are expected for <code>map</code> and <code>mrr</code>. For <code>ndcg</code>
and <code>mrr</code>, a cut-off can be chosen using a positive integer parameter
<code>max.rank</code>. If left unspecified, all ranks are taken into account.
</p>
<p>Note that splitting of instances into training and validation sets follows
group boundaries and therefore only approximates the specified
<code>train.fraction</code> ratio (the same applies to cross-validation folds).
Internally, queries are randomly shuffled before training, to avoid bias.
</p>
<p>Weights can be used in conjunction with pairwise metrics, however it is
assumed that they are constant for instances from the same group.
</p>
<p>For details and background on the algorithm, see e.g. Burges (2010).</p>
</td></tr>
<tr><td><code id="gbm_+3A_data">data</code></td>
<td>
<p>an optional data frame containing the variables in the model. By
default the variables are taken from <code>environment(formula)</code>, typically
the environment from which <code>gbm</code> is called. If <code>keep.data=TRUE</code> in
the initial call to <code>gbm</code> then <code>gbm</code> stores a copy with the
object. If <code>keep.data=FALSE</code> then subsequent calls to
<code><a href="#topic+gbm.more">gbm.more</a></code> must resupply the same dataset. It becomes the user's
responsibility to resupply the same data at this point.</p>
</td></tr>
<tr><td><code id="gbm_+3A_weights">weights</code></td>
<td>
<p>an optional vector of weights to be used in the fitting
process. Must be positive but do not need to be normalized. If
<code>keep.data=FALSE</code> in the initial call to <code>gbm</code> then it is the
user's responsibility to resupply the weights to <code><a href="#topic+gbm.more">gbm.more</a></code>.</p>
</td></tr>
<tr><td><code id="gbm_+3A_var.monotone">var.monotone</code></td>
<td>
<p>an optional vector, the same length as the number of
predictors, indicating which variables have a monotone increasing (+1),
decreasing (-1), or arbitrary (0) relationship with the outcome.</p>
</td></tr>
<tr><td><code id="gbm_+3A_n.trees">n.trees</code></td>
<td>
<p>Integer specifying the total number of trees to fit. This is 
equivalent to the number of iterations and the number of basis functions in 
the additive expansion. Default is 100.</p>
</td></tr>
<tr><td><code id="gbm_+3A_interaction.depth">interaction.depth</code></td>
<td>
<p>Integer specifying the maximum depth of each tree 
(i.e., the highest level of variable interactions allowed). A value of 1 
implies an additive model, a value of 2 implies a model with up to 2-way 
interactions, etc. Default is 1.</p>
</td></tr>
<tr><td><code id="gbm_+3A_n.minobsinnode">n.minobsinnode</code></td>
<td>
<p>Integer specifying the minimum number of observations 
in the terminal nodes of the trees. Note that this is the actual number of 
observations, not the total weight.</p>
</td></tr>
<tr><td><code id="gbm_+3A_shrinkage">shrinkage</code></td>
<td>
<p>a shrinkage parameter applied to each tree in the
expansion. Also known as the learning rate or step-size reduction; 0.001 to 
0.1 usually work, but a smaller learning rate typically requires more trees.
Default is 0.1.</p>
</td></tr>
<tr><td><code id="gbm_+3A_bag.fraction">bag.fraction</code></td>
<td>
<p>the fraction of the training set observations randomly
selected to propose the next tree in the expansion. This introduces
randomnesses into the model fit. If <code>bag.fraction</code> &lt; 1 then running the
same model twice will result in similar but different fits. <code>gbm</code> uses
the R random number generator so <code>set.seed</code> can ensure that the model
can be reconstructed. Preferably, the user can save the returned
<code><a href="#topic+gbm.object">gbm.object</a></code> using <code><a href="base.html#topic+save">save</a></code>. Default is 0.5.</p>
</td></tr>
<tr><td><code id="gbm_+3A_train.fraction">train.fraction</code></td>
<td>
<p>The first <code>train.fraction * nrows(data)</code>
observations are used to fit the <code>gbm</code> and the remainder are used for
computing out-of-sample estimates of the loss function.</p>
</td></tr>
<tr><td><code id="gbm_+3A_cv.folds">cv.folds</code></td>
<td>
<p>Number of cross-validation folds to perform. If
<code>cv.folds</code>&gt;1 then <code>gbm</code>, in addition to the usual fit, will
perform a cross-validation, calculate an estimate of generalization error
returned in <code>cv.error</code>.</p>
</td></tr>
<tr><td><code id="gbm_+3A_keep.data">keep.data</code></td>
<td>
<p>a logical variable indicating whether to keep the data and
an index of the data stored with the object. Keeping the data and index
makes subsequent calls to <code><a href="#topic+gbm.more">gbm.more</a></code> faster at the cost of
storing an extra copy of the dataset.</p>
</td></tr>
<tr><td><code id="gbm_+3A_verbose">verbose</code></td>
<td>
<p>Logical indicating whether or not to print out progress and 
performance indicators (<code>TRUE</code>). If this option is left unspecified for 
<code>gbm.more</code>, then it uses <code>verbose</code> from <code>object</code>. Default is
<code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="gbm_+3A_class.stratify.cv">class.stratify.cv</code></td>
<td>
<p>Logical indicating whether or not the 
cross-validation should be stratified by class. Defaults to <code>TRUE</code> for
<code>distribution = "multinomial"</code> and is only implemented for
<code>"multinomial"</code> and <code>"bernoulli"</code>. The purpose of stratifying the
cross-validation is to help avoiding situations in which training sets do
not contain all classes.</p>
</td></tr>
<tr><td><code id="gbm_+3A_n.cores">n.cores</code></td>
<td>
<p>The number of CPU cores to use. The cross-validation loop
will attempt to send different CV folds off to different cores. If
<code>n.cores</code> is not specified by the user, it is guessed using the
<code>detectCores</code> function in the <code>parallel</code> package. Note that the
documentation for <code>detectCores</code> makes clear that it is not failsafe and
could return a spurious number of available cores.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>gbm.fit</code> provides the link between R and the C++ gbm engine.
<code>gbm</code> is a front-end to <code>gbm.fit</code> that uses the familiar R
modeling formulas. However, <code><a href="stats.html#topic+model.frame">model.frame</a></code> is very slow if
there are many predictor variables. For power-users with many variables use
<code>gbm.fit</code>. For general practice <code>gbm</code> is preferable.
</p>
<p>This package implements the generalized boosted modeling framework. Boosting
is the process of iteratively adding basis functions in a greedy fashion so
that each additional basis function further reduces the selected loss
function. This implementation closely follows Friedman's Gradient Boosting
Machine (Friedman, 2001).
</p>
<p>In addition to many of the features documented in the Gradient Boosting
Machine, <code>gbm</code> offers additional features including the out-of-bag
estimator for the optimal number of iterations, the ability to store and
manipulate the resulting <code>gbm</code> object, and a variety of other loss
functions that had not previously had associated boosting algorithms,
including the Cox partial likelihood for censored data, the poisson
likelihood for count outcomes, and a gradient boosting implementation to
minimize the AdaBoost exponential loss function. This gbm package is no
longer under further development. Consider
https://github.com/gbm-developers/gbm3 for the latest version.
</p>


<h3>Value</h3>

<p>A <code><a href="#topic+gbm.object">gbm.object</a></code> object.
</p>


<h3>Author(s)</h3>

<p>Greg Ridgeway <a href="mailto:gregridgeway@gmail.com">gregridgeway@gmail.com</a>
</p>
<p>Quantile regression code developed by Brian Kriegler
<a href="mailto:bk@stat.ucla.edu">bk@stat.ucla.edu</a>
</p>
<p>t-distribution, and multinomial code developed by Harry Southworth and
Daniel Edwards
</p>
<p>Pairwise code developed by Stefan Schroedl <a href="mailto:schroedl@a9.com">schroedl@a9.com</a>
</p>


<h3>References</h3>

<p>Y. Freund and R.E. Schapire (1997) &ldquo;A decision-theoretic
generalization of on-line learning and an application to boosting,&rdquo;
<em>Journal of Computer and System Sciences,</em> 55(1):119-139.
</p>
<p>G. Ridgeway (1999). &ldquo;The state of boosting,&rdquo; <em>Computing Science
and Statistics</em> 31:172-181.
</p>
<p>J.H. Friedman, T. Hastie, R. Tibshirani (2000). &ldquo;Additive Logistic
Regression: a Statistical View of Boosting,&rdquo; <em>Annals of Statistics</em>
28(2):337-374.
</p>
<p>J.H. Friedman (2001). &ldquo;Greedy Function Approximation: A Gradient
Boosting Machine,&rdquo; <em>Annals of Statistics</em> 29(5):1189-1232.
</p>
<p>J.H. Friedman (2002). &ldquo;Stochastic Gradient Boosting,&rdquo;
<em>Computational Statistics and Data Analysis</em> 38(4):367-378.
</p>
<p>B. Kriegler (2007). Cost-Sensitive Stochastic Gradient Boosting Within a 
Quantitative Regression Framework. Ph.D. Dissertation. University of 
California at Los Angeles, Los Angeles, CA, USA. Advisor(s) Richard A. Berk. 
<a href="https://dl.acm.org/doi/book/10.5555/1354603">https://dl.acm.org/doi/book/10.5555/1354603</a>.
</p>
<p>C. Burges (2010). &ldquo;From RankNet to LambdaRank to LambdaMART: An
Overview,&rdquo; Microsoft Research Technical Report MSR-TR-2010-82.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+gbm.object">gbm.object</a></code>, <code><a href="#topic+gbm.perf">gbm.perf</a></code>, 
<code><a href="#topic+plot.gbm">plot.gbm</a></code>, <code><a href="#topic+predict.gbm">predict.gbm</a></code>, <code><a href="#topic+summary.gbm">summary.gbm</a></code>, 
and <code><a href="#topic+pretty.gbm.tree">pretty.gbm.tree</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#
# A least squares regression example 
#

# Simulate data
set.seed(101)  # for reproducibility
N &lt;- 1000
X1 &lt;- runif(N)
X2 &lt;- 2 * runif(N)
X3 &lt;- ordered(sample(letters[1:4], N, replace = TRUE), levels = letters[4:1])
X4 &lt;- factor(sample(letters[1:6], N, replace = TRUE))
X5 &lt;- factor(sample(letters[1:3], N, replace = TRUE))
X6 &lt;- 3 * runif(N) 
mu &lt;- c(-1, 0, 1, 2)[as.numeric(X3)]
SNR &lt;- 10  # signal-to-noise ratio
Y &lt;- X1 ^ 1.5 + 2 * (X2 ^ 0.5) + mu
sigma &lt;- sqrt(var(Y) / SNR)
Y &lt;- Y + rnorm(N, 0, sigma)
X1[sample(1:N,size=500)] &lt;- NA  # introduce some missing values
X4[sample(1:N,size=300)] &lt;- NA  # introduce some missing values
data &lt;- data.frame(Y, X1, X2, X3, X4, X5, X6)

# Fit a GBM
set.seed(102)  # for reproducibility
gbm1 &lt;- gbm(Y ~ ., data = data, var.monotone = c(0, 0, 0, 0, 0, 0),
            distribution = "gaussian", n.trees = 100, shrinkage = 0.1,             
            interaction.depth = 3, bag.fraction = 0.5, train.fraction = 0.5,  
            n.minobsinnode = 10, cv.folds = 5, keep.data = TRUE, 
            verbose = FALSE, n.cores = 1)  

# Check performance using the out-of-bag (OOB) error; the OOB error typically
# underestimates the optimal number of iterations
best.iter &lt;- gbm.perf(gbm1, method = "OOB")
print(best.iter)

# Check performance using the 50% heldout test set
best.iter &lt;- gbm.perf(gbm1, method = "test")
print(best.iter)

# Check performance using 5-fold cross-validation
best.iter &lt;- gbm.perf(gbm1, method = "cv")
print(best.iter)

# Plot relative influence of each variable
par(mfrow = c(1, 2))
summary(gbm1, n.trees = 1)          # using first tree
summary(gbm1, n.trees = best.iter)  # using estimated best number of trees

# Compactly print the first and last trees for curiosity
print(pretty.gbm.tree(gbm1, i.tree = 1))
print(pretty.gbm.tree(gbm1, i.tree = gbm1$n.trees))

# Simulate new data
set.seed(103)  # for reproducibility
N &lt;- 1000
X1 &lt;- runif(N)
X2 &lt;- 2 * runif(N)
X3 &lt;- ordered(sample(letters[1:4], N, replace = TRUE))
X4 &lt;- factor(sample(letters[1:6], N, replace = TRUE))
X5 &lt;- factor(sample(letters[1:3], N, replace = TRUE))
X6 &lt;- 3 * runif(N) 
mu &lt;- c(-1, 0, 1, 2)[as.numeric(X3)]
Y &lt;- X1 ^ 1.5 + 2 * (X2 ^ 0.5) + mu + rnorm(N, 0, sigma)
data2 &lt;- data.frame(Y, X1, X2, X3, X4, X5, X6)

# Predict on the new data using the "best" number of trees; by default,
# predictions will be on the link scale
Yhat &lt;- predict(gbm1, newdata = data2, n.trees = best.iter, type = "link")

# least squares error
print(sum((data2$Y - Yhat)^2))

# Construct univariate partial dependence plots
plot(gbm1, i.var = 1, n.trees = best.iter)
plot(gbm1, i.var = 2, n.trees = best.iter)
plot(gbm1, i.var = "X3", n.trees = best.iter)  # can use index or name

# Construct bivariate partial dependence plots
plot(gbm1, i.var = 1:2, n.trees = best.iter)
plot(gbm1, i.var = c("X2", "X3"), n.trees = best.iter)
plot(gbm1, i.var = 3:4, n.trees = best.iter)

# Construct trivariate partial dependence plots
plot(gbm1, i.var = c(1, 2, 6), n.trees = best.iter, 
     continuous.resolution = 20)
plot(gbm1, i.var = 1:3, n.trees = best.iter)
plot(gbm1, i.var = 2:4, n.trees = best.iter)
plot(gbm1, i.var = 3:5, n.trees = best.iter)

# Add more (i.e., 100) boosting iterations to the ensemble
gbm2 &lt;- gbm.more(gbm1, n.new.trees = 100, verbose = FALSE)
</code></pre>

<hr>
<h2 id='gbm.fit'>Generalized Boosted Regression Modeling (GBM)</h2><span id='topic+gbm.fit'></span>

<h3>Description</h3>

<p>Workhorse function providing the link between R and the C++ gbm engine.
<code>gbm</code> is a front-end to <code>gbm.fit</code> that uses the familiar R
modeling formulas. However, <code><a href="stats.html#topic+model.frame">model.frame</a></code> is very slow if
there are many predictor variables. For power-users with many variables use
<code>gbm.fit</code>. For general practice <code>gbm</code> is preferable.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gbm.fit(
  x,
  y,
  offset = NULL,
  misc = NULL,
  distribution = "bernoulli",
  w = NULL,
  var.monotone = NULL,
  n.trees = 100,
  interaction.depth = 1,
  n.minobsinnode = 10,
  shrinkage = 0.001,
  bag.fraction = 0.5,
  nTrain = NULL,
  train.fraction = NULL,
  keep.data = TRUE,
  verbose = TRUE,
  var.names = NULL,
  response.name = "y",
  group = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gbm.fit_+3A_x">x</code></td>
<td>
<p>A data frame or matrix containing the predictor variables. The 
number of rows in <code>x</code> must be the same as the length of <code>y</code>.</p>
</td></tr>
<tr><td><code id="gbm.fit_+3A_y">y</code></td>
<td>
<p>A vector of outcomes. The number of rows in <code>x</code> must be the 
same as the length of <code>y</code>.</p>
</td></tr>
<tr><td><code id="gbm.fit_+3A_offset">offset</code></td>
<td>
<p>A vector of offset values.</p>
</td></tr>
<tr><td><code id="gbm.fit_+3A_misc">misc</code></td>
<td>
<p>An R object that is simply passed on to the gbm engine. It can be 
used for additional data for the specific distribution. Currently it is only 
used for passing the censoring indicator for the Cox proportional hazards 
model.</p>
</td></tr>
<tr><td><code id="gbm.fit_+3A_distribution">distribution</code></td>
<td>
<p>Either a character string specifying the name of the
distribution to use or a list with a component <code>name</code> specifying the
distribution and any additional parameters needed. If not specified,
<code>gbm</code> will try to guess: if the response has only 2 unique values,
bernoulli is assumed; otherwise, if the response is a factor, multinomial is
assumed; otherwise, if the response has class <code>"Surv"</code>, coxph is 
assumed; otherwise, gaussian is assumed.
</p>
<p>Currently available options are <code>"gaussian"</code> (squared error), 
<code>"laplace"</code> (absolute loss), <code>"tdist"</code> (t-distribution loss), 
<code>"bernoulli"</code> (logistic regression for 0-1 outcomes), 
<code>"huberized"</code> (huberized hinge loss for 0-1 outcomes), 
<code>"adaboost"</code> (the AdaBoost exponential loss for 0-1 outcomes),
<code>"poisson"</code> (count outcomes), <code>"coxph"</code> (right censored 
observations), <code>"quantile"</code>, or <code>"pairwise"</code> (ranking measure 
using the LambdaMart algorithm).
</p>
<p>If quantile regression is specified, <code>distribution</code> must be a list of
the form <code>list(name = "quantile", alpha = 0.25)</code> where <code>alpha</code> is 
the quantile to estimate. The current version's quantile regression method 
does not handle non-constant weights and will stop.
</p>
<p>If <code>"tdist"</code> is specified, the default degrees of freedom is 4 and 
this can be controlled by specifying 
<code>distribution = list(name = "tdist", df = DF)</code> where <code>DF</code> is your 
chosen degrees of freedom.
</p>
<p>If &quot;pairwise&quot; regression is specified, <code>distribution</code> must be a list of
the form <code>list(name="pairwise",group=...,metric=...,max.rank=...)</code>
(<code>metric</code> and <code>max.rank</code> are optional, see below). <code>group</code> is
a character vector with the column names of <code>data</code> that jointly
indicate the group an instance belongs to (typically a query in Information
Retrieval applications). For training, only pairs of instances from the same
group and with different target labels can be considered. <code>metric</code> is
the IR measure to use, one of 
</p>
 
<dl>
<dt>list(&quot;conc&quot;)</dt><dd><p>Fraction of concordant pairs; for binary labels, this 
is equivalent to the Area under the ROC Curve</p>
</dd>
<dt>:</dt><dd><p>Fraction of concordant pairs; for binary labels, this is 
equivalent to the Area under the ROC Curve</p>
</dd> 
<dt>list(&quot;mrr&quot;)</dt><dd><p>Mean reciprocal rank of the highest-ranked positive 
instance</p>
</dd>
<dt>:</dt><dd><p>Mean reciprocal rank of the highest-ranked positive instance</p>
</dd>
<dt>list(&quot;map&quot;)</dt><dd><p>Mean average precision, a generalization of <code>mrr</code> 
to multiple positive instances</p>
</dd><dt>:</dt><dd><p>Mean average precision, a
generalization of <code>mrr</code> to multiple positive instances</p>
</dd>
<dt>list(&quot;ndcg:&quot;)</dt><dd><p>Normalized discounted cumulative gain. The score is 
the weighted sum (DCG) of the user-supplied target values, weighted 
by log(rank+1), and normalized to the maximum achievable value. This 
is the default if the user did not specify a metric.</p>
</dd> 
</dl>

<p><code>ndcg</code> and <code>conc</code> allow arbitrary target values, while binary
targets {0,1} are expected for <code>map</code> and <code>mrr</code>. For <code>ndcg</code>
and <code>mrr</code>, a cut-off can be chosen using a positive integer parameter
<code>max.rank</code>. If left unspecified, all ranks are taken into account.
</p>
<p>Note that splitting of instances into training and validation sets follows
group boundaries and therefore only approximates the specified
<code>train.fraction</code> ratio (the same applies to cross-validation folds).
Internally, queries are randomly shuffled before training, to avoid bias.
</p>
<p>Weights can be used in conjunction with pairwise metrics, however it is
assumed that they are constant for instances from the same group.
</p>
<p>For details and background on the algorithm, see e.g. Burges (2010).</p>
</td></tr>
<tr><td><code id="gbm.fit_+3A_w">w</code></td>
<td>
<p>A vector of weights of the same length as the <code>y</code>.</p>
</td></tr>
<tr><td><code id="gbm.fit_+3A_var.monotone">var.monotone</code></td>
<td>
<p>an optional vector, the same length as the number of
predictors, indicating which variables have a monotone increasing (+1),
decreasing (-1), or arbitrary (0) relationship with the outcome.</p>
</td></tr>
<tr><td><code id="gbm.fit_+3A_n.trees">n.trees</code></td>
<td>
<p>the total number of trees to fit. This is equivalent to the
number of iterations and the number of basis functions in the additive
expansion.</p>
</td></tr>
<tr><td><code id="gbm.fit_+3A_interaction.depth">interaction.depth</code></td>
<td>
<p>The maximum depth of variable interactions. A value
of 1 implies an additive model, a value of 2 implies a model with up to 2-way 
interactions, etc. Default is <code>1</code>.</p>
</td></tr>
<tr><td><code id="gbm.fit_+3A_n.minobsinnode">n.minobsinnode</code></td>
<td>
<p>Integer specifying the minimum number of observations 
in the trees terminal nodes. Note that this is the actual number of 
observations not the total weight.</p>
</td></tr>
<tr><td><code id="gbm.fit_+3A_shrinkage">shrinkage</code></td>
<td>
<p>The shrinkage parameter applied to each tree in the
expansion. Also known as the learning rate or step-size reduction; 0.001 to 
0.1 usually work, but a smaller learning rate typically requires more trees.
Default is <code>0.1</code>.</p>
</td></tr>
<tr><td><code id="gbm.fit_+3A_bag.fraction">bag.fraction</code></td>
<td>
<p>The fraction of the training set observations randomly
selected to propose the next tree in the expansion. This introduces
randomnesses into the model fit. If <code>bag.fraction</code> &lt; 1 then running the
same model twice will result in similar but different fits. <code>gbm</code> uses
the R random number generator so <code>set.seed</code> can ensure that the model
can be reconstructed. Preferably, the user can save the returned
<code><a href="#topic+gbm.object">gbm.object</a></code> using <code><a href="base.html#topic+save">save</a></code>. Default is <code>0.5</code>.</p>
</td></tr>
<tr><td><code id="gbm.fit_+3A_ntrain">nTrain</code></td>
<td>
<p>An integer representing the number of cases on which to train.
This is the preferred way of specification for <code>gbm.fit</code>; The option
<code>train.fraction</code> in <code>gbm.fit</code> is deprecated and only maintained
for backward compatibility. These two parameters are mutually exclusive. If
both are unspecified, all data is used for training.</p>
</td></tr>
<tr><td><code id="gbm.fit_+3A_train.fraction">train.fraction</code></td>
<td>
<p>The first <code>train.fraction * nrows(data)</code>
observations are used to fit the <code>gbm</code> and the remainder are used for
computing out-of-sample estimates of the loss function.</p>
</td></tr>
<tr><td><code id="gbm.fit_+3A_keep.data">keep.data</code></td>
<td>
<p>Logical indicating whether or not to keep the data and an 
index of the data stored with the object. Keeping the data and index makes 
subsequent calls to <code><a href="#topic+gbm.more">gbm.more</a></code> faster at the cost of storing an 
extra copy of the dataset.</p>
</td></tr>
<tr><td><code id="gbm.fit_+3A_verbose">verbose</code></td>
<td>
<p>Logical indicating whether or not to print out progress and 
performance indicators (<code>TRUE</code>). If this option is left unspecified for 
<code>gbm.more</code>, then it uses <code>verbose</code> from <code>object</code>. Default is
<code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="gbm.fit_+3A_var.names">var.names</code></td>
<td>
<p>Vector of strings of length equal to the number of columns 
of <code>x</code> containing the names of the predictor variables.</p>
</td></tr>
<tr><td><code id="gbm.fit_+3A_response.name">response.name</code></td>
<td>
<p>Character string label for the response variable.</p>
</td></tr>
<tr><td><code id="gbm.fit_+3A_group">group</code></td>
<td>
<p>The <code>group</code> to use when <code>distribution = "pairwise"</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This package implements the generalized boosted modeling framework. Boosting
is the process of iteratively adding basis functions in a greedy fashion so
that each additional basis function further reduces the selected loss
function. This implementation closely follows Friedman's Gradient Boosting
Machine (Friedman, 2001).
</p>
<p>In addition to many of the features documented in the Gradient Boosting
Machine, <code>gbm</code> offers additional features including the out-of-bag
estimator for the optimal number of iterations, the ability to store and
manipulate the resulting <code>gbm</code> object, and a variety of other loss
functions that had not previously had associated boosting algorithms,
including the Cox partial likelihood for censored data, the poisson
likelihood for count outcomes, and a gradient boosting implementation to
minimize the AdaBoost exponential loss function.
</p>


<h3>Value</h3>

<p>A <code><a href="#topic+gbm.object">gbm.object</a></code> object.
</p>


<h3>Author(s)</h3>

<p>Greg Ridgeway <a href="mailto:gregridgeway@gmail.com">gregridgeway@gmail.com</a>
</p>
<p>Quantile regression code developed by Brian Kriegler
<a href="mailto:bk@stat.ucla.edu">bk@stat.ucla.edu</a>
</p>
<p>t-distribution, and multinomial code developed by Harry Southworth and
Daniel Edwards
</p>
<p>Pairwise code developed by Stefan Schroedl <a href="mailto:schroedl@a9.com">schroedl@a9.com</a>
</p>


<h3>References</h3>

<p>Y. Freund and R.E. Schapire (1997) &ldquo;A decision-theoretic
generalization of on-line learning and an application to boosting,&rdquo;
<em>Journal of Computer and System Sciences,</em> 55(1):119-139.
</p>
<p>G. Ridgeway (1999). &ldquo;The state of boosting,&rdquo; <em>Computing Science
and Statistics</em> 31:172-181.
</p>
<p>J.H. Friedman, T. Hastie, R. Tibshirani (2000). &ldquo;Additive Logistic
Regression: a Statistical View of Boosting,&rdquo; <em>Annals of Statistics</em>
28(2):337-374.
</p>
<p>J.H. Friedman (2001). &ldquo;Greedy Function Approximation: A Gradient
Boosting Machine,&rdquo; <em>Annals of Statistics</em> 29(5):1189-1232.
</p>
<p>J.H. Friedman (2002). &ldquo;Stochastic Gradient Boosting,&rdquo;
<em>Computational Statistics and Data Analysis</em> 38(4):367-378.
</p>
<p>B. Kriegler (2007). Cost-Sensitive Stochastic Gradient Boosting Within a 
Quantitative Regression Framework. Ph.D. Dissertation. University of 
California at Los Angeles, Los Angeles, CA, USA. Advisor(s) Richard A. Berk. 
<a href="https://dl.acm.org/doi/book/10.5555/1354603">https://dl.acm.org/doi/book/10.5555/1354603</a>.
</p>
<p>C. Burges (2010). &ldquo;From RankNet to LambdaRank to LambdaMART: An
Overview,&rdquo; Microsoft Research Technical Report MSR-TR-2010-82.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+gbm.object">gbm.object</a></code>, <code><a href="#topic+gbm.perf">gbm.perf</a></code>, 
<code><a href="#topic+plot.gbm">plot.gbm</a></code>, <code><a href="#topic+predict.gbm">predict.gbm</a></code>, <code><a href="#topic+summary.gbm">summary.gbm</a></code>, 
and <code><a href="#topic+pretty.gbm.tree">pretty.gbm.tree</a></code>.
</p>

<hr>
<h2 id='gbm.more'>Generalized Boosted Regression Modeling (GBM)</h2><span id='topic+gbm.more'></span>

<h3>Description</h3>

<p>Adds additional trees to a <code><a href="#topic+gbm.object">gbm.object</a></code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gbm.more(
  object,
  n.new.trees = 100,
  data = NULL,
  weights = NULL,
  offset = NULL,
  verbose = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gbm.more_+3A_object">object</code></td>
<td>
<p>A <code><a href="#topic+gbm.object">gbm.object</a></code> object created from an initial call 
to <code><a href="#topic+gbm">gbm</a></code>.</p>
</td></tr>
<tr><td><code id="gbm.more_+3A_n.new.trees">n.new.trees</code></td>
<td>
<p>Integer specifying the number of additional trees to add 
to <code>object</code>. Default is 100.</p>
</td></tr>
<tr><td><code id="gbm.more_+3A_data">data</code></td>
<td>
<p>An optional data frame containing the variables in the model. By
default the variables are taken from <code>environment(formula)</code>, typically
the environment from which <code>gbm</code> is called. If <code>keep.data=TRUE</code> in
the initial call to <code>gbm</code> then <code>gbm</code> stores a copy with the
object. If <code>keep.data=FALSE</code> then subsequent calls to
<code><a href="#topic+gbm.more">gbm.more</a></code> must resupply the same dataset. It becomes the user's
responsibility to resupply the same data at this point.</p>
</td></tr>
<tr><td><code id="gbm.more_+3A_weights">weights</code></td>
<td>
<p>An optional vector of weights to be used in the fitting
process. Must be positive but do not need to be normalized. If
<code>keep.data=FALSE</code> in the initial call to <code>gbm</code> then it is the
user's responsibility to resupply the weights to <code><a href="#topic+gbm.more">gbm.more</a></code>.</p>
</td></tr>
<tr><td><code id="gbm.more_+3A_offset">offset</code></td>
<td>
<p>A vector of offset values.</p>
</td></tr>
<tr><td><code id="gbm.more_+3A_verbose">verbose</code></td>
<td>
<p>Logical indicating whether or not to print out progress and 
performance indicators (<code>TRUE</code>). If this option is left unspecified for 
<code>gbm.more</code>, then it uses <code>verbose</code> from <code>object</code>. Default is
<code>FALSE</code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A <code><a href="#topic+gbm.object">gbm.object</a></code> object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#
# A least squares regression example 
#

# Simulate data
set.seed(101)  # for reproducibility
N &lt;- 1000
X1 &lt;- runif(N)
X2 &lt;- 2 * runif(N)
X3 &lt;- ordered(sample(letters[1:4], N, replace = TRUE), levels = letters[4:1])
X4 &lt;- factor(sample(letters[1:6], N, replace = TRUE))
X5 &lt;- factor(sample(letters[1:3], N, replace = TRUE))
X6 &lt;- 3 * runif(N) 
mu &lt;- c(-1, 0, 1, 2)[as.numeric(X3)]
SNR &lt;- 10  # signal-to-noise ratio
Y &lt;- X1 ^ 1.5 + 2 * (X2 ^ 0.5) + mu
sigma &lt;- sqrt(var(Y) / SNR)
Y &lt;- Y + rnorm(N, 0, sigma)
X1[sample(1:N,size=500)] &lt;- NA  # introduce some missing values
X4[sample(1:N,size=300)] &lt;- NA  # introduce some missing values
data &lt;- data.frame(Y, X1, X2, X3, X4, X5, X6)

# Fit a GBM
set.seed(102)  # for reproducibility
gbm1 &lt;- gbm(Y ~ ., data = data, var.monotone = c(0, 0, 0, 0, 0, 0),
            distribution = "gaussian", n.trees = 100, shrinkage = 0.1,             
            interaction.depth = 3, bag.fraction = 0.5, train.fraction = 0.5,  
            n.minobsinnode = 10, cv.folds = 5, keep.data = TRUE, 
            verbose = FALSE, n.cores = 1)  

# Check performance using the out-of-bag (OOB) error; the OOB error typically
# underestimates the optimal number of iterations
best.iter &lt;- gbm.perf(gbm1, method = "OOB")
print(best.iter)

# Check performance using the 50% heldout test set
best.iter &lt;- gbm.perf(gbm1, method = "test")
print(best.iter)

# Check performance using 5-fold cross-validation
best.iter &lt;- gbm.perf(gbm1, method = "cv")
print(best.iter)

# Plot relative influence of each variable
par(mfrow = c(1, 2))
summary(gbm1, n.trees = 1)          # using first tree
summary(gbm1, n.trees = best.iter)  # using estimated best number of trees

# Compactly print the first and last trees for curiosity
print(pretty.gbm.tree(gbm1, i.tree = 1))
print(pretty.gbm.tree(gbm1, i.tree = gbm1$n.trees))

# Simulate new data
set.seed(103)  # for reproducibility
N &lt;- 1000
X1 &lt;- runif(N)
X2 &lt;- 2 * runif(N)
X3 &lt;- ordered(sample(letters[1:4], N, replace = TRUE))
X4 &lt;- factor(sample(letters[1:6], N, replace = TRUE))
X5 &lt;- factor(sample(letters[1:3], N, replace = TRUE))
X6 &lt;- 3 * runif(N) 
mu &lt;- c(-1, 0, 1, 2)[as.numeric(X3)]
Y &lt;- X1 ^ 1.5 + 2 * (X2 ^ 0.5) + mu + rnorm(N, 0, sigma)
data2 &lt;- data.frame(Y, X1, X2, X3, X4, X5, X6)

# Predict on the new data using the "best" number of trees; by default,
# predictions will be on the link scale
Yhat &lt;- predict(gbm1, newdata = data2, n.trees = best.iter, type = "link")

# least squares error
print(sum((data2$Y - Yhat)^2))

# Construct univariate partial dependence plots
plot(gbm1, i.var = 1, n.trees = best.iter)
plot(gbm1, i.var = 2, n.trees = best.iter)
plot(gbm1, i.var = "X3", n.trees = best.iter)  # can use index or name

# Construct bivariate partial dependence plots
plot(gbm1, i.var = 1:2, n.trees = best.iter)
plot(gbm1, i.var = c("X2", "X3"), n.trees = best.iter)
plot(gbm1, i.var = 3:4, n.trees = best.iter)

# Construct trivariate partial dependence plots
plot(gbm1, i.var = c(1, 2, 6), n.trees = best.iter, 
     continuous.resolution = 20)
plot(gbm1, i.var = 1:3, n.trees = best.iter)
plot(gbm1, i.var = 2:4, n.trees = best.iter)
plot(gbm1, i.var = 3:5, n.trees = best.iter)

# Add more (i.e., 100) boosting iterations to the ensemble
gbm2 &lt;- gbm.more(gbm1, n.new.trees = 100, verbose = FALSE)
</code></pre>

<hr>
<h2 id='gbm.object'>Generalized Boosted Regression Model Object</h2><span id='topic+gbm.object'></span>

<h3>Description</h3>

<p>These are objects representing fitted <code>gbm</code>s.
</p>


<h3>Value</h3>

<table>
<tr><td><code>initF</code></td>
<td>
<p>The &quot;intercept&quot; term, the initial predicted value to
which trees make adjustments.</p>
</td></tr> <tr><td><code>fit</code></td>
<td>
<p>A vector containing the fitted
values on the scale of regression function (e.g. log-odds scale for
bernoulli, log scale for poisson).</p>
</td></tr> <tr><td><code>train.error</code></td>
<td>
<p>A vector of length
equal to the number of fitted trees containing the value of the loss
function for each boosting iteration evaluated on the training data.</p>
</td></tr>
<tr><td><code>valid.error</code></td>
<td>
<p>A vector of length equal to the number of fitted trees
containing the value of the loss function for each boosting iteration
evaluated on the validation data.</p>
</td></tr> <tr><td><code>cv.error</code></td>
<td>
<p>If <code>cv.folds</code> &lt; 2 this
component is <code>NULL</code>. Otherwise, this component is a vector of length equal to
the number of fitted trees containing a cross-validated estimate of the loss
function for each boosting iteration.</p>
</td></tr> <tr><td><code>oobag.improve</code></td>
<td>
<p>A vector of
length equal to the number of fitted trees containing an out-of-bag estimate
of the marginal reduction in the expected value of the loss function. The
out-of-bag estimate uses only the training data and is useful for estimating
the optimal number of boosting iterations. See <code><a href="#topic+gbm.perf">gbm.perf</a></code>.</p>
</td></tr>
<tr><td><code>trees</code></td>
<td>
<p>A list containing the tree structures. The components are best
viewed using <code><a href="#topic+pretty.gbm.tree">pretty.gbm.tree</a></code>.</p>
</td></tr> <tr><td><code>c.splits</code></td>
<td>
<p>A list of all
the categorical splits in the collection of trees. If the <code>trees[[i]]</code>
component of a <code>gbm</code> object describes a categorical split then the
splitting value will refer to a component of <code>c.splits</code>. That component
of <code>c.splits</code> will be a vector of length equal to the number of levels
in the categorical split variable. -1 indicates left, +1 indicates right,
and 0 indicates that the level was not present in the training data.</p>
</td></tr>
<tr><td><code>cv.fitted</code></td>
<td>
<p>If cross-validation was performed, the cross-validation
predicted values on the scale of the linear predictor. That is, the fitted
values from the i-th CV-fold, for the model having been trained on the data
in all other folds.</p>
</td></tr>
</table>


<h3>Structure</h3>

<p>The following components must be included in a
legitimate <code>gbm</code> object.
</p>


<h3>Author(s)</h3>

<p>Greg Ridgeway <a href="mailto:gregridgeway@gmail.com">gregridgeway@gmail.com</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+gbm">gbm</a></code>
</p>

<hr>
<h2 id='gbm.perf'>GBM performance</h2><span id='topic+gbm.perf'></span>

<h3>Description</h3>

<p>Estimates the optimal number of boosting iterations for a <code>gbm</code> object
and optionally plots various performance measures
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gbm.perf(object, plot.it = TRUE, oobag.curve = FALSE, overlay = TRUE, method)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gbm.perf_+3A_object">object</code></td>
<td>
<p>A <code><a href="#topic+gbm.object">gbm.object</a></code> created from an initial call to
<code><a href="#topic+gbm">gbm</a></code>.</p>
</td></tr>
<tr><td><code id="gbm.perf_+3A_plot.it">plot.it</code></td>
<td>
<p>An indicator of whether or not to plot the performance
measures. Setting <code>plot.it = TRUE</code> creates two plots. The first plot
plots <code>object$train.error</code> (in black) and <code>object$valid.error</code> 
(in red) versus the iteration number. The scale of the error measurement, 
shown on the left vertical axis, depends on the <code>distribution</code> 
argument used in the initial call to <code><a href="#topic+gbm">gbm</a></code>.</p>
</td></tr>
<tr><td><code id="gbm.perf_+3A_oobag.curve">oobag.curve</code></td>
<td>
<p>Indicates whether to plot the out-of-bag performance
measures in a second plot.</p>
</td></tr>
<tr><td><code id="gbm.perf_+3A_overlay">overlay</code></td>
<td>
<p>If TRUE and oobag.curve=TRUE then a right y-axis is added to
the training and test error plot and the estimated cumulative improvement 
in the loss function is plotted versus the iteration number.</p>
</td></tr>
<tr><td><code id="gbm.perf_+3A_method">method</code></td>
<td>
<p>Indicate the method used to estimate the optimal number of
boosting iterations. <code>method = "OOB"</code> computes the out-of-bag estimate
and <code>method = "test"</code> uses the test (or validation) dataset to compute 
an out-of-sample estimate. <code>method = "cv"</code> extracts the optimal number 
of iterations using cross-validation if <code>gbm</code> was called with
<code>cv.folds</code> &gt; 1.</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>gbm.perf</code> Returns the estimated optimal number of iterations.
The method of computation depends on the <code>method</code> argument.
</p>


<h3>Author(s)</h3>

<p>Greg Ridgeway <a href="mailto:gregridgeway@gmail.com">gregridgeway@gmail.com</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+gbm">gbm</a></code>, <code><a href="#topic+gbm.object">gbm.object</a></code>
</p>

<hr>
<h2 id='gbm.roc.area'>Compute Information Retrieval measures.</h2><span id='topic+gbm.roc.area'></span><span id='topic+gbm.conc'></span><span id='topic+ir.measure.conc'></span><span id='topic+ir.measure.auc'></span><span id='topic+ir.measure.mrr'></span><span id='topic+ir.measure.map'></span><span id='topic+ir.measure.ndcg'></span><span id='topic+perf.pairwise'></span>

<h3>Description</h3>

<p>Functions to compute Information Retrieval measures for pairwise loss for a
single group. The function returns the respective metric, or a negative
value if it is undefined for the given group.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gbm.roc.area(obs, pred)

gbm.conc(x)

ir.measure.conc(y.f, max.rank = 0)

ir.measure.auc(y.f, max.rank = 0)

ir.measure.mrr(y.f, max.rank)

ir.measure.map(y.f, max.rank = 0)

ir.measure.ndcg(y.f, max.rank)

perf.pairwise(y, f, group, metric = "ndcg", w = NULL, max.rank = 0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gbm.roc.area_+3A_obs">obs</code></td>
<td>
<p>Observed value.</p>
</td></tr>
<tr><td><code id="gbm.roc.area_+3A_pred">pred</code></td>
<td>
<p>Predicted value.</p>
</td></tr>
<tr><td><code id="gbm.roc.area_+3A_x">x</code></td>
<td>
<p>Numeric vector.</p>
</td></tr>
<tr><td><code id="gbm.roc.area_+3A_y">y</code>, <code id="gbm.roc.area_+3A_y.f">y.f</code>, <code id="gbm.roc.area_+3A_f">f</code>, <code id="gbm.roc.area_+3A_w">w</code>, <code id="gbm.roc.area_+3A_group">group</code>, <code id="gbm.roc.area_+3A_max.rank">max.rank</code></td>
<td>
<p>Used internally.</p>
</td></tr>
<tr><td><code id="gbm.roc.area_+3A_metric">metric</code></td>
<td>
<p>What type of performance measure to compute.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For simplicity, we have no special handling for ties; instead, we break ties
randomly. This is slightly inaccurate for individual groups, but should have
only a small effect on the overall measure.
</p>
<p><code>gbm.conc</code> computes the concordance index: Fraction of all pairs (i,j)
with i&lt;j, x[i] != x[j], such that x[j] &lt; x[i]
</p>
<p>If <code>obs</code> is binary, then <code>gbm.roc.area(obs, pred) =
gbm.conc(obs[order(-pred)])</code>.
</p>
<p><code>gbm.conc</code> is more general as it allows non-binary targets, but is
significantly slower.
</p>


<h3>Value</h3>

<p>The requested performance measure.
</p>


<h3>Author(s)</h3>

<p>Stefan Schroedl
</p>


<h3>References</h3>

<p>C. Burges (2010). &quot;From RankNet to LambdaRank to LambdaMART: An
Overview&quot;, Microsoft Research Technical Report MSR-TR-2010-82.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+gbm">gbm</a></code>
</p>

<hr>
<h2 id='gbmCrossVal'>Cross-validate a gbm</h2><span id='topic+gbmCrossVal'></span><span id='topic+gbmCrossValModelBuild'></span><span id='topic+gbmDoFold'></span><span id='topic+gbmCrossValErr'></span><span id='topic+gbmCrossValPredictions'></span>

<h3>Description</h3>

<p>Functions for cross-validating gbm. These functions are used internally and
are not intended for end-user direct usage.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>gbmCrossVal(
  cv.folds,
  nTrain,
  n.cores,
  class.stratify.cv,
  data,
  x,
  y,
  offset,
  distribution,
  w,
  var.monotone,
  n.trees,
  interaction.depth,
  n.minobsinnode,
  shrinkage,
  bag.fraction,
  var.names,
  response.name,
  group
)

gbmCrossValErr(cv.models, cv.folds, cv.group, nTrain, n.trees)

gbmCrossValPredictions(
  cv.models,
  cv.folds,
  cv.group,
  best.iter.cv,
  distribution,
  data,
  y
)

gbmCrossValModelBuild(
  cv.folds,
  cv.group,
  n.cores,
  i.train,
  x,
  y,
  offset,
  distribution,
  w,
  var.monotone,
  n.trees,
  interaction.depth,
  n.minobsinnode,
  shrinkage,
  bag.fraction,
  var.names,
  response.name,
  group
)

gbmDoFold(
  X,
  i.train,
  x,
  y,
  offset,
  distribution,
  w,
  var.monotone,
  n.trees,
  interaction.depth,
  n.minobsinnode,
  shrinkage,
  bag.fraction,
  cv.group,
  var.names,
  response.name,
  group,
  s
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="gbmCrossVal_+3A_cv.folds">cv.folds</code></td>
<td>
<p>The number of cross-validation folds.</p>
</td></tr>
<tr><td><code id="gbmCrossVal_+3A_ntrain">nTrain</code></td>
<td>
<p>The number of training samples.</p>
</td></tr>
<tr><td><code id="gbmCrossVal_+3A_n.cores">n.cores</code></td>
<td>
<p>The number of cores to use.</p>
</td></tr>
<tr><td><code id="gbmCrossVal_+3A_class.stratify.cv">class.stratify.cv</code></td>
<td>
<p>Whether or not stratified cross-validation samples
are used.</p>
</td></tr>
<tr><td><code id="gbmCrossVal_+3A_data">data</code></td>
<td>
<p>The data.</p>
</td></tr>
<tr><td><code id="gbmCrossVal_+3A_x">x</code></td>
<td>
<p>The model matrix.</p>
</td></tr>
<tr><td><code id="gbmCrossVal_+3A_y">y</code></td>
<td>
<p>The response variable.</p>
</td></tr>
<tr><td><code id="gbmCrossVal_+3A_offset">offset</code></td>
<td>
<p>The offset.</p>
</td></tr>
<tr><td><code id="gbmCrossVal_+3A_distribution">distribution</code></td>
<td>
<p>The type of loss function. See <code><a href="#topic+gbm">gbm</a></code>.</p>
</td></tr>
<tr><td><code id="gbmCrossVal_+3A_w">w</code></td>
<td>
<p>Observation weights.</p>
</td></tr>
<tr><td><code id="gbmCrossVal_+3A_var.monotone">var.monotone</code></td>
<td>
<p>See <code><a href="#topic+gbm">gbm</a></code>.</p>
</td></tr>
<tr><td><code id="gbmCrossVal_+3A_n.trees">n.trees</code></td>
<td>
<p>The number of trees to fit.</p>
</td></tr>
<tr><td><code id="gbmCrossVal_+3A_interaction.depth">interaction.depth</code></td>
<td>
<p>The degree of allowed interactions. See
<code><a href="#topic+gbm">gbm</a></code>.</p>
</td></tr>
<tr><td><code id="gbmCrossVal_+3A_n.minobsinnode">n.minobsinnode</code></td>
<td>
<p>See <code><a href="#topic+gbm">gbm</a></code>.</p>
</td></tr>
<tr><td><code id="gbmCrossVal_+3A_shrinkage">shrinkage</code></td>
<td>
<p>See <code><a href="#topic+gbm">gbm</a></code>.</p>
</td></tr>
<tr><td><code id="gbmCrossVal_+3A_bag.fraction">bag.fraction</code></td>
<td>
<p>See <code><a href="#topic+gbm">gbm</a></code>.</p>
</td></tr>
<tr><td><code id="gbmCrossVal_+3A_var.names">var.names</code></td>
<td>
<p>See <code><a href="#topic+gbm">gbm</a></code>.</p>
</td></tr>
<tr><td><code id="gbmCrossVal_+3A_response.name">response.name</code></td>
<td>
<p>See <code><a href="#topic+gbm">gbm</a></code>.</p>
</td></tr>
<tr><td><code id="gbmCrossVal_+3A_group">group</code></td>
<td>
<p>Used when <code>distribution = "pairwise"</code>. See
<code><a href="#topic+gbm">gbm</a></code>.</p>
</td></tr>
<tr><td><code id="gbmCrossVal_+3A_cv.models">cv.models</code></td>
<td>
<p>A list containing the models for each fold.</p>
</td></tr>
<tr><td><code id="gbmCrossVal_+3A_cv.group">cv.group</code></td>
<td>
<p>A vector indicating the cross-validation fold for each
member of the training set.</p>
</td></tr>
<tr><td><code id="gbmCrossVal_+3A_best.iter.cv">best.iter.cv</code></td>
<td>
<p>The iteration with lowest cross-validation error.</p>
</td></tr>
<tr><td><code id="gbmCrossVal_+3A_i.train">i.train</code></td>
<td>
<p>Items in the training set.</p>
</td></tr>
<tr><td><code id="gbmCrossVal_+3A_x">X</code></td>
<td>
<p>Index (cross-validation fold) on which to subset.</p>
</td></tr>
<tr><td><code id="gbmCrossVal_+3A_s">s</code></td>
<td>
<p>Random seed.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>These functions are not intended for end-user direct usage, but are used
internally by <code>gbm</code>.
</p>


<h3>Value</h3>

<p>A list containing the cross-validation error and predictions.
</p>


<h3>Author(s)</h3>

<p>Greg Ridgeway <a href="mailto:gregridgeway@gmail.com">gregridgeway@gmail.com</a>
</p>


<h3>References</h3>

<p>J.H. Friedman (2001). &quot;Greedy Function Approximation: A Gradient
Boosting Machine,&quot; Annals of Statistics 29(5):1189-1232.
</p>
<p>L. Breiman (2001).
<a href="https://www.stat.berkeley.edu/users/breiman/randomforest2001.pdf">https://www.stat.berkeley.edu/users/breiman/randomforest2001.pdf</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+gbm">gbm</a></code>
</p>

<hr>
<h2 id='guessDist'>gbm internal functions</h2><span id='topic+guessDist'></span><span id='topic+getStratify'></span><span id='topic+getCVgroup'></span><span id='topic+checkMissing'></span><span id='topic+checkID'></span><span id='topic+checkWeights'></span><span id='topic+checkOffset'></span><span id='topic+getVarNames'></span><span id='topic+gbmCluster'></span>

<h3>Description</h3>

<p>Helper functions for preprocessing data prior to building a <code>"gbm"</code>
object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>guessDist(y)

getCVgroup(distribution, class.stratify.cv, y, i.train, cv.folds, group)

getStratify(strat, d)

checkMissing(x, y)

checkWeights(w, n)

checkID(id)

checkOffset(o, y)

getVarNames(x)

gbmCluster(n)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="guessDist_+3A_y">y</code></td>
<td>
<p>The response variable.</p>
</td></tr>
<tr><td><code id="guessDist_+3A_class.stratify.cv">class.stratify.cv</code></td>
<td>
<p>Whether or not to stratify, if provided by the user.</p>
</td></tr>
<tr><td><code id="guessDist_+3A_i.train">i.train</code></td>
<td>
<p>Computed internally by <code>gbm</code>.</p>
</td></tr>
<tr><td><code id="guessDist_+3A_cv.folds">cv.folds</code></td>
<td>
<p>The number of cross-validation folds.</p>
</td></tr>
<tr><td><code id="guessDist_+3A_group">group</code></td>
<td>
<p>The group, if using <code>distibution = "pairwise"</code>.</p>
</td></tr>
<tr><td><code id="guessDist_+3A_strat">strat</code></td>
<td>
<p>Whether or not to stratify.</p>
</td></tr>
<tr><td><code id="guessDist_+3A_d">d</code>, <code id="guessDist_+3A_distribution">distribution</code></td>
<td>
<p>The distribution, either specified by the user or
implied.</p>
</td></tr>
<tr><td><code id="guessDist_+3A_x">x</code></td>
<td>
<p>The design matrix.</p>
</td></tr>
<tr><td><code id="guessDist_+3A_w">w</code></td>
<td>
<p>The weights.</p>
</td></tr>
<tr><td><code id="guessDist_+3A_n">n</code></td>
<td>
<p>The number of cores to use in the cluster.</p>
</td></tr>
<tr><td><code id="guessDist_+3A_id">id</code></td>
<td>
<p>The interaction depth.</p>
</td></tr>
<tr><td><code id="guessDist_+3A_o">o</code></td>
<td>
<p>The offset.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>These are functions used internally by <code>gbm</code> and not intended for direct 
use by the user.
</p>

<hr>
<h2 id='interact.gbm'>Estimate the strength of interaction effects</h2><span id='topic+interact.gbm'></span>

<h3>Description</h3>

<p>Computes Friedman's H-statistic to assess the strength of variable
interactions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>interact.gbm(x, data, i.var = 1, n.trees = x$n.trees)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="interact.gbm_+3A_x">x</code></td>
<td>
<p>A <code><a href="#topic+gbm.object">gbm.object</a></code> fitted using a call to <code><a href="#topic+gbm">gbm</a></code>.</p>
</td></tr>
<tr><td><code id="interact.gbm_+3A_data">data</code></td>
<td>
<p>The dataset used to construct <code>x</code>. If the original dataset
is large, a random subsample may be used to accelerate the computation in
<code>interact.gbm</code>.</p>
</td></tr>
<tr><td><code id="interact.gbm_+3A_i.var">i.var</code></td>
<td>
<p>A vector of indices or the names of the variables for compute
the interaction effect. If using indices, the variables are indexed in the
same order that they appear in the initial <code>gbm</code> formula.</p>
</td></tr>
<tr><td><code id="interact.gbm_+3A_n.trees">n.trees</code></td>
<td>
<p>The number of trees used to generate the plot. Only the first
<code>n.trees</code> trees will be used.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>interact.gbm</code> computes Friedman's H-statistic to assess the relative
strength of interaction effects in non-linear models. H is on the scale of
[0-1] with higher values indicating larger interaction effects. To connect
to a more familiar measure, if <code class="reqn">x_1</code> and <code class="reqn">x_2</code> are uncorrelated
covariates with mean 0 and variance 1 and the model is of the form
</p>
<p style="text-align: center;"><code class="reqn">y=\beta_0+\beta_1x_1+\beta_2x_2+\beta_3x_3</code>
</p>
<p> then
</p>
<p style="text-align: center;"><code class="reqn">H=\frac{\beta_3}{\sqrt{\beta_1^2+\beta_2^2+\beta_3^2}}</code>
</p>

<p>Note that if the main effects are weak, the estimated H will be unstable.
For example, if (in the case of a two-way interaction) neither main effect
is in the selected model (relative influence is zero), the result will be
0/0. Also, with weak main effects, rounding errors can result in values of H
&gt; 1 which are not possible.
</p>


<h3>Value</h3>

<p>Returns the value of <code class="reqn">H</code>.
</p>


<h3>Author(s)</h3>

<p>Greg Ridgeway <a href="mailto:gregridgeway@gmail.com">gregridgeway@gmail.com</a>
</p>


<h3>References</h3>

<p>J.H. Friedman and B.E. Popescu (2005). &ldquo;Predictive
Learning via Rule Ensembles.&rdquo; Section 8.1
</p>


<h3>See Also</h3>

<p><code><a href="#topic+gbm">gbm</a></code>, <code><a href="#topic+gbm.object">gbm.object</a></code>
</p>

<hr>
<h2 id='plot.gbm'>Marginal plots of fitted gbm objects</h2><span id='topic+plot.gbm'></span>

<h3>Description</h3>

<p>Plots the marginal effect of the selected variables by &quot;integrating&quot; out the
other variables.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'gbm'
plot(
  x,
  i.var = 1,
  n.trees = x$n.trees,
  continuous.resolution = 100,
  return.grid = FALSE,
  type = c("link", "response"),
  level.plot = TRUE,
  contour = FALSE,
  number = 4,
  overlap = 0.1,
  col.regions = viridis::viridis,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plot.gbm_+3A_x">x</code></td>
<td>
<p>A <code><a href="#topic+gbm.object">gbm.object</a></code> that was fit using a call to 
<code><a href="#topic+gbm">gbm</a></code>.</p>
</td></tr>
<tr><td><code id="plot.gbm_+3A_i.var">i.var</code></td>
<td>
<p>Vector of indices or the names of the variables to plot. If
using indices, the variables are indexed in the same order that they appear
in the initial <code>gbm</code> formula. If <code>length(i.var)</code> is between 1 and
3 then <code>plot.gbm</code> produces the plots. Otherwise, <code>plot.gbm</code>
returns only the grid of evaluation points and their average predictions</p>
</td></tr>
<tr><td><code id="plot.gbm_+3A_n.trees">n.trees</code></td>
<td>
<p>Integer specifying the number of trees to use to generate the 
plot. Default is to use <code>x$n.trees</code> (i.e., the entire ensemble).</p>
</td></tr>
<tr><td><code id="plot.gbm_+3A_continuous.resolution">continuous.resolution</code></td>
<td>
<p>Integer specifying the number of equally space 
points at which to evaluate continuous predictors.</p>
</td></tr>
<tr><td><code id="plot.gbm_+3A_return.grid">return.grid</code></td>
<td>
<p>Logical indicating whether or not to produce graphics 
<code>FALSE</code> or only return the grid of evaluation points and their average
predictions <code>TRUE</code>. This is useful for customizing the graphics for 
special variable types, or for higher dimensional graphs.</p>
</td></tr>
<tr><td><code id="plot.gbm_+3A_type">type</code></td>
<td>
<p>Character string specifying the type of prediction to plot on the 
vertical axis. See <code><a href="#topic+predict.gbm">predict.gbm</a></code> for details.</p>
</td></tr>
<tr><td><code id="plot.gbm_+3A_level.plot">level.plot</code></td>
<td>
<p>Logical indicating whether or not to use a false color 
level plot (<code>TRUE</code>) or a 3-D surface (<code>FALSE</code>). Default is 
<code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="plot.gbm_+3A_contour">contour</code></td>
<td>
<p>Logical indicating whether or not to add contour lines to the
level plot. Only used when <code>level.plot = TRUE</code>. Default is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="plot.gbm_+3A_number">number</code></td>
<td>
<p>Integer specifying the number of conditional intervals to use
for the continuous panel variables. See <code><a href="graphics.html#topic+coplot">co.intervals</a></code>
and <code><a href="lattice.html#topic+shingles">equal.count</a></code> for further details.</p>
</td></tr>
<tr><td><code id="plot.gbm_+3A_overlap">overlap</code></td>
<td>
<p>The fraction of overlap of the conditioning variables. See
<code><a href="graphics.html#topic+coplot">co.intervals</a></code> and <code><a href="lattice.html#topic+shingles">equal.count</a></code>
for further details.</p>
</td></tr>
<tr><td><code id="plot.gbm_+3A_col.regions">col.regions</code></td>
<td>
<p>Color vector to be used if <code>level.plot</code> is
<code>TRUE</code>. Defaults to the wonderful Matplotlib 'viridis' color map
provided by the <code>viridis</code> package. See <code><a href="viridis.html#topic+reexports">viridis</a></code>
for details.</p>
</td></tr>
<tr><td><code id="plot.gbm_+3A_...">...</code></td>
<td>
<p>Additional optional arguments to be passed onto 
<code><a href="graphics.html#topic+plot.default">plot</a></code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>plot.gbm</code> produces low dimensional projections of the
<code><a href="#topic+gbm.object">gbm.object</a></code> by integrating out the variables not included in
the <code>i.var</code> argument. The function selects a grid of points and uses
the weighted tree traversal method described in Friedman (2001) to do the
integration. Based on the variable types included in the projection,
<code>plot.gbm</code> selects an appropriate display choosing amongst line plots,
contour plots, and <code><a href="lattice.html#topic+Lattice">lattice</a></code> plots. If the default
graphics are not sufficient the user may set <code>return.grid = TRUE</code>, store
the result of the function, and develop another graphic display more
appropriate to the particular example.
</p>


<h3>Value</h3>

<p>If <code>return.grid = TRUE</code>, a grid of evaluation points and their 
average predictions. Otherwise, a plot is returned.
</p>


<h3>Note</h3>

<p>More flexible plotting is available using the 
<code><a href="pdp.html#topic+partial">partial</a></code> and <code><a href="pdp.html#topic+plotPartial">plotPartial</a></code> functions.
</p>


<h3>References</h3>

<p>J. H. Friedman (2001). &quot;Greedy Function Approximation: A Gradient
Boosting Machine,&quot; Annals of Statistics 29(4).
</p>
<p>B. M. Greenwell (2017). &quot;pdp: An R Package for Constructing 
Partial Dependence Plots,&quot; The R Journal 9(1), 421&ndash;436. 
<a href="https://journal.r-project.org/archive/2017/RJ-2017-016/index.html">https://journal.r-project.org/archive/2017/RJ-2017-016/index.html</a>.
</p>


<h3>See Also</h3>

<p><code><a href="pdp.html#topic+partial">partial</a></code>, <code><a href="pdp.html#topic+plotPartial">plotPartial</a></code>, 
<code><a href="#topic+gbm">gbm</a></code>, and <code><a href="#topic+gbm.object">gbm.object</a></code>.
</p>

<hr>
<h2 id='predict.gbm'>Predict method for GBM Model Fits</h2><span id='topic+predict.gbm'></span>

<h3>Description</h3>

<p>Predicted values based on a generalized boosted model object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'gbm'
predict(object, newdata, n.trees, type = "link", single.tree = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="predict.gbm_+3A_object">object</code></td>
<td>
<p>Object of class inheriting from (<code><a href="#topic+gbm.object">gbm.object</a></code>)</p>
</td></tr>
<tr><td><code id="predict.gbm_+3A_newdata">newdata</code></td>
<td>
<p>Data frame of observations for which to make predictions</p>
</td></tr>
<tr><td><code id="predict.gbm_+3A_n.trees">n.trees</code></td>
<td>
<p>Number of trees used in the prediction. <code>n.trees</code> may be
a vector in which case predictions are returned for each iteration specified</p>
</td></tr>
<tr><td><code id="predict.gbm_+3A_type">type</code></td>
<td>
<p>The scale on which gbm makes the predictions</p>
</td></tr>
<tr><td><code id="predict.gbm_+3A_single.tree">single.tree</code></td>
<td>
<p>If <code>single.tree=TRUE</code> then <code>predict.gbm</code>
returns only the predictions from tree(s) <code>n.trees</code></p>
</td></tr>
<tr><td><code id="predict.gbm_+3A_...">...</code></td>
<td>
<p>further arguments passed to or from other methods</p>
</td></tr>
</table>


<h3>Details</h3>

<p><code>predict.gbm</code> produces predicted values for each observation in
<code>newdata</code> using the the first <code>n.trees</code> iterations of the boosting
sequence. If <code>n.trees</code> is a vector than the result is a matrix with
each column representing the predictions from gbm models with
<code>n.trees[1]</code> iterations, <code>n.trees[2]</code> iterations, and so on.
</p>
<p>The predictions from <code>gbm</code> do not include the offset term. The user may
add the value of the offset to the predicted value if desired.
</p>
<p>If <code>object</code> was fit using <code><a href="#topic+gbm.fit">gbm.fit</a></code> there will be no
<code>Terms</code> component. Therefore, the user has greater responsibility to
make sure that <code>newdata</code> is of the same format (order and number of
variables) as the one originally used to fit the model.
</p>


<h3>Value</h3>

<p>Returns a vector of predictions. By default the predictions are on
the scale of f(x). For example, for the Bernoulli loss the returned value is
on the log odds scale, poisson loss on the log scale, and coxph is on the
log hazard scale.
</p>
<p>If <code>type="response"</code> then <code>gbm</code> converts back to the same scale as
the outcome. Currently the only effect this will have is returning
probabilities for bernoulli and expected counts for poisson. For the other
distributions &quot;response&quot; and &quot;link&quot; return the same.
</p>


<h3>Author(s)</h3>

<p>Greg Ridgeway <a href="mailto:gregridgeway@gmail.com">gregridgeway@gmail.com</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+gbm">gbm</a></code>, <code><a href="#topic+gbm.object">gbm.object</a></code>
</p>

<hr>
<h2 id='pretty.gbm.tree'>Print gbm tree components</h2><span id='topic+pretty.gbm.tree'></span>

<h3>Description</h3>

<p><code>gbm</code> stores the collection of trees used to construct the model in a
compact matrix structure. This function extracts the information from a
single tree and displays it in a slightly more readable form. This function
is mostly for debugging purposes and to satisfy some users' curiosity.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'gbm.tree'
pretty(object, i.tree = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="pretty.gbm.tree_+3A_object">object</code></td>
<td>
<p>a <code><a href="#topic+gbm.object">gbm.object</a></code> initially fit using
<code><a href="#topic+gbm">gbm</a></code></p>
</td></tr>
<tr><td><code id="pretty.gbm.tree_+3A_i.tree">i.tree</code></td>
<td>
<p>the index of the tree component to extract from <code>object</code>
and display</p>
</td></tr>
</table>


<h3>Value</h3>

<p><code>pretty.gbm.tree</code> returns a data frame. Each row corresponds to
a node in the tree. Columns indicate </p>
<table>
<tr><td><code>SplitVar</code></td>
<td>
<p>index of which variable
is used to split. -1 indicates a terminal node.</p>
</td></tr> <tr><td><code>SplitCodePred</code></td>
<td>
<p>if the
split variable is continuous then this component is the split point. If the
split variable is categorical then this component contains the index of
<code>object$c.split</code> that describes the categorical split. If the node is a
terminal node then this is the prediction.</p>
</td></tr> <tr><td><code>LeftNode</code></td>
<td>
<p>the index of the
row corresponding to the left node.</p>
</td></tr> <tr><td><code>RightNode</code></td>
<td>
<p>the index of the row
corresponding to the right node.</p>
</td></tr> <tr><td><code>ErrorReduction</code></td>
<td>
<p>the reduction in the
loss function as a result of splitting this node.</p>
</td></tr> <tr><td><code>Weight</code></td>
<td>
<p>the total
weight of observations in the node. If weights are all equal to 1 then this
is the number of observations in the node.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Greg Ridgeway <a href="mailto:gregridgeway@gmail.com">gregridgeway@gmail.com</a>
</p>


<h3>See Also</h3>

<p><code><a href="#topic+gbm">gbm</a></code>, <code><a href="#topic+gbm.object">gbm.object</a></code>
</p>

<hr>
<h2 id='print.gbm'>Print model summary</h2><span id='topic+print.gbm'></span><span id='topic+show.gbm'></span>

<h3>Description</h3>

<p>Display basic information about a <code>gbm</code> object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'gbm'
print(x, ...)

show.gbm(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="print.gbm_+3A_x">x</code></td>
<td>
<p>an object of class <code>gbm</code>.</p>
</td></tr>
<tr><td><code id="print.gbm_+3A_...">...</code></td>
<td>
<p>arguments passed to <code>print.default</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Prints some information about the model object. In particular, this method
prints the call to <code>gbm()</code>, the type of loss function that was used,
and the total number of iterations.
</p>
<p>If cross-validation was performed, the 'best' number of trees as estimated
by cross-validation error is displayed. If a test set was used, the 'best'
number of trees as estimated by the test set error is displayed.
</p>
<p>The number of available predictors, and the number of those having non-zero
influence on predictions is given (which might be interesting in data mining
applications).
</p>
<p>If multinomial, bernoulli or adaboost was used, the confusion matrix and
prediction accuracy are printed (objects being allocated to the class with
highest probability for multinomial and bernoulli). These classifications
are performed on the entire training data using the model with the 'best'
number of trees as described above, or the maximum number of trees if the
'best' cannot be computed.
</p>
<p>If the 'distribution' was specified as gaussian, laplace, quantile or
t-distribution, a summary of the residuals is displayed.  The residuals are
for the training data with the model at the 'best' number of trees, as
described above, or the maximum number of trees if the 'best' cannot be
computed.
</p>


<h3>Author(s)</h3>

<p>Harry Southworth, Daniel Edwards
</p>


<h3>See Also</h3>

<p><code><a href="#topic+gbm">gbm</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
data(iris)
iris.mod &lt;- gbm(Species ~ ., distribution="multinomial", data=iris,
                 n.trees=2000, shrinkage=0.01, cv.folds=5,
                 verbose=FALSE, n.cores=1)
iris.mod
#data(lung)
#lung.mod &lt;- gbm(Surv(time, status) ~ ., distribution="coxph", data=lung,
#                 n.trees=2000, shrinkage=0.01, cv.folds=5,verbose =FALSE)
#lung.mod
</code></pre>

<hr>
<h2 id='quantile.rug'>Quantile rug plot</h2><span id='topic+quantile.rug'></span>

<h3>Description</h3>

<p>Marks the quantiles on the axes of the current plot.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'rug'
quantile(x, prob = 0:10/10, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="quantile.rug_+3A_x">x</code></td>
<td>
<p>A numeric vector.</p>
</td></tr>
<tr><td><code id="quantile.rug_+3A_prob">prob</code></td>
<td>
<p>The quantiles of x to mark on the x-axis.</p>
</td></tr>
<tr><td><code id="quantile.rug_+3A_...">...</code></td>
<td>
<p>Additional optional arguments to be passed onto 
<code><a href="graphics.html#topic+rug">rug</a></code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>No return values.
</p>


<h3>Author(s)</h3>

<p>Greg Ridgeway <a href="mailto:gregridgeway@gmail.com">gregridgeway@gmail.com</a>.
</p>


<h3>See Also</h3>

<p><code><a href="graphics.html#topic+plot.default">plot</a></code>, <code><a href="stats.html#topic+quantile">quantile</a></code>,
<code><a href="base.html#topic+jitter">jitter</a></code>, <code><a href="graphics.html#topic+rug">rug</a></code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>x &lt;- rnorm(100)
y &lt;- rnorm(100)
plot(x, y)
quantile.rug(x)
</code></pre>

<hr>
<h2 id='reconstructGBMdata'>Reconstruct a GBM's Source Data</h2><span id='topic+reconstructGBMdata'></span>

<h3>Description</h3>

<p>Helper function to reconstitute the data for plots and summaries. This
function is not intended for the user to call directly.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>reconstructGBMdata(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="reconstructGBMdata_+3A_x">x</code></td>
<td>
<p>a <code><a href="#topic+gbm.object">gbm.object</a></code> initially fit using <code><a href="#topic+gbm">gbm</a></code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>Returns a data used to fit the gbm in a format that can subsequently
be used for plots and summaries
</p>


<h3>Author(s)</h3>

<p>Harry Southworth
</p>


<h3>See Also</h3>

<p><code><a href="#topic+gbm">gbm</a></code>, <code><a href="#topic+gbm.object">gbm.object</a></code>
</p>

<hr>
<h2 id='relative.influence'>Methods for estimating relative influence</h2><span id='topic+relative.influence'></span><span id='topic+permutation.test.gbm'></span><span id='topic+gbm.loss'></span>

<h3>Description</h3>

<p>Helper functions for computing the relative influence of each variable in
the gbm object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>relative.influence(object, n.trees, scale. = FALSE, sort. = FALSE)

permutation.test.gbm(object, n.trees)

gbm.loss(y, f, w, offset, dist, baseline, group = NULL, max.rank = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="relative.influence_+3A_object">object</code></td>
<td>
<p>a <code>gbm</code> object created from an initial call to
<code><a href="#topic+gbm">gbm</a></code>.</p>
</td></tr>
<tr><td><code id="relative.influence_+3A_n.trees">n.trees</code></td>
<td>
<p>the number of trees to use for computations. If not provided,
the the function will guess: if a test set was used in fitting, the number
of trees resulting in lowest test set error will be used; otherwise, if
cross-validation was performed, the number of trees resulting in lowest
cross-validation error will be used; otherwise, all trees will be used.</p>
</td></tr>
<tr><td><code id="relative.influence_+3A_scale.">scale.</code></td>
<td>
<p>whether or not the result should be scaled. Defaults to
<code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="relative.influence_+3A_sort.">sort.</code></td>
<td>
<p>whether or not the results should be (reverse) sorted.
Defaults to <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="relative.influence_+3A_y">y</code>, <code id="relative.influence_+3A_f">f</code>, <code id="relative.influence_+3A_w">w</code>, <code id="relative.influence_+3A_offset">offset</code>, <code id="relative.influence_+3A_dist">dist</code>, <code id="relative.influence_+3A_baseline">baseline</code></td>
<td>
<p>For <code>gbm.loss</code>: These components are
the outcome, predicted value, observation weight, offset, distribution, and
comparison loss function, respectively.</p>
</td></tr>
<tr><td><code id="relative.influence_+3A_group">group</code>, <code id="relative.influence_+3A_max.rank">max.rank</code></td>
<td>
<p>Used internally when <code>distribution =
\'pairwise\'</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is not intended for end-user use. These functions offer the different
methods for computing the relative influence in <code><a href="#topic+summary.gbm">summary.gbm</a></code>.
<code>gbm.loss</code> is a helper function for <code>permutation.test.gbm</code>.
</p>


<h3>Value</h3>

<p>By default, returns an unprocessed vector of estimated relative
influences. If the <code>scale.</code> and <code>sort.</code> arguments are used,
returns a processed version of the same.
</p>


<h3>Author(s)</h3>

<p>Greg Ridgeway <a href="mailto:gregridgeway@gmail.com">gregridgeway@gmail.com</a>
</p>


<h3>References</h3>

<p>J.H. Friedman (2001). &quot;Greedy Function Approximation: A Gradient
Boosting Machine,&quot; Annals of Statistics 29(5):1189-1232.
</p>
<p>L. Breiman (2001).
<a href="https://www.stat.berkeley.edu/users/breiman/randomforest2001.pdf">https://www.stat.berkeley.edu/users/breiman/randomforest2001.pdf</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+summary.gbm">summary.gbm</a></code>
</p>

<hr>
<h2 id='summary.gbm'>Summary of a gbm object</h2><span id='topic+summary.gbm'></span>

<h3>Description</h3>

<p>Computes the relative influence of each variable in the gbm object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'gbm'
summary(
  object,
  cBars = length(object$var.names),
  n.trees = object$n.trees,
  plotit = TRUE,
  order = TRUE,
  method = relative.influence,
  normalize = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="summary.gbm_+3A_object">object</code></td>
<td>
<p>a <code>gbm</code> object created from an initial call to
<code><a href="#topic+gbm">gbm</a></code>.</p>
</td></tr>
<tr><td><code id="summary.gbm_+3A_cbars">cBars</code></td>
<td>
<p>the number of bars to plot. If <code>order=TRUE</code> the only the
variables with the <code>cBars</code> largest relative influence will appear in
the barplot. If <code>order=FALSE</code> then the first <code>cBars</code> variables
will appear in the plot. In either case, the function will return the
relative influence of all of the variables.</p>
</td></tr>
<tr><td><code id="summary.gbm_+3A_n.trees">n.trees</code></td>
<td>
<p>the number of trees used to generate the plot. Only the first
<code>n.trees</code> trees will be used.</p>
</td></tr>
<tr><td><code id="summary.gbm_+3A_plotit">plotit</code></td>
<td>
<p>an indicator as to whether the plot is generated.</p>
</td></tr>
<tr><td><code id="summary.gbm_+3A_order">order</code></td>
<td>
<p>an indicator as to whether the plotted and/or returned relative
influences are sorted.</p>
</td></tr>
<tr><td><code id="summary.gbm_+3A_method">method</code></td>
<td>
<p>The function used to compute the relative influence.
<code><a href="#topic+relative.influence">relative.influence</a></code> is the default and is the same as that
described in Friedman (2001). The other current (and experimental) choice is
<code><a href="#topic+permutation.test.gbm">permutation.test.gbm</a></code>. This method randomly permutes each
predictor variable at a time and computes the associated reduction in
predictive performance. This is similar to the variable importance measures
Breiman uses for random forests, but <code>gbm</code> currently computes using the
entire training dataset (not the out-of-bag observations).</p>
</td></tr>
<tr><td><code id="summary.gbm_+3A_normalize">normalize</code></td>
<td>
<p>if <code>FALSE</code> then <code>summary.gbm</code> returns the
unnormalized influence.</p>
</td></tr>
<tr><td><code id="summary.gbm_+3A_...">...</code></td>
<td>
<p>other arguments passed to the plot function.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>For <code>distribution="gaussian"</code> this returns exactly the reduction of
squared error attributable to each variable. For other loss functions this
returns the reduction attributable to each variable in sum of squared error
in predicting the gradient on each iteration. It describes the relative
influence of each variable in reducing the loss function. See the references
below for exact details on the computation.
</p>


<h3>Value</h3>

<p>Returns a data frame where the first component is the variable name
and the second is the computed relative influence, normalized to sum to 100.
</p>


<h3>Author(s)</h3>

<p>Greg Ridgeway <a href="mailto:gregridgeway@gmail.com">gregridgeway@gmail.com</a>
</p>


<h3>References</h3>

<p>J.H. Friedman (2001). &quot;Greedy Function Approximation: A Gradient
Boosting Machine,&quot; Annals of Statistics 29(5):1189-1232.
</p>
<p>L. Breiman
(2001).<a href="https://www.stat.berkeley.edu/users/breiman/randomforest2001.pdf">https://www.stat.berkeley.edu/users/breiman/randomforest2001.pdf</a>.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+gbm">gbm</a></code>
</p>

<hr>
<h2 id='test.gbm'>Test the <code>gbm</code> package.</h2><span id='topic+test.gbm'></span><span id='topic+validate.gbm'></span><span id='topic+test.relative.influence'></span>

<h3>Description</h3>

<p>Run tests on <code>gbm</code> functions to perform logical checks and
reproducibility.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>test.gbm()
</code></pre>


<h3>Details</h3>

<p>The function uses functionality in the <code>RUnit</code> package. A fairly small
validation suite is executed that checks to see that relative influence
identifies sensible variables from simulated data, and that predictions from
GBMs with Gaussian, Cox or binomial distributions are sensible,
</p>


<h3>Value</h3>

<p>An object of class <code>RUnitTestData</code>. See the help for
<code>RUnit</code> for details.
</p>


<h3>Note</h3>

<p>The test suite is not comprehensive.
</p>


<h3>Author(s)</h3>

<p>Harry Southworth
</p>


<h3>See Also</h3>

<p><code><a href="#topic+gbm">gbm</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Uncomment the following lines to run - commented out to make CRAN happy
#library(RUnit)
#val &lt;- validate.texmex()
#printHTMLProtocol(val, "texmexReport.html")
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
