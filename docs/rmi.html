<!DOCTYPE html><html><head><title>Help for package rmi</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {rmi}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#rmi'><p>Mutual Information Estimators</p></a></li>
<li><a href='#estimate_mse'><p>Estimate MSE of LNC Estimator</p></a></li>
<li><a href='#knn_mi'><p>kNN Mutual Information Estimators</p></a></li>
<li><a href='#lnn_entropy'><p>Local Nearest Neighbor (LNN) Entropy Estimator</p></a></li>
<li><a href='#lnn_mi'><p>Local Nearest Neighbor (LNN) MI Estimator</p></a></li>
<li><a href='#nearest_neighbors'><p>Compute Nearest Neighbors</p></a></li>
<li><a href='#optimize_mse'><p>Optimize MSE of LNC Estimator</p></a></li>
<li><a href='#rmvn'><p>Random Multivariate Normal MVN Generator</p></a></li>
<li><a href='#simulate_mvn'><p>Calibration Random Multivariate Normal</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Mutual Information Estimators</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.1</td>
</tr>
<tr>
<td>Author:</td>
<td>Isaac Michaud [cre, aut]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Isaac Michaud &lt;ijmichau@ncsu.edu&gt;</td>
</tr>
<tr>
<td>Description:</td>
<td>Provides mutual information estimators based on k-nearest neighbor estimators by A. Kraskov, et al. (2004) &lt;<a href="https://doi.org/10.1103%2FPhysRevE.69.066138">doi:10.1103/PhysRevE.69.066138</a>&gt;, S. Gao, et al. (2015) <a href="http://proceedings.mlr.press/v38/gao15.pdf">http://proceedings.mlr.press/v38/gao15.pdf</a> and local density estimators by W. Gao, et al. (2017) &lt;<a href="https://doi.org/10.1109%2FISIT.2017.8006749">doi:10.1109/ISIT.2017.8006749</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>6.1.0</td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppArmadillo, BH</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp, stats, graphics</td>
</tr>
<tr>
<td>Suggests:</td>
<td>testthat, parallel, tgp</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2018-07-31 17:15:50 UTC; isaacmichaud</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2018-08-02 11:40:06 UTC</td>
</tr>
</table>
<hr>
<h2 id='rmi'>Mutual Information Estimators</h2><span id='topic+rmi'></span><span id='topic+rmi-package'></span>

<h3>Description</h3>

<p>The <code>rmi</code> package offers a collection of mutual information estimators based on k-Nearest Neighbor and local density estimators. Currently, <code>rmi</code> provides the Kraskov et al. algorithm (KSG) 1 and 2, Local Non-uniformity Corrected (LNC) KSG, and the Local Nearest Neighbor (LNN) estimator. More estimators and examples will be incorporated in the future.
</p>


<h3>References</h3>

<p>Gao, S., Ver Steeg G., &amp; Galstyan A. (2015). Efficient estimation of mutual information for strongly dependent variables. Artificial Intelligence and Statistics: 277-286.
</p>
<p>Gao, W., Oh, S., &amp; Viswanath, P. (2017). Density functional estimators with k-nearest neighbor bandwidths. IEEE International Symposium on Information Theory - Proceedings, 1, 1351â€“1355.
</p>
<p>Kraskov, A., Stogbauer, H., &amp; Grassberger, P. (2004). Estimating mutual information. Physical review E 69(6): 066138.
</p>


<h3>Author(s)</h3>

<p>Isaac Michaud
</p>

<hr>
<h2 id='estimate_mse'>Estimate MSE of LNC Estimator</h2><span id='topic+estimate_mse'></span>

<h3>Description</h3>

<p>Computes the MSE of the Local Non-Uniformity Correct (LNC) KSG estimator for a given value of the tuning parameter <code>alpha</code>, dimension, neighborhood order, and sample size.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>estimate_mse(k = 5, alpha = 0, d = 2, rho = 0, N = 1000,
  M = 100, cluster = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="estimate_mse_+3A_k">k</code></td>
<td>
<p>Neighborhood order.</p>
</td></tr>
<tr><td><code id="estimate_mse_+3A_alpha">alpha</code></td>
<td>
<p>Non-uniformity threshold (see details).</p>
</td></tr>
<tr><td><code id="estimate_mse_+3A_d">d</code></td>
<td>
<p>Dimension.</p>
</td></tr>
<tr><td><code id="estimate_mse_+3A_rho">rho</code></td>
<td>
<p>Reference correlation (see details).</p>
</td></tr>
<tr><td><code id="estimate_mse_+3A_n">N</code></td>
<td>
<p>Sample size.</p>
</td></tr>
<tr><td><code id="estimate_mse_+3A_m">M</code></td>
<td>
<p>Number of replications.</p>
</td></tr>
<tr><td><code id="estimate_mse_+3A_cluster">cluster</code></td>
<td>
<p>A <code>parallel</code> cluster object.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The parameter <code>alpha</code> controls the threshold for the application of the non-uniformity correction to a particular point's neighborhood. Roughly, <code>alpha</code> is the ratio of the PCA aligned neighborhood volume to the rectangular aligned neighborhood volume below which indicates non-uniformity and the correction is applied.
</p>
<p>If <code>alpha &lt; 0</code> then a log scale is assumed; otherwise [0,1] scale is used. <code>alpha &gt; 1</code> are unacceptable values. A value of <code>alpha = 0</code> forces no correction and LNC reverts to the KSG estimator.
</p>
<p>The reference distribution that is assumed is a mean-zero multivariate normal distribution with a compound-symmetric covariance. The covariance matrix has a single correlation parameter supplied by <code>rho</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>estimate_mse(N = 100,M = 2)
</code></pre>

<hr>
<h2 id='knn_mi'>kNN Mutual Information Estimators</h2><span id='topic+knn_mi'></span>

<h3>Description</h3>

<p>Computes mutual information based on the distribution of nearest neighborhood distances. Method available are KSG1 and KSG2 as described by Kraskov, et. al (2004) and the Local Non-Uniformity Corrected (LNC) KSG as described by Gao, et. al (2015). The LNC method is based on KSG2  but with PCA volume corrections to adjust for observed non-uniformity of the local neighborhood of each point in the sample.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>knn_mi(data, splits, options)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="knn_mi_+3A_data">data</code></td>
<td>
<p>Matrix of sample observations, each row is an observation.</p>
</td></tr>
<tr><td><code id="knn_mi_+3A_splits">splits</code></td>
<td>
<p>A vector that describes which sets of columns in <code>data</code> to compute the mutual information between. For example, to compute mutual information between two variables use <code>splits = c(1,1)</code>. To compute <em>redundancy</em> among multiple random variables use <code>splits = rep(1,ncol(data))</code>. To compute the mutual information between two random vector list the dimensions of each vector.</p>
</td></tr>
<tr><td><code id="knn_mi_+3A_options">options</code></td>
<td>
<p>A list that specifies the estimator and its necessary parameters (see details).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Current available methods are LNC, KSG1 and KSG2.
</p>
<p>For KSG1 use: <code>options = list(method = "KSG1", k = 5)</code>
</p>
<p>For KSG2 use: <code>options = list(method = "KSG2", k = 5)</code>
</p>
<p>For LNC use: <code>options = list(method = "LNC", k = 10, alpha = 0.65)</code>, order needed <code>k &gt; ncol(data)</code>.
</p>


<h3>Author</h3>

<p>Isaac Michaud, North Carolina State University, <a href="mailto:ijmichau@ncsu.edu">ijmichau@ncsu.edu</a>
</p>


<h3>References</h3>

<p>Gao, S., Ver Steeg G., &amp; Galstyan A. (2015). Efficient estimation of mutual information for strongly dependent variables. Artificial Intelligence and Statistics: 277-286.
</p>
<p>Kraskov, A., Stogbauer, H., &amp; Grassberger, P. (2004). Estimating mutual information. Physical review E 69(6): 066138.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(123)
x &lt;- rnorm(1000)
y &lt;- x + rnorm(1000)
knn_mi(cbind(x,y),c(1,1),options = list(method = "KSG2", k = 6))

set.seed(123)
x &lt;- rnorm(1000)
y &lt;- 100*x + rnorm(1000)
knn_mi(cbind(x,y),c(1,1),options = list(method = "LNC", alpha = 0.65, k = 10))
#approximate analytic value of mutual information
-0.5*log(1-cor(x,y)^2)

z &lt;- rnorm(1000)
#redundancy I(x;y;z) is approximately the same as I(x;y)
knn_mi(cbind(x,y,z),c(1,1,1),options = list(method = "LNC", alpha = c(0.5,0,0,0), k = 10))
#mutual information I((x,y);z) is approximately 0
knn_mi(cbind(x,y,z),c(2,1),options = list(method = "LNC", alpha = c(0.5,0.65,0), k = 10))

</code></pre>

<hr>
<h2 id='lnn_entropy'>Local Nearest Neighbor (LNN) Entropy Estimator</h2><span id='topic+lnn_entropy'></span>

<h3>Description</h3>

<p>Local Nearest Neighbor entropy estimator using Gaussian kernel and kNN selected bandwidth. Entropy is estimated by taking a Monte Carlo estimate using local kernel density estimate of the negative-log density.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lnn_entropy(data, k = 5, tr = 30, bw = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lnn_entropy_+3A_data">data</code></td>
<td>
<p>Matrix of sample observations, each row is an observation.</p>
</td></tr>
<tr><td><code id="lnn_entropy_+3A_k">k</code></td>
<td>
<p>Order of the local kNN bandwidth selection.</p>
</td></tr>
<tr><td><code id="lnn_entropy_+3A_tr">tr</code></td>
<td>
<p>Order of truncation (number of neighbors to include in entropy).</p>
</td></tr>
<tr><td><code id="lnn_entropy_+3A_bw">bw</code></td>
<td>
<p>Bandwidth (optional) manually fix bandwidth instead of using local kNN bandwidth selection.</p>
</td></tr>
</table>


<h3>References</h3>

<p>Loader, C. (1999). Local regression and likelihood. Springer Science &amp; Business Media.
</p>
<p>Gao, W., Oh, S., &amp; Viswanath, P. (2017). Density functional estimators with k-nearest neighbor bandwidths. IEEE International Symposium on Information Theory - Proceedings, 1, 1351â€“1355.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(123)
x &lt;- rnorm(1000)
print(lnn_entropy(x))
#analytic entropy
print(0.5*log(2*pi*exp(1)))

</code></pre>

<hr>
<h2 id='lnn_mi'>Local Nearest Neighbor (LNN) MI Estimator</h2><span id='topic+lnn_mi'></span>

<h3>Description</h3>

<p>Local Nearest Neighbor (LNN) mutual information estimator by Gao et al. 2017. This estimator uses the LNN entropy (<code>lnn_entropy</code>) estimator into the mutual information identity.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lnn_mi(data, splits, k = 5, tr = 30)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lnn_mi_+3A_data">data</code></td>
<td>
<p>Matrix of sample observations, each row is an observation.</p>
</td></tr>
<tr><td><code id="lnn_mi_+3A_splits">splits</code></td>
<td>
<p>A vector that describes which sets of columns in <code>data</code> to compute the mutual information between. For example, to compute mutual information between two variables use <code>splits = c(1,1)</code>. To compute <em>redundancy</em> among multiple random variables use <code>splits = rep(1,ncol(data))</code>. To compute the mutual information between two random vector list the dimensions of each vector.</p>
</td></tr>
<tr><td><code id="lnn_mi_+3A_k">k</code></td>
<td>
<p>Order of the local kNN bandwidth selection.</p>
</td></tr>
<tr><td><code id="lnn_mi_+3A_tr">tr</code></td>
<td>
<p>Order of truncation (number of neighbors to include in the local density estimation).</p>
</td></tr>
</table>


<h3>References</h3>

<p>Gao, W., Oh, S., &amp; Viswanath, P. (2017). Density functional estimators with k-nearest neighbor bandwidths. IEEE International Symposium on Information Theory - Proceedings, 1, 1351â€“1355.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>set.seed(123)
x &lt;- rnorm(1000)
y &lt;- x + rnorm(1000)
lnn_mi(cbind(x,y),c(1,1))

</code></pre>

<hr>
<h2 id='nearest_neighbors'>Compute Nearest Neighbors</h2><span id='topic+nearest_neighbors'></span>

<h3>Description</h3>

<p>Computes the nearest neighbor distances and indices of a sample using the infinite norm.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nearest_neighbors(data, k)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nearest_neighbors_+3A_data">data</code></td>
<td>
<p>Matrix of sample observations, each row is an observation.</p>
</td></tr>
<tr><td><code id="nearest_neighbors_+3A_k">k</code></td>
<td>
<p>Neighborhood order.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Nearest neighbors are computed using the brute-force method.
</p>


<h3>Value</h3>

<p>List of distances and indices of the k-nearest neighbors of each point in <code>data</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>X &lt;- cbind(1:10)
nearest_neighbors(X,3)

set.seed(123)
X &lt;- cbind(runif(100),runif(100))
plot(X,pch=20)
points(X[3,1],X[3,2],col='blue',pch=19, cex=1.5)
nn &lt;- nearest_neighbors(X,5)
a = X[nn$nn_inds[3,-1],1]
b = X[nn$nn_inds[3,-1],2]
points(a,b,col='red',pch=19, cex=1.5)
</code></pre>

<hr>
<h2 id='optimize_mse'>Optimize MSE of LNC Estimator</h2><span id='topic+optimize_mse'></span>

<h3>Description</h3>

<p>Gaussian process (GP) optimization is used to minimize the MSE of the LNC estimator with respect to the non-uniformity threshold parameter <code>alpha</code>. A normal distribution with compound-symmetric covariance is used as a reference distribution to optimize the MSE of LNC with respect to.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>optimize_mse(rho, N, M, d, k, lower = -10, upper = -1e-10,
  num_iter = 10, init_size = 20, cluster = NULL, verbose = TRUE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="optimize_mse_+3A_rho">rho</code></td>
<td>
<p>Reference correlation.</p>
</td></tr>
<tr><td><code id="optimize_mse_+3A_n">N</code></td>
<td>
<p>Sample size.</p>
</td></tr>
<tr><td><code id="optimize_mse_+3A_m">M</code></td>
<td>
<p>Number of replications.</p>
</td></tr>
<tr><td><code id="optimize_mse_+3A_d">d</code></td>
<td>
<p>Dimension.</p>
</td></tr>
<tr><td><code id="optimize_mse_+3A_k">k</code></td>
<td>
<p>Neighborhood order.</p>
</td></tr>
<tr><td><code id="optimize_mse_+3A_lower">lower</code></td>
<td>
<p>Lower bound for optimization.</p>
</td></tr>
<tr><td><code id="optimize_mse_+3A_upper">upper</code></td>
<td>
<p>Upper bound for optimization.</p>
</td></tr>
<tr><td><code id="optimize_mse_+3A_num_iter">num_iter</code></td>
<td>
<p>Number of iterations of GP optimization.</p>
</td></tr>
<tr><td><code id="optimize_mse_+3A_init_size">init_size</code></td>
<td>
<p>Number of initial evaluation to estimating GP.</p>
</td></tr>
<tr><td><code id="optimize_mse_+3A_cluster">cluster</code></td>
<td>
<p>A <code>parallel</code> cluster object.</p>
</td></tr>
<tr><td><code id="optimize_mse_+3A_verbose">verbose</code></td>
<td>
<p>If <code>TRUE</code> then print runtime diagnostic output.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The package <code>tgp</code> is used to fit a treed-GP to the MSE estimates of LNC. A treed-GP is used because the MSE of LNC with respect to <code>alpha</code> exhibits clear non-stationarity. A treed-GP is able to identify the function's different correlation lengths which improves optimization.
</p>

<hr>
<h2 id='rmvn'>Random Multivariate Normal MVN Generator</h2><span id='topic+rmvn'></span>

<h3>Description</h3>

<p>Random Multivariate Normal MVN Generator
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rmvn(n, mu = rep(0, d), cov_mat)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="rmvn_+3A_n">n</code></td>
<td>
<p>number of observations</p>
</td></tr>
<tr><td><code id="rmvn_+3A_mu">mu</code></td>
<td>
<p>d dimensional mean vector</p>
</td></tr>
<tr><td><code id="rmvn_+3A_cov_mat">cov_mat</code></td>
<td>
<p>d x d covariance matrix</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Matrix (n x d) of random MVN draws
</p>

<hr>
<h2 id='simulate_mvn'>Calibration Random Multivariate Normal</h2><span id='topic+simulate_mvn'></span>

<h3>Description</h3>

<p>Calibration Random Multivariate Normal
</p>


<h3>Usage</h3>

<pre><code class='language-R'>simulate_mvn(n, d, rho)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="simulate_mvn_+3A_n">n</code></td>
<td>
<p>number of observations</p>
</td></tr>
<tr><td><code id="simulate_mvn_+3A_d">d</code></td>
<td>
<p>number of dimension</p>
</td></tr>
<tr><td><code id="simulate_mvn_+3A_rho">rho</code></td>
<td>
<p>correlation of compound-symmetric covariance</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Matrix (n x d) of random MVN draws
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
