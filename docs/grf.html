<!DOCTYPE html><html lang="en"><head><title>Help for package grf</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {grf}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#grf-package'><p>grf: Generalized Random Forests</p></a></li>
<li><a href='#average_treatment_effect'><p>Get doubly robust estimates of average treatment effects.</p></a></li>
<li><a href='#best_linear_projection'><p>Estimate the best linear projection of a conditional average treatment effect.</p></a></li>
<li><a href='#boosted_regression_forest'><p>Boosted regression forest</p></a></li>
<li><a href='#boot_grf'><p>Simple clustered bootstrap.</p></a></li>
<li><a href='#causal_forest'><p>Causal forest</p></a></li>
<li><a href='#causal_survival_forest'><p>Causal survival forest</p></a></li>
<li><a href='#create_dot_body'><p>Writes each node information</p>
If it is a leaf node: show it in different color, show number of samples, show leaf id
If it is a non-leaf node: show its splitting variable and splitting value
If trained with missing values, the edge arrow is filled according to which direction the NAs are sent.</a></li>
<li><a href='#estimate_rate'><p>Compute rate estimates, a function to be passed on to bootstrap routine.</p></a></li>
<li><a href='#expected_survival'><p>Compute E[T | X]</p></a></li>
<li><a href='#export_graphviz'><p>Export a tree in DOT format.</p>
This function generates a GraphViz representation of the tree,
which is then written into 'dot_string'.</a></li>
<li><a href='#generate_causal_data'><p>Generate causal forest data</p></a></li>
<li><a href='#generate_causal_survival_data'><p>Simulate causal survival data</p></a></li>
<li><a href='#get_forest_weights'><p>Given a trained forest and test data, compute the kernel weights for each test point.</p></a></li>
<li><a href='#get_leaf_node'><p>Find the leaf node for a test sample.</p></a></li>
<li><a href='#get_scores'><p>Compute doubly robust scores for a GRF forest object</p></a></li>
<li><a href='#get_scores.causal_forest'><p>Compute doubly robust scores for a causal forest.</p></a></li>
<li><a href='#get_scores.causal_survival_forest'><p>Compute doubly robust scores for a causal survival forest.</p></a></li>
<li><a href='#get_scores.instrumental_forest'><p>Doubly robust scores for estimating the average conditional local average treatment effect.</p></a></li>
<li><a href='#get_scores.multi_arm_causal_forest'><p>Compute doubly robust scores for a multi arm causal forest.</p></a></li>
<li><a href='#get_tree'><p>Retrieve a single tree from a trained forest object.</p></a></li>
<li><a href='#grf_options'><p>grf package options</p></a></li>
<li><a href='#instrumental_forest'><p>Intrumental forest</p></a></li>
<li><a href='#leaf_stats.causal_forest'><p>Calculate summary stats given a set of samples for causal forests.</p></a></li>
<li><a href='#leaf_stats.default'><p>A default leaf_stats for forests classes without a leaf_stats method</p>
that always returns NULL.</a></li>
<li><a href='#leaf_stats.instrumental_forest'><p>Calculate summary stats given a set of samples for instrumental forests.</p></a></li>
<li><a href='#leaf_stats.regression_forest'><p>Calculate summary stats given a set of samples for regression forests.</p></a></li>
<li><a href='#ll_regression_forest'><p>Local linear forest</p></a></li>
<li><a href='#lm_forest'><p>LM Forest</p></a></li>
<li><a href='#merge_forests'><p>Merges a list of forests that were grown using the same data into one large forest.</p></a></li>
<li><a href='#multi_arm_causal_forest'><p>Multi-arm/multi-outcome causal forest</p></a></li>
<li><a href='#multi_regression_forest'><p>Multi-task regression forest</p></a></li>
<li><a href='#plot.grf_tree'><p>Plot a GRF tree object.</p></a></li>
<li><a href='#plot.rank_average_treatment_effect'><p>Plot the Targeting Operator Characteristic curve.</p></a></li>
<li><a href='#predict.boosted_regression_forest'><p>Predict with a boosted regression forest.</p></a></li>
<li><a href='#predict.causal_forest'><p>Predict with a causal forest</p></a></li>
<li><a href='#predict.causal_survival_forest'><p>Predict with a causal survival forest forest</p></a></li>
<li><a href='#predict.instrumental_forest'><p>Predict with an instrumental forest</p></a></li>
<li><a href='#predict.ll_regression_forest'><p>Predict with a local linear forest</p></a></li>
<li><a href='#predict.lm_forest'><p>Predict with a lm forest</p></a></li>
<li><a href='#predict.multi_arm_causal_forest'><p>Predict with a multi arm causal forest</p></a></li>
<li><a href='#predict.multi_regression_forest'><p>Predict with a multi regression forest</p></a></li>
<li><a href='#predict.probability_forest'><p>Predict with a probability forest</p></a></li>
<li><a href='#predict.quantile_forest'><p>Predict with a quantile forest</p></a></li>
<li><a href='#predict.regression_forest'><p>Predict with a regression forest</p></a></li>
<li><a href='#predict.survival_forest'><p>Predict with a survival forest</p></a></li>
<li><a href='#print.boosted_regression_forest'><p>Print a boosted regression forest</p></a></li>
<li><a href='#print.grf'><p>Print a GRF forest object.</p></a></li>
<li><a href='#print.grf_tree'><p>Print a GRF tree object.</p></a></li>
<li><a href='#print.rank_average_treatment_effect'><p>Print the Rank-Weighted Average Treatment Effect (RATE).</p></a></li>
<li><a href='#print.tuning_output'><p>Print tuning output.</p>
Displays average error for q-quantiles of tuned parameters.</a></li>
<li><a href='#probability_forest'><p>Probability forest</p></a></li>
<li><a href='#quantile_forest'><p>Quantile forest</p></a></li>
<li><a href='#rank_average_treatment_effect'><p>Estimate a Rank-Weighted Average Treatment Effect (RATE).</p></a></li>
<li><a href='#rank_average_treatment_effect.fit'><p>Fitter function for Rank-Weighted Average Treatment Effect (RATE).</p></a></li>
<li><a href='#regression_forest'><p>Regression forest</p></a></li>
<li><a href='#split_frequencies'><p>Calculate which features the forest split on at each depth.</p></a></li>
<li><a href='#survival_forest'><p>Survival forest</p></a></li>
<li><a href='#test_calibration'><p>Omnibus evaluation of the quality of the random forest estimates via calibration.</p></a></li>
<li><a href='#tune_forest'><p>Tune a forest</p></a></li>
<li><a href='#tune_ll_causal_forest'><p>Local linear forest tuning</p></a></li>
<li><a href='#tune_ll_regression_forest'><p>Local linear forest tuning</p></a></li>
<li><a href='#variable_importance'><p>Calculate a simple measure of 'importance' for each feature.</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Title:</td>
<td>Generalized Random Forests</td>
</tr>
<tr>
<td>Version:</td>
<td>2.4.0</td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/grf-labs/grf/issues">https://github.com/grf-labs/grf/issues</a></td>
</tr>
<tr>
<td>Description:</td>
<td>Forest-based statistical estimation and inference.
  GRF provides non-parametric methods for heterogeneous treatment effects estimation
  (optionally using right-censored outcomes, multiple treatment arms or outcomes, or instrumental variables),
  as well as least-squares regression, quantile regression, and survival regression,
  all with support for missing covariates.</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5.0)</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a></td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp, RcppEigen</td>
</tr>
<tr>
<td>Imports:</td>
<td>DiceKriging, lmtest, Matrix, methods, Rcpp (&ge; 0.12.15),
sandwich (&ge; 2.4-0)</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.2</td>
</tr>
<tr>
<td>Suggests:</td>
<td>DiagrammeR, MASS, rdd, survival (&ge; 3.2-8), testthat (&ge;
3.0.4)</td>
</tr>
<tr>
<td>SystemRequirements:</td>
<td>GNU make</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/grf-labs/grf">https://github.com/grf-labs/grf</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-11-15 03:09:39 UTC; erikcs</td>
</tr>
<tr>
<td>Author:</td>
<td>Julie Tibshirani [aut],
  Susan Athey [aut],
  Rina Friedberg [ctb],
  Vitor Hadad [ctb],
  David Hirshberg [ctb],
  Luke Miner [ctb],
  Erik Sverdrup [aut, cre],
  Stefan Wager [aut],
  Marvin Wright [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Erik Sverdrup &lt;erik.sverdrup@monash.edu&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-11-15 10:10:09 UTC</td>
</tr>
</table>
<hr>
<h2 id='grf-package'>grf: Generalized Random Forests</h2><span id='topic+grf'></span><span id='topic+grf-package'></span>

<h3>Description</h3>

<p>A package for forest-based statistical estimation and inference. GRF provides non-parametric methods for heterogeneous treatment effects estimation (optionally using right-censored outcomes, multiple treatment arms or outcomes, or instrumental variables), as well as least-squares regression, quantile regression, and survival regression, all with support for missing covariates.
</p>
<p>In addition, GRF supports 'honest' estimation (where one subset of the data is used for choosing splits, and another for populating the leaves of the tree), and confidence intervals for least-squares regression and treatment effect estimation.
</p>
<p>Some helpful links for getting started:
</p>
<p>* The R package documentation contains usage examples and method reference (<a href="https://grf-labs.github.io/grf/">https://grf-labs.github.io/grf/</a>).
</p>
<p>* The GRF reference gives a detailed description of the GRF algorithm and includes troubleshooting suggestions (<a href="https://grf-labs.github.io/grf/REFERENCE.html">https://grf-labs.github.io/grf/REFERENCE.html</a>).
</p>
<p>* For community questions and answers around usage, see Github issues labelled 'question' (<a href="https://github.com/grf-labs/grf/issues?q=label%3Aquestion">https://github.com/grf-labs/grf/issues?q=label%3Aquestion</a>).
</p>


<h3>Author(s)</h3>

<p><strong>Maintainer</strong>: Erik Sverdrup <a href="mailto:erik.sverdrup@monash.edu">erik.sverdrup@monash.edu</a>
</p>
<p>Authors:
</p>

<ul>
<li><p> Julie Tibshirani <a href="mailto:jtibs@cs.stanford.edu">jtibs@cs.stanford.edu</a>
</p>
</li>
<li><p> Susan Athey
</p>
</li>
<li><p> Stefan Wager
</p>
</li></ul>

<p>Other contributors:
</p>

<ul>
<li><p> Rina Friedberg [contributor]
</p>
</li>
<li><p> Vitor Hadad [contributor]
</p>
</li>
<li><p> David Hirshberg [contributor]
</p>
</li>
<li><p> Luke Miner [contributor]
</p>
</li>
<li><p> Marvin Wright [contributor]
</p>
</li></ul>



<h3>See Also</h3>

<p>Useful links:
</p>

<ul>
<li> <p><a href="https://github.com/grf-labs/grf">https://github.com/grf-labs/grf</a>
</p>
</li>
<li><p> Report bugs at <a href="https://github.com/grf-labs/grf/issues">https://github.com/grf-labs/grf/issues</a>
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>
# The following script demonstrates how to use GRF for heterogeneous treatment
# effect estimation. For examples of how to use other types of forest,
# please consult the documentation on the relevant forest methods (quantile_forest,
# instrumental_forest, etc.).

# Generate data.
n &lt;- 2000; p &lt;- 10
X &lt;- matrix(rnorm(n*p), n, p)
X.test &lt;- matrix(0, 101, p)
X.test[,1] &lt;- seq(-2, 2, length.out = 101)

# Train a causal forest.
W &lt;- rbinom(n, 1, 0.4 + 0.2 * (X[,1] &gt; 0))
Y &lt;- pmax(X[,1], 0) * W + X[,2] + pmin(X[,3], 0) + rnorm(n)
tau.forest &lt;- causal_forest(X, Y, W)

# Estimate treatment effects for the training data using out-of-bag prediction.
tau.hat.oob &lt;- predict(tau.forest)
hist(tau.hat.oob$predictions)

# Estimate treatment effects for the test sample.
tau.hat &lt;- predict(tau.forest, X.test)
plot(X.test[,1], tau.hat$predictions, ylim = range(tau.hat$predictions, 0, 2),
	xlab = "x", ylab = "tau", type = "l")
lines(X.test[,1], pmax(0, X.test[,1]), col = 2, lty = 2)

# Estimate the conditional average treatment effect on the full sample (CATE).
average_treatment_effect(tau.forest, target.sample = "all")

# Estimate the conditional average treatment effect on the treated sample (CATT).
average_treatment_effect(tau.forest, target.sample = "treated")

# Add confidence intervals for heterogeneous treatment effects; growing more
# trees is now recommended.
tau.forest &lt;- causal_forest(X, Y, W, num.trees = 4000)
tau.hat &lt;- predict(tau.forest, X.test, estimate.variance = TRUE)
sigma.hat &lt;- sqrt(tau.hat$variance.estimates)

ylim &lt;- range(tau.hat$predictions + 1.96 * sigma.hat, tau.hat$predictions - 1.96 * sigma.hat, 0, 2)
plot(X.test[,1], tau.hat$predictions, ylim = ylim, xlab = "x", ylab = "tau", type = "l")
lines(X.test[,1], tau.hat$predictions + 1.96 * sigma.hat, col = 1, lty = 2)
lines(X.test[,1], tau.hat$predictions - 1.96 * sigma.hat, col = 1, lty = 2)
lines(X.test[,1], pmax(0, X.test[,1]), col = 2, lty = 1)

# In some examples, pre-fitting models for Y and W separately may
# be helpful (e.g., if different models use different covariates).
# In some applications, one may even want to get Y.hat and W.hat
# using a completely different method (e.g., boosting).

# Generate new data.
n &lt;- 4000; p &lt;- 20
X &lt;- matrix(rnorm(n * p), n, p)
TAU &lt;- 1 / (1 + exp(-X[, 3]))
W &lt;- rbinom(n ,1, 1 / (1 + exp(-X[, 1] - X[, 2])))
Y &lt;- pmax(X[, 2] + X[, 3], 0) + rowMeans(X[, 4:6]) / 2 + W * TAU + rnorm(n)

forest.W &lt;- regression_forest(X, W, tune.parameters = "all")
W.hat &lt;- predict(forest.W)$predictions

forest.Y &lt;- regression_forest(X, Y, tune.parameters = "all")
Y.hat &lt;- predict(forest.Y)$predictions

forest.Y.varimp &lt;- variable_importance(forest.Y)

# Note: Forests may have a hard time when trained on very few variables
# (e.g., ncol(X) = 1, 2, or 3). We recommend not being too aggressive
# in selection.
selected.vars &lt;- which(forest.Y.varimp / mean(forest.Y.varimp) &gt; 0.2)

tau.forest &lt;- causal_forest(X[, selected.vars], Y, W,
                           W.hat = W.hat, Y.hat = Y.hat,
                           tune.parameters = "all")

# See if a causal forest succeeded in capturing heterogeneity by plotting
# the TOC and calculating a 95% CI for the AUTOC.
train &lt;- sample(1:n, n / 2)
train.forest &lt;- causal_forest(X[train, ], Y[train], W[train])
eval.forest &lt;- causal_forest(X[-train, ], Y[-train], W[-train])
rate &lt;- rank_average_treatment_effect(eval.forest,
                                      predict(train.forest, X[-train, ])$predictions)
plot(rate)
paste("AUTOC:", round(rate$estimate, 2), "+/", round(1.96 * rate$std.err, 2))


</code></pre>

<hr>
<h2 id='average_treatment_effect'>Get doubly robust estimates of average treatment effects.</h2><span id='topic+average_treatment_effect'></span>

<h3>Description</h3>

<p>In the case of a causal forest with binary treatment, we provide
estimates of one of the following:
</p>

<ul>
<li><p> The average treatment effect (target.sample = all): E[Y(1) - Y(0)]
</p>
</li>
<li><p> The average treatment effect on the treated (target.sample = treated): E[Y(1) - Y(0) | Wi = 1]
</p>
</li>
<li><p> The average treatment effect on the controls (target.sample = control): E[Y(1) - Y(0) | Wi = 0]
</p>
</li>
<li><p> The overlap-weighted average treatment effect (target.sample = overlap):
E[e(X) (1 - e(X)) (Y(1) - Y(0))] / E[e(X) (1 - e(X)), where e(x) = P[Wi = 1 | Xi = x].
</p>
</li></ul>

<p>This last estimand is recommended by Li, Morgan, and Zaslavsky (2018)
in case of poor overlap (i.e., when the propensities e(x) may be very close
to 0 or 1), as it doesn't involve dividing by estimated propensities.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>average_treatment_effect(
  forest,
  target.sample = c("all", "treated", "control", "overlap"),
  method = c("AIPW", "TMLE"),
  subset = NULL,
  debiasing.weights = NULL,
  compliance.score = NULL,
  num.trees.for.weights = 500
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="average_treatment_effect_+3A_forest">forest</code></td>
<td>
<p>The trained forest.</p>
</td></tr>
<tr><td><code id="average_treatment_effect_+3A_target.sample">target.sample</code></td>
<td>
<p>Which sample to aggregate treatment effects over.
Note: Options other than &quot;all&quot; are only currently implemented
for causal forests.</p>
</td></tr>
<tr><td><code id="average_treatment_effect_+3A_method">method</code></td>
<td>
<p>Method used for doubly robust inference. Can be either
augmented inverse-propensity weighting (AIPW), or
targeted maximum likelihood estimation (TMLE). Note:
TMLE is currently only implemented for causal forests with
a binary treatment.</p>
</td></tr>
<tr><td><code id="average_treatment_effect_+3A_subset">subset</code></td>
<td>
<p>Specifies subset of the training examples over which we
estimate the ATE. WARNING: For valid statistical performance,
the subset should be defined only using features Xi, not using
the treatment Wi or the outcome Yi.</p>
</td></tr>
<tr><td><code id="average_treatment_effect_+3A_debiasing.weights">debiasing.weights</code></td>
<td>
<p>A vector of length n (or the subset length) of debiasing weights.
If NULL (default) these are obtained via the appropriate doubly robust score
construction, e.g., in the case of causal_forests with a binary treatment, they
are obtained via inverse-propensity weighting.</p>
</td></tr>
<tr><td><code id="average_treatment_effect_+3A_compliance.score">compliance.score</code></td>
<td>
<p>Only used with instrumental forests. An estimate of the causal
effect of Z on W, i.e., Delta(X) = E[W | X, Z = 1] - E[W | X, Z = 0],
which can then be used to produce debiasing.weights. If not provided,
this is estimated via an auxiliary causal forest.</p>
</td></tr>
<tr><td><code id="average_treatment_effect_+3A_num.trees.for.weights">num.trees.for.weights</code></td>
<td>
<p>In some cases (e.g., with causal forests with a continuous
treatment), we need to train auxiliary forests to learn debiasing weights.
This is the number of trees used for this task. Note: this argument is only
used when debiasing.weights = NULL.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>In the case of a causal forest with continuous treatment, we provide estimates of the
average partial effect, i.e., E[Cov[W, Y | X] / Var[W | X]]. In the case of a binary treatment,
the average partial effect matches the average treatment effect. Computing the average partial
effect is somewhat more involved, as the relevant doubly robust scores require an estimate
of Var[Wi | Xi = x]. By default, we get such estimates by training an auxiliary forest;
however, these weights can also be passed manually by specifying debiasing.weights.
</p>
<p>In the case of instrumental forests with a binary treatment, we provide an estimate
of the the Average (Conditional) Local Average Treatment (ACLATE).
Specifically, given an outcome Y, treatment W and instrument Z, the (conditional) local
average treatment effect is tau(x) = Cov[Y, Z | X = x] / Cov[W, Z | X = x].
This is the quantity that is estimated with an instrumental forest.
It can be intepreted causally in various ways. Given a homogeneity
assumption, tau(x) is simply the CATE at x. When W is binary
and there are no &quot;defiers&quot;, Imbens and Angrist (1994) show that tau(x) can
be interpreted as an average treatment effect on compliers. This function
provides an estimate of tau = E[tau(X)]. See Chernozhukov
et al. (2022) for a discussion, and Section 5.2 of Athey and Wager (2021)
for an example using forests.
</p>
<p>If clusters are specified, then each unit gets equal weight by default. For
example, if there are 10 clusters with 1 unit each and per-cluster ATE = 1,
and there are 10 clusters with 19 units each and per-cluster ATE = 0, then
the overall ATE is 0.05 (additional sample.weights allow for custom
weighting). If equalize.cluster.weights = TRUE each cluster gets equal weight
and the overall ATE is 0.5.
</p>


<h3>Value</h3>

<p>An estimate of the average treatment effect, along with standard error.
</p>


<h3>References</h3>

<p>Athey, Susan, and Stefan Wager. &quot;Policy Learning With Observational Data.&quot;
Econometrica 89.1 (2021): 133-161.
</p>
<p>Chernozhukov, Victor, Juan Carlos Escanciano, Hidehiko Ichimura,
Whitney K. Newey, and James M. Robins. &quot;Locally robust semiparametric
estimation.&quot; Econometrica 90(4), 2022.
</p>
<p>Imbens, Guido W., and Joshua D. Angrist. &quot;Identification and Estimation of
Local Average Treatment Effects.&quot; Econometrica 62(2), 1994.
</p>
<p>Li, Fan, Kari Lock Morgan, and Alan M. Zaslavsky.
&quot;Balancing covariates via propensity score weighting.&quot;
Journal of the American Statistical Association 113(521), 2018.
</p>
<p>Mayer, Imke, Erik Sverdrup, Tobias Gauss, Jean-Denis Moyer, Stefan Wager, and Julie Josse.
&quot;Doubly robust treatment effect estimation with missing attributes.&quot;
Annals of Applied Statistics, 14(3), 2020.
</p>
<p>Robins, James M., and Andrea Rotnitzky. &quot;Semiparametric efficiency in
multivariate regression models with missing data.&quot; Journal of the
American Statistical Association 90(429), 1995.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Train a causal forest.
n &lt;- 50
p &lt;- 10
X &lt;- matrix(rnorm(n * p), n, p)
W &lt;- rbinom(n, 1, 0.5)
Y &lt;- pmax(X[, 1], 0) * W + X[, 2] + pmin(X[, 3], 0) + rnorm(n)
c.forest &lt;- causal_forest(X, Y, W)

# Predict using the forest.
X.test &lt;- matrix(0, 101, p)
X.test[, 1] &lt;- seq(-2, 2, length.out = 101)
c.pred &lt;- predict(c.forest, X.test)
# Estimate the conditional average treatment effect on the full sample (CATE).
average_treatment_effect(c.forest, target.sample = "all")

# Estimate the conditional average treatment effect on the treated sample (CATT).
# We don't expect much difference between the CATE and the CATT in this example,
# since treatment assignment was randomized.
average_treatment_effect(c.forest, target.sample = "treated")

# Estimate the conditional average treatment effect on samples with positive X[,1].
average_treatment_effect(c.forest, target.sample = "all", subset = X[, 1] &gt; 0)

# Example for causal forests with a continuous treatment.
n &lt;- 2000
p &lt;- 10
X &lt;- matrix(rnorm(n * p), n, p)
W &lt;- rbinom(n, 1, 1 / (1 + exp(-X[, 2]))) + rnorm(n)
Y &lt;- pmax(X[, 1], 0) * W + X[, 2] + pmin(X[, 3], 0) + rnorm(n)
tau.forest &lt;- causal_forest(X, Y, W)
tau.hat &lt;- predict(tau.forest)
average_treatment_effect(tau.forest)
average_treatment_effect(tau.forest, subset = X[, 1] &gt; 0)


</code></pre>

<hr>
<h2 id='best_linear_projection'>Estimate the best linear projection of a conditional average treatment effect.</h2><span id='topic+best_linear_projection'></span>

<h3>Description</h3>

<p>Let tau(Xi) = E[Y(1) - Y(0) | X = Xi] be the CATE, and Ai be a vector of user-provided
covariates. This function provides a (doubly robust) fit to the linear model tau(Xi) ~ beta_0 + Ai * beta.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>best_linear_projection(
  forest,
  A = NULL,
  subset = NULL,
  debiasing.weights = NULL,
  compliance.score = NULL,
  num.trees.for.weights = 500,
  vcov.type = "HC3",
  target.sample = c("all", "overlap")
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="best_linear_projection_+3A_forest">forest</code></td>
<td>
<p>The trained forest.</p>
</td></tr>
<tr><td><code id="best_linear_projection_+3A_a">A</code></td>
<td>
<p>The covariates we want to project the CATE onto.</p>
</td></tr>
<tr><td><code id="best_linear_projection_+3A_subset">subset</code></td>
<td>
<p>Specifies subset of the training examples over which we
estimate the ATE. WARNING: For valid statistical performance,
the subset should be defined only using features Xi, not using
the treatment Wi or the outcome Yi.</p>
</td></tr>
<tr><td><code id="best_linear_projection_+3A_debiasing.weights">debiasing.weights</code></td>
<td>
<p>A vector of length n (or the subset length) of debiasing weights.
If NULL (default) these are obtained via the appropriate doubly robust score
construction, e.g., in the case of causal_forests with a binary treatment, they
are obtained via inverse-propensity weighting.</p>
</td></tr>
<tr><td><code id="best_linear_projection_+3A_compliance.score">compliance.score</code></td>
<td>
<p>Only used with instrumental forests. An estimate of the causal
effect of Z on W, i.e., Delta(X) = E[W | X, Z = 1] - E[W | X, Z = 0],
which can then be used to produce debiasing.weights. If not provided,
this is estimated via an auxiliary causal forest.</p>
</td></tr>
<tr><td><code id="best_linear_projection_+3A_num.trees.for.weights">num.trees.for.weights</code></td>
<td>
<p>In some cases (e.g., with causal forests with a continuous
treatment), we need to train auxiliary forests to learn debiasing weights.
This is the number of trees used for this task. Note: this argument is only
used when debiasing.weights = NULL.</p>
</td></tr>
<tr><td><code id="best_linear_projection_+3A_vcov.type">vcov.type</code></td>
<td>
<p>Optional covariance type for standard errors. The possible
options are HC0, ..., HC3. The default is &quot;HC3&quot;, which is recommended in small
samples and corresponds to the &quot;shortcut formula&quot; for the jackknife
(see MacKinnon &amp; White for more discussion, and Cameron &amp; Miller for a review).
For large data sets with clusters, &quot;HC0&quot; or &quot;HC1&quot; are significantly faster to compute.</p>
</td></tr>
<tr><td><code id="best_linear_projection_+3A_target.sample">target.sample</code></td>
<td>
<p>Which sample to compute the BLP over. The default is &quot;all&quot;.
Option &quot;overlap&quot; uses weights equal to e(X)(1 - e(X)), where e(x) are estimates of
the propensity score.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Procedurally, we do so by regressing doubly robust scores derived from the
forest against the Ai. Note the covariates Ai may consist of a subset of the Xi,
or they may be distinct. The case of the null model tau(Xi) ~ beta_0 is equivalent
to fitting an average treatment effect via AIPW.
</p>
<p>In the event the treatment is continuous the inverse-propensity weight component of the
double robust scores are replaced with a component based on a forest based
estimate of Var[Wi | Xi = x]. These weights can also be passed manually by specifying
debiasing.weights.
</p>


<h3>Value</h3>

<p>An estimate of the best linear projection, along with coefficient standard errors.
</p>


<h3>References</h3>

<p>Cameron, A. Colin, and Douglas L. Miller. &quot;A practitioner's guide to
cluster-robust inference.&quot; Journal of Human Resources 50, no. 2 (2015): 317-372.
</p>
<p>Cui, Yifan, Michael R. Kosorok, Erik Sverdrup, Stefan Wager, and Ruoqing Zhu.
&quot;Estimating Heterogeneous Treatment Effects with Right-Censored Data via Causal Survival Forests.&quot;
Journal of the Royal Statistical Society: Series B, 85(2), 2023.
</p>
<p>MacKinnon, James G., and Halbert White. &quot;Some heteroskedasticity-consistent
covariance matrix estimators with improved finite sample properties.&quot;
Journal of Econometrics 29.3 (1985): 305-325.
</p>
<p>Semenova, Vira, and Victor Chernozhukov. &quot;Debiased Machine Learning of
Conditional Average Treatment Effects and Other Causal Functions&quot;.
The Econometrics Journal 24.2 (2021).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
n &lt;- 800
p &lt;- 5
X &lt;- matrix(rnorm(n * p), n, p)
W &lt;- rbinom(n, 1, 0.25 + 0.5 * (X[, 1] &gt; 0))
Y &lt;- pmax(X[, 1], 0) * W + X[, 2] + pmin(X[, 3], 0) + rnorm(n)
forest &lt;- causal_forest(X, Y, W)
best_linear_projection(forest, X[,1:2])


</code></pre>

<hr>
<h2 id='boosted_regression_forest'>Boosted regression forest</h2><span id='topic+boosted_regression_forest'></span>

<h3>Description</h3>

<p>Trains a boosted regression forest that can be used to estimate
the conditional mean function mu(x) = E[Y | X = x]. Selects
number of boosting iterations based on cross-validation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>boosted_regression_forest(
  X,
  Y,
  num.trees = 2000,
  sample.weights = NULL,
  clusters = NULL,
  equalize.cluster.weights = FALSE,
  sample.fraction = 0.5,
  mtry = min(ceiling(sqrt(ncol(X)) + 20), ncol(X)),
  min.node.size = 5,
  honesty = TRUE,
  honesty.fraction = 0.5,
  honesty.prune.leaves = TRUE,
  alpha = 0.05,
  imbalance.penalty = 0,
  ci.group.size = 2,
  tune.parameters = "none",
  tune.num.trees = 10,
  tune.num.reps = 100,
  tune.num.draws = 1000,
  boost.steps = NULL,
  boost.error.reduction = 0.97,
  boost.max.steps = 5,
  boost.trees.tune = 10,
  num.threads = NULL,
  seed = runif(1, 0, .Machine$integer.max)
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="boosted_regression_forest_+3A_x">X</code></td>
<td>
<p>The covariates used in the regression.</p>
</td></tr>
<tr><td><code id="boosted_regression_forest_+3A_y">Y</code></td>
<td>
<p>The outcome.</p>
</td></tr>
<tr><td><code id="boosted_regression_forest_+3A_num.trees">num.trees</code></td>
<td>
<p>Number of trees grown in the forest. Note: Getting accurate
confidence intervals generally requires more trees than
getting accurate predictions. Default is 2000.</p>
</td></tr>
<tr><td><code id="boosted_regression_forest_+3A_sample.weights">sample.weights</code></td>
<td>
<p>Weights given to each observation in estimation.
If NULL, each observation receives the same weight. Default is NULL.</p>
</td></tr>
<tr><td><code id="boosted_regression_forest_+3A_clusters">clusters</code></td>
<td>
<p>Vector of integers or factors specifying which cluster each observation corresponds to.
Default is NULL (ignored).</p>
</td></tr>
<tr><td><code id="boosted_regression_forest_+3A_equalize.cluster.weights">equalize.cluster.weights</code></td>
<td>
<p>If FALSE, each unit is given the same weight (so that bigger
clusters get more weight). If TRUE, each cluster is given equal weight in the forest. In this case,
during training, each tree uses the same number of observations from each drawn cluster: If the
smallest cluster has K units, then when we sample a cluster during training, we only give a random
K elements of the cluster to the tree-growing procedure. When estimating average treatment effects,
each observation is given weight 1/cluster size, so that the total weight of each cluster is the
same. Note that, if this argument is FALSE, sample weights may also be directly adjusted via the
sample.weights argument. If this argument is TRUE, sample.weights must be set to NULL. Default is
FALSE.</p>
</td></tr>
<tr><td><code id="boosted_regression_forest_+3A_sample.fraction">sample.fraction</code></td>
<td>
<p>Fraction of the data used to build each tree.
Note: If honesty = TRUE, these subsamples will
further be cut by a factor of honesty.fraction. Default is 0.5.</p>
</td></tr>
<tr><td><code id="boosted_regression_forest_+3A_mtry">mtry</code></td>
<td>
<p>Number of variables tried for each split. Default is
<code class="reqn">\sqrt p + 20</code> where p is the number of variables.</p>
</td></tr>
<tr><td><code id="boosted_regression_forest_+3A_min.node.size">min.node.size</code></td>
<td>
<p>A target for the minimum number of observations in each tree leaf. Note that nodes
with size smaller than min.node.size can occur, as in the original randomForest package.
Default is 5.</p>
</td></tr>
<tr><td><code id="boosted_regression_forest_+3A_honesty">honesty</code></td>
<td>
<p>Whether to use honest splitting (i.e., sub-sample splitting). Default is TRUE.
For a detailed description of honesty, honesty.fraction, honesty.prune.leaves, and recommendations for
parameter tuning, see the grf algorithm reference.</p>
</td></tr>
<tr><td><code id="boosted_regression_forest_+3A_honesty.fraction">honesty.fraction</code></td>
<td>
<p>The fraction of data that will be used for determining splits if honesty = TRUE. Corresponds
to set J1 in the notation of the paper. Default is 0.5 (i.e. half of the data is used for
determining splits).</p>
</td></tr>
<tr><td><code id="boosted_regression_forest_+3A_honesty.prune.leaves">honesty.prune.leaves</code></td>
<td>
<p>If TRUE, prunes the estimation sample tree such that no leaves
are empty. If FALSE, keep the same tree as determined in the splits sample (if an empty leave is encountered, that
tree is skipped and does not contribute to the estimate). Setting this to FALSE may improve performance on
small/marginally powered data, but requires more trees (note: tuning does not adjust the number of trees).
Only applies if honesty is enabled. Default is TRUE.</p>
</td></tr>
<tr><td><code id="boosted_regression_forest_+3A_alpha">alpha</code></td>
<td>
<p>A tuning parameter that controls the maximum imbalance of a split. Default is 0.05.</p>
</td></tr>
<tr><td><code id="boosted_regression_forest_+3A_imbalance.penalty">imbalance.penalty</code></td>
<td>
<p>A tuning parameter that controls how harshly imbalanced splits are penalized. Default is 0.</p>
</td></tr>
<tr><td><code id="boosted_regression_forest_+3A_ci.group.size">ci.group.size</code></td>
<td>
<p>The forest will grow ci.group.size trees on each subsample.
In order to provide confidence intervals, ci.group.size must
be at least 2. Default is 2.</p>
</td></tr>
<tr><td><code id="boosted_regression_forest_+3A_tune.parameters">tune.parameters</code></td>
<td>
<p>If true, NULL parameters are tuned by cross-validation; if FALSE
NULL parameters are set to defaults. Default is FALSE.</p>
</td></tr>
<tr><td><code id="boosted_regression_forest_+3A_tune.num.trees">tune.num.trees</code></td>
<td>
<p>The number of trees in each 'mini forest' used to fit the tuning model. Default is 10.</p>
</td></tr>
<tr><td><code id="boosted_regression_forest_+3A_tune.num.reps">tune.num.reps</code></td>
<td>
<p>The number of forests used to fit the tuning model. Default is 100.</p>
</td></tr>
<tr><td><code id="boosted_regression_forest_+3A_tune.num.draws">tune.num.draws</code></td>
<td>
<p>The number of random parameter values considered when using the model
to select the optimal parameters. Default is 1000.</p>
</td></tr>
<tr><td><code id="boosted_regression_forest_+3A_boost.steps">boost.steps</code></td>
<td>
<p>The number of boosting iterations. If NULL, selected by cross-validation. Default is NULL.</p>
</td></tr>
<tr><td><code id="boosted_regression_forest_+3A_boost.error.reduction">boost.error.reduction</code></td>
<td>
<p>If boost.steps is NULL, the percentage of previous steps' error that must be estimated
by cross validation in order to take a new step, default 0.97.</p>
</td></tr>
<tr><td><code id="boosted_regression_forest_+3A_boost.max.steps">boost.max.steps</code></td>
<td>
<p>The maximum number of boosting iterations to try when boost.steps=NULL. Default is 5.</p>
</td></tr>
<tr><td><code id="boosted_regression_forest_+3A_boost.trees.tune">boost.trees.tune</code></td>
<td>
<p>If boost.steps is NULL, the number of trees used to test a new boosting step when tuning
boost.steps. Default is 10.</p>
</td></tr>
<tr><td><code id="boosted_regression_forest_+3A_num.threads">num.threads</code></td>
<td>
<p>Number of threads used in training. If set to NULL, the software
automatically selects an appropriate amount.</p>
</td></tr>
<tr><td><code id="boosted_regression_forest_+3A_seed">seed</code></td>
<td>
<p>The seed for the C++ random number generator.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A boosted regression forest object. $error contains the mean debiased error for each step, and $forests
contains the trained regression forest for each step.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Train a boosted regression forest.
n &lt;- 50
p &lt;- 10
X &lt;- matrix(rnorm(n * p), n, p)
Y &lt;- X[, 1] * rnorm(n)
boosted.forest &lt;- boosted_regression_forest(X, Y)

# Predict using the forest.
X.test &lt;- matrix(0, 101, p)
X.test[, 1] &lt;- seq(-2, 2, length.out = 101)
boost.pred &lt;- predict(boosted.forest, X.test)

# Predict on out-of-bag training samples.
boost.pred &lt;- predict(boosted.forest)

# Check how many boosting iterations were used
print(length(boosted.forest$forests))


</code></pre>

<hr>
<h2 id='boot_grf'>Simple clustered bootstrap.</h2><span id='topic+boot_grf'></span>

<h3>Description</h3>

<p>Inspired by the 'boot' function in the bootstrap package with clusters + half-sampling added.
A future TODO could be to add parallel (not necessarily worth it)
https://stat.ethz.ch/R-manual/R-devel/library/parallel/doc/parallel.pdf
</p>


<h3>Usage</h3>

<pre><code class='language-R'>boot_grf(data, statistic, R, clusters, half.sample = TRUE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="boot_grf_+3A_data">data</code></td>
<td>
<p>A data frame with the original data.</p>
</td></tr>
<tr><td><code id="boot_grf_+3A_statistic">statistic</code></td>
<td>
<p>A function computing estimate(s) with signature (data, indices, ...) where
data is the original data, and indices a vector which defines the bootstrap sample.</p>
</td></tr>
<tr><td><code id="boot_grf_+3A_r">R</code></td>
<td>
<p>The number of bootstrap replications.</p>
</td></tr>
<tr><td><code id="boot_grf_+3A_clusters">clusters</code></td>
<td>
<p>Integer vector of cluster assignment, setting to 1:N corresponds to an ordinary
unclustered bootstrap.</p>
</td></tr>
<tr><td><code id="boot_grf_+3A_half.sample">half.sample</code></td>
<td>
<p>Whether to do half sample bootstrap (half the clusters are drawn). Default is TRUE.</p>
</td></tr>
<tr><td><code id="boot_grf_+3A_...">...</code></td>
<td>
<p>Additional arguments passed on to statistic.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with the original estimate t0, and bootstrap estimates t.
</p>


<h3>References</h3>

<p>Angelo Canty and Brian Ripley (2021). boot: Bootstrap R (S-Plus) Functions.
</p>

<hr>
<h2 id='causal_forest'>Causal forest</h2><span id='topic+causal_forest'></span>

<h3>Description</h3>

<p>Trains a causal forest that can be used to estimate
conditional average treatment effects tau(X). When
the treatment assignment W is binary and unconfounded,
we have tau(X) = E[Y(1) - Y(0) | X = x], where Y(0) and
Y(1) are potential outcomes corresponding to the two possible
treatment states. When W is continuous, we effectively estimate
an average partial effect Cov[Y, W | X = x] / Var[W | X = x],
and interpret it as a treatment effect given unconfoundedness.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>causal_forest(
  X,
  Y,
  W,
  Y.hat = NULL,
  W.hat = NULL,
  num.trees = 2000,
  sample.weights = NULL,
  clusters = NULL,
  equalize.cluster.weights = FALSE,
  sample.fraction = 0.5,
  mtry = min(ceiling(sqrt(ncol(X)) + 20), ncol(X)),
  min.node.size = 5,
  honesty = TRUE,
  honesty.fraction = 0.5,
  honesty.prune.leaves = TRUE,
  alpha = 0.05,
  imbalance.penalty = 0,
  stabilize.splits = TRUE,
  ci.group.size = 2,
  tune.parameters = "none",
  tune.num.trees = 200,
  tune.num.reps = 50,
  tune.num.draws = 1000,
  compute.oob.predictions = TRUE,
  num.threads = NULL,
  seed = runif(1, 0, .Machine$integer.max)
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="causal_forest_+3A_x">X</code></td>
<td>
<p>The covariates used in the causal regression.</p>
</td></tr>
<tr><td><code id="causal_forest_+3A_y">Y</code></td>
<td>
<p>The outcome (must be a numeric vector with no NAs).</p>
</td></tr>
<tr><td><code id="causal_forest_+3A_w">W</code></td>
<td>
<p>The treatment assignment (must be a binary or real numeric vector with no NAs).</p>
</td></tr>
<tr><td><code id="causal_forest_+3A_y.hat">Y.hat</code></td>
<td>
<p>Estimates of the expected responses E[Y | Xi], marginalizing
over treatment. If Y.hat = NULL, these are estimated using
a separate regression forest. See section 6.1.1 of the GRF paper for
further discussion of this quantity. Default is NULL.</p>
</td></tr>
<tr><td><code id="causal_forest_+3A_w.hat">W.hat</code></td>
<td>
<p>Estimates of the treatment propensities E[W | Xi]. If W.hat = NULL,
these are estimated using a separate regression forest. Default is NULL.</p>
</td></tr>
<tr><td><code id="causal_forest_+3A_num.trees">num.trees</code></td>
<td>
<p>Number of trees grown in the forest. Note: Getting accurate
confidence intervals generally requires more trees than
getting accurate predictions. Default is 2000.</p>
</td></tr>
<tr><td><code id="causal_forest_+3A_sample.weights">sample.weights</code></td>
<td>
<p>Weights given to each sample in estimation.
If NULL, each observation receives the same weight.
Note: To avoid introducing confounding, weights should be
independent of the potential outcomes given X. Default is NULL.</p>
</td></tr>
<tr><td><code id="causal_forest_+3A_clusters">clusters</code></td>
<td>
<p>Vector of integers or factors specifying which cluster each observation corresponds to.
Default is NULL (ignored).</p>
</td></tr>
<tr><td><code id="causal_forest_+3A_equalize.cluster.weights">equalize.cluster.weights</code></td>
<td>
<p>If FALSE, each unit is given the same weight (so that bigger
clusters get more weight). If TRUE, each cluster is given equal weight in the forest. In this case,
during training, each tree uses the same number of observations from each drawn cluster: If the
smallest cluster has K units, then when we sample a cluster during training, we only give a random
K elements of the cluster to the tree-growing procedure. When estimating average treatment effects,
each observation is given weight 1/cluster size, so that the total weight of each cluster is the
same. Note that, if this argument is FALSE, sample weights may also be directly adjusted via the
sample.weights argument. If this argument is TRUE, sample.weights must be set to NULL. Default is
FALSE.</p>
</td></tr>
<tr><td><code id="causal_forest_+3A_sample.fraction">sample.fraction</code></td>
<td>
<p>Fraction of the data used to build each tree.
Note: If honesty = TRUE, these subsamples will
further be cut by a factor of honesty.fraction. Default is 0.5.</p>
</td></tr>
<tr><td><code id="causal_forest_+3A_mtry">mtry</code></td>
<td>
<p>Number of variables tried for each split. Default is
<code class="reqn">\sqrt p + 20</code> where p is the number of variables.</p>
</td></tr>
<tr><td><code id="causal_forest_+3A_min.node.size">min.node.size</code></td>
<td>
<p>A target for the minimum number of observations in each tree leaf. Note that nodes
with size smaller than min.node.size can occur, as in the original randomForest package.
Default is 5.</p>
</td></tr>
<tr><td><code id="causal_forest_+3A_honesty">honesty</code></td>
<td>
<p>Whether to use honest splitting (i.e., sub-sample splitting). Default is TRUE.
For a detailed description of honesty, honesty.fraction, honesty.prune.leaves, and recommendations for
parameter tuning, see the grf algorithm reference.</p>
</td></tr>
<tr><td><code id="causal_forest_+3A_honesty.fraction">honesty.fraction</code></td>
<td>
<p>The fraction of data that will be used for determining splits if honesty = TRUE. Corresponds
to set J1 in the notation of the paper. Default is 0.5 (i.e. half of the data is used for
determining splits).</p>
</td></tr>
<tr><td><code id="causal_forest_+3A_honesty.prune.leaves">honesty.prune.leaves</code></td>
<td>
<p>If TRUE, prunes the estimation sample tree such that no leaves
are empty. If FALSE, keep the same tree as determined in the splits sample (if an empty leave is encountered, that
tree is skipped and does not contribute to the estimate). Setting this to FALSE may improve performance on
small/marginally powered data, but requires more trees (note: tuning does not adjust the number of trees).
Only applies if honesty is enabled. Default is TRUE.</p>
</td></tr>
<tr><td><code id="causal_forest_+3A_alpha">alpha</code></td>
<td>
<p>A tuning parameter that controls the maximum imbalance of a split. Default is 0.05.</p>
</td></tr>
<tr><td><code id="causal_forest_+3A_imbalance.penalty">imbalance.penalty</code></td>
<td>
<p>A tuning parameter that controls how harshly imbalanced splits are penalized. Default is 0.</p>
</td></tr>
<tr><td><code id="causal_forest_+3A_stabilize.splits">stabilize.splits</code></td>
<td>
<p>Whether or not the treatment should be taken into account when
determining the imbalance of a split. Default is TRUE.</p>
</td></tr>
<tr><td><code id="causal_forest_+3A_ci.group.size">ci.group.size</code></td>
<td>
<p>The forest will grow ci.group.size trees on each subsample.
In order to provide confidence intervals, ci.group.size must
be at least 2. Default is 2.</p>
</td></tr>
<tr><td><code id="causal_forest_+3A_tune.parameters">tune.parameters</code></td>
<td>
<p>A vector of parameter names to tune.
If &quot;all&quot;: all tunable parameters are tuned by cross-validation. The following parameters are
tunable: (&quot;sample.fraction&quot;, &quot;mtry&quot;, &quot;min.node.size&quot;, &quot;honesty.fraction&quot;,
&quot;honesty.prune.leaves&quot;, &quot;alpha&quot;, &quot;imbalance.penalty&quot;). If honesty is FALSE the honesty.* parameters are not tuned.
Default is &quot;none&quot; (no parameters are tuned).</p>
</td></tr>
<tr><td><code id="causal_forest_+3A_tune.num.trees">tune.num.trees</code></td>
<td>
<p>The number of trees in each 'mini forest' used to fit the tuning model. Default is 200.</p>
</td></tr>
<tr><td><code id="causal_forest_+3A_tune.num.reps">tune.num.reps</code></td>
<td>
<p>The number of forests used to fit the tuning model. Default is 50.</p>
</td></tr>
<tr><td><code id="causal_forest_+3A_tune.num.draws">tune.num.draws</code></td>
<td>
<p>The number of random parameter values considered when using the model
to select the optimal parameters. Default is 1000.</p>
</td></tr>
<tr><td><code id="causal_forest_+3A_compute.oob.predictions">compute.oob.predictions</code></td>
<td>
<p>Whether OOB predictions on training set should be precomputed. Default is TRUE.</p>
</td></tr>
<tr><td><code id="causal_forest_+3A_num.threads">num.threads</code></td>
<td>
<p>Number of threads used in training. By default, the number of threads is set
to the maximum hardware concurrency.</p>
</td></tr>
<tr><td><code id="causal_forest_+3A_seed">seed</code></td>
<td>
<p>The seed of the C++ random number generator.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A trained causal forest object. If tune.parameters is enabled,
then tuning information will be included through the 'tuning.output' attribute.
</p>


<h3>References</h3>

<p>Athey, Susan, Julie Tibshirani, and Stefan Wager. &quot;Generalized Random Forests&quot;.
Annals of Statistics, 47(2), 2019.
</p>
<p>Wager, Stefan, and Susan Athey. &quot;Estimation and Inference of Heterogeneous Treatment Effects using Random Forests&quot;.
Journal of the American Statistical Association, 113(523), 2018.
</p>
<p>Nie, Xinkun, and Stefan Wager. &quot;Quasi-Oracle Estimation of Heterogeneous Treatment Effects&quot;.
Biometrika, 108(2), 2021.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Train a causal forest.
n &lt;- 500
p &lt;- 10
X &lt;- matrix(rnorm(n * p), n, p)
W &lt;- rbinom(n, 1, 0.5)
Y &lt;- pmax(X[, 1], 0) * W + X[, 2] + pmin(X[, 3], 0) + rnorm(n)
c.forest &lt;- causal_forest(X, Y, W)

# Predict using the forest.
X.test &lt;- matrix(0, 101, p)
X.test[, 1] &lt;- seq(-2, 2, length.out = 101)
c.pred &lt;- predict(c.forest, X.test)

# Predict on out-of-bag training samples.
c.pred &lt;- predict(c.forest)

# Predict with confidence intervals; growing more trees is now recommended.
c.forest &lt;- causal_forest(X, Y, W, num.trees = 4000)
c.pred &lt;- predict(c.forest, X.test, estimate.variance = TRUE)

# In some examples, pre-fitting models for Y and W separately may
# be helpful (e.g., if different models use different covariates).
# In some applications, one may even want to get Y.hat and W.hat
# using a completely different method (e.g., boosting).
n &lt;- 2000
p &lt;- 20
X &lt;- matrix(rnorm(n * p), n, p)
TAU &lt;- 1 / (1 + exp(-X[, 3]))
W &lt;- rbinom(n, 1, 1 / (1 + exp(-X[, 1] - X[, 2])))
Y &lt;- pmax(X[, 2] + X[, 3], 0) + rowMeans(X[, 4:6]) / 2 + W * TAU + rnorm(n)

forest.W &lt;- regression_forest(X, W, tune.parameters = "all")
W.hat &lt;- predict(forest.W)$predictions

forest.Y &lt;- regression_forest(X, Y, tune.parameters = "all")
Y.hat &lt;- predict(forest.Y)$predictions

forest.Y.varimp &lt;- variable_importance(forest.Y)

# Note: Forests may have a hard time when trained on very few variables
# (e.g., ncol(X) = 1, 2, or 3). We recommend not being too aggressive
# in selection.
selected.vars &lt;- which(forest.Y.varimp / mean(forest.Y.varimp) &gt; 0.2)

tau.forest &lt;- causal_forest(X[, selected.vars], Y, W,
  W.hat = W.hat, Y.hat = Y.hat,
  tune.parameters = "all"
)
tau.hat &lt;- predict(tau.forest)$predictions

# See if a causal forest succeeded in capturing heterogeneity by plotting
# the TOC and calculating a 95% CI for the AUTOC.
train &lt;- sample(1:n, n / 2)
train.forest &lt;- causal_forest(X[train, ], Y[train], W[train])
eval.forest &lt;- causal_forest(X[-train, ], Y[-train], W[-train])
rate &lt;- rank_average_treatment_effect(eval.forest,
                                      predict(train.forest, X[-train, ])$predictions)
plot(rate)
paste("AUTOC:", round(rate$estimate, 2), "+/", round(1.96 * rate$std.err, 2))


</code></pre>

<hr>
<h2 id='causal_survival_forest'>Causal survival forest</h2><span id='topic+causal_survival_forest'></span>

<h3>Description</h3>

<p>Trains a causal survival forest that can be used to estimate
conditional treatment effects tau(X) with right-censored outcomes.
We estimate either 1)
tau(X) = E[min(T(1), horizon) - min(T(0), horizon) | X = x],
where T(1) and T(0) are potental outcomes corresponding to the two possible treatment states
and 'horizon' is the maximum follow-up time, or 2)
tau(X) = P[T(1) &gt; horizon | X = x] - P[T(0) &gt; horizon | X = x], for a chosen time point 'horizon'.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>causal_survival_forest(
  X,
  Y,
  W,
  D,
  W.hat = NULL,
  target = c("RMST", "survival.probability"),
  horizon = NULL,
  failure.times = NULL,
  num.trees = 2000,
  sample.weights = NULL,
  clusters = NULL,
  equalize.cluster.weights = FALSE,
  sample.fraction = 0.5,
  mtry = min(ceiling(sqrt(ncol(X)) + 20), ncol(X)),
  min.node.size = 5,
  honesty = TRUE,
  honesty.fraction = 0.5,
  honesty.prune.leaves = TRUE,
  alpha = 0.05,
  imbalance.penalty = 0,
  stabilize.splits = TRUE,
  ci.group.size = 2,
  tune.parameters = "none",
  compute.oob.predictions = TRUE,
  num.threads = NULL,
  seed = runif(1, 0, .Machine$integer.max)
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="causal_survival_forest_+3A_x">X</code></td>
<td>
<p>The covariates.</p>
</td></tr>
<tr><td><code id="causal_survival_forest_+3A_y">Y</code></td>
<td>
<p>The event time (must be non-negative).</p>
</td></tr>
<tr><td><code id="causal_survival_forest_+3A_w">W</code></td>
<td>
<p>The treatment assignment (must be a binary or real numeric vector with no NAs).</p>
</td></tr>
<tr><td><code id="causal_survival_forest_+3A_d">D</code></td>
<td>
<p>The event type (0: censored, 1: failure/observed event).</p>
</td></tr>
<tr><td><code id="causal_survival_forest_+3A_w.hat">W.hat</code></td>
<td>
<p>Estimates of the treatment propensities E[W | X = x]. If W.hat = NULL,
these are estimated using a separate regression forest. Default is NULL.</p>
</td></tr>
<tr><td><code id="causal_survival_forest_+3A_target">target</code></td>
<td>
<p>The target estimand. Choices are Restricted Mean Survival Time (&quot;RMST&quot;) which estimates 1)
E[min(T(1), horizon) - min(T(0), horizon) | X = x], or &quot;survival.probability&quot; which estimates 2)
P[T(1) &gt; horizon | X = x] - P[T(0) &gt; horizon | X = x]. Default is &quot;RMST&quot;.</p>
</td></tr>
<tr><td><code id="causal_survival_forest_+3A_horizon">horizon</code></td>
<td>
<p>A scalar that defines the estimand (required). If target is &quot;RMST&quot; then this defines
the maximum follow-up time. If target is &quot;survival.probability&quot;, then this defines the time point
for the absolute risk difference estimate.</p>
</td></tr>
<tr><td><code id="causal_survival_forest_+3A_failure.times">failure.times</code></td>
<td>
<p>A vector of event times to fit the survival curves at. If NULL, then all the unique
event times are used. This speeds up forest estimation by constraining the event grid. Observed event
times are rounded down to the last sorted occurance less than or equal to the specified failure time.
The time points should be in increasing order.
Default is NULL.</p>
</td></tr>
<tr><td><code id="causal_survival_forest_+3A_num.trees">num.trees</code></td>
<td>
<p>Number of trees grown in the forest. Note: Getting accurate
confidence intervals generally requires more trees than
getting accurate predictions. Default is 2000.</p>
</td></tr>
<tr><td><code id="causal_survival_forest_+3A_sample.weights">sample.weights</code></td>
<td>
<p>Weights given to each sample in estimation.
If NULL, each observation receives the same weight.
Note: To avoid introducing confounding, weights should be
independent of the potential outcomes given X. Sample weights
are not used in survival spliting. Default is NULL.</p>
</td></tr>
<tr><td><code id="causal_survival_forest_+3A_clusters">clusters</code></td>
<td>
<p>Vector of integers or factors specifying which cluster each observation corresponds to.
Default is NULL (ignored).</p>
</td></tr>
<tr><td><code id="causal_survival_forest_+3A_equalize.cluster.weights">equalize.cluster.weights</code></td>
<td>
<p>If FALSE, each unit is given the same weight (so that bigger
clusters get more weight). If TRUE, each cluster is given equal weight in the forest. In this case,
during training, each tree uses the same number of observations from each drawn cluster: If the
smallest cluster has K units, then when we sample a cluster during training, we only give a random
K elements of the cluster to the tree-growing procedure. When estimating average treatment effects,
each observation is given weight 1/cluster size, so that the total weight of each cluster is the
same. Note that, if this argument is FALSE, sample weights may also be directly adjusted via the
sample.weights argument. If this argument is TRUE, sample.weights must be set to NULL. Default is
FALSE.</p>
</td></tr>
<tr><td><code id="causal_survival_forest_+3A_sample.fraction">sample.fraction</code></td>
<td>
<p>Fraction of the data used to build each tree.
Note: If honesty = TRUE, these subsamples will
further be cut by a factor of honesty.fraction. Default is 0.5.</p>
</td></tr>
<tr><td><code id="causal_survival_forest_+3A_mtry">mtry</code></td>
<td>
<p>Number of variables tried for each split. Default is
<code class="reqn">\sqrt p + 20</code> where p is the number of variables.</p>
</td></tr>
<tr><td><code id="causal_survival_forest_+3A_min.node.size">min.node.size</code></td>
<td>
<p>A target for the minimum number of observations in each tree leaf. Note that nodes
with size smaller than min.node.size can occur, as in the original randomForest package.
Default is 5.</p>
</td></tr>
<tr><td><code id="causal_survival_forest_+3A_honesty">honesty</code></td>
<td>
<p>Whether to use honest splitting (i.e., sub-sample splitting). Default is TRUE.
For a detailed description of honesty, honesty.fraction, honesty.prune.leaves, and recommendations for
parameter tuning, see the grf algorithm reference.</p>
</td></tr>
<tr><td><code id="causal_survival_forest_+3A_honesty.fraction">honesty.fraction</code></td>
<td>
<p>The fraction of data that will be used for determining splits if honesty = TRUE. Corresponds
to set J1 in the notation of the paper. Default is 0.5 (i.e. half of the data is used for
determining splits).</p>
</td></tr>
<tr><td><code id="causal_survival_forest_+3A_honesty.prune.leaves">honesty.prune.leaves</code></td>
<td>
<p>If TRUE, prunes the estimation sample tree such that no leaves
are empty. If FALSE, keep the same tree as determined in the splits sample (if an empty leave is encountered, that
tree is skipped and does not contribute to the estimate). Setting this to FALSE may improve performance on
small/marginally powered data, but requires more trees (note: tuning does not adjust the number of trees).
Only applies if honesty is enabled. Default is TRUE.</p>
</td></tr>
<tr><td><code id="causal_survival_forest_+3A_alpha">alpha</code></td>
<td>
<p>A tuning parameter that controls the maximum imbalance of a split. This parameter plays the same
role as in causal forest and survival forest, where for the latter the number of failures in
each child has to be at least one or 'alpha' times the number of samples in the parent node. Default is 0.05.
(On data with very low event rate the default value may be too high for the forest to split
and lowering it may be beneficial).</p>
</td></tr>
<tr><td><code id="causal_survival_forest_+3A_imbalance.penalty">imbalance.penalty</code></td>
<td>
<p>A tuning parameter that controls how harshly imbalanced splits are penalized. Default is 0.</p>
</td></tr>
<tr><td><code id="causal_survival_forest_+3A_stabilize.splits">stabilize.splits</code></td>
<td>
<p>Whether or not the treatment and censoring status should be taken into account when
determining the imbalance of a split. The requirement for valid split candidates is the same as in causal_forest
with the additional constraint that num.failures(child) &gt;= num.samples(parent) * alpha. Default is TRUE.</p>
</td></tr>
<tr><td><code id="causal_survival_forest_+3A_ci.group.size">ci.group.size</code></td>
<td>
<p>The forest will grow ci.group.size trees on each subsample.
In order to provide confidence intervals, ci.group.size must
be at least 2. Default is 2.</p>
</td></tr>
<tr><td><code id="causal_survival_forest_+3A_tune.parameters">tune.parameters</code></td>
<td>
<p>(Currently only applies to the regression forest used in W.hat estimation)
A vector of parameter names to tune.
If &quot;all&quot;: all tunable parameters are tuned by cross-validation. The following parameters are
tunable: (&quot;sample.fraction&quot;, &quot;mtry&quot;, &quot;min.node.size&quot;, &quot;honesty.fraction&quot;,
&quot;honesty.prune.leaves&quot;, &quot;alpha&quot;, &quot;imbalance.penalty&quot;). If honesty is FALSE the honesty.* parameters are not tuned.
Default is &quot;none&quot; (no parameters are tuned).</p>
</td></tr>
<tr><td><code id="causal_survival_forest_+3A_compute.oob.predictions">compute.oob.predictions</code></td>
<td>
<p>Whether OOB predictions on training set should be precomputed. Default is TRUE.</p>
</td></tr>
<tr><td><code id="causal_survival_forest_+3A_num.threads">num.threads</code></td>
<td>
<p>Number of threads used in training. By default, the number of threads is set
to the maximum hardware concurrency.</p>
</td></tr>
<tr><td><code id="causal_survival_forest_+3A_seed">seed</code></td>
<td>
<p>The seed of the C++ random number generator.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>When W is continuous, we effectively estimate an average partial effect corresponding to
1) Cov[min(T, horizon), W | X = x] / Var[W | X = x] or 2) Cov[1(T &gt; horizon), W | X = x] / Var[W | X = x],
and interpret it as a treatment effect given unconfoundedness.
</p>


<h3>Value</h3>

<p>A trained causal_survival_forest forest object.
</p>


<h3>References</h3>

<p>Cui, Yifan, Michael R. Kosorok, Erik Sverdrup, Stefan Wager, and Ruoqing Zhu.
&quot;Estimating Heterogeneous Treatment Effects with Right-Censored Data via Causal Survival Forests&quot;.
Journal of the Royal Statistical Society: Series B, 85(2), 2023.
</p>
<p>Sverdrup, Erik, and Stefan Wager.
&quot;Treatment Heterogeneity with Right-Censored Outcomes Using grf&quot;.
ASA Lifetime Data Science Newsletter, January 2024
(<a href="https://arxiv.org/abs/2312.02482">arXiv:2312.02482</a>).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Train a causal survival forest targeting a Restricted Mean Survival Time (RMST)
# with maximum follow-up time set to `horizon`.
n &lt;- 2000
p &lt;- 5
X &lt;- matrix(runif(n * p), n, p)
W &lt;- rbinom(n, 1, 0.5)
horizon &lt;- 1
failure.time &lt;- pmin(rexp(n) * X[, 1] + W, horizon)
censor.time &lt;- 2 * runif(n)
Y &lt;- pmin(failure.time, censor.time)
D &lt;- as.integer(failure.time &lt;= censor.time)
# Save computation time by constraining the event grid by discretizing (rounding) continuous events.
cs.forest &lt;- causal_survival_forest(X, round(Y, 2), W, D, horizon = horizon)
# Or do so more flexibly by defining your own time grid using the failure.times argument.
# grid &lt;- seq(min(Y), max(Y), length.out = 150)
# cs.forest &lt;- causal_survival_forest(X, Y, W, D, horizon = horizon, failure.times = grid)

# Predict using the forest.
X.test &lt;- matrix(0.5, 10, p)
X.test[, 1] &lt;- seq(0, 1, length.out = 10)
cs.pred &lt;- predict(cs.forest, X.test)

# Predict on out-of-bag training samples.
cs.pred &lt;- predict(cs.forest)

# Predict with confidence intervals; growing more trees is now recommended.
c.pred &lt;- predict(cs.forest, X.test, estimate.variance = TRUE)

# Compute a doubly robust estimate of the average treatment effect.
average_treatment_effect(cs.forest)

# Compute the best linear projection on the first covariate.
best_linear_projection(cs.forest, X[, 1])

# See if a causal survival forest succeeded in capturing heterogeneity by plotting
# the TOC and calculating a 95% CI for the AUTOC.
train &lt;- sample(1:n, n / 2)
eval &lt;- -train
train.forest &lt;- causal_survival_forest(X[train, ], Y[train], W[train], D[train], horizon = horizon)
eval.forest &lt;- causal_survival_forest(X[eval, ], Y[eval], W[eval], D[eval], horizon = horizon)
rate &lt;- rank_average_treatment_effect(eval.forest,
                                      predict(train.forest, X[eval, ])$predictions)
plot(rate)
paste("AUTOC:", round(rate$estimate, 2), "+/", round(1.96 * rate$std.err, 2))


</code></pre>

<hr>
<h2 id='create_dot_body'>Writes each node information
If it is a leaf node: show it in different color, show number of samples, show leaf id
If it is a non-leaf node: show its splitting variable and splitting value
If trained with missing values, the edge arrow is filled according to which direction the NAs are sent.</h2><span id='topic+create_dot_body'></span>

<h3>Description</h3>

<p>Writes each node information
If it is a leaf node: show it in different color, show number of samples, show leaf id
If it is a non-leaf node: show its splitting variable and splitting value
If trained with missing values, the edge arrow is filled according to which direction the NAs are sent.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>create_dot_body(tree, index = 1, include.na.path)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="create_dot_body_+3A_tree">tree</code></td>
<td>
<p>the tree to convert</p>
</td></tr>
<tr><td><code id="create_dot_body_+3A_index">index</code></td>
<td>
<p>the index of the current node</p>
</td></tr>
<tr><td><code id="create_dot_body_+3A_include.na.path">include.na.path</code></td>
<td>
<p>A boolean toggling whether to include the path of missing values or not.</p>
</td></tr>
</table>

<hr>
<h2 id='estimate_rate'>Compute rate estimates, a function to be passed on to bootstrap routine.</h2><span id='topic+estimate_rate'></span>

<h3>Description</h3>

<p>Compute rate estimates, a function to be passed on to bootstrap routine.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>estimate_rate(data, indices, q, wtd.mean)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="estimate_rate_+3A_data">data</code></td>
<td>
<p>A data.frame with the original data, column 1: DR.scores*sample.weights, column 2: sample.weights (positive),
column 3: priority scores (integer vector with scores 1,...,num.unique.prios).</p>
</td></tr>
<tr><td><code id="estimate_rate_+3A_indices">indices</code></td>
<td>
<p>A vector of indices which define the bootstrap sample.</p>
</td></tr>
<tr><td><code id="estimate_rate_+3A_q">q</code></td>
<td>
<p>A vector of fractions to compute TOC on, with last entry = 1.</p>
</td></tr>
<tr><td><code id="estimate_rate_+3A_wtd.mean">wtd.mean</code></td>
<td>
<p>A weighting function determining the type of RATE estimate.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>an estimate of RATE, together with the TOC curve.
</p>

<hr>
<h2 id='expected_survival'>Compute E[T | X]</h2><span id='topic+expected_survival'></span>

<h3>Description</h3>

<p>Compute E[T | X]
</p>


<h3>Usage</h3>

<pre><code class='language-R'>expected_survival(S.hat, Y.grid)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="expected_survival_+3A_s.hat">S.hat</code></td>
<td>
<p>The estimated survival curve.</p>
</td></tr>
<tr><td><code id="expected_survival_+3A_y.grid">Y.grid</code></td>
<td>
<p>The time values corresponding to S.hat.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of expected values.
</p>

<hr>
<h2 id='export_graphviz'>Export a tree in DOT format.
This function generates a GraphViz representation of the tree,
which is then written into 'dot_string'.</h2><span id='topic+export_graphviz'></span>

<h3>Description</h3>

<p>Export a tree in DOT format.
This function generates a GraphViz representation of the tree,
which is then written into 'dot_string'.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>export_graphviz(tree, include.na.path)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="export_graphviz_+3A_tree">tree</code></td>
<td>
<p>the tree to convert</p>
</td></tr>
<tr><td><code id="export_graphviz_+3A_include.na.path">include.na.path</code></td>
<td>
<p>A boolean toggling whether to include the path of missing values or not.</p>
</td></tr>
</table>

<hr>
<h2 id='generate_causal_data'>Generate causal forest data</h2><span id='topic+generate_causal_data'></span>

<h3>Description</h3>

<p>The following DGPs are available for benchmarking purposes:
</p>

<ul>
<li><p> &quot;simple&quot;: tau = max(X1, 0), e = 0.4 + 0.2 * 1(X1 &gt; 0).
</p>
</li>
<li><p> &quot;aw1&quot;: equation (27) of https://arxiv.org/pdf/1510.04342.pdf
</p>
</li>
<li><p> &quot;aw2&quot;: equation (28) of https://arxiv.org/pdf/1510.04342.pdf
</p>
</li>
<li><p> &quot;aw3&quot;: confounding is from &quot;aw1&quot; and tau is from &quot;aw2&quot;
</p>
</li>
<li><p> &quot;aw3reverse&quot;: Same as aw3, but HTEs anticorrelated with baseline
</p>
</li>
<li><p> &quot;ai1&quot;: &quot;Setup 1&quot; from section 6 of https://arxiv.org/pdf/1504.01132.pdf
</p>
</li>
<li><p> &quot;ai2&quot;: &quot;Setup 2&quot; from section 6 of https://arxiv.org/pdf/1504.01132.pdf
</p>
</li>
<li><p> &quot;kunzel&quot;: &quot;Simulation 1&quot; from A.1 in https://arxiv.org/pdf/1706.03461.pdf
</p>
</li>
<li><p> &quot;nw1&quot;: &quot;Setup A&quot; from Section 4 of https://arxiv.org/pdf/1712.04912.pdf
</p>
</li>
<li><p> &quot;nw2&quot;: &quot;Setup B&quot; from Section 4 of https://arxiv.org/pdf/1712.04912.pdf
</p>
</li>
<li><p> &quot;nw3&quot;: &quot;Setup C&quot; from Section 4 of https://arxiv.org/pdf/1712.04912.pdf
</p>
</li>
<li><p> &quot;nw4&quot;: &quot;Setup D&quot; from Section 4 of https://arxiv.org/pdf/1712.04912.pdf
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>generate_causal_data(
  n,
  p,
  sigma.m = 1,
  sigma.tau = 0.1,
  sigma.noise = 1,
  dgp = c("simple", "aw1", "aw2", "aw3", "aw3reverse", "ai1", "ai2", "kunzel", "nw1",
    "nw2", "nw3", "nw4")
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="generate_causal_data_+3A_n">n</code></td>
<td>
<p>The number of observations.</p>
</td></tr>
<tr><td><code id="generate_causal_data_+3A_p">p</code></td>
<td>
<p>The number of covariates (note: the minimum varies by DGP).</p>
</td></tr>
<tr><td><code id="generate_causal_data_+3A_sigma.m">sigma.m</code></td>
<td>
<p>The standard deviation of the unconditional mean of Y. Default is 1.</p>
</td></tr>
<tr><td><code id="generate_causal_data_+3A_sigma.tau">sigma.tau</code></td>
<td>
<p>The standard deviation of the treatment effect. Default  is 0.1.</p>
</td></tr>
<tr><td><code id="generate_causal_data_+3A_sigma.noise">sigma.noise</code></td>
<td>
<p>The conditional variance of Y. Default is 1.</p>
</td></tr>
<tr><td><code id="generate_causal_data_+3A_dgp">dgp</code></td>
<td>
<p>The kind of dgp. Default is &quot;simple&quot;.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Each DGP is parameterized by
X: observables,
m: conditional mean of Y,
tau: treatment effect,
e: propensity scores,
V: conditional variance of Y.
</p>
<p>The following rescaled data is returned
m = m / sd(m) * sigma.m,
tau = tau / sd(tau) * sigma.tau,
V = V / mean(V) * sigma.noise^2,
W = rbinom(e),
Y = m + (W - e) * tau + sqrt(V) + rnorm(n).
</p>


<h3>Value</h3>

<p>A list consisting of:
X, Y, W, tau, m, e, dgp.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Generate simple benchmark data
data &lt;- generate_causal_data(100, 5, dgp = "simple")
# Generate data from Wager and Athey (2018)
data &lt;- generate_causal_data(100, 5, dgp = "aw1")
data2 &lt;- generate_causal_data(100, 5, dgp = "aw2")

</code></pre>

<hr>
<h2 id='generate_causal_survival_data'>Simulate causal survival data</h2><span id='topic+generate_causal_survival_data'></span>

<h3>Description</h3>

<p>The following DGPs are available for benchmarking purposes, T is the failure time
and C the censoring time:
</p>

<ul>
<li><p> &quot;simple1&quot;: T = X1*eps + W, C ~ U(0, 2) where eps ~ Exp(1) and Y.max = 1.
</p>
</li>
<li><p>  &quot;type1&quot;: T is drawn from an accelerated failure time model and C from a Cox model (scenario 1 in https://arxiv.org/abs/2001.09887)
</p>
</li>
<li><p>  &quot;type2&quot;: T is drawn from a proportional hazard model and C from a accelerated failure time (scenario 2 in https://arxiv.org/abs/2001.09887)
</p>
</li>
<li><p>  &quot;type3&quot;: T and C are drawn from a Poisson distribution  (scenario 3 in https://arxiv.org/abs/2001.09887)
</p>
</li>
<li><p>  &quot;type4&quot;: T and C are drawn from a Poisson distribution  (scenario 4 in https://arxiv.org/abs/2001.09887)
</p>
</li>
<li><p>  &quot;type5&quot;: is similar to &quot;type2&quot; but with censoring generated from an accelerated failure time model.
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>generate_causal_survival_data(
  n,
  p,
  Y.max = NULL,
  y0 = NULL,
  X = NULL,
  rho = 0,
  n.mc = 10000,
  dgp = c("simple1", "type1", "type2", "type3", "type4", "type5")
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="generate_causal_survival_data_+3A_n">n</code></td>
<td>
<p>The number of samples.</p>
</td></tr>
<tr><td><code id="generate_causal_survival_data_+3A_p">p</code></td>
<td>
<p>The number of covariates.</p>
</td></tr>
<tr><td><code id="generate_causal_survival_data_+3A_y.max">Y.max</code></td>
<td>
<p>The maximum follow-up time (optional).</p>
</td></tr>
<tr><td><code id="generate_causal_survival_data_+3A_y0">y0</code></td>
<td>
<p>Query time to estimate P(T(1) &gt; y0 | X) - P(T(0) &gt; y0 | X) (optional).</p>
</td></tr>
<tr><td><code id="generate_causal_survival_data_+3A_x">X</code></td>
<td>
<p>The covariates (optional).</p>
</td></tr>
<tr><td><code id="generate_causal_survival_data_+3A_rho">rho</code></td>
<td>
<p>The correlation coefficient of the X's covariance matrix V_(ij) = rho^|i-j|. Default is 0.</p>
</td></tr>
<tr><td><code id="generate_causal_survival_data_+3A_n.mc">n.mc</code></td>
<td>
<p>The number of monte carlo draws to estimate the treatment effect with. Default is 10000.</p>
</td></tr>
<tr><td><code id="generate_causal_survival_data_+3A_dgp">dgp</code></td>
<td>
<p>The type of DGP.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with entries:
'X': the covariates, 'Y': the event times, 'W': the treatment indicator, 'D': the censoring indicator,
'cate': the treatment effect (RMST) estimated by monte carlo, 'cate.prob' the difference in survival probability,
'cate.sign': the true sign of the cate for ITR comparison, 'dgp': the dgp name, 'Y.max': the maximum follow-up time,
'y0': the query time for difference in survival probability.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Generate data
n &lt;- 1000
p &lt;- 5
data &lt;- generate_causal_survival_data(n, p)
# Get true CATE on a test set
X.test &lt;- matrix(seq(0, 1, length.out = 5), 5, p)
cate.test &lt;- generate_causal_survival_data(n, p, X = X.test)$cate


</code></pre>

<hr>
<h2 id='get_forest_weights'>Given a trained forest and test data, compute the kernel weights for each test point.</h2><span id='topic+get_forest_weights'></span>

<h3>Description</h3>

<p>During normal prediction, these weights (named alpha in the GRF paper) are computed as an intermediate
step towards producing estimates. This function allows for examining the weights directly, so they
could be potentially be used as the input to a different analysis.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_forest_weights(forest, newdata = NULL, num.threads = NULL)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_forest_weights_+3A_forest">forest</code></td>
<td>
<p>The trained forest.</p>
</td></tr>
<tr><td><code id="get_forest_weights_+3A_newdata">newdata</code></td>
<td>
<p>Points at which predictions should be made. If NULL,
makes out-of-bag predictions on the training set instead
(i.e., provides predictions at Xi using only trees that did
not use the i-th training example).</p>
</td></tr>
<tr><td><code id="get_forest_weights_+3A_num.threads">num.threads</code></td>
<td>
<p>Number of threads used in training. If set to NULL, the software
automatically selects an appropriate amount.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A sparse matrix where each row represents a test sample, and each column is a sample in the
training data. The value at (i, j) gives the weight of training sample j for test sample i.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
p &lt;- 10
n &lt;- 100
X &lt;- matrix(2 * runif(n * p) - 1, n, p)
Y &lt;- (X[, 1] &gt; 0) + 2 * rnorm(n)
rrf &lt;- regression_forest(X, Y, mtry = p)
forest.weights.oob &lt;- get_forest_weights(rrf)

n.test &lt;- 15
X.test &lt;- matrix(2 * runif(n.test * p) - 1, n.test, p)
forest.weights &lt;- get_forest_weights(rrf, X.test)


</code></pre>

<hr>
<h2 id='get_leaf_node'>Find the leaf node for a test sample.</h2><span id='topic+get_leaf_node'></span>

<h3>Description</h3>

<p>Given a GRF tree object, compute the leaf node a test sample falls into. The nodes in a GRF tree
are numbered breadth first, and the returned numbers will be the leaf integer according
to this ordering. To get kernel weights based on leaf membership, see the function
<code><a href="#topic+get_forest_weights">get_forest_weights</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_leaf_node(tree, newdata, node.id = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_leaf_node_+3A_tree">tree</code></td>
<td>
<p>A GRF tree object (retrieved by 'get_tree').</p>
</td></tr>
<tr><td><code id="get_leaf_node_+3A_newdata">newdata</code></td>
<td>
<p>Points at which leaf predictions should be made.</p>
</td></tr>
<tr><td><code id="get_leaf_node_+3A_node.id">node.id</code></td>
<td>
<p>Boolean indicating whether to return the node.id for each query sample (default), or
if FALSE, a list of node numbers with the samples contained.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of integers indicating the leaf number for each sample in the given tree.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
p &lt;- 10
n &lt;- 100
X &lt;- matrix(2 * runif(n * p) - 1, n, p)
Y &lt;- (X[, 1] &gt; 0) + 2 * rnorm(n)
r.forest &lt;- regression_forest(X, Y, num.tree = 50)

n.test &lt;- 5
X.test &lt;- matrix(2 * runif(n.test * p) - 1, n.test, p)
tree &lt;- get_tree(r.forest, 1)
# Get a vector of node numbers for each sample.
get_leaf_node(tree, X.test)
# Get a list of samples per node.
get_leaf_node(tree, X.test, node.id = FALSE)


</code></pre>

<hr>
<h2 id='get_scores'>Compute doubly robust scores for a GRF forest object</h2><span id='topic+get_scores'></span>

<h3>Description</h3>

<p>Compute doubly robust scores for a GRF forest object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_scores(forest, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_scores_+3A_forest">forest</code></td>
<td>
<p>A grf forest object</p>
</td></tr>
<tr><td><code id="get_scores_+3A_...">...</code></td>
<td>
<p>Additional arguments</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of scores
</p>

<hr>
<h2 id='get_scores.causal_forest'>Compute doubly robust scores for a causal forest.</h2><span id='topic+get_scores.causal_forest'></span>

<h3>Description</h3>

<p>Compute doubly robust (AIPW) scores for average treatment effect estimation
or average partial effect estimation with continuous treatment,
using a causal forest. Under regularity conditions, the average of the DR.scores
is an efficient estimate of the average treatment effect.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'causal_forest'
get_scores(
  forest,
  subset = NULL,
  debiasing.weights = NULL,
  num.trees.for.weights = 500,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_scores.causal_forest_+3A_forest">forest</code></td>
<td>
<p>A trained causal forest.</p>
</td></tr>
<tr><td><code id="get_scores.causal_forest_+3A_subset">subset</code></td>
<td>
<p>Specifies subset of the training examples over which we
estimate the ATE. WARNING: For valid statistical performance,
the subset should be defined only using features Xi, not using
the treatment Wi or the outcome Yi.</p>
</td></tr>
<tr><td><code id="get_scores.causal_forest_+3A_debiasing.weights">debiasing.weights</code></td>
<td>
<p>A vector of length n (or the subset length) of debiasing weights.
If NULL (default) they are obtained via inverse-propensity weighting in the case
of binary treatment or by estimating Var[W | X = x] using a new forest
in the case of a continuous treatment.</p>
</td></tr>
<tr><td><code id="get_scores.causal_forest_+3A_num.trees.for.weights">num.trees.for.weights</code></td>
<td>
<p>Number of trees used to estimate Var[W | X = x]. Note: this
argument is only used when debiasing.weights = NULL.</p>
</td></tr>
<tr><td><code id="get_scores.causal_forest_+3A_...">...</code></td>
<td>
<p>Additional arguments (currently ignored).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of scores.
</p>


<h3>References</h3>

<p>Farrell, Max H. &quot;Robust inference on average treatment effects with
possibly more covariates than observations.&quot; Journal of Econometrics
189(1), 2015.
</p>
<p>Graham, Bryan S., and Cristine Campos de Xavier Pinto. &quot;Semiparametrically
efficient estimation of the average linear regression function.&quot;
Journal of Econometrics 226(1), 2022.
</p>
<p>Hirshberg, David A., and Stefan Wager. &quot;Augmented minimax linear estimation.&quot;
The Annals of Statistics 49(6), 2021.
</p>
<p>Robins, James M., and Andrea Rotnitzky. &quot;Semiparametric efficiency in
multivariate regression models with missing data.&quot; Journal of the
American Statistical Association 90(429), 1995.
</p>

<hr>
<h2 id='get_scores.causal_survival_forest'>Compute doubly robust scores for a causal survival forest.</h2><span id='topic+get_scores.causal_survival_forest'></span>

<h3>Description</h3>

<p>For details see section 3.2 in the causal survival forest paper.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'causal_survival_forest'
get_scores(forest, subset = NULL, num.trees.for.weights = 500, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_scores.causal_survival_forest_+3A_forest">forest</code></td>
<td>
<p>A trained causal survival forest.</p>
</td></tr>
<tr><td><code id="get_scores.causal_survival_forest_+3A_subset">subset</code></td>
<td>
<p>Specifies subset of the training examples over which we
estimate the ATE. WARNING: For valid statistical performance,
the subset should be defined only using features Xi, not using
the treatment Wi or the outcome Yi.</p>
</td></tr>
<tr><td><code id="get_scores.causal_survival_forest_+3A_num.trees.for.weights">num.trees.for.weights</code></td>
<td>
<p>Number of trees used to estimate Var[W | X = x]. Note: this
argument is only used in the case of a continuous treatment
(see <code><a href="#topic+get_scores.causal_forest">get_scores.causal_forest</a></code> for details).</p>
</td></tr>
<tr><td><code id="get_scores.causal_survival_forest_+3A_...">...</code></td>
<td>
<p>Additional arguments (currently ignored).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of scores.
</p>

<hr>
<h2 id='get_scores.instrumental_forest'>Doubly robust scores for estimating the average conditional local average treatment effect.</h2><span id='topic+get_scores.instrumental_forest'></span>

<h3>Description</h3>

<p>Given an outcome Y, treatment W and instrument Z, the (conditional) local
average treatment effect is tau(x) = Cov[Y, Z | X = x] / Cov[W, Z | X = x].
This is the quantity that is estimated with an instrumental forest.
It can be intepreted causally in various ways. Given a homogeneity
assumption, tau(x) is simply the CATE at x. When W is binary
and there are no &quot;defiers&quot;, Imbens and Angrist (1994) show that tau(x) can
be interpreted as an average treatment effect on compliers. This doubly robust
scores provided here are for estimating tau = E[tau(X)].
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'instrumental_forest'
get_scores(
  forest,
  subset = NULL,
  debiasing.weights = NULL,
  compliance.score = NULL,
  num.trees.for.weights = 500,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_scores.instrumental_forest_+3A_forest">forest</code></td>
<td>
<p>A trained instrumental forest.</p>
</td></tr>
<tr><td><code id="get_scores.instrumental_forest_+3A_subset">subset</code></td>
<td>
<p>Specifies subset of the training examples over which we
estimate the ATE. WARNING: For valid statistical performance,
the subset should be defined only using features Xi, not using
the treatment Wi or the outcome Yi.</p>
</td></tr>
<tr><td><code id="get_scores.instrumental_forest_+3A_debiasing.weights">debiasing.weights</code></td>
<td>
<p>A vector of length n (or the subset length) of debiasing weights.
If NULL (default) these are obtained via the appropriate doubly robust score
construction, e.g., in the case of causal_forests with a binary treatment, they
are obtained via inverse-propensity weighting.</p>
</td></tr>
<tr><td><code id="get_scores.instrumental_forest_+3A_compliance.score">compliance.score</code></td>
<td>
<p>An estimate of the causal effect of Z on W, i.e., Delta(X) = E[W | X, Z = 1]
- E[W | X, Z = 0], which can then be used to produce debiasing.weights. If not provided,
this is estimated via an auxiliary causal forest.</p>
</td></tr>
<tr><td><code id="get_scores.instrumental_forest_+3A_num.trees.for.weights">num.trees.for.weights</code></td>
<td>
<p>In some cases (e.g., with causal forests with a continuous
treatment), we need to train auxiliary forests to learn debiasing weights.
This is the number of trees used for this task. Note: this argument is only
used when debiasing.weights = NULL.</p>
</td></tr>
<tr><td><code id="get_scores.instrumental_forest_+3A_...">...</code></td>
<td>
<p>Additional arguments (currently ignored).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of scores.
</p>


<h3>References</h3>

<p>Aronow, Peter M., and Allison Carnegie. &quot;Beyond LATE: Estimation of the
average treatment effect with an instrumental variable.&quot; Political
Analysis 21(4), 2013.
</p>
<p>Chernozhukov, Victor, Juan Carlos Escanciano, Hidehiko Ichimura,
Whitney K. Newey, and James M. Robins. &quot;Locally robust semiparametric
estimation.&quot; Econometrica 90(4), 2022.
</p>
<p>Imbens, Guido W., and Joshua D. Angrist. &quot;Identification and Estimation of
Local Average Treatment Effects.&quot; Econometrica 62(2), 1994.
</p>

<hr>
<h2 id='get_scores.multi_arm_causal_forest'>Compute doubly robust scores for a multi arm causal forest.</h2><span id='topic+get_scores.multi_arm_causal_forest'></span>

<h3>Description</h3>

<p>Compute doubly robust (AIPW) scores for average treatment effect estimation
using a multi arm causal forest. Under regularity conditions, the average of the DR.scores
is an efficient estimate of the average treatment effect.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'multi_arm_causal_forest'
get_scores(forest, subset = NULL, drop = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_scores.multi_arm_causal_forest_+3A_forest">forest</code></td>
<td>
<p>A trained multi arm causal forest.</p>
</td></tr>
<tr><td><code id="get_scores.multi_arm_causal_forest_+3A_subset">subset</code></td>
<td>
<p>Specifies subset of the training examples over which we
estimate the ATE. WARNING: For valid statistical performance,
the subset should be defined only using features Xi, not using
the treatment Wi or the outcome Yi.</p>
</td></tr>
<tr><td><code id="get_scores.multi_arm_causal_forest_+3A_drop">drop</code></td>
<td>
<p>If TRUE, coerce the result to the lowest possible dimension. Default is FALSE.</p>
</td></tr>
<tr><td><code id="get_scores.multi_arm_causal_forest_+3A_...">...</code></td>
<td>
<p>Additional arguments (currently ignored).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An array of scores for each contrast and outcome.
</p>

<hr>
<h2 id='get_tree'>Retrieve a single tree from a trained forest object.</h2><span id='topic+get_tree'></span>

<h3>Description</h3>

<p>Retrieve a single tree from a trained forest object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>get_tree(forest, index)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="get_tree_+3A_forest">forest</code></td>
<td>
<p>The trained forest.</p>
</td></tr>
<tr><td><code id="get_tree_+3A_index">index</code></td>
<td>
<p>The index of the tree to retrieve.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A GRF tree object containing the below attributes.
drawn_samples: a list of examples that were used in training the tree. This includes
examples that were used in choosing splits, as well as the examples that populate the leaf
nodes. Put another way, if honesty is enabled, this list includes both subsamples from the
split (J1 and J2 in the notation of the paper).
num_samples: the number of examples used in training the tree.
nodes: a list of objects representing the nodes in the tree, starting with the root node. Each
node will contain an 'is_leaf' attribute, which indicates whether it is an interior or leaf node.
Interior nodes contain the attributes 'left_child' and 'right_child', which give the indices of
their children in the list, as well as 'split_variable', and 'split_value', which describe the
split that was chosen. Leaf nodes only have the attribute 'samples', which is a list of the
training examples that the leaf contains. Note that if honesty is enabled, this list will only
contain examples from the second subsample that was used to 'repopulate' the tree (J2 in the
notation of the paper).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Train a quantile forest.
n &lt;- 50
p &lt;- 10
X &lt;- matrix(rnorm(n * p), n, p)
Y &lt;- X[, 1] * rnorm(n)
q.forest &lt;- quantile_forest(X, Y, quantiles = c(0.1, 0.5, 0.9))

# Examine a particular tree.
q.tree &lt;- get_tree(q.forest, 3)
q.tree$nodes


</code></pre>

<hr>
<h2 id='grf_options'>grf package options</h2><span id='topic+grf_options'></span>

<h3>Description</h3>

<p>grf package options can be set using R's <code><a href="base.html#topic+options">options</a></code> command.
The current available options are:
</p>

<ul>
<li><p> &lsquo;grf.legacy.seed': controls whether grf&rsquo;s random seed behavior depends on
the number of CPU threads used to train the forest. The default value is 'FALSE'.
Set to 'TRUE' to recover results produced with grf versions prior to 2.4.0.
</p>
</li></ul>



<h3>Usage</h3>

<pre><code class='language-R'>grf_options()
</code></pre>


<h3>Value</h3>

<p>Prints the current grf package options.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Use random seed behavior prior to version 2.4.0.
options(grf.legacy.seed = TRUE)

# Print current package options.
grf_options()

# Use random seed independent of num.threads (default as of version 2.4.0 and higher).
options(grf.legacy.seed = FALSE)


</code></pre>

<hr>
<h2 id='instrumental_forest'>Intrumental forest</h2><span id='topic+instrumental_forest'></span>

<h3>Description</h3>

<p>Trains an instrumental forest that can be used to estimate
conditional local average treatment effects tau(X) identified
using instruments. Formally, the forest estimates
tau(X) = Cov[Y, Z | X = x] / Cov[W, Z | X = x].
Note that when the instrument Z and treatment assignment W
coincide, an instrumental forest is equivalent to a causal forest.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>instrumental_forest(
  X,
  Y,
  W,
  Z,
  Y.hat = NULL,
  W.hat = NULL,
  Z.hat = NULL,
  num.trees = 2000,
  sample.weights = NULL,
  clusters = NULL,
  equalize.cluster.weights = FALSE,
  sample.fraction = 0.5,
  mtry = min(ceiling(sqrt(ncol(X)) + 20), ncol(X)),
  min.node.size = 5,
  honesty = TRUE,
  honesty.fraction = 0.5,
  honesty.prune.leaves = TRUE,
  alpha = 0.05,
  imbalance.penalty = 0,
  stabilize.splits = TRUE,
  ci.group.size = 2,
  reduced.form.weight = 0,
  tune.parameters = "none",
  tune.num.trees = 200,
  tune.num.reps = 50,
  tune.num.draws = 1000,
  compute.oob.predictions = TRUE,
  num.threads = NULL,
  seed = runif(1, 0, .Machine$integer.max)
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="instrumental_forest_+3A_x">X</code></td>
<td>
<p>The covariates used in the instrumental regression.</p>
</td></tr>
<tr><td><code id="instrumental_forest_+3A_y">Y</code></td>
<td>
<p>The outcome.</p>
</td></tr>
<tr><td><code id="instrumental_forest_+3A_w">W</code></td>
<td>
<p>The treatment assignment (may be binary or real).</p>
</td></tr>
<tr><td><code id="instrumental_forest_+3A_z">Z</code></td>
<td>
<p>The instrument (may be binary or real).</p>
</td></tr>
<tr><td><code id="instrumental_forest_+3A_y.hat">Y.hat</code></td>
<td>
<p>Estimates of the expected responses E[Y | Xi], marginalizing
over treatment. If Y.hat = NULL, these are estimated using
a separate regression forest. Default is NULL.</p>
</td></tr>
<tr><td><code id="instrumental_forest_+3A_w.hat">W.hat</code></td>
<td>
<p>Estimates of the treatment propensities E[W | Xi]. If W.hat = NULL,
these are estimated using a separate regression forest. Default is NULL.</p>
</td></tr>
<tr><td><code id="instrumental_forest_+3A_z.hat">Z.hat</code></td>
<td>
<p>Estimates of the instrument propensities E[Z | Xi]. If Z.hat = NULL,
these are estimated using a separate regression forest. Default is NULL.</p>
</td></tr>
<tr><td><code id="instrumental_forest_+3A_num.trees">num.trees</code></td>
<td>
<p>Number of trees grown in the forest. Note: Getting accurate
confidence intervals generally requires more trees than
getting accurate predictions. Default is 2000.</p>
</td></tr>
<tr><td><code id="instrumental_forest_+3A_sample.weights">sample.weights</code></td>
<td>
<p>Weights given to each observation in estimation.
If NULL, each observation receives equal weight. Default is NULL.</p>
</td></tr>
<tr><td><code id="instrumental_forest_+3A_clusters">clusters</code></td>
<td>
<p>Vector of integers or factors specifying which cluster each observation corresponds to.
Default is NULL (ignored).</p>
</td></tr>
<tr><td><code id="instrumental_forest_+3A_equalize.cluster.weights">equalize.cluster.weights</code></td>
<td>
<p>If FALSE, each unit is given the same weight (so that bigger
clusters get more weight). If TRUE, each cluster is given equal weight in the forest. In this case,
during training, each tree uses the same number of observations from each drawn cluster: If the
smallest cluster has K units, then when we sample a cluster during training, we only give a random
K elements of the cluster to the tree-growing procedure. When estimating average treatment effects,
each observation is given weight 1/cluster size, so that the total weight of each cluster is the
same. Note that, if this argument is FALSE, sample weights may also be directly adjusted via the
sample.weights argument. If this argument is TRUE, sample.weights must be set to NULL. Default is
FALSE.</p>
</td></tr>
<tr><td><code id="instrumental_forest_+3A_sample.fraction">sample.fraction</code></td>
<td>
<p>Fraction of the data used to build each tree.
Note: If honesty = TRUE, these subsamples will
further be cut by a factor of honesty.fraction. Default is 0.5.</p>
</td></tr>
<tr><td><code id="instrumental_forest_+3A_mtry">mtry</code></td>
<td>
<p>Number of variables tried for each split. Default is
<code class="reqn">\sqrt p + 20</code> where p is the number of variables.</p>
</td></tr>
<tr><td><code id="instrumental_forest_+3A_min.node.size">min.node.size</code></td>
<td>
<p>A target for the minimum number of observations in each tree leaf. Note that nodes
with size smaller than min.node.size can occur, as in the original randomForest package.
Default is 5.</p>
</td></tr>
<tr><td><code id="instrumental_forest_+3A_honesty">honesty</code></td>
<td>
<p>Whether to use honest splitting (i.e., sub-sample splitting). Default is TRUE.
For a detailed description of honesty, honesty.fraction, honesty.prune.leaves, and recommendations for
parameter tuning, see the grf algorithm reference.</p>
</td></tr>
<tr><td><code id="instrumental_forest_+3A_honesty.fraction">honesty.fraction</code></td>
<td>
<p>The fraction of data that will be used for determining splits if honesty = TRUE. Corresponds
to set J1 in the notation of the paper. Default is 0.5 (i.e. half of the data is used for
determining splits).</p>
</td></tr>
<tr><td><code id="instrumental_forest_+3A_honesty.prune.leaves">honesty.prune.leaves</code></td>
<td>
<p>If TRUE, prunes the estimation sample tree such that no leaves
are empty. If FALSE, keep the same tree as determined in the splits sample (if an empty leave is encountered, that
tree is skipped and does not contribute to the estimate). Setting this to FALSE may improve performance on
small/marginally powered data, but requires more trees (note: tuning does not adjust the number of trees).
Only applies if honesty is enabled. Default is TRUE.</p>
</td></tr>
<tr><td><code id="instrumental_forest_+3A_alpha">alpha</code></td>
<td>
<p>A tuning parameter that controls the maximum imbalance of a split. Default is 0.05.</p>
</td></tr>
<tr><td><code id="instrumental_forest_+3A_imbalance.penalty">imbalance.penalty</code></td>
<td>
<p>A tuning parameter that controls how harshly imbalanced splits are penalized. Default is 0.</p>
</td></tr>
<tr><td><code id="instrumental_forest_+3A_stabilize.splits">stabilize.splits</code></td>
<td>
<p>Whether or not the instrument should be taken into account when
determining the imbalance of a split. Default is TRUE.</p>
</td></tr>
<tr><td><code id="instrumental_forest_+3A_ci.group.size">ci.group.size</code></td>
<td>
<p>The forst will grow ci.group.size trees on each subsample.
In order to provide confidence intervals, ci.group.size must
be at least 2. Default is 2.</p>
</td></tr>
<tr><td><code id="instrumental_forest_+3A_reduced.form.weight">reduced.form.weight</code></td>
<td>
<p>Whether splits should be regularized towards a naive
splitting criterion that ignores the instrument (and
instead emulates a causal forest).</p>
</td></tr>
<tr><td><code id="instrumental_forest_+3A_tune.parameters">tune.parameters</code></td>
<td>
<p>A vector of parameter names to tune.
If &quot;all&quot;: all tunable parameters are tuned by cross-validation. The following parameters are
tunable: (&quot;sample.fraction&quot;, &quot;mtry&quot;, &quot;min.node.size&quot;, &quot;honesty.fraction&quot;,
&quot;honesty.prune.leaves&quot;, &quot;alpha&quot;, &quot;imbalance.penalty&quot;). If honesty is FALSE the honesty.* parameters are not tuned.
Default is &quot;none&quot; (no parameters are tuned).</p>
</td></tr>
<tr><td><code id="instrumental_forest_+3A_tune.num.trees">tune.num.trees</code></td>
<td>
<p>The number of trees in each 'mini forest' used to fit the tuning model. Default is 200.</p>
</td></tr>
<tr><td><code id="instrumental_forest_+3A_tune.num.reps">tune.num.reps</code></td>
<td>
<p>The number of forests used to fit the tuning model. Default is 50.</p>
</td></tr>
<tr><td><code id="instrumental_forest_+3A_tune.num.draws">tune.num.draws</code></td>
<td>
<p>The number of random parameter values considered when using the model
to select the optimal parameters. Default is 1000.</p>
</td></tr>
<tr><td><code id="instrumental_forest_+3A_compute.oob.predictions">compute.oob.predictions</code></td>
<td>
<p>Whether OOB predictions on training set should be precomputed. Default is TRUE.</p>
</td></tr>
<tr><td><code id="instrumental_forest_+3A_num.threads">num.threads</code></td>
<td>
<p>Number of threads used in training. By default, the number of threads is set
to the maximum hardware concurrency.</p>
</td></tr>
<tr><td><code id="instrumental_forest_+3A_seed">seed</code></td>
<td>
<p>The seed of the C++ random number generator.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A trained instrumental forest object.
</p>


<h3>References</h3>

<p>Athey, Susan, Julie Tibshirani, and Stefan Wager. &quot;Generalized Random Forests&quot;.
Annals of Statistics, 47(2), 2019.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Train an instrumental forest.
n &lt;- 2000
p &lt;- 5
X &lt;- matrix(rbinom(n * p, 1, 0.5), n, p)
Z &lt;- rbinom(n, 1, 0.5)
Q &lt;- rbinom(n, 1, 0.5)
W &lt;- Q * Z
tau &lt;-  X[, 1] / 2
Y &lt;- rowSums(X[, 1:3]) + tau * W + Q + rnorm(n)
iv.forest &lt;- instrumental_forest(X, Y, W, Z)

# Predict on out-of-bag training samples.
iv.pred &lt;- predict(iv.forest)

# Estimate a (local) average treatment effect.
average_treatment_effect(iv.forest)


</code></pre>

<hr>
<h2 id='leaf_stats.causal_forest'>Calculate summary stats given a set of samples for causal forests.</h2><span id='topic+leaf_stats.causal_forest'></span>

<h3>Description</h3>

<p>Calculate summary stats given a set of samples for causal forests.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'causal_forest'
leaf_stats(forest, samples, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="leaf_stats.causal_forest_+3A_forest">forest</code></td>
<td>
<p>The GRF forest</p>
</td></tr>
<tr><td><code id="leaf_stats.causal_forest_+3A_samples">samples</code></td>
<td>
<p>The samples to include in the calculations.</p>
</td></tr>
<tr><td><code id="leaf_stats.causal_forest_+3A_...">...</code></td>
<td>
<p>Additional arguments (currently ignored).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A named vector containing summary stats
</p>

<hr>
<h2 id='leaf_stats.default'>A default leaf_stats for forests classes without a leaf_stats method
that always returns NULL.</h2><span id='topic+leaf_stats.default'></span>

<h3>Description</h3>

<p>A default leaf_stats for forests classes without a leaf_stats method
that always returns NULL.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## Default S3 method:
leaf_stats(forest, samples, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="leaf_stats.default_+3A_forest">forest</code></td>
<td>
<p>Any forest</p>
</td></tr>
<tr><td><code id="leaf_stats.default_+3A_samples">samples</code></td>
<td>
<p>The samples to include in the calculations.</p>
</td></tr>
<tr><td><code id="leaf_stats.default_+3A_...">...</code></td>
<td>
<p>Additional arguments (currently ignored).</p>
</td></tr>
</table>

<hr>
<h2 id='leaf_stats.instrumental_forest'>Calculate summary stats given a set of samples for instrumental forests.</h2><span id='topic+leaf_stats.instrumental_forest'></span>

<h3>Description</h3>

<p>Calculate summary stats given a set of samples for instrumental forests.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'instrumental_forest'
leaf_stats(forest, samples, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="leaf_stats.instrumental_forest_+3A_forest">forest</code></td>
<td>
<p>The GRF forest</p>
</td></tr>
<tr><td><code id="leaf_stats.instrumental_forest_+3A_samples">samples</code></td>
<td>
<p>The samples to include in the calculations.</p>
</td></tr>
<tr><td><code id="leaf_stats.instrumental_forest_+3A_...">...</code></td>
<td>
<p>Additional arguments (currently ignored).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A named vector containing summary stats
</p>

<hr>
<h2 id='leaf_stats.regression_forest'>Calculate summary stats given a set of samples for regression forests.</h2><span id='topic+leaf_stats.regression_forest'></span>

<h3>Description</h3>

<p>Calculate summary stats given a set of samples for regression forests.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'regression_forest'
leaf_stats(forest, samples, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="leaf_stats.regression_forest_+3A_forest">forest</code></td>
<td>
<p>The GRF forest</p>
</td></tr>
<tr><td><code id="leaf_stats.regression_forest_+3A_samples">samples</code></td>
<td>
<p>The samples to include in the calculations.</p>
</td></tr>
<tr><td><code id="leaf_stats.regression_forest_+3A_...">...</code></td>
<td>
<p>Additional arguments (currently ignored).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A named vector containing summary stats
</p>

<hr>
<h2 id='ll_regression_forest'>Local linear forest</h2><span id='topic+ll_regression_forest'></span>

<h3>Description</h3>

<p>Trains a local linear forest that can be used to estimate
the conditional mean function mu(x) = E[Y | X = x]
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ll_regression_forest(
  X,
  Y,
  enable.ll.split = FALSE,
  ll.split.weight.penalty = FALSE,
  ll.split.lambda = 0.1,
  ll.split.variables = NULL,
  ll.split.cutoff = NULL,
  num.trees = 2000,
  clusters = NULL,
  equalize.cluster.weights = FALSE,
  sample.fraction = 0.5,
  mtry = min(ceiling(sqrt(ncol(X)) + 20), ncol(X)),
  min.node.size = 5,
  honesty = TRUE,
  honesty.fraction = 0.5,
  honesty.prune.leaves = TRUE,
  alpha = 0.05,
  imbalance.penalty = 0,
  ci.group.size = 2,
  tune.parameters = "none",
  tune.num.trees = 50,
  tune.num.reps = 100,
  tune.num.draws = 1000,
  num.threads = NULL,
  seed = runif(1, 0, .Machine$integer.max)
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ll_regression_forest_+3A_x">X</code></td>
<td>
<p>The covariates used in the regression.</p>
</td></tr>
<tr><td><code id="ll_regression_forest_+3A_y">Y</code></td>
<td>
<p>The outcome.</p>
</td></tr>
<tr><td><code id="ll_regression_forest_+3A_enable.ll.split">enable.ll.split</code></td>
<td>
<p>(experimental) Optional choice to make forest splits based on ridge residuals as opposed to
standard CART splits. Defaults to FALSE.</p>
</td></tr>
<tr><td><code id="ll_regression_forest_+3A_ll.split.weight.penalty">ll.split.weight.penalty</code></td>
<td>
<p>If using local linear splits, user can specify whether or not to use a
covariance ridge penalty, analogously to the prediction case. Defaults to FALSE.</p>
</td></tr>
<tr><td><code id="ll_regression_forest_+3A_ll.split.lambda">ll.split.lambda</code></td>
<td>
<p>Ridge penalty for splitting. Defaults to 0.1.</p>
</td></tr>
<tr><td><code id="ll_regression_forest_+3A_ll.split.variables">ll.split.variables</code></td>
<td>
<p>Linear correction variables for splitting. Defaults to all variables.</p>
</td></tr>
<tr><td><code id="ll_regression_forest_+3A_ll.split.cutoff">ll.split.cutoff</code></td>
<td>
<p>Enables the option to use regression coefficients from the full dataset for LL splitting
once leaves get sufficiently small. Leaf size after which we use the overall beta.
Defaults to the square root of the number of samples. If desired, users can enforce no
regulation (i.e., using the leaf betas at each step) by setting this parameter to zero.</p>
</td></tr>
<tr><td><code id="ll_regression_forest_+3A_num.trees">num.trees</code></td>
<td>
<p>Number of trees grown in the forest. Note: Getting accurate
confidence intervals generally requires more trees than
getting accurate predictions. Default is 2000.</p>
</td></tr>
<tr><td><code id="ll_regression_forest_+3A_clusters">clusters</code></td>
<td>
<p>Vector of integers or factors specifying which cluster each observation corresponds to.
Default is NULL (ignored).</p>
</td></tr>
<tr><td><code id="ll_regression_forest_+3A_equalize.cluster.weights">equalize.cluster.weights</code></td>
<td>
<p>If FALSE, each unit is given the same weight (so that bigger
clusters get more weight). If TRUE, each cluster is given equal weight in the forest. In this case,
during training, each tree uses the same number of observations from each drawn cluster: If the
smallest cluster has K units, then when we sample a cluster during training, we only give a random
K elements of the cluster to the tree-growing procedure. When estimating average treatment effects,
each observation is given weight 1/cluster size, so that the total weight of each cluster is the
same. Default is FALSE.</p>
</td></tr>
<tr><td><code id="ll_regression_forest_+3A_sample.fraction">sample.fraction</code></td>
<td>
<p>Fraction of the data used to build each tree.
Note: If honesty = TRUE, these subsamples will
further be cut by a factor of honesty.fraction. Default is 0.5.</p>
</td></tr>
<tr><td><code id="ll_regression_forest_+3A_mtry">mtry</code></td>
<td>
<p>Number of variables tried for each split. Default is
<code class="reqn">\sqrt p + 20</code> where p is the number of variables.</p>
</td></tr>
<tr><td><code id="ll_regression_forest_+3A_min.node.size">min.node.size</code></td>
<td>
<p>A target for the minimum number of observations in each tree leaf. Note that nodes
with size smaller than min.node.size can occur, as in the original randomForest package.
Default is 5.</p>
</td></tr>
<tr><td><code id="ll_regression_forest_+3A_honesty">honesty</code></td>
<td>
<p>Whether to use honest splitting (i.e., sub-sample splitting). Default is TRUE.
For a detailed description of honesty, honesty.fraction, honesty.prune.leaves, and recommendations for
parameter tuning, see the grf algorithm reference.</p>
</td></tr>
<tr><td><code id="ll_regression_forest_+3A_honesty.fraction">honesty.fraction</code></td>
<td>
<p>The fraction of data that will be used for determining splits if honesty = TRUE. Corresponds
to set J1 in the notation of the paper. Default is 0.5 (i.e. half of the data is used for
determining splits).</p>
</td></tr>
<tr><td><code id="ll_regression_forest_+3A_honesty.prune.leaves">honesty.prune.leaves</code></td>
<td>
<p>If TRUE, prunes the estimation sample tree such that no leaves
are empty. If FALSE, keep the same tree as determined in the splits sample (if an empty leave is encountered, that
tree is skipped and does not contribute to the estimate). Setting this to FALSE may improve performance on
small/marginally powered data, but requires more trees (note: tuning does not adjust the number of trees).
Only applies if honesty is enabled. Default is TRUE.</p>
</td></tr>
<tr><td><code id="ll_regression_forest_+3A_alpha">alpha</code></td>
<td>
<p>A tuning parameter that controls the maximum imbalance of a split. Default is 0.05.</p>
</td></tr>
<tr><td><code id="ll_regression_forest_+3A_imbalance.penalty">imbalance.penalty</code></td>
<td>
<p>A tuning parameter that controls how harshly imbalanced splits are penalized. Default is 0.</p>
</td></tr>
<tr><td><code id="ll_regression_forest_+3A_ci.group.size">ci.group.size</code></td>
<td>
<p>The forest will grow ci.group.size trees on each subsample.
In order to provide confidence intervals, ci.group.size must
be at least 2. Default is 1.</p>
</td></tr>
<tr><td><code id="ll_regression_forest_+3A_tune.parameters">tune.parameters</code></td>
<td>
<p>If true, NULL parameters are tuned by cross-validation; if FALSE
NULL parameters are set to defaults. Default is FALSE. Currently, local linear tuning
is based on regression forest fit, and is only supported for 'enable.ll.split = FALSE'.</p>
</td></tr>
<tr><td><code id="ll_regression_forest_+3A_tune.num.trees">tune.num.trees</code></td>
<td>
<p>The number of trees in each 'mini forest' used to fit the tuning model. Default is 10.</p>
</td></tr>
<tr><td><code id="ll_regression_forest_+3A_tune.num.reps">tune.num.reps</code></td>
<td>
<p>The number of forests used to fit the tuning model. Default is 100.</p>
</td></tr>
<tr><td><code id="ll_regression_forest_+3A_tune.num.draws">tune.num.draws</code></td>
<td>
<p>The number of random parameter values considered when using the model
to select the optimal parameters. Default is 1000.</p>
</td></tr>
<tr><td><code id="ll_regression_forest_+3A_num.threads">num.threads</code></td>
<td>
<p>Number of threads used in training. By default, the number of threads is set
to the maximum hardware concurrency.</p>
</td></tr>
<tr><td><code id="ll_regression_forest_+3A_seed">seed</code></td>
<td>
<p>The seed of the C++ random number generator.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A trained local linear forest object.
</p>


<h3>References</h3>

<p>Friedberg, Rina, Julie Tibshirani, Susan Athey, and Stefan Wager. &quot;Local Linear Forests&quot;.
Journal of Computational and Graphical Statistics, 30(2), 2020.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Train a standard regression forest.
n &lt;- 50
p &lt;- 10
X &lt;- matrix(rnorm(n * p), n, p)
Y &lt;- X[, 1] * rnorm(n)
forest &lt;- ll_regression_forest(X, Y)


</code></pre>

<hr>
<h2 id='lm_forest'>LM Forest</h2><span id='topic+lm_forest'></span>

<h3>Description</h3>

<p>Trains a linear model forest that can be used to estimate
<code class="reqn">h_k(x)</code>, k = 1..K at X = x in the the conditional linear model
<code class="reqn">Y = c(x) + h_1(x)W_1 + ... + h_K(x)W_K</code>,
where Y is a (potentially vector-valued) response and
W a set of regressors.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lm_forest(
  X,
  Y,
  W,
  Y.hat = NULL,
  W.hat = NULL,
  num.trees = 2000,
  sample.weights = NULL,
  gradient.weights = NULL,
  clusters = NULL,
  equalize.cluster.weights = FALSE,
  sample.fraction = 0.5,
  mtry = min(ceiling(sqrt(ncol(X)) + 20), ncol(X)),
  min.node.size = 5,
  honesty = TRUE,
  honesty.fraction = 0.5,
  honesty.prune.leaves = TRUE,
  alpha = 0.05,
  imbalance.penalty = 0,
  stabilize.splits = FALSE,
  ci.group.size = 2,
  compute.oob.predictions = TRUE,
  num.threads = NULL,
  seed = runif(1, 0, .Machine$integer.max)
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="lm_forest_+3A_x">X</code></td>
<td>
<p>The covariates used in the regression.</p>
</td></tr>
<tr><td><code id="lm_forest_+3A_y">Y</code></td>
<td>
<p>The outcome (must be a numeric vector or matrix [one column per outcome] with no NAs).
Multiple outcomes should be on the same scale.</p>
</td></tr>
<tr><td><code id="lm_forest_+3A_w">W</code></td>
<td>
<p>The conditional regressors (must be a vector or matrix with no NAs).</p>
</td></tr>
<tr><td><code id="lm_forest_+3A_y.hat">Y.hat</code></td>
<td>
<p>Estimates of the conditional means E[Y | Xi].
If Y.hat = NULL, these are estimated using
a separate multi-task regression forest. Default is NULL.</p>
</td></tr>
<tr><td><code id="lm_forest_+3A_w.hat">W.hat</code></td>
<td>
<p>Estimates of the conditional means E[Wk | Xi].
If W.hat = NULL, these are estimated using
a separate multi-task regression forest. Default is NULL.</p>
</td></tr>
<tr><td><code id="lm_forest_+3A_num.trees">num.trees</code></td>
<td>
<p>Number of trees grown in the forest. Note: Getting accurate
confidence intervals generally requires more trees than
getting accurate predictions. Default is 2000.</p>
</td></tr>
<tr><td><code id="lm_forest_+3A_sample.weights">sample.weights</code></td>
<td>
<p>Weights given to each sample in estimation.
If NULL, each observation receives the same weight.
Default is NULL.</p>
</td></tr>
<tr><td><code id="lm_forest_+3A_gradient.weights">gradient.weights</code></td>
<td>
<p>Weights given to each coefficient h_k(x) when targeting heterogeneity
in the estimates. These enter the GRF algorithm through the split criterion <code class="reqn">\Delta</code>:
the k-th coordinate of this is <code class="reqn">\Delta_k</code> * gradient.weights[k].
If NULL, each coefficient is given the same weight.
Default is NULL.</p>
</td></tr>
<tr><td><code id="lm_forest_+3A_clusters">clusters</code></td>
<td>
<p>Vector of integers or factors specifying which cluster each observation corresponds to.
Default is NULL (ignored).</p>
</td></tr>
<tr><td><code id="lm_forest_+3A_equalize.cluster.weights">equalize.cluster.weights</code></td>
<td>
<p>If FALSE, each unit is given the same weight (so that bigger
clusters get more weight). If TRUE, each cluster is given equal weight in the forest. In this case,
during training, each tree uses the same number of observations from each drawn cluster: If the
smallest cluster has K units, then when we sample a cluster during training, we only give a random
K elements of the cluster to the tree-growing procedure. When estimating average treatment effects,
each observation is given weight 1/cluster size, so that the total weight of each cluster is the
same. Note that, if this argument is FALSE, sample weights may also be directly adjusted via the
sample.weights argument. If this argument is TRUE, sample.weights must be set to NULL. Default is
FALSE.</p>
</td></tr>
<tr><td><code id="lm_forest_+3A_sample.fraction">sample.fraction</code></td>
<td>
<p>Fraction of the data used to build each tree.
Note: If honesty = TRUE, these subsamples will
further be cut by a factor of honesty.fraction. Default is 0.5.</p>
</td></tr>
<tr><td><code id="lm_forest_+3A_mtry">mtry</code></td>
<td>
<p>Number of variables tried for each split. Default is
<code class="reqn">\sqrt p + 20</code> where p is the number of variables.</p>
</td></tr>
<tr><td><code id="lm_forest_+3A_min.node.size">min.node.size</code></td>
<td>
<p>A target for the minimum number of observations in each tree leaf. Note that nodes
with size smaller than min.node.size can occur, as in the original randomForest package.
Default is 5.</p>
</td></tr>
<tr><td><code id="lm_forest_+3A_honesty">honesty</code></td>
<td>
<p>Whether to use honest splitting (i.e., sub-sample splitting). Default is TRUE.
For a detailed description of honesty, honesty.fraction, honesty.prune.leaves, and recommendations for
parameter tuning, see the grf algorithm reference.</p>
</td></tr>
<tr><td><code id="lm_forest_+3A_honesty.fraction">honesty.fraction</code></td>
<td>
<p>The fraction of data that will be used for determining splits if honesty = TRUE. Corresponds
to set J1 in the notation of the paper. Default is 0.5 (i.e. half of the data is used for
determining splits).</p>
</td></tr>
<tr><td><code id="lm_forest_+3A_honesty.prune.leaves">honesty.prune.leaves</code></td>
<td>
<p>If TRUE, prunes the estimation sample tree such that no leaves
are empty. If FALSE, keep the same tree as determined in the splits sample (if an empty leave is encountered, that
tree is skipped and does not contribute to the estimate). Setting this to FALSE may improve performance on
small/marginally powered data, but requires more trees (note: tuning does not adjust the number of trees).
Only applies if honesty is enabled. Default is TRUE.</p>
</td></tr>
<tr><td><code id="lm_forest_+3A_alpha">alpha</code></td>
<td>
<p>A tuning parameter that controls the maximum imbalance of a split. Default is 0.05.</p>
</td></tr>
<tr><td><code id="lm_forest_+3A_imbalance.penalty">imbalance.penalty</code></td>
<td>
<p>A tuning parameter that controls how harshly imbalanced splits are penalized. Default is 0.</p>
</td></tr>
<tr><td><code id="lm_forest_+3A_stabilize.splits">stabilize.splits</code></td>
<td>
<p>Whether or not Wk should be taken into account when
determining the imbalance of a split. It is an exact extension of the single-arm constraints (detailed
in the causal forest algorithm reference) to multiple arms, where the constraints apply to each regressor Wk.
Default is FALSE.</p>
</td></tr>
<tr><td><code id="lm_forest_+3A_ci.group.size">ci.group.size</code></td>
<td>
<p>The forest will grow ci.group.size trees on each subsample.
In order to provide confidence intervals, ci.group.size must
be at least 2. Default is 2. (Confidence intervals are
currently only supported for univariate outcomes Y).</p>
</td></tr>
<tr><td><code id="lm_forest_+3A_compute.oob.predictions">compute.oob.predictions</code></td>
<td>
<p>Whether OOB predictions on training set should be precomputed. Default is TRUE.</p>
</td></tr>
<tr><td><code id="lm_forest_+3A_num.threads">num.threads</code></td>
<td>
<p>Number of threads used in training. By default, the number of threads is set
to the maximum hardware concurrency.</p>
</td></tr>
<tr><td><code id="lm_forest_+3A_seed">seed</code></td>
<td>
<p>The seed of the C++ random number generator.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A trained lm forest object.
</p>


<h3>References</h3>

<p>Athey, Susan, Julie Tibshirani, and Stefan Wager. &quot;Generalized Random Forests&quot;.
Annals of Statistics, 47(2), 2019.
</p>
<p>Zeileis, Achim, Torsten Hothorn, and Kurt Hornik. &quot;Model-based Recursive Partitioning.&quot;
Journal of Computational and Graphical Statistics 17(2), 2008.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
if (require("rdd", quietly = TRUE)) {
# Train a LM Forest to estimate CATEs in a regression discontinuity design.
# Simulate a simple example with a heterogeneous jump in the CEF.
n &lt;- 2000
p &lt;- 5
X &lt;- matrix(rnorm(n * p), n, p)
Z &lt;- runif(n, -4, 4)
cutoff &lt;- 0
W &lt;- as.numeric(Z &gt;= cutoff)
tau &lt;- pmax(0.5 * X[, 1], 0)
Y &lt;- tau * W  + 1 / (1 + exp(2 * Z)) + 0.2 * rnorm(n)

# Compute the Imbens-Kalyanaraman MSE-optimal bandwidth for a local linear regression.
bandwidth &lt;- IKbandwidth(Z, Y, cutoff)
# Compute kernel weights for a triangular kernel.
sample.weights &lt;- kernelwts(Z, cutoff, bandwidth, "triangular")

# Alternatively, specify bandwith and triangular kernel weights without using the `rdd` package.
# bandwidth &lt;- # user can hand-specify this.
# dist &lt;- abs((Z - cutoff) / bandwidth)
# sample.weights &lt;- (1 - dist) * (dist &lt;= 1) / bandwidth

# Estimate a local linear regression with the running variable Z conditional on covariates X = x:
# Y = c(x) + tau(x) W + b(x) Z.
# Specify gradient.weights = c(1, 0) to target heterogeneity in the RDD coefficient tau(x).
# Also, fit forest on subset with non-zero weights for faster estimation.
subset &lt;- sample.weights &gt; 0
lmf &lt;- lm_forest(X[subset, ], Y[subset], cbind(W, Z)[subset, ],
                 sample.weights = sample.weights[subset], gradient.weights = c(1, 0))
tau.hat &lt;- predict(lmf)$predictions[, 1, ]

# Plot estimated tau(x) vs simulated ground truth.
plot(X[subset, 1], tau.hat)
points(X[subset, 1], tau[subset], col = "red", cex = 0.1)
}


</code></pre>

<hr>
<h2 id='merge_forests'>Merges a list of forests that were grown using the same data into one large forest.</h2><span id='topic+merge_forests'></span>

<h3>Description</h3>

<p>Merges a list of forests that were grown using the same data into one large forest.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>merge_forests(forest_list, compute.oob.predictions = TRUE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="merge_forests_+3A_forest_list">forest_list</code></td>
<td>
<p>A 'list' of forests to be concatenated.
All forests must be of the same type, and the type must be a subclass of 'grf'.
In addition, all forests must have the same 'ci.group.size'.
Other tuning parameters (e.g. alpha, mtry, min.node.size, imbalance.penalty) are
allowed to differ across forests.</p>
</td></tr>
<tr><td><code id="merge_forests_+3A_compute.oob.predictions">compute.oob.predictions</code></td>
<td>
<p>Whether OOB predictions on training set should be precomputed.
Note that even if OOB predictions have already been precomputed for the forests in 'forest_list',
those predictions are not used. Instead, a new set of oob predictions is computed anew using the
larger forest. Default is TRUE.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A single forest containing all the trees in each forest in the input list.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Train standard regression forests
n &lt;- 50
p &lt;- 10
X &lt;- matrix(rnorm(n * p), n, p)
Y &lt;- X[, 1] * rnorm(n)
r.forest1 &lt;- regression_forest(X, Y, compute.oob.predictions = FALSE, num.trees = 100)
r.forest2 &lt;- regression_forest(X, Y, compute.oob.predictions = FALSE, num.trees = 100)

# Join the forests together. The resulting forest will contain 200 trees.
big_rf &lt;- merge_forests(list(r.forest1, r.forest2))


</code></pre>

<hr>
<h2 id='multi_arm_causal_forest'>Multi-arm/multi-outcome causal forest</h2><span id='topic+multi_arm_causal_forest'></span>

<h3>Description</h3>

<p>Trains a causal forest that can be used to estimate
conditional average treatment effects tau_k(X). When
the treatment assignment W is {1, ..., K} and unconfounded,
we have tau_k(X) = E[Y(k) - Y(1) | X = x] where Y(k) and
Y(1) are potential outcomes corresponding to the treatment
state for arm k and the baseline arm 1.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>multi_arm_causal_forest(
  X,
  Y,
  W,
  Y.hat = NULL,
  W.hat = NULL,
  num.trees = 2000,
  sample.weights = NULL,
  clusters = NULL,
  equalize.cluster.weights = FALSE,
  sample.fraction = 0.5,
  mtry = min(ceiling(sqrt(ncol(X)) + 20), ncol(X)),
  min.node.size = 5,
  honesty = TRUE,
  honesty.fraction = 0.5,
  honesty.prune.leaves = TRUE,
  alpha = 0.05,
  imbalance.penalty = 0,
  stabilize.splits = TRUE,
  ci.group.size = 2,
  compute.oob.predictions = TRUE,
  num.threads = NULL,
  seed = runif(1, 0, .Machine$integer.max)
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="multi_arm_causal_forest_+3A_x">X</code></td>
<td>
<p>The covariates used in the causal regression.</p>
</td></tr>
<tr><td><code id="multi_arm_causal_forest_+3A_y">Y</code></td>
<td>
<p>The outcome (must be a numeric vector or matrix [one column per outcome] with no NAs).
Multiple outcomes should be on the same scale.</p>
</td></tr>
<tr><td><code id="multi_arm_causal_forest_+3A_w">W</code></td>
<td>
<p>The treatment assignment (must be a factor vector with no NAs). The reference treatment
is set to the first treatment according to the ordinality of the factors, this can be changed
with the 'relevel' function in R.</p>
</td></tr>
<tr><td><code id="multi_arm_causal_forest_+3A_y.hat">Y.hat</code></td>
<td>
<p>Estimates of the expected responses E[Y | Xi], marginalizing
over treatment. If Y.hat = NULL, these are estimated using
a separate multi-task regression forest. Default is NULL.</p>
</td></tr>
<tr><td><code id="multi_arm_causal_forest_+3A_w.hat">W.hat</code></td>
<td>
<p>Matrix with estimates of the treatment propensities E[Wk | Xi].
If W.hat = NULL, these are estimated using a probability forest.</p>
</td></tr>
<tr><td><code id="multi_arm_causal_forest_+3A_num.trees">num.trees</code></td>
<td>
<p>Number of trees grown in the forest. Note: Getting accurate
confidence intervals generally requires more trees than
getting accurate predictions. Default is 2000.</p>
</td></tr>
<tr><td><code id="multi_arm_causal_forest_+3A_sample.weights">sample.weights</code></td>
<td>
<p>Weights given to each sample in estimation.
If NULL, each observation receives the same weight.
Note: To avoid introducing confounding, weights should be
independent of the potential outcomes given X. Default is NULL.</p>
</td></tr>
<tr><td><code id="multi_arm_causal_forest_+3A_clusters">clusters</code></td>
<td>
<p>Vector of integers or factors specifying which cluster each observation corresponds to.
Default is NULL (ignored).</p>
</td></tr>
<tr><td><code id="multi_arm_causal_forest_+3A_equalize.cluster.weights">equalize.cluster.weights</code></td>
<td>
<p>If FALSE, each unit is given the same weight (so that bigger
clusters get more weight). If TRUE, each cluster is given equal weight in the forest. In this case,
during training, each tree uses the same number of observations from each drawn cluster: If the
smallest cluster has K units, then when we sample a cluster during training, we only give a random
K elements of the cluster to the tree-growing procedure. When estimating average treatment effects,
each observation is given weight 1/cluster size, so that the total weight of each cluster is the
same. Note that, if this argument is FALSE, sample weights may also be directly adjusted via the
sample.weights argument. If this argument is TRUE, sample.weights must be set to NULL. Default is
FALSE.</p>
</td></tr>
<tr><td><code id="multi_arm_causal_forest_+3A_sample.fraction">sample.fraction</code></td>
<td>
<p>Fraction of the data used to build each tree.
Note: If honesty = TRUE, these subsamples will
further be cut by a factor of honesty.fraction. Default is 0.5.</p>
</td></tr>
<tr><td><code id="multi_arm_causal_forest_+3A_mtry">mtry</code></td>
<td>
<p>Number of variables tried for each split. Default is
<code class="reqn">\sqrt p + 20</code> where p is the number of variables.</p>
</td></tr>
<tr><td><code id="multi_arm_causal_forest_+3A_min.node.size">min.node.size</code></td>
<td>
<p>A target for the minimum number of observations in each tree leaf. Note that nodes
with size smaller than min.node.size can occur, as in the original randomForest package.
Default is 5.</p>
</td></tr>
<tr><td><code id="multi_arm_causal_forest_+3A_honesty">honesty</code></td>
<td>
<p>Whether to use honest splitting (i.e., sub-sample splitting). Default is TRUE.
For a detailed description of honesty, honesty.fraction, honesty.prune.leaves, and recommendations for
parameter tuning, see the grf algorithm reference.</p>
</td></tr>
<tr><td><code id="multi_arm_causal_forest_+3A_honesty.fraction">honesty.fraction</code></td>
<td>
<p>The fraction of data that will be used for determining splits if honesty = TRUE. Corresponds
to set J1 in the notation of the paper. Default is 0.5 (i.e. half of the data is used for
determining splits).</p>
</td></tr>
<tr><td><code id="multi_arm_causal_forest_+3A_honesty.prune.leaves">honesty.prune.leaves</code></td>
<td>
<p>If TRUE, prunes the estimation sample tree such that no leaves
are empty. If FALSE, keep the same tree as determined in the splits sample (if an empty leave is encountered, that
tree is skipped and does not contribute to the estimate). Setting this to FALSE may improve performance on
small/marginally powered data, but requires more trees (note: tuning does not adjust the number of trees).
Only applies if honesty is enabled. Default is TRUE.</p>
</td></tr>
<tr><td><code id="multi_arm_causal_forest_+3A_alpha">alpha</code></td>
<td>
<p>A tuning parameter that controls the maximum imbalance of a split. Default is 0.05.</p>
</td></tr>
<tr><td><code id="multi_arm_causal_forest_+3A_imbalance.penalty">imbalance.penalty</code></td>
<td>
<p>A tuning parameter that controls how harshly imbalanced splits are penalized. Default is 0.</p>
</td></tr>
<tr><td><code id="multi_arm_causal_forest_+3A_stabilize.splits">stabilize.splits</code></td>
<td>
<p>Whether or not the treatment should be taken into account when
determining the imbalance of a split. It is an exact extension of the single-arm constraints (detailed
in the causal forest algorithm reference) to multiple arms, where the constraints apply to each treatment arm independently.
Default is TRUE.</p>
</td></tr>
<tr><td><code id="multi_arm_causal_forest_+3A_ci.group.size">ci.group.size</code></td>
<td>
<p>The forest will grow ci.group.size trees on each subsample.
In order to provide confidence intervals, ci.group.size must
be at least 2. Default is 2. (Confidence intervals are
currently only supported for univariate outcomes Y).</p>
</td></tr>
<tr><td><code id="multi_arm_causal_forest_+3A_compute.oob.predictions">compute.oob.predictions</code></td>
<td>
<p>Whether OOB predictions on training set should be precomputed. Default is TRUE.</p>
</td></tr>
<tr><td><code id="multi_arm_causal_forest_+3A_num.threads">num.threads</code></td>
<td>
<p>Number of threads used in training. By default, the number of threads is set
to the maximum hardware concurrency.</p>
</td></tr>
<tr><td><code id="multi_arm_causal_forest_+3A_seed">seed</code></td>
<td>
<p>The seed of the C++ random number generator.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This forest fits a multi-arm treatment estimate following the multivariate
extension of the &quot;R-learner&quot; suggested in Nie and Wager (2021), with kernel
weights derived by the GRF algorithm (Athey, Tibshirani, and Wager, 2019).
In particular, with K arms, and W encoded as {0, 1}^(K-1), we estimate, for
a target sample x, and a chosen baseline arm:
</p>
<p><code class="reqn">\hat \tau(x) = argmin_{\tau} \left\{ \sum_{i=1}^{n}
\alpha_i (x) \left( Y_i - \hat m^{(-i)}(X_i) - c(x) -
\left\langle W_i - \hat e^{(-i)}(X_i), \,  \tau(X_i)  \right\rangle
\right)^2 \right\}</code>,
</p>
<p>where the angle brackets indicates an inner product, e(X) = E[W | X = x] is
a (vector valued) generalized propensity score, and m(x) = E[Y | X = x].
The forest weights alpha(x) are derived from a generalized random forest
splitting on the vector-valued gradient of tau(x). (The intercept c(x)
is a nuisance parameter not directly estimated). By default, e(X) and
m(X) are estimated using two separate random forests, a probability forest
and regression forest respectively (optionally provided through the arguments
W.hat and Y.hat). The k-th element of tau(x) measures the conditional average
treatment effect of the k-th treatment arm at X = x for k = 1, ..., K-1.
The treatment effect for multiple outcomes can be estimated jointly (i.e. Y can
be vector-valued) - in which case the splitting rule takes into account
all outcomes simultaneously (specifically, we concatenate the gradient
vector for each outcome).
</p>
<p>For a single treatment and outcome, this forest is equivalent to a causal forest, however,
they may produce different results due to differences in numerics.
</p>


<h3>Value</h3>

<p>A trained multi arm causal forest object.
</p>


<h3>References</h3>

<p>Athey, Susan, Julie Tibshirani, and Stefan Wager. &quot;Generalized Random Forests&quot;.
Annals of Statistics, 47(2), 2019.
</p>
<p>Nie, Xinkun, and Stefan Wager. &quot;Quasi-Oracle Estimation of Heterogeneous Treatment Effects&quot;.
Biometrika, 108(2), 2021.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Train a multi arm causal forest.
n &lt;- 500
p &lt;- 10
X &lt;- matrix(rnorm(n * p), n, p)
W &lt;- as.factor(sample(c("A", "B", "C"), n, replace = TRUE))
Y &lt;- X[, 1] + X[, 2] * (W == "B") - 1.5 * X[, 2] * (W == "C") + rnorm(n)
mc.forest &lt;- multi_arm_causal_forest(X, Y, W)

# Predict contrasts (out-of-bag) using the forest.
# Fitting several outcomes jointly is supported, and the returned prediction array has
# dimension [num.samples, num.contrasts, num.outcomes]. Since num.outcomes is one in
# this example, we use drop = TRUE to ignore this singleton dimension.
mc.pred &lt;- predict(mc.forest, drop = TRUE)

# By default, the first ordinal treatment is used as baseline ("A" in this example),
# giving two contrasts tau_B = Y(B) - Y(A), tau_C = Y(C) - Y(A)
tau.hat &lt;- mc.pred$predictions

plot(X[, 2], tau.hat[, "B - A"], ylab = "tau.contrast")
abline(0, 1, col = "red")
points(X[, 2], tau.hat[, "C - A"], col = "blue")
abline(0, -1.5, col = "red")
legend("topleft", c("B - A", "C - A"), col = c("black", "blue"), pch = 19)

# A doubly robust estimate (AIPW) of the average treatment effect of the arms.
average_treatment_effect(mc.forest)

# The conditional response surfaces mu_k(X) for a single outcome can be reconstructed from
# the contrasts tau_k(x), the treatment propensities e_k(x), and the conditional mean m(x).
# Given treatment "A" as baseline we have:
# m(x) := E[Y | X] = E[Y(A) | X] + E[W_B (Y(B) - Y(A))] + E[W_C (Y(C) - Y(A))]
# which given unconfoundedness is equal to:
# m(x) = mu(A, x) + e_B(x) tau_B(X) + e_C(x) tau_C(x)
# Rearranging and plugging in the above expressions, we obtain the following estimates
# * mu(A, x) = m(x) - e_B(x) tau_B(x) - e_C(x) tau_C(x)
# * mu(B, x) = m(x) + (1 - e_B(x)) tau_B(x) - e_C(x) tau_C(x)
# * mu(C, x) = m(x) - e_B(x) tau_B(x) + (1 - e_C(x)) tau_C(x)
Y.hat &lt;- mc.forest$Y.hat
W.hat &lt;- mc.forest$W.hat

muA &lt;- Y.hat - W.hat[, "B"] * tau.hat[, "B - A"] - W.hat[, "C"] * tau.hat[, "C - A"]
muB &lt;- Y.hat + (1 - W.hat[, "B"]) * tau.hat[, "B - A"] - W.hat[, "C"] * tau.hat[, "C - A"]
muC &lt;- Y.hat - W.hat[, "B"] * tau.hat[, "B - A"] + (1 - W.hat[, "C"]) * tau.hat[, "C - A"]

# These can also be obtained with some array manipulations.
# (the first column is always the baseline arm)
Y.hat.baseline &lt;- Y.hat - rowSums(W.hat[, -1, drop = FALSE] * tau.hat)
mu.hat.matrix &lt;- cbind(Y.hat.baseline, c(Y.hat.baseline) + tau.hat)
colnames(mu.hat.matrix) &lt;- levels(W)
head(mu.hat.matrix)

# The reference level for contrast prediction can be changed with `relevel`.
# Fit and predict with treatment B as baseline:
W &lt;- relevel(W, ref = "B")
mc.forest.B &lt;- multi_arm_causal_forest(X, Y, W)


</code></pre>

<hr>
<h2 id='multi_regression_forest'>Multi-task regression forest</h2><span id='topic+multi_regression_forest'></span>

<h3>Description</h3>

<p>Trains a regression forest that can be used to estimate
the conditional mean functions mu_i(x) = E[Y_i | X = x]
</p>


<h3>Usage</h3>

<pre><code class='language-R'>multi_regression_forest(
  X,
  Y,
  num.trees = 2000,
  sample.weights = NULL,
  clusters = NULL,
  equalize.cluster.weights = FALSE,
  sample.fraction = 0.5,
  mtry = min(ceiling(sqrt(ncol(X)) + 20), ncol(X)),
  min.node.size = 5,
  honesty = TRUE,
  honesty.fraction = 0.5,
  honesty.prune.leaves = TRUE,
  alpha = 0.05,
  imbalance.penalty = 0,
  compute.oob.predictions = TRUE,
  num.threads = NULL,
  seed = runif(1, 0, .Machine$integer.max)
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="multi_regression_forest_+3A_x">X</code></td>
<td>
<p>The covariates used in the regression.</p>
</td></tr>
<tr><td><code id="multi_regression_forest_+3A_y">Y</code></td>
<td>
<p>The outcomes (must be a numeric vector/matrix with no NAs).</p>
</td></tr>
<tr><td><code id="multi_regression_forest_+3A_num.trees">num.trees</code></td>
<td>
<p>Number of trees grown in the forest. Note: Getting accurate
confidence intervals generally requires more trees than
getting accurate predictions. Default is 2000.</p>
</td></tr>
<tr><td><code id="multi_regression_forest_+3A_sample.weights">sample.weights</code></td>
<td>
<p>Weights given to an observation in estimation.
If NULL, each observation is given the same weight. Default is NULL.</p>
</td></tr>
<tr><td><code id="multi_regression_forest_+3A_clusters">clusters</code></td>
<td>
<p>Vector of integers or factors specifying which cluster each observation corresponds to.
Default is NULL (ignored).</p>
</td></tr>
<tr><td><code id="multi_regression_forest_+3A_equalize.cluster.weights">equalize.cluster.weights</code></td>
<td>
<p>If FALSE, each unit is given the same weight (so that bigger
clusters get more weight). If TRUE, each cluster is given equal weight in the forest. In this case,
during training, each tree uses the same number of observations from each drawn cluster: If the
smallest cluster has K units, then when we sample a cluster during training, we only give a random
K elements of the cluster to the tree-growing procedure. When estimating average treatment effects,
each observation is given weight 1/cluster size, so that the total weight of each cluster is the
same. Note that, if this argument is FALSE, sample weights may also be directly adjusted via the
sample.weights argument. If this argument is TRUE, sample.weights must be set to NULL. Default is
FALSE.</p>
</td></tr>
<tr><td><code id="multi_regression_forest_+3A_sample.fraction">sample.fraction</code></td>
<td>
<p>Fraction of the data used to build each tree.
Note: If honesty = TRUE, these subsamples will
further be cut by a factor of honesty.fraction. Default is 0.5.</p>
</td></tr>
<tr><td><code id="multi_regression_forest_+3A_mtry">mtry</code></td>
<td>
<p>Number of variables tried for each split. Default is
<code class="reqn">\sqrt p + 20</code> where p is the number of variables.</p>
</td></tr>
<tr><td><code id="multi_regression_forest_+3A_min.node.size">min.node.size</code></td>
<td>
<p>A target for the minimum number of observations in each tree leaf. Note that nodes
with size smaller than min.node.size can occur, as in the original randomForest package.
Default is 5.</p>
</td></tr>
<tr><td><code id="multi_regression_forest_+3A_honesty">honesty</code></td>
<td>
<p>Whether to use honest splitting (i.e., sub-sample splitting). Default is TRUE.
For a detailed description of honesty, honesty.fraction, honesty.prune.leaves, and recommendations for
parameter tuning, see the grf algorithm reference.</p>
</td></tr>
<tr><td><code id="multi_regression_forest_+3A_honesty.fraction">honesty.fraction</code></td>
<td>
<p>The fraction of data that will be used for determining splits if honesty = TRUE. Corresponds
to set J1 in the notation of the paper. Default is 0.5 (i.e. half of the data is used for
determining splits).</p>
</td></tr>
<tr><td><code id="multi_regression_forest_+3A_honesty.prune.leaves">honesty.prune.leaves</code></td>
<td>
<p>If TRUE, prunes the estimation sample tree such that no leaves
are empty. If FALSE, keep the same tree as determined in the splits sample (if an empty leave is encountered, that
tree is skipped and does not contribute to the estimate). Setting this to FALSE may improve performance on
small/marginally powered data, but requires more trees (note: tuning does not adjust the number of trees).
Only applies if honesty is enabled. Default is TRUE.</p>
</td></tr>
<tr><td><code id="multi_regression_forest_+3A_alpha">alpha</code></td>
<td>
<p>A tuning parameter that controls the maximum imbalance of a split. Default is 0.05.</p>
</td></tr>
<tr><td><code id="multi_regression_forest_+3A_imbalance.penalty">imbalance.penalty</code></td>
<td>
<p>A tuning parameter that controls how harshly imbalanced splits are penalized. Default is 0.</p>
</td></tr>
<tr><td><code id="multi_regression_forest_+3A_compute.oob.predictions">compute.oob.predictions</code></td>
<td>
<p>Whether OOB predictions on training set should be precomputed. Default is TRUE.</p>
</td></tr>
<tr><td><code id="multi_regression_forest_+3A_num.threads">num.threads</code></td>
<td>
<p>Number of threads used in training. By default, the number of threads is set
to the maximum hardware concurrency.</p>
</td></tr>
<tr><td><code id="multi_regression_forest_+3A_seed">seed</code></td>
<td>
<p>The seed of the C++ random number generator.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A trained multi regression forest object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Train a standard regression forest.
n &lt;- 500
p &lt;- 5
X &lt;- matrix(rnorm(n * p), n, p)
Y &lt;-  X[, 1, drop = FALSE] %*% cbind(1, 2) + rnorm(n)
mr.forest &lt;- multi_regression_forest(X, Y)

# Predict using the forest.
X.test &lt;- matrix(0, 101, p)
X.test[, 1] &lt;- seq(-2, 2, length.out = 101)
mr.pred &lt;- predict(mr.forest, X.test)

# Predict on out-of-bag training samples.
mr.pred &lt;- predict(mr.forest)


</code></pre>

<hr>
<h2 id='plot.grf_tree'>Plot a GRF tree object.</h2><span id='topic+plot.grf_tree'></span>

<h3>Description</h3>

<p>The direction NAs are sent are indicated with the arrow fill. An empty arrow indicates
that NAs are sent that way. If trained without missing values, both arrows are filled.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'grf_tree'
plot(x, include.na.path = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.grf_tree_+3A_x">x</code></td>
<td>
<p>The tree to plot</p>
</td></tr>
<tr><td><code id="plot.grf_tree_+3A_include.na.path">include.na.path</code></td>
<td>
<p>A boolean toggling whether to include the path of missing values or not.
It defaults to whether the forest was trained with NAs.</p>
</td></tr>
<tr><td><code id="plot.grf_tree_+3A_...">...</code></td>
<td>
<p>Additional arguments (currently ignored).</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
# Plot a tree in the forest (requires the `DiagrammeR` package).
n &lt;- 500
p &lt;- 10
X &lt;- matrix(rnorm(n * p), n, p)
W &lt;- rbinom(n, 1, 0.5)
Y &lt;- pmax(X[, 1], 0) * W + X[, 2] + pmin(X[, 3], 0) + rnorm(n)
c.forest &lt;- causal_forest(X, Y, W)
plot(tree &lt;- get_tree(c.forest, 1))
# Compute the leaf nodes the first five samples falls into.
leaf.nodes &lt;- get_leaf_node(tree, X[1:5, ])

# Saving a plot in .svg can be done with the `DiagrammeRsvg` package.
install.packages("DiagrammeRsvg")
tree.plot = plot(tree)
cat(DiagrammeRsvg::export_svg(tree.plot), file = 'plot.svg')

## End(Not run)
</code></pre>

<hr>
<h2 id='plot.rank_average_treatment_effect'>Plot the Targeting Operator Characteristic curve.</h2><span id='topic+plot.rank_average_treatment_effect'></span>

<h3>Description</h3>

<p>Plot the Targeting Operator Characteristic curve.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'rank_average_treatment_effect'
plot(x, ..., ci.args = list(), abline.args = list(), legend.args = list())
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot.rank_average_treatment_effect_+3A_x">x</code></td>
<td>
<p>The output of rank_average_treatment_effect.</p>
</td></tr>
<tr><td><code id="plot.rank_average_treatment_effect_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to plot.</p>
</td></tr>
<tr><td><code id="plot.rank_average_treatment_effect_+3A_ci.args">ci.args</code></td>
<td>
<p>Additional arguments passed to points.</p>
</td></tr>
<tr><td><code id="plot.rank_average_treatment_effect_+3A_abline.args">abline.args</code></td>
<td>
<p>Additional arguments passed to abline.</p>
</td></tr>
<tr><td><code id="plot.rank_average_treatment_effect_+3A_legend.args">legend.args</code></td>
<td>
<p>Additional arguments passed to legend.</p>
</td></tr>
</table>

<hr>
<h2 id='predict.boosted_regression_forest'>Predict with a boosted regression forest.</h2><span id='topic+predict.boosted_regression_forest'></span>

<h3>Description</h3>

<p>Gets estimates of E[Y|X=x] using a trained regression forest.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'boosted_regression_forest'
predict(
  object,
  newdata = NULL,
  boost.predict.steps = NULL,
  num.threads = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.boosted_regression_forest_+3A_object">object</code></td>
<td>
<p>The trained forest.</p>
</td></tr>
<tr><td><code id="predict.boosted_regression_forest_+3A_newdata">newdata</code></td>
<td>
<p>Points at which predictions should be made. If NULL, makes out-of-bag
predictions on the training set instead (i.e., provides predictions at
Xi using only trees that did not use the i-th training example). Note
that this matrix should have the number of columns as the training
matrix, and that the columns must appear in the same order</p>
</td></tr>
<tr><td><code id="predict.boosted_regression_forest_+3A_boost.predict.steps">boost.predict.steps</code></td>
<td>
<p>Number of boosting iterations to use for prediction. If blank, uses the full number of
steps for the object given</p>
</td></tr>
<tr><td><code id="predict.boosted_regression_forest_+3A_num.threads">num.threads</code></td>
<td>
<p>the number of threads used in prediction</p>
</td></tr>
<tr><td><code id="predict.boosted_regression_forest_+3A_...">...</code></td>
<td>
<p>Additional arguments (currently ignored).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of predictions.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Train a boosted regression forest.
n &lt;- 50
p &lt;- 10
X &lt;- matrix(rnorm(n * p), n, p)
Y &lt;- X[, 1] * rnorm(n)
r.boosted.forest &lt;- boosted_regression_forest(X, Y)

# Predict using the forest.
X.test &lt;- matrix(0, 101, p)
X.test[, 1] &lt;- seq(-2, 2, length.out = 101)
r.pred &lt;- predict(r.boosted.forest, X.test)

# Predict on out-of-bag training samples.
r.pred &lt;- predict(r.boosted.forest)


</code></pre>

<hr>
<h2 id='predict.causal_forest'>Predict with a causal forest</h2><span id='topic+predict.causal_forest'></span>

<h3>Description</h3>

<p>Gets estimates of tau(x) using a trained causal forest.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'causal_forest'
predict(
  object,
  newdata = NULL,
  linear.correction.variables = NULL,
  ll.lambda = NULL,
  ll.weight.penalty = FALSE,
  num.threads = NULL,
  estimate.variance = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.causal_forest_+3A_object">object</code></td>
<td>
<p>The trained forest.</p>
</td></tr>
<tr><td><code id="predict.causal_forest_+3A_newdata">newdata</code></td>
<td>
<p>Points at which predictions should be made. If NULL, makes out-of-bag
predictions on the training set instead (i.e., provides predictions at
Xi using only trees that did not use the i-th training example). Note
that this matrix should have the number of columns as the training
matrix, and that the columns must appear in the same order.</p>
</td></tr>
<tr><td><code id="predict.causal_forest_+3A_linear.correction.variables">linear.correction.variables</code></td>
<td>
<p>Optional subset of indexes for variables to be used in local
linear prediction. If NULL, standard GRF prediction is used. Otherwise,
we run a locally weighted linear regression on the included variables.
Please note that this is a beta feature still in development, and may slow down
prediction considerably. Defaults to NULL.</p>
</td></tr>
<tr><td><code id="predict.causal_forest_+3A_ll.lambda">ll.lambda</code></td>
<td>
<p>Ridge penalty for local linear predictions. Defaults to NULL and will be cross-validated.</p>
</td></tr>
<tr><td><code id="predict.causal_forest_+3A_ll.weight.penalty">ll.weight.penalty</code></td>
<td>
<p>Option to standardize ridge penalty by covariance (TRUE),
or penalize all covariates equally (FALSE). Penalizes equally by default.</p>
</td></tr>
<tr><td><code id="predict.causal_forest_+3A_num.threads">num.threads</code></td>
<td>
<p>Number of threads used in prediction. If set to NULL, the software
automatically selects an appropriate amount.</p>
</td></tr>
<tr><td><code id="predict.causal_forest_+3A_estimate.variance">estimate.variance</code></td>
<td>
<p>Whether variance estimates for <code class="reqn">\hat\tau(x)</code> are desired
(for confidence intervals).</p>
</td></tr>
<tr><td><code id="predict.causal_forest_+3A_...">...</code></td>
<td>
<p>Additional arguments (currently ignored).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Vector of predictions, along with estimates of the error and
(optionally) its variance estimates. Column 'predictions' contains estimates
of the conditional average treatent effect (CATE). The square-root of
column 'variance.estimates' is the standard error of CATE.
For out-of-bag estimates, we also output the following error measures.
First, column 'debiased.error' contains estimates of the 'R-loss' criterion,
(See Nie and Wager, 2021 for a justification). Second, column 'excess.error'
contains jackknife estimates of the Monte-carlo error (Wager, Hastie, Efron 2014),
a measure of how unstable estimates are if we grow forests of the same size
on the same data set. The sum of 'debiased.error' and 'excess.error' is the raw error
attained by the current forest, and 'debiased.error' alone is an estimate of the error
attained by a forest with an infinite number of trees. We recommend that users grow
enough forests to make the 'excess.error' negligible.
</p>


<h3>References</h3>

<p>Friedberg, Rina, Julie Tibshirani, Susan Athey, and Stefan Wager. &quot;Local Linear Forests&quot;.
Journal of Computational and Graphical Statistics, 30(2), 2020.
</p>
<p>Wager, Stefan, Trevor Hastie, and Bradley Efron.
&quot;Confidence intervals for random forests: The jackknife and the infinitesimal jackknife.&quot;
The Journal of Machine Learning Research 15(1), 2014.
</p>
<p>Nie, Xinkun, and Stefan Wager. &quot;Quasi-Oracle Estimation of Heterogeneous Treatment Effects&quot;.
Biometrika, 108(2), 2021.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Train a causal forest.
n &lt;- 100
p &lt;- 10
X &lt;- matrix(rnorm(n * p), n, p)
W &lt;- rbinom(n, 1, 0.5)
Y &lt;- pmax(X[, 1], 0) * W + X[, 2] + pmin(X[, 3], 0) + rnorm(n)
c.forest &lt;- causal_forest(X, Y, W)

# Predict using the forest.
X.test &lt;- matrix(0, 101, p)
X.test[, 1] &lt;- seq(-2, 2, length.out = 101)
c.pred &lt;- predict(c.forest, X.test)

# Predict on out-of-bag training samples.
c.pred &lt;- predict(c.forest)

# Predict with confidence intervals; growing more trees is now recommended.
c.forest &lt;- causal_forest(X, Y, W, num.trees = 500)
c.pred &lt;- predict(c.forest, X.test, estimate.variance = TRUE)


</code></pre>

<hr>
<h2 id='predict.causal_survival_forest'>Predict with a causal survival forest forest</h2><span id='topic+predict.causal_survival_forest'></span>

<h3>Description</h3>

<p>Gets estimates of tau(X) using a trained causal survival forest.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'causal_survival_forest'
predict(
  object,
  newdata = NULL,
  num.threads = NULL,
  estimate.variance = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.causal_survival_forest_+3A_object">object</code></td>
<td>
<p>The trained forest.</p>
</td></tr>
<tr><td><code id="predict.causal_survival_forest_+3A_newdata">newdata</code></td>
<td>
<p>Points at which predictions should be made. If NULL, makes out-of-bag
predictions on the training set instead (i.e., provides predictions at
Xi using only trees that did not use the i-th training example). Note
that this matrix should have the number of columns as the training
matrix, and that the columns must appear in the same order.</p>
</td></tr>
<tr><td><code id="predict.causal_survival_forest_+3A_num.threads">num.threads</code></td>
<td>
<p>Number of threads used in prediction. If set to NULL, the software
automatically selects an appropriate amount.</p>
</td></tr>
<tr><td><code id="predict.causal_survival_forest_+3A_estimate.variance">estimate.variance</code></td>
<td>
<p>Whether variance estimates for <code class="reqn">\hat\tau(x)</code> are desired
(for confidence intervals).</p>
</td></tr>
<tr><td><code id="predict.causal_survival_forest_+3A_...">...</code></td>
<td>
<p>Additional arguments (currently ignored).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Vector of predictions along with optional variance estimates.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Train a causal survival forest targeting a Restricted Mean Survival Time (RMST)
# with maximum follow-up time set to `horizon`.
n &lt;- 2000
p &lt;- 5
X &lt;- matrix(runif(n * p), n, p)
W &lt;- rbinom(n, 1, 0.5)
horizon &lt;- 1
failure.time &lt;- pmin(rexp(n) * X[, 1] + W, horizon)
censor.time &lt;- 2 * runif(n)
Y &lt;- pmin(failure.time, censor.time)
D &lt;- as.integer(failure.time &lt;= censor.time)
# Save computation time by constraining the event grid by discretizing (rounding) continuous events.
cs.forest &lt;- causal_survival_forest(X, round(Y, 2), W, D, horizon = horizon)
# Or do so more flexibly by defining your own time grid using the failure.times argument.
# grid &lt;- seq(min(Y), max(Y), length.out = 150)
# cs.forest &lt;- causal_survival_forest(X, Y, W, D, horizon = horizon, failure.times = grid)

# Predict using the forest.
X.test &lt;- matrix(0.5, 10, p)
X.test[, 1] &lt;- seq(0, 1, length.out = 10)
cs.pred &lt;- predict(cs.forest, X.test)

# Predict on out-of-bag training samples.
cs.pred &lt;- predict(cs.forest)

# Predict with confidence intervals; growing more trees is now recommended.
c.pred &lt;- predict(cs.forest, X.test, estimate.variance = TRUE)

# Compute a doubly robust estimate of the average treatment effect.
average_treatment_effect(cs.forest)

# Compute the best linear projection on the first covariate.
best_linear_projection(cs.forest, X[, 1])

# See if a causal survival forest succeeded in capturing heterogeneity by plotting
# the TOC and calculating a 95% CI for the AUTOC.
train &lt;- sample(1:n, n / 2)
eval &lt;- -train
train.forest &lt;- causal_survival_forest(X[train, ], Y[train], W[train], D[train], horizon = horizon)
eval.forest &lt;- causal_survival_forest(X[eval, ], Y[eval], W[eval], D[eval], horizon = horizon)
rate &lt;- rank_average_treatment_effect(eval.forest,
                                      predict(train.forest, X[eval, ])$predictions)
plot(rate)
paste("AUTOC:", round(rate$estimate, 2), "+/", round(1.96 * rate$std.err, 2))


</code></pre>

<hr>
<h2 id='predict.instrumental_forest'>Predict with an instrumental forest</h2><span id='topic+predict.instrumental_forest'></span>

<h3>Description</h3>

<p>Gets estimates of tau(x) using a trained instrumental forest.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'instrumental_forest'
predict(
  object,
  newdata = NULL,
  num.threads = NULL,
  estimate.variance = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.instrumental_forest_+3A_object">object</code></td>
<td>
<p>The trained forest.</p>
</td></tr>
<tr><td><code id="predict.instrumental_forest_+3A_newdata">newdata</code></td>
<td>
<p>Points at which predictions should be made. If NULL, makes out-of-bag
predictions on the training set instead (i.e., provides predictions at
Xi using only trees that did not use the i-th training example). Note
that this matrix should have the number of columns as the training
matrix, and that the columns must appear in the same order.</p>
</td></tr>
<tr><td><code id="predict.instrumental_forest_+3A_num.threads">num.threads</code></td>
<td>
<p>Number of threads used in prediction. If set to NULL, the software
automatically selects an appropriate amount.</p>
</td></tr>
<tr><td><code id="predict.instrumental_forest_+3A_estimate.variance">estimate.variance</code></td>
<td>
<p>Whether variance estimates for <code class="reqn">\hat\tau(x)</code> are desired
(for confidence intervals).</p>
</td></tr>
<tr><td><code id="predict.instrumental_forest_+3A_...">...</code></td>
<td>
<p>Additional arguments (currently ignored).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Vector of predictions, along with (optional) variance estimates.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Train an instrumental forest.
n &lt;- 2000
p &lt;- 5
X &lt;- matrix(rbinom(n * p, 1, 0.5), n, p)
Z &lt;- rbinom(n, 1, 0.5)
Q &lt;- rbinom(n, 1, 0.5)
W &lt;- Q * Z
tau &lt;-  X[, 1] / 2
Y &lt;- rowSums(X[, 1:3]) + tau * W + Q + rnorm(n)
iv.forest &lt;- instrumental_forest(X, Y, W, Z)

# Predict on out-of-bag training samples.
iv.pred &lt;- predict(iv.forest)

# Estimate a (local) average treatment effect.
average_treatment_effect(iv.forest)


</code></pre>

<hr>
<h2 id='predict.ll_regression_forest'>Predict with a local linear forest</h2><span id='topic+predict.ll_regression_forest'></span>

<h3>Description</h3>

<p>Gets estimates of E[Y|X=x] using a trained regression forest.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'll_regression_forest'
predict(
  object,
  newdata = NULL,
  linear.correction.variables = NULL,
  ll.lambda = NULL,
  ll.weight.penalty = FALSE,
  num.threads = NULL,
  estimate.variance = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.ll_regression_forest_+3A_object">object</code></td>
<td>
<p>The trained forest.</p>
</td></tr>
<tr><td><code id="predict.ll_regression_forest_+3A_newdata">newdata</code></td>
<td>
<p>Points at which predictions should be made. If NULL, makes out-of-bag
predictions on the training set instead (i.e., provides predictions at
Xi using only trees that did not use the i-th training example). Note
that this matrix should have the number of columns as the training
matrix, and that the columns must appear in the same order.</p>
</td></tr>
<tr><td><code id="predict.ll_regression_forest_+3A_linear.correction.variables">linear.correction.variables</code></td>
<td>
<p>Optional subset of indexes for variables to be used in local
linear prediction. If left NULL, all variables are used.
We run a locally weighted linear regression on the included variables.
Please note that this is a beta feature still in development, and may slow down
prediction considerably. Defaults to NULL.</p>
</td></tr>
<tr><td><code id="predict.ll_regression_forest_+3A_ll.lambda">ll.lambda</code></td>
<td>
<p>Ridge penalty for local linear predictions. Defaults to NULL and will be cross-validated.</p>
</td></tr>
<tr><td><code id="predict.ll_regression_forest_+3A_ll.weight.penalty">ll.weight.penalty</code></td>
<td>
<p>Option to standardize ridge penalty by covariance (TRUE),
or penalize all covariates equally (FALSE). Defaults to FALSE.</p>
</td></tr>
<tr><td><code id="predict.ll_regression_forest_+3A_num.threads">num.threads</code></td>
<td>
<p>Number of threads used in prediction. If set to NULL, the software
automatically selects an appropriate amount.</p>
</td></tr>
<tr><td><code id="predict.ll_regression_forest_+3A_estimate.variance">estimate.variance</code></td>
<td>
<p>Whether variance estimates for <code class="reqn">\hat\tau(x)</code> are desired
(for confidence intervals).</p>
</td></tr>
<tr><td><code id="predict.ll_regression_forest_+3A_...">...</code></td>
<td>
<p>Additional arguments (currently ignored).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A vector of predictions.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Train the forest.
n &lt;- 50
p &lt;- 5
X &lt;- matrix(rnorm(n * p), n, p)
Y &lt;- X[, 1] * rnorm(n)
forest &lt;- ll_regression_forest(X, Y)

# Predict using the forest.
X.test &lt;- matrix(0, 101, p)
X.test[, 1] &lt;- seq(-2, 2, length.out = 101)
predictions &lt;- predict(forest, X.test)

# Predict on out-of-bag training samples.
predictions.oob &lt;- predict(forest)


</code></pre>

<hr>
<h2 id='predict.lm_forest'>Predict with a lm forest</h2><span id='topic+predict.lm_forest'></span>

<h3>Description</h3>

<p>Gets estimates of <code class="reqn">h_k(x)</code>, k = 1..K in the conditionally linear model
<code class="reqn">Y = c(x) + h_1(x)W_1 + ... + h_K(x)W_K</code>, for a target sample X = x.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'lm_forest'
predict(
  object,
  newdata = NULL,
  num.threads = NULL,
  estimate.variance = FALSE,
  drop = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.lm_forest_+3A_object">object</code></td>
<td>
<p>The trained forest.</p>
</td></tr>
<tr><td><code id="predict.lm_forest_+3A_newdata">newdata</code></td>
<td>
<p>Points at which predictions should be made. If NULL, makes out-of-bag
predictions on the training set instead (i.e., provides predictions at
Xi using only trees that did not use the i-th training example). Note
that this matrix should have the number of columns as the training
matrix, and that the columns must appear in the same order.</p>
</td></tr>
<tr><td><code id="predict.lm_forest_+3A_num.threads">num.threads</code></td>
<td>
<p>Number of threads used in prediction. If set to NULL, the software
automatically selects an appropriate amount.</p>
</td></tr>
<tr><td><code id="predict.lm_forest_+3A_estimate.variance">estimate.variance</code></td>
<td>
<p>Whether variance estimates for <code class="reqn">\hat h_k(x)</code> are desired
(for confidence intervals). This option is currently
only supported for univariate outcomes Y.</p>
</td></tr>
<tr><td><code id="predict.lm_forest_+3A_drop">drop</code></td>
<td>
<p>If TRUE, coerce the prediction result to the lowest possible dimension. Default is FALSE.</p>
</td></tr>
<tr><td><code id="predict.lm_forest_+3A_...">...</code></td>
<td>
<p>Additional arguments (currently ignored).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with elements 'predictions': a 3d array of dimension [num.samples, K, M] with
predictions for regressor W, for each outcome 1,..,M (singleton dimensions in this array can
be dropped by passing the 'drop' argument to '[', or with the shorthand '$predictions[,,]'),
and optionally 'variance.estimates': a matrix with K columns with variance estimates.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
if (require("rdd", quietly = TRUE)) {
# Train a LM Forest to estimate CATEs in a regression discontinuity design.
# Simulate a simple example with a heterogeneous jump in the CEF.
n &lt;- 2000
p &lt;- 5
X &lt;- matrix(rnorm(n * p), n, p)
Z &lt;- runif(n, -4, 4)
cutoff &lt;- 0
W &lt;- as.numeric(Z &gt;= cutoff)
tau &lt;- pmax(0.5 * X[, 1], 0)
Y &lt;- tau * W  + 1 / (1 + exp(2 * Z)) + 0.2 * rnorm(n)

# Compute the Imbens-Kalyanaraman MSE-optimal bandwidth for a local linear regression.
bandwidth &lt;- IKbandwidth(Z, Y, cutoff)
# Compute kernel weights for a triangular kernel.
sample.weights &lt;- kernelwts(Z, cutoff, bandwidth, "triangular")

# Alternatively, specify bandwith and triangular kernel weights without using the `rdd` package.
# bandwidth &lt;- # user can hand-specify this.
# dist &lt;- abs((Z - cutoff) / bandwidth)
# sample.weights &lt;- (1 - dist) * (dist &lt;= 1) / bandwidth

# Estimate a local linear regression with the running variable Z conditional on covariates X = x:
# Y = c(x) + tau(x) W + b(x) Z.
# Specify gradient.weights = c(1, 0) to target heterogeneity in the RDD coefficient tau(x).
# Also, fit forest on subset with non-zero weights for faster estimation.
subset &lt;- sample.weights &gt; 0
lmf &lt;- lm_forest(X[subset, ], Y[subset], cbind(W, Z)[subset, ],
                 sample.weights = sample.weights[subset], gradient.weights = c(1, 0))
tau.hat &lt;- predict(lmf)$predictions[, 1, ]

# Plot estimated tau(x) vs simulated ground truth.
plot(X[subset, 1], tau.hat)
points(X[subset, 1], tau[subset], col = "red", cex = 0.1)
}


</code></pre>

<hr>
<h2 id='predict.multi_arm_causal_forest'>Predict with a multi arm causal forest</h2><span id='topic+predict.multi_arm_causal_forest'></span>

<h3>Description</h3>

<p>Gets estimates of contrasts tau_k(x) using a trained multi arm causal forest (k = 1,...,K-1
where K is the number of treatments).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'multi_arm_causal_forest'
predict(
  object,
  newdata = NULL,
  num.threads = NULL,
  estimate.variance = FALSE,
  drop = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.multi_arm_causal_forest_+3A_object">object</code></td>
<td>
<p>The trained forest.</p>
</td></tr>
<tr><td><code id="predict.multi_arm_causal_forest_+3A_newdata">newdata</code></td>
<td>
<p>Points at which predictions should be made. If NULL, makes out-of-bag
predictions on the training set instead (i.e., provides predictions at
Xi using only trees that did not use the i-th training example). Note
that this matrix should have the number of columns as the training
matrix, and that the columns must appear in the same order.</p>
</td></tr>
<tr><td><code id="predict.multi_arm_causal_forest_+3A_num.threads">num.threads</code></td>
<td>
<p>Number of threads used in prediction. If set to NULL, the software
automatically selects an appropriate amount.</p>
</td></tr>
<tr><td><code id="predict.multi_arm_causal_forest_+3A_estimate.variance">estimate.variance</code></td>
<td>
<p>Whether variance estimates for <code class="reqn">\hat\tau(x)</code> are desired
(for confidence intervals). This option is currently
only supported for univariate outcomes Y.</p>
</td></tr>
<tr><td><code id="predict.multi_arm_causal_forest_+3A_drop">drop</code></td>
<td>
<p>If TRUE, coerce the prediction result to the lowest possible dimension. Default is FALSE.</p>
</td></tr>
<tr><td><code id="predict.multi_arm_causal_forest_+3A_...">...</code></td>
<td>
<p>Additional arguments (currently ignored).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with elements 'predictions': a 3d array of dimension [num.samples, K-1, M] with
predictions for each contrast, for each outcome 1,..,M (singleton dimensions in this array can
be dropped by passing the 'drop' argument to '[', or with the shorthand '$predictions[,,]'),
and optionally 'variance.estimates': a matrix with K-1 columns with variance estimates for each contrast.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Train a multi arm causal forest.
n &lt;- 500
p &lt;- 10
X &lt;- matrix(rnorm(n * p), n, p)
W &lt;- as.factor(sample(c("A", "B", "C"), n, replace = TRUE))
Y &lt;- X[, 1] + X[, 2] * (W == "B") - 1.5 * X[, 2] * (W == "C") + rnorm(n)
mc.forest &lt;- multi_arm_causal_forest(X, Y, W)

# Predict contrasts (out-of-bag) using the forest.
# Fitting several outcomes jointly is supported, and the returned prediction array has
# dimension [num.samples, num.contrasts, num.outcomes]. Since num.outcomes is one in
# this example, we use drop = TRUE to ignore this singleton dimension.
mc.pred &lt;- predict(mc.forest, drop = TRUE)

# By default, the first ordinal treatment is used as baseline ("A" in this example),
# giving two contrasts tau_B = Y(B) - Y(A), tau_C = Y(C) - Y(A)
tau.hat &lt;- mc.pred$predictions

plot(X[, 2], tau.hat[, "B - A"], ylab = "tau.contrast")
abline(0, 1, col = "red")
points(X[, 2], tau.hat[, "C - A"], col = "blue")
abline(0, -1.5, col = "red")
legend("topleft", c("B - A", "C - A"), col = c("black", "blue"), pch = 19)

# The average treatment effect of the arms with "A" as baseline.
average_treatment_effect(mc.forest)

# The conditional response surfaces mu_k(X) for a single outcome can be reconstructed from
# the contrasts tau_k(x), the treatment propensities e_k(x), and the conditional mean m(x).
# Given treatment "A" as baseline we have:
# m(x) := E[Y | X] = E[Y(A) | X] + E[W_B (Y(B) - Y(A))] + E[W_C (Y(C) - Y(A))]
# which given unconfoundedness is equal to:
# m(x) = mu(A, x) + e_B(x) tau_B(X) + e_C(x) tau_C(x)
# Rearranging and plugging in the above expressions, we obtain the following estimates
# * mu(A, x) = m(x) - e_B(x) tau_B(x) - e_C(x) tau_C(x)
# * mu(B, x) = m(x) + (1 - e_B(x)) tau_B(x) - e_C(x) tau_C(x)
# * mu(C, x) = m(x) - e_B(x) tau_B(x) + (1 - e_C(x)) tau_C(x)
Y.hat &lt;- mc.forest$Y.hat
W.hat &lt;- mc.forest$W.hat

muA &lt;- Y.hat - W.hat[, "B"] * tau.hat[, "B - A"] - W.hat[, "C"] * tau.hat[, "C - A"]
muB &lt;- Y.hat + (1 - W.hat[, "B"]) * tau.hat[, "B - A"] - W.hat[, "C"] * tau.hat[, "C - A"]
muC &lt;- Y.hat - W.hat[, "B"] * tau.hat[, "B - A"] + (1 - W.hat[, "C"]) * tau.hat[, "C - A"]

# These can also be obtained with some array manipulations.
# (the first column is always the baseline arm)
Y.hat.baseline &lt;- Y.hat - rowSums(W.hat[, -1, drop = FALSE] * tau.hat)
mu.hat.matrix &lt;- cbind(Y.hat.baseline, c(Y.hat.baseline) + tau.hat)
colnames(mu.hat.matrix) &lt;- levels(W)
head(mu.hat.matrix)

# The reference level for contrast prediction can be changed with `relevel`.
# Fit and predict with treatment B as baseline:
W &lt;- relevel(W, ref = "B")
mc.forest.B &lt;- multi_arm_causal_forest(X, Y, W)


</code></pre>

<hr>
<h2 id='predict.multi_regression_forest'>Predict with a multi regression forest</h2><span id='topic+predict.multi_regression_forest'></span>

<h3>Description</h3>

<p>Gets estimates of E[Y_i | X = x] using a trained multi regression forest.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'multi_regression_forest'
predict(object, newdata = NULL, num.threads = NULL, drop = FALSE, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.multi_regression_forest_+3A_object">object</code></td>
<td>
<p>The trained forest.</p>
</td></tr>
<tr><td><code id="predict.multi_regression_forest_+3A_newdata">newdata</code></td>
<td>
<p>Points at which predictions should be made. If NULL, makes out-of-bag
predictions on the training set instead (i.e., provides predictions at
Xi using only trees that did not use the i-th training example). Note
that this matrix should have the number of columns as the training
matrix, and that the columns must appear in the same order.</p>
</td></tr>
<tr><td><code id="predict.multi_regression_forest_+3A_num.threads">num.threads</code></td>
<td>
<p>Number of threads used in prediction. If set to NULL, the software
automatically selects an appropriate amount.</p>
</td></tr>
<tr><td><code id="predict.multi_regression_forest_+3A_drop">drop</code></td>
<td>
<p>If TRUE, coerce the prediction result to the lowest possible dimension. Default is FALSE.</p>
</td></tr>
<tr><td><code id="predict.multi_regression_forest_+3A_...">...</code></td>
<td>
<p>Additional arguments (currently ignored).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list containing 'predictions': a matrix of predictions for each outcome.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Train a standard regression forest.
n &lt;- 500
p &lt;- 5
X &lt;- matrix(rnorm(n * p), n, p)
Y &lt;-  X[, 1, drop = FALSE] %*% cbind(1, 2) + rnorm(n)
mr.forest &lt;- multi_regression_forest(X, Y)

# Predict using the forest.
X.test &lt;- matrix(0, 101, p)
X.test[, 1] &lt;- seq(-2, 2, length.out = 101)
mr.pred &lt;- predict(mr.forest, X.test)

# Predict on out-of-bag training samples.
mr.pred &lt;- predict(mr.forest)


</code></pre>

<hr>
<h2 id='predict.probability_forest'>Predict with a probability forest</h2><span id='topic+predict.probability_forest'></span>

<h3>Description</h3>

<p>Gets estimates of P[Y = k | X = x] using a trained forest.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'probability_forest'
predict(
  object,
  newdata = NULL,
  num.threads = NULL,
  estimate.variance = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.probability_forest_+3A_object">object</code></td>
<td>
<p>The trained forest.</p>
</td></tr>
<tr><td><code id="predict.probability_forest_+3A_newdata">newdata</code></td>
<td>
<p>Points at which predictions should be made. If NULL, makes out-of-bag
predictions on the training set instead (i.e., provides predictions at
Xi using only trees that did not use the i-th training example). Note
that this matrix should have the number of columns as the training
matrix, and that the columns must appear in the same order.</p>
</td></tr>
<tr><td><code id="predict.probability_forest_+3A_num.threads">num.threads</code></td>
<td>
<p>Number of threads used in prediction. If set to NULL, the software
automatically selects an appropriate amount.</p>
</td></tr>
<tr><td><code id="predict.probability_forest_+3A_estimate.variance">estimate.variance</code></td>
<td>
<p>Whether variance estimates for P[Y = k | X] are desired (for confidence intervals).</p>
</td></tr>
<tr><td><code id="predict.probability_forest_+3A_...">...</code></td>
<td>
<p>Additional arguments (currently ignored).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with attributes 'predictions': a matrix of predictions for each class, and optionally
the attribute 'variance.estimates': a matrix of variance estimates for each class.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Train a probability forest.
p &lt;- 5
n &lt;- 2000
X &lt;- matrix(rnorm(n*p), n, p)
prob &lt;- 1 / (1 + exp(-X[, 1] - X[, 2]))
Y &lt;- as.factor(rbinom(n, 1, prob))
p.forest &lt;- probability_forest(X, Y)

# Predict using the forest.
X.test &lt;- matrix(0, 10, p)
X.test[, 1] &lt;- seq(-1.5, 1.5, length.out = 10)
p.hat &lt;- predict(p.forest, X.test, estimate.variance = TRUE)

# Plot the estimated success probabilities with 95 % confidence bands.
prob.test &lt;- 1 / (1 + exp(-X.test[, 1] - X.test[, 2]))
p.true &lt;- cbind(`0` = 1 - prob.test, `1` = prob.test)
plot(X.test[, 1], p.true[, "1"], col = 'red', ylim = c(0, 1))
points(X.test[, 1], p.hat$predictions[, "1"], pch = 16)
lines(X.test[, 1], (p.hat$predictions + 2 * sqrt(p.hat$variance.estimates))[, "1"])
lines(X.test[, 1], (p.hat$predictions - 2 * sqrt(p.hat$variance.estimates))[, "1"])

# Predict on out-of-bag training samples.
p.hat &lt;- predict(p.forest)


</code></pre>

<hr>
<h2 id='predict.quantile_forest'>Predict with a quantile forest</h2><span id='topic+predict.quantile_forest'></span>

<h3>Description</h3>

<p>Gets estimates of the conditional quantiles of Y given X using a trained forest.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'quantile_forest'
predict(object, newdata = NULL, quantiles = NULL, num.threads = NULL, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.quantile_forest_+3A_object">object</code></td>
<td>
<p>The trained forest.</p>
</td></tr>
<tr><td><code id="predict.quantile_forest_+3A_newdata">newdata</code></td>
<td>
<p>Points at which predictions should be made. If NULL, makes out-of-bag
predictions on the training set instead (i.e., provides predictions at
Xi using only trees that did not use the i-th training example). Note
that this matrix should have the number of columns as the training
matrix, and that the columns must appear in the same order.</p>
</td></tr>
<tr><td><code id="predict.quantile_forest_+3A_quantiles">quantiles</code></td>
<td>
<p>Vector of quantiles at which estimates are required. If NULL, the quantiles
used to train the forest is used. Default is NULL.</p>
</td></tr>
<tr><td><code id="predict.quantile_forest_+3A_num.threads">num.threads</code></td>
<td>
<p>Number of threads used in prediction. If set to NULL, the software
automatically selects an appropriate amount.</p>
</td></tr>
<tr><td><code id="predict.quantile_forest_+3A_...">...</code></td>
<td>
<p>Additional arguments (currently ignored).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with elements 'predictions': a matrix with predictions at each test point for each desired quantile.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Train a quantile forest.
n &lt;- 50
p &lt;- 10
X &lt;- matrix(rnorm(n * p), n, p)
Y &lt;- X[, 1] * rnorm(n)
q.forest &lt;- quantile_forest(X, Y, quantiles = c(0.1, 0.5, 0.9))

# Predict on out-of-bag training samples.
q.pred &lt;- predict(q.forest)

# Predict using the forest.
X.test &lt;- matrix(0, 101, p)
X.test[, 1] &lt;- seq(-2, 2, length.out = 101)
q.pred &lt;- predict(q.forest, X.test)


</code></pre>

<hr>
<h2 id='predict.regression_forest'>Predict with a regression forest</h2><span id='topic+predict.regression_forest'></span>

<h3>Description</h3>

<p>Gets estimates of E[Y|X=x] using a trained regression forest.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'regression_forest'
predict(
  object,
  newdata = NULL,
  linear.correction.variables = NULL,
  ll.lambda = NULL,
  ll.weight.penalty = FALSE,
  num.threads = NULL,
  estimate.variance = FALSE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.regression_forest_+3A_object">object</code></td>
<td>
<p>The trained forest.</p>
</td></tr>
<tr><td><code id="predict.regression_forest_+3A_newdata">newdata</code></td>
<td>
<p>Points at which predictions should be made. If NULL, makes out-of-bag
predictions on the training set instead (i.e., provides predictions at
Xi using only trees that did not use the i-th training example). Note
that this matrix should have the number of columns as the training
matrix, and that the columns must appear in the same order.</p>
</td></tr>
<tr><td><code id="predict.regression_forest_+3A_linear.correction.variables">linear.correction.variables</code></td>
<td>
<p>Optional subset of indexes for variables to be used in local
linear prediction. If NULL, standard GRF prediction is used. Otherwise,
we run a locally weighted linear regression on the included variables.
Please note that this is a beta feature still in development, and may slow down
prediction considerably. Defaults to NULL.</p>
</td></tr>
<tr><td><code id="predict.regression_forest_+3A_ll.lambda">ll.lambda</code></td>
<td>
<p>Ridge penalty for local linear predictions. Defaults to NULL and will be cross-validated.</p>
</td></tr>
<tr><td><code id="predict.regression_forest_+3A_ll.weight.penalty">ll.weight.penalty</code></td>
<td>
<p>Option to standardize ridge penalty by covariance (TRUE),
or penalize all covariates equally (FALSE). Defaults to FALSE.</p>
</td></tr>
<tr><td><code id="predict.regression_forest_+3A_num.threads">num.threads</code></td>
<td>
<p>Number of threads used in prediction. If set to NULL, the software
automatically selects an appropriate amount.</p>
</td></tr>
<tr><td><code id="predict.regression_forest_+3A_estimate.variance">estimate.variance</code></td>
<td>
<p>Whether variance estimates for <code class="reqn">\hat\tau(x)</code> are desired
(for confidence intervals).</p>
</td></tr>
<tr><td><code id="predict.regression_forest_+3A_...">...</code></td>
<td>
<p>Additional arguments (currently ignored).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Vector of predictions, along with estimates of the error and
(optionally) its variance estimates. Column 'predictions' contains
estimates of E[Y|X=x]. The square-root of column 'variance.estimates' is the standard error
the test mean-squared error. Column 'excess.error' contains
jackknife estimates of the Monte-carlo error. The sum of 'debiased.error'
and 'excess.error' is the raw error attained by the current forest, and
'debiased.error' alone is an estimate of the error attained by a forest with
an infinite number of trees. We recommend that users grow
enough forests to make the 'excess.error' negligible.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Train a standard regression forest.
n &lt;- 50
p &lt;- 10
X &lt;- matrix(rnorm(n * p), n, p)
Y &lt;- X[, 1] * rnorm(n)
r.forest &lt;- regression_forest(X, Y)

# Predict using the forest.
X.test &lt;- matrix(0, 101, p)
X.test[, 1] &lt;- seq(-2, 2, length.out = 101)
r.pred &lt;- predict(r.forest, X.test)

# Predict on out-of-bag training samples.
r.pred &lt;- predict(r.forest)

# Predict with confidence intervals; growing more trees is now recommended.
r.forest &lt;- regression_forest(X, Y, num.trees = 100)
r.pred &lt;- predict(r.forest, X.test, estimate.variance = TRUE)


</code></pre>

<hr>
<h2 id='predict.survival_forest'>Predict with a survival forest</h2><span id='topic+predict.survival_forest'></span>

<h3>Description</h3>

<p>Gets estimates of the conditional survival function S(t, x) = P[T &gt; t | X = x] using a trained survival forest.
The curve can be estimated by Kaplan-Meier, or Nelson-Aalen.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'survival_forest'
predict(
  object,
  newdata = NULL,
  failure.times = NULL,
  prediction.times = c("curve", "time"),
  prediction.type = c("Kaplan-Meier", "Nelson-Aalen"),
  num.threads = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="predict.survival_forest_+3A_object">object</code></td>
<td>
<p>The trained forest.</p>
</td></tr>
<tr><td><code id="predict.survival_forest_+3A_newdata">newdata</code></td>
<td>
<p>Points at which predictions should be made. If NULL, makes out-of-bag
predictions on the training set instead (i.e., provides predictions at
Xi using only trees that did not use the i-th training example). Note
that this matrix should have the number of columns as the training
matrix, and that the columns must appear in the same order.</p>
</td></tr>
<tr><td><code id="predict.survival_forest_+3A_failure.times">failure.times</code></td>
<td>
<p>A vector of survival times to make predictions at. If NULL, then the
failure times used for training the forest is used. If prediction.times = &quot;curve&quot; then the
time points should be in increasing order. Default is NULL.</p>
</td></tr>
<tr><td><code id="predict.survival_forest_+3A_prediction.times">prediction.times</code></td>
<td>
<p>&quot;curve&quot; predicts the survival curve S(t, x) on grid t = failure.times for each sample Xi.
&quot;time&quot; predicts S(t, x) at an event time t = failure.times[i] for each sample Xi.
Default is &quot;curve&quot;.</p>
</td></tr>
<tr><td><code id="predict.survival_forest_+3A_prediction.type">prediction.type</code></td>
<td>
<p>The type of estimate of the survival function, choices are &quot;Kaplan-Meier&quot; or &quot;Nelson-Aalen&quot;.
The default is the prediction.type used to train the forest.</p>
</td></tr>
<tr><td><code id="predict.survival_forest_+3A_num.threads">num.threads</code></td>
<td>
<p>Number of threads used in prediction. If set to NULL, the software
automatically selects an appropriate amount.</p>
</td></tr>
<tr><td><code id="predict.survival_forest_+3A_...">...</code></td>
<td>
<p>Additional arguments (currently ignored).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with elements </p>

<ul>
<li><p> predictions: a matrix of survival curves. If prediction.times = &quot;curve&quot; then each row
is the survival curve for sample Xi: predictions[i, j] = S(failure.times[j], Xi).
If prediction.times = &quot;time&quot; then each row is the survival curve at time point failure.times[i]
for sample Xi: predictions[i, ] = S(failure.times[i], Xi).
</p>
</li>
<li><p> failure.times: a vector of event times t for the survival curve.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>
# Train a standard survival forest.
n &lt;- 2000
p &lt;- 5
X &lt;- matrix(rnorm(n * p), n, p)
failure.time &lt;- exp(0.5 * X[, 1]) * rexp(n)
censor.time &lt;- 2 * rexp(n)
Y &lt;- pmin(failure.time, censor.time)
D &lt;- as.integer(failure.time &lt;= censor.time)
# Save computation time by constraining the event grid by discretizing (rounding) continuous events.
s.forest &lt;- survival_forest(X, round(Y, 2), D)
# Or do so more flexibly by defining your own time grid using the failure.times argument.
# grid &lt;- seq(min(Y[D==1]), max(Y[D==1]), length.out = 150)
# s.forest &lt;- survival_forest(X, Y, D, failure.times = grid)

# Predict using the forest.
X.test &lt;- matrix(0, 3, p)
X.test[, 1] &lt;- seq(-2, 2, length.out = 3)
s.pred &lt;- predict(s.forest, X.test)

# Plot the survival curve.
plot(NA, NA, xlab = "failure time", ylab = "survival function",
     xlim = range(s.pred$failure.times),
     ylim = c(0, 1))
for(i in 1:3) {
  lines(s.pred$failure.times, s.pred$predictions[i,], col = i)
  s.true = exp(-s.pred$failure.times / exp(0.5 * X.test[i, 1]))
  lines(s.pred$failure.times, s.true, col = i, lty = 2)
}

# Predict on out-of-bag training samples.
s.pred &lt;- predict(s.forest)

# Compute OOB concordance based on the mortality score in Ishwaran et al. (2008).
s.pred.nelson.aalen &lt;- predict(s.forest, prediction.type = "Nelson-Aalen")
chf.score &lt;- rowSums(-log(s.pred.nelson.aalen$predictions))
if (require("survival", quietly = TRUE)) {
 concordance(Surv(Y, D) ~ chf.score, reverse = TRUE)
}


</code></pre>

<hr>
<h2 id='print.boosted_regression_forest'>Print a boosted regression forest</h2><span id='topic+print.boosted_regression_forest'></span>

<h3>Description</h3>

<p>Print a boosted regression forest
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'boosted_regression_forest'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print.boosted_regression_forest_+3A_x">x</code></td>
<td>
<p>The boosted forest to print.</p>
</td></tr>
<tr><td><code id="print.boosted_regression_forest_+3A_...">...</code></td>
<td>
<p>Additional arguments (currently ignored).</p>
</td></tr>
</table>

<hr>
<h2 id='print.grf'>Print a GRF forest object.</h2><span id='topic+print.grf'></span>

<h3>Description</h3>

<p>Print a GRF forest object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'grf'
print(x, decay.exponent = 2, max.depth = 4, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print.grf_+3A_x">x</code></td>
<td>
<p>The tree to print.</p>
</td></tr>
<tr><td><code id="print.grf_+3A_decay.exponent">decay.exponent</code></td>
<td>
<p>A tuning parameter that controls the importance of split depth.</p>
</td></tr>
<tr><td><code id="print.grf_+3A_max.depth">max.depth</code></td>
<td>
<p>The maximum depth of splits to consider.</p>
</td></tr>
<tr><td><code id="print.grf_+3A_...">...</code></td>
<td>
<p>Additional arguments (currently ignored).</p>
</td></tr>
</table>

<hr>
<h2 id='print.grf_tree'>Print a GRF tree object.</h2><span id='topic+print.grf_tree'></span>

<h3>Description</h3>

<p>Print a GRF tree object.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'grf_tree'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print.grf_tree_+3A_x">x</code></td>
<td>
<p>The tree to print.</p>
</td></tr>
<tr><td><code id="print.grf_tree_+3A_...">...</code></td>
<td>
<p>Additional arguments (currently ignored).</p>
</td></tr>
</table>

<hr>
<h2 id='print.rank_average_treatment_effect'>Print the Rank-Weighted Average Treatment Effect (RATE).</h2><span id='topic+print.rank_average_treatment_effect'></span>

<h3>Description</h3>

<p>Print the Rank-Weighted Average Treatment Effect (RATE).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'rank_average_treatment_effect'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print.rank_average_treatment_effect_+3A_x">x</code></td>
<td>
<p>The output of rank_average_treatment_effect.</p>
</td></tr>
<tr><td><code id="print.rank_average_treatment_effect_+3A_...">...</code></td>
<td>
<p>Additional arguments (currently ignored).</p>
</td></tr>
</table>

<hr>
<h2 id='print.tuning_output'>Print tuning output.
Displays average error for q-quantiles of tuned parameters.</h2><span id='topic+print.tuning_output'></span>

<h3>Description</h3>

<p>Print tuning output.
Displays average error for q-quantiles of tuned parameters.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>## S3 method for class 'tuning_output'
print(x, tuning.quantiles = seq(0, 1, 0.2), ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="print.tuning_output_+3A_x">x</code></td>
<td>
<p>The tuning output to print.</p>
</td></tr>
<tr><td><code id="print.tuning_output_+3A_tuning.quantiles">tuning.quantiles</code></td>
<td>
<p>vector of quantiles to display average error over.
Default: seq(0, 1, 0.2) (quintiles)</p>
</td></tr>
<tr><td><code id="print.tuning_output_+3A_...">...</code></td>
<td>
<p>Additional arguments (currently ignored).</p>
</td></tr>
</table>

<hr>
<h2 id='probability_forest'>Probability forest</h2><span id='topic+probability_forest'></span>

<h3>Description</h3>

<p>Trains a probability forest that can be used to estimate
the conditional class probabilities P[Y = k | X = x]
</p>


<h3>Usage</h3>

<pre><code class='language-R'>probability_forest(
  X,
  Y,
  num.trees = 2000,
  sample.weights = NULL,
  clusters = NULL,
  equalize.cluster.weights = FALSE,
  sample.fraction = 0.5,
  mtry = min(ceiling(sqrt(ncol(X)) + 20), ncol(X)),
  min.node.size = 5,
  honesty = TRUE,
  honesty.fraction = 0.5,
  honesty.prune.leaves = TRUE,
  alpha = 0.05,
  imbalance.penalty = 0,
  ci.group.size = 2,
  compute.oob.predictions = TRUE,
  num.threads = NULL,
  seed = runif(1, 0, .Machine$integer.max)
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="probability_forest_+3A_x">X</code></td>
<td>
<p>The covariates.</p>
</td></tr>
<tr><td><code id="probability_forest_+3A_y">Y</code></td>
<td>
<p>The class label (must be a factor vector with no NAs).</p>
</td></tr>
<tr><td><code id="probability_forest_+3A_num.trees">num.trees</code></td>
<td>
<p>Number of trees grown in the forest. Note: Getting accurate
confidence intervals generally requires more trees than
getting accurate predictions. Default is 2000.</p>
</td></tr>
<tr><td><code id="probability_forest_+3A_sample.weights">sample.weights</code></td>
<td>
<p>Weights given to an observation in estimation.
If NULL, each observation is given the same weight. Default is NULL.</p>
</td></tr>
<tr><td><code id="probability_forest_+3A_clusters">clusters</code></td>
<td>
<p>Vector of integers or factors specifying which cluster each observation corresponds to.
Default is NULL (ignored).</p>
</td></tr>
<tr><td><code id="probability_forest_+3A_equalize.cluster.weights">equalize.cluster.weights</code></td>
<td>
<p>If FALSE, each unit is given the same weight (so that bigger
clusters get more weight). If TRUE, each cluster is given equal weight in the forest. In this case,
during training, each tree uses the same number of observations from each drawn cluster: If the
smallest cluster has K units, then when we sample a cluster during training, we only give a random
K elements of the cluster to the tree-growing procedure. When estimating average treatment effects,
each observation is given weight 1/cluster size, so that the total weight of each cluster is the
same.</p>
</td></tr>
<tr><td><code id="probability_forest_+3A_sample.fraction">sample.fraction</code></td>
<td>
<p>Fraction of the data used to build each tree.
Note: If honesty = TRUE, these subsamples will
further be cut by a factor of honesty.fraction. Default is 0.5.</p>
</td></tr>
<tr><td><code id="probability_forest_+3A_mtry">mtry</code></td>
<td>
<p>Number of variables tried for each split. Default is
<code class="reqn">\sqrt p + 20</code> where p is the number of variables.</p>
</td></tr>
<tr><td><code id="probability_forest_+3A_min.node.size">min.node.size</code></td>
<td>
<p>A target for the minimum number of observations in each tree leaf. Note that nodes
with size smaller than min.node.size can occur, as in the original randomForest package.
Default is 5.</p>
</td></tr>
<tr><td><code id="probability_forest_+3A_honesty">honesty</code></td>
<td>
<p>Whether to use honest splitting (i.e., sub-sample splitting). Default is TRUE.
For a detailed description of honesty, honesty.fraction, honesty.prune.leaves, and recommendations for
parameter tuning, see the grf algorithm reference.</p>
</td></tr>
<tr><td><code id="probability_forest_+3A_honesty.fraction">honesty.fraction</code></td>
<td>
<p>The fraction of data that will be used for determining splits if honesty = TRUE. Corresponds
to set J1 in the notation of the paper. Default is 0.5 (i.e. half of the data is used for
determining splits).</p>
</td></tr>
<tr><td><code id="probability_forest_+3A_honesty.prune.leaves">honesty.prune.leaves</code></td>
<td>
<p>If TRUE, prunes the estimation sample tree such that no leaves
are empty. If FALSE, keep the same tree as determined in the splits sample (if an empty leave is encountered, that
tree is skipped and does not contribute to the estimate). Setting this to FALSE may improve performance on
small/marginally powered data, but requires more trees (note: tuning does not adjust the number of trees).
Only applies if honesty is enabled. Default is TRUE.</p>
</td></tr>
<tr><td><code id="probability_forest_+3A_alpha">alpha</code></td>
<td>
<p>A tuning parameter that controls the maximum imbalance of a split. Default is 0.05.</p>
</td></tr>
<tr><td><code id="probability_forest_+3A_imbalance.penalty">imbalance.penalty</code></td>
<td>
<p>A tuning parameter that controls how harshly imbalanced splits are penalized. Default is 0.</p>
</td></tr>
<tr><td><code id="probability_forest_+3A_ci.group.size">ci.group.size</code></td>
<td>
<p>The forest will grow ci.group.size trees on each subsample.
In order to provide confidence intervals, ci.group.size must
be at least 2. Default is 2.</p>
</td></tr>
<tr><td><code id="probability_forest_+3A_compute.oob.predictions">compute.oob.predictions</code></td>
<td>
<p>Whether OOB predictions on training set should be precomputed. Default is TRUE.</p>
</td></tr>
<tr><td><code id="probability_forest_+3A_num.threads">num.threads</code></td>
<td>
<p>Number of threads used in training. By default, the number of threads is set
to the maximum hardware concurrency.</p>
</td></tr>
<tr><td><code id="probability_forest_+3A_seed">seed</code></td>
<td>
<p>The seed of the C++ random number generator.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A trained probability forest object.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Train a probability forest.
p &lt;- 5
n &lt;- 2000
X &lt;- matrix(rnorm(n*p), n, p)
prob &lt;- 1 / (1 + exp(-X[, 1] - X[, 2]))
Y &lt;- as.factor(rbinom(n, 1, prob))
p.forest &lt;- probability_forest(X, Y)

# Predict using the forest.
X.test &lt;- matrix(0, 10, p)
X.test[, 1] &lt;- seq(-1.5, 1.5, length.out = 10)
p.hat &lt;- predict(p.forest, X.test, estimate.variance = TRUE)

# Plot the estimated success probabilities with 95 % confidence bands.
prob.test &lt;- 1 / (1 + exp(-X.test[, 1] - X.test[, 2]))
p.true &lt;- cbind(`0` = 1 - prob.test, `1` = prob.test)
plot(X.test[, 1], p.true[, "1"], col = 'red', ylim = c(0, 1))
points(X.test[, 1], p.hat$predictions[, "1"], pch = 16)
lines(X.test[, 1], (p.hat$predictions + 2 * sqrt(p.hat$variance.estimates))[, "1"])
lines(X.test[, 1], (p.hat$predictions - 2 * sqrt(p.hat$variance.estimates))[, "1"])

# Predict on out-of-bag training samples.
p.hat &lt;- predict(p.forest)


</code></pre>

<hr>
<h2 id='quantile_forest'>Quantile forest</h2><span id='topic+quantile_forest'></span>

<h3>Description</h3>

<p>Trains a regression forest that can be used to estimate
quantiles of the conditional distribution of Y given X = x.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>quantile_forest(
  X,
  Y,
  num.trees = 2000,
  quantiles = c(0.1, 0.5, 0.9),
  regression.splitting = FALSE,
  clusters = NULL,
  equalize.cluster.weights = FALSE,
  sample.fraction = 0.5,
  mtry = min(ceiling(sqrt(ncol(X)) + 20), ncol(X)),
  min.node.size = 5,
  honesty = TRUE,
  honesty.fraction = 0.5,
  honesty.prune.leaves = TRUE,
  alpha = 0.05,
  imbalance.penalty = 0,
  compute.oob.predictions = FALSE,
  num.threads = NULL,
  seed = runif(1, 0, .Machine$integer.max)
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="quantile_forest_+3A_x">X</code></td>
<td>
<p>The covariates used in the quantile regression.</p>
</td></tr>
<tr><td><code id="quantile_forest_+3A_y">Y</code></td>
<td>
<p>The outcome.</p>
</td></tr>
<tr><td><code id="quantile_forest_+3A_num.trees">num.trees</code></td>
<td>
<p>Number of trees grown in the forest. Note: Getting accurate
confidence intervals generally requires more trees than
getting accurate predictions. Default is 2000.</p>
</td></tr>
<tr><td><code id="quantile_forest_+3A_quantiles">quantiles</code></td>
<td>
<p>Vector of quantiles used to calibrate the forest. Default is (0.1, 0.5, 0.9).</p>
</td></tr>
<tr><td><code id="quantile_forest_+3A_regression.splitting">regression.splitting</code></td>
<td>
<p>Whether to use regression splits when growing trees instead
of specialized splits based on the quantiles (the default).
Setting this flag to true corresponds to the approach to
quantile forests from Meinshausen (2006). Default is FALSE.</p>
</td></tr>
<tr><td><code id="quantile_forest_+3A_clusters">clusters</code></td>
<td>
<p>Vector of integers or factors specifying which cluster each observation corresponds to.
Default is NULL (ignored).</p>
</td></tr>
<tr><td><code id="quantile_forest_+3A_equalize.cluster.weights">equalize.cluster.weights</code></td>
<td>
<p>If FALSE, each unit is given the same weight (so that bigger
clusters get more weight). If TRUE, each cluster is given equal weight in the forest. In this case,
during training, each tree uses the same number of observations from each drawn cluster: If the
smallest cluster has K units, then when we sample a cluster during training, we only give a random
K elements of the cluster to the tree-growing procedure. When estimating average treatment effects,
each observation is given weight 1/cluster size, so that the total weight of each cluster is the
same.</p>
</td></tr>
<tr><td><code id="quantile_forest_+3A_sample.fraction">sample.fraction</code></td>
<td>
<p>Fraction of the data used to build each tree.
Note: If honesty = TRUE, these subsamples will
further be cut by a factor of honesty.fraction. Default is 0.5.</p>
</td></tr>
<tr><td><code id="quantile_forest_+3A_mtry">mtry</code></td>
<td>
<p>Number of variables tried for each split. Default is
<code class="reqn">\sqrt p + 20</code> where p is the number of variables.</p>
</td></tr>
<tr><td><code id="quantile_forest_+3A_min.node.size">min.node.size</code></td>
<td>
<p>A target for the minimum number of observations in each tree leaf. Note that nodes
with size smaller than min.node.size can occur, as in the original randomForest package.
Default is 5.</p>
</td></tr>
<tr><td><code id="quantile_forest_+3A_honesty">honesty</code></td>
<td>
<p>Whether to use honest splitting (i.e., sub-sample splitting). Default is TRUE.
For a detailed description of honesty, honesty.fraction, honesty.prune.leaves, and recommendations for
parameter tuning, see the grf algorithm reference.</p>
</td></tr>
<tr><td><code id="quantile_forest_+3A_honesty.fraction">honesty.fraction</code></td>
<td>
<p>The fraction of data that will be used for determining splits if honesty = TRUE. Corresponds
to set J1 in the notation of the paper. Default is 0.5 (i.e. half of the data is used for
determining splits).</p>
</td></tr>
<tr><td><code id="quantile_forest_+3A_honesty.prune.leaves">honesty.prune.leaves</code></td>
<td>
<p>If TRUE, prunes the estimation sample tree such that no leaves
are empty. If FALSE, keep the same tree as determined in the splits sample (if an empty leave is encountered, that
tree is skipped and does not contribute to the estimate). Setting this to FALSE may improve performance on
small/marginally powered data, but requires more trees (note: tuning does not adjust the number of trees).
Only applies if honesty is enabled. Default is TRUE.</p>
</td></tr>
<tr><td><code id="quantile_forest_+3A_alpha">alpha</code></td>
<td>
<p>A tuning parameter that controls the maximum imbalance of a split. Default is 0.05.</p>
</td></tr>
<tr><td><code id="quantile_forest_+3A_imbalance.penalty">imbalance.penalty</code></td>
<td>
<p>A tuning parameter that controls how harshly imbalanced splits are penalized. Default is 0.</p>
</td></tr>
<tr><td><code id="quantile_forest_+3A_compute.oob.predictions">compute.oob.predictions</code></td>
<td>
<p>Whether OOB predictions on training set should be precomputed. Default is FALSE.</p>
</td></tr>
<tr><td><code id="quantile_forest_+3A_num.threads">num.threads</code></td>
<td>
<p>Number of threads used in training. By default, the number of threads is set
to the maximum hardware concurrency.</p>
</td></tr>
<tr><td><code id="quantile_forest_+3A_seed">seed</code></td>
<td>
<p>The seed of the C++ random number generator.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A trained quantile forest object.
</p>


<h3>References</h3>

<p>Athey, Susan, Julie Tibshirani, and Stefan Wager. &quot;Generalized Random Forests&quot;.
Annals of Statistics, 47(2), 2019.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Generate data.
n &lt;- 50
p &lt;- 10
X &lt;- matrix(rnorm(n * p), n, p)
X.test &lt;- matrix(0, 101, p)
X.test[, 1] &lt;- seq(-2, 2, length.out = 101)
Y &lt;- X[, 1] * rnorm(n)

# Train a quantile forest.
q.forest &lt;- quantile_forest(X, Y, quantiles = c(0.1, 0.5, 0.9))

# Make predictions.
q.hat &lt;- predict(q.forest, X.test)

# Make predictions for different quantiles than those used in training.
q.hat &lt;- predict(q.forest, X.test, quantiles = c(0.1, 0.9))

# Train a quantile forest using regression splitting instead of quantile-based
# splits, emulating the approach in Meinshausen (2006).
meins.forest &lt;- quantile_forest(X, Y, regression.splitting = TRUE)

# Make predictions for the desired quantiles.
q.hat &lt;- predict(meins.forest, X.test, quantiles = c(0.1, 0.5, 0.9))


</code></pre>

<hr>
<h2 id='rank_average_treatment_effect'>Estimate a Rank-Weighted Average Treatment Effect (RATE).</h2><span id='topic+rank_average_treatment_effect'></span>

<h3>Description</h3>

<p>Consider a rule <code class="reqn">S(X_i)</code> assigning scores to units in decreasing order of treatment prioritization.
In the case of a forest with binary treatment, we provide estimates of the following, where
1/n &lt;= q &lt;= 1 represents the fraction of treated units:
</p>

<ul>
<li><p> The Rank-Weighted Average Treatment Effect (RATE):
<code class="reqn">\int_{0}^{1} alpha(q) TOC(q; S) dq</code>, where alpha is a weighting method
corresponding to either 'AUTOC' or 'QINI'.
</p>
</li>
<li><p> The Targeting Operator Characteristic (TOC):
<code class="reqn">E[Y_i(1) - Y_i(0) | F(S(X_i)) \geq 1 - q] - E[Y_i(1) - Y_i(0)]</code>,
where <code class="reqn">F(\cdot)</code> is the distribution function of <code class="reqn">S(X_i)</code>.
</p>
</li></ul>

<p>The Targeting Operator Characteristic (TOC) is a curve comparing the benefit of treating only a certain
fraction q of units (as prioritized by <code class="reqn">S(X_i)</code>), to the overall average treatment effect.
The Rank-Weighted Average Treatment Effect (RATE) is a weighted sum of this curve,
and is a measure designed to identify prioritization rules that effectively targets treatment
(and can thus be used to test for the presence of heterogeneous treatment effects).
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rank_average_treatment_effect(
  forest,
  priorities,
  target = c("AUTOC", "QINI"),
  q = seq(0.1, 1, by = 0.1),
  R = 200,
  subset = NULL,
  debiasing.weights = NULL,
  compliance.score = NULL,
  num.trees.for.weights = 500
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="rank_average_treatment_effect_+3A_forest">forest</code></td>
<td>
<p>The evaluation set forest.</p>
</td></tr>
<tr><td><code id="rank_average_treatment_effect_+3A_priorities">priorities</code></td>
<td>
<p>Treatment prioritization scores S(Xi) for the units used to train the evaluation forest.
Two prioritization rules can be compared by supplying a two-column array or named list of priorities
(yielding paired standard errors that account for the correlation between RATE metrics estimated on
the same evaluation data).
WARNING: for valid statistical performance, these scores should be constructed independently from the evaluation
forest training data.</p>
</td></tr>
<tr><td><code id="rank_average_treatment_effect_+3A_target">target</code></td>
<td>
<p>The type of RATE estimate, options are &quot;AUTOC&quot; (exhibits greater power when only a small subset
of the population experience nontrivial heterogeneous treatment effects) or &quot;QINI&quot; (exhibits greater power
when the entire population experience diffuse or substantial heterogeneous treatment effects).
Default is &quot;AUTOC&quot;.</p>
</td></tr>
<tr><td><code id="rank_average_treatment_effect_+3A_q">q</code></td>
<td>
<p>The grid q to compute the TOC curve on. Default is
(10%, 20%, ..., 100%).</p>
</td></tr>
<tr><td><code id="rank_average_treatment_effect_+3A_r">R</code></td>
<td>
<p>Number of bootstrap replicates for SEs. Default is 200.</p>
</td></tr>
<tr><td><code id="rank_average_treatment_effect_+3A_subset">subset</code></td>
<td>
<p>Specifies subset of the training examples over which we
estimate the RATE. WARNING: For valid statistical performance,
the subset should be defined only using features Xi, not using
the treatment Wi or the outcome Yi.</p>
</td></tr>
<tr><td><code id="rank_average_treatment_effect_+3A_debiasing.weights">debiasing.weights</code></td>
<td>
<p>A vector of length n (or the subset length) of debiasing weights.
If NULL (default) these are obtained via the appropriate doubly robust score
construction, e.g., in the case of causal_forests with a binary treatment, they
are obtained via inverse-propensity weighting.</p>
</td></tr>
<tr><td><code id="rank_average_treatment_effect_+3A_compliance.score">compliance.score</code></td>
<td>
<p>Only used with instrumental forests. An estimate of the causal
effect of Z on W, i.e., Delta(X) = E[W | X, Z = 1] - E[W | X, Z = 0],
which can then be used to produce debiasing.weights. If not provided,
this is estimated via an auxiliary causal forest.</p>
</td></tr>
<tr><td><code id="rank_average_treatment_effect_+3A_num.trees.for.weights">num.trees.for.weights</code></td>
<td>
<p>In some cases (e.g., with causal forests with a continuous
treatment), we need to train auxiliary forests to learn debiasing weights.
This is the number of trees used for this task. Note: this argument is only
used when debiasing.weights = NULL.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of class 'rank_average_treatment_effect' with elements </p>

<ul>
<li><p> estimate: the RATE estimate.
</p>
</li>
<li><p> std.err: bootstrapped standard error of RATE.
</p>
</li>
<li><p> target: the type of estimate.
</p>
</li>
<li><p> TOC: a data.frame with the Targeting Operator Characteristic curve
estimated on grid q, along with bootstrapped SEs.
</p>
</li></ul>



<h3>References</h3>

<p>Yadlowsky, Steve, Scott Fleming, Nigam Shah, Emma Brunskill, and Stefan Wager.
&quot;Evaluating Treatment Prioritization Rules via Rank-Weighted Average Treatment Effects.&quot;
arXiv preprint arXiv:2111.07966, 2021.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+rank_average_treatment_effect.fit">rank_average_treatment_effect.fit</a></code> for computing a RATE with user-supplied
doubly robust scores.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Simulate a simple medical example with a binary outcome and heterogeneous treatment effects.
# We're imagining that the treatment W decreases the risk of getting a stroke for some units,
# while having no effect on the other units (those with X1 &lt; 0).
n &lt;- 2000
p &lt;- 5
X &lt;- matrix(rnorm(n * p), n, p)
W &lt;- rbinom(n, 1, 0.5)
stroke.probability &lt;- 1 / (1 + exp(2 * (pmax(2 * X[, 1], 0) * W - X[, 2])))
Y.stroke &lt;- rbinom(n, 1, stroke.probability)

# We'll label the outcome Y such that "large" values are "good" to make interpretation easier.
# With Y=1 ("no stroke") and Y=0 ("stroke"), then an average treatment effect,
# E[Y(1) - Y(0)] = P[Y(1) = 1] - P[Y(0) = 1], quantifies the counterfactual risk difference
# of being stroke-free with treatment over being stroke-free without treatment.
# This will be positive if the treatment decreases the risk of getting a stroke.
Y &lt;- 1 - Y.stroke

# Train a CATE estimator on a training set.
train &lt;- sample(1:n, n / 2)
cf.cate &lt;- causal_forest(X[train, ], Y[train], W[train])

# Predict treatment effects on a held-out test set.
test &lt;- -train
cate.hat &lt;-  predict(cf.cate, X[test, ])$predictions

# Next, use the RATE metric to assess heterogeneity.

# Fit an evaluation forest for estimating the RATE.
cf.eval &lt;- causal_forest(X[test, ], Y[test], W[test])

# Form a doubly robust RATE estimate on the held-out test set.
rate &lt;- rank_average_treatment_effect(cf.eval, cate.hat)

# Plot the Targeting Operator Characteristic (TOC) curve.
# In this example, the ATE among the units with high predicted CATEs
# is substantially larger than the overall ATE.
plot(rate)

# Get an estimate of the area under the TOC (AUTOC).
rate

# Construct a 95% CI for the AUTOC.
# A significant result suggests that there are HTEs and that the CATE-based prioritization rule
# is effective at stratifying the sample.
# A non-significant result would suggest that either there are no HTEs
# or that the treatment prioritization rule does not predict them effectively.
rate$estimate + 1.96*c(-1, 1)*rate$std.err

# In some applications, we may be interested in other ways to target treatment.
# One example is baseline risk. In our example, we could estimate the probability of getting
# a stroke in the absence of treatment, and then use this as a non-causal heuristic
# to prioritize individuals with a high baseline risk.
# The hope would be that patients with a high predicted risk of getting a stroke,
# also have a high treatment effect.

# We can use the RATE metric to evaluate this treatment prioritization rule.

# First, fit a baseline risk model on the training set control group (W=0).
train.control &lt;- train[W[train] == 0]
rf.risk &lt;- regression_forest(X[train.control, ], Y.stroke[train.control])

# Then, on the test set, predict the baseline risk of getting a stroke.
baseline.risk.hat &lt;- predict(rf.risk, X[test, ])$predictions

# Use RATE to compare CATE and risk-based prioritization rules.
rate.diff &lt;- rank_average_treatment_effect(cf.eval, cbind(cate.hat, baseline.risk.hat))
plot(rate.diff)

# Construct a 95 % CI for the AUTOC and the difference in AUTOC.
rate.diff$estimate + data.frame(lower = -1.96 * rate.diff$std.err,
                                upper = 1.96 * rate.diff$std.err,
                                row.names = rate.diff$target)


</code></pre>

<hr>
<h2 id='rank_average_treatment_effect.fit'>Fitter function for Rank-Weighted Average Treatment Effect (RATE).</h2><span id='topic+rank_average_treatment_effect.fit'></span>

<h3>Description</h3>

<p>Provides an optional interface to <code><a href="#topic+rank_average_treatment_effect">rank_average_treatment_effect</a></code> which allows for user-supplied
evaluation scores.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rank_average_treatment_effect.fit(
  DR.scores,
  priorities,
  target = c("AUTOC", "QINI"),
  q = seq(0.1, 1, by = 0.1),
  R = 200,
  sample.weights = NULL,
  clusters = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="rank_average_treatment_effect.fit_+3A_dr.scores">DR.scores</code></td>
<td>
<p>A vector with the evaluation set scores.</p>
</td></tr>
<tr><td><code id="rank_average_treatment_effect.fit_+3A_priorities">priorities</code></td>
<td>
<p>Treatment prioritization scores S(Xi) for the units in the evaluation set.
Two prioritization rules can be compared by supplying a two-column array or named list of priorities
(yielding paired standard errors that account for the correlation between RATE metrics estimated on
the same evaluation data).
WARNING: for valid statistical performance, these scores should be constructed independently from the evaluation
dataset used to construct DR.scores.</p>
</td></tr>
<tr><td><code id="rank_average_treatment_effect.fit_+3A_target">target</code></td>
<td>
<p>The type of RATE estimate, options are &quot;AUTOC&quot; (exhibits greater power when only a small subset
of the population experience nontrivial heterogeneous treatment effects) or &quot;QINI&quot; (exhibits greater power
when the entire population experience diffuse or substantial heterogeneous treatment effects).
Default is &quot;AUTOC&quot;.</p>
</td></tr>
<tr><td><code id="rank_average_treatment_effect.fit_+3A_q">q</code></td>
<td>
<p>The grid q to compute the TOC curve on. Default is
(10%, 20%, ..., 100%).</p>
</td></tr>
<tr><td><code id="rank_average_treatment_effect.fit_+3A_r">R</code></td>
<td>
<p>Number of bootstrap replicates for SEs. Default is 200.</p>
</td></tr>
<tr><td><code id="rank_average_treatment_effect.fit_+3A_sample.weights">sample.weights</code></td>
<td>
<p>Weights given to an observation in estimation.
If NULL, each observation is given the same weight. Default is NULL.</p>
</td></tr>
<tr><td><code id="rank_average_treatment_effect.fit_+3A_clusters">clusters</code></td>
<td>
<p>Vector of integers or factors specifying which cluster each observation corresponds to.
Default is NULL (ignored).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of class 'rank_average_treatment_effect' with elements </p>

<ul>
<li><p> estimate: the RATE estimate.
</p>
</li>
<li><p> std.err: bootstrapped standard error of RATE.
</p>
</li>
<li><p> target: the type of estimate.
</p>
</li>
<li><p> TOC: a data.frame with the Targeting Operator Characteristic curve
estimated on grid q, along with bootstrapped SEs.
</p>
</li></ul>



<h3>Examples</h3>

<pre><code class='language-R'>
# Estimate CATEs with a causal forest.
n &lt;- 2000
p &lt;- 5
X &lt;- matrix(rnorm(n * p), n, p)
W &lt;- rbinom(n, 1, 0.5)
event.probability &lt;- 1 / (1 + exp(2 * (pmax(2 * X[, 1], 0) * W - X[, 2])))
Y &lt;- 1 - rbinom(n, 1, event.probability)

train &lt;- sample(1:n, n / 2)
cf.cate &lt;- causal_forest(X[train, ], Y[train], W[train])

# Predict treatment effects on a held-out test set.
test &lt;- -train
cate.hat &lt;-  predict(cf.cate, X[test, ])$predictions

# Estimate AIPW nuisance components on the held-out test set.
Y.forest.eval &lt;- regression_forest(X[test, ], Y[test], num.trees = 500)
Y.hat.eval &lt;- predict(Y.forest.eval)$predictions
W.forest.eval &lt;- regression_forest(X[test, ], W[test], num.trees = 500)
W.hat.eval &lt;- predict(W.forest.eval)$predictions
cf.eval &lt;- causal_forest(X[test, ], Y[test], W[test],
                         Y.hat = Y.hat.eval,
                         W.hat = W.hat.eval)

# Form doubly robust scores.
tau.hat.eval &lt;- predict(cf.eval)$predictions
debiasing.weights.eval &lt;- (W[test] - W.hat.eval) / (W.hat.eval * (1 - W.hat.eval))
Y.residual.eval &lt;- Y[test] - (Y.hat.eval + tau.hat.eval * (W[test] - W.hat.eval))
DR.scores &lt;- tau.hat.eval + debiasing.weights.eval * Y.residual.eval

# Could equivalently be obtained by
# DR.scores &lt;- get_scores(cf.eval)

# Form a doubly robust RATE estimate on the held-out test set.
rate &lt;- rank_average_treatment_effect.fit(DR.scores, cate.hat)
rate

# Same as
# rate &lt;- rank_average_treatment_effect(cf.eval, cate.hat)

# In settings where the treatment randomization probabilities W.hat are known, an
# alternative to AIPW scores is to use inverse-propensity weighting (IPW):
# 1(W=1) * Y / W.hat - 1(W=0) * Y / (1 - W.hat).
# Here, W.hat = 0.5, and an IPW-based estimate of RATE is:
IPW.scores &lt;- ifelse(W[test] == 1, Y[test] / 0.5, -Y[test] / 0.5)
rate.ipw &lt;- rank_average_treatment_effect.fit(IPW.scores, cate.hat)
rate.ipw

# IPW-based estimators typically have higher variance. For details on
# score constructions for other causal estimands, please see the RATE paper.


</code></pre>

<hr>
<h2 id='regression_forest'>Regression forest</h2><span id='topic+regression_forest'></span>

<h3>Description</h3>

<p>Trains a regression forest that can be used to estimate
the conditional mean function mu(x) = E[Y | X = x]
</p>


<h3>Usage</h3>

<pre><code class='language-R'>regression_forest(
  X,
  Y,
  num.trees = 2000,
  sample.weights = NULL,
  clusters = NULL,
  equalize.cluster.weights = FALSE,
  sample.fraction = 0.5,
  mtry = min(ceiling(sqrt(ncol(X)) + 20), ncol(X)),
  min.node.size = 5,
  honesty = TRUE,
  honesty.fraction = 0.5,
  honesty.prune.leaves = TRUE,
  alpha = 0.05,
  imbalance.penalty = 0,
  ci.group.size = 2,
  tune.parameters = "none",
  tune.num.trees = 50,
  tune.num.reps = 100,
  tune.num.draws = 1000,
  compute.oob.predictions = TRUE,
  num.threads = NULL,
  seed = runif(1, 0, .Machine$integer.max)
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="regression_forest_+3A_x">X</code></td>
<td>
<p>The covariates used in the regression.</p>
</td></tr>
<tr><td><code id="regression_forest_+3A_y">Y</code></td>
<td>
<p>The outcome.</p>
</td></tr>
<tr><td><code id="regression_forest_+3A_num.trees">num.trees</code></td>
<td>
<p>Number of trees grown in the forest. Note: Getting accurate
confidence intervals generally requires more trees than
getting accurate predictions. Default is 2000.</p>
</td></tr>
<tr><td><code id="regression_forest_+3A_sample.weights">sample.weights</code></td>
<td>
<p>Weights given to an observation in estimation.
If NULL, each observation is given the same weight. Default is NULL.</p>
</td></tr>
<tr><td><code id="regression_forest_+3A_clusters">clusters</code></td>
<td>
<p>Vector of integers or factors specifying which cluster each observation corresponds to.
Default is NULL (ignored).</p>
</td></tr>
<tr><td><code id="regression_forest_+3A_equalize.cluster.weights">equalize.cluster.weights</code></td>
<td>
<p>If FALSE, each unit is given the same weight (so that bigger
clusters get more weight). If TRUE, each cluster is given equal weight in the forest. In this case,
during training, each tree uses the same number of observations from each drawn cluster: If the
smallest cluster has K units, then when we sample a cluster during training, we only give a random
K elements of the cluster to the tree-growing procedure. When estimating average treatment effects,
each observation is given weight 1/cluster size, so that the total weight of each cluster is the
same. Note that, if this argument is FALSE, sample weights may also be directly adjusted via the
sample.weights argument. If this argument is TRUE, sample.weights must be set to NULL. Default is
FALSE.</p>
</td></tr>
<tr><td><code id="regression_forest_+3A_sample.fraction">sample.fraction</code></td>
<td>
<p>Fraction of the data used to build each tree.
Note: If honesty = TRUE, these subsamples will
further be cut by a factor of honesty.fraction. Default is 0.5.</p>
</td></tr>
<tr><td><code id="regression_forest_+3A_mtry">mtry</code></td>
<td>
<p>Number of variables tried for each split. Default is
<code class="reqn">\sqrt p + 20</code> where p is the number of variables.</p>
</td></tr>
<tr><td><code id="regression_forest_+3A_min.node.size">min.node.size</code></td>
<td>
<p>A target for the minimum number of observations in each tree leaf. Note that nodes
with size smaller than min.node.size can occur, as in the original randomForest package.
Default is 5.</p>
</td></tr>
<tr><td><code id="regression_forest_+3A_honesty">honesty</code></td>
<td>
<p>Whether to use honest splitting (i.e., sub-sample splitting). Default is TRUE.
For a detailed description of honesty, honesty.fraction, honesty.prune.leaves, and recommendations for
parameter tuning, see the grf algorithm reference.</p>
</td></tr>
<tr><td><code id="regression_forest_+3A_honesty.fraction">honesty.fraction</code></td>
<td>
<p>The fraction of data that will be used for determining splits if honesty = TRUE. Corresponds
to set J1 in the notation of the paper. Default is 0.5 (i.e. half of the data is used for
determining splits).</p>
</td></tr>
<tr><td><code id="regression_forest_+3A_honesty.prune.leaves">honesty.prune.leaves</code></td>
<td>
<p>If TRUE, prunes the estimation sample tree such that no leaves
are empty. If FALSE, keep the same tree as determined in the splits sample (if an empty leave is encountered, that
tree is skipped and does not contribute to the estimate). Setting this to FALSE may improve performance on
small/marginally powered data, but requires more trees (note: tuning does not adjust the number of trees).
Only applies if honesty is enabled. Default is TRUE.</p>
</td></tr>
<tr><td><code id="regression_forest_+3A_alpha">alpha</code></td>
<td>
<p>A tuning parameter that controls the maximum imbalance of a split. Default is 0.05.</p>
</td></tr>
<tr><td><code id="regression_forest_+3A_imbalance.penalty">imbalance.penalty</code></td>
<td>
<p>A tuning parameter that controls how harshly imbalanced splits are penalized. Default is 0.</p>
</td></tr>
<tr><td><code id="regression_forest_+3A_ci.group.size">ci.group.size</code></td>
<td>
<p>The forest will grow ci.group.size trees on each subsample.
In order to provide confidence intervals, ci.group.size must
be at least 2. Default is 2.</p>
</td></tr>
<tr><td><code id="regression_forest_+3A_tune.parameters">tune.parameters</code></td>
<td>
<p>A vector of parameter names to tune.
If &quot;all&quot;: all tunable parameters are tuned by cross-validation. The following parameters are
tunable: (&quot;sample.fraction&quot;, &quot;mtry&quot;, &quot;min.node.size&quot;, &quot;honesty.fraction&quot;,
&quot;honesty.prune.leaves&quot;, &quot;alpha&quot;, &quot;imbalance.penalty&quot;). If honesty is FALSE the honesty.* parameters are not tuned.
Default is &quot;none&quot; (no parameters are tuned).</p>
</td></tr>
<tr><td><code id="regression_forest_+3A_tune.num.trees">tune.num.trees</code></td>
<td>
<p>The number of trees in each 'mini forest' used to fit the tuning model. Default is 50.</p>
</td></tr>
<tr><td><code id="regression_forest_+3A_tune.num.reps">tune.num.reps</code></td>
<td>
<p>The number of forests used to fit the tuning model. Default is 100.</p>
</td></tr>
<tr><td><code id="regression_forest_+3A_tune.num.draws">tune.num.draws</code></td>
<td>
<p>The number of random parameter values considered when using the model
to select the optimal parameters. Default is 1000.</p>
</td></tr>
<tr><td><code id="regression_forest_+3A_compute.oob.predictions">compute.oob.predictions</code></td>
<td>
<p>Whether OOB predictions on training set should be precomputed. Default is TRUE.</p>
</td></tr>
<tr><td><code id="regression_forest_+3A_num.threads">num.threads</code></td>
<td>
<p>Number of threads used in training. By default, the number of threads is set
to the maximum hardware concurrency.</p>
</td></tr>
<tr><td><code id="regression_forest_+3A_seed">seed</code></td>
<td>
<p>The seed of the C++ random number generator.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A trained regression forest object. If tune.parameters is enabled,
then tuning information will be included through the 'tuning.output' attribute.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Train a standard regression forest.
n &lt;- 500
p &lt;- 10
X &lt;- matrix(rnorm(n * p), n, p)
Y &lt;- X[, 1] * rnorm(n)
r.forest &lt;- regression_forest(X, Y)

# Predict using the forest.
X.test &lt;- matrix(0, 101, p)
X.test[, 1] &lt;- seq(-2, 2, length.out = 101)
r.pred &lt;- predict(r.forest, X.test)

# Predict on out-of-bag training samples.
r.pred &lt;- predict(r.forest)

# Predict with confidence intervals; growing more trees is now recommended.
r.forest &lt;- regression_forest(X, Y, num.trees = 100)
r.pred &lt;- predict(r.forest, X.test, estimate.variance = TRUE)


</code></pre>

<hr>
<h2 id='split_frequencies'>Calculate which features the forest split on at each depth.</h2><span id='topic+split_frequencies'></span>

<h3>Description</h3>

<p>Calculate which features the forest split on at each depth.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>split_frequencies(forest, max.depth = 4)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="split_frequencies_+3A_forest">forest</code></td>
<td>
<p>The trained forest.</p>
</td></tr>
<tr><td><code id="split_frequencies_+3A_max.depth">max.depth</code></td>
<td>
<p>Maximum depth of splits to consider.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A matrix of split depth by feature index, where each value
is the number of times the feature was split on at that depth.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Train a quantile forest.
n &lt;- 250
p &lt;- 10
X &lt;- matrix(rnorm(n * p), n, p)
Y &lt;- X[, 1] * rnorm(n)
q.forest &lt;- quantile_forest(X, Y, quantiles = c(0.1, 0.5, 0.9))

# Calculate the split frequencies for this forest.
split_frequencies(q.forest)


</code></pre>

<hr>
<h2 id='survival_forest'>Survival forest</h2><span id='topic+survival_forest'></span>

<h3>Description</h3>

<p>Trains a forest for right-censored surival data that can be used to estimate the
conditional survival function S(t, x) = P[T &gt; t | X = x]
</p>


<h3>Usage</h3>

<pre><code class='language-R'>survival_forest(
  X,
  Y,
  D,
  failure.times = NULL,
  num.trees = 1000,
  sample.weights = NULL,
  clusters = NULL,
  equalize.cluster.weights = FALSE,
  sample.fraction = 0.5,
  mtry = min(ceiling(sqrt(ncol(X)) + 20), ncol(X)),
  min.node.size = 15,
  honesty = TRUE,
  honesty.fraction = 0.5,
  honesty.prune.leaves = TRUE,
  alpha = 0.05,
  prediction.type = c("Kaplan-Meier", "Nelson-Aalen"),
  compute.oob.predictions = TRUE,
  num.threads = NULL,
  seed = runif(1, 0, .Machine$integer.max)
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="survival_forest_+3A_x">X</code></td>
<td>
<p>The covariates.</p>
</td></tr>
<tr><td><code id="survival_forest_+3A_y">Y</code></td>
<td>
<p>The event time (must be non-negative).</p>
</td></tr>
<tr><td><code id="survival_forest_+3A_d">D</code></td>
<td>
<p>The event type (0: censored, 1: failure/observed event).</p>
</td></tr>
<tr><td><code id="survival_forest_+3A_failure.times">failure.times</code></td>
<td>
<p>A vector of event times to fit the survival curve at. If NULL, then all the observed
failure times are used. This speeds up forest estimation by constraining the event grid. Observed event
times are rounded down to the last sorted occurance less than or equal to the specified failure time.
The time points should be in increasing order. Default is NULL.</p>
</td></tr>
<tr><td><code id="survival_forest_+3A_num.trees">num.trees</code></td>
<td>
<p>Number of trees grown in the forest. Default is 1000.</p>
</td></tr>
<tr><td><code id="survival_forest_+3A_sample.weights">sample.weights</code></td>
<td>
<p>Weights given to an observation in prediction.
If NULL, each observation is given the same weight. Default is NULL.</p>
</td></tr>
<tr><td><code id="survival_forest_+3A_clusters">clusters</code></td>
<td>
<p>Vector of integers or factors specifying which cluster each observation corresponds to.
Default is NULL (ignored).</p>
</td></tr>
<tr><td><code id="survival_forest_+3A_equalize.cluster.weights">equalize.cluster.weights</code></td>
<td>
<p>If FALSE, each unit is given the same weight (so that bigger
clusters get more weight). If TRUE, each cluster is given equal weight in the forest. In this case,
during training, each tree uses the same number of observations from each drawn cluster: If the
smallest cluster has K units, then when we sample a cluster during training, we only give a random
K elements of the cluster to the tree-growing procedure. When estimating average treatment effects,
each observation is given weight 1/cluster size, so that the total weight of each cluster is the
same. Note that, if this argument is FALSE, sample weights may also be directly adjusted via the
sample.weights argument. If this argument is TRUE, sample.weights must be set to NULL. Default is
FALSE.</p>
</td></tr>
<tr><td><code id="survival_forest_+3A_sample.fraction">sample.fraction</code></td>
<td>
<p>Fraction of the data used to build each tree.
Note: If honesty = TRUE, these subsamples will
further be cut by a factor of honesty.fraction. Default is 0.5.</p>
</td></tr>
<tr><td><code id="survival_forest_+3A_mtry">mtry</code></td>
<td>
<p>Number of variables tried for each split. Default is
<code class="reqn">\sqrt p + 20</code> where p is the number of variables.</p>
</td></tr>
<tr><td><code id="survival_forest_+3A_min.node.size">min.node.size</code></td>
<td>
<p>A target for the minimum number of observations in each tree leaf. Note that nodes
with size smaller than min.node.size can occur, as in the original randomForest package.
Default is 15.</p>
</td></tr>
<tr><td><code id="survival_forest_+3A_honesty">honesty</code></td>
<td>
<p>Whether to use honest splitting (i.e., sub-sample splitting). Default is TRUE.
For a detailed description of honesty, honesty.fraction, honesty.prune.leaves, and recommendations for
parameter tuning, see the grf algorithm reference.</p>
</td></tr>
<tr><td><code id="survival_forest_+3A_honesty.fraction">honesty.fraction</code></td>
<td>
<p>The fraction of data that will be used for determining splits if honesty = TRUE. Corresponds
to set J1 in the notation of the paper. Default is 0.5 (i.e. half of the data is used for
determining splits).</p>
</td></tr>
<tr><td><code id="survival_forest_+3A_honesty.prune.leaves">honesty.prune.leaves</code></td>
<td>
<p>If TRUE, prunes the estimation sample tree such that no leaves
are empty. If FALSE, keep the same tree as determined in the splits sample (if an empty leave is encountered, that
tree is skipped and does not contribute to the estimate). Setting this to FALSE may improve performance on
small/marginally powered data, but requires more trees (note: tuning does not adjust the number of trees).
Only applies if honesty is enabled. Default is TRUE.</p>
</td></tr>
<tr><td><code id="survival_forest_+3A_alpha">alpha</code></td>
<td>
<p>A tuning parameter that controls the maximum imbalance of a split. The number of failures in
each child has to be at least one or 'alpha' times the number of samples in the parent node. Default is 0.05.
(On data with very low event rate the default value may be too high for the forest to split
and lowering it may be beneficial).</p>
</td></tr>
<tr><td><code id="survival_forest_+3A_prediction.type">prediction.type</code></td>
<td>
<p>The type of estimate of the survival function, choices are &quot;Kaplan-Meier&quot; or &quot;Nelson-Aalen&quot;.
Only relevant if 'compute.oob.predictions' is TRUE. Default is &quot;Kaplan-Meier&quot;.</p>
</td></tr>
<tr><td><code id="survival_forest_+3A_compute.oob.predictions">compute.oob.predictions</code></td>
<td>
<p>Whether OOB predictions on training set should be precomputed. Default is TRUE.</p>
</td></tr>
<tr><td><code id="survival_forest_+3A_num.threads">num.threads</code></td>
<td>
<p>Number of threads used in training. By default, the number of threads is set
to the maximum hardware concurrency.</p>
</td></tr>
<tr><td><code id="survival_forest_+3A_seed">seed</code></td>
<td>
<p>The seed of the C++ random number generator.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A trained survival_forest forest object.
</p>


<h3>References</h3>

<p>Cui, Yifan, Michael R. Kosorok, Erik Sverdrup, Stefan Wager, and Ruoqing Zhu.
&quot;Estimating Heterogeneous Treatment Effects with Right-Censored Data via Causal Survival Forests.&quot;
Journal of the Royal Statistical Society: Series B, 85(2), 2023.
</p>
<p>Ishwaran, Hemant, Udaya B. Kogalur, Eugene H. Blackstone, and Michael S. Lauer.
&quot;Random survival forests.&quot; The Annals of Applied Statistics 2.3 (2008): 841-860.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Train a standard survival forest.
n &lt;- 2000
p &lt;- 5
X &lt;- matrix(rnorm(n * p), n, p)
failure.time &lt;- exp(0.5 * X[, 1]) * rexp(n)
censor.time &lt;- 2 * rexp(n)
Y &lt;- pmin(failure.time, censor.time)
D &lt;- as.integer(failure.time &lt;= censor.time)
# Save computation time by constraining the event grid by discretizing (rounding) continuous events.
s.forest &lt;- survival_forest(X, round(Y, 2), D)
# Or do so more flexibly by defining your own time grid using the failure.times argument.
# grid &lt;- seq(min(Y[D==1]), max(Y[D==1]), length.out = 150)
# s.forest &lt;- survival_forest(X, Y, D, failure.times = grid)

# Predict using the forest.
X.test &lt;- matrix(0, 3, p)
X.test[, 1] &lt;- seq(-2, 2, length.out = 3)
s.pred &lt;- predict(s.forest, X.test)

# Plot the survival curve.
plot(NA, NA, xlab = "failure time", ylab = "survival function",
     xlim = range(s.pred$failure.times),
     ylim = c(0, 1))
for(i in 1:3) {
  lines(s.pred$failure.times, s.pred$predictions[i,], col = i)
  s.true = exp(-s.pred$failure.times / exp(0.5 * X.test[i, 1]))
  lines(s.pred$failure.times, s.true, col = i, lty = 2)
}

# Predict on out-of-bag training samples.
s.pred &lt;- predict(s.forest)

# Compute OOB concordance based on the mortality score in Ishwaran et al. (2008).
s.pred.nelson.aalen &lt;- predict(s.forest, prediction.type = "Nelson-Aalen")
chf.score &lt;- rowSums(-log(s.pred.nelson.aalen$predictions))
if (require("survival", quietly = TRUE)) {
 concordance(Surv(Y, D) ~ chf.score, reverse = TRUE)
}


</code></pre>

<hr>
<h2 id='test_calibration'>Omnibus evaluation of the quality of the random forest estimates via calibration.</h2><span id='topic+test_calibration'></span>

<h3>Description</h3>

<p>Test calibration of the forest. Computes the best linear fit of the target
estimand using the forest prediction (on held-out data) as well as the mean
forest prediction as the sole two regressors. A coefficient of 1 for
'mean.forest.prediction' suggests that the mean forest prediction is correct,
whereas a coefficient of 1 for 'differential.forest.prediction' additionally suggests
that the heterogeneity estimates from the forest are well calibrated.
The p-value of the 'differential.forest.prediction' coefficient
also acts as an omnibus test for the presence of heterogeneity: If the coefficient
is significantly greater than 0, then we can reject the null of
no heterogeneity. For another class of omnnibus tests see <code><a href="#topic+rank_average_treatment_effect">rank_average_treatment_effect</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>test_calibration(forest, vcov.type = "HC3")
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="test_calibration_+3A_forest">forest</code></td>
<td>
<p>The trained forest.</p>
</td></tr>
<tr><td><code id="test_calibration_+3A_vcov.type">vcov.type</code></td>
<td>
<p>Optional covariance type for standard errors. The possible
options are HC0, ..., HC3. The default is &quot;HC3&quot;, which is recommended in small
samples and corresponds to the &quot;shortcut formula&quot; for the jackknife
(see MacKinnon &amp; White for more discussion, and Cameron &amp; Miller for a review).
For large data sets with clusters, &quot;HC0&quot; or &quot;HC1&quot; are significantly faster to compute.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A heteroskedasticity-consistent test of calibration.
</p>


<h3>References</h3>

<p>Cameron, A. Colin, and Douglas L. Miller. &quot;A practitioner's guide to
cluster-robust inference.&quot; Journal of Human Resources 50, no. 2 (2015): 317-372.
</p>
<p>Chernozhukov, Victor, Mert Demirer, Esther Duflo, and Ivan Fernandez-Val.
&quot;Generic Machine Learning Inference on Heterogenous Treatment Effects in
Randomized Experiments.&quot; arXiv preprint arXiv:1712.04802 (2017).
</p>
<p>MacKinnon, James G., and Halbert White. &quot;Some heteroskedasticity-consistent
covariance matrix estimators with improved finite sample properties.&quot;
Journal of Econometrics 29.3 (1985): 305-325.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
n &lt;- 800
p &lt;- 5
X &lt;- matrix(rnorm(n * p), n, p)
W &lt;- rbinom(n, 1, 0.25 + 0.5 * (X[, 1] &gt; 0))
Y &lt;- pmax(X[, 1], 0) * W + X[, 2] + pmin(X[, 3], 0) + rnorm(n)
forest &lt;- causal_forest(X, Y, W)
test_calibration(forest)


</code></pre>

<hr>
<h2 id='tune_forest'>Tune a forest</h2><span id='topic+tune_forest'></span>

<h3>Description</h3>

<p>Finds the optimal parameters to be used in training a forest.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tune_forest(
  data,
  nrow.X,
  ncol.X,
  args,
  tune.parameters,
  tune.parameters.defaults,
  tune.num.trees,
  tune.num.reps,
  tune.num.draws,
  train
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tune_forest_+3A_data">data</code></td>
<td>
<p>The data arguments (output from create_train_matrices) for the forest.</p>
</td></tr>
<tr><td><code id="tune_forest_+3A_nrow.x">nrow.X</code></td>
<td>
<p>The number of observations.</p>
</td></tr>
<tr><td><code id="tune_forest_+3A_ncol.x">ncol.X</code></td>
<td>
<p>The number of variables.</p>
</td></tr>
<tr><td><code id="tune_forest_+3A_args">args</code></td>
<td>
<p>The remaining call arguments for the forest.</p>
</td></tr>
<tr><td><code id="tune_forest_+3A_tune.parameters">tune.parameters</code></td>
<td>
<p>The vector of parameter names to tune.</p>
</td></tr>
<tr><td><code id="tune_forest_+3A_tune.parameters.defaults">tune.parameters.defaults</code></td>
<td>
<p>The grf default values for the vector of parameter names to tune.</p>
</td></tr>
<tr><td><code id="tune_forest_+3A_tune.num.trees">tune.num.trees</code></td>
<td>
<p>The number of trees in each 'mini forest' used to fit the tuning model.</p>
</td></tr>
<tr><td><code id="tune_forest_+3A_tune.num.reps">tune.num.reps</code></td>
<td>
<p>The number of forests used to fit the tuning model.</p>
</td></tr>
<tr><td><code id="tune_forest_+3A_tune.num.draws">tune.num.draws</code></td>
<td>
<p>The number of random parameter values considered when using the model
to select the optimal parameters.</p>
</td></tr>
<tr><td><code id="tune_forest_+3A_train">train</code></td>
<td>
<p>The grf forest training function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>tuning output
</p>

<hr>
<h2 id='tune_ll_causal_forest'>Local linear forest tuning</h2><span id='topic+tune_ll_causal_forest'></span>

<h3>Description</h3>

<p>Finds the optimal ridge penalty for local linear causal prediction.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tune_ll_causal_forest(
  forest,
  linear.correction.variables = NULL,
  ll.weight.penalty = FALSE,
  num.threads = NULL,
  lambda.path = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tune_ll_causal_forest_+3A_forest">forest</code></td>
<td>
<p>The forest used for prediction.</p>
</td></tr>
<tr><td><code id="tune_ll_causal_forest_+3A_linear.correction.variables">linear.correction.variables</code></td>
<td>
<p>Variables to use for local linear prediction. If left null,
all variables are used. Default is NULL.</p>
</td></tr>
<tr><td><code id="tune_ll_causal_forest_+3A_ll.weight.penalty">ll.weight.penalty</code></td>
<td>
<p>Option to standardize ridge penalty by covariance (TRUE),
or penalize all covariates equally (FALSE). Defaults to FALSE.</p>
</td></tr>
<tr><td><code id="tune_ll_causal_forest_+3A_num.threads">num.threads</code></td>
<td>
<p>Number of threads used in training. If set to NULL, the software
automatically selects an appropriate amount.</p>
</td></tr>
<tr><td><code id="tune_ll_causal_forest_+3A_lambda.path">lambda.path</code></td>
<td>
<p>Optional list of lambdas to use for cross-validation.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of lambdas tried, corresponding errors, and optimal ridge penalty lambda.
</p>

<hr>
<h2 id='tune_ll_regression_forest'>Local linear forest tuning</h2><span id='topic+tune_ll_regression_forest'></span>

<h3>Description</h3>

<p>Finds the optimal ridge penalty for local linear prediction.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tune_ll_regression_forest(
  forest,
  linear.correction.variables = NULL,
  ll.weight.penalty = FALSE,
  num.threads = NULL,
  lambda.path = NULL
)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="tune_ll_regression_forest_+3A_forest">forest</code></td>
<td>
<p>The forest used for prediction.</p>
</td></tr>
<tr><td><code id="tune_ll_regression_forest_+3A_linear.correction.variables">linear.correction.variables</code></td>
<td>
<p>Variables to use for local linear prediction. If left null,
all variables are used. Default is NULL.</p>
</td></tr>
<tr><td><code id="tune_ll_regression_forest_+3A_ll.weight.penalty">ll.weight.penalty</code></td>
<td>
<p>Option to standardize ridge penalty by covariance (TRUE),
or penalize all covariates equally (FALSE). Defaults to FALSE.</p>
</td></tr>
<tr><td><code id="tune_ll_regression_forest_+3A_num.threads">num.threads</code></td>
<td>
<p>Number of threads used in training. If set to NULL, the software
automatically selects an appropriate amount.</p>
</td></tr>
<tr><td><code id="tune_ll_regression_forest_+3A_lambda.path">lambda.path</code></td>
<td>
<p>Optional list of lambdas to use for cross-validation.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list of lambdas tried, corresponding errors, and optimal ridge penalty lambda.
</p>

<hr>
<h2 id='variable_importance'>Calculate a simple measure of 'importance' for each feature.</h2><span id='topic+variable_importance'></span>

<h3>Description</h3>

<p>A simple weighted sum of how many times feature i was split on at each depth in the forest.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>variable_importance(forest, decay.exponent = 2, max.depth = 4)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="variable_importance_+3A_forest">forest</code></td>
<td>
<p>The trained forest.</p>
</td></tr>
<tr><td><code id="variable_importance_+3A_decay.exponent">decay.exponent</code></td>
<td>
<p>A tuning parameter that controls the importance of split depth.</p>
</td></tr>
<tr><td><code id="variable_importance_+3A_max.depth">max.depth</code></td>
<td>
<p>Maximum depth of splits to consider.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list specifying an 'importance value' for each feature.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Train a quantile forest.
n &lt;- 250
p &lt;- 10
X &lt;- matrix(rnorm(n * p), n, p)
Y &lt;- X[, 1] * rnorm(n)
q.forest &lt;- quantile_forest(X, Y, quantiles = c(0.1, 0.5, 0.9))

# Calculate the 'importance' of each feature.
variable_importance(q.forest)


</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
