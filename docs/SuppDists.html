<!DOCTYPE html><html><head><title>Help for package SuppDists</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {SuppDists}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#Friedman'><p>Friedman's chi-square</p></a></li>
<li><a href='#ghyper'><p>Generalized hypergeometric distributions</p></a></li>
<li><a href='#ghyper.types'><p>Kemp and Kemp generalized hypergeometric types</p></a></li>
<li><a href='#invGauss'><p>The inverse Gaussian and Wald distributions</p></a></li>
<li><a href='#Johnson'><p>The Johnson distributions</p></a></li>
<li><a href='#Kendall'><p>The distribution of Kendall's tau</p></a></li>
<li><a href='#KruskalWallis'><p>Kruskall-Wallis distribution</p></a></li>
<li><a href='#maxFratio'><p>The maximum F-ratio distribution</p></a></li>
<li><a href='#NormalScore'><p>Normal Scores distribution</p></a></li>
<li><a href='#Pearson'><p>The Pearson product moment correlation coefficient</p></a></li>
<li><a href='#Spearman'><p>Spearman's rho</p></a></li>
<li><a href='#SuppDists-defunct'><p>Defunct Functions in Package <span class="pkg">SuppDists</span></p></a></li>
<li><a href='#SuppDists-internal'><p>Internal SuppDists function</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Supplementary Distributions</td>
</tr>
<tr>
<td>Version:</td>
<td>1.1-9.7</td>
</tr>
<tr>
<td>Date:</td>
<td>2022-01-02</td>
</tr>
<tr>
<td>Description:</td>
<td>Ten distributions supplementing those built into R.
  Inverse Gauss, Kruskal-Wallis, Kendall's Tau, Friedman's chi
  squared, Spearman's rho, maximum F ratio, the Pearson product
  moment correlation coefficient, Johnson distributions, normal
  scores and generalized hypergeometric distributions.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-2">GPL-2</a> | <a href="https://www.r-project.org/Licenses/GPL-3">GPL-3</a> [expanded from: GPL (&ge; 2)]</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.3.0)</td>
</tr>
<tr>
<td>Suggests:</td>
<td>RcppZiggurat</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2022-01-02 12:50:22 UTC; thorsten</td>
</tr>
<tr>
<td>Author:</td>
<td>Bob Wheeler [aut],
  Thorsten Pohlert <a href="https://orcid.org/0000-0003-3855-3025"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [ctb, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Thorsten Pohlert &lt;thorsten.pohlert@gmx.de&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2022-01-03 18:10:06 UTC</td>
</tr>
</table>
<hr>
<h2 id='Friedman'>Friedman's chi-square</h2><span id='topic+dFriedman'></span><span id='topic+pFriedman'></span><span id='topic+qFriedman'></span><span id='topic+rFriedman'></span><span id='topic+sFriedman'></span>

<h3>Description</h3>

<p>Density, distribution function, quantile function, random generator and summary function for Friedman's chi square.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dFriedman(x, r, N, log=FALSE)
pFriedman(q, r, N, lower.tail=TRUE, log.p=FALSE)
qFriedman(p, r, N, lower.tail=TRUE, log.p=FALSE)
rFriedman(n, r, N)
sFriedman(r, N)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Friedman_+3A_x">x</code>, <code id="Friedman_+3A_q">q</code></td>
<td>
<p>vector of non-negative quantities</p>
</td></tr>
<tr><td><code id="Friedman_+3A_p">p</code></td>
<td>
<p>vector of probabilities</p>
</td></tr>
<tr><td><code id="Friedman_+3A_n">n</code></td>
<td>
<p>number of values to generate. If n is a vector, length(n) values will be generated</p>
</td></tr>
<tr><td><code id="Friedman_+3A_r">r</code></td>
<td>
<p>vector of number of treatments</p>
</td></tr>
<tr><td><code id="Friedman_+3A_n">N</code></td>
<td>
<p>(N &gt;= 2) vector of number of replications of each treatment</p>
</td></tr>
<tr><td><code id="Friedman_+3A_log">log</code>, <code id="Friedman_+3A_log.p">log.p</code></td>
<td>
<p>  logical vector; if TRUE, probabilities p are given as log(p)</p>
</td></tr>
<tr><td><code id="Friedman_+3A_lower.tail">lower.tail</code></td>
<td>
<p>  logical vector; if TRUE (default), probabilities are <code class="reqn">P[X &lt;= x]</code>, otherwise, <code class="reqn">P[X &gt; x]</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Freidman chi-squared is used for nonparametric ANOVA. The data in N rows of an <code class="reqn">N \times r</code> table are ranked separately such that the ranks take the values from 1 to r in the N different rows. The distributions are obtained on the assumption that there is no relationship between the N rows. 
</p>
<p>Formulae:
</p>
<p>Let <code class="reqn">R_j</code> be the sum of ranks for treatment <code class="reqn">j (j=1\dots r)</code>, then the Friedman statistic is 
</p>
<p style="text-align: center;"><code class="reqn">
x=\frac{12}{N r (r+1)}\sum_{j=1}^{r}R_j^2 -3N(r+1)</code>
</p>

<p>this is asymptotically equivalent to a <code class="reqn">\chi^2</code> random variable. One may also calculate the chi squared statistic for the usual analysis of variance which gives 
</p>
<p style="text-align: center;"><code class="reqn">
F=\frac{(N-1)x}{N(r-1)-x}</code>
</p>

<p>which may be used with the F distribution functions in R for degrees of freedom <code class="reqn">(r-1)</code> and <code class="reqn">(N-1)(r-1)</code>.
</p>


<h3>Value</h3>

<p>The output values conform to the output from other such functions in <span class="rlang"><b>R</b></span>. <code>dFriedman()</code> gives the density, <code>pFriedman()</code> the distribution function and <code>qFriedman()</code> its inverse. <code>rFriedman()</code> generates random numbers. <code>sFriedman()</code> produces a list containing parameters corresponding to the arguments &ndash; mean, median, mode, variance, sd, third cental moment, fourth central moment, Pearson's skewness, skewness, and kurtosis.
</p>


<h3>Note</h3>

<p>Exact calculations are made for the following values of the parameters:
</p>

<table>
<tr>
 <td style="text-align: left;">
	r </td><td style="text-align: left;"> N </td>
</tr>
<tr>
 <td style="text-align: left;">
	2 </td><td style="text-align: left;"> 100 </td>
</tr>
<tr>
 <td style="text-align: left;">
	3 </td><td style="text-align: left;"> 30 </td>
</tr>
<tr>
 <td style="text-align: left;">
	4 </td><td style="text-align: left;"> 15 </td>
</tr>
<tr>
 <td style="text-align: left;">
	5 </td><td style="text-align: left;"> 8 
</td>
</tr>

</table>

<p>These exact calculations are made using the algorithm of Kendall and Smith (1939). 
</p>
<p>The incomplete beta, with continuity correction, is used for calculations outside these ranges.  Some appreciation for the accuracy of the approximation may be obtained by comparing the calculated values with exact tables such as Odeh (1977).  Iman and Davenport (1980) have studied the accuracy of various approximations.
</p>


<h3>Author(s)</h3>

<p>Bob Wheeler 
</p>


<h3>References</h3>

<p>Kendall, M. and Smith, B.B. (1939). The problem of m rankings. <em>Ann. Math. Stat.</em> <b>10.</b> 275-287.
</p>
<p>Iman, R.L. and Davenport, J.M. (1980). Approximations of the critical region of the Friedman statistic. <em>Comm. Stat. Theor. Meth.</em> <b>A9(6).</b> 571-595.
</p>
<p>Odeh, R.E. (1977). Extended tables of the distribution of Friedman's S-statistic in the two-way layout. <em>Commun. Statist.-Simula. Computa.</em> <b>B6(1).</b> 29-48.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
pFriedman(2, r=5, N=10)
pFriedman(c(.8,3.5,9.3), r=5, N=10) ## approximately 5% 50% and 95%
sFriedman(r=5, N=10)
plot(function(x)dFriedman(x, r=5, N=10),0,10)


</code></pre>

<hr>
<h2 id='ghyper'>Generalized hypergeometric distributions</h2><span id='topic+dghyper'></span><span id='topic+pghyper'></span><span id='topic+qghyper'></span><span id='topic+rghyper'></span><span id='topic+sghyper'></span><span id='topic+tghyper'></span><span id='topic+Generalized+20hypergeometric'></span><span id='topic+Negative+20hypergeometric'></span><span id='topic+Inverse+20hypergeometric'></span><span id='topic+Hypergeometric+20waiting+20time'></span><span id='topic+Beta-binomial'></span><span id='topic+Beta-negative-binomial'></span><span id='topic+Beta-Pascal'></span><span id='topic+Generalized+20Waring'></span>

<h3>Description</h3>

<p>Density, distribution function, quantile function, random generator and summary function for generalized hypergeometric distributions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dghyper(x, a, k, N, log=FALSE)
pghyper(q, a, k, N, lower.tail=TRUE, log.p=FALSE)
qghyper(p, a, k, N, lower.tail=TRUE, log.p=FALSE)
rghyper(n, a, k, N)
sghyper(a, k, N)
tghyper(a, k, N) ## scalar arguments only
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="ghyper_+3A_x">x</code>, <code id="ghyper_+3A_q">q</code>, <code id="ghyper_+3A_n">n</code></td>
<td>
<p>vector of non-negative <b>integer</b> quantities</p>
</td></tr>
<tr><td><code id="ghyper_+3A_p">p</code></td>
<td>
<p>vector of probabilities</p>
</td></tr>
<tr><td><code id="ghyper_+3A_a">a</code></td>
<td>
<p>vector of real values giving the first column total</p>
</td></tr>
<tr><td><code id="ghyper_+3A_k">k</code></td>
<td>
<p>vector  of real values giving the first row total</p>
</td></tr>
<tr><td><code id="ghyper_+3A_n">N</code></td>
<td>
<p>vector of real values giving the grand total</p>
</td></tr>
<tr><td><code id="ghyper_+3A_log">log</code>, <code id="ghyper_+3A_log.p">log.p</code></td>
<td>
<p>logical vector; if TRUE, probabilities p are given as log(p)</p>
</td></tr>
<tr><td><code id="ghyper_+3A_lower.tail">lower.tail</code></td>
<td>
<p>logical vector; if TRUE (default), probabilities are <code class="reqn">P[X &lt;= x]</code>, otherwise, <code class="reqn">P[X &gt; x]</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>The basic representation is in terms of a two-way table:
</p>

<table>
<tr>
 <td style="text-align: center;">
x </td><td style="text-align: center;">  k-x </td><td style="text-align: center;">  k</td>
</tr>
<tr>
 <td style="text-align: center;">
a-x </td><td style="text-align: center;">  b-k+x </td><td style="text-align: center;">  N-k</td>
</tr>
<tr>
 <td style="text-align: center;">
a </td><td style="text-align: center;">     b </td><td style="text-align: center;">  N</td>
</tr>

</table>

<p>and the associated hypergeometric probability <code class="reqn">P(x)=C_x^a C_{k-x}^b / C_k^N</code>.
</p>
<p>The table is constrained so that rows and columns add to the margins.  In all cases x is an integer or zero, but meaningful probability distributions occur when the other parameters are real. Johnson, Kotz and Kemp (1992) give a general discussion.
</p>
<p>Kemp and Kemp (1956) classify the possible probability distributions that can occur when real values are allowed, into eight types. The <em>classic</em> hypergeometric with integer values forms a ninth type.  Five of the eight types correspond to known distributions used in various contexts.  Three of the eight types, appear to have no practical applications, but for completeness they have been implemented.  
</p>
<p>The Kemp and Kemp types are defined in terms of the ranges of the a, k, and N parameters and are given in <code><a href="#topic+ghyper.types">ghyper.types</a></code>. The function <code>tghyper()</code> will give details for specific values of a, k, and N.
</p>
<p>These distributions apply to many important problems, which has lead to a variety of names:
</p>
<p>The Kemp and Kemp types IIA and IIIA are known as:
</p>

<ul>
<li><p> Negative hypergeometric
</p>
</li>
<li><p>	Inverse hypergeometric
</p>
</li>
<li><p>	Hypergeometric waiting time
</p>
</li>
<li><p>	Beta-binomial
</p>
</li></ul>

<p>The advantages of the conditional argument are considerable. Consider a few examples:
</p>

<ol>
<li><p> Future event: Consider two events which have occurred u and v times respectively.  The distribution function of x occurrences of the first event in a sample of k new trials is calculated.  Here a = -u-1, and N = -u-v-2. 
</p>
<p>Example: Suppose Toronto has won 3 games and Atlanta 1 in the World Series. What is the probability that Toronto will win the series by taking 2 or more of the remaining 3 games?
</p>
</li>
<li><p> Exceedance: Consider two samples of size m and k, then the distribution function of x, the number of elements out of k which exceed the r th largest element in the size m sample is calculated.  Here a = -r, and N = -m-1. 
</p>
<p>Example: Suppose that only once in the last century has the high-water mark at the St. Joe bridge exceeded 12 feet, what is the probability that it will not do so in the next ten years?
</p>
</li>
<li><p> Waiting time: Consider an urn with T balls, m of which are white, and that drawing without replacement is continued until w white balls are obtained, then the distribution function of x, the number of balls in excess of w that must be drawn is desired.  Here a = -w , N = -m-1, and k = T - m. 
</p>
<p>Example: Suppose a lot of 100 contains 5 defectives. What is the mean number of items that must be inspected before a defective item is found?
</p>
</li>
<li><p> Mixture: Suppose x has a binomial distribution with parameter p, and number of trials k.  Suppose that p is not fixed, but itself distributed like a beta variable with parameters A and B, then the distribution of x is calculated with a = -A and N = -A -B.
</p>
</li></ol>

<p>Names for Kemp and Kemp type IV are:
</p>

<ul>
<li><p>	Beta-negative-binomial
</p>
</li>
<li><p>	Beta-Pascal
</p>
</li>
<li><p>	Generalized Waring 
</p>
</li></ul>

<p>One application is accidents:
</p>
<p>Suppose accidents follow a Poisson distribution with mean L, and suppose L varies with individuals according to accident proneness, m.  In particular, suppose L follows a gamma distribution with parameter r and scale factor m , and that the scale factor n itself follows a beta distribution with parameters A and B, then the distribution of accidents, x, is beta-negative-binomial with a = -B, k = -r , and N = A -1.  See Xekalki (1983) for a discussion of this as well as a discussion of accident models for proneness, contagion and spells.
</p>


<h3>Value</h3>

<p>The output values conform to the output from other such functions in R. <code>dghyper()</code> gives the density, <code>pghyper()</code> the distribution function and <code>qghyper()</code> its inverse. <code>rghyper()</code> generates random numbers. <code>sghyper()</code> produces a list containing parameters corresponding to the arguments &ndash; mean, median, mode, variance, sd, third central moment, fourth central moment, Pearson's skewness, skewness, and kurtosis.
<br />
The function <code>tghyper()</code> returns the hypergeometric type and the range of values for x.	
</p>


<h3>Note</h3>

<p>The parameters of these functions differ from those of the
hypergeometric functions of R. To translate between the two use the
following as a model: phyper(x,m,n,k) = pghyper(x,k,m,m+n).
</p>


<h3>Author(s)</h3>

<p>Bob Wheeler 
</p>


<h3>References</h3>

<p>Johnson, N.L., Kotz, S. and Kemp, A. (1992) <em>Univariate discrete distributions</em>. Wiley, N.Y.
</p>
<p>Kemp, C.D., and Kemp, A.W. (1956). Generalized hypergeometric distributions. <em>Jour. Roy. Statist. Soc. B.</em> <b>18.</b> 202-211.
</p>
<p>Xekalaki, E. (1983). The univariate generalized Waring distribution in relation to accident theory: proneness, spells or contagion. <em>Biometrics.</em> <b>39.</b>  887-895.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
tghyper(a=4, k=4, N=10) 		## classic
tghyper(a=4.1, k=5, N=10) 		## type IA(i) Real classic
tghyper(a=5, k=4.1, N=10) 		## type IA(ii) Real classic
tghyper(a=4.2, k=4.6, N=12.2) 		## type IB
tghyper(a=-5.1, k=10, N=-7) 		## type IIA
tghyper(a=-0.5, k=5.9, N=-0.7) 		## type IIB
tghyper(a=10, k=-5.1, N=-7) 		## type IIIA Negative hypergeometric
tghyper(a=5.9, k=-0.5, N=-0.7) 		## type IIIB
tghyper(a=-1, k=-1, N=5) 		## type IV Generalized Waring

sghyper(a=-1, k=-1, N=5)
plot(function(x)dghyper(x,a=-1,k=-1,N=5),0,5)

#Fisher's exact test: contingency table with rows (1,3),(3,1) 
pghyper(1,4,4,8)
pghyper(3,4,4,8,lower.tail=FALSE)



#Beta-binomial applications:

#Application examples:
tghyper(-4,3,-6)
pghyper(2,-4,3,-6,lower=FALSE)
pghyper(0,-2,10,-101)
sghyper(-1,95,-6)$Mean+1



</code></pre>

<hr>
<h2 id='ghyper.types'>Kemp and Kemp generalized hypergeometric types</h2><span id='topic+ghyper.types'></span>

<h3>Description</h3>

<p>Generalized hypergeometric types as given by Kemp and Kemp	
</p>


<h3>Two-way table</h3>

<p>The basic representation is in terms of a two-way table:
</p>

<table>
<tr>
 <td style="text-align: center;">
x </td><td style="text-align: center;">  k-x </td><td style="text-align: center;">  k </td>
</tr>
<tr>
 <td style="text-align: center;">
a-x </td><td style="text-align: center;">  b-k+x </td><td style="text-align: center;">  N-k </td>
</tr>
<tr>
 <td style="text-align: center;">
  a </td><td style="text-align: center;">     b </td><td style="text-align: center;">  N
</td>
</tr>

</table>

<p>and the associated hypergeometric probability <code class="reqn">P(x)=C_x^a C_{k-x}^b / C_k^N</code>.
</p>
<p>The types are classified according to ranges of a, k, and N.
</p>


<h3>Kemp and Kemp types</h3>

<p>Minor modifications in the definition of three of the types have been made to avoid numerical difficulties. Note, J denotes a nonnegative integer.
</p>

<table>
<tr>
 <td style="text-align: left;">
[Classic] </td><td style="text-align: left;"> </td>
</tr>
<tr>
 <td style="text-align: left;">
	</td><td style="text-align: left;"> <code class="reqn">0&lt;a, 0&lt;N, 0&lt;k</code> </td>
</tr>
<tr>
 <td style="text-align: left;">
 	</td><td style="text-align: left;">	integers: a, N, k.</td>
</tr>
<tr>
 <td style="text-align: left;">
	</td><td style="text-align: left;">	<code class="reqn">max(0,a+k-N)  \le  x  \le  min(a,k)</code></td>
</tr>
<tr>
 <td style="text-align: left;">
[IA(i)] (Real classic) </td><td style="text-align: left;"> at least one noninteger parameter</td>
</tr>
<tr>
 <td style="text-align: left;">
	</td><td style="text-align: left;"> <code class="reqn">0&lt;a, 0&lt;N, 0&lt;k, k-1&lt;a&lt;N-(k-1)</code> </td>
</tr>
<tr>
 <td style="text-align: left;">
	</td><td style="text-align: left;">	integer: k </td>
</tr>
<tr>
 <td style="text-align: left;">
	</td><td style="text-align: left;"> <code class="reqn">	0  \le  x  \le  a</code></td>
</tr>
<tr>
 <td style="text-align: left;">

[IA(ii)] (Real classic)</td><td style="text-align: left;"> at least one noninteger parameter</td>
</tr>
<tr>
 <td style="text-align: left;">
	</td><td style="text-align: left;"> <code class="reqn">0&lt;a, 0&lt;N, 0&lt;k, a-1&lt;k&lt;N-(a-1)</code> </td>
</tr>
<tr>
 <td style="text-align: left;">
	</td><td style="text-align: left;"> integer: a </td>
</tr>
<tr>
 <td style="text-align: left;"> 
	</td><td style="text-align: left;"> <code class="reqn">0  \le  x  \le  a</code> </td>
</tr>
<tr>
 <td style="text-align: left;"> 
	</td><td style="text-align: left;"> Interchanging a and k transforms this to type IA(i) </td>
</tr>
<tr>
 <td style="text-align: left;">

[IB] </td><td style="text-align: left;"> </td>
</tr>
<tr>
 <td style="text-align: left;">
	</td><td style="text-align: left;"> <code class="reqn">0&lt;a, 0&lt;N, 0&lt;k, a+k-1&lt;N, J &lt; (a,k) &lt; J+1</code> </td>
</tr>
<tr>
 <td style="text-align: left;">
	</td><td style="text-align: left;"> integer: <code class="reqn">0 \le J</code> </td>
</tr>
<tr>
 <td style="text-align: left;">
	</td><td style="text-align: left;"> non-integer: a, k </td>
</tr>
<tr>
 <td style="text-align: left;">
	</td><td style="text-align: left;"> <code class="reqn">0  &lt;=  x  \dots </code></td>
</tr>
<tr>
 <td style="text-align: left;">
	
	</td><td style="text-align: left;"> NOTE: Kemp and Kemp specify <code class="reqn">-1&lt;N</code>. </td>
</tr>
<tr>
 <td style="text-align: left;">
	</td><td style="text-align: left;"> No practical applications for this distribution. </td>
</tr>
<tr>
 <td style="text-align: left;">
[IIA] (negative hypergeometric) </td><td style="text-align: left;"> </td>
</tr>
<tr>
 <td style="text-align: left;">
	</td><td style="text-align: left;"> <code class="reqn">a&lt;0, N&lt;a-1,0&lt;k</code> </td>
</tr>
<tr>
 <td style="text-align: left;">
	</td><td style="text-align: left;"> integer: k </td>
</tr>
<tr>
 <td style="text-align: left;">
	</td><td style="text-align: left;"> <code class="reqn">0  \le  x  \le  k</code> </td>
</tr>
<tr>
 <td style="text-align: left;">
	
	</td><td style="text-align: left;"> NOTE: Kemp and Kemp specify <code class="reqn">N&lt;a,  N \ne a-1</code> </td>
</tr>
<tr>
 <td style="text-align: left;">
[IIB] </td><td style="text-align: left;"> </td>
</tr>
<tr>
 <td style="text-align: left;">
	</td><td style="text-align: left;"> <code class="reqn">a&lt;0, -1&lt;N&lt;k+a-1, 0&lt;k, J &lt; (k,k+a-1-N) &lt; J+1</code> </td>
</tr>
<tr>
 <td style="text-align: left;">

	</td><td style="text-align: left;"> non-integer: k </td>
</tr>
<tr>
 <td style="text-align: left;">
	</td><td style="text-align: left;"> integer: <code class="reqn">0 \le J</code> </td>
</tr>
<tr>
 <td style="text-align: left;">
	</td><td style="text-align: left;"> <code class="reqn">0  \le  x  ....</code> </td>
</tr>
<tr>
 <td style="text-align: left;">

	</td><td style="text-align: left;"> This is a very strange distribution.  Special calculations were used.</td>
</tr>
<tr>
 <td style="text-align: left;">

	</td><td style="text-align: left;"> Note: No practical applications.</td>
</tr>
<tr>
 <td style="text-align: left;">
[IIIA] (negative hypergeometric) </td><td style="text-align: left;"> </td>
</tr>
<tr>
 <td style="text-align: left;">
	</td><td style="text-align: left;"> <code class="reqn">0&lt;a,N&lt;k-1,k&lt;0</code></td>
</tr>
<tr>
 <td style="text-align: left;">
	</td><td style="text-align: left;"> integer: a</td>
</tr>
<tr>
 <td style="text-align: left;">
	</td><td style="text-align: left;"> <code class="reqn">0 \le x \le a</code></td>
</tr>
<tr>
 <td style="text-align: left;">
	
	</td><td style="text-align: left;"> Interchanging a and k transforms this to type IIA </td>
</tr>
<tr>
 <td style="text-align: left;">

	</td><td style="text-align: left;"> NOTE: Kemp and Kemp specify <code class="reqn">N&lt;k, N \ne k-1</code> </td>
</tr>
<tr>
 <td style="text-align: left;">

[IIIB]</td><td style="text-align: left;"> </td>
</tr>
<tr>
 <td style="text-align: left;">
	</td><td style="text-align: left;">  <code class="reqn">0&lt;a,-1&lt;N&lt;a+k-1,k&lt;0, J&lt;(a,a+k-1-N)&lt;J+1</code> </td>
</tr>
<tr>
 <td style="text-align: left;">
	</td><td style="text-align: left;"> non integer: a</td>
</tr>
<tr>
 <td style="text-align: left;">
	</td><td style="text-align: left;"> integer: <code class="reqn">0 \le J</code></td>
</tr>
<tr>
 <td style="text-align: left;">
	</td><td style="text-align: left;"> <code class="reqn">0 \le x \dots </code></td>
</tr>
<tr>
 <td style="text-align: left;">

	</td><td style="text-align: left;"> Interchanging a and k transforms this to type IIB </td>
</tr>
<tr>
 <td style="text-align: left;">
	</td><td style="text-align: left;"> Note: No practical applications</td>
</tr>
<tr>
 <td style="text-align: left;">

[IV] (Generalized Waring) </td><td style="text-align: left;"> </td>
</tr>
<tr>
 <td style="text-align: left;">
	</td><td style="text-align: left;"> <code class="reqn">a&lt;0,-1&lt;N, k&lt;0</code></td>
</tr>
<tr>
 <td style="text-align: left;">
	</td><td style="text-align: left;"> <code class="reqn">0 \le x \dots</code>

</td>
</tr>

</table>



<h3>Author(s)</h3>

<p>Bob Wheeler 
</p>


<h3>References</h3>

<p>Kemp, C.D., and Kemp, A.W. (1956). Generalized hypergeometric distributions. <em>Jour. Roy. Statist. Soc. B.</em> <b>18.</b> 202-211.
<b>39.</b>  887-895.
</p>

<hr>
<h2 id='invGauss'>The inverse Gaussian and Wald distributions</h2><span id='topic+inverse+20Gaussian'></span><span id='topic+Wald+20distribution'></span><span id='topic+dinvGauss'></span><span id='topic+pinvGauss'></span><span id='topic+qinvGauss'></span><span id='topic+rinvGauss'></span><span id='topic+sinvGauss'></span>

<h3>Description</h3>

<p>Density, distribution function, quantile function, random generator and summary function for the inverse Gaussian and Wald distributions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dinvGauss(x, nu, lambda, log=FALSE)
pinvGauss(q, nu, lambda, lower.tail=TRUE, log.p=FALSE)
qinvGauss(p, nu, lambda, lower.tail=TRUE, log.p=FALSE)
rinvGauss(n, nu, lambda)
sinvGauss(nu, lambda)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="invGauss_+3A_x">x</code>, <code id="invGauss_+3A_q">q</code></td>
<td>
<p>vector of non-negative quantities</p>
</td></tr>
<tr><td><code id="invGauss_+3A_p">p</code></td>
<td>
<p>vector of probabilities</p>
</td></tr>
<tr><td><code id="invGauss_+3A_n">n</code></td>
<td>
<p>vector of numbers of observations</p>
</td></tr>
<tr><td><code id="invGauss_+3A_nu">nu</code></td>
<td>
<p>vector real and non-negative parameter &ndash; the Wald distribution results when nu=1</p>
</td></tr>
<tr><td><code id="invGauss_+3A_lambda">lambda</code></td>
<td>
<p>vector real and non-negative parameter</p>
</td></tr>
<tr><td><code id="invGauss_+3A_log">log</code>, <code id="invGauss_+3A_log.p">log.p</code></td>
<td>
<p>logical vector; if TRUE, probabilities p are given as log(p)</p>
</td></tr>
<tr><td><code id="invGauss_+3A_lower.tail">lower.tail</code></td>
<td>
<p>logical vector; if TRUE (default), probabilities are <code class="reqn">P[X &lt;= x]</code>, otherwise, <code class="reqn">P[X &gt; x]</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Probability functions:
</p>
<p style="text-align: center;"><code class="reqn">f(x,\nu,\lambda)=\sqrt{\frac{\lambda}{2\pi x^3}}\exp\left[-\lambda\frac{(x-\nu)^2}{2\nu^2 x}\right] \mbox{-- the density}</code>
</p>

<p style="text-align: center;"><code class="reqn">F(x,\nu,\lambda)=\Phi\left[\sqrt{\frac{\lambda}{x}}\left(\frac{x}{\nu}-1\right)\right]+e^{2\lambda/\nu}\Phi\left[\sqrt{\frac{\lambda}{x}}\left(\frac{x}{\nu}+1\right)\right] \mbox{-- the distribution function}</code>
</p>

<p>where <code class="reqn">\Phi[]</code> is the standard normal distribution function.
</p>
<p>The calculations are accurate to at least seven significant figures over
an extended range - much larger than that of any existing tables. We
have tested them for <code class="reqn">\lambda / \nu = 10e-20</code>,
and <code class="reqn">\lambda/\nu=10e4</code>. Accessible tables are
those of Wassan and Roy (1969), which unfortunately, are sometimes good
to only two significant digits. Much better tables are available in an
expensive CRC Handbook (1989), which are accurate to at least 7 significant digits for <code class="reqn">\lambda/\nu \ge 0.02</code> to <code class="reqn">\lambda/\nu \le 4000</code>. 
</p>
<p>These are first passage time distributions of Brownian motion with positive drift. See Whitmore and Seshadri (1987) for a heuristic derivation. The Wald (1947) form represents the average sample number in sequential analysis. The distribution has a non-monotonic failure rate, and is of considerable interest in lifetime studies: Ckhhikara and Folks (1977). A general reference is Seshadri (1993). 
</p>
<p>This is an extremely difficult distribution to treat numerically, and it would not have been possible without some extraordinary contributions. An elegant derivation of the distribution function is to be found in Shuster (1968). The first such derivation seems to be that of Zigangirov (1962), which because of its inaccessibility, the author has not read. The method of generating random numbers is due to Michael, Schucany, and Haas (1976). The approximation of Whitmore and Yalovsky (1978) makes it possible to find starting values for inverting the distribution. All three papers are short, elegant, and non- trivial. 
</p>


<h3>Value</h3>

<p>The output values conform to the output from other such functions in R. <code>dinvGauss()</code> gives the density, <code>pinvGauss()</code> the distribution function and <code>qinvGauss()</code> its inverse. <code>rinvGauss()</code> generates random numbers. <code>sinvGauss()</code> produces a list containing parameters corresponding to the arguments &ndash; mean, median, mode, variance, sd, third cental moment, fourth central moment, Pearson's skewness, skewness, and kurtosis.
</p>


<h3>Author(s)</h3>

<p>Bob Wheeler 
</p>


<h3>References</h3>

<p>Ckhhikara, R.S. and Folks, J.L. (1977) The inverse Gaussian distribution as a lifetime model. <em>Technometrics.</em> <b>19-4.</b> 461-468. 
</p>
<p>CRC Handbook. (1989). <em>Percentile points of the inverse Gaussian distribution.</em> J.A. Koziol (ed.) Boca Raton, FL. 
</p>
<p>Michael, J.R., Schucany, W.R. and Haas, R.W. (1976). Generating random variates using transformations with multiple roots. <em>American Statistician.</em> <b>30-2.</b> 88-90. 
</p>
<p>Seshadri, V. (1993). <em>The inverse Gaussian distribution.</em> Clarendon, Oxford 
</p>
<p>Shuster, J. (1968). On the inverse Gaussian distribution function. Jour. <em>Am. Stat. Assoc.</em> <b>63.</b> 1514-1516. 
</p>
<p>Wasan, M.T. and Roy, L.K. (1969). Tables of inverse Gaussian percentage points. <em>Technometrics.</em> <b>11-3.</b> 591-604. 
</p>
<p>Wald, A. (1947). <em>Sequential analysis.</em> Wiley, NY 
</p>
<p>Whitmore, G.A. and Seshadri, V. (1987). A heuristic derivation of the inverse Gaussian distribution. <em>American Statistician.</em> <b>41-4.</b> 280-281. 
</p>
<p>Whitmore, G.A. and Yalovsky, M. (1978). A normalizing logarithmic transformation for inverse Gaussian random variables. <em>Technometrics.</em> <b>20-2.</b> 207-208. 
</p>
<p>Zigangirov, K.S. (1962). Expression for the Wald distribution in terms of normal distribution. <em>Radiotech.Electron.</em> <b>7.</b> 164-166. 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
pinvGauss(1, 1, 16)
pinvGauss(c(.65,1,1.45), 1, 16) ## approximately 5% 50% and 95%
pars&lt;-sinvGauss(1, 16)
plot(function(x)dinvGauss(x,1, 16),pars$Mean-3*pars$SD,pars$Mean+3*pars$SD)
</code></pre>

<hr>
<h2 id='Johnson'>The Johnson distributions</h2><span id='topic+dJohnson'></span><span id='topic+pJohnson'></span><span id='topic+qJohnson'></span><span id='topic+rJohnson'></span><span id='topic+sJohnson'></span><span id='topic+moments'></span><span id='topic+JohnsonFit'></span>

<h3>Description</h3>

<p>Density, distribution function, quantile function, random generator and summary function for the Johnson distributions.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dJohnson(x, parms, log=FALSE)
pJohnson(q, parms, lower.tail=TRUE, log.p=FALSE)
qJohnson(p, parms, lower.tail=TRUE, log.p=FALSE)
rJohnson(n, parms)
sJohnson(parms)
JohnsonFit(t,moment="quant") 
moments(x)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Johnson_+3A_x">x</code>, <code id="Johnson_+3A_q">q</code></td>
<td>
<p>vector of quantities</p>
</td></tr>
<tr><td><code id="Johnson_+3A_t">t</code></td>
<td>
<p>observation vector, t=x, or moment vector, t=[mean,m2,m3,m4]</p>
</td></tr>
<tr><td><code id="Johnson_+3A_p">p</code></td>
<td>
<p>vector of probabilities</p>
</td></tr>
<tr><td><code id="Johnson_+3A_n">n</code></td>
<td>
<p>vector of numbers of observations</p>
</td></tr>
<tr><td><code id="Johnson_+3A_parms">parms</code></td>
<td>
<p>list or list of lists each containing output of JohnsonFit()</p>
</td></tr>
<tr><td><code id="Johnson_+3A_moment">moment</code></td>
<td>
<p>character scalar specifying t: &quot;quant&quot; (default), or &quot;use,&quot; or &quot;find&quot;</p>
</td></tr>
<tr><td><code id="Johnson_+3A_log">log</code>, <code id="Johnson_+3A_log.p">log.p</code></td>
<td>
<p>logical vector; if TRUE, probabilities p are given as log(p)</p>
</td></tr>
<tr><td><code id="Johnson_+3A_lower.tail">lower.tail</code></td>
<td>
<p>logical vector; if TRUE (default), probabilities are <code class="reqn">P[X &lt;= x]</code>, otherwise, <code class="reqn">P[X &gt; x]</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>The Johnson system (Johnson 1949) is a very flexible system for describing statistical distributions. It is defined by
</p>
<p style="text-align: center;"><code class="reqn">z=\gamma+\delta \log{f(u)}, u=(x-\xi)/\lambda</code>
</p>

<p>and where <code class="reqn">f( )</code> has four possible forms:
</p>

<table>
<tr>
 <td style="text-align: left;">
	SL:</td><td style="text-align: left;"> <code class="reqn">f(u)=u</code> the log normal </td>
</tr>
<tr>
 <td style="text-align: left;">
	SU:</td><td style="text-align: left;"> <code class="reqn">f(u)=u+\sqrt{1+u^2}</code> an unbounded distribution</td>
</tr>
<tr>
 <td style="text-align: left;">
	SB:</td><td style="text-align: left;"> <code class="reqn">f(u)=u/(1-u)</code> a bounded distribution</td>
</tr>
<tr>
 <td style="text-align: left;">
	SN:</td><td style="text-align: left;"> <code class="reqn">\exp(u)</code> the normal
</td>
</tr>

</table>

<p>Estimation of the Johnson parameters may be done from quantiles. The procedure of Wheeler (1980) is used.
</p>
<p>They may also be estimated from the moments.  Applied Statistics algorithm 99, due to Hill, Hill, and Holder (1976) has been translated into C for this implementation.  
</p>


<h3>Value</h3>

<p>The output values conform to the output from other such functions in <span class="rlang"><b>R</b></span>. <code>dJohnson()</code> gives the density, <code>pJohnson()</code> the distribution function and <code>qJohnson()</code> its inverse. <code>rJohnson()</code> generates random numbers. <code>sJohnson()</code> produces a list containing parameters corresponding to the arguments &ndash; mean, median, mode, variance, sd, third cental moment, fourth central moment, Pearson's skewness, skewness, and kurtosis.
<br />
<code>moments()</code> calculates the moment statistics of x as a vector with elements (mu, sigma, skew, kurt), where mu is the mean of x, sigma the SD of x with divisor <code>length(x)</code>, skew is the skewness and kurt the kurtosis. 
<br />
<code>JohnsonFit()</code> outputs a list containing the Johnson parameters (gamma, delta, xi, lambda, type), where type is one of the Johnson types: &quot;SN&quot;, &quot;SL&quot;, &quot;SB&quot;, or &quot;SU&quot;. <code>JohnsonFit()</code> does this using 5 order statistics when moment=&quot;quant&quot;, when moment=&quot;find&quot; it does this by using the first four moments of t calculated by the function <code>moments()</code>, when moment=&quot;use&quot; it assumes that the vector t is [mean,m2,m3,m4], where mi is the ith moment about the mean. 
<br />
Fitting by moments is difficult numerically and often <code>JohnsonFit()</code> will report an error.
</p>


<h3>Author(s)</h3>

<p>Bob Wheeler 
</p>


<h3>References</h3>

<p>Hill, I.D., Hill, R., and Holder, R.L. (1976). Fitting Johnson curves by moments. <em>Applied Statistics.</em> AS99.
</p>
<p>Johnson, N.L. (1949). Systems of frequency curves generated by methods of translation. <em>Biometrika,</em> <b>36.</b> 149-176.
</p>
<p>Wheeler, R.E. (1980). Quantile estimators of Johnson curve parameters. <em>Biometrika.</em> <b>67-3</b> 725-728
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
xx&lt;-rnorm(500)
parms&lt;-JohnsonFit(xx)
sJohnson(parms)
plot(function(xx)dJohnson(xx,parms),-2,2)
pJohnson(1,parms)
parms2&lt;-JohnsonFit(rexp(50))
qJohnson(p=0.5,list(parms,parms2))

## JohnsonFit with moment="find" and moment="use" is not always possible,
## and even when possible, may produce odd results.
## parms&lt;-JohnsonFit(x,moment="find")

parms&lt;-JohnsonFit(c(0,1,-.5,4),moment="use")

sJohnson(parms) 

# Fit illustration
data(cars)
xx&lt;-cars$speed
parms&lt;-JohnsonFit(xx)
hist(xx,freq=FALSE)
plot(function(x)dJohnson(x,parms),0,25,add=TRUE)


</code></pre>

<hr>
<h2 id='Kendall'>The distribution of Kendall's tau</h2><span id='topic+dKendall'></span><span id='topic+pKendall'></span><span id='topic+qKendall'></span><span id='topic+rKendall'></span><span id='topic+sKendall'></span>

<h3>Description</h3>

<p>Density, distribution function, quantile function, random generator and summary function for Kendall's tau.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dKendall(x, N, log=FALSE)
pKendall(q, N, lower.tail=TRUE, log.p=FALSE)
qKendall(p, N, lower.tail=TRUE, log.p=FALSE)
rKendall(n, N)
sKendall(N)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Kendall_+3A_x">x</code>, <code id="Kendall_+3A_q">q</code></td>
<td>
<p>vector of non-negative quantities</p>
</td></tr>
<tr><td><code id="Kendall_+3A_p">p</code></td>
<td>
<p>vector of probabilities</p>
</td></tr>
<tr><td><code id="Kendall_+3A_n">n</code></td>
<td>
<p>number of values to generate. If n is a vector, length(n) values will be generated</p>
</td></tr>
<tr><td><code id="Kendall_+3A_n">N</code></td>
<td>
<p>vector number of treatments</p>
</td></tr>
<tr><td><code id="Kendall_+3A_log">log</code>, <code id="Kendall_+3A_log.p">log.p</code></td>
<td>
<p>logical vector; if TRUE, probabilities p are given as log(p)</p>
</td></tr>
<tr><td><code id="Kendall_+3A_lower.tail">lower.tail</code></td>
<td>
<p>logical vector; if TRUE (default), probabilities are <code class="reqn">P[X &lt;= x]</code>, otherwise, <code class="reqn">P[X &gt; x]</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>There are two categories with N treatments each.  The treatments are ranked for each category, and then sorted according to the ranks for the first category.  This produces a 2 by N array in which the numbers in the first row are increasing from 1 to N.  The array is scanned, and every time two adjacent ranks in the second row are not in order, they are exchanged.  The scanning is repeated until the second row is in increasing order.  Let s denote the number of exchanges, then Kendall's tau is given by
</p>
<p style="text-align: center;"><code class="reqn">\tau=1-\frac{4s}{N(N-1)}</code>
</p>

<p>This too is a product-moment correlation coefficient.  See Kendall (1975), Chapter 2.  Other methods for calculating the statistic are also discussed there.
</p>
<p>The calculated values are exact for <code class="reqn">N &lt; 13</code>, thereafter an Edgeworth expansion is used.
</p>


<h3>Value</h3>

<p>The output values conform to the output from other such functions in <span class="rlang"><b>R</b></span>. <code>dKendall()</code> gives the density, <code>pKendall()</code> the distribution function and <code>qKendall()</code> its inverse. <code>rKendall()</code> generates random numbers. <code>sKendall()</code> produces a list containing parameters corresponding to the arguments &ndash; mean, median, mode, variance, sd, third cental moment, fourth central moment, Pearson's skewness, skewness, and kurtosis.
</p>


<h3>Author(s)</h3>

<p>Bob Wheeler 
</p>


<h3>References</h3>

<p>Kendall, M. (1975). <em>Rank Correlation Methods.</em> Griffin, London.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
pKendall(0, N=10)
pKendall(c(-.42,0.02,.42), N=10) ## approximately 5% 50% and 95% 
qKendall(.95,N=c(10,20))
sKendall(N=10)
plot(function(x)dKendall(x, N=10),-0.5,0.5)


</code></pre>

<hr>
<h2 id='KruskalWallis'>Kruskall-Wallis distribution</h2><span id='topic+Kruskal'></span><span id='topic+KruskalWallis'></span><span id='topic+dKruskalWallis'></span><span id='topic+pKruskalWallis'></span><span id='topic+qKruskalWallis'></span><span id='topic+rKruskalWallis'></span><span id='topic+sKruskalWallis'></span>

<h3>Description</h3>

<p>Density, distribution function, quantile function, random generator and summary function for the  Kruskal-Wallis test. 
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dKruskalWallis(x, c, N, U, log=FALSE)
pKruskalWallis(q, c, N, U, lower.tail=TRUE, log.p=FALSE)
qKruskalWallis(p, c, N, U, lower.tail=TRUE, log.p=FALSE)
rKruskalWallis(n, c, N, U)
sKruskalWallis(c, N, U)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="KruskalWallis_+3A_x">x</code>, <code id="KruskalWallis_+3A_q">q</code></td>
<td>
<p>vector of non-negative quantities</p>
</td></tr>
<tr><td><code id="KruskalWallis_+3A_p">p</code></td>
<td>
<p>vector of probabilities</p>
</td></tr>
<tr><td><code id="KruskalWallis_+3A_n">n</code></td>
<td>
<p>number of values to generate. If n is a vector, length(n) values will be generated</p>
</td></tr>
<tr><td><code id="KruskalWallis_+3A_c">c</code></td>
<td>
<p>vector number of treatments</p>
</td></tr>
<tr><td><code id="KruskalWallis_+3A_n">N</code></td>
<td>
<p>vector total number of observations</p>
</td></tr>
<tr><td><code id="KruskalWallis_+3A_u">U</code></td>
<td>
<p>vector sum of reciprocals of the number of the c sample sizes</p>
</td></tr>
<tr><td><code id="KruskalWallis_+3A_log">log</code>, <code id="KruskalWallis_+3A_log.p">log.p</code></td>
<td>
<p>logical vector; if TRUE, probabilities p are given as log(p)</p>
</td></tr>
<tr><td><code id="KruskalWallis_+3A_lower.tail">lower.tail</code></td>
<td>
<p>logical vector; if TRUE (default), probabilities are <code class="reqn">P[X &lt;= x]</code>, otherwise, <code class="reqn">P[X &gt; x]</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is a one-way layout with, perhaps, unequal sample sizes for each treatment. There are c treatments with sample sizes <code class="reqn">n_j, j=1 \dots c</code>. The total sample size is <code class="reqn">N=\sum_1^c n_j</code>. The distribution depends on c, N, and U, where <code class="reqn">U=\sum_1^c (1/n_j)</code>. 
</p>
<p>Let <code class="reqn">R_j</code> be the sum of the ranks for treatment <code class="reqn">j (j=1\dots c)</code> then the Kruskal-Wallis statistic is
</p>
<p style="text-align: center;"><code class="reqn">x=\frac{12}{N(N+1)}\sum_{j=1}^{c}\frac{R_j^2}{n_j} - 3(N+1)</code>
</p>

<p>This is asymptotically equivalent to a chi-squared variable with c-1 degrees of freedom.
</p>
<p>The original paper is Kruskal and Wallis (1952) with errata appearing in Kruskal and Wallis (1953).  No attempt is made to calculate exact values, rather an incomplete beta approximation is used following Wallace (1959).
</p>


<h3>Value</h3>

<p>The output values conform to the output from other such functions in <span class="rlang"><b>R</b></span>. <code>dKruskalWallis()</code> gives the density, <code>pKruskalWallis()</code> the distribution function and <code>qKruskalWallis()</code> its inverse. <code>rKruskalWallis()</code> generates random numbers. <code>sKruskalWallis()</code> produces a list containing parameters corresponding to the arguments &ndash; mean, median, mode, variance, sd, third cental moment, fourth central moment, Pearson's skewness, skewness, and kurtosis. 
</p>


<h3>Author(s)</h3>

<p>Bob Wheeler 
</p>


<h3>References</h3>

<p>Kruskal, W.H. and Wallis, W.A. (1952) Use of ranks in one-criterion variance analysis. <em>Jour. Am. Stat. Assoc.</em> <b>47.</b> 583-634
</p>
<p>Kruskal, W.H. and Wallis, W.A. (1953) Errata to Use of ranks in one-criterion variance analysis. <em>Jour. Am. Stat. Assoc.</em> <b>48.</b> 907-911.
</p>
<p>Wallace, D.L. (1959). Simplified beta-approximations to the Kruskal-Wallis H test. <em>Jour. Am. Stat. Assoc.</em> <b>54.</b> 225-230.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# Assuming three treatments, each with a sample size of 5.
pKruskalWallis(1, 3, 15, 0.6)
pKruskalWallis(c(.1,1.5,5.7), 3, 15, 0.6) ## approximately 5% 50% and 95%
sKruskalWallis(3, 15, 0.6)
plot(function(x)dKruskalWallis(x, 3, 15, 0.6),0.5,8)

</code></pre>

<hr>
<h2 id='maxFratio'>The maximum F-ratio distribution</h2><span id='topic+maxFratio'></span><span id='topic+dmaxFratio'></span><span id='topic+pmaxFratio'></span><span id='topic+qmaxFratio'></span><span id='topic+rmaxFratio'></span><span id='topic+smaxFratio'></span>

<h3>Description</h3>

<p>Density, distribution function, quantile function, random generator and summary function for the maximum F-ratio.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dmaxFratio(x, df, k, log=FALSE)
pmaxFratio(q, df, k, lower.tail=TRUE, log.p=FALSE)
qmaxFratio(p, df, k, lower.tail=TRUE, log.p=FALSE)
rmaxFratio(n, df, k)
smaxFratio(df, k)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="maxFratio_+3A_x">x</code>, <code id="maxFratio_+3A_q">q</code></td>
<td>
<p>vector of non-negative quantities</p>
</td></tr>
<tr><td><code id="maxFratio_+3A_p">p</code></td>
<td>
<p>vector of probabilities</p>
</td></tr>
<tr><td><code id="maxFratio_+3A_n">n</code></td>
<td>
<p>number of values to generate. If n is a vector, length(n) values will be generated</p>
</td></tr>
<tr><td><code id="maxFratio_+3A_df">df</code></td>
<td>
<p>vector non-negative, integer degrees of freedom</p>
</td></tr>
<tr><td><code id="maxFratio_+3A_k">k</code></td>
<td>
<p>vector non-negative, integer number of mean squares</p>
</td></tr>
<tr><td><code id="maxFratio_+3A_log">log</code>, <code id="maxFratio_+3A_log.p">log.p</code></td>
<td>
<p>logical vector; if TRUE, probabilities p are given as log(p)</p>
</td></tr>
<tr><td><code id="maxFratio_+3A_lower.tail">lower.tail</code></td>
<td>
<p>logical vector; if TRUE (default), probabilities are <code class="reqn">P[X &lt;= x]</code>, otherwise, <code class="reqn">P[X &gt; x]</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>The maximum F-ratio is the ratio of the largest to the smallest of k independent mean squares, all with the same df. The usual use is to test for homogeneity of normal variances.
</p>


<h3>Value</h3>

<p>The output values conform to the output from other such functions in <span class="rlang"><b>R</b></span>. <code>dmaxFratio()</code> gives the density, <code>pmaxFratio()</code> the distribution function and qmaxFratio its inverse. <code>rmaxFratio()</code> generates random numbers. <code>smaxFratio()</code> produces a list containing parameters corresponding to the arguments &ndash; mean, median, mode, variance, sd, third cental moment, fourth central moment, Pearson's skewness, skewness, and kurtosis.
</p>


<h3>Limitations</h3>

<p>The literature contains no information on numerical procedures for this distribution, with the result that all calculations are slow.
</p>
<p>Finding p from x should give results for almost any values of df and k &ndash; of course absolutely enormous values will take a while.
</p>
<p>Finding x from p is an iterative calculation dependent on a good starting guess.  Such good guesses have been made for <code class="reqn">df \le 24</code> and <code class="reqn">k \le 160</code>.  NA will be returned if larger values are attempted.
</p>


<h3>Note</h3>

<p>The maximum F-ratio was introduced by Hartley (1950) as a shortcut test of the homogeneity of variances from normal data.  Given a set of k mean squares, each based on the same number of degrees of freedom, df, the test statistic is the ratio of the largest to the smallest.  Several tables have been constructed.  The first by David, H.A. (1952).  Currently the most extensive are those by Nelson (1987).
</p>
<p>It is important to note that tests of this sort are substantially dependent on the assumption of normality, and can not be used robustly as can variance ratios in the analysis of variance.
</p>


<h3>Author(s)</h3>

<p>Bob Wheeler 
</p>


<h3>References</h3>

<p>Hartley, H.O. (1950) The maximum F-ratio as a short cut test for heterogeneity of variance. <em>Biometrika.</em> <b>37.</b> 308-312.
</p>
<p>David, H.A. (1952). Upper 5 and 1% points of the maximum F-ratio. <em>Biometrika.</em> <b>38.</b> 422-424.
</p>
<p>Nelson, L.S. (1987). Upper 10%, 5% and 1% points of the maximum F-ratio, <em>Jour. Qual. Tech.</em> <b>19-3.</b> 165-167.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
pmaxFratio(4, 10, 10)
pmaxFratio(c(2.3, 4, 8.5), 10, 10)	## approximately 5% 50% and 95% 
qmaxFratio(p=.95,df=c(10,20), k=10)
smaxFratio(10, 10) ## Wait for this, it may take a while
plot(function(x)dmaxFratio(x, 10, 10),1,10)

</code></pre>

<hr>
<h2 id='NormalScore'>Normal Scores distribution</h2><span id='topic+NormScore'></span><span id='topic+dNormScore'></span><span id='topic+pNormScore'></span><span id='topic+qNormScore'></span><span id='topic+rNormScore'></span><span id='topic+sNormScore'></span><span id='topic+normOrder'></span>

<h3>Description</h3>

<p>Density, distribution function, quantile function, random generator and summary function for the normal scores test. A function to calculate expected values of normal order statistics is included.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dNormScore(x, c, N, U, log=FALSE)
pNormScore(q, c, N, U, lower.tail=TRUE, log.p=FALSE)
qNormScore(p, c, N, U, lower.tail=TRUE, log.p=FALSE)
rNormScore(n, c, N, U)
sNormScore(c, N, U)
normOrder(N)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="NormalScore_+3A_x">x</code>, <code id="NormalScore_+3A_q">q</code></td>
<td>
<p>vector of non-negative quantities</p>
</td></tr>
<tr><td><code id="NormalScore_+3A_p">p</code></td>
<td>
<p>vector of probabilities</p>
</td></tr>
<tr><td><code id="NormalScore_+3A_n">n</code></td>
<td>
<p>number of values to generate. If n is a vector, length(n) values will be generated</p>
</td></tr>
<tr><td><code id="NormalScore_+3A_c">c</code></td>
<td>
<p>vector number of treatments</p>
</td></tr>
<tr><td><code id="NormalScore_+3A_n">N</code></td>
<td>
<p>vector total number of observations</p>
</td></tr>
<tr><td><code id="NormalScore_+3A_u">U</code></td>
<td>
<p>vector sum of reciprocals of the number of the c sample sizes</p>
</td></tr>
<tr><td><code id="NormalScore_+3A_log">log</code>, <code id="NormalScore_+3A_log.p">log.p</code></td>
<td>
<p>logical vector; if TRUE, probabilities p are given as log(p)</p>
</td></tr>
<tr><td><code id="NormalScore_+3A_lower.tail">lower.tail</code></td>
<td>
<p>logical vector; if TRUE (default), probabilities are <code class="reqn">P[X &lt;= x]</code>, otherwise, <code class="reqn">P[X &gt; x]</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>This is the Kruskal-Wallis statistic with ranks replaced by the expected values of normal order statistics. There are c treatments with sample sizes <code class="reqn">n_j, j=1 \dots c</code>. The total sample size is <code class="reqn">N=\sum_1^c n_j</code>. The distribution depends on c, N, and U, where <code class="reqn">U=\sum_1^c (1/n_j)</code>. 
</p>
<p>Let <code class="reqn">e_N(k)</code> be the expected value of the <code class="reqn">k_{th}</code> smallest observation in a sample of N independent normal variates. Rank all observations together, and let <code class="reqn">R_{ij}</code> denote the rank of observation <code class="reqn">X_{ij}</code>, <code class="reqn">i=1 \dots n_j</code> for treatment <code class="reqn">j=1 \dots c</code>, then the normal scores test statistic is
</p>
<p style="text-align: center;"><code class="reqn">x=(N-1)\frac{1}{\sum_{k=1}^{N} e_N(k)^2} \sum_{j=1}^{c}\frac{S_j^2}{n_j}</code>
</p>

<p>where <code class="reqn">S_j=\sum_{i=1}^{n_j}(e_N(R_{ij}))</code>.
</p>
<p>See Lu and Smith (1979) for a thorough discussion and some exact tables for small r and n.  The calculations made here use an incomplete beta approximation &ndash; the same one used for Kruskal-Wallis, only differing in the calculation of the variance of the statistic.
</p>
<p>The expected values of the normal order statistics use a modification of M.Maechler's C version of the Fortran algorithm given by Royston (1982). Spot checking the values against Harter (1969) confirms the accuracy to 4 decimal places as claimed by Royston.
</p>


<h3>Value</h3>

<p>The output values conform to the output from other such functions in <span class="rlang"><b>R</b></span>. <code>dNormScore()</code> gives the density, <code>pNormScore()</code> the distribution function and <code>qNormScore()</code> its inverse. <code>rNormScore()</code> generates random numbers. <code>sNormScore()</code> produces a list containing parameters corresponding to the arguments &ndash; mean, median, mode, variance, sd, third cental moment, fourth central moment, Pearson's skewness, skewness, and kurtosis. <code>normOrder()</code> gives the expected values of the normal order statistics for a sample of size N.
</p>


<h3>Author(s)</h3>

<p>Bob Wheeler 
</p>


<h3>References</h3>

<p>Harter, H.L. (1969). <em>Order statistics and their use in testing and estimation, volume 2.</em> U.S. Supp. of Doc. 
</p>
<p>Lu, H.T. and Smith, P.J. (1979) Distribution of normal scores statistic for nonparametric one-way analysis of variance. <em>Jour. Am Stat. Assoc.</em> <b>74.</b> 715-722.
</p>
<p>Royston, J.P. (1982). Expected normal order statistics (exact and approximate) AS 177. <em>Applied Statistics.</em> <b>31.</b> 161-165. 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
#Assuming three treatments, each with a sample size of 5
pNormScore(2, 3, 15, 0.6)
pNormScore(c(0.11,1.5,5.6), 3, 15, 0.6) ## approximately 5% 50% and 95%
sNormScore(3, 15, 0.6)
plot(function(x)dNormScore(x,c=5, N=15, U=0.6),0,5)

</code></pre>

<hr>
<h2 id='Pearson'>The Pearson product moment correlation coefficient</h2><span id='topic+Pearson'></span><span id='topic+dPearson'></span><span id='topic+pPearson'></span><span id='topic+qPearson'></span><span id='topic+rPearson'></span><span id='topic+sPearson'></span>

<h3>Description</h3>

<p>Density, distribution function, quantile function, random generator and summary function for the distribution of Pearson's product moment correlation.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dPearson(x, N, rho=0.0, log=FALSE)
pPearson(q, N, rho=0.0, lower.tail=TRUE, log.p=FALSE)
qPearson(p, N, rho=0.0, lower.tail=TRUE, log.p=FALSE)
rPearson(n, N, rho=0.0)
sPearson(N, rho=0.0)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Pearson_+3A_x">x</code>, <code id="Pearson_+3A_q">q</code></td>
<td>
<p>vector of sample correlations</p>
</td></tr>
<tr><td><code id="Pearson_+3A_p">p</code></td>
<td>
<p>vector of probabilities</p>
</td></tr>
<tr><td><code id="Pearson_+3A_rho">rho</code></td>
<td>
<p>vector of population correlations</p>
</td></tr>
<tr><td><code id="Pearson_+3A_n">N</code></td>
<td>
<p>vector of numbers of observations, <code class="reqn">(N &gt; 3)</code></p>
</td></tr>
<tr><td><code id="Pearson_+3A_n">n</code></td>
<td>
<p>number of values to generate. If n is a vector, length(n) values will be generated</p>
</td></tr>
<tr><td><code id="Pearson_+3A_log">log</code>, <code id="Pearson_+3A_log.p">log.p</code></td>
<td>
<p>logical vector; if TRUE, probabilities p are given as log(p)</p>
</td></tr>
<tr><td><code id="Pearson_+3A_lower.tail">lower.tail</code></td>
<td>
<p>logical vector; if TRUE (default), probabilities are <code class="reqn">P[R &lt;= r]</code>, otherwise, <code class="reqn">P[R &gt; r]</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>The output values conform to the output from other such functions in <span class="rlang"><b>R</b></span>. <code>dPearson()</code> gives the density, <code>pPearson()</code> the distribution function and <code>qPearson()</code> its inverse. <code>rPearson()</code> generates random numbers. <code>sPearson()</code> produces a list containing parameters corresponding to the arguments &ndash; mean, median, mode, variance, sd, third cental moment, fourth central moment, Pearson's skewness, skewness, and kurtosis.
</p>


<h3>Author(s)</h3>

<p>Bob Wheeler 
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
pPearson(0.5, N=10)
pPearson(q=0.5, N=10, rho=0.3) 
sPearson(N=10)
plot(function(x)dPearson(x,N=10,rho=0.7),-1,1)

</code></pre>

<hr>
<h2 id='Spearman'>Spearman's rho</h2><span id='topic+dSpearman'></span><span id='topic+pSpearman'></span><span id='topic+qSpearman'></span><span id='topic+rSpearman'></span><span id='topic+sSpearman'></span>

<h3>Description</h3>

<p>Density, distribution function, quantile function, random generator and summary function for Spearman's rho.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>dSpearman(x, r, log=FALSE)
pSpearman(q, r,  lower.tail=TRUE, log.p=FALSE)
qSpearman(p, r,  lower.tail=TRUE, log.p=FALSE)
rSpearman(n, r)
sSpearman(r)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Spearman_+3A_x">x</code>, <code id="Spearman_+3A_q">q</code></td>
<td>
<p>vector of non-negative quantities</p>
</td></tr>
<tr><td><code id="Spearman_+3A_p">p</code></td>
<td>
<p>vector of probabilities</p>
</td></tr>
<tr><td><code id="Spearman_+3A_n">n</code></td>
<td>
<p>number of values to generate. If n is a vector, length(n) values will be generated</p>
</td></tr>
<tr><td><code id="Spearman_+3A_r">r</code></td>
<td>
<p>(r &gt;= 3) vector of number of observations</p>
</td></tr>
<tr><td><code id="Spearman_+3A_log">log</code>, <code id="Spearman_+3A_log.p">log.p</code></td>
<td>
<p>logical vector; if TRUE, probabilities p are given as log(p)</p>
</td></tr>
<tr><td><code id="Spearman_+3A_lower.tail">lower.tail</code></td>
<td>
<p>logical vector; if TRUE (default), probabilities are <code class="reqn">P[X &lt;= x]</code>, otherwise, <code class="reqn">P[X &gt; x]</code></p>
</td></tr>
</table>


<h3>Details</h3>

<p>Spearman's rho is the rank correlation coefficient between r pairs of items. It ranges from -1 to 1. Denote by d, the sum of squares of the differences between the matched ranks, then x is given by:
</p>
<p style="text-align: center;"><code class="reqn">1-\frac{6d}{r(r^2-1)}</code>
</p>

<p>This is, in fact, the product-moment correlation coefficient of rank differences.  See Kendall (1975), Chapter 2. It is identical to Friedman's chi-squared for two treatments scaled to the -1, 1 range &ndash; if X is the Friedman statistic, then <code class="reqn">\rho=frac{X}{r-1)-1}</code>.
</p>
<p>Exact calculations are made for <code class="reqn">r \le 100</code>
</p>
<p>These exact calculations are made using the algorithm of Kendall and Smith (1939). 
</p>
<p>The incomplete beta, with continuity correction, is used for calculations outside this range.  
</p>


<h3>Value</h3>

<p>The output values conform to the output from other such functions in <span class="rlang"><b>R</b></span>. <code>dSpearman()</code> gives the density, <code>pSpearman()</code> the distribution function and <code>qSpearman()</code> its inverse. <code>rSpearman()</code> generates random numbers. <code>sSpearman()</code> produces a list containing parameters corresponding to the arguments &ndash; mean, median, mode, variance, sd, third cental moment, fourth central moment, Pearson's skewness, skewness, and kurtosis.
</p>


<h3>Author(s)</h3>

<p>Bob Wheeler 
</p>


<h3>References</h3>

<p>Kendall, M. (1975). <em>Rank Correlation Methods.</em> Griffin, London.
</p>
<p>Kendall, M. and Smith, B.B. (1939). The problem of m rankings. <em>Ann. Math. Stat.</em> <b>10.</b> 275-287.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
pSpearman(.95, 10)
pSpearman(c(-0.55,0,0.55), 10) ## approximately 5% 50% and 95% 
sSpearman(10)
plot(function(x)dSpearman(x, 10),-.9,.9)



</code></pre>

<hr>
<h2 id='SuppDists-defunct'>Defunct Functions in Package <span class="pkg">SuppDists</span></h2><span id='topic+SuppDists-defunct'></span><span id='topic+rziggurat'></span><span id='topic+rMWC1019'></span>

<h3>Description</h3>

<p>The functions listed here are no longer part of <span class="pkg">SuppDists</span> as
they do not work on 64bit architectures.
</p>


<h3>Usage</h3>

<pre><code class='language-R'># Defunct in version 1.1-9.5
rziggurat(n, normal=TRUE, new.start=FALSE, seed=556677)
rMWC1019(n, new.start=FALSE, seed=556677)
</code></pre>


<h3>Details</h3>

<p>Both RNG functions were developed for 32Bit architecture and do
not work correctly on 64Bit machines. Furthermore, the underlying
C++ code produces memory leaks as detected by <code>R CMD check --use-valgrind</code>.
</p>
<p><code>rziggurat</code> can be substituted with <code>zrnorm</code> of the package <span class="pkg">RcppZiggurat</span>.
</p>


<h3>See Also</h3>

<p><code><a href="base.html#topic+Defunct">Defunct</a></code>, <code><a href="RcppZiggurat.html#topic+zrnorm">zrnorm</a></code>
</p>

<hr>
<h2 id='SuppDists-internal'>Internal SuppDists function</h2><span id='topic+makeStatList'></span>

<h3>Description</h3>

<p>Internal SuppDists function
</p>


<h3>Usage</h3>

<pre><code class='language-R'>makeStatList(head, mn, med, var, mod, third, fourth, dig) 
</code></pre>


<h3>Details</h3>

<p>This is not to be called by the user.
</p>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
