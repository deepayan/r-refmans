<!DOCTYPE html><html><head><title>Help for package OOR</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {OOR}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#OOR'><p>Package OOR</p></a></li>
<li><a href='#plotStoSOO'><p>Plot 2-D <code>StoSOO</code> result</p></a></li>
<li><a href='#POO'><p>Parallel Optimistic Optimization</p></a></li>
<li><a href='#StoSOO'><p>StoSOO and SOO algorithms</p></a></li>
<li><a href='#Test functions'><p>Test functions of <code>x</code></p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Type:</td>
<td>Package</td>
</tr>
<tr>
<td>Title:</td>
<td>Optimistic Optimization in R</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.4</td>
</tr>
<tr>
<td>Date:</td>
<td>2023-08-22</td>
</tr>
<tr>
<td>Description:</td>
<td>Implementation of optimistic optimization methods for global optimization of deterministic or stochastic functions. The algorithms feature guarantees of the convergence to a global optimum. They require minimal assumptions on the (only local) smoothness, where the smoothness parameter does not need to be known. They are expected to be useful for the most difficult functions when we have no information on smoothness and the gradients are unknown or do not exist. Due to the weak assumptions, however, they can be mostly effective only in small dimensions, for example, for hyperparameter tuning.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/LGPL-2">LGPL-2</a> | <a href="https://www.r-project.org/Licenses/LGPL-2.1">LGPL-2.1</a> | <a href="https://www.r-project.org/Licenses/LGPL-3">LGPL-3</a> [expanded from: LGPL]</td>
</tr>
<tr>
<td>Depends:</td>
<td>methods</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/mbinois/OOR">https://github.com/mbinois/OOR</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/mbinois/OOR/issues">https://github.com/mbinois/OOR/issues</a></td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-08-22 08:23:56 UTC; mickael</td>
</tr>
<tr>
<td>Author:</td>
<td>M. Binois [cre, aut, trl] (R port),
  A. Carpentier [aut] (Matlab original),
  J.-B. Grill [aut] (Python original),
  R. Munos [aut] (Python and Matlab original),
  M. Valko [aut, ctb] (Python and Matlab original)</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>M. Binois &lt;mickael.binois@inria.fr&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-08-23 00:40:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='OOR'>Package OOR</h2><span id='topic+OOR'></span><span id='topic+OOR-package'></span>

<h3>Description</h3>

<p>This package implements optimistic optimization methods [1,2,3] for global optimization of deterministic or stochastic functions.
The algorithms feature guarantees of the convergence to a global optimum.
They require minimal assumptions on the (only local) smoothness, where the smoothness parameter does not need to be known.
They are expected to be useful for the most difficult functions when we have no information on smoothness and the gradients are unknown or do not exist.
Due to the weak assumptions, however, they can be mostly effective only in small dimensions, for example, for hyperparameter tuning [4].
</p>


<h3>Details</h3>

<p>Important functions: <br />
<code><a href="#topic+StoSOO">StoSOO</a></code> <br />
<code><a href="#topic+POO">POO</a></code> <br />
</p>


<h3>Note</h3>

<p>This package is based on the Matlab and Python implementations from the corresponding publications,
available from the following webpage: <a href="https://team.inria.fr/sequel/software/">https://team.inria.fr/sequel/software/</a>.
</p>


<h3>References</h3>

<p>[1] R. Munos (2011), Optimistic optimization of deterministic functions without the knowledge of its smoothness,
<em>NIPS</em>, 783-791. <br /> <br />
[2] M. Valko, A. Carpentier and R. Munos (2013), Stochastic Simultaneous Optimistic Optimization,
<em>ICML</em>, 19-27 <a href="https://inria.hal.science/hal-00789606">https://inria.hal.science/hal-00789606</a>. <br /> <br />
[3] J.-B. Grill, M. Valko and R. Munos (2015), Black-box optimization of noisy functions with unknown smoothness,
<em>NIPS</em>, 667-675 <a href="https://inria.hal.science/hal-01222915">https://inria.hal.science/hal-01222915</a>. <br /> <br />
[4] S. Samothrakis, D. Perz, S. Lucas (2013), Training gradient boosting machines using curve-fitting and information-theoretic features for causal direction detection,
<em>NIPS Workshop on Causality</em>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#------------------------------------------------------------
# Example 1 : Deterministic optimization with SOO
#------------------------------------------------------------
## Define objective
fun1 &lt;- function(x) return(-guirland(x))

## Optimization
Sol1 &lt;- StoSOO(par = NA, fn = fun1, nb_iter = 1000, control = list(type = "det", verbose = 1))

## Display objective function and solution fund
curve(fun1, n = 1001)
abline(v = Sol1$par, col = 'red')

#------------------------------------------------------------
# Example 2 : Stochastic optimization with StoSOO
#------------------------------------------------------------
set.seed(42)

## 2-dimensional noisy objective function, defined on [0, pi/4]^2
fun2 &lt;- function(x){return(-sin1(x[1]) * sin1(1 - x[2]) + runif(1, min = -0.05, max = 0.05))}

## Optimizing
Sol2 &lt;- StoSOO(par = rep(NA, 2), fn = fun2, upper = rep(pi/4, 2), nb_iter = 1000)

## Display solution
xgrid &lt;- seq(0, pi/4, length.out = 101)
Xgrid &lt;- expand.grid(xgrid, xgrid)
ref &lt;- apply(Xgrid, 1, function(x){(-sin1(x[1]) * sin1(1 - x[2]))})
filled.contour(xgrid, xgrid, matrix(ref, 101), color.palette  = terrain.colors,
plot.axes = {axis(1); axis(2); points(Xgrid[which.min(ref),, drop = FALSE], pch = 21);
             points(Sol2$par[1], Sol2$par[2], pch = 13)})

## Not run: 
#------------------------------------------------------------
# Example 3 : Stochastic optimization with POO
#------------------------------------------------------------
set.seed(10)
noise.level &lt;- 0.05

## Define and display objective
fun3 &lt;- function(x){return(double_sine(x) + runif(1, min = -noise.level, max = noise.level))}
xgrid &lt;- seq(0, 1, length.out = 1000)
plot(xgrid, sapply(xgrid, double_sine), type = 'l', ylab = "double_sine(x)", xlab = 'x')

## Maximization
Sol3 &lt;- POO(fun3, horizon = 1000, noise.level = noise.level)

## Display result
abline(v = Sol3$par)

## End(Not run)
</code></pre>

<hr>
<h2 id='plotStoSOO'>Plot 2-D <code><a href="#topic+StoSOO">StoSOO</a></code> result</h2><span id='topic+plotStoSOO'></span>

<h3>Description</h3>

<p>Plot bivariate tree structure obtained when running <code><a href="#topic+StoSOO">StoSOO</a></code>
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plotStoSOO(
  sol,
  lower = rep(0, length(sol$par)),
  upper = rep(1, length(sol$par)),
  levels = NULL,
  add = FALSE,
  cpch = ".",
  lcols = 1,
  ylim = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="plotStoSOO_+3A_sol">sol</code></td>
<td>
<p>outcome of running <code><a href="#topic+StoSOO">StoSOO</a></code>, with <code>control$light</code> set to <code>FALSE</code></p>
</td></tr>
<tr><td><code id="plotStoSOO_+3A_lower">lower</code>, <code id="plotStoSOO_+3A_upper">upper</code></td>
<td>
<p>vectors of bounds on the variables.</p>
</td></tr>
<tr><td><code id="plotStoSOO_+3A_levels">levels</code></td>
<td>
<p>which levels to print. Default to all levels</p>
</td></tr>
<tr><td><code id="plotStoSOO_+3A_add">add</code></td>
<td>
<p>if <code>TRUE</code>, use existing plot</p>
</td></tr>
<tr><td><code id="plotStoSOO_+3A_cpch">cpch</code></td>
<td>
<p><code><a href="graphics.html#topic+points">points</a></code> <code>pch</code> code for the centers</p>
</td></tr>
<tr><td><code id="plotStoSOO_+3A_lcols">lcols</code></td>
<td>
<p>color at each level, or a single color for all levels (default)</p>
</td></tr>
<tr><td><code id="plotStoSOO_+3A_ylim">ylim</code></td>
<td>
<p>vector of bounds, required in the 1d case</p>
</td></tr>
</table>


<h3>Examples</h3>

<pre><code class='language-R'>#------------------------------------------------------------
# Example 1 : Deterministic optimization with SOO, 1-dimensional function
#------------------------------------------------------------
## Define objective
fun1 &lt;- function(x) return(-guirland(x))

## Optimization
Sol1 &lt;- StoSOO(par = NA, fn = fun1, nb_iter = 1000, 
  control = list(type = "det", verbose = 1, light = FALSE))

## Display objective function and solution found
curve(fun1, n = 1001, ylim = c(-1.3, 0))
abline(v = Sol1$par, col = 'red')
plotStoSOO(Sol1, ylim = c(-1.3, -1.1), add = TRUE)

#------------------------------------------------------------
# Example 2 : Deterministic optimization with SOO, 2-dimensional function
#------------------------------------------------------------
set.seed(42)

## 2-dimensional noiseless objective function, defined on [0, pi/4]^2
fun &lt;- function(x){return(-sin1(x[1]) * sin1(1 - x[2]))}

## Optimizing
Sol &lt;- StoSOO(par = rep(NA, 2), fn = fun, upper = rep(pi/4, 2), nb_iter = 1000,
  control = list(type = 'det', light = FALSE))
  
## Display solution
plotStoSOO(Sol, upper = rep(pi/4, 2))
</code></pre>

<hr>
<h2 id='POO'>Parallel Optimistic Optimization</h2><span id='topic+POO'></span>

<h3>Description</h3>

<p>Global optimization of a blackbox function given a finite budget of noisy evaluations,
via the Parallel Optimistic Optimization algorithm.
The knowledge of the function's smoothness is not required.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>POO(f, horizon = 100, noise.level, rhomax = 20, nu = 1)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="POO_+3A_f">f</code></td>
<td>
<p>function to maximize.</p>
</td></tr>
<tr><td><code id="POO_+3A_horizon">horizon</code></td>
<td>
<p>maximum number of function evaluations.</p>
</td></tr>
<tr><td><code id="POO_+3A_noise.level">noise.level</code></td>
<td>
<p>scalar bound on the noise value.</p>
</td></tr>
<tr><td><code id="POO_+3A_rhomax">rhomax</code></td>
<td>
<p>number of equidistant <code>rho</code> values in [0,1], that are used by the corresponding HOO subroutines, see Details.</p>
</td></tr>
<tr><td><code id="POO_+3A_nu">nu</code></td>
<td>
<p>scalar (&gt; 0) assessing the complexity of the function, along with <code>rho</code> (see the near optimality definition in the reference below).</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Only 1-dimensional functions defined on [0, 1] are handled so far.
POO uses Hierarchical Optimistic Optimisation (HOO) as a subroutine, whose number is set by <code>rhomax</code>.
<code>POO</code> handles more difficult functions than <code><a href="#topic+StoSOO">StoSOO</a></code>.
</p>


<h3>Value</h3>

<p>Random point evaluated by the best HOO, in the form of a list with elements:
</p>

<ul>
<li><p> par parameter value at this point,
</p>
</li>
<li><p> value noisy value at <code>par</code>,
</p>
</li>
<li><p> best_rho best <code>rho</code> value.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>M. Binois (translation in R code), J.-B. Grill, M. Valko and R. Munos (Python code)
</p>


<h3>References</h3>

<p>J.-B. Grill, M. Valko and R. Munos (2015), Black-box optimization of noisy functions with unknown smoothness,
<em>NIPS</em>, 667-675 <a href="https://inria.hal.science/hal-01222915">https://inria.hal.science/hal-01222915</a>. Python code: <a href="https://team.inria.fr/sequel/software/POO/">https://team.inria.fr/sequel/software/POO/</a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
#------------------------------------------------------------
# Maximization with POO
#------------------------------------------------------------
set.seed(10)
noise.level &lt;- 0.05

## Define and display objective
ftest &lt;- function(x){return(double_sine(x) + runif(1, min = -noise.level, max = noise.level))}
xgrid &lt;- seq(0, 1, length.out = 1000)
plot(xgrid, sapply(xgrid, double_sine), type = 'l', ylab = "double_sine(x)", xlab = 'x')

## Optimization
Sol &lt;- POO(ftest, horizon = 1000, noise.level = noise.level)

## Display result
abline(v = Sol$par)

## End(Not run)
</code></pre>

<hr>
<h2 id='StoSOO'>StoSOO and SOO algorithms</h2><span id='topic+StoSOO'></span>

<h3>Description</h3>

<p>Global optimization of a blackbox function given a finite budget of noisy evaluations,
via the Stochastic-Simultaneous Optimistic Optimisation algorithm.
The deterministic-SOO method is available for noiseless observations.
The knowledge of the function's smoothness is not required.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>StoSOO(
  par,
  fn,
  ...,
  lower = rep(0, length(par)),
  upper = rep(1, length(par)),
  nb_iter,
  control = list(verbose = 0, type = "sto", max = FALSE, light = TRUE)
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="StoSOO_+3A_par">par</code></td>
<td>
<p>vector with length defining the dimensionality of the optimization problem.
Providing actual values of <code>par</code> is not necessary (<code>NA</code>s are just fine).
Included primarily for compatibility with <code><a href="stats.html#topic+optim">optim</a></code>.</p>
</td></tr>
<tr><td><code id="StoSOO_+3A_fn">fn</code></td>
<td>
<p>scalar function to be minimized, with first argument to be optimized over.</p>
</td></tr>
<tr><td><code id="StoSOO_+3A_...">...</code></td>
<td>
<p>optional additional arguments to <code>fn</code>.</p>
</td></tr>
<tr><td><code id="StoSOO_+3A_lower">lower</code>, <code id="StoSOO_+3A_upper">upper</code></td>
<td>
<p>vectors of bounds on the variables.</p>
</td></tr>
<tr><td><code id="StoSOO_+3A_nb_iter">nb_iter</code></td>
<td>
<p>number of function evaluations allocated to optimization.</p>
</td></tr>
<tr><td><code id="StoSOO_+3A_control">control</code></td>
<td>
<p>list of control parameters:
</p>

<ul>
<li><p> verbose: verbosity level, either <code>0</code> (default), <code>1</code> or greater than <code>1</code>,
</p>
</li>
<li><p> type: either '<code>det</code>' for optimizing a deterministic function or '<code>sto</code>' for a stochastic one,
</p>
</li>
<li><p> k_max: maximum number of evaluations per leaf (default: from analysis),
</p>
</li>
<li><p> h_max: maximum depth of the tree (default: from analysis),
</p>
</li>
<li><p> delta: confidence (default: <code>1/sqrt(nb_iter)</code> - from analysis),
</p>
</li>
<li><p> light: set to <code>FALSE</code> to return the search tree,
</p>
</li>
<li><p> max: if <code>TRUE</code>, performs maximization.
</p>
</li></ul>
</td></tr>
</table>


<h3>Details</h3>

<p>The optional <code>tree</code> element returned is a list, whose first element is the root node and the last element the deepest nodes.
A each level, <code>x</code> provides the center(s), one per row, whose corresponding bounds are given by <code>x_min</code> and <code>x_max</code>. Then:
</p>

<ul>
<li> <p><code>leaf</code> indicates if <code>x</code> is a leaf (1 if <code>TRUE</code>);
</p>
</li>
<li> <p><code>new</code> indicates if <code>x</code> has been sampled last;
</p>
</li>
<li> <p><code>sums</code> gives the sum of values at <code>x</code>;
</p>
</li>
<li> <p><code>bs</code> is for the upper bounds at <code>x</code>;
</p>
</li>
<li> <p><code>ks</code> is the number of evaluations at <code>x</code>;
</p>
</li>
<li> <p><code>values</code> stores the values evaluated as they come (mostly useful in the deterministic case)
</p>
</li></ul>



<h3>Value</h3>

<p>list with components:
</p>

<ul>
<li> <p><code>par</code> best set of parameters (for a stochastic function, it corresponds to the minimum reached over the deepest unexpanded node),
</p>
</li>
<li> <p><code>value</code> value of <code>fn</code> at <code>par</code>,
</p>
</li>
<li> <p><code>tree</code> search tree built during the execution, not returned unless <code>control$light == TRUE</code>.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>M. Binois (translation in R code), M. Valko, A. Carpentier, R. Munos (Matlab code)
</p>


<h3>References</h3>

<p>R. Munos (2011), Optimistic optimization of deterministic functions without the knowledge of its smoothness,
<em>NIPS</em>, 783-791. <br /> <br />
M. Valko, A. Carpentier and R. Munos (2013), Stochastic Simultaneous Optimistic Optimization,
<em>ICML</em>, 19-27 <a href="https://inria.hal.science/hal-00789606">https://inria.hal.science/hal-00789606</a>. Matlab code: <a href="https://team.inria.fr/sequel/software/StoSOO/">https://team.inria.fr/sequel/software/StoSOO/</a>. <br /> <br />
P. Preux, R. Munos, M. Valko (2014), Bandits attack function optimization, <em>IEEE Congress on Evolutionary Computation (CEC)</em>, 2245-2252.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>#------------------------------------------------------------
# Example 1 : Deterministic optimization with SOO
#------------------------------------------------------------
## Define objective
fun1 &lt;- function(x) return(-guirland(x))

## Optimization
Sol1 &lt;- StoSOO(par = NA, fn = fun1, nb_iter = 1000, control = list(type = "det", verbose = 1))

## Display objective function and solution found
curve(fun1, n = 1001)
abline(v = Sol1$par, col = 'red')

#------------------------------------------------------------
# Example 2 : Stochastic optimization with StoSOO
#------------------------------------------------------------
set.seed(42)

## Same objective function with uniform noise
fun2 &lt;- function(x){return(fun1(x) + runif(1, min = -0.1, max = 0.1))}

## Optimization
Sol2 &lt;- StoSOO(par = NA, fn = fun2, nb_iter = 1000, control = list(type = "sto", verbose = 1))

## Display solution
abline(v = Sol2$par, col = 'blue')

#------------------------------------------------------------
# Example 3 : Stochastic optimization with StoSOO, 2-dimensional function
#------------------------------------------------------------

set.seed(42)

## 2-dimensional noisy objective function, defined on [0, pi/4]^2
fun3 &lt;- function(x){return(-sin1(x[1]) * sin1(1 - x[2]) + runif(1, min = -0.05, max = 0.05))}

## Optimizing
Sol3 &lt;- StoSOO(par = rep(NA, 2), fn = fun3, upper = rep(pi/4, 2), nb_iter = 1000)

## Display solution
xgrid &lt;- seq(0, pi/4, length.out = 101)
Xgrid &lt;- expand.grid(xgrid, xgrid)
ref &lt;- apply(Xgrid, 1, function(x){(-sin1(x[1]) * sin1(1 - x[2]))})
filled.contour(xgrid, xgrid, matrix(ref, 101), color.palette  = terrain.colors,
plot.axes = {axis(1); axis(2); points(Xgrid[which.min(ref),, drop = FALSE], pch = 21);
             points(Sol3$par[1],Sol3$par[2], pch = 13)})


#------------------------------------------------------------
# Example 4 : Deterministic optimization with StoSOO, 2-dimensional function with plots
#------------------------------------------------------------
set.seed(42)

## 2-dimensional noiseless objective function, defined on [0, pi/4]^2
fun4 &lt;- function(x){return(-sin1(x[1]) * sin1(1 - x[2]))}

## Optimizing
Sol4 &lt;- StoSOO(par = rep(NA, 2), fn = fun4, upper = rep(pi/4, 2), nb_iter = 1000,
  control = list(type = 'det', light = FALSE))

## Display solution
xgrid &lt;- seq(0, pi/4, length.out = 101)
Xgrid &lt;- expand.grid(xgrid, xgrid)
ref &lt;- apply(Xgrid, 1, function(x){(-sin1(x[1]) * sin1(1 - x[2]))})
filled.contour(xgrid, xgrid, matrix(ref, 101), color.palette  = terrain.colors,
plot.axes = {axis(1); axis(2); plotStoSOO(Sol4, add = TRUE, upper = rep(pi/4, 2));
             points(Xgrid[which.min(ref),, drop = FALSE], pch = 21);
             points(Sol4$par[1], Sol4$par[2], pch = 13, col = 2)})
</code></pre>

<hr>
<h2 id='Test+20functions'>Test functions of <code>x</code></h2><span id='topic+Test+20functions'></span><span id='topic+guirland'></span><span id='topic+sin1'></span><span id='topic+difficult'></span><span id='topic+difficult2'></span><span id='topic+double_sine'></span>

<h3>Description</h3>

<p>Several test functions of varying complexity are available. They are defined on [0,1].
</p>


<h3>Usage</h3>

<pre><code class='language-R'>guirland(x)

sin1(x)

difficult(x)

difficult2(x)

double_sine(x, rho1 = 0.3, rho2 = 0.8, tmax = 0.5)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="Test+2B20functions_+3A_x">x</code></td>
<td>
<p>vector specifying the location where the function is to be evaluated.</p>
</td></tr>
<tr><td><code id="Test+2B20functions_+3A_rho1">rho1</code>, <code id="Test+2B20functions_+3A_rho2">rho2</code>, <code id="Test+2B20functions_+3A_tmax">tmax</code></td>
<td>
<p>additional parameters for double_sine.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>These test functions are translated from the Matlab and Python codes in the references.
</p>


<h3>References</h3>

<p>M. Valko, A. Carpentier and R. Munos (2013), Stochastic Simultaneous Optimistic Optimization,
<em>ICML</em>, 19-27 <a href="https://inria.hal.science/hal-00789606">https://inria.hal.science/hal-00789606</a>. Matlab code: <a href="https://team.inria.fr/sequel/software/StoSOO/">https://team.inria.fr/sequel/software/StoSOO/</a>. <br /> <br />
J.-B. Grill, M. Valko and R. Munos (2015), Black-box optimization of noisy functions with unknown smoothness,
<em>NIPS</em>, 667-675 <a href="https://inria.hal.science/hal-01222915">https://inria.hal.science/hal-01222915</a>. Python code: <a href="https://team.inria.fr/sequel/software/POO/">https://team.inria.fr/sequel/software/POO/</a>. <br /> <br />
</p>


<h3>Examples</h3>

<pre><code class='language-R'>par(mfrow = c(2,3))

curve(guirland, n = 501)
curve(sin1)
curve(difficult, xlim = c(1e-8, 1), n = 1001)
xgrid &lt;- seq(0, 1, length.out = 500)
plot(xgrid, sapply(xgrid, difficult2), type = 'l', ylab = "difficult2(x)")
plot(xgrid, sapply(xgrid, double_sine), type = 'l', ylab = "double_sine(x) (default)")
double_sine2 &lt;- function(x) double_sine(x, rho1 = 0.8, rho2 = 0.3)
plot(xgrid, sapply(xgrid, double_sine2), type = 'l', ylab = "double_sine(x) (modified)")

par(mfrow = c(1,1))
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
