<!DOCTYPE html><html><head><title>Help for package nlmixr2</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {nlmixr2}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#addCwres'><p>Add CWRES</p></a></li>
<li><a href='#addNpde'><p>NPDE calculation for nlmixr2</p></a></li>
<li><a href='#addTable'><p>Add table information to nlmixr2 fit object without tables</p></a></li>
<li><a href='#bobyqaControl'><p>Control for bobyqa estimation method in nlmixr2</p></a></li>
<li><a href='#bootplot'><p>Produce delta objective function for boostrap</p></a></li>
<li><a href='#bootstrapFit'><p>Bootstrap nlmixr2 fit</p></a></li>
<li><a href='#covarSearchAuto'><p>Stepwise Covariate Model-selection (SCM) method</p></a></li>
<li><a href='#foceiControl'><p>Control Options for FOCEi</p></a></li>
<li><a href='#lbfgsb3cControl'><p>Control for lbfgsb3c estimation method in nlmixr2</p></a></li>
<li><a href='#n1qn1Control'><p>Control for n1qn1 estimation method in nlmixr2</p></a></li>
<li><a href='#newuoaControl'><p>Control for newuoa estimation method in nlmixr2</p></a></li>
<li><a href='#nlmControl'><p>nlmixr2 defaults controls for nlm</p></a></li>
<li><a href='#nlmeControl'><p>Control Values for nlme Fit with extra options for nlmixr</p></a></li>
<li><a href='#nlminbControl'><p>nlmixr2 nlminb defaults</p></a></li>
<li><a href='#nlmixr2'><p>nlmixr2 fits population PK and PKPD non-linear mixed effects models.</p></a></li>
<li><a href='#nlmixr2CheckInstall'><p>Check your nlmixr2 installation for potential issues</p></a></li>
<li><a href='#nlsControl'><p>nlmixr2 defaults controls for nls</p></a></li>
<li><a href='#optimControl'><p>nlmixr2 optim defaults</p></a></li>
<li><a href='#preconditionFit'><p>Linearly re-parameterize the model to be less sensitive to rounding errors</p></a></li>
<li><a href='#reexports'><p>Objects exported from other packages</p></a></li>
<li><a href='#saemControl'><p>Control Options for SAEM</p></a></li>
<li><a href='#setOfv'><p>Set/get Objective function type for a nlmixr2 object</p></a></li>
<li><a href='#tableControl'><p>Output table/data.frame options</p></a></li>
<li><a href='#traceplot'><p>Produce trace-plot for fit if applicable</p></a></li>
<li><a href='#uobyqaControl'><p>Control for uobyqa estimation method in nlmixr2</p></a></li>
<li><a href='#vpcCens'><p>VPC based on ui model</p></a></li>
<li><a href='#vpcCensTad'><p>VPC based on ui model</p></a></li>
<li><a href='#vpcPlot'><p>VPC based on ui model</p></a></li>
<li><a href='#vpcPlotTad'><p>VPC based on ui model</p></a></li>
<li><a href='#vpcSim'><p>VPC simulation</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table>
<tr>
<td>Title:</td>
<td>Nonlinear Mixed Effects Models in Population PK/PD</td>
</tr>
<tr>
<td>Version:</td>
<td>2.1.2</td>
</tr>
<tr>
<td>Description:</td>
<td>Fit and compare nonlinear mixed-effects models in differential
    equations with flexible dosing information commonly seen in pharmacokinetics
    and pharmacodynamics (Almquist, Leander, and Jirstrand 2015
    &lt;<a href="https://doi.org/10.1007%2Fs10928-015-9409-1">doi:10.1007/s10928-015-9409-1</a>&gt;). Differential equation solving is
    by compiled C code provided in the 'rxode2' package
    (Wang, Hallow, and James 2015 &lt;<a href="https://doi.org/10.1002%2Fpsp4.12052">doi:10.1002/psp4.12052</a>&gt;).</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.3.1</td>
</tr>
<tr>
<td>Imports:</td>
<td>nlmixr2est (&ge; 2.2.2), nlmixr2extra (&ge; 2.0.10), rxode2 (&ge;
2.1.3), lotri (&ge; 0.4.3), nlmixr2plot (&ge; 2.0.8), magrittr,
crayon, cli</td>
</tr>
<tr>
<td>Depends:</td>
<td>nlmixr2data</td>
</tr>
<tr>
<td>Suggests:</td>
<td>rmarkdown, knitr, devtools, ggplot2, testthat, n1qn1,
rxode2parse (&ge; 2.0.19), withr</td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/nlmixr2/nlmixr2/issues/">https://github.com/nlmixr2/nlmixr2/issues/</a></td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://nlmixr2.org/">https://nlmixr2.org/</a>, <a href="https://github.com/nlmixr2/nlmixr2/">https://github.com/nlmixr2/nlmixr2/</a></td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>no</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2024-05-30 01:22:08 UTC; matt</td>
</tr>
<tr>
<td>Author:</td>
<td>Matthew Fidler <a href="https://orcid.org/0000-0001-8538-6691"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre],
  Yuan Xiong [ctb],
  Rik Schoemaker <a href="https://orcid.org/0000-0002-7538-3005"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [ctb],
  Justin Wilkins <a href="https://orcid.org/0000-0002-7099-9396"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [ctb],
  Wenping Wang [ctb],
  Mirjam Trame [ctb],
  Huijuan Xu [ctb],
  John Harrold [ctb],
  Bill Denney <a href="https://orcid.org/0000-0002-5759-428X"><img alt="ORCID iD"src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a> [ctb],
  Theodoros Papathanasiou [ctb],
  Teun Post [ctb],
  Richard Hooijmaijers [ctb]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Matthew Fidler &lt;matthew.fidler@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2024-05-30 03:50:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='addCwres'>Add CWRES</h2><span id='topic+addCwres'></span>

<h3>Description</h3>

<p>This returns a new fit object with CWRES attached
</p>


<h3>Usage</h3>

<pre><code class='language-R'>addCwres(fit, focei = TRUE, updateObject = TRUE, envir = parent.frame(1))
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="addCwres_+3A_fit">fit</code></td>
<td>
<p>nlmixr2 fit without WRES/CWRES</p>
</td></tr>
<tr><td><code id="addCwres_+3A_focei">focei</code></td>
<td>
<p>Boolean indicating if the focei objective function is
added.  If not the foce objective function is added.</p>
</td></tr>
<tr><td><code id="addCwres_+3A_updateobject">updateObject</code></td>
<td>
<p>Boolean indicating if the original fit object
should be updated. By default this is true.</p>
</td></tr>
<tr><td><code id="addCwres_+3A_envir">envir</code></td>
<td>
<p>Environment that should be checked for object to
update.  By default this is the global environment.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>fit with CWRES
</p>


<h3>Author(s)</h3>

<p>Matthew L. Fidler
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

one.cmt &lt;- function() {
  ini({
    ## You may label each parameter with a comment
    tka &lt;- 0.45 # Log Ka
    tcl &lt;- log(c(0, 2.7, 100)) # Log Cl
    ## This works with interactive models
    ## You may also label the preceding line with label("label text")
    tv &lt;- 3.45; label("log V")
    ## the label("Label name") works with all models
    eta.ka ~ 0.6
    eta.cl ~ 0.3
    eta.v ~ 0.1
    add.sd &lt;- 0.7
  })
  model({
    ka &lt;- exp(tka + eta.ka)
    cl &lt;- exp(tcl + eta.cl)
    v &lt;- exp(tv + eta.v)
    linCmt() ~ add(add.sd)
  })
}

f &lt;- try(nlmixr2(one.cmt, theo_sd, "saem"))

print(f)

# even though you may have forgotten to add the cwres, you can add it to the data.frame:

if (!inherits(f, "try-error")) {
  f &lt;- try(addCwres(f))
  print(f)
}

# Note this also adds the FOCEi objective function

</code></pre>

<hr>
<h2 id='addNpde'>NPDE calculation for nlmixr2</h2><span id='topic+addNpde'></span>

<h3>Description</h3>

<p>NPDE calculation for nlmixr2
</p>


<h3>Usage</h3>

<pre><code class='language-R'>addNpde(
  object,
  updateObject = TRUE,
  table = tableControl(),
  ...,
  envir = parent.frame(1)
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="addNpde_+3A_object">object</code></td>
<td>
<p>nlmixr2 fit object</p>
</td></tr>
<tr><td><code id="addNpde_+3A_updateobject">updateObject</code></td>
<td>
<p>Boolean indicating if original object should be updated.  By default this is TRUE.</p>
</td></tr>
<tr><td><code id="addNpde_+3A_table">table</code></td>
<td>
<p>'tableControl()' list of options</p>
</td></tr>
<tr><td><code id="addNpde_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <code><a href="nlmixr2est.html#topic+addNpde">nlmixr2est::addNpde()</a></code>.</p>
</td></tr>
<tr><td><code id="addNpde_+3A_envir">envir</code></td>
<td>
<p>Environment that should be checked for object to
update.  By default this is the global environment.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>New nlmixr2 fit object
</p>


<h3>Author(s)</h3>

<p>Matthew L. Fidler
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

one.cmt &lt;- function() {
  ini({
    ## You may label each parameter with a comment
    tka &lt;- 0.45 # Log Ka
    tcl &lt;- log(c(0, 2.7, 100)) # Log Cl
    ## This works with interactive models
    ## You may also label the preceding line with label("label text")
    tv &lt;- 3.45; label("log V")
    ## the label("Label name") works with all models
    eta.ka ~ 0.6
    eta.cl ~ 0.3
    eta.v ~ 0.1
    add.sd &lt;- 0.7
  })
  model({
    ka &lt;- exp(tka + eta.ka)
    cl &lt;- exp(tcl + eta.cl)
    v &lt;- exp(tv + eta.v)
    linCmt() ~ add(add.sd)
  })
}

f &lt;- nlmixr2(one.cmt, theo_sd, "saem")

# even though you may have forgotten to add the NPDE, you can add it to the data.frame:

f &lt;- addNpde(f)


</code></pre>

<hr>
<h2 id='addTable'>Add table information to nlmixr2 fit object without tables</h2><span id='topic+addTable'></span>

<h3>Description</h3>

<p>Add table information to nlmixr2 fit object without tables
</p>


<h3>Usage</h3>

<pre><code class='language-R'>addTable(
  object,
  updateObject = FALSE,
  data = object$dataSav,
  thetaEtaParameters = object$foceiThetaEtaParameters,
  table = tableControl(),
  keep = NULL,
  drop = NULL,
  envir = parent.frame(1)
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="addTable_+3A_object">object</code></td>
<td>
<p>nlmixr2 family of objects</p>
</td></tr>
<tr><td><code id="addTable_+3A_updateobject">updateObject</code></td>
<td>
<p>Update the object (default FALSE)</p>
</td></tr>
<tr><td><code id="addTable_+3A_data">data</code></td>
<td>
<p>Saved data from</p>
</td></tr>
<tr><td><code id="addTable_+3A_thetaetaparameters">thetaEtaParameters</code></td>
<td>
<p>Internal theta/eta parameters</p>
</td></tr>
<tr><td><code id="addTable_+3A_table">table</code></td>
<td>
<p>a 'tableControl()' list of options</p>
</td></tr>
<tr><td><code id="addTable_+3A_keep">keep</code></td>
<td>
<p>Character Vector of items to keep</p>
</td></tr>
<tr><td><code id="addTable_+3A_drop">drop</code></td>
<td>
<p>Character Vector of items to drop or NULL</p>
</td></tr>
<tr><td><code id="addTable_+3A_envir">envir</code></td>
<td>
<p>Environment to search for updating</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Fit with table information attached
</p>


<h3>Author(s)</h3>

<p>Matthew Fidler
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

one.cmt &lt;- function() {
  ini({
    ## You may label each parameter with a comment
    tka &lt;- 0.45 # Log Ka
    tcl &lt;- log(c(0, 2.7, 100)) # Log Cl
    ## This works with interactive models
    ## You may also label the preceding line with label("label text")
    tv &lt;- 3.45; label("log V")
    ## the label("Label name") works with all models
    eta.ka ~ 0.6
    eta.cl ~ 0.3
    eta.v ~ 0.1
    add.sd &lt;- 0.7
  })
  model({
    ka &lt;- exp(tka + eta.ka)
    cl &lt;- exp(tcl + eta.cl)
    v &lt;- exp(tv + eta.v)
    linCmt() ~ add(add.sd)
  })
}

# run without tables step
f &lt;- nlmixr2(one.cmt, theo_sd, "saem", control=list(calcTables=FALSE))

print(f)

# Now add the tables

f &lt;- addTable(f)

print(f)


</code></pre>

<hr>
<h2 id='bobyqaControl'>Control for bobyqa estimation method in nlmixr2</h2><span id='topic+bobyqaControl'></span>

<h3>Description</h3>

<p>Control for bobyqa estimation method in nlmixr2
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bobyqaControl(
  npt = NULL,
  rhobeg = NULL,
  rhoend = NULL,
  iprint = 0L,
  maxfun = 100000L,
  returnBobyqa = FALSE,
  stickyRecalcN = 4,
  maxOdeRecalc = 5,
  odeRecalcFactor = 10^(0.5),
  useColor = crayon::has_color(),
  printNcol = floor((getOption("width") - 23)/12),
  print = 1L,
  normType = c("rescale2", "mean", "rescale", "std", "len", "constant"),
  scaleType = c("nlmixr2", "norm", "mult", "multAdd"),
  scaleCmax = 1e+05,
  scaleCmin = 1e-05,
  scaleC = NULL,
  scaleTo = 1,
  rxControl = NULL,
  optExpression = TRUE,
  sumProd = FALSE,
  literalFix = TRUE,
  addProp = c("combined2", "combined1"),
  calcTables = TRUE,
  compress = TRUE,
  covMethod = c("r", ""),
  adjObf = TRUE,
  ci = 0.95,
  sigdig = 4,
  sigdigTable = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bobyqaControl_+3A_npt">npt</code></td>
<td>
<p>The number of points used to approximate the objective
function via a quadratic approximation. The value of npt must be
in the interval [n+2,(n+1)(n+2)/2] where n is the number of
parameters in 'par'. Choices that exceed 2*n+1 are not
recommended.  If not defined, it will be set to min(n * 2, n+2).</p>
</td></tr>
<tr><td><code id="bobyqaControl_+3A_rhobeg">rhobeg</code></td>
<td>
<p>'rhobeg' and 'rhoend' must be set to the initial and
final values of a trust region radius, so both must be positive
with '0 &lt; rhoend &lt; rhobeg'. Typically 'rhobeg' should be about
one tenth of the greatest expected change to a variable.  If the
user does not provide a value, this will be set to 'min(0.95, 0.2
* max(abs(par)))'.  Note also that smallest difference
'abs(upper-lower)' should be greater than or equal to 'rhobeg*2'.
If this is not the case then 'rhobeg' will be adjusted.</p>
</td></tr>
<tr><td><code id="bobyqaControl_+3A_rhoend">rhoend</code></td>
<td>
<p>The smallest value of the trust region radius that is
allowed. If not defined, then 1e-6 times the value set for
'rhobeg' will be used.</p>
</td></tr>
<tr><td><code id="bobyqaControl_+3A_iprint">iprint</code></td>
<td>
<p>The value of 'iprint' should be set to an integer
value in '0, 1, 2, 3, ...', which controls the amount of
printing.  Specifically, there is no output if 'iprint=0' and
there is output only at the start and the return if 'iprint=1'.
Otherwise, each new value of 'rho' is printed, with the best
vector of variables so far and the corresponding value of the
objective function. Further, each new value of the objective
function with its variables are output if 'iprint=3'.  If 'iprint
&gt; 3', the objective function value and corresponding variables
are output every 'iprint' evaluations.  Default value is '0'.</p>
</td></tr>
<tr><td><code id="bobyqaControl_+3A_maxfun">maxfun</code></td>
<td>
<p>The maximum allowed number of function
evaluations. If this is exceeded, the method will terminate.</p>
</td></tr>
<tr><td><code id="bobyqaControl_+3A_returnbobyqa">returnBobyqa</code></td>
<td>
<p>return the bobyqa output instead of the nlmixr2
fit</p>
</td></tr>
<tr><td><code id="bobyqaControl_+3A_stickyrecalcn">stickyRecalcN</code></td>
<td>
<p>The number of bad ODE solves before reducing
the atol/rtol for the rest of the problem.</p>
</td></tr>
<tr><td><code id="bobyqaControl_+3A_maxoderecalc">maxOdeRecalc</code></td>
<td>
<p>Maximum number of times to reduce the ODE
tolerances and try to resolve the system if there was a bad
ODE solve.</p>
</td></tr>
<tr><td><code id="bobyqaControl_+3A_oderecalcfactor">odeRecalcFactor</code></td>
<td>
<p>The ODE recalculation factor when ODE
solving goes bad, this is the factor the rtol/atol is reduced</p>
</td></tr>
<tr><td><code id="bobyqaControl_+3A_usecolor">useColor</code></td>
<td>
<p>Boolean indicating if focei can use ASCII color codes</p>
</td></tr>
<tr><td><code id="bobyqaControl_+3A_printncol">printNcol</code></td>
<td>
<p>Number of columns to printout before wrapping
parameter estimates/gradient</p>
</td></tr>
<tr><td><code id="bobyqaControl_+3A_print">print</code></td>
<td>
<p>Integer representing when the outer step is
printed. When this is 0 or do not print the iterations.  1 is
print every function evaluation (default), 5 is print every 5
evaluations.</p>
</td></tr>
<tr><td><code id="bobyqaControl_+3A_normtype">normType</code></td>
<td>
<p>This is the type of parameter
normalization/scaling used to get the scaled initial values
for nlmixr2.  These are used with <code>scaleType</code> of.
</p>
<p>With the exception of <code>rescale2</code>, these come
from
<a href="https://en.wikipedia.org/wiki/Feature_scaling">Feature
Scaling</a>. The <code>rescale2</code> The rescaling is the same type
described in the
<a href="http://apmonitor.com/me575/uploads/Main/optimization_book.pdf">OptdesX</a>
software manual.
</p>
<p>In general, all all scaling formula can be described by:
</p>
<p style="text-align: center;"><code class="reqn">v_{scaled}</code>
</p>
<p> = (</p>
<p style="text-align: center;"><code class="reqn">v_{unscaled}-C_{1}</code>
</p>
<p>)/</p>
<p style="text-align: center;"><code class="reqn">C_{2}</code>
</p>

<p>Where
</p>
<p>The other data normalization approaches follow the following formula
</p>
<p style="text-align: center;"><code class="reqn">v_{scaled}</code>
</p>
<p> = (</p>
<p style="text-align: center;"><code class="reqn">v_{unscaled}-C_{1}</code>
</p>
<p>)/</p>
<p style="text-align: center;"><code class="reqn">C_{2}</code>
</p>


<ul>
<li> <p><code>rescale2</code> This scales all parameters from (-1 to 1).
The relative differences between the parameters are preserved
with this approach and the constants are:
</p>
<p style="text-align: center;"><code class="reqn">C_{1}</code>
</p>
<p> = (max(all unscaled values)+min(all unscaled values))/2
</p>
<p style="text-align: center;"><code class="reqn">C_{2}</code>
</p>
<p> = (max(all unscaled values) - min(all unscaled values))/2
</p>
</li>
<li> <p><code>rescale</code> or min-max normalization. This rescales all
parameters from (0 to 1).  As in the <code>rescale2</code> the
relative differences are preserved.  In this approach:
</p>
<p style="text-align: center;"><code class="reqn">C_{1}</code>
</p>
<p> = min(all unscaled values)
</p>
<p style="text-align: center;"><code class="reqn">C_{2}</code>
</p>
<p> = max(all unscaled values) - min(all unscaled values)
</p>
</li>
<li> <p><code>mean</code> or mean normalization.  This rescales to center
the parameters around the mean but the parameters are from 0
to 1.  In this approach:
</p>
<p style="text-align: center;"><code class="reqn">C_{1}</code>
</p>
<p> = mean(all unscaled values)
</p>
<p style="text-align: center;"><code class="reqn">C_{2}</code>
</p>
<p> = max(all unscaled values) - min(all unscaled values)
</p>
</li>
<li> <p><code>std</code> or standardization.  This standardizes by the mean
and standard deviation.  In this approach:
</p>
<p style="text-align: center;"><code class="reqn">C_{1}</code>
</p>
<p> = mean(all unscaled values)
</p>
<p style="text-align: center;"><code class="reqn">C_{2}</code>
</p>
<p> = sd(all unscaled values)
</p>
</li>
<li> <p><code>len</code> or unit length scaling.  This scales the
parameters to the unit length.  For this approach we use the Euclidean length, that
is:
</p>
<p style="text-align: center;"><code class="reqn">C_{1}</code>
</p>
<p> = 0
</p>
<p style="text-align: center;"><code class="reqn">C_{2}</code>
</p>
<p> = </p>
<p style="text-align: center;"><code class="reqn">\sqrt(v_1^2 + v_2^2 + \cdots + v_n^2)</code>
</p>

</li>
<li> <p><code>constant</code> which does not perform data normalization. That is
</p>
<p style="text-align: center;"><code class="reqn">C_{1}</code>
</p>
<p> = 0
</p>
<p style="text-align: center;"><code class="reqn">C_{2}</code>
</p>
<p> = 1
</p>
</li></ul>
</td></tr>
<tr><td><code id="bobyqaControl_+3A_scaletype">scaleType</code></td>
<td>
<p>The scaling scheme for nlmixr2.  The supported types are:
</p>

<ul>
<li> <p><code>nlmixr2</code>  In this approach the scaling is performed by the following equation:
</p>
<p style="text-align: center;"><code class="reqn">v_{scaled}</code>
</p>
<p> = (</p>
<p style="text-align: center;"><code class="reqn">v_{current} - v_{init}</code>
</p>
<p>)*scaleC[i] + scaleTo
</p>
<p>The <code>scaleTo</code> parameter is specified by the <code>normType</code>,
and the scales are specified by <code>scaleC</code>.
</p>
</li>
<li> <p><code>norm</code> This approach uses the simple scaling provided
by the <code>normType</code> argument.
</p>
</li>
<li> <p><code>mult</code> This approach does not use the data
normalization provided by <code>normType</code>, but rather uses
multiplicative scaling to a constant provided by the <code>scaleTo</code>
argument.
</p>
<p>In this case:
</p>
<p style="text-align: center;"><code class="reqn">v_{scaled}</code>
</p>
<p> = </p>
<p style="text-align: center;"><code class="reqn">v_{current}</code>
</p>
<p>/</p>
<p style="text-align: center;"><code class="reqn">v_{init}</code>
</p>
<p>*scaleTo
</p>
</li>
<li> <p><code>multAdd</code> This approach changes the scaling based on
the parameter being specified.  If a parameter is defined in an
exponential block (ie exp(theta)), then it is scaled on a
linearly, that is:
</p>
<p style="text-align: center;"><code class="reqn">v_{scaled}</code>
</p>
<p> = (</p>
<p style="text-align: center;"><code class="reqn">v_{current}-v_{init}</code>
</p>
<p>) + scaleTo
</p>
<p>Otherwise the parameter is scaled multiplicatively.
</p>
<p style="text-align: center;"><code class="reqn">v_{scaled}</code>
</p>
<p> = </p>
<p style="text-align: center;"><code class="reqn">v_{current}</code>
</p>
<p>/</p>
<p style="text-align: center;"><code class="reqn">v_{init}</code>
</p>
<p>*scaleTo
</p>
</li></ul>
</td></tr>
<tr><td><code id="bobyqaControl_+3A_scalecmax">scaleCmax</code></td>
<td>
<p>Maximum value of the scaleC to prevent overflow.</p>
</td></tr>
<tr><td><code id="bobyqaControl_+3A_scalecmin">scaleCmin</code></td>
<td>
<p>Minimum value of the scaleC to prevent underflow.</p>
</td></tr>
<tr><td><code id="bobyqaControl_+3A_scalec">scaleC</code></td>
<td>
<p>The scaling constant used with
<code>scaleType=nlmixr2</code>.  When not specified, it is based on
the type of parameter that is estimated.  The idea is to keep
the derivatives similar on a log scale to have similar
gradient sizes.  Hence parameters like log(exp(theta)) would
have a scaling factor of 1 and log(theta) would have a scaling
factor of ini_value (to scale by 1/value; ie
d/dt(log(ini_value)) = 1/ini_value or scaleC=ini_value)
</p>

<ul>
<li><p> For parameters in an exponential (ie exp(theta)) or
parameters specifying powers, boxCox or yeoJohnson
transformations , this is 1.
</p>
</li>
<li><p> For additive, proportional, lognormal error structures,
these are given by 0.5*abs(initial_estimate)
</p>
</li>
<li><p> Factorials are scaled by abs(1/digamma(initial_estimate+1))
</p>
</li>
<li><p> parameters in a log scale (ie log(theta)) are transformed
by log(abs(initial_estimate))*abs(initial_estimate)
</p>
</li></ul>

<p>These parameter scaling coefficients are chose to try to keep
similar slopes among parameters.  That is they all follow the
slopes approximately on a log-scale.
</p>
<p>While these are chosen in a logical manner, they may not always
apply.  You can specify each parameters scaling factor by this
parameter if you wish.</p>
</td></tr>
<tr><td><code id="bobyqaControl_+3A_scaleto">scaleTo</code></td>
<td>
<p>Scale the initial parameter estimate to this value.
By default this is 1.  When zero or below, no scaling is performed.</p>
</td></tr>
<tr><td><code id="bobyqaControl_+3A_rxcontrol">rxControl</code></td>
<td>
<p>'rxode2' ODE solving options during fitting, created with 'rxControl()'</p>
</td></tr>
<tr><td><code id="bobyqaControl_+3A_optexpression">optExpression</code></td>
<td>
<p>Optimize the rxode2 expression to speed up
calculation. By default this is turned on.</p>
</td></tr>
<tr><td><code id="bobyqaControl_+3A_sumprod">sumProd</code></td>
<td>
<p>Is a boolean indicating if the model should change
multiplication to high precision multiplication and sums to
high precision sums using the PreciseSums package.  By default
this is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="bobyqaControl_+3A_literalfix">literalFix</code></td>
<td>
<p>boolean, substitute fixed population values as
literals and re-adjust ui and parameter estimates after
optimization; Default is 'TRUE'.</p>
</td></tr>
<tr><td><code id="bobyqaControl_+3A_addprop">addProp</code></td>
<td>
<p>specifies the type of additive plus proportional
errors, the one where standard deviations add (combined1) or the
type where the variances add (combined2).
</p>
<p>The combined1 error type can be described by the following equation:
</p>
<p style="text-align: center;"><code class="reqn">y = f + (a + b\times f^c) \times \varepsilon</code>
</p>

<p>The combined2 error model can be described by the following equation:
</p>
<p style="text-align: center;"><code class="reqn">y = f + \sqrt{a^2 + b^2\times f^{2\times c}} \times \varepsilon</code>
</p>

<p>Where:
</p>
<p>- y represents the observed value
</p>
<p>- f represents the predicted value
</p>
<p>- a  is the additive standard deviation
</p>
<p>- b is the proportional/power standard deviation
</p>
<p>- c is the power exponent (in the proportional case c=1)</p>
</td></tr>
<tr><td><code id="bobyqaControl_+3A_calctables">calcTables</code></td>
<td>
<p>This boolean is to determine if the foceiFit
will calculate tables. By default this is <code>TRUE</code></p>
</td></tr>
<tr><td><code id="bobyqaControl_+3A_compress">compress</code></td>
<td>
<p>Should the object have compressed items</p>
</td></tr>
<tr><td><code id="bobyqaControl_+3A_covmethod">covMethod</code></td>
<td>
<p>Method for calculating covariance.  In this
discussion, R is the Hessian matrix of the objective
function. The S matrix is the sum of individual
gradient cross-product (evaluated at the individual empirical
Bayes estimates).
</p>

<ul>
<li><p> &quot;<code>r,s</code>&quot; Uses the sandwich matrix to calculate the
covariance, that is: <code>solve(R) %*% S %*% solve(R)</code>
</p>
</li>
<li><p> &quot;<code>r</code>&quot; Uses the Hessian matrix to calculate the
covariance as <code>2 %*% solve(R)</code>
</p>
</li>
<li><p> &quot;<code>s</code>&quot; Uses the cross-product matrix to calculate the
covariance as <code>4 %*% solve(S)</code>
</p>
</li>
<li><p> &quot;&quot; Does not calculate the covariance step.
</p>
</li></ul>
</td></tr>
<tr><td><code id="bobyqaControl_+3A_adjobf">adjObf</code></td>
<td>
<p>is a boolean to indicate if the objective function
should be adjusted to be closer to NONMEM's default objective
function.  By default this is <code>TRUE</code></p>
</td></tr>
<tr><td><code id="bobyqaControl_+3A_ci">ci</code></td>
<td>
<p>Confidence level for some tables.  By default this is
0.95 or 95% confidence.</p>
</td></tr>
<tr><td><code id="bobyqaControl_+3A_sigdig">sigdig</code></td>
<td>
<p>Optimization significant digits. This controls:
</p>

<ul>
<li><p> The tolerance of the inner and outer optimization is <code>10^-sigdig</code>
</p>
</li>
<li><p> The tolerance of the ODE solvers is
<code>0.5*10^(-sigdig-2)</code>; For the sensitivity equations and
steady-state solutions the default is <code>0.5*10^(-sigdig-1.5)</code>
(sensitivity changes only applicable for liblsoda)
</p>
</li>
<li><p> The tolerance of the boundary check is <code>5 * 10 ^ (-sigdig + 1)</code>
</p>
</li></ul>
</td></tr>
<tr><td><code id="bobyqaControl_+3A_sigdigtable">sigdigTable</code></td>
<td>
<p>Significant digits in the final output table.
If not specified, then it matches the significant digits in the
'sigdig' optimization algorithm.  If 'sigdig' is NULL, use 3.</p>
</td></tr>
<tr><td><code id="bobyqaControl_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <code><a href="nlmixr2est.html#topic+bobyqaControl">nlmixr2est::bobyqaControl()</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>bobqya control structure
</p>


<h3>Author(s)</h3>

<p>Matthew L. Fidler
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# A logit regression example with emax model

dsn &lt;- data.frame(i=1:1000)
dsn$time &lt;- exp(rnorm(1000))
dsn$DV=rbinom(1000,1,exp(-1+dsn$time)/(1+exp(-1+dsn$time)))

mod &lt;- function() {
 ini({
   E0 &lt;- 0.5
   Em &lt;- 0.5
   E50 &lt;- 2
   g &lt;- fix(2)
 })
 model({
   v &lt;- E0+Em*time^g/(E50^g+time^g)
   ll(bin) ~ DV * v - log(1 + exp(v))
 })
}

fit2 &lt;- nlmixr(mod, dsn, est="bobyqa")

print(fit2)

# you can also get the nlm output with

fit2$bobyqa

# The nlm control has been modified slightly to include
# extra components and name the parameters

</code></pre>

<hr>
<h2 id='bootplot'>Produce delta objective function for boostrap</h2><span id='topic+bootplot'></span>

<h3>Description</h3>

<p>Produce delta objective function for boostrap
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bootplot(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bootplot_+3A_x">x</code></td>
<td>
<p>fit object</p>
</td></tr>
<tr><td><code id="bootplot_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <code><a href="nlmixr2extra.html#topic+bootplot">nlmixr2extra::bootplot()</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Fit traceplot or nothing.
</p>


<h3>Author(s)</h3>

<p>Vipul Mann,  Matthew L. Fidler
</p>


<h3>References</h3>

<p>R Niebecker,  MO Karlsson. (2013)
<em>Are datasets for NLME models large enough for a bootstrap to provide reliable parameter uncertainty distributions?</em>
PAGE 2013.
<a href="https://www.page-meeting.org/?abstract=2899">https://www.page-meeting.org/?abstract=2899</a>
</p>

<hr>
<h2 id='bootstrapFit'>Bootstrap nlmixr2 fit</h2><span id='topic+bootstrapFit'></span>

<h3>Description</h3>

<p>Bootstrap input dataset and rerun the model to get confidence bounds and aggregated parameters
</p>


<h3>Usage</h3>

<pre><code class='language-R'>bootstrapFit(
  fit,
  nboot = 200,
  nSampIndiv,
  stratVar,
  stdErrType = c("perc", "sd", "se"),
  ci = 0.95,
  pvalues = NULL,
  restart = FALSE,
  plotHist = FALSE,
  fitName = as.character(substitute(fit))
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="bootstrapFit_+3A_fit">fit</code></td>
<td>
<p>the nlmixr2 fit object</p>
</td></tr>
<tr><td><code id="bootstrapFit_+3A_nboot">nboot</code></td>
<td>
<p>an integer giving the number of bootstrapped models to
be fit; default value is 200</p>
</td></tr>
<tr><td><code id="bootstrapFit_+3A_nsampindiv">nSampIndiv</code></td>
<td>
<p>an integer specifying the number of samples in
each bootstrapped sample; default is the number of unique
subjects in the original dataset</p>
</td></tr>
<tr><td><code id="bootstrapFit_+3A_stratvar">stratVar</code></td>
<td>
<p>Variable in the original dataset to stratify on;
This is useful to distinguish between sparse and full sampling
and other features you may wish to keep distinct in your
bootstrap</p>
</td></tr>
<tr><td><code id="bootstrapFit_+3A_stderrtype">stdErrType</code></td>
<td>
<p>This gives the standard error type for the
updated standard errors; The current possibilities are: <code>"perc"</code>
which gives the standard errors by percentiles (default), <code>"sd"</code>
which gives the standard errors by the using the normal
approximation of the mean with standard devaition, or <code>"se"</code>
which uses the normal approximation with standard errors
calculated with <code>nSampIndiv</code></p>
</td></tr>
<tr><td><code id="bootstrapFit_+3A_ci">ci</code></td>
<td>
<p>Confidence interval level to calculate.  Default is 0.95
for a 95 percent confidence interval</p>
</td></tr>
<tr><td><code id="bootstrapFit_+3A_pvalues">pvalues</code></td>
<td>
<p>a vector of pvalues indicating the probability of
each subject to get selected; default value is <code>NULL</code> implying
that probability of each subject is the same</p>
</td></tr>
<tr><td><code id="bootstrapFit_+3A_restart">restart</code></td>
<td>
<p>A boolean to try to restart an interrupted or
incomplete boostrap.  By default this is <code>FALSE</code></p>
</td></tr>
<tr><td><code id="bootstrapFit_+3A_plothist">plotHist</code></td>
<td>
<p>A boolean indicating if a histogram plot to assess
how well the bootstrap is doing.  By default this is turned off
(<code>FALSE</code>)</p>
</td></tr>
<tr><td><code id="bootstrapFit_+3A_fitname">fitName</code></td>
<td>
<p>is the fit name that is used for the name of the
boostrap files.  By default it is the fit provided though it
could be something else.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Nothing, called for the side effects; The original fit is
updated with the bootstrap confidence bands
</p>


<h3>Author(s)</h3>

<p>Vipul Mann, Matthew Fidler
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
one.cmt &lt;- function() {
  ini({
    tka &lt;- 0.45; label("Ka")
    tcl &lt;- 1; label("Cl")
    tv &lt;- 3.45; label("V")
    eta.ka ~ 0.6
    eta.cl ~ 0.3
    eta.v ~ 0.1
    add.sd &lt;- 0.7
  })
  model({
    ka &lt;- exp(tka + eta.ka)
    cl &lt;- exp(tcl + eta.cl)
    v &lt;- exp(tv + eta.v)
    linCmt() ~ add(add.sd)
  })
}

fit &lt;- nlmixr2(one.cmt, nlmixr2data::theo_sd, est = "saem", control = list(print = 0))

withr::with_tempdir({ # Run example in temp dir

bootstrapFit(fit, nboot = 5, restart = TRUE) # overwrites any of the existing data or model files
bootstrapFit(fit, nboot = 7) # resumes fitting using the stored data and model files

# Note this resumes because the total number of bootstrap samples is not 10

bootstrapFit(fit, nboot=10)

# Note the boostrap standard error and variance/covariance matrix is retained.
# If you wish to switch back you can change the covariance matrix by

nlmixr2est::setCov(fit, "linFim")

# And change it back again

nlmixr2est::setCov(fit, "boot10")

# This change will affect any simulations with uncertainty in their parameters

# You may also do a chi-square diagnostic plot check for the bootstrap with
bootplot(fit)
})

## End(Not run)
</code></pre>

<hr>
<h2 id='covarSearchAuto'>Stepwise Covariate Model-selection (SCM) method</h2><span id='topic+covarSearchAuto'></span>

<h3>Description</h3>

<p>Stepwise Covariate Model-selection (SCM) method
</p>


<h3>Usage</h3>

<pre><code class='language-R'>covarSearchAuto(
  fit,
  varsVec,
  covarsVec,
  pVal = list(fwd = 0.05, bck = 0.01),
  catvarsVec = NULL,
  searchType = c("scm", "forward", "backward"),
  restart = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="covarSearchAuto_+3A_fit">fit</code></td>
<td>
<p>an nlmixr2 'fit' object</p>
</td></tr>
<tr><td><code id="covarSearchAuto_+3A_varsvec">varsVec</code></td>
<td>
<p>a list of candidate variables to which the
covariates could be added</p>
</td></tr>
<tr><td><code id="covarSearchAuto_+3A_covarsvec">covarsVec</code></td>
<td>
<p>a list of candidate covariates that need to be
tested</p>
</td></tr>
<tr><td><code id="covarSearchAuto_+3A_pval">pVal</code></td>
<td>
<p>a named list with names 'fwd' and 'bck' for specifying
the p-values for the forward and backward searches, respectively</p>
</td></tr>
<tr><td><code id="covarSearchAuto_+3A_catvarsvec">catvarsVec</code></td>
<td>
<p>character vector of categorical covariates that need to be added</p>
</td></tr>
<tr><td><code id="covarSearchAuto_+3A_searchtype">searchType</code></td>
<td>
<p>one of 'scm', 'forward' and 'backward' to specify
the covariate search method; default is 'scm'</p>
</td></tr>
<tr><td><code id="covarSearchAuto_+3A_restart">restart</code></td>
<td>
<p>a boolean that controls if the search should be
restarted; default is FALSE</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list summarizing the covariate selection steps and
output; This list has the &quot;summaryTable&quot; for the overall summary
of the covariate selection as well as &quot;resFwd&quot; for the forward
selection method and &quot;resBck&quot; for the backward selection method.
</p>


<h3>Author(s)</h3>

<p>Vipul Mann, Matthew Fidler, Vishal Sarsani
</p>


<h3>Examples</h3>

<pre><code class='language-R'>## Not run: 
one.cmt &lt;- function() {
  ini({
    tka &lt;- 0.45; label("Ka")
    tcl &lt;- log(c(0, 2.7, 100)); label("Cl")
    tv &lt;- 3.45; label("V")
    eta.ka ~ 0.6
    eta.cl ~ 0.3
    eta.v ~ 0.1
    add.sd &lt;- 0.7
  })
  model({
    ka &lt;- exp(tka + eta.ka)
    cl &lt;- exp(tcl + eta.cl)
    v &lt;- exp(tv + eta.v)
    linCmt() ~ add(add.sd)
  })
}

fit &lt;- nlmixr2(one.cmt, nlmixr2data::theo_sd, est = "saem", control = list(print = 0))
rxode2::.rxWithWd(tempdir(), {# with temporary directory

auto1 &lt;- covarSearchAuto(fit, varsVec = c("ka", "cl"),
    covarsVec = c("WT"))

})

## Note that this didn't include sex, add it to dataset and restart model


d &lt;- nlmixr2data::theo_sd
d$SEX &lt;-0
d$SEX[d$ID&lt;=6] &lt;-1

fit &lt;- nlmixr2(one.cmt, d, est = "saem", control = list(print = 0))

# This would restart if for some reason the search crashed:

rxode2::.rxWithWd(tempdir(), {# with temporary directory

auto2 &lt;- covarSearchAuto(fit, varsVec = c("ka", "cl"), covarsVec = c("WT"),
                catvarsVec= c("SEX"), restart = TRUE)

auto3 &lt;- covarSearchAuto(fit, varsVec = c("ka", "cl"), covarsVec = c("WT"),
                catvarsVec=  c("SEX"), restart = TRUE,
                searchType = "forward")
})

## End(Not run)
</code></pre>

<hr>
<h2 id='foceiControl'>Control Options for FOCEi</h2><span id='topic+foceiControl'></span>

<h3>Description</h3>

<p>Control Options for FOCEi
</p>


<h3>Usage</h3>

<pre><code class='language-R'>foceiControl(
  sigdig = 3,
  ...,
  epsilon = NULL,
  maxInnerIterations = 1000,
  maxOuterIterations = 5000,
  n1qn1nsim = NULL,
  print = 1L,
  printNcol = floor((getOption("width") - 23)/12),
  scaleTo = 1,
  scaleObjective = 0,
  normType = c("rescale2", "mean", "rescale", "std", "len", "constant"),
  scaleType = c("nlmixr2", "norm", "mult", "multAdd"),
  scaleCmax = 1e+05,
  scaleCmin = 1e-05,
  scaleC = NULL,
  scaleC0 = 1e+05,
  derivEps = rep(20 * sqrt(.Machine$double.eps), 2),
  derivMethod = c("switch", "forward", "central"),
  derivSwitchTol = NULL,
  covDerivMethod = c("central", "forward"),
  covMethod = c("r,s", "r", "s", ""),
  hessEps = (.Machine$double.eps)^(1/3),
  hessEpsLlik = (.Machine$double.eps)^(1/3),
  optimHessType = c("central", "forward"),
  optimHessCovType = c("central", "forward"),
  eventType = c("central", "forward"),
  centralDerivEps = rep(20 * sqrt(.Machine$double.eps), 2),
  lbfgsLmm = 7L,
  lbfgsPgtol = 0,
  lbfgsFactr = NULL,
  eigen = TRUE,
  addPosthoc = TRUE,
  diagXform = c("sqrt", "log", "identity"),
  sumProd = FALSE,
  optExpression = TRUE,
  literalFix = TRUE,
  ci = 0.95,
  useColor = crayon::has_color(),
  boundTol = NULL,
  calcTables = TRUE,
  noAbort = TRUE,
  interaction = TRUE,
  cholSEtol = (.Machine$double.eps)^(1/3),
  cholAccept = 0.001,
  resetEtaP = 0.15,
  resetThetaP = 0.05,
  resetThetaFinalP = 0.15,
  diagOmegaBoundUpper = 5,
  diagOmegaBoundLower = 100,
  cholSEOpt = FALSE,
  cholSECov = FALSE,
  fo = FALSE,
  covTryHarder = FALSE,
  outerOpt = c("nlminb", "bobyqa", "lbfgsb3c", "L-BFGS-B", "mma", "lbfgsbLG", "slsqp",
    "Rvmmin"),
  innerOpt = c("n1qn1", "BFGS"),
  rhobeg = 0.2,
  rhoend = NULL,
  npt = NULL,
  rel.tol = NULL,
  x.tol = NULL,
  eval.max = 4000,
  iter.max = 2000,
  abstol = NULL,
  reltol = NULL,
  resetHessianAndEta = FALSE,
  stateTrim = Inf,
  shi21maxOuter = 0L,
  shi21maxInner = 20L,
  shi21maxInnerCov = 20L,
  shi21maxFD = 20L,
  gillK = 10L,
  gillStep = 4,
  gillFtol = 0,
  gillRtol = sqrt(.Machine$double.eps),
  gillKcov = 10L,
  gillKcovLlik = 10L,
  gillStepCovLlik = 4.5,
  gillStepCov = 2,
  gillFtolCov = 0,
  gillFtolCovLlik = 0,
  rmatNorm = TRUE,
  rmatNormLlik = TRUE,
  smatNorm = TRUE,
  smatNormLlik = TRUE,
  covGillF = TRUE,
  optGillF = TRUE,
  covSmall = 1e-05,
  adjLik = TRUE,
  gradTrim = Inf,
  maxOdeRecalc = 5,
  odeRecalcFactor = 10^(0.5),
  gradCalcCentralSmall = 1e-04,
  gradCalcCentralLarge = 10000,
  etaNudge = qnorm(1 - 0.05/2)/sqrt(3),
  etaNudge2 = qnorm(1 - 0.05/2) * sqrt(3/5),
  nRetries = 3,
  seed = 42,
  resetThetaCheckPer = 0.1,
  etaMat = NULL,
  repeatGillMax = 1,
  stickyRecalcN = 4,
  gradProgressOfvTime = 10,
  addProp = c("combined2", "combined1"),
  badSolveObjfAdj = 100,
  compress = TRUE,
  rxControl = NULL,
  sigdigTable = NULL,
  fallbackFD = FALSE,
  smatPer = 0.6,
  sdLowerFact = 0.001
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="foceiControl_+3A_sigdig">sigdig</code></td>
<td>
<p>Optimization significant digits. This controls:
</p>

<ul>
<li><p> The tolerance of the inner and outer optimization is <code>10^-sigdig</code>
</p>
</li>
<li><p> The tolerance of the ODE solvers is
<code>0.5*10^(-sigdig-2)</code>; For the sensitivity equations and
steady-state solutions the default is <code>0.5*10^(-sigdig-1.5)</code>
(sensitivity changes only applicable for liblsoda)
</p>
</li>
<li><p> The tolerance of the boundary check is <code>5 * 10 ^ (-sigdig + 1)</code>
</p>
</li></ul>
</td></tr>
<tr><td><code id="foceiControl_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <code><a href="nlmixr2est.html#topic+foceiControl">nlmixr2est::foceiControl()</a></code>.</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_epsilon">epsilon</code></td>
<td>
<p>Precision of estimate for n1qn1 optimization.</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_maxinneriterations">maxInnerIterations</code></td>
<td>
<p>Number of iterations for n1qn1
optimization.</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_maxouteriterations">maxOuterIterations</code></td>
<td>
<p>Maximum number of L-BFGS-B optimization
for outer problem.</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_n1qn1nsim">n1qn1nsim</code></td>
<td>
<p>Number of function evaluations for n1qn1
optimization.</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_print">print</code></td>
<td>
<p>Integer representing when the outer step is
printed. When this is 0 or do not print the iterations.  1 is
print every function evaluation (default), 5 is print every 5
evaluations.</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_printncol">printNcol</code></td>
<td>
<p>Number of columns to printout before wrapping
parameter estimates/gradient</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_scaleto">scaleTo</code></td>
<td>
<p>Scale the initial parameter estimate to this value.
By default this is 1.  When zero or below, no scaling is performed.</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_scaleobjective">scaleObjective</code></td>
<td>
<p>Scale the initial objective function to this
value.  By default this is 0 (meaning do not scale)</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_normtype">normType</code></td>
<td>
<p>This is the type of parameter
normalization/scaling used to get the scaled initial values
for nlmixr2.  These are used with <code>scaleType</code> of.
</p>
<p>With the exception of <code>rescale2</code>, these come
from
<a href="https://en.wikipedia.org/wiki/Feature_scaling">Feature
Scaling</a>. The <code>rescale2</code> The rescaling is the same type
described in the
<a href="http://apmonitor.com/me575/uploads/Main/optimization_book.pdf">OptdesX</a>
software manual.
</p>
<p>In general, all all scaling formula can be described by:
</p>
<p style="text-align: center;"><code class="reqn">v_{scaled}</code>
</p>
<p> = (</p>
<p style="text-align: center;"><code class="reqn">v_{unscaled}-C_{1}</code>
</p>
<p>)/</p>
<p style="text-align: center;"><code class="reqn">C_{2}</code>
</p>

<p>Where
</p>
<p>The other data normalization approaches follow the following formula
</p>
<p style="text-align: center;"><code class="reqn">v_{scaled}</code>
</p>
<p> = (</p>
<p style="text-align: center;"><code class="reqn">v_{unscaled}-C_{1}</code>
</p>
<p>)/</p>
<p style="text-align: center;"><code class="reqn">C_{2}</code>
</p>


<ul>
<li> <p><code>rescale2</code> This scales all parameters from (-1 to 1).
The relative differences between the parameters are preserved
with this approach and the constants are:
</p>
<p style="text-align: center;"><code class="reqn">C_{1}</code>
</p>
<p> = (max(all unscaled values)+min(all unscaled values))/2
</p>
<p style="text-align: center;"><code class="reqn">C_{2}</code>
</p>
<p> = (max(all unscaled values) - min(all unscaled values))/2
</p>
</li>
<li> <p><code>rescale</code> or min-max normalization. This rescales all
parameters from (0 to 1).  As in the <code>rescale2</code> the
relative differences are preserved.  In this approach:
</p>
<p style="text-align: center;"><code class="reqn">C_{1}</code>
</p>
<p> = min(all unscaled values)
</p>
<p style="text-align: center;"><code class="reqn">C_{2}</code>
</p>
<p> = max(all unscaled values) - min(all unscaled values)
</p>
</li>
<li> <p><code>mean</code> or mean normalization.  This rescales to center
the parameters around the mean but the parameters are from 0
to 1.  In this approach:
</p>
<p style="text-align: center;"><code class="reqn">C_{1}</code>
</p>
<p> = mean(all unscaled values)
</p>
<p style="text-align: center;"><code class="reqn">C_{2}</code>
</p>
<p> = max(all unscaled values) - min(all unscaled values)
</p>
</li>
<li> <p><code>std</code> or standardization.  This standardizes by the mean
and standard deviation.  In this approach:
</p>
<p style="text-align: center;"><code class="reqn">C_{1}</code>
</p>
<p> = mean(all unscaled values)
</p>
<p style="text-align: center;"><code class="reqn">C_{2}</code>
</p>
<p> = sd(all unscaled values)
</p>
</li>
<li> <p><code>len</code> or unit length scaling.  This scales the
parameters to the unit length.  For this approach we use the Euclidean length, that
is:
</p>
<p style="text-align: center;"><code class="reqn">C_{1}</code>
</p>
<p> = 0
</p>
<p style="text-align: center;"><code class="reqn">C_{2}</code>
</p>
<p> = </p>
<p style="text-align: center;"><code class="reqn">\sqrt(v_1^2 + v_2^2 + \cdots + v_n^2)</code>
</p>

</li>
<li> <p><code>constant</code> which does not perform data normalization. That is
</p>
<p style="text-align: center;"><code class="reqn">C_{1}</code>
</p>
<p> = 0
</p>
<p style="text-align: center;"><code class="reqn">C_{2}</code>
</p>
<p> = 1
</p>
</li></ul>
</td></tr>
<tr><td><code id="foceiControl_+3A_scaletype">scaleType</code></td>
<td>
<p>The scaling scheme for nlmixr2.  The supported types are:
</p>

<ul>
<li> <p><code>nlmixr2</code>  In this approach the scaling is performed by the following equation:
</p>
<p style="text-align: center;"><code class="reqn">v_{scaled}</code>
</p>
<p> = (</p>
<p style="text-align: center;"><code class="reqn">v_{current} - v_{init}</code>
</p>
<p>)*scaleC[i] + scaleTo
</p>
<p>The <code>scaleTo</code> parameter is specified by the <code>normType</code>,
and the scales are specified by <code>scaleC</code>.
</p>
</li>
<li> <p><code>norm</code> This approach uses the simple scaling provided
by the <code>normType</code> argument.
</p>
</li>
<li> <p><code>mult</code> This approach does not use the data
normalization provided by <code>normType</code>, but rather uses
multiplicative scaling to a constant provided by the <code>scaleTo</code>
argument.
</p>
<p>In this case:
</p>
<p style="text-align: center;"><code class="reqn">v_{scaled}</code>
</p>
<p> = </p>
<p style="text-align: center;"><code class="reqn">v_{current}</code>
</p>
<p>/</p>
<p style="text-align: center;"><code class="reqn">v_{init}</code>
</p>
<p>*scaleTo
</p>
</li>
<li> <p><code>multAdd</code> This approach changes the scaling based on
the parameter being specified.  If a parameter is defined in an
exponential block (ie exp(theta)), then it is scaled on a
linearly, that is:
</p>
<p style="text-align: center;"><code class="reqn">v_{scaled}</code>
</p>
<p> = (</p>
<p style="text-align: center;"><code class="reqn">v_{current}-v_{init}</code>
</p>
<p>) + scaleTo
</p>
<p>Otherwise the parameter is scaled multiplicatively.
</p>
<p style="text-align: center;"><code class="reqn">v_{scaled}</code>
</p>
<p> = </p>
<p style="text-align: center;"><code class="reqn">v_{current}</code>
</p>
<p>/</p>
<p style="text-align: center;"><code class="reqn">v_{init}</code>
</p>
<p>*scaleTo
</p>
</li></ul>
</td></tr>
<tr><td><code id="foceiControl_+3A_scalecmax">scaleCmax</code></td>
<td>
<p>Maximum value of the scaleC to prevent overflow.</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_scalecmin">scaleCmin</code></td>
<td>
<p>Minimum value of the scaleC to prevent underflow.</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_scalec">scaleC</code></td>
<td>
<p>The scaling constant used with
<code>scaleType=nlmixr2</code>.  When not specified, it is based on
the type of parameter that is estimated.  The idea is to keep
the derivatives similar on a log scale to have similar
gradient sizes.  Hence parameters like log(exp(theta)) would
have a scaling factor of 1 and log(theta) would have a scaling
factor of ini_value (to scale by 1/value; ie
d/dt(log(ini_value)) = 1/ini_value or scaleC=ini_value)
</p>

<ul>
<li><p> For parameters in an exponential (ie exp(theta)) or
parameters specifying powers, boxCox or yeoJohnson
transformations , this is 1.
</p>
</li>
<li><p> For additive, proportional, lognormal error structures,
these are given by 0.5*abs(initial_estimate)
</p>
</li>
<li><p> Factorials are scaled by abs(1/digamma(initial_estimate+1))
</p>
</li>
<li><p> parameters in a log scale (ie log(theta)) are transformed
by log(abs(initial_estimate))*abs(initial_estimate)
</p>
</li></ul>

<p>These parameter scaling coefficients are chose to try to keep
similar slopes among parameters.  That is they all follow the
slopes approximately on a log-scale.
</p>
<p>While these are chosen in a logical manner, they may not always
apply.  You can specify each parameters scaling factor by this
parameter if you wish.</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_scalec0">scaleC0</code></td>
<td>
<p>Number to adjust the scaling factor by if the initial
gradient is zero.</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_deriveps">derivEps</code></td>
<td>
<p>Forward difference tolerances, which is a
vector of relative difference and absolute difference.  The
central/forward difference step size h is calculated as:
</p>
<p><code>h = abs(x)*derivEps[1] + derivEps[2]</code></p>
</td></tr>
<tr><td><code id="foceiControl_+3A_derivmethod">derivMethod</code></td>
<td>
<p>indicates the method for calculating
derivatives of the outer problem.  Currently supports
&quot;switch&quot;, &quot;central&quot; and &quot;forward&quot; difference methods.  Switch
starts with forward differences.  This will switch to central
differences when abs(delta(OFV)) &lt;= derivSwitchTol and switch
back to forward differences when abs(delta(OFV)) &gt;
derivSwitchTol.</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_derivswitchtol">derivSwitchTol</code></td>
<td>
<p>The tolerance to switch forward to central
differences.</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_covderivmethod">covDerivMethod</code></td>
<td>
<p>indicates the method for calculating the
derivatives while calculating the covariance components
(Hessian and S).</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_covmethod">covMethod</code></td>
<td>
<p>Method for calculating covariance.  In this
discussion, R is the Hessian matrix of the objective
function. The S matrix is the sum of individual
gradient cross-product (evaluated at the individual empirical
Bayes estimates).
</p>

<ul>
<li><p> &quot;<code>r,s</code>&quot; Uses the sandwich matrix to calculate the
covariance, that is: <code>solve(R) %*% S %*% solve(R)</code>
</p>
</li>
<li><p> &quot;<code>r</code>&quot; Uses the Hessian matrix to calculate the
covariance as <code>2 %*% solve(R)</code>
</p>
</li>
<li><p> &quot;<code>s</code>&quot; Uses the cross-product matrix to calculate the
covariance as <code>4 %*% solve(S)</code>
</p>
</li>
<li><p> &quot;&quot; Does not calculate the covariance step.
</p>
</li></ul>
</td></tr>
<tr><td><code id="foceiControl_+3A_hesseps">hessEps</code></td>
<td>
<p>is a double value representing the epsilon for the
Hessian calculation. This is used for the R matrix calculation.</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_hessepsllik">hessEpsLlik</code></td>
<td>
<p>is a double value representing the epsilon for
the Hessian calculation when doing focei generalized
log-likelihood estimation.  This is used for the R matrix
calculation.</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_optimhesstype">optimHessType</code></td>
<td>
<p>The hessian type for when calculating the
individual hessian by numeric differences (in generalized
log-likelihood estimation).  The options are &quot;central&quot;, and
&quot;forward&quot;.  The central differences is what R's 'optimHess()'
uses and is the default for this method. (Though the &quot;forward&quot; is
faster and still reasonable for most cases).  The Shi21 cannot be
changed for the Gill83 algorithm with the optimHess in a
generalized likelihood problem.</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_optimhesscovtype">optimHessCovType</code></td>
<td>
<p>The hessian type for when calculating the
individual hessian by numeric differences (in generalized
log-likelihood estimation).  The options are &quot;central&quot;, and
&quot;forward&quot;.  The central differences is what R's 'optimHess()'
uses.  While this takes longer in optimization, it is more
accurate, so for calculating the covariance and final likelihood,
the central differences are used. This also uses the modified
Shi21 method</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_eventtype">eventType</code></td>
<td>
<p>Event gradient type for dosing events; Can be
&quot;central&quot; or &quot;forward&quot;</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_centralderiveps">centralDerivEps</code></td>
<td>
<p>Central difference tolerances.  This is a
numeric vector of relative difference and absolute difference.
The central/forward difference step size h is calculated as:
</p>
<p><code>h = abs(x)*derivEps[1] + derivEps[2]</code></p>
</td></tr>
<tr><td><code id="foceiControl_+3A_lbfgslmm">lbfgsLmm</code></td>
<td>
<p>An integer giving the number of BFGS updates
retained in the &quot;L-BFGS-B&quot; method, It defaults to 7.</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_lbfgspgtol">lbfgsPgtol</code></td>
<td>
<p>is a double precision variable.
</p>
<p>On entry pgtol &gt;= 0 is specified by the user.  The iteration
will stop when:
</p>
<p><code>max(\| proj g_i \| i = 1, ..., n) &lt;= lbfgsPgtol</code>
</p>
<p>where pg_i is the ith component of the projected gradient.
</p>
<p>On exit pgtol is unchanged.  This defaults to zero, when the
check is suppressed.</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_lbfgsfactr">lbfgsFactr</code></td>
<td>
<p>Controls the convergence of the &quot;L-BFGS-B&quot;
method.  Convergence occurs when the reduction in the
objective is within this factor of the machine
tolerance. Default is 1e10, which gives a tolerance of about
<code>2e-6</code>, approximately 4 sigdigs.  You can check your
exact tolerance by multiplying this value by
<code>.Machine$double.eps</code></p>
</td></tr>
<tr><td><code id="foceiControl_+3A_eigen">eigen</code></td>
<td>
<p>A boolean indicating if eigenvectors are calculated
to include a condition number calculation.</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_addposthoc">addPosthoc</code></td>
<td>
<p>Boolean indicating if posthoc parameters are
added to the table output.</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_diagxform">diagXform</code></td>
<td>
<p>This is the transformation used on the diagonal
of the <code>chol(solve(omega))</code>. This matrix and values are the
parameters estimated in FOCEi. The possibilities are:
</p>

<ul>
<li> <p><code>sqrt</code> Estimates the sqrt of the diagonal elements of <code>chol(solve(omega))</code>.  This is the default method.
</p>
</li>
<li> <p><code>log</code> Estimates the log of the diagonal elements of <code>chol(solve(omega))</code>
</p>
</li>
<li> <p><code>identity</code> Estimates the diagonal elements without any transformations
</p>
</li></ul>
</td></tr>
<tr><td><code id="foceiControl_+3A_sumprod">sumProd</code></td>
<td>
<p>Is a boolean indicating if the model should change
multiplication to high precision multiplication and sums to
high precision sums using the PreciseSums package.  By default
this is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_optexpression">optExpression</code></td>
<td>
<p>Optimize the rxode2 expression to speed up
calculation. By default this is turned on.</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_literalfix">literalFix</code></td>
<td>
<p>boolean, substitute fixed population values as
literals and re-adjust ui and parameter estimates after
optimization; Default is 'TRUE'.</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_ci">ci</code></td>
<td>
<p>Confidence level for some tables.  By default this is
0.95 or 95% confidence.</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_usecolor">useColor</code></td>
<td>
<p>Boolean indicating if focei can use ASCII color codes</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_boundtol">boundTol</code></td>
<td>
<p>Tolerance for boundary issues.</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_calctables">calcTables</code></td>
<td>
<p>This boolean is to determine if the foceiFit
will calculate tables. By default this is <code>TRUE</code></p>
</td></tr>
<tr><td><code id="foceiControl_+3A_noabort">noAbort</code></td>
<td>
<p>Boolean to indicate if you should abort the FOCEi
evaluation if it runs into troubles.  (default TRUE)</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_interaction">interaction</code></td>
<td>
<p>Boolean indicate FOCEi should be used (TRUE)
instead of FOCE (FALSE)</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_cholsetol">cholSEtol</code></td>
<td>
<p>tolerance for Generalized Cholesky
Decomposition.  Defaults to suggested (.Machine$double.eps)^(1/3)</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_cholaccept">cholAccept</code></td>
<td>
<p>Tolerance to accept a Generalized Cholesky
Decomposition for a R or S matrix.</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_resetetap">resetEtaP</code></td>
<td>
<p>represents the p-value for reseting the
individual ETA to 0 during optimization (instead of the saved
value).  The two test statistics used in the z-test are either
chol(omega^-1) %*% eta or eta/sd(allEtas).  A p-value of 0
indicates the ETAs never reset.  A p-value of 1 indicates the
ETAs always reset.</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_resetthetap">resetThetaP</code></td>
<td>
<p>represents the p-value for reseting the
population mu-referenced THETA parameters based on ETA drift
during optimization, and resetting the optimization.  A
p-value of 0 indicates the THETAs never reset.  A p-value of 1
indicates the THETAs always reset and is not allowed.  The
theta reset is checked at the beginning and when nearing a
local minima.  The percent change in objective function where
a theta reset check is initiated is controlled in
<code>resetThetaCheckPer</code>.</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_resetthetafinalp">resetThetaFinalP</code></td>
<td>
<p>represents the p-value for reseting the
population mu-referenced THETA parameters based on ETA drift
during optimization, and resetting the optimization one final time.</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_diagomegaboundupper">diagOmegaBoundUpper</code></td>
<td>
<p>This represents the upper bound of the
diagonal omega matrix.  The upper bound is given by
diag(omega)*diagOmegaBoundUpper.  If
<code>diagOmegaBoundUpper</code> is 1, there is no upper bound on
Omega.</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_diagomegaboundlower">diagOmegaBoundLower</code></td>
<td>
<p>This represents the lower bound of the
diagonal omega matrix.  The lower bound is given by
diag(omega)/diagOmegaBoundUpper.  If
<code>diagOmegaBoundLower</code> is 1, there is no lower bound on
Omega.</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_cholseopt">cholSEOpt</code></td>
<td>
<p>Boolean indicating if the generalized Cholesky
should be used while optimizing.</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_cholsecov">cholSECov</code></td>
<td>
<p>Boolean indicating if the generalized Cholesky
should be used while calculating the Covariance Matrix.</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_fo">fo</code></td>
<td>
<p>is a boolean indicating if this is a FO approximation routine.</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_covtryharder">covTryHarder</code></td>
<td>
<p>If the R matrix is non-positive definite and
cannot be corrected to be non-positive definite try estimating
the Hessian on the unscaled parameter space.</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_outeropt">outerOpt</code></td>
<td>
<p>optimization method for the outer problem</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_inneropt">innerOpt</code></td>
<td>
<p>optimization method for the inner problem (not
implemented yet.)</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_rhobeg">rhobeg</code></td>
<td>
<p>Beginning change in parameters for bobyqa algorithm
(trust region).  By default this is 0.2 or 20
parameters when the parameters are scaled to 1. rhobeg and
rhoend must be set to the initial and final values of a trust
region radius, so both must be positive with 0 &lt; rhoend &lt;
rhobeg. Typically rhobeg should be about one tenth of the
greatest expected change to a variable.  Note also that
smallest difference abs(upper-lower) should be greater than or
equal to rhobeg*2. If this is not the case then rhobeg will be
adjusted. (bobyqa)</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_rhoend">rhoend</code></td>
<td>
<p>The smallest value of the trust region radius that
is allowed. If not defined, then 10^(-sigdig-1) will be used. (bobyqa)</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_npt">npt</code></td>
<td>
<p>The number of points used to approximate the objective
function via a quadratic approximation for bobyqa. The value
of npt must be in the interval [n+2,(n+1)(n+2)/2] where n is
the number of parameters in par. Choices that exceed 2*n+1 are
not recommended. If not defined, it will be set to 2*n + 1. (bobyqa)</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_rel.tol">rel.tol</code></td>
<td>
<p>Relative tolerance before nlminb stops (nlmimb).</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_x.tol">x.tol</code></td>
<td>
<p>X tolerance for nlmixr2 optimizer</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_eval.max">eval.max</code></td>
<td>
<p>Number of maximum evaluations of the objective function (nlmimb)</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_iter.max">iter.max</code></td>
<td>
<p>Maximum number of iterations allowed (nlmimb)</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_abstol">abstol</code></td>
<td>
<p>Absolute tolerance for nlmixr2 optimizer (BFGS)</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_reltol">reltol</code></td>
<td>
<p>tolerance for nlmixr2 (BFGS)</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_resethessianandeta">resetHessianAndEta</code></td>
<td>
<p>is a boolean representing if the
individual Hessian is reset when ETAs are reset using the
option <code>resetEtaP</code>.</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_statetrim">stateTrim</code></td>
<td>
<p>Trim state amounts/concentrations to this value.</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_shi21maxouter">shi21maxOuter</code></td>
<td>
<p>The maximum number of steps for the
optimization of the forward-difference step size.  When not zero,
use this instead of Gill differences.</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_shi21maxinner">shi21maxInner</code></td>
<td>
<p>The maximum number of steps for the
optimization of the individual Hessian matrices in the
generalized likelihood problem. When 0, un-optimized finite differences
are used.</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_shi21maxinnercov">shi21maxInnerCov</code></td>
<td>
<p>The maximum number of steps for the
optimization of the individual Hessian matrices in the
generalized likelihood problem for the covariance step. When 0,
un-optimized finite differences are used.</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_shi21maxfd">shi21maxFD</code></td>
<td>
<p>The maximum number of steps for the optimization
of the forward difference step size when using dosing events (lag
time, modeled duration/rate and bioavailability)</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_gillk">gillK</code></td>
<td>
<p>The total number of possible steps to determine the
optimal forward/central difference step size per parameter (by
the Gill 1983 method).  If 0, no optimal step size is
determined.  Otherwise this is the optimal step size
determined.</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_gillstep">gillStep</code></td>
<td>
<p>When looking for the optimal forward difference
step size, this is This is the step size to increase the
initial estimate by.  So each iteration the new step size =
(prior step size)*gillStep</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_gillftol">gillFtol</code></td>
<td>
<p>The gillFtol is the gradient error tolerance that
is acceptable before issuing a warning/error about the gradient estimates.</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_gillrtol">gillRtol</code></td>
<td>
<p>The relative tolerance used for Gill 1983
determination of optimal step size.</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_gillkcov">gillKcov</code></td>
<td>
<p>The total number of possible steps to determine
the optimal forward/central difference step size per parameter
(by the Gill 1983 method) during the covariance step.  If 0,
no optimal step size is determined.  Otherwise this is the
optimal step size determined.</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_gillkcovllik">gillKcovLlik</code></td>
<td>
<p>The total number of possible steps to determine
the optimal forward/central difference step per parameter when
using the generalized focei log-likelihood method (by the Gill
1986 method).  If 0, no optimal step size is
determined. Otherwise this is the optimal step size is determined</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_gillstepcovllik">gillStepCovLlik</code></td>
<td>
<p>Same as above but during generalized focei
log-likelihood</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_gillstepcov">gillStepCov</code></td>
<td>
<p>When looking for the optimal forward difference
step size, this is This is the step size to increase the
initial estimate by.  So each iteration during the covariance
step is equal to the new step size = (prior step size)*gillStepCov</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_gillftolcov">gillFtolCov</code></td>
<td>
<p>The gillFtol is the gradient error tolerance
that is acceptable before issuing a warning/error about the
gradient estimates during the covariance step.</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_gillftolcovllik">gillFtolCovLlik</code></td>
<td>
<p>Same as above but applied during generalized
log-likelihood estimation.</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_rmatnorm">rmatNorm</code></td>
<td>
<p>A parameter to normalize gradient step size by the
parameter value during the calculation of the R matrix</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_rmatnormllik">rmatNormLlik</code></td>
<td>
<p>A parameter to normalize gradient step size by
the parameter value during the calculation of the R matrix if you
are using generalized log-likelihood Hessian matrix.</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_smatnorm">smatNorm</code></td>
<td>
<p>A parameter to normalize gradient step size by the
parameter value during the calculation of the S matrix</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_smatnormllik">smatNormLlik</code></td>
<td>
<p>A parameter to normalize gradient step size by
the parameter value during the calculation of the S matrix if you
are using the generalized log-likelihood.</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_covgillf">covGillF</code></td>
<td>
<p>Use the Gill calculated optimal Forward difference
step size for the instead of the central difference step size
during the central difference gradient calculation.</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_optgillf">optGillF</code></td>
<td>
<p>Use the Gill calculated optimal Forward difference
step size for the instead of the central difference step size
during the central differences for optimization.</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_covsmall">covSmall</code></td>
<td>
<p>The covSmall is the small number to compare
covariance numbers before rejecting an estimate of the
covariance as the final estimate (when comparing sandwich vs
R/S matrix estimates of the covariance).  This number controls
how small the variance is before the covariance matrix is
rejected.</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_adjlik">adjLik</code></td>
<td>
<p>In nlmixr2, the objective function matches NONMEM's
objective function, which removes a 2*pi constant from the
likelihood calculation. If this is TRUE, the likelihood
function is adjusted by this 2*pi factor.  When adjusted this
number more closely matches the likelihood approximations of
nlme, and SAS approximations.  Regardless of if this is turned
on or off the objective function matches NONMEM's objective
function.</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_gradtrim">gradTrim</code></td>
<td>
<p>The parameter to adjust the gradient to if the
|gradient| is very large.</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_maxoderecalc">maxOdeRecalc</code></td>
<td>
<p>Maximum number of times to reduce the ODE
tolerances and try to resolve the system if there was a bad
ODE solve.</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_oderecalcfactor">odeRecalcFactor</code></td>
<td>
<p>The ODE recalculation factor when ODE
solving goes bad, this is the factor the rtol/atol is reduced</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_gradcalccentralsmall">gradCalcCentralSmall</code></td>
<td>
<p>A small number that represents the value
where |grad| &lt; gradCalcCentralSmall where forward differences
switch to central differences.</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_gradcalccentrallarge">gradCalcCentralLarge</code></td>
<td>
<p>A large number that represents the value
where |grad| &gt; gradCalcCentralLarge where forward differences
switch to central differences.</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_etanudge">etaNudge</code></td>
<td>
<p>By default initial ETA estimates start at zero;
Sometimes this doesn't optimize appropriately.  If this value is
non-zero, when the n1qn1 optimization didn't perform
appropriately, reset the Hessian, and nudge the ETA up by this
value; If the ETA still doesn't move, nudge the ETA down by this
value. By default this value is qnorm(1-0.05/2)*1/sqrt(3), the
first of the Gauss Quadrature numbers times by the 0.95% normal
region. If this is not successful try the second eta nudge
number (below).  If +-etaNudge2 is not successful, then assign
to zero and do not optimize any longer</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_etanudge2">etaNudge2</code></td>
<td>
<p>This is the second eta nudge.  By default it is
qnorm(1-0.05/2)*sqrt(3/5), which is the n=3 quadrature point
(excluding zero) times by the 0.95% normal region</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_nretries">nRetries</code></td>
<td>
<p>If FOCEi doesn't fit with the current parameter
estimates, randomly sample new parameter estimates and restart
the problem.  This is similar to 'PsN' resampling.</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_seed">seed</code></td>
<td>
<p>an object specifying if and how the random number
generator should be initialized</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_resetthetacheckper">resetThetaCheckPer</code></td>
<td>
<p>represents objective function
% percentage below which resetThetaP is checked.</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_etamat">etaMat</code></td>
<td>
<p>Eta matrix for initial estimates or final estimates
of the ETAs.</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_repeatgillmax">repeatGillMax</code></td>
<td>
<p>If the tolerances were reduced when
calculating the initial Gill differences, the Gill difference
is repeated up to a maximum number of times defined by this
parameter.</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_stickyrecalcn">stickyRecalcN</code></td>
<td>
<p>The number of bad ODE solves before reducing
the atol/rtol for the rest of the problem.</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_gradprogressofvtime">gradProgressOfvTime</code></td>
<td>
<p>This is the time for a single objective
function evaluation (in seconds) to start progress bars on gradient evaluations</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_addprop">addProp</code></td>
<td>
<p>specifies the type of additive plus proportional
errors, the one where standard deviations add (combined1) or the
type where the variances add (combined2).
</p>
<p>The combined1 error type can be described by the following equation:
</p>
<p style="text-align: center;"><code class="reqn">y = f + (a + b\times f^c) \times \varepsilon</code>
</p>

<p>The combined2 error model can be described by the following equation:
</p>
<p style="text-align: center;"><code class="reqn">y = f + \sqrt{a^2 + b^2\times f^{2\times c}} \times \varepsilon</code>
</p>

<p>Where:
</p>
<p>- y represents the observed value
</p>
<p>- f represents the predicted value
</p>
<p>- a  is the additive standard deviation
</p>
<p>- b is the proportional/power standard deviation
</p>
<p>- c is the power exponent (in the proportional case c=1)</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_badsolveobjfadj">badSolveObjfAdj</code></td>
<td>
<p>The objective function adjustment when the
ODE system cannot be solved.  It is based on each individual bad
solve.</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_compress">compress</code></td>
<td>
<p>Should the object have compressed items</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_rxcontrol">rxControl</code></td>
<td>
<p>'rxode2' ODE solving options during fitting, created with 'rxControl()'</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_sigdigtable">sigdigTable</code></td>
<td>
<p>Significant digits in the final output table.
If not specified, then it matches the significant digits in the
'sigdig' optimization algorithm.  If 'sigdig' is NULL, use 3.</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_fallbackfd">fallbackFD</code></td>
<td>
<p>Fallback to the finite differences if the
sensitivity equations do not solve.</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_smatper">smatPer</code></td>
<td>
<p>A percentage representing the number of failed
parameter gradients for each individual (which are replaced with
the overall gradient for the parameter) out of the total number
of gradients parameters (ie 'ntheta*nsub') before the S matrix is
considered to be a bad matrix.</p>
</td></tr>
<tr><td><code id="foceiControl_+3A_sdlowerfact">sdLowerFact</code></td>
<td>
<p>A factor for multiplying the estimate by when
the lower estimate is zero and the error is known to represent a
standard deviation of a parameter (like add.sd, prop.sd, pow.sd,
lnorm.sd, etc).  When zero, no factor is applied.  If your
initial estimate is 0.15 and your lower bound is zero, then the
lower bound would be assumed to be 0.00015.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>Note this uses the R's L-BFGS-B in <code><a href="stats.html#topic+optim">optim</a></code> for the
outer problem and the BFGS <code><a href="n1qn1.html#topic+n1qn1">n1qn1</a></code> with that
allows restoring the prior individual Hessian (for faster
optimization speed).
</p>
<p>However the inner problem is not scaled.  Since most eta estimates
start near zero, scaling for these parameters do not make sense.
</p>
<p>This process of scaling can fix some ill conditioning for the
unscaled problem.  The covariance step is performed on the
unscaled problem, so the condition number of that matrix may not
be reflective of the scaled problem's condition-number.
</p>


<h3>Value</h3>

<p>The control object that changes the options for the FOCEi
family of estimation methods
</p>


<h3>Author(s)</h3>

<p>Matthew L. Fidler
</p>


<h3>References</h3>

<p>Gill, P.E., Murray, W., Saunders, M.A., &amp; Wright,
M.H. (1983). Computing Forward-Difference Intervals for Numerical
Optimization. Siam Journal on Scientific and Statistical Computing,
4, 310-321.
</p>
<p>Shi, H.M., Xie, Y., Xuan, M.Q., &amp; Nocedal, J. (2021). Adaptive
Finite-Difference Interval Estimation for Noisy Derivative-Free
Optimization.
</p>


<h3>See Also</h3>

<p><code><a href="stats.html#topic+optim">optim</a></code>
</p>
<p><code><a href="n1qn1.html#topic+n1qn1">n1qn1</a></code>
</p>
<p><code><a href="rxode2.html#topic+rxSolve">rxSolve</a></code>
</p>
<p>Other Estimation control: 
<code><a href="nlmixr2est.html#topic+nlmixr2NlmeControl">nlmixr2NlmeControl</a>()</code>,
<code><a href="nlmixr2est.html#topic+saemControl">saemControl</a>()</code>
</p>

<hr>
<h2 id='lbfgsb3cControl'>Control for lbfgsb3c estimation method in nlmixr2</h2><span id='topic+lbfgsb3cControl'></span>

<h3>Description</h3>

<p>Control for lbfgsb3c estimation method in nlmixr2
</p>


<h3>Usage</h3>

<pre><code class='language-R'>lbfgsb3cControl(
  trace = 0,
  factr = 1e+07,
  pgtol = 0,
  abstol = 0,
  reltol = 0,
  lmm = 5L,
  maxit = 10000L,
  returnLbfgsb3c = FALSE,
  stickyRecalcN = 4,
  maxOdeRecalc = 5,
  odeRecalcFactor = 10^(0.5),
  useColor = crayon::has_color(),
  printNcol = floor((getOption("width") - 23)/12),
  print = 1L,
  normType = c("rescale2", "mean", "rescale", "std", "len", "constant"),
  scaleType = c("nlmixr2", "norm", "mult", "multAdd"),
  scaleCmax = 1e+05,
  scaleCmin = 1e-05,
  scaleC = NULL,
  scaleTo = 1,
  gradTo = 1,
  rxControl = NULL,
  optExpression = TRUE,
  sumProd = FALSE,
  literalFix = TRUE,
  addProp = c("combined2", "combined1"),
  calcTables = TRUE,
  compress = TRUE,
  covMethod = c("r", ""),
  adjObf = TRUE,
  ci = 0.95,
  sigdig = 4,
  sigdigTable = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="lbfgsb3cControl_+3A_trace">trace</code></td>
<td>
<p>If positive, tracing information on the progress of
the optimization is produced. Higher values may produce more
tracing information: for method &quot;L-BFGS-B&quot; there are six levels
of tracing. (To understand exactly what these do see the source
code: higher levels give more detail.)</p>
</td></tr>
<tr><td><code id="lbfgsb3cControl_+3A_factr">factr</code></td>
<td>
<p>controls the convergence of the &quot;L-BFGS-B&quot; method.
Convergence occurs when the reduction in the objective is within
this factor of the machine tolerance. Default is 1e7, that is a
tolerance of about 1e-8.</p>
</td></tr>
<tr><td><code id="lbfgsb3cControl_+3A_pgtol">pgtol</code></td>
<td>
<p>helps control the convergence of the &quot;L-BFGS-B&quot;
method. It is a tolerance on the projected gradient in the
current search direction. This defaults to zero, when the check
is suppressed.</p>
</td></tr>
<tr><td><code id="lbfgsb3cControl_+3A_abstol">abstol</code></td>
<td>
<p>helps control the convergence of the &quot;L-BFGS-B&quot;
method. It is an absolute tolerance difference in x values. This
defaults to zero, when the check is suppressed.</p>
</td></tr>
<tr><td><code id="lbfgsb3cControl_+3A_reltol">reltol</code></td>
<td>
<p>helps control the convergence of the &quot;L-BFGS-B&quot;
method. It is an relative tolerance difference in x values. This
defaults to zero, when the check is suppressed.</p>
</td></tr>
<tr><td><code id="lbfgsb3cControl_+3A_lmm">lmm</code></td>
<td>
<p>is an integer giving the number of BFGS updates retained
in the &quot;L-BFGS-B&quot; method, It defaults to 5.</p>
</td></tr>
<tr><td><code id="lbfgsb3cControl_+3A_maxit">maxit</code></td>
<td>
<p>maximum number of iterations.</p>
</td></tr>
<tr><td><code id="lbfgsb3cControl_+3A_returnlbfgsb3c">returnLbfgsb3c</code></td>
<td>
<p>return the lbfgsb3c output instead of the nlmixr2
fit</p>
</td></tr>
<tr><td><code id="lbfgsb3cControl_+3A_stickyrecalcn">stickyRecalcN</code></td>
<td>
<p>The number of bad ODE solves before reducing
the atol/rtol for the rest of the problem.</p>
</td></tr>
<tr><td><code id="lbfgsb3cControl_+3A_maxoderecalc">maxOdeRecalc</code></td>
<td>
<p>Maximum number of times to reduce the ODE
tolerances and try to resolve the system if there was a bad
ODE solve.</p>
</td></tr>
<tr><td><code id="lbfgsb3cControl_+3A_oderecalcfactor">odeRecalcFactor</code></td>
<td>
<p>The ODE recalculation factor when ODE
solving goes bad, this is the factor the rtol/atol is reduced</p>
</td></tr>
<tr><td><code id="lbfgsb3cControl_+3A_usecolor">useColor</code></td>
<td>
<p>Boolean indicating if focei can use ASCII color codes</p>
</td></tr>
<tr><td><code id="lbfgsb3cControl_+3A_printncol">printNcol</code></td>
<td>
<p>Number of columns to printout before wrapping
parameter estimates/gradient</p>
</td></tr>
<tr><td><code id="lbfgsb3cControl_+3A_print">print</code></td>
<td>
<p>Integer representing when the outer step is
printed. When this is 0 or do not print the iterations.  1 is
print every function evaluation (default), 5 is print every 5
evaluations.</p>
</td></tr>
<tr><td><code id="lbfgsb3cControl_+3A_normtype">normType</code></td>
<td>
<p>This is the type of parameter
normalization/scaling used to get the scaled initial values
for nlmixr2.  These are used with <code>scaleType</code> of.
</p>
<p>With the exception of <code>rescale2</code>, these come
from
<a href="https://en.wikipedia.org/wiki/Feature_scaling">Feature
Scaling</a>. The <code>rescale2</code> The rescaling is the same type
described in the
<a href="http://apmonitor.com/me575/uploads/Main/optimization_book.pdf">OptdesX</a>
software manual.
</p>
<p>In general, all all scaling formula can be described by:
</p>
<p style="text-align: center;"><code class="reqn">v_{scaled}</code>
</p>
<p> = (</p>
<p style="text-align: center;"><code class="reqn">v_{unscaled}-C_{1}</code>
</p>
<p>)/</p>
<p style="text-align: center;"><code class="reqn">C_{2}</code>
</p>

<p>Where
</p>
<p>The other data normalization approaches follow the following formula
</p>
<p style="text-align: center;"><code class="reqn">v_{scaled}</code>
</p>
<p> = (</p>
<p style="text-align: center;"><code class="reqn">v_{unscaled}-C_{1}</code>
</p>
<p>)/</p>
<p style="text-align: center;"><code class="reqn">C_{2}</code>
</p>


<ul>
<li> <p><code>rescale2</code> This scales all parameters from (-1 to 1).
The relative differences between the parameters are preserved
with this approach and the constants are:
</p>
<p style="text-align: center;"><code class="reqn">C_{1}</code>
</p>
<p> = (max(all unscaled values)+min(all unscaled values))/2
</p>
<p style="text-align: center;"><code class="reqn">C_{2}</code>
</p>
<p> = (max(all unscaled values) - min(all unscaled values))/2
</p>
</li>
<li> <p><code>rescale</code> or min-max normalization. This rescales all
parameters from (0 to 1).  As in the <code>rescale2</code> the
relative differences are preserved.  In this approach:
</p>
<p style="text-align: center;"><code class="reqn">C_{1}</code>
</p>
<p> = min(all unscaled values)
</p>
<p style="text-align: center;"><code class="reqn">C_{2}</code>
</p>
<p> = max(all unscaled values) - min(all unscaled values)
</p>
</li>
<li> <p><code>mean</code> or mean normalization.  This rescales to center
the parameters around the mean but the parameters are from 0
to 1.  In this approach:
</p>
<p style="text-align: center;"><code class="reqn">C_{1}</code>
</p>
<p> = mean(all unscaled values)
</p>
<p style="text-align: center;"><code class="reqn">C_{2}</code>
</p>
<p> = max(all unscaled values) - min(all unscaled values)
</p>
</li>
<li> <p><code>std</code> or standardization.  This standardizes by the mean
and standard deviation.  In this approach:
</p>
<p style="text-align: center;"><code class="reqn">C_{1}</code>
</p>
<p> = mean(all unscaled values)
</p>
<p style="text-align: center;"><code class="reqn">C_{2}</code>
</p>
<p> = sd(all unscaled values)
</p>
</li>
<li> <p><code>len</code> or unit length scaling.  This scales the
parameters to the unit length.  For this approach we use the Euclidean length, that
is:
</p>
<p style="text-align: center;"><code class="reqn">C_{1}</code>
</p>
<p> = 0
</p>
<p style="text-align: center;"><code class="reqn">C_{2}</code>
</p>
<p> = </p>
<p style="text-align: center;"><code class="reqn">\sqrt(v_1^2 + v_2^2 + \cdots + v_n^2)</code>
</p>

</li>
<li> <p><code>constant</code> which does not perform data normalization. That is
</p>
<p style="text-align: center;"><code class="reqn">C_{1}</code>
</p>
<p> = 0
</p>
<p style="text-align: center;"><code class="reqn">C_{2}</code>
</p>
<p> = 1
</p>
</li></ul>
</td></tr>
<tr><td><code id="lbfgsb3cControl_+3A_scaletype">scaleType</code></td>
<td>
<p>The scaling scheme for nlmixr2.  The supported types are:
</p>

<ul>
<li> <p><code>nlmixr2</code>  In this approach the scaling is performed by the following equation:
</p>
<p style="text-align: center;"><code class="reqn">v_{scaled}</code>
</p>
<p> = (</p>
<p style="text-align: center;"><code class="reqn">v_{current} - v_{init}</code>
</p>
<p>)*scaleC[i] + scaleTo
</p>
<p>The <code>scaleTo</code> parameter is specified by the <code>normType</code>,
and the scales are specified by <code>scaleC</code>.
</p>
</li>
<li> <p><code>norm</code> This approach uses the simple scaling provided
by the <code>normType</code> argument.
</p>
</li>
<li> <p><code>mult</code> This approach does not use the data
normalization provided by <code>normType</code>, but rather uses
multiplicative scaling to a constant provided by the <code>scaleTo</code>
argument.
</p>
<p>In this case:
</p>
<p style="text-align: center;"><code class="reqn">v_{scaled}</code>
</p>
<p> = </p>
<p style="text-align: center;"><code class="reqn">v_{current}</code>
</p>
<p>/</p>
<p style="text-align: center;"><code class="reqn">v_{init}</code>
</p>
<p>*scaleTo
</p>
</li>
<li> <p><code>multAdd</code> This approach changes the scaling based on
the parameter being specified.  If a parameter is defined in an
exponential block (ie exp(theta)), then it is scaled on a
linearly, that is:
</p>
<p style="text-align: center;"><code class="reqn">v_{scaled}</code>
</p>
<p> = (</p>
<p style="text-align: center;"><code class="reqn">v_{current}-v_{init}</code>
</p>
<p>) + scaleTo
</p>
<p>Otherwise the parameter is scaled multiplicatively.
</p>
<p style="text-align: center;"><code class="reqn">v_{scaled}</code>
</p>
<p> = </p>
<p style="text-align: center;"><code class="reqn">v_{current}</code>
</p>
<p>/</p>
<p style="text-align: center;"><code class="reqn">v_{init}</code>
</p>
<p>*scaleTo
</p>
</li></ul>
</td></tr>
<tr><td><code id="lbfgsb3cControl_+3A_scalecmax">scaleCmax</code></td>
<td>
<p>Maximum value of the scaleC to prevent overflow.</p>
</td></tr>
<tr><td><code id="lbfgsb3cControl_+3A_scalecmin">scaleCmin</code></td>
<td>
<p>Minimum value of the scaleC to prevent underflow.</p>
</td></tr>
<tr><td><code id="lbfgsb3cControl_+3A_scalec">scaleC</code></td>
<td>
<p>The scaling constant used with
<code>scaleType=nlmixr2</code>.  When not specified, it is based on
the type of parameter that is estimated.  The idea is to keep
the derivatives similar on a log scale to have similar
gradient sizes.  Hence parameters like log(exp(theta)) would
have a scaling factor of 1 and log(theta) would have a scaling
factor of ini_value (to scale by 1/value; ie
d/dt(log(ini_value)) = 1/ini_value or scaleC=ini_value)
</p>

<ul>
<li><p> For parameters in an exponential (ie exp(theta)) or
parameters specifying powers, boxCox or yeoJohnson
transformations , this is 1.
</p>
</li>
<li><p> For additive, proportional, lognormal error structures,
these are given by 0.5*abs(initial_estimate)
</p>
</li>
<li><p> Factorials are scaled by abs(1/digamma(initial_estimate+1))
</p>
</li>
<li><p> parameters in a log scale (ie log(theta)) are transformed
by log(abs(initial_estimate))*abs(initial_estimate)
</p>
</li></ul>

<p>These parameter scaling coefficients are chose to try to keep
similar slopes among parameters.  That is they all follow the
slopes approximately on a log-scale.
</p>
<p>While these are chosen in a logical manner, they may not always
apply.  You can specify each parameters scaling factor by this
parameter if you wish.</p>
</td></tr>
<tr><td><code id="lbfgsb3cControl_+3A_scaleto">scaleTo</code></td>
<td>
<p>Scale the initial parameter estimate to this value.
By default this is 1.  When zero or below, no scaling is performed.</p>
</td></tr>
<tr><td><code id="lbfgsb3cControl_+3A_gradto">gradTo</code></td>
<td>
<p>this is the factor that the gradient is scaled to
before optimizing.  This only works with
scaleType=&quot;nlmixr2&quot;.</p>
</td></tr>
<tr><td><code id="lbfgsb3cControl_+3A_rxcontrol">rxControl</code></td>
<td>
<p>'rxode2' ODE solving options during fitting, created with 'rxControl()'</p>
</td></tr>
<tr><td><code id="lbfgsb3cControl_+3A_optexpression">optExpression</code></td>
<td>
<p>Optimize the rxode2 expression to speed up
calculation. By default this is turned on.</p>
</td></tr>
<tr><td><code id="lbfgsb3cControl_+3A_sumprod">sumProd</code></td>
<td>
<p>Is a boolean indicating if the model should change
multiplication to high precision multiplication and sums to
high precision sums using the PreciseSums package.  By default
this is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="lbfgsb3cControl_+3A_literalfix">literalFix</code></td>
<td>
<p>boolean, substitute fixed population values as
literals and re-adjust ui and parameter estimates after
optimization; Default is 'TRUE'.</p>
</td></tr>
<tr><td><code id="lbfgsb3cControl_+3A_addprop">addProp</code></td>
<td>
<p>specifies the type of additive plus proportional
errors, the one where standard deviations add (combined1) or the
type where the variances add (combined2).
</p>
<p>The combined1 error type can be described by the following equation:
</p>
<p style="text-align: center;"><code class="reqn">y = f + (a + b\times f^c) \times \varepsilon</code>
</p>

<p>The combined2 error model can be described by the following equation:
</p>
<p style="text-align: center;"><code class="reqn">y = f + \sqrt{a^2 + b^2\times f^{2\times c}} \times \varepsilon</code>
</p>

<p>Where:
</p>
<p>- y represents the observed value
</p>
<p>- f represents the predicted value
</p>
<p>- a  is the additive standard deviation
</p>
<p>- b is the proportional/power standard deviation
</p>
<p>- c is the power exponent (in the proportional case c=1)</p>
</td></tr>
<tr><td><code id="lbfgsb3cControl_+3A_calctables">calcTables</code></td>
<td>
<p>This boolean is to determine if the foceiFit
will calculate tables. By default this is <code>TRUE</code></p>
</td></tr>
<tr><td><code id="lbfgsb3cControl_+3A_compress">compress</code></td>
<td>
<p>Should the object have compressed items</p>
</td></tr>
<tr><td><code id="lbfgsb3cControl_+3A_covmethod">covMethod</code></td>
<td>
<p>Method for calculating covariance.  In this
discussion, R is the Hessian matrix of the objective
function. The S matrix is the sum of individual
gradient cross-product (evaluated at the individual empirical
Bayes estimates).
</p>

<ul>
<li><p> &quot;<code>r,s</code>&quot; Uses the sandwich matrix to calculate the
covariance, that is: <code>solve(R) %*% S %*% solve(R)</code>
</p>
</li>
<li><p> &quot;<code>r</code>&quot; Uses the Hessian matrix to calculate the
covariance as <code>2 %*% solve(R)</code>
</p>
</li>
<li><p> &quot;<code>s</code>&quot; Uses the cross-product matrix to calculate the
covariance as <code>4 %*% solve(S)</code>
</p>
</li>
<li><p> &quot;&quot; Does not calculate the covariance step.
</p>
</li></ul>
</td></tr>
<tr><td><code id="lbfgsb3cControl_+3A_adjobf">adjObf</code></td>
<td>
<p>is a boolean to indicate if the objective function
should be adjusted to be closer to NONMEM's default objective
function.  By default this is <code>TRUE</code></p>
</td></tr>
<tr><td><code id="lbfgsb3cControl_+3A_ci">ci</code></td>
<td>
<p>Confidence level for some tables.  By default this is
0.95 or 95% confidence.</p>
</td></tr>
<tr><td><code id="lbfgsb3cControl_+3A_sigdig">sigdig</code></td>
<td>
<p>Optimization significant digits. This controls:
</p>

<ul>
<li><p> The tolerance of the inner and outer optimization is <code>10^-sigdig</code>
</p>
</li>
<li><p> The tolerance of the ODE solvers is
<code>0.5*10^(-sigdig-2)</code>; For the sensitivity equations and
steady-state solutions the default is <code>0.5*10^(-sigdig-1.5)</code>
(sensitivity changes only applicable for liblsoda)
</p>
</li>
<li><p> The tolerance of the boundary check is <code>5 * 10 ^ (-sigdig + 1)</code>
</p>
</li></ul>
</td></tr>
<tr><td><code id="lbfgsb3cControl_+3A_sigdigtable">sigdigTable</code></td>
<td>
<p>Significant digits in the final output table.
If not specified, then it matches the significant digits in the
'sigdig' optimization algorithm.  If 'sigdig' is NULL, use 3.</p>
</td></tr>
<tr><td><code id="lbfgsb3cControl_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <code><a href="nlmixr2est.html#topic+lbfgsb3cControl">nlmixr2est::lbfgsb3cControl()</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>bobqya control structure
</p>


<h3>Author(s)</h3>

<p>Matthew L. Fidler
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# A logit regression example with emax model

dsn &lt;- data.frame(i=1:1000)
dsn$time &lt;- exp(rnorm(1000))
dsn$DV=rbinom(1000,1,exp(-1+dsn$time)/(1+exp(-1+dsn$time)))

mod &lt;- function() {
 ini({
   E0 &lt;- 0.5
   Em &lt;- 0.5
   E50 &lt;- 2
   g &lt;- fix(2)
 })
 model({
   v &lt;- E0+Em*time^g/(E50^g+time^g)
   ll(bin) ~ DV * v - log(1 + exp(v))
 })
}

fit2 &lt;- nlmixr(mod, dsn, est="lbfgsb3c")

print(fit2)

# you can also get the nlm output with fit2$lbfgsb3c

fit2$lbfgsb3c

# The nlm control has been modified slightly to include
# extra components and name the parameters

</code></pre>

<hr>
<h2 id='n1qn1Control'>Control for n1qn1 estimation method in nlmixr2</h2><span id='topic+n1qn1Control'></span>

<h3>Description</h3>

<p>Control for n1qn1 estimation method in nlmixr2
</p>


<h3>Usage</h3>

<pre><code class='language-R'>n1qn1Control(
  epsilon = (.Machine$double.eps)^0.25,
  max_iterations = 10000,
  nsim = 10000,
  imp = 0,
  print.functions = FALSE,
  returnN1qn1 = FALSE,
  stickyRecalcN = 4,
  maxOdeRecalc = 5,
  odeRecalcFactor = 10^(0.5),
  useColor = crayon::has_color(),
  printNcol = floor((getOption("width") - 23)/12),
  print = 1L,
  normType = c("rescale2", "mean", "rescale", "std", "len", "constant"),
  scaleType = c("nlmixr2", "norm", "mult", "multAdd"),
  scaleCmax = 1e+05,
  scaleCmin = 1e-05,
  scaleC = NULL,
  scaleTo = 1,
  gradTo = 1,
  rxControl = NULL,
  optExpression = TRUE,
  sumProd = FALSE,
  literalFix = TRUE,
  addProp = c("combined2", "combined1"),
  calcTables = TRUE,
  compress = TRUE,
  covMethod = c("r", "n1qn1", ""),
  adjObf = TRUE,
  ci = 0.95,
  sigdig = 4,
  sigdigTable = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="n1qn1Control_+3A_epsilon">epsilon</code></td>
<td>
<p>Precision of estimate for n1qn1 optimization.</p>
</td></tr>
<tr><td><code id="n1qn1Control_+3A_max_iterations">max_iterations</code></td>
<td>
<p>Number of iterations</p>
</td></tr>
<tr><td><code id="n1qn1Control_+3A_nsim">nsim</code></td>
<td>
<p>Number of function evaluations</p>
</td></tr>
<tr><td><code id="n1qn1Control_+3A_imp">imp</code></td>
<td>
<p>Verbosity of messages.</p>
</td></tr>
<tr><td><code id="n1qn1Control_+3A_print.functions">print.functions</code></td>
<td>
<p>Boolean to control if the function value
and parameter estimates are echoed every time a function is
called.</p>
</td></tr>
<tr><td><code id="n1qn1Control_+3A_returnn1qn1">returnN1qn1</code></td>
<td>
<p>return the n1qn1 output instead of the nlmixr2
fit</p>
</td></tr>
<tr><td><code id="n1qn1Control_+3A_stickyrecalcn">stickyRecalcN</code></td>
<td>
<p>The number of bad ODE solves before reducing
the atol/rtol for the rest of the problem.</p>
</td></tr>
<tr><td><code id="n1qn1Control_+3A_maxoderecalc">maxOdeRecalc</code></td>
<td>
<p>Maximum number of times to reduce the ODE
tolerances and try to resolve the system if there was a bad
ODE solve.</p>
</td></tr>
<tr><td><code id="n1qn1Control_+3A_oderecalcfactor">odeRecalcFactor</code></td>
<td>
<p>The ODE recalculation factor when ODE
solving goes bad, this is the factor the rtol/atol is reduced</p>
</td></tr>
<tr><td><code id="n1qn1Control_+3A_usecolor">useColor</code></td>
<td>
<p>Boolean indicating if focei can use ASCII color codes</p>
</td></tr>
<tr><td><code id="n1qn1Control_+3A_printncol">printNcol</code></td>
<td>
<p>Number of columns to printout before wrapping
parameter estimates/gradient</p>
</td></tr>
<tr><td><code id="n1qn1Control_+3A_print">print</code></td>
<td>
<p>Integer representing when the outer step is
printed. When this is 0 or do not print the iterations.  1 is
print every function evaluation (default), 5 is print every 5
evaluations.</p>
</td></tr>
<tr><td><code id="n1qn1Control_+3A_normtype">normType</code></td>
<td>
<p>This is the type of parameter
normalization/scaling used to get the scaled initial values
for nlmixr2.  These are used with <code>scaleType</code> of.
</p>
<p>With the exception of <code>rescale2</code>, these come
from
<a href="https://en.wikipedia.org/wiki/Feature_scaling">Feature
Scaling</a>. The <code>rescale2</code> The rescaling is the same type
described in the
<a href="http://apmonitor.com/me575/uploads/Main/optimization_book.pdf">OptdesX</a>
software manual.
</p>
<p>In general, all all scaling formula can be described by:
</p>
<p style="text-align: center;"><code class="reqn">v_{scaled}</code>
</p>
<p> = (</p>
<p style="text-align: center;"><code class="reqn">v_{unscaled}-C_{1}</code>
</p>
<p>)/</p>
<p style="text-align: center;"><code class="reqn">C_{2}</code>
</p>

<p>Where
</p>
<p>The other data normalization approaches follow the following formula
</p>
<p style="text-align: center;"><code class="reqn">v_{scaled}</code>
</p>
<p> = (</p>
<p style="text-align: center;"><code class="reqn">v_{unscaled}-C_{1}</code>
</p>
<p>)/</p>
<p style="text-align: center;"><code class="reqn">C_{2}</code>
</p>


<ul>
<li> <p><code>rescale2</code> This scales all parameters from (-1 to 1).
The relative differences between the parameters are preserved
with this approach and the constants are:
</p>
<p style="text-align: center;"><code class="reqn">C_{1}</code>
</p>
<p> = (max(all unscaled values)+min(all unscaled values))/2
</p>
<p style="text-align: center;"><code class="reqn">C_{2}</code>
</p>
<p> = (max(all unscaled values) - min(all unscaled values))/2
</p>
</li>
<li> <p><code>rescale</code> or min-max normalization. This rescales all
parameters from (0 to 1).  As in the <code>rescale2</code> the
relative differences are preserved.  In this approach:
</p>
<p style="text-align: center;"><code class="reqn">C_{1}</code>
</p>
<p> = min(all unscaled values)
</p>
<p style="text-align: center;"><code class="reqn">C_{2}</code>
</p>
<p> = max(all unscaled values) - min(all unscaled values)
</p>
</li>
<li> <p><code>mean</code> or mean normalization.  This rescales to center
the parameters around the mean but the parameters are from 0
to 1.  In this approach:
</p>
<p style="text-align: center;"><code class="reqn">C_{1}</code>
</p>
<p> = mean(all unscaled values)
</p>
<p style="text-align: center;"><code class="reqn">C_{2}</code>
</p>
<p> = max(all unscaled values) - min(all unscaled values)
</p>
</li>
<li> <p><code>std</code> or standardization.  This standardizes by the mean
and standard deviation.  In this approach:
</p>
<p style="text-align: center;"><code class="reqn">C_{1}</code>
</p>
<p> = mean(all unscaled values)
</p>
<p style="text-align: center;"><code class="reqn">C_{2}</code>
</p>
<p> = sd(all unscaled values)
</p>
</li>
<li> <p><code>len</code> or unit length scaling.  This scales the
parameters to the unit length.  For this approach we use the Euclidean length, that
is:
</p>
<p style="text-align: center;"><code class="reqn">C_{1}</code>
</p>
<p> = 0
</p>
<p style="text-align: center;"><code class="reqn">C_{2}</code>
</p>
<p> = </p>
<p style="text-align: center;"><code class="reqn">\sqrt(v_1^2 + v_2^2 + \cdots + v_n^2)</code>
</p>

</li>
<li> <p><code>constant</code> which does not perform data normalization. That is
</p>
<p style="text-align: center;"><code class="reqn">C_{1}</code>
</p>
<p> = 0
</p>
<p style="text-align: center;"><code class="reqn">C_{2}</code>
</p>
<p> = 1
</p>
</li></ul>
</td></tr>
<tr><td><code id="n1qn1Control_+3A_scaletype">scaleType</code></td>
<td>
<p>The scaling scheme for nlmixr2.  The supported types are:
</p>

<ul>
<li> <p><code>nlmixr2</code>  In this approach the scaling is performed by the following equation:
</p>
<p style="text-align: center;"><code class="reqn">v_{scaled}</code>
</p>
<p> = (</p>
<p style="text-align: center;"><code class="reqn">v_{current} - v_{init}</code>
</p>
<p>)*scaleC[i] + scaleTo
</p>
<p>The <code>scaleTo</code> parameter is specified by the <code>normType</code>,
and the scales are specified by <code>scaleC</code>.
</p>
</li>
<li> <p><code>norm</code> This approach uses the simple scaling provided
by the <code>normType</code> argument.
</p>
</li>
<li> <p><code>mult</code> This approach does not use the data
normalization provided by <code>normType</code>, but rather uses
multiplicative scaling to a constant provided by the <code>scaleTo</code>
argument.
</p>
<p>In this case:
</p>
<p style="text-align: center;"><code class="reqn">v_{scaled}</code>
</p>
<p> = </p>
<p style="text-align: center;"><code class="reqn">v_{current}</code>
</p>
<p>/</p>
<p style="text-align: center;"><code class="reqn">v_{init}</code>
</p>
<p>*scaleTo
</p>
</li>
<li> <p><code>multAdd</code> This approach changes the scaling based on
the parameter being specified.  If a parameter is defined in an
exponential block (ie exp(theta)), then it is scaled on a
linearly, that is:
</p>
<p style="text-align: center;"><code class="reqn">v_{scaled}</code>
</p>
<p> = (</p>
<p style="text-align: center;"><code class="reqn">v_{current}-v_{init}</code>
</p>
<p>) + scaleTo
</p>
<p>Otherwise the parameter is scaled multiplicatively.
</p>
<p style="text-align: center;"><code class="reqn">v_{scaled}</code>
</p>
<p> = </p>
<p style="text-align: center;"><code class="reqn">v_{current}</code>
</p>
<p>/</p>
<p style="text-align: center;"><code class="reqn">v_{init}</code>
</p>
<p>*scaleTo
</p>
</li></ul>
</td></tr>
<tr><td><code id="n1qn1Control_+3A_scalecmax">scaleCmax</code></td>
<td>
<p>Maximum value of the scaleC to prevent overflow.</p>
</td></tr>
<tr><td><code id="n1qn1Control_+3A_scalecmin">scaleCmin</code></td>
<td>
<p>Minimum value of the scaleC to prevent underflow.</p>
</td></tr>
<tr><td><code id="n1qn1Control_+3A_scalec">scaleC</code></td>
<td>
<p>The scaling constant used with
<code>scaleType=nlmixr2</code>.  When not specified, it is based on
the type of parameter that is estimated.  The idea is to keep
the derivatives similar on a log scale to have similar
gradient sizes.  Hence parameters like log(exp(theta)) would
have a scaling factor of 1 and log(theta) would have a scaling
factor of ini_value (to scale by 1/value; ie
d/dt(log(ini_value)) = 1/ini_value or scaleC=ini_value)
</p>

<ul>
<li><p> For parameters in an exponential (ie exp(theta)) or
parameters specifying powers, boxCox or yeoJohnson
transformations , this is 1.
</p>
</li>
<li><p> For additive, proportional, lognormal error structures,
these are given by 0.5*abs(initial_estimate)
</p>
</li>
<li><p> Factorials are scaled by abs(1/digamma(initial_estimate+1))
</p>
</li>
<li><p> parameters in a log scale (ie log(theta)) are transformed
by log(abs(initial_estimate))*abs(initial_estimate)
</p>
</li></ul>

<p>These parameter scaling coefficients are chose to try to keep
similar slopes among parameters.  That is they all follow the
slopes approximately on a log-scale.
</p>
<p>While these are chosen in a logical manner, they may not always
apply.  You can specify each parameters scaling factor by this
parameter if you wish.</p>
</td></tr>
<tr><td><code id="n1qn1Control_+3A_scaleto">scaleTo</code></td>
<td>
<p>Scale the initial parameter estimate to this value.
By default this is 1.  When zero or below, no scaling is performed.</p>
</td></tr>
<tr><td><code id="n1qn1Control_+3A_gradto">gradTo</code></td>
<td>
<p>this is the factor that the gradient is scaled to
before optimizing.  This only works with
scaleType=&quot;nlmixr2&quot;.</p>
</td></tr>
<tr><td><code id="n1qn1Control_+3A_rxcontrol">rxControl</code></td>
<td>
<p>'rxode2' ODE solving options during fitting, created with 'rxControl()'</p>
</td></tr>
<tr><td><code id="n1qn1Control_+3A_optexpression">optExpression</code></td>
<td>
<p>Optimize the rxode2 expression to speed up
calculation. By default this is turned on.</p>
</td></tr>
<tr><td><code id="n1qn1Control_+3A_sumprod">sumProd</code></td>
<td>
<p>Is a boolean indicating if the model should change
multiplication to high precision multiplication and sums to
high precision sums using the PreciseSums package.  By default
this is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="n1qn1Control_+3A_literalfix">literalFix</code></td>
<td>
<p>boolean, substitute fixed population values as
literals and re-adjust ui and parameter estimates after
optimization; Default is 'TRUE'.</p>
</td></tr>
<tr><td><code id="n1qn1Control_+3A_addprop">addProp</code></td>
<td>
<p>specifies the type of additive plus proportional
errors, the one where standard deviations add (combined1) or the
type where the variances add (combined2).
</p>
<p>The combined1 error type can be described by the following equation:
</p>
<p style="text-align: center;"><code class="reqn">y = f + (a + b\times f^c) \times \varepsilon</code>
</p>

<p>The combined2 error model can be described by the following equation:
</p>
<p style="text-align: center;"><code class="reqn">y = f + \sqrt{a^2 + b^2\times f^{2\times c}} \times \varepsilon</code>
</p>

<p>Where:
</p>
<p>- y represents the observed value
</p>
<p>- f represents the predicted value
</p>
<p>- a  is the additive standard deviation
</p>
<p>- b is the proportional/power standard deviation
</p>
<p>- c is the power exponent (in the proportional case c=1)</p>
</td></tr>
<tr><td><code id="n1qn1Control_+3A_calctables">calcTables</code></td>
<td>
<p>This boolean is to determine if the foceiFit
will calculate tables. By default this is <code>TRUE</code></p>
</td></tr>
<tr><td><code id="n1qn1Control_+3A_compress">compress</code></td>
<td>
<p>Should the object have compressed items</p>
</td></tr>
<tr><td><code id="n1qn1Control_+3A_covmethod">covMethod</code></td>
<td>
<p>Method for calculating covariance.  In this
discussion, R is the Hessian matrix of the objective
function. The S matrix is the sum of individual
gradient cross-product (evaluated at the individual empirical
Bayes estimates).
</p>

<ul>
<li><p> &quot;<code>r,s</code>&quot; Uses the sandwich matrix to calculate the
covariance, that is: <code>solve(R) %*% S %*% solve(R)</code>
</p>
</li>
<li><p> &quot;<code>r</code>&quot; Uses the Hessian matrix to calculate the
covariance as <code>2 %*% solve(R)</code>
</p>
</li>
<li><p> &quot;<code>s</code>&quot; Uses the cross-product matrix to calculate the
covariance as <code>4 %*% solve(S)</code>
</p>
</li>
<li><p> &quot;&quot; Does not calculate the covariance step.
</p>
</li></ul>
</td></tr>
<tr><td><code id="n1qn1Control_+3A_adjobf">adjObf</code></td>
<td>
<p>is a boolean to indicate if the objective function
should be adjusted to be closer to NONMEM's default objective
function.  By default this is <code>TRUE</code></p>
</td></tr>
<tr><td><code id="n1qn1Control_+3A_ci">ci</code></td>
<td>
<p>Confidence level for some tables.  By default this is
0.95 or 95% confidence.</p>
</td></tr>
<tr><td><code id="n1qn1Control_+3A_sigdig">sigdig</code></td>
<td>
<p>Optimization significant digits. This controls:
</p>

<ul>
<li><p> The tolerance of the inner and outer optimization is <code>10^-sigdig</code>
</p>
</li>
<li><p> The tolerance of the ODE solvers is
<code>0.5*10^(-sigdig-2)</code>; For the sensitivity equations and
steady-state solutions the default is <code>0.5*10^(-sigdig-1.5)</code>
(sensitivity changes only applicable for liblsoda)
</p>
</li>
<li><p> The tolerance of the boundary check is <code>5 * 10 ^ (-sigdig + 1)</code>
</p>
</li></ul>
</td></tr>
<tr><td><code id="n1qn1Control_+3A_sigdigtable">sigdigTable</code></td>
<td>
<p>Significant digits in the final output table.
If not specified, then it matches the significant digits in the
'sigdig' optimization algorithm.  If 'sigdig' is NULL, use 3.</p>
</td></tr>
<tr><td><code id="n1qn1Control_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <code><a href="nlmixr2est.html#topic+n1qn1Control">nlmixr2est::n1qn1Control()</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>bobqya control structure
</p>


<h3>Author(s)</h3>

<p>Matthew L. Fidler
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# A logit regression example with emax model

dsn &lt;- data.frame(i=1:1000)
dsn$time &lt;- exp(rnorm(1000))
dsn$DV=rbinom(1000,1,exp(-1+dsn$time)/(1+exp(-1+dsn$time)))

mod &lt;- function() {
 ini({
   E0 &lt;- 0.5
   Em &lt;- 0.5
   E50 &lt;- 2
   g &lt;- fix(2)
 })
 model({
   v &lt;- E0+Em*time^g/(E50^g+time^g)
   ll(bin) ~ DV * v - log(1 + exp(v))
 })
}

fit2 &lt;- nlmixr(mod, dsn, est="n1qn1")

print(fit2)

# you can also get the nlm output with fit2$n1qn1

fit2$n1qn1

# The nlm control has been modified slightly to include
# extra components and name the parameters

</code></pre>

<hr>
<h2 id='newuoaControl'>Control for newuoa estimation method in nlmixr2</h2><span id='topic+newuoaControl'></span>

<h3>Description</h3>

<p>Control for newuoa estimation method in nlmixr2
</p>


<h3>Usage</h3>

<pre><code class='language-R'>newuoaControl(
  npt = NULL,
  rhobeg = NULL,
  rhoend = NULL,
  iprint = 0L,
  maxfun = 100000L,
  returnNewuoa = FALSE,
  stickyRecalcN = 4,
  maxOdeRecalc = 5,
  odeRecalcFactor = 10^(0.5),
  useColor = crayon::has_color(),
  printNcol = floor((getOption("width") - 23)/12),
  print = 1L,
  normType = c("rescale2", "mean", "rescale", "std", "len", "constant"),
  scaleType = c("nlmixr2", "norm", "mult", "multAdd"),
  scaleCmax = 1e+05,
  scaleCmin = 1e-05,
  scaleC = NULL,
  scaleTo = 1,
  rxControl = NULL,
  optExpression = TRUE,
  sumProd = FALSE,
  literalFix = TRUE,
  addProp = c("combined2", "combined1"),
  calcTables = TRUE,
  compress = TRUE,
  covMethod = c("r", ""),
  adjObf = TRUE,
  ci = 0.95,
  sigdig = 4,
  sigdigTable = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="newuoaControl_+3A_npt">npt</code></td>
<td>
<p>The number of points used to approximate the objective
function via a quadratic approximation for bobyqa. The value
of npt must be in the interval [n+2,(n+1)(n+2)/2] where n is
the number of parameters in par. Choices that exceed 2*n+1 are
not recommended. If not defined, it will be set to 2*n + 1. (bobyqa)</p>
</td></tr>
<tr><td><code id="newuoaControl_+3A_rhobeg">rhobeg</code></td>
<td>
<p>Beginning change in parameters for bobyqa algorithm
(trust region).  By default this is 0.2 or 20
parameters when the parameters are scaled to 1. rhobeg and
rhoend must be set to the initial and final values of a trust
region radius, so both must be positive with 0 &lt; rhoend &lt;
rhobeg. Typically rhobeg should be about one tenth of the
greatest expected change to a variable.  Note also that
smallest difference abs(upper-lower) should be greater than or
equal to rhobeg*2. If this is not the case then rhobeg will be
adjusted. (bobyqa)</p>
</td></tr>
<tr><td><code id="newuoaControl_+3A_rhoend">rhoend</code></td>
<td>
<p>The smallest value of the trust region radius that
is allowed. If not defined, then 10^(-sigdig-1) will be used. (bobyqa)</p>
</td></tr>
<tr><td><code id="newuoaControl_+3A_iprint">iprint</code></td>
<td>
<p>The value of 'iprint' should be set to an integer
value in '0, 1, 2, 3, ...', which controls the amount of
printing.  Specifically, there is no output if 'iprint=0' and
there is output only at the start and the return if 'iprint=1'.
Otherwise, each new value of 'rho' is printed, with the best
vector of variables so far and the corresponding value of the
objective function. Further, each new value of the objective
function with its variables are output if 'iprint=3'.  If 'iprint
&gt; 3', the objective function value and corresponding variables
are output every 'iprint' evaluations.  Default value is '0'.</p>
</td></tr>
<tr><td><code id="newuoaControl_+3A_maxfun">maxfun</code></td>
<td>
<p>The maximum allowed number of function
evaluations. If this is exceeded, the method will terminate.</p>
</td></tr>
<tr><td><code id="newuoaControl_+3A_returnnewuoa">returnNewuoa</code></td>
<td>
<p>return the newuoa output instead of the nlmixr2
fit</p>
</td></tr>
<tr><td><code id="newuoaControl_+3A_stickyrecalcn">stickyRecalcN</code></td>
<td>
<p>The number of bad ODE solves before reducing
the atol/rtol for the rest of the problem.</p>
</td></tr>
<tr><td><code id="newuoaControl_+3A_maxoderecalc">maxOdeRecalc</code></td>
<td>
<p>Maximum number of times to reduce the ODE
tolerances and try to resolve the system if there was a bad
ODE solve.</p>
</td></tr>
<tr><td><code id="newuoaControl_+3A_oderecalcfactor">odeRecalcFactor</code></td>
<td>
<p>The ODE recalculation factor when ODE
solving goes bad, this is the factor the rtol/atol is reduced</p>
</td></tr>
<tr><td><code id="newuoaControl_+3A_usecolor">useColor</code></td>
<td>
<p>Boolean indicating if focei can use ASCII color codes</p>
</td></tr>
<tr><td><code id="newuoaControl_+3A_printncol">printNcol</code></td>
<td>
<p>Number of columns to printout before wrapping
parameter estimates/gradient</p>
</td></tr>
<tr><td><code id="newuoaControl_+3A_print">print</code></td>
<td>
<p>Integer representing when the outer step is
printed. When this is 0 or do not print the iterations.  1 is
print every function evaluation (default), 5 is print every 5
evaluations.</p>
</td></tr>
<tr><td><code id="newuoaControl_+3A_normtype">normType</code></td>
<td>
<p>This is the type of parameter
normalization/scaling used to get the scaled initial values
for nlmixr2.  These are used with <code>scaleType</code> of.
</p>
<p>With the exception of <code>rescale2</code>, these come
from
<a href="https://en.wikipedia.org/wiki/Feature_scaling">Feature
Scaling</a>. The <code>rescale2</code> The rescaling is the same type
described in the
<a href="http://apmonitor.com/me575/uploads/Main/optimization_book.pdf">OptdesX</a>
software manual.
</p>
<p>In general, all all scaling formula can be described by:
</p>
<p style="text-align: center;"><code class="reqn">v_{scaled}</code>
</p>
<p> = (</p>
<p style="text-align: center;"><code class="reqn">v_{unscaled}-C_{1}</code>
</p>
<p>)/</p>
<p style="text-align: center;"><code class="reqn">C_{2}</code>
</p>

<p>Where
</p>
<p>The other data normalization approaches follow the following formula
</p>
<p style="text-align: center;"><code class="reqn">v_{scaled}</code>
</p>
<p> = (</p>
<p style="text-align: center;"><code class="reqn">v_{unscaled}-C_{1}</code>
</p>
<p>)/</p>
<p style="text-align: center;"><code class="reqn">C_{2}</code>
</p>


<ul>
<li> <p><code>rescale2</code> This scales all parameters from (-1 to 1).
The relative differences between the parameters are preserved
with this approach and the constants are:
</p>
<p style="text-align: center;"><code class="reqn">C_{1}</code>
</p>
<p> = (max(all unscaled values)+min(all unscaled values))/2
</p>
<p style="text-align: center;"><code class="reqn">C_{2}</code>
</p>
<p> = (max(all unscaled values) - min(all unscaled values))/2
</p>
</li>
<li> <p><code>rescale</code> or min-max normalization. This rescales all
parameters from (0 to 1).  As in the <code>rescale2</code> the
relative differences are preserved.  In this approach:
</p>
<p style="text-align: center;"><code class="reqn">C_{1}</code>
</p>
<p> = min(all unscaled values)
</p>
<p style="text-align: center;"><code class="reqn">C_{2}</code>
</p>
<p> = max(all unscaled values) - min(all unscaled values)
</p>
</li>
<li> <p><code>mean</code> or mean normalization.  This rescales to center
the parameters around the mean but the parameters are from 0
to 1.  In this approach:
</p>
<p style="text-align: center;"><code class="reqn">C_{1}</code>
</p>
<p> = mean(all unscaled values)
</p>
<p style="text-align: center;"><code class="reqn">C_{2}</code>
</p>
<p> = max(all unscaled values) - min(all unscaled values)
</p>
</li>
<li> <p><code>std</code> or standardization.  This standardizes by the mean
and standard deviation.  In this approach:
</p>
<p style="text-align: center;"><code class="reqn">C_{1}</code>
</p>
<p> = mean(all unscaled values)
</p>
<p style="text-align: center;"><code class="reqn">C_{2}</code>
</p>
<p> = sd(all unscaled values)
</p>
</li>
<li> <p><code>len</code> or unit length scaling.  This scales the
parameters to the unit length.  For this approach we use the Euclidean length, that
is:
</p>
<p style="text-align: center;"><code class="reqn">C_{1}</code>
</p>
<p> = 0
</p>
<p style="text-align: center;"><code class="reqn">C_{2}</code>
</p>
<p> = </p>
<p style="text-align: center;"><code class="reqn">\sqrt(v_1^2 + v_2^2 + \cdots + v_n^2)</code>
</p>

</li>
<li> <p><code>constant</code> which does not perform data normalization. That is
</p>
<p style="text-align: center;"><code class="reqn">C_{1}</code>
</p>
<p> = 0
</p>
<p style="text-align: center;"><code class="reqn">C_{2}</code>
</p>
<p> = 1
</p>
</li></ul>
</td></tr>
<tr><td><code id="newuoaControl_+3A_scaletype">scaleType</code></td>
<td>
<p>The scaling scheme for nlmixr2.  The supported types are:
</p>

<ul>
<li> <p><code>nlmixr2</code>  In this approach the scaling is performed by the following equation:
</p>
<p style="text-align: center;"><code class="reqn">v_{scaled}</code>
</p>
<p> = (</p>
<p style="text-align: center;"><code class="reqn">v_{current} - v_{init}</code>
</p>
<p>)*scaleC[i] + scaleTo
</p>
<p>The <code>scaleTo</code> parameter is specified by the <code>normType</code>,
and the scales are specified by <code>scaleC</code>.
</p>
</li>
<li> <p><code>norm</code> This approach uses the simple scaling provided
by the <code>normType</code> argument.
</p>
</li>
<li> <p><code>mult</code> This approach does not use the data
normalization provided by <code>normType</code>, but rather uses
multiplicative scaling to a constant provided by the <code>scaleTo</code>
argument.
</p>
<p>In this case:
</p>
<p style="text-align: center;"><code class="reqn">v_{scaled}</code>
</p>
<p> = </p>
<p style="text-align: center;"><code class="reqn">v_{current}</code>
</p>
<p>/</p>
<p style="text-align: center;"><code class="reqn">v_{init}</code>
</p>
<p>*scaleTo
</p>
</li>
<li> <p><code>multAdd</code> This approach changes the scaling based on
the parameter being specified.  If a parameter is defined in an
exponential block (ie exp(theta)), then it is scaled on a
linearly, that is:
</p>
<p style="text-align: center;"><code class="reqn">v_{scaled}</code>
</p>
<p> = (</p>
<p style="text-align: center;"><code class="reqn">v_{current}-v_{init}</code>
</p>
<p>) + scaleTo
</p>
<p>Otherwise the parameter is scaled multiplicatively.
</p>
<p style="text-align: center;"><code class="reqn">v_{scaled}</code>
</p>
<p> = </p>
<p style="text-align: center;"><code class="reqn">v_{current}</code>
</p>
<p>/</p>
<p style="text-align: center;"><code class="reqn">v_{init}</code>
</p>
<p>*scaleTo
</p>
</li></ul>
</td></tr>
<tr><td><code id="newuoaControl_+3A_scalecmax">scaleCmax</code></td>
<td>
<p>Maximum value of the scaleC to prevent overflow.</p>
</td></tr>
<tr><td><code id="newuoaControl_+3A_scalecmin">scaleCmin</code></td>
<td>
<p>Minimum value of the scaleC to prevent underflow.</p>
</td></tr>
<tr><td><code id="newuoaControl_+3A_scalec">scaleC</code></td>
<td>
<p>The scaling constant used with
<code>scaleType=nlmixr2</code>.  When not specified, it is based on
the type of parameter that is estimated.  The idea is to keep
the derivatives similar on a log scale to have similar
gradient sizes.  Hence parameters like log(exp(theta)) would
have a scaling factor of 1 and log(theta) would have a scaling
factor of ini_value (to scale by 1/value; ie
d/dt(log(ini_value)) = 1/ini_value or scaleC=ini_value)
</p>

<ul>
<li><p> For parameters in an exponential (ie exp(theta)) or
parameters specifying powers, boxCox or yeoJohnson
transformations , this is 1.
</p>
</li>
<li><p> For additive, proportional, lognormal error structures,
these are given by 0.5*abs(initial_estimate)
</p>
</li>
<li><p> Factorials are scaled by abs(1/digamma(initial_estimate+1))
</p>
</li>
<li><p> parameters in a log scale (ie log(theta)) are transformed
by log(abs(initial_estimate))*abs(initial_estimate)
</p>
</li></ul>

<p>These parameter scaling coefficients are chose to try to keep
similar slopes among parameters.  That is they all follow the
slopes approximately on a log-scale.
</p>
<p>While these are chosen in a logical manner, they may not always
apply.  You can specify each parameters scaling factor by this
parameter if you wish.</p>
</td></tr>
<tr><td><code id="newuoaControl_+3A_scaleto">scaleTo</code></td>
<td>
<p>Scale the initial parameter estimate to this value.
By default this is 1.  When zero or below, no scaling is performed.</p>
</td></tr>
<tr><td><code id="newuoaControl_+3A_rxcontrol">rxControl</code></td>
<td>
<p>'rxode2' ODE solving options during fitting, created with 'rxControl()'</p>
</td></tr>
<tr><td><code id="newuoaControl_+3A_optexpression">optExpression</code></td>
<td>
<p>Optimize the rxode2 expression to speed up
calculation. By default this is turned on.</p>
</td></tr>
<tr><td><code id="newuoaControl_+3A_sumprod">sumProd</code></td>
<td>
<p>Is a boolean indicating if the model should change
multiplication to high precision multiplication and sums to
high precision sums using the PreciseSums package.  By default
this is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="newuoaControl_+3A_literalfix">literalFix</code></td>
<td>
<p>boolean, substitute fixed population values as
literals and re-adjust ui and parameter estimates after
optimization; Default is 'TRUE'.</p>
</td></tr>
<tr><td><code id="newuoaControl_+3A_addprop">addProp</code></td>
<td>
<p>specifies the type of additive plus proportional
errors, the one where standard deviations add (combined1) or the
type where the variances add (combined2).
</p>
<p>The combined1 error type can be described by the following equation:
</p>
<p style="text-align: center;"><code class="reqn">y = f + (a + b\times f^c) \times \varepsilon</code>
</p>

<p>The combined2 error model can be described by the following equation:
</p>
<p style="text-align: center;"><code class="reqn">y = f + \sqrt{a^2 + b^2\times f^{2\times c}} \times \varepsilon</code>
</p>

<p>Where:
</p>
<p>- y represents the observed value
</p>
<p>- f represents the predicted value
</p>
<p>- a  is the additive standard deviation
</p>
<p>- b is the proportional/power standard deviation
</p>
<p>- c is the power exponent (in the proportional case c=1)</p>
</td></tr>
<tr><td><code id="newuoaControl_+3A_calctables">calcTables</code></td>
<td>
<p>This boolean is to determine if the foceiFit
will calculate tables. By default this is <code>TRUE</code></p>
</td></tr>
<tr><td><code id="newuoaControl_+3A_compress">compress</code></td>
<td>
<p>Should the object have compressed items</p>
</td></tr>
<tr><td><code id="newuoaControl_+3A_covmethod">covMethod</code></td>
<td>
<p>Method for calculating covariance.  In this
discussion, R is the Hessian matrix of the objective
function. The S matrix is the sum of individual
gradient cross-product (evaluated at the individual empirical
Bayes estimates).
</p>

<ul>
<li><p> &quot;<code>r,s</code>&quot; Uses the sandwich matrix to calculate the
covariance, that is: <code>solve(R) %*% S %*% solve(R)</code>
</p>
</li>
<li><p> &quot;<code>r</code>&quot; Uses the Hessian matrix to calculate the
covariance as <code>2 %*% solve(R)</code>
</p>
</li>
<li><p> &quot;<code>s</code>&quot; Uses the cross-product matrix to calculate the
covariance as <code>4 %*% solve(S)</code>
</p>
</li>
<li><p> &quot;&quot; Does not calculate the covariance step.
</p>
</li></ul>
</td></tr>
<tr><td><code id="newuoaControl_+3A_adjobf">adjObf</code></td>
<td>
<p>is a boolean to indicate if the objective function
should be adjusted to be closer to NONMEM's default objective
function.  By default this is <code>TRUE</code></p>
</td></tr>
<tr><td><code id="newuoaControl_+3A_ci">ci</code></td>
<td>
<p>Confidence level for some tables.  By default this is
0.95 or 95% confidence.</p>
</td></tr>
<tr><td><code id="newuoaControl_+3A_sigdig">sigdig</code></td>
<td>
<p>Optimization significant digits. This controls:
</p>

<ul>
<li><p> The tolerance of the inner and outer optimization is <code>10^-sigdig</code>
</p>
</li>
<li><p> The tolerance of the ODE solvers is
<code>0.5*10^(-sigdig-2)</code>; For the sensitivity equations and
steady-state solutions the default is <code>0.5*10^(-sigdig-1.5)</code>
(sensitivity changes only applicable for liblsoda)
</p>
</li>
<li><p> The tolerance of the boundary check is <code>5 * 10 ^ (-sigdig + 1)</code>
</p>
</li></ul>
</td></tr>
<tr><td><code id="newuoaControl_+3A_sigdigtable">sigdigTable</code></td>
<td>
<p>Significant digits in the final output table.
If not specified, then it matches the significant digits in the
'sigdig' optimization algorithm.  If 'sigdig' is NULL, use 3.</p>
</td></tr>
<tr><td><code id="newuoaControl_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <code><a href="nlmixr2est.html#topic+newuoaControl">nlmixr2est::newuoaControl()</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>newuoa control structure
</p>


<h3>Author(s)</h3>

<p>Matthew L. Fidler
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# A logit regression example with emax model

dsn &lt;- data.frame(i=1:1000)
dsn$time &lt;- exp(rnorm(1000))
dsn$DV=rbinom(1000,1,exp(-1+dsn$time)/(1+exp(-1+dsn$time)))

mod &lt;- function() {
 ini({
   E0 &lt;- 0.5
   Em &lt;- 0.5
   E50 &lt;- 2
   g &lt;- fix(2)
 })
 model({
   v &lt;- E0+Em*time^g/(E50^g+time^g)
   ll(bin) ~ DV * v - log(1 + exp(v))
 })
}

fit2 &lt;- nlmixr(mod, dsn, est="newuoa")

print(fit2)

# you can also get the nlm output with

fit2$newuoa

# The nlm control has been modified slightly to include
# extra components and name the parameters

</code></pre>

<hr>
<h2 id='nlmControl'>nlmixr2 defaults controls for nlm</h2><span id='topic+nlmControl'></span>

<h3>Description</h3>

<p>nlmixr2 defaults controls for nlm
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nlmControl(
  typsize = NULL,
  fscale = 1,
  print.level = 0,
  ndigit = NULL,
  gradtol = 1e-06,
  stepmax = NULL,
  steptol = 1e-06,
  iterlim = 10000,
  check.analyticals = FALSE,
  returnNlm = FALSE,
  solveType = c("hessian", "grad", "fun"),
  stickyRecalcN = 4,
  maxOdeRecalc = 5,
  odeRecalcFactor = 10^(0.5),
  eventType = c("central", "forward"),
  shiErr = (.Machine$double.eps)^(1/3),
  shi21maxFD = 20L,
  optimHessType = c("central", "forward"),
  hessErr = (.Machine$double.eps)^(1/3),
  shi21maxHess = 20L,
  useColor = crayon::has_color(),
  printNcol = floor((getOption("width") - 23)/12),
  print = 1L,
  normType = c("rescale2", "mean", "rescale", "std", "len", "constant"),
  scaleType = c("nlmixr2", "norm", "mult", "multAdd"),
  scaleCmax = 1e+05,
  scaleCmin = 1e-05,
  scaleC = NULL,
  scaleTo = 1,
  gradTo = 1,
  rxControl = NULL,
  optExpression = TRUE,
  sumProd = FALSE,
  literalFix = TRUE,
  addProp = c("combined2", "combined1"),
  calcTables = TRUE,
  compress = TRUE,
  covMethod = c("r", "nlm", ""),
  adjObf = TRUE,
  ci = 0.95,
  sigdig = 4,
  sigdigTable = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nlmControl_+3A_typsize">typsize</code></td>
<td>
<p>an estimate of the size of each parameter
at the minimum.</p>
</td></tr>
<tr><td><code id="nlmControl_+3A_fscale">fscale</code></td>
<td>
<p>an estimate of the size of <code>f</code> at the minimum.</p>
</td></tr>
<tr><td><code id="nlmControl_+3A_print.level">print.level</code></td>
<td>
<p>this argument determines the level of printing
which is done during the minimization process.  The default
value of <code>0</code> means that no printing occurs, a value of <code>1</code>
means that initial and final details are printed and a value
of 2 means that full tracing information is printed.</p>
</td></tr>
<tr><td><code id="nlmControl_+3A_ndigit">ndigit</code></td>
<td>
<p>the number of significant digits in the function <code>f</code>.</p>
</td></tr>
<tr><td><code id="nlmControl_+3A_gradtol">gradtol</code></td>
<td>
<p>a positive scalar giving the tolerance at which the
scaled gradient is considered close enough to zero to
terminate the algorithm.  The scaled gradient is a
measure of the relative change in <code>f</code> in each direction
<code>p[i]</code> divided by the relative change in <code>p[i]</code>.</p>
</td></tr>
<tr><td><code id="nlmControl_+3A_stepmax">stepmax</code></td>
<td>
<p>a positive scalar which gives the maximum allowable
scaled step length.  <code>stepmax</code> is used to prevent steps which
would cause the optimization function to overflow, to prevent the
algorithm from leaving the area of interest in parameter space, or to
detect divergence in the algorithm. <code>stepmax</code> would be chosen
small enough to prevent the first two of these occurrences, but should
be larger than any anticipated reasonable step.</p>
</td></tr>
<tr><td><code id="nlmControl_+3A_steptol">steptol</code></td>
<td>
<p>A positive scalar providing the minimum allowable
relative step length.</p>
</td></tr>
<tr><td><code id="nlmControl_+3A_iterlim">iterlim</code></td>
<td>
<p>a positive integer specifying the maximum number of
iterations to be performed before the program is terminated.</p>
</td></tr>
<tr><td><code id="nlmControl_+3A_check.analyticals">check.analyticals</code></td>
<td>
<p>a logical scalar specifying whether the
analytic gradients and Hessians, if they are supplied, should be
checked against numerical derivatives at the initial parameter
values. This can help detect incorrectly formulated gradients or
Hessians.</p>
</td></tr>
<tr><td><code id="nlmControl_+3A_returnnlm">returnNlm</code></td>
<td>
<p>is a logical that allows a return of the 'nlm'
object</p>
</td></tr>
<tr><td><code id="nlmControl_+3A_solvetype">solveType</code></td>
<td>
<p>tells if &lsquo;nlm' will use nlmixr2&rsquo;s analytical
gradients when available (finite differences will be used for
event-related parameters like parameters controlling lag time,
duration/rate of infusion, and modeled bioavailability). This can
be:
</p>
<p>- '&quot;hessian&quot;' which will use the analytical gradients to create a
Hessian with finite differences.
</p>
<p>- '&quot;gradient&quot;' which will use the gradient and let 'nlm' calculate
the finite difference hessian
</p>
<p>- '&quot;fun&quot;' where nlm will calculate both the finite difference
gradient and the finite difference Hessian
</p>
<p>When using nlmixr2's finite differences, the &quot;ideal&quot; step size for
either central or forward differences are optimized for with the
Shi2021 method which may give more accurate derivatives</p>
</td></tr>
<tr><td><code id="nlmControl_+3A_stickyrecalcn">stickyRecalcN</code></td>
<td>
<p>The number of bad ODE solves before reducing
the atol/rtol for the rest of the problem.</p>
</td></tr>
<tr><td><code id="nlmControl_+3A_maxoderecalc">maxOdeRecalc</code></td>
<td>
<p>Maximum number of times to reduce the ODE
tolerances and try to resolve the system if there was a bad
ODE solve.</p>
</td></tr>
<tr><td><code id="nlmControl_+3A_oderecalcfactor">odeRecalcFactor</code></td>
<td>
<p>The ODE recalculation factor when ODE
solving goes bad, this is the factor the rtol/atol is reduced</p>
</td></tr>
<tr><td><code id="nlmControl_+3A_eventtype">eventType</code></td>
<td>
<p>Event gradient type for dosing events; Can be
&quot;central&quot; or &quot;forward&quot;</p>
</td></tr>
<tr><td><code id="nlmControl_+3A_shierr">shiErr</code></td>
<td>
<p>This represents the epsilon when optimizing the ideal
step size for numeric differentiation using the Shi2021 method</p>
</td></tr>
<tr><td><code id="nlmControl_+3A_shi21maxfd">shi21maxFD</code></td>
<td>
<p>The maximum number of steps for the optimization
of the forward difference step size when using dosing events (lag
time, modeled duration/rate and bioavailability)</p>
</td></tr>
<tr><td><code id="nlmControl_+3A_optimhesstype">optimHessType</code></td>
<td>
<p>The hessian type for when calculating the
individual hessian by numeric differences (in generalized
log-likelihood estimation).  The options are &quot;central&quot;, and
&quot;forward&quot;.  The central differences is what R's 'optimHess()'
uses and is the default for this method. (Though the &quot;forward&quot; is
faster and still reasonable for most cases).  The Shi21 cannot be
changed for the Gill83 algorithm with the optimHess in a
generalized likelihood problem.</p>
</td></tr>
<tr><td><code id="nlmControl_+3A_hesserr">hessErr</code></td>
<td>
<p>This represents the epsilon when optimizing the
Hessian step size using the Shi2021 method.</p>
</td></tr>
<tr><td><code id="nlmControl_+3A_shi21maxhess">shi21maxHess</code></td>
<td>
<p>Maximum number of times to optimize the best
step size for the hessian calculation</p>
</td></tr>
<tr><td><code id="nlmControl_+3A_usecolor">useColor</code></td>
<td>
<p>Boolean indicating if focei can use ASCII color codes</p>
</td></tr>
<tr><td><code id="nlmControl_+3A_printncol">printNcol</code></td>
<td>
<p>Number of columns to printout before wrapping
parameter estimates/gradient</p>
</td></tr>
<tr><td><code id="nlmControl_+3A_print">print</code></td>
<td>
<p>Integer representing when the outer step is
printed. When this is 0 or do not print the iterations.  1 is
print every function evaluation (default), 5 is print every 5
evaluations.</p>
</td></tr>
<tr><td><code id="nlmControl_+3A_normtype">normType</code></td>
<td>
<p>This is the type of parameter
normalization/scaling used to get the scaled initial values
for nlmixr2.  These are used with <code>scaleType</code> of.
</p>
<p>With the exception of <code>rescale2</code>, these come
from
<a href="https://en.wikipedia.org/wiki/Feature_scaling">Feature
Scaling</a>. The <code>rescale2</code> The rescaling is the same type
described in the
<a href="http://apmonitor.com/me575/uploads/Main/optimization_book.pdf">OptdesX</a>
software manual.
</p>
<p>In general, all all scaling formula can be described by:
</p>
<p style="text-align: center;"><code class="reqn">v_{scaled}</code>
</p>
<p> = (</p>
<p style="text-align: center;"><code class="reqn">v_{unscaled}-C_{1}</code>
</p>
<p>)/</p>
<p style="text-align: center;"><code class="reqn">C_{2}</code>
</p>

<p>Where
</p>
<p>The other data normalization approaches follow the following formula
</p>
<p style="text-align: center;"><code class="reqn">v_{scaled}</code>
</p>
<p> = (</p>
<p style="text-align: center;"><code class="reqn">v_{unscaled}-C_{1}</code>
</p>
<p>)/</p>
<p style="text-align: center;"><code class="reqn">C_{2}</code>
</p>


<ul>
<li> <p><code>rescale2</code> This scales all parameters from (-1 to 1).
The relative differences between the parameters are preserved
with this approach and the constants are:
</p>
<p style="text-align: center;"><code class="reqn">C_{1}</code>
</p>
<p> = (max(all unscaled values)+min(all unscaled values))/2
</p>
<p style="text-align: center;"><code class="reqn">C_{2}</code>
</p>
<p> = (max(all unscaled values) - min(all unscaled values))/2
</p>
</li>
<li> <p><code>rescale</code> or min-max normalization. This rescales all
parameters from (0 to 1).  As in the <code>rescale2</code> the
relative differences are preserved.  In this approach:
</p>
<p style="text-align: center;"><code class="reqn">C_{1}</code>
</p>
<p> = min(all unscaled values)
</p>
<p style="text-align: center;"><code class="reqn">C_{2}</code>
</p>
<p> = max(all unscaled values) - min(all unscaled values)
</p>
</li>
<li> <p><code>mean</code> or mean normalization.  This rescales to center
the parameters around the mean but the parameters are from 0
to 1.  In this approach:
</p>
<p style="text-align: center;"><code class="reqn">C_{1}</code>
</p>
<p> = mean(all unscaled values)
</p>
<p style="text-align: center;"><code class="reqn">C_{2}</code>
</p>
<p> = max(all unscaled values) - min(all unscaled values)
</p>
</li>
<li> <p><code>std</code> or standardization.  This standardizes by the mean
and standard deviation.  In this approach:
</p>
<p style="text-align: center;"><code class="reqn">C_{1}</code>
</p>
<p> = mean(all unscaled values)
</p>
<p style="text-align: center;"><code class="reqn">C_{2}</code>
</p>
<p> = sd(all unscaled values)
</p>
</li>
<li> <p><code>len</code> or unit length scaling.  This scales the
parameters to the unit length.  For this approach we use the Euclidean length, that
is:
</p>
<p style="text-align: center;"><code class="reqn">C_{1}</code>
</p>
<p> = 0
</p>
<p style="text-align: center;"><code class="reqn">C_{2}</code>
</p>
<p> = </p>
<p style="text-align: center;"><code class="reqn">\sqrt(v_1^2 + v_2^2 + \cdots + v_n^2)</code>
</p>

</li>
<li> <p><code>constant</code> which does not perform data normalization. That is
</p>
<p style="text-align: center;"><code class="reqn">C_{1}</code>
</p>
<p> = 0
</p>
<p style="text-align: center;"><code class="reqn">C_{2}</code>
</p>
<p> = 1
</p>
</li></ul>
</td></tr>
<tr><td><code id="nlmControl_+3A_scaletype">scaleType</code></td>
<td>
<p>The scaling scheme for nlmixr2.  The supported types are:
</p>

<ul>
<li> <p><code>nlmixr2</code>  In this approach the scaling is performed by the following equation:
</p>
<p style="text-align: center;"><code class="reqn">v_{scaled}</code>
</p>
<p> = (</p>
<p style="text-align: center;"><code class="reqn">v_{current} - v_{init}</code>
</p>
<p>)*scaleC[i] + scaleTo
</p>
<p>The <code>scaleTo</code> parameter is specified by the <code>normType</code>,
and the scales are specified by <code>scaleC</code>.
</p>
</li>
<li> <p><code>norm</code> This approach uses the simple scaling provided
by the <code>normType</code> argument.
</p>
</li>
<li> <p><code>mult</code> This approach does not use the data
normalization provided by <code>normType</code>, but rather uses
multiplicative scaling to a constant provided by the <code>scaleTo</code>
argument.
</p>
<p>In this case:
</p>
<p style="text-align: center;"><code class="reqn">v_{scaled}</code>
</p>
<p> = </p>
<p style="text-align: center;"><code class="reqn">v_{current}</code>
</p>
<p>/</p>
<p style="text-align: center;"><code class="reqn">v_{init}</code>
</p>
<p>*scaleTo
</p>
</li>
<li> <p><code>multAdd</code> This approach changes the scaling based on
the parameter being specified.  If a parameter is defined in an
exponential block (ie exp(theta)), then it is scaled on a
linearly, that is:
</p>
<p style="text-align: center;"><code class="reqn">v_{scaled}</code>
</p>
<p> = (</p>
<p style="text-align: center;"><code class="reqn">v_{current}-v_{init}</code>
</p>
<p>) + scaleTo
</p>
<p>Otherwise the parameter is scaled multiplicatively.
</p>
<p style="text-align: center;"><code class="reqn">v_{scaled}</code>
</p>
<p> = </p>
<p style="text-align: center;"><code class="reqn">v_{current}</code>
</p>
<p>/</p>
<p style="text-align: center;"><code class="reqn">v_{init}</code>
</p>
<p>*scaleTo
</p>
</li></ul>
</td></tr>
<tr><td><code id="nlmControl_+3A_scalecmax">scaleCmax</code></td>
<td>
<p>Maximum value of the scaleC to prevent overflow.</p>
</td></tr>
<tr><td><code id="nlmControl_+3A_scalecmin">scaleCmin</code></td>
<td>
<p>Minimum value of the scaleC to prevent underflow.</p>
</td></tr>
<tr><td><code id="nlmControl_+3A_scalec">scaleC</code></td>
<td>
<p>The scaling constant used with
<code>scaleType=nlmixr2</code>.  When not specified, it is based on
the type of parameter that is estimated.  The idea is to keep
the derivatives similar on a log scale to have similar
gradient sizes.  Hence parameters like log(exp(theta)) would
have a scaling factor of 1 and log(theta) would have a scaling
factor of ini_value (to scale by 1/value; ie
d/dt(log(ini_value)) = 1/ini_value or scaleC=ini_value)
</p>

<ul>
<li><p> For parameters in an exponential (ie exp(theta)) or
parameters specifying powers, boxCox or yeoJohnson
transformations , this is 1.
</p>
</li>
<li><p> For additive, proportional, lognormal error structures,
these are given by 0.5*abs(initial_estimate)
</p>
</li>
<li><p> Factorials are scaled by abs(1/digamma(initial_estimate+1))
</p>
</li>
<li><p> parameters in a log scale (ie log(theta)) are transformed
by log(abs(initial_estimate))*abs(initial_estimate)
</p>
</li></ul>

<p>These parameter scaling coefficients are chose to try to keep
similar slopes among parameters.  That is they all follow the
slopes approximately on a log-scale.
</p>
<p>While these are chosen in a logical manner, they may not always
apply.  You can specify each parameters scaling factor by this
parameter if you wish.</p>
</td></tr>
<tr><td><code id="nlmControl_+3A_scaleto">scaleTo</code></td>
<td>
<p>Scale the initial parameter estimate to this value.
By default this is 1.  When zero or below, no scaling is performed.</p>
</td></tr>
<tr><td><code id="nlmControl_+3A_gradto">gradTo</code></td>
<td>
<p>this is the factor that the gradient is scaled to
before optimizing.  This only works with
scaleType=&quot;nlmixr2&quot;.</p>
</td></tr>
<tr><td><code id="nlmControl_+3A_rxcontrol">rxControl</code></td>
<td>
<p>'rxode2' ODE solving options during fitting, created with 'rxControl()'</p>
</td></tr>
<tr><td><code id="nlmControl_+3A_optexpression">optExpression</code></td>
<td>
<p>Optimize the rxode2 expression to speed up
calculation. By default this is turned on.</p>
</td></tr>
<tr><td><code id="nlmControl_+3A_sumprod">sumProd</code></td>
<td>
<p>Is a boolean indicating if the model should change
multiplication to high precision multiplication and sums to
high precision sums using the PreciseSums package.  By default
this is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="nlmControl_+3A_literalfix">literalFix</code></td>
<td>
<p>boolean, substitute fixed population values as
literals and re-adjust ui and parameter estimates after
optimization; Default is 'TRUE'.</p>
</td></tr>
<tr><td><code id="nlmControl_+3A_addprop">addProp</code></td>
<td>
<p>specifies the type of additive plus proportional
errors, the one where standard deviations add (combined1) or the
type where the variances add (combined2).
</p>
<p>The combined1 error type can be described by the following equation:
</p>
<p style="text-align: center;"><code class="reqn">y = f + (a + b\times f^c) \times \varepsilon</code>
</p>

<p>The combined2 error model can be described by the following equation:
</p>
<p style="text-align: center;"><code class="reqn">y = f + \sqrt{a^2 + b^2\times f^{2\times c}} \times \varepsilon</code>
</p>

<p>Where:
</p>
<p>- y represents the observed value
</p>
<p>- f represents the predicted value
</p>
<p>- a  is the additive standard deviation
</p>
<p>- b is the proportional/power standard deviation
</p>
<p>- c is the power exponent (in the proportional case c=1)</p>
</td></tr>
<tr><td><code id="nlmControl_+3A_calctables">calcTables</code></td>
<td>
<p>This boolean is to determine if the foceiFit
will calculate tables. By default this is <code>TRUE</code></p>
</td></tr>
<tr><td><code id="nlmControl_+3A_compress">compress</code></td>
<td>
<p>Should the object have compressed items</p>
</td></tr>
<tr><td><code id="nlmControl_+3A_covmethod">covMethod</code></td>
<td>
<p>allows selection of &quot;r&quot;, which uses nlmixr2's
'nlmixr2Hess()' for the hessian calculation or &quot;nlm&quot; which uses
the hessian from 'stats::nlm(.., hessian=TRUE)'. When using
&lsquo;nlmixr2&rsquo;s&lsquo; hessian for optimization or 'nlmixr2&rsquo;s' gradient for
solving this defaults to &quot;nlm&quot; since 'stats::optimHess()' assumes
an accurate gradient and is faster than 'nlmixr2Hess'</p>
</td></tr>
<tr><td><code id="nlmControl_+3A_adjobf">adjObf</code></td>
<td>
<p>is a boolean to indicate if the objective function
should be adjusted to be closer to NONMEM's default objective
function.  By default this is <code>TRUE</code></p>
</td></tr>
<tr><td><code id="nlmControl_+3A_ci">ci</code></td>
<td>
<p>Confidence level for some tables.  By default this is
0.95 or 95% confidence.</p>
</td></tr>
<tr><td><code id="nlmControl_+3A_sigdig">sigdig</code></td>
<td>
<p>Optimization significant digits. This controls:
</p>

<ul>
<li><p> The tolerance of the inner and outer optimization is <code>10^-sigdig</code>
</p>
</li>
<li><p> The tolerance of the ODE solvers is
<code>0.5*10^(-sigdig-2)</code>; For the sensitivity equations and
steady-state solutions the default is <code>0.5*10^(-sigdig-1.5)</code>
(sensitivity changes only applicable for liblsoda)
</p>
</li>
<li><p> The tolerance of the boundary check is <code>5 * 10 ^ (-sigdig + 1)</code>
</p>
</li></ul>
</td></tr>
<tr><td><code id="nlmControl_+3A_sigdigtable">sigdigTable</code></td>
<td>
<p>Significant digits in the final output table.
If not specified, then it matches the significant digits in the
'sigdig' optimization algorithm.  If 'sigdig' is NULL, use 3.</p>
</td></tr>
<tr><td><code id="nlmControl_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <code><a href="nlmixr2est.html#topic+nlmControl">nlmixr2est::nlmControl()</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>nlm control object
</p>


<h3>Author(s)</h3>

<p>Matthew L. Fidler
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# A logit regression example with emax model

dsn &lt;- data.frame(i=1:1000)
dsn$time &lt;- exp(rnorm(1000))
dsn$DV=rbinom(1000,1,exp(-1+dsn$time)/(1+exp(-1+dsn$time)))

mod &lt;- function() {
 ini({
   E0 &lt;- 0.5
   Em &lt;- 0.5
   E50 &lt;- 2
   g &lt;- fix(2)
 })
 model({
   v &lt;- E0+Em*time^g/(E50^g+time^g)
   ll(bin) ~ DV * v - log(1 + exp(v))
 })
}

fit2 &lt;- nlmixr(mod, dsn, est="nlm")

print(fit2)

# you can also get the nlm output with fit2$nlm

fit2$nlm

# The nlm control has been modified slightly to include
# extra components and name the parameters

</code></pre>

<hr>
<h2 id='nlmeControl'>Control Values for nlme Fit with extra options for nlmixr</h2><span id='topic+nlmeControl'></span>

<h3>Description</h3>

<p>The values supplied in the function call replace the defaults and
a list with all possible arguments is returned.  The returned list
is used as the control argument to the nlme function.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nlmeControl(
  maxIter = 100,
  pnlsMaxIter = 100,
  msMaxIter = 100,
  minScale = 0.001,
  tolerance = 1e-05,
  niterEM = 25,
  pnlsTol = 0.001,
  msTol = 1e-06,
  returnObject = FALSE,
  msVerbose = FALSE,
  msWarnNoConv = TRUE,
  gradHess = TRUE,
  apVar = TRUE,
  .relStep = .Machine$double.eps^(1/3),
  minAbsParApVar = 0.05,
  opt = c("nlminb", "nlm"),
  natural = TRUE,
  sigma = NULL,
  optExpression = TRUE,
  literalFix = TRUE,
  sumProd = FALSE,
  rxControl = NULL,
  method = c("ML", "REML"),
  random = NULL,
  fixed = NULL,
  weights = NULL,
  verbose = TRUE,
  returnNlme = FALSE,
  addProp = c("combined2", "combined1"),
  calcTables = TRUE,
  compress = TRUE,
  adjObf = TRUE,
  ci = 0.95,
  sigdig = 4,
  sigdigTable = NULL,
  muRefCovAlg = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nlmeControl_+3A_maxiter">maxIter</code></td>
<td>
<p>maximum number of iterations for the <code>nlme</code>
optimization algorithm.  Default is 50.</p>
</td></tr>
<tr><td><code id="nlmeControl_+3A_pnlsmaxiter">pnlsMaxIter</code></td>
<td>
<p>maximum number of iterations
for the <code>PNLS</code> optimization step inside the <code>nlme</code>
optimization.  Default is 7.</p>
</td></tr>
<tr><td><code id="nlmeControl_+3A_msmaxiter">msMaxIter</code></td>
<td>
<p>maximum number of iterations for <code><a href="stats.html#topic+nlminb">nlminb</a></code>
(<code>iter.max</code>) or the <code><a href="stats.html#topic+nlm">nlm</a></code> (<code>iterlim</code>, from the
10-th step) optimization step inside the <code>nlme</code>
optimization.  Default is 50 (which may be too small for e.g. for
overparametrized cases).</p>
</td></tr>
<tr><td><code id="nlmeControl_+3A_minscale">minScale</code></td>
<td>
<p>minimum factor by which to shrink the default step size
in an attempt to decrease the sum of squares in the <code>PNLS</code> step.
Default <code>0.001</code>.</p>
</td></tr>
<tr><td><code id="nlmeControl_+3A_tolerance">tolerance</code></td>
<td>
<p>tolerance for the convergence criterion in the
<code>nlme</code> algorithm.  Default is <code>1e-6</code>.</p>
</td></tr>
<tr><td><code id="nlmeControl_+3A_niterem">niterEM</code></td>
<td>
<p>number of iterations for the EM algorithm used to refine
the initial estimates of the random effects variance-covariance
coefficients.  Default is 25.</p>
</td></tr>
<tr><td><code id="nlmeControl_+3A_pnlstol">pnlsTol</code></td>
<td>
<p>tolerance for the convergence criterion in <code>PNLS</code>
step.  Default is <code>1e-3</code>.</p>
</td></tr>
<tr><td><code id="nlmeControl_+3A_mstol">msTol</code></td>
<td>
<p>tolerance for the convergence criterion in <code>nlm</code>,
passed as the <code>gradtol</code> argument to the function (see
documentation on <code>nlm</code>).  Default is <code>1e-7</code>. </p>
</td></tr>
<tr><td><code id="nlmeControl_+3A_returnobject">returnObject</code></td>
<td>
<p>a logical value indicating whether the fitted
object should be returned when the maximum number of iterations is
reached without convergence of the algorithm.  Default is
<code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="nlmeControl_+3A_msverbose">msVerbose</code></td>
<td>
<p>a logical value passed as the <code>trace</code> to
<code><a href="stats.html#topic+nlminb">nlminb</a>(.., control= list(trace = *, ..))</code> or
as argument <code>print.level</code> to <code><a href="stats.html#topic+nlm">nlm</a>()</code>.  Default is
<code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="nlmeControl_+3A_mswarnnoconv">msWarnNoConv</code></td>
<td>
<p>logical indicating if a <code><a href="base.html#topic+warning">warning</a></code>
should be signalled whenever the minimization (by <code>opt</code>) in the
LME step does not converge; defaults to <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="nlmeControl_+3A_gradhess">gradHess</code></td>
<td>
<p>a logical value indicating whether numerical gradient
vectors and Hessian matrices of the log-likelihood function should
be used in the <code>nlm</code> optimization. This option is only available
when the correlation structure (<code>corStruct</code>) and the variance
function structure (<code>varFunc</code>) have no &quot;varying&quot; parameters and
the <code>pdMat</code> classes used in the random effects structure are
<code>pdSymm</code> (general positive-definite), <code>pdDiag</code> (diagonal),
<code>pdIdent</code> (multiple of the identity),  or
<code>pdCompSymm</code> (compound symmetry).  Default is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="nlmeControl_+3A_apvar">apVar</code></td>
<td>
<p>a logical value indicating whether the approximate
covariance matrix of the variance-covariance parameters should be
calculated.  Default is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="nlmeControl_+3A_.relstep">.relStep</code></td>
<td>
<p>relative step for numerical derivatives
calculations.  Default is <code>.Machine$double.eps^(1/3)</code>.</p>
</td></tr>
<tr><td><code id="nlmeControl_+3A_minabsparapvar">minAbsParApVar</code></td>
<td>
<p>numeric value - minimum absolute parameter value
in the approximate variance calculation.  The default is <code>0.05</code>.</p>
</td></tr>
<tr><td><code id="nlmeControl_+3A_opt">opt</code></td>
<td>
<p>the optimizer to be used, either <code>"<a href="stats.html#topic+nlminb">nlminb</a>"</code> (the
default) or <code>"<a href="stats.html#topic+nlm">nlm</a>"</code>.</p>
</td></tr>
<tr><td><code id="nlmeControl_+3A_natural">natural</code></td>
<td>
<p>a logical value indicating whether the <code>pdNatural</code>
parametrization should be used for general positive-definite matrices
(<code>pdSymm</code>) in <code>reStruct</code>, when the approximate covariance
matrix of the estimators is calculated.  Default is <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="nlmeControl_+3A_sigma">sigma</code></td>
<td>
<p>optionally a positive number to fix the residual error at.
If <code>NULL</code>, as by default, or <code>0</code>, sigma is estimated.</p>
</td></tr>
<tr><td><code id="nlmeControl_+3A_optexpression">optExpression</code></td>
<td>
<p>Optimize the rxode2 expression to speed up
calculation. By default this is turned on.</p>
</td></tr>
<tr><td><code id="nlmeControl_+3A_literalfix">literalFix</code></td>
<td>
<p>boolean, substitute fixed population values as
literals and re-adjust ui and parameter estimates after
optimization; Default is 'TRUE'.</p>
</td></tr>
<tr><td><code id="nlmeControl_+3A_sumprod">sumProd</code></td>
<td>
<p>Is a boolean indicating if the model should change
multiplication to high precision multiplication and sums to
high precision sums using the PreciseSums package.  By default
this is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="nlmeControl_+3A_rxcontrol">rxControl</code></td>
<td>
<p>'rxode2' ODE solving options during fitting, created with 'rxControl()'</p>
</td></tr>
<tr><td><code id="nlmeControl_+3A_method">method</code></td>
<td>
<p>a character string.  If <code>"REML"</code> the model is fit by
maximizing the restricted log-likelihood.  If <code>"ML"</code> the
log-likelihood is maximized.  Defaults to <code>"ML"</code>.</p>
</td></tr>
<tr><td><code id="nlmeControl_+3A_random">random</code></td>
<td>
<p>optionally, any of the following: (i) a two-sided formula
of the form <code>r1+...+rn~x1+...+xm | g1/.../gQ</code>, with
<code>r1,...,rn</code> naming parameters included on the right
hand side of <code>model</code>, <code>x1+...+xm</code> specifying the
random-effects model for
these parameters and <code>g1/.../gQ</code> the grouping structure
(<code>Q</code> may be equal to 1, in which case no <code>/</code> is
required). The random effects formula will be repeated
for all levels of grouping, in the case of multiple levels of
grouping; (ii) a two-sided formula of the form
<code>r1+...+rn~x1+..+xm</code>, a list of two-sided formulas of the form
<code>r1~x1+...+xm</code>, with possibly different random-effects models
for different parameters, a <code>pdMat</code> object with a two-sided
formula, or list of two-sided formulas (i.e. a non-<code>NULL</code> value for
<code>formula(random)</code>), or a list of pdMat objects with two-sided
formulas, or lists of two-sided formulas. In this case, the grouping
structure formula will be given in <code>groups</code>, or derived from the
data used to fit the nonlinear mixed-effects model, which should
inherit from class  <code>groupedData</code>,; (iii) a named list
of formulas, lists of formulas, or <code>pdMat</code> objects as in (ii),
with the grouping factors as names. The order of nesting will be
assumed the same as the order of the order of the elements in the
list; (iv) an <code>reStruct</code> object. See the documentation on
<code><a href="nlme.html#topic+pdClasses">pdClasses</a></code> for a description of the available <code>pdMat</code>
classes. Defaults to <code>fixed</code>,
resulting in all fixed effects having also random effects.</p>
</td></tr>
<tr><td><code id="nlmeControl_+3A_fixed">fixed</code></td>
<td>
<p>a two-sided linear formula of the form
<code>f1+...+fn~x1+...+xm</code>, or a list of two-sided formulas of the form
<code>f1~x1+...+xm</code>, with possibly different models for different
parameters. The <code>f1,...,fn</code> are the names of parameters included on
the right hand side of <code>model</code> and the <code>x1+...+xm</code>
expressions define linear models for these parameters (when the left
hand side of the formula contains several parameters, they all are
assumed to follow the same linear model, described by the right hand
side expression).
A <code>1</code> on the right hand side of the formula(s) indicates a single
fixed effects for the corresponding parameter(s).</p>
</td></tr>
<tr><td><code id="nlmeControl_+3A_weights">weights</code></td>
<td>
<p>an optional <code>varFunc</code> object or one-sided formula
describing the within-group heteroscedasticity structure. If given as
a formula, it is used as the argument to <code>varFixed</code>,
corresponding to fixed variance weights. See the documentation on
<code><a href="nlme.html#topic+varClasses">varClasses</a></code> for a description of the available <code>varFunc</code>
classes. Defaults to <code>NULL</code>, corresponding to homoscedastic
within-group errors.</p>
</td></tr>
<tr><td><code id="nlmeControl_+3A_verbose">verbose</code></td>
<td>
<p>an optional logical value. If <code>TRUE</code> information on
the evolution of the iterative algorithm is printed. Default is
<code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="nlmeControl_+3A_returnnlme">returnNlme</code></td>
<td>
<p>Returns the nlme object instead of the nlmixr
object (by default FALSE).  If any of the nlme specific options
of 'random', 'fixed', 'sens', the nlme object is returned</p>
</td></tr>
<tr><td><code id="nlmeControl_+3A_addprop">addProp</code></td>
<td>
<p>specifies the type of additive plus proportional
errors, the one where standard deviations add (combined1) or the
type where the variances add (combined2).
</p>
<p>The combined1 error type can be described by the following equation:
</p>
<p style="text-align: center;"><code class="reqn">y = f + (a + b\times f^c) \times \varepsilon</code>
</p>

<p>The combined2 error model can be described by the following equation:
</p>
<p style="text-align: center;"><code class="reqn">y = f + \sqrt{a^2 + b^2\times f^{2\times c}} \times \varepsilon</code>
</p>

<p>Where:
</p>
<p>- y represents the observed value
</p>
<p>- f represents the predicted value
</p>
<p>- a  is the additive standard deviation
</p>
<p>- b is the proportional/power standard deviation
</p>
<p>- c is the power exponent (in the proportional case c=1)</p>
</td></tr>
<tr><td><code id="nlmeControl_+3A_calctables">calcTables</code></td>
<td>
<p>This boolean is to determine if the foceiFit
will calculate tables. By default this is <code>TRUE</code></p>
</td></tr>
<tr><td><code id="nlmeControl_+3A_compress">compress</code></td>
<td>
<p>Should the object have compressed items</p>
</td></tr>
<tr><td><code id="nlmeControl_+3A_adjobf">adjObf</code></td>
<td>
<p>is a boolean to indicate if the objective function
should be adjusted to be closer to NONMEM's default objective
function.  By default this is <code>TRUE</code></p>
</td></tr>
<tr><td><code id="nlmeControl_+3A_ci">ci</code></td>
<td>
<p>Confidence level for some tables.  By default this is
0.95 or 95% confidence.</p>
</td></tr>
<tr><td><code id="nlmeControl_+3A_sigdig">sigdig</code></td>
<td>
<p>Optimization significant digits. This controls:
</p>

<ul>
<li><p> The tolerance of the inner and outer optimization is <code>10^-sigdig</code>
</p>
</li>
<li><p> The tolerance of the ODE solvers is
<code>0.5*10^(-sigdig-2)</code>; For the sensitivity equations and
steady-state solutions the default is <code>0.5*10^(-sigdig-1.5)</code>
(sensitivity changes only applicable for liblsoda)
</p>
</li>
<li><p> The tolerance of the boundary check is <code>5 * 10 ^ (-sigdig + 1)</code>
</p>
</li></ul>
</td></tr>
<tr><td><code id="nlmeControl_+3A_sigdigtable">sigdigTable</code></td>
<td>
<p>Significant digits in the final output table.
If not specified, then it matches the significant digits in the
'sigdig' optimization algorithm.  If 'sigdig' is NULL, use 3.</p>
</td></tr>
<tr><td><code id="nlmeControl_+3A_murefcovalg">muRefCovAlg</code></td>
<td>
<p>This controls if algebraic expressions that can
be mu-referenced are treated as mu-referenced covariates by:
</p>
<p>1. Creating a internal data-variable 'nlmixrMuDerCov#' for each
algebraic mu-referenced expression
</p>
<p>2. Change the algebraic expression to 'nlmixrMuDerCov# * mu_cov_theta'
</p>
<p>3. Use the internal mu-referenced covariate for saem
</p>
<p>4. After optimization is completed, replace 'model()' with old
'model()' expression
</p>
<p>5. Remove 'nlmixrMuDerCov#' from nlmix2 output
</p>
<p>In general, these covariates should be more accurate since it
changes the system to a linear compartment model.  Therefore, by default this is 'TRUE'.</p>
</td></tr>
<tr><td><code id="nlmeControl_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <code><a href="nlmixr2est.html#topic+nlmixr2NlmeControl">nlmixr2est::nlmeControl()</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a nlmixr-nlme list
</p>


<h3>See Also</h3>

<p>Other Estimation control: 
<code><a href="nlmixr2est.html#topic+foceiControl">foceiControl</a>()</code>,
<code><a href="nlmixr2est.html#topic+saemControl">saemControl</a>()</code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>nlmeControl()
nlmixr2NlmeControl()
</code></pre>

<hr>
<h2 id='nlminbControl'>nlmixr2 nlminb defaults</h2><span id='topic+nlminbControl'></span>

<h3>Description</h3>

<p>nlmixr2 nlminb defaults
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nlminbControl(
  eval.max = 200,
  iter.max = 150,
  trace = 0,
  abs.tol = 0,
  rel.tol = 1e-10,
  x.tol = 1.5e-08,
  xf.tol = 2.2e-14,
  step.min = 1,
  step.max = 1,
  sing.tol = rel.tol,
  scale = 1,
  scale.init = NULL,
  diff.g = NULL,
  rxControl = NULL,
  optExpression = TRUE,
  sumProd = FALSE,
  literalFix = TRUE,
  returnNlminb = FALSE,
  solveType = c("hessian", "grad", "fun"),
  stickyRecalcN = 4,
  maxOdeRecalc = 5,
  odeRecalcFactor = 10^(0.5),
  eventType = c("central", "forward"),
  shiErr = (.Machine$double.eps)^(1/3),
  shi21maxFD = 20L,
  optimHessType = c("central", "forward"),
  hessErr = (.Machine$double.eps)^(1/3),
  shi21maxHess = 20L,
  useColor = crayon::has_color(),
  printNcol = floor((getOption("width") - 23)/12),
  print = 1L,
  normType = c("rescale2", "mean", "rescale", "std", "len", "constant"),
  scaleType = c("nlmixr2", "norm", "mult", "multAdd"),
  scaleCmax = 1e+05,
  scaleCmin = 1e-05,
  scaleC = NULL,
  scaleTo = 1,
  gradTo = 1,
  addProp = c("combined2", "combined1"),
  calcTables = TRUE,
  compress = TRUE,
  covMethod = c("r", "nlminb", ""),
  adjObf = TRUE,
  ci = 0.95,
  sigdig = 4,
  sigdigTable = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nlminbControl_+3A_eval.max">eval.max</code></td>
<td>
<p>Maximum number of evaluations of the objective
function allowed.  Defaults to 200.</p>
</td></tr>
<tr><td><code id="nlminbControl_+3A_iter.max">iter.max</code></td>
<td>
<p>Maximum number of iterations allowed.  Defaults to
150.</p>
</td></tr>
<tr><td><code id="nlminbControl_+3A_trace">trace</code></td>
<td>
<p>The value of the objective function and the parameters
is printed every trace'th iteration.  When 0 no trace information
is to be printed</p>
</td></tr>
<tr><td><code id="nlminbControl_+3A_abs.tol">abs.tol</code></td>
<td>
<p>Absolute tolerance.  Defaults to 0 so the absolute
convergence test is not used.  If the objective function is known
to be non-negative, the previous default of '1e-20' would be more
appropriate</p>
</td></tr>
<tr><td><code id="nlminbControl_+3A_rel.tol">rel.tol</code></td>
<td>
<p>Relative tolerance.  Defaults to '1e-10'.</p>
</td></tr>
<tr><td><code id="nlminbControl_+3A_x.tol">x.tol</code></td>
<td>
<p>X tolerance.  Defaults to '1.5e-8'.</p>
</td></tr>
<tr><td><code id="nlminbControl_+3A_xf.tol">xf.tol</code></td>
<td>
<p>false convergence tolerance.  Defaults to '2.2e-14'.</p>
</td></tr>
<tr><td><code id="nlminbControl_+3A_step.min">step.min</code></td>
<td>
<p>Minimum step size.  Default to 1..</p>
</td></tr>
<tr><td><code id="nlminbControl_+3A_step.max">step.max</code></td>
<td>
<p>Maximum step size.  Default to 1..</p>
</td></tr>
<tr><td><code id="nlminbControl_+3A_sing.tol">sing.tol</code></td>
<td>
<p>singular convergence tolerance; defaults to 'rel.tol;.</p>
</td></tr>
<tr><td><code id="nlminbControl_+3A_scale">scale</code></td>
<td>
<p>See PORT documentation (or leave alone).</p>
</td></tr>
<tr><td><code id="nlminbControl_+3A_scale.init">scale.init</code></td>
<td>
<p>... probably need to check PORT documentation</p>
</td></tr>
<tr><td><code id="nlminbControl_+3A_diff.g">diff.g</code></td>
<td>
<p>an estimated bound on the relative error in the
objective function value</p>
</td></tr>
<tr><td><code id="nlminbControl_+3A_rxcontrol">rxControl</code></td>
<td>
<p>'rxode2' ODE solving options during fitting, created with 'rxControl()'</p>
</td></tr>
<tr><td><code id="nlminbControl_+3A_optexpression">optExpression</code></td>
<td>
<p>Optimize the rxode2 expression to speed up
calculation. By default this is turned on.</p>
</td></tr>
<tr><td><code id="nlminbControl_+3A_sumprod">sumProd</code></td>
<td>
<p>Is a boolean indicating if the model should change
multiplication to high precision multiplication and sums to
high precision sums using the PreciseSums package.  By default
this is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="nlminbControl_+3A_literalfix">literalFix</code></td>
<td>
<p>boolean, substitute fixed population values as
literals and re-adjust ui and parameter estimates after
optimization; Default is 'TRUE'.</p>
</td></tr>
<tr><td><code id="nlminbControl_+3A_returnnlminb">returnNlminb</code></td>
<td>
<p>logical; when TRUE this will return the nlminb
result instead of the nlmixr2 fit object</p>
</td></tr>
<tr><td><code id="nlminbControl_+3A_solvetype">solveType</code></td>
<td>
<p>tells if &lsquo;nlm' will use nlmixr2&rsquo;s analytical
gradients when available (finite differences will be used for
event-related parameters like parameters controlling lag time,
duration/rate of infusion, and modeled bioavailability). This can
be:
</p>
<p>- '&quot;hessian&quot;' which will use the analytical gradients to create a
Hessian with finite differences.
</p>
<p>- '&quot;gradient&quot;' which will use the gradient and let 'nlm' calculate
the finite difference hessian
</p>
<p>- '&quot;fun&quot;' where nlm will calculate both the finite difference
gradient and the finite difference Hessian
</p>
<p>When using nlmixr2's finite differences, the &quot;ideal&quot; step size for
either central or forward differences are optimized for with the
Shi2021 method which may give more accurate derivatives</p>
</td></tr>
<tr><td><code id="nlminbControl_+3A_stickyrecalcn">stickyRecalcN</code></td>
<td>
<p>The number of bad ODE solves before reducing
the atol/rtol for the rest of the problem.</p>
</td></tr>
<tr><td><code id="nlminbControl_+3A_maxoderecalc">maxOdeRecalc</code></td>
<td>
<p>Maximum number of times to reduce the ODE
tolerances and try to resolve the system if there was a bad
ODE solve.</p>
</td></tr>
<tr><td><code id="nlminbControl_+3A_oderecalcfactor">odeRecalcFactor</code></td>
<td>
<p>The ODE recalculation factor when ODE
solving goes bad, this is the factor the rtol/atol is reduced</p>
</td></tr>
<tr><td><code id="nlminbControl_+3A_eventtype">eventType</code></td>
<td>
<p>Event gradient type for dosing events; Can be
&quot;central&quot; or &quot;forward&quot;</p>
</td></tr>
<tr><td><code id="nlminbControl_+3A_shierr">shiErr</code></td>
<td>
<p>This represents the epsilon when optimizing the ideal
step size for numeric differentiation using the Shi2021 method</p>
</td></tr>
<tr><td><code id="nlminbControl_+3A_shi21maxfd">shi21maxFD</code></td>
<td>
<p>The maximum number of steps for the optimization
of the forward difference step size when using dosing events (lag
time, modeled duration/rate and bioavailability)</p>
</td></tr>
<tr><td><code id="nlminbControl_+3A_optimhesstype">optimHessType</code></td>
<td>
<p>The hessian type for when calculating the
individual hessian by numeric differences (in generalized
log-likelihood estimation).  The options are &quot;central&quot;, and
&quot;forward&quot;.  The central differences is what R's 'optimHess()'
uses and is the default for this method. (Though the &quot;forward&quot; is
faster and still reasonable for most cases).  The Shi21 cannot be
changed for the Gill83 algorithm with the optimHess in a
generalized likelihood problem.</p>
</td></tr>
<tr><td><code id="nlminbControl_+3A_hesserr">hessErr</code></td>
<td>
<p>This represents the epsilon when optimizing the
Hessian step size using the Shi2021 method.</p>
</td></tr>
<tr><td><code id="nlminbControl_+3A_shi21maxhess">shi21maxHess</code></td>
<td>
<p>Maximum number of times to optimize the best
step size for the hessian calculation</p>
</td></tr>
<tr><td><code id="nlminbControl_+3A_usecolor">useColor</code></td>
<td>
<p>Boolean indicating if focei can use ASCII color codes</p>
</td></tr>
<tr><td><code id="nlminbControl_+3A_printncol">printNcol</code></td>
<td>
<p>Number of columns to printout before wrapping
parameter estimates/gradient</p>
</td></tr>
<tr><td><code id="nlminbControl_+3A_print">print</code></td>
<td>
<p>Integer representing when the outer step is
printed. When this is 0 or do not print the iterations.  1 is
print every function evaluation (default), 5 is print every 5
evaluations.</p>
</td></tr>
<tr><td><code id="nlminbControl_+3A_normtype">normType</code></td>
<td>
<p>This is the type of parameter
normalization/scaling used to get the scaled initial values
for nlmixr2.  These are used with <code>scaleType</code> of.
</p>
<p>With the exception of <code>rescale2</code>, these come
from
<a href="https://en.wikipedia.org/wiki/Feature_scaling">Feature
Scaling</a>. The <code>rescale2</code> The rescaling is the same type
described in the
<a href="http://apmonitor.com/me575/uploads/Main/optimization_book.pdf">OptdesX</a>
software manual.
</p>
<p>In general, all all scaling formula can be described by:
</p>
<p style="text-align: center;"><code class="reqn">v_{scaled}</code>
</p>
<p> = (</p>
<p style="text-align: center;"><code class="reqn">v_{unscaled}-C_{1}</code>
</p>
<p>)/</p>
<p style="text-align: center;"><code class="reqn">C_{2}</code>
</p>

<p>Where
</p>
<p>The other data normalization approaches follow the following formula
</p>
<p style="text-align: center;"><code class="reqn">v_{scaled}</code>
</p>
<p> = (</p>
<p style="text-align: center;"><code class="reqn">v_{unscaled}-C_{1}</code>
</p>
<p>)/</p>
<p style="text-align: center;"><code class="reqn">C_{2}</code>
</p>


<ul>
<li> <p><code>rescale2</code> This scales all parameters from (-1 to 1).
The relative differences between the parameters are preserved
with this approach and the constants are:
</p>
<p style="text-align: center;"><code class="reqn">C_{1}</code>
</p>
<p> = (max(all unscaled values)+min(all unscaled values))/2
</p>
<p style="text-align: center;"><code class="reqn">C_{2}</code>
</p>
<p> = (max(all unscaled values) - min(all unscaled values))/2
</p>
</li>
<li> <p><code>rescale</code> or min-max normalization. This rescales all
parameters from (0 to 1).  As in the <code>rescale2</code> the
relative differences are preserved.  In this approach:
</p>
<p style="text-align: center;"><code class="reqn">C_{1}</code>
</p>
<p> = min(all unscaled values)
</p>
<p style="text-align: center;"><code class="reqn">C_{2}</code>
</p>
<p> = max(all unscaled values) - min(all unscaled values)
</p>
</li>
<li> <p><code>mean</code> or mean normalization.  This rescales to center
the parameters around the mean but the parameters are from 0
to 1.  In this approach:
</p>
<p style="text-align: center;"><code class="reqn">C_{1}</code>
</p>
<p> = mean(all unscaled values)
</p>
<p style="text-align: center;"><code class="reqn">C_{2}</code>
</p>
<p> = max(all unscaled values) - min(all unscaled values)
</p>
</li>
<li> <p><code>std</code> or standardization.  This standardizes by the mean
and standard deviation.  In this approach:
</p>
<p style="text-align: center;"><code class="reqn">C_{1}</code>
</p>
<p> = mean(all unscaled values)
</p>
<p style="text-align: center;"><code class="reqn">C_{2}</code>
</p>
<p> = sd(all unscaled values)
</p>
</li>
<li> <p><code>len</code> or unit length scaling.  This scales the
parameters to the unit length.  For this approach we use the Euclidean length, that
is:
</p>
<p style="text-align: center;"><code class="reqn">C_{1}</code>
</p>
<p> = 0
</p>
<p style="text-align: center;"><code class="reqn">C_{2}</code>
</p>
<p> = </p>
<p style="text-align: center;"><code class="reqn">\sqrt(v_1^2 + v_2^2 + \cdots + v_n^2)</code>
</p>

</li>
<li> <p><code>constant</code> which does not perform data normalization. That is
</p>
<p style="text-align: center;"><code class="reqn">C_{1}</code>
</p>
<p> = 0
</p>
<p style="text-align: center;"><code class="reqn">C_{2}</code>
</p>
<p> = 1
</p>
</li></ul>
</td></tr>
<tr><td><code id="nlminbControl_+3A_scaletype">scaleType</code></td>
<td>
<p>The scaling scheme for nlmixr2.  The supported types are:
</p>

<ul>
<li> <p><code>nlmixr2</code>  In this approach the scaling is performed by the following equation:
</p>
<p style="text-align: center;"><code class="reqn">v_{scaled}</code>
</p>
<p> = (</p>
<p style="text-align: center;"><code class="reqn">v_{current} - v_{init}</code>
</p>
<p>)*scaleC[i] + scaleTo
</p>
<p>The <code>scaleTo</code> parameter is specified by the <code>normType</code>,
and the scales are specified by <code>scaleC</code>.
</p>
</li>
<li> <p><code>norm</code> This approach uses the simple scaling provided
by the <code>normType</code> argument.
</p>
</li>
<li> <p><code>mult</code> This approach does not use the data
normalization provided by <code>normType</code>, but rather uses
multiplicative scaling to a constant provided by the <code>scaleTo</code>
argument.
</p>
<p>In this case:
</p>
<p style="text-align: center;"><code class="reqn">v_{scaled}</code>
</p>
<p> = </p>
<p style="text-align: center;"><code class="reqn">v_{current}</code>
</p>
<p>/</p>
<p style="text-align: center;"><code class="reqn">v_{init}</code>
</p>
<p>*scaleTo
</p>
</li>
<li> <p><code>multAdd</code> This approach changes the scaling based on
the parameter being specified.  If a parameter is defined in an
exponential block (ie exp(theta)), then it is scaled on a
linearly, that is:
</p>
<p style="text-align: center;"><code class="reqn">v_{scaled}</code>
</p>
<p> = (</p>
<p style="text-align: center;"><code class="reqn">v_{current}-v_{init}</code>
</p>
<p>) + scaleTo
</p>
<p>Otherwise the parameter is scaled multiplicatively.
</p>
<p style="text-align: center;"><code class="reqn">v_{scaled}</code>
</p>
<p> = </p>
<p style="text-align: center;"><code class="reqn">v_{current}</code>
</p>
<p>/</p>
<p style="text-align: center;"><code class="reqn">v_{init}</code>
</p>
<p>*scaleTo
</p>
</li></ul>
</td></tr>
<tr><td><code id="nlminbControl_+3A_scalecmax">scaleCmax</code></td>
<td>
<p>Maximum value of the scaleC to prevent overflow.</p>
</td></tr>
<tr><td><code id="nlminbControl_+3A_scalecmin">scaleCmin</code></td>
<td>
<p>Minimum value of the scaleC to prevent underflow.</p>
</td></tr>
<tr><td><code id="nlminbControl_+3A_scalec">scaleC</code></td>
<td>
<p>The scaling constant used with
<code>scaleType=nlmixr2</code>.  When not specified, it is based on
the type of parameter that is estimated.  The idea is to keep
the derivatives similar on a log scale to have similar
gradient sizes.  Hence parameters like log(exp(theta)) would
have a scaling factor of 1 and log(theta) would have a scaling
factor of ini_value (to scale by 1/value; ie
d/dt(log(ini_value)) = 1/ini_value or scaleC=ini_value)
</p>

<ul>
<li><p> For parameters in an exponential (ie exp(theta)) or
parameters specifying powers, boxCox or yeoJohnson
transformations , this is 1.
</p>
</li>
<li><p> For additive, proportional, lognormal error structures,
these are given by 0.5*abs(initial_estimate)
</p>
</li>
<li><p> Factorials are scaled by abs(1/digamma(initial_estimate+1))
</p>
</li>
<li><p> parameters in a log scale (ie log(theta)) are transformed
by log(abs(initial_estimate))*abs(initial_estimate)
</p>
</li></ul>

<p>These parameter scaling coefficients are chose to try to keep
similar slopes among parameters.  That is they all follow the
slopes approximately on a log-scale.
</p>
<p>While these are chosen in a logical manner, they may not always
apply.  You can specify each parameters scaling factor by this
parameter if you wish.</p>
</td></tr>
<tr><td><code id="nlminbControl_+3A_scaleto">scaleTo</code></td>
<td>
<p>Scale the initial parameter estimate to this value.
By default this is 1.  When zero or below, no scaling is performed.</p>
</td></tr>
<tr><td><code id="nlminbControl_+3A_gradto">gradTo</code></td>
<td>
<p>this is the factor that the gradient is scaled to
before optimizing.  This only works with
scaleType=&quot;nlmixr2&quot;.</p>
</td></tr>
<tr><td><code id="nlminbControl_+3A_addprop">addProp</code></td>
<td>
<p>specifies the type of additive plus proportional
errors, the one where standard deviations add (combined1) or the
type where the variances add (combined2).
</p>
<p>The combined1 error type can be described by the following equation:
</p>
<p style="text-align: center;"><code class="reqn">y = f + (a + b\times f^c) \times \varepsilon</code>
</p>

<p>The combined2 error model can be described by the following equation:
</p>
<p style="text-align: center;"><code class="reqn">y = f + \sqrt{a^2 + b^2\times f^{2\times c}} \times \varepsilon</code>
</p>

<p>Where:
</p>
<p>- y represents the observed value
</p>
<p>- f represents the predicted value
</p>
<p>- a  is the additive standard deviation
</p>
<p>- b is the proportional/power standard deviation
</p>
<p>- c is the power exponent (in the proportional case c=1)</p>
</td></tr>
<tr><td><code id="nlminbControl_+3A_calctables">calcTables</code></td>
<td>
<p>This boolean is to determine if the foceiFit
will calculate tables. By default this is <code>TRUE</code></p>
</td></tr>
<tr><td><code id="nlminbControl_+3A_compress">compress</code></td>
<td>
<p>Should the object have compressed items</p>
</td></tr>
<tr><td><code id="nlminbControl_+3A_covmethod">covMethod</code></td>
<td>
<p>Method for calculating covariance.  In this
discussion, R is the Hessian matrix of the objective
function. The S matrix is the sum of individual
gradient cross-product (evaluated at the individual empirical
Bayes estimates).
</p>

<ul>
<li><p> &quot;<code>r,s</code>&quot; Uses the sandwich matrix to calculate the
covariance, that is: <code>solve(R) %*% S %*% solve(R)</code>
</p>
</li>
<li><p> &quot;<code>r</code>&quot; Uses the Hessian matrix to calculate the
covariance as <code>2 %*% solve(R)</code>
</p>
</li>
<li><p> &quot;<code>s</code>&quot; Uses the cross-product matrix to calculate the
covariance as <code>4 %*% solve(S)</code>
</p>
</li>
<li><p> &quot;&quot; Does not calculate the covariance step.
</p>
</li></ul>
</td></tr>
<tr><td><code id="nlminbControl_+3A_adjobf">adjObf</code></td>
<td>
<p>is a boolean to indicate if the objective function
should be adjusted to be closer to NONMEM's default objective
function.  By default this is <code>TRUE</code></p>
</td></tr>
<tr><td><code id="nlminbControl_+3A_ci">ci</code></td>
<td>
<p>Confidence level for some tables.  By default this is
0.95 or 95% confidence.</p>
</td></tr>
<tr><td><code id="nlminbControl_+3A_sigdig">sigdig</code></td>
<td>
<p>Optimization significant digits. This controls:
</p>

<ul>
<li><p> The tolerance of the inner and outer optimization is <code>10^-sigdig</code>
</p>
</li>
<li><p> The tolerance of the ODE solvers is
<code>0.5*10^(-sigdig-2)</code>; For the sensitivity equations and
steady-state solutions the default is <code>0.5*10^(-sigdig-1.5)</code>
(sensitivity changes only applicable for liblsoda)
</p>
</li>
<li><p> The tolerance of the boundary check is <code>5 * 10 ^ (-sigdig + 1)</code>
</p>
</li></ul>
</td></tr>
<tr><td><code id="nlminbControl_+3A_sigdigtable">sigdigTable</code></td>
<td>
<p>Significant digits in the final output table.
If not specified, then it matches the significant digits in the
'sigdig' optimization algorithm.  If 'sigdig' is NULL, use 3.</p>
</td></tr>
<tr><td><code id="nlminbControl_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <code><a href="nlmixr2est.html#topic+nlminbControl">nlmixr2est::nlminbControl()</a></code>.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>Matthew L. Fidler
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# A logit regression example with emax model

dsn &lt;- data.frame(i=1:1000)
dsn$time &lt;- exp(rnorm(1000))
dsn$DV=rbinom(1000,1,exp(-1+dsn$time)/(1+exp(-1+dsn$time)))

mod &lt;- function() {
 ini({
   E0 &lt;- 0.5
   Em &lt;- 0.5
   E50 &lt;- 2
   g &lt;- fix(2)
 })
 model({
   v &lt;- E0+Em*time^g/(E50^g+time^g)
   ll(bin) ~ DV * v - log(1 + exp(v))
 })
}

fit2 &lt;- nlmixr(mod, dsn, est="nlminb")

print(fit2)

# you can also get the nlm output with fit2$nlminb

fit2$nlminb

</code></pre>

<hr>
<h2 id='nlmixr2'>nlmixr2 fits population PK and PKPD non-linear mixed effects models.</h2><span id='topic+nlmixr2'></span>

<h3>Description</h3>

<p>nlmixr2 is an R package for fitting population pharmacokinetic (PK)
and pharmacokinetic-pharmacodynamic (PKPD) models.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nlmixr2(
  object,
  data,
  est = NULL,
  control = list(),
  table = tableControl(),
  ...,
  save = NULL,
  envir = parent.frame()
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nlmixr2_+3A_object">object</code></td>
<td>
<p>Fitted object or function specifying the model.</p>
</td></tr>
<tr><td><code id="nlmixr2_+3A_data">data</code></td>
<td>
<p>nlmixr data</p>
</td></tr>
<tr><td><code id="nlmixr2_+3A_est">est</code></td>
<td>
<p>estimation method (all methods are shown by
'nlmixr2AllEst()'). Methods can be added for other tools</p>
</td></tr>
<tr><td><code id="nlmixr2_+3A_control">control</code></td>
<td>
<p>The estimation control object.  These are expected
to be different for each type of estimation method</p>
</td></tr>
<tr><td><code id="nlmixr2_+3A_table">table</code></td>
<td>
<p>The output table control object (like
'tableControl()')</p>
</td></tr>
<tr><td><code id="nlmixr2_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <code><a href="nlmixr2est.html#topic+nlmixr2">nlmixr2est::nlmixr2()</a></code>.</p>
</td></tr>
<tr><td><code id="nlmixr2_+3A_save">save</code></td>
<td>
<p>Boolean to save a nlmixr2 object in a rds file in the
working directory.  If <code>NULL</code>, uses option &quot;nlmixr2.save&quot;</p>
</td></tr>
<tr><td><code id="nlmixr2_+3A_envir">envir</code></td>
<td>
<p>Environment where the nlmixr object/function is
evaluated before running the estimation routine.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>The nlmixr2 generalized function allows common access to the nlmixr2
estimation routines.
</p>
<p>The nlmixr object has the following fields:</p>

<table>
<tr>
 <td style="text-align: left;">
   Field </td><td style="text-align: left;"> Description </td>
</tr>
<tr>
 <td style="text-align: left;">
   conditionNumber </td><td style="text-align: left;"> Condition number, that is the highest divided by the lowest eigenvalue in the population covariance matrix </td>
</tr>
<tr>
 <td style="text-align: left;">
   cor </td><td style="text-align: left;"> Correlation matrix </td>
</tr>
<tr>
 <td style="text-align: left;">
   phiR </td><td style="text-align: left;"> correlation matrix of each individuals eta (if present) </td>
</tr>
<tr>
 <td style="text-align: left;">
   objDF </td><td style="text-align: left;"> Data frame containing objective function information (AIC, BIC, etc.) </td>
</tr>
<tr>
 <td style="text-align: left;">
   time </td><td style="text-align: left;"> Duration of different parts of the analysis (e.g.setup, optimization, calculation of covariance, etc.) </td>
</tr>
<tr>
 <td style="text-align: left;">
   theta </td><td style="text-align: left;"> Estimates for eta for each individual </td>
</tr>
<tr>
 <td style="text-align: left;">
   etaObf </td><td style="text-align: left;"> Estimates for eta for each individual, This also includes the objective function for each individual </td>
</tr>
<tr>
 <td style="text-align: left;">
   fixef </td><td style="text-align: left;"> Estimates of fixed effects </td>
</tr>
<tr>
 <td style="text-align: left;">
   foceiControl </td><td style="text-align: left;"> Estimation options if focei was used </td>
</tr>
<tr>
 <td style="text-align: left;">
   ui </td><td style="text-align: left;"> Final estimates for the model </td>
</tr>
<tr>
 <td style="text-align: left;">
   dataMergeFull </td><td style="text-align: left;"> Full data merge with the fit output and the original dataset; Also includes nlmixrLlikObs which includes the individual observation contribution to the likelihood </td>
</tr>
<tr>
 <td style="text-align: left;">
   censInfo </td><td style="text-align: left;"> Gives the censorng information abot the fit (the type of censoring that was seend and handled in the dataset) </td>
</tr>
<tr>
 <td style="text-align: left;">
   dataLloq </td><td style="text-align: left;"> Gives the lloq from the dataset (average) when cesoring has occured; Requires the fit to have a table step </td>
</tr>
<tr>
 <td style="text-align: left;">
   dataUloq </td><td style="text-align: left;"> Gives the uloq from the dataset (average) when censoring has occured; requires the fit to have a table step </td>
</tr>
<tr>
 <td style="text-align: left;">
   eta </td><td style="text-align: left;"> IIV values for each indiviudal </td>
</tr>
<tr>
 <td style="text-align: left;">
   dataMergeInner </td><td style="text-align: left;"> Inner data merge with the fit output and the original dataset; Also includes nlmixrLlikObs which includes the individual observation contribution to the likelihood </td>
</tr>
<tr>
 <td style="text-align: left;">
   rxControl </td><td style="text-align: left;"> Integration options used to control rxode2 </td>
</tr>
<tr>
 <td style="text-align: left;">
   dataMergeLeft </td><td style="text-align: left;"> Left data merge with the fit output and the original dataset; Also includes nlmixrLlikObs which includes the individual observation contribution to the likelihood </td>
</tr>
<tr>
 <td style="text-align: left;">
   omega </td><td style="text-align: left;"> Matrix containing the estimates of the multivarte normal covariance matrix for between subject varaibilities (omega) </td>
</tr>
<tr>
 <td style="text-align: left;">
   covMethod </td><td style="text-align: left;"> Method used to calculate covariance of the fixed effects </td>
</tr>
<tr>
 <td style="text-align: left;">
   modelName </td><td style="text-align: left;"> Name of the R object containing the model </td>
</tr>
<tr>
 <td style="text-align: left;">
   origData </td><td style="text-align: left;"> Original dataset </td>
</tr>
<tr>
 <td style="text-align: left;">
   phiRSE </td><td style="text-align: left;"> Relative standard error of each individuals eta </td>
</tr>
<tr>
 <td style="text-align: left;">
   dataMergeRight </td><td style="text-align: left;"> Right data merge with the fit output and the original dataset; Also includes nlmixrLlikObs which includes the individual observation contribution to the likelihood </td>
</tr>
<tr>
 <td style="text-align: left;">
   ipredModel </td><td style="text-align: left;"> rxode2 estimation model for fit (internal will likely be removed from visibility </td>
</tr>
<tr>
 <td style="text-align: left;">
   phiSE </td><td style="text-align: left;"> Standard error of each individuals eta </td>
</tr>
<tr>
 <td style="text-align: left;">
   parFixed </td><td style="text-align: left;"> Table of parameter estimates (rounded and pretty looking) </td>
</tr>
<tr>
 <td style="text-align: left;">
   parFixedDF </td><td style="text-align: left;"> Table of parameter estimates as a data frame </td>
</tr>
<tr>
 <td style="text-align: left;">
   omegaR </td><td style="text-align: left;"> The correlation matirx of omega with standard deviations for the diagonal pieces </td>
</tr>
<tr>
 <td style="text-align: left;">
   iniUi </td><td style="text-align: left;"> The initial model used to start the estimation </td>
</tr>
<tr>
 <td style="text-align: left;">
   finalUi </td><td style="text-align: left;"> The model with the estimates replaced as values </td>
</tr>
<tr>
 <td style="text-align: left;">
   scaleInfo </td><td style="text-align: left;"> The scaling factors used for nlmixr2 estimation in focei; The can be changed by foceiControl(scaleC=) if you think these are unreasonable. It also tells the Gill83 outcome of trying to find the best step size (High gradient error, bad gradient etc) </td>
</tr>
<tr>
 <td style="text-align: left;">
   table </td><td style="text-align: left;"> These are the table options that were used when generating the table output (were CWRES included, etc </td>
</tr>
<tr>
 <td style="text-align: left;">
   shrink </td><td style="text-align: left;"> This is a table of shrinkages for all the individual ETAs as well as the variance shrinkage as well as summary statistics for the ETAs and Residual Error </td>
</tr>
<tr>
 <td style="text-align: left;">
   env </td><td style="text-align: left;"> This is the environment where all the information for the fit is stored outside of the data-frame. It is an R environment hence $env </td>
</tr>
<tr>
 <td style="text-align: left;">
   seed </td><td style="text-align: left;"> This is the initial seed used for saem </td>
</tr>
<tr>
 <td style="text-align: left;">
   simInfo </td><td style="text-align: left;"> This returns a list of all the fit information used for a traditional rxode2 simulation, which you can tweak yourself if you wish </td>
</tr>
<tr>
 <td style="text-align: left;">
   runInfo </td><td style="text-align: left;"> This returns a list of all the warnings or fit information </td>
</tr>
<tr>
 <td style="text-align: left;">
   parHistStacked </td><td style="text-align: left;"> Value of objective function and parameters at each iteration (tall format) </td>
</tr>
<tr>
 <td style="text-align: left;">
   parHist </td><td style="text-align: left;"> Value of objective function and parameters at each iteration (wide format) </td>
</tr>
<tr>
 <td style="text-align: left;">
   cov </td><td style="text-align: left;"> Variance-covariance matrix </td>
</tr>
<tr>
 <td style="text-align: left;">
</td>
</tr>

</table>



<h3>Value</h3>

<p>Either a nlmixr2 model or a nlmixr2 fit object
</p>


<h3>nlmixr modeling mini-language</h3>

<p><b>Rationale</b>
</p>
<p>nlmixr estimation routines each have their own way of specifying
models.  Often the models are specified in ways that are most
intuitive for one estimation routine, but do not make sense for
another estimation routine.  Sometimes, legacy estimation
routines like <code><a href="nlme.html#topic+nlme">nlme</a></code> have their own syntax that
is outside of the control of the nlmixr package.
</p>
<p>The unique syntax of each routine makes the routines themselves
easier to maintain and expand, and allows interfacing with
existing packages that are outside of nlmixr (like
<code><a href="nlme.html#topic+nlme">nlme</a></code>).  However, a model definition language
that is common between estimation methods, and an output object
that is uniform, will make it easier to switch between estimation
routines and will facilitate interfacing output with external
packages like Xpose.
</p>
<p>The nlmixr mini-modeling language, attempts to address this issue
by incorporating a common language.  This language is inspired by
both R and NONMEM, since these languages are familiar to many
pharmacometricians.
</p>
<p><b>Initial Estimates and boundaries for population parameters</b>
</p>
<p>nlmixr models are contained in a R function with two blocks:
<code>ini</code> and <code>model</code>.  This R function can be named
anything, but is not meant to be called directly from R.  In fact
if you try you will likely get an error such as <code>Error: could
not find function "ini"</code>.
</p>
<p>The <code>ini</code> model block is meant to hold the initial estimates
for the model, and the boundaries of the parameters for estimation
routines that support boundaries (note nlmixr's <code>saem</code>
and <code>nlme</code> do not currently support parameter boundaries).
</p>
<p>To explain how these initial estimates are specified we will start
with an annotated example:
</p>
<pre>
f &lt;- function(){ ## Note the arguments to the function are currently
                 ## ignored by nlmixr
    ini({
        ## Initial conditions for population parameters (sometimes
        ## called theta parameters) are defined by either `&lt;-` or '='
        lCl &lt;- 1.6      #log Cl (L/hr)
        ## Note that simple expressions that evaluate to a number are
        ## OK for defining initial conditions (like in R)
        lVc = log(90)  #log V (L)
        ## Also a comment on a parameter is captured as a parameter label
        lKa &lt;- 1 #log Ka (1/hr)
        ## Bounds may be specified by c(lower, est, upper), like NONMEM:
        ## Residuals errors are assumed to be population parameters
        prop.err &lt;- c(0, 0.2, 1)
    })
    ## The model block will be discussed later
    model({})
}
</pre>
<p>As shown in the above examples:
</p>

<ul>
<li><p> Simple parameter values are specified as a R-compatible assignment
</p>
</li>
<li><p> Boundaries my be specified by <code>c(lower, est, upper)</code>.
</p>
</li>
<li><p> Like NONMEM, <code>c(lower,est)</code> is equivalent to <code>c(lower,est,Inf)</code>
</p>
</li>
<li><p> Also like NONMEM, <code>c(est)</code> does not specify a lower bound, and is equivalent
to specifying the parameter  without R's 'c' function.
</p>
</li>
<li><p> The initial estimates are specified on the variance scale, and in analogy with
NONMEM, the square roots of the diagonal elements correspond to coefficients of
variation when used in the exponential IIV implementation
</p>
</li></ul>

<p>These parameters can be named almost any R compatible name.  Please note that:
</p>

<ul>
<li><p> Residual error estimates should be coded as population estimates (i.e. using an
'=' or '&lt;-' statement, not a '~').
</p>
</li>
<li><p> Naming variables that start with &quot;<code>_</code>&quot; are not supported.  Note that R does not
allow variable starting with &quot;<code>_</code>&quot; to be assigned without quoting them.
</p>
</li>
<li><p> Naming variables that start with &quot;<code>rx_</code>&quot; or &quot;<code>nlmixr_</code>&quot; is
not supported since <a href="rxode2.html#topic+rxode2">rxode2</a> and nlmixr2 use these prefixes
internally for certain estimation routines and calculating residuals.
</p>
</li>
<li><p> Variable names are case sensitive, just like they are in R. &quot;<code>CL</code>&quot; is not the
same as &quot;<code>Cl</code>&quot;.
</p>
</li></ul>

<p><b>Initial Estimates for between subject error distribution (NONMEM's  $OMEGA)</b>
</p>
<p>In mixture models, multivariate normal individual deviations from
the population parameters are estimated (in NONMEM these are
called <code>eta</code> parameters).  Additionally the
variance/covariance matrix of these deviations is also estimated
(in NONMEM this is the OMEGA matrix).  These also have initial
estimates.  In nlmixr these are specified by the '~' operator that
is typically used in R for &quot;modeled by&quot;, and was chosen to
distinguish these estimates from the population and residual error
parameters.
</p>
<p>Continuing the prior example, we can annotate the estimates for
the between subject error distribution
</p>
<pre>
f &lt;- function(){
    ini({
        lCl &lt;- 1.6      #log Cl (L/hr)
        lVc = log(90)  #log V (L)
        lKa &lt;- 1 #log Ka (1/hr)
        prop.err &lt;- c(0, 0.2, 1)
        ## Initial estimate for ka IIV variance
        ## Labels work for single parameters
        eta.ka ~ 0.1 # BSV Ka

        ## For correlated parameters, you specify the names of each
        ## correlated parameter separated by a addition operator `+`
        ## and the left handed side specifies the lower triangular
        ## matrix initial of the covariance matrix.
        eta.cl + eta.vc ~ c(0.1,
                            0.005, 0.1)
        ## Note that labels do not currently work for correlated
        ## parameters.  Also do not put comments inside the lower
        ## triangular matrix as this will currently break the model.
    })
    ## The model block will be discussed later
    model({})
}
</pre>
<p>As shown in the above examples:
</p>

<ul>
<li><p> Simple variances are specified by the variable name  and the
estimate separated by '~'.
</p>
</li>
<li><p> Correlated parameters are specified by the sum of the variable
labels and then the lower triangular matrix of the covariance is
specified on the left handed side of the equation. This is also
separated by '~'.
</p>
</li></ul>

<p>Currently the model syntax does not allow comments inside the
lower triangular matrix.
</p>
<p><b>Model Syntax for ODE based models (NONMEM's $PK, $PRED, $DES and $ERROR)</b>
</p>
<p>Once the initialization block has been defined, you can define a
model in terms of the defined variables in the <code>ini</code> block.  You can
also mix in RxODE blocks into the model.
</p>
<p>The current method of defining a nlmixr model is to specify the
parameters, and then possibly the RxODE lines:
</p>
<p>Continuing describing the syntax with an annotated example:
</p>
<pre>
f &lt;- function(){
    ini({
        lCl &lt;- 1.6      #log Cl (L/hr)
        lVc &lt;- log(90)   #log Vc (L)
        lKA &lt;- 0.1      #log Ka (1/hr)
        prop.err &lt;- c(0, 0.2, 1)
        eta.Cl ~ 0.1 ## BSV Cl
        eta.Vc ~ 0.1 ## BSV Vc
        eta.KA ~ 0.1 ## BSV Ka
    })
    model({
        ## First parameters are defined in terms of the initial estimates
        ## parameter names.
        Cl &lt;- exp(lCl + eta.Cl)
        Vc = exp(lVc + eta.Vc)
        KA &lt;- exp(lKA + eta.KA)
        ## After the differential equations are defined
        kel &lt;- Cl / Vc;
        d/dt(depot)    = -KA*depot;
        d/dt(centr)  =  KA*depot-kel*centr;
        ## And the concentration is then calculated
        cp = centr / Vc;
        ## Last, nlmixr is told that the plasma concentration follows
        ## a proportional error (estimated by the parameter prop.err)
        cp ~ prop(prop.err)
    })
}
</pre>
<p>A few points to note:
</p>

<ul>
<li><p> Parameters are often defined before the differential equations.
</p>
</li>
<li><p> The differential equations, parameters and error terms are in a single
block, instead of multiple sections.
</p>
</li>
<li><p> State names, calculated variables cannot start with either &quot;<code>rx_</code>&quot;
or &quot;<code>nlmixr_</code>&quot; since these are used internally in some estimation routines.
</p>
</li>
<li><p> Errors are specified using the '~'.  Currently you can use either <code>add(parameter)</code>
for additive error,  prop(parameter) for proportional error or <code>add(parameter1) + prop(parameter2)</code>
for additive plus proportional error.  You can also specify <code>norm(parameter)</code> for the additive error,
since it follows a normal distribution.
</p>
</li>
<li><p> Some routines, like <code>saem</code> require  parameters in terms of <code>Pop.Parameter + Individual.Deviation.Parameter + Covariate*Covariate.Parameter</code>.
The order of these parameters do not matter.  This is similar to NONMEM's mu-referencing, though
not quite so restrictive.
</p>
</li>
<li><p> The type of parameter in the model is determined by the initial block;  Covariates used in the
model are missing in the <code>ini</code> block.  These variables need to be present in the modeling
dataset for the model to run.
</p>
</li></ul>

<p><b>Model Syntax for solved PK systems</b>
</p>
<p>Solved PK systems are also currently supported by nlmixr with the
'linCmt()' pseudo-function.  An annotated example of a solved
system is below:
</p>
<p>##' </p>
<pre>
f &lt;- function(){
    ini({
        lCl &lt;- 1.6      #log Cl (L/hr)
        lVc &lt;- log(90)   #log Vc (L)
        lKA &lt;- 0.1      #log Ka (1/hr)
        prop.err &lt;- c(0, 0.2, 1)
        eta.Cl ~ 0.1 ## BSV Cl
        eta.Vc ~ 0.1 ## BSV Vc
        eta.KA ~ 0.1 ## BSV Ka
    })
    model({
        Cl &lt;- exp(lCl + eta.Cl)
        Vc = exp(lVc + eta.Vc)
        KA &lt;- exp(lKA + eta.KA)
        ## Instead of specifying the ODEs, you can use
        ## the linCmt() function to use the solved system.
        ##
        ## This function determines the type of PK solved system
        ## to use by the parameters that are defined.  In this case
        ## it knows that this is a one-compartment model with first-order
        ## absorption.
        linCmt() ~ prop(prop.err)
    })
}
</pre>
<p>A few things to keep in mind:
</p>

<ul>
<li><p> While RxODE allows mixing of solved systems and ODEs, this has not
been implemented in nlmixr yet.
</p>
</li>
<li><p> The solved systems implemented are the one, two and three compartment
models with or without first-order absorption.  Each of the models support a
lag time with a tlag parameter.
</p>
</li>
<li><p> In general the linear compartment model figures out the model by the parameter names.
nlmixr currently knows about numbered volumes, Vc/Vp, Clearances in terms of both Cl and
Q/CLD.  Additionally nlmixr knows about elimination micro-constants (ie K12).  Mixing of
these parameters for these models is currently not supported.
</p>
</li></ul>

<p><b>Checking model syntax</b>
</p>
<p>After specifying the model syntax you can check that nlmixr is
interpreting it correctly by using the <code>nlmixr</code> function on
it.
</p>
<p>Using the above function we can get:
</p>
<pre>
&gt; nlmixr(f)
## 1-compartment model with first-order absorption in terms of Cl
## Initialization:
################################################################################
Fixed Effects ($theta):
    lCl     lVc     lKA
1.60000 4.49981 0.10000

Omega ($omega):
     [,1] [,2] [,3]
[1,]  0.1  0.0  0.0
[2,]  0.0  0.1  0.0
[3,]  0.0  0.0  0.1

## Model:
################################################################################
Cl &lt;- exp(lCl + eta.Cl)
Vc = exp(lVc + eta.Vc)
KA &lt;- exp(lKA + eta.KA)
## Instead of specifying the ODEs, you can use
## the linCmt() function to use the solved system.
##
## This function determines the type of PK solved system
## to use by the parameters that are defined.  In this case
## it knows that this is a one-compartment model with first-order
## absorption.
linCmt() ~ prop(prop.err)
</pre>
<p>In general this gives you information about the model (what type
of solved system/RxODE), initial estimates as well as the code for
the model block.
</p>
<p><b>Using the model syntax for estimating a model</b>
</p>
<p>Once the model function has been created, you can use it and a
dataset to estimate the parameters for a model given a dataset.
</p>
<p>This dataset has to have RxODE compatible events IDs.  Both
Monolix and NONMEM use a a very similar standard to what nlmixr can support.
</p>
<p>Once the data has been converted to the appropriate format, you
can use the <code>nlmixr</code> function to run the appropriate code.
</p>
<p>The method to estimate the model is:
</p>
<pre>
fit &lt;- nlmixr(model.function, dataset, est="est", control=estControl(options))
</pre>
<p>Currently <code>nlme</code> and <code>saem</code> are implemented.  For example, to run the
above model with <code>saem</code>, we could have the following:
</p>
<pre>
&gt; f &lt;- function(){
    ini({
        lCl &lt;- 1.6      #log Cl (L/hr)
        lVc &lt;- log(90)   #log Vc (L)
        lKA &lt;- 0.1      #log Ka (1/hr)
        prop.err &lt;- c(0, 0.2, 1)
        eta.Cl ~ 0.1 ## BSV Cl
        eta.Vc ~ 0.1 ## BSV Vc
        eta.KA ~ 0.1 ## BSV Ka
    })
    model({
        ## First parameters are defined in terms of the initial estimates
        ## parameter names.
        Cl &lt;- exp(lCl + eta.Cl)
        Vc = exp(lVc + eta.Vc)
        KA &lt;- exp(lKA + eta.KA)
        ## After the differential equations are defined
        kel &lt;- Cl / Vc;
        d/dt(depot)    = -KA*depot;
        d/dt(centr)  =  KA*depot-kel*centr;
        ## And the concentration is then calculated
        cp = centr / Vc;
        ## Last, nlmixr is told that the plasma concentration follows
        ## a proportional error (estimated by the parameter prop.err)
        cp ~ prop(prop.err)
    })
}
&gt; fit.s &lt;- nlmixr(f,d,est="saem",control=saemControl(n.burn=50,n.em=100,print=50));
Compiling RxODE differential equations...done.
c:/Rtools/mingw_64/bin/g++  -I"c:/R/R-34~1.1/include" -DNDEBUG     -I"d:/Compiler/gcc-4.9.3/local330/include"  -Ic:/nlmixr/inst/include -Ic:/R/R-34~1.1/library/STANHE~1/include -Ic:/R/R-34~1.1/library/Rcpp/include -Ic:/R/R-34~1.1/library/RCPPAR~1/include -Ic:/R/R-34~1.1/library/RCPPEI~1/include -Ic:/R/R-34~1.1/library/BH/include   -O2 -Wall  -mtune=core2 -c saem3090757b4bd1x64.cpp -o saem3090757b4bd1x64.o
In file included from c:/R/R-34~1.1/library/RCPPAR~1/include/armadillo:52:0,
                 from c:/R/R-34~1.1/library/RCPPAR~1/include/RcppArmadilloForward.h:46,
                 from c:/R/R-34~1.1/library/RCPPAR~1/include/RcppArmadillo.h:31,
                 from saem3090757b4bd1x64.cpp:1:
c:/R/R-34~1.1/library/RCPPAR~1/include/armadillo_bits/compiler_setup.hpp:474:96: note: #pragma message: WARNING: use of OpenMP disabled; this compiler doesn't support OpenMP 3.0+
   #pragma message ("WARNING: use of OpenMP disabled; this compiler doesn't support OpenMP 3.0+")
                                                                                                ^
c:/Rtools/mingw_64/bin/g++ -shared -s -static-libgcc -o saem3090757b4bd1x64.dll tmp.def saem3090757b4bd1x64.o c:/nlmixr/R/rx_855815def56a50f0e7a80e48811d947c_x64.dll -Lc:/R/R-34~1.1/bin/x64 -lRblas -Lc:/R/R-34~1.1/bin/x64 -lRlapack -lgfortran -lm -lquadmath -Ld:/Compiler/gcc-4.9.3/local330/lib/x64 -Ld:/Compiler/gcc-4.9.3/local330/lib -Lc:/R/R-34~1.1/bin/x64 -lR
done.
1:    1.8174   4.6328   0.0553   0.0950   0.0950   0.0950   0.6357
50:    1.3900   4.2039   0.0001   0.0679   0.0784   0.1082   0.1992
100:    1.3894   4.2054   0.0107   0.0686   0.0777   0.1111   0.1981
150:    1.3885   4.2041   0.0089   0.0683   0.0778   0.1117   0.1980
Using sympy via SnakeCharmR
## Calculate ETA-based prediction and error derivatives:
Calculate Jacobian...................done.
Calculate sensitivities.......
done.
## Calculate d(f)/d(eta)
## ...
## done
## ...
## done
The model-based sensitivities have been calculated
Calculating Table Variables...
done
</pre>
<p>The options for <code>saem</code> are controlled by <code><a href="nlmixr2est.html#topic+saemControl">saemControl</a></code>.
You may wish to make sure the minimization is complete in the case
of <code>saem</code>.  You can do that with <code>traceplot</code> which shows the
iteration history with the divided by burn-in and EM phases.  In
this case, the burn in seems reasonable; you may wish to increase
the number of iterations in the EM phase of the estimation.
Overall it is probably a semi-reasonable solution.
</p>
<p><b>nlmixr output objects</b>
</p>
<p>In addition to unifying the modeling language sent to each of the
estimation routines, the outputs currently have a unified structure.
</p>
<p>You can see the fit object by typing the object name:
</p>
<pre>
&gt; fit.s
 -- nlmixr SAEM fit (ODE); OBJF calculated from FOCEi approximation -------------
      OBJF      AIC      BIC Log-likelihood Condition Number
  62337.09 62351.09 62399.01      -31168.55          82.6086

 -- Time (sec; fit.s$time): -----------------------------------------------------
           saem setup Likelihood Calculation covariance table
 elapsed 430.25 31.64                   1.19          0  3.44

 -- Parameters (fit.s$par.fixed): -----------------------------------------------
              Parameter Estimate     SE  
 lCl      log Cl (L/hr)     1.39 0.0240  1.73       4.01 (3.83, 4.20)    26.6
 lVc         log Vc (L)     4.20 0.0256 0.608       67.0 (63.7, 70.4)    28.5
 lKA      log Ka (1/hr)  0.00924 0.0323  349.      1.01 (0.947, 1.08)    34.3
 prop.err      prop.err    0.198                             19.8
          Shrink(SD)
 lCl          0.248
 lVc           1.09
 lKA           4.19
 prop.err      1.81

   No correlations in between subject variability (BSV) matrix
   Full BSV covariance (fit.s$omega) or correlation (fit.s$omega.R; diagonals=SDs)
   Distribution stats (mean/skewness/kurtosis/p-value) available in fit.s$shrink

 -- Fit Data (object fit.s is a modified data.frame): ---------------------------
 # A tibble: 6,947 x 22
   ID     TIME    DV  PRED    RES    WRES IPRED  IRES  IWRES CPRED   CRES
 * &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;
 1 1      0.25  205.  198.   6.60  0.0741  189.  16.2  0.434  198.   6.78
 2 1      0.5   311.  349. -38.7  -0.261   330. -19.0 -0.291  349. -38.3
 3 1      0.75  389.  464. -74.5  -0.398   434. -45.2 -0.526  463. -73.9
 # ... with 6,944 more rows, and 11 more variables: CWRES &lt;dbl&gt;, eta.Cl &lt;dbl&gt;,
 #   eta.Vc &lt;dbl&gt;, eta.KA &lt;dbl&gt;, depot &lt;dbl&gt;, centr &lt;dbl&gt;, Cl &lt;dbl&gt;, Vc &lt;dbl&gt;,
 #   KA &lt;dbl&gt;, kel &lt;dbl&gt;, cp &lt;dbl&gt;
</pre>
<p>This example shows what is typical printout of a nlmixr fit object.  The elements of the fit are:
</p>

<ul>
<li><p> The type of fit (<code><a href="nlme.html#topic+nlme">nlme</a></code>, <code>saem</code>, etc)
</p>
</li>
<li><p> Metrics of goodness of fit (<code><a href="stats.html#topic+AIC">AIC</a></code>, <code><a href="stats.html#topic+BIC">BIC</a></code>,
and <code><a href="stats.html#topic+logLik">logLik</a></code>).
</p>

<ul>
<li><p> To align the comparison between methods, the FOCEi likelihood objective is calculated
regardless of the method used and used for goodness of fit metrics.
</p>
</li>
<li><p> This FOCEi likelihood has been compared to NONMEM's objective function and gives
the same values (based on the data in Wang 2007)
</p>
</li>
<li><p> Also note that <code>saem</code> does not calculate an objective function,
and the FOCEi is used as the only objective function for the fit.
</p>
</li>
<li><p> Even though the objective functions are calculated in the same manner, caution should
be used when comparing fits from various estimation routines.
</p>
</li></ul>

</li>
<li><p> The next item is the timing of each of the steps of the fit.
</p>

<ul>
<li><p> These can be also accessed by (<code>fit.s$time</code>).
</p>
</li>
<li><p> As a mnemonic, the access for this item is shown in the printout.
This is true for almost all of the other items in the printout.
</p>
</li></ul>

</li>
<li><p> After the timing of the fit, the parameter estimates are displayed (can be accessed by
<code>fit.s$par.fixed</code>)
</p>

<ul>
<li><p> While the items are rounded for R printing, each estimate without rounding is still accessible by the '$' syntax.
For example, the '$Untransformed' gives the untransformed parameter values.
</p>
</li>
<li><p> The Untransformed parameter takes log-space parameters and back-transforms them to normal parameters.  Not the CIs
are listed on the back-transformed parameter space.
</p>
</li>
<li><p> Proportional Errors are converted to 
</p>
</li></ul>

</li>
<li><p> Omega block (accessed by <code>fit.s$omega</code>)
</p>
</li>
<li><p> The table of fit data. Please note:
</p>

<ul>
<li><p> A nlmixr fit object is actually a data frame.  Saving it as a Rdata object and then loading it
without nlmixr will just show the data by itself.  Don't worry; the fit information has not vanished,
you can bring it back by simply loading nlmixr, and then accessing the data.
</p>
</li>
<li><p> Special access to fit information (like the <code>$omega</code>) needs nlmixr to extract the information.
</p>
</li>
<li><p> If you use the <code>$</code> to access information, the order of precedence is:
</p>

<ul>
<li><p> Fit data from the overall data.frame
</p>
</li>
<li><p> Information about the parsed nlmixr model (via <code>$uif</code>)
</p>
</li>
<li><p> Parameter history if available (via <code>$par.hist</code> and <code>$par.hist.stacked</code>)
</p>
</li>
<li><p> Fixed effects table (via <code>$par.fixed</code>)
</p>
</li>
<li><p> Individual differences from the typical population parameters (via <code>$eta</code>)
</p>
</li>
<li><p> Fit information from the list of information generated during the post-hoc
residual calculation.
</p>
</li>
<li><p> Fit information from the environment where the post-hoc residual were calculated
</p>
</li>
<li><p> Fit information about how the data and options interacted with the specified model
(such as estimation options or if the solved system is for an infusion or an IV bolus).
</p>
</li></ul>

</li>
<li><p> While the printout may displays the data as a <code>data.table</code> object or <code>tbl</code>
object, the data is NOT any of these objects, but rather a derived data frame.
</p>
</li>
<li><p> Since the object <em>is</em> a data.frame, you can treat it like one.
</p>
</li></ul>

</li></ul>

<p>In addition to the above properties of the fit object, there are a
few additional that may be helpful for the modeler:
</p>

<ul>
<li> <p><code>$theta</code> gives the fixed effects parameter estimates (in NONMEM the
<code>theta</code>s). This can also be accessed in <code><a href="nlme.html#topic+fixed.effects">fixed.effects</a></code>
function. Note that the residual variability is treated as a fixed effect parameter
and is included in this list.
</p>
</li>
<li> <p><code>$eta</code> gives the random effects parameter estimates, or in NONMEM the
<code>eta</code>s.  This can also be accessed in using the <code><a href="nlmixr2est.html#topic+random.effects">random.effects</a></code>
function.
</p>
</li></ul>



<h3>Author(s)</h3>

<p>Matthew L. Fidler
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

one.cmt &lt;- function() {
 ini({
   ## You may label each parameter with a comment
   tka &lt;- 0.45 # Ka
   tcl &lt;- log(c(0, 2.7, 100)) # Log Cl
   ## This works with interactive models
   ## You may also label the preceding line with label("label text")
   tv &lt;- 3.45; label("log V")
   ## the label("Label name") works with all models
   eta.ka ~ 0.6
   eta.cl ~ 0.3
   eta.v ~ 0.1
   add.sd &lt;- 0.7
   prop.sd &lt;- 0.01
 })
 model({
   ka &lt;- exp(tka + eta.ka)
   cl &lt;- exp(tcl + eta.cl)
   v &lt;- exp(tv + eta.v)
   linCmt() ~ add(add.sd) + prop(prop.sd)
 })
}

# fitF &lt;- nlmixr(one.cmt, theo_sd, "focei")

fitS &lt;- nlmixr(one.cmt, theo_sd, "saem")


</code></pre>

<hr>
<h2 id='nlmixr2CheckInstall'>Check your nlmixr2 installation for potential issues</h2><span id='topic+nlmixr2CheckInstall'></span>

<h3>Description</h3>

<p>Check your nlmixr2 installation for potential issues
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nlmixr2CheckInstall()
</code></pre>


<h3>Examples</h3>

<pre><code class='language-R'>nlmixr2CheckInstall()
</code></pre>

<hr>
<h2 id='nlsControl'>nlmixr2 defaults controls for nls</h2><span id='topic+nlsControl'></span>

<h3>Description</h3>

<p>nlmixr2 defaults controls for nls
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nlsControl(
  maxiter = 10000,
  tol = 1e-05,
  minFactor = 1/1024,
  printEval = FALSE,
  warnOnly = FALSE,
  scaleOffset = 0,
  nDcentral = FALSE,
  algorithm = c("LM", "default", "plinear", "port"),
  ftol = sqrt(.Machine$double.eps),
  ptol = sqrt(.Machine$double.eps),
  gtol = 0,
  diag = list(),
  epsfcn = 0,
  factor = 100,
  maxfev = integer(),
  nprint = 0,
  solveType = c("grad", "fun"),
  stickyRecalcN = 4,
  maxOdeRecalc = 5,
  odeRecalcFactor = 10^(0.5),
  eventType = c("central", "forward"),
  shiErr = (.Machine$double.eps)^(1/3),
  shi21maxFD = 20L,
  useColor = crayon::has_color(),
  printNcol = floor((getOption("width") - 23)/12),
  print = 1L,
  normType = c("rescale2", "mean", "rescale", "std", "len", "constant"),
  scaleType = c("nlmixr2", "norm", "mult", "multAdd"),
  scaleCmax = 1e+05,
  scaleCmin = 1e-05,
  scaleC = NULL,
  scaleTo = 1,
  gradTo = 1,
  trace = FALSE,
  rxControl = NULL,
  optExpression = TRUE,
  sumProd = FALSE,
  literalFix = TRUE,
  returnNls = FALSE,
  addProp = c("combined2", "combined1"),
  calcTables = TRUE,
  compress = TRUE,
  adjObf = TRUE,
  ci = 0.95,
  sigdig = 4,
  sigdigTable = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="nlsControl_+3A_maxiter">maxiter</code></td>
<td>
<p>A positive integer specifying the maximum number of
iterations allowed.</p>
</td></tr>
<tr><td><code id="nlsControl_+3A_tol">tol</code></td>
<td>
<p>A positive numeric value specifying the tolerance level for
the relative offset convergence criterion.</p>
</td></tr>
<tr><td><code id="nlsControl_+3A_minfactor">minFactor</code></td>
<td>
<p>A positive numeric value specifying the minimum
step-size factor allowed on any step in the iteration.  The
increment is calculated with a Gauss-Newton algorithm and
successively halved until the residual sum of squares has been
decreased or until the step-size factor has been reduced below this
limit.</p>
</td></tr>
<tr><td><code id="nlsControl_+3A_printeval">printEval</code></td>
<td>
<p>a logical specifying whether the number of evaluations
(steps in the gradient direction taken each iteration) is printed.</p>
</td></tr>
<tr><td><code id="nlsControl_+3A_warnonly">warnOnly</code></td>
<td>
<p>a logical specifying whether <code><a href="stats.html#topic+nls">nls</a>()</code> should
return instead of signalling an error in the case of termination
before convergence.
Termination before convergence happens upon completion of <code>maxiter</code>
iterations, in the case of a singular gradient, and in the case that the
step-size factor is reduced below <code>minFactor</code>.</p>
</td></tr>
<tr><td><code id="nlsControl_+3A_scaleoffset">scaleOffset</code></td>
<td>
<p>a constant to be added to the denominator of the relative
offset convergence criterion calculation to avoid a zero divide in the case
where the fit of a model to data is very close.  The default value of
<code>0</code> keeps the legacy behaviour of <code>nls()</code>.  A value such as
<code>1</code> seems to work for problems of reasonable scale with very small
residuals.</p>
</td></tr>
<tr><td><code id="nlsControl_+3A_ndcentral">nDcentral</code></td>
<td>
<p>only when <em>numerical</em> derivatives are used:
<code><a href="base.html#topic+logical">logical</a></code> indicating if <em>central</em> differences
should be employed, i.e., <code><a href="stats.html#topic+numericDeriv">numericDeriv</a>(*, central=TRUE)</code>
be used.</p>
</td></tr>
<tr><td><code id="nlsControl_+3A_algorithm">algorithm</code></td>
<td>
<p>character string specifying the algorithm to use.
The default algorithm is a Gauss-Newton algorithm.  Other possible
values are <code>"plinear"</code> for the Golub-Pereyra algorithm for
partially linear least-squares models and <code>"port"</code> for the
&lsquo;nl2sol&rsquo; algorithm from the Port library &ndash; see the
references.  Can be abbreviated.</p>
</td></tr>
<tr><td><code id="nlsControl_+3A_ftol">ftol</code></td>
<td>
<p>non-negative numeric. Termination occurs when
both the actual and predicted relative reductions in the sum of
squares are at most <code>ftol</code>. Therefore, <code>ftol</code> measures
the relative error desired in the sum of squares.</p>
</td></tr>
<tr><td><code id="nlsControl_+3A_ptol">ptol</code></td>
<td>
<p>non-negative numeric. Termination occurs when
the relative error between two consecutive iterates is at most
<code>ptol</code>. Therefore, <code>ptol</code> measures the relative error
desired in the approximate solution.</p>
</td></tr>
<tr><td><code id="nlsControl_+3A_gtol">gtol</code></td>
<td>
<p>non-negative numeric. Termination occurs when
the cosine of the angle between result of <code>fn</code> evaluation
<code class="reqn">fvec</code> and any column of the Jacobian is at most <code>gtol</code>
in absolute value. Therefore, <code>gtol</code> measures the
orthogonality desired between the function vector and the
columns of the Jacobian.</p>
</td></tr>
<tr><td><code id="nlsControl_+3A_diag">diag</code></td>
<td>
<p>a list or numeric vector containing positive
entries that serve as multiplicative scale factors for the
parameters. Length of <code>diag</code> should be equal to that of
<code>par</code>. If not, user-provided <code>diag</code> is ignored and
<code>diag</code> is internally set.</p>
</td></tr>
<tr><td><code id="nlsControl_+3A_epsfcn">epsfcn</code></td>
<td>
<p>(used if <code>jac</code> is not provided) is a
numeric used in determining a suitable step for the
forward-difference approximation. This approximation assumes
that the relative errors in the functions are of the order of
<code>epsfcn</code>. If <code>epsfcn</code> is less than the machine
precision, it is assumed that the relative errors in the
functions are of the order of the machine precision.</p>
</td></tr>
<tr><td><code id="nlsControl_+3A_factor">factor</code></td>
<td>
<p>positive numeric, used in determining the
initial step bound.  This bound is set to the product of
<code>factor</code> and the <code class="reqn">|\code{diag}*\code{par}|</code> if nonzero,
or else to <code>factor</code> itself. In most cases <code>factor</code>
should lie in the interval (0.1,100). 100 is a generally
recommended value.</p>
</td></tr>
<tr><td><code id="nlsControl_+3A_maxfev">maxfev</code></td>
<td>
<p>integer; termination occurs
when the number of calls to <code>fn</code> has reached <code>maxfev</code>.
Note that <code>nls.lm</code> sets the value of <code>maxfev</code> to 
<code>100*(length(par) + 1)</code> if 
<code>maxfev = integer()</code>, where <code>par</code> is the list or
vector of parameters to be optimized.  </p>
</td></tr>
<tr><td><code id="nlsControl_+3A_nprint">nprint</code></td>
<td>
<p>is an integer; set <code>nprint</code> to be positive
to enable printing of iterates</p>
</td></tr>
<tr><td><code id="nlsControl_+3A_solvetype">solveType</code></td>
<td>
<p>tells if &lsquo;nlm' will use nlmixr2&rsquo;s analytical
gradients when available (finite differences will be used for
event-related parameters like parameters controlling lag time,
duration/rate of infusion, and modeled bioavailability). This can
be:
</p>
<p>- '&quot;hessian&quot;' which will use the analytical gradients to create a
Hessian with finite differences.
</p>
<p>- '&quot;gradient&quot;' which will use the gradient and let 'nlm' calculate
the finite difference hessian
</p>
<p>- '&quot;fun&quot;' where nlm will calculate both the finite difference
gradient and the finite difference Hessian
</p>
<p>When using nlmixr2's finite differences, the &quot;ideal&quot; step size for
either central or forward differences are optimized for with the
Shi2021 method which may give more accurate derivatives</p>
</td></tr>
<tr><td><code id="nlsControl_+3A_stickyrecalcn">stickyRecalcN</code></td>
<td>
<p>The number of bad ODE solves before reducing
the atol/rtol for the rest of the problem.</p>
</td></tr>
<tr><td><code id="nlsControl_+3A_maxoderecalc">maxOdeRecalc</code></td>
<td>
<p>Maximum number of times to reduce the ODE
tolerances and try to resolve the system if there was a bad
ODE solve.</p>
</td></tr>
<tr><td><code id="nlsControl_+3A_oderecalcfactor">odeRecalcFactor</code></td>
<td>
<p>The ODE recalculation factor when ODE
solving goes bad, this is the factor the rtol/atol is reduced</p>
</td></tr>
<tr><td><code id="nlsControl_+3A_eventtype">eventType</code></td>
<td>
<p>Event gradient type for dosing events; Can be
&quot;central&quot; or &quot;forward&quot;</p>
</td></tr>
<tr><td><code id="nlsControl_+3A_shierr">shiErr</code></td>
<td>
<p>This represents the epsilon when optimizing the ideal
step size for numeric differentiation using the Shi2021 method</p>
</td></tr>
<tr><td><code id="nlsControl_+3A_shi21maxfd">shi21maxFD</code></td>
<td>
<p>The maximum number of steps for the optimization
of the forward difference step size when using dosing events (lag
time, modeled duration/rate and bioavailability)</p>
</td></tr>
<tr><td><code id="nlsControl_+3A_usecolor">useColor</code></td>
<td>
<p>Boolean indicating if focei can use ASCII color codes</p>
</td></tr>
<tr><td><code id="nlsControl_+3A_printncol">printNcol</code></td>
<td>
<p>Number of columns to printout before wrapping
parameter estimates/gradient</p>
</td></tr>
<tr><td><code id="nlsControl_+3A_print">print</code></td>
<td>
<p>Integer representing when the outer step is
printed. When this is 0 or do not print the iterations.  1 is
print every function evaluation (default), 5 is print every 5
evaluations.</p>
</td></tr>
<tr><td><code id="nlsControl_+3A_normtype">normType</code></td>
<td>
<p>This is the type of parameter
normalization/scaling used to get the scaled initial values
for nlmixr2.  These are used with <code>scaleType</code> of.
</p>
<p>With the exception of <code>rescale2</code>, these come
from
<a href="https://en.wikipedia.org/wiki/Feature_scaling">Feature
Scaling</a>. The <code>rescale2</code> The rescaling is the same type
described in the
<a href="http://apmonitor.com/me575/uploads/Main/optimization_book.pdf">OptdesX</a>
software manual.
</p>
<p>In general, all all scaling formula can be described by:
</p>
<p style="text-align: center;"><code class="reqn">v_{scaled}</code>
</p>
<p> = (</p>
<p style="text-align: center;"><code class="reqn">v_{unscaled}-C_{1}</code>
</p>
<p>)/</p>
<p style="text-align: center;"><code class="reqn">C_{2}</code>
</p>

<p>Where
</p>
<p>The other data normalization approaches follow the following formula
</p>
<p style="text-align: center;"><code class="reqn">v_{scaled}</code>
</p>
<p> = (</p>
<p style="text-align: center;"><code class="reqn">v_{unscaled}-C_{1}</code>
</p>
<p>)/</p>
<p style="text-align: center;"><code class="reqn">C_{2}</code>
</p>


<ul>
<li> <p><code>rescale2</code> This scales all parameters from (-1 to 1).
The relative differences between the parameters are preserved
with this approach and the constants are:
</p>
<p style="text-align: center;"><code class="reqn">C_{1}</code>
</p>
<p> = (max(all unscaled values)+min(all unscaled values))/2
</p>
<p style="text-align: center;"><code class="reqn">C_{2}</code>
</p>
<p> = (max(all unscaled values) - min(all unscaled values))/2
</p>
</li>
<li> <p><code>rescale</code> or min-max normalization. This rescales all
parameters from (0 to 1).  As in the <code>rescale2</code> the
relative differences are preserved.  In this approach:
</p>
<p style="text-align: center;"><code class="reqn">C_{1}</code>
</p>
<p> = min(all unscaled values)
</p>
<p style="text-align: center;"><code class="reqn">C_{2}</code>
</p>
<p> = max(all unscaled values) - min(all unscaled values)
</p>
</li>
<li> <p><code>mean</code> or mean normalization.  This rescales to center
the parameters around the mean but the parameters are from 0
to 1.  In this approach:
</p>
<p style="text-align: center;"><code class="reqn">C_{1}</code>
</p>
<p> = mean(all unscaled values)
</p>
<p style="text-align: center;"><code class="reqn">C_{2}</code>
</p>
<p> = max(all unscaled values) - min(all unscaled values)
</p>
</li>
<li> <p><code>std</code> or standardization.  This standardizes by the mean
and standard deviation.  In this approach:
</p>
<p style="text-align: center;"><code class="reqn">C_{1}</code>
</p>
<p> = mean(all unscaled values)
</p>
<p style="text-align: center;"><code class="reqn">C_{2}</code>
</p>
<p> = sd(all unscaled values)
</p>
</li>
<li> <p><code>len</code> or unit length scaling.  This scales the
parameters to the unit length.  For this approach we use the Euclidean length, that
is:
</p>
<p style="text-align: center;"><code class="reqn">C_{1}</code>
</p>
<p> = 0
</p>
<p style="text-align: center;"><code class="reqn">C_{2}</code>
</p>
<p> = </p>
<p style="text-align: center;"><code class="reqn">\sqrt(v_1^2 + v_2^2 + \cdots + v_n^2)</code>
</p>

</li>
<li> <p><code>constant</code> which does not perform data normalization. That is
</p>
<p style="text-align: center;"><code class="reqn">C_{1}</code>
</p>
<p> = 0
</p>
<p style="text-align: center;"><code class="reqn">C_{2}</code>
</p>
<p> = 1
</p>
</li></ul>
</td></tr>
<tr><td><code id="nlsControl_+3A_scaletype">scaleType</code></td>
<td>
<p>The scaling scheme for nlmixr2.  The supported types are:
</p>

<ul>
<li> <p><code>nlmixr2</code>  In this approach the scaling is performed by the following equation:
</p>
<p style="text-align: center;"><code class="reqn">v_{scaled}</code>
</p>
<p> = (</p>
<p style="text-align: center;"><code class="reqn">v_{current} - v_{init}</code>
</p>
<p>)*scaleC[i] + scaleTo
</p>
<p>The <code>scaleTo</code> parameter is specified by the <code>normType</code>,
and the scales are specified by <code>scaleC</code>.
</p>
</li>
<li> <p><code>norm</code> This approach uses the simple scaling provided
by the <code>normType</code> argument.
</p>
</li>
<li> <p><code>mult</code> This approach does not use the data
normalization provided by <code>normType</code>, but rather uses
multiplicative scaling to a constant provided by the <code>scaleTo</code>
argument.
</p>
<p>In this case:
</p>
<p style="text-align: center;"><code class="reqn">v_{scaled}</code>
</p>
<p> = </p>
<p style="text-align: center;"><code class="reqn">v_{current}</code>
</p>
<p>/</p>
<p style="text-align: center;"><code class="reqn">v_{init}</code>
</p>
<p>*scaleTo
</p>
</li>
<li> <p><code>multAdd</code> This approach changes the scaling based on
the parameter being specified.  If a parameter is defined in an
exponential block (ie exp(theta)), then it is scaled on a
linearly, that is:
</p>
<p style="text-align: center;"><code class="reqn">v_{scaled}</code>
</p>
<p> = (</p>
<p style="text-align: center;"><code class="reqn">v_{current}-v_{init}</code>
</p>
<p>) + scaleTo
</p>
<p>Otherwise the parameter is scaled multiplicatively.
</p>
<p style="text-align: center;"><code class="reqn">v_{scaled}</code>
</p>
<p> = </p>
<p style="text-align: center;"><code class="reqn">v_{current}</code>
</p>
<p>/</p>
<p style="text-align: center;"><code class="reqn">v_{init}</code>
</p>
<p>*scaleTo
</p>
</li></ul>
</td></tr>
<tr><td><code id="nlsControl_+3A_scalecmax">scaleCmax</code></td>
<td>
<p>Maximum value of the scaleC to prevent overflow.</p>
</td></tr>
<tr><td><code id="nlsControl_+3A_scalecmin">scaleCmin</code></td>
<td>
<p>Minimum value of the scaleC to prevent underflow.</p>
</td></tr>
<tr><td><code id="nlsControl_+3A_scalec">scaleC</code></td>
<td>
<p>The scaling constant used with
<code>scaleType=nlmixr2</code>.  When not specified, it is based on
the type of parameter that is estimated.  The idea is to keep
the derivatives similar on a log scale to have similar
gradient sizes.  Hence parameters like log(exp(theta)) would
have a scaling factor of 1 and log(theta) would have a scaling
factor of ini_value (to scale by 1/value; ie
d/dt(log(ini_value)) = 1/ini_value or scaleC=ini_value)
</p>

<ul>
<li><p> For parameters in an exponential (ie exp(theta)) or
parameters specifying powers, boxCox or yeoJohnson
transformations , this is 1.
</p>
</li>
<li><p> For additive, proportional, lognormal error structures,
these are given by 0.5*abs(initial_estimate)
</p>
</li>
<li><p> Factorials are scaled by abs(1/digamma(initial_estimate+1))
</p>
</li>
<li><p> parameters in a log scale (ie log(theta)) are transformed
by log(abs(initial_estimate))*abs(initial_estimate)
</p>
</li></ul>

<p>These parameter scaling coefficients are chose to try to keep
similar slopes among parameters.  That is they all follow the
slopes approximately on a log-scale.
</p>
<p>While these are chosen in a logical manner, they may not always
apply.  You can specify each parameters scaling factor by this
parameter if you wish.</p>
</td></tr>
<tr><td><code id="nlsControl_+3A_scaleto">scaleTo</code></td>
<td>
<p>Scale the initial parameter estimate to this value.
By default this is 1.  When zero or below, no scaling is performed.</p>
</td></tr>
<tr><td><code id="nlsControl_+3A_gradto">gradTo</code></td>
<td>
<p>this is the factor that the gradient is scaled to
before optimizing.  This only works with
scaleType=&quot;nlmixr2&quot;.</p>
</td></tr>
<tr><td><code id="nlsControl_+3A_trace">trace</code></td>
<td>
<p>logical value indicating if a trace of the iteration
progress should be printed.  Default is <code>FALSE</code>.  If
<code>TRUE</code> the residual (weighted) sum-of-squares, the convergence
criterion and the parameter values are printed at the conclusion of
each iteration.  Note that <code><a href="base.html#topic+format">format</a>()</code> is used, so these
mostly depend on <code><a href="base.html#topic+getOption">getOption</a>("digits")</code>.
When the <code>"plinear"</code> algorithm is used, the conditional
estimates of the linear parameters are printed after the nonlinear
parameters.  When the <code>"port"</code> algorithm is used the
objective function value printed is half the residual (weighted)
sum-of-squares.</p>
</td></tr>
<tr><td><code id="nlsControl_+3A_rxcontrol">rxControl</code></td>
<td>
<p>'rxode2' ODE solving options during fitting, created with 'rxControl()'</p>
</td></tr>
<tr><td><code id="nlsControl_+3A_optexpression">optExpression</code></td>
<td>
<p>Optimize the rxode2 expression to speed up
calculation. By default this is turned on.</p>
</td></tr>
<tr><td><code id="nlsControl_+3A_sumprod">sumProd</code></td>
<td>
<p>Is a boolean indicating if the model should change
multiplication to high precision multiplication and sums to
high precision sums using the PreciseSums package.  By default
this is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="nlsControl_+3A_literalfix">literalFix</code></td>
<td>
<p>boolean, substitute fixed population values as
literals and re-adjust ui and parameter estimates after
optimization; Default is 'TRUE'.</p>
</td></tr>
<tr><td><code id="nlsControl_+3A_returnnls">returnNls</code></td>
<td>
<p>logical; when TRUE, will return the nls object
instead of the nlmixr object</p>
</td></tr>
<tr><td><code id="nlsControl_+3A_addprop">addProp</code></td>
<td>
<p>specifies the type of additive plus proportional
errors, the one where standard deviations add (combined1) or the
type where the variances add (combined2).
</p>
<p>The combined1 error type can be described by the following equation:
</p>
<p style="text-align: center;"><code class="reqn">y = f + (a + b\times f^c) \times \varepsilon</code>
</p>

<p>The combined2 error model can be described by the following equation:
</p>
<p style="text-align: center;"><code class="reqn">y = f + \sqrt{a^2 + b^2\times f^{2\times c}} \times \varepsilon</code>
</p>

<p>Where:
</p>
<p>- y represents the observed value
</p>
<p>- f represents the predicted value
</p>
<p>- a  is the additive standard deviation
</p>
<p>- b is the proportional/power standard deviation
</p>
<p>- c is the power exponent (in the proportional case c=1)</p>
</td></tr>
<tr><td><code id="nlsControl_+3A_calctables">calcTables</code></td>
<td>
<p>This boolean is to determine if the foceiFit
will calculate tables. By default this is <code>TRUE</code></p>
</td></tr>
<tr><td><code id="nlsControl_+3A_compress">compress</code></td>
<td>
<p>Should the object have compressed items</p>
</td></tr>
<tr><td><code id="nlsControl_+3A_adjobf">adjObf</code></td>
<td>
<p>is a boolean to indicate if the objective function
should be adjusted to be closer to NONMEM's default objective
function.  By default this is <code>TRUE</code></p>
</td></tr>
<tr><td><code id="nlsControl_+3A_ci">ci</code></td>
<td>
<p>Confidence level for some tables.  By default this is
0.95 or 95% confidence.</p>
</td></tr>
<tr><td><code id="nlsControl_+3A_sigdig">sigdig</code></td>
<td>
<p>Optimization significant digits. This controls:
</p>

<ul>
<li><p> The tolerance of the inner and outer optimization is <code>10^-sigdig</code>
</p>
</li>
<li><p> The tolerance of the ODE solvers is
<code>0.5*10^(-sigdig-2)</code>; For the sensitivity equations and
steady-state solutions the default is <code>0.5*10^(-sigdig-1.5)</code>
(sensitivity changes only applicable for liblsoda)
</p>
</li>
<li><p> The tolerance of the boundary check is <code>5 * 10 ^ (-sigdig + 1)</code>
</p>
</li></ul>
</td></tr>
<tr><td><code id="nlsControl_+3A_sigdigtable">sigdigTable</code></td>
<td>
<p>Significant digits in the final output table.
If not specified, then it matches the significant digits in the
'sigdig' optimization algorithm.  If 'sigdig' is NULL, use 3.</p>
</td></tr>
<tr><td><code id="nlsControl_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <code><a href="nlmixr2est.html#topic+nlsControl">nlmixr2est::nlsControl()</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>nls control object
</p>


<h3>Author(s)</h3>

<p>Matthew L. Fidler
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

if (rxode2parse::.linCmtSens()) {

one.cmt &lt;- function() {
  ini({
   tka &lt;- 0.45
   tcl &lt;- log(c(0, 2.7, 100))
   tv &lt;- 3.45
   add.sd &lt;- 0.7
 })
 model({
   ka &lt;- exp(tka)
   cl &lt;- exp(tcl)
   v &lt;- exp(tv)
   linCmt() ~ add(add.sd)
 })
}

# Uses nlsLM from minpack.lm if available

fit1 &lt;- nlmixr(one.cmt, nlmixr2data::theo_sd, est="nls", nlsControl(algorithm="LM"))

# Uses port and respect parameter boundaries
fit2 &lt;- nlmixr(one.cmt, nlmixr2data::theo_sd, est="nls", nlsControl(algorithm="port"))

# You can access the underlying nls object with `$nls`
fit2$nls
}

</code></pre>

<hr>
<h2 id='optimControl'>nlmixr2 optim defaults</h2><span id='topic+optimControl'></span>

<h3>Description</h3>

<p>nlmixr2 optim defaults
</p>


<h3>Usage</h3>

<pre><code class='language-R'>optimControl(
  method = c("Nelder-Mead", "BFGS", "CG", "L-BFGS-B", "SANN", "Brent"),
  trace = 0,
  fnscale = 1,
  parscale = 1,
  ndeps = 0.001,
  maxit = 10000,
  abstol = 1e-08,
  reltol = 1e-08,
  alpha = 1,
  beta = 0.5,
  gamma = 2,
  REPORT = NULL,
  warn.1d.NelderMead = TRUE,
  type = NULL,
  lmm = 5,
  factr = 1e+07,
  pgtol = 0,
  temp = 10,
  tmax = 10,
  stickyRecalcN = 4,
  maxOdeRecalc = 5,
  odeRecalcFactor = 10^(0.5),
  eventType = c("central", "forward"),
  shiErr = (.Machine$double.eps)^(1/3),
  shi21maxFD = 20L,
  solveType = c("grad", "fun"),
  useColor = crayon::has_color(),
  printNcol = floor((getOption("width") - 23)/12),
  print = 1L,
  normType = c("rescale2", "mean", "rescale", "std", "len", "constant"),
  scaleType = c("nlmixr2", "norm", "mult", "multAdd"),
  scaleCmax = 1e+05,
  scaleCmin = 1e-05,
  scaleC = NULL,
  scaleTo = 1,
  gradTo = 1,
  rxControl = NULL,
  optExpression = TRUE,
  sumProd = FALSE,
  literalFix = TRUE,
  returnOptim = FALSE,
  addProp = c("combined2", "combined1"),
  calcTables = TRUE,
  compress = TRUE,
  covMethod = c("r", "optim", ""),
  adjObf = TRUE,
  ci = 0.95,
  sigdig = 4,
  sigdigTable = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="optimControl_+3A_method">method</code></td>
<td>
<p>The method to be used. See &lsquo;Details&rsquo;.  Can be abbreviated.</p>
</td></tr>
<tr><td><code id="optimControl_+3A_trace">trace</code></td>
<td>
<p>Non-negative integer. If positive, tracing information
on the progress of the optimization is produced. Higher values
may produce more tracing information: for method '&quot;L-BFGS-B&quot;',
there are six levels of tracing. See 'optim()' for more
information</p>
</td></tr>
<tr><td><code id="optimControl_+3A_fnscale">fnscale</code></td>
<td>
<p>An overall scaling to be applied to the value of 'fn'
and 'gr' during optimization. If negative, turns the problem
into a maximization problem. Optimization is performed on
'fn(par)/fnscale'</p>
</td></tr>
<tr><td><code id="optimControl_+3A_parscale">parscale</code></td>
<td>
<p>A vector of scaling values for the parameters.
Optimization is performed on 'par/parscale' and these should be
comparable in the sense that a unit change in any element
produces about a unit change in the scaled value.  Not used (nor
needed) for 'method = &quot;Brent&quot;'</p>
</td></tr>
<tr><td><code id="optimControl_+3A_ndeps">ndeps</code></td>
<td>
<p>A vector of step sizes for the finite-difference
approximation to the gradient, on 'par/parscale' scale.  Defaults
to '1e-3'</p>
</td></tr>
<tr><td><code id="optimControl_+3A_maxit">maxit</code></td>
<td>
<p>The maximum number of iterations. Defaults to '100'
for the derivative-based methods, and '500' for '&quot;Nelder-Mead&quot;'.</p>
</td></tr>
<tr><td><code id="optimControl_+3A_abstol">abstol</code></td>
<td>
<p>The absolute convergence tolerance. Only useful for
non-negative functions, as a tolerance for reaching zero.</p>
</td></tr>
<tr><td><code id="optimControl_+3A_reltol">reltol</code></td>
<td>
<p>Relative convergence tolerance.  The algorithm stops
if it is unable to reduce the value by a factor of 'reltol *
(abs(val) + reltol)' at a step</p>
</td></tr>
<tr><td><code id="optimControl_+3A_alpha">alpha</code></td>
<td>
<p>Reflection factor for the '&quot;Nelder-Mead&quot;' method.</p>
</td></tr>
<tr><td><code id="optimControl_+3A_beta">beta</code></td>
<td>
<p>Contraction factor for the '&quot;Nelder-Mead&quot;' method</p>
</td></tr>
<tr><td><code id="optimControl_+3A_gamma">gamma</code></td>
<td>
<p>Expansion  factor for the '&quot;Nelder-Mead&quot;' method</p>
</td></tr>
<tr><td><code id="optimControl_+3A_report">REPORT</code></td>
<td>
<p>The frequency of reports for the '&quot;BFGS&quot;',
'&quot;L-BFGS-B&quot;' and '&quot;SANN&quot;' methods if 'control$trace' is
positive. Defaults to every 10 iterations for '&quot;BFGS&quot;' and
'&quot;L-BFGS-B&quot;', or every 100 temperatures for '&quot;SANN&quot;'</p>
</td></tr>
<tr><td><code id="optimControl_+3A_warn.1d.neldermead">warn.1d.NelderMead</code></td>
<td>
<p>a logical indicating if the (default)
'&quot;Nelder-Mead&quot;' method should signal a warning when used for
one-dimensional minimization.  As the warning is sometimes
inappropriate, you can suppress it by setting this option to
'FALSE'</p>
</td></tr>
<tr><td><code id="optimControl_+3A_type">type</code></td>
<td>
<p>for the conjugate-gradients method.  Takes value '1'
for the Fletcher-Reeves update, '2' for Polak-Ribiere and '3' for
Beale-Sorenson.</p>
</td></tr>
<tr><td><code id="optimControl_+3A_lmm">lmm</code></td>
<td>
<p>is an integer giving the number of BFGS updates retained
in the '&quot;L-BFGS-B&quot;' method, It defaults to '5'</p>
</td></tr>
<tr><td><code id="optimControl_+3A_factr">factr</code></td>
<td>
<p>controls the convergence of the '&quot;L-BFGS-B&quot;' method.
Convergence occurs when the reduction in the objective is within
this factor of the machine tolerance. Default is '1e7', that is a
tolerance of about '1e-8'.</p>
</td></tr>
<tr><td><code id="optimControl_+3A_pgtol">pgtol</code></td>
<td>
<p>helps control the convergence of the &quot;L-BFGS-B&quot;
method.  It is a tolerance on the projected gradient in the
current search direction. This defaults to zero, when the check
is suppressed</p>
</td></tr>
<tr><td><code id="optimControl_+3A_temp">temp</code></td>
<td>
<p>controls the '&quot;SANN&quot;' method. It is the starting
temperature for the cooling schedule. Defaults to '10'.</p>
</td></tr>
<tr><td><code id="optimControl_+3A_tmax">tmax</code></td>
<td>
<p>is the number of function evaluations at each
temperature for the '&quot;SANN&quot;' method. Defaults to '10'.</p>
</td></tr>
<tr><td><code id="optimControl_+3A_stickyrecalcn">stickyRecalcN</code></td>
<td>
<p>The number of bad ODE solves before reducing
the atol/rtol for the rest of the problem.</p>
</td></tr>
<tr><td><code id="optimControl_+3A_maxoderecalc">maxOdeRecalc</code></td>
<td>
<p>Maximum number of times to reduce the ODE
tolerances and try to resolve the system if there was a bad
ODE solve.</p>
</td></tr>
<tr><td><code id="optimControl_+3A_oderecalcfactor">odeRecalcFactor</code></td>
<td>
<p>The ODE recalculation factor when ODE
solving goes bad, this is the factor the rtol/atol is reduced</p>
</td></tr>
<tr><td><code id="optimControl_+3A_eventtype">eventType</code></td>
<td>
<p>Event gradient type for dosing events; Can be
&quot;central&quot; or &quot;forward&quot;</p>
</td></tr>
<tr><td><code id="optimControl_+3A_shierr">shiErr</code></td>
<td>
<p>This represents the epsilon when optimizing the ideal
step size for numeric differentiation using the Shi2021 method</p>
</td></tr>
<tr><td><code id="optimControl_+3A_shi21maxfd">shi21maxFD</code></td>
<td>
<p>The maximum number of steps for the optimization
of the forward difference step size when using dosing events (lag
time, modeled duration/rate and bioavailability)</p>
</td></tr>
<tr><td><code id="optimControl_+3A_solvetype">solveType</code></td>
<td>
<p>tells if &lsquo;optim' will use nlmixr2&rsquo;s analytical
gradients when available (finite differences will be used for
event-related parameters like parameters controlling lag time,
duration/rate of infusion, and modeled bioavailability). This can
be:
</p>
<p>- '&quot;gradient&quot;' which will use the gradient and let 'optim' calculate
the finite difference hessian
</p>
<p>- '&quot;fun&quot;' where optim will calculate both the finite difference
gradient and the finite difference Hessian
</p>
<p>When using nlmixr2's finite differences, the &quot;ideal&quot; step size for
either central or forward differences are optimized for with the
Shi2021 method which may give more accurate derivatives
</p>
<p>These are only applied in the gradient based methods: &quot;BFGS&quot;, &quot;CG&quot;,
&quot;L-BFGS-B&quot;</p>
</td></tr>
<tr><td><code id="optimControl_+3A_usecolor">useColor</code></td>
<td>
<p>Boolean indicating if focei can use ASCII color codes</p>
</td></tr>
<tr><td><code id="optimControl_+3A_printncol">printNcol</code></td>
<td>
<p>Number of columns to printout before wrapping
parameter estimates/gradient</p>
</td></tr>
<tr><td><code id="optimControl_+3A_print">print</code></td>
<td>
<p>Integer representing when the outer step is
printed. When this is 0 or do not print the iterations.  1 is
print every function evaluation (default), 5 is print every 5
evaluations.</p>
</td></tr>
<tr><td><code id="optimControl_+3A_normtype">normType</code></td>
<td>
<p>This is the type of parameter
normalization/scaling used to get the scaled initial values
for nlmixr2.  These are used with <code>scaleType</code> of.
</p>
<p>With the exception of <code>rescale2</code>, these come
from
<a href="https://en.wikipedia.org/wiki/Feature_scaling">Feature
Scaling</a>. The <code>rescale2</code> The rescaling is the same type
described in the
<a href="http://apmonitor.com/me575/uploads/Main/optimization_book.pdf">OptdesX</a>
software manual.
</p>
<p>In general, all all scaling formula can be described by:
</p>
<p style="text-align: center;"><code class="reqn">v_{scaled}</code>
</p>
<p> = (</p>
<p style="text-align: center;"><code class="reqn">v_{unscaled}-C_{1}</code>
</p>
<p>)/</p>
<p style="text-align: center;"><code class="reqn">C_{2}</code>
</p>

<p>Where
</p>
<p>The other data normalization approaches follow the following formula
</p>
<p style="text-align: center;"><code class="reqn">v_{scaled}</code>
</p>
<p> = (</p>
<p style="text-align: center;"><code class="reqn">v_{unscaled}-C_{1}</code>
</p>
<p>)/</p>
<p style="text-align: center;"><code class="reqn">C_{2}</code>
</p>


<ul>
<li> <p><code>rescale2</code> This scales all parameters from (-1 to 1).
The relative differences between the parameters are preserved
with this approach and the constants are:
</p>
<p style="text-align: center;"><code class="reqn">C_{1}</code>
</p>
<p> = (max(all unscaled values)+min(all unscaled values))/2
</p>
<p style="text-align: center;"><code class="reqn">C_{2}</code>
</p>
<p> = (max(all unscaled values) - min(all unscaled values))/2
</p>
</li>
<li> <p><code>rescale</code> or min-max normalization. This rescales all
parameters from (0 to 1).  As in the <code>rescale2</code> the
relative differences are preserved.  In this approach:
</p>
<p style="text-align: center;"><code class="reqn">C_{1}</code>
</p>
<p> = min(all unscaled values)
</p>
<p style="text-align: center;"><code class="reqn">C_{2}</code>
</p>
<p> = max(all unscaled values) - min(all unscaled values)
</p>
</li>
<li> <p><code>mean</code> or mean normalization.  This rescales to center
the parameters around the mean but the parameters are from 0
to 1.  In this approach:
</p>
<p style="text-align: center;"><code class="reqn">C_{1}</code>
</p>
<p> = mean(all unscaled values)
</p>
<p style="text-align: center;"><code class="reqn">C_{2}</code>
</p>
<p> = max(all unscaled values) - min(all unscaled values)
</p>
</li>
<li> <p><code>std</code> or standardization.  This standardizes by the mean
and standard deviation.  In this approach:
</p>
<p style="text-align: center;"><code class="reqn">C_{1}</code>
</p>
<p> = mean(all unscaled values)
</p>
<p style="text-align: center;"><code class="reqn">C_{2}</code>
</p>
<p> = sd(all unscaled values)
</p>
</li>
<li> <p><code>len</code> or unit length scaling.  This scales the
parameters to the unit length.  For this approach we use the Euclidean length, that
is:
</p>
<p style="text-align: center;"><code class="reqn">C_{1}</code>
</p>
<p> = 0
</p>
<p style="text-align: center;"><code class="reqn">C_{2}</code>
</p>
<p> = </p>
<p style="text-align: center;"><code class="reqn">\sqrt(v_1^2 + v_2^2 + \cdots + v_n^2)</code>
</p>

</li>
<li> <p><code>constant</code> which does not perform data normalization. That is
</p>
<p style="text-align: center;"><code class="reqn">C_{1}</code>
</p>
<p> = 0
</p>
<p style="text-align: center;"><code class="reqn">C_{2}</code>
</p>
<p> = 1
</p>
</li></ul>
</td></tr>
<tr><td><code id="optimControl_+3A_scaletype">scaleType</code></td>
<td>
<p>The scaling scheme for nlmixr2.  The supported types are:
</p>

<ul>
<li> <p><code>nlmixr2</code>  In this approach the scaling is performed by the following equation:
</p>
<p style="text-align: center;"><code class="reqn">v_{scaled}</code>
</p>
<p> = (</p>
<p style="text-align: center;"><code class="reqn">v_{current} - v_{init}</code>
</p>
<p>)*scaleC[i] + scaleTo
</p>
<p>The <code>scaleTo</code> parameter is specified by the <code>normType</code>,
and the scales are specified by <code>scaleC</code>.
</p>
</li>
<li> <p><code>norm</code> This approach uses the simple scaling provided
by the <code>normType</code> argument.
</p>
</li>
<li> <p><code>mult</code> This approach does not use the data
normalization provided by <code>normType</code>, but rather uses
multiplicative scaling to a constant provided by the <code>scaleTo</code>
argument.
</p>
<p>In this case:
</p>
<p style="text-align: center;"><code class="reqn">v_{scaled}</code>
</p>
<p> = </p>
<p style="text-align: center;"><code class="reqn">v_{current}</code>
</p>
<p>/</p>
<p style="text-align: center;"><code class="reqn">v_{init}</code>
</p>
<p>*scaleTo
</p>
</li>
<li> <p><code>multAdd</code> This approach changes the scaling based on
the parameter being specified.  If a parameter is defined in an
exponential block (ie exp(theta)), then it is scaled on a
linearly, that is:
</p>
<p style="text-align: center;"><code class="reqn">v_{scaled}</code>
</p>
<p> = (</p>
<p style="text-align: center;"><code class="reqn">v_{current}-v_{init}</code>
</p>
<p>) + scaleTo
</p>
<p>Otherwise the parameter is scaled multiplicatively.
</p>
<p style="text-align: center;"><code class="reqn">v_{scaled}</code>
</p>
<p> = </p>
<p style="text-align: center;"><code class="reqn">v_{current}</code>
</p>
<p>/</p>
<p style="text-align: center;"><code class="reqn">v_{init}</code>
</p>
<p>*scaleTo
</p>
</li></ul>
</td></tr>
<tr><td><code id="optimControl_+3A_scalecmax">scaleCmax</code></td>
<td>
<p>Maximum value of the scaleC to prevent overflow.</p>
</td></tr>
<tr><td><code id="optimControl_+3A_scalecmin">scaleCmin</code></td>
<td>
<p>Minimum value of the scaleC to prevent underflow.</p>
</td></tr>
<tr><td><code id="optimControl_+3A_scalec">scaleC</code></td>
<td>
<p>The scaling constant used with
<code>scaleType=nlmixr2</code>.  When not specified, it is based on
the type of parameter that is estimated.  The idea is to keep
the derivatives similar on a log scale to have similar
gradient sizes.  Hence parameters like log(exp(theta)) would
have a scaling factor of 1 and log(theta) would have a scaling
factor of ini_value (to scale by 1/value; ie
d/dt(log(ini_value)) = 1/ini_value or scaleC=ini_value)
</p>

<ul>
<li><p> For parameters in an exponential (ie exp(theta)) or
parameters specifying powers, boxCox or yeoJohnson
transformations , this is 1.
</p>
</li>
<li><p> For additive, proportional, lognormal error structures,
these are given by 0.5*abs(initial_estimate)
</p>
</li>
<li><p> Factorials are scaled by abs(1/digamma(initial_estimate+1))
</p>
</li>
<li><p> parameters in a log scale (ie log(theta)) are transformed
by log(abs(initial_estimate))*abs(initial_estimate)
</p>
</li></ul>

<p>These parameter scaling coefficients are chose to try to keep
similar slopes among parameters.  That is they all follow the
slopes approximately on a log-scale.
</p>
<p>While these are chosen in a logical manner, they may not always
apply.  You can specify each parameters scaling factor by this
parameter if you wish.</p>
</td></tr>
<tr><td><code id="optimControl_+3A_scaleto">scaleTo</code></td>
<td>
<p>Scale the initial parameter estimate to this value.
By default this is 1.  When zero or below, no scaling is performed.</p>
</td></tr>
<tr><td><code id="optimControl_+3A_gradto">gradTo</code></td>
<td>
<p>this is the factor that the gradient is scaled to
before optimizing.  This only works with
scaleType=&quot;nlmixr2&quot;.</p>
</td></tr>
<tr><td><code id="optimControl_+3A_rxcontrol">rxControl</code></td>
<td>
<p>'rxode2' ODE solving options during fitting, created with 'rxControl()'</p>
</td></tr>
<tr><td><code id="optimControl_+3A_optexpression">optExpression</code></td>
<td>
<p>Optimize the rxode2 expression to speed up
calculation. By default this is turned on.</p>
</td></tr>
<tr><td><code id="optimControl_+3A_sumprod">sumProd</code></td>
<td>
<p>Is a boolean indicating if the model should change
multiplication to high precision multiplication and sums to
high precision sums using the PreciseSums package.  By default
this is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="optimControl_+3A_literalfix">literalFix</code></td>
<td>
<p>boolean, substitute fixed population values as
literals and re-adjust ui and parameter estimates after
optimization; Default is 'TRUE'.</p>
</td></tr>
<tr><td><code id="optimControl_+3A_returnoptim">returnOptim</code></td>
<td>
<p>logical; when TRUE this will return the optim
list instead of the nlmixr2 fit object</p>
</td></tr>
<tr><td><code id="optimControl_+3A_addprop">addProp</code></td>
<td>
<p>specifies the type of additive plus proportional
errors, the one where standard deviations add (combined1) or the
type where the variances add (combined2).
</p>
<p>The combined1 error type can be described by the following equation:
</p>
<p style="text-align: center;"><code class="reqn">y = f + (a + b\times f^c) \times \varepsilon</code>
</p>

<p>The combined2 error model can be described by the following equation:
</p>
<p style="text-align: center;"><code class="reqn">y = f + \sqrt{a^2 + b^2\times f^{2\times c}} \times \varepsilon</code>
</p>

<p>Where:
</p>
<p>- y represents the observed value
</p>
<p>- f represents the predicted value
</p>
<p>- a  is the additive standard deviation
</p>
<p>- b is the proportional/power standard deviation
</p>
<p>- c is the power exponent (in the proportional case c=1)</p>
</td></tr>
<tr><td><code id="optimControl_+3A_calctables">calcTables</code></td>
<td>
<p>This boolean is to determine if the foceiFit
will calculate tables. By default this is <code>TRUE</code></p>
</td></tr>
<tr><td><code id="optimControl_+3A_compress">compress</code></td>
<td>
<p>Should the object have compressed items</p>
</td></tr>
<tr><td><code id="optimControl_+3A_covmethod">covMethod</code></td>
<td>
<p>allows selection of &quot;r&quot;, which uses nlmixr2's
'nlmixr2Hess()' for the hessian calculation or &quot;optim&quot; which uses
the hessian from 'stats::optim(.., hessian=TRUE)'</p>
</td></tr>
<tr><td><code id="optimControl_+3A_adjobf">adjObf</code></td>
<td>
<p>is a boolean to indicate if the objective function
should be adjusted to be closer to NONMEM's default objective
function.  By default this is <code>TRUE</code></p>
</td></tr>
<tr><td><code id="optimControl_+3A_ci">ci</code></td>
<td>
<p>Confidence level for some tables.  By default this is
0.95 or 95% confidence.</p>
</td></tr>
<tr><td><code id="optimControl_+3A_sigdig">sigdig</code></td>
<td>
<p>Optimization significant digits. This controls:
</p>

<ul>
<li><p> The tolerance of the inner and outer optimization is <code>10^-sigdig</code>
</p>
</li>
<li><p> The tolerance of the ODE solvers is
<code>0.5*10^(-sigdig-2)</code>; For the sensitivity equations and
steady-state solutions the default is <code>0.5*10^(-sigdig-1.5)</code>
(sensitivity changes only applicable for liblsoda)
</p>
</li>
<li><p> The tolerance of the boundary check is <code>5 * 10 ^ (-sigdig + 1)</code>
</p>
</li></ul>
</td></tr>
<tr><td><code id="optimControl_+3A_sigdigtable">sigdigTable</code></td>
<td>
<p>Significant digits in the final output table.
If not specified, then it matches the significant digits in the
'sigdig' optimization algorithm.  If 'sigdig' is NULL, use 3.</p>
</td></tr>
<tr><td><code id="optimControl_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <code><a href="nlmixr2est.html#topic+optimControl">nlmixr2est::optimControl()</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>optimControl object for nlmixr2
</p>


<h3>Author(s)</h3>

<p>Matthew L. Fidler
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# A logit regression example with emax model

dsn &lt;- data.frame(i=1:1000)
dsn$time &lt;- exp(rnorm(1000))
dsn$DV=rbinom(1000,1,exp(-1+dsn$time)/(1+exp(-1+dsn$time)))

mod &lt;- function() {
 ini({
   E0 &lt;- 0.5
   Em &lt;- 0.5
   E50 &lt;- 2
   g &lt;- fix(2)
 })
 model({
   v &lt;- E0+Em*time^g/(E50^g+time^g)
   ll(bin) ~ DV * v - log(1 + exp(v))
 })
}

fit2 &lt;- nlmixr(mod, dsn, est="optim", optimControl(method="BFGS"))
fit2

</code></pre>

<hr>
<h2 id='preconditionFit'>Linearly re-parameterize the model to be less sensitive to rounding errors</h2><span id='topic+preconditionFit'></span>

<h3>Description</h3>

<p>Linearly re-parameterize the model to be less sensitive to rounding errors
</p>


<h3>Usage</h3>

<pre><code class='language-R'>preconditionFit(fit, estType = c("full", "posthoc", "none"), ntry = 10L)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="preconditionFit_+3A_fit">fit</code></td>
<td>
<p>A nlmixr2 fit to be preconditioned</p>
</td></tr>
<tr><td><code id="preconditionFit_+3A_esttype">estType</code></td>
<td>
<p>Once the fit has been linearly reparameterized,
should a &quot;full&quot; estimation, &quot;posthoc&quot; estimation or simply a
estimation of the covariance matrix &quot;none&quot; before the fit is
updated</p>
</td></tr>
<tr><td><code id="preconditionFit_+3A_ntry">ntry</code></td>
<td>
<p>number of tries before giving up on a pre-conditioned
covariance estimate</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A nlmixr2 fit object that was preconditioned to stabilize
the variance/covariance calculation
</p>


<h3>References</h3>

<p>Aoki Y, Nordgren R, Hooker AC. Preconditioning of
Nonlinear Mixed Effects Models for Stabilisation of
Variance-Covariance Matrix Computations. AAPS
J. 2016;18(2):505-518. doi:10.1208/s12248-016-9866-5
</p>

<hr>
<h2 id='reexports'>Objects exported from other packages</h2><span id='topic+reexports'></span><span id='topic+rxode2'></span><span id='topic+rxode'></span><span id='topic+RxODE'></span><span id='topic+rxDerived'></span><span id='topic++25+3E+25'></span><span id='topic+ini'></span><span id='topic+model'></span><span id='topic+lotri'></span><span id='topic+expit'></span><span id='topic+probit'></span><span id='topic+probitInv'></span><span id='topic+logit'></span><span id='topic+rxSolve'></span><span id='topic+rxClean'></span><span id='topic+rxCat'></span><span id='topic+rxSetPipingAuto'></span><span id='topic+eventTable'></span><span id='topic+add.dosing'></span><span id='topic+add.sampling'></span><span id='topic+rxFun'></span><span id='topic+nlmixr2AllEst'></span><span id='topic+pdDiag'></span><span id='topic+pdSymm'></span><span id='topic+pdLogChol'></span><span id='topic+pdIdent'></span><span id='topic+pdCompSymm'></span><span id='topic+pdBlocked'></span><span id='topic+pdNatural'></span><span id='topic+pdConstruct'></span><span id='topic+pdFactor'></span><span id='topic+pdMat'></span><span id='topic+pdMatrix'></span><span id='topic+reStruct'></span><span id='topic+varWeights'></span><span id='topic+varPower'></span><span id='topic+varFixed'></span><span id='topic+varFunc'></span><span id='topic+varExp'></span><span id='topic+varConstPower'></span><span id='topic+varIdent'></span><span id='topic+varComb'></span><span id='topic+groupedData'></span><span id='topic+getData'></span><span id='topic+model+3C-'></span><span id='topic+ini+3C-'></span><span id='topic+etExpand'></span><span id='topic+et'></span><span id='topic+rxParams'></span><span id='topic+rxParam'></span><span id='topic+geom_cens'></span><span id='topic+geom_amt'></span><span id='topic+stat_cens'></span><span id='topic+stat_amt'></span><span id='topic+rxControl'></span><span id='topic+nlme'></span><span id='topic+ACF'></span><span id='topic+VarCorr'></span><span id='topic+getVarCov'></span><span id='topic+augPred'></span><span id='topic+fixef'></span><span id='topic+fixed.effects'></span><span id='topic+ranef'></span><span id='topic+random.effects'></span><span id='topic+.nlmixrNlmeFun'></span><span id='topic+nlmixr2NlmeControl'></span><span id='topic+nlmixr'></span>

<h3>Description</h3>

<p>These objects are imported from other packages. Follow the links
below to see their documentation.
</p>

<dl>
<dt>lotri</dt><dd><p><code><a href="lotri.html#topic+lotri">lotri</a></code></p>
</dd>
<dt>magrittr</dt><dd><p><code><a href="magrittr.html#topic+pipe">%&gt;%</a></code></p>
</dd>
<dt>nlmixr2est</dt><dd><p><code><a href="nlmixr2est.html#topic+dot-nlmixrNlmeFun">.nlmixrNlmeFun</a></code>, <code><a href="nlmixr2est.html#topic+reexports">ACF</a></code>, <code><a href="nlmixr2est.html#topic+reexports">augPred</a></code>, <code><a href="nlmixr2est.html#topic+reexports">fixed.effects</a></code>, <code><a href="nlmixr2est.html#topic+reexports">fixef</a></code>, <code><a href="nlmixr2est.html#topic+reexports">getData</a></code>, <code><a href="nlmixr2est.html#topic+reexports">getVarCov</a></code>, <code><a href="nlmixr2est.html#topic+reexports">groupedData</a></code>, <code><a href="nlmixr2est.html#topic+reexports">nlme</a></code>, <code><a href="nlmixr2est.html#topic+nlmixr2">nlmixr</a></code>, <code><a href="nlmixr2est.html#topic+nlmixr2AllEst">nlmixr2AllEst</a></code>, <code><a href="nlmixr2est.html#topic+nlmixr2NlmeControl">nlmixr2NlmeControl</a></code>, <code><a href="nlmixr2est.html#topic+reexports">pdBlocked</a></code>, <code><a href="nlmixr2est.html#topic+reexports">pdCompSymm</a></code>, <code><a href="nlmixr2est.html#topic+reexports">pdConstruct</a></code>, <code><a href="nlmixr2est.html#topic+reexports">pdDiag</a></code>, <code><a href="nlmixr2est.html#topic+reexports">pdFactor</a></code>, <code><a href="nlmixr2est.html#topic+reexports">pdIdent</a></code>, <code><a href="nlmixr2est.html#topic+reexports">pdLogChol</a></code>, <code><a href="nlmixr2est.html#topic+reexports">pdMat</a></code>, <code><a href="nlmixr2est.html#topic+reexports">pdMatrix</a></code>, <code><a href="nlmixr2est.html#topic+reexports">pdNatural</a></code>, <code><a href="nlmixr2est.html#topic+reexports">pdSymm</a></code>, <code><a href="nlmixr2est.html#topic+reexports">random.effects</a></code>, <code><a href="nlmixr2est.html#topic+reexports">ranef</a></code>, <code><a href="nlmixr2est.html#topic+reexports">reStruct</a></code>, <code><a href="nlmixr2est.html#topic+reexports">varComb</a></code>, <code><a href="nlmixr2est.html#topic+reexports">varConstPower</a></code>, <code><a href="nlmixr2est.html#topic+reexports">VarCorr</a></code>, <code><a href="nlmixr2est.html#topic+reexports">varExp</a></code>, <code><a href="nlmixr2est.html#topic+reexports">varFixed</a></code>, <code><a href="nlmixr2est.html#topic+reexports">varFunc</a></code>, <code><a href="nlmixr2est.html#topic+reexports">varIdent</a></code>, <code><a href="nlmixr2est.html#topic+reexports">varPower</a></code>, <code><a href="nlmixr2est.html#topic+reexports">varWeights</a></code></p>
</dd>
<dt>rxode2</dt><dd><p><code><a href="rxode2.html#topic+reexports">add.dosing</a></code>, <code><a href="rxode2.html#topic+reexports">add.sampling</a></code>, <code><a href="rxode2.html#topic+reexports">et</a></code>, <code><a href="rxode2.html#topic+reexports">etExpand</a></code>, <code><a href="rxode2.html#topic+reexports">eventTable</a></code>, <code><a href="rxode2.html#topic+logit">expit</a></code>, <code><a href="rxode2.html#topic+stat_amt">geom_amt</a></code>, <code><a href="rxode2.html#topic+stat_cens">geom_cens</a></code>, <code><a href="rxode2.html#topic+ini">ini</a></code>, <code><a href="rxode2.html#topic+ini-set">ini&lt;-</a></code>, <code><a href="rxode2.html#topic+logit">logit</a></code>, <code><a href="rxode2.html#topic+reexports">lotri</a></code>, <code><a href="rxode2.html#topic+model">model</a></code>, <code><a href="rxode2.html#topic+model-set">model&lt;-</a></code>, <code><a href="rxode2.html#topic+probit">probit</a></code>, <code><a href="rxode2.html#topic+probit">probitInv</a></code>, <code><a href="rxode2.html#topic+rxCat">rxCat</a></code>, <code><a href="rxode2.html#topic+rxClean">rxClean</a></code>, <code><a href="rxode2.html#topic+rxSolve">rxControl</a></code>, <code><a href="rxode2.html#topic+reexports">rxDerived</a></code>, <code><a href="rxode2.html#topic+rxFun">rxFun</a></code>, <code><a href="rxode2.html#topic+rxode2">rxode</a></code>, <code><a href="rxode2.html#topic+rxode2">RxODE</a></code>, <code><a href="rxode2.html#topic+rxode2">rxode2</a></code>, <code><a href="rxode2.html#topic+rxParams">rxParam</a></code>, <code><a href="rxode2.html#topic+rxParams">rxParams</a></code>, <code><a href="rxode2.html#topic+rxSetPipingAuto">rxSetPipingAuto</a></code>, <code><a href="rxode2.html#topic+rxSolve">rxSolve</a></code>, <code><a href="rxode2.html#topic+stat_amt">stat_amt</a></code>, <code><a href="rxode2.html#topic+stat_cens">stat_cens</a></code></p>
</dd>
</dl>

<hr>
<h2 id='saemControl'>Control Options for SAEM</h2><span id='topic+saemControl'></span>

<h3>Description</h3>

<p>Control Options for SAEM
</p>


<h3>Usage</h3>

<pre><code class='language-R'>saemControl(
  seed = 99,
  nBurn = 200,
  nEm = 300,
  nmc = 3,
  nu = c(2, 2, 2),
  print = 1,
  trace = 0,
  covMethod = c("linFim", "fim", "r,s", "r", "s", ""),
  calcTables = TRUE,
  logLik = FALSE,
  nnodesGq = 3,
  nsdGq = 1.6,
  optExpression = TRUE,
  literalFix = TRUE,
  adjObf = TRUE,
  sumProd = FALSE,
  addProp = c("combined2", "combined1"),
  tol = 1e-06,
  itmax = 30,
  type = c("nelder-mead", "newuoa"),
  powRange = 10,
  lambdaRange = 3,
  odeRecalcFactor = 10^(0.5),
  maxOdeRecalc = 5L,
  perSa = 0.75,
  perNoCor = 0.75,
  perFixOmega = 0.1,
  perFixResid = 0.1,
  compress = TRUE,
  rxControl = NULL,
  sigdig = NULL,
  sigdigTable = NULL,
  ci = 0.95,
  muRefCov = TRUE,
  muRefCovAlg = TRUE,
  handleUninformativeEtas = TRUE,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="saemControl_+3A_seed">seed</code></td>
<td>
<p>Random Seed for SAEM step.  (Needs to be set for
reproducibility.)  By default this is 99.</p>
</td></tr>
<tr><td><code id="saemControl_+3A_nburn">nBurn</code></td>
<td>
<p>Number of iterations in the first phase, ie the  MCMC/Stochastic Approximation
steps. This is equivalent to Monolix's <code>K_0</code> or <code>K_b</code>.</p>
</td></tr>
<tr><td><code id="saemControl_+3A_nem">nEm</code></td>
<td>
<p>Number of iterations in the Expectation-Maximization
(EM) Step. This is equivalent to Monolix's <code>K_1</code>.</p>
</td></tr>
<tr><td><code id="saemControl_+3A_nmc">nmc</code></td>
<td>
<p>Number of Markov Chains. By default this is 3.  When
you increase the number of chains the numerical integration by
MC method will be more accurate at the cost of more
computation.  In Monolix this is equivalent to <code>L</code>.</p>
</td></tr>
<tr><td><code id="saemControl_+3A_nu">nu</code></td>
<td>
<p>This is a vector of 3 integers. They represent the
numbers of transitions of the three different kernels used in
the Hasting-Metropolis algorithm.  The default value is <code>c(2,2,2)</code>,
representing 40 for each transition initially (each value is
multiplied by 20).
</p>
<p>The first value represents the initial number of multi-variate
Gibbs samples are taken from a normal distribution.
</p>
<p>The second value represents the number of uni-variate, or multi-
dimensional random walk Gibbs samples are taken.
</p>
<p>The third value represents the number of bootstrap/reshuffling or
uni-dimensional random samples are taken.</p>
</td></tr>
<tr><td><code id="saemControl_+3A_print">print</code></td>
<td>
<p>The number it iterations that are completed before
anything is printed to the console.  By default, this is 1.</p>
</td></tr>
<tr><td><code id="saemControl_+3A_trace">trace</code></td>
<td>
<p>An integer indicating if you want to trace(1) the
SAEM algorithm process.  Useful for debugging, but not for
typical fitting.</p>
</td></tr>
<tr><td><code id="saemControl_+3A_covmethod">covMethod</code></td>
<td>
<p>Method for calculating covariance.  In this
discussion, R is the Hessian matrix of the objective
function. The S matrix is the sum of each individual's
gradient cross-product (evaluated at the individual empirical
Bayes estimates).
</p>
<p>&quot;<code>linFim</code>&quot; Use the Linearized Fisher Information Matrix to calculate the covariance.
</p>
<p>&quot;<code>fim</code>&quot; Use the SAEM-calculated Fisher Information Matrix to calculate the covariance.
</p>
<p>&quot;<code>r,s</code>&quot; Uses the sandwich matrix to calculate the covariance, that is: <code class="reqn">R^-1 \times S \times R^-1</code>
</p>
<p>&quot;<code>r</code>&quot; Uses the Hessian matrix to calculate the covariance as <code class="reqn">2\times R^-1</code>
</p>
<p>&quot;<code>s</code>&quot; Uses the crossproduct matrix to calculate the covariance as <code class="reqn">4\times S^-1</code>
</p>
<p>&quot;&quot; Does not calculate the covariance step.</p>
</td></tr>
<tr><td><code id="saemControl_+3A_calctables">calcTables</code></td>
<td>
<p>This boolean is to determine if the foceiFit
will calculate tables. By default this is <code>TRUE</code></p>
</td></tr>
<tr><td><code id="saemControl_+3A_loglik">logLik</code></td>
<td>
<p>boolean indicating that log-likelihood should be
calculate by Gaussian quadrature.</p>
</td></tr>
<tr><td><code id="saemControl_+3A_nnodesgq">nnodesGq</code></td>
<td>
<p>number of nodes to use for the Gaussian
quadrature when computing the likelihood with this method
(defaults to 1, equivalent to the Laplacian likelihood)</p>
</td></tr>
<tr><td><code id="saemControl_+3A_nsdgq">nsdGq</code></td>
<td>
<p>span (in SD) over which to integrate when computing
the likelihood by Gaussian quadrature. Defaults to 3 (eg 3
times the SD)</p>
</td></tr>
<tr><td><code id="saemControl_+3A_optexpression">optExpression</code></td>
<td>
<p>Optimize the rxode2 expression to speed up
calculation. By default this is turned on.</p>
</td></tr>
<tr><td><code id="saemControl_+3A_literalfix">literalFix</code></td>
<td>
<p>boolean, substitute fixed population values as
literals and re-adjust ui and parameter estimates after
optimization; Default is 'TRUE'.</p>
</td></tr>
<tr><td><code id="saemControl_+3A_adjobf">adjObf</code></td>
<td>
<p>is a boolean to indicate if the objective function
should be adjusted to be closer to NONMEM's default objective
function.  By default this is <code>TRUE</code></p>
</td></tr>
<tr><td><code id="saemControl_+3A_sumprod">sumProd</code></td>
<td>
<p>Is a boolean indicating if the model should change
multiplication to high precision multiplication and sums to
high precision sums using the PreciseSums package.  By default
this is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="saemControl_+3A_addprop">addProp</code></td>
<td>
<p>specifies the type of additive plus proportional
errors, the one where standard deviations add (combined1) or the
type where the variances add (combined2).
</p>
<p>The combined1 error type can be described by the following equation:
</p>
<p style="text-align: center;"><code class="reqn">y = f + (a + b\times f^c) \times \varepsilon</code>
</p>

<p>The combined2 error model can be described by the following equation:
</p>
<p style="text-align: center;"><code class="reqn">y = f + \sqrt{a^2 + b^2\times f^{2\times c}} \times \varepsilon</code>
</p>

<p>Where:
</p>
<p>- y represents the observed value
</p>
<p>- f represents the predicted value
</p>
<p>- a  is the additive standard deviation
</p>
<p>- b is the proportional/power standard deviation
</p>
<p>- c is the power exponent (in the proportional case c=1)</p>
</td></tr>
<tr><td><code id="saemControl_+3A_tol">tol</code></td>
<td>
<p>This is the tolerance for the regression models used
for complex residual errors (ie add+prop etc)</p>
</td></tr>
<tr><td><code id="saemControl_+3A_itmax">itmax</code></td>
<td>
<p>This is the maximum number of iterations for the
regression models used for complex residual errors.  The number
of iterations is itmax*number of parameters</p>
</td></tr>
<tr><td><code id="saemControl_+3A_type">type</code></td>
<td>
<p>indicates the type of optimization for the residuals; Can be one of c(&quot;nelder-mead&quot;, &quot;newuoa&quot;)</p>
</td></tr>
<tr><td><code id="saemControl_+3A_powrange">powRange</code></td>
<td>
<p>This indicates the range that powers can take for residual errors;  By default this is 10 indicating the range is c(-10, 10)</p>
</td></tr>
<tr><td><code id="saemControl_+3A_lambdarange">lambdaRange</code></td>
<td>
<p>This indicates the range that Box-Cox and Yeo-Johnson parameters are constrained to be;  The default is 3 indicating the range c(-3,3)</p>
</td></tr>
<tr><td><code id="saemControl_+3A_oderecalcfactor">odeRecalcFactor</code></td>
<td>
<p>The ODE recalculation factor when ODE
solving goes bad, this is the factor the rtol/atol is reduced</p>
</td></tr>
<tr><td><code id="saemControl_+3A_maxoderecalc">maxOdeRecalc</code></td>
<td>
<p>Maximum number of times to reduce the ODE
tolerances and try to resolve the system if there was a bad
ODE solve.</p>
</td></tr>
<tr><td><code id="saemControl_+3A_persa">perSa</code></td>
<td>
<p>This is the percent of the time the 'nBurn'
iterations in phase runs runs a simulated annealing.</p>
</td></tr>
<tr><td><code id="saemControl_+3A_pernocor">perNoCor</code></td>
<td>
<p>This is the percentage of the MCMC phase of the SAEM
algorithm where the variance/covariance matrix has no
correlations. By default this is 0.75 or 75
Monte-carlo iteration.</p>
</td></tr>
<tr><td><code id="saemControl_+3A_perfixomega">perFixOmega</code></td>
<td>
<p>This is the percentage of the 'nBurn' phase
where the omega values are unfixed to allow better exploration
of the likelihood surface.  After this time, the omegas are
fixed during optimization.</p>
</td></tr>
<tr><td><code id="saemControl_+3A_perfixresid">perFixResid</code></td>
<td>
<p>This is the percentage of the 'nBurn' phase
where the residual components are unfixed to allow better
exploration of the likelihood surface.</p>
</td></tr>
<tr><td><code id="saemControl_+3A_compress">compress</code></td>
<td>
<p>Should the object have compressed items</p>
</td></tr>
<tr><td><code id="saemControl_+3A_rxcontrol">rxControl</code></td>
<td>
<p>'rxode2' ODE solving options during fitting, created with 'rxControl()'</p>
</td></tr>
<tr><td><code id="saemControl_+3A_sigdig">sigdig</code></td>
<td>
<p>Specifies the &quot;significant digits&quot; that the ode
solving requests.  When specified this controls the relative and
absolute tolerances of the ODE solvers.  By default the tolerance
is <code>0.5*10^(-sigdig-2)</code> for regular ODEs. For the
sensitivity equations the default is <code style="white-space: pre;">&#8288;0.5*10\^(-sigdig-1.5)&#8288;</code>
(sensitivity changes only applicable for liblsoda).  This also
controls the <code>atol</code>/<code>rtol</code> of the steady state solutions. The
<code>ssAtol</code>/<code>ssRtol</code> is <code style="white-space: pre;">&#8288;0.5*10\^(-sigdig)&#8288;</code> and for the sensitivities
<code style="white-space: pre;">&#8288;0.5*10\^(-sigdig+0.625)&#8288;</code>.  By default
this is unspecified (<code>NULL</code>) and uses the standard <code>atol</code>/<code>rtol</code>.</p>
</td></tr>
<tr><td><code id="saemControl_+3A_sigdigtable">sigdigTable</code></td>
<td>
<p>Significant digits in the final output table.
If not specified, then it matches the significant digits in the
'sigdig' optimization algorithm.  If 'sigdig' is NULL, use 3.</p>
</td></tr>
<tr><td><code id="saemControl_+3A_ci">ci</code></td>
<td>
<p>Confidence level for some tables.  By default this is
0.95 or 95% confidence.</p>
</td></tr>
<tr><td><code id="saemControl_+3A_murefcov">muRefCov</code></td>
<td>
<p>This controls if mu-referenced covariates in 'saem'
are handled differently than non mu-referenced covariates.  When
'TRUE', mu-referenced covariates have special handling.  When
'FALSE' mu-referenced covariates are treated the same as any
other input parameter.</p>
</td></tr>
<tr><td><code id="saemControl_+3A_murefcovalg">muRefCovAlg</code></td>
<td>
<p>This controls if algebraic expressions that can
be mu-referenced are treated as mu-referenced covariates by:
</p>
<p>1. Creating a internal data-variable 'nlmixrMuDerCov#' for each
algebraic mu-referenced expression
</p>
<p>2. Change the algebraic expression to 'nlmixrMuDerCov# * mu_cov_theta'
</p>
<p>3. Use the internal mu-referenced covariate for saem
</p>
<p>4. After optimization is completed, replace 'model()' with old
'model()' expression
</p>
<p>5. Remove 'nlmixrMuDerCov#' from nlmix2 output
</p>
<p>In general, these covariates should be more accurate since it
changes the system to a linear compartment model.  Therefore, by default this is 'TRUE'.</p>
</td></tr>
<tr><td><code id="saemControl_+3A_handleuninformativeetas">handleUninformativeEtas</code></td>
<td>
<p>boolean that tells nlmixr2's saem to
calculate uninformative etas and handle them specially (default
is 'TRUE').</p>
</td></tr>
<tr><td><code id="saemControl_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <code><a href="nlmixr2est.html#topic+saemControl">nlmixr2est::saemControl()</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>List of options to be used in <code><a href="nlmixr2est.html#topic+nlmixr2">nlmixr2</a></code> fit for
SAEM.
</p>


<h3>Author(s)</h3>

<p>Wenping Wang &amp; Matthew L. Fidler
</p>


<h3>See Also</h3>

<p>Other Estimation control: 
<code><a href="nlmixr2est.html#topic+foceiControl">foceiControl</a>()</code>,
<code><a href="nlmixr2est.html#topic+nlmixr2NlmeControl">nlmixr2NlmeControl</a>()</code>
</p>

<hr>
<h2 id='setOfv'>Set/get Objective function type for a nlmixr2 object</h2><span id='topic+setOfv'></span>

<h3>Description</h3>

<p>Set/get Objective function type for a nlmixr2 object
</p>


<h3>Usage</h3>

<pre><code class='language-R'>setOfv(x, type)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="setOfv_+3A_x">x</code></td>
<td>
<p>nlmixr2 fit object</p>
</td></tr>
<tr><td><code id="setOfv_+3A_type">type</code></td>
<td>
<p>Type of objective function to use for AIC, BIC, and
$objective</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Nothing
</p>


<h3>Author(s)</h3>

<p>Matthew L. Fidler
</p>

<hr>
<h2 id='tableControl'>Output table/data.frame options</h2><span id='topic+tableControl'></span>

<h3>Description</h3>

<p>Output table/data.frame options
</p>


<h3>Usage</h3>

<pre><code class='language-R'>tableControl(
  npde = NULL,
  cwres = NULL,
  nsim = 300,
  ties = TRUE,
  censMethod = c("truncated-normal", "cdf", "ipred", "pred", "epred", "omit"),
  seed = 1009,
  cholSEtol = (.Machine$double.eps)^(1/3),
  state = TRUE,
  lhs = TRUE,
  eta = TRUE,
  covariates = TRUE,
  addDosing = FALSE,
  subsetNonmem = TRUE,
  cores = NULL,
  keep = NULL,
  drop = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="tableControl_+3A_npde">npde</code></td>
<td>
<p>When TRUE, request npde regardless of the algorithm used.</p>
</td></tr>
<tr><td><code id="tableControl_+3A_cwres">cwres</code></td>
<td>
<p>When TRUE, request CWRES and FOCEi likelihood
regardless of the algorithm used.</p>
</td></tr>
<tr><td><code id="tableControl_+3A_nsim">nsim</code></td>
<td>
<p>represents the number of simulations.  For rxode2, if
you supply single subject event tables (created with
<code style="white-space: pre;">&#8288;[eventTable()]&#8288;</code>)</p>
</td></tr>
<tr><td><code id="tableControl_+3A_ties">ties</code></td>
<td>
<p>When 'TRUE' jitter prediction-discrepancy points to discourage ties in cdf.</p>
</td></tr>
<tr><td><code id="tableControl_+3A_censmethod">censMethod</code></td>
<td>
<p>Handle censoring method:
</p>
<p>- '&quot;truncated-normal&quot;' Simulates from a truncated normal distribution under the assumption of the model and censoring.
</p>
<p>- '&quot;cdf&quot;' Use the cdf-method for censoring with npde and use this for any other residuals ('cwres' etc)
</p>
<p>- '&quot;omit&quot;' omit the residuals for censoring</p>
</td></tr>
<tr><td><code id="tableControl_+3A_seed">seed</code></td>
<td>
<p>an object specifying if and how the random number
generator should be initialized</p>
</td></tr>
<tr><td><code id="tableControl_+3A_cholsetol">cholSEtol</code></td>
<td>
<p>The tolerance for the 'rxode2::choleSE' function</p>
</td></tr>
<tr><td><code id="tableControl_+3A_state">state</code></td>
<td>
<p>is a Boolean indicating if 'state' values will be included (default 'TRUE')</p>
</td></tr>
<tr><td><code id="tableControl_+3A_lhs">lhs</code></td>
<td>
<p>is a Boolean indicating if remaining 'lhs' values will be included (default 'TRUE')</p>
</td></tr>
<tr><td><code id="tableControl_+3A_eta">eta</code></td>
<td>
<p>is a Boolean indicating if 'eta' values will be included (default 'TRUE')</p>
</td></tr>
<tr><td><code id="tableControl_+3A_covariates">covariates</code></td>
<td>
<p>is a Boolean indicating if covariates will be included (default 'TRUE')</p>
</td></tr>
<tr><td><code id="tableControl_+3A_adddosing">addDosing</code></td>
<td>
<p>Boolean indicating if the solve should add rxode2
EVID and related columns.  This will also include dosing
information and estimates at the doses.  Be default, rxode2
only includes estimates at the observations. (default
<code>FALSE</code>). When <code>addDosing</code> is <code>NULL</code>, only
include <code>EVID=0</code> on solve and exclude any model-times or
<code>EVID=2</code>. If <code>addDosing</code> is <code>NA</code> the classic
<code>rxode2</code> EVID events are returned. When <code>addDosing</code> is <code>TRUE</code>
add the event information in NONMEM-style format; If
<code>subsetNonmem=FALSE</code> rxode2 will also include extra event types
(<code>EVID</code>) for ending infusion and modeled times:
</p>

<ul>
<li> <p><code>EVID=-1</code> when the modeled rate infusions are turned
off (matches <code>rate=-1</code>)
</p>
</li>
<li> <p><code>EVID=-2</code> When the modeled duration infusions are
turned off (matches <code>rate=-2</code>)
</p>
</li>
<li> <p><code>EVID=-10</code> When the specified <code>rate</code> infusions are
turned off (matches <code>rate&gt;0</code>)
</p>
</li>
<li> <p><code>EVID=-20</code> When the specified <code>dur</code> infusions are
turned off (matches <code>dur&gt;0</code>)
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;EVID=101,102,103,...&#8288;</code> Modeled time where 101 is the
first model time, 102 is the second etc.
</p>
</li></ul>
</td></tr>
<tr><td><code id="tableControl_+3A_subsetnonmem">subsetNonmem</code></td>
<td>
<p>subset to NONMEM compatible EVIDs only.  By
default <code>TRUE</code>.</p>
</td></tr>
<tr><td><code id="tableControl_+3A_cores">cores</code></td>
<td>
<p>Number of cores used in parallel ODE solving.  This
is equivalent to calling <code><a href="rxode2.html#topic+setRxThreads">setRxThreads()</a></code></p>
</td></tr>
<tr><td><code id="tableControl_+3A_keep">keep</code></td>
<td>
<p>is the keep sent to the table</p>
</td></tr>
<tr><td><code id="tableControl_+3A_drop">drop</code></td>
<td>
<p>is the dropped variables sent to the table</p>
</td></tr>
</table>


<h3>Details</h3>

<p>If you ever want to add CWRES/FOCEi objective function you can use the <code><a href="nlmixr2est.html#topic+addCwres">addCwres</a></code>
</p>
<p>If you ever want to add NPDE/EPRED columns you can use the <code><a href="nlmixr2est.html#topic+addNpde">addNpde</a></code>
</p>


<h3>Value</h3>

<p>A list of table options for nlmixr2
</p>


<h3>Author(s)</h3>

<p>Matthew L. Fidler
</p>

<hr>
<h2 id='traceplot'>Produce trace-plot for fit if applicable</h2><span id='topic+traceplot'></span>

<h3>Description</h3>

<p>Produce trace-plot for fit if applicable
</p>


<h3>Usage</h3>

<pre><code class='language-R'>traceplot(x, ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="traceplot_+3A_x">x</code></td>
<td>
<p>fit object</p>
</td></tr>
<tr><td><code id="traceplot_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <code><a href="nlmixr2plot.html#topic+traceplot">nlmixr2plot::traceplot()</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Fit traceplot or nothing.
</p>


<h3>Author(s)</h3>

<p>Rik Schoemaker, Wenping Wang &amp; Matthew L. Fidler
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

library(nlmixr2est)
## The basic model consiss of an ini block that has initial estimates
one.compartment &lt;- function() {
  ini({
    tka &lt;- 0.45 # Log Ka
    tcl &lt;- 1 # Log Cl
    tv &lt;- 3.45    # Log V
    eta.ka ~ 0.6
    eta.cl ~ 0.3
    eta.v ~ 0.1
    add.sd &lt;- 0.7
  })
  # and a model block with the error sppecification and model specification
  model({
    ka &lt;- exp(tka + eta.ka)
    cl &lt;- exp(tcl + eta.cl)
    v &lt;- exp(tv + eta.v)
    d/dt(depot) = -ka * depot
    d/dt(center) = ka * depot - cl / v * center
    cp = center / v
    cp ~ add(add.sd)
  })
}

## The fit is performed by the function nlmixr/nlmix2 specifying the model, data and estimate
fit &lt;- nlmixr2(one.compartment, theo_sd,  est="saem", saemControl(print=0))

# This shows the traceplot of the fit (useful for saem)
traceplot(fit)


</code></pre>

<hr>
<h2 id='uobyqaControl'>Control for uobyqa estimation method in nlmixr2</h2><span id='topic+uobyqaControl'></span>

<h3>Description</h3>

<p>Control for uobyqa estimation method in nlmixr2
</p>


<h3>Usage</h3>

<pre><code class='language-R'>uobyqaControl(
  npt = NULL,
  rhobeg = NULL,
  rhoend = NULL,
  iprint = 0L,
  maxfun = 100000L,
  returnUobyqa = FALSE,
  stickyRecalcN = 4,
  maxOdeRecalc = 5,
  odeRecalcFactor = 10^(0.5),
  useColor = crayon::has_color(),
  printNcol = floor((getOption("width") - 23)/12),
  print = 1L,
  normType = c("rescale2", "mean", "rescale", "std", "len", "constant"),
  scaleType = c("nlmixr2", "norm", "mult", "multAdd"),
  scaleCmax = 1e+05,
  scaleCmin = 1e-05,
  scaleC = NULL,
  scaleTo = 1,
  rxControl = NULL,
  optExpression = TRUE,
  sumProd = FALSE,
  literalFix = TRUE,
  addProp = c("combined2", "combined1"),
  calcTables = TRUE,
  compress = TRUE,
  covMethod = c("r", ""),
  adjObf = TRUE,
  ci = 0.95,
  sigdig = 4,
  sigdigTable = NULL,
  ...
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="uobyqaControl_+3A_npt">npt</code></td>
<td>
<p>The number of points used to approximate the objective
function via a quadratic approximation for bobyqa. The value
of npt must be in the interval [n+2,(n+1)(n+2)/2] where n is
the number of parameters in par. Choices that exceed 2*n+1 are
not recommended. If not defined, it will be set to 2*n + 1. (bobyqa)</p>
</td></tr>
<tr><td><code id="uobyqaControl_+3A_rhobeg">rhobeg</code></td>
<td>
<p>Beginning change in parameters for bobyqa algorithm
(trust region).  By default this is 0.2 or 20
parameters when the parameters are scaled to 1. rhobeg and
rhoend must be set to the initial and final values of a trust
region radius, so both must be positive with 0 &lt; rhoend &lt;
rhobeg. Typically rhobeg should be about one tenth of the
greatest expected change to a variable.  Note also that
smallest difference abs(upper-lower) should be greater than or
equal to rhobeg*2. If this is not the case then rhobeg will be
adjusted. (bobyqa)</p>
</td></tr>
<tr><td><code id="uobyqaControl_+3A_rhoend">rhoend</code></td>
<td>
<p>The smallest value of the trust region radius that
is allowed. If not defined, then 10^(-sigdig-1) will be used. (bobyqa)</p>
</td></tr>
<tr><td><code id="uobyqaControl_+3A_iprint">iprint</code></td>
<td>
<p>The value of 'iprint' should be set to an integer
value in '0, 1, 2, 3, ...', which controls the amount of
printing.  Specifically, there is no output if 'iprint=0' and
there is output only at the start and the return if 'iprint=1'.
Otherwise, each new value of 'rho' is printed, with the best
vector of variables so far and the corresponding value of the
objective function. Further, each new value of the objective
function with its variables are output if 'iprint=3'.  If 'iprint
&gt; 3', the objective function value and corresponding variables
are output every 'iprint' evaluations.  Default value is '0'.</p>
</td></tr>
<tr><td><code id="uobyqaControl_+3A_maxfun">maxfun</code></td>
<td>
<p>The maximum allowed number of function
evaluations. If this is exceeded, the method will terminate.</p>
</td></tr>
<tr><td><code id="uobyqaControl_+3A_returnuobyqa">returnUobyqa</code></td>
<td>
<p>return the uobyqa output instead of the nlmixr2
fit</p>
</td></tr>
<tr><td><code id="uobyqaControl_+3A_stickyrecalcn">stickyRecalcN</code></td>
<td>
<p>The number of bad ODE solves before reducing
the atol/rtol for the rest of the problem.</p>
</td></tr>
<tr><td><code id="uobyqaControl_+3A_maxoderecalc">maxOdeRecalc</code></td>
<td>
<p>Maximum number of times to reduce the ODE
tolerances and try to resolve the system if there was a bad
ODE solve.</p>
</td></tr>
<tr><td><code id="uobyqaControl_+3A_oderecalcfactor">odeRecalcFactor</code></td>
<td>
<p>The ODE recalculation factor when ODE
solving goes bad, this is the factor the rtol/atol is reduced</p>
</td></tr>
<tr><td><code id="uobyqaControl_+3A_usecolor">useColor</code></td>
<td>
<p>Boolean indicating if focei can use ASCII color codes</p>
</td></tr>
<tr><td><code id="uobyqaControl_+3A_printncol">printNcol</code></td>
<td>
<p>Number of columns to printout before wrapping
parameter estimates/gradient</p>
</td></tr>
<tr><td><code id="uobyqaControl_+3A_print">print</code></td>
<td>
<p>Integer representing when the outer step is
printed. When this is 0 or do not print the iterations.  1 is
print every function evaluation (default), 5 is print every 5
evaluations.</p>
</td></tr>
<tr><td><code id="uobyqaControl_+3A_normtype">normType</code></td>
<td>
<p>This is the type of parameter
normalization/scaling used to get the scaled initial values
for nlmixr2.  These are used with <code>scaleType</code> of.
</p>
<p>With the exception of <code>rescale2</code>, these come
from
<a href="https://en.wikipedia.org/wiki/Feature_scaling">Feature
Scaling</a>. The <code>rescale2</code> The rescaling is the same type
described in the
<a href="http://apmonitor.com/me575/uploads/Main/optimization_book.pdf">OptdesX</a>
software manual.
</p>
<p>In general, all all scaling formula can be described by:
</p>
<p style="text-align: center;"><code class="reqn">v_{scaled}</code>
</p>
<p> = (</p>
<p style="text-align: center;"><code class="reqn">v_{unscaled}-C_{1}</code>
</p>
<p>)/</p>
<p style="text-align: center;"><code class="reqn">C_{2}</code>
</p>

<p>Where
</p>
<p>The other data normalization approaches follow the following formula
</p>
<p style="text-align: center;"><code class="reqn">v_{scaled}</code>
</p>
<p> = (</p>
<p style="text-align: center;"><code class="reqn">v_{unscaled}-C_{1}</code>
</p>
<p>)/</p>
<p style="text-align: center;"><code class="reqn">C_{2}</code>
</p>


<ul>
<li> <p><code>rescale2</code> This scales all parameters from (-1 to 1).
The relative differences between the parameters are preserved
with this approach and the constants are:
</p>
<p style="text-align: center;"><code class="reqn">C_{1}</code>
</p>
<p> = (max(all unscaled values)+min(all unscaled values))/2
</p>
<p style="text-align: center;"><code class="reqn">C_{2}</code>
</p>
<p> = (max(all unscaled values) - min(all unscaled values))/2
</p>
</li>
<li> <p><code>rescale</code> or min-max normalization. This rescales all
parameters from (0 to 1).  As in the <code>rescale2</code> the
relative differences are preserved.  In this approach:
</p>
<p style="text-align: center;"><code class="reqn">C_{1}</code>
</p>
<p> = min(all unscaled values)
</p>
<p style="text-align: center;"><code class="reqn">C_{2}</code>
</p>
<p> = max(all unscaled values) - min(all unscaled values)
</p>
</li>
<li> <p><code>mean</code> or mean normalization.  This rescales to center
the parameters around the mean but the parameters are from 0
to 1.  In this approach:
</p>
<p style="text-align: center;"><code class="reqn">C_{1}</code>
</p>
<p> = mean(all unscaled values)
</p>
<p style="text-align: center;"><code class="reqn">C_{2}</code>
</p>
<p> = max(all unscaled values) - min(all unscaled values)
</p>
</li>
<li> <p><code>std</code> or standardization.  This standardizes by the mean
and standard deviation.  In this approach:
</p>
<p style="text-align: center;"><code class="reqn">C_{1}</code>
</p>
<p> = mean(all unscaled values)
</p>
<p style="text-align: center;"><code class="reqn">C_{2}</code>
</p>
<p> = sd(all unscaled values)
</p>
</li>
<li> <p><code>len</code> or unit length scaling.  This scales the
parameters to the unit length.  For this approach we use the Euclidean length, that
is:
</p>
<p style="text-align: center;"><code class="reqn">C_{1}</code>
</p>
<p> = 0
</p>
<p style="text-align: center;"><code class="reqn">C_{2}</code>
</p>
<p> = </p>
<p style="text-align: center;"><code class="reqn">\sqrt(v_1^2 + v_2^2 + \cdots + v_n^2)</code>
</p>

</li>
<li> <p><code>constant</code> which does not perform data normalization. That is
</p>
<p style="text-align: center;"><code class="reqn">C_{1}</code>
</p>
<p> = 0
</p>
<p style="text-align: center;"><code class="reqn">C_{2}</code>
</p>
<p> = 1
</p>
</li></ul>
</td></tr>
<tr><td><code id="uobyqaControl_+3A_scaletype">scaleType</code></td>
<td>
<p>The scaling scheme for nlmixr2.  The supported types are:
</p>

<ul>
<li> <p><code>nlmixr2</code>  In this approach the scaling is performed by the following equation:
</p>
<p style="text-align: center;"><code class="reqn">v_{scaled}</code>
</p>
<p> = (</p>
<p style="text-align: center;"><code class="reqn">v_{current} - v_{init}</code>
</p>
<p>)*scaleC[i] + scaleTo
</p>
<p>The <code>scaleTo</code> parameter is specified by the <code>normType</code>,
and the scales are specified by <code>scaleC</code>.
</p>
</li>
<li> <p><code>norm</code> This approach uses the simple scaling provided
by the <code>normType</code> argument.
</p>
</li>
<li> <p><code>mult</code> This approach does not use the data
normalization provided by <code>normType</code>, but rather uses
multiplicative scaling to a constant provided by the <code>scaleTo</code>
argument.
</p>
<p>In this case:
</p>
<p style="text-align: center;"><code class="reqn">v_{scaled}</code>
</p>
<p> = </p>
<p style="text-align: center;"><code class="reqn">v_{current}</code>
</p>
<p>/</p>
<p style="text-align: center;"><code class="reqn">v_{init}</code>
</p>
<p>*scaleTo
</p>
</li>
<li> <p><code>multAdd</code> This approach changes the scaling based on
the parameter being specified.  If a parameter is defined in an
exponential block (ie exp(theta)), then it is scaled on a
linearly, that is:
</p>
<p style="text-align: center;"><code class="reqn">v_{scaled}</code>
</p>
<p> = (</p>
<p style="text-align: center;"><code class="reqn">v_{current}-v_{init}</code>
</p>
<p>) + scaleTo
</p>
<p>Otherwise the parameter is scaled multiplicatively.
</p>
<p style="text-align: center;"><code class="reqn">v_{scaled}</code>
</p>
<p> = </p>
<p style="text-align: center;"><code class="reqn">v_{current}</code>
</p>
<p>/</p>
<p style="text-align: center;"><code class="reqn">v_{init}</code>
</p>
<p>*scaleTo
</p>
</li></ul>
</td></tr>
<tr><td><code id="uobyqaControl_+3A_scalecmax">scaleCmax</code></td>
<td>
<p>Maximum value of the scaleC to prevent overflow.</p>
</td></tr>
<tr><td><code id="uobyqaControl_+3A_scalecmin">scaleCmin</code></td>
<td>
<p>Minimum value of the scaleC to prevent underflow.</p>
</td></tr>
<tr><td><code id="uobyqaControl_+3A_scalec">scaleC</code></td>
<td>
<p>The scaling constant used with
<code>scaleType=nlmixr2</code>.  When not specified, it is based on
the type of parameter that is estimated.  The idea is to keep
the derivatives similar on a log scale to have similar
gradient sizes.  Hence parameters like log(exp(theta)) would
have a scaling factor of 1 and log(theta) would have a scaling
factor of ini_value (to scale by 1/value; ie
d/dt(log(ini_value)) = 1/ini_value or scaleC=ini_value)
</p>

<ul>
<li><p> For parameters in an exponential (ie exp(theta)) or
parameters specifying powers, boxCox or yeoJohnson
transformations , this is 1.
</p>
</li>
<li><p> For additive, proportional, lognormal error structures,
these are given by 0.5*abs(initial_estimate)
</p>
</li>
<li><p> Factorials are scaled by abs(1/digamma(initial_estimate+1))
</p>
</li>
<li><p> parameters in a log scale (ie log(theta)) are transformed
by log(abs(initial_estimate))*abs(initial_estimate)
</p>
</li></ul>

<p>These parameter scaling coefficients are chose to try to keep
similar slopes among parameters.  That is they all follow the
slopes approximately on a log-scale.
</p>
<p>While these are chosen in a logical manner, they may not always
apply.  You can specify each parameters scaling factor by this
parameter if you wish.</p>
</td></tr>
<tr><td><code id="uobyqaControl_+3A_scaleto">scaleTo</code></td>
<td>
<p>Scale the initial parameter estimate to this value.
By default this is 1.  When zero or below, no scaling is performed.</p>
</td></tr>
<tr><td><code id="uobyqaControl_+3A_rxcontrol">rxControl</code></td>
<td>
<p>'rxode2' ODE solving options during fitting, created with 'rxControl()'</p>
</td></tr>
<tr><td><code id="uobyqaControl_+3A_optexpression">optExpression</code></td>
<td>
<p>Optimize the rxode2 expression to speed up
calculation. By default this is turned on.</p>
</td></tr>
<tr><td><code id="uobyqaControl_+3A_sumprod">sumProd</code></td>
<td>
<p>Is a boolean indicating if the model should change
multiplication to high precision multiplication and sums to
high precision sums using the PreciseSums package.  By default
this is <code>FALSE</code>.</p>
</td></tr>
<tr><td><code id="uobyqaControl_+3A_literalfix">literalFix</code></td>
<td>
<p>boolean, substitute fixed population values as
literals and re-adjust ui and parameter estimates after
optimization; Default is 'TRUE'.</p>
</td></tr>
<tr><td><code id="uobyqaControl_+3A_addprop">addProp</code></td>
<td>
<p>specifies the type of additive plus proportional
errors, the one where standard deviations add (combined1) or the
type where the variances add (combined2).
</p>
<p>The combined1 error type can be described by the following equation:
</p>
<p style="text-align: center;"><code class="reqn">y = f + (a + b\times f^c) \times \varepsilon</code>
</p>

<p>The combined2 error model can be described by the following equation:
</p>
<p style="text-align: center;"><code class="reqn">y = f + \sqrt{a^2 + b^2\times f^{2\times c}} \times \varepsilon</code>
</p>

<p>Where:
</p>
<p>- y represents the observed value
</p>
<p>- f represents the predicted value
</p>
<p>- a  is the additive standard deviation
</p>
<p>- b is the proportional/power standard deviation
</p>
<p>- c is the power exponent (in the proportional case c=1)</p>
</td></tr>
<tr><td><code id="uobyqaControl_+3A_calctables">calcTables</code></td>
<td>
<p>This boolean is to determine if the foceiFit
will calculate tables. By default this is <code>TRUE</code></p>
</td></tr>
<tr><td><code id="uobyqaControl_+3A_compress">compress</code></td>
<td>
<p>Should the object have compressed items</p>
</td></tr>
<tr><td><code id="uobyqaControl_+3A_covmethod">covMethod</code></td>
<td>
<p>Method for calculating covariance.  In this
discussion, R is the Hessian matrix of the objective
function. The S matrix is the sum of individual
gradient cross-product (evaluated at the individual empirical
Bayes estimates).
</p>

<ul>
<li><p> &quot;<code>r,s</code>&quot; Uses the sandwich matrix to calculate the
covariance, that is: <code>solve(R) %*% S %*% solve(R)</code>
</p>
</li>
<li><p> &quot;<code>r</code>&quot; Uses the Hessian matrix to calculate the
covariance as <code>2 %*% solve(R)</code>
</p>
</li>
<li><p> &quot;<code>s</code>&quot; Uses the cross-product matrix to calculate the
covariance as <code>4 %*% solve(S)</code>
</p>
</li>
<li><p> &quot;&quot; Does not calculate the covariance step.
</p>
</li></ul>
</td></tr>
<tr><td><code id="uobyqaControl_+3A_adjobf">adjObf</code></td>
<td>
<p>is a boolean to indicate if the objective function
should be adjusted to be closer to NONMEM's default objective
function.  By default this is <code>TRUE</code></p>
</td></tr>
<tr><td><code id="uobyqaControl_+3A_ci">ci</code></td>
<td>
<p>Confidence level for some tables.  By default this is
0.95 or 95% confidence.</p>
</td></tr>
<tr><td><code id="uobyqaControl_+3A_sigdig">sigdig</code></td>
<td>
<p>Optimization significant digits. This controls:
</p>

<ul>
<li><p> The tolerance of the inner and outer optimization is <code>10^-sigdig</code>
</p>
</li>
<li><p> The tolerance of the ODE solvers is
<code>0.5*10^(-sigdig-2)</code>; For the sensitivity equations and
steady-state solutions the default is <code>0.5*10^(-sigdig-1.5)</code>
(sensitivity changes only applicable for liblsoda)
</p>
</li>
<li><p> The tolerance of the boundary check is <code>5 * 10 ^ (-sigdig + 1)</code>
</p>
</li></ul>
</td></tr>
<tr><td><code id="uobyqaControl_+3A_sigdigtable">sigdigTable</code></td>
<td>
<p>Significant digits in the final output table.
If not specified, then it matches the significant digits in the
'sigdig' optimization algorithm.  If 'sigdig' is NULL, use 3.</p>
</td></tr>
<tr><td><code id="uobyqaControl_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <code><a href="nlmixr2est.html#topic+uobyqaControl">nlmixr2est::uobyqaControl()</a></code>.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>uobyqa control structure
</p>


<h3>Author(s)</h3>

<p>Matthew L. Fidler
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
# A logit regression example with emax model

dsn &lt;- data.frame(i=1:1000)
dsn$time &lt;- exp(rnorm(1000))
dsn$DV=rbinom(1000,1,exp(-1+dsn$time)/(1+exp(-1+dsn$time)))

mod &lt;- function() {
 ini({
   E0 &lt;- 0.5
   Em &lt;- 0.5
   E50 &lt;- 2
   g &lt;- fix(2)
 })
 model({
   v &lt;- E0+Em*time^g/(E50^g+time^g)
   ll(bin) ~ DV * v - log(1 + exp(v))
 })
}

fit2 &lt;- nlmixr(mod, dsn, est="uobyqa")

print(fit2)

# you can also get the nlm output with fit2$nlm

fit2$uobyqa

# The nlm control has been modified slightly to include
# extra components and name the parameters

</code></pre>

<hr>
<h2 id='vpcCens'>VPC based on ui model</h2><span id='topic+vpcCens'></span>

<h3>Description</h3>

<p>VPC based on ui model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vpcCens(..., cens = TRUE, idv = "time")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vpcCens_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <code><a href="nlmixr2plot.html#topic+vpcPlot">nlmixr2plot::vpcCens()</a></code>.</p>
</td></tr>
<tr><td><code id="vpcCens_+3A_cens">cens</code></td>
<td>
<p>is a boolean to show if this is a censoring plot or
not.  When <code>cens=TRUE</code> this is actually a censoring vpc plot
(with <code>vpcCens()</code> and <code>vpcCensTad()</code>).  When <code>cens=FALSE</code> this is
traditional VPC plot (<code>vpcPlot()</code> and <code>vpcPlotTad()</code>).</p>
</td></tr>
<tr><td><code id="vpcCens_+3A_idv">idv</code></td>
<td>
<p>Name of independent variable. For <code>vpcPlot()</code> and
<code>vpcCens()</code> the default is <code>"time"</code> for <code>vpcPlotTad()</code> and
<code>vpcCensTad()</code> this is <code>"tad"</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>Simulated dataset (invisibly)
</p>


<h3>Author(s)</h3>

<p>Matthew L. Fidler
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
one.cmt &lt;- function() {
 ini({
   tka &lt;- 0.45; label("Ka")
   tcl &lt;- log(c(0, 2.7, 100)); label("Cl")
   tv &lt;- 3.45; label("V")
   eta.ka ~ 0.6
   eta.cl ~ 0.3
   eta.v ~ 0.1
   add.sd &lt;- 0.7; label("Additive residual error")
 })
 model({
   ka &lt;- exp(tka + eta.ka)
   cl &lt;- exp(tcl + eta.cl)
   v &lt;- exp(tv + eta.v)
   linCmt() ~ add(add.sd)
 })
}

fit &lt;-
  nlmixr2est::nlmixr(
    one.cmt,
    data = nlmixr2data::theo_sd,
    est = "saem",
    control = list(print = 0)
  )

vpcPlot(fit)

</code></pre>

<hr>
<h2 id='vpcCensTad'>VPC based on ui model</h2><span id='topic+vpcCensTad'></span>

<h3>Description</h3>

<p>VPC based on ui model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vpcCensTad(..., cens = TRUE, idv = "tad")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vpcCensTad_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <code><a href="nlmixr2plot.html#topic+vpcPlot">nlmixr2plot::vpcCensTad()</a></code>.</p>
</td></tr>
<tr><td><code id="vpcCensTad_+3A_cens">cens</code></td>
<td>
<p>is a boolean to show if this is a censoring plot or
not.  When <code>cens=TRUE</code> this is actually a censoring vpc plot
(with <code>vpcCens()</code> and <code>vpcCensTad()</code>).  When <code>cens=FALSE</code> this is
traditional VPC plot (<code>vpcPlot()</code> and <code>vpcPlotTad()</code>).</p>
</td></tr>
<tr><td><code id="vpcCensTad_+3A_idv">idv</code></td>
<td>
<p>Name of independent variable. For <code>vpcPlot()</code> and
<code>vpcCens()</code> the default is <code>"time"</code> for <code>vpcPlotTad()</code> and
<code>vpcCensTad()</code> this is <code>"tad"</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>Simulated dataset (invisibly)
</p>


<h3>Author(s)</h3>

<p>Matthew L. Fidler
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
one.cmt &lt;- function() {
 ini({
   tka &lt;- 0.45; label("Ka")
   tcl &lt;- log(c(0, 2.7, 100)); label("Cl")
   tv &lt;- 3.45; label("V")
   eta.ka ~ 0.6
   eta.cl ~ 0.3
   eta.v ~ 0.1
   add.sd &lt;- 0.7; label("Additive residual error")
 })
 model({
   ka &lt;- exp(tka + eta.ka)
   cl &lt;- exp(tcl + eta.cl)
   v &lt;- exp(tv + eta.v)
   linCmt() ~ add(add.sd)
 })
}

fit &lt;-
  nlmixr2est::nlmixr(
    one.cmt,
    data = nlmixr2data::theo_sd,
    est = "saem",
    control = list(print = 0)
  )

vpcPlot(fit)

</code></pre>

<hr>
<h2 id='vpcPlot'>VPC based on ui model</h2><span id='topic+vpcPlot'></span>

<h3>Description</h3>

<p>VPC based on ui model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vpcPlot(
  fit,
  data = NULL,
  n = 300,
  bins = "jenks",
  n_bins = "auto",
  bin_mid = "mean",
  show = NULL,
  stratify = NULL,
  pred_corr = FALSE,
  pred_corr_lower_bnd = 0,
  pi = c(0.05, 0.95),
  ci = c(0.05, 0.95),
  uloq = fit$dataUloq,
  lloq = fit$dataLloq,
  log_y = FALSE,
  log_y_min = 0.001,
  xlab = NULL,
  ylab = NULL,
  title = NULL,
  smooth = TRUE,
  vpc_theme = NULL,
  facet = "wrap",
  scales = "fixed",
  labeller = NULL,
  vpcdb = FALSE,
  verbose = FALSE,
  ...,
  seed = 1009,
  idv = "time",
  cens = FALSE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vpcPlot_+3A_fit">fit</code></td>
<td>
<p>nlmixr2 fit object</p>
</td></tr>
<tr><td><code id="vpcPlot_+3A_data">data</code></td>
<td>
<p>this is the data to use to augment the VPC fit.  By
default is the fitted data, (can be retrieved by
<code><a href="nlme.html#topic+getData">getData</a></code>), but it can be changed by specifying
this argument.</p>
</td></tr>
<tr><td><code id="vpcPlot_+3A_n">n</code></td>
<td>
<p>Number of VPC simulations</p>
</td></tr>
<tr><td><code id="vpcPlot_+3A_bins">bins</code></td>
<td>
<p>either &quot;density&quot;, &quot;time&quot;, or &quot;data&quot;, &quot;none&quot;, or one of the approaches available in classInterval() such as &quot;jenks&quot; (default) or &quot;pretty&quot;, or a numeric vector specifying the bin separators.</p>
</td></tr>
<tr><td><code id="vpcPlot_+3A_n_bins">n_bins</code></td>
<td>
<p>when using the &quot;auto&quot; binning method, what number of bins to aim for</p>
</td></tr>
<tr><td><code id="vpcPlot_+3A_bin_mid">bin_mid</code></td>
<td>
<p>either &quot;mean&quot; for the mean of all timepoints (default) or &quot;middle&quot; to use the average of the bin boundaries.</p>
</td></tr>
<tr><td><code id="vpcPlot_+3A_show">show</code></td>
<td>
<p>what to show in VPC (obs_dv, obs_ci, pi, pi_as_area, pi_ci, obs_median, sim_median, sim_median_ci)</p>
</td></tr>
<tr><td><code id="vpcPlot_+3A_stratify">stratify</code></td>
<td>
<p>character vector of stratification variables. Only 1 or 2 stratification variables can be supplied.</p>
</td></tr>
<tr><td><code id="vpcPlot_+3A_pred_corr">pred_corr</code></td>
<td>
<p>perform prediction-correction?</p>
</td></tr>
<tr><td><code id="vpcPlot_+3A_pred_corr_lower_bnd">pred_corr_lower_bnd</code></td>
<td>
<p>lower bound for the prediction-correction</p>
</td></tr>
<tr><td><code id="vpcPlot_+3A_pi">pi</code></td>
<td>
<p>simulated prediction interval to plot. Default is c(0.05, 0.95),</p>
</td></tr>
<tr><td><code id="vpcPlot_+3A_ci">ci</code></td>
<td>
<p>confidence interval to plot. Default is (0.05, 0.95)</p>
</td></tr>
<tr><td><code id="vpcPlot_+3A_uloq">uloq</code></td>
<td>
<p>Number or NULL indicating upper limit of quantification. Default is NULL.</p>
</td></tr>
<tr><td><code id="vpcPlot_+3A_lloq">lloq</code></td>
<td>
<p>Number or NULL indicating lower limit of quantification. Default is NULL.</p>
</td></tr>
<tr><td><code id="vpcPlot_+3A_log_y">log_y</code></td>
<td>
<p>Boolean indicting whether y-axis should be shown as logarithmic. Default is FALSE.</p>
</td></tr>
<tr><td><code id="vpcPlot_+3A_log_y_min">log_y_min</code></td>
<td>
<p>minimal value when using log_y argument. Default is 1e-3.</p>
</td></tr>
<tr><td><code id="vpcPlot_+3A_xlab">xlab</code></td>
<td>
<p>label for x axis</p>
</td></tr>
<tr><td><code id="vpcPlot_+3A_ylab">ylab</code></td>
<td>
<p>label for y axis</p>
</td></tr>
<tr><td><code id="vpcPlot_+3A_title">title</code></td>
<td>
<p>title</p>
</td></tr>
<tr><td><code id="vpcPlot_+3A_smooth">smooth</code></td>
<td>
<p>&quot;smooth&quot; the VPC (connect bin midpoints) or show bins as rectangular boxes. Default is TRUE.</p>
</td></tr>
<tr><td><code id="vpcPlot_+3A_vpc_theme">vpc_theme</code></td>
<td>
<p>theme to be used in VPC. Expects list of class vpc_theme created with function vpc_theme()</p>
</td></tr>
<tr><td><code id="vpcPlot_+3A_facet">facet</code></td>
<td>
<p>either &quot;wrap&quot;, &quot;columns&quot;, or &quot;rows&quot;</p>
</td></tr>
<tr><td><code id="vpcPlot_+3A_scales">scales</code></td>
<td>
<p>either &quot;fixed&quot; (default), &quot;free_y&quot;, &quot;free_x&quot; or &quot;free&quot;</p>
</td></tr>
<tr><td><code id="vpcPlot_+3A_labeller">labeller</code></td>
<td>
<p>ggplot2 labeller function to be passed to underlying ggplot object</p>
</td></tr>
<tr><td><code id="vpcPlot_+3A_vpcdb">vpcdb</code></td>
<td>
<p>Boolean whether to return the underlying vpcdb rather than the plot</p>
</td></tr>
<tr><td><code id="vpcPlot_+3A_verbose">verbose</code></td>
<td>
<p>show debugging information (TRUE or FALSE)</p>
</td></tr>
<tr><td><code id="vpcPlot_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <code><a href="nlmixr2plot.html#topic+vpcPlot">nlmixr2plot::vpcPlot()</a></code>.</p>
</td></tr>
<tr><td><code id="vpcPlot_+3A_seed">seed</code></td>
<td>
<p>an object specifying if and how the random number
generator should be initialized</p>
</td></tr>
<tr><td><code id="vpcPlot_+3A_idv">idv</code></td>
<td>
<p>Name of independent variable. For <code>vpcPlot()</code> and
<code>vpcCens()</code> the default is <code>"time"</code> for <code>vpcPlotTad()</code> and
<code>vpcCensTad()</code> this is <code>"tad"</code></p>
</td></tr>
<tr><td><code id="vpcPlot_+3A_cens">cens</code></td>
<td>
<p>is a boolean to show if this is a censoring plot or
not.  When <code>cens=TRUE</code> this is actually a censoring vpc plot
(with <code>vpcCens()</code> and <code>vpcCensTad()</code>).  When <code>cens=FALSE</code> this is
traditional VPC plot (<code>vpcPlot()</code> and <code>vpcPlotTad()</code>).</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Simulated dataset (invisibly)
</p>


<h3>Author(s)</h3>

<p>Matthew L. Fidler
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
one.cmt &lt;- function() {
 ini({
   tka &lt;- 0.45; label("Ka")
   tcl &lt;- log(c(0, 2.7, 100)); label("Cl")
   tv &lt;- 3.45; label("V")
   eta.ka ~ 0.6
   eta.cl ~ 0.3
   eta.v ~ 0.1
   add.sd &lt;- 0.7; label("Additive residual error")
 })
 model({
   ka &lt;- exp(tka + eta.ka)
   cl &lt;- exp(tcl + eta.cl)
   v &lt;- exp(tv + eta.v)
   linCmt() ~ add(add.sd)
 })
}

fit &lt;-
  nlmixr2est::nlmixr(
    one.cmt,
    data = nlmixr2data::theo_sd,
    est = "saem",
    control = list(print = 0)
  )

vpcPlot(fit)

</code></pre>

<hr>
<h2 id='vpcPlotTad'>VPC based on ui model</h2><span id='topic+vpcPlotTad'></span>

<h3>Description</h3>

<p>VPC based on ui model
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vpcPlotTad(..., idv = "tad")
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vpcPlotTad_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <code><a href="nlmixr2plot.html#topic+vpcPlot">nlmixr2plot::vpcPlotTad()</a></code>.</p>
</td></tr>
<tr><td><code id="vpcPlotTad_+3A_idv">idv</code></td>
<td>
<p>Name of independent variable. For <code>vpcPlot()</code> and
<code>vpcCens()</code> the default is <code>"time"</code> for <code>vpcPlotTad()</code> and
<code>vpcCensTad()</code> this is <code>"tad"</code></p>
</td></tr>
</table>


<h3>Value</h3>

<p>Simulated dataset (invisibly)
</p>


<h3>Author(s)</h3>

<p>Matthew L. Fidler
</p>


<h3>Examples</h3>

<pre><code class='language-R'>
one.cmt &lt;- function() {
 ini({
   tka &lt;- 0.45; label("Ka")
   tcl &lt;- log(c(0, 2.7, 100)); label("Cl")
   tv &lt;- 3.45; label("V")
   eta.ka ~ 0.6
   eta.cl ~ 0.3
   eta.v ~ 0.1
   add.sd &lt;- 0.7; label("Additive residual error")
 })
 model({
   ka &lt;- exp(tka + eta.ka)
   cl &lt;- exp(tcl + eta.cl)
   v &lt;- exp(tv + eta.v)
   linCmt() ~ add(add.sd)
 })
}

fit &lt;-
  nlmixr2est::nlmixr(
    one.cmt,
    data = nlmixr2data::theo_sd,
    est = "saem",
    control = list(print = 0)
  )

vpcPlot(fit)

</code></pre>

<hr>
<h2 id='vpcSim'>VPC simulation</h2><span id='topic+vpcSim'></span>

<h3>Description</h3>

<p>VPC simulation
</p>


<h3>Usage</h3>

<pre><code class='language-R'>vpcSim(
  object,
  ...,
  keep = NULL,
  n = 300,
  pred = FALSE,
  seed = 1009,
  nretry = 50,
  minN = 10,
  normRelated = TRUE
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="vpcSim_+3A_object">object</code></td>
<td>
<p>This is the nlmixr2 fit object</p>
</td></tr>
<tr><td><code id="vpcSim_+3A_...">...</code></td>
<td>
<p>Additional arguments passed to <code><a href="nlmixr2est.html#topic+vpcSim">nlmixr2est::vpcSim()</a></code>.</p>
</td></tr>
<tr><td><code id="vpcSim_+3A_keep">keep</code></td>
<td>
<p>Column names to keep in the output simulated dataset</p>
</td></tr>
<tr><td><code id="vpcSim_+3A_n">n</code></td>
<td>
<p>Number of simulations</p>
</td></tr>
<tr><td><code id="vpcSim_+3A_pred">pred</code></td>
<td>
<p>Should predictions be added to the simulation</p>
</td></tr>
<tr><td><code id="vpcSim_+3A_seed">seed</code></td>
<td>
<p>Seed to set for the VPC simulation</p>
</td></tr>
<tr><td><code id="vpcSim_+3A_nretry">nretry</code></td>
<td>
<p>Number of times to retry the simulation if there is
NA values in the simulation</p>
</td></tr>
<tr><td><code id="vpcSim_+3A_minn">minN</code></td>
<td>
<p>With retries, the minimum number of studies to
restimulate (by default 10)</p>
</td></tr>
<tr><td><code id="vpcSim_+3A_normrelated">normRelated</code></td>
<td>
<p>should the VPC style simulation be for normal
related variables only</p>
</td></tr>
</table>


<h3>Value</h3>

<p>data frame of the VPC simulation
</p>


<h3>Author(s)</h3>

<p>Matthew L. Fidler
</p>


<h3>Examples</h3>

<pre><code class='language-R'>

if (rxode2parse::.linCmtSens()) {

one.cmt &lt;- function() {
 ini({
   ## You may label each parameter with a comment
   tka &lt;- 0.45 # Log Ka
   tcl &lt;- log(c(0, 2.7, 100)) # Log Cl
   ## This works with interactive models
   ## You may also label the preceding line with label("label text")
   tv &lt;- 3.45; label("log V")
   ## the label("Label name") works with all models
   eta.ka ~ 0.6
   eta.cl ~ 0.3
   eta.v ~ 0.1
   add.sd &lt;- 0.7
 })
 model({
   ka &lt;- exp(tka + eta.ka)
   cl &lt;- exp(tcl + eta.cl)
   v &lt;- exp(tv + eta.v)
   linCmt() ~ add(add.sd)
 })
}

fit &lt;- nlmixr(one.cmt, theo_sd, est="focei")

head(vpcSim(fit, pred=TRUE))

}


</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
