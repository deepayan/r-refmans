<!DOCTYPE html><html lang="en"><head><title>Help for package sweater</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism.min.css" rel="stylesheet" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R-nav.css" />
</head><body><div class="container"><nav class="package" aria-label="Topic Navigation">
<div class="dropdown-menu">
<h1>Package {sweater}</h1>
<h2>Contents</h2>
<ul class="menu">
<li><a href='#calculate_es'><p>Calculate the effect size of a query</p></a></li>
<li><a href='#ect'><p>Embedding Coherence Test</p></a></li>
<li><a href='#ect_es'><p>Calculate the Spearman Coefficient of an ECT result</p></a></li>
<li><a href='#glove_math'><p>A subset of the pretrained GLoVE word vectors</p></a></li>
<li><a href='#googlenews'><p>A subset of the pretrained word2vec word vectors</p></a></li>
<li><a href='#mac'><p>Mean average cosine similarity</p></a></li>
<li><a href='#mac_es'><p>Calculation of MAC Effect Size</p></a></li>
<li><a href='#nas'><p>Calculate Normalized Association Score</p></a></li>
<li><a href='#plot_bias'><p>Visualize the bias of words in S</p></a></li>
<li><a href='#plot_ect'><p>Plot an ECT result on a two-dimensional plane</p></a></li>
<li><a href='#query'><p>A common interface for making query</p></a></li>
<li><a href='#read_word2vec'><p>A helper function for reading word2vec format</p></a></li>
<li><a href='#rnd'><p>Relative Norm Distance</p></a></li>
<li><a href='#rnd_es'><p>Calculation of sum of all relative norm distances</p></a></li>
<li><a href='#rnsb'><p>Relative Negative Sentiment Bias</p></a></li>
<li><a href='#rnsb_es'><p>Calculation the Kullback-Leibler divergence</p></a></li>
<li><a href='#semaxis'><p>Characterise word semantics using the SemAxis framework</p></a></li>
<li><a href='#small_reddit'><p>A subset of the pretrained word2vec word vectors on Reddit</p></a></li>
<li><a href='#weat'><p>Speedy Word Embedding Association Test</p></a></li>
<li><a href='#weat_es'><p>Calculation of WEAT effect size</p></a></li>
<li><a href='#weat_exact'><p>Test of significance for WEAT</p></a></li>
</ul>
</div>
<hr>
</nav>
<main>
<table role='presentation'>
<tr>
<td>Title:</td>
<td>Speedy Word Embedding Association Test and Extras Using R</td>
</tr>
<tr>
<td>Version:</td>
<td>0.1.8</td>
</tr>
<tr>
<td>Description:</td>
<td>Conduct various tests for evaluating implicit biases in word embeddings: Word Embedding Association Test (Caliskan et al., 2017), &lt;<a href="https://doi.org/10.1126%2Fscience.aal4230">doi:10.1126/science.aal4230</a>&gt;, Relative Norm Distance (Garg et al., 2018), &lt;<a href="https://doi.org/10.1073%2Fpnas.1720347115">doi:10.1073/pnas.1720347115</a>&gt;, Mean Average Cosine Similarity (Mazini et al., 2019) &lt;<a href="https://doi.org/10.48550/arXiv.1904.04047">doi:10.48550/arXiv.1904.04047</a>&gt;, SemAxis (An et al., 2018) &lt;<a href="https://doi.org/10.48550/arXiv.1806.05521">doi:10.48550/arXiv.1806.05521</a>&gt;, Relative Negative Sentiment Bias (Sweeney &amp; Najafian, 2019) &lt;<a href="https://doi.org/10.18653%2Fv1%2FP19-1162">doi:10.18653/v1/P19-1162</a>&gt;, and Embedding Coherence Test (Dev &amp; Phillips, 2019) &lt;<a href="https://doi.org/10.48550/arXiv.1901.07656">doi:10.48550/arXiv.1901.07656</a>&gt;.</td>
</tr>
<tr>
<td>License:</td>
<td><a href="https://www.r-project.org/Licenses/GPL-3">GPL (&ge; 3)</a></td>
</tr>
<tr>
<td>Encoding:</td>
<td>UTF-8</td>
</tr>
<tr>
<td>LazyData:</td>
<td>true</td>
</tr>
<tr>
<td>RoxygenNote:</td>
<td>7.2.3</td>
</tr>
<tr>
<td>URL:</td>
<td><a href="https://github.com/gesistsa/sweater">https://github.com/gesistsa/sweater</a></td>
</tr>
<tr>
<td>BugReports:</td>
<td><a href="https://github.com/gesistsa/sweater/issues">https://github.com/gesistsa/sweater/issues</a></td>
</tr>
<tr>
<td>LinkingTo:</td>
<td>Rcpp</td>
</tr>
<tr>
<td>Imports:</td>
<td>Rcpp, purrr, quanteda, LiblineaR, proxy, data.table, cli,
combinat</td>
</tr>
<tr>
<td>Suggests:</td>
<td>covr, testthat (&ge; 3.0.0)</td>
</tr>
<tr>
<td>Config/testthat/edition:</td>
<td>3</td>
</tr>
<tr>
<td>Depends:</td>
<td>R (&ge; 3.5)</td>
</tr>
<tr>
<td>NeedsCompilation:</td>
<td>yes</td>
</tr>
<tr>
<td>Packaged:</td>
<td>2023-11-07 15:44:00 UTC; chainsawriot</td>
</tr>
<tr>
<td>Author:</td>
<td>Chung-hong Chan <a href="https://orcid.org/0000-0002-6232-7530"><img alt="ORCID iD"  src="https://cloud.R-project.org/web/orcid.svg" style="width:16px; height:16px; margin-left:4px; margin-right:4px; vertical-align:middle" /></a>
    [aut, cre]</td>
</tr>
<tr>
<td>Maintainer:</td>
<td>Chung-hong Chan &lt;chainsawtiney@gmail.com&gt;</td>
</tr>
<tr>
<td>Repository:</td>
<td>CRAN</td>
</tr>
<tr>
<td>Date/Publication:</td>
<td>2023-11-07 16:00:02 UTC</td>
</tr>
</table>
<hr>
<h2 id='calculate_es'>Calculate the effect size of a query</h2><span id='topic+calculate_es'></span>

<h3>Description</h3>

<p>This function calculates the effect of a query.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>calculate_es(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="calculate_es_+3A_x">x</code></td>
<td>
<p>an S3 object returned from a query, either by the function <code><a href="#topic+query">query()</a></code> or underlying functions such as <code><a href="#topic+mac">mac()</a></code></p>
</td></tr>
<tr><td><code id="calculate_es_+3A_...">...</code></td>
<td>
<p>additional parameters for the effect size functions
</p>

<ul>
<li> <p><code>r</code> for <code>weat</code>: a boolean to denote whether convert the effect size to biserial correlation coefficient.
</p>
</li>
<li> <p><code>standardize</code> for <code>weat</code>: a boolean to denote whether to correct the difference by the standard division. The standardized version can be interpreted the same way as Cohen's d.
</p>
</li></ul>
</td></tr>
</table>


<h3>Details</h3>

<p>The following methods are supported.
</p>

<ul>
<li> <p><code>mac</code> mean cosine distance value. The value makes sense only for comparison (e.g. before and after debiasing). But a lower value indicates greater association between the target words and the attribute words.
</p>
</li>
<li> <p><code>rnd</code> sum of all relative norm distances. It equals to zero when there is no bias.
</p>
</li>
<li> <p><code>rnsb</code> Kullback-Leibler divergence of the predicted negative probabilities, P, from the uniform distribution. A lower value indicates less bias.
</p>
</li>
<li> <p><code>ect</code> Spearman Coefficient of an Embedding Coherence Test. The value ranges from -1 to +1 and a larger value indicates less bias.
</p>
</li>
<li> <p><code>weat</code> The standardized effect size (default) can be interpreted the same way as Cohen's D.
</p>
</li></ul>



<h3>Value</h3>

<p>effect size
</p>


<h3>References</h3>

<p>Caliskan, A., Bryson, J. J., &amp; Narayanan, A. (2017). Semantics derived automatically from language corpora contain human-like biases. Science, 356(6334), 183-186. <a href="https://doi.org/10.1126/science.aal4230">doi:10.1126/science.aal4230</a>
</p>
<p>Dev, S., &amp; Phillips, J. (2019, April). <a href="https://proceedings.mlr.press/v89/dev19a.html">Attenuating bias in word vectors.</a> In The 22nd International Conference on Artificial Intelligence and Statistics (pp. 879-887). PMLR.
</p>
<p>Garg, N., Schiebinger, L., Jurafsky, D., &amp; Zou, J. (2018). Word embeddings quantify 100 years of gender and ethnic stereotypes. Proceedings of the National Academy of Sciences, 115(16), E3635-E3644. <a href="https://doi.org/10.1073/pnas.1720347115">doi:10.1073/pnas.1720347115</a>
</p>
<p>Manzini, T., Lim, Y. C., Tsvetkov, Y., &amp; Black, A. W. (2019). <a href="https://arxiv.org/abs/1904.04047">Black is to criminal as caucasian is to police: Detecting and removing multiclass bias in word embeddings.</a> arXiv preprint arXiv:1904.04047.
</p>
<p>Sweeney, C., &amp; Najafian, M. (2019, July). <a href="https://aclanthology.org/P19-1162/">A transparent framework for evaluating unintended demographic bias in word embeddings.</a> In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (pp. 1662-1667).
</p>


<h3>See Also</h3>

<p><code><a href="#topic+weat_es">weat_es()</a></code>, <code><a href="#topic+mac_es">mac_es()</a></code>, <code><a href="#topic+rnd_es">rnd_es()</a></code>, <code><a href="#topic+rnsb_es">rnsb_es()</a></code>, <code><a href="#topic+ect_es">ect_es()</a></code>
</p>

<hr>
<h2 id='ect'>Embedding Coherence Test</h2><span id='topic+ect'></span>

<h3>Description</h3>

<p>This function estimate the Embedding Coherence Test (ECT) of word embeddings (Dev &amp; Philips, 2019). If possible, please use <code><a href="#topic+query">query()</a></code> instead.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ect(w, S_words, A_words, B_words, verbose = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ect_+3A_w">w</code></td>
<td>
<p>a numeric matrix of word embeddings, e.g. from <code><a href="#topic+read_word2vec">read_word2vec()</a></code></p>
</td></tr>
<tr><td><code id="ect_+3A_s_words">S_words</code></td>
<td>
<p>a character vector of the first set of target words. In an example of studying gender stereotype, it can include occupations such as programmer, engineer, scientists...</p>
</td></tr>
<tr><td><code id="ect_+3A_a_words">A_words</code></td>
<td>
<p>a character vector of the first set of attribute words. In an example of studying gender stereotype, it can include words such as man, male, he, his.</p>
</td></tr>
<tr><td><code id="ect_+3A_b_words">B_words</code></td>
<td>
<p>a character vector of the second set of attribute words. In an example of studying gender stereotype, it can include words such as woman, female, she, her.</p>
</td></tr>
<tr><td><code id="ect_+3A_verbose">verbose</code></td>
<td>
<p>logical, whether to display information</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with class <code>"ect"</code> containing the following components:
</p>

<ul>
<li> <p><code style="white-space: pre;">&#8288;$A_words&#8288;</code> the input A_words
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$B_words&#8288;</code> the input B_words
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$S_words&#8288;</code> the input S_words
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$u_a&#8288;</code> Cosine similarity between each word vector of S_words and average vector of A_words
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$u_b&#8288;</code> Cosine similarity between each word vector of S_words and average vector of B_words
</p>
</li></ul>



<h3>References</h3>

<p>Dev, S., &amp; Phillips, J. (2019, April). <a href="https://proceedings.mlr.press/v89/dev19a.html">Attenuating bias in word vectors.</a> In The 22nd International Conference on Artificial Intelligence and Statistics (pp. 879-887). PMLR.
</p>


<h3>See Also</h3>

<p><code><a href="#topic+ect_es">ect_es()</a></code> can be used to obtain the effect size of the test.
<code><a href="#topic+plot_ect">plot_ect()</a></code> can be used to visualize the result.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(googlenews)
S1 &lt;- c("janitor", "statistician", "midwife", "bailiff", "auctioneer",
"photographer", "geologist", "shoemaker", "athlete", "cashier", "dancer",
"housekeeper", "accountant", "physicist", "gardener", "dentist", "weaver",
"blacksmith", "psychologist", "supervisor", "mathematician", "surveyor",
"tailor", "designer", "economist", "mechanic", "laborer", "postmaster",
"broker", "chemist", "librarian", "attendant", "clerical", "musician",
"porter", "scientist", "carpenter", "sailor", "instructor", "sheriff",
"pilot", "inspector", "mason", "baker", "administrator", "architect",
"collector", "operator", "surgeon", "driver", "painter", "conductor",
"nurse", "cook", "engineer", "retired", "sales", "lawyer", "clergy",
"physician", "farmer", "clerk", "manager", "guard", "artist", "smith",
"official", "police", "doctor", "professor", "student", "judge",
"teacher", "author", "secretary", "soldier")
A1 &lt;- c("he", "son", "his", "him", "father", "man", "boy", "himself",
"male", "brother", "sons", "fathers", "men", "boys", "males", "brothers",
"uncle", "uncles", "nephew", "nephews")
B1 &lt;- c("she", "daughter", "hers", "her", "mother", "woman", "girl",
"herself", "female", "sister", "daughters", "mothers", "women", "girls",
"females", "sisters", "aunt", "aunts", "niece", "nieces")
garg_f1 &lt;- ect(googlenews, S1, A1, B1)
plot_ect(garg_f1)
</code></pre>

<hr>
<h2 id='ect_es'>Calculate the Spearman Coefficient of an ECT result</h2><span id='topic+ect_es'></span>

<h3>Description</h3>

<p>This functions calculates the Spearman Coefficient of an Embedding Coherence Test. The value ranges from -1 to +1 and a larger value indicates less bias. If possible, please use <code><a href="#topic+calculate_es">calculate_es()</a></code> instead.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>ect_es(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="ect_es_+3A_x">x</code></td>
<td>
<p>an ect object from the <code><a href="#topic+ect">ect()</a></code> function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>Spearman Coefficient
</p>


<h3>References</h3>

<p>Dev, S., &amp; Phillips, J. (2019, April). <a href="https://proceedings.mlr.press/v89/dev19a.html">Attenuating bias in word vectors.</a> In The 22nd International Conference on Artificial Intelligence and Statistics (pp. 879-887). PMLR.
</p>

<hr>
<h2 id='glove_math'>A subset of the pretrained GLoVE word vectors</h2><span id='topic+glove_math'></span>

<h3>Description</h3>

<p>This is a subset of the original pretrained GLoVE word vectors provided by Pennington et al (2017). The same word vectors were used in Caliskan et al. (2017) to study biases.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>glove_math
</code></pre>


<h3>Format</h3>

<p>An object of class <code>matrix</code> (inherits from <code>array</code>) with 32 rows and 300 columns.
</p>


<h3>References</h3>

<p>Pennington, J., Socher, R., &amp; Manning, C. D. (2014, October). <a href="https://aclanthology.org/D14-1162/">Glove: Global vectors for word representation.</a> In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP) (pp. 1532-1543).
</p>
<p>Caliskan, A., Bryson, J. J., &amp; Narayanan, A. (2017). Semantics derived automatically from language corpora contain human-like biases. Science, 356(6334), 183-186. <a href="https://doi.org/10.1126/science.aal4230">doi:10.1126/science.aal4230</a>
</p>

<hr>
<h2 id='googlenews'>A subset of the pretrained word2vec word vectors</h2><span id='topic+googlenews'></span>

<h3>Description</h3>

<p>This is a subset of the original pretrained word2vec word vectors trained on Google News. The same word vectors were used in Garg et al. (2018) to study biases.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>googlenews
</code></pre>


<h3>Format</h3>

<p>An object of class <code>matrix</code> (inherits from <code>array</code>) with 116 rows and 300 columns.
</p>


<h3>References</h3>

<p>Garg, N., Schiebinger, L., Jurafsky, D., &amp; Zou, J. (2018). Word embeddings quantify 100 years of gender and ethnic stereotypes. Proceedings of the National Academy of Sciences, 115(16), E3635-E3644. <a href="https://doi.org/10.1073/pnas.1720347115">doi:10.1073/pnas.1720347115</a>
</p>

<hr>
<h2 id='mac'>Mean average cosine similarity</h2><span id='topic+mac'></span>

<h3>Description</h3>

<p>This function calculates the mean average cosine similarity (MAC) score proposed in Manzini et al (2019). If possible, please use <code><a href="#topic+query">query()</a></code> instead.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mac(w, S_words, A_words, verbose = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mac_+3A_w">w</code></td>
<td>
<p>a numeric matrix of word embeddings, e.g. from <code><a href="#topic+read_word2vec">read_word2vec()</a></code></p>
</td></tr>
<tr><td><code id="mac_+3A_s_words">S_words</code></td>
<td>
<p>a character vector of the first set of target words. In an example of studying gender stereotype, it can include occupations such as programmer, engineer, scientists...</p>
</td></tr>
<tr><td><code id="mac_+3A_a_words">A_words</code></td>
<td>
<p>a character vector of the first set of attribute words. In an example of studying gender stereotype, it can include words such as man, male, he, his.</p>
</td></tr>
<tr><td><code id="mac_+3A_verbose">verbose</code></td>
<td>
<p>logical, whether to display information</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with class <code>"mac"</code> containing the following components:
</p>

<ul>
<li> <p><code style="white-space: pre;">&#8288;$P&#8288;</code> a vector of cosine similarity values for every word in S_words
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$S_words&#8288;</code> the input S_words
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$A_words&#8288;</code> the input A_words
<code><a href="#topic+mac_es">mac_es()</a></code> can be used to obtain the effect size of the test.
</p>
</li></ul>



<h3>References</h3>

<p>Manzini, T., Lim, Y. C., Tsvetkov, Y., &amp; Black, A. W. (2019). <a href="https://arxiv.org/abs/1904.04047">Black is to criminal as caucasian is to police: Detecting and removing multiclass bias in word embeddings.</a> arXiv preprint arXiv:1904.04047.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(googlenews)
S1 &lt;- c("janitor", "statistician", "midwife", "bailiff", "auctioneer",
"photographer", "geologist", "shoemaker", "athlete", "cashier", "dancer",
"housekeeper", "accountant", "physicist", "gardener", "dentist", "weaver",
"blacksmith", "psychologist", "supervisor", "mathematician", "surveyor",
"tailor", "designer", "economist", "mechanic", "laborer", "postmaster",
"broker", "chemist", "librarian", "attendant", "clerical", "musician",
"porter", "scientist", "carpenter", "sailor", "instructor", "sheriff",
"pilot", "inspector", "mason", "baker", "administrator", "architect",
"collector", "operator", "surgeon", "driver", "painter", "conductor",
"nurse", "cook", "engineer", "retired", "sales", "lawyer", "clergy",
"physician", "farmer", "clerk", "manager", "guard", "artist", "smith",
"official", "police", "doctor", "professor", "student", "judge", "teacher",
"author", "secretary", "soldier")
A1 &lt;- c("he", "son", "his", "him", "father", "man", "boy", "himself",
"male", "brother", "sons", "fathers", "men", "boys", "males", "brothers",
"uncle", "uncles", "nephew", "nephews")
x &lt;- mac(googlenews, S1, A1)
x$P
</code></pre>

<hr>
<h2 id='mac_es'>Calculation of MAC Effect Size</h2><span id='topic+mac_es'></span>

<h3>Description</h3>

<p>This function calculates the mean of cosine distance values. If possible, please use <code><a href="#topic+calculate_es">calculate_es()</a></code> instead.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>mac_es(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="mac_es_+3A_x">x</code></td>
<td>
<p>an object from the function <a href="#topic+mac">mac</a></p>
</td></tr>
</table>


<h3>Value</h3>

<p>Mean of all cosine similarity values
</p>


<h3>Author(s)</h3>

<p>Chung-hong Chan
</p>


<h3>References</h3>

<p>Manzini, T., Lim, Y. C., Tsvetkov, Y., &amp; Black, A. W. (2019). <a href="https://arxiv.org/abs/1904.04047">Black is to criminal as caucasian is to police: Detecting and removing multiclass bias in word embeddings.</a> arXiv preprint arXiv:1904.04047.
</p>

<hr>
<h2 id='nas'>Calculate Normalized Association Score</h2><span id='topic+nas'></span>

<h3>Description</h3>

<p>This functions quantifies the bias in a set of word embeddings by Caliskan et al (2017). In comparison to WEAT introduced in the same paper, this method is more suitable for continuous ground truth data. See Figure 1 and Figure 2 of the original paper. If possible, please use <code><a href="#topic+query">query()</a></code> instead.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>nas(w, S_words, A_words, B_words, verbose = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="nas_+3A_w">w</code></td>
<td>
<p>a numeric matrix of word embeddings, e.g. from <code><a href="#topic+read_word2vec">read_word2vec()</a></code></p>
</td></tr>
<tr><td><code id="nas_+3A_s_words">S_words</code></td>
<td>
<p>a character vector of the first set of target words. In an example of studying gender stereotype, it can include occupations such as programmer, engineer, scientists...</p>
</td></tr>
<tr><td><code id="nas_+3A_a_words">A_words</code></td>
<td>
<p>a character vector of the first set of attribute words. In an example of studying gender stereotype, it can include words such as man, male, he, his.</p>
</td></tr>
<tr><td><code id="nas_+3A_b_words">B_words</code></td>
<td>
<p>a character vector of the second set of attribute words. In an example of studying gender stereotype, it can include words such as woman, female, she, her.</p>
</td></tr>
<tr><td><code id="nas_+3A_verbose">verbose</code></td>
<td>
<p>logical, whether to display information</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with class <code>"nas"</code> containing the following components:
</p>

<ul>
<li> <p><code style="white-space: pre;">&#8288;$P&#8288;</code> a vector of normalized association score for every word in S
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$raw&#8288;</code> a list of raw results used for calculating normalized association scores
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$S_words&#8288;</code> the input S_words
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$A_words&#8288;</code> the input A_words
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$B_words&#8288;</code> the input B_words
</p>
</li></ul>



<h3>References</h3>

<p>Caliskan, A., Bryson, J. J., &amp; Narayanan, A. (2017). Semantics derived automatically from language corpora contain human-like biases. Science, 356(6334), 183-186. <a href="https://doi.org/10.1126/science.aal4230">doi:10.1126/science.aal4230</a>
</p>

<hr>
<h2 id='plot_bias'>Visualize the bias of words in S</h2><span id='topic+plot_bias'></span><span id='topic+plot.sweater'></span>

<h3>Description</h3>

<p>For <code>ect</code>, this function calls <code><a href="#topic+plot_ect">plot_ect()</a></code>. For other tests (except <code>weat</code>), this function plots the bias of words in <code>S</code> as a Cleveland Dot Plot. Plotting the result of <code>weat</code> is not supported.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_bias(x)

## S3 method for class 'sweater'
plot(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot_bias_+3A_x">x</code></td>
<td>
<p>an S3 object returned from mac, rnd, semaxis, nas or rnsb</p>
</td></tr>
<tr><td><code id="plot_bias_+3A_...">...</code></td>
<td>
<p>other parameters</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a plot
</p>

<hr>
<h2 id='plot_ect'>Plot an ECT result on a two-dimensional plane</h2><span id='topic+plot_ect'></span>

<h3>Description</h3>

<p>This functions plot the words in <code>S_words</code> on a 2D plane according to their association with the average vectors of <code>A_words</code> and <code>B_words</code>. A equality line is also added. Words along the equality line have less bias. Words located on the upper side of the equality line have a stronger association with <code>A_words</code> and vice versa.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>plot_ect(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="plot_ect_+3A_x">x</code></td>
<td>
<p>an ect object from the <a href="#topic+ect">ect</a> function.</p>
</td></tr>
<tr><td><code id="plot_ect_+3A_...">...</code></td>
<td>
<p>additional parameters to the underlying <code><a href="base.html#topic+plot">plot()</a></code> function</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a plot
</p>

<hr>
<h2 id='query'>A common interface for making query</h2><span id='topic+query'></span><span id='topic+print.sweater'></span>

<h3>Description</h3>

<p>This function makes a query based on the supplied parameters. The object can then be displayed by the S3 method <code><a href="#topic+print.sweater">print.sweater()</a></code> and plotted by <code><a href="#topic+plot.sweater">plot.sweater()</a></code>.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>query(
  w,
  S_words,
  T_words,
  A_words,
  B_words,
  method = "guess",
  verbose = FALSE,
  ...
)

## S3 method for class 'sweater'
print(x, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="query_+3A_w">w</code></td>
<td>
<p>a numeric matrix of word embeddings, e.g. from <code><a href="#topic+read_word2vec">read_word2vec()</a></code></p>
</td></tr>
<tr><td><code id="query_+3A_s_words">S_words</code></td>
<td>
<p>a character vector of the first set of target words. In an example of studying gender stereotype, it can include occupations such as programmer, engineer, scientists...</p>
</td></tr>
<tr><td><code id="query_+3A_t_words">T_words</code></td>
<td>
<p>a character vector of the second set of target words. In an example of studying gender stereotype, it can include occupations such as nurse, teacher, librarian...</p>
</td></tr>
<tr><td><code id="query_+3A_a_words">A_words</code></td>
<td>
<p>a character vector of the first set of attribute words. In an example of studying gender stereotype, it can include words such as man, male, he, his.</p>
</td></tr>
<tr><td><code id="query_+3A_b_words">B_words</code></td>
<td>
<p>a character vector of the second set of attribute words. In an example of studying gender stereotype, it can include words such as woman, female, she, her.</p>
</td></tr>
<tr><td><code id="query_+3A_method">method</code></td>
<td>
<p>string, the method to be used to make the query. Available options are: <code>weat</code>, <code>mac</code>, <code>nas</code>, <code>semaxis</code>, <code>rnsb</code>, <code>rnd</code>, <code>nas</code>, <code>ect</code> and <code>guess</code>. If &quot;guess&quot;, the function selects one of the following methods based on your provided wordsets.
</p>

<ul>
<li><p> S_words &amp; A_words -  &quot;mac&quot;
</p>
</li>
<li><p> S_words, A_words &amp; B_words -  &quot;rnd&quot;
</p>
</li>
<li><p> S_words, T_words, A_words &amp; B_words -  &quot;weat&quot;
</p>
</li></ul>
</td></tr>
<tr><td><code id="query_+3A_verbose">verbose</code></td>
<td>
<p>logical, whether to display information</p>
</td></tr>
<tr><td><code id="query_+3A_...">...</code></td>
<td>
<p>additional parameters for the underlying function
</p>

<ul>
<li> <p><code>l</code> for &quot;semaxis&quot;: an integer indicates the number of words to augment each word in A and B based on cosine , see An et al (2018). Default to 0 (no augmentation).
</p>
</li>
<li> <p><code>levels</code> for &quot;rnsb&quot;: levels of entries in a hierarchical dictionary that will be applied (see <code><a href="quanteda.html#topic+dfm_lookup">quanteda::dfm_lookup()</a></code>)
</p>
</li></ul>
</td></tr>
<tr><td><code id="query_+3A_x">x</code></td>
<td>
<p>a sweater S3 object</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a sweater S3 object
</p>


<h3>See Also</h3>

<p><code><a href="#topic+weat">weat()</a></code>, <code><a href="#topic+mac">mac()</a></code>, <code><a href="#topic+nas">nas()</a></code>, <code><a href="#topic+semaxis">semaxis()</a></code>, <code><a href="#topic+rnsb">rnsb()</a></code>, <code><a href="#topic+rnd">rnd()</a></code>, <code><a href="#topic+nas">nas()</a></code>, <code><a href="#topic+ect">ect()</a></code>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(googlenews)
S1 &lt;- c("janitor", "statistician", "midwife", "bailiff", "auctioneer",
"photographer", "geologist", "shoemaker", "athlete", "cashier", "dancer",
"housekeeper", "accountant", "physicist", "gardener", "dentist", "weaver",
"blacksmith", "psychologist", "supervisor", "mathematician", "surveyor",
"tailor", "designer", "economist", "mechanic", "laborer", "postmaster",
"broker", "chemist", "librarian", "attendant", "clerical", "musician",
"porter", "scientist", "carpenter", "sailor", "instructor", "sheriff",
"pilot", "inspector", "mason", "baker", "administrator", "architect",
"collector", "operator", "surgeon", "driver", "painter", "conductor",
"nurse", "cook", "engineer", "retired", "sales", "lawyer", "clergy",
"physician", "farmer", "clerk", "manager", "guard", "artist", "smith",
"official", "police", "doctor", "professor", "student", "judge",
"teacher", "author", "secretary", "soldier")
A1 &lt;- c("he", "son", "his", "him", "father", "man", "boy", "himself",
"male", "brother", "sons", "fathers", "men", "boys", "males", "brothers",
"uncle", "uncles", "nephew", "nephews")
B1 &lt;- c("she", "daughter", "hers", "her", "mother", "woman", "girl",
"herself", "female", "sister", "daughters", "mothers", "women", "girls",
"females", "sisters", "aunt", "aunts", "niece", "nieces")
garg_f1 &lt;- query(googlenews, S_words = S1, A_words = A1, B_words = B1)
garg_f1
plot(garg_f1)
</code></pre>

<hr>
<h2 id='read_word2vec'>A helper function for reading word2vec format</h2><span id='topic+read_word2vec'></span>

<h3>Description</h3>

<p>This function reads word2vec text format and return a dense matrix that can be used by this package.
The file can have or have not the &quot;verification line&quot;, i.e. the first line contains the dimensionality of the matrix. If the verification line exists, the function will check the returned matrix for correctness.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>read_word2vec(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="read_word2vec_+3A_x">x</code></td>
<td>
<p>path to your text file</p>
</td></tr>
</table>


<h3>Value</h3>

<p>a dense matrix
</p>

<hr>
<h2 id='rnd'>Relative Norm Distance</h2><span id='topic+rnd'></span>

<h3>Description</h3>

<p>This function calculate the relative norm distance (RND) of word embeddings. If possible, please use <code><a href="#topic+query">query()</a></code> instead.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rnd(w, S_words, A_words, B_words, verbose = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="rnd_+3A_w">w</code></td>
<td>
<p>a numeric matrix of word embeddings, e.g. from <code><a href="#topic+read_word2vec">read_word2vec()</a></code></p>
</td></tr>
<tr><td><code id="rnd_+3A_s_words">S_words</code></td>
<td>
<p>a character vector of the first set of target words. In an example of studying gender stereotype, it can include occupations such as programmer, engineer, scientists...</p>
</td></tr>
<tr><td><code id="rnd_+3A_a_words">A_words</code></td>
<td>
<p>a character vector of the first set of attribute words. In an example of studying gender stereotype, it can include words such as man, male, he, his.</p>
</td></tr>
<tr><td><code id="rnd_+3A_b_words">B_words</code></td>
<td>
<p>a character vector of the second set of attribute words. In an example of studying gender stereotype, it can include words such as woman, female, she, her.</p>
</td></tr>
<tr><td><code id="rnd_+3A_verbose">verbose</code></td>
<td>
<p>logical, whether to display information</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with class <code>"rnd"</code> containing the following components:
</p>

<ul>
<li> <p><code style="white-space: pre;">&#8288;$norm_diff&#8288;</code> a vector of relative norm distances for every word in S_words
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$S_words&#8288;</code> the input S_words
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$A_words&#8288;</code> the input A_words
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$B_words&#8288;</code> the input B_words
<code><a href="#topic+rnd_es">rnd_es()</a></code> can be used to obtain the effect size of the test.
</p>
</li></ul>



<h3>References</h3>

<p>Garg, N., Schiebinger, L., Jurafsky, D., &amp; Zou, J. (2018). Word embeddings quantify 100 years of gender and ethnic stereotypes. Proceedings of the National Academy of Sciences, 115(16), E3635-E3644. <a href="https://doi.org/10.1073/pnas.1720347115">doi:10.1073/pnas.1720347115</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(googlenews)
S1 &lt;- c("janitor", "statistician", "midwife", "bailiff", "auctioneer",
"photographer", "geologist", "shoemaker", "athlete", "cashier", "dancer",
"housekeeper", "accountant", "physicist", "gardener", "dentist", "weaver",
"blacksmith", "psychologist", "supervisor", "mathematician", "surveyor",
"tailor", "designer", "economist", "mechanic", "laborer", "postmaster",
"broker", "chemist", "librarian", "attendant", "clerical", "musician",
"porter", "scientist", "carpenter", "sailor", "instructor", "sheriff",
"pilot", "inspector", "mason", "baker", "administrator", "architect",
"collector", "operator", "surgeon", "driver", "painter", "conductor",
"nurse", "cook", "engineer", "retired", "sales", "lawyer", "clergy",
"physician", "farmer", "clerk", "manager", "guard", "artist", "smith",
"official", "police", "doctor", "professor", "student", "judge",
"teacher", "author", "secretary", "soldier")
A1 &lt;- c("he", "son", "his", "him", "father", "man", "boy", "himself",
"male", "brother", "sons", "fathers", "men", "boys", "males", "brothers",
"uncle", "uncles", "nephew", "nephews")
B1 &lt;- c("she", "daughter", "hers", "her", "mother", "woman", "girl",
"herself", "female", "sister", "daughters", "mothers", "women", "girls",
"females", "sisters", "aunt", "aunts", "niece", "nieces")
garg_f1 &lt;- rnd(googlenews, S1, A1, B1)
plot_bias(garg_f1)
</code></pre>

<hr>
<h2 id='rnd_es'>Calculation of sum of all relative norm distances</h2><span id='topic+rnd_es'></span>

<h3>Description</h3>

<p>This function calculates the sum of all relative norm distances from the relative norm distance test. If possible, please use <code><a href="#topic+calculate_es">calculate_es()</a></code> instead.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rnd_es(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="rnd_es_+3A_x">x</code></td>
<td>
<p>an object from the function <a href="#topic+rnd">rnd</a></p>
</td></tr>
</table>


<h3>Value</h3>

<p>Sum of all relative norm distances
</p>


<h3>References</h3>

<p>Garg, N., Schiebinger, L., Jurafsky, D., &amp; Zou, J. (2018). Word embeddings quantify 100 years of gender and ethnic stereotypes. Proceedings of the National Academy of Sciences, 115(16), E3635-E3644. <a href="https://doi.org/10.1073/pnas.1720347115">doi:10.1073/pnas.1720347115</a>
</p>

<hr>
<h2 id='rnsb'>Relative Negative Sentiment Bias</h2><span id='topic+rnsb'></span>

<h3>Description</h3>

<p>This function estimate the Relative Negative Sentiment Bias (RNSB) of word embeddings (Sweeney &amp; Najafian, 2 019). If possible, please use <code><a href="#topic+query">query()</a></code> instead.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rnsb(w, S_words, A_words, B_words, levels = 1, verbose = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="rnsb_+3A_w">w</code></td>
<td>
<p>a numeric matrix of word embeddings, e.g. from <code><a href="#topic+read_word2vec">read_word2vec()</a></code></p>
</td></tr>
<tr><td><code id="rnsb_+3A_s_words">S_words</code></td>
<td>
<p>a character vector of the first set of target words. In an example of studying gender stereotype, it can include occupations such as programmer, engineer, scientists...</p>
</td></tr>
<tr><td><code id="rnsb_+3A_a_words">A_words</code></td>
<td>
<p>a character vector of the first set of attribute words. In an example of studying gender stereotype, it can include words such as man, male, he, his.</p>
</td></tr>
<tr><td><code id="rnsb_+3A_b_words">B_words</code></td>
<td>
<p>a character vector of the second set of attribute words. In an example of studying gender stereotype, it can include words such as woman, female, she, her.</p>
</td></tr>
<tr><td><code id="rnsb_+3A_levels">levels</code></td>
<td>
<p>levels of entries in a hierarchical dictionary that will be applied (see <code><a href="quanteda.html#topic+dfm_lookup">quanteda::dfm_lookup()</a></code>)</p>
</td></tr>
<tr><td><code id="rnsb_+3A_verbose">verbose</code></td>
<td>
<p>logical, whether to display information</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with class <code>"rnsb"</code> containing the following components:
</p>

<ul>
<li> <p><code style="white-space: pre;">&#8288;$classifer&#8288;</code>  a logistic regression model with L2 regularization trained with LiblineaR
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$A_words&#8288;</code> the input A_words
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$B_words&#8288;</code> the input B_words
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$S_words&#8288;</code> the input S_words
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$P&#8288;</code> the predicted negative sentiment probabilities
<code><a href="#topic+rnsb_es">rnsb_es()</a></code> can be used to obtain the effect size of the test.
</p>
</li></ul>



<h3>References</h3>

<p>Sweeney, C., &amp; Najafian, M. (2019, July). <a href="https://aclanthology.org/P19-1162/">A transparent framework for evaluating unintended demographic bias in word embeddings.</a> In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (pp. 1662-1667).
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(googlenews)
S1 &lt;- c("janitor", "statistician", "midwife", "bailiff", "auctioneer",
"photographer", "geologist", "shoemaker", "athlete", "cashier", "dancer",
"housekeeper", "accountant", "physicist", "gardener", "dentist", "weaver",
"blacksmith", "psychologist", "supervisor", "mathematician", "surveyor",
"tailor", "designer", "economist", "mechanic", "laborer", "postmaster",
"broker", "chemist", "librarian", "attendant", "clerical", "musician",
"porter", "scientist", "carpenter", "sailor", "instructor", "sheriff",
"pilot", "inspector", "mason", "baker", "administrator", "architect",
"collector", "operator", "surgeon", "driver", "painter", "conductor",
"nurse", "cook", "engineer", "retired", "sales", "lawyer", "clergy",
"physician", "farmer", "clerk", "manager", "guard", "artist", "smith",
"official", "police", "doctor", "professor", "student", "judge",
"teacher", "author", "secretary", "soldier")
A1 &lt;- c("he", "son", "his", "him", "father", "man", "boy", "himself",
"male", "brother", "sons", "fathers", "men", "boys", "males", "brothers",
"uncle", "uncles", "nephew", "nephews")
B1 &lt;- c("she", "daughter", "hers", "her", "mother", "woman", "girl",
"herself", "female", "sister", "daughters", "mothers", "women", "girls",
"females", "sisters", "aunt", "aunts", "niece", "nieces")
garg_f1 &lt;- rnsb(googlenews, S1, A1, B1)
plot_bias(garg_f1)
</code></pre>

<hr>
<h2 id='rnsb_es'>Calculation the Kullback-Leibler divergence</h2><span id='topic+rnsb_es'></span>

<h3>Description</h3>

<p>This function calculates the Kullback-Leibler divergence of the predicted negative probabilities, P, from the uniform distribution. If possible, please use <code><a href="#topic+calculate_es">calculate_es()</a></code> instead.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>rnsb_es(x)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="rnsb_es_+3A_x">x</code></td>
<td>
<p>an rnsb object from the <a href="#topic+rnsb">rnsb</a> function.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the Kullback-Leibler divergence.
</p>


<h3>References</h3>

<p>Sweeney, C., &amp; Najafian, M. (2019, July). <a href="https://aclanthology.org/P19-1162/">A transparent framework for evaluating unintended demographic bias in word embeddings.</a> In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (pp. 1662-1667).
</p>

<hr>
<h2 id='semaxis'>Characterise word semantics using the SemAxis framework</h2><span id='topic+semaxis'></span>

<h3>Description</h3>

<p>This function calculates the axis and the score using the SemAxis framework proposed in An et al (2018). If possible, please use <code><a href="#topic+query">query()</a></code> instead.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>semaxis(w, S_words, A_words, B_words, l = 0, verbose = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="semaxis_+3A_w">w</code></td>
<td>
<p>a numeric matrix of word embeddings, e.g. from <code><a href="#topic+read_word2vec">read_word2vec()</a></code></p>
</td></tr>
<tr><td><code id="semaxis_+3A_s_words">S_words</code></td>
<td>
<p>a character vector of the first set of target words. In an example of studying gender stereotype, it can include occupations such as programmer, engineer, scientists...</p>
</td></tr>
<tr><td><code id="semaxis_+3A_a_words">A_words</code></td>
<td>
<p>a character vector of the first set of attribute words. In an example of studying gender stereotype, it can include words such as man, male, he, his.</p>
</td></tr>
<tr><td><code id="semaxis_+3A_b_words">B_words</code></td>
<td>
<p>a character vector of the second set of attribute words. In an example of studying gender stereotype, it can include words such as woman, female, she, her.</p>
</td></tr>
<tr><td><code id="semaxis_+3A_l">l</code></td>
<td>
<p>an integer indicates the number of words to augment each word in A and B based on cosine , see An et al (2018). Default to 0 (no augmentation).</p>
</td></tr>
<tr><td><code id="semaxis_+3A_verbose">verbose</code></td>
<td>
<p>logical, whether to display information</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with class <code>"semaxis"</code> containing the following components:
</p>

<ul>
<li> <p><code style="white-space: pre;">&#8288;$P&#8288;</code> for each of words in S, the score according to SemAxis
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$V&#8288;</code> the semantic axis vector
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$S_words&#8288;</code> the input S_words
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$A_words&#8288;</code> the input A_words
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$B_words&#8288;</code> the input B_words
</p>
</li></ul>



<h3>References</h3>

<p>An, J., Kwak, H., &amp; Ahn, Y. Y. (2018). <a href="https://arxiv.org/abs/1806.05521">SemAxis: A lightweight framework to characterize domain-specific word semantics beyond sentiment.</a> arXiv preprint arXiv:1806.05521.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>data(glove_math)
S1 &lt;- c("math", "algebra", "geometry", "calculus", "equations",
"computation", "numbers", "addition")
A1 &lt;- c("male", "man", "boy", "brother", "he", "him", "his", "son")
B1 &lt;- c("female", "woman", "girl", "sister", "she", "her", "hers", "daughter")
semaxis(glove_math, S1, A1, B1, l = 0)$P
</code></pre>

<hr>
<h2 id='small_reddit'>A subset of the pretrained word2vec word vectors on Reddit</h2><span id='topic+small_reddit'></span>

<h3>Description</h3>

<p>This is a subset of the pretrained word2vec word vectors on Reddit provided by An et al. (2018). With this dataset, you can try with the &quot;l&quot; parameter of <code><a href="#topic+semaxis">semaxis()</a></code> up to 10.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>small_reddit
</code></pre>


<h3>Format</h3>

<p>An object of class <code>matrix</code> (inherits from <code>array</code>) with 106 rows and 300 columns.
</p>


<h3>References</h3>

<p>An, J., Kwak, H., &amp; Ahn, Y. Y. (2018). <a href="https://arxiv.org/abs/1806.05521">SemAxis: A lightweight framework to characterize domain-specific word semantics beyond sentiment.</a> arXiv preprint arXiv:1806.05521.
</p>

<hr>
<h2 id='weat'>Speedy Word Embedding Association Test</h2><span id='topic+weat'></span>

<h3>Description</h3>

<p>This functions test the bias in a set of word embeddings using the method by Caliskan et al (2017). If possible, please use <code><a href="#topic+query">query()</a></code> instead.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>weat(w, S_words, T_words, A_words, B_words, verbose = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="weat_+3A_w">w</code></td>
<td>
<p>a numeric matrix of word embeddings, e.g. from <code><a href="#topic+read_word2vec">read_word2vec()</a></code></p>
</td></tr>
<tr><td><code id="weat_+3A_s_words">S_words</code></td>
<td>
<p>a character vector of the first set of target words. In an example of studying gender stereotype, it can include occupations such as programmer, engineer, scientists...</p>
</td></tr>
<tr><td><code id="weat_+3A_t_words">T_words</code></td>
<td>
<p>a character vector of the second set of target words. In an example of studying gender stereotype, it can include occupations such as nurse, teacher, librarian...</p>
</td></tr>
<tr><td><code id="weat_+3A_a_words">A_words</code></td>
<td>
<p>a character vector of the first set of attribute words. In an example of studying gender stereotype, it can include words such as man, male, he, his.</p>
</td></tr>
<tr><td><code id="weat_+3A_b_words">B_words</code></td>
<td>
<p>a character vector of the second set of attribute words. In an example of studying gender stereotype, it can include words such as woman, female, she, her.</p>
</td></tr>
<tr><td><code id="weat_+3A_verbose">verbose</code></td>
<td>
<p>logical, whether to display information</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with class <code>"weat"</code> containing the following components:
</p>

<ul>
<li> <p><code style="white-space: pre;">&#8288;$S_diff&#8288;</code> for each of words in S_words, mean of the mean differences in cosine similarity between words in A_words and words in B_words
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$T_diff&#8288;</code> for each of words in T_words, mean of the mean differences in cosine similarity between words in A_words and words in B_words
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$S_words&#8288;</code> the input S_words
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$T_words&#8288;</code> the input T_words
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$A_words&#8288;</code> the input A_words
</p>
</li>
<li> <p><code style="white-space: pre;">&#8288;$B_words&#8288;</code> the input B_words
<code><a href="#topic+weat_es">weat_es()</a></code> can be used to obtain the effect size of the test; <code><a href="#topic+weat_resampling">weat_resampling()</a></code> for a test of significance.
</p>
</li></ul>



<h3>References</h3>

<p>Caliskan, A., Bryson, J. J., &amp; Narayanan, A. (2017). Semantics derived automatically from language corpora contain human-like biases. Science, 356(6334), 183-186. <a href="https://doi.org/10.1126/science.aal4230">doi:10.1126/science.aal4230</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Reproduce the number in Caliskan et al. (2017) - Table 1, "Math vs. Arts"
data(glove_math)
S1 &lt;- c("math", "algebra", "geometry", "calculus", "equations",
"computation", "numbers", "addition")
T1 &lt;- c("poetry", "art", "dance", "literature", "novel", "symphony", "drama", "sculpture")
A1 &lt;- c("male", "man", "boy", "brother", "he", "him", "his", "son")
B1 &lt;- c("female", "woman", "girl", "sister", "she", "her", "hers", "daughter")
sw &lt;- weat(glove_math, S1, T1, A1, B1)
weat_es(sw)
</code></pre>

<hr>
<h2 id='weat_es'>Calculation of WEAT effect size</h2><span id='topic+weat_es'></span>

<h3>Description</h3>

<p>This function calculates the effect size from a sweater object. The original implementation in Caliskan et al. (2017) assumes the numbers of words in S and in T must be equal. The current implementation eases this assumption by adjusting the variance with the difference in sample sizes. This adjustment works not so great when the length of S and T are short. It is also possible to convert the Cohen's d to Pearson's correlation coefficient (r). If possible, please use <code><a href="#topic+calculate_es">calculate_es()</a></code> instead.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>weat_es(x, standardize = TRUE, r = FALSE)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="weat_es_+3A_x">x</code></td>
<td>
<p>an object from the <a href="#topic+weat">weat</a> function.</p>
</td></tr>
<tr><td><code id="weat_es_+3A_standardize">standardize</code></td>
<td>
<p>a boolean to denote whether to correct the difference by the standard division. The standardized version can be interpreted the same way as Cohen's d.</p>
</td></tr>
<tr><td><code id="weat_es_+3A_r">r</code></td>
<td>
<p>a boolean to denote whether convert the effect size to biserial correlation coefficient.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>the effect size of the query
</p>


<h3>References</h3>

<p>Caliskan, A., Bryson, J. J., &amp; Narayanan, A. (2017). Semantics derived automatically from language corpora contain human-like biases. Science, 356(6334), 183-186. <a href="https://doi.org/10.1126/science.aal4230">doi:10.1126/science.aal4230</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Reproduce the number in Caliskan et al. (2017) - Table 1, "Math vs. Arts"
data(glove_math)
S1 &lt;- c("math", "algebra", "geometry", "calculus", "equations",
"computation", "numbers", "addition")
T1 &lt;- c("poetry", "art", "dance", "literature", "novel", "symphony", "drama", "sculpture")
A1 &lt;- c("male", "man", "boy", "brother", "he", "him", "his", "son")
B1 &lt;- c("female", "woman", "girl", "sister", "she", "her", "hers", "daughter")
sw &lt;- weat(glove_math, S1, T1, A1, B1)
weat_es(sw)
</code></pre>

<hr>
<h2 id='weat_exact'>Test of significance for WEAT</h2><span id='topic+weat_exact'></span><span id='topic+weat_resampling'></span>

<h3>Description</h3>

<p>This function conducts the test of significance for WEAT as described in Caliskan et al. (2017). The exact test (proposed in Caliskan et al.) takes an unreasonably long time, if the total number of words in S and T is larger than 10. The resampling test is an approximation of the exact test.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>weat_exact(x)

weat_resampling(x, n_resampling = 9999)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="weat_exact_+3A_x">x</code></td>
<td>
<p>an object from the <a href="#topic+weat">weat</a> function.</p>
</td></tr>
<tr><td><code id="weat_exact_+3A_n_resampling">n_resampling</code></td>
<td>
<p>an integer specifying the number of replicates used to estimate the exact test</p>
</td></tr>
</table>


<h3>Value</h3>

<p>A list with class <code>"htest"</code>
</p>


<h3>References</h3>

<p>Caliskan, A., Bryson, J. J., &amp; Narayanan, A. (2017). Semantics derived automatically from language corpora contain human-like biases. Science, 356(6334), 183-186. <a href="https://doi.org/10.1126/science.aal4230">doi:10.1126/science.aal4230</a>
</p>


<h3>Examples</h3>

<pre><code class='language-R'># Reproduce the number in Caliskan et al. (2017) - Table 1, "Math vs. Arts"
data(glove_math)
S1 &lt;- c("math", "algebra", "geometry", "calculus", "equations",
"computation", "numbers", "addition")
T1 &lt;- c("poetry", "art", "dance", "literature", "novel", "symphony", "drama", "sculpture")
A1 &lt;- c("male", "man", "boy", "brother", "he", "him", "his", "son")
B1 &lt;- c("female", "woman", "girl", "sister", "she", "her", "hers", "daughter")
sw &lt;- weat(glove_math, S1, T1, A1, B1)
weat_resampling(sw)
</code></pre>

</main>

</div>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/prism.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-r.min.js"></script>
</body></html>
